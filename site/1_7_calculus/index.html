<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/1_7_calculus/">
      
      
        <link rel="prev" href="../1_6_projections/">
      
      
        <link rel="next" href="../0a%200intro/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>7. Calculus Essentials - Gradients, Jacobians, and Hessians - Convex Optimization</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Convex Optimization" class="md-header__button md-logo" aria-label="Convex Optimization" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Convex Optimization
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              7. Calculus Essentials -  Gradients, Jacobians, and Hessians
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Convex Optimization" class="md-nav__button md-logo" aria-label="Convex Optimization" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Convex Optimization
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Basics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Basics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_0_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_1_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Vector Spaces and Linear Mappings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_2_innerproducts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Inner Product Spaces and Orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_3_norms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Norms and Metric Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_4_linearoperator/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Linear Operators, Spectral Norms, and SVD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_5_eigenvalues/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Eigenvalues, Positive Definiteness, and Quadratic Forms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_6_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Projections onto Subspaces and Convex Sets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    7. Calculus Essentials -  Gradients, Jacobians, and Hessians
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%200intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%201la_foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra Review
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%202innerproducts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inner Products
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%203norms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Norms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%204operators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%205eigenvalues/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Eigenvalues and Eigenvectors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%206projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%207calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculus Review
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%208convexity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Smoothness and Convexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%209Convex%20Sets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Sets and Related Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%2010Convex%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Analysis
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Convex Analysis
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%201Separation%20Theorems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Separation Theorems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%202Support%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Support Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%203Convex%20Conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Conjugates
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%204Subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%205Fenchel%20Duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fenchel Duality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%206Moreau%20and%20Proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Moreau Envelopes &amp; Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%207Indicator%20and%20Distance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator &amp; Distance Functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4">
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Identifying Convex Problems and Tractability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Identifying Convex Problems and Tractability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%201Convex%20Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%202Optimality%20Conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimality Conditions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%203Tractable%20Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tractable Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5">
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Optimization Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e%20Optimization%20Algos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e1%20Gradient%20Descent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e2%20subgradient%20method/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Subgradient Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e3%20accelerated%20gs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerated GD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0f%20Convergence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convergence Properties
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0g%20Proximal%20and%20Projected%20Gradient%20Descent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections &amp; Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0g1%20proximal%20ga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Proximal Gradient Algorithm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0h%20lasso/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LASSO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0k%20mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mirror Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0m%20sgd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stochastic Gradient Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0n%20newtons/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Newton's Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0o%20quasi_n/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quasi-Newton Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0q%20interior/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interior Point Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0h AdvancedAlgos.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6">
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Common Convex Optimization Problems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Common Convex Optimization Problems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Common Convex Optimization Problems.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1a%20LP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1c%20Least%20Square/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Least Squares
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1b%20QP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1d%20QCQP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QCQP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1e%20SOCP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SOCP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1f%20GeometricInterpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Geometric Interpretation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1g%20GP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GP
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7">
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Duality &amp; Regularization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Duality &amp; Regularization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2a%20Duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Duality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3a%20Huber/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Huber
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3b%20Penalty%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Penalty Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3c%20Regularized/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularized Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3d%20Robust%20Approximation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robust Approximation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3e%20MLE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MLE
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8">
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Discrimination &amp; Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Discrimination &amp; Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4a%20Linear%20Discrimination/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Discrimination
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6a%20First%20Order%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    First Order Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9">
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Miscellaneous
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Miscellaneous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7a%20pareto%20optimal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pareto Optimality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Example
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/1_7_calculus.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/1_7_calculus.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


  <h1>7. Calculus Essentials -  Gradients, Jacobians, and Hessians</h1>

<p>Calculus provides the analytical tools to optimize functions: it describes how functions change when we tweak the input. In convex optimization we often assume differentiability (at least for the objective, if not constraints), so we rely on gradients and Hessians to characterize optimal points and design algorithms.</p>
<p><strong>Gradient and directional derivative:</strong> Let <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> be differentiable. The gradient <span class="arithmatex">\(\nabla f(x)\)</span> is the vector of partial derivatives</p>
<div class="arithmatex">\[
\nabla f(x) =
\begin{bmatrix}
\dfrac{\partial f}{\partial x_1} \\
\vdots \\
\dfrac{\partial f}{\partial x_n}
\end{bmatrix}
\]</div>
<p>so that for a small step <span class="arithmatex">\(h\)</span>, <span class="arithmatex">\(f(x+h) \approx f(x) + \langle \nabla f(x),h\rangle\)</span>. This linear approximation is the first-order Taylor expansion. The gradient <span class="arithmatex">\(\nabla f(x)\)</span> points in the direction of steepest increase of <span class="arithmatex">\(f\)</span>; <span class="arithmatex">\(-\nabla f(x)\)</span> is the direction of steepest decrease. Specifically, <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to level sets of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span>. In optimization, setting <span class="arithmatex">\(\nabla f(x) = 0\)</span> finds stationary points (candidates for optima). Gradient descent uses the update <span class="arithmatex">\(x_{k+1} = x_k - \alpha \nabla f(x_k)\)</span>, taking a small step opposite the gradient to reduce <span class="arithmatex">\(f\)</span>. The magnitude <span class="arithmatex">\(|\nabla f(x)|\)</span> indicates how steep <span class="arithmatex">\(f\)</span> is; when <span class="arithmatex">\(\nabla f(\hat{x})=0\)</span>, the function is flat to first order at <span class="arithmatex">\(\hat{x}\)</span>. For convex <span class="arithmatex">\(f\)</span>, any stationary point is a global minimum.</p>
<p>The <strong>directional derivative</strong> in direction <span class="arithmatex">\(u\)</span> is <span class="arithmatex">\(D_u f(x) = \lim_{t\to0} \frac{f(x+tu)-f(x)}{t} = \langle \nabla f(x), u\rangle\)</span>. This shows how the gradient inner product with <span class="arithmatex">\(u\)</span> gives the instantaneous rate of change of <span class="arithmatex">\(f\)</span> along <span class="arithmatex">\(u\)</span>. In particular, <span class="arithmatex">\(D_u f(x)\)</span> is maximized when <span class="arithmatex">\(u\)</span> points along <span class="arithmatex">\(\nabla f(x)\)</span> (steepest ascent) and minimized when <span class="arithmatex">\(u\)</span> is opposite.</p>
<p><strong>Jacobian for vector-valued mappings:</strong> If <span class="arithmatex">\(g: \mathbb{R}^n \to \mathbb{R}^m\)</span> (with <span class="arithmatex">\(m&gt;1\)</span> outputs), the Jacobian matrix <span class="arithmatex">\(J_g(x)\)</span> is the <span class="arithmatex">\(m \times n\)</span> matrix of partial derivatives: its <span class="arithmatex">\((i,j)\)</span> entry is <span class="arithmatex">\(\partial g_i/\partial x_j\)</span>. The <span class="arithmatex">\(i\)</span> th row is <span class="arithmatex">\((\nabla g_i(x))^T\)</span>. For example, if <span class="arithmatex">\(g(x) = Ax\)</span> (linear map), then <span class="arithmatex">\(J_g(x)=A\)</span> constant. If <span class="arithmatex">\(g(x) = (f(x), h(x))\)</span> combines two scalars, the Jacobian has two rows: <span class="arithmatex">\(\nabla f(x)^T\)</span> and <span class="arithmatex">\(\nabla h(x)^T\)</span>. The Jacobian represents the best linear approximation of <span class="arithmatex">\(g\)</span> near <span class="arithmatex">\(x\)</span>: <span class="arithmatex">\(g(x+h) \approx g(x) + J_g(x),h\)</span>. When <span class="arithmatex">\(m=n\)</span> and <span class="arithmatex">\(J_g(x)\)</span> is invertible, <span class="arithmatex">\(g\)</span> is locally invertible (by the Inverse Function Theorem) and the Jacobian’s determinant indicates how volumes scale under <span class="arithmatex">\(g\)</span>. In optimization, Jacobians appear in constraints: if we have vector constraints <span class="arithmatex">\(g(x)=0\)</span>, <span class="arithmatex">\(J_g(x)\)</span> is the constraint Jacobian matrix used in KKT conditions. They also appear when optimizing compositions of functions (via chain rule, below). In machine learning, the Jacobian of a network’s layers is used to propagate gradients backward (backpropagation is an application of chain rule on a composed function).</p>
<p><strong>Chain rule:</strong> If <span class="arithmatex">\(h(x) = f(g(x))\)</span> is a composition <span class="arithmatex">\(\mathbb{R}^n \xrightarrow{g} \mathbb{R}^m \xrightarrow{f} \mathbb{R}\)</span>, then by the chain rule the gradient is</p>
<div class="arithmatex">\[
\nabla h(x) = J_g(x)^\top \, \nabla f(g(x))
\]</div>
<p>In coordinates, <span class="arithmatex">\(\frac{\partial h}{\partial x_j} = \sum_{i=1}^m \frac{\partial f}{\partial y_i}(g(x)) \frac{\partial g_i}{\partial x_j}(x)\)</span>. This general rule shows that to compute the gradient of a nested function, we multiply the Jacobians going backward. For example, if <span class="arithmatex">\(f(y)\)</span> is scalar and <span class="arithmatex">\(g(x)\)</span> yields features, <span class="arithmatex">\(\nabla_x f(g(x)) = J_g(x)^T \nabla f(y)|_{y=g(x)}\)</span>. This is exactly how backpropagation in neural networks works: the gradient w.r.t. inputs is obtained by propagating the output error gradient through each layer’s Jacobian (which are often simple elementwise operations or linear weight matrices). Thus, the chain rule is fundamental for efficient gradient calculations. In convex optimization, if <span class="arithmatex">\(g(x)\)</span> is an affine function and <span class="arithmatex">\(f\)</span> is convex and differentiable, then <span class="arithmatex">\(h(x)=f(g(x))\)</span> is convex and <span class="arithmatex">\(\nabla h(x) = J_g(x)^T \nabla f(g(x))\)</span> provides the needed gradient for algorithms.</p>
<p>Hessian and second-order derivatives: The Hessian of <span class="arithmatex">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> is the <span class="arithmatex">\(n \times n\)</span> symmetric matrix of second partials, <span class="arithmatex">\(\nabla^2 f(x)\)</span>, where <span class="arithmatex">\((\nabla^2 f(x))_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}\)</span>. The Hessian matrix captures the quadratic curvature of <span class="arithmatex">\(f\)</span> around <span class="arithmatex">\(x\)</span>. Specifically, the second-order Taylor expansion is</p>
<p>𝑓
(
𝑥
+
ℎ
)
≈
𝑓
(
𝑥
)
+
⟨
∇
𝑓
(
𝑥
)
,
ℎ
⟩
+
1
2
ℎ
𝑇
(
∇
2
𝑓
(
𝑥
)
)
 
ℎ
.
f(x+h)≈f(x)+⟨∇f(x),h⟩+
2
1
    ​</p>
<p>h
T
(∇
2
f(x))h.</p>
<p>The quadratic term <span class="arithmatex">\(h^T \nabla^2 f(x) h / 2\)</span> approximates how the gradient itself changes with <span class="arithmatex">\(h\)</span>. Properties of Hessian: if <span class="arithmatex">\(\nabla^2 f(x) \succeq 0\)</span> (PSD) for all <span class="arithmatex">\(x\)</span> in a region, <span class="arithmatex">\(f\)</span> is convex on that region. If <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> (PD) for all <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(f\)</span> is strictly convex (one minimizer). On the other hand, if <span class="arithmatex">\(\nabla^2 f(x)\)</span> has a negative eigenvalue, <span class="arithmatex">\(f\)</span> is locally concave in that direction (not convex). Thus Hessian definiteness is a local convexity test. Many convex functions have constant Hessians (e.g. <span class="arithmatex">\(f(x)=\frac{1}{2}x^TQx\)</span> has <span class="arithmatex">\(\nabla^2 f = Q\)</span>).</p>
<p>In optimization algorithms, Hessians are used in Newton’s method, which iteratively updates</p>
<p>𝑥
𝑘
+
1
=
𝑥
𝑘
−
[
∇
2
𝑓
(
𝑥
𝑘
)
]
−
1
 
∇
𝑓
(
𝑥
𝑘
)
.
x
k+1
    ​</p>
<p>=x
k
    ​</p>
<p>−[∇
2
f(x
k
    ​</p>
<p>)]
−1
∇f(x
k
    ​</p>
<p>).</p>
<p>This uses the Hessian inverse as a linear approximation to the curvature, jumping to where the gradient would be zero if the quadratic model were exact. Newton’s method converges in a few iterations for quadratic objectives and generally superlinearly for well-behaved convex functions, but it requires solving linear systems involving <span class="arithmatex">\(\nabla^2 f(x)\)</span>, which can be expensive for large <span class="arithmatex">\(n\)</span>. Quasi-Newton methods (like BFGS) build approximations to the Hessian on the fly. Regardless, understanding Hessian is crucial for high-dimensional convex optimization: it tells us how sensitive the gradient is to changes in <span class="arithmatex">\(x\)</span>, which directly affects step sizes and convergence.</p>
<p>Example – quadratic function: <span class="arithmatex">\(f(x) = \frac{1}{2}x^TQx - b^T x\)</span>. Here <span class="arithmatex">\(\nabla f(x) = Qx - b\)</span> (linear), and <span class="arithmatex">\(\nabla^2 f(x) = Q\)</span>. Solving <span class="arithmatex">\(\nabla f=0\)</span> yields <span class="arithmatex">\(Qx=b\)</span>, so if <span class="arithmatex">\(Q \succ 0\)</span> the unique minimizer is <span class="arithmatex">\(x^* = Q^{-1}b\)</span>. The Hessian being <span class="arithmatex">\(Q \succ 0\)</span> confirms convexity. If <span class="arithmatex">\(Q\)</span> has large eigenvalues, gradient <span class="arithmatex">\(Qx - b\)</span> changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of <span class="arithmatex">\(Q\)</span> controls difficulty of minimizing <span class="arithmatex">\(f\)</span>.</p>
<p>Optimality conditions (unconstrained): For an unconstrained differentiable problem <span class="arithmatex">\(\min_x f(x)\)</span>, the first-order necessary condition is <span class="arithmatex">\(\nabla f(x^) = 0\)</span>. If <span class="arithmatex">\(f\)</span> is convex, this is also sufficient: any <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x)=0\)</span> is a global minimizer. If <span class="arithmatex">\(f\)</span> is twice differentiable, second-order conditions say: <span class="arithmatex">\(\nabla f(x^)=0\)</span> and <span class="arithmatex">\(\nabla^2 f(x^*) \succeq 0\)</span> for a local minimum. In convex problems the Hessian condition is automatically satisfied everywhere (since convex <span class="arithmatex">\(f\)</span> has PSD Hessian throughout), so checking <span class="arithmatex">\(\nabla f(x)=0\)</span> is enough.</p>
<p>Gradient Lipschitz continuity: A concept often used in convergence analysis is Lipschitz continuity of the gradient. If there exists <span class="arithmatex">\(L\)</span> such that <span class="arithmatex">\(|\nabla f(x) - \nabla f(y)| \le L |x-y|\)</span> for all <span class="arithmatex">\(x,y\)</span>, we say the gradient is <span class="arithmatex">\(L\)</span>-Lipschitz (or <span class="arithmatex">\(f\)</span> is <span class="arithmatex">\(L\)</span>-smooth). <span class="arithmatex">\(L\)</span> is essentially an upper bound on the Hessian eigenvalues (for <span class="arithmatex">\(\ell_2\)</span> norm): <span class="arithmatex">\(L \ge \lambda_{\max}(\nabla^2 f(x))\)</span> for all <span class="arithmatex">\(x\)</span>. Smoothness is important because it ensures gradient descent with step <span class="arithmatex">\(\alpha = 1/L\)</span> converges, and it gives a bound <span class="arithmatex">\(f(x_{k+1}) \le f(x_k) - \frac{1}{2L}|\nabla f(x_k)|^2\)</span> (so the function value decreases at least proportionally to the squared gradient norm). Many convex functions in optimization are <span class="arithmatex">\(L\)</span>-smooth (e.g. quadratic forms with <span class="arithmatex">\(\lambda_{\max}(Q)=L\)</span>). Smoothness together with strong convexity (defined shortly) yields linear convergence rates for gradient descent.</p>
<p>Strong convexity: A differentiable function <span class="arithmatex">\(f\)</span> is <span class="arithmatex">\(\mu\)</span>-strongly convex if <span class="arithmatex">\(f(y) \ge f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2}|y-x|^2\)</span> for all <span class="arithmatex">\(x,y\)</span>. Equivalently, <span class="arithmatex">\(f(x) - \frac{\mu}{2}|x|^2\)</span> is convex, which implies <span class="arithmatex">\(\nabla^2 f(x) \succeq \mu I\)</span> (Hessian bounded below by <span class="arithmatex">\(\mu\)</span>) when <span class="arithmatex">\(f\)</span> is twice differentiable. Strong convexity means <span class="arithmatex">\(f\)</span> has a quadratic curvature of at least <span class="arithmatex">\(\mu\)</span> – it grows at least as fast as a parabola. Strongly convex functions have unique minimizers (the bowl can’t flatten out). They also yield much faster convergence: for <span class="arithmatex">\(\mu\)</span>-strongly convex and <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(f\)</span>, gradient descent with <span class="arithmatex">\(\alpha=1/L\)</span> converges like <span class="arithmatex">\((1-\mu/L)^k\)</span> (linear rate). Intuitively, the condition number <span class="arithmatex">\(\kappa = L/\mu\)</span> comes into play. Examples: the quadratic form above is strongly convex with <span class="arithmatex">\(\mu = \lambda_{\min}(Q)\)</span>. Adding a small ridge term <span class="arithmatex">\(\frac{\mu}{2}|x|^2\)</span> to any convex <span class="arithmatex">\(f\)</span> makes it <span class="arithmatex">\(\mu\)</span>-strongly convex and improves conditioning at the cost of bias.</p>
<p>In summary, the tools of calculus — gradients for direction of improvement, Hessians for curvature, Jacobians for constraint and composite mappings, and inequalities like Lipschitz bounds — all feed into understanding and solving convex optimization problems. The optimality conditions formalize the simple idea: at optimum, the gradient must vanish or be balanced by constraints. The next chapter will build on this by considering those constraints explicitly and introducing Lagrange multipliers and duality, giving deeper insight into optimality in constrained problems.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>