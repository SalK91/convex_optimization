<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/3_1_gradientdescent/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>3 1 gradientdescent - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3 1 gradientdescent
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1">
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/3_1_gradientdescent.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/3_1_gradientdescent.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


  <h1>3 1 gradientdescent</h1>

<p>Gradient descent is the most fundamental first-order method for convex optimization. It capitalizes on the fact that for a differentiable convex function <span class="arithmatex">\(f:\mathbb{R}^n\to\mathbb{R}\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> indicates the direction of steepest increase (and <span class="arithmatex">\(-\nabla f(x)\)</span> the steepest decrease) at <span class="arithmatex">\(x\)</span> (see Section A, Chapter 6). At a global minimizer <span class="arithmatex">\(x^*\)</span> of a convex differentiable <span class="arithmatex">\(f\)</span>, the first-order optimality condition <span class="arithmatex">\(\nabla f(x^*) = 0\)</span> holds, and no descent direction exists. Gradient descent iteratively moves opposite to the gradient, seeking such a stationary point which, in convex problems, guarantees optimality.</p>
<h3 id="gradient-descent-algorithm-and-geometry">Gradient Descent: Algorithm and Geometry<a class="headerlink" href="#gradient-descent-algorithm-and-geometry" title="Permanent link">¶</a></h3>
<p>Starting from an initial guess <span class="arithmatex">\(x_0\in\mathcal{X}\)</span>, the gradient descent update (for unconstrained problems <span class="arithmatex">\(\mathcal{X}=\mathbb{R}^n\)</span>) is:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha \nabla f(x_k)
\]</div>
<p>where <span class="arithmatex">\(\alpha&gt;0\)</span> is a step size (also called learning rate). Geometrically, this update follows the tangent plane of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x_k\)</span> a short distance in the negative gradient direction. Intuitively, one approximates <span class="arithmatex">\(f(x)\)</span> near <span class="arithmatex">\(x_k\)</span> by the first-order Taylor expansion <span class="arithmatex">\(f(x)\approx f(x_k) + \langle \nabla f(x_k),,x - x_k\rangle\)</span> and then minimizes this linear model plus a small quadratic regularization to keep the step local (see Section A’s discussion of Taylor expansions). The result is exactly the gradient step <span class="arithmatex">\(x_{k+1}=x_k - \alpha \nabla f(x_k)\)</span>. This can be viewed as the steepest descent direction in the Euclidean metric (the gradient direction is orthogonal to level sets of <span class="arithmatex">\(f\)</span> and points toward lower values). By choosing <span class="arithmatex">\(\alpha\)</span> appropriately (e.g. via a line search or a bound on the Lipschitz constant of <span class="arithmatex">\(\nabla f\)</span>), each step guarantees a sufficient decrease in <span class="arithmatex">\(f\)</span>.</p>
<p>Recall: If <span class="arithmatex">\(f\)</span> is <span class="arithmatex">\(L\)</span>-smooth (i.e. <span class="arithmatex">\(\nabla f\)</span> is Lipschitz continuous with constant <span class="arithmatex">\(L\)</span>; see Section B, Lipschitz Continuity), then taking <span class="arithmatex">\(\alpha \le 1/L\)</span> ensures <span class="arithmatex">\(f(x_{k+1}) \le f(x_k)\)</span> for gradient descent. The descent lemma (from Section A) ensures the update is stable and <span class="arithmatex">\(f\)</span> decreases because the linear improvement <span class="arithmatex">\(- \alpha |\nabla f(x_k)|^2\)</span> dominates the quadratic error <span class="arithmatex">\(\frac{L\alpha^2}{2}|\nabla f(x_k)|^2\)</span> when <span class="arithmatex">\(\alpha \le 1/L\)</span>.</p>
<p>Choosing the step size <span class="arithmatex">\(\alpha\)</span>: A smaller <span class="arithmatex">\(\alpha\)</span> yields cautious, stable steps (useful if the landscape is ill-conditioned), whereas a larger <span class="arithmatex">\(\alpha\)</span> accelerates progress but can overshoot and even diverge if too large. In practice one may use backtracking or exact line search to adapt <span class="arithmatex">\(\alpha\)</span>. Under constant <span class="arithmatex">\(\alpha\)</span> within the safe range, convergence is guaranteed for convex <span class="arithmatex">\(f\)</span>.</p>
<h3 id="convergence-rates-convex-vs-strongly-convex">Convergence Rates: Convex vs Strongly Convex<a class="headerlink" href="#convergence-rates-convex-vs-strongly-convex" title="Permanent link">¶</a></h3>
<p>For general convex and <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(f\)</span>, gradient descent achieves a sublinear convergence rate in function value. In particular, after <span class="arithmatex">\(k\)</span> iterations one can guarantee</p>
<div class="arithmatex">\[
f(x_k) - f(x^*) \le \frac{L \|x_0 - x^*\|^2}{2k},
\quad \text{so} \quad
f(x_k) - f(x^*) = O(1/k)
\]</div>
<p>This means to get within <span class="arithmatex">\(\varepsilon\)</span> of the optimal value, on the order of <span class="arithmatex">\(1/\varepsilon\)</span> iterations are needed. This <span class="arithmatex">\(O(1/k)\)</span> rate is often called sublinear convergence. If we further assume <span class="arithmatex">\(f\)</span> is <span class="arithmatex">\(\mu\)</span>-strongly convex (see Section B: strong convexity ensures a unique minimizer and a quadratic lower bound), gradient descent’s convergence improves dramatically to a linear rate. In fact, for an <span class="arithmatex">\(L\)</span>-smooth, <span class="arithmatex">\(\mu\)</span>-strongly convex <span class="arithmatex">\(f\)</span>, there exist constants <span class="arithmatex">\(0&lt;c&lt;1\)</span> such that</p>
<div class="arithmatex">\[
f(x_k) - f(x^*) = O(c^k)
\]</div>
<p>i.e. <span class="arithmatex">\(f(x_k)\)</span> approaches <span class="arithmatex">\(f(x^)\)</span> geometrically fast. Equivalently, the error decreases by a constant factor <span class="arithmatex">\((1-\tfrac{\mu}{L})\)</span> (or similar) at every iteration. For example, choosing <span class="arithmatex">\(\alpha = 2/(\mu+L)\)</span> yields <span class="arithmatex">\(|x_{k+1}-x^| \le \frac{L-\mu}{L+\mu},|x_k - x^|\)</span>, so <span class="arithmatex">\(|x_k - x^| = O((\frac{L-\mu}{L+\mu})^k)\)</span>. Such linear convergence (also called exponential convergence) implies <span class="arithmatex">\(O(\log(1/\varepsilon))\)</span> iterations to reach accuracy <span class="arithmatex">\(\varepsilon\)</span>. Strong convexity essentially provides a uniformly curved bowl shape, preventing flat regions and guaranteeing that gradient steps won’t diminish to zero too early.</p>
<p><strong>Summary:</strong> For convex <span class="arithmatex">\(f\)</span>: <span class="arithmatex">\(f(x_k)-f(x^) = O(1/k)\)</span>; if <span class="arithmatex">\(f\)</span> is <span class="arithmatex">\(\mu\)</span>-strongly convex: <span class="arithmatex">\(f(x_k)-f(x^) = O((1-\eta\mu)^k)\)</span> (linear). These rates assume appropriate constant step sizes. (See Section B, “Function Properties,” for a detailed table of convergence rates of first-order methods.)</p>
<p>When does gradient descent perform poorly? If the problem is ill-conditioned (the Hessian has a high condition number <span class="arithmatex">\(\kappa = L/\mu\)</span>), progress along some directions is much slower than others. The iterates may “zig-zag” through narrow valleys, requiring very small <span class="arithmatex">\(\alpha\)</span> to maintain stability. In fact, the iteration complexity of gradient descent is <span class="arithmatex">\(O(\kappa \log(1/\varepsilon))\)</span> for strongly convex problems – linearly dependent on the condition number. Chapter C3 on Newton’s method will address this issue by preconditioning with second-order information.</p>
<h3 id="subgradient-method-for-nondifferentiable-convex-functions">Subgradient Method for Nondifferentiable Convex Functions<a class="headerlink" href="#subgradient-method-for-nondifferentiable-convex-functions" title="Permanent link">¶</a></h3>
<p>Gradient descent requires <span class="arithmatex">\(\nabla f(x)\)</span> to exist. Many convex problems in machine learning involve nondifferentiable objectives (e.g. hinge loss in SVMs, <span class="arithmatex">\(L_1\)</span>-regularization in Lasso, ReLU activations). Subgradient methods generalize gradient descent to such functions using a subgradient in place of the gradient. Recall from Section B that for a convex function <span class="arithmatex">\(f\)</span>, a subgradient <span class="arithmatex">\(g\)</span> at <span class="arithmatex">\(x\)</span> is any vector that supports <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span>, meaning <span class="arithmatex">\(f(y) \ge f(x) + \langle g,,y-x\rangle\)</span> for all <span class="arithmatex">\(y\)</span>. The set of all subgradients at <span class="arithmatex">\(x\)</span> is the subdifferential <span class="arithmatex">\(\partial f(x)\)</span>. If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then <span class="arithmatex">\(\partial f(x)={\nabla f(x)}\)</span>; if <span class="arithmatex">\(f\)</span> has a kink, <span class="arithmatex">\(\partial f(x)\)</span> is a whole set (e.g. for <span class="arithmatex">\(f(u)=|u|\)</span>, any <span class="arithmatex">\(g\in[-1,1]\)</span> is a subgradient at <span class="arithmatex">\(u=0\)</span>).</p>
<p>The subgradient method update mirrors gradient descent:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \eta_k g_k
\]</div>
<p>where <span class="arithmatex">\(g_k \in \partial f(x_k)\)</span> is any chosen subgradient at <span class="arithmatex">\(x_k\)</span>, and <span class="arithmatex">\(\eta_k&gt;0\)</span> is a step size. If <span class="arithmatex">\(x\)</span> is constrained to a convex set <span class="arithmatex">\(\mathcal{X}\)</span>, one includes a projection onto <span class="arithmatex">\(\mathcal{X}\)</span>: <span class="arithmatex">\(x_{k+1} = \Pi_{\mathcal{X}}!(x_k - \eta_k g_k)\)</span> (this reduces to the unconstrained update if <span class="arithmatex">\(\mathcal{X}=\mathbb{R}^n\)</span>). Geometrically, even though <span class="arithmatex">\(f\)</span>’s graph may have corners, a subgradient <span class="arithmatex">\(g_k\)</span> defines a supporting hyperplane at <span class="arithmatex">\((x_k, f(x_k))\)</span>. The update <span class="arithmatex">\(-\eta_k g_k\)</span> is a feasible descent direction because <span class="arithmatex">\(\langle g_k, x^* - x_k\rangle \ge f(x^*)-f(x_k)\)</span> for the minimizer <span class="arithmatex">\(x^*\)</span>, by convexity. Thus, moving a small amount opposite to <span class="arithmatex">\(g_k\)</span> tends to decrease <span class="arithmatex">\(f\)</span>. One can still “snap back” to the feasible region via projection as needed, analogous to the projected gradient step for constraints. This guarantees <span class="arithmatex">\(x_k\)</span> stays in <span class="arithmatex">\(\mathcal{X}\)</span> (see below for projection geometry).</p>
<p><strong>Convergence of subgradient methods:</strong> Unlike gradient descent, we cannot generally use a fixed <span class="arithmatex">\(\eta\)</span> to get convergence (the method won’t settle to a single point because of the zig-zagging on corners). Instead, a common strategy is a diminishing step size or an averaging of iterates. A typical result for minimizing a convex <span class="arithmatex">\(f\)</span> with Lipschitz-bounded subgradients (<span class="arithmatex">\(|g_k|\le G\)</span>) is: using <span class="arithmatex">\(\eta_k = \frac{R}{G\sqrt{k}}\)</span> (with <span class="arithmatex">\(R = |x_0 - x^*|\)</span>), the averaged iterate <span class="arithmatex">\(\bar{x}T = \frac{1}{T}\sum{t=1}^T x_t\)</span> satisfies</p>
<div class="arithmatex">\[
f(\bar{x}_T) - f(x^*) \le \frac{R G}{T} = O(1/T)
\]</div>
<p>This is a sublinear rate slower than <span class="arithmatex">\(O(1/T)\)</span>, reflecting the cost of nondifferentiability. In fact, <span class="arithmatex">\(O(1/\sqrt{T})\)</span> is the worst-case optimal rate for first-order methods on nonsmooth convex problems (no algorithm can generally do better than this without additional structure). If <span class="arithmatex">\(f\)</span> is also strongly convex, faster subgradient convergence is possible (e.g. <span class="arithmatex">\(O(\log T / T)\)</span> with specialized step schemes or using a known optimal value in Polyak’s step size), but it’s still slower than the smooth case. Importantly, the subgradient method does not converge to the exact minimizer unless <span class="arithmatex">\(\eta_k\to 0\)</span>; typically one gets arbitrarily close but keeps bouncing around the optimum. This is why an ergodic average <span class="arithmatex">\(\bar{x}_T\)</span> is used in the guarantee above – it smooths out oscillations.</p>
<p><strong>Projection and feasibility:</strong> When constraints <span class="arithmatex">\(\mathcal{X}\)</span> are present, the subgradient update includes a projection <span class="arithmatex">\(\Pi_{\mathcal{X}}(\cdot)\)</span>. Recall from Section A (Geometry of Orthogonal Projection) that for a closed convex set <span class="arithmatex">\(\mathcal{X}\)</span>, the projection <span class="arithmatex">\(\Pi_{\mathcal{X}}(y) = \arg\min_{x\in\mathcal{X}}|x - y|\)</span> yields the closest feasible point to <span class="arithmatex">\(y\)</span>. The projection error <span class="arithmatex">\(y - \Pi_{\mathcal{X}}(y)\)</span> is orthogonal to <span class="arithmatex">\(\mathcal{X}\)</span> at the projection point (no improvement can be made along the feasible surface). Thus, <span class="arithmatex">\(x_{k+1} = \Pi_{\mathcal{X}}(x_k - \eta g_k)\)</span> can be seen as: take a step in the subgradient direction, then drop perpendicularly back into the set. This ensures feasibility of iterates while still achieving descent on <span class="arithmatex">\(f\)</span>. For example, if <span class="arithmatex">\(\mathcal{X}\)</span> is the <span class="arithmatex">\(\ell_2\)</span> unit ball, the projection <span class="arithmatex">\(\Pi_{\mathcal{X}}(y)\)</span> simply scales <span class="arithmatex">\(y\)</span> to have norm 1 if it was outside.</p>
<p><strong>Use cases:</strong> Subgradient methods shine in nonsmooth problems like L1-regularized models (Lasso), SVM hinge loss, and combinatorial convex relaxations, where gradients are not available. They are very simple (each step is like gradient descent), but one must carefully tune the step schedule. In practice, subgradient methods can be slow to get high accuracy; however, their simplicity and ability to handle nondifferentiability make them a go-to baseline. Techniques like momentum or adaptive step sizes can sometimes improve practical performance, but fundamentally <span class="arithmatex">\(O(1/\sqrt{T})\)</span> is the regime for nonsmooth convex minimization.</p>
<p><strong>Example:</strong> Consider <span class="arithmatex">\(f(x) = |x|\)</span> (which is nonsmooth at 0). The subgradient method for <span class="arithmatex">\(\min_x |x|1\)</span> (with no smooth part) would at iteration <span class="arithmatex">\(k\)</span> choose some <span class="arithmatex">\(g_k \in \partial |x_k|\)</span> (which could be <span class="arithmatex">\(g_k = \text{sign}(x_k)\)</span> componentwise), and do <span class="arithmatex">\(x{k+1}=x_k - \eta g_k\)</span>. This essentially performs soft-thresholding on each coordinate: if <span class="arithmatex">\(x_k\)</span> was positive, it decreases it, if negative, increases it, if zero, it stays within [-<span class="arithmatex">\(\eta,\eta\)</span>]. Indeed, with an appropriate choice of <span class="arithmatex">\(\eta\)</span>, one can show <span class="arithmatex">\(x_k\)</span> converges to 0 (the minimizer). This principle underlies the ISTA/FISTA algorithms for Lasso: each step is <span class="arithmatex">\(x{k+1} = S{\lambda\eta}(x_k - \eta \nabla f_{\text{smooth}}(x_k))\)</span>, where <span class="arithmatex">\(S_{\tau}(y)=\text{sign}(y)\max{|y|-\tau,,0}\)</span> is the soft-thresholding operator. We will see proximal gradient methods in Chapter C4 that formalize this approach.</p>
<p><strong>Summary:</strong> Gradient descent is the workhorse for smooth convex problems, enjoying <span class="arithmatex">\(O(1/k)\)</span> or better convergence and simplicity. Its limitation is the requirement of differentiability and sometimes slow progress in ill-conditioned settings. The subgradient method extends applicability to nondifferentiable convex functions at the expense of slower convergence. In both cases, the geometry of convexity (supporting hyperplanes, gradient orthogonality, etc.) underpins why moving opposite to a (sub)gradient leads toward the optimum.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>