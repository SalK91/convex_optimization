
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Print/PDF - Machine Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("/convex_optimization/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#index" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Machine Learning Lecture Notes" class="md-header__button md-logo" aria-label="Machine Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print/PDF
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../convex/11_intro/" class="md-tabs__link">
          
  
  
    
  
  Convex Optimization

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../reinforcement/1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../deeplearning/1_mlp/" class="md-tabs__link">
          
  
  
    
  
  Deep Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../informationtheory/1_intro_to_infotheory/" class="md-tabs__link">
          
  
  
    
  
  Information Theory

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../cheatsheets/20a_cheatsheet/" class="md-tabs__link">
          
  
  
    
  
  Cheat Sheets

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../appendices/120_ineqaulities/" class="md-tabs__link">
          
  
  
    
  
  Appendices

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Machine Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Machine Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16a_optimality_conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. First-Order Optimality Conditions in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18a_pareto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Pareto Optimality and Multi-Objective Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18b_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Regularized Approximation – Balancing Fit and Complexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19a_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Optimization Algorithms for Equality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19b_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Optimization Algorithms for Inequality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/30_canonical_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Canonical Problems in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/35_modern/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    18. Modern Optimizers in Machine Learning Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/40_nonconvex/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    19. Beyond Convexity – Nonconvex and Global Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/42_derivativefree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    20. Derivative-Free and Black-Box Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/44_metaheuristic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    21. Metaheuristic and Evolutionary Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/48_advanced_combinatorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    22. Advanced Topics in Combinatorial Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/50_future/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    23. The Future of Optimization — Learning, Adaptation, and Intelligence
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs & Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/1_mlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/2_convnets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Convolutional Neural Networks (CNNs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/3_sequence_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Sequence Data and Recurrent Neural Networks (RNNs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/4_nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Natural Language Processing (NLP) with Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/5_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Transformers and Attention Mechanisms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/6_gans/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Generative Models and GANs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/7_unsuper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Unsupervised and Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/8_latentvariables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Latent Variable Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Optimization Algorithms for Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Regularization Techniques in Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Advanced Topics in Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Information Theory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Information Theory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/1_intro_to_infotheory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/2_entropy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Entropy, Self-Information & Cross-Entropy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/3_KL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Kullback-Leibler Divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/4_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Bayesian Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/5_mc_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Probability toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/6_mc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Monte Carlo Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/7a_vi_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization-Based Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/7b_vi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Variatonal Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/8_representation.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Representation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Cheat Sheets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Cheat Sheets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheatsheets/20a_cheatsheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algos - Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Appendices
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Appendices
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/160_conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix D - Convex Conjugates and Fenchel Duality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/170_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix E - Convexity in Probability and Statistics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/180_subgradient_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix F - Subgradient Method and Variants
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/190_proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix G - Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/200_mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix H - Mirror Descent and Bregman Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/300_matrixfactorization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix I - Matrix Factorization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Convex Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Convex Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convex-11_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction and Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-12_vector" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Linear Algebra Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-13_calculus" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Multivariable Calculus for Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-14_convexsets" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Convex Sets and Geometric Fundamentals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-15_convexfunctions" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Convex Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16_subgradients" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Nonsmooth Convex Optimization – Subgradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16a_optimality_conditions" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. First-Order Optimality Conditions in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-17_kkt" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Optimization Principles – From Gradient Descent to KKT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18_duality" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Lagrange Duality Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18a_pareto" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Pareto Optimality and Multi-Objective Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18b_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Regularized Approximation – Balancing Fit and Complexity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19_optimizationalgo" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Algorithms for Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19a_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Optimization Algorithms for Equality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19b_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Optimization Algorithms for Inequality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-20_advanced" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Advanced Large-Scale and Structured Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-21_models" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Modelling Patterns and Algorithm Selection in Practice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-30_canonical_problems" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Canonical Problems in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-35_modern" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 18. Modern Optimizers in Machine Learning Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-40_nonconvex" class="md-nav__link">
    <span class="md-ellipsis">
      2.19 19. Beyond Convexity – Nonconvex and Global Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-42_derivativefree" class="md-nav__link">
    <span class="md-ellipsis">
      2.20 20. Derivative-Free and Black-Box Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-44_metaheuristic" class="md-nav__link">
    <span class="md-ellipsis">
      2.21 21. Metaheuristic and Evolutionary Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-48_advanced_combinatorial" class="md-nav__link">
    <span class="md-ellipsis">
      2.22 22. Advanced Topics in Combinatorial Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-50_future" class="md-nav__link">
    <span class="md-ellipsis">
      2.23 23. The Future of Optimization — Learning, Adaptation, and Intelligence
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-3" class="md-nav__link">
    <span class="md-ellipsis">
      3 Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reinforcement-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 1. Introduction to Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-2_mdp" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 2. MDPs & Dynamic Programming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-3_modelfree" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 3. Model-Free Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-4_model_free_control" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 4. Model-Free Control
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-5_policy_gradient" class="md-nav__link">
    <span class="md-ellipsis">
      3.5 5. Policy Gradient Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-6_pg2" class="md-nav__link">
    <span class="md-ellipsis">
      3.6 6. Policy Gradient Variance Reduction and Actor-Critic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-7_gae" class="md-nav__link">
    <span class="md-ellipsis">
      3.7 7. Advances in Policy Optimization – GAE, TRPO, and PPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-8_imitation_learning" class="md-nav__link">
    <span class="md-ellipsis">
      3.8 8. Imitation Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-9_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      3.9 9. RLHF
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-10_offline_rl" class="md-nav__link">
    <span class="md-ellipsis">
      3.10 10. Offline Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-11_fast_rl" class="md-nav__link">
    <span class="md-ellipsis">
      3.11 11. Data-Efficient Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-12_fast_mdps" class="md-nav__link">
    <span class="md-ellipsis">
      3.12 12. Fast Reinforcement Learning in MDPs and Generalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-13_montecarlo" class="md-nav__link">
    <span class="md-ellipsis">
      3.13 13. Monte Carlo Tree Search and Planning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-14_final" class="md-nav__link">
    <span class="md-ellipsis">
      3.14 14. Summary and Overview
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-4" class="md-nav__link">
    <span class="md-ellipsis">
      4 Deep Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Deep Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deeplearning-1_mlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 1. Introduction to Deep Learning Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-2_convnets" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 2. Convolutional Neural Networks (CNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-3_sequence_data" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 3. Sequence Data and Recurrent Neural Networks (RNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-4_nlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 4. Natural Language Processing (NLP) with Deep Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-5_attention" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 5. Transformers and Attention Mechanisms
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-6_gans" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 6. Generative Models and GANs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-7_unsuper" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 7. Unsupervised and Self-Supervised Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-8_latentvariables" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 8. Latent Variable Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-5" class="md-nav__link">
    <span class="md-ellipsis">
      5 Information Theory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 Information Theory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#informationtheory-1_intro_to_infotheory" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 1. Introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-2_entropy" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 2. Entropy, Self-Information & Cross-Entropy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-3_kl" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 3. Kullback-Leibler Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-4_bayes" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 4. Bayesian Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-5_mc_intro" class="md-nav__link">
    <span class="md-ellipsis">
      5.5 5. Probability toolbox
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-6_mc" class="md-nav__link">
    <span class="md-ellipsis">
      5.6 6. Monte Carlo Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7a_vi_intro" class="md-nav__link">
    <span class="md-ellipsis">
      5.7 8. Optimization-Based Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7b_vi" class="md-nav__link">
    <span class="md-ellipsis">
      5.8 7. Variatonal Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-6" class="md-nav__link">
    <span class="md-ellipsis">
      6 Cheat Sheets
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 Cheat Sheets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cheatsheets-20a_cheatsheet" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Optimization Algos - Cheat Sheet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-7" class="md-nav__link">
    <span class="md-ellipsis">
      7 Appendices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7 Appendices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendices-120_ineqaulities" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 Appendix A - Common Inequalities and Identities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-130_projections" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Appendix B - Projection and Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-140_support" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 Appendix C - Support Functions and Dual Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-160_conjugates" class="md-nav__link">
    <span class="md-ellipsis">
      7.4 Appendix D - Convex Conjugates and Fenchel Duality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-170_probability" class="md-nav__link">
    <span class="md-ellipsis">
      7.5 Appendix E - Convexity in Probability and Statistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-180_subgradient_methods" class="md-nav__link">
    <span class="md-ellipsis">
      7.6 Appendix F - Subgradient Method and Variants
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-190_proximal" class="md-nav__link">
    <span class="md-ellipsis">
      7.7 Appendix G - Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-200_mirror" class="md-nav__link">
    <span class="md-ellipsis">
      7.8 Appendix H - Mirror Descent and Bregman Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-300_matrixfactorization" class="md-nav__link">
    <span class="md-ellipsis">
      7.9 Appendix I - Matrix Factorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Convex Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Convex Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convex-11_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction and Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-12_vector" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Linear Algebra Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-13_calculus" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Multivariable Calculus for Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-14_convexsets" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Convex Sets and Geometric Fundamentals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-15_convexfunctions" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Convex Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16_subgradients" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Nonsmooth Convex Optimization – Subgradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16a_optimality_conditions" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. First-Order Optimality Conditions in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-17_kkt" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Optimization Principles – From Gradient Descent to KKT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18_duality" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Lagrange Duality Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18a_pareto" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Pareto Optimality and Multi-Objective Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18b_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Regularized Approximation – Balancing Fit and Complexity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19_optimizationalgo" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Algorithms for Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19a_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Optimization Algorithms for Equality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19b_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Optimization Algorithms for Inequality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-20_advanced" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Advanced Large-Scale and Structured Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-21_models" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Modelling Patterns and Algorithm Selection in Practice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-30_canonical_problems" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Canonical Problems in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-35_modern" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 18. Modern Optimizers in Machine Learning Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-40_nonconvex" class="md-nav__link">
    <span class="md-ellipsis">
      2.19 19. Beyond Convexity – Nonconvex and Global Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-42_derivativefree" class="md-nav__link">
    <span class="md-ellipsis">
      2.20 20. Derivative-Free and Black-Box Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-44_metaheuristic" class="md-nav__link">
    <span class="md-ellipsis">
      2.21 21. Metaheuristic and Evolutionary Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-48_advanced_combinatorial" class="md-nav__link">
    <span class="md-ellipsis">
      2.22 22. Advanced Topics in Combinatorial Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-50_future" class="md-nav__link">
    <span class="md-ellipsis">
      2.23 23. The Future of Optimization — Learning, Adaptation, and Intelligence
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-3" class="md-nav__link">
    <span class="md-ellipsis">
      3 Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reinforcement-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 1. Introduction to Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-2_mdp" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 2. MDPs & Dynamic Programming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-3_modelfree" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 3. Model-Free Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-4_model_free_control" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 4. Model-Free Control
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-5_policy_gradient" class="md-nav__link">
    <span class="md-ellipsis">
      3.5 5. Policy Gradient Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-6_pg2" class="md-nav__link">
    <span class="md-ellipsis">
      3.6 6. Policy Gradient Variance Reduction and Actor-Critic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-7_gae" class="md-nav__link">
    <span class="md-ellipsis">
      3.7 7. Advances in Policy Optimization – GAE, TRPO, and PPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-8_imitation_learning" class="md-nav__link">
    <span class="md-ellipsis">
      3.8 8. Imitation Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-9_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      3.9 9. RLHF
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-10_offline_rl" class="md-nav__link">
    <span class="md-ellipsis">
      3.10 10. Offline Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-11_fast_rl" class="md-nav__link">
    <span class="md-ellipsis">
      3.11 11. Data-Efficient Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-12_fast_mdps" class="md-nav__link">
    <span class="md-ellipsis">
      3.12 12. Fast Reinforcement Learning in MDPs and Generalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-13_montecarlo" class="md-nav__link">
    <span class="md-ellipsis">
      3.13 13. Monte Carlo Tree Search and Planning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-14_final" class="md-nav__link">
    <span class="md-ellipsis">
      3.14 14. Summary and Overview
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-4" class="md-nav__link">
    <span class="md-ellipsis">
      4 Deep Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Deep Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deeplearning-1_mlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 1. Introduction to Deep Learning Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-2_convnets" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 2. Convolutional Neural Networks (CNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-3_sequence_data" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 3. Sequence Data and Recurrent Neural Networks (RNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-4_nlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 4. Natural Language Processing (NLP) with Deep Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-5_attention" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 5. Transformers and Attention Mechanisms
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-6_gans" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 6. Generative Models and GANs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-7_unsuper" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 7. Unsupervised and Self-Supervised Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-8_latentvariables" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 8. Latent Variable Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-5" class="md-nav__link">
    <span class="md-ellipsis">
      5 Information Theory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 Information Theory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#informationtheory-1_intro_to_infotheory" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 1. Introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-2_entropy" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 2. Entropy, Self-Information & Cross-Entropy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-3_kl" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 3. Kullback-Leibler Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-4_bayes" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 4. Bayesian Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-5_mc_intro" class="md-nav__link">
    <span class="md-ellipsis">
      5.5 5. Probability toolbox
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-6_mc" class="md-nav__link">
    <span class="md-ellipsis">
      5.6 6. Monte Carlo Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7a_vi_intro" class="md-nav__link">
    <span class="md-ellipsis">
      5.7 8. Optimization-Based Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7b_vi" class="md-nav__link">
    <span class="md-ellipsis">
      5.8 7. Variatonal Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-6" class="md-nav__link">
    <span class="md-ellipsis">
      6 Cheat Sheets
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 Cheat Sheets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cheatsheets-20a_cheatsheet" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Optimization Algos - Cheat Sheet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-7" class="md-nav__link">
    <span class="md-ellipsis">
      7 Appendices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7 Appendices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendices-120_ineqaulities" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 Appendix A - Common Inequalities and Identities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-130_projections" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Appendix B - Projection and Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-140_support" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 Appendix C - Support Functions and Dual Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-160_conjugates" class="md-nav__link">
    <span class="md-ellipsis">
      7.4 Appendix D - Convex Conjugates and Fenchel Duality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-170_probability" class="md-nav__link">
    <span class="md-ellipsis">
      7.5 Appendix E - Convexity in Probability and Statistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-180_subgradient_methods" class="md-nav__link">
    <span class="md-ellipsis">
      7.6 Appendix F - Subgradient Method and Variants
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-190_proximal" class="md-nav__link">
    <span class="md-ellipsis">
      7.7 Appendix G - Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-200_mirror" class="md-nav__link">
    <span class="md-ellipsis">
      7.8 Appendix H - Mirror Descent and Bregman Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-300_matrixfactorization" class="md-nav__link">
    <span class="md-ellipsis">
      7.9 Appendix I - Matrix Factorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index" heading-number="1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="mathematics-for-machine-learning">Mathematics for Machine Learning<a class="headerlink" href="#index-mathematics-for-machine-learning" title="Permanent link">¶</a></h1>
<p>Welcome to <em>Mathematics for Machine Learning</em>, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p>
<p>This digital book provides a unified, intuition-driven exploration of key mathematical tools — from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>
<h2 id="index-motivation">Motivation<a class="headerlink" href="#index-motivation" title="Permanent link">¶</a></h2>
<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p>
<ul>
<li>Concise derivations of essential results</li>
<li>Geometric intuition and figures where helpful</li>
<li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li>
<li>Appendices that extend into more advanced or specialized topics</li>
</ul>
<p>Whether you’re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p></body></html></section>
                    <section class='print-page md-section' id='section-2' heading-number='2'>
                        <h1>Convex Optimization<a class='headerlink' href='#section-2' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="convex-11_intro" heading-number="2.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-and-overview">Chapter 1:  Introduction and Overview<a class="headerlink" href="#convex-11_intro-chapter-1-introduction-and-overview" title="Permanent link">¶</a></h1>
<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p>
<ol>
<li>No bad local minima: any local minimum is also the global minimum.  </li>
<li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li>
<li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li>
</ol>
<p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p>
<p>This web-book is written for practitioners who have basic familiarity with optimization, especially gradient-based methods, and want to understand how convex optimization principles help guide reliable machine-learning practice.</p>
<h2 id="convex-11_intro-11-motivation-optimization-in-machine-learning">1.1 Motivation: Optimization in Machine Learning<a class="headerlink" href="#convex-11_intro-11-motivation-optimization-in-machine-learning" title="Permanent link">¶</a></h2>
<p>Many supervised learning problems can be written in a common form:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} 
\; \frac{1}{N}\sum_{i=1}^{N} \ell(a_i^\top x, b_i) 
+ \lambda R(x),
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(\ell(\cdot,\cdot)\)</span> is a loss function that measures how well the model predicts <span class="arithmatex">\(b_i\)</span> from <span class="arithmatex">\(a_i\)</span>,  </li>
<li><span class="arithmatex">\(R(x)\)</span> is a regularizer that encourages certain structure (such as sparsity or small weights),  </li>
<li><span class="arithmatex">\(\mathcal{X}\)</span> is a set of allowed parameter values, often simple and convex.</li>
</ul>
<p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the <span class="arithmatex">\(\ell_1\)</span> norm, and the <span class="arithmatex">\(\ell_2\)</span> norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>
<h2 id="convex-11_intro-12-convex-sets-and-convex-functions-first-intuition">1.2 Convex Sets and Convex Functions — First Intuition<a class="headerlink" href="#convex-11_intro-12-convex-sets-and-convex-functions-first-intuition" title="Permanent link">¶</a></h2>
<p>A set <span class="arithmatex">\(\mathcal{C}\)</span> is convex if, whenever you pick two points in the set, the line segment between them stays entirely inside the set:</p>
<div class="arithmatex">\[
\theta x + (1-\theta)y \in \mathcal{C} 
\quad \text{for all } x,y \in \mathcal{C},\; \theta \in [0,1].
\]</div>
<p>Convex functions follow a similar idea. A function <span class="arithmatex">\(f\)</span> is convex if its graph never dips below the straight line connecting two points on the function:</p>
<div class="arithmatex">\[
f(\theta x + (1-\theta)y)
\le
\theta f(x) + (1-\theta) f(y).
\]</div>
<p>Intuitively, convex functions look like bowls: they curve upward and have at most one global minimum. Affine functions are both convex and concave, and quadratics with positive semidefinite Hessians are convex. Many ML loss functions share this shape, which makes them easy to optimize.</p>
<h2 id="convex-11_intro-13-why-convex-optimization-remains-central-in-ml">1.3 Why Convex Optimization Remains Central in ML<a class="headerlink" href="#convex-11_intro-13-why-convex-optimization-remains-central-in-ml" title="Permanent link">¶</a></h2>
<p>Although many modern models are nonconvex, convex optimization continues to play a major role in three ways:</p>
<ol>
<li>
<p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the <span class="arithmatex">\(0\text{–}1\)</span> loss. They make optimization practical while still leading to models that generalize well.</p>
</li>
<li>
<p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p>
</li>
<li>
<p>Implicit bias in linear models:  In overparameterized linear least-squares problems, gradient descent starting from zero converges to the minimum-norm solution. This phenomenon helps explain generalization and implicit regularization in linear and kernel models.</p>
</li>
</ol>
<p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>
<h2 id="convex-11_intro-14-from-global-optima-to-algorithms">1.4 From Global Optima to Algorithms<a class="headerlink" href="#convex-11_intro-14-from-global-optima-to-algorithms" title="Permanent link">¶</a></h2>
<p>A major advantage of convex optimization is that it eliminates the possibility of non-global local minima. For a differentiable convex function on an open domain:</p>
<div class="arithmatex">\[
\nabla f(x^*) = 0 
\quad \Rightarrow \quad
x^* \text{ is a global minimizer}.
\]</div>
<p>This means that simply finding a point where the gradient is zero is enough. For constrained or nondifferentiable problems, optimality is checked using subgradients or KKT conditions:</p>
<div class="arithmatex">\[
0 \in \partial f(x^*) + N_{\mathcal{X}}(x^*),
\]</div>
<p>where <span class="arithmatex">\(N_{\mathcal{X}}(x^*)\)</span> represents the outward directions that are blocked by the constraint set. These conditions are useful because many iterative algorithms aim to drive the gradient or subgradient toward zero.</p>
<h2 id="convex-11_intro-15-canonical-convex-ml-problems-at-a-glance">1.5 Canonical Convex ML Problems at a Glance<a class="headerlink" href="#convex-11_intro-15-canonical-convex-ml-problems-at-a-glance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Objective</th>
<th>Typical Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td>Least squares</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Gradient descent, conjugate gradient</td>
</tr>
<tr>
<td>Ridge regression</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_2^2\)</span></td>
<td>Closed form, gradient methods</td>
</tr>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td>Proximal gradient (ISTA/FISTA)</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\sum_i \log(1+\exp(-y_i a_i^\top x)) + \lambda\|x\|_2^2\)</span></td>
<td>Newton, quasi-Newton, SGD</td>
</tr>
<tr>
<td>SVM (hinge loss)</td>
<td><span class="arithmatex">\(\tfrac{1}{2}\|x\|^2 + C\sum_i \max(0,1-y_i a_i^\top x)\)</span></td>
<td>Subgradient, coordinate methods, SMO</td>
</tr>
<tr>
<td>Robust regression</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Linear programming</td>
</tr>
<tr>
<td>Elastic Net</td>
<td><span class="arithmatex">\(\|A x-b\|_2^2 + \lambda_1\|x\|_1 + \lambda_2\|x\|_2^2\)</span></td>
<td>Coordinate descent</td>
</tr>
</tbody>
</table>
<p>These problems illustrate how convex models appear throughout ML.</p>
<h2 id="convex-11_intro-16-web-book-roadmap-and-how-to-use-it">1.6 Web-Book Roadmap and How to Use It<a class="headerlink" href="#convex-11_intro-16-web-book-roadmap-and-how-to-use-it" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Question</th>
<th>Where to Look</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>What makes a function or set convex?</td>
<td>Chapters 2–5</td>
<td>Geometry and basic properties of convexity</td>
</tr>
<tr>
<td>How do gradients, subgradients, and KKT conditions define optimality?</td>
<td>Chapters 6–9</td>
<td>Optimality conditions and duality</td>
</tr>
<tr>
<td>How are convex problems solved in practice?</td>
<td>Chapters 10–14</td>
<td>First-order, second-order, and interior-point methods</td>
</tr>
<tr>
<td>How to choose an algorithm for a given optimization problem?</td>
<td>Chapters 15–17</td>
<td>Large-scale and structured optimization techniques</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-12_vector" heading-number="2.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-linear-algebra-foundations">Chapter 2: Linear Algebra Foundations<a class="headerlink" href="#convex-12_vector-chapter-2-linear-algebra-foundations" title="Permanent link">¶</a></h1>
<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model <span class="arithmatex">\(x\)</span> to data <span class="arithmatex">\((A, b)\)</span> takes the form:</p>
<div class="arithmatex">\[
\min_x \ \|A x - b\|_2^2.
\]</div>
<p>Later in this chapter, we will see that this objective finds the point in the column space of <span class="arithmatex">\(A\)</span> that is closest to <span class="arithmatex">\(b\)</span>. Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p>
<p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas — projections, subspaces, orthogonality, eigenvalues, singular values, and norms — because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>
<h2 id="convex-12_vector-21-vector-spaces-subspaces-and-affine-sets">2.1 Vector spaces, subspaces, and affine sets<a class="headerlink" href="#convex-12_vector-21-vector-spaces-subspaces-and-affine-sets" title="Permanent link">¶</a></h2>
<p>A vector space over <span class="arithmatex">\(\mathbb{R}\)</span> is a set of vectors that can be added and scaled without leaving the set. The familiar example is <span class="arithmatex">\(\mathbb{R}^n\)</span>, where operations like <span class="arithmatex">\(\alpha x + \beta y\)</span> keep us within the same space.</p>
<p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are “flat” objects that always pass through the origin, such as lines or planes in <span class="arithmatex">\(\mathbb{R}^3\)</span>. </p>
<p>Affine sets extend this idea by allowing a shift away from the origin. A set <span class="arithmatex">\(A\)</span> is affine if it contains the entire line passing through any two of its points. Equivalently, for any <span class="arithmatex">\(x,y \in A\)</span> and any <span class="arithmatex">\(\theta \in \mathbb{R}\)</span>,  <span class="arithmatex">\(\theta x + (1 - \theta) y \in A.\)</span> That is, the entire line passing through any two points in <span class="arithmatex">\(A\)</span> lies within <span class="arithmatex">\(A\)</span>. By contrast, a convex set only requires this property for <span class="arithmatex">\(\theta \in [0,1]\)</span>, meaning only the line segment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> must lie within the set. </p>
<p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: <span class="arithmatex">\(A = x_0 + S = \{\, x_0 + s : s \in S \,\},\)</span> where <span class="arithmatex">\(S\)</span> is a subspace and <span class="arithmatex">\(x_0\)</span> is any point in the set. This representation is extremely useful in optimization. If <span class="arithmatex">\(Ax = b\)</span> is a linear constraint, then its solution set is an affine set. A single particular solution <span class="arithmatex">\(x_0\)</span> gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of <span class="arithmatex">\(A\)</span>. Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p>
<p>Finally, affine transformations play a central role in both machine learning and optimization. A mapping of the form</p>
<p>Affine Transformations: An affine transformation (or affine map) is a function <span class="arithmatex">\(f : V \to W\)</span> that can be written as <span class="arithmatex">\(f(x) = A x + b,\)</span> where <span class="arithmatex">\(A\)</span> is a linear map and <span class="arithmatex">\(b\)</span> is a fixed vector. Affine transformations preserve both affinity and convexity:
if <span class="arithmatex">\(C\)</span> is convex, then <span class="arithmatex">\(A C + b\)</span> is also convex.
is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p>
<p>In summary, vector spaces describe the ambient space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the book.</p>
<h2 id="convex-12_vector-22-linear-combinations-span-basis-dimension">2.2 Linear combinations, span, basis, dimension<a class="headerlink" href="#convex-12_vector-22-linear-combinations-span-basis-dimension" title="Permanent link">¶</a></h2>
<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p>
<p>Given vectors <span class="arithmatex">\(v_1,\dots,v_k\)</span>, any vector of the form<script type="math/tex">
\alpha_1 v_1 + \cdots + \alpha_k v_k</script> is a linear combination. The set of all linear combinations is called the span:
<script type="math/tex; mode=display">
\mathrm{span}\{v_1,\dots,v_k\} = \left\{ \sum_{i=1}^k \alpha_i v_i : \alpha_i \in \mathbb{R} \right\}.
</script>
The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p>
<p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p>
<p>A basis of a space <span class="arithmatex">\(V\)</span> is a linearly independent set whose span equals <span class="arithmatex">\(V\)</span>. The number of basis vectors is the dimension <span class="arithmatex">\(\dim(V)\)</span>.</p>
<p>Rank and nullity facts:</p>
<ul>
<li>The column space of <span class="arithmatex">\(A\)</span> is the span of its columns. Its dimension is <span class="arithmatex">\(\mathrm{rank}(A)\)</span>.</li>
<li>The nullspace of <span class="arithmatex">\(A\)</span> is <span class="arithmatex">\(\{ x : Ax = 0 \}\)</span>.</li>
<li>The rank-nullity theorem states: <span class="arithmatex">\(\mathrm{rank}(A) + \mathrm{nullity}(A) = n,\)</span> where <span class="arithmatex">\(n\)</span> is the number of columns of <span class="arithmatex">\(A\)</span>.</li>
</ul>
<blockquote>
<p>Column Space: The column space of a matrix <script type="math/tex"> A </script>, denoted <script type="math/tex"> C(A) </script>, is the set of all possible output vectors <script type="math/tex"> b </script> that can be written as <script type="math/tex"> Ax </script> for some <script type="math/tex"> x </script>. In other words, it contains all vectors that the matrix can “reach” through linear combinations of its columns. The question “Does the system <script type="math/tex"> Ax = b </script> have a solution?” is equivalent to asking whether <script type="math/tex"> b \in C(A) </script>. If <script type="math/tex"> b </script> lies in the column space, a solution exists; otherwise, it does not.</p>
<p>Null Space: The null space (or kernel) of <script type="math/tex"> A </script>, denoted <script type="math/tex"> N(A) </script>, is the set of all input vectors <script type="math/tex"> x </script> that are mapped to zero:  <script type="math/tex"> N(A) = \{ x : Ax = 0 \} </script>. It answers a different question: <em>If a solution to <script type="math/tex"> Ax = b </script> exists, is it unique?</em> If the null space contains only the zero vector (<script type="math/tex"> \mathrm{nullity}(A) = 0 </script>), the solution is unique. But if <script type="math/tex"> N(A) </script> contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p>
<p>Multicollinearity: When one feature in the data matrix <script type="math/tex"> A </script> is a linear combination of others for example, <script type="math/tex"> \text{feature}_3 = 2 \times \text{feature}_1 + \text{feature}_2 </script>—the columns of <script type="math/tex"> A </script> become linearly dependent. This creates a nonzero vector in the null space of <script type="math/tex"> A </script>, meaning multiple weight vectors <script type="math/tex"> x </script> can produce the same predictions. The model is then <em>unidentifiable</em> (Underdetermined – the number of unknowns (parameters) exceeds the number of independent equations (information)), and <script type="math/tex"> A^\top A </script> becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p>
<blockquote>
<p>Regularization introduces an additional constraint or penalty that selects a <em>single, stable</em> solution from among the infinite possibilities.</p>
<ul>
<li>
<p>Ridge regression (L2 regularization) adds a penalty on the norm of <span class="arithmatex">\(x\)</span>:
  <script type="math/tex; mode=display">
  \min_x \|A x - b\|_2^2 + \lambda \|x\|_2^2,
  </script>
  which modifies the normal equations to
  <script type="math/tex; mode=display">
  (A^\top A + \lambda I)x = A^\top b.
  </script>
  The added term <span class="arithmatex">\(\lambda I\)</span> ensures invertibility and numerical stability.</p>
</li>
<li>
<p>Lasso regression (L1 regularization) instead penalizes <span class="arithmatex">\(\|x\|_1\)</span>, promoting sparsity by driving some coefficients exactly to zero.</p>
</li>
</ul>
<p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors—and making the regression problem well-posed even when <span class="arithmatex">\(A\)</span> is rank-deficient.</p>
</blockquote>
<p>Feasible Directions: In a constrained optimization problem of the form <script type="math/tex"> Ax = b </script>, the null space of <script type="math/tex"> A </script> characterizes the directions along which one can move without violating the constraints. If <script type="math/tex"> d \in N(A) </script>, then moving from a feasible point <script type="math/tex"> x </script> to <script type="math/tex"> x + d </script> preserves feasibility, since  <script type="math/tex"> A(x + d) = Ax + Ad = b </script>. Thus, the null space defines the <em>space of free movement</em> directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p>
<p>Row Space: The row space of <script type="math/tex"> A </script>, denoted <script type="math/tex"> R(A) </script>, is the span of the rows of <script type="math/tex"> A </script> (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to <script type="math/tex"> \mathrm{rank}(A) </script>. The row space is orthogonal to the null space of <script type="math/tex"> A </script>:  <script type="math/tex"> R(A) \perp N(A) </script>.  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in <script type="math/tex"> x </script> affect the constraints.</p>
<p>Left Null Space: The left null space, denoted <script type="math/tex"> N(A^\top) </script>, is the set of all vectors <script type="math/tex"> y </script> such that <script type="math/tex"> A^\top y = 0 </script>. These vectors are orthogonal to the columns of <script type="math/tex"> A </script>, and therefore orthogonal to the column space itself. In least squares problems, <script type="math/tex"> N(A^\top) </script> represents residual directions—components of <script type="math/tex"> b </script> that cannot be explained by the model <script type="math/tex"> Ax = b </script>.</p>
<p>Projection Interpretation (Least Squares):  When <script type="math/tex"> Ax = b </script> has no exact solution (as in overdetermined systems), the least squares solution finds <script type="math/tex"> x </script> such that <script type="math/tex"> Ax </script> is the projection of <script type="math/tex"> b </script> onto the column space of <script type="math/tex"> A </script>:<br>
<script type="math/tex; mode=display">
x^* = (A^\top A)^{-1} A^\top b,
</script>
and the residual<br>
<script type="math/tex; mode=display">
r = b - Ax^*
</script>
lies in the left null space <script type="math/tex"> N(A^\top) </script>.<br>
This provides a geometric view: the solution projects <script type="math/tex"> b </script> onto the closest point in the subspace that <script type="math/tex"> A </script> can reach.</p>
<p>Rank–Nullity Relationship: The rank of <script type="math/tex"> A </script> is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank–Nullity Theorem:
<script type="math/tex; mode=display">
\mathrm{rank}(A) + \mathrm{nullity}(A) = n,
</script>
where <script type="math/tex"> n </script> is the number of columns of <script type="math/tex"> A </script>. This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in <script type="math/tex"> x </script>.</p>
<p>Geometric Interpretation:  </p>
<ul>
<li>The column space represents all <em>reachable outputs</em>.  </li>
<li>The null space represents all <em>indistinguishable inputs</em> that map to zero.  </li>
<li>The row space represents all <em>independent constraints</em> imposed by <script type="math/tex"> A </script>.  </li>
<li>The left null space captures <em>inconsistencies</em> or residual directions that cannot be explained by the model.  </li>
</ul>
<p>Together, these four subspaces define the complete geometry of the linear map <script type="math/tex"> A: \mathbb{R}^n \to \mathbb{R}^m </script>.</p>
</blockquote>
<h2 id="convex-12_vector-23-inner-products-and-orthogonality">2.3 Inner products and orthogonality<a class="headerlink" href="#convex-12_vector-23-inner-products-and-orthogonality" title="Permanent link">¶</a></h2>
<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality—concepts that appear repeatedly in convex optimization and machine learning.</p>
<p>An inner product on <span class="arithmatex">\(\mathbb{R}^n\)</span> is a map <span class="arithmatex">\(\langle \cdot,\cdot\rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\)</span> such that for all <span class="arithmatex">\(x,y,z\)</span> and all scalars <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\langle x,y \rangle = \langle y,x\rangle\)</span> (symmetry),</li>
<li><span class="arithmatex">\(\langle x+y,z \rangle = \langle x,z \rangle + \langle y,z\rangle\)</span> (linearity in first argument),</li>
<li><span class="arithmatex">\(\langle \alpha x, y\rangle = \alpha \langle x, y\rangle\)</span>,</li>
<li><span class="arithmatex">\(\langle x, x\rangle \ge 0\)</span> with equality iff <span class="arithmatex">\(x=0\)</span> (positive definiteness).</li>
</ol>
<p>The inner product induces:</p>
<ul>
<li>length (norm): <span class="arithmatex">\(\|x\|_2 = \sqrt{\langle x,x\rangle}\)</span>,</li>
<li>angle: <script type="math/tex">
\cos \theta = \frac{\langle x,y\rangle}{\|x\|\|y\|}~.
</script>
</li>
</ul>
<p>Two vectors are orthogonal if <span class="arithmatex">\(\langle x,y\rangle = 0\)</span>. A set of vectors <span class="arithmatex">\(\{v_i\}\)</span> is orthonormal if each <span class="arithmatex">\(\|v_i\| = 1\)</span> and <span class="arithmatex">\(\langle v_i, v_j\rangle = 0\)</span> for <span class="arithmatex">\(i\ne j\)</span>.</p>
<blockquote>
<p>More generally, an inner product endows <span class="arithmatex">\(V\)</span> with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p>
<p>Geometry from the inner product: An inner product induces a norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x \rangle}\)</span> and a notion of distance <span class="arithmatex">\(d(x,y) = \|x-y\|\)</span>. It also defines angles: <span class="arithmatex">\(\langle x,y \rangle = 0\)</span> means <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: <span class="arithmatex">\(\|x+y\|^2 + \|x-y\|^2 = 2\|x\|^2 + 2\|y\|^2\)</span>.  </p>
</blockquote>
<p>The Cauchy–Schwarz inequality: For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>:
<script type="math/tex; mode=display">
|\langle x,y\rangle| \le \|x\|\|y\|~,
</script>
with equality iff <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent Geometrically, it means the absolute inner product is maximized when <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> point in the same or opposite direction. </p>
<p>Examples of inner products:</p>
<ul>
<li>
<p>Standard (Euclidean) inner product: <span class="arithmatex">\(\langle x,y\rangle = x^\top y = \sum_i x_i y_i\)</span>. This underlies most optimization algorithms on <span class="arithmatex">\(\mathbb{R}^n\)</span>, where <span class="arithmatex">\(\nabla f(x)\)</span> is defined via this inner product (so that <span class="arithmatex">\(\langle \nabla f(x), h\rangle\)</span> gives the directional derivative in direction <span class="arithmatex">\(h\)</span>).  </p>
</li>
<li>
<p>Weighted inner product: <span class="arithmatex">\(\langle x,y\rangle_W = x^\top W y\)</span> for some symmetric positive-definite matrix <span class="arithmatex">\(W\)</span>. Here <span class="arithmatex">\(\|x\|_W = \sqrt{x^\top W x}\)</span> is a weighted length. Such inner products appear in preconditioning: by choosing <span class="arithmatex">\(W\)</span> cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses <span class="arithmatex">\(W = \Sigma^{-1}\)</span> for covariance <span class="arithmatex">\(\Sigma\)</span>).  </p>
</li>
<li>
<p>Function space inner product: <span class="arithmatex">\(\langle f, g \rangle = \int_a^b f(t)\,g(t)\,dt\)</span>. This turns the space of square-integrable functions on <span class="arithmatex">\([a,b]\)</span> into an inner product space (a Hilbert space, <span class="arithmatex">\(L^2[a,b]\)</span>). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p>
</li>
</ul>
<blockquote>
<p>Any vector space with an inner product has an orthonormal basis (via the Gram–Schmidt process). Gram–Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as <span class="arithmatex">\(A = QR\)</span> where <span class="arithmatex">\(Q\)</span> has orthonormal columns and <span class="arithmatex">\(R\)</span> is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve <span class="arithmatex">\(Ax=b\)</span> and to analyze subspaces. For example, for an overdetermined system (<span class="arithmatex">\(m&gt;n\)</span> i.e. more equations than unknowns), <span class="arithmatex">\(Ax=b\)</span> has a least-squares solution <span class="arithmatex">\(x = R^{-1}(Q^\top b)\)</span>, and for underdetermined (<span class="arithmatex">\(m&lt;n\)</span>), <span class="arithmatex">\(Ax=b\)</span> has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p>
</blockquote>
<p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p>
<ul>
<li>
<p>Gradients: The gradient <span class="arithmatex">\(\nabla f(x)\)</span> is defined as the vector satisfying <span class="arithmatex">\(f(x+h)\approx f(x) + \langle \nabla f(x), h\rangle\)</span>. Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction <span class="arithmatex">\(-\nabla f(x)\)</span> because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix <span class="arithmatex">\(W\)</span>), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p>
</li>
<li>
<p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints <span class="arithmatex">\(Ax=b\)</span> (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(\mathrm{range}(A)\)</span>) and quadratic programs (where each iteration might involve a projection).  </p>
</li>
<li>
<p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p>
</li>
<li>
<p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix <span class="arithmatex">\(G_{ij} = \langle x_i, x_j\rangle\)</span> for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: <span class="arithmatex">\(X^\top X\)</span> is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix’s condition number and thus algorithm performance.</p>
</li>
</ul>
<h2 id="convex-12_vector-24-norms-and-distances">2.4 Norms and distances<a class="headerlink" href="#convex-12_vector-24-norms-and-distances" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(\|\cdot\|: \mathbb{R}^n \to \mathbb{R}\)</span> is a norm if for all <span class="arithmatex">\(x,y\)</span> and scalar <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\|x\| \ge 0\)</span> and <span class="arithmatex">\(\|x\| = 0 \iff x=0\)</span>,</li>
<li><span class="arithmatex">\(\|\alpha x\| = |\alpha|\|x\|\)</span> (absolute homogeneity),</li>
<li><span class="arithmatex">\(\|x+y\| \le \|x\| + \|y\|\)</span> (triangle inequality).</li>
</ol>
<p>If the vector space has an inner product, the norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x\rangle}\)</span> is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry.<br>
Common examples on <span class="arithmatex">\(\mathbb{R}^n\)</span>:  </p>
<ul>
<li>
<p><span class="arithmatex">\(\ell_2\)</span> norm (Euclidean): <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span>, the usual length in space.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>, measuring taxicab distance. In <span class="arithmatex">\(\mathbb{R}^2\)</span>, its unit ball is a diamond.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span>, measuring the largest coordinate magnitude. Its unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a square.  </p>
</li>
<li>
<p>General <span class="arithmatex">\(\ell_p\)</span> norm: <span class="arithmatex">\(\|x\|_p = \left(\sum_i |x_i|^p\right)^{1/p}\)</span> for <span class="arithmatex">\(p\ge1\)</span>. This interpolates between <span class="arithmatex">\(\ell_1\)</span> and <span class="arithmatex">\(\ell_2\)</span>, and approaches <span class="arithmatex">\(\ell_\infty\)</span> as <span class="arithmatex">\(p\to\infty\)</span>. All <span class="arithmatex">\(\ell_p\)</span> norms are convex and satisfy the norm axioms.  </p>
</li>
</ul>
<p>Every norm induces a metric (distance) <span class="arithmatex">\(d(x,y) = |x-y|\)</span> on the space. Norms thus define the shape of “balls” (sets <span class="arithmatex">\({x: |x|\le \text{constant}}\)</span>) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm’s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../convex/images/norms.png" data-desc-position="bottom"><img alt="Alt text" src="../convex/images/norms.png" style="float:right; margin-right:15px; width:400px;"></a></p>
<p>Unit-ball geometry: The shape of the unit ball <span class="arithmatex">\({x: |x| \le 1}\)</span> reveals how a norm treats different directions. For example, the <span class="arithmatex">\(\ell_2\)</span> unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a perfect circle, treating all directions uniformly, whereas the <span class="arithmatex">\(\ell_1\)</span> unit ball is a diamond with corners along the axes, indicating that <span class="arithmatex">\(\ell_1\)</span> treats the coordinate axes as special (those are “cheaper” directions since the ball extends further along axes, touching them at <span class="arithmatex">\((\pm1,0)\)</span> and <span class="arithmatex">\((0,\pm1)\)</span>). The <span class="arithmatex">\(\ell_\infty\)</span> unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (<span class="arithmatex">\(\ell_1\)</span>), green circle (<span class="arithmatex">\(\ell_2\)</span>), and blue square (<span class="arithmatex">\(\ell_\infty\)</span>) in <span class="arithmatex">\(\mathbb{R}^2\)</span> . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an <span class="arithmatex">\(\ell_1\)</span> norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an <span class="arithmatex">\(\ell_2\)</span> ball encourages more evenly-distributed changes. An <span class="arithmatex">\(\ell_\infty\)</span> constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p>
<p>Dual norms: Each norm <span class="arithmatex">\(\|\cdot\|\)</span> has a dual norm <span class="arithmatex">\(\|\cdot\|_*\)</span> defined by
<script type="math/tex; mode=display">
\|y\|_* = \sup_{\|x\|\le 1} x^\top y~.
</script>
For example, the dual of <span class="arithmatex">\(\ell_1\)</span> is <span class="arithmatex">\(\ell_\infty\)</span>, and the dual of <span class="arithmatex">\(\ell_2\)</span> is itself.</p>
<blockquote>
<p>Imagine the vector <span class="arithmatex">\(x\)</span> lives inside the original norm ball (<span class="arithmatex">\(\|x\| \le 1\)</span>). The term <span class="arithmatex">\(x^\top y\)</span> is the dot product, which measures the alignment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>. The dual norm <span class="arithmatex">\(\|y\|_*\)</span> is the maximum possible value you can get by taking the dot product of <span class="arithmatex">\(y\)</span> with any vector <span class="arithmatex">\(x\)</span> that fits inside the original norm ball.If the dual norm <span class="arithmatex">\(\|y\|_*\)</span> is large, it means <span class="arithmatex">\(y\)</span> is strongly aligned with a direction <span class="arithmatex">\(x\)</span> that is "small" (size <span class="arithmatex">\(\le 1\)</span>) according to the original norm.If the dual norm is small, <span class="arithmatex">\(y\)</span> must be poorly aligned with all vectors <span class="arithmatex">\(x\)</span> in the ball.</p>
</blockquote>
<p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use <span class="arithmatex">\(\ell_\infty\)</span> (since one coordinate move at a time is like a step in <span class="arithmatex">\(\ell_\infty\)</span> unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using <span class="arithmatex">\(\ell_1\)</span> norm for sparse problems). The norm also figures in complexity bounds: an algorithm’s convergence rate may depend on the diameter of the feasible set in the chosen norm, <span class="arithmatex">\(D = \max_{\text{feasible}}|x - x^*|\)</span>. For instance, in subgradient methods, having a smaller <span class="arithmatex">\(\ell_2\)</span> diameter or <span class="arithmatex">\(\ell_1\)</span> diameter can improve bounds. Moreover, when constraints are given by norms (like <span class="arithmatex">\(|x|_1 \le t\)</span>), projections and proximal operators with respect to that norm become subroutines in algorithms.</p>
<p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (<span class="arithmatex">\(|x_k - x^*|\)</span>), how to constrain solutions (<span class="arithmatex">\(|x| \le R\)</span>), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>
<h2 id="convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices">2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices<a class="headerlink" href="#convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{n\times n}\)</span> is linear, a nonzero <span class="arithmatex">\(v\)</span> is an eigenvector with eigenvalue <span class="arithmatex">\(\lambda\)</span> if</p>
<div class="arithmatex">\[
Av = \lambda v~.
\]</div>
<p>When <span class="arithmatex">\(A\)</span> is symmetric (<span class="arithmatex">\(A = A^\top\)</span>), it has:</p>
<ul>
<li>real eigenvalues,</li>
<li>an orthonormal eigenbasis,</li>
<li>a spectral decomposition</li>
</ul>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^\top,
</script>
where <span class="arithmatex">\(Q\)</span> is orthonormal and <span class="arithmatex">\(\Lambda\)</span> is diagonal.</p>
<p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along <span class="arithmatex">\(n\)</span> orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by <span class="arithmatex">\(\lambda_i\)</span>.</p>
<blockquote>
<p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p>
<p>In optimization, the Hessian matrix of a multivariate function <span class="arithmatex">\(f(x)\)</span> is symmetric. Its eigenvalues <span class="arithmatex">\(\lambda_i(\nabla^2 f(x))\)</span> tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there’s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p>
</blockquote>
<p>Positive semidefinite matrices: A symmetric matrix <span class="arithmatex">\(Q\)</span> is positive semidefinite (PSD) if</p>
<div class="arithmatex">\[
x^\top Q x \ge 0 \quad \text{for all } x~.
\]</div>
<p>If <span class="arithmatex">\(x^\top Q x &gt; 0\)</span> for all <span class="arithmatex">\(x\ne 0\)</span>, then <span class="arithmatex">\(Q\)</span> is positive definite (PD).</p>
<p>Why this matters: if <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top Q x + c^\top x + d\)</span>, then</p>
<div class="arithmatex">\[
\nabla^2 f(x) = Q~.
\]</div>
<p>So <span class="arithmatex">\(f\)</span> is convex iff <span class="arithmatex">\(Q\)</span> is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p>
<blockquote>
<p>Implications of definiteness: If <span class="arithmatex">\(A \succ 0\)</span>, the quadratic function <span class="arithmatex">\(x^T A x\)</span> is strictly convex and has a unique minimizer at <span class="arithmatex">\(x=0\)</span>. If <span class="arithmatex">\(A \succeq 0\)</span>, <span class="arithmatex">\(x^T A x\)</span> is convex but could be flat in some directions (if some <span class="arithmatex">\(\lambda_i = 0\)</span>, those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> means <span class="arithmatex">\(f\)</span> has a unique local (and global, if domain convex) minimum at that <span class="arithmatex">\(x\)</span> (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater’s condition for strong duality.</p>
<p>Condition number and convergence: For iterative methods on convex quadratics <span class="arithmatex">\(f(x) = \frac{1}{2}x^T Q x - b^T x\)</span>, the eigenvalues of <span class="arithmatex">\(Q\)</span> dictate convergence speed. Gradient descent’s error after <span class="arithmatex">\(k\)</span> steps satisfies roughly <span class="arithmatex">\(|x_k - x^*| \le (\frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}})^k |x_0 - x^*|\)</span> (for normalized step). So the ratio <span class="arithmatex">\(\frac{\lambda_{\max}}{\lambda_{\min}} = \kappa(Q)\)</span> appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton’s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to <span class="arithmatex">\(\kappa\)</span> (locally). This explains why second-order methods shine on ill-conditioned problems: they “whiten” the curvature by dividing by eigenvalues.</p>
<p>Optimization interpretation of eigenvectors: The eigenvectors of <span class="arithmatex">\(\nabla^2 f(x^*)\)</span> at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton’s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>
</blockquote>
<h2 id="convex-12_vector-26-orthogonal-projections-and-least-squares">2.6 Orthogonal projections and least squares<a class="headerlink" href="#convex-12_vector-26-orthogonal-projections-and-least-squares" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(S\)</span> be a subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span>. The orthogonal projection of a vector <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is the unique vector <span class="arithmatex">\(p \in S\)</span> minimising <span class="arithmatex">\(\|b - p\|_2\)</span>. Geometrically, <span class="arithmatex">\(p\)</span> is the closest point in <span class="arithmatex">\(S\)</span> to <span class="arithmatex">\(b\)</span>. If <span class="arithmatex">\(S = \mathrm{span}\{a_1,\dots,a_k\}\)</span> and <span class="arithmatex">\(A = [a_1~\cdots~a_k]\)</span>, then projecting <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is equivalent to solving the least-squares problem</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2~.
\]</div>
<p>The solution <span class="arithmatex">\(x^*\)</span> satisfies the normal equations</p>
<div class="arithmatex">\[
A^\top A x^* = A^\top b~.
\]</div>
<p>This is our first real convex optimisation problem:</p>
<ul>
<li>the objective <span class="arithmatex">\(\|Ax-b\|_2^2\)</span> is convex,</li>
<li>there are no constraints,</li>
<li>we can solve it in closed form.</li>
</ul>
<h2 id="convex-12_vector-27-operator-norms-singular-values-and-spectral-structure">2.7 Operator norms, singular values, and spectral structure<a class="headerlink" href="#convex-12_vector-27-operator-norms-singular-values-and-spectral-structure" title="Permanent link">¶</a></h2>
<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>
<h3 id="convex-12_vector-operator-norms">Operator norms<a class="headerlink" href="#convex-12_vector-operator-norms" title="Permanent link">¶</a></h3>
<p>Given a matrix <span class="arithmatex">\(A : \mathbb{R}^n \to \mathbb{R}^m\)</span> and norms <span class="arithmatex">\(\|\cdot\|_p\)</span> on <span class="arithmatex">\(\mathbb{R}^n\)</span> and <span class="arithmatex">\(\|\cdot\|_q\)</span> on <span class="arithmatex">\(\mathbb{R}^m\)</span>, the induced operator norm is defined as
<script type="math/tex; mode=display">
\|A\|_{p \to q}
=
\sup_{x \neq 0}
\frac{\|A x\|_q}{\|x\|_p}
=
\sup_{\|x\|_p \le 1} \|A x\|_q.
</script>
This quantity measures the largest amount by which <span class="arithmatex">\(A\)</span> can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p>
<ul>
<li><span class="arithmatex">\(\|A\|_{2 \to 2}\)</span>, the spectral norm, equals the largest singular value of <span class="arithmatex">\(A\)</span>.</li>
<li><span class="arithmatex">\(\|A\|_{1 \to 1}\)</span> is the maximum absolute column sum.</li>
<li><span class="arithmatex">\(\|A\|_{\infty \to \infty}\)</span> is the maximum absolute row sum.</li>
</ul>
<p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function<br>
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2} x^\top Q x - b^\top x
</script>
converges for step sizes <span class="arithmatex">\(\alpha &lt; 2 / \|Q\|_2\)</span>. This shows that controlling the operator norm of the Hessian—or a Lipschitz constant of the gradient—directly governs how aggressively an algorithm can move.</p>
<h3 id="convex-12_vector-singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#convex-12_vector-singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>Any matrix <span class="arithmatex">\(A \in \mathbb{R}^{m \times n}\)</span> admits a factorization
<script type="math/tex; mode=display">
A = U \Sigma V^\top,
</script>
where <span class="arithmatex">\(U\)</span> and <span class="arithmatex">\(V\)</span> are orthogonal matrices and <span class="arithmatex">\(\Sigma\)</span> is diagonal with nonnegative entries <span class="arithmatex">\(\sigma_1 \ge \sigma_2 \ge \cdots\)</span>. The <span class="arithmatex">\(\sigma_i\)</span> are the singular values of <span class="arithmatex">\(A\)</span>.</p>
<p>Geometrically, the SVD shows how <span class="arithmatex">\(A\)</span> transforms the unit ball into an ellipsoid. The columns of <span class="arithmatex">\(V\)</span> give the principal input directions, the singular values are the lengths of the ellipsoid’s axes, and the columns of <span class="arithmatex">\(U\)</span> give the output directions. The largest singular value <span class="arithmatex">\(\sigma_{\max}\)</span> equals the spectral norm <span class="arithmatex">\(\|A\|_2\)</span>, while the smallest <span class="arithmatex">\(\sigma_{\min}\)</span> describes the least expansion (or exact flattening if <span class="arithmatex">\(\sigma_{\min} = 0\)</span>).</p>
<p>SVD is a powerful diagnostic tool in optimization. The ratio
<script type="math/tex; mode=display">
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
</script>
is the condition number of <span class="arithmatex">\(A\)</span>. A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means <span class="arithmatex">\(A\)</span> behaves more like a uniform scaling, which is ideal for optimization.</p>
<h3 id="convex-12_vector-low-rank-structure">Low-rank structure<a class="headerlink" href="#convex-12_vector-low-rank-structure" title="Permanent link">¶</a></h3>
<p>The rank of <span class="arithmatex">\(A\)</span> is the number of nonzero singular values. When <span class="arithmatex">\(A\)</span> has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p>
<p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>
<h3 id="convex-12_vector-operator-norms-and-optimization-algorithms">Operator norms and optimization algorithms<a class="headerlink" href="#convex-12_vector-operator-norms-and-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>Operator norms help determine step sizes, convergence rates, and preconditioning strategies. For a general smooth convex function, the Lipschitz constant of its gradient often corresponds to a spectral norm of a Hessian or Jacobian, and this constant controls the safe step size for gradient descent. Preconditioning modifies the geometry of the problem—changing the inner product or scaling the variables—in order to reduce the effective operator norm and improve conditioning.</p>
<p>These spectral considerations appear in both first-order and second-order methods. Newton’s method, for example, implicitly rescales the space using the inverse Hessian, which equalizes curvature by transforming eigenvalues toward 1. This explains its rapid local convergence when the Hessian is well behaved.</p>
<h3 id="convex-12_vector-summary">Summary<a class="headerlink" href="#convex-12_vector-summary" title="Permanent link">¶</a></h3>
<ul>
<li>The operator norm measures the maximum stretching effect of a matrix.</li>
<li>Singular values give a complete geometric description of this stretching.</li>
<li>The condition number captures how unevenly the matrix acts in different directions.</li>
<li>Low-rank structure reveals underlying dimension and enables efficient computation.</li>
<li>All of these properties strongly influence the behavior and design of optimization algorithms.</li>
</ul>
<p>Understanding operator norms and singular values provides valuable insight into when optimization problems are well conditioned, how algorithms will behave, and how to modify a problem to improve performance.</p></body></html></section><section class="print-page" id="convex-13_calculus" heading-number="2.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-multivariable-calculus-for-optimization">Chapter 3: Multivariable Calculus for Optimization<a class="headerlink" href="#convex-13_calculus-chapter-3-multivariable-calculus-for-optimization" title="Permanent link">¶</a></h1>
<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p>
<p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton’s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p>
<p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>
<h2 id="convex-13_calculus-31-gradients-and-directional-derivatives">3.1 Gradients and Directional Derivatives<a class="headerlink" href="#convex-13_calculus-31-gradients-and-directional-derivatives" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>. The function is differentiable at a point <span class="arithmatex">\(x\)</span> if there exists a vector <span class="arithmatex">\(\nabla f(x)\)</span> such that
<script type="math/tex; mode=display">
f(x + h)
=
f(x) + \nabla f(x)^\top h + o(\|h\|),
</script>
meaning that the linear function <span class="arithmatex">\(h \mapsto \nabla f(x)^\top h\)</span> provides the best local approximation to <span class="arithmatex">\(f\)</span> near <span class="arithmatex">\(x\)</span>. The gradient is the unique vector with this property.</p>
<p>A closely related concept is the directional derivative. For any direction <span class="arithmatex">\(v \in \mathbb{R}^n\)</span>, the directional derivative of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> in the direction <span class="arithmatex">\(v\)</span> is
<script type="math/tex; mode=display">
D_v f(x)
=
\lim_{t \to 0} \frac{f(x + tv) - f(x)}{t}.
</script>
If <span class="arithmatex">\(f\)</span> is differentiable, then
<script type="math/tex; mode=display">
D_v f(x) = \nabla f(x)^\top v.
</script>
Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction <span class="arithmatex">\(v\)</span> tells us how rapidly <span class="arithmatex">\(f\)</span> increases when we move infinitesimally along <span class="arithmatex">\(v\)</span>.</p>
<p>This immediately yields an important geometric fact. Among all unit directions <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
D_u f(x) = \langle \nabla f(x), u \rangle
</script>
is maximized when <span class="arithmatex">\(u\)</span> points in the direction of <span class="arithmatex">\(\nabla f(x)\)</span>, the direction of steepest ascent. The steepest descent direction is therefore <span class="arithmatex">\(-\nabla f(x)\)</span>, which motivates gradient-descent algorithms for minimizing functions.</p>
<blockquote>
<p>For any real number <span class="arithmatex">\(c\)</span>, the level set of <span class="arithmatex">\(f\)</span> is 
<script type="math/tex; mode=display">
L_c = \{\, x \in \mathbb{R}^n : f(x) = c \,\}.
</script>
</p>
<p>At any point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x) \ne 0\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to the level set <span class="arithmatex">\(L_{f(x)}\)</span>. Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them — in the direction of the steepest ascent of <span class="arithmatex">\(f\)</span>. If we wish to decrease <span class="arithmatex">\(f\)</span>, we move roughly in the opposite direction, <span class="arithmatex">\(-\nabla f(x)\)</span> (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>
</blockquote>
<h2 id="convex-13_calculus-32-jacobians">3.2  Jacobians<a class="headerlink" href="#convex-13_calculus-32-jacobians" title="Permanent link">¶</a></h2>
<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>
<h3 id="convex-13_calculus-from-derivative-to-gradient">From derivative to gradient<a class="headerlink" href="#convex-13_calculus-from-derivative-to-gradient" title="Permanent link">¶</a></h3>
<p>For a scalar function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, differentiability means that near any point <script type="math/tex"> x </script>,
<script type="math/tex; mode=display">
f(x + h) \approx f(x) + \nabla f(x)^\top h.
</script>
The gradient vector
<script type="math/tex; mode=display">
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x) \\
\vdots \\
\frac{\partial f}{\partial x_n}(x)
\end{bmatrix}
</script>
collects all partial derivatives. Each component measures how sensitive <span class="arithmatex">\(f\)</span> is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>
<h3 id="convex-13_calculus-from-gradient-to-jacobian">From gradient to Jacobian<a class="headerlink" href="#convex-13_calculus-from-gradient-to-jacobian" title="Permanent link">¶</a></h3>
<p>Now consider a vector-valued function <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span>,
<script type="math/tex; mode=display">
F(x) =
\begin{bmatrix}
F_1(x) \\
\vdots \\
F_m(x)
\end{bmatrix}.
</script>
Each output <span class="arithmatex">\(F_i\)</span> has its own gradient. Stacking these row vectors yields the Jacobian matrix:
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>The Jacobian provides the best linear approximation of <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x)\, h.
</script>
Thus, locally, the nonlinear map <span class="arithmatex">\(F\)</span> behaves like the linear map <span class="arithmatex">\(h \mapsto J_F(x)h\)</span>. A small displacement <span class="arithmatex">\(h\)</span> in input space is transformed into an output change governed by the Jacobian.</p>
<h3 id="convex-13_calculus-interpreting-the-jacobian">Interpreting the Jacobian<a class="headerlink" href="#convex-13_calculus-interpreting-the-jacobian" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Component of <span class="arithmatex">\(J_F(x)\)</span></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row <span class="arithmatex">\(i\)</span></td>
<td>Gradient of output <span class="arithmatex">\(F_i(x)\)</span>: how the <span class="arithmatex">\(i\)</span>-th output changes with each input variable.</td>
</tr>
<tr>
<td>Column <span class="arithmatex">\(j\)</span></td>
<td>Sensitivity of all outputs to <span class="arithmatex">\(x_j\)</span>: how varying input <span class="arithmatex">\(x_j\)</span> affects the entire output vector.</td>
</tr>
<tr>
<td>Determinant (when <span class="arithmatex">\(m=n\)</span>)</td>
<td>Local volume scaling: how <span class="arithmatex">\(F\)</span> expands or compresses space near <span class="arithmatex">\(x\)</span>.</td>
</tr>
<tr>
<td>Rank</td>
<td>Local dimension of the image: whether any input directions are lost or collapsed.</td>
</tr>
</tbody>
</table>
<p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>
<h2 id="convex-13_calculus-33-the-hessian-and-curvature">3.3 The Hessian and Curvature<a class="headerlink" href="#convex-13_calculus-33-the-hessian-and-curvature" title="Permanent link">¶</a></h2>
<p>For a twice–differentiable function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, the Hessian matrix collects all second-order partial derivatives:
<script type="math/tex; mode=display">
\nabla^{2} f(x) \;=\;
\begin{bmatrix}
\frac{\partial^{2} f}{\partial x_{1}^{2}} & \cdots & \frac{\partial^{2} f}{\partial x_{1}\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial^{2} f}{\partial x_{n}\partial x_{1}} & \cdots & \frac{\partial^{2} f}{\partial x_{n}^{2}}
\end{bmatrix}.
</script>
</p>
<p>The Hessian describes the <strong>local curvature</strong> of the function. While the gradient indicates the direction of steepest change, the Hessian tells us <em>how that directional change itself varies</em>—whether the surface curves upward, curves downward, or remains nearly flat.</p>
<h3 id="convex-13_calculus-curvature-and-positive-definiteness">Curvature and positive definiteness<a class="headerlink" href="#convex-13_calculus-curvature-and-positive-definiteness" title="Permanent link">¶</a></h3>
<p>The eigenvalues of the Hessian determine its geometric behavior:</p>
<ul>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succeq 0 </script> (all eigenvalues nonnegative), the function is locally convex near <span class="arithmatex">\(x\)</span>.  </li>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succ 0 </script>, the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li>
<li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li>
</ul>
<p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>
<h3 id="convex-13_calculus-example-quadratic-functions">Example: Quadratic functions<a class="headerlink" href="#convex-13_calculus-example-quadratic-functions" title="Permanent link">¶</a></h3>
<p>Consider the quadratic function
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2} x^\top Q x - b^\top x,
</script>
where <span class="arithmatex">\(Q\)</span> is symmetric. The gradient and Hessian are
<script type="math/tex; mode=display">
\nabla f(x) = Qx - b, \qquad \nabla^2 f(x) = Q.
</script>
Setting the gradient to zero gives the stationary point
<script type="math/tex; mode=display">
Qx = b.
</script>
If <span class="arithmatex">\(Q \succ 0\)</span>, the solution
<script type="math/tex; mode=display">
x^* = Q^{-1} b
</script>
is the unique minimizer. The Hessian <span class="arithmatex">\(Q\)</span> being positive definite confirms strict convexity.</p>
<p>The eigenvalues of <span class="arithmatex">\(Q\)</span> also explain the difficulty of minimizing <span class="arithmatex">\(f\)</span>:</p>
<ul>
<li>Large eigenvalues produce very steep, narrow directions—optimization methods must take small steps.  </li>
<li>Small eigenvalues produce flat directions—progress is slow, especially for gradient descent.  </li>
</ul>
<p>The ratio of largest to smallest eigenvalue, the <strong>condition number</strong>, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>
<h3 id="convex-13_calculus-why-the-hessian-matters-in-optimization">Why the Hessian matters in optimization<a class="headerlink" href="#convex-13_calculus-why-the-hessian-matters-in-optimization" title="Permanent link">¶</a></h3>
<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p>
<ul>
<li>Newton’s method uses the Hessian to rescale directions, effectively “whitening’’ curvature and often converging rapidly.  </li>
<li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li>
<li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li>
</ul>
<p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>
<h2 id="convex-13_calculus-34-taylor-approximation">3.4 Taylor approximation<a class="headerlink" href="#convex-13_calculus-34-taylor-approximation" title="Permanent link">¶</a></h2>
<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>
<h3 id="convex-13_calculus-first-order-approximation">First-order approximation<a class="headerlink" href="#convex-13_calculus-first-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then for small steps <span class="arithmatex">\(d\)</span>,
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x) + \nabla f(x)^\top d.
</script>
The gradient gives the best linear model of the function near <span class="arithmatex">\(x\)</span>. This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>
<h3 id="convex-13_calculus-second-order-approximation">Second-order approximation<a class="headerlink" href="#convex-13_calculus-second-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is twice differentiable, we can include curvature information:
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x)
+ \nabla f(x)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x)\, d.
</script>
The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p>
<ul>
<li>If <script type="math/tex"> \nabla^2 f(x) \succeq 0 </script>, the quadratic term is nonnegative and the function curves upward—locally bowl-shaped.</li>
<li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others—characteristic of saddle points.</li>
</ul>
<h3 id="convex-13_calculus-role-in-optimization-algorithms">Role in optimization algorithms<a class="headerlink" href="#convex-13_calculus-role-in-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>Second-order Taylor models are the basis of Newton-type methods. Newton’s method chooses <span class="arithmatex">\(d\)</span> by approximately minimizing the quadratic model,
<script type="math/tex; mode=display">
d \approx - \left(\nabla^2 f(x)\right)^{-1} \nabla f(x),
</script>
which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p>
<p>Thus, Taylor expansions connect a function’s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>
<h2 id="convex-13_calculus-35-smoothness-and-strong-convexity">3.5 Smoothness and Strong Convexity<a class="headerlink" href="#convex-13_calculus-35-smoothness-and-strong-convexity" title="Permanent link">¶</a></h2>
<p>In optimization, the behavior of a function’s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>
<h3 id="convex-13_calculus-lipschitz-continuous-gradients-l-smoothness">Lipschitz continuous gradients (L-smoothness)<a class="headerlink" href="#convex-13_calculus-lipschitz-continuous-gradients-l-smoothness" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> has an <span class="arithmatex">\(L\)</span>-Lipschitz continuous gradient if
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\| \qquad \forall x, y.
</script>
This condition limits how quickly the gradient can change. Intuitively, an <span class="arithmatex">\(L\)</span>-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:
<script type="math/tex; mode=display">
f(y)
\le
f(x)
+
\nabla f(x)^\top (y - x)
+
\frac{L}{2}\|y - x\|^2.
</script>
This inequality states that every <span class="arithmatex">\(L\)</span>-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p>
<p>In gradient descent, smoothness directly determines a safe step size: choosing
<script type="math/tex; mode=display">
\eta \le \frac{1}{L}
</script>
ensures that each update decreases the function value for convex objectives. In machine learning, the constant <span class="arithmatex">\(L\)</span> effectively controls how large the learning rate can be before training becomes unstable.</p>
<h3 id="convex-13_calculus-strong-convexity">Strong convexity<a class="headerlink" href="#convex-13_calculus-strong-convexity" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> is <script type="math/tex"> \mu </script>-strongly convex if, for some <script type="math/tex"> \mu > 0 </script>,
<script type="math/tex; mode=display">
f(y)
\ge
f(x)
+
\langle \nabla f(x),\, y - x \rangle
+
\frac{\mu}{2}\|y - x\|^2
\qquad \forall x, y.
</script>
This condition guarantees that <span class="arithmatex">\(f\)</span> has at least <span class="arithmatex">\(\mu\)</span> amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p>
<p>Strong convexity has major optimization implications:</p>
<ul>
<li>The minimizer is unique.  </li>
<li>Gradient descent converges linearly with step size <span class="arithmatex">\(\eta \le 1/L\)</span>.  </li>
<li>The ratio <span class="arithmatex">\(L / \mu\)</span> (the condition number) dictates convergence speed.</li>
</ul>
<h3 id="convex-13_calculus-curvature-in-both-directions">Curvature in both directions<a class="headerlink" href="#convex-13_calculus-curvature-in-both-directions" title="Permanent link">¶</a></h3>
<p>Together, smoothness and strong convexity bound the curvature of <span class="arithmatex">\(f\)</span>:
<script type="math/tex; mode=display">
\mu I \;\preceq\; \nabla^2 f(x) \;\preceq\; L I.
</script>
Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p>
<p>These concepts—, imiting curvature from above via <span class="arithmatex">\(L\)</span> and from below via <span class="arithmatex">\(\mu\)</span>, form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p></body></html></section><section class="print-page" id="convex-14_convexsets" heading-number="2.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-4-convex-sets-and-geometric-fundamentals">Chapter 4: Convex Sets and Geometric Fundamentals<a class="headerlink" href="#convex-14_convexsets-chapter-4-convex-sets-and-geometric-fundamentals" title="Permanent link">¶</a></h1>
<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p>
<p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>
<h2 id="convex-14_convexsets-41-convex-sets">4.1 Convex sets<a class="headerlink" href="#convex-14_convexsets-41-convex-sets" title="Permanent link">¶</a></h2>
<p>A set <script type="math/tex"> C \subseteq \mathbb{R}^n </script> is convex if for any two points <script type="math/tex"> x, y \in C </script> and any <script type="math/tex"> \theta \in [0,1] </script>,
<script type="math/tex; mode=display">
\theta x + (1 - \theta) y \in C.
</script>
That is, the entire line segment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> lies inside the set. Convex sets have no “holes” or “indentations,” and this geometric regularity is what makes optimization over them tractable.</p>
<h3 id="convex-14_convexsets-examples">Examples<a class="headerlink" href="#convex-14_convexsets-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Affine subspaces: <script type="math/tex"> \{ x : Ax = b \} </script>.  </li>
<li>Halfspaces: <script type="math/tex"> \{ x : a^\top x \le b \} </script>.  </li>
<li>Euclidean balls: <script type="math/tex"> \{ x : \|x\|_2 \le r \} </script>.  </li>
<li>
<script type="math/tex"> \ell_\infty </script> balls (axis-aligned boxes): <script type="math/tex"> \{ x : \|x\|_\infty \le r \} </script>.  </li>
<li>Probability simplex: <script type="math/tex"> \{ x \in \mathbb{R}^n : x \ge 0, \ \sum_i x_i = 1 \} </script>.  </li>
</ul>
<p>A set fails to be convex whenever some segment between two feasible points leaves the set—for example, a crescent or an annulus.</p>
<h2 id="convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces">4.2 Affine sets, hyperplanes, and halfspaces<a class="headerlink" href="#convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces" title="Permanent link">¶</a></h2>
<p>Affine sets generalize linear subspaces by allowing a shift. A set <span class="arithmatex">\(A\)</span> is affine if for some point <span class="arithmatex">\(x_0\)</span> and subspace <span class="arithmatex">\(S\)</span>,
<script type="math/tex; mode=display">
A = \{ x_0 + v : v \in S \}.
</script>
Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p>
<p>A hyperplane is an affine set defined by a single linear equation:
<script type="math/tex; mode=display">
H = \{ x : a^\top x = b \}, \qquad a \neq 0.
</script>
Hyperplanes act as the “flat boundaries” of higher-dimensional space and are the fundamental building blocks of polyhedra.</p>
<p>A halfspace is one side of a hyperplane:
<script type="math/tex; mode=display">
\{ x : a^\top x \le b \}.
</script>
Halfspaces are convex and serve as basic local approximations to general convex sets.</p>
<h2 id="convex-14_convexsets-43-convex-combinations-and-convex-hulls">4.3 Convex combinations and convex hulls<a class="headerlink" href="#convex-14_convexsets-43-convex-combinations-and-convex-hulls" title="Permanent link">¶</a></h2>
<p>A convex combination of points <script type="math/tex"> x_1, \dots, x_k </script> is a weighted average
<script type="math/tex; mode=display">
\sum_{i=1}^k \theta_i x_i, 
\qquad
\theta_i \ge 0, \qquad \sum_{i=1}^k \theta_i = 1.
</script>
Convex sets are precisely those that contain all convex combinations of their points.</p>
<p>The convex hull of a set <span class="arithmatex">\(S\)</span>, denoted <span class="arithmatex">\(\operatorname{conv}(S)\)</span>, is the set of all convex combinations of finitely many points in <span class="arithmatex">\(S\)</span>. It is the smallest convex set containing <span class="arithmatex">\(S\)</span>. Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p>
<p>Convex hulls are important because:</p>
<ul>
<li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li>
<li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li>
</ul>
<h2 id="convex-14_convexsets-44-polyhedra-and-polytopes">4.4 Polyhedra and polytopes<a class="headerlink" href="#convex-14_convexsets-44-polyhedra-and-polytopes" title="Permanent link">¶</a></h2>
<p>A polyhedron is an intersection of finitely many halfspaces:
<script type="math/tex; mode=display">
P = \{ x : Ax \le b \}.
</script>
Polyhedra are always convex; they may be bounded or unbounded.</p>
<p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>
<h2 id="convex-14_convexsets-45-extreme-points">4.5 Extreme points<a class="headerlink" href="#convex-14_convexsets-45-extreme-points" title="Permanent link">¶</a></h2>
<p>Let <script type="math/tex"> C </script> be a convex set. A point <span class="arithmatex">\(x \in C\)</span> is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if
<script type="math/tex; mode=display">
x = \theta y + (1 - \theta) z,
\qquad 0 < \theta < 1, \qquad y, z \in C,
</script>
implies <script type="math/tex"> y = z = x </script>.</p>
<p>Geometrically, extreme points are the “corners” of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems—such as linear programs—achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>
<h2 id="convex-14_convexsets-46-cones">4.6 Cones<a class="headerlink" href="#convex-14_convexsets-46-cones" title="Permanent link">¶</a></h2>
<p>Cones generalize the idea of “directions” in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>
<h3 id="convex-14_convexsets-basic-definition">Basic definition<a class="headerlink" href="#convex-14_convexsets-basic-definition" title="Permanent link">¶</a></h3>
<p>A set <span class="arithmatex">\(K \subseteq \mathbb{R}^n\)</span> is a cone if
<script type="math/tex; mode=display">
x \in K, \ \alpha \ge 0
\quad\Longrightarrow\quad
\alpha x \in K.
</script>
A cone is convex if it is also closed under addition:
<script type="math/tex; mode=display">
x, y \in K \quad\Longrightarrow\quad x + y \in K.
</script>
</p>
<p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant <script type="math/tex"> \mathbb{R}^n_{\ge 0} </script>, each coordinate axis direction is an extreme ray.</p>
<h3 id="convex-14_convexsets-conic-hull">Conic hull<a class="headerlink" href="#convex-14_convexsets-conic-hull" title="Permanent link">¶</a></h3>
<p>Given any set <span class="arithmatex">\(S\)</span>, its conic hull is the set of all conic combinations:
<script type="math/tex; mode=display">
\operatorname{cone}(S)
=
\left\{
\sum_{i=1}^k \alpha_i s_i : \alpha_i \ge 0,\ s_i \in S
\right\}.
</script>
This is the smallest convex cone containing <span class="arithmatex">\(S\)</span>. Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>
<h3 id="convex-14_convexsets-polar-cones">Polar cones<a class="headerlink" href="#convex-14_convexsets-polar-cones" title="Permanent link">¶</a></h3>
<p>For a cone <span class="arithmatex">\(K\)</span>, the polar cone is defined as
<script type="math/tex; mode=display">
K^\circ
=
\left\{
y \in \mathbb{R}^n : \langle y, x \rangle \le 0 \ \forall x \in K
\right\}.
</script>
</p>
<p>Intuition:</p>
<ul>
<li>Polar vectors make a nonacute angle with every vector in <span class="arithmatex">\(K\)</span>.  </li>
</ul>
<p>Key properties:</p>
<ul>
<li><span class="arithmatex">\(K^\circ\)</span> is always a closed convex cone.  </li>
<li>If <span class="arithmatex">\(K\)</span> is a subspace, then <span class="arithmatex">\(K^\circ\)</span> is the orthogonal complement.  </li>
<li>For any closed convex cone,<br>
<script type="math/tex; mode=display">
  (K^\circ)^\circ = K.
  </script>
</li>
</ul>
<p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>
<h3 id="convex-14_convexsets-tangent-cones">Tangent cones<a class="headerlink" href="#convex-14_convexsets-tangent-cones" title="Permanent link">¶</a></h3>
<p>For a set <span class="arithmatex">\(C\)</span> and a point <span class="arithmatex">\(x \in C\)</span>, the tangent cone <span class="arithmatex">\(T_C(x)\)</span> consists of all feasible “infinitesimal directions” from <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
T_C(x)
=
\left\{
d : \exists\, t_k \downarrow 0,\ x_k \in C,\ x_k \to x,\ 
\frac{x_k - x}{t_k} \to d
\right\}.
</script>
</p>
<p>Intuition:</p>
<ul>
<li>At an interior point, <span class="arithmatex">\(T_C(x) = \mathbb{R}^n\)</span>: all small moves are allowed.  </li>
<li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li>
</ul>
<p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>
<h3 id="convex-14_convexsets-normal-cones">Normal cones<a class="headerlink" href="#convex-14_convexsets-normal-cones" title="Permanent link">¶</a></h3>
<p>For a convex set <span class="arithmatex">\(C\)</span>, the normal cone at a point <span class="arithmatex">\(x \in C\)</span> is
<script type="math/tex; mode=display">
N_C(x)
=
\left\{
v : \langle v, y - x \rangle \le 0 \ \forall y \in C
\right\}.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>Every <span class="arithmatex">\(v \in N_C(x)\)</span> defines a supporting hyperplane to <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(x\)</span>.  </li>
<li>At interior points, the normal cone is <span class="arithmatex">\(\{0\}\)</span>.  </li>
<li>At boundary or corner points, it becomes a pointed cone of outward normals.</li>
</ul>
<p>A fundamental relationship ties tangent and normal cones together:
<script type="math/tex; mode=display">
N_C(x) = \big( T_C(x) \big)^\circ.
</script>
</p>
<p>Normal cones appear directly in first-order optimality conditions. For a constrained problem<br>
<script type="math/tex; mode=display">
\min_{x \in C} f(x),
</script>
a point <span class="arithmatex">\(x^*\)</span> is optimal only if
<script type="math/tex; mode=display">
0 \in \nabla f(x^*) + N_C(x^*).
</script>
This expresses a balance between the objective’s slope and the “pushback’’ from the constraint set.</p>
<p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>
<h2 id="convex-14_convexsets-47-supporting-hyperplanes-and-separation">4.7 Supporting Hyperplanes and Separation<a class="headerlink" href="#convex-14_convexsets-47-supporting-hyperplanes-and-separation" title="Permanent link">¶</a></h2>
<p>One of the most important geometric facts about convex sets is that they can be <em>supported</em> or <em>separated</em> by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>
<h3 id="convex-14_convexsets-supporting-hyperplane-theorem">Supporting Hyperplane Theorem<a class="headerlink" href="#convex-14_convexsets-supporting-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be nonempty, closed, and convex, and let <span class="arithmatex">\(x_0\)</span> be a boundary point of <span class="arithmatex">\(C\)</span>. Then there exists a nonzero vector <span class="arithmatex">\(a\)</span> such that</p>
<div class="arithmatex">\[
a^\top x \le a^\top x_0 \qquad \forall x \in C.
\]</div>
<p>This means that the hyperplane</p>
<div class="arithmatex">\[
a^\top x = a^\top x_0
\]</div>
<p>touches <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(x_0\)</span> but does not cut through it. The vector <span class="arithmatex">\(a\)</span> is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>
<h3 id="convex-14_convexsets-separating-hyperplane-theorem">Separating Hyperplane Theorem<a class="headerlink" href="#convex-14_convexsets-separating-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(C\)</span> and <span class="arithmatex">\(D\)</span> are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector <span class="arithmatex">\(a\)</span> and scalar <span class="arithmatex">\(b\)</span> such that</p>
<div class="arithmatex">\[
a^\top x \le b \quad \forall x \in C,
\qquad
a^\top y \ge b \quad \forall y \in D.
\]</div>
<p>The hyperplane <span class="arithmatex">\(a^\top x = b\)</span> places all points of <span class="arithmatex">\(C\)</span> on one side and all points of <span class="arithmatex">\(D\)</span> on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p>
<h3 id="convex-14_convexsets-why-this-matters-for-optimisation">Why This Matters for Optimisation<a class="headerlink" href="#convex-14_convexsets-why-this-matters-for-optimisation" title="Permanent link">¶</a></h3>
<p>These geometric results are central to convex optimisation:</p>
<ul>
<li>Subgradients correspond to supporting hyperplanes of the epigraph of a convex function.</li>
<li>Dual variables arise from separating infeasible points from the feasible region.</li>
<li>KKT conditions express the balance between the gradient of the objective and the normals of active constraints.</li>
<li>Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.</li>
</ul>
<p>Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p></body></html></section><section class="print-page" id="convex-15_convexfunctions" heading-number="2.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-convex-functions">Chapter 5: Convex Functions<a class="headerlink" href="#convex-15_convexfunctions-chapter-5-convex-functions" title="Permanent link">¶</a></h1>
<p>Convex functions play a central role in optimisation and machine learning. When the objective function is convex, the optimisation landscape has a single global minimum, gradient-based algorithms behave predictably, and optimality conditions have clean geometric interpretations. Many common ML losses—least squares, logistic loss, hinge loss, Huber loss—are convex precisely for these reasons.</p>
<p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>
<h2 id="convex-15_convexfunctions-51-definitions-of-convexity">5.1 Definitions of convexity<a class="headerlink" href="#convex-15_convexfunctions-51-definitions-of-convexity" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is convex if for all <span class="arithmatex">\(x,y\)</span> in its domain and all <span class="arithmatex">\(\theta \in [0,1]\)</span>,
<script type="math/tex; mode=display">
f(\theta x + (1-\theta) y)
\le
\theta f(x) + (1-\theta) f(y).
</script>
</p>
<p>The graph of <span class="arithmatex">\(f\)</span> never dips below the straight line between <span class="arithmatex">\((x,f(x))\)</span> and <span class="arithmatex">\((y,f(y))\)</span>. If the inequality is strict whenever <span class="arithmatex">\(x \neq y\)</span>, the function is <em>strictly convex</em>.</p>
<p>A powerful geometric viewpoint comes from the epigraph:
<script type="math/tex; mode=display">
\mathrm{epi}(f) = 
\{ (x,t) \in \mathbb{R}^n \times \mathbb{R} : f(x) \le t \}.
</script>
The function <span class="arithmatex">\(f\)</span> is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>
<h2 id="convex-15_convexfunctions-52-first-order-characterisation">5.2 First-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-52-first-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, then <span class="arithmatex">\(f\)</span> is convex if and only if
<script type="math/tex; mode=display">
f(y) \ge f(x) + \nabla f(x)^\top (y - x)
\quad\text{for all } x,y.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The tangent plane at any point <span class="arithmatex">\(x\)</span> lies below the function everywhere.</li>
<li><span class="arithmatex">\(\nabla f(x)\)</span> defines a supporting hyperplane to the epigraph.</li>
<li>The gradient provides a global linear underestimator of <span class="arithmatex">\(f\)</span>.</li>
</ul>
<p>This geometric picture is crucial in optimisation:
at a minimiser <span class="arithmatex">\(x^\star\)</span>, convexity implies<br>
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0 \quad \Longleftrightarrow \quad x^\star \text{ is a global minimiser}.
</script>
</p>
<p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>
<h2 id="convex-15_convexfunctions-53-second-order-characterisation">5.3 Second-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-53-second-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is twice continuously differentiable, then convexity can be checked via curvature:</p>
<div class="arithmatex">\[
f \text{ is convex } \iff \nabla^2 f(x) \succeq 0 \text{ for all } x.
\]</div>
<ul>
<li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li>
<li>If <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> everywhere, the function is strictly convex.  </li>
<li>Negative eigenvalues indicate directions of negative curvature — impossible for convex functions.</li>
</ul>
<p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>
<h2 id="convex-15_convexfunctions-54-examples-of-convex-functions">5.4 Examples of convex functions<a class="headerlink" href="#convex-15_convexfunctions-54-examples-of-convex-functions" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Affine functions:<br>
<script type="math/tex; mode=display">
   f(x) = a^\top x + b.
   </script>
<br>
   Always convex (and concave). They define supporting hyperplanes.</p>
</li>
<li>
<p>Quadratic functions with PSD Hessian:<br>
<script type="math/tex; mode=display">
   f(x) = \tfrac12 x^\top Q x + c^\top x + d,\quad Q \succeq 0.
   </script>
<br>
   Convex because the curvature matrix <span class="arithmatex">\(Q\)</span> is PSD.</p>
</li>
<li>
<p>Norms:<br>
<script type="math/tex; mode=display">
   f(x) = \|x\|_p \quad (p \ge 1).
   </script>
<br>
   All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p>
</li>
<li>
<p>Maximum of affine functions:<br>
<script type="math/tex; mode=display">
   f(x) = \max_i (a_i^\top x + b_i).
   </script>
<br>
   Convex because the maximum of convex functions is convex.<br>
   (Important in SVM hinge loss.)</p>
</li>
<li>
<p>Log-sum-exp:<br>
<script type="math/tex; mode=display">
   f(x) = \log\!\left( \sum_{i=1}^k \exp(a_i^\top x + b_i) \right).
   </script>
<br>
   A smooth approximation to the max; convex by Jensen’s inequality. Appears in softmax, logistic regression, partition functions.</p>
</li>
</ol>
<h2 id="convex-15_convexfunctions-55-jensens-inequality">5.5 Jensen’s inequality<a class="headerlink" href="#convex-15_convexfunctions-55-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex and <span class="arithmatex">\(X\)</span> a random variable in its domain. Then:
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>This generalises the definition of convexity from finite averages to expectations.<br>
Practically:</p>
<ul>
<li>convex functions “pull upward” under averaging,</li>
<li>log-sum-exp is convex because exponential is convex,</li>
<li>EM and variational methods rely on Jensen to construct lower bounds.</li>
</ul>
<p>As a finite form, for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<h2 id="convex-15_convexfunctions-56-operations-that-preserve-convexity">5.6 Operations that preserve convexity<a class="headerlink" href="#convex-15_convexfunctions-56-operations-that-preserve-convexity" title="Permanent link">¶</a></h2>
<p>Convexity is preserved under many natural constructions:</p>
<ul>
<li>
<p>Nonnegative scaling:<br>
  If <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\alpha \ge 0\)</span>, then <span class="arithmatex">\(\alpha f\)</span> is convex.</p>
</li>
<li>
<p>Addition:<br>
  If <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are convex, then <span class="arithmatex">\(f+g\)</span> is convex.</p>
</li>
<li>
<p>Maximum:<br>
<span class="arithmatex">\(\max\{f,g\}\)</span> is convex.</p>
</li>
<li>
<p>Affine pre-composition:<br>
  If <span class="arithmatex">\(A\)</span> is a matrix,
  <script type="math/tex; mode=display">
  x \mapsto f(Ax + b)
  </script>
  is convex.</p>
</li>
<li>
<p>Monotone composition rule:<br>
  If <span class="arithmatex">\(f\)</span> is convex and nondecreasing in each argument, and each <span class="arithmatex">\(g_i\)</span> is convex,<br>
  then <span class="arithmatex">\(x \mapsto f(g_1(x), \dots, g_k(x))\)</span> is convex.</p>
</li>
</ul>
<p>These rules allow construction of complex convex models from simple building blocks.</p>
<h2 id="convex-15_convexfunctions-57-level-sets-of-convex-functions">5.7 Level sets of convex functions<a class="headerlink" href="#convex-15_convexfunctions-57-level-sets-of-convex-functions" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(\alpha \in \mathbb{R}\)</span>, the sublevel set is
<script type="math/tex; mode=display">
\{ x : f(x) \le \alpha \}.
</script>
</p>
<p>If <span class="arithmatex">\(f\)</span> is convex, every sublevel set is convex.<br>
This property is crucial because inequalities <span class="arithmatex">\(f(x) \le \alpha\)</span> are ubiquitous in constraints.</p>
<p>Examples:</p>
<ul>
<li>Norm balls:<br>
<span class="arithmatex">\(\{ x : \|x\|_2 \le r \}\)</span>  </li>
<li>Linear regression confidence ellipsoids:<br>
<span class="arithmatex">\(\{ x : \|Ax - b\|_2 \le \epsilon \}\)</span></li>
</ul>
<p>These sets enable convex constrained optimisation formulations.</p>
<h2 id="convex-15_convexfunctions-58-strict-and-strong-convexity">5.8 Strict and strong convexity<a class="headerlink" href="#convex-15_convexfunctions-58-strict-and-strong-convexity" title="Permanent link">¶</a></h2>
<h3 id="convex-15_convexfunctions-strict-convexity">Strict convexity<a class="headerlink" href="#convex-15_convexfunctions-strict-convexity" title="Permanent link">¶</a></h3>
<p>A function is strictly convex if
<script type="math/tex; mode=display">
f(\theta x + (1-\theta) y)
<
\theta f(x) + (1-\theta) f(y)
</script>
for all <span class="arithmatex">\(x \neq y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>.</p>
<p>Strict convexity implies unique minimisers.</p>
<h3 id="convex-15_convexfunctions-strong-convexity">Strong convexity<a class="headerlink" href="#convex-15_convexfunctions-strong-convexity" title="Permanent link">¶</a></h3>
<p>A differentiable function is <span class="arithmatex">\(\mu\)</span>-strongly convex if
<script type="math/tex; mode=display">
f(y) 
\ge 
f(x) + \nabla f(x)^\top (y - x) + \frac{\mu}{2}\|y - x\|_2^2.
</script>
</p>
<p>Strong convexity adds <em>quantitative curvature</em>: the function grows at least quadratically away from its minimiser.</p>
<p>Consequences:</p>
<ul>
<li>unique minimiser,</li>
<li>gradient descent achieves linear convergence rate,<br>
  error shrinks as<br>
<script type="math/tex; mode=display">
  \|x_{k+1} - x^\star\| \le (1 - \eta\mu)\|x_k - x^\star\| ,
  </script>
</li>
<li>conditioning (<span class="arithmatex">\(\kappa = L/\mu\)</span>) governs algorithmic difficulty.</li>
</ul>
<p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds <span class="arithmatex">\(\tfrac{\lambda}{2}\|x\|_2^2\)</span>).</p>
<h2 id="convex-15_convexfunctions-summary">Summary<a class="headerlink" href="#convex-15_convexfunctions-summary" title="Permanent link">¶</a></h2>
<p>Convex functions form the analytical backbone of convex optimisation.<br>
They provide:</p>
<ul>
<li>predictable geometry,</li>
<li>clean gradient conditions,</li>
<li>reliable convergence behaviour,</li>
<li>tractable constraints via convex sublevel sets,</li>
<li>stability under composition and modelling operations.</li>
</ul>
<p>These properties make convex objectives indispensable across machine learning, signal processing, and optimisation theory.</p></body></html></section><section class="print-page" id="convex-16_subgradients" heading-number="2.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-nonsmooth-convex-optimization-subgradients">Chapter 6: Nonsmooth Convex Optimization – Subgradients<a class="headerlink" href="#convex-16_subgradients-chapter-6-nonsmooth-convex-optimization-subgradients" title="Permanent link">¶</a></h1>
<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p>
<ul>
<li>the <script type="math/tex"> \ell_1 </script> norm <script type="math/tex"> \|x\|_1 = \sum_i |x_i| </script> (nondifferentiable at zero),</li>
<li>pointwise-max functions such as <script type="math/tex"> f(x) = \max_i (a_i^\top x + b_i) </script>,</li>
<li>the hinge loss <script type="math/tex"> \max\{0,\, 1 - y w^\top x\} </script> used in SVMs,</li>
<li>regularisers like total variation or indicator functions of convex sets.</li>
</ul>
<p>Although these functions have “kinks”, they remain convex—and convexity guarantees the existence of supporting hyperplanes at every point. Subgradients formalise this idea and allow optimisation algorithms to operate even when no derivative exists.</p>
<p>This chapter introduces subgradients, subdifferentials, subgradient calculus, and the basic subgradient method.</p>
<h2 id="convex-16_subgradients-61-subgradients-and-the-subdifferential">6.1 Subgradients and the Subdifferential<a class="headerlink" href="#convex-16_subgradients-61-subgradients-and-the-subdifferential" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> be convex.  A vector <span class="arithmatex">\(g \in \mathbb{R}^n\)</span> is a subgradient of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> if</p>
<div class="arithmatex">\[
f(y) \ge f(x) + g^\top (y - x) \quad \text{for all } y.
\]</div>
<p>Geometric interpretation:</p>
<ul>
<li>The affine function <span class="arithmatex">\(y \mapsto f(x) + g^\top(y-x)\)</span> is a global underestimator of <span class="arithmatex">\(f\)</span>.</li>
<li>Each subgradient defines a supporting hyperplane touching the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((x, f(x))\)</span>.</li>
<li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li>
<li>At kinks, there may be infinitely many supporting hyperplanes.</li>
</ul>
<p>The subdifferential of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> is the set
<script type="math/tex; mode=display">
\partial f(x)
=
\{ g : f(y) \ge f(x) + g^\top(y-x) \ \forall y \}.
</script>
</p>
<p>Properties:</p>
<ul>
<li>
<script type="math/tex"> \partial f(x) </script> is always a nonempty convex set (if <span class="arithmatex">\(x\)</span> is in the interior of the domain).</li>
<li>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then<br>
<script type="math/tex; mode=display">
  \partial f(x) = \{\nabla f(x)\}.
  </script>
</li>
<li>If <span class="arithmatex">\(f\)</span> is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li>
</ul>
<p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>
<h2 id="convex-16_subgradients-62-examples">6.2 Examples<a class="headerlink" href="#convex-16_subgradients-62-examples" title="Permanent link">¶</a></h2>
<h3 id="convex-16_subgradients-absolute-value-in-1d">Absolute value in 1D<a class="headerlink" href="#convex-16_subgradients-absolute-value-in-1d" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(f(t) = |t|\)</span>.<br>
Then:</p>
<ul>
<li>If <span class="arithmatex">\(t &gt; 0\)</span>,  <span class="arithmatex">\(\partial f(t) = \{1\}\)</span>.</li>
<li>If <span class="arithmatex">\(t &lt; 0\)</span>,  <span class="arithmatex">\(\partial f(t) = \{-1\}\)</span>.</li>
<li>If <span class="arithmatex">\(t = 0\)</span>,<br>
<script type="math/tex; mode=display">
  \partial f(0) = [-1,\, 1].
  </script>
</li>
</ul>
<p>At the kink, any slope between <span class="arithmatex">\(-1\)</span> and <span class="arithmatex">\(1\)</span> supports the graph from below.</p>
<h3 id="convex-16_subgradients-the-ell_1-norm">The <script type="math/tex"> \ell_1 </script> norm<a class="headerlink" href="#convex-16_subgradients-the-ell_1-norm" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(f(x) = \|x\|_1 = \sum_i |x_i|\)</span>:</p>
<div class="arithmatex">\[
g \in \partial \|x\|_1
\quad\Longleftrightarrow\quad
g_i \in \partial |x_i|.
\]</div>
<p>Thus:</p>
<ul>
<li>if <span class="arithmatex">\(x_i &gt; 0\)</span>, then <span class="arithmatex">\(g_i = 1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i &lt; 0\)</span>, then <span class="arithmatex">\(g_i = -1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i = 0\)</span>, then <span class="arithmatex">\(g_i \in [-1,1]\)</span>.</li>
</ul>
<p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>
<h3 id="convex-16_subgradients-pointwise-maximum-of-affine-functions">Pointwise maximum of affine functions<a class="headerlink" href="#convex-16_subgradients-pointwise-maximum-of-affine-functions" title="Permanent link">¶</a></h3>
<p>Let<br>
<script type="math/tex; mode=display">
f(x) = \max_{i=1,\dots,k} (a_i^\top x + b_i).
</script>
</p>
<ul>
<li>
<p>If only one index <span class="arithmatex">\(i^\star\)</span> achieves the maximum at <span class="arithmatex">\(x\)</span>, then<br>
<script type="math/tex; mode=display">
  \partial f(x) = \{ a_{i^\star} \}.
  </script>
</p>
</li>
<li>
<p>If multiple indices are tied, then<br>
<script type="math/tex; mode=display">
  \partial f(x)
  = \mathrm{conv}\{ a_i : i \text{ active at } x \},
  </script>
  the convex hull of the active slopes.</p>
</li>
</ul>
<p>This structure underlies SVM hinge loss and ReLU-type functions.</p>
<hr>
<h2 id="convex-16_subgradients-63-subgradient-optimality-condition">6.3 Subgradient Optimality Condition<a class="headerlink" href="#convex-16_subgradients-63-subgradient-optimality-condition" title="Permanent link">¶</a></h2>
<p>For the unconstrained convex minimisation problem</p>
<div class="arithmatex">\[
\min_x f(x),
\]</div>
<p>a point <span class="arithmatex">\(x^\star\)</span> is optimal if and only if</p>
<div class="arithmatex">\[
0 \in \partial f(x^\star).
\]</div>
<p>Interpretation:</p>
<ul>
<li>At optimality, no subgradient points to a direction that would decrease <span class="arithmatex">\(f\)</span>.</li>
<li>Geometrically, the supporting hyperplane at <span class="arithmatex">\(x^\star\)</span> is horizontal, forming the flat bottom of the convex bowl.</li>
<li>This generalises the smooth condition <script type="math/tex"> \nabla f(x^\star)=0 </script>.</li>
</ul>
<h2 id="convex-16_subgradients-64-subgradient-calculus-useful-rules">6.4 Subgradient Calculus (Useful Rules)<a class="headerlink" href="#convex-16_subgradients-64-subgradient-calculus-useful-rules" title="Permanent link">¶</a></h2>
<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> be convex.</p>
<h3 id="convex-16_subgradients-sum-rule">Sum rule<a class="headerlink" href="#convex-16_subgradients-sum-rule" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\partial(f+g)(x) \subseteq \partial f(x) + \partial g(x)
=
\{ u+v : u \in \partial f(x),\ v \in \partial g(x) \}.
\]</div>
<p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>
<h3 id="convex-16_subgradients-affine-composition">Affine composition<a class="headerlink" href="#convex-16_subgradients-affine-composition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(h(x) = f(Ax + b)\)</span>, then
<script type="math/tex; mode=display">
\partial h(x) = A^\top \partial f(Ax+b).
</script>
</p>
<p>This rule is heavily used in machine learning models, where losses depend on linear predictions <span class="arithmatex">\(Ax\)</span>.</p>
<h3 id="convex-16_subgradients-maximum-of-convex-functions">Maximum of convex functions<a class="headerlink" href="#convex-16_subgradients-maximum-of-convex-functions" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f(x) = \max_i f_i(x)\)</span>, then
<script type="math/tex; mode=display">
\partial f(x)
= \mathrm{conv}\{ \partial f_i(x) : i \text{ active at } x \}.
</script>
</p>
<p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>
<h2 id="convex-16_subgradients-65-subgradient-methods">6.5 Subgradient Methods<a class="headerlink" href="#convex-16_subgradients-65-subgradient-methods" title="Permanent link">¶</a></h2>
<p>Even when <span class="arithmatex">\(f\)</span> is not differentiable, we can minimise it using subgradient descent:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha_k g_k,
\qquad g_k \in \partial f(x_k).
\]</div>
<p>Key features:</p>
<ul>
<li>Requires only a subgradient (no differentiability needed).</li>
<li>Works for any convex function.</li>
<li>Stepsizes must typically decrease (e.g. <script type="math/tex"> \alpha_k = c/\sqrt{k} </script>, <script type="math/tex"> \alpha_k = c/k </script>).</li>
<li>Guaranteed convergence for convex <span class="arithmatex">\(f\)</span>, but generally slow.</li>
</ul>
<h3 id="convex-16_subgradients-convergence-rates-worst-case">Convergence rates (worst case)<a class="headerlink" href="#convex-16_subgradients-convergence-rates-worst-case" title="Permanent link">¶</a></h3>
<ul>
<li>Smooth convex gradient descent: <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span>.  </li>
<li>Nonsmooth subgradient descent:<br>
<script type="math/tex; mode=display">
  f(x_k) - f(x^\star) = O(1/\sqrt{k}).
  </script>
</li>
</ul>
<p>This slower rate reflects the lack of curvature information at kinks.</p>
<h3 id="convex-16_subgradients-why-it-still-matters-in-ml">Why it still matters in ML<a class="headerlink" href="#convex-16_subgradients-why-it-still-matters-in-ml" title="Permanent link">¶</a></h3>
<p>Many training objectives behave nonsmoothly:</p>
<ul>
<li>SVM hinge loss  </li>
<li>
<script type="math/tex"> \ell_1 </script>-regularised models (sparse optimisation)  </li>
<li>ReLUs and piecewise-linear networks  </li>
<li>Projections onto convex sets  </li>
</ul>
<p>Even modern deep-learning optimisers operate as subgradient methods whenever the network contains nonsmooth operations.</p>
<h2 id="convex-16_subgradients-66-proximal-and-smoothed-alternatives">6.6 Proximal and Smoothed Alternatives<a class="headerlink" href="#convex-16_subgradients-66-proximal-and-smoothed-alternatives" title="Permanent link">¶</a></h2>
<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>
<h3 id="convex-16_subgradients-1-proximal-methods">(1) Proximal methods<a class="headerlink" href="#convex-16_subgradients-1-proximal-methods" title="Permanent link">¶</a></h3>
<p>For a convex function <span class="arithmatex">\(f\)</span>, the proximal operator is
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha f}(y)
=
\arg\min_x \left\{
f(x) + \frac{1}{2\alpha}\|x-y\|^2
\right\}.
</script>
</p>
<p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p>
<ul>
<li>
<script type="math/tex"> \ell_1 </script> regularisation,</li>
<li>indicator functions of convex sets,</li>
<li>total variation penalties.</li>
</ul>
<p>They achieve faster and more stable convergence than basic subgradient descent.</p>
<h3 id="convex-16_subgradients-2-smoothing-techniques">(2) Smoothing techniques<a class="headerlink" href="#convex-16_subgradients-2-smoothing-techniques" title="Permanent link">¶</a></h3>
<p>Many nonsmooth convex functions have smooth approximations:</p>
<ul>
<li>Replace <script type="math/tex"> |t| </script> with the Huber loss.</li>
<li>Replace <script type="math/tex"> \max\{0,z\} </script> with softplus.</li>
<li>Replace <script type="math/tex"> \max_i(a_i^\top x) </script> with log-sum-exp, a smooth convex approximation.</li>
</ul>
<p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>
<h2 id="convex-16_subgradients-summary">Summary<a class="headerlink" href="#convex-16_subgradients-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Nonsmooth convex functions arise naturally in ML.  </li>
<li>Subgradients generalise gradients: they give supporting hyperplanes.  </li>
<li>Optimality: <span class="arithmatex">\(0 \in \partial f(x^\star)\)</span>.  </li>
<li>Subgradient calculus enables reasoning about complex nonsmooth models.  </li>
<li>Subgradient descent converges globally but slowly.  </li>
<li>Proximal and smoothing methods yield faster practical algorithms.</li>
</ul>
<p>Subgradients complete the picture of convex analysis by extending optimisation tools beyond differentiable functions, setting the stage for modern first-order methods.</p></body></html></section><section class="print-page" id="convex-16a_optimality_conditions" heading-number="2.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-first-order-and-geometric-optimality-conditions">Chapter 7: First-Order and Geometric Optimality Conditions<a class="headerlink" href="#convex-16a_optimality_conditions-chapter-7-first-order-and-geometric-optimality-conditions" title="Permanent link">¶</a></h1>
<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar “zero gradient” condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p>
<p>These conditions form the conceptual bridge between unconstrained minimization and the Karush–Kuhn–Tucker (KKT) framework developed in the next chapter.</p>
<h2 id="convex-16a_optimality_conditions-71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization">7.1 Orders of Optimality: Why First Order is Enough in Convex Optimization<a class="headerlink" href="#convex-16a_optimality_conditions-71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization" title="Permanent link">¶</a></h2>
<p>For a differentiable function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, the “order’’ of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer <span class="arithmatex">\(x^\star\)</span>:</p>
<table>
<thead>
<tr>
<th>Order</th>
<th>Object inspected</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-order</td>
<td><span class="arithmatex">\(\nabla f(x^\star)\)</span> or subgradients</td>
<td>Detects existence of a local descent direction</td>
</tr>
<tr>
<td>Second-order</td>
<td>Hessian <span class="arithmatex">\(\nabla^2 f(x^\star)\)</span></td>
<td>Examines curvature (minimum vs saddle vs maximum)</td>
</tr>
<tr>
<td>Higher-order</td>
<td>Third derivative and beyond</td>
<td>Rarely used; only for degenerate cases with vanishing curvature</td>
</tr>
</tbody>
</table>
<p>In general nonconvex optimization, these conditions are used together: a point may have <span class="arithmatex">\(\nabla f(x^\star) = 0\)</span> but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p>
<p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0 \quad \text{whenever the Hessian exists}.
\]</div>
<p>Therefore:</p>
<ul>
<li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li>
<li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li>
</ul>
<p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>
<h2 id="convex-16a_optimality_conditions-72-motivation">7.2 Motivation<a class="headerlink" href="#convex-16a_optimality_conditions-72-motivation" title="Permanent link">¶</a></h2>
<p>Consider the basic convex problem
<script type="math/tex; mode=display">
\min_{x \in \mathcal{X}} f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X}\)</span> is a convex set.</p>
<p>Intuitively, a point <span class="arithmatex">\(\hat{x}\)</span> is optimal if there is no feasible direction in which we can move and strictly decrease <span class="arithmatex">\(f\)</span>. In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside <span class="arithmatex">\(\mathcal{X}\)</span> are allowed.</p>
<p>Thus, optimality can be seen as an equilibrium:</p>
<ul>
<li>the objective’s tendency to decrease (captured by its gradient or subgradient)  </li>
<li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li>
</ul>
<p>In machine learning, this appears as:</p>
<ul>
<li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li>
<li>training until the force from regularization/constraints balances the data fit term (e.g., in <span class="arithmatex">\(\ell_1\)</span>-regularized models).</li>
</ul>
<p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>
<h2 id="convex-16a_optimality_conditions-73-unconstrained-convex-problems">7.3 Unconstrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-73-unconstrained-convex-problems" title="Permanent link">¶</a></h2>
<p>For the unconstrained problem
<script type="math/tex; mode=display">
\min_x f(x),
</script>
with <span class="arithmatex">\(f\)</span> convex, the optimality conditions are especially simple.</p>
<h3 id="convex-16a_optimality_conditions-smooth-case">Smooth case<a class="headerlink" href="#convex-16a_optimality_conditions-smooth-case" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, then a point <span class="arithmatex">\(\hat{x}\)</span> is optimal if and only if
<script type="math/tex; mode=display">
\nabla f(\hat{x}) = 0.
</script>
</p>
<p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>
<h3 id="convex-16a_optimality_conditions-nonsmooth-case">Nonsmooth case<a class="headerlink" href="#convex-16a_optimality_conditions-nonsmooth-case" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x}).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The origin lies in the set of all subgradients at <span class="arithmatex">\(\hat{x}\)</span>.  </li>
<li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((\hat{x}, f(\hat{x}))\)</span>.  </li>
<li>No direction in <span class="arithmatex">\(\mathbb{R}^n\)</span> gives a first-order improvement in the objective.</li>
</ul>
<p>For smooth <span class="arithmatex">\(f\)</span>, this reduces to the usual condition <span class="arithmatex">\(\nabla f(\hat{x}) = 0\)</span>.</p>
<h2 id="convex-16a_optimality_conditions-74-constrained-convex-problems">7.4 Constrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-74-constrained-convex-problems" title="Permanent link">¶</a></h2>
<p>Now consider the constrained problem
<script type="math/tex; mode=display">
\min_{x \in \mathcal{X}} f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X} \subseteq \mathbb{R}^n\)</span> is a nonempty closed convex set.</p>
<p>If <span class="arithmatex">\(\hat{x}\)</span> lies strictly inside <span class="arithmatex">\(\mathcal{X}\)</span>, then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x})
</script>
remains the necessary and sufficient condition for optimality.</p>
<p>The interesting case is when <span class="arithmatex">\(\hat{x}\)</span> lies on the boundary of <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<h3 id="convex-16a_optimality_conditions-first-order-condition-with-constraints">First-order condition with constraints<a class="headerlink" href="#convex-16a_optimality_conditions-first-order-condition-with-constraints" title="Permanent link">¶</a></h3>
<p>The general first-order optimality condition for the constrained convex problem is:
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x}) + N_{\mathcal{X}}(\hat{x}).
</script>
</p>
<p>That is, there exist</p>
<ul>
<li>a subgradient <span class="arithmatex">\(g \in \partial f(\hat{x})\)</span>, and  </li>
<li>a normal vector <span class="arithmatex">\(v \in N_{\mathcal{X}}(\hat{x})\)</span></li>
</ul>
<p>such that
<script type="math/tex; mode=display">
g + v = 0.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The objective’s slope <span class="arithmatex">\(g\)</span> is exactly balanced by a normal vector <span class="arithmatex">\(v\)</span> coming from the constraint set.  </li>
<li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which <span class="arithmatex">\(f\)</span> can decrease.  </li>
<li>Geometrically, the epigraph of <span class="arithmatex">\(f\)</span> and the feasible set meet with aligned supporting hyperplanes at <span class="arithmatex">\(\hat{x}\)</span>.</li>
</ul>
<p>Special cases:</p>
<ul>
<li>If <span class="arithmatex">\(\hat{x}\)</span> is an interior point, then <span class="arithmatex">\(N_{\mathcal{X}}(\hat{x}) = \{0\}\)</span>, so we recover the unconstrained condition <span class="arithmatex">\(0 \in \partial f(\hat{x})\)</span>.  </li>
<li>If <span class="arithmatex">\(\mathcal{X}\)</span> is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li>
</ul></body></html></section><section class="print-page" id="convex-17_kkt" heading-number="2.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-8-lagrange-multipliers-and-the-kkt-framework">Chapter 8: Lagrange Multipliers and the KKT Framework<a class="headerlink" href="#convex-17_kkt-chapter-8-lagrange-multipliers-and-the-kkt-framework" title="Permanent link">¶</a></h1>
<p>We now have the ingredients for understanding optimality in convex optimization:</p>
<ul>
<li>convex functions define well-behaved objectives,</li>
<li>convex sets describe feasible regions,</li>
<li>gradients and subgradients encode descent directions.</li>
</ul>
<p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions—the Karush–Kuhn–Tucker (KKT) conditions—is the central optimality framework for constrained convex optimization.</p>
<p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by “forces’’ coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>
<h2 id="convex-17_kkt-81-unconstrained-convex-minimization">8.1 Unconstrained Convex Minimization<a class="headerlink" href="#convex-17_kkt-81-unconstrained-convex-minimization" title="Permanent link">¶</a></h2>
<p>Consider the problem
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and differentiable.</p>
<p>Gradient descent iteratively updates
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} - \alpha_k \nabla f(x^{(k)}),
</script>
with step size <span class="arithmatex">\(\alpha_k &gt; 0\)</span>.</p>
<p>Intuition:</p>
<ul>
<li>Moving opposite the gradient decreases <span class="arithmatex">\(f\)</span>.</li>
<li>If the gradient is Lipschitz continuous and the step size is small enough (<span class="arithmatex">\(\alpha_k \le 1/L\)</span>), then gradient descent converges to a global minimizer.</li>
<li>If <span class="arithmatex">\(f\)</span> is <em>strongly convex</em>, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li>
</ul>
<p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>
<h2 id="convex-17_kkt-82-equality-constrained-problems-and-lagrange-multipliers">8.2 Equality-Constrained Problems and Lagrange Multipliers<a class="headerlink" href="#convex-17_kkt-82-equality-constrained-problems-and-lagrange-multipliers" title="Permanent link">¶</a></h2>
<p>Now consider minimizing <span class="arithmatex">\(f\)</span> subject to equality constraints:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimize} & f(x) \\
\text{subject to} & h_j(x) = 0,\quad j = 1,\dots,p.
\end{array}
</script>
</p>
<p>Define the Lagrangian
<script type="math/tex; mode=display">
L(x, \lambda) = f(x) + \sum_{j=1}^p \lambda_j h_j(x),
</script>
where <span class="arithmatex">\(\lambda = (\lambda_1,\dots,\lambda_p)\)</span> are the Lagrange multipliers.</p>
<p>Under differentiability and regularity assumptions, a point <span class="arithmatex">\(x^*\)</span> is optimal only if:</p>
<ol>
<li>
<p>Primal feasibility
   <script type="math/tex; mode=display">
   h_j(x^*) = 0,\quad \forall j.
   </script>
</p>
</li>
<li>
<p>Stationarity
   <script type="math/tex; mode=display">
   \nabla f(x^*) + \sum_{j=1}^p \lambda_j^* \nabla h_j(x^*) = 0.
   </script>
</p>
</li>
</ol>
<p>Geometric meaning:</p>
<ul>
<li>The feasible set <script type="math/tex"> \{x : h_j(x)=0\} </script> is typically a smooth manifold.</li>
<li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li>
<li>The multipliers <span class="arithmatex">\(\lambda_j^*\)</span> weight the constraint normals to exactly cancel the objective’s gradient.</li>
</ul>
<p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>
<h2 id="convex-17_kkt-83-inequality-constraints-and-the-kkt-conditions">8.3 Inequality Constraints and the KKT Conditions<a class="headerlink" href="#convex-17_kkt-83-inequality-constraints-and-the-kkt-conditions" title="Permanent link">¶</a></h2>
<p>Now consider the general convex problem:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimize} & f(x) \\
\text{subject to} 
 & g_i(x) \le 0,\quad i=1,\dots,m, \\
 & h_j(x) = 0,\quad j=1,\dots,p.
\end{array}
</script>
</p>
<p>Form the Lagrangian
<script type="math/tex; mode=display">
L(x,\lambda,\mu) 
= f(x) 
+ \sum_{j=1}^p \lambda_j h_j(x)
+ \sum_{i=1}^m \mu_i g_i(x),
</script>
with:</p>
<ul>
<li>
<script type="math/tex"> \lambda_j \in \mathbb{R} </script> (equality multipliers),</li>
<li>
<script type="math/tex"> \mu_i \ge 0 </script> (inequality multipliers).</li>
</ul>
<p>A point <span class="arithmatex">\(x^*\)</span> with multipliers <span class="arithmatex">\((\lambda^*,\mu^*)\)</span> satisfies the KKT conditions:</p>
<h3 id="convex-17_kkt-1-primal-feasibility">1. Primal feasibility<a class="headerlink" href="#convex-17_kkt-1-primal-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
g_i(x^*) \le 0,\quad \forall i,
\qquad
h_j(x^*) = 0,\quad \forall j.
\]</div>
<h3 id="convex-17_kkt-2-dual-feasibility">2. Dual feasibility<a class="headerlink" href="#convex-17_kkt-2-dual-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^* \ge 0,\quad \forall i.
\]</div>
<h3 id="convex-17_kkt-3-stationarity">3. Stationarity<a class="headerlink" href="#convex-17_kkt-3-stationarity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\nabla f(x^*) 
+ \sum_{j=1}^p \lambda_j^* \nabla h_j(x^*)
+ \sum_{i=1}^m \mu_i^* \nabla g_i(x^*)
= 0.
\]</div>
<h3 id="convex-17_kkt-4-complementary-slackness">4. Complementary slackness<a class="headerlink" href="#convex-17_kkt-4-complementary-slackness" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^*\, g_i(x^*) = 0, \quad i=1,\dots,m.
\]</div>
<p>Complementary slackness expresses a clear dichotomy:</p>
<ul>
<li>If constraint <span class="arithmatex">\(g_i(x) \le 0\)</span> is inactive (strictly <span class="arithmatex">\(&lt;0\)</span>), then it applies no force: <span class="arithmatex">\(\mu_i^* = 0\)</span>.</li>
<li>If a constraint is active at the boundary, it may exert a force: <span class="arithmatex">\(\mu_i^* &gt; 0\)</span>, and then <span class="arithmatex">\(g_i(x^*) = 0\)</span>.</li>
</ul>
<p>Only active constraints can push back against the objective.</p>
<h2 id="convex-17_kkt-84-slaters-condition-guaranteeing-strong-duality">8.4 Slater’s Condition — Guaranteeing Strong Duality<a class="headerlink" href="#convex-17_kkt-84-slaters-condition-guaranteeing-strong-duality" title="Permanent link">¶</a></h2>
<p>The KKT conditions always provide <em>necessary</em> conditions for optimality. For them to also be <em>sufficient</em> (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p>
<p>For convex problems with convex <span class="arithmatex">\(g_i\)</span> and affine <span class="arithmatex">\(h_j\)</span>, Slater’s condition holds if there exists a strictly feasible point:
<script type="math/tex; mode=display">
\exists\, x^{\text{slater}}:
\quad h_j(x^{\text{slater}})=0,\ \forall j,
\qquad
g_i(x^{\text{slater}}) < 0,\ \forall i.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The feasible region contains an interior point.</li>
<li>The constraints are not “tight” everywhere.</li>
<li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li>
</ul>
<p>When Slater’s condition holds:</p>
<ol>
<li>
<p>Strong duality holds:<br>
<script type="math/tex; mode=display">
   p^* = d^*.
   </script>
</p>
</li>
<li>
<p>The dual optimum is attained.</p>
</li>
<li>
<p>The KKT conditions are both necessary and sufficient for optimality.</p>
</li>
</ol>
<h3 id="convex-17_kkt-duality-gap">Duality gap<a class="headerlink" href="#convex-17_kkt-duality-gap" title="Permanent link">¶</a></h3>
<p>For a primal problem with optimum <span class="arithmatex">\(p^*\)</span> and its dual with optimum <span class="arithmatex">\(d^*\)</span>, the duality gap is
<script type="math/tex; mode=display">
p^* - d^* \ge 0.
</script>
</p>
<ul>
<li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li>
<li>Slater’s condition ensures the gap is zero.</li>
</ul>
<p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>
<hr>
<h2 id="convex-17_kkt-85-geometric-and-physical-interpretation">8.5 Geometric and Physical Interpretation<a class="headerlink" href="#convex-17_kkt-85-geometric-and-physical-interpretation" title="Permanent link">¶</a></h2>
<p>The KKT conditions describe an equilibrium of forces:</p>
<ul>
<li>The objective gradient pushes the point in the direction of steepest decrease.</li>
<li>Active constraints push back through normal vectors scaled by multipliers.</li>
<li>At optimality, these forces exactly cancel.</li>
</ul>
<p>Physically:</p>
<ul>
<li>Lagrange multipliers are “reaction forces’’ keeping a system on the constraint surface.</li>
<li>In economics, they are “shadow prices’’ indicating how much the objective would improve if a constraint were relaxed.</li>
<li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li>
</ul>
<p>KKT theory unifies all earlier ideas—convexity, gradients/subgradients, feasible regions, tangent and normal cones—into one clean, general optimality framework.</p></body></html></section><section class="print-page" id="convex-18_duality" heading-number="2.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-9-lagrange-duality-theory">Chapter 9: Lagrange Duality Theory<a class="headerlink" href="#convex-18_duality-chapter-9-lagrange-duality-theory" title="Permanent link">¶</a></h1>
<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p>
<ul>
<li>lower bounds on the primal optimal value,</li>
<li>certificates of optimality,</li>
<li>interpretations of constraint “prices,”</li>
<li>and alternative algorithmic routes to solutions.</li>
</ul>
<p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality — <em>strong duality</em> — lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>
<h2 id="convex-18_duality-91-the-primal-problem">9.1 The Primal Problem<a class="headerlink" href="#convex-18_duality-91-the-primal-problem" title="Permanent link">¶</a></h2>
<p>Consider the general convex problem</p>
<div class="arithmatex">\[
\begin{array}{ll}
\text{minimize} &amp; f(x) \\
\text{subject to} &amp; g_i(x) \le 0,\quad i=1,\dots,m, \\
 &amp; h_j(x) = 0,\quad j=1,\dots,p,
\end{array}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> and each <span class="arithmatex">\(g_i\)</span> are convex,</li>
<li>each equality constraint <span class="arithmatex">\(h_j\)</span> is affine.</li>
</ul>
<p>The optimal value is</p>
<div class="arithmatex">\[
f^\star = \inf\{ f(x) : g_i(x) \le 0,\ h_j(x)=0 \}.
\]</div>
<p>The infimum allows for the possibility that the best value is approached but not attained.</p>
<h2 id="convex-18_duality-92-why-duality">9.2 Why Duality?<a class="headerlink" href="#convex-18_duality-92-why-duality" title="Permanent link">¶</a></h2>
<p>A constrained problem can be viewed as:</p>
<blockquote>
<p>minimize <span class="arithmatex">\(f(x)\)</span> but pay a penalty whenever constraints are violated.</p>
</blockquote>
<p>If the penalties are chosen “correctly,” one can recover the original constrained problem from an unconstrained penalized problem. Dual variables — <span class="arithmatex">\(\mu_i\)</span> for inequalities and <span class="arithmatex">\(\lambda_j\)</span> for equalities — precisely encode these penalties:</p>
<ul>
<li><span class="arithmatex">\(\mu_i\)</span> measures how costly it is to violate <span class="arithmatex">\(g_i(x)\le 0\)</span>,</li>
<li><span class="arithmatex">\(\lambda_j\)</span> measures the sensitivity of the objective to relaxing <span class="arithmatex">\(h_j(x)=0\)</span>.</li>
</ul>
<p>Duality converts constraints into prices, and transforms geometry into algebra.</p>
<h2 id="convex-18_duality-93-the-lagrangian">9.3 The Lagrangian<a class="headerlink" href="#convex-18_duality-93-the-lagrangian" title="Permanent link">¶</a></h2>
<p>The Lagrangian function is</p>
<div class="arithmatex">\[
L(x, \lambda, \mu)
= f(x) + \sum_{i=1}^m \mu_i g_i(x)
+ \sum_{j=1}^p \lambda_j h_j(x),
\]</div>
<p>with:</p>
<ul>
<li><span class="arithmatex">\(\mu_i \ge 0\)</span> for inequality constraints,</li>
<li><span class="arithmatex">\(\lambda_j \in \mathbb{R}\)</span> unrestricted for equalities.</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>If <span class="arithmatex">\(\mu_i &gt; 0\)</span>, violating <span class="arithmatex">\(g_i(x)\le 0\)</span> incurs a penalty proportional to <span class="arithmatex">\(\mu_i\)</span>.</li>
<li>If <span class="arithmatex">\(\mu_i = 0\)</span>, that constraint does not influence the Lagrangian at that point.</li>
</ul>
<h2 id="convex-18_duality-94-the-dual-function-lower-bounds-from-penalties">9.4 The Dual Function: Lower Bounds from Penalties<a class="headerlink" href="#convex-18_duality-94-the-dual-function-lower-bounds-from-penalties" title="Permanent link">¶</a></h2>
<p>Fix <span class="arithmatex">\((\lambda,\mu)\)</span> and minimize the Lagrangian with respect to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
\theta(\lambda, \mu) = \inf_x L(x,\lambda,\mu).
\]</div>
<p>Because <span class="arithmatex">\(g_i(x) \le 0\)</span> for feasible <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(\mu_i \ge 0\)</span>,</p>
<div class="arithmatex">\[
L(x,\lambda,\mu) \le f(x),
\]</div>
<p>so taking the infimum over all <span class="arithmatex">\(x\)</span> yields</p>
<div class="arithmatex">\[
\theta(\lambda,\mu) \le f^\star.
\]</div>
<p>Thus <span class="arithmatex">\(\theta\)</span> always produces lower bounds on the true optimal value (weak duality).</p>
<h3 id="convex-18_duality-properties-of-the-dual-function">Properties of the Dual Function<a class="headerlink" href="#convex-18_duality-properties-of-the-dual-function" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\theta(\lambda,\mu)\)</span> is always concave in <span class="arithmatex">\((\lambda,\mu)\)</span> (infimum of affine functions).</li>
<li>It may be <span class="arithmatex">\(-\infty\)</span> if the Lagrangian is unbounded below.</li>
</ul>
<h2 id="convex-18_duality-95-the-dual-problem">9.5 The Dual Problem<a class="headerlink" href="#convex-18_duality-95-the-dual-problem" title="Permanent link">¶</a></h2>
<p>The dual problem maximizes these lower bounds:</p>
<div class="arithmatex">\[
\begin{array}{ll}
\text{maximize}_{\lambda,\mu} &amp; \theta(\lambda,\mu) \\
\text{subject to} &amp; \mu \ge 0.
\end{array}
\]</div>
<p>Let <span class="arithmatex">\(d^\star\)</span> be the optimal dual value.<br>
Weak duality guarantees:</p>
<div class="arithmatex">\[
d^\star \le f^\star.
\]</div>
<p>The dual problem is always a concave maximization, i.e., a convex optimization problem in <span class="arithmatex">\((\lambda,\mu)\)</span>.</p>
<h2 id="convex-18_duality-96-strong-duality-and-the-duality-gap">9.6 Strong Duality and the Duality Gap<a class="headerlink" href="#convex-18_duality-96-strong-duality-and-the-duality-gap" title="Permanent link">¶</a></h2>
<p>If</p>
<div class="arithmatex">\[
d^\star = f^\star,
\]</div>
<p>we say strong duality holds. The duality gap is zero.</p>
<h3 id="convex-18_duality-slaters-condition">Slater’s Condition<a class="headerlink" href="#convex-18_duality-slaters-condition" title="Permanent link">¶</a></h3>
<p>If:</p>
<ul>
<li><span class="arithmatex">\(g_i\)</span> are convex,</li>
<li><span class="arithmatex">\(h_j\)</span> are affine,</li>
<li>and there exists a <span class="arithmatex">\(\tilde{x}\)</span> such that<br>
<script type="math/tex; mode=display">
  g_i(\tilde{x}) < 0,\quad h_j(\tilde{x}) = 0,
  </script>
</li>
</ul>
<p>then:</p>
<ul>
<li>strong duality holds (<span class="arithmatex">\(f^\star = d^\star\)</span>),</li>
<li>dual maximizers exist,</li>
<li>KKT conditions fully characterize primal–dual optimality.</li>
</ul>
<p>Slater’s condition ensures the feasible region has interior — the constraints are not tight everywhere.</p>
<h2 id="convex-18_duality-97-duality-and-the-kkt-conditions">9.7 Duality and the KKT Conditions<a class="headerlink" href="#convex-18_duality-97-duality-and-the-kkt-conditions" title="Permanent link">¶</a></h2>
<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>
<h3 id="convex-18_duality-1-primal-feasibility">1. Primal feasibility<a class="headerlink" href="#convex-18_duality-1-primal-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
g_i(x^\star) \le 0,\qquad h_j(x^\star)=0.
\]</div>
<h3 id="convex-18_duality-2-dual-feasibility">2. Dual feasibility<a class="headerlink" href="#convex-18_duality-2-dual-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^\star \ge 0.
\]</div>
<h3 id="convex-18_duality-3-stationarity">3. Stationarity<a class="headerlink" href="#convex-18_duality-3-stationarity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\nabla f(x^\star)
+ \sum_{i=1}^m \mu_i^\star \nabla g_i(x^\star)
+ \sum_{j=1}^p \lambda_j^\star \nabla h_j(x^\star)
= 0.
\]</div>
<h3 id="convex-18_duality-4-complementary-slackness">4. Complementary slackness<a class="headerlink" href="#convex-18_duality-4-complementary-slackness" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^\star g_i(x^\star) = 0,\qquad \forall i.
\]</div>
<p>Together these conditions ensure:</p>
<div class="arithmatex">\[
f(x^\star) = \theta(\lambda^\star,\mu^\star)
= f^\star = d^\star.
\]</div>
<p>Geometrically, the gradients of the active constraints form a supporting hyperplane that “touches’’ the objective exactly at the optimum.</p>
<h2 id="convex-18_duality-98-interpretation-of-dual-variables">9.8 Interpretation of Dual Variables<a class="headerlink" href="#convex-18_duality-98-interpretation-of-dual-variables" title="Permanent link">¶</a></h2>
<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>
<h3 id="convex-18_duality-shadow-prices-constraint-forces">Shadow Prices / Constraint Forces<a class="headerlink" href="#convex-18_duality-shadow-prices-constraint-forces" title="Permanent link">¶</a></h3>
<ul>
<li>
<p><span class="arithmatex">\(\mu_i^\star\)</span>: the <em>shadow price</em> for relaxing <span class="arithmatex">\(g_i(x)\le 0\)</span>.<br>
  Large <span class="arithmatex">\(\mu_i^\star\)</span> means the constraint is tight and costly to relax.</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda_j^\star\)</span>: sensitivity of the optimal value to perturbations of <span class="arithmatex">\(h_j(x)=0\)</span>.</p>
</li>
</ul>
<h3 id="convex-18_duality-ml-interpretations">ML Interpretations<a class="headerlink" href="#convex-18_duality-ml-interpretations" title="Permanent link">¶</a></h3>
<ul>
<li>Support Vector Machines: dual variables select support vectors (only points with <span class="arithmatex">\(\mu_i^\star &gt; 0\)</span> matter).</li>
<li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li>
<li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li>
</ul>
<p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p></body></html></section><section class="print-page" id="convex-18a_pareto" heading-number="2.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-10-multi-objective-convex-optimization">Chapter 10: Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-chapter-10-multi-objective-convex-optimization" title="Permanent link">¶</a></h1>
<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p>
<ul>
<li>accuracy vs. regularity,</li>
<li>loss vs. fairness,</li>
<li>return vs. risk,</li>
<li>reconstruction vs. compression,</li>
<li>energy use vs. performance.</li>
</ul>
<p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p>
<p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>
<h2 id="convex-18a_pareto-101-classical-optimality-one-objective">10.1 Classical Optimality (One Objective)<a class="headerlink" href="#convex-18a_pareto-101-classical-optimality-one-objective" title="Permanent link">¶</a></h2>
<p>In standard convex optimization, we solve:</p>
<div class="arithmatex">\[
x^* \in \arg\min_{x \in \mathcal{X}} f(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X}\)</span> is convex.<br>
In this setting, it is natural to speak of the minimizer — or set of minimizers — because the task is governed by a single quantitative measure.</p>
<p>However, when multiple objectives <span class="arithmatex">\((f_1,\dots,f_k)\)</span> must be minimized simultaneously, a single “best” point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>
<h2 id="convex-18a_pareto-102-multi-objective-convex-optimization">10.2 Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-102-multi-objective-convex-optimization" title="Permanent link">¶</a></h2>
<p>A multi-objective optimization problem takes the form</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} F(x) = (f_1(x), \dots, f_k(x)),
\]</div>
<p>where each <span class="arithmatex">\(f_i\)</span> is convex.<br>
This framework appears in many ML and statistical tasks:</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Objective 1</th>
<th>Objective 2</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression</td>
<td>Fit error</td>
<td>Regularization</td>
<td>Accuracy vs. complexity</td>
</tr>
<tr>
<td>Fair ML</td>
<td>Loss</td>
<td>Fairness metric</td>
<td>Utility vs. fairness</td>
</tr>
<tr>
<td>Portfolio</td>
<td>Return</td>
<td>Risk</td>
<td>Profit vs. stability</td>
</tr>
<tr>
<td>Autoencoders</td>
<td>Reconstruction</td>
<td>KL divergence</td>
<td>Fidelity vs. disentanglement</td>
</tr>
</tbody>
</table>
<p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes <em>Pareto efficiency</em>.</p>
<h2 id="convex-18a_pareto-103-pareto-optimality">10.3 Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-103-pareto-optimality" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-strong-pareto-optimality">Strong Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-strong-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^*\)</span> is Pareto optimal if there is no other <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[
f_i(x) \le f_i(x^*)\quad \forall i,
\]</div>
<p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>
<h3 id="convex-18a_pareto-weak-pareto-optimality">Weak Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-weak-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^*\)</span> is weakly Pareto optimal if no feasible point satisfies</p>
<div class="arithmatex">\[
f_i(x) &lt; f_i(x^*)\quad \forall i.
\]</div>
<p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>
<h3 id="convex-18a_pareto-geometric-view">Geometric View<a class="headerlink" href="#convex-18a_pareto-geometric-view" title="Permanent link">¶</a></h3>
<p>For two objectives <span class="arithmatex">\((f_1, f_2)\)</span>, the feasible set in objective space is a region in <span class="arithmatex">\(\mathbb{R}^2\)</span>. Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p>
<ul>
<li>Points <em>on</em> the frontier are the best achievable trade-offs.</li>
<li>Points <em>above</em> or <em>inside</em> the region are dominated and thus suboptimal.</li>
</ul>
<p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>
<h2 id="convex-18a_pareto-104-scalarization-turning-many-objectives-into-one">10.4 Scalarization: Turning Many Objectives into One<a class="headerlink" href="#convex-18a_pareto-104-scalarization-turning-many-objectives-into-one" title="Permanent link">¶</a></h2>
<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>
<h3 id="convex-18a_pareto-weighted-sum-scalarization">Weighted-Sum Scalarization<a class="headerlink" href="#convex-18a_pareto-weighted-sum-scalarization" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} \sum_{i=1}^k w_i f_i(x),
\qquad w_i \ge 0,\quad \sum_i w_i = 1.
\]</div>
<ul>
<li>The weights encode relative importance.  </li>
<li>Varying <span class="arithmatex">\(w\)</span> traces (part of) the Pareto frontier.  </li>
<li>When <span class="arithmatex">\(f_i\)</span> and <span class="arithmatex">\(\mathcal{X}\)</span> are convex, this method recovers the convex portion of the frontier.</li>
</ul>
<h3 id="convex-18a_pareto--constraint-method">ε-Constraint Method<a class="headerlink" href="#convex-18a_pareto--constraint-method" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_{x} \ f_1(x)
\quad \text{s.t. } f_i(x) \le \varepsilon_i,\ \ i = 2,\dots,k.
\]</div>
<ul>
<li>Here the tolerances <span class="arithmatex">\(\varepsilon_i\)</span> act as performance budgets.  </li>
<li>Each choice of <span class="arithmatex">\(\varepsilon\)</span> yields a different Pareto-efficient point.</li>
</ul>
<p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>
<h3 id="convex-18a_pareto-duality-connection">Duality Connection<a class="headerlink" href="#convex-18a_pareto-duality-connection" title="Permanent link">¶</a></h3>
<p>Scalarization has a tight relationship with duality (Chapter 9):</p>
<ul>
<li>Weights <span class="arithmatex">\(w_i\)</span> in a weighted sum act like dual variables.</li>
<li>Regularization parameters (e.g., the <span class="arithmatex">\(\lambda\)</span> in L2 or L1 regularization) correspond to dual multipliers.</li>
<li>Moving along <span class="arithmatex">\(\lambda\)</span> traces the Pareto frontier between data fit and model complexity.</li>
</ul>
<p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>
<h2 id="convex-18a_pareto-105-examples-and-applications">10.5 Examples and Applications<a class="headerlink" href="#convex-18a_pareto-105-examples-and-applications" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-example-1-regularized-least-squares">Example 1: Regularized Least Squares<a class="headerlink" href="#convex-18a_pareto-example-1-regularized-least-squares" title="Permanent link">¶</a></h3>
<p>Consider</p>
<div class="arithmatex">\[
f_1(x) = \|Ax - b\|_2^2,\qquad 
f_2(x) = \|x\|_2^2.
\]</div>
<p>Two scalarizations:</p>
<ol>
<li>
<p>Weighted:
   <script type="math/tex; mode=display">
   \min_x \ \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
   </script>
</p>
</li>
<li>
<p>ε-constraint:
   <script type="math/tex; mode=display">
   \min_x \ \|Ax - b\|_2^2 \quad \text{s.t. } \|x\|_2^2 \le \tau.
   </script>
</p>
</li>
</ol>
<p><span class="arithmatex">\(\lambda\)</span> and <span class="arithmatex">\(\tau\)</span> trace the same Pareto curve — the classical bias–variance trade-off.</p>
<h3 id="convex-18a_pareto-example-2-portfolio-optimization-riskreturn">Example 2: Portfolio Optimization (Risk–Return)<a class="headerlink" href="#convex-18a_pareto-example-2-portfolio-optimization-riskreturn" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(w\)</span> be portfolio weights, <span class="arithmatex">\(\mu\)</span> expected returns, and <span class="arithmatex">\(\Sigma\)</span> the covariance matrix. Objectives:</p>
<div class="arithmatex">\[
f_1(w) = -\mu^\top w, \qquad
f_2(w) = w^\top \Sigma w.
\]</div>
<p>Weighted scalarization:</p>
<div class="arithmatex">\[
\min_w \ -\alpha \mu^\top w + (1-\alpha) w^\top \Sigma w,
\quad 0 \le \alpha \le 1.
\]</div>
<p>Varying <span class="arithmatex">\(\alpha\)</span> recovers the efficient frontier of Modern Portfolio Theory.</p>
<h3 id="convex-18a_pareto-example-3-fairnessaccuracy-in-ml">Example 3: Fairness–Accuracy in ML<a class="headerlink" href="#convex-18a_pareto-example-3-fairnessaccuracy-in-ml" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_\theta \ \mathbb{E}[\ell(y, f_\theta(x))]
\quad \text{s.t. } D(f_\theta(x),y) \le \varepsilon,
\]</div>
<p>where <span class="arithmatex">\(D\)</span> is a fairness metric.<br>
Scalarized form:</p>
<div class="arithmatex">\[
\min_\theta\  \mathbb{E}[\ell(y, f_\theta(x))] + \lambda D(f_\theta(x), y).
\]</div>
<p>Tuning <span class="arithmatex">\(\lambda\)</span> walks across the fairness–accuracy Pareto frontier.</p>
<h3 id="convex-18a_pareto-example-4-variational-autoencoders-and-vae">Example 4: Variational Autoencoders and β-VAE<a class="headerlink" href="#convex-18a_pareto-example-4-variational-autoencoders-and-vae" title="Permanent link">¶</a></h3>
<p>The ELBO is:</p>
<div class="arithmatex">\[
\mathbb{E}_{q(z)}[\log p(x|z)] - \mathrm{KL}(q(z)\|p(z)).
\]</div>
<p>Objectives:</p>
<ul>
<li>Reconstruction fidelity,</li>
<li>Latent simplicity.</li>
</ul>
<p>β-VAE scalarization:</p>
<div class="arithmatex">\[
\max_q \ \mathbb{E}[\log p(x|z)] - \beta \,\mathrm{KL}(q(z)\|p(z)).
\]</div>
<p><span class="arithmatex">\(\beta\)</span> controls the trade-off between reconstruction and disentanglement — a Pareto frontier in latent space.</p>
<p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p></body></html></section><section class="print-page" id="convex-18b_regularization" heading-number="2.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-11-balancing-fit-and-complexity">Chapter 11:  Balancing Fit and Complexity<a class="headerlink" href="#convex-18b_regularization-chapter-11-balancing-fit-and-complexity" title="Permanent link">¶</a></h1>
<p>Most real-world learning and estimation problems must balance two competing goals:</p>
<ol>
<li>Fit the observed data well, and  </li>
<li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li>
</ol>
<p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>
<h2 id="convex-18b_regularization-111-motivation-fit-vs-complexity">11.1 Motivation: Fit vs. Complexity<a class="headerlink" href="#convex-18b_regularization-111-motivation-fit-vs-complexity" title="Permanent link">¶</a></h2>
<p>Suppose we wish to estimate parameters <span class="arithmatex">\(x\)</span> from data via a loss function <span class="arithmatex">\(f(x)\)</span>. If the data are noisy or the model is high-dimensional, solutions minimizing <span class="arithmatex">\(f\)</span> alone may be unstable or overly complex. We introduce a regularizer <span class="arithmatex">\(R(x)\)</span>, typically convex, to encourage desirable structure:</p>
<div class="arithmatex">\[
\min_{x} \; f(x) + \lambda R(x), \qquad \lambda &gt; 0.
\]</div>
<ul>
<li><span class="arithmatex">\(f(x)\)</span>: measures data misfit (e.g., squared loss, logistic loss).  </li>
<li><span class="arithmatex">\(R(x)\)</span>: penalizes complexity (e.g., <span class="arithmatex">\(\ell_1\)</span> norm for sparsity, <span class="arithmatex">\(\ell_2\)</span> norm for smoothness).  </li>
<li><span class="arithmatex">\(\lambda\)</span>: controls the trade-off.<ul>
<li>Small <span class="arithmatex">\(\lambda\)</span>: excellent data fit, potentially overfitting.  </li>
<li>Large <span class="arithmatex">\(\lambda\)</span>: simpler model, potentially underfitting.</li>
</ul>
</li>
</ul>
<p>This is a scalarized multi-objective optimization problem of <span class="arithmatex">\((f, R)\)</span>.</p>
<h2 id="convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier">11.2 Bicriterion Optimization and the Pareto Frontier<a class="headerlink" href="#convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier" title="Permanent link">¶</a></h2>
<p>Regularization corresponds to the bicriterion objective:</p>
<div class="arithmatex">\[
\min_{x} \; (f(x), R(x)).
\]</div>
<p>A point <span class="arithmatex">\(x^*\)</span> is Pareto optimal if there is no feasible <span class="arithmatex">\(x\)</span> such that:
<script type="math/tex; mode=display">
f(x) \le f(x^*),\quad R(x) \le R(x^*),
</script>
with strict inequality in at least one component.</p>
<p>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(R\)</span>:</p>
<ul>
<li>Every <span class="arithmatex">\(\lambda \ge 0\)</span> yields a Pareto-optimal point,</li>
<li>The mapping from <span class="arithmatex">\(\lambda\)</span> to constraint level <span class="arithmatex">\(R(x^*)\)</span> is monotone,</li>
<li>The Pareto frontier is convex and can be traced continuously by varying <span class="arithmatex">\(\lambda\)</span>.</li>
</ul>
<p>Thus, tuning <span class="arithmatex">\(\lambda\)</span> moves the solution along the fit–complexity frontier.</p>
<h2 id="convex-18b_regularization-113-why-control-the-size-of-the-solution">11.3 Why Control the Size of the Solution?<a class="headerlink" href="#convex-18b_regularization-113-why-control-the-size-of-the-solution" title="Permanent link">¶</a></h2>
<p>Inverse problems such as <span class="arithmatex">\(Ax \approx b\)</span> are often ill-posed or ill-conditioned:</p>
<ul>
<li>Small noise in <span class="arithmatex">\(b\)</span> may cause large variability in the solution <span class="arithmatex">\(x\)</span>.  </li>
<li>If <span class="arithmatex">\(A\)</span> is rank-deficient or nearly singular, infinitely many solutions exist.</li>
</ul>
<p>Example: ridge regression</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
\]</div>
<p>The optimality condition is</p>
<div class="arithmatex">\[
(A^\top A + \lambda I)x = A^\top b.
\]</div>
<p>Benefits of L2 regularization:</p>
<ul>
<li><span class="arithmatex">\(A^\top A + \lambda I\)</span> becomes positive definite for any <span class="arithmatex">\(\lambda &gt; 0\)</span>,  </li>
<li>the solution becomes unique and stable,  </li>
<li>small singular directions of <span class="arithmatex">\(A\)</span> are suppressed.</li>
</ul>
<p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>
<h2 id="convex-18b_regularization-114-constrained-vs-penalized-formulations">11.4 Constrained vs. Penalized Formulations<a class="headerlink" href="#convex-18b_regularization-114-constrained-vs-penalized-formulations" title="Permanent link">¶</a></h2>
<p>Regularized problems can be expressed equivalently as constrained problems:</p>
<div class="arithmatex">\[
\min_x f(x) 
\quad \text{s.t. } R(x) \le t.
\]</div>
<p>The Lagrangian is</p>
<div class="arithmatex">\[
\mathcal{L}(x,\lambda)
= f(x) + \lambda (R(x) - t),
\qquad \lambda \ge 0.
\]</div>
<p>The penalized form</p>
<div class="arithmatex">\[
\min_x f(x) + \lambda R(x)
\]</div>
<p>is the dual of the constrained form. Under convexity and Slater’s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p>
<div class="arithmatex">\[
0 \in \partial f(x^*) + \lambda^* \partial R(x^*), 
\]</div>
<div class="arithmatex">\[
R(x^*) \le t,\qquad \lambda^* \ge 0,\qquad \lambda^*(R(x^*) - t) = 0.
\]</div>
<p>Here:</p>
<ul>
<li>If <span class="arithmatex">\(R(x^*) &lt; t\)</span>, then <span class="arithmatex">\(\lambda^* = 0\)</span>.  </li>
<li>If <span class="arithmatex">\(\lambda^* &gt; 0\)</span>, then <span class="arithmatex">\(R(x^*) = t\)</span> (constraint active).</li>
</ul>
<p>Thus <span class="arithmatex">\(\lambda\)</span> is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>
<h2 id="convex-18b_regularization-115-common-regularizers-and-their-effects">11.5 Common Regularizers and Their Effects<a class="headerlink" href="#convex-18b_regularization-115-common-regularizers-and-their-effects" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-l2-regularization-ridge">(a) L2 Regularization (Ridge)<a class="headerlink" href="#convex-18b_regularization-a-l2-regularization-ridge" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \|x\|_2^2.
\]</div>
<ul>
<li>Smooth and strongly convex.  </li>
<li>Shrinks coefficients uniformly.  </li>
<li>Improves conditioning.  </li>
<li>MAP interpretation: Gaussian prior <span class="arithmatex">\(x \sim \mathcal{N}(0,\tau^2 I)\)</span>.</li>
</ul>
<h3 id="convex-18b_regularization-b-l1-regularization-lasso">(b) L1 Regularization (Lasso)<a class="headerlink" href="#convex-18b_regularization-b-l1-regularization-lasso" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \|x\|_1 = \sum_i |x_i|.
\]</div>
<ul>
<li>Convex but not differentiable → promotes sparsity.  </li>
<li>The <span class="arithmatex">\(\ell_1\)</span> ball has corners aligned with coordinate axes, encouraging zeros in <span class="arithmatex">\(x\)</span>.  </li>
<li>Proximal operator (soft-thresholding):</li>
</ul>
<div class="arithmatex">\[
\operatorname{prox}_{\tau\|\cdot\|_1}(v)
= \operatorname{sign}(v)\,\max(|v|-\tau, 0).
\]</div>
<ul>
<li>MAP interpretation: Laplace prior.</li>
</ul>
<h3 id="convex-18b_regularization-c-elastic-net">(c) Elastic Net<a class="headerlink" href="#convex-18b_regularization-c-elastic-net" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \alpha \|x\|_1 + (1-\alpha)\|x\|_2^2.
\]</div>
<ul>
<li>Combines sparsity with numerical stability.  </li>
<li>Useful with correlated features.</li>
</ul>
<h3 id="convex-18b_regularization-d-beyond-l1l2-structured-regularizers">(d) Beyond L1/L2: Structured Regularizers<a class="headerlink" href="#convex-18b_regularization-d-beyond-l1l2-structured-regularizers" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Regularizer</th>
<th>Formula</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tikhonov</td>
<td><span class="arithmatex">\(\|Lx\|_2^2\)</span></td>
<td>smoothness via operator <span class="arithmatex">\(L\)</span></td>
</tr>
<tr>
<td>Total Variation</td>
<td><span class="arithmatex">\(\|\nabla x\|_1\)</span></td>
<td>piecewise-constant signals/images</td>
</tr>
<tr>
<td>Group Lasso</td>
<td><span class="arithmatex">\(\sum_g \|x_g\|_2\)</span></td>
<td>structured sparsity across groups</td>
</tr>
<tr>
<td>Nuclear Norm</td>
<td><span class="arithmatex">\(\|X\|_* = \sum_i \sigma_i\)</span></td>
<td>low-rank matrices</td>
</tr>
</tbody>
</table>
<p>Each regularizer defines a geometry for the solution — ellipsoids, diamonds, polytopes, or spectral shapes.</p>
<h2 id="convex-18b_regularization-116-choosing-the-regularization-parameter-lambda">11.6 Choosing the Regularization Parameter <span class="arithmatex">\(\lambda\)</span><a class="headerlink" href="#convex-18b_regularization-116-choosing-the-regularization-parameter-lambda" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-trade-off-behavior">(a) Trade-Off Behavior<a class="headerlink" href="#convex-18b_regularization-a-trade-off-behavior" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\lambda \downarrow\)</span>: favors small training error, high variance.  </li>
<li><span class="arithmatex">\(\lambda \uparrow\)</span>: favors simplicity, higher bias.  </li>
</ul>
<p><span class="arithmatex">\(\lambda\)</span> selects a point on the fit–complexity Pareto frontier.</p>
<h3 id="convex-18b_regularization-b-cross-validation">(b) Cross-Validation<a class="headerlink" href="#convex-18b_regularization-b-cross-validation" title="Permanent link">¶</a></h3>
<p>The most common practice:</p>
<ol>
<li>Split data into folds.  </li>
<li>Train on <span class="arithmatex">\(k-1\)</span> folds, validate on the remaining fold.  </li>
<li>Choose <span class="arithmatex">\(\lambda\)</span> minimizing average validation error.</li>
</ol>
<p>Guidelines:</p>
<ul>
<li>Standardize features for L1/Elastic Net.  </li>
<li>Use time-aware CV for dependent data.  </li>
<li>Use the “one-standard-error rule” for simpler models.</li>
</ul>
<h3 id="convex-18b_regularization-c-other-selection-methods">(c) Other Selection Methods<a class="headerlink" href="#convex-18b_regularization-c-other-selection-methods" title="Permanent link">¶</a></h3>
<ul>
<li>Information criteria (AIC, BIC) for sparsity.  </li>
<li>L-curve or discrepancy principle in inverse problems.  </li>
<li>Regularization paths: computing <span class="arithmatex">\(x^*(\lambda)\)</span> for many <span class="arithmatex">\(\lambda\)</span>.</li>
</ul>
<h2 id="convex-18b_regularization-117-algorithmic-view">11.7 Algorithmic View<a class="headerlink" href="#convex-18b_regularization-117-algorithmic-view" title="Permanent link">¶</a></h2>
<p>Most regularized problems have the form:</p>
<div class="arithmatex">\[
\min_x \ f(x) + R(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is smooth convex and <span class="arithmatex">\(R\)</span> is convex (possibly nonsmooth).</p>
<p>Common algorithms:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Idea</th>
<th>When Useful</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA/FISTA)</td>
<td>Gradient step on <span class="arithmatex">\(f\)</span>, proximal step on <span class="arithmatex">\(R\)</span></td>
<td>L1, TV, nuclear norm</td>
</tr>
<tr>
<td>Coordinate Descent</td>
<td>Update coordinates cyclically</td>
<td>Lasso, Elastic Net</td>
</tr>
<tr>
<td>ADMM</td>
<td>Split problem to exploit structure</td>
<td>Large-scale or distributed settings</td>
</tr>
</tbody>
</table>
<p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal <span class="arithmatex">\(O(1/k^2)\)</span> rate for smooth+convex problems.</p>
<h2 id="convex-18b_regularization-118-bayesian-interpretation">11.8 Bayesian Interpretation<a class="headerlink" href="#convex-18b_regularization-118-bayesian-interpretation" title="Permanent link">¶</a></h2>
<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p>
<p>Linear model:</p>
<div class="arithmatex">\[
b = Ax + \varepsilon,\qquad \varepsilon \sim \mathcal{N}(0,\sigma^2 I).
\]</div>
<p>With prior <span class="arithmatex">\(x \sim p(x)\)</span>, MAP estimation solves:</p>
<div class="arithmatex">\[
\min_x \ \frac{1}{2\sigma^2}\|Ax - b\|_2^2 - \log p(x).
\]</div>
<p>Examples:</p>
<ul>
<li>Gaussian prior <span class="arithmatex">\(p(x) \propto e^{-\|x\|_2^2 / (2\tau^2)}\)</span><br>
  → L2 penalty with <span class="arithmatex">\(\lambda = \sigma^2/(2\tau^2)\)</span>.  </li>
<li>Laplace prior<br>
  → L1 penalty and sparse MAP estimate.</li>
</ul>
<p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p>
<p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p></body></html></section><section class="print-page" id="convex-19_optimizationalgo" heading-number="2.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-12-algorithms-for-convex-optimization">Chapter 12: Algorithms for Convex Optimization<a class="headerlink" href="#convex-19_optimizationalgo-chapter-12-algorithms-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p>
<p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3–11) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept — gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13–15) extend these ideas to constrained, stochastic, and large-scale environments.</p>
<h2 id="convex-19_optimizationalgo-121-problem-classes-vs-method-classes">12.1 Problem classes vs method classes<a class="headerlink" href="#convex-19_optimizationalgo-121-problem-classes-vs-method-classes" title="Permanent link">¶</a></h2>
<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Formulation</th>
<th>Representative Methods</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth, unconstrained</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span>, convex and differentiable</td>
<td>Gradient descent, Accelerated gradient, Newton</td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Smooth with simple constraints</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span> s.t. <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex)</td>
<td>Projected gradient</td>
<td>Constrained regression, probability simplex</td>
</tr>
<tr>
<td>Composite convex (smooth + nonsmooth)</td>
<td><span class="arithmatex">\(\min_x f(x) + R(x)\)</span></td>
<td>Proximal gradient, coordinate descent</td>
<td>Lasso, Elastic Net, TV minimization</td>
</tr>
<tr>
<td>General constrained convex</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(g_i(x) \le 0, h_j(x)=0\)</span></td>
<td>Interior-point, primal–dual methods</td>
<td>LP, QP, SDP, SOCP</td>
</tr>
</tbody>
</table>
<h2 id="convex-19_optimizationalgo-122-first-order-methods-gradient-descent">12.2 First-order methods: Gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-122-first-order-methods-gradient-descent" title="Permanent link">¶</a></h2>
<p>We solve
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex, differentiable, and (ideally) <span class="arithmatex">\(L\)</span>-smooth: its gradient is Lipschitz with constant <span class="arithmatex">\(L\)</span>, meaning
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\|_2 \le L \|x-y\|_2 \quad \text{for all } x,y.
</script>
</p>
<blockquote>
<p>Smoothness lets us control step sizes.</p>
</blockquote>
<p>Gradient descent iterates
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k),
</script>
where <span class="arithmatex">\(\alpha_k&gt;0\)</span> is the step size (also called learning rate in machine learning). Typical choices:</p>
<ul>
<li>constant <span class="arithmatex">\(\alpha_k = 1/L\)</span> when <span class="arithmatex">\(L\)</span> is known,</li>
<li>backtracking line search when <span class="arithmatex">\(L\)</span> is unknown,</li>
<li>diminishing step sizes in some settings.</li>
</ul>
<blockquote>
<p>Derivation: </p>
<p>Around <span class="arithmatex">\(x_t\)</span>, we can approximate <span class="arithmatex">\(f\)</span> using its Taylor expansion:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle.
\]</div>
<p>We assume <span class="arithmatex">\(f\)</span> behaves approximately like its tangent plane near <span class="arithmatex">\(x_t\)</span>.  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent <span class="arithmatex">\(-\nabla f(x_t)\)</span>, which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near <span class="arithmatex">\(x_t\)</span>, not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from <span class="arithmatex">\(x_t\)</span>:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2,
\]</div>
<p>where <span class="arithmatex">\(\eta &gt; 0\)</span> is the learning rate or step size.</p>
<ul>
<li>The linear term pulls <span class="arithmatex">\(x\)</span> in the steepest descent direction.</li>
<li>The quadratic term acts like a trust region, discouraging large deviations from <span class="arithmatex">\(x_t\)</span>.</li>
<li><span class="arithmatex">\(\eta\)</span> trades off aggressive progress vs stability:<ul>
<li>Small <span class="arithmatex">\(\eta\)</span> → cautious updates.</li>
<li>Large <span class="arithmatex">\(\eta\)</span> → bold updates (risk of divergence).</li>
</ul>
</li>
</ul>
<p>We define the next iterate as the minimizer of the surrogate objective:</p>
<div class="arithmatex">\[
x_{t+1} = \arg\min_{x \in \mathcal{X}} \Big[ f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2 \Big].
\]</div>
<p>Ignoring the constant term <span class="arithmatex">\(f(x_t)\)</span> and differentiating w.r.t. <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
\nabla f(x_t) + \frac{1}{\eta}(x - x_t) = 0
\]</div>
<p>Solving:</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t)
\]</div>
</blockquote>
<p>Convergence: For convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(f\)</span>, gradient descent with a suitable fixed step size satisfies
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k}\right),
</script>
where <span class="arithmatex">\(f^\star\)</span> is the global minimum. This <span class="arithmatex">\(O(1/k)\)</span> sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need <span class="arithmatex">\(\nabla f(x_k)\)</span>.</p>
<p>When to use gradient descent:</p>
<ul>
<li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li>
<li>You can compute gradients cheaply.</li>
<li>You only need moderate accuracy.</li>
<li>Memory constraints rule out storing or factoring Hessians.</li>
</ul>
<h2 id="convex-19_optimizationalgo-123-accelerated-first-order-methods">12.3 Accelerated first-order methods<a class="headerlink" href="#convex-19_optimizationalgo-123-accelerated-first-order-methods" title="Permanent link">¶</a></h2>
<p>Plain gradient descent has an <span class="arithmatex">\(O(1/k)\)</span> rate for smooth convex problems. Remarkably, we can do better — and in fact, provably optimal — by adding <em>momentum</em>.</p>
<h3 id="convex-19_optimizationalgo-1231-nesterov-acceleration">12.3.1 Nesterov acceleration<a class="headerlink" href="#convex-19_optimizationalgo-1231-nesterov-acceleration" title="Permanent link">¶</a></h3>
<p>Nesterov’s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences <span class="arithmatex">\(x_k\)</span> and <span class="arithmatex">\(y_k\)</span>:</p>
<ol>
<li>Maintain two sequences <span class="arithmatex">\(x_k\)</span> and <span class="arithmatex">\(y_k\)</span>.</li>
<li>Take a gradient step from <span class="arithmatex">\(y_k\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = y_k - \alpha \nabla f(y_k).
   </script>
</li>
<li>Extrapolate:
   <script type="math/tex; mode=display">
   y_{k+1} = x_{k+1} + \beta_k (x_{k+1} - x_k).
   </script>
</li>
</ol>
<p>The extra momentum term <span class="arithmatex">\(\beta_k (x_{k+1}-x_k)\)</span> uses past iterates to “look ahead” and can significantly accelerate convergence.</p>
<p>Convergece: For smooth convex <span class="arithmatex">\(f\)</span>, accelerated gradient achieves
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k^2}\right),
</script>
which is <em>optimal</em> for any algorithm that uses only gradient information and not higher derivatives.</p>
<ul>
<li>Acceleration is effective for well-behaved smooth convex problems.</li>
<li>It can be more sensitive to step size and noise than plain gradient descent.</li>
<li>Variants such as FISTA apply acceleration in the composite setting <span class="arithmatex">\(f + R\)</span>.</li>
</ul>
<blockquote>
<p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned—that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-124-steepest-descent-method">12.4 Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-124-steepest-descent-method" title="Permanent link">¶</a></h2>
<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of <em>maximum decrease</em> of the objective function under a unit norm constraint.</p>
<blockquote>
<p>The norm defines the “geometry” of optimization.cGradient descent is steepest descent under the Euclidean norm. Changing the norm changes what “steepest” means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p>
</blockquote>
<p>At a point <span class="arithmatex">\(x\)</span>, and for a chosen norm <span class="arithmatex">\(|\cdot|\)</span>:</p>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = \arg\min_{|v| = 1} \nabla f(x)^T v
\]</div>
<p>This defines the normalized steepest descent direction — the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of <span class="arithmatex">\(f\)</span>).</p>
<ul>
<li><span class="arithmatex">\(\Delta x_{\text{nsd}}\)</span>: normalized steepest descent direction</li>
<li><span class="arithmatex">\(\Delta x_{\text{sd}}\)</span>: unnormalized direction (scaled by the gradient norm)</li>
</ul>
<p>For small steps <span class="arithmatex">\(v\)</span>,
<script type="math/tex; mode=display">
f(x + v) \approx f(x) + \nabla f(x)^T v.
</script>
The term <span class="arithmatex">\(\nabla f(x)^T v\)</span> describes how fast <span class="arithmatex">\(f\)</span> increases in direction <span class="arithmatex">\(v\)</span>.
To decrease <span class="arithmatex">\(f\)</span> most rapidly, we pick <span class="arithmatex">\(v\)</span> that minimizes this inner product — subject to <span class="arithmatex">\(|v| = 1\)</span>.</p>
<ul>
<li>The result depends on which norm we use to measure the “size” of <span class="arithmatex">\(v\)</span>.</li>
<li>The corresponding dual norm <span class="arithmatex">\(|\cdot|_*\)</span> determines how we measure the gradient’s magnitude.</li>
</ul>
<p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>
<h2 id="convex-19_optimizationalgo-1241-mathematical-properties">12.4.1. Mathematical Properties<a class="headerlink" href="#convex-19_optimizationalgo-1241-mathematical-properties" title="Permanent link">¶</a></h2>
<h3 id="convex-19_optimizationalgo-a-normalized-direction">(a) Normalized direction<a class="headerlink" href="#convex-19_optimizationalgo-a-normalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{nsd}} = \arg\min_{|v|=1} \nabla f(x)^T v
</script>
→ unit vector with the most negative directional derivative.</p>
<h3 id="convex-19_optimizationalgo-b-unnormalized-direction">(b) Unnormalized direction<a class="headerlink" href="#convex-19_optimizationalgo-b-unnormalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{sd}} = |\nabla f(x)| , \Delta x*{\text{nsd}}
</script>
This gives the actual direction and magnitude used in updates.</p>
<h3 id="convex-19_optimizationalgo-c-key-identity">(c) Key identity<a class="headerlink" href="#convex-19_optimizationalgo-c-key-identity" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\nabla f(x)^T \Delta x_{\text{sd}} = -|\nabla f(x)|_*^2
</script>
The directional derivative equals the negative squared dual norm of the gradient.</p>
<h3 id="convex-19_optimizationalgo-1242-the-steepest-descent-method">12.4.2. The Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-1242-the-steepest-descent-method" title="Permanent link">¶</a></h3>
<p>The iterative update rule is:
<script type="math/tex; mode=display">
x_{k+1} = x_k + t_k , \Delta x_{\text{sd}},
</script>
where <span class="arithmatex">\(t_k &gt; 0\)</span> is a step size (from line search or a fixed rule).</p>
<ul>
<li>For the Euclidean norm, this reduces to ordinary gradient descent.</li>
<li>For other norms, it adapts the search direction to the geometry of the problem.</li>
</ul>
<p>Convergence: Similar to gradient descent — linear for general convex functions, potentially faster when level sets are well-conditioned.</p>
<h3 id="convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence">12.4.3. Role of the Norm and Its Influence<a class="headerlink" href="#convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence" title="Permanent link">¶</a></h3>
<p>The choice of norm determines:</p>
<ol>
<li>The shape of the unit ball <span class="arithmatex">\({v : |v| \le 1}\)</span>,</li>
<li>The direction of steepest descent, since the minimization is constrained by that shape,</li>
<li>The dual norm <span class="arithmatex">\(|\nabla f(x)|_*\)</span> that measures the gradient’s size.</li>
</ol>
<p>Different norms yield different “geometries” of descent:</p>
<table>
<thead>
<tr>
<th>Norm</th>
<th>Unit Ball Shape</th>
<th>Dual Norm</th>
<th>Effect on Direction</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Circle / sphere</td>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Direction is opposite to gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Diamond</td>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Moves along coordinate of largest gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Square</td>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Moves opposite to sum of all gradient signs</td>
</tr>
<tr>
<td>Quadratic <span class="arithmatex">\((x^T P x)^{1/2}\)</span></td>
<td>Ellipsoid</td>
<td>Weighted <span class="arithmatex">\(\ell_2\)</span></td>
<td>Scales direction by preconditioner <span class="arithmatex">\(P^{-1}\)</span></td>
</tr>
</tbody>
</table>
<p>Thus, the norm defines how “distance” and “steepness” are perceived, shaping how the algorithm moves through the landscape of <span class="arithmatex">\(f(x)\)</span>.</p>
<h3 id="convex-19_optimizationalgo-a-euclidean-norm-v_2">(a) Euclidean Norm <span class="arithmatex">\(|v|_2\)</span><a class="headerlink" href="#convex-19_optimizationalgo-a-euclidean-norm-v_2" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -\frac{\nabla f(x)}{|\nabla f(x)|*2},
\quad
\Delta x*{\text{sd}} = -\nabla f(x)
\]</div>
<p>This is standard gradient descent.
The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>
<h3 id="convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0">(b) Quadratic Norm <span class="arithmatex">\(|v|_P = (v^T P v)^{1/2}\)</span>, with <span class="arithmatex">\(P \succ 0\)</span><a class="headerlink" href="#convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0" title="Permanent link">¶</a></h3>
<p>Here, <span class="arithmatex">\(P\)</span> defines an ellipsoidal metric.
The dual norm is <span class="arithmatex">\(|y|_* = (y^T P^{-1} y)^{1/2}\)</span>.</p>
<div class="arithmatex">\[
\Delta x_{\text{sd}} = -P^{-1}\nabla f(x)
\]</div>
<p>This corresponds to preconditioned gradient descent, where <span class="arithmatex">\(P\)</span> rescales directions to counter anisotropy in level sets.</p>
<p>Interpretation:</p>
<ul>
<li>If <span class="arithmatex">\(P\)</span> approximates the Hessian, this becomes Newton’s method.</li>
<li>If <span class="arithmatex">\(P\)</span> is diagonal, it acts like an adaptive step size per coordinate.</li>
</ul>
<h3 id="convex-19_optimizationalgo-c-ell_1-norm">(c) <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#convex-19_optimizationalgo-c-ell_1-norm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -e_i, \quad i = \arg\max_j \left|\frac{\partial f}{\partial x_j}\right|
\]</div>
<p>and</p>
<div class="arithmatex">\[
\Delta x_{\text{sd}} = -|\nabla f(x)|_\infty e_i
\]</div>
<p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p>
<p>Geometric intuition:
The <span class="arithmatex">\(\ell_1\)</span>-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p>
<ul>
<li>In <span class="arithmatex">\(\ell_2\)</span>-norm: the unit ball is a circle → the steepest direction is exactly opposite the gradient.</li>
<li>In <span class="arithmatex">\(\ell_1\)</span>-norm: the unit ball is a diamond → the steepest direction points to a corner (one coordinate).</li>
<li>In quadratic norms: the unit ball is an ellipsoid → the steepest direction follows the metric-adjusted gradient.</li>
</ul>
<p>Hence, the norm defines the geometry of what “steepest” means.</p>
<h2 id="convex-19_optimizationalgo-125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives">12.5 Conjugate Gradient Method — Fast Optimization for Quadratic Objectives<a class="headerlink" href="#convex-19_optimizationalgo-125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives" title="Permanent link">¶</a></h2>
<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p>
<p>CG is a <em>first-order</em> method that behaves like a <em>second-order</em> method for quadratics.</p>
<p>For a quadratic objective function:</p>
<div class="arithmatex">\[
f(x) = \tfrac12 x^\top A x - b^\top x 
\]</div>
<p>with <span class="arithmatex">\(A \succ 0\)</span>, the level sets are ellipses shaped by the eigenvalues of <span class="arithmatex">\(A\)</span>. If <span class="arithmatex">\(A\)</span> is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p>
<p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors <span class="arithmatex">\(p_i, p_j\)</span> are said to be A-conjugate if</p>
<div class="arithmatex">\[
p_i^\top A p_j = 0.
\]</div>
<p>This is orthogonality measured in the geometry induced by the Hessian <span class="arithmatex">\(A\)</span>. Why is this useful?</p>
<ul>
<li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of <span class="arithmatex">\(A\)</span>.</li>
<li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li>
<li>After <span class="arithmatex">\(n\)</span> mutually A-conjugate directions, all curvature directions are resolved → exact solution.</li>
</ul>
<p>In contrast, gradient descent repeatedly re-corrects previous progress.</p>
<p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system <span class="arithmatex">\(Ax = b\)</span>. Let</p>
<div class="arithmatex">\[
r_0 = b - A x_0, \qquad p_0 = r_0.
\]</div>
<p>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:</p>
<ol>
<li>
<p>Step size
   <script type="math/tex; mode=display">
   \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k}.
   </script>
</p>
</li>
<li>
<p>Update iterate
   <script type="math/tex; mode=display">
   x_{k+1} = x_k + \alpha_k p_k.
   </script>
</p>
</li>
<li>
<p>Update residual (negative gradient)
   <script type="math/tex; mode=display">
   r_{k+1} = r_k - \alpha_k A p_k.
   </script>
</p>
</li>
<li>
<p>Direction scaling
   <script type="math/tex; mode=display">
   \beta_k = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
   </script>
</p>
</li>
<li>
<p>New conjugate direction
   <script type="math/tex; mode=display">
   p_{k+1} = r_{k+1} + \beta_k p_k.
   </script>
</p>
</li>
</ol>
<p>Stop when <span class="arithmatex">\(\|r_k\|\)</span> is below tolerance.</p>
<p>Every new direction <span class="arithmatex">\(p_{k+1}\)</span> is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p>
<p>Why CG Is Fast: For an <span class="arithmatex">\(n\)</span>-dimensional quadratic, CG solves the problem in at most <span class="arithmatex">\(n\)</span> iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in <span class="arithmatex">\(O(\sqrt{\kappa})\)</span> iterations, where <span class="arithmatex">\(\kappa = \lambda_{\max}/\lambda_{\min}\)</span> is the condition number. The convergence bound in the A-norm is:</p>
<div class="arithmatex">\[
\|x_k - x^\star\|_A \le 
2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k 
\|x_0 - x^\star\|_A.
\]</div>
<p>This is dramatically better than the <span class="arithmatex">\(O(1/k)\)</span> rate of gradient descent.</p>
<p>CG is ideal when:</p>
<ul>
<li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix <span class="arithmatex">\(A\)</span>.</li>
<li><span class="arithmatex">\(A\)</span> is large and sparse or available as a matrix–vector product.</li>
<li>You cannot form or store <span class="arithmatex">\(A^{-1}\)</span> or even the full matrix <span class="arithmatex">\(A\)</span>.</li>
<li>You want a Hessian-aware method but cannot afford Newton’s method.</li>
</ul>
<p>Typical scenarios:</p>
<table>
<thead>
<tr>
<th>Application</th>
<th>Why CG fits</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large linear systems <span class="arithmatex">\(A x = b\)</span></td>
<td>Only requires <span class="arithmatex">\(A p\)</span>, not factorization.</td>
</tr>
<tr>
<td>Ridge regression</td>
<td>Normal equations form an SPD matrix.</td>
</tr>
<tr>
<td>Kernel ridge regression</td>
<td>Solves <span class="arithmatex">\((K+\lambda I)\alpha = y\)</span> efficiently.</td>
</tr>
<tr>
<td>Newton steps in ML</td>
<td>Inner solver for Hessian systems without forming Hessian.</td>
</tr>
<tr>
<td>PDEs and scientific computing</td>
<td>Sparse SPD matrices, ideal for CG.</td>
</tr>
</tbody>
</table>
<p>Assumptions Required for CG: To guarantee correctness of <em>linear CG</em>, we require:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is symmetric</li>
<li><span class="arithmatex">\(A\)</span> is positive definite</li>
<li>Objective is strictly convex quadratic</li>
<li>Arithmetic is exact (for the finite-step guarantee)</li>
</ul>
<p>If the function is <em>not</em> quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p>
<p>Practical Notes:</p>
<ul>
<li>You only need matrix–vector products <span class="arithmatex">\(Ap\)</span>.  </li>
<li>Storage cost is <span class="arithmatex">\(O(n)\)</span>.  </li>
<li>Preconditioning (replacing the system with <span class="arithmatex">\(M^{-1} A\)</span>) improves conditioning and accelerates convergence dramatically.  </li>
<li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li>
</ul>
<blockquote>
<p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-126-newtons-method-and-second-order-methods">12.6 Newton’s method and second-order methods<a class="headerlink" href="#convex-19_optimizationalgo-126-newtons-method-and-second-order-methods" title="Permanent link">¶</a></h2>
<p>First-order methods (like gradient descent) only use gradient information. Newton’s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p>
<p>From Chapter 3, the second-order Taylor approximation of <span class="arithmatex">\(f(x)\)</span> around a point <span class="arithmatex">\(x_k\)</span> is:</p>
<div class="arithmatex">\[
f(x_k + d)
\approx
f(x_k)
+ \nabla f(x_k)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x_k) d.
\]</div>
<p>If we temporarily trust this quadratic model, we can choose <span class="arithmatex">\(d\)</span> to minimize the right-hand side. Differentiating with respect to <span class="arithmatex">\(d\)</span> and setting to zero gives:</p>
<div class="arithmatex">\[
\nabla^2 f(x_k) \, d_{\text{newton}} = - \nabla f(x_k).
\]</div>
<p>Hence, the Newton step is:</p>
<div class="arithmatex">\[
d_{\text{newton}} = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k),
\quad
x_{k+1} = x_k + d_{\text{newton}}.
\]</div>
<p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex <span class="arithmatex">\(f\)</span>, Newton’s method achieves quadratic convergence—dramatically faster than the <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span> rates typical of first-order algorithms.</p>
<p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton’s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>
<h3 id="convex-19_optimizationalgo-solving-the-newton-system">Solving the Newton System<a class="headerlink" href="#convex-19_optimizationalgo-solving-the-newton-system" title="Permanent link">¶</a></h3>
<p>Each iteration requires solving</p>
<div class="arithmatex">\[
H \,\Delta x = -g,
\qquad
H = \nabla^2 f(x), \;\; g = \nabla f(x).
\]</div>
<p>If <span class="arithmatex">\(H\)</span> is symmetric positive definite, a Cholesky factorization</p>
<div class="arithmatex">\[
H = L L^\top
\]</div>
<p>allows efficient and numerically stable solution via two triangular solves:</p>
<ol>
<li><span class="arithmatex">\(L y = -g\)</span></li>
<li><span class="arithmatex">\(L^\top \Delta x_{\text{nt}} = y\)</span></li>
</ol>
<p>This avoids forming <span class="arithmatex">\(H^{-1}\)</span> explicitly.</p>
<p>The Newton decrement:</p>
<div class="arithmatex">\[
\lambda(x) = \|L^{-1} g\|_2
\]</div>
<p>gauges proximity to the optimum and provides a natural stopping criterion: <span class="arithmatex">\(\lambda(x)^2/2 &lt; \varepsilon\)</span>.</p>
<p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs <span class="arithmatex">\(\approx (1/3)n^3\)</span> operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton’s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix–free iterative methods.</p>
<h3 id="convex-19_optimizationalgo-gaussnewton-method">Gauss–Newton Method<a class="headerlink" href="#convex-19_optimizationalgo-gaussnewton-method" title="Permanent link">¶</a></h3>
<p>The Gauss–Newton method is a specialization of Newton’s method for nonlinear least squares problems</p>
<div class="arithmatex">\[
f(x) = \tfrac12 \| r(x) \|^2,
\]</div>
<p>where <span class="arithmatex">\(r(x)\)</span> is a vector of residual functions and a nonlinear function of <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(J\)</span> is its Jacobian. Newton’s Hessian decomposes as</p>
<div class="arithmatex">\[
\nabla^2 f(x) = J^\top J \;+\; \sum_i r_i(x)\, \nabla^2 r_i(x).
\]</div>
<p>The second term involves the curvature of the residuals. When <span class="arithmatex">\(r(x)\)</span> is approximately linear near the optimum, this term is small. Gauss–Newton drops it, giving the approximation</p>
<div class="arithmatex">\[
\nabla^2 f(x) \approx J^\top J,
\]</div>
<p>leading to the Gauss–Newton step:</p>
<div class="arithmatex">\[
(J^\top J)\, \Delta = -J^\top r.
\]</div>
<p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg–Marquardt method adds a damping term,</p>
<div class="arithmatex">\[
(J^\top J + \lambda I)\, \Delta = -J^\top r,
\]</div>
<p>which interpolates smoothly between  </p>
<ul>
<li>gradient descent (large <span class="arithmatex">\(\lambda\)</span>), and  </li>
<li>Gauss–Newton (small <span class="arithmatex">\(\lambda\)</span>).</li>
</ul>
<p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss–Newton and Levenberg–Marquardt are highly effective when the residuals are nearly linear—common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning—yielding fast convergence without the expense of full second derivatives.</p>
<h3 id="convex-19_optimizationalgo-quasi-newton-methods">Quasi-Newton methods<a class="headerlink" href="#convex-19_optimizationalgo-quasi-newton-methods" title="Permanent link">¶</a></h3>
<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> or its inverse. These methods use gradient information from previous steps to estimate curvature.</p>
<p>The most famous examples are:</p>
<ul>
<li>BFGS (Broyden–Fletcher–Goldfarb–Shanno)  </li>
<li>DFP (Davidon–Fletcher–Powell)  </li>
<li>L-BFGS (Limited-memory BFGS) — for very large-scale problems.</li>
</ul>
<p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton’s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation <span class="arithmatex">\(B_k \approx \nabla^2 f(x_k)^{-1}\)</span> updated via gradient and step differences:</p>
<div class="arithmatex">\[
B_{k+1} = B_k + \frac{(s_k^\top y_k + y_k^\top B_k y_k)}{(s_k^\top y_k)^2} s_k s_k^\top
- \frac{B_k y_k s_k^\top + s_k y_k^\top B_k}{s_k^\top y_k},
\]</div>
<p>where <span class="arithmatex">\(s_k = x_{k+1} - x_k\)</span> and <span class="arithmatex">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_k)\)</span>.</p>
<p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p>
<p>When to use Newton or quasi-Newton methods:</p>
<ul>
<li>You need high-accuracy solutions.  </li>
<li>The problem is smooth and reasonably well-conditioned.  </li>
<li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li>
</ul>
<p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>
<h2 id="convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods">12.8 Constraints and nonsmooth terms: projection and proximal methods<a class="headerlink" href="#convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods" title="Permanent link">¶</a></h2>
<p>In practice, most convex objectives are not just “nice smooth <span class="arithmatex">\(f(x)\)</span>”. They often have:</p>
<ul>
<li>constraints <span class="arithmatex">\(x \in \mathcal{X}\)</span>,</li>
<li>nonsmooth regularisers like <span class="arithmatex">\(\|x\|_1\)</span>,</li>
<li>penalties that encode robustness or sparsity (Chapter 6).</li>
</ul>
<p>Two core ideas handle this: projected gradient and proximal gradient.</p>
<h3 id="convex-19_optimizationalgo-1281-projected-gradient-descent">12.8.1 Projected gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-1281-projected-gradient-descent" title="Permanent link">¶</a></h3>
<p>Setting: Minimise convex, differentiable <span class="arithmatex">\(f(x)\)</span> subject to <span class="arithmatex">\(x \in \mathcal{X}\)</span>, where <span class="arithmatex">\(\mathcal{X}\)</span> is a simple closed convex set (Chapter 4).</p>
<p>Algorithm:</p>
<ol>
<li>Gradient step:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Projection:
   <script type="math/tex; mode=display">
   x_{k+1}
   =
   \Pi_{\mathcal{X}}(y_k)
   :=
   \arg\min_{x \in \mathcal{X}} \|x - y_k\|_2^2~.
   </script>
</li>
</ol>
<p>Interpretation:</p>
<ul>
<li>You take an unconstrained step downhill,</li>
<li>then you “snap back” to feasibility by Euclidean projection.</li>
</ul>
<p>Examples of <span class="arithmatex">\(\mathcal{X}\)</span> where projection is cheap:</p>
<ul>
<li>A box: <span class="arithmatex">\(l \le x \le u\)</span> (clip each coordinate).</li>
<li>The probability simplex <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span> (there are fast projection routines).</li>
<li>An <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\(\{x : \|x\|_2 \le R\}\)</span> (scale down if needed).</li>
</ul>
<p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>
<h3 id="convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting">12.8.2 Proximal gradient (forward–backward splitting)<a class="headerlink" href="#convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting" title="Permanent link">¶</a></h3>
<p>Setting: Composite convex minimisation
<script type="math/tex; mode=display">
\min_x \; F(x) := f(x) + R(x),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> is convex, differentiable, with Lipschitz gradient,</li>
<li><span class="arithmatex">\(R\)</span> is convex, possibly nonsmooth.</li>
</ul>
<p>Typical choices of <span class="arithmatex">\(R(x)\)</span>:</p>
<ul>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_1\)</span> (sparsity),</li>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_2^2\)</span> (ridge),</li>
<li><span class="arithmatex">\(R(x)\)</span> is the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>, i.e. <span class="arithmatex">\(R(x)=0\)</span> if <span class="arithmatex">\(x \in \mathcal{X}\)</span> and <span class="arithmatex">\(+\infty\)</span> otherwise — this encodes a hard constraint.</li>
</ul>
<p>Define the proximal operator of <span class="arithmatex">\(R\)</span>:
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha R}(y)
=
\arg\min_x
\left(
R(x) + \frac{1}{2\alpha} \|x-y\|_2^2
\right).
</script>
</p>
<p>Proximal gradient method:</p>
<ol>
<li>Gradient step on <span class="arithmatex">\(f\)</span>:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Proximal step on <span class="arithmatex">\(R\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = \mathrm{prox}_{\alpha R}(y_k).
   </script>
</li>
</ol>
<p>This is also called forward–backward splitting: “forward” = gradient step, “backward” = prox step.</p>
<h4 id="convex-19_optimizationalgo-interpretation">Interpretation:<a class="headerlink" href="#convex-19_optimizationalgo-interpretation" title="Permanent link">¶</a></h4>
<ul>
<li>The prox step “handles” the nonsmooth or constrained part exactly.</li>
<li>For <span class="arithmatex">\(R(x)=\lambda \|x\|_1\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> is soft-thresholding, which promotes sparsity in <span class="arithmatex">\(x\)</span>.<br>
  This is the heart of <span class="arithmatex">\(\ell_1\)</span>-regularised least-squares (LASSO) and many sparse recovery problems.</li>
<li>For <span class="arithmatex">\(R\)</span> as an indicator of <span class="arithmatex">\(\mathcal{X}\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R} = \Pi_\mathcal{X}\)</span>, so projected gradient is a special case of proximal gradient.</li>
</ul>
<p>This unifies constraints and regularisation.</p>
<h4 id="convex-19_optimizationalgo-when-to-use-proximal-projected-gradient">When to use proximal / projected gradient<a class="headerlink" href="#convex-19_optimizationalgo-when-to-use-proximal-projected-gradient" title="Permanent link">¶</a></h4>
<ul>
<li>High-dimensional ML/statistics problems.</li>
<li>Objectives with <span class="arithmatex">\(\ell_1\)</span>, group sparsity, total variation, hinge loss, or indicator constraints.</li>
<li>You can evaluate <span class="arithmatex">\(\nabla f\)</span> and compute <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> cheaply.</li>
<li>You don’t need absurdly high accuracy, but you do need scalability.</li>
</ul>
<p>This is the standard tool for modern large-scale convex learning problems.</p>
<h2 id="convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods">12.9 Penalties, barriers, and interior-point methods<a class="headerlink" href="#convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods" title="Permanent link">¶</a></h2>
<p>So far we’ve assumed either:</p>
<ul>
<li>simple constraints we can project onto,</li>
<li>or nonsmooth terms we can prox.</li>
</ul>
<p>What if the constraints are general convex inequalities <span class="arithmatex">\(g_i(x)\le0\)</span>: Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>
<h3 id="convex-19_optimizationalgo-1291-penalty-methods">12.9.1 Penalty methods<a class="headerlink" href="#convex-19_optimizationalgo-1291-penalty-methods" title="Permanent link">¶</a></h3>
<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want
<script type="math/tex; mode=display">
\min_x f(x)
\quad \text{s.t.} \quad g_i(x) \le 0,\ i=1,\dots,m.
</script>
</p>
<p>A penalty method solves instead
<script type="math/tex; mode=display">
\min_x \; f(x) + \rho \sum_{i=1}^m \phi(g_i(x)),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(\phi(r)\)</span> is <span class="arithmatex">\(0\)</span> when <span class="arithmatex">\(r \le 0\)</span> (feasible),</li>
<li><span class="arithmatex">\(\phi(r)\)</span> grows when <span class="arithmatex">\(r&gt;0\)</span> (infeasible),</li>
<li><span class="arithmatex">\(\rho &gt; 0\)</span> is a penalty weight.</li>
</ul>
<p>As <span class="arithmatex">\(\rho \to \infty\)</span>, infeasible points become extremely expensive, so minimisers approach feasibility.  </p>
<p>This is conceptually simple and is sometimes effective, but:</p>
<ul>
<li>choosing <span class="arithmatex">\(\rho\)</span> is tricky,</li>
<li>very large <span class="arithmatex">\(\rho\)</span> can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li>
</ul>
<h3 id="convex-19_optimizationalgo-algorithm-basic-penalty-method-quadratic-or-general-penalization">Algorithm: Basic Penalty Method (Quadratic or General Penalization)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-basic-penalty-method-quadratic-or-general-penalization" title="Permanent link">¶</a></h3>
<p>Goal:  Solve<br>
<script type="math/tex; mode=display">
\min_x f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Penalty formulation:<br>
<script type="math/tex; mode=display">
F_\rho(x) = f(x) + \rho \sum_{i=1}^m \phi(g_i(x)),
</script>
where  </p>
<ul>
<li><span class="arithmatex">\(\phi(r) = 0\)</span> if <span class="arithmatex">\(r \le 0\)</span>,  </li>
<li><span class="arithmatex">\(\phi(r)\)</span> grows when <span class="arithmatex">\(r&gt;0\)</span> (e.g., <span class="arithmatex">\(\phi(r)=\max\{0,r\}^2\)</span>),  </li>
<li><span class="arithmatex">\(\rho &gt; 0\)</span> is the penalty weight.</li>
</ul>
<p>Inputs:  </p>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>penalty function <span class="arithmatex">\(\phi\)</span>  </li>
<li>initial point <span class="arithmatex">\(x_0\)</span>  </li>
<li>initial penalty parameter <span class="arithmatex">\(\rho_0 &gt; 0\)</span>  </li>
<li>penalty update factor <span class="arithmatex">\(\gamma &gt; 1\)</span>  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<p>Procedure:</p>
<ol>
<li>Choose <span class="arithmatex">\(x_0\)</span>, <span class="arithmatex">\(\rho_0 &gt; 0\)</span>.  </li>
<li>For <span class="arithmatex">\(k = 0, 1, 2, \dots\)</span>:  <ol>
<li>Solve the penalized subproblem  <span class="arithmatex">\(x_{k+1} = \arg\min_x F_{\rho_k}(x)\)</span> using Newton’s method, gradient descent, quasi-Newton, etc.  </li>
<li>Check feasibility / stopping:  If <span class="arithmatex">\(\max_i g_i(x_{k+1}) \le \varepsilon, \quad   \|x_{k+1} - x_k\| \le \varepsilon\)</span>  stop and return <span class="arithmatex">\(x_{k+1}\)</span>.  </li>
<li>Increase penalty parameter  <span class="arithmatex">\(\rho_{k+1} = \gamma\, \rho_k\)</span>   with typical <span class="arithmatex">\(\gamma \in [5,10]\)</span>.  </li>
</ol>
</li>
<li>End.</li>
</ol>
<h3 id="convex-19_optimizationalgo-1292-barrier-methods">12.9.2 Barrier methods<a class="headerlink" href="#convex-19_optimizationalgo-1292-barrier-methods" title="Permanent link">¶</a></h3>
<p>Penalty methods penalise violation <em>after</em> you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints <span class="arithmatex">\(g_i(x) \le 0\)</span>, define the logarithmic barrier
<script type="math/tex; mode=display">
b(x) = - \sum_{i=1}^m \log(-g_i(x)).
</script>
This is finite only if <span class="arithmatex">\(g_i(x) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>, i.e. <span class="arithmatex">\(x\)</span> is strictly feasible. As you approach the boundary <span class="arithmatex">\(g_i(x)=0\)</span>, <span class="arithmatex">\(b(x)\)</span> blows up to <span class="arithmatex">\(+\infty\)</span>.</p>
<p>We then solve, for a sequence of increasing parameters <span class="arithmatex">\(t\)</span>:
<script type="math/tex; mode=display">
\min_x \; F_t(x) := t f(x) + b(x),
</script>
subject to strict feasibility <span class="arithmatex">\(g_i(x)&lt;0\)</span>.</p>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, minimisers of <span class="arithmatex">\(F_t\)</span> approach the true constrained optimum. The path of minimisers <span class="arithmatex">\(x^*(t)\)</span> is called the central path.</p>
<p>Key points:</p>
<ul>
<li><span class="arithmatex">\(F_t\)</span> is smooth on the interior of the feasible region.</li>
<li>We can apply Newton’s method to <span class="arithmatex">\(F_t\)</span>.</li>
<li>Each Newton step solves a linear system involving the Hessian of <span class="arithmatex">\(F_t\)</span>, so the inner loop looks like a damped Newton method.</li>
<li>Increasing <span class="arithmatex">\(t\)</span> tightens the approximation; we “home in” on the boundary of feasibility.</li>
</ul>
<h3 id="convex-19_optimizationalgo-algorithm-barrier-method-logarithmic-barrier-interior-approximation">Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-barrier-method-logarithmic-barrier-interior-approximation" title="Permanent link">¶</a></h3>
<p>Goal: Solve the constrained problem<br>
<script type="math/tex; mode=display">
\min_x f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Logarithmic barrier:<br>
<script type="math/tex; mode=display">
b(x) = -\sum_{i=1}^m \log\!\big(-g_i(x)\big),
</script>
defined only for strictly feasible points <span class="arithmatex">\(g_i(x)&lt;0\)</span>.</p>
<p>Barrier subproblem:<br>
<script type="math/tex; mode=display">
F_t(x) = t\, f(x) + b(x),
</script>
where <span class="arithmatex">\(t&gt;0\)</span> is the barrier parameter.</p>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, minimizers of <span class="arithmatex">\(F_t\)</span> approach the constrained optimum.</p>
<p>Inputs:  </p>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>inequality constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>barrier function <span class="arithmatex">\(b(x)\)</span>  </li>
<li>strictly feasible starting point <span class="arithmatex">\(x_0\)</span> (<span class="arithmatex">\(g_i(x_0) &lt; 0\)</span>)  </li>
<li>initial barrier parameter <span class="arithmatex">\(t_0 &gt; 0\)</span>  </li>
<li>barrier growth factor <span class="arithmatex">\(\mu &gt; 1\)</span> (often <span class="arithmatex">\(\mu = 10\)</span>)  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<p>Procedure:</p>
<ol>
<li>Choose strictly feasible <span class="arithmatex">\(x_0\)</span>, and pick <span class="arithmatex">\(t_0 &gt; 0\)</span>.  </li>
<li>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:  <ol>
<li>Centering step (inner loop):  Solve the barrier subproblem  <script type="math/tex">
  x_{k+1} = \arg\min_x F_{t_k}(x)
  \quad\text{with} g_i(x)<0. </script>  Typically use Newton’s method (damped) on <span class="arithmatex">\(F_{t_k}\)</span>.  Stop when the Newton decrement satisfies  <span class="arithmatex">\(\lambda(x_{k+1})^2/2 \le \varepsilon\)</span></li>
<li>Optimality / stopping test:    If  <span class="arithmatex">\(\frac{m}{t_k} \le \varepsilon,\)</span>
  then <span class="arithmatex">\(x_{k+1}\)</span> is an <span class="arithmatex">\(\varepsilon\)</span>-approximate solution of the original constrained problem; stop and return <span class="arithmatex">\(x_{k+1}\)</span>.  </li>
<li>Increase barrier parameter:  <span class="arithmatex">\(t_{k+1} = \mu\, t_k,\)</span>   which tightens the approximation and moves closer to the boundary.  </li>
</ol>
</li>
<li>End.</li>
</ol>
<h3 id="convex-19_optimizationalgo-1293-interior-point-methods">12.9.3 Interior-point methods<a class="headerlink" href="#convex-19_optimizationalgo-1293-interior-point-methods" title="Permanent link">¶</a></h3>
<p>Interior-point methods combine barrier functions with Newton’s method to solve general convex programs:</p>
<ul>
<li>They maintain strict feasibility throughout.</li>
<li>Each iteration solves a Newton system for the barrier-augmented objective.</li>
<li>They naturally generate primal–dual pairs and duality gap estimates.</li>
<li>Under standard assumptions (e.g., Slater’s condition), they converge in a predictable number of iterations.</li>
</ul>
<p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>
<h3 id="convex-19_optimizationalgo-algorithm-primaldual-interior-point-method-for-convex-inequality-constraints">Algorithm: Primal–Dual Interior-Point Method (for convex inequality constraints)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-primaldual-interior-point-method-for-convex-inequality-constraints" title="Permanent link">¶</a></h3>
<p>We consider the problem
<script type="math/tex; mode=display">
\min_x\; f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Introduce Lagrange multipliers <span class="arithmatex">\(\lambda \ge 0\)</span>. The KKT conditions are
<script type="math/tex; mode=display">
\begin{aligned}
\nabla f(x) + \sum_i \lambda_i \nabla g_i(x) &= 0, \\
g_i(x) &\le 0, \\
\lambda_i &\ge 0, \\
\lambda_i\, g_i(x) &= 0.
\end{aligned}
</script>
</p>
<p>Interior-point methods enforce the relaxed condition
<script type="math/tex; mode=display">
\lambda_i\, g_i(x) = -\frac{1}{t},
</script>
which keeps iterates strictly feasible.</p>
<h3 id="convex-19_optimizationalgo-inputs">Inputs<a class="headerlink" href="#convex-19_optimizationalgo-inputs" title="Permanent link">¶</a></h3>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>inequality constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>initial primal point <span class="arithmatex">\(x_0\)</span> with <span class="arithmatex">\(g_i(x_0)&lt;0\)</span>  </li>
<li>initial dual variable <span class="arithmatex">\(\lambda_0 &gt; 0\)</span>  </li>
<li>initial barrier parameter <span class="arithmatex">\(t_0 &gt; 0\)</span>  </li>
<li>growth factor <span class="arithmatex">\(\mu &gt; 1\)</span>  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<h3 id="convex-19_optimizationalgo-procedure">Procedure<a class="headerlink" href="#convex-19_optimizationalgo-procedure" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Choose strictly feasible <span class="arithmatex">\(x_0\)</span>, positive <span class="arithmatex">\(\lambda_0\)</span>, and <span class="arithmatex">\(t_0\)</span>.</p>
</li>
<li>
<p>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:</p>
<p>(a) Form the perturbed KKT system.  Solve for the Newton direction <span class="arithmatex">\((\Delta x, \Delta \lambda)\)</span>:</p>
<p>
<script type="math/tex; mode=display">
   \begin{bmatrix}
   \nabla^2 f(x) + \sum_i \lambda_i \nabla^2 g_i(x) & \nabla g(x) \\
   \text{diag}(\lambda)\,\nabla g(x)^\top & \text{diag}(g(x))
   \end{bmatrix}
   \begin{bmatrix}
   \Delta x \\
   \Delta \lambda
   \end{bmatrix}
   =
   -
   \begin{bmatrix}
   \nabla f(x) + \sum_i \lambda_i \nabla g_i(x) \\
   \lambda \circ g(x) + \tfrac{1}{t}\mathbf{1}
   \end{bmatrix}.
   </script>
</p>
<p>(b) Line search to keep strict feasibility. Choose the maximum <span class="arithmatex">\(\alpha\in(0,1]\)</span> such that:</p>
<ul>
<li><span class="arithmatex">\(g_i(x + \alpha \Delta x) &lt; 0\)</span>,</li>
<li><span class="arithmatex">\(\lambda + \alpha \Delta \lambda &gt; 0\)</span>.</li>
</ul>
<p>(c) Update: <span class="arithmatex">\(x \leftarrow x + \alpha \Delta x,
   \qquad  \lambda \leftarrow \lambda + \alpha \Delta \lambda.\)</span></p>
<p>(d) Check duality gap: <span class="arithmatex">\(\text{gap} = - g(x)^\top \lambda\)</span> If <span class="arithmatex">\(\text{gap} \le \varepsilon\)</span>, stop.</p>
<p>(e) Increase barrier parameter <span class="arithmatex">\(t \leftarrow \mu t.\)</span></p>
</li>
<li>
<p>Return <span class="arithmatex">\(x\)</span>.</p>
</li>
</ol>
<h2 id="convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice">12.10 Choosing the right method in practice<a class="headerlink" href="#convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice" title="Permanent link">¶</a></h2>
<p>Case A. Smooth, unconstrained, very high dimensional.<br>
Example: logistic regression on millions of samples.<br>
Use: gradient descent or (better) accelerated gradient.<br>
Why: cheap iterations, easy to implement, scales.  </p>
<p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy.<br>
Example: convex nonlinear fitting with well-behaved Hessian.<br>
Use: Newton or quasi-Newton.<br>
Why: quadratic (or near-quadratic) convergence near optimum.  </p>
<p>Case C. Convex with simple feasible set <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex).<br>
Use: projected gradient.<br>
Why: projection is easy, maintains feasibility at each step.  </p>
<p>Case D. Composite objective <span class="arithmatex">\(f(x) + R(x)\)</span> where <span class="arithmatex">\(R\)</span> is nonsmooth (e.g. <span class="arithmatex">\(\ell_1\)</span>, indicator of a constraint set).<br>
Use: proximal gradient.<br>
Why: prox handles nonsmooth/constraint part exactly each step.  </p>
<p>Case E. General convex program with inequalities <span class="arithmatex">\(g_i(x)\le 0\)</span>.<br>
Use: interior-point methods.<br>
Why: they solve smooth barrier subproblems via Newton steps and give primal–dual certificates through KKT and duality (Chapters 7–8).  </p></body></html></section><section class="print-page" id="convex-19a_optimization_constraints" heading-number="2.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-13-optimization-algorithms-for-equality-constrained-problems">Chapter 13: Optimization Algorithms for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-chapter-13-optimization-algorithms-for-equality-constrained-problems" title="Permanent link">¶</a></h1>
<p>Equality-constrained optimization arises whenever variables must satisfy exact relationships, such as conservation laws, normalization, or linear invariants. In this chapter we focus on problems of the form</p>
<div class="arithmatex">\[
\min_x \; f(x) \quad \text{s.t.} \quad A x = b.
\]</div>
<p>where <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is (typically convex and differentiable) and <span class="arithmatex">\(A \in \mathbb{R}^{p \times n}\)</span> has rank <span class="arithmatex">\(p\)</span>. This linear equality structure appears in constrained least squares, portfolio optimization, and many ML formulations that impose exact balance or normalization constraints.</p>
<h2 id="convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold">13.1 Geometric View — Optimization on an Affine Manifold<a class="headerlink" href="#convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold" title="Permanent link">¶</a></h2>
<p>The constraint <span class="arithmatex">\(A x = b\)</span> defines an affine set</p>
<div class="arithmatex">\[
\mathcal{X} = \{ x \in \mathbb{R}^n \mid A x = b \}.
\]</div>
<p>If <span class="arithmatex">\(\operatorname{rank}(A) = p\)</span>, then <span class="arithmatex">\(\mathcal{X}\)</span> is an <span class="arithmatex">\((n-p)\)</span>-dimensional affine subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span>: a “flat” lower-dimensional plane embedded in the ambient space. Optimization now happens <em>along this plane</em>, not in all of <span class="arithmatex">\(\mathbb{R}^n\)</span>. Any feasible direction <span class="arithmatex">\(d\)</span> must keep us in <span class="arithmatex">\(\mathcal{X}\)</span>, so it must satisfy</p>
<div class="arithmatex">\[
A (x + d) = b \quad \Rightarrow \quad A d = 0.
\]</div>
<p>Thus, feasible directions lie in the null space of <span class="arithmatex">\(A\)</span>:</p>
<div class="arithmatex">\[
\mathcal{D}_{\text{feas}} = \{ d \in \mathbb{R}^n \mid A d = 0 \} = \operatorname{Null}(A).
\]</div>
<p>At an optimal point <span class="arithmatex">\(x^\star \in \mathcal{X}\)</span>, moving in any feasible direction <span class="arithmatex">\(d\)</span> cannot decrease <span class="arithmatex">\(f\)</span>. For differentiable <span class="arithmatex">\(f\)</span>, this means</p>
<div class="arithmatex">\[
\nabla f(x^\star)^\top d \ge 0 \quad \text{for all } d \text{ with } A d = 0.
\]</div>
<p>Equivalently, <span class="arithmatex">\(\nabla f(x^\star)\)</span> must be orthogonal to all feasible directions, i.e. it lies in the row space of <span class="arithmatex">\(A\)</span>. Therefore there exists a vector of Lagrange multipliers <span class="arithmatex">\(\nu^\star\)</span> such that</p>
<div class="arithmatex">\[
\nabla f(x^\star) = A^\top \nu^\star.
\]</div>
<p>This is the basic geometric optimality condition: at the optimum, the gradient of <span class="arithmatex">\(f\)</span> is a linear combination of the constraint normals (rows of <span class="arithmatex">\(A\)</span>), and every feasible direction is orthogonal to <span class="arithmatex">\(\nabla f(x^\star)\)</span>.</p>
<h2 id="convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system">13.2 Lagrange Function and KKT System<a class="headerlink" href="#convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system" title="Permanent link">¶</a></h2>
<p>The Lagrangian for the equality-constrained problem is</p>
<div class="arithmatex">\[
\mathcal{L}(x,\nu)
=
f(x) + \nu^\top (A x - b),
\]</div>
<p>where <span class="arithmatex">\(\nu \in \mathbb{R}^p\)</span> are Lagrange multipliers. The first-order (KKT) conditions for a point <span class="arithmatex">\((x^\star,\nu^\star)\)</span> to be optimal are</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_x \mathcal{L}(x^\star,\nu^\star) &amp;= \nabla f(x^\star) + A^\top \nu^\star = 0 
\quad &amp;\text{(stationarity)},\\
A x^\star &amp;= b 
\quad &amp;\text{(primal feasibility)}.
\end{aligned}
\]</div>
<p>When <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(A\)</span> has full row rank, these conditions are necessary and sufficient for global optimality. For Newton-type methods we linearize these conditions around a current iterate <span class="arithmatex">\((x,\nu)\)</span> and solve for corrections <span class="arithmatex">\((\Delta x,\Delta \nu)\)</span> from</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\ \Delta \nu
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) + A^\top \nu \\
A x - b
\end{bmatrix}.
\]</div>
<p>This linear system is called the (equality-constrained) KKT system. At the optimum the right-hand side is zero.</p>
<h2 id="convex-19a_optimization_constraints-133-quadratic-objectives">13.3 Quadratic Objectives<a class="headerlink" href="#convex-19a_optimization_constraints-133-quadratic-objectives" title="Permanent link">¶</a></h2>
<p>A particularly important case is a convex quadratic objective</p>
<div class="arithmatex">\[
f(x) = \tfrac{1}{2} x^\top P x + q^\top x + r,
\]</div>
<p>with <span class="arithmatex">\(P \succeq 0\)</span>. The equality-constrained problem</p>
<div class="arithmatex">\[
\min_x \tfrac{1}{2} x^\top P x + q^\top x + r 
\quad \text{s.t.} \quad A x = b
\]</div>
<p>has KKT conditions</p>
<div class="arithmatex">\[
\begin{bmatrix}
P &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
x^\star \\ \nu^\star
\end{bmatrix}
=
-
\begin{bmatrix}
q \\ -b
\end{bmatrix}.
\]</div>
<p>If <span class="arithmatex">\(P \succ 0\)</span> and <span class="arithmatex">\(A\)</span> has full row rank, this system has a unique solution <span class="arithmatex">\((x^\star,\nu^\star)\)</span>. This is the standard linear system solved in equality-constrained least squares and quadratic programming.</p>
<p>Examples in ML and statistics:</p>
<ul>
<li>constrained least squares with sum-to-one constraints on coefficients;  </li>
<li>portfolio optimization with <script type="math/tex"> \mathbf{1}^\top w = 1</script>;  </li>
<li>quadratic surrogate subproblems inside second-order methods.</li>
</ul>
<p>The structure of the KKT matrix (symmetric, indefinite, with blocks <span class="arithmatex">\(P\)</span>, <span class="arithmatex">\(A\)</span>) can be exploited by specialized linear solvers and factorizations.</p>
<h2 id="convex-19a_optimization_constraints-134-null-space-reduced-variable-method">13.4 Null-Space (Reduced Variable) Method<a class="headerlink" href="#convex-19a_optimization_constraints-134-null-space-reduced-variable-method" title="Permanent link">¶</a></h2>
<p>When the constraints are linear and of full row rank, a natural approach is to eliminate them explicitly.</p>
<p>Choose:</p>
<ul>
<li>a particular feasible point <span class="arithmatex">\(x_0\)</span> satisfying <span class="arithmatex">\(A x_0 = b\)</span>,  </li>
<li>a matrix <span class="arithmatex">\(Z \in \mathbb{R}^{n \times (n-p)}\)</span> whose columns form a basis of the null space of <span class="arithmatex">\(A\)</span>:
  <script type="math/tex; mode=display">
  A Z = 0.
  </script>
</li>
</ul>
<p>Then every feasible <span class="arithmatex">\(x\)</span> can be written as</p>
<div class="arithmatex">\[
x = x_0 + Z y, \quad y \in \mathbb{R}^{n-p}.
\]</div>
<p>Substituting into the objective yields an unconstrained reduced problem in the smaller variable <span class="arithmatex">\(y\)</span>:</p>
<div class="arithmatex">\[
\min_{y} \; \phi(y) := f(x_0 + Z y).
\]</div>
<p>Gradients and Hessians transform as</p>
<div class="arithmatex">\[
\nabla_y \phi(y) = Z^\top \nabla_x f(x_0 + Z y), \qquad
\nabla_y^2 \phi(y) = Z^\top \nabla_x^2 f(x_0 + Z y) \, Z.
\]</div>
<p>We can now apply any unconstrained method (gradient descent, CG, Newton) to <span class="arithmatex">\(\phi(y)\)</span>. The corresponding updates in the original space are mapped back via <span class="arithmatex">\(x = x_0 + Z y\)</span>.</p>
<p>Key points:</p>
<ul>
<li>Optimization is restricted to feasible directions <span class="arithmatex">\(\operatorname{Null}(A)\)</span> by construction.  </li>
<li>The dimension drops from <span class="arithmatex">\(n\)</span> to <span class="arithmatex">\(n-p\)</span>, which can be advantageous if <span class="arithmatex">\(p\)</span> is large.  </li>
<li>The cost is computing and storing a suitable null-space basis <span class="arithmatex">\(Z\)</span>, which may destroy sparsity and be expensive for large-scale problems.</li>
</ul>
<p>Null-space methods are attractive when:</p>
<ul>
<li>the number of constraints is moderate,  </li>
<li>a good factorization of <span class="arithmatex">\(A\)</span> is available,  </li>
<li>and we want an unconstrained algorithm in reduced coordinates.</li>
</ul>
<hr>
<h2 id="convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems">13.5 Newton’s Method for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems" title="Permanent link">¶</a></h2>
<p>For a twice-differentiable convex <span class="arithmatex">\(f\)</span>, we can derive an equality-constrained Newton step by solving a local quadratic approximation subject to linearized constraints.</p>
<p>At a point <span class="arithmatex">\(x\)</span>, approximate <span class="arithmatex">\(f(x+d)\)</span> by its second-order Taylor expansion:</p>
<div class="arithmatex">\[
f(x+d) \approx f(x)
+ \nabla f(x)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x) d.
\]</div>
<p>We seek a step <span class="arithmatex">\(d\)</span> that approximately minimizes this quadratic model while remaining feasible to first order, i.e.</p>
<div class="arithmatex">\[
\begin{aligned}
\min_d &amp; \quad \tfrac{1}{2} d^\top \nabla^2 f(x) d + \nabla f(x)^\top d\\
\text{s.t.} &amp; \quad A d = 0.
\end{aligned}
\]</div>
<p>The KKT conditions for this quadratic subproblem are</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
d \\ \lambda
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) \\ 0
\end{bmatrix}.
\]</div>
<p>Solving this system gives the Newton step <span class="arithmatex">\(d_{\text{nt}}\)</span> and a multiplier update <span class="arithmatex">\(\lambda\)</span>. The primal update is</p>
<div class="arithmatex">\[
x_{k+1} = x_k + \alpha_k d_{\text{nt}},
\]</div>
<p>with a step size <span class="arithmatex">\(\alpha_k \in (0,1]\)</span> chosen by line search to ensure sufficient decrease and preservation of feasibility (for equality constraints, <span class="arithmatex">\(A d_{\text{nt}} = 0\)</span> guarantees <span class="arithmatex">\(A x_{k+1} = b\)</span> whenever <span class="arithmatex">\(A x_k = b\)</span>).</p>
<p>Geometrically:</p>
<ul>
<li>unconstrained Newton would move by <span class="arithmatex">\(-\nabla^2 f(x)^{-1} \nabla f(x)\)</span>;  </li>
<li>equality-constrained Newton projects this step onto the tangent space <span class="arithmatex">\(\{ d : A d = 0 \}\)</span> of the affine constraint set.</li>
</ul>
<p>For strictly convex <span class="arithmatex">\(f\)</span> with positive definite Hessian on the feasible directions, this method enjoys quadratic convergence near the solution, much like the unconstrained Newton method.</p>
<h2 id="convex-19a_optimization_constraints-136-connections-to-machine-learning-and-signal-processing">13.6 Connections to Machine Learning and Signal Processing<a class="headerlink" href="#convex-19a_optimization_constraints-136-connections-to-machine-learning-and-signal-processing" title="Permanent link">¶</a></h2>
<p>Linear equality constraints appear naturally in ML and related areas:</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Equality constraint</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Portfolio optimization</td>
<td><span class="arithmatex">\(\mathbf{1}^\top w = 1\)</span></td>
<td>Weights sum to one (full investment)</td>
</tr>
<tr>
<td>Constrained regression</td>
<td><span class="arithmatex">\(C x = d\)</span></td>
<td>Enforce domain-specific linear relations between coefficients</td>
</tr>
<tr>
<td>Mixture models / convex combinations</td>
<td><span class="arithmatex">\(\mathbf{1}^\top \alpha = 1, \; \alpha \ge 0\)</span></td>
<td>Mixture weights form a probability simplex</td>
</tr>
<tr>
<td>Fairness constraints (linearized)</td>
<td><span class="arithmatex">\(A w = 0\)</span></td>
<td>Enforce equal averages across groups or balance conditions</td>
</tr>
<tr>
<td>Physics-informed models (discretized)</td>
<td><span class="arithmatex">\(A x = b\)</span></td>
<td>Discrete conservation laws (mass, charge, energy)</td>
</tr>
</tbody>
</table>
<p>More generally, nonlinear equality constraints (e.g. <span class="arithmatex">\(W^\top W = I\)</span> for orthonormal embeddings, or <span class="arithmatex">\(\|w\|_2^2 = 1\)</span> for normalized weights) lead to optimization on curved manifolds. Techniques from this chapter extend to those settings when combined with Riemannian optimization or local parameterizations, but here we focus on the linear case as the fundamental building block.</p></body></html></section><section class="print-page" id="convex-19b_optimization_constraints" heading-number="2.14"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-14-optimization-algorithms-for-inequality-constrained-problems">Chapter 14: Optimization Algorithms for Inequality-Constrained Problems<a class="headerlink" href="#convex-19b_optimization_constraints-chapter-14-optimization-algorithms-for-inequality-constrained-problems" title="Permanent link">¶</a></h1>
<p>In many applications, we must optimize an objective while respecting <em>inequality</em> constraints: nonnegativity of variables, margin constraints in SVMs, capacity or safety limits, physical bounds, fairness budgets, and more. Mathematically, the feasible region is now a convex set with a boundary, and the optimizer often lies on that boundary.</p>
<p>This chapter introduces algorithms for solving such problems, focusing on <em>logarithmic barrier</em> and <em>interior-point</em> methods. These are the workhorses behind modern general-purpose convex solvers (for LP, QP, SOCP, SDP) and provide a smooth way to enforce inequalities while still using Newton-type methods.</p>
<h2 id="convex-19b_optimization_constraints-141-problem-setup">14.1 Problem Setup<a class="headerlink" href="#convex-19b_optimization_constraints-141-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider the general convex problem with inequality and equality constraints
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad & f_0(x) \\
\text{subject to} \quad & f_i(x) \le 0,\quad i = 1,\dots,m,\\
& A x = b,
\end{aligned}
</script>
where</p>
<ul>
<li><span class="arithmatex">\(f_0, f_1,\dots,f_m\)</span> are convex, typically twice differentiable,</li>
<li><span class="arithmatex">\(A \in \mathbb{R}^{p \times n}\)</span> has full row rank,</li>
<li>there exists a strictly feasible point <span class="arithmatex">\(\bar{x}\)</span> such that
  <span class="arithmatex">\(f_i(\bar{x}) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(A\bar{x} = b\)</span> (Slater’s condition).</li>
</ul>
<p>Under these assumptions:</p>
<ul>
<li>the problem is convex,</li>
<li>strong duality holds (zero duality gap),</li>
<li>and the KKT conditions characterize optimality.</li>
</ul>
<h3 id="convex-19b_optimization_constraints-examples">Examples<a class="headerlink" href="#convex-19b_optimization_constraints-examples" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Problem type</th>
<th><span class="arithmatex">\(f_0(x)\)</span></th>
<th>Constraints <span class="arithmatex">\(f_i(x)\le0\)</span></th>
<th>ML / applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear program (LP)</td>
<td><span class="arithmatex">\(c^\top x\)</span></td>
<td><span class="arithmatex">\(a_i^\top x - b_i \le 0\)</span></td>
<td>resource allocation, feature selection</td>
</tr>
<tr>
<td>Quadratic program</td>
<td><span class="arithmatex">\(\tfrac12 x^\top P x + q^\top x\)</span></td>
<td>linear</td>
<td>SVMs, ridge with box constraints</td>
</tr>
<tr>
<td>QCQP</td>
<td>quadratic</td>
<td>quadratic</td>
<td>portfolio optimization, control</td>
</tr>
<tr>
<td>Entropy models</td>
<td><span class="arithmatex">\(\sum_i x_i \log x_i\)</span></td>
<td><span class="arithmatex">\(F x - g \le 0\)</span></td>
<td>probability calibration, max-entropy</td>
</tr>
<tr>
<td>Nonnegativity</td>
<td>arbitrary convex</td>
<td><span class="arithmatex">\(-x_i \le 0\)</span></td>
<td>sparse coding, nonnegative factorization</td>
</tr>
</tbody>
</table>
<p>Many machine-learning training problems can be written in this template by expressing regularization, margins, fairness, or safety conditions as convex inequalities.</p>
<h2 id="convex-19b_optimization_constraints-142-indicator-function-view-of-constraints">14.2 Indicator-Function View of Constraints<a class="headerlink" href="#convex-19b_optimization_constraints-142-indicator-function-view-of-constraints" title="Permanent link">¶</a></h2>
<p>Conceptually, we can write inequality constraints using an indicator function. Define
<script type="math/tex; mode=display">
I_{-}(u) =
\begin{cases}
0, & u \le 0,\\[4pt]
+\infty, & u > 0.
\end{cases}
</script>
</p>
<p>Then the inequality-constrained problem is equivalent to
<script type="math/tex; mode=display">
\min_x \; f_0(x)
      + \sum_{i=1}^m I_{-}\big(f_i(x)\big)
\quad \text{subject to } A x = b.
</script>
</p>
<ul>
<li>If <span class="arithmatex">\(x\)</span> is feasible (<span class="arithmatex">\(f_i(x) \le 0\)</span> for all <span class="arithmatex">\(i\)</span>), the indicators contribute <span class="arithmatex">\(0\)</span>.</li>
<li>If any constraint is violated (<span class="arithmatex">\(f_i(x) &gt; 0\)</span>), the objective becomes <span class="arithmatex">\(+\infty\)</span>.</li>
</ul>
<p>This formulation is clean but not numerically friendly:</p>
<ul>
<li><span class="arithmatex">\(I_{-}\)</span> is discontinuous and nonsmooth.</li>
<li>We cannot directly apply Newton-type methods.</li>
</ul>
<p>The key idea of barrier methods is to <em>replace</em> the hard indicator with a smooth approximation that grows to <span class="arithmatex">\(+\infty\)</span> as we approach the boundary.</p>
<h2 id="convex-19b_optimization_constraints-143-logarithmic-barrier-approximation">14.3 Logarithmic Barrier Approximation<a class="headerlink" href="#convex-19b_optimization_constraints-143-logarithmic-barrier-approximation" title="Permanent link">¶</a></h2>
<p>We approximate the indicator <span class="arithmatex">\(I_{-}\)</span> with a smooth barrier function
<script type="math/tex; mode=display">
\Phi(u) = -\log(-u), \quad u < 0.
</script>
</p>
<p>For each inequality <span class="arithmatex">\(f_i(x) \le 0\)</span>, we introduce a barrier term <span class="arithmatex">\(-\log(-f_i(x))\)</span>. For a given parameter <span class="arithmatex">\(t &gt; 0\)</span>, we solve the <em>barrier subproblem</em>
<script type="math/tex; mode=display">
\min_x \; f_0(x) + \frac{1}{t} \,\phi(x)
\quad \text{subject to } A x = b,
</script>
where
<script type="math/tex; mode=display">
\phi(x) = \sum_{i=1}^m -\log(-f_i(x)), \qquad
\mathrm{dom}\,\phi = \{x : f_i(x) < 0 \ \forall i\}.
</script>
</p>
<p>Equivalently,
<script type="math/tex; mode=display">
\min_x \; t f_0(x) + \phi(x)
\quad \text{subject to } A x = b.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The barrier term <span class="arithmatex">\(\phi(x)\)</span> is finite only for strictly feasible points (<span class="arithmatex">\(f_i(x) &lt; 0\)</span>).</li>
<li>As <span class="arithmatex">\(x\)</span> approaches the boundary <span class="arithmatex">\(f_i(x) \to 0^-\)</span>, the term <span class="arithmatex">\(-\log(-f_i(x)) \to +\infty\)</span>.</li>
<li>The parameter <span class="arithmatex">\(t\)</span> controls the trade-off:</li>
<li>small <span class="arithmatex">\(t\)</span> (large <span class="arithmatex">\(1/t\)</span>) → strong barrier, solution stays deep inside the feasible set;</li>
<li>large <span class="arithmatex">\(t\)</span> → barrier is weaker, solutions can move closer to the boundary.</li>
</ul>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, solutions of the barrier subproblem approach the solution of the original constrained problem.</p>
<h2 id="convex-19b_optimization_constraints-144-derivatives-of-the-barrier">14.4 Derivatives of the Barrier<a class="headerlink" href="#convex-19b_optimization_constraints-144-derivatives-of-the-barrier" title="Permanent link">¶</a></h2>
<p>Let
<script type="math/tex; mode=display">
\phi(x) = -\sum_{i=1}^m \log(-f_i(x)).
</script>
Then <span class="arithmatex">\(\phi\)</span> is convex and twice differentiable on its domain. Its gradient and Hessian are
<script type="math/tex; mode=display">
\nabla \phi(x) = \sum_{i=1}^m \frac{1}{-f_i(x)} \, \nabla f_i(x),
</script>
<script type="math/tex; mode=display">
\nabla^2 \phi(x)
=
\sum_{i=1}^m \frac{1}{[f_i(x)]^2} \, \nabla f_i(x)\nabla f_i(x)^\top
+ \sum_{i=1}^m \frac{1}{-f_i(x)} \, \nabla^2 f_i(x).
</script>
</p>
<p>Key features:</p>
<ul>
<li>As <span class="arithmatex">\(f_i(x) \uparrow 0\)</span> (approaching the boundary from inside), the factor <span class="arithmatex">\(1/(-f_i(x))\)</span> blows up, so <span class="arithmatex">\(\|\nabla \phi(x)\|\)</span> becomes very large.</li>
<li>This creates a strong <em>repulsive force</em> that prevents iterates from crossing the boundary.</li>
<li>The barrier “pushes” the solution away from constraint violation, while the original objective <span class="arithmatex">\(f_0(x)\)</span> pulls toward lower cost.</li>
</ul>
<p>The barrier subproblem
<script type="math/tex; mode=display">
\min_x \; t f_0(x) + \phi(x)
\quad \text{s.t. } A x = b
</script>
is a <em>smooth equality-constrained</em> problem. We can therefore apply equality-constrained Newton methods (Chapter 13) at each fixed <span class="arithmatex">\(t\)</span>.</p>
<h2 id="convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions">14.5 Central Path and Approximate KKT Conditions<a class="headerlink" href="#convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions" title="Permanent link">¶</a></h2>
<p>For each <span class="arithmatex">\(t &gt; 0\)</span>, let <span class="arithmatex">\(x^\star(t)\)</span> be a minimizer of the barrier problem
<script type="math/tex; mode=display">
\min_x \; t f_0(x) + \phi(x)
\quad \text{s.t. } A x = b.
</script>
</p>
<p>The set <span class="arithmatex">\(\{x^\star(t) : t &gt; 0\}\)</span> is called the <em>central path</em>. As <span class="arithmatex">\(t \to \infty\)</span>, <span class="arithmatex">\(x^\star(t)\)</span> converges to a solution <span class="arithmatex">\(x^\star\)</span> of the original inequality-constrained problem.</p>
<p>We can associate approximate dual variables to <span class="arithmatex">\(x^\star(t)\)</span>:
<script type="math/tex; mode=display">
\lambda_i^\star(t) = \frac{1}{t\,(-f_i(x^\star(t)))}, \quad i=1,\dots,m.
</script>
</p>
<p>Then the KKT-like relations hold:
<script type="math/tex; mode=display">
\begin{aligned}
\nabla f_0(x^\star(t)) + \sum_{i=1}^m \lambda_i^\star(t)\,\nabla f_i(x^\star(t)) + A^\top v^\star(t) &= 0,\\[4pt]
A x^\star(t) &= b,\\[4pt]
\lambda_i^\star(t) &\ge 0,\\[4pt]
-\lambda_i^\star(t) f_i(x^\star(t)) &= \frac{1}{t}, \quad i = 1,\dots,m.
\end{aligned}
</script>
</p>
<p>Compare with the exact KKT conditions (for optimal <span class="arithmatex">\((x^\star,\lambda^\star,v^\star)\)</span>):
<script type="math/tex; mode=display">
\lambda_i^\star f_i(x^\star) = 0.
</script>
</p>
<p>Along the central path we have the <em>relaxed</em> complementarity condition
<script type="math/tex; mode=display">
\lambda_i^\star(t) \,f_i\big(x^\star(t)\big) = -\frac{1}{t},
</script>
which tends to <span class="arithmatex">\(0\)</span> as <span class="arithmatex">\(t \to \infty\)</span>. Hence the barrier formulation naturally yields approximate primal–dual solutions whose KKT residuals shrink as we increase <span class="arithmatex">\(t\)</span>.</p>
<h2 id="convex-19b_optimization_constraints-146-geometric-and-physical-intuition">14.6 Geometric and Physical Intuition<a class="headerlink" href="#convex-19b_optimization_constraints-146-geometric-and-physical-intuition" title="Permanent link">¶</a></h2>
<p>Consider the barrier-augmented objective
<script type="math/tex; mode=display">
\Psi_t(x) = t f_0(x) + \phi(x)
= t f_0(x) - \sum_{i=1}^m \log(-f_i(x)).
</script>
</p>
<p>We can interpret this as:</p>
<ul>
<li><span class="arithmatex">\(t f_0(x)\)</span>: an “external potential” pulling us toward low objective values.</li>
<li><span class="arithmatex">\(-\log(-f_i(x))\)</span>: repulsive potentials that become infinite near the boundary <span class="arithmatex">\(f_i(x)=0\)</span>.</li>
</ul>
<p>At a minimizer <span class="arithmatex">\(x^\star(t)\)</span>, we have
<script type="math/tex; mode=display">
\nabla \Psi_t(x^\star(t))
= t \nabla f_0(x^\star(t)) + \sum_{i=1}^m \frac{1}{-f_i(x^\star(t))}\,\nabla f_i(x^\star(t)) = 0
\quad (\text{up to components orthogonal to } \operatorname{Null}(A)).
</script>
</p>
<p>The gradient of the objective is exactly balanced by a weighted sum of constraint gradients. This is a <em>force-balance condition</em>:</p>
<ul>
<li>constraints “push back” more strongly when <span class="arithmatex">\(x\)</span> is close to their boundary,</li>
<li>the interior-point iterates follow a smooth path that stays strictly feasible
  and moves gradually toward the optimal boundary point.</li>
</ul>
<p>This picture explains both:</p>
<ul>
<li>why iterates never leave the feasible region, and  </li>
<li>why the method naturally generates dual variables (the weights on constraint gradients).</li>
</ul>
<h2 id="convex-19b_optimization_constraints-147-the-barrier-method">14.7 The Barrier Method<a class="headerlink" href="#convex-19b_optimization_constraints-147-the-barrier-method" title="Permanent link">¶</a></h2>
<p>The barrier method solves the original inequality-constrained problem by solving a sequence of barrier subproblems with increasing <span class="arithmatex">\(t\)</span>.</p>
<h3 id="convex-19b_optimization_constraints-algorithm-barrier-method-conceptual-form">Algorithm: Barrier Method (Conceptual Form)<a class="headerlink" href="#convex-19b_optimization_constraints-algorithm-barrier-method-conceptual-form" title="Permanent link">¶</a></h3>
<p>Given:</p>
<ul>
<li>a strictly feasible starting point <span class="arithmatex">\(x\)</span> (<span class="arithmatex">\(f_i(x) &lt; 0\)</span>, <span class="arithmatex">\(A x = b\)</span>),</li>
<li>initial barrier parameter <span class="arithmatex">\(t &gt; 0\)</span>,</li>
<li>barrier growth factor <span class="arithmatex">\(\mu &gt; 1\)</span> (e.g. <span class="arithmatex">\(\mu \in [10,20]\)</span>),</li>
<li>accuracy tolerance <span class="arithmatex">\(\varepsilon &gt; 0\)</span>,</li>
</ul>
<p>repeat:</p>
<ol>
<li>
<p>Centering step<br>
   Solve the equality-constrained problem
   <script type="math/tex; mode=display">
   \min_x \; t f_0(x) - \sum_{i=1}^m \log(-f_i(x))
   \quad \text{s.t. } A x = b
   </script>
   using an equality-constrained Newton method.<br>
   (In practice, we start from the previous solution and take a small number of Newton steps rather than “solve exactly”.)</p>
</li>
<li>
<p>Update iterate<br>
   Let <span class="arithmatex">\(x\)</span> be the resulting point (the approximate minimizer for current <span class="arithmatex">\(t\)</span>).</p>
</li>
<li>
<p>Check stopping criterion<br>
   For the barrier problem, one can show
   <script type="math/tex; mode=display">
   f_0(x) - p^\star \le \frac{m}{t},
   </script>
   where <span class="arithmatex">\(p^\star\)</span> is the optimal value of the original problem.<br>
   If
   <script type="math/tex; mode=display">
   \frac{m}{t} < \varepsilon,
   </script>
   then stop: <span class="arithmatex">\(x\)</span> is guaranteed to be within <span class="arithmatex">\(\varepsilon\)</span> (in objective value) of optimal.</p>
</li>
<li>
<p>Increase <span class="arithmatex">\(t\)</span><br>
   Set <span class="arithmatex">\(t := \mu t\)</span> to weaken the barrier and move closer to the true boundary, then go back to Step 1.</p>
</li>
</ol>
<p>Key parameters:</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(t\)</span></td>
<td>barrier strength (larger <span class="arithmatex">\(t\)</span> = weaker barrier, closer to solution)</td>
</tr>
<tr>
<td><span class="arithmatex">\(\mu\)</span></td>
<td>growth factor for <span class="arithmatex">\(t\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(\varepsilon\)</span></td>
<td>desired accuracy (duality-gap based)</td>
</tr>
<tr>
<td><span class="arithmatex">\(m\)</span></td>
<td>number of inequality constraints</td>
</tr>
</tbody>
</table>
<p>In practice:</p>
<ul>
<li><span class="arithmatex">\(\varepsilon\)</span> is often in the range <span class="arithmatex">\(10^{-3}\)</span>–<span class="arithmatex">\(10^{-8}\)</span>,</li>
<li><span class="arithmatex">\(\mu\)</span> is chosen to balance outer iterations vs inner Newton steps,</li>
<li>the centering step is usually solved to modest accuracy, not exactness.</li>
</ul>
<hr>
<h2 id="convex-19b_optimization_constraints-148-from-barrier-methods-to-interior-point-methods">14.8 From Barrier Methods to Interior-Point Methods<a class="headerlink" href="#convex-19b_optimization_constraints-148-from-barrier-methods-to-interior-point-methods" title="Permanent link">¶</a></h2>
<p>Pure barrier methods conceptually “solve a sequence of problems for increasing <span class="arithmatex">\(t\)</span>”. Modern <em>interior-point methods</em> refine this idea:</p>
<ul>
<li>they update <em>both</em> primal variables <span class="arithmatex">\(x\)</span> and dual variables <span class="arithmatex">\((\lambda, v)\)</span>,</li>
<li>they use Newton’s method on the (perturbed) KKT system,</li>
<li>they follow the central path by simultaneously enforcing:</li>
<li>primal feasibility (<span class="arithmatex">\(f_i(x) \le 0\)</span>, <span class="arithmatex">\(A x = b\)</span>),</li>
<li>dual feasibility (<span class="arithmatex">\(\lambda_i \ge 0\)</span>),</li>
<li>relaxed complementarity (<span class="arithmatex">\(-\lambda_i f_i(x) \approx 1/t\)</span>).</li>
</ul>
<p>A typical <em>primal–dual</em> step solves a linearized KKT system of the form
<script type="math/tex; mode=display">
\begin{aligned}
\nabla f_0(x) + \sum_i \lambda_i \nabla f_i(x) + A^\top v &= 0,\\
f_i(x) &\le 0,\quad \lambda_i \ge 0,\\
-\lambda_i f_i(x) &\approx \frac{1}{t},\\
A x &= b.
\end{aligned}
</script>
</p>
<p>Newton’s method applied to these equations yields search directions for <span class="arithmatex">\((x,\lambda,v)\)</span> that move toward the central path and reduce primal and dual residuals simultaneously. This is what modern LP/QP/SOCP/SDP solvers implement.</p>
<p>You do not need to implement these methods from scratch to use them: in practice, you describe your problem in a modeling language (e.g. CVX, CVXPY, JuMP) and rely on an interior-point solver under the hood.</p>
<hr>
<h2 id="convex-19b_optimization_constraints-149-computational-and-practical-notes">14.9 Computational and Practical Notes<a class="headerlink" href="#convex-19b_optimization_constraints-149-computational-and-practical-notes" title="Permanent link">¶</a></h2>
<p>Some important practical aspects:</p>
<ol>
<li>
<p>Equality-constrained Newton inside<br>
   Each barrier subproblem is solved by equality-constrained Newton (Chapter 13). The main cost is solving the KKT linear system at each Newton step.</p>
</li>
<li>
<p>Strict feasibility<br>
   Barrier and interior-point methods require a strictly feasible starting point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(f_i(x) &lt; 0\)</span>.  </p>
</li>
<li>Sometimes this is easy (e.g. nonnegativity constraints with a positive initial vector).  </li>
<li>
<p>Otherwise, a separate <em>phase I</em> problem is solved to find such a point or to certify infeasibility.</p>
</li>
<li>
<p>Step size control<br>
   Because the barrier blows up near the boundary, too aggressive Newton steps may try to leave the feasible region. A backtracking line search is used to ensure:</p>
</li>
<li>sufficient decrease in the barrier objective,</li>
<li>
<p>and preservation of strict feasibility (<span class="arithmatex">\(f_i(x) &lt; 0\)</span> remains true).</p>
</li>
<li>
<p>Accuracy vs cost<br>
   The duality-gap bound <span class="arithmatex">\(m/t\)</span> provides a clear trade-off:</p>
</li>
<li>small <span class="arithmatex">\(m/t\)</span> (large <span class="arithmatex">\(t\)</span>) → high accuracy, more iterations,</li>
<li>
<p>larger <span class="arithmatex">\(m/t\)</span> → faster but less precise.</p>
</li>
<li>
<p>Sparsity and structure<br>
   For large problems, exploiting sparsity in <span class="arithmatex">\(A\)</span> and in the Hessians <span class="arithmatex">\(\nabla^2 f_i(x)\)</span> is crucial. Interior-point methods scale well when linear algebra is carefully optimized.</p>
</li>
</ol>
<hr>
<h2 id="convex-19b_optimization_constraints-1410-equality-vs-inequality-constrained-algorithms">14.10 Equality vs Inequality-Constrained Algorithms<a class="headerlink" href="#convex-19b_optimization_constraints-1410-equality-vs-inequality-constrained-algorithms" title="Permanent link">¶</a></h2>
<p>Finally, it is helpful to contrast the equality-only case (Chapter 13) with the inequality case.</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Equality constraints <span class="arithmatex">\(A x = b\)</span></th>
<th>Inequality constraints <span class="arithmatex">\(f_i(x) \le 0\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasible set</td>
<td>Affine subspace</td>
<td>General convex region with boundary</td>
</tr>
<tr>
<td>Typical algorithms</td>
<td>Lagrange/KKT, equality-constrained Newton, null-space</td>
<td>Barrier methods, primal–dual interior-point methods</td>
</tr>
<tr>
<td>Feasibility during iteration</td>
<td>Can start infeasible and converge to <span class="arithmatex">\(A x = b\)</span></td>
<td>Iterates kept strictly feasible (<span class="arithmatex">\(f_i(x) &lt; 0\)</span>)</td>
</tr>
<tr>
<td>Complementarity</td>
<td>Not present (only equalities)</td>
<td><span class="arithmatex">\(\lambda_i f_i(x) = 0\)</span> at optimum, or <span class="arithmatex">\(\approx -1/t\)</span> along central path</td>
</tr>
<tr>
<td>Geometric picture</td>
<td>Optimization on a flat manifold</td>
<td>Optimization in a convex region, repelled from boundary</td>
</tr>
<tr>
<td>ML relevance</td>
<td>Normalization, linear invariants, balance constraints</td>
<td>Nonnegativity, margin constraints, safety/fairness limits</td>
</tr>
</tbody>
</table>
<p>In summary:</p>
<ul>
<li>Equality-constrained methods operate directly on an affine manifold using KKT and Newton.  </li>
<li>Inequality-constrained methods use smooth barriers (or primal–dual perturbed KKT systems) to stay in the interior and gradually approach the boundary and the optimal point.</li>
</ul>
<p>Interior-point methods unify these perspectives and are the backbone of modern convex optimization software.</p></body></html></section><section class="print-page" id="convex-20_advanced" heading-number="2.15"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-15-advanced-large-scale-and-structured-methods">Chapter 15: Advanced Large-Scale and Structured Methods<a class="headerlink" href="#convex-20_advanced-chapter-15-advanced-large-scale-and-structured-methods" title="Permanent link">¶</a></h1>
<p>Modern convex optimization often runs at massive scale: millions (or billions) of variables, datasets too large to fit in memory, and constraints spread across machines or devices. Classical Newton or interior-point methods are beautiful mathematically, but their per-iteration cost and memory usage often make them impractical for these regimes.</p>
<p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to make convex optimization scalable. These ideas underpin the optimization engines behind most modern machine learning systems.</p>
<h2 id="convex-20_advanced-151-motivation-structure-and-scale">15.1 Motivation: Structure and Scale<a class="headerlink" href="#convex-20_advanced-151-motivation-structure-and-scale" title="Permanent link">¶</a></h2>
<p>In large-scale convex optimization, the challenge is not “does a solution exist?” but rather “can we compute it in time and memory?”.</p>
<p>Bottlenecks include:</p>
<ul>
<li>Memory: storing Hessians (or even full gradients) may be impossible.</li>
<li>Data size: one full pass over all samples can already be expensive.</li>
<li>Distributed data: samples are spread across devices / workers.</li>
<li>Sparsity and separability: the problem often decomposes into many small pieces.</li>
</ul>
<p>A common template is the empirical risk + regularizer form
<script type="math/tex; mode=display">
f(x)
= \frac{1}{N}\sum_{i=1}^N f_i(x) + R(x),
</script>
where</p>
<ul>
<li>each <span class="arithmatex">\(f_i(x)\)</span> is a loss term for sample <span class="arithmatex">\(i\)</span>,</li>
<li><span class="arithmatex">\(R(x)\)</span> is a regularizer (possibly nonsmooth, e.g. <span class="arithmatex">\(\lambda\|x\|_1\)</span>).</li>
</ul>
<p>The methods in this chapter are designed to exploit this structure:</p>
<ul>
<li>update only <em>parts</em> of <span class="arithmatex">\(x\)</span> at a time (coordinate/block methods),</li>
<li>use only <em>some</em> data per step (stochastic methods),</li>
<li>split the problem into simpler subproblems (proximal / ADMM),</li>
<li>or distribute computation across multiple machines (consensus methods).</li>
</ul>
<h2 id="convex-20_advanced-152-coordinate-descent">15.2 Coordinate Descent<a class="headerlink" href="#convex-20_advanced-152-coordinate-descent" title="Permanent link">¶</a></h2>
<p>Coordinate descent updates one coordinate (or a small block of coordinates) at a time, holding all others fixed. It is especially effective when updates along a single coordinate are cheap to compute. Given <span class="arithmatex">\(x^{(k)}\)</span>, choose coordinate <span class="arithmatex">\(i_k\)</span> and define</p>
<div class="arithmatex">\[
x^{(k+1)}_i =
\begin{cases}
\displaystyle
\arg\min_{z \in \mathbb{R}}
\; f\big(x_1^{(k+1)},\dots,x_{i_k-1}^{(k+1)},z,x_{i_k+1}^{(k)},\dots,x_n^{(k)}\big),
&amp; i = i_k,\\[4pt]
x_i^{(k)}, &amp; i \ne i_k.
\end{cases}
\]</div>
<p>In practice:</p>
<ul>
<li><span class="arithmatex">\(i_k\)</span> is chosen either cyclically (<span class="arithmatex">\(1,2,\dots,n,1,2,\dots\)</span>) or randomly.</li>
<li>Each coordinate update often has a closed form (e.g. soft-thresholding for LASSO).</li>
<li>You never form or store the full gradient; you only need partial derivatives.</li>
</ul>
<p>Why it scales:</p>
<ul>
<li>Each step is <em>very</em> cheap — often <span class="arithmatex">\(O(1)\)</span> or proportional to the number of nonzeros in the column corresponding to coordinate <span class="arithmatex">\(i_k\)</span>.</li>
<li>In high dimensions (e.g., millions of features), this can be far more efficient than updating all coordinates at once.</li>
</ul>
<p>Convergence:<br>
If <span class="arithmatex">\(f\)</span> is convex and has Lipschitz-continuous partial derivatives, coordinate descent (cyclic or randomized) converges to the global minimizer. Randomized coordinate descent often has clean expected convergence rates.</p>
<p>ML context:</p>
<ul>
<li>LASSO / Elastic Net regression (coordinate updates are soft-thresholding),</li>
<li><span class="arithmatex">\(\ell_1\)</span>-penalized logistic regression,</li>
<li>matrix factorization and dictionary learning (updating one factor vector at a time),</li>
<li>problems where <span class="arithmatex">\(R(x)\)</span> is separable: <span class="arithmatex">\(R(x) = \sum_i R_i(x_i)\)</span>.</li>
</ul>
<h2 id="convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods">15.3 Stochastic Gradient and Variance-Reduced Methods<a class="headerlink" href="#convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods" title="Permanent link">¶</a></h2>
<p>When <span class="arithmatex">\(N\)</span> (number of samples) is huge, computing the full gradient
<script type="math/tex; mode=display">
\nabla f(x) = \frac{1}{N}\sum_{i=1}^N \nabla f_i(x)
</script>
every iteration is too expensive. Stochastic methods replace this full gradient with cheap, unbiased <em>estimates</em> based on small random subsets (mini-batches) of data.</p>
<h3 id="convex-20_advanced-1531-stochastic-gradient-descent-sgd">15.3.1 Stochastic Gradient Descent (SGD)<a class="headerlink" href="#convex-20_advanced-1531-stochastic-gradient-descent-sgd" title="Permanent link">¶</a></h3>
<p>At iteration <span class="arithmatex">\(k\)</span>:</p>
<ol>
<li>Sample a mini-batch <span class="arithmatex">\(\mathcal{B}_k \subset \{1,\dots,N\}\)</span>.</li>
<li>Form the stochastic gradient
   <script type="math/tex; mode=display">
   \widehat{\nabla f}(x_k)
   =
   \frac{1}{|\mathcal{B}_k|}
   \sum_{i \in \mathcal{B}_k} \nabla f_i(x_k).
   </script>
</li>
<li>Update
   <script type="math/tex; mode=display">
   x_{k+1} = x_k - \eta_k \,\widehat{\nabla f}(x_k),
   </script>
   where <span class="arithmatex">\(\eta_k &gt; 0\)</span> is the learning rate.</li>
</ol>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(\mathbb{E}[\widehat{\nabla f}(x_k) \mid x_k] = \nabla f(x_k)\)</span> (unbiased),</li>
<li>Each iteration is cheap (depends only on <span class="arithmatex">\(|\mathcal{B}_k|\)</span>, not <span class="arithmatex">\(N\)</span>),</li>
<li>The noise can help escape shallow nonconvex traps (in deep learning).</li>
</ul>
<p>In convex settings, SGD trades off per-iteration cost against convergence speed: many cheap noisy steps instead of fewer expensive precise ones.</p>
<h3 id="convex-20_advanced-1532-step-sizes-and-averaging">15.3.2 Step Sizes and Averaging<a class="headerlink" href="#convex-20_advanced-1532-step-sizes-and-averaging" title="Permanent link">¶</a></h3>
<p>The step size <span class="arithmatex">\(\eta_k\)</span> is crucial:</p>
<ul>
<li>Too large → iterates diverge or oscillate.</li>
<li>Too small → extremely slow progress.</li>
</ul>
<p>Typical schedules for convex problems:</p>
<ul>
<li>General convex: <span class="arithmatex">\(\eta_k = \frac{c}{\sqrt{k}}\)</span>,</li>
<li>Strongly convex: <span class="arithmatex">\(\eta_k = \frac{c}{k}\)</span>.</li>
</ul>
<p>Two important stabilization techniques:</p>
<ol>
<li>Decay the learning rate over time.</li>
<li>Polyak–Ruppert averaging: return the average
   <script type="math/tex; mode=display">
   \bar{x}_k = \frac{1}{k}\sum_{t=1}^k x_t
   </script>
   instead of the last iterate. Averaging cancels noise and leads to optimal <span class="arithmatex">\(O(1/k)\)</span> rates in strongly convex settings.</li>
</ol>
<p>Mini-batch size can also grow with <span class="arithmatex">\(k\)</span>, gradually reducing variance while keeping early iterations cheap.</p>
<h3 id="convex-20_advanced-1533-convergence-rates">15.3.3 Convergence Rates<a class="headerlink" href="#convex-20_advanced-1533-convergence-rates" title="Permanent link">¶</a></h3>
<p>For convex <span class="arithmatex">\(f\)</span>:</p>
<ul>
<li>With appropriate diminishing <span class="arithmatex">\(\eta_k\)</span>,<br>
<span class="arithmatex">\(\mathbb{E}[f(x_k)] - f^\star = O(k^{-1/2})\)</span>.</li>
</ul>
<p>For strongly convex <span class="arithmatex">\(f\)</span>:</p>
<ul>
<li>With <span class="arithmatex">\(\eta_k = O(1/k)\)</span> and averaging,<br>
<span class="arithmatex">\(\mathbb{E}[\|x_k - x^\star\|^2] = O(1/k)\)</span>.</li>
</ul>
<p>These rates are optimal for unbiased first-order stochastic methods.</p>
<h3 id="convex-20_advanced-1534-variance-reduced-methods-svrg-saga-sarah">15.3.4 Variance-Reduced Methods (SVRG, SAGA, SARAH)<a class="headerlink" href="#convex-20_advanced-1534-variance-reduced-methods-svrg-saga-sarah" title="Permanent link">¶</a></h3>
<p>Plain SGD cannot easily reach very high accuracy because the gradient noise never fully disappears. Variance-reduced methods reduce this noise, especially near the solution, by periodically using the full gradient.</p>
<p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p>
<ul>
<li>Pick a reference point <span class="arithmatex">\(\tilde{x}\)</span> and compute <span class="arithmatex">\(\nabla f(\tilde{x})\)</span>.</li>
<li>For inner iterations:
  <script type="math/tex; mode=display">
  v_k = \nabla f_{i_k}(x_k) - \nabla f_{i_k}(\tilde{x}) + \nabla f(\tilde{x}),
  \quad
  x_{k+1} = x_k - \eta v_k,
  </script>
  where <span class="arithmatex">\(i_k\)</span> is a random sample index.</li>
</ul>
<p>Here <span class="arithmatex">\(v_k\)</span> is still an unbiased estimator of <span class="arithmatex">\(\nabla f(x_k)\)</span>, but its variance decays as <span class="arithmatex">\(x_k\)</span> approaches <span class="arithmatex">\(\tilde{x}\)</span>. For strongly convex <span class="arithmatex">\(f\)</span>, methods like SVRG and SAGA achieve linear convergence, comparable to full gradient descent but at near-SGD cost.</p>
<h3 id="convex-20_advanced-1535-momentum-and-adaptive-methods">15.3.5 Momentum and Adaptive Methods<a class="headerlink" href="#convex-20_advanced-1535-momentum-and-adaptive-methods" title="Permanent link">¶</a></h3>
<p>In practice, large-scale learning often uses SGD with various modifications:</p>
<ul>
<li>
<p>Momentum / Nesterov: keep a moving average of gradients
  <script type="math/tex; mode=display">
  m_k = \beta m_{k-1} + (1-\beta)\widehat{\nabla f}(x_k),
  \quad
  x_{k+1} = x_k - \eta m_k,
  </script>
  which accelerates progress along consistent directions and damps oscillations.</p>
</li>
<li>
<p>Adaptive methods (Adagrad, RMSProp, Adam): maintain coordinate-wise scales based on past squared gradients, effectively using a diagonal preconditioner that adapts to curvature and feature scales.</p>
</li>
</ul>
<p>These methods are especially popular in deep learning. For convex problems, their theoretical behavior is subtle, but empirically they often converge faster in wall-clock time.</p>
<h2 id="convex-20_advanced-154-proximal-and-composite-optimization">15.4 Proximal and Composite Optimization<a class="headerlink" href="#convex-20_advanced-154-proximal-and-composite-optimization" title="Permanent link">¶</a></h2>
<p>Many large-scale convex problems are composite:
<script type="math/tex; mode=display">
\min_x \; F(x) := g(x) + R(x),
</script>
where</p>
<ul>
<li><span class="arithmatex">\(g\)</span> is smooth convex with Lipschitz gradient (e.g. data-fitting term),</li>
<li><span class="arithmatex">\(R\)</span> is convex but possibly nonsmooth (e.g. <span class="arithmatex">\(\lambda\|x\|_1\)</span>, indicator of a constraint set, nuclear norm).</li>
</ul>
<p>The proximal gradient method (a.k.a. ISTA) updates as
<script type="math/tex; mode=display">
x_{k+1} = \operatorname{prox}_{\alpha R}\big(x_k - \alpha \nabla g(x_k)\big),
</script>
where the proximal operator is
<script type="math/tex; mode=display">
\operatorname{prox}_{\alpha R}(v)
=
\arg\min_x \left(
R(x) + \frac{1}{2\alpha}\|x-v\|_2^2
\right).
</script>
</p>
<p>Intuition:</p>
<ul>
<li>The gradient step moves <span class="arithmatex">\(x\)</span> in a direction that lowers the smooth term <span class="arithmatex">\(g\)</span>.</li>
<li>The prox step solves a small “regularized” problem, pulling <span class="arithmatex">\(x\)</span> toward a structure favored by <span class="arithmatex">\(R\)</span> (sparsity, low rank, feasibility, etc.).</li>
</ul>
<p>Examples of prox operators:</p>
<ul>
<li><span class="arithmatex">\(R(x) = \lambda\|x\|_1\)</span> → soft-thresholding (coordinate-wise shrinkage).</li>
<li><span class="arithmatex">\(R\)</span> = indicator of a convex set <span class="arithmatex">\(\mathcal{X}\)</span> → projection onto <span class="arithmatex">\(\mathcal{X}\)</span> (so projected gradient is a special case).</li>
<li><span class="arithmatex">\(R(X) = \|X\|_*\)</span> (nuclear norm) → singular value soft-thresholding.</li>
</ul>
<p>For large-scale problems:</p>
<ul>
<li>Proximal gradient scales like gradient descent: each iteration uses only <span class="arithmatex">\(\nabla g\)</span> and a prox (often cheap and parallelizable).</li>
<li>Accelerated variants (FISTA) achieve <span class="arithmatex">\(O(1/k^2)\)</span> rates for smooth <span class="arithmatex">\(g\)</span>.</li>
</ul>
<h2 id="convex-20_advanced-155-alternating-direction-method-of-multipliers-admm">15.5 Alternating Direction Method of Multipliers (ADMM)<a class="headerlink" href="#convex-20_advanced-155-alternating-direction-method-of-multipliers-admm" title="Permanent link">¶</a></h2>
<p>When objectives naturally split into simpler pieces depending on different variables, ADMM is a powerful tool. It is especially useful when:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> have simple prox operators,</li>
<li>the problem is distributed or separable across machines.</li>
</ul>
<p>Consider
<script type="math/tex; mode=display">
\min_{x,z}\; f(x) + g(z)
\quad
\text{s.t. } A x + B z = c.
</script>
</p>
<p>The augmented Lagrangian is
<script type="math/tex; mode=display">
L_\rho(x,z,y)
=
f(x) + g(z)
+ y^\top(Ax + Bz - c)
+ \frac{\rho}{2}\|A x + B z - c\|_2^2,
</script>
with dual variable <span class="arithmatex">\(y\)</span> and penalty parameter <span class="arithmatex">\(\rho &gt; 0\)</span>.</p>
<p>ADMM performs the iterations:
<script type="math/tex; mode=display">
\begin{aligned}
x^{k+1} &= \arg\min_x L_\rho(x, z^k, y^k),\\[4pt]
z^{k+1} &= \arg\min_z L_\rho(x^{k+1}, z, y^k),\\[4pt]
y^{k+1} &= y^k + \rho \big(A x^{k+1} + B z^{k+1} - c\big).
\end{aligned}
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The <span class="arithmatex">\(x\)</span>-update solves a subproblem involving <span class="arithmatex">\(f\)</span> only.</li>
<li>The <span class="arithmatex">\(z\)</span>-update solves a subproblem involving <span class="arithmatex">\(g\)</span> only.</li>
<li>The <span class="arithmatex">\(y\)</span>-update nudges the constraint <span class="arithmatex">\(A x + B z = c\)</span> toward satisfaction.</li>
</ul>
<p>For convex <span class="arithmatex">\(f,g\)</span>, ADMM converges to a primal–dual optimal point. It is particularly effective when the <span class="arithmatex">\(x\)</span>- and <span class="arithmatex">\(z\)</span>-subproblems have closed-form prox solutions or can be solved cheaply in parallel.</p>
<p>ML use cases:</p>
<ul>
<li>Distributed LASSO / logistic regression,</li>
<li>matrix completion and robust PCA,</li>
<li>consensus optimization (each worker has local data but shares a global model),</li>
<li>some federated learning formulations.</li>
</ul>
<h2 id="convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms">15.6 Majorization–Minimization (MM) and EM Algorithms<a class="headerlink" href="#convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms" title="Permanent link">¶</a></h2>
<p>The Majorization–Minimization (MM) principle constructs at each iterate <span class="arithmatex">\(x_k\)</span> a surrogate function <span class="arithmatex">\(g(\cdot \mid x_k)\)</span> that upper-bounds <span class="arithmatex">\(f\)</span> and is easier to minimize.</p>
<p>Requirements:
<script type="math/tex; mode=display">
g(x \mid x_k) \ge f(x)\ \text{ for all } x, 
\quad
g(x_k \mid x_k) = f(x_k).
</script>
</p>
<p>Then define
<script type="math/tex; mode=display">
x_{k+1} = \arg\min_x g(x \mid x_k).
</script>
</p>
<p>This guarantees monotone decrease:
<script type="math/tex; mode=display">
f(x_{k+1}) \le g(x_{k+1}\mid x_k) \le g(x_k\mid x_k) = f(x_k).
</script>
</p>
<p>The famous Expectation–Maximization (EM) algorithm is an MM method for latent-variable models, where the surrogate arises from Jensen’s inequality and missing-data structure.</p>
<p>Other examples:</p>
<ul>
<li>Iteratively Reweighted Least Squares (IRLS) for logistic regression and robust regression,</li>
<li>MM surrogates for nonconvex penalties (e.g. smoothly approximating <span class="arithmatex">\(\ell_0\)</span>),</li>
<li>mixture models and variational inference.</li>
</ul>
<h2 id="convex-20_advanced-157-distributed-and-parallel-optimization">15.7 Distributed and Parallel Optimization<a class="headerlink" href="#convex-20_advanced-157-distributed-and-parallel-optimization" title="Permanent link">¶</a></h2>
<p>When data or variables are split across machines, we need distributed or parallel optimization schemes.</p>
<h3 id="convex-20_advanced-1571-synchronous-vs-asynchronous">15.7.1 Synchronous vs Asynchronous<a class="headerlink" href="#convex-20_advanced-1571-synchronous-vs-asynchronous" title="Permanent link">¶</a></h3>
<ul>
<li>Synchronous methods: all workers compute local gradients or updates, then synchronize (e.g. parameter server, federated averaging).</li>
<li>Asynchronous methods: workers update parameters without global synchronization, improving hardware utilization but introducing staleness and variance.</li>
</ul>
<h3 id="convex-20_advanced-1572-consensus-optimization">15.7.2 Consensus Optimization<a class="headerlink" href="#convex-20_advanced-1572-consensus-optimization" title="Permanent link">¶</a></h3>
<p>A standard pattern is consensus form:
<script type="math/tex; mode=display">
\min_{x_1,\dots,x_P,z}
\sum_{i=1}^P f_i(x_i)
\quad \text{s.t. } x_i = z,\; i = 1,\dots,P,
</script>
where <span class="arithmatex">\(f_i\)</span> is the local objective on worker <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(z\)</span> is the global consensus variable.</p>
<p>ADMM applied to this problem:</p>
<ul>
<li>Each worker updates its local <span class="arithmatex">\(x_i\)</span> using only local data,</li>
<li>The global variable <span class="arithmatex">\(z\)</span> is updated by averaging or aggregation,</li>
<li>Dual variables enforce agreement <span class="arithmatex">\(x_i \approx z\)</span>.</li>
</ul>
<p>This template underlies many federated learning and parameter-server architectures.</p>
<h3 id="convex-20_advanced-1573-ml-context">15.7.3 ML Context<a class="headerlink" href="#convex-20_advanced-1573-ml-context" title="Permanent link">¶</a></h3>
<ul>
<li>Federated learning (phone/edge devices update local models and send summaries to a server),</li>
<li>Large-scale convex optimization over sharded datasets,</li>
<li>Distributed sparse regression, matrix factorization, and graphical model learning.</li>
</ul>
<h2 id="convex-20_advanced-158-handling-structure-sparsity-and-low-rank">15.8 Handling Structure: Sparsity and Low Rank<a class="headerlink" href="#convex-20_advanced-158-handling-structure-sparsity-and-low-rank" title="Permanent link">¶</a></h2>
<p>Large-scale convex problems often have additional structure that we can exploit algorithmically:</p>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Typical Regularizer / Constraint</th>
<th>Algorithmic Benefit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sparsity</td>
<td><span class="arithmatex">\(\ell_1\)</span>, group lasso</td>
<td>Cheap coordinate updates, soft-thresholding</td>
</tr>
<tr>
<td>Low rank</td>
<td>Nuclear norm <span class="arithmatex">\(\|X\|_*\)</span></td>
<td>SVD-based prox; rank truncation</td>
</tr>
<tr>
<td>Block separability</td>
<td><span class="arithmatex">\(\sum_i f_i(x_i)\)</span></td>
<td>Parallel or distributed block updates</td>
</tr>
<tr>
<td>Graph structure</td>
<td>Total variation on graphs</td>
<td>Local neighborhood computations</td>
</tr>
<tr>
<td>Probability simplex</td>
<td>simplex constraint or entropy term</td>
<td>Mirror descent, simplex projections</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<ul>
<li>In compressed sensing, <span class="arithmatex">\(\ell_1\)</span> regularization + sparse sensing matrices → very cheap mat–vecs + prox operations.</li>
<li>In matrix completion, nuclear norm structure + low-rank iterates → approximate SVD instead of full SVD.</li>
<li>In TV denoising, local difference structure → each prox step involves only neighboring pixels/vertices.</li>
</ul>
<p>Exploiting structure can yield orders-of-magnitude speedups compared to generic solvers.</p>
<h2 id="convex-20_advanced-159-summary-and-practical-guidance">15.9 Summary and Practical Guidance<a class="headerlink" href="#convex-20_advanced-159-summary-and-practical-guidance" title="Permanent link">¶</a></h2>
<p>Different large-scale methods are appropriate in different regimes:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Gradient Access</th>
<th>Scalability</th>
<th>Parallelization</th>
<th>Convexity Needed</th>
<th>Typical Uses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent</td>
<td>Partial gradients</td>
<td>High</td>
<td>Easy (blockwise)</td>
<td>Convex</td>
<td>LASSO, sparse GLMs, matrix factorization</td>
</tr>
<tr>
<td>SGD / Mini-batch SGD</td>
<td>Stochastic gradients</td>
<td>Excellent</td>
<td>Natural (data parallel)</td>
<td>Convex / nonconvex</td>
<td>Deep learning, logistic regression</td>
</tr>
<tr>
<td>SVRG / SAGA (VR methods)</td>
<td>Stochastic + periodic full gradient</td>
<td>High</td>
<td>Data parallel</td>
<td>Convex, often strongly convex</td>
<td>Large-scale convex ML, GLMs</td>
</tr>
<tr>
<td>Proximal Gradient (ISTA/FISTA)</td>
<td>Full gradient + prox</td>
<td>Moderate–High</td>
<td>Easy</td>
<td>Convex</td>
<td>Composite objectives with structure</td>
</tr>
<tr>
<td>ADMM</td>
<td>Local subproblems</td>
<td>High</td>
<td>Designed for distributed</td>
<td>Convex</td>
<td>Consensus, distributed convex solvers</td>
</tr>
<tr>
<td>MM / EM</td>
<td>Surrogates</td>
<td>Moderate</td>
<td>Model-specific</td>
<td>Convex / nonconvex</td>
<td>Latent-variable models, IRLS</td>
</tr>
<tr>
<td>Distributed / Federated</td>
<td>Local gradients</td>
<td>Very high</td>
<td>Essential</td>
<td>Often convex / smooth</td>
<td>Federated learning, multi-agent systems</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-21_models" heading-number="2.16"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-16-modelling-patterns-and-algorithm-selection">Chapter 16: Modelling Patterns and Algorithm Selection<a class="headerlink" href="#convex-21_models-chapter-16-modelling-patterns-and-algorithm-selection" title="Permanent link">¶</a></h1>
<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often <em>tells</em> us which solver class to use.  In practice, solving machine learning problems looks like: modeling → recognize structure → pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>
<h2 id="convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff">16.1 Regularized estimation and the accuracy–simplicity tradeoff<a class="headerlink" href="#convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff" title="Permanent link">¶</a></h2>
<p>Many learning tasks use a regularized risk minimization form:
<script type="math/tex; mode=display">
\min_x \; \underbrace{\text{loss}(x)}_{\text{data-fit}} \;+\; \lambda\;\underbrace{\text{penalty}(x)}_{\text{complexity}}.
</script>
Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing <span class="arithmatex">\(\lambda\)</span> trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p>
<ul>
<li>
<p>Ridge regression (ℓ₂ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
  </script>
<br>
  This arises from Gaussian noise (squared-error loss) plus a quadratic prior on <span class="arithmatex">\(x\)</span>.  It is a smooth, strongly convex quadratic problem (Hessian <span class="arithmatex">\(A^TA + \lambda I \succ 0\)</span>).  One can solve it via Newton’s method or closed‐form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p>
</li>
<li>
<p>LASSO / Sparse regression (ℓ₁ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \tfrac12\|Ax - b\|_2^2 + \lambda \|x\|_1.
  </script>
<br>
  The <span class="arithmatex">\(\ell_1\)</span> penalty encourages many <span class="arithmatex">\(x_i=0\)</span> (sparsity) for interpretability.  The problem is convex but nonsmooth (since <span class="arithmatex">\(|\cdot|\)</span> is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for <span class="arithmatex">\(\ell_1\)</span>, which sets small entries to zero.  Coordinate descent is another popular solver – updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p>
</li>
<li>
<p>Elastic net (mixed ℓ₁+ℓ₂):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda_1\|x\|_1 + \lambda_2\|x\|_2^2.
  </script>
<br>
  This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for <span class="arithmatex">\(\lambda_2&gt;0\)</span>) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the ℓ₂ term, the objective is smooth and unique solution.</p>
</li>
<li>
<p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block <span class="arithmatex">\(\ell_{2,1}\)</span> norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p>
</li>
</ul>
<p>Algorithms Summary:  </p>
<ul>
<li>Smooth + ℓ₂ (strongly convex):<br>
  Newton / quasi-Newton, conjugate gradient, or accelerated gradient.</li>
<li>Smooth + ℓ₁ (and variants):<br>
  proximal gradient or coordinate descent; for huge data, stochastic/proximal variants.</li>
<li>Mixed penalties (ℓ₁ + ℓ₂):<br>
  treat as composite smooth + nonsmooth; prox and coordinate methods still apply.</li>
<li>Large <span class="arithmatex">\(N\)</span> or <span class="arithmatex">\(n\)</span>:<br>
  mini-batch / stochastic gradients (SGD, SVRG, etc.) on the smooth part + prox for the regulariser.</li>
</ul>
<p>Regularisation strength <span class="arithmatex">\(\lambda\)</span> is usually chosen via cross-validation or a validation set, exploring the accuracy–simplicity trade-off.</p>
<h2 id="convex-21_models-162-robust-regression-and-outlier-resistance">16.2 Robust regression and outlier resistance<a class="headerlink" href="#convex-21_models-162-robust-regression-and-outlier-resistance" title="Permanent link">¶</a></h2>
<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>
<h3 id="convex-21_models-1621-least-absolute-deviations-l1-loss">16.2.1 Least absolute deviations (ℓ₁ loss)<a class="headerlink" href="#convex-21_models-1621-least-absolute-deviations-l1-loss" title="Permanent link">¶</a></h3>
<p>Formulation:
<script type="math/tex; mode=display">
\min_x \sum_i \lvert a_i^\top x - b_i \rvert.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li>
<li>Unlike squared error, it penalizes big residuals <em>linearly</em>, not quadratically, so outliers hurt less.</li>
</ul>
<p>Geometry/structure:
- The objective is convex but nondifferentiable at zero residual (the kink in <span class="arithmatex">\(|r|\)</span> at <span class="arithmatex">\(r=0\)</span>).</p>
<p>How to solve it:</p>
<ol>
<li>
<p>As a linear program (LP).<br>
   Introduce slack variables <span class="arithmatex">\(t_i \ge 0\)</span> and rewrite:</p>
<ul>
<li>constraints:<br>
<span class="arithmatex">\(-t_i \le a_i^\top x - b_i \le t_i\)</span>,</li>
<li>objective:<br>
<span class="arithmatex">\(\min \sum_i t_i\)</span>.</li>
</ul>
<p>This is now a standard LP. You can solve it with:</p>
<ul>
<li>an interior-point LP solver,</li>
<li>or simplex.</li>
</ul>
<p>These methods give high-accuracy solutions and certificates.</p>
</li>
<li>
<p>First-order methods for large scale.  </p>
<p>For <em>very</em> large problems (millions of samples/features), you can apply:</p>
<ul>
<li>subgradient methods,</li>
<li>proximal methods (using the prox of <span class="arithmatex">\(|\cdot|\)</span>).</li>
</ul>
<p>These are slower in theory (subgradient is only <span class="arithmatex">\(O(1/\sqrt{t})\)</span> convergence), but they scale to huge data where generic LP solvers would struggle.</p>
</li>
</ol>
<h3 id="convex-21_models-1622-huber-loss">16.2.2 Huber loss<a class="headerlink" href="#convex-21_models-1622-huber-loss" title="Permanent link">¶</a></h3>
<p>Definition of the Huber penalty for residual <span class="arithmatex">\(r\)</span>:
<script type="math/tex; mode=display">
\rho_\delta(r) =
\begin{cases}
\frac{1}{2} r^2, & |r| \le \delta, \\
\delta |r| - \frac{1}{2}\delta^2, & |r| > \delta.
\end{cases}
</script>
</p>
<p>Huber regression solves:
<script type="math/tex; mode=display">
\min_x \sum_i \rho_\delta(a_i^\top x - b_i).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For small residuals (<span class="arithmatex">\(|r|\le\delta\)</span>): it acts like least-squares (<span class="arithmatex">\(\tfrac{1}{2}r^2\)</span>). So inliers are fit tightly.</li>
<li>For large residuals (<span class="arithmatex">\(|r|&gt;\delta\)</span>): it acts like <span class="arithmatex">\(\ell_1\)</span> (linear penalty), so outliers get down-weighted.</li>
<li>Intuition: “be aggressive on normal data, be forgiving on outliers.”</li>
</ul>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(\rho_\delta\)</span> is convex.</li>
<li>It is smooth except for a kink in its second derivative at <span class="arithmatex">\(|r|=\delta\)</span>.</li>
<li>Its gradient exists everywhere (the function is once-differentiable).</li>
</ul>
<p>How to solve it:</p>
<ol>
<li>
<p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.<br>
    Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p>
</li>
<li>
<p>Proximal / first-order methods.<br>
    You can apply proximal gradient methods, since each term is simple and has a known prox.</p>
</li>
<li>
<p>As a conic program (SOCP).<br>
    The Huber objective can be written with auxiliary variables and second-order cone constraints. That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly. This is attractive when you want high accuracy and dual certificates.</p>
</li>
</ol>
<h3 id="convex-21_models-1623-worst-case-robust-regression">16.2.3 Worst-case robust regression<a class="headerlink" href="#convex-21_models-1623-worst-case-robust-regression" title="Permanent link">¶</a></h3>
<p>Sometimes we don’t just want “fit the data we saw,” but “fit any data within some uncertainty set.” This leads to min–max problems of the form:
<script type="math/tex; mode=display">
\min_x \;\max_{u \in \mathcal{U}} \; \| (A + u)x - b \|_2.
</script>
</p>
<p>Meaning:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{U}\)</span> is an uncertainty set describing how much you distrust the matrix <span class="arithmatex">\(A\)</span>, the inputs, or the measurements.</li>
<li>You choose <span class="arithmatex">\(x\)</span> that performs well even in the worst allowed perturbation.</li>
</ul>
<p>Why this is still tractable:</p>
<ul>
<li>
<p>If <span class="arithmatex">\(\mathcal{U}\)</span> is convex (for example, an <span class="arithmatex">\(\ell_2\)</span> ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p>
</li>
<li>
<p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p>
<ul>
<li>Example: if the rows of <span class="arithmatex">\(A\)</span> can move within an <span class="arithmatex">\(\ell_2\)</span> ball of radius <span class="arithmatex">\(\epsilon\)</span>, the robustified problem often picks up an additional <span class="arithmatex">\(\ell_2\)</span> term like <span class="arithmatex">\(\gamma \|x\|_2\)</span> in the objective.</li>
<li>The final problem is still convex (often a QP or SOCP).</li>
</ul>
</li>
</ul>
<p>How to solve it:</p>
<ul>
<li>
<p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p>
</li>
<li>
<p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p>
</li>
</ul>
<h2 id="convex-21_models-163-maximum-likelihood-and-loss-design">16.3 Maximum likelihood and loss design<a class="headerlink" href="#convex-21_models-163-maximum-likelihood-and-loss-design" title="Permanent link">¶</a></h2>
<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p>
<ul>
<li>
<p>Gaussian (normal) noise</p>
<p>Model:
<script type="math/tex; mode=display">
b = A x + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I).
</script>
</p>
<p>The negative log-likelihood (NLL) is proportional to:
<script type="math/tex; mode=display">
|A x - b|_2^2.
</script>
</p>
<p>This recovers the classic least-squares loss (as in linear regression).<br>
It is smooth and convex (strongly convex if <span class="arithmatex">\(A^T A\)</span> is full rank).</p>
<p>Algorithms:</p>
<ul>
<li>
<p>Closed-form via <span class="arithmatex">\((A^T A + \lambda I)^{-1} A^T b\)</span> (for ridge regression),</p>
</li>
<li>
<p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p>
</li>
<li>
<p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian <span class="arithmatex">\(A^T A\)</span>.</p>
</li>
</ul>
</li>
<li>
<p>Laplace (double-exponential) noise</p>
<p>If <span class="arithmatex">\(\varepsilon_i \sim \text{Laplace}(0, b)\)</span> i.i.d., the NLL is proportional to:
<script type="math/tex; mode=display">
\sum_i |a_i^T x - b_i|.
</script>
</p>
<p>This is exactly the ℓ₁ regression (least absolute deviations).<br>
It can be solved as an LP or with robust optimization solvers (interior-point),<br>
or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p>
</li>
<li>
<p>Logistic model (binary classification)</p>
<p>For <span class="arithmatex">\(y_i \in \{0,1\}\)</span>, model:
<script type="math/tex; mode=display">
\Pr(y_i = 1 \mid a_i, x) = \sigma(a_i^T x),
\quad \text{where } \sigma(z) = \frac{1}{1 + e^{-z}}.
</script>
</p>
<p>The negative log-likelihood (logistic loss) is:
<script type="math/tex; mode=display">
\sum_i \left[ -y_i (a_i^T x) + \log(1 + e^{a_i^T x}) \right].
</script>
</p>
<p>This loss is convex and smooth in <span class="arithmatex">\(x\)</span>.<br>
No closed-form solution exists.</p>
<p>Algorithms:</p>
<ul>
<li>With ℓ₂ regularization: smooth and (if <span class="arithmatex">\(\lambda&gt;0\)</span>) strongly convex → use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li>
<li>With ℓ₁ regularization (sparse logistic): composite convex → use proximal gradient (soft-thresholding) or coordinate descent.</li>
</ul>
</li>
<li>
<p>Softmax / Multinomial logistic (multiclass)</p>
<p>For <span class="arithmatex">\(K\)</span> classes with one-hot labels <span class="arithmatex">\(y_i \in \{e_1, \dots, e_K\}\)</span>, the softmax model gives NLL:
<script type="math/tex; mode=display">
-\sum_i \sum_{k=1}^K y_{ik}(a_i^T x_k)
+ \log\!\left(\sum_{j=1}^K e^{a_i^T x_j}\right).
</script>
</p>
<p>This loss is convex in the weight vectors <span class="arithmatex">\(\{x_k\}\)</span> and generalizes binary logistic to multiclass.</p>
<p>Algorithms:</p>
<ul>
<li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li>
<li>Stochastic gradient (SGD, Adam) for large datasets.</li>
</ul>
</li>
<li>
<p>Generalized linear models (GLMs)</p>
<p>In GLMs, <span class="arithmatex">\(y_i\)</span> given <span class="arithmatex">\(x\)</span> has an exponential-family distribution (Poisson, binomial, etc.) with mean related to <span class="arithmatex">\(a_i^T x\)</span>.<br>
The NLL is convex in <span class="arithmatex">\(x\)</span> for canonical links (e.g. log-link for Poisson, logit for binomial).</p>
<p>Examples:</p>
<ul>
<li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li>
<li>Probit models: convex but require iterative solvers.</li>
</ul>
</li>
</ul>
<h2 id="convex-21_models-164-structured-constraints-in-engineering-and-design">16.4 Structured constraints in engineering and design<a class="headerlink" href="#convex-21_models-164-structured-constraints-in-engineering-and-design" title="Permanent link">¶</a></h2>
<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<ul>
<li>
<p>Simple (projection-friendly) constraints</p>
<p>Examples:</p>
<ul>
<li>
<p>Box constraints: <span class="arithmatex">\(l \le x \le u\)</span><br>
    → Projection: clip each entry to <span class="arithmatex">\([\ell_i, u_i]\)</span>.</p>
</li>
<li>
<p>ℓ₂-ball: <span class="arithmatex">\(\|x\|_2 \le R\)</span><br>
    → Projection: rescale <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(\|x\|_2 &gt; R\)</span>.</p>
</li>
<li>
<p>Simplex: <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span><br>
    → Projection: sort and threshold coordinates (simple <span class="arithmatex">\(O(n \log n)\)</span> algorithm).</p>
</li>
</ul>
</li>
<li>
<p>General convex constraints (non-projection-friendly)
If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p>
<ol>
<li>
<p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p>
</li>
<li>
<p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p>
</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers:</p>
<ul>
<li>Projection-friendly constraints → Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li>
<li>Complex constraints (cones, PSD, many linear) → Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li>
<li>LP/QP special cases → Use simplex or specialized LP/QP solvers (Section 11.5).</li>
</ul>
<p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection → projective methods; otherwise → interior-point or operator-splitting.</p>
<h2 id="convex-21_models-165-linear-and-conic-programming-the-canonical-models">16.5 Linear and conic programming: the canonical models<a class="headerlink" href="#convex-21_models-165-linear-and-conic-programming-the-canonical-models" title="Permanent link">¶</a></h2>
<p>Many practical problems reduce to linear programming (LP) or its convex extensions.<br>
LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p>
<ul>
<li>
<p>Linear programs: standard form</p>
<p>
<script type="math/tex; mode=display">
\min_x \; c^T x 
\quad \text{s.t.} \quad A x = b, \; x \ge 0.
</script>
Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed.
- Quadratic, SOCP, SDP:
Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p>
</li>
<li>
<p>Practical patterns:</p>
<ol>
<li>Resource allocation/flow (LP): linear costs and constraints.</li>
<li>Minimax/regret problems: e.g. <span class="arithmatex">\(\min_{x}\max_{i}|a_i^T x - b_i|\)</span> → LP (as in Chebyshev regression).</li>
<li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers for 11.5:
- Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable).
- Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale.
- Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. ℓ∞ regression → LP, ℓ2 regression with ℓ2 constraint → SOCP.)</p>
<h2 id="convex-21_models-166-risk-safety-margins-and-robust-design">16.6 Risk, safety margins, and robust design<a class="headerlink" href="#convex-21_models-166-risk-safety-margins-and-robust-design" title="Permanent link">¶</a></h2>
<p>Modern design often includes risk measures or robustness. Two common patterns:</p>
<ul>
<li>
<p>Chance constraints / risk-adjusted objectives
    E.g. require that <span class="arithmatex">\(Pr(\text{loss}(x,\xi) &gt; \tau) \le \delta\)</span>. A convex surrogate is to include mean and a multiple of the standard deviation:
    <script type="math/tex; mode=display">
    \min_x \; \mathbb{E}[\ell(x, \xi)] + \kappa \sqrt{\mathrm{Var}[\ell(x, \xi)]}.
    </script>
    Algebra often leads to second-order cone constraints (e.g. forcing <span class="arithmatex">\(\mathbb{E}\pm \kappa\sqrt{\mathrm{Var}}\)</span> below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p>
</li>
<li>
<p>Worst-case (robust) optimization:
    Specify an uncertainty set <span class="arithmatex">\(\mathcal{U}\)</span> for data (e.g. <span class="arithmatex">\(u\)</span> in a norm-ball) and minimize the worst-case cost <span class="arithmatex">\(\max_{u\in\mathcal{U}}\ell(x,u)\)</span>. Many losses <span class="arithmatex">\(\ell\)</span> and convex <span class="arithmatex">\(\mathcal{U}\)</span> yield a convex max-term (a support function or norm). The result is often a conic constraint (for ℓ₂ norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p>
</li>
</ul>
<p>Algorithmic pointers for 11.6:</p>
<ul>
<li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li>
<li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li>
<li>Distributed or iterative solutions: If <span class="arithmatex">\(\mathcal{U}\)</span> or loss separable, ADMM can distribute the computation (Chapter 10).</li>
</ul>
<h2 id="convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that">16.7 Cheat sheet: If your problem looks like this, use that<a class="headerlink" href="#convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that" title="Permanent link">¶</a></h2>
<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p>
<ul>
<li>
<p>(A) Smooth least-squares + ℓ₂:</p>
<ul>
<li>Model: <span class="arithmatex">\(|Ax-b|_2^2 + \lambda|x|_2^2\)</span>. </li>
<li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic ⇒ fast second-order methods.)</li>
</ul>
</li>
<li>
<p>(B) Sparse regression (ℓ₁):</p>
<ul>
<li>Model: <span class="arithmatex">\(\tfrac12|Ax-b|_2^2 + \lambda|x|_1\)</span>. </li>
<li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li>
</ul>
</li>
<li>
<p>(C) Robust regression (outliers):</p>
<ul>
<li>Models: <span class="arithmatex">\(\sum|a_i^T x - b_i|\)</span>, Huber loss, etc. </li>
<li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li>
</ul>
</li>
<li>
<p>(D) Logistic / log-loss (classification):</p>
<ul>
<li>Model: <span class="arithmatex">\(\sum[-y_i(w^Ta_i)+\log(1+e^{w^Ta_i})] + \lambda R(w)\)</span> with <span class="arithmatex">\(R(w)=|w|_2^2\)</span> or <span class="arithmatex">\(|w|_1\)</span>. </li>
<li>Solve:<ul>
<li>If <span class="arithmatex">\(R=\ell_2\)</span>: use Newton/gradient (smooth, strongly convex).</li>
<li>If <span class="arithmatex">\(R=\ell_1\)</span>: use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; ℓ₁ adds nonsmoothness.)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>(E) Constraints (hard limits):</p>
<ul>
<li>Model: <span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(x\in\mathcal{X}\)</span> with <span class="arithmatex">\(\mathcal{X}\)</span> simple. </li>
<li>Solve: Projected (stochastic) gradient or proximal methods if projection <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> is cheap (e.g. box, ball, simplex). If <span class="arithmatex">\(\mathcal{X}\)</span> is complex (second-order or SDP), use interior-point.</li>
</ul>
</li>
<li>
<p>(F) Separable structure:</p>
<ul>
<li>Model: <span class="arithmatex">\(\min_{x,z} f(x)+g(z)\)</span> s.t. <span class="arithmatex">\(Ax+Bz=c\)</span>. </li>
<li>Solve: ADMM (Chapter 10) – it decouples updates in <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(z\)</span>; suits distributed or block-structured data.</li>
</ul>
</li>
<li>
<p>(G) LP/QP/SOCP/SDP:</p>
<ul>
<li>Model: linear/quadratic objective with linear/conic constraints. </li>
<li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li>
</ul>
</li>
<li>
<p>(H) Nonconvex patterns:</p>
<ul>
<li>
<p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p>
</li>
<li>
<p>Solve: There is no single global solver – typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p>
</li>
</ul>
</li>
<li>
<p>(I) Logistic (multi-class softmax):</p>
<ul>
<li>
<p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p>
</li>
<li>
<p>Solve: Similar to binary case – Newton/gradient with L2, or proximal/coordinate with ℓ₁.</p>
</li>
</ul>
</li>
<li>
<p>(J) Poisson and count models:</p>
<ul>
<li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li>
<li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li>
</ul>
</li>
</ul>
<p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p>
<ul>
<li>Smooth &amp; strongly convex → (quasi-)Newton or accelerated gradient.</li>
<li>Smooth + ℓ₁ → Proximal gradient/coordinate.</li>
<li>Nonsmooth separable → Proximal or coordinate.</li>
<li>Easy projection constraint → Projected gradient.</li>
<li>Hard constraints or conic structure → Interior-point.</li>
<li>Large-scale separable → Stochastic gradient/ADMM.</li>
</ul>
<p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>
<h2 id="convex-21_models-167-matching-model-structure-to-algorithm-type">16.7 Matching Model Structure to Algorithm Type<a class="headerlink" href="#convex-21_models-167-matching-model-structure-to-algorithm-type" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Problem Form</th>
<th>Recommended Algorithms</th>
<th>Notes / ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span></td>
<td>Gradient descent, Newton, LBFGS</td>
<td>Small to medium problems; logistic regression, ridge regression</td>
</tr>
<tr>
<td>Nonsmooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x) + R(x)\)</span></td>
<td>Subgradient, proximal (ISTA/FISTA), coordinate descent</td>
<td>LASSO, hinge loss SVM</td>
</tr>
<tr>
<td>Equality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(A x = b\)</span></td>
<td>Projected gradient, augmented Lagrangian</td>
<td>Constrained least squares, balance conditions</td>
</tr>
<tr>
<td>Inequality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le 0\)</span></td>
<td>Barrier, primal–dual, interior-point</td>
<td>Quadratic programming, LPs, constrained regression</td>
</tr>
<tr>
<td>Separable / block structure</td>
<td><span class="arithmatex">\(\min \sum_i f_i(x_i)\)</span></td>
<td>ADMM, coordinate updates</td>
<td>Distributed optimization, federated learning</td>
</tr>
<tr>
<td>Stochastic / large data</td>
<td><span class="arithmatex">\(\min \frac{1}{N}\sum_i f_i(x_i)\)</span></td>
<td>SGD, SVRG, adaptive variants</td>
<td>Deep learning, online convex optimization</td>
</tr>
<tr>
<td>Low-rank / matrix structure</td>
<td><span class="arithmatex">\(\min f(X) + \lambda \|X\|_*\)</span></td>
<td>Proximal (SVD shrinkage), ADMM</td>
<td>Matrix completion, PCA variants</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-30_canonical_problems" heading-number="2.17"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-17-canonical-problems-in-convex-optimization">Chapter 17: Canonical Problems in Convex Optimization<a class="headerlink" href="#convex-30_canonical_problems-chapter-17-canonical-problems-in-convex-optimization" title="Permanent link">¶</a></h1>
<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms — each with characteristic geometry, structure, and algorithms.</p>
<h2 id="convex-30_canonical_problems-171-hierarchy-of-canonical-problems">17.1 Hierarchy of Canonical Problems<a class="headerlink" href="#convex-30_canonical_problems-171-hierarchy-of-canonical-problems" title="Permanent link">¶</a></h2>
<p>Convex programs form a nested hierarchy:</p>
<div class="arithmatex">\[
\text{LP} \subseteq \text{QP} \subseteq \text{SOCP} \subseteq \text{SDP}.
\]</div>
<p>Each inclusion represents an extension of representational power — from linear to quadratic, to conic, and finally to semidefinite constraints.<br>
Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Canonical Form</th>
<th>Key Condition</th>
<th>Typical Algorithms</th>
<th>ML / Applied Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td><span class="arithmatex">\(\min_x c^\top x\)</span> s.t. <span class="arithmatex">\(A x=b,\,x\ge0\)</span></td>
<td>Linear constraints</td>
<td>Simplex, Interior-point</td>
<td>Resource allocation, Chebyshev regression</td>
</tr>
<tr>
<td>QP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top Q x + c^\top x\)</span> s.t. <span class="arithmatex">\(A x\le b\)</span></td>
<td><span class="arithmatex">\(Q\succeq0\)</span></td>
<td>Interior-point, Active-set, CG</td>
<td>Ridge, SVM, Portfolio optimization</td>
</tr>
<tr>
<td>QCQP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top P_0 x + q_0^\top x\)</span> s.t. <span class="arithmatex">\(\tfrac12 x^\top P_i x + q_i^\top x \le0\)</span></td>
<td>All <span class="arithmatex">\(P_i\succeq0\)</span></td>
<td>Interior-point, SOCP reformulation</td>
<td>Robust regression, trust-region</td>
</tr>
<tr>
<td>SOCP</td>
<td><span class="arithmatex">\(\min_x f^\top x\)</span> s.t. <span class="arithmatex">\(\|A_i x + b_i\|_2 \le c_i^\top x + d_i\)</span></td>
<td>Cone constraints</td>
<td>Conic interior-point</td>
<td>Robust least-squares, risk limits</td>
</tr>
<tr>
<td>SDP</td>
<td><span class="arithmatex">\(\min_X \mathrm{Tr}(C^\top X)\)</span> s.t. <span class="arithmatex">\(\mathrm{Tr}(A_i^\top X)=b_i\)</span>, <span class="arithmatex">\(X\succeq0\)</span></td>
<td>Matrix PSD constraint</td>
<td>Interior-point, low-rank first-order</td>
<td>Covariance estimation, control</td>
</tr>
<tr>
<td>GP</td>
<td><span class="arithmatex">\(\min_{x&gt;0} f_0(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le1,\,g_j(x)=1\)</span></td>
<td>Log-convex after <span class="arithmatex">\(y=\log x\)</span></td>
<td>Log-transform + IPM</td>
<td>Circuit design, power control</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>$\min_x -\sum_i \log p(b_i</td>
<td>a_i^\top x)+\mathcal{R}(x)$</td>
<td>Log-concave likelihood</td>
<td>Newton, L-BFGS, Prox / SGD</td>
</tr>
</tbody>
</table>
<h2 id="convex-30_canonical_problems-172-linear-programming-lp">17.2 Linear Programming (LP)<a class="headerlink" href="#convex-30_canonical_problems-172-linear-programming-lp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x c^\top x \quad \text{s.t. } A x=b,\, x\ge0
\]</div>
<p>Geometry: Feasible region = polyhedron; optimum = vertex.<br>
Applications: Resource allocation, shortest path, flow, scheduling.<br>
Algorithms:</p>
<ol>
<li>Simplex: walks along edges (vertex-based).  </li>
<li>Interior-point: moves through the interior using log barriers.  </li>
<li>Decomposition: exploits block structure for large LPs.</li>
</ol>
<h2 id="convex-30_canonical_problems-173-quadratic-programming-qp">17.3 Quadratic Programming (QP)<a class="headerlink" href="#convex-30_canonical_problems-173-quadratic-programming-qp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top Q x + c^\top x 
\quad \text{s.t. } A x \le b,\, F x = g, \quad Q\succeq0
\]</div>
<p>Geometry: Objective = ellipsoids; feasible = polyhedron.<br>
Examples: Ridge regression, Markowitz portfolio, SVM.<br>
Algorithms:<br>
- Interior-point (smooth path).<br>
- Active-set (edge-following).<br>
- Conjugate Gradient for large unconstrained QPs.<br>
- First-order methods for massive <span class="arithmatex">\(n\)</span>.</p>
<h2 id="convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp">17.4 Quadratically Constrained QP (QCQP)<a class="headerlink" href="#convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top P_0x + q_0^\top x
\quad \text{s.t. } \tfrac12 x^\top P_i x + q_i^\top x + r_i \le 0
\]</div>
<p>Convex if all <span class="arithmatex">\(P_i\succeq0\)</span>.<br>
Geometry: Intersection of ellipsoids and half-spaces.<br>
Applications: Robust control, filter design, trust-region.<br>
Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>
<h2 id="convex-30_canonical_problems-175-second-order-cone-programming-socp">17.5 Second-Order Cone Programming (SOCP)<a class="headerlink" href="#convex-30_canonical_problems-175-second-order-cone-programming-socp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x f^\top x
\quad \text{s.t. } 
\|A_i x + b_i\|_2 \le c_i^\top x + d_i,\;
F x = g
\]</div>
<p>Interpretation: Linear objective, norm-bounded constraints.<br>
Applications: Robust regression, risk-aware portfolio, engineering design.<br>
Algorithms: Conic interior-point; scalable ADMM variants.<br>
Special case: Any QP or norm constraint can be written as an SOCP.</p>
<h2 id="convex-30_canonical_problems-176-semidefinite-programming-sdp">17.6 Semidefinite Programming (SDP)<a class="headerlink" href="#convex-30_canonical_problems-176-semidefinite-programming-sdp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_X \mathrm{Tr}(C^\top X)
\quad \text{s.t. } \mathrm{Tr}(A_i^\top X)=b_i,\; X\succeq0
\]</div>
<p>Meaning: Variable = PSD matrix <span class="arithmatex">\(X\)</span>; constraints = affine.<br>
Geometry: Feasible region = intersection of affine space with PSD cone.<br>
Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion.<br>
Algorithms: Interior-point for moderate <span class="arithmatex">\(n\)</span>; low-rank proximal / Frank–Wolfe for large-scale.</p>
<h2 id="convex-30_canonical_problems-177-geometric-programming-gp">17.7 Geometric Programming (GP)<a class="headerlink" href="#convex-30_canonical_problems-177-geometric-programming-gp" title="Permanent link">¶</a></h2>
<p>Original form</p>
<div class="arithmatex">\[
\min_{x&gt;0} f_0(x)
\quad \text{s.t. } f_i(x)\le1,\; g_j(x)=1
\]</div>
<p>where <span class="arithmatex">\(f_i\)</span> are posynomials and <span class="arithmatex">\(g_j\)</span> monomials.  </p>
<p>Log transformation: With <span class="arithmatex">\(y=\log x\)</span>, the problem becomes convex in <span class="arithmatex">\(y\)</span>.<br>
Applications: Circuit sizing, communication power control, resource allocation.<br>
Solvers: Convert to convex form → interior-point or primal-dual methods.</p>
<h2 id="convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms">17.8 Likelihood-Based Convex Models (MLE and GLMs)<a class="headerlink" href="#convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms" title="Permanent link">¶</a></h2>
<p>General form</p>
<div class="arithmatex">\[
\min_x -\sum_i \log p(b_i|a_i^\top x) + \mathcal{R}(x)
\]</div>
<p>Examples</p>
<table>
<thead>
<tr>
<th>Noise Model</th>
<th>Objective</th>
<th>Equivalent Problem</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Least squares</td>
</tr>
<tr>
<td>Laplacian</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Robust regression (LP)</td>
</tr>
<tr>
<td>Bernoulli</td>
<td><span class="arithmatex">\(\sum_i \log(1+e^{-y_i a_i^\top x})\)</span></td>
<td>Logistic regression</td>
</tr>
<tr>
<td>Poisson</td>
<td><span class="arithmatex">\(\sum_i [a_i^\top x - y_i\log(a_i^\top x)]\)</span></td>
<td>Poisson GLM</td>
</tr>
</tbody>
</table>
<p>Algorithms<br>
- Newton or IRLS (small–medium).<br>
- Quasi-Newton / L-BFGS (moderate).<br>
- Proximal or SGD (large-scale).</p>
<h2 id="convex-30_canonical_problems-179-solver-selection-summary">17.9 Solver Selection Summary<a class="headerlink" href="#convex-30_canonical_problems-179-solver-selection-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Convex Form</th>
<th>Key Solvers</th>
<th>ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td>Linear</td>
<td>Simplex, Interior-point</td>
<td>Minimax regression</td>
</tr>
<tr>
<td>QP</td>
<td>Quadratic</td>
<td>Interior-point, CG, Active-set</td>
<td>Ridge, SVM</td>
</tr>
<tr>
<td>QCQP</td>
<td>Quadratic + constraints</td>
<td>IPM, SOCP / SDP reformulation</td>
<td>Robust regression</td>
</tr>
<tr>
<td>SOCP</td>
<td>Cone</td>
<td>Conic IPM, ADMM</td>
<td>Robust least-squares</td>
</tr>
<tr>
<td>SDP</td>
<td>PSD cone</td>
<td>Interior-point, low-rank FW</td>
<td>Covariance, Max-cut relaxations</td>
</tr>
<tr>
<td>GP</td>
<td>Log-convex</td>
<td>Log-transform + IPM</td>
<td>Power allocation</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>Log-concave</td>
<td>Newton, L-BFGS, Prox-SGD</td>
<td>Logistic regression</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-35_modern" heading-number="2.18"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-18-modern-optimizers-in-machine-learning">Chapter 18: Modern Optimizers in Machine Learning<a class="headerlink" href="#convex-35_modern-chapter-18-modern-optimizers-in-machine-learning" title="Permanent link">¶</a></h1>
<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization—balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p>
<p>This chapter connects the principles of convex optimization to the modern optimizers that power today’s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>
<h2 id="convex-35_modern-181-stochastic-optimization-overview">18.1 Stochastic Optimization Overview<a class="headerlink" href="#convex-35_modern-181-stochastic-optimization-overview" title="Permanent link">¶</a></h2>
<p>In machine learning, we often minimize an empirical risk:
<script type="math/tex; mode=display">
\min_{x \in \mathbb{R}^n} \; f(x) = \frac{1}{N} \sum_{i=1}^N \ell(x; z_i),
</script>
where <span class="arithmatex">\(\ell(x; z_i)\)</span> is the loss on data sample <span class="arithmatex">\(z_i\)</span>.</p>
<p>Computing the full gradient <span class="arithmatex">\(\nabla f(x)\)</span> is infeasible when <span class="arithmatex">\(N\)</span> is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p>
<div class="arithmatex">\[
g_k = \frac{1}{|B_k|} \sum_{i \in B_k} \nabla \ell(x_k; z_i).
$$
This yields the Stochastic Gradient Descent (SGD) update:
$$
x_{k+1} = x_k - \alpha_k g_k.
\]</div>
<p>SGD is the foundation for nearly all deep learning optimizers.</p>
<h2 id="convex-35_modern-182-momentum-and-acceleration">18.2 Momentum and Acceleration<a class="headerlink" href="#convex-35_modern-182-momentum-and-acceleration" title="Permanent link">¶</a></h2>
<p>SGD’s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p>
<p>
<script type="math/tex; mode=display">
v_{k+1} = \beta v_k + (1-\beta) g_k, \quad x_{k+1} = x_k - \alpha v_{k+1},
</script>
where <span class="arithmatex">\(\beta \in [0,1)\)</span> controls inertia.</p>
<p>Nesterov momentum adds a correction term anticipating the future position:</p>
<div class="arithmatex">\[
v_{k+1} = \beta v_k + g(x_k - \alpha \beta v_k), \quad x_{k+1} = x_k - \alpha v_{k+1}.
\]</div>
<p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>
<h2 id="convex-35_modern-183-adaptive-learning-rate-methods">18.3 Adaptive Learning Rate Methods<a class="headerlink" href="#convex-35_modern-183-adaptive-learning-rate-methods" title="Permanent link">¶</a></h2>
<p>Different parameters often require different step sizes.<br>
Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>
<h3 id="convex-35_modern-1831-adagrad">18.3.1 AdaGrad<a class="headerlink" href="#convex-35_modern-1831-adagrad" title="Permanent link">¶</a></h3>
<p>Keeps a cumulative sum of squared gradients:</p>
<p>
<script type="math/tex; mode=display">
G_k = \sum_{t=1}^k g_t \odot g_t,
</script>
and updates parameters as:</p>
<p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - \frac{\alpha}{\sqrt{G_k + \epsilon}} \odot g_k.
</script>
Good for sparse data, but the learning rate can shrink too quickly.</p>
<h3 id="convex-35_modern-1832-rmsprop">18.3.2 RMSProp<a class="headerlink" href="#convex-35_modern-1832-rmsprop" title="Permanent link">¶</a></h3>
<p>A refinement of AdaGrad using exponential averaging:</p>
<div class="arithmatex">\[
E[g^2]_k = \beta E[g^2]_{k-1} + (1-\beta) g_k^2,
\]</div>
<div class="arithmatex">\[
x_{k+1} = x_k - \frac{\alpha}{\sqrt{E[g^2]_k + \epsilon}} g_k.
\]</div>
<p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>
<h3 id="convex-35_modern-1833-adam-adaptive-moment-estimation">18.3.3 Adam: Adaptive Moment Estimation<a class="headerlink" href="#convex-35_modern-1833-adam-adaptive-moment-estimation" title="Permanent link">¶</a></h3>
<p>Adam combines momentum and adaptive scaling:</p>
<div class="arithmatex">\[
m_k = \beta_1 m_{k-1} + (1-\beta_1) g_k, \quad v_k = \beta_2 v_{k-1} + (1-\beta_2) g_k^2,
\]</div>
<div class="arithmatex">\[
\hat{m}_k = \frac{m_k}{1-\beta_1^k}, \quad \hat{v}_k = \frac{v_k}{1-\beta_2^k},
\]</div>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha \frac{\hat{m}_k}{\sqrt{\hat{v}_k} + \epsilon}.
\]</div>
<p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>
<h2 id="convex-35_modern-184-variants-and-modern-extensions">18.4 Variants and Modern Extensions<a class="headerlink" href="#convex-35_modern-184-variants-and-modern-extensions" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Key Idea</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>AdamW</td>
<td>Decoupled weight decay from gradient update</td>
<td>Better regularization</td>
</tr>
<tr>
<td>RAdam</td>
<td>Rectified Adam—adaptive variance correction</td>
<td>Improves stability early in training</td>
</tr>
<tr>
<td>Lookahead</td>
<td>Combines fast and slow weights</td>
<td>Enhances robustness and convergence</td>
</tr>
<tr>
<td>AdaBelief</td>
<td>Uses prediction error instead of raw gradient variance</td>
<td>More adaptive learning rates</td>
</tr>
<tr>
<td>Lion</td>
<td>Uses sign-based updates and momentum</td>
<td>Efficient for large-scale training</td>
</tr>
</tbody>
</table>
<p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>
<h2 id="convex-35_modern-185-implicit-regularization-and-generalization">18.5 Implicit Regularization and Generalization<a class="headerlink" href="#convex-35_modern-185-implicit-regularization-and-generalization" title="Permanent link">¶</a></h2>
<p>Modern optimizers not only minimize loss—they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p>
<p>Empirical findings suggest:</p>
<ul>
<li>Large-batch training finds sharper minima (risk of overfitting).  </li>
<li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li>
<li>Adaptive optimizers may converge faster but generalize slightly worse.</li>
</ul>
<p>This trade-off drives ongoing research into optimizer design.</p>
<h2 id="convex-35_modern-186-practical-considerations">18.6 Practical Considerations<a class="headerlink" href="#convex-35_modern-186-practical-considerations" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>Most critical hyperparameter; use warm-up and decay schedules</td>
</tr>
<tr>
<td>Batch Size</td>
<td>Balances gradient noise and hardware efficiency</td>
</tr>
<tr>
<td>Initialization</td>
<td>Affects early dynamics, especially for Adam variants</td>
</tr>
<tr>
<td>Gradient Clipping</td>
<td>Prevents instability in exploding gradients</td>
</tr>
<tr>
<td>Mixed Precision</td>
<td>Use with adaptive optimizers for speed and memory savings</td>
</tr>
</tbody>
</table>
<h2 id="convex-35_modern-187-comparative-behavior">18.7 Comparative Behavior<a class="headerlink" href="#convex-35_modern-187-comparative-behavior" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Adaptivity</th>
<th>Speed</th>
<th>Memory</th>
<th>Typical Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD + Momentum</td>
<td>Moderate</td>
<td>Slow-medium</td>
<td>Low</td>
<td>General-purpose, good generalization</td>
</tr>
<tr>
<td>RMSProp</td>
<td>Adaptive per-parameter</td>
<td>Medium-fast</td>
<td>Medium</td>
<td>Recurrent networks, nonstationary data</td>
</tr>
<tr>
<td>Adam / AdamW</td>
<td>Fully adaptive</td>
<td>Fast</td>
<td>High</td>
<td>Deep networks, large-scale training</td>
</tr>
<tr>
<td>RAdam / AdaBelief / Lion</td>
<td>Advanced adaptivity</td>
<td>Fast</td>
<td>Medium</td>
<td>Cutting-edge training tasks</td>
</tr>
</tbody>
</table>
<h2 id="convex-35_modern-188-optimization-in-modern-deep-networks">18.8 Optimization in Modern Deep Networks<a class="headerlink" href="#convex-35_modern-188-optimization-in-modern-deep-networks" title="Permanent link">¶</a></h2>
<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p>
<ul>
<li>Batch normalization modifies effective learning rates.  </li>
<li>Skip connections ease gradient flow.  </li>
<li>Large-scale distributed training relies on adaptive optimizers for stability.  </li>
</ul>
<p>Optimization is no longer an isolated procedure but part of the model’s design philosophy.</p>
<hr>
<p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning—adapting dynamically, balancing efficiency and generalization.</p></body></html></section><section class="print-page" id="convex-40_nonconvex" heading-number="2.19"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-19-beyond-convexity-nonconvex-and-global-optimization">Chapter 19: Beyond Convexity – Nonconvex and Global Optimization<a class="headerlink" href="#convex-40_nonconvex-chapter-19-beyond-convexity-nonconvex-and-global-optimization" title="Permanent link">¶</a></h1>
<p>Optimization extends far beyond the comfortable world of convexity. In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p>
<p>Convex optimization gives us strong guarantees — every local minimum is global, and algorithms converge predictably.<br>
But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>
<h2 id="convex-40_nonconvex-191-the-landscape-of-nonconvex-optimization">19.1 The Landscape of Nonconvex Optimization<a class="headerlink" href="#convex-40_nonconvex-191-the-landscape-of-nonconvex-optimization" title="Permanent link">¶</a></h2>
<p>A nonconvex function <span class="arithmatex">\(f:\mathbb{R}^n \to \mathbb{R}\)</span> violates convexity; i.e., for some <span class="arithmatex">\(x, y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>,
<script type="math/tex; mode=display">
f(\theta x + (1-\theta)y) > \theta f(x) + (1-\theta)f(y).
</script>
Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p>
<p>A typical nonconvex landscape looks like a mountainous terrain — smooth in some regions, rugged in others. An optimization algorithm’s path depends strongly on initialization and stochastic effects.</p>
<h3 id="convex-40_nonconvex-example-a-simple-nonconvex-function">Example: A Simple Nonconvex Function<a class="headerlink" href="#convex-40_nonconvex-example-a-simple-nonconvex-function" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
f(x,y) = x^4 + y^4 - 4xy + x^2.
</script>
This function has multiple stationary points:
- <span class="arithmatex">\((0,0)\)</span> (a saddle),
- <span class="arithmatex">\((1,1)\)</span> and <span class="arithmatex">\((-1,-1)\)</span> (local minima),
- <span class="arithmatex">\((1,-1)\)</span> and <span class="arithmatex">\((-1,1)\)</span> (local maxima).</p>
<p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>
<h2 id="convex-40_nonconvex-192-local-vs-global-minima">19.2 Local vs. Global Minima<a class="headerlink" href="#convex-40_nonconvex-192-local-vs-global-minima" title="Permanent link">¶</a></h2>
<p>A point <span class="arithmatex">\(x^*\)</span> is a local minimum if:
<script type="math/tex; mode=display">
f(x^*) \le f(x) \quad \text{for all } x \text{ near } x^*.
</script>
</p>
<p>A global minimum satisfies the stronger condition:
<script type="math/tex; mode=display">
f(x^*) \le f(x) \quad \text{for all } x \in \mathbb{R}^n.
</script>
</p>
<p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad — and there may be exponentially many of them.</p>
<h2 id="convex-40_nonconvex-193-classes-of-nonconvex-problems">19.3 Classes of Nonconvex Problems<a class="headerlink" href="#convex-40_nonconvex-193-classes-of-nonconvex-problems" title="Permanent link">¶</a></h2>
<p>Nonconvex problems appear in several distinct forms:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example</th>
<th>Challenge</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth nonconvex</td>
<td>Neural network training</td>
<td>Multiple minima, saddle points</td>
</tr>
<tr>
<td>Nonsmooth nonconvex</td>
<td>Sparse regularization, ReLU activations</td>
<td>Undefined gradients</td>
</tr>
<tr>
<td>Discrete / combinatorial</td>
<td>Scheduling, routing, integer programs</td>
<td>Exponential search space</td>
</tr>
<tr>
<td>Black-box</td>
<td>Simulation-based optimization</td>
<td>No derivatives or analytical form</td>
</tr>
</tbody>
</table>
<p>Each category requires different algorithmic strategies — from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>
<h2 id="convex-40_nonconvex-194-local-optimization-strategies">19.4 Local Optimization Strategies<a class="headerlink" href="#convex-40_nonconvex-194-local-optimization-strategies" title="Permanent link">¶</a></h2>
<p>Even in nonconvex settings, local optimization remains useful when:
- The problem is nearly convex (e.g., locally convex around good minima),
- The initialization is close to a desired basin of attraction,
- Or the goal is approximate, not exact, optimality.</p>
<h3 id="convex-40_nonconvex-gradient-descent-and-its-variants">Gradient Descent and Its Variants<a class="headerlink" href="#convex-40_nonconvex-gradient-descent-and-its-variants" title="Permanent link">¶</a></h3>
<p>Gradient descent behaves well if <span class="arithmatex">\(f\)</span> is smooth and Lipschitz-continuous:
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k).
</script>
However, convergence is only to a <em>stationary point</em> — not necessarily a minimum.</p>
<p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>
<h2 id="convex-40_nonconvex-195-global-optimization-strategies">19.5 Global Optimization Strategies<a class="headerlink" href="#convex-40_nonconvex-195-global-optimization-strategies" title="Permanent link">¶</a></h2>
<p>To seek the <em>global</em> minimum, algorithms must explore the search space more broadly.<br>
Common strategies include:</p>
<ol>
<li>
<p>Multiple Starts:<br>
   Run local optimization from diverse random initial points and keep the best solution.</p>
</li>
<li>
<p>Continuation and Homotopy Methods:<br>
   Start from a smooth, convex approximation <span class="arithmatex">\(f_\lambda\)</span> of <span class="arithmatex">\(f\)</span> and gradually transform it into the true objective as <span class="arithmatex">\(\lambda \to 0\)</span>.</p>
</li>
<li>
<p>Stochastic Search and Simulated Annealing:<br>
   Introduce randomness in updates to jump between basins.</p>
</li>
<li>
<p>Population-Based Methods:<br>
   Maintain a swarm or population of candidate solutions evolving by selection and variation — leading to metaheuristic algorithms like GA and PSO.</p>
</li>
</ol>
<h2 id="convex-40_nonconvex-196-theoretical-challenges">19.6 Theoretical Challenges<a class="headerlink" href="#convex-40_nonconvex-196-theoretical-challenges" title="Permanent link">¶</a></h2>
<p>Without convexity, most strong results vanish:</p>
<ul>
<li>Global optimality cannot be guaranteed.</li>
<li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li>
<li>Complexity often grows exponentially with problem size.</li>
</ul>
<p>However, theory is not hopeless:</p>
<ul>
<li>Many nonconvex problems are “benign” — e.g., matrix factorization, phase retrieval, or deep linear networks — having no bad local minima.  </li>
<li>Random initialization and overparameterization often aid convergence to global minima in practice.</li>
</ul>
<h2 id="convex-40_nonconvex-197-geometry-of-saddle-points">19.7 Geometry of Saddle Points<a class="headerlink" href="#convex-40_nonconvex-197-geometry-of-saddle-points" title="Permanent link">¶</a></h2>
<p>A saddle point satisfies <span class="arithmatex">\(\nabla f(x)=0\)</span> but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p>
<p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>
<h2 id="convex-40_nonconvex-198-deterministic-vs-stochastic-global-methods">19.8 Deterministic vs. Stochastic Global Methods<a class="headerlink" href="#convex-40_nonconvex-198-deterministic-vs-stochastic-global-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Deterministic Methods</th>
<th>Stochastic Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Systematic exploration of space (branch &amp; bound, interval analysis)</td>
<td>Randomized search (simulated annealing, evolutionary algorithms)</td>
</tr>
<tr>
<td>Can provide certificates of global optimality</td>
<td>Typically approximate but scalable</td>
</tr>
<tr>
<td>High computational cost</td>
<td>Naturally parallelizable</td>
</tr>
</tbody>
</table>
<p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>
<h2 id="convex-40_nonconvex-199-a-taxonomy-of-optimization-beyond-convexity">19.9 A Taxonomy of Optimization Beyond Convexity<a class="headerlink" href="#convex-40_nonconvex-199-a-taxonomy-of-optimization-beyond-convexity" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Family</th>
<th>Typical Algorithms</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Derivative-Free (Black-Box)</td>
<td>Nelder–Mead, CMA-ES, Bayesian Opt.</td>
<td>When gradients unavailable</td>
</tr>
<tr>
<td>Metaheuristic (Evolutionary)</td>
<td>GA, PSO, DE, ACO</td>
<td>Complex landscapes, combinatorial problems</td>
</tr>
<tr>
<td>Modern Stochastic Gradient</td>
<td>Adam, RMSProp, Lion</td>
<td>Deep learning, large-scale models</td>
</tr>
<tr>
<td>Combinatorial / Discrete</td>
<td>Branch &amp; Bound, Tabu, SA</td>
<td>Integer or graph-based problems</td>
</tr>
<tr>
<td>Learning-Based Optimizers</td>
<td>Meta-learning, Reinforcement methods</td>
<td>Adaptive, data-driven optimization</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-42_derivativefree" heading-number="2.20"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-20-derivative-free-and-black-box-optimization">Chapter 20: Derivative-Free and Black-Box Optimization<a class="headerlink" href="#convex-42_derivativefree-chapter-20-derivative-free-and-black-box-optimization" title="Permanent link">¶</a></h1>
<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p>
<p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>
<h2 id="convex-42_derivativefree-201-motivation-and-challenges">20.1 Motivation and Challenges<a class="headerlink" href="#convex-42_derivativefree-201-motivation-and-challenges" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> be an objective function.  </p>
<p>A derivative-free algorithm seeks to minimize <span class="arithmatex">\(f(x)\)</span> using only evaluations of <span class="arithmatex">\(f(x)\)</span>, without access to <span class="arithmatex">\(\nabla f(x)\)</span> or <span class="arithmatex">\(\nabla^2 f(x)\)</span>.</p>
<p>Key challenges:</p>
<ul>
<li>No gradient information → difficult to infer descent directions.  </li>
<li>Expensive evaluations → every call to <span class="arithmatex">\(f(x)\)</span> might require a simulation or experiment.  </li>
<li>Noise and stochasticity → evaluations may be corrupted by measurement or sampling error.  </li>
<li>High-dimensionality → sampling-based methods scale poorly with <span class="arithmatex">\(n\)</span>.</li>
</ul>
<p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>
<h2 id="convex-42_derivativefree-202-classification-of-derivative-free-methods">20.2 Classification of Derivative-Free Methods<a class="headerlink" href="#convex-42_derivativefree-202-classification-of-derivative-free-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Representative Algorithms</th>
<th>Main Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Direct Search</td>
<td>Nelder–Mead, Pattern Search, MADS</td>
<td>Explore the space via geometric moves or meshes</td>
</tr>
<tr>
<td>Model-Based</td>
<td>BOBYQA, Trust-Region DFO</td>
<td>Build local quadratic or surrogate models of <span class="arithmatex">\(f\)</span></td>
</tr>
<tr>
<td>Evolutionary / Population-Based</td>
<td>CMA-ES, Differential Evolution</td>
<td>Evolve a population using stochastic operators</td>
</tr>
<tr>
<td>Probabilistic / Bayesian</td>
<td>Bayesian Optimization</td>
<td>Use probabilistic surrogate models to guide exploration</td>
</tr>
</tbody>
</table>
<h2 id="convex-42_derivativefree-203-direct-search-methods">20.3 Direct Search Methods<a class="headerlink" href="#convex-42_derivativefree-203-direct-search-methods" title="Permanent link">¶</a></h2>
<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>
<h3 id="convex-42_derivativefree-2031-neldermead-simplex-method">20.3.1 Nelder–Mead Simplex Method<a class="headerlink" href="#convex-42_derivativefree-2031-neldermead-simplex-method" title="Permanent link">¶</a></h3>
<p>Perhaps the most famous derivative-free algorithm, Nelder–Mead maintains a simplex — a polytope of <span class="arithmatex">\(n+1\)</span> vertices in <span class="arithmatex">\(\mathbb{R}^n\)</span>.</p>
<p>At each iteration:</p>
<ol>
<li>Evaluate <span class="arithmatex">\(f\)</span> at all simplex vertices.</li>
<li>Reflect, expand, contract, or shrink the simplex depending on performance.</li>
<li>Continue until simplex collapses near a minimum.</li>
</ol>
<p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>
<h3 id="convex-42_derivativefree-2032-pattern-search-methods">20.3.2 Pattern Search Methods<a class="headerlink" href="#convex-42_derivativefree-2032-pattern-search-methods" title="Permanent link">¶</a></h3>
<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p>
<p>Typical update rule:
<script type="math/tex; mode=display">
x_{k+1} = x_k + \Delta_k d_i,
</script>
</p>
<p>where <span class="arithmatex">\(d_i\)</span> is a direction from a finite set (e.g., coordinate axes).<br>
If a direction yields improvement, move there; otherwise, shrink <span class="arithmatex">\(\Delta_k\)</span>.</p>
<h3 id="convex-42_derivativefree-2033-mesh-adaptive-direct-search-mads">20.3.3 Mesh Adaptive Direct Search (MADS)<a class="headerlink" href="#convex-42_derivativefree-2033-mesh-adaptive-direct-search-mads" title="Permanent link">¶</a></h3>
<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>
<h2 id="convex-42_derivativefree-204-model-based-methods">20.4 Model-Based Methods<a class="headerlink" href="#convex-42_derivativefree-204-model-based-methods" title="Permanent link">¶</a></h2>
<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>
<h3 id="convex-42_derivativefree-2041-trust-region-dfo">20.4.1 Trust-Region DFO<a class="headerlink" href="#convex-42_derivativefree-2041-trust-region-dfo" title="Permanent link">¶</a></h3>
<p>A local model <span class="arithmatex">\(m_k(x)\)</span> (often quadratic) is built to approximate <span class="arithmatex">\(f\)</span> near the current iterate <span class="arithmatex">\(x_k\)</span>:
<script type="math/tex; mode=display">
m_k(x) \approx f(x_k) + g_k^\top (x - x_k) + \tfrac{1}{2}(x - x_k)^\top H_k (x - x_k).
</script>
The next iterate solves a trust-region subproblem:
<script type="math/tex; mode=display">
\min_{\|x - x_k\| \le \Delta_k} m_k(x).
</script>
The trust region size <span class="arithmatex">\(\Delta_k\)</span> adapts based on how well <span class="arithmatex">\(m_k\)</span> predicts true function values.</p>
<h3 id="convex-42_derivativefree-2042-bobyqa-bound-optimization-by-quadratic-approximation">20.4.2 BOBYQA (Bound Optimization BY Quadratic Approximation)<a class="headerlink" href="#convex-42_derivativefree-2042-bobyqa-bound-optimization-by-quadratic-approximation" title="Permanent link">¶</a></h3>
<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>
<h2 id="convex-42_derivativefree-205-evolution-strategies-and-population-methods">20.5 Evolution Strategies and Population Methods<a class="headerlink" href="#convex-42_derivativefree-205-evolution-strategies-and-population-methods" title="Permanent link">¶</a></h2>
<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>
<h3 id="convex-42_derivativefree-2051-covariance-matrix-adaptation-evolution-strategy-cma-es">20.5.1 Covariance Matrix Adaptation Evolution Strategy (CMA-ES)<a class="headerlink" href="#convex-42_derivativefree-2051-covariance-matrix-adaptation-evolution-strategy-cma-es" title="Permanent link">¶</a></h3>
<p>CMA-ES is a powerful stochastic search algorithm.<br>
It iteratively samples new points from a multivariate Gaussian distribution:
<script type="math/tex; mode=display">
x^{(i)}_{k+1} \sim \mathcal{N}(m_k, \sigma_k^2 C_k),
</script>
where <span class="arithmatex">\(m_k\)</span> is the current mean, <span class="arithmatex">\(\sigma_k\)</span> the global step-size, and <span class="arithmatex">\(C_k\)</span> the covariance matrix.</p>
<p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p>
<p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>
<hr>
<h3 id="convex-42_derivativefree-2052-differential-evolution-de">20.5.2 Differential Evolution (DE)<a class="headerlink" href="#convex-42_derivativefree-2052-differential-evolution-de" title="Permanent link">¶</a></h3>
<p>DE evolves a population <span class="arithmatex">\(\{x_i\}\)</span> via vector differences:
<script type="math/tex; mode=display">
v_i = x_{r1} + F(x_{r2} - x_{r3}),
</script>
<script type="math/tex; mode=display">
u_i = \text{crossover}(x_i, v_i),
</script>
<script type="math/tex; mode=display">
x_i' = \begin{cases} 
u_i, & \text{if } f(u_i) < f(x_i),\\
x_i, & \text{otherwise.}
\end{cases}
</script>
where <span class="arithmatex">\(r1, r2, r3\)</span> are random distinct indices and <span class="arithmatex">\(F\)</span> controls mutation strength.</p>
<p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>
<h2 id="convex-42_derivativefree-206-bayesian-optimization">20.6 Bayesian Optimization<a class="headerlink" href="#convex-42_derivativefree-206-bayesian-optimization" title="Permanent link">¶</a></h2>
<p>When function evaluations are <em>expensive</em> (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>
<h3 id="convex-42_derivativefree-2061-core-idea">20.6.1 Core Idea<a class="headerlink" href="#convex-42_derivativefree-2061-core-idea" title="Permanent link">¶</a></h3>
<p>Model the objective as a random function <span class="arithmatex">\(f(x) \sim \mathcal{GP}(m(x), k(x,x'))\)</span> (Gaussian Process prior).<br>
After each evaluation, update the posterior mean and variance to quantify uncertainty.</p>
<p>Use an acquisition function <span class="arithmatex">\(a(x)\)</span> to select the next evaluation point:
<script type="math/tex; mode=display">
x_{k+1} = \arg\max_x a(x),
</script>
balancing <em>exploration</em> (high uncertainty) and <em>exploitation</em> (low expected value).</p>
<p>Common acquisition functions:</p>
<ul>
<li>Expected Improvement (EI)</li>
<li>Probability of Improvement (PI)</li>
<li>Upper Confidence Bound (UCB)</li>
</ul>
<h3 id="convex-42_derivativefree-2062-surrogate-models-beyond-gaussian-processes">20.6.2 Surrogate Models Beyond Gaussian Processes<a class="headerlink" href="#convex-42_derivativefree-2062-surrogate-models-beyond-gaussian-processes" title="Permanent link">¶</a></h3>
<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs:
- Tree-structured Parzen Estimators (TPE)
- Random forests (SMAC)
- Neural network surrogates (Bayesian neural networks)</p>
<p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>
<h2 id="convex-42_derivativefree-207-hybrid-and-adaptive-approaches">20.7 Hybrid and Adaptive Approaches<a class="headerlink" href="#convex-42_derivativefree-207-hybrid-and-adaptive-approaches" title="Permanent link">¶</a></h2>
<p>Modern applications often combine derivative-free and gradient-based techniques:</p>
<ul>
<li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li>
<li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li>
<li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li>
</ul>
<p>Such hybridization reflects a pragmatic view: no single optimizer is best — adaptability matters most.</p>
<h2 id="convex-42_derivativefree-208-practical-considerations">20.8 Practical Considerations<a class="headerlink" href="#convex-42_derivativefree-208-practical-considerations" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Function evaluations expensive</td>
<td>Use Bayesian or model-based methods</td>
</tr>
<tr>
<td>Noisy evaluations</td>
<td>Use averaging, smoothing, or robust estimators</td>
</tr>
<tr>
<td>High dimension (<span class="arithmatex">\(n &gt; 50\)</span>)</td>
<td>Prefer CMA-ES or evolutionary strategies</td>
</tr>
<tr>
<td>Box constraints</td>
<td>Methods like BOBYQA, DE, or PSO</td>
</tr>
<tr>
<td>Parallel computation available</td>
<td>Population-based methods excel</td>
</tr>
</tbody>
</table>
<hr>
<p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize <em>anything we can evaluate</em>. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p>
<p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors — turning randomness into a powerful search strategy.</p></body></html></section><section class="print-page" id="convex-44_metaheuristic" heading-number="2.21"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-21-metaheuristic-and-evolutionary-algorithms">Chapter 21: Metaheuristic and Evolutionary Algorithms<a class="headerlink" href="#convex-44_metaheuristic-chapter-21-metaheuristic-and-evolutionary-algorithms" title="Permanent link">¶</a></h1>
<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions.<br>
In these settings, metaheuristic algorithms—inspired by nature, biology, and collective behavior—provide robust and flexible alternatives.</p>
<p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas.<br>
They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>
<h2 id="convex-44_metaheuristic-211-principles-of-metaheuristic-optimization">21.1 Principles of Metaheuristic Optimization<a class="headerlink" href="#convex-44_metaheuristic-211-principles-of-metaheuristic-optimization" title="Permanent link">¶</a></h2>
<p>All metaheuristics share three key principles:</p>
<ol>
<li>
<p>Population-Based Search:<br>
   Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p>
</li>
<li>
<p>Variation Operators:<br>
   Create new solutions via mutation, recombination, or stochastic perturbations.</p>
</li>
<li>
<p>Selection and Adaptation:<br>
   Favor candidates with better objective values, guiding the search toward promising regions.</p>
</li>
</ol>
<p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>
<h2 id="convex-44_metaheuristic-212-genetic-algorithms-ga">21.2 Genetic Algorithms (GA)<a class="headerlink" href="#convex-44_metaheuristic-212-genetic-algorithms-ga" title="Permanent link">¶</a></h2>
<h3 id="convex-44_metaheuristic-2121-biological-inspiration">21.2.1 Biological Inspiration<a class="headerlink" href="#convex-44_metaheuristic-2121-biological-inspiration" title="Permanent link">¶</a></h3>
<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>
<h3 id="convex-44_metaheuristic-2122-representation">21.2.2 Representation<a class="headerlink" href="#convex-44_metaheuristic-2122-representation" title="Permanent link">¶</a></h3>
<p>A solution (individual) is represented as a chromosome—often a binary string, vector of reals, or permutation.<br>
Each position (gene) encodes part of the decision variable.</p>
<h3 id="convex-44_metaheuristic-2123-algorithm-outline">21.2.3 Algorithm Outline<a class="headerlink" href="#convex-44_metaheuristic-2123-algorithm-outline" title="Permanent link">¶</a></h3>
<ol>
<li>Initialize a population <span class="arithmatex">\(\{x_i\}_{i=1}^N\)</span> randomly.  </li>
<li>Evaluate fitness <span class="arithmatex">\(f(x_i)\)</span> for all individuals.  </li>
<li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li>
<li>
<p>Apply:</p>
<ul>
<li>Crossover: combine genetic material of two parents.  </li>
<li>Mutation: randomly alter some genes to maintain diversity.  </li>
</ul>
</li>
<li>
<p>Form a new population and repeat until convergence.</p>
</li>
</ol>
<h3 id="convex-44_metaheuristic-2124-crossover-and-mutation-examples">21.2.4 Crossover and Mutation Examples<a class="headerlink" href="#convex-44_metaheuristic-2124-crossover-and-mutation-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Single-point crossover: exchange genes after a random index.  </li>
<li>Gaussian mutation: add small noise to continuous parameters.  </li>
</ul>
<h3 id="convex-44_metaheuristic-2125-strengths-and-weaknesses">21.2.5 Strengths and Weaknesses<a class="headerlink" href="#convex-44_metaheuristic-2125-strengths-and-weaknesses" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Highly parallel, robust, domain-independent</td>
<td>Requires many function evaluations</td>
</tr>
<tr>
<td>Effective for combinatorial and discrete optimization</td>
<td>Parameter tuning (mutation, crossover rates) is nontrivial</td>
</tr>
</tbody>
</table>
<h2 id="convex-44_metaheuristic-213-differential-evolution-de">21.3 Differential Evolution (DE)<a class="headerlink" href="#convex-44_metaheuristic-213-differential-evolution-de" title="Permanent link">¶</a></h2>
<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>
<h3 id="convex-44_metaheuristic-2131-core-idea">21.3.1 Core Idea<a class="headerlink" href="#convex-44_metaheuristic-2131-core-idea" title="Permanent link">¶</a></h3>
<p>Mutation is performed using differences of population members:
<script type="math/tex; mode=display">
v_i = x_{r1} + F(x_{r2} - x_{r3}),
</script>
where <span class="arithmatex">\(r1, r2, r3\)</span> are random distinct indices and <span class="arithmatex">\(F \in [0,2]\)</span> controls mutation amplitude.</p>
<p>Then crossover forms trial vectors:
<script type="math/tex; mode=display">
u_i = \text{crossover}(x_i, v_i),
</script>
and selection chooses between <span class="arithmatex">\(x_i\)</span> and <span class="arithmatex">\(u_i\)</span> based on objective value.</p>
<h3 id="convex-44_metaheuristic-2132-features">21.3.2 Features<a class="headerlink" href="#convex-44_metaheuristic-2132-features" title="Permanent link">¶</a></h3>
<ul>
<li>Self-adaptive exploration of the search space.</li>
<li>Suitable for continuous, multimodal functions.</li>
<li>Simple to implement, with few control parameters.</li>
</ul>
<h2 id="convex-44_metaheuristic-214-particle-swarm-optimization-pso">21.4 Particle Swarm Optimization (PSO)<a class="headerlink" href="#convex-44_metaheuristic-214-particle-swarm-optimization-pso" title="Permanent link">¶</a></h2>
<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p>
<p>Each particle <span class="arithmatex">\(i\)</span> has position <span class="arithmatex">\(x_i\)</span> and velocity <span class="arithmatex">\(v_i\)</span>, updated as:
<script type="math/tex; mode=display">
v_i \leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i),
</script>
<script type="math/tex; mode=display">
x_i \leftarrow x_i + v_i,
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(p_i\)</span> = personal best position of particle <span class="arithmatex">\(i\)</span>,</li>
<li><span class="arithmatex">\(g\)</span> = best global position found by the swarm,</li>
<li><span class="arithmatex">\(w\)</span>, <span class="arithmatex">\(c_1\)</span>, <span class="arithmatex">\(c_2\)</span> are weight and learning coefficients,</li>
<li><span class="arithmatex">\(r_1\)</span>, <span class="arithmatex">\(r_2\)</span> are random numbers in <span class="arithmatex">\([0,1]\)</span>.</li>
</ul>
<p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>
<h3 id="convex-44_metaheuristic-2141-convergence-behavior">21.4.1 Convergence Behavior<a class="headerlink" href="#convex-44_metaheuristic-2141-convergence-behavior" title="Permanent link">¶</a></h3>
<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>
<h3 id="convex-44_metaheuristic-2142-strengths">21.4.2 Strengths<a class="headerlink" href="#convex-44_metaheuristic-2142-strengths" title="Permanent link">¶</a></h3>
<ul>
<li>Few parameters, easy to implement.</li>
<li>Works well for noisy or discontinuous problems.</li>
<li>Naturally parallelizable.</li>
</ul>
<h2 id="convex-44_metaheuristic-215-simulated-annealing-sa">21.5 Simulated Annealing (SA)<a class="headerlink" href="#convex-44_metaheuristic-215-simulated-annealing-sa" title="Permanent link">¶</a></h2>
<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy — a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual “cooling” in the search for a global minimum.</p>
<h3 id="convex-44_metaheuristic-2151-physical-analogy">21.5.1 Physical Analogy<a class="headerlink" href="#convex-44_metaheuristic-2151-physical-analogy" title="Permanent link">¶</a></h3>
<p>In thermodynamics, a system at temperature <span class="arithmatex">\(T\)</span> has probability of occupying a state with energy <span class="arithmatex">\(E\)</span> given by the Boltzmann distribution:</p>
<div class="arithmatex">\[
P(E) \propto e^{-E / (kT)}.
\]</div>
<p>At high temperature, the system freely explores many states. As <span class="arithmatex">\(T\)</span> decreases, it becomes increasingly likely to remain near states of minimal energy.</p>
<p>Simulated Annealing maps this principle to optimization by treating:</p>
<ul>
<li>The objective function <span class="arithmatex">\(f(x)\)</span> as the system’s energy.</li>
<li>The solution vector <span class="arithmatex">\(x\)</span> as a configuration.</li>
<li>The temperature <span class="arithmatex">\(T\)</span> as a control parameter determining randomness.</li>
</ul>
<h3 id="convex-44_metaheuristic-2152-algorithm-outline">21.5.2 Algorithm Outline<a class="headerlink" href="#convex-44_metaheuristic-2152-algorithm-outline" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Initialization</p>
<ul>
<li>Choose an initial solution <span class="arithmatex">\(x_0\)</span> and initial temperature <span class="arithmatex">\(T_0\)</span>.</li>
<li>Set a cooling schedule <span class="arithmatex">\(T_{k+1} = \alpha T_k\)</span>, with <span class="arithmatex">\(\alpha \in (0,1)\)</span>.</li>
</ul>
</li>
<li>
<p>Iteration</p>
<ul>
<li>Generate a candidate <span class="arithmatex">\(x'\)</span> from <span class="arithmatex">\(x_k\)</span> via a small random perturbation.</li>
<li>Compute <span class="arithmatex">\(\Delta f = f(x') - f(x_k)\)</span>.</li>
<li>Accept or reject based on the Metropolis criterion:</li>
</ul>
<p>
<script type="math/tex; mode=display">
 P_{\text{accept}} = 
 \begin{cases}
 1, & \text{if } \Delta f \le 0, \\
 e^{-\Delta f / T_k}, & \text{if } \Delta f > 0.
 \end{cases}
 </script>
</p>
</li>
<li>
<p>Cooling</p>
<ul>
<li>
<p>Reduce the temperature gradually according to the schedule.</p>
</li>
<li>
<p>Repeat until <span class="arithmatex">\(T\)</span> becomes sufficiently small or the system stabilizes.</p>
</li>
</ul>
</li>
</ol>
<h3 id="convex-44_metaheuristic-2153-interpretation">21.5.3 Interpretation<a class="headerlink" href="#convex-44_metaheuristic-2153-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>At high temperatures, SA accepts both better and worse moves → exploration.  </p>
</li>
<li>
<p>At low temperatures, it becomes increasingly selective → exploitation.</p>
</li>
</ul>
<p>This balance allows SA to escape local minima and approach the global optimum over time.</p>
<h3 id="convex-44_metaheuristic-2154-cooling-schedules">21.5.4 Cooling Schedules<a class="headerlink" href="#convex-44_metaheuristic-2154-cooling-schedules" title="Permanent link">¶</a></h3>
<p>The temperature schedule determines convergence quality:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Formula</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>Exponential</td>
<td><span class="arithmatex">\(T_{k+1} = \alpha T_k\)</span></td>
<td>Simple, widely used</td>
</tr>
<tr>
<td>Linear</td>
<td><span class="arithmatex">\(T_{k+1} = T_0 - \beta k\)</span></td>
<td>Faster cooling, less exploration</td>
</tr>
<tr>
<td>Logarithmic</td>
<td><span class="arithmatex">\(T_k = \frac{T_0}{\log(k + c)}\)</span></td>
<td>Theoretically convergent (slow)</td>
</tr>
<tr>
<td>Adaptive</td>
<td>Adjust based on recent acceptance rates</td>
<td>Practical and self-tuning</td>
</tr>
</tbody>
</table>
<p>A slower cooling schedule improves accuracy but increases computational cost.</p>
<h2 id="convex-44_metaheuristic-216-ant-colony-optimization-aco">21.6 Ant Colony Optimization (ACO)<a class="headerlink" href="#convex-44_metaheuristic-216-ant-colony-optimization-aco" title="Permanent link">¶</a></h2>
<h3 id="convex-44_metaheuristic-2161-biological-basis">21.6.1 Biological Basis<a class="headerlink" href="#convex-44_metaheuristic-2161-biological-basis" title="Permanent link">¶</a></h3>
<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p>
<p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity <span class="arithmatex">\(\tau_{ij}\)</span> and heuristic visibility <span class="arithmatex">\(\eta_{ij}\)</span>:
<script type="math/tex; mode=display">
P_{ij} = \frac{[\tau_{ij}]^\alpha [\eta_{ij}]^\beta}{\sum_k [\tau_{ik}]^\alpha [\eta_{ik}]^\beta}.
</script>
</p>
<h3 id="convex-44_metaheuristic-2162-pheromone-update">21.6.2 Pheromone Update<a class="headerlink" href="#convex-44_metaheuristic-2162-pheromone-update" title="Permanent link">¶</a></h3>
<p>After all ants construct their tours:
<script type="math/tex; mode=display">
\tau_{ij} \leftarrow (1 - \rho)\tau_{ij} + \sum_{\text{ants}} \Delta \tau_{ij},
</script>
where <span class="arithmatex">\(\rho\)</span> controls evaporation and <span class="arithmatex">\(\Delta\tau_{ij}\)</span> reinforces paths used by good solutions.</p>
<p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>
<h2 id="convex-44_metaheuristic-217-exploration-vs-exploitation">21.7 Exploration vs. Exploitation<a class="headerlink" href="#convex-44_metaheuristic-217-exploration-vs-exploitation" title="Permanent link">¶</a></h2>
<p>Every metaheuristic must balance:
- Exploration: sampling diverse regions to escape local minima.<br>
- Exploitation: refining known good solutions to reach local optima.</p>
<table>
<thead>
<tr>
<th>High Exploration</th>
<th>High Exploitation</th>
</tr>
</thead>
<tbody>
<tr>
<td>GA with strong mutation</td>
<td>PSO with low inertia</td>
</tr>
<tr>
<td>DE with high <span class="arithmatex">\(F\)</span></td>
<td>ACO with low evaporation rate</td>
</tr>
<tr>
<td>Random restarts</td>
<td>Local refinement</td>
</tr>
</tbody>
</table>
<p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>
<h2 id="convex-44_metaheuristic-218-hybrid-and-memetic-algorithms">21.8 Hybrid and Memetic Algorithms<a class="headerlink" href="#convex-44_metaheuristic-218-hybrid-and-memetic-algorithms" title="Permanent link">¶</a></h2>
<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p>
<p>Example:</p>
<ol>
<li>Use PSO or GA to explore broadly.  </li>
<li>Apply gradient descent or Nelder–Mead locally near promising candidates.</li>
</ol>
<p>This hybridization often yields faster convergence and improved accuracy.</p>
<h2 id="convex-44_metaheuristic-219-performance-and-practical-tips">21.9 Performance and Practical Tips<a class="headerlink" href="#convex-44_metaheuristic-219-performance-and-practical-tips" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialization</td>
<td>Use wide, random distributions to promote diversity</td>
</tr>
<tr>
<td>Parameter Tuning</td>
<td>Use adaptive schedules (e.g., cooling, inertia decay)</td>
</tr>
<tr>
<td>Population Size</td>
<td>Larger for global search, smaller for fine-tuning</td>
</tr>
<tr>
<td>Parallelism</td>
<td>Evaluate populations concurrently for efficiency</td>
</tr>
<tr>
<td>Stopping Criteria</td>
<td>Use both iteration limits and stagnation detection</td>
</tr>
</tbody>
</table>
<p>Metaheuristics are heuristic by design — they do not guarantee global optimality, but offer practical success across many fields.</p>
<hr>
<p>Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p>
<p>In the next chapter, we turn to modern stochastic optimizers that bridge theoretical foundations and practical success in machine learning—methods like Adam, RMSProp, and Lion that dominate large-scale nonconvex optimization.</p></body></html></section><section class="print-page" id="convex-48_advanced_combinatorial" heading-number="2.22"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-22-advanced-topics-in-combinatorial-optimization">Chapter 22: Advanced Topics in Combinatorial Optimization<a class="headerlink" href="#convex-48_advanced_combinatorial-chapter-22-advanced-topics-in-combinatorial-optimization" title="Permanent link">¶</a></h1>
<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p>
<p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>
<h2 id="convex-48_advanced_combinatorial-221-nature-of-combinatorial-problems">22.1 Nature of Combinatorial Problems<a class="headerlink" href="#convex-48_advanced_combinatorial-221-nature-of-combinatorial-problems" title="Permanent link">¶</a></h2>
<p>A combinatorial optimization problem can be expressed as:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{F}} f(x),
\]</div>
<p>where <span class="arithmatex">\(\mathcal{F}\)</span> is a finite or countable set of feasible solutions, often exponentially large in size.</p>
<p>Example forms include:</p>
<ul>
<li>Binary decisions: <span class="arithmatex">\(x_i \in \{0,1\}\)</span></li>
<li>Integer constraints: <span class="arithmatex">\(x_i \in \mathbb{Z}\)</span></li>
<li>Permutations: ordering or ranking elements</li>
</ul>
<p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>
<h2 id="convex-48_advanced_combinatorial-222-graph-theoretic-foundations">22.2 Graph-Theoretic Foundations<a class="headerlink" href="#convex-48_advanced_combinatorial-222-graph-theoretic-foundations" title="Permanent link">¶</a></h2>
<p>Many combinatorial problems are naturally represented as graphs <span class="arithmatex">\(G = (V, E)\)</span>.</p>
<h3 id="convex-48_advanced_combinatorial-2221-shortest-path-problem">22.2.1 Shortest Path Problem<a class="headerlink" href="#convex-48_advanced_combinatorial-2221-shortest-path-problem" title="Permanent link">¶</a></h3>
<p>Given edge weights <span class="arithmatex">\(w_{ij}\)</span>, find a path from <span class="arithmatex">\(s\)</span> to <span class="arithmatex">\(t\)</span> minimizing total weight:
<script type="math/tex; mode=display">
\min_{\text{path } P} \sum_{(i,j)\in P} w_{ij}.
</script>
Efficiently solvable by Dijkstra’s or Bellman–Ford algorithms.</p>
<h3 id="convex-48_advanced_combinatorial-2222-minimum-spanning-tree-mst">22.2.2 Minimum Spanning Tree (MST)<a class="headerlink" href="#convex-48_advanced_combinatorial-2222-minimum-spanning-tree-mst" title="Permanent link">¶</a></h3>
<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal’s or Prim’s algorithm in <span class="arithmatex">\(O(E\log V)\)</span> time.</p>
<h3 id="convex-48_advanced_combinatorial-2223-maximum-flow-minimum-cut">22.2.3 Maximum Flow / Minimum Cut<a class="headerlink" href="#convex-48_advanced_combinatorial-2223-maximum-flow-minimum-cut" title="Permanent link">¶</a></h3>
<p>Determine how much “flow” can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>
<h2 id="convex-48_advanced_combinatorial-223-integer-linear-programming-ilp">22.3 Integer Linear Programming (ILP)<a class="headerlink" href="#convex-48_advanced_combinatorial-223-integer-linear-programming-ilp" title="Permanent link">¶</a></h2>
<p>An integer program seeks:
<script type="math/tex; mode=display">
\min_x \; c^\top x \quad \text{s.t. } A x \le b, \; x \in \mathbb{Z}^n.
</script>
</p>
<p>It generalizes many classical problems:</p>
<ul>
<li>Knapsack  </li>
<li>Assignment  </li>
<li>Scheduling  </li>
<li>Facility location</li>
</ul>
<p>Relaxing <span class="arithmatex">\(x \in \mathbb{Z}^n\)</span> to <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>
<h2 id="convex-48_advanced_combinatorial-224-relaxation-and-rounding">22.4 Relaxation and Rounding<a class="headerlink" href="#convex-48_advanced_combinatorial-224-relaxation-and-rounding" title="Permanent link">¶</a></h2>
<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>
<h3 id="convex-48_advanced_combinatorial-2241-lp-relaxation">22.4.1 LP Relaxation<a class="headerlink" href="#convex-48_advanced_combinatorial-2241-lp-relaxation" title="Permanent link">¶</a></h3>
<p>For binary variables <span class="arithmatex">\(x_i \in \{0,1\}\)</span>, relax to <span class="arithmatex">\(0 \le x_i \le 1\)</span> and solve via simplex or interior-point methods.</p>
<h3 id="convex-48_advanced_combinatorial-2242-semidefinite-relaxation">22.4.2 Semidefinite Relaxation<a class="headerlink" href="#convex-48_advanced_combinatorial-2242-semidefinite-relaxation" title="Permanent link">¶</a></h3>
<p>For quadratic binary problems, lift to a positive semidefinite matrix <span class="arithmatex">\(X = xx^\top\)</span>:
<script type="math/tex; mode=display">
\min \langle C, X \rangle \quad \text{s.t. } X_{ii} = 1, \; X \succeq 0.
</script>
Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>
<h3 id="convex-48_advanced_combinatorial-2243-randomized-rounding">22.4.3 Randomized Rounding<a class="headerlink" href="#convex-48_advanced_combinatorial-2243-randomized-rounding" title="Permanent link">¶</a></h3>
<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>
<h2 id="convex-48_advanced_combinatorial-225-branch-and-bound-and-search-trees">22.5 Branch-and-Bound and Search Trees<a class="headerlink" href="#convex-48_advanced_combinatorial-225-branch-and-bound-and-search-trees" title="Permanent link">¶</a></h2>
<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>
<h3 id="convex-48_advanced_combinatorial-2251-basic-principle">22.5.1 Basic Principle<a class="headerlink" href="#convex-48_advanced_combinatorial-2251-basic-principle" title="Permanent link">¶</a></h3>
<ol>
<li>Partition the feasible set into subsets (branching).  </li>
<li>Compute upper/lower bounds for each subset.  </li>
<li>Prune branches that cannot contain the optimum.  </li>
</ol>
<p>The algorithm systematically explores a search tree, guided by bounds.</p>
<h3 id="convex-48_advanced_combinatorial-2252-bounding-via-relaxations">22.5.2 Bounding via Relaxations<a class="headerlink" href="#convex-48_advanced_combinatorial-2252-bounding-via-relaxations" title="Permanent link">¶</a></h3>
<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>
<h2 id="convex-48_advanced_combinatorial-226-dynamic-programming">22.6 Dynamic Programming<a class="headerlink" href="#convex-48_advanced_combinatorial-226-dynamic-programming" title="Permanent link">¶</a></h2>
<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p>
<div class="arithmatex">\[
\text{OPT}(S) = \min_{x \in S} \{ c(x) + \text{OPT}(S') \}.
\]</div>
<p>It is exact but can suffer from exponential growth (“curse of dimensionality”).</p>
<p>Applications:</p>
<ul>
<li>Shortest paths</li>
<li>Sequence alignment</li>
<li>Knapsack</li>
<li>Resource allocation</li>
</ul>
<p>DP offers exact solutions when structure allows sequential decomposition.</p>
<h2 id="convex-48_advanced_combinatorial-227-heuristics-and-metaheuristics-for-combinatorial-problems">22.7 Heuristics and Metaheuristics for Combinatorial Problems<a class="headerlink" href="#convex-48_advanced_combinatorial-227-heuristics-and-metaheuristics-for-combinatorial-problems" title="Permanent link">¶</a></h2>
<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>
<h3 id="convex-48_advanced_combinatorial-2271-greedy-heuristics">22.7.1 Greedy Heuristics<a class="headerlink" href="#convex-48_advanced_combinatorial-2271-greedy-heuristics" title="Permanent link">¶</a></h3>
<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal’s MST). Fast but not always globally optimal.</p>
<h3 id="convex-48_advanced_combinatorial-2272-local-search-and-hill-climbing">22.7.2 Local Search and Hill Climbing<a class="headerlink" href="#convex-48_advanced_combinatorial-2272-local-search-and-hill-climbing" title="Permanent link">¶</a></h3>
<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>
<h3 id="convex-48_advanced_combinatorial-2273-metaheuristic-extensions">22.7.3 Metaheuristic Extensions<a class="headerlink" href="#convex-48_advanced_combinatorial-2273-metaheuristic-extensions" title="Permanent link">¶</a></h3>
<ul>
<li>Simulated Annealing: controlled random acceptance of worse moves.  </li>
<li>Tabu Search: memory-based diversification.  </li>
<li>Ant Colony Optimization: probabilistic path construction.  </li>
<li>Genetic Algorithms and PSO: population-based evolution.  </li>
</ul>
<p>These approaches generalize to discrete structures with minimal problem-specific design.</p>
<h2 id="convex-48_advanced_combinatorial-228-approximation-algorithms">22.8 Approximation Algorithms<a class="headerlink" href="#convex-48_advanced_combinatorial-228-approximation-algorithms" title="Permanent link">¶</a></h2>
<p>Some combinatorial problems are provably intractable but allow approximation guarantees:
<script type="math/tex; mode=display">
f(x_{\text{approx}}) \le \alpha \, f(x^*),
</script>
where <span class="arithmatex">\(\alpha \ge 1\)</span> is the approximation ratio.</p>
<p>Examples:</p>
<ul>
<li>Greedy Set Cover: <span class="arithmatex">\(\alpha = \ln n + 1\)</span>  </li>
<li>Christofides’ Algorithm for TSP: <span class="arithmatex">\(\alpha = 1.5\)</span>  </li>
<li>MAX-CUT SDP Relaxation: <span class="arithmatex">\(\alpha \approx 0.878\)</span></li>
</ul>
<p>Approximation theory blends combinatorics with convex relaxation insights.</p>
<h2 id="convex-48_advanced_combinatorial-229-advanced-topics-constraint-programming-and-decomposition">22.9 Advanced Topics: Constraint Programming and Decomposition<a class="headerlink" href="#convex-48_advanced_combinatorial-229-advanced-topics-constraint-programming-and-decomposition" title="Permanent link">¶</a></h2>
<h3 id="convex-48_advanced_combinatorial-2291-constraint-programming-cp">22.9.1 Constraint Programming (CP)<a class="headerlink" href="#convex-48_advanced_combinatorial-2291-constraint-programming-cp" title="Permanent link">¶</a></h3>
<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>
<h3 id="convex-48_advanced_combinatorial-2292-benders-and-dantzigwolfe-decomposition">22.9.2 Benders and Dantzig–Wolfe Decomposition<a class="headerlink" href="#convex-48_advanced_combinatorial-2292-benders-and-dantzigwolfe-decomposition" title="Permanent link">¶</a></h3>
<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>
<h3 id="convex-48_advanced_combinatorial-2293-cutting-plane-methods">22.9.3 Cutting Plane Methods<a class="headerlink" href="#convex-48_advanced_combinatorial-2293-cutting-plane-methods" title="Permanent link">¶</a></h3>
<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>
<h2 id="convex-48_advanced_combinatorial-2210-applications-across-domains">22.10 Applications Across Domains<a class="headerlink" href="#convex-48_advanced_combinatorial-2210-applications-across-domains" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Field</th>
<th>Combinatorial Problem Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistics</td>
<td>Vehicle routing, warehouse layout</td>
</tr>
<tr>
<td>Telecommunications</td>
<td>Network design, channel allocation</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Feature selection, clustering, model compression</td>
</tr>
<tr>
<td>Finance</td>
<td>Portfolio optimization with integer positions</td>
</tr>
<tr>
<td>Bioinformatics</td>
<td>Genome assembly, protein structure inference</td>
</tr>
</tbody>
</table>
<p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p>
<hr>
<p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p></body></html></section><section class="print-page" id="convex-50_future" heading-number="2.23"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence">Chapter 23: The Future of Optimization — Learning, Adaptation, and Intelligence<a class="headerlink" href="#convex-50_future-chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence" title="Permanent link">¶</a></h1>
<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead — toward optimization methods that learn, adapt, and reason — merging human insight, data-driven modeling, and algorithmic intelligence.</p>
<h2 id="convex-50_future-231-from-fixed-algorithms-to-adaptive-systems">23.1 From Fixed Algorithms to Adaptive Systems<a class="headerlink" href="#convex-50_future-231-from-fixed-algorithms-to-adaptive-systems" title="Permanent link">¶</a></h2>
<p>Traditional optimization algorithms are designed by experts and fixed in form:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha_k \nabla f(x_k),
\]</div>
<p>or</p>
<div class="arithmatex">\[
x_{k+1} = \text{Update}(x_k, \nabla f(x_k); \theta_{\text{fixed}}).
\]</div>
<p>But real-world problems change over time — data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>
<h2 id="convex-50_future-232-optimization-as-learning">23.2 Optimization as Learning<a class="headerlink" href="#convex-50_future-232-optimization-as-learning" title="Permanent link">¶</a></h2>
<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p>
<p>A meta-optimizer <span class="arithmatex">\(\text{Opt}_\theta\)</span> is parameterized by <span class="arithmatex">\(\theta\)</span>, and trained to minimize:</p>
<div class="arithmatex">\[
\mathcal{L}(\theta) = \mathbb{E}_{f \sim \mathcal{D}}[f(\text{Opt}_\theta(f))],
\]</div>
<p>where <span class="arithmatex">\(\mathcal{D}\)</span> is a distribution over problem instances.</p>
<p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>
<h2 id="convex-50_future-233-reinforcement-learned-optimization">23.3 Reinforcement-Learned Optimization<a class="headerlink" href="#convex-50_future-233-reinforcement-learned-optimization" title="Permanent link">¶</a></h2>
<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p>
<p>At each iteration:</p>
<ul>
<li>State: current iterate <span class="arithmatex">\(x_t\)</span>, gradient <span class="arithmatex">\(\nabla f(x_t)\)</span>, and loss <span class="arithmatex">\(f(x_t)\)</span>  </li>
<li>Action: choose an update <span class="arithmatex">\(\Delta x_t\)</span>  </li>
<li>Reward: improvement in objective, <span class="arithmatex">\(r_t = -[f(x_{t+1}) - f(x_t)]\)</span></li>
</ul>
<p>A policy <span class="arithmatex">\(\pi_\theta\)</span> learns to output update steps that maximize expected reward.<br>
This creates an optimizer that discovers efficient update strategies through experience.</p>
<p>RL-based optimizers have been successfully applied in:</p>
<ul>
<li>Hyperparameter tuning  </li>
<li>Neural architecture search  </li>
<li>Online control systems  </li>
<li>Adaptive sampling and scheduling</li>
</ul>
<h2 id="convex-50_future-234-neuroevolution-and-population-learning">23.4 Neuroevolution and Population Learning<a class="headerlink" href="#convex-50_future-234-neuroevolution-and-population-learning" title="Permanent link">¶</a></h2>
<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly.<br>
Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p>
<p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p>
<div class="arithmatex">\[
\nabla_\theta \mathbb{E}[f(\theta)] \approx \frac{1}{\sigma} \mathbb{E}[f(\theta + \sigma \epsilon)\epsilon].
\]</div>
<p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration–exploitation.</p>
<h2 id="convex-50_future-235-optimization-and-generative-models">23.5 Optimization and Generative Models<a class="headerlink" href="#convex-50_future-235-optimization-and-generative-models" title="Permanent link">¶</a></h2>
<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective:<br>
Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p>
<p>For example:</p>
<ul>
<li>Optimize a latent vector <span class="arithmatex">\(z\)</span> to generate a design with desired properties.  </li>
<li>Use differentiable surrogates to backpropagate through generative pipelines.  </li>
<li>Apply gradient-based search within learned manifolds.</li>
</ul>
<p>This blending of optimization and generation enables creativity — from molecule design to engineering shape synthesis.</p>
<h2 id="convex-50_future-236-federated-and-decentralized-optimization">23.6 Federated and Decentralized Optimization<a class="headerlink" href="#convex-50_future-236-federated-and-decentralized-optimization" title="Permanent link">¶</a></h2>
<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization.<br>
Each client <span class="arithmatex">\(i\)</span> holds local data <span class="arithmatex">\(D_i\)</span> and solves:</p>
<div class="arithmatex">\[
\min_x \; F(x) = \frac{1}{N}\sum_i f_i(x),
\]</div>
<p>without sharing raw data.</p>
<p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p>
<p>Challenges include:</p>
<ul>
<li>Communication efficiency  </li>
<li>Heterogeneity of data and computation  </li>
<li>Privacy and fairness constraints</li>
</ul>
<h2 id="convex-50_future-237-optimization-under-uncertainty">23.7 Optimization Under Uncertainty<a class="headerlink" href="#convex-50_future-237-optimization-under-uncertainty" title="Permanent link">¶</a></h2>
<p>Modern systems often face uncertain environments:
- Random perturbations in data<br>
- Dynamic constraints<br>
- Unpredictable feedback</p>
<p>Approaches to manage uncertainty include:</p>
<ol>
<li>
<p>Robust Optimization:<br>
   Minimize worst-case loss under bounded perturbations:
   <script type="math/tex; mode=display">
   \min_x \max_{\delta \in \Delta} f(x + \delta).
   </script>
</p>
</li>
<li>
<p>Stochastic Programming:<br>
   Optimize expected value or risk measure:
   <script type="math/tex; mode=display">
   \min_x \mathbb{E}_\xi[f(x, \xi)].
   </script>
</p>
</li>
<li>
<p>Distributionally Robust Optimization (DRO):<br>
   Hedge against model misspecification by optimizing over nearby probability distributions.</p>
</li>
</ol>
<p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>
<h2 id="convex-50_future-238-quantum-and-analog-optimization">23.8 Quantum and Analog Optimization<a class="headerlink" href="#convex-50_future-238-quantum-and-analog-optimization" title="Permanent link">¶</a></h2>
<p>As hardware advances, new paradigms emerge:
- Quantum Annealing: uses quantum tunneling to escape local minima.
- Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem.
- Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p>
<p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>
<h2 id="convex-50_future-239-optimization-and-intelligence">23.9 Optimization and Intelligence<a class="headerlink" href="#convex-50_future-239-optimization-and-intelligence" title="Permanent link">¶</a></h2>
<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes — in essence, optimization loops.</p>
<p>Emerging research seeks to unify:</p>
<ul>
<li>Learning to optimize — algorithms that adapt through data.  </li>
<li>Optimizing to learn — systems that adjust representations via optimization.  </li>
<li>Self-improving optimizers — algorithms that recursively tune their own parameters.</li>
</ul>
<p>This convergence blurs the line between <em>optimizer</em> and <em>learner</em>.</p>
<p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve — autonomously, efficiently, and creatively.</p>
<p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems — and thinkers — to improve themselves.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-3' heading-number='3'>
                        <h1>Reinforcement Learning<a class='headerlink' href='#section-3' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="reinforcement-1_intro" heading-number="3.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-to-reinforcement-learning">Chapter 1: Introduction to Reinforcement Learning<a class="headerlink" href="#reinforcement-1_intro-chapter-1-introduction-to-reinforcement-learning" title="Permanent link">¶</a></h1>
<p>Reinforcement Learning is a paradigm in machine learning where an agent learns to make sequential decisions through interaction with an environment. Unlike supervised learning, where the agent learns from labeled examples, or unsupervised learning, where it learns patterns from unlabeled data, reinforcement learning is driven by the goal of maximizing cumulative reward through trial and error. The agent is not told which actions to take but must discover them by exploring the consequences of its actions.</p>
<p>Sequential decision-making under uncertainty is at the heart of reinforcement learning. The agent must balance exploration and exploitation. Exploration is needed to gather information about the environment, while exploitation uses this information to select actions that appear best.</p>
<div class="highlight"><pre><span></span><code> RL vs Supervised Learning (Key Differences)
       ┌─────────────────────────────────────────────────────┐
       │ Supervised Learning:                                │
       │   – Learns from labeled examples (input → target)   │
       │   – Feedback is immediate and correct               │
       │   – IID data; no sequential dependence              │
       │                                                     │
       │ Reinforcement Learning:                             │
       │   – Learns from interaction (trial &amp; error)         │
       │   – Feedback (reward) may be delayed or sparse      │
       │   – Data depends on agent's actions (non-IID)       │
       │   – Must balance exploration vs exploitation        │
       │   – Must solve temporal credit assignment           │
       └─────────────────────────────────────────────────────┘
</code></pre></div>
<p>A key characteristic of reinforcement learning is that the outcome of an action may not be immediately known. Rewards can be delayed, making it hard to determine which past actions are responsible for future outcomes. This challenge is known as temporal credit assignment. Successful reinforcement learning algorithms must learn to attribute long-term consequences to earlier decisions.</p>
<p>At each time step, the agent observes some representation of the world, takes an action, and receives a reward. The world then transitions to a new state. This interaction continues over time, forming an experience trajectory:</p>
<div class="arithmatex">\[
s_0, a_0, r_1, s_1, a_1, r_2, \dots
\]</div>
<p>The agent’s goal is to learn a policy, which is a mapping from states to actions, that maximizes the total reward it collects over time.</p>
<p>The total future reward is defined through the notion of return. The most common formulation is the discounted return:</p>
<div class="arithmatex">\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]</div>
<p>where <span class="arithmatex">\(0 \le \gamma \le 1\)</span> is called the discount factor. It determines how much the agent values immediate rewards compared to future rewards. A smaller <span class="arithmatex">\(\gamma\)</span> encourages short-term decisions, while a larger <span class="arithmatex">\(\gamma\)</span> favors long-term planning.</p>
<p>Reinforcement learning involves four fundamental challenges:</p>
<ol>
<li>Optimization: The agent must find an optimal policy that maximizes expected return.</li>
<li>Delayed consequences: Actions can affect rewards far into the future, making credit assignment difficult.</li>
<li>Exploration: The agent must try actions to learn their consequences, even though some actions may seem suboptimal in the short term.</li>
<li>Generalization: The agent must use limited experience to generalize to states it has never seen before.</li>
</ol>
<p>The main components of a reinforcement learning system are the agent, the environment, actions, states, and rewards. The agent chooses an action based on its current state. The environment responds with the next state and a reward. From this interaction, the agent must infer how to improve its decisions over time.</p>
<p>The concept of state is crucial. A state is a summary of information that can influence future outcomes. In theory, a state is Markov if it satisfies:</p>
<div class="arithmatex">\[
p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t)
\]</div>
<p>where <span class="arithmatex">\(h_t\)</span> is the full history of past observations, actions, and rewards. This means that the future depends only on the current state, not on the entire past. The Markov property is important because it simplifies the learning problem and allows powerful mathematical tools to be applied.</p>
<blockquote>
<p>State (environment)  →  Action (agent)  →  Next State (environment)</p>
</blockquote>
<p>Reinforcement learning is particularly useful in domains where optimal behavior is not easily specified, data is limited or must be collected through interaction, and long-term consequences matter. Examples include robotics, autonomous vehicles, game playing, resource allocation, recommendation systems, and online decision-making.</p>
<h2 id="reinforcement-1_intro-key-concepts">Key Concepts:<a class="headerlink" href="#reinforcement-1_intro-key-concepts" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-1_intro-episodic-vs-continuing">Episodic vs Continuing:<a class="headerlink" href="#reinforcement-1_intro-episodic-vs-continuing" title="Permanent link">¶</a></h3>
<p>Reinforcement learning problems can be episodic or continuing. In episodic tasks, interactions end after a finite number of steps, and the agent resets for a new episode. In continuing tasks, the interactions never formally end, and the agent must learn to behave well indefinitely. In episodic settings, the return is naturally finite. In continuing tasks, discounting or average reward formulations are used to ensure the return is well-defined.</p>
<h3 id="reinforcement-1_intro-types-of-rl-tasks">Types of RL tasks:<a class="headerlink" href="#reinforcement-1_intro-types-of-rl-tasks" title="Permanent link">¶</a></h3>
<p>There are several types of learning tasks in RL:</p>
<ol>
<li>Prediction/Policy Evaluation: Estimating how good a given policy is.</li>
<li>Control: Finding an optimal policy that maximizes expected return.</li>
<li>Planning: Computing optimal policies using a known model of the environment.</li>
</ol>
<h3 id="reinforcement-1_intro-model-based-vs-model-free">Model based vs Model-free<a class="headerlink" href="#reinforcement-1_intro-model-based-vs-model-free" title="Permanent link">¶</a></h3>
<p>Reinforcement learning algorithms can be classified into two major categories: model-based and model-free. Model-based methods assume that the transition dynamics and reward function of the environment are known or learned. They use this information for planning. Model-free methods do not assume access to this knowledge and must learn directly from interaction.</p>
<h3 id="reinforcement-1_intro-on-policy-vs-off-policy">On-policy vs off-policy<a class="headerlink" href="#reinforcement-1_intro-on-policy-vs-off-policy" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>On-policy learning:</p>
<ul>
<li>Direct experience.</li>
<li>Learn to estimate and evaluate a policy from experience obtained from following that policy.</li>
</ul>
</li>
<li>
<p>Off-policy Learning</p>
<ul>
<li>Learn to estimate and evaluate a policy using experience gathered from following a different policy.</li>
</ul>
</li>
</ul>
<h3 id="reinforcement-1_intro-tabular-vs-function-approximation">Tabular vs Function Approximation<a class="headerlink" href="#reinforcement-1_intro-tabular-vs-function-approximation" title="Permanent link">¶</a></h3>
<p>In small environments with a limited number of states and actions, value functions and policies can be represented using tables. This is known as the tabular setting. However, real-world problems often involve very large or continuous state spaces, where it is impossible to maintain a separate entry for every state or action.</p>
<p>In such cases, we approximate the value function or policy using a parameterized function, such as a linear model or neural network. This approach is called function approximation. Function approximation enables generalization: knowledge gained from one state can be applied to many similar states, making learning feasible in large or continuous environments.</p></body></html></section><section class="print-page" id="reinforcement-2_mdp" heading-number="3.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-markov-decision-processes-and-dynamic-programming">Chapter 2: Markov Decision Processes and Dynamic Programming<a class="headerlink" href="#reinforcement-2_mdp-chapter-2-markov-decision-processes-and-dynamic-programming" title="Permanent link">¶</a></h1>
<p>Reinforcement Learning relies on the mathematical framework of Markov Decision Processes (MDPs) to formalize sequential decision-making under uncertainty. The key idea is that an agent interacts with an environment, making decisions that influence both immediate and future rewards.</p>
<blockquote>
<p>Reinforcement Learning is about selecting actions over time to maximize long-term reward.</p>
</blockquote>
<h2 id="reinforcement-2_mdp-the-markovian-hierarchy">The Markovian Hierarchy<a class="headerlink" href="#reinforcement-2_mdp-the-markovian-hierarchy" title="Permanent link">¶</a></h2>
<p>The RL framework is built upon three foundational models, each adding complexity and agency.</p>
<h3 id="reinforcement-2_mdp-the-markov-process">The Markov Process<a class="headerlink" href="#reinforcement-2_mdp-the-markov-process" title="Permanent link">¶</a></h3>
<p>A Markov Process, or Markov Chain, is the simplest model, concerned only with the flow of states. It is defined by the set of States (<span class="arithmatex">\(S\)</span>) and the Transition Model (<span class="arithmatex">\(P(s' \mid s)\)</span>). The defining characteristic is the Markov Property: the next state is independent of the past states, given only the current state.</p>
<div class="arithmatex">\[
P(s_{t+1} \mid s_t, s_{t-1}, \ldots) = P(s_{t+1} \mid s_t)
\]</div>
<blockquote>
<p>The future is conditionally independent of the past given the present. <em>Intuition: MPs describe what happens but do not assign any value to these events.</em></p>
</blockquote>
<h3 id="reinforcement-2_mdp-the-markov-reward-process-mrp">The Markov Reward Process (MRP)<a class="headerlink" href="#reinforcement-2_mdp-the-markov-reward-process-mrp" title="Permanent link">¶</a></h3>
<p>A Markov Reward Process (MRP) extends an MP by adding rewards and discounting. An MRP is a tuple <span class="arithmatex">\((S, P, R, \gamma)\)</span> where <span class="arithmatex">\(R(s)\)</span> is the expected reward for being in state <span class="arithmatex">\(s\)</span> and <span class="arithmatex">\(\gamma\)</span> is the discount factor. The return is:</p>
<div class="arithmatex">\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]</div>
<p>The goal is to compute the value function, which is the expected return starting from a state <span class="arithmatex">\(s\)</span>:</p>
<div class="arithmatex">\[
V(s) = \mathbb{E}[G_t | s_t = s]
\]</div>
<p>The value function satisfies the Bellman Expectation Equation:</p>
<div class="arithmatex">\[
V(s) = R(s) + \gamma \sum_{s'} P(s'|s)V(s')
\]</div>
<p>This recursive structure relates the value of a state to the values of its successor states.</p>
<h3 id="reinforcement-2_mdp-the-markov-decision-process-mdp">The Markov Decision Process (MDP)<a class="headerlink" href="#reinforcement-2_mdp-the-markov-decision-process-mdp" title="Permanent link">¶</a></h3>
<p>An MDP introduces agency. Defined by the tuple <span class="arithmatex">\((S, A, P, R, \gamma)\)</span>, it extends the MRP by giving the agent a set of Actions (<span class="arithmatex">\(A\)</span>) to choose from.</p>
<ul>
<li>Action-Dependent Transition: <span class="arithmatex">\(P(s' \mid s, a)\)</span></li>
<li>Action-Dependent Reward: <span class="arithmatex">\(R(s, a)\)</span></li>
</ul>
<p>The agent's strategy is described by a Policy (<span class="arithmatex">\(\pi(a \mid s)\)</span>), the probability of selecting action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. A key insight is that fixing any policy <span class="arithmatex">\(\pi\)</span> reduces an MDP back into an MRP, allowing all tools developed for MRPs to be applied to the MDP.</p>
<div class="arithmatex">\[
R_\pi(s) = \sum_a \pi(a|s) R(s,a)
\]</div>
<div class="arithmatex">\[
P_\pi(s'|s) = \sum_a \pi(a|s) P(s'|s,a)
\]</div>
<blockquote>
<p>Once actions are introduced in an MDP, it becomes useful to evaluate not only how good a state is, but how good a particular action is <em>relative to the policy’s expected behavior</em>. This leads to the advantage function.</p>
<p>The state-value function measures how good it is to be in a state: <span class="arithmatex">\(V_\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s]\)</span>.</p>
<p>The action-value function measures how good it is to take action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>:<span class="arithmatex">\(Q_\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s,\; a_t = a]\)</span></p>
<p>The <strong>advantage function</strong> compares these two: <span class="arithmatex">\(A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s).\)</span></p>
<p><span class="arithmatex">\(V_\pi(s)\)</span> is how well the policy performs <em>on average</em> from state <span class="arithmatex">\(s\)</span>.</p>
<p><span class="arithmatex">\(Q_\pi(s,a)\)</span> is how well it performs if it specifically takes action <span class="arithmatex">\(a\)</span>.</p>
<p>Therefore, the advantage tells us: How much better or worse action <span class="arithmatex">\(a\)</span> is compared to what the policy would normally do in state <span class="arithmatex">\(s\)</span>.</p>
</blockquote>
<h2 id="reinforcement-2_mdp-value-functions-and-expectation">Value Functions and Expectation<a class="headerlink" href="#reinforcement-2_mdp-value-functions-and-expectation" title="Permanent link">¶</a></h2>
<p>To evaluate a fixed policy <span class="arithmatex">\(\pi\)</span>, we define two inter-related value functions based on the Bellman Expectation Equations.</p>
<h3 id="reinforcement-2_mdp-state-value-function-vpis">State Value Function (<span class="arithmatex">\(V^\pi(s)\)</span>)<a class="headerlink" href="#reinforcement-2_mdp-state-value-function-vpis" title="Permanent link">¶</a></h3>
<p><span class="arithmatex">\(V^\pi(s)\)</span> quantifies the long-term expected return starting from state <span class="arithmatex">\(s\)</span> and strictly following policy <span class="arithmatex">\(\pi\)</span>.
<script type="math/tex; mode=display">
V^\pi(s) = \mathbb{E}[G_t \mid s_t = s, \pi]
</script>
</p>
<blockquote>
<p>How much total reward should I expect if I start in state s and follow policy <span class="arithmatex">\(\pi\)</span>: forever?</p>
</blockquote>
<h3 id="reinforcement-2_mdp-state-action-value-function-qpisa">State-Action Value Function (<span class="arithmatex">\(Q^\pi(s,a)\)</span>)<a class="headerlink" href="#reinforcement-2_mdp-state-action-value-function-qpisa" title="Permanent link">¶</a></h3>
<p><span class="arithmatex">\(Q^\pi(s,a)\)</span> is a more granular measure, quantifying the expected return if the agent takes action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> first, and <em>then</em> follows policy <span class="arithmatex">\(\pi\)</span>.
<script type="math/tex; mode=display">
Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')
</script>
</p>
<blockquote>
<p><em>Intuition:</em> The <span class="arithmatex">\(Q\)</span>-function is the value of doing a specific action; the <span class="arithmatex">\(V\)</span>-function is the value of being in a state (the weighted average of the <span class="arithmatex">\(Q\)</span>-values offered by the policy <span class="arithmatex">\(\pi\)</span> in that state):
<script type="math/tex; mode=display">
V^\pi(s) = \sum_a \pi(a \mid s) Q^\pi(s,a)
</script>
</p>
</blockquote>
<p>The Bellman Expectation Equation for <span class="arithmatex">\(V^\pi\)</span> links the value of a state to the values of the actions chosen by <span class="arithmatex">\(\pi\)</span> and the resulting future states:
<script type="math/tex; mode=display">
V^\pi(s) = \sum_a \pi(a \mid s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]
</script>
</p>
<h2 id="reinforcement-2_mdp-optimal-control-finding-pi">Optimal Control: Finding <span class="arithmatex">\(\pi^*\)</span><a class="headerlink" href="#reinforcement-2_mdp-optimal-control-finding-pi" title="Permanent link">¶</a></h2>
<p>The ultimate goal of solving an MDP is to find the optimal policy (<span class="arithmatex">\(\pi^*\)</span>) that maximizes the expected return from every state <span class="arithmatex">\(s\)</span>.</p>
<div class="arithmatex">\[
\pi^* = \operatorname*{arg\,max}_{\pi} V^\pi(s) \quad \text{for all } s \in S
\]</div>
<p>This optimal policy is characterized by the Optimal Value Functions (<span class="arithmatex">\(V^*\)</span> and <span class="arithmatex">\(Q^*\)</span>).</p>
<h3 id="reinforcement-2_mdp-the-bellman-optimality-equations">The Bellman Optimality Equations<a class="headerlink" href="#reinforcement-2_mdp-the-bellman-optimality-equations" title="Permanent link">¶</a></h3>
<p>These equations are fundamental, describing the unique value functions that arise when acting optimally. Unlike the expectation equations, they contain a <span class="arithmatex">\(\max\)</span> operator, making them non-linear.</p>
<ul>
<li>
<p>Optimal State Value (<span class="arithmatex">\(V^*\)</span>): The optimal value of a state equals the maximum expected return achievable from any single action <span class="arithmatex">\(a\)</span> taken from that state:</p>
<div class="arithmatex">\[
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
\]</div>
</li>
<li>
<p>Optimal Action-Value (<span class="arithmatex">\(Q^*\)</span>): The optimal value of taking action <span class="arithmatex">\(a\)</span> is the immediate reward plus the discounted value of the optimal subsequent actions (<span class="arithmatex">\(\max_{a'}\)</span>) in the next state <span class="arithmatex">\(s'\)</span>:</p>
<div class="arithmatex">\[
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s', a')
\]</div>
</li>
</ul>
<p>Once <span class="arithmatex">\(Q^*\)</span> is known, the optimal policy <span class="arithmatex">\(\pi^*\)</span> is easily extracted by simply choosing the action that maximizes <span class="arithmatex">\(Q^*(s,a)\)</span> in every state:
<script type="math/tex; mode=display">
\pi^*(s) = \operatorname*{arg\,max}_{a} Q^*(s,a)
</script>
</p>
<p>These equations are non-linear due to the max operator and must be solved iteratively.</p>
<h2 id="reinforcement-2_mdp-dynamic-programming-algorithms">Dynamic Programming Algorithms<a class="headerlink" href="#reinforcement-2_mdp-dynamic-programming-algorithms" title="Permanent link">¶</a></h2>
<p>For MDPs where the model (<span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(R\)</span>) is fully known, Dynamic Programming methods are used to solve the Bellman Optimality Equations iteratively.</p>
<h3 id="reinforcement-2_mdp-policy-iteration">Policy Iteration<a class="headerlink" href="#reinforcement-2_mdp-policy-iteration" title="Permanent link">¶</a></h3>
<p>Policy Iteration follows an alternating cycle of Evaluation and Improvement. It takes fewer, but more expensive, iterations to converge.</p>
<ol>
<li>Policy Evaluation: For the current policy <span class="arithmatex">\(\pi_k\)</span>, compute <span class="arithmatex">\(V^{\pi_k}\)</span> by iteratively applying the Bellman Expectation Equation until full convergence. This is the computationally intensive step.
    <script type="math/tex; mode=display">
    V^{\pi_k}(s) \leftarrow \text{solve } V^{\pi_k} = R_{\pi_k} + \gamma P_{\pi_k} V^{\pi_k}
    </script>
</li>
<li>Policy Improvement: Update the policy <span class="arithmatex">\(\pi_{k+1}\)</span> by choosing an action that is greedy with respect to the fully converged <span class="arithmatex">\(V^{\pi_k}\)</span>.
    <script type="math/tex; mode=display">
    \pi_{k+1}(s) \leftarrow \operatorname*{arg\,max}_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi_k}(s') \right]
    </script>
</li>
</ol>
<p>The process repeats until the policy stabilizes (<span class="arithmatex">\(\pi_{k+1} = \pi_k\)</span>), guaranteeing convergence to <span class="arithmatex">\(\pi^*\)</span>.</p>
<h3 id="reinforcement-2_mdp-value-iteration">Value Iteration<a class="headerlink" href="#reinforcement-2_mdp-value-iteration" title="Permanent link">¶</a></h3>
<p>Value Iteration is a single, continuous process that combines evaluation and improvement by repeatedly applying the Bellman Optimality Equation. It takes many, but computationally cheap, iterations.</p>
<ol>
<li>Iterative Update: For every state <span class="arithmatex">\(s\)</span>, update the value function <span class="arithmatex">\(V_k(s)\)</span> using the <span class="arithmatex">\(\max\)</span> operation. This immediately incorporates a greedy improvement step into the value update.
    <script type="math/tex; mode=display">
    V_{k+1}(s) \leftarrow \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]
    </script>
</li>
<li>Convergence: The iterations stop when <span class="arithmatex">\(V_{k+1}\)</span> is sufficiently close to <span class="arithmatex">\(V^*\)</span>.</li>
<li>Extraction: The optimal policy <span class="arithmatex">\(\pi^*\)</span> is then extracted greedily from the final <span class="arithmatex">\(V^*\)</span>.</li>
</ol>
<h3 id="reinforcement-2_mdp-pi-vs-vi">PI vs VI<a class="headerlink" href="#reinforcement-2_mdp-pi-vs-vi" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Policy Iteration (PI)</th>
<th style="text-align: left;">Value Iteration (VI)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Core Idea</td>
<td style="text-align: left;">Evaluate completely, then improve.</td>
<td style="text-align: left;">Greedily improve values in every step.</td>
</tr>
<tr>
<td style="text-align: left;">Equation</td>
<td style="text-align: left;">Uses Bellman Expectation (inner loop)</td>
<td style="text-align: left;">Uses Bellman Optimality (max)</td>
</tr>
<tr>
<td style="text-align: left;">Convergence</td>
<td style="text-align: left;">Few, large policy steps. Policy guaranteed to stabilize faster.</td>
<td style="text-align: left;">Many, small value steps. Value function converges slowly to <span class="arithmatex">\(V^*\)</span>.</td>
</tr>
<tr>
<td style="text-align: left;">Cost</td>
<td style="text-align: left;">High cost per iteration (due to full evaluation).</td>
<td style="text-align: left;">Low cost per iteration (due to one-step backup).</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-2_mdp-mdps-mental-map">MDPs Mental Map<a class="headerlink" href="#reinforcement-2_mdp-mdps-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                   Markov Decision Processes (MDPs)
        Formalizing Sequential Decision-Making under Uncertainty
                                  │
                                  ▼
                       Progression of Markov Models
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Process (MP): States &amp; Transition Probabilities │
       │   [S, P(s'|s)] — No rewards, no decisions               │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Reward Process (MRP): MP + Rewards + γ          │
       │  [S, P(s'|s), R(s), γ]                                  │
       │    Value Function: V(s) = E[Gt | st = s]                │
       │     Bellman Expectation Eqn:                            │
       │     V(s) = R(s) + γ ∑ P(s'|s)V(s')                      │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Decision Process (MDP): MRP + Actions           │
       │   [S, A, P(s'|s,a), R(s,a), γ]                          │
       │    Adds Agency: Agent chooses actions                   │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                             Policy π(a|s)
                        Agent’s decision strategy
                                  │
                                  ▼
                          Value Functions
     ┌────────────────────────────────────────────────────────────────┐
     │ State Value Vπ(s): Expected return following π                 │
     │ Qπ(s,a): Expected return from (s,a) then follow π              │
     │ Relationship: Vπ(s) = ∑ π(a|s) Qπ(s,a)                         │
     └────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                    Bellman Expectation Equations
     ┌────────────────────────────────────────────────────────────────┐
     │ Vπ(s) = ∑ π(a|s)[R(s,a) + γ ∑ P(s'|s,a)Vπ(s')]                 │
     │ Qπ(s,a) = R(s,a) + γ ∑ P(s'|s,a) Vπ(s')                        │
     └────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
               Goal: Find Optimal Policy π*
     ┌─────────────────────────────────────────────────────────────┐
     │ π*(s) = argmaxₐ Q*(s,a)                                     │
     │ V*(s): Max possible value from state s under the optimal    |
     |        policy                                               │
     │ Q*(s,a): Max possible return state s by taking action a     |
     |          and thereafter following the optimal policy        │
     └─────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                      Bellman Optimality Equations
     ┌─────────────────────────────────────────────────────────────┐
     │ V*(s) = maxₐ [R(s,a) + γ ∑ P(s'|s,a)V*(s')]                 │
     │ Q*(s,a) = R(s,a) + γ ∑ P(s'|s,a) maxₐ' Q*(s',a')            │
     └─────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                 Solution when Model (P,R) is known:
                    Dynamic Programming (DP)
     ┌───────────────────────────────────────────────┬───────────────────┐
     │ Policy Iteration                              │ Value Iteration - │
     │ (Alternating Evaluation &amp; Improvement)        │ Single update step│
     │                                               │ repeatedly        │
     └───────────────────────────────────────────────┴───────────────────┘
          │                                               │
          ▼                                               ▼
  ┌─────────────────┐                          ┌─────────────────────────┐
  │ Policy Eval     │                          │ Bellman Optimality      │
  │ Using Vπ until  │                          │ Update every iteration  │
  │ convergence     │                          │ V_(k+1) = max_a[....]   │
  └─────────────────┘                          └─────────────────────────┘
          │                                               │
          ▼                                               ▼
  ┌─────────────────┐                          ┌─────────────────────────┐
  │ Policy          │                          │ After convergence:      │
  │ Improvement:    │                          │ extract π* from Q*      │
  │ π_(k+1)=argmax Q│                          └─────────────────────────┘
  └─────────────────┘
                                  │
                                  ▼
             Outcome: Optimal Policy and Value Functions
       ┌─────────────────────────────────────────────────────┐
       │ π*(s) — Best action at each state                   │
       │ V*(s) — Max return achievable                       │
       │ Q*(s,a) — Max return from (s,a)                     │
       └─────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-3_modelfree" heading-number="3.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy">Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy<a class="headerlink" href="#reinforcement-3_modelfree-chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy" title="Permanent link">¶</a></h1>
<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function <span class="arithmatex">\(V^\pi(s)\)</span> or <span class="arithmatex">\(Q^\pi(s,a)\)</span> for a given policy <span class="arithmatex">\(\pi\)</span> using data of the form:</p>
<div class="arithmatex">\[
s_0, a_0, r_1, s_1, a_1, r_2, s_2, \dots
\]</div>
<p>The true value of a state under policy <span class="arithmatex">\(\pi\)</span> is still defined by the expected return:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]
\]</div>
<p>but the agent must approximate this expectation using sampled experience.</p>
<p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p>
<ol>
<li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li>
<li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li>
</ol>
<h2 id="reinforcement-3_modelfree-monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation<a class="headerlink" href="#reinforcement-3_modelfree-monte-carlo-policy-evaluation" title="Permanent link">¶</a></h2>
<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function <span class="arithmatex">\(V^\pi(s)\)</span> is the expected return, we can approximate it by simply averaging the observed returns (<span class="arithmatex">\(G_t\)</span>) from many episodes that start at state <span class="arithmatex">\(s\)</span>.</p>
<div class="arithmatex">\[
V^\pi(s) \approx \text{Average of observed returns } G_t \text{ starting from } s
\]</div>
<h3 id="reinforcement-3_modelfree-key-properties-of-mc">Key Properties of MC<a class="headerlink" href="#reinforcement-3_modelfree-key-properties-of-mc" title="Permanent link">¶</a></h3>
<ol>
<li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (<span class="arithmatex">\(s_T\)</span>) to calculate the full return <span class="arithmatex">\(G_t\)</span>.</li>
<li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li>
</ol>
<p>We can maintain the value estimates <span class="arithmatex">\(V(s)\)</span> using counts and sums, or through incremental updates.</p>
<h4 id="reinforcement-3_modelfree-a-first-visit-vs-every-visit-mc">A. First-Visit vs. Every-Visit MC<a class="headerlink" href="#reinforcement-3_modelfree-a-first-visit-vs-every-visit-mc" title="Permanent link">¶</a></h4>
<p>When computing the return <span class="arithmatex">\(G_t\)</span> for a state <span class="arithmatex">\(s\)</span> in a single trajectory, a state might be visited multiple times.</p>
<ul>
<li>First-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> only the first time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: First-Visit MC is an unbiased estimator of <span class="arithmatex">\(V^\pi(s)\)</span>. It is also consistent (converges to the true value as data <span class="arithmatex">\(\rightarrow \infty\)</span>) by the Law of Large Numbers.</li>
</ul>
</li>
<li>Every-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> every time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li>
</ul>
</li>
</ul>
<h4 id="reinforcement-3_modelfree-b-incremental-monte-carlo">B. Incremental Monte Carlo<a class="headerlink" href="#reinforcement-3_modelfree-b-incremental-monte-carlo" title="Permanent link">¶</a></h4>
<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p>
<div class="arithmatex">\[
V(s) \leftarrow V(s) + \alpha \left[ G_t - V(s) \right]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(G_t\)</span>: The actual observed return (our target).</li>
<li><span class="arithmatex">\(V(s)\)</span>: Our current estimate (our old value).</li>
<li><span class="arithmatex">\(\alpha\)</span>: The learning rate (<span class="arithmatex">\(\alpha \in (0, 1]\)</span>), which can be fixed or decayed.</li>
</ul>
<p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (<span class="arithmatex">\(V^\pi\)</span>), the learning rate <span class="arithmatex">\(\alpha_t\)</span> (which may be <span class="arithmatex">\(1/N(s)\)</span> or a fixed constant) must satisfy the following conditions:</p>
<ol>
<li>The sum of all learning rates for state <span class="arithmatex">\(s\)</span> must diverge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s) = \infty\)</span></li>
<li>The sum of the squared learning rates must converge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s)^2 &lt; \infty\)</span></li>
</ol>
<h2 id="reinforcement-3_modelfree-temporal-difference-td-learning">Temporal Difference (TD) Learning<a class="headerlink" href="#reinforcement-3_modelfree-temporal-difference-td-learning" title="Permanent link">¶</a></h2>
<p>While MC uses the full return <span class="arithmatex">\(G_t\)</span>, TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>
<h3 id="reinforcement-3_modelfree-bootstrapping-the-core-idea">Bootstrapping: The Core Idea<a class="headerlink" href="#reinforcement-3_modelfree-bootstrapping-the-core-idea" title="Permanent link">¶</a></h3>
<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the <em>next</em> state, <span class="arithmatex">\(V(s_{t+1})\)</span>, to update the value of the <em>current</em> state, <span class="arithmatex">\(V(s_t)\)</span>. The standard TD algorithm is TD(0) (or one-step TD).</p>
<h3 id="reinforcement-3_modelfree-the-td0-update-rule">The TD(0) Update Rule<a class="headerlink" href="#reinforcement-3_modelfree-the-td0-update-rule" title="Permanent link">¶</a></h3>
<p>The TD(0) update replaces the full return <span class="arithmatex">\(G_t\)</span> with the TD Target (<span class="arithmatex">\(r_t + \gamma V(s_{t+1})\)</span>):</p>
<div class="arithmatex">\[
V(s_t) \leftarrow V(s_t) + \alpha \left[ \underbrace{r_{t+1} + \gamma V(s_{t+1})}_{\text{TD Target}} - V(s_t) \right]
\]</div>
<p>The term inside the brackets is the TD Error (<span class="arithmatex">\(\delta_t\)</span>):
<script type="math/tex; mode=display">
\delta_t = (r_{t+1} + \gamma V(s_{t+1})) - V(s_t)
</script>
This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>
<h3 id="reinforcement-3_modelfree-td-vs-monte-carlo">TD vs. Monte Carlo<a class="headerlink" href="#reinforcement-3_modelfree-td-vs-monte-carlo" title="Permanent link">¶</a></h3>
<p>The distinction between TD and MC centers on what is used as the target value:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Monte Carlo (MC)</th>
<th style="text-align: left;">Temporal Difference (TD)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: left;"><span class="arithmatex">\(G_t\)</span> (Full observed return to episode end)</td>
<td style="text-align: left;"><span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span> (One-step return + estimated future value)</td>
</tr>
<tr>
<td style="text-align: left;">Bootstrapping</td>
<td style="text-align: left;">No (waits until episode end)</td>
<td style="text-align: left;">Yes (uses <span class="arithmatex">\(V(s_{t+1})\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;">Bias</td>
<td style="text-align: left;">Unbiased (First-Visit MC)</td>
<td style="text-align: left;">Biased (because <span class="arithmatex">\(V(s_{t+1})\)</span> is an estimate)</td>
</tr>
<tr>
<td style="text-align: left;">Variance</td>
<td style="text-align: left;">High Variance (Return <span class="arithmatex">\(G_t\)</span> is a sum of many random steps)</td>
<td style="text-align: left;">Low Variance (TD target depends on only one random reward/next state)</td>
</tr>
<tr>
<td style="text-align: left;">Convergence</td>
<td style="text-align: left;">Consistent (converges to true <span class="arithmatex">\(V^\pi\)</span>)</td>
<td style="text-align: left;">TD(0) converges to true <span class="arithmatex">\(V^\pi\)</span> in the tabular case</td>
</tr>
</tbody>
</table>
<p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>
<blockquote>
<h2 id="reinforcement-3_modelfree-example-setup">Example Setup<a class="headerlink" href="#reinforcement-3_modelfree-example-setup" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-3_modelfree-parameters">Parameters<a class="headerlink" href="#reinforcement-3_modelfree-parameters" title="Permanent link">¶</a></h3>
<ul>
<li>States (<span class="arithmatex">\(S\)</span>): <span class="arithmatex">\(s_A, s_B, s_C\)</span></li>
<li>Discount Factor (<span class="arithmatex">\(\gamma\)</span>): <span class="arithmatex">\(0.9\)</span></li>
<li>Learning Rate (<span class="arithmatex">\(\alpha\)</span>): <span class="arithmatex">\(0.5\)</span> (Used for TD updates)</li>
<li>Initial Value Estimates (<span class="arithmatex">\(V_0\)</span>): <span class="arithmatex">\(V(s_A)=0, V(s_B)=0, V(s_C)=0\)</span></li>
</ul>
<h3 id="reinforcement-3_modelfree-episodes-and-returns">Episodes and Returns<a class="headerlink" href="#reinforcement-3_modelfree-episodes-and-returns" title="Permanent link">¶</a></h3>
<p>The full return (<span class="arithmatex">\(G_t\)</span>) is calculated for every visit in every episode:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Episode (E)</th>
<th style="text-align: left;">Trajectory (State <span class="arithmatex">\(\xrightarrow{r}\)</span> Next State)</th>
<th style="text-align: left;">Visit Time (<span class="arithmatex">\(t\)</span>)</th>
<th style="text-align: left;">State (<span class="arithmatex">\(s_t\)</span>)</th>
<th style="text-align: left;">Full Return (<span class="arithmatex">\(G_t\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B \xrightarrow{r=0} s_C \xrightarrow{r=5} s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.508}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.12}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.8}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{2.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{5.2}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{8.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C \xrightarrow{r=-5} s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.31}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{-4.1}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{1.0}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-3_modelfree-1-first-visit-monte-carlo-mc">1. First-Visit Monte Carlo (MC)<a class="headerlink" href="#reinforcement-3_modelfree-1-first-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: Only the first return for a state in any given episode is used.</p>
<h3 id="reinforcement-3_modelfree-a-data-selection-and-counts-ns">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-a-data-selection-and-counts-ns" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508\)</span> (E1), <span class="arithmatex">\(5.2\)</span> (E2)</td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12\)</span> (E1), <span class="arithmatex">\(6.31\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(12.43\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8\)</span> (E1), <span class="arithmatex">\(8.0\)</span> (E2), <span class="arithmatex">\(-4.1\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{12.43}{2} = \mathbf{6.215} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
</script>
</p>
<h2 id="reinforcement-3_modelfree-2-every-visit-monte-carlo-mc">2. Every-Visit Monte Carlo (MC)<a class="headerlink" href="#reinforcement-3_modelfree-2-every-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: The return from every time a state is encountered in any episode is used.</p>
<h3 id="reinforcement-3_modelfree-a-data-selection-and-counts-ns_1">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-a-data-selection-and-counts-ns_1" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508, 5.2\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12, 2.0, 6.31, 1.0\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(15.43\)</span></td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8, 8.0, -4.1\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns_1">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns_1" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{15.43}{4} = \mathbf{3.858} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
\]</div>
<h2 id="reinforcement-3_modelfree-3-temporal-difference-td0">3. Temporal Difference (TD(0))<a class="headerlink" href="#reinforcement-3_modelfree-3-temporal-difference-td0" title="Permanent link">¶</a></h2>
<p>Rule: The value is updated after every step using the TD Target (<span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span>) and the learning rate <span class="arithmatex">\(\alpha\)</span>. The updated <span class="arithmatex">\(V(s)\)</span> estimates are carried over to the next step and episode.</p>
<h3 id="reinforcement-3_modelfree-a-step-by-step-td-calculation-summary">A. Step-by-Step TD Calculation Summary<a class="headerlink" href="#reinforcement-3_modelfree-a-step-by-step-td-calculation-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Step</th>
<th style="text-align: left;">Transition</th>
<th style="text-align: left;">Old <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;">New <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_A)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_B)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_C)\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=0} s_C\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=5} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E1-4</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C\)</span></td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C\)</span></td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=-5} s_B\)</span></td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">3.663</td>
</tr>
<tr>
<td style="text-align: left;">E3-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">3.663</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-v_td0">B. Final Estimates (<span class="arithmatex">\(V_{TD(0)}\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-v_td0" title="Permanent link">¶</a></h3>
<blockquote>
<div class="arithmatex">\[
V(s_A) = \mathbf{0.375} \\
V(s_B) = \mathbf{4.431} \\
V(s_C) = \mathbf{3.663}
\]</div>
</blockquote>
<h2 id="reinforcement-3_modelfree-comparison-of-results">Comparison of Results<a class="headerlink" href="#reinforcement-3_modelfree-comparison-of-results" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">State</th>
<th style="text-align: left;">First-Visit MC</th>
<th style="text-align: left;">Every-Visit MC</th>
<th style="text-align: left;">TD(0) (<span class="arithmatex">\(\alpha=0.5, \gamma=0.9\)</span>)</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">TD heavily penalized <span class="arithmatex">\(s_A\)</span> in E2 (Target 0.25), while MC averaged the full observed high returns.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;">6.215</td>
<td style="text-align: left;">3.858</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">All methods are close for <span class="arithmatex">\(s_C\)</span>.</td>
</tr>
</tbody>
</table>
<p>This comparison illustrates the bias-variance trade-off:
* MC uses the sample return (<span class="arithmatex">\(G_t\)</span>), which has high variance but is an unbiased target (First-Visit).
* TD uses a bootstrapped estimate (<span class="arithmatex">\(r + \gamma V(s')\)</span>), which has lower variance but introduces bias by relying on an estimated successor value.</p>
</blockquote>
<h2 id="reinforcement-3_modelfree-model-free-prediction-mental-map">Model Free Prediction Mental Map<a class="headerlink" href="#reinforcement-3_modelfree-model-free-prediction-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>            Model-Free Prediction
     (Policy Evaluation without Model P or R)
                        │
                        ▼
                Goal: Estimate
       ┌───────────────────────────────────┐
       │ State Value: Vπ(s)                │
       │ Action Value: Qπ(s,a)             │
       └───────────────────────────────────┘
                        │
              Using Sampled Experience
        (s₀,a₀,r₁,s₁,a₁,r₂,... from π)
                        │
                        ▼
            Two Families of Methods
    ┌───────────────────────────────┬───────────────────────────────┐
    │ Monte Carlo (MC)              │ Temporal Difference (TD)      │
    │ "Learn from full episodes"    │ "Learn step-by-step"          │
    └───────────────────────────────┴───────────────────────────────┘
                        │                           │
                        │                           │
                        ▼                           ▼
             Monte Carlo (MC)              Temporal Difference (TD)
      ┌─────────────────────────┐       ┌────────────────────────────┐
      │ Needs full episodes     │       │Works on incomplete episodes│
      │ No bootstrapping        │       │Uses bootstrapping          │
      │ High variance           │       │Low variance                │
      │ Unbiased (first visit)  │       │Biased                      │
      └─────────────────────────┘       └────────────────────────────┘
                        │                           │
                        │                           │
        ┌───────────────┴───────────────┐           │
        │                               │           │
        ▼                               ▼           ▼
 First-Visit MC                  Every-Visit MC     TD(0) Update Rule
 (One update per episode          (Multiple updates │ V(s) ← V(s) +
  per state)                      per episode)      │ α[ r + γV(s') − V(s) ]
                        │                           │
                        │                           │
                        └──────────┬────────────────┘
                                   │
                                   ▼
                        Comparison (Bias–Variance)
               ┌─────────────────────────────────────────┐
               │ MC: Unbiased, High variance             │
               │ TD: Biased, Lower variance              │
               │ MC: Not bootstrapping                   │
               │ TD: Bootstraps using V(s’)              │
               │ MC: Episodic only                       │
               │ TD: Works for continuing tasks          │
               └─────────────────────────────────────────┘
                                   │
                                   ▼
                      Outcome: Learned Value Function
               ┌───────────────────────────────────────┐
               │ Vπ(s) or Qπ(s,a) from real experience │
               │ (No model of environment required)    │
               └───────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-4_model_free_control" heading-number="3.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-4-model-free-control-learning-optimal-behavior-without-a-model">Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model<a class="headerlink" href="#reinforcement-4_model_free_control-chapter-4-model-free-control-learning-optimal-behavior-without-a-model" title="Permanent link">¶</a></h1>
<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy <span class="arithmatex">\(\pi^*\)</span> without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>
<h2 id="reinforcement-4_model_free_control-from-state-values-to-action-values">From State Values to Action Values<a class="headerlink" href="#reinforcement-4_model_free_control-from-state-values-to-action-values" title="Permanent link">¶</a></h2>
<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p>
<p>
<script type="math/tex; mode=display">
\pi_{k+1}(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi_k}(s') \right]
</script>
This update requires two things:</p>
<ul>
<li>the transition probabilities <span class="arithmatex">\(P(s'|s,a)\)</span></li>
<li>the expected reward R(s,a)$</li>
</ul>
<p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p>
<p>Instead of learning the state-value function <span class="arithmatex">\(V^\pi(s)\)</span> and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.
<script type="math/tex; mode=display">
Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]
</script>
</p>
<p>The Model-Free Policy Iteration loop:</p>
<ol>
<li>Policy Evaluation: Compute <span class="arithmatex">\(Q^{\pi}\)</span> from experience.</li>
<li>Policy Improvement: Update the policy <span class="arithmatex">\(\pi\)</span> given the estimated <span class="arithmatex">\(Q^{\pi}\)</span>.</li>
</ol>
<p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>
<h2 id="reinforcement-4_model_free_control-exploration-vs-exploitation">Exploration vs. Exploitation<a class="headerlink" href="#reinforcement-4_model_free_control-exploration-vs-exploitation" title="Permanent link">¶</a></h2>
<p>To learn optimal behavior, the agent must balance two goals:</p>
<ol>
<li>Exploitation: choose actions believed to yield high rewards.</li>
<li>Exploration: try actions whose consequences are uncertain or poorly understood.</li>
</ol>
<p>A common solution is the <span class="arithmatex">\(\epsilon\)</span>-greedy policy:</p>
<p>With probability <span class="arithmatex">\(1 - \epsilon\)</span>, choose the action with the highest estimated value.<br>
With probability <span class="arithmatex">\(\epsilon\)</span>, choose a random action.</p>
<p>Formally:</p>
<div class="arithmatex">\[
\pi(a|s) =
\begin{cases}
1 - \epsilon + \frac{\epsilon}{|A|} &amp; \text{if } a = \arg\max_{a'} Q(s,a') \\
\frac{\epsilon}{|A|} &amp; \text{otherwise}
\end{cases}
\]</div>
<p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>
<h2 id="reinforcement-4_model_free_control-monte-carlo-control">Monte Carlo Control<a class="headerlink" href="#reinforcement-4_model_free_control-monte-carlo-control" title="Permanent link">¶</a></h2>
<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating <span class="arithmatex">\(V(s)\)</span>, it estimates <span class="arithmatex">\(Q(s,a)\)</span> using sampled returns.</p>
<p>Monte Carlo Policy Evaluation, Now for Q:      <br>
1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  <span class="arithmatex">\(k=1\)</span>,  Input <span class="arithmatex">\(\epsilon=1\)</span>, <span class="arithmatex">\(\pi\)</span><br>
2: loop over epiosdes     <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
5: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
6: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
9: <span class="arithmatex">\(\quad\quad\)</span>       end if<br>
10: <span class="arithmatex">\(\quad\)</span>  end for<br>
11: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span><br>
12: end loop</p>
<p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using <span class="arithmatex">\(\epsilon\)</span>-greedy policies for exploration.</p>
<ul>
<li>Policy Evaluation: <span class="arithmatex">\(Q(s, a)\)</span> is updated using the full return (<span class="arithmatex">\(G_t\)</span>) observed after the state-action pair <span class="arithmatex">\((s_t, a_t)\)</span> has occurred in an episode. The incremental update uses the formula <span class="arithmatex">\(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\)</span>. </li>
<li>Policy Improvement: The new policy <span class="arithmatex">\(\pi_{k+1}\)</span> is set to be <span class="arithmatex">\(\epsilon\)</span>-greedy with respect to the updated <span class="arithmatex">\(Q\)</span> function.</li>
</ul>
<h3 id="reinforcement-4_model_free_control-greedy-in-the-limit-of-infinite-exploration-glie">Greedy in the Limit of Infinite Exploration (GLIE)<a class="headerlink" href="#reinforcement-4_model_free_control-greedy-in-the-limit-of-infinite-exploration-glie" title="Permanent link">¶</a></h3>
<p>For Monte Carlo Control to converge to the optimal action-value function <span class="arithmatex">\(Q^*(s, a)\)</span>, the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p>
<ol>
<li>Infinite Visits: All state-action pairs <span class="arithmatex">\((s, a)\)</span> must be visited an infinite number of times (<span class="arithmatex">\(\lim_{i \rightarrow \infty} N_i(s, a) \rightarrow \infty\)</span>).</li>
<li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li>
</ol>
<p>A simple strategy to satisfy GLIE is to use an <span class="arithmatex">\(\epsilon\)</span>-greedy policy where <span class="arithmatex">\(\epsilon\)</span> is decayed over time, such as <span class="arithmatex">\(\epsilon_i = 1/i\)</span> (where <span class="arithmatex">\(i\)</span> is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function <span class="arithmatex">\(Q^*(s, a)\)</span>.</p>
<p>Monte Carlo Online Control/On Policy Improvement:    </p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  Set <span class="arithmatex">\(k=1\)</span>, <span class="arithmatex">\(\epsilon=1\)</span>.   <br>
2: <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> // Create initial <span class="arithmatex">\(\epsilon\)</span> - greedy policy.<br>
3: loop over epiosdes   <br>
4: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
5: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
6: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
7: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
9: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
10: <span class="arithmatex">\(\quad\quad\)</span>       end if <br>
11: <span class="arithmatex">\(\quad\)</span>  end for  <br>
12:  <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> //Policy improvement<br>
12: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{k}\)</span>   <br>
13: end loop    </p>
<p>This process gradually adjusts the policy and the value estimates until they converge.</p>
<h2 id="reinforcement-4_model_free_control-iv-temporal-difference-td-control">IV. Temporal Difference (TD) Control<a class="headerlink" href="#reinforcement-4_model_free_control-iv-temporal-difference-td-control" title="Permanent link">¶</a></h2>
<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>
<h3 id="reinforcement-4_model_free_control-on-policy-td-control-sarsa">On-Policy TD Control: SARSA<a class="headerlink" href="#reinforcement-4_model_free_control-on-policy-td-control-sarsa" title="Permanent link">¶</a></h3>
<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy <em>currently being followed</em> (<span class="arithmatex">\(\pi\)</span>). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p>
<p>The update for the action-value <span class="arithmatex">\(Q(s_t, a_t)\)</span> uses the value of the <em>next</em> state-action pair, <span class="arithmatex">\((s_{t+1}, a_{t+1})\)</span>, selected by the current policy <span class="arithmatex">\(\pi\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</div>
<p>The TD Target here is <span class="arithmatex">\(r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})\)</span>. SARSA learns <span class="arithmatex">\(Q^{\pi}\)</span> while <span class="arithmatex">\(\pi\)</span> is improved greedily with respect to <span class="arithmatex">\(Q^{\pi}\)</span>, allowing it to find the optimal policy <span class="arithmatex">\(\pi^*\)</span>.</p>
<p>1: Set initial <span class="arithmatex">\(\epsilon\)</span>-greedy policy <span class="arithmatex">\(\pi\)</span> randomly, <span class="arithmatex">\(t=0\)</span>, initial state <span class="arithmatex">\(s_t=s_0\)</span>    <br>
2: Take <span class="arithmatex">\(a_t \sim \pi(s_t)\)</span>      <br>
3: Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>       <br>
4: loop       <br>
5: <span class="arithmatex">\(\quad\)</span> Take action <span class="arithmatex">\(a_{t+1} \sim \pi(s_{t+1})\)</span> // Sample action from policy        <br>
6: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_{t+1}, s_{t+2})\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Update <span class="arithmatex">\(Q\)</span> given <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\)</span>:    <script type="math/tex; mode=display">
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]</script>
8: <span class="arithmatex">\(\quad\)</span> Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p>
<div class="arithmatex">\[\forall s \in S,\;\;
\pi(s) =
\begin{cases}
\arg\max\limits_a Q(s,a) &amp; \text{with probability } 1 - \epsilon \\
\text{a random action}   &amp; \text{with probability } \epsilon
\end{cases}\]</div>
<p>9: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{t}\)</span>           <br>
10: end loop        </p>
<h3 id="reinforcement-4_model_free_control-b-off-policy-td-control-q-learning">B. Off-Policy TD Control: Q-Learning<a class="headerlink" href="#reinforcement-4_model_free_control-b-off-policy-td-control-q-learning" title="Permanent link">¶</a></h3>
<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (<span class="arithmatex">\(\pi^*\)</span>, the <em>target policy</em>) using experience gathered by a different behavior policy (<span class="arithmatex">\(\pi_b\)</span>).</p>
<p>In Q-Learning, the agent acts using a soft, exploratory <span class="arithmatex">\(\pi_b\)</span> (like <span class="arithmatex">\(\epsilon\)</span>-greedy) but the value function update is based on the <em>best</em> possible action from the next state, effectively estimating <span class="arithmatex">\(Q^*\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]</div>
<p>The key difference is the target: Q-Learning uses the value of the max action (<span class="arithmatex">\(\max_{a'} Q(s_{t+1}, a')\)</span>), regardless of what action was actually taken in the next step. This makes it a greedy update towards <span class="arithmatex">\(Q^*\)</span>.</p>
<p>Q-Learning (Off-Policy TD Control):</p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0 \quad \forall s \in S, a \in A\)</span>, set <span class="arithmatex">\(t = 0\)</span>, initial state <span class="arithmatex">\(s_t = s_0\)</span>       <br>
2: Set <span class="arithmatex">\(\pi_b\)</span> to be <span class="arithmatex">\(\epsilon\)</span>-greedy w.r.t. <span class="arithmatex">\(Q\)</span>     <br>
3: loop   <br>
4: <span class="arithmatex">\(\quad\)</span> Take <span class="arithmatex">\(a_t \sim \pi_b(s_t)\)</span> // Sample action from behavior policy   <br>
5: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>   <br>
6: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_t + \gamma \max\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \right]\)</span>      <br>
7: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi(s_t) =
\begin{cases}
\arg\max\limits_a Q(s_t,a) &amp; \text{with probability } 1 - \epsilon \
\text{a random action} &amp; \text{with probability } \epsilon
\end{cases}\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>    <br>
9: end loop     </p>
<h2 id="reinforcement-4_model_free_control-value-function-approximation-vfa">Value Function Approximation (VFA)<a class="headerlink" href="#reinforcement-4_model_free_control-value-function-approximation-vfa" title="Permanent link">¶</a></h2>
<p>All methods discussed so far assume a tabular representation, where a separate entry for <span class="arithmatex">\(Q(s, a)\)</span> is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>
<h3 id="reinforcement-4_model_free_control-motivation-for-approximation">Motivation for Approximation<a class="headerlink" href="#reinforcement-4_model_free_control-motivation-for-approximation" title="Permanent link">¶</a></h3>
<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p>
<ol>
<li>Memory: Explicitly storing every <span class="arithmatex">\(V\)</span> or <span class="arithmatex">\(Q\)</span> value is impossible.</li>
<li>Computation: Computing or updating every value is too slow.</li>
<li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li>
</ol>
<p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w}) \approx Q(s, a)\)</span>. The goal shifts from filling a table to finding the parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that minimizes the error between the true value and the estimate.</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \mathbb{E}_{\pi} \left[ \left( Q^{\pi}(s, a) - \hat{Q}(s, a; \mathbf{w}) \right)^2 \right]
\]</div>
<p>The parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function <span class="arithmatex">\(J(\mathbf{w})\)</span>.</p>
<h3 id="reinforcement-4_model_free_control-model-free-control-with-vfa-policy-evaluation">Model-Free Control with VFA Policy Evaluation<a class="headerlink" href="#reinforcement-4_model_free_control-model-free-control-with-vfa-policy-evaluation" title="Permanent link">¶</a></h3>
<p>When using function approximation, we substitute the old <span class="arithmatex">\(Q(s, a)\)</span> in the update rules (MC, SARSA, Q-Learning) with the function approximator <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span>.</p>
<ul>
<li>
<p>MC VFA for Policy Evaluation: </p>
<p>The return <span class="arithmatex">\(G_t\)</span> is used as the target in an SGD update: <span class="arithmatex">\(\Delta \mathbf{w} \propto \alpha (G_t - \hat{Q}(s_t, a_t; \mathbf{w})) \nabla_{\mathbf{w}} \hat{Q}(s_t, a_t; \mathbf{w})\)</span>.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, set <span class="arithmatex">\(k = 1\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,L_k})\)</span> given <span class="arithmatex">\(\pi\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1, \dots, L_k\)</span> do     <br>
5: <span class="arithmatex">\(\quad\quad\)</span> if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then     <br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(G_t(s,a) = \sum_{j=t}^{L_k} r_{k,j}\)</span>    <br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 \left[ G_t(s,a) - \hat{Q}(s_t,a_t;\mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})\)</span> // Compute Gradient   <br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span> Update weights: <span class="arithmatex">\(\Delta \mathbf{w}\)</span>      <br>
9: <span class="arithmatex">\(\quad\quad\)</span> end if<br>
10: <span class="arithmatex">\(\quad\)</span> end for 
11: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(k = k + 1\)</span>       <br>
12: end loop    </p>
</li>
<li>
<p>SARSA with VFA: The TD target is <span class="arithmatex">\(r + \gamma \hat{Q}(s', a'; \mathbf{w})\)</span>, leveraging the current function approximation.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(s\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Given <span class="arithmatex">\(s\)</span>, sample <span class="arithmatex">\(a \sim \pi(s)\)</span>, observe <span class="arithmatex">\(r(s,a)\)</span>, and <span class="arithmatex">\(s' \sim p(s'|s,a)\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 [r + \gamma \hat{V}(s';\mathbf{w}) - \hat{V}(s;\mathbf{w})] \nabla_{\mathbf{w}} \hat{V}(s;\mathbf{w})\)</span>     <br>
5: <span class="arithmatex">\(\quad\)</span> Update weights <span class="arithmatex">\(\Delta \mathbf{w}\)</span>         <br>
6: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(s'\)</span> is not a terminal state then   <br>
7: <span class="arithmatex">\(\quad\quad\)</span> Set <span class="arithmatex">\(s = s'\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> else       <br>
9: <span class="arithmatex">\(\quad\quad\)</span> Restart episode, sample initial state <span class="arithmatex">\(s\)</span>     <br>
10: <span class="arithmatex">\(\quad\)</span> end if    <br>
11: end loop      <br>
* Q-Learning with VFA: The TD target is <span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s', a'; \mathbf{w})\)</span>.</p>
</li>
</ul>
<h3 id="reinforcement-4_model_free_control-control-using-vfa">Control using VFA<a class="headerlink" href="#reinforcement-4_model_free_control-control-using-vfa" title="Permanent link">¶</a></h3>
<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular <span class="arithmatex">\(Q(s,a)\)</span> with a function approximator <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>, we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>.</p>
<p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each <span class="arithmatex">\((s,a)\)</span>, the agent learns a parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function <span class="arithmatex">\(Q^*(s,a)\)</span>.</p>
<p>The learning problem becomes:</p>
<div class="arithmatex">\[
\min_{\mathbf{w}} \; J(\mathbf{w}) = \mathbb{E} \left[ \left( Q^*(s,a) - \hat{Q}(s,a;\mathbf{w}) \right)^2 \right]
\]</div>
<p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p>
<div class="arithmatex">\[
\Delta \mathbf{w} \propto \left( \text{target} - \hat{Q}(s_t,a_t;\mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})
\]</div>
<p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Target for updating <span class="arithmatex">\(\mathbf{w}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Monte Carlo</td>
<td><span class="arithmatex">\(G_t\)</span></td>
</tr>
<tr>
<td>SARSA</td>
<td><span class="arithmatex">\(r + \gamma \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
<tr>
<td>Q-Learning</td>
<td><span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
</tbody>
</table>
<p>These methods now operate in the same way as before, except instead of updating a single <span class="arithmatex">\(Q(s,a)\)</span> entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>
<h3 id="reinforcement-4_model_free_control-challenges-the-deadly-triad">Challenges: The Deadly Triad<a class="headerlink" href="#reinforcement-4_model_free_control-challenges-the-deadly-triad" title="Permanent link">¶</a></h3>
<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p>
<div class="arithmatex">\[
\text{Function Approximation} \;+\; \text{Bootstrapping} \;+\; \text{Off-policy Learning}
\]</div>
<p>This combination is known as the Deadly Triad .</p>
<ul>
<li>Function Approximation : Generalizes across states but may introduce bias.</li>
<li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li>
<li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li>
</ul>
<p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p>
<p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate <span class="arithmatex">\(Q(s,a)\)</span>.</p>
<h3 id="reinforcement-4_model_free_control-deep-q-networks-dqn">Deep Q-Networks (DQN)<a class="headerlink" href="#reinforcement-4_model_free_control-deep-q-networks-dqn" title="Permanent link">¶</a></h3>
<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p>
<p>DQN stabilizes the non-linear learning process using two critical techniques:</p>
<ol>
<li>
<p>Experience Replay (ER): Transitions <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> are stored in a replay buffer (<span class="arithmatex">\(\mathcal{D}\)</span>). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from <span class="arithmatex">\(\mathcal{D}\)</span> for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p>
</li>
<li>
<p>Fixed Q-Targets: The Q-Learning update requires a target value <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w})\)</span>. To prevent the estimate <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> from chasing its own rapidly changing target, the parameters <span class="arithmatex">\(\mathbf{w}^{-}\)</span> used to compute the target are fixed for a period of time, then synchronized with the current parameters <span class="arithmatex">\(\mathbf{w}\)</span>. This provides a stable target <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w}^{-})\)</span>.</p>
</li>
</ol>
<p>Deep Q-Network (DQN) Algorithm:</p>
<p>1: Input <span class="arithmatex">\(C\)</span>, <span class="arithmatex">\(\alpha\)</span>, <span class="arithmatex">\(D = {}\)</span>, Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(\mathbf{w}^- = \mathbf{w}\)</span>, <span class="arithmatex">\(t = 0\)</span>       <br>
2: Get initial state <span class="arithmatex">\(s_0\)</span>            <br>
3: loop       <br>
4: <span class="arithmatex">\(\quad\)</span> Sample action <span class="arithmatex">\(a_t\)</span> using <span class="arithmatex">\(\epsilon\)</span>-greedy policy w.r.t. current <span class="arithmatex">\(\hat{Q}(s_t, a; \mathbf{w})\)</span>    <br>
5: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span>      <br>
6: <span class="arithmatex">\(\quad\)</span> Store transition <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> in replay buffer <span class="arithmatex">\(D\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Sample a random minibatch of tuples <span class="arithmatex">\((s_i, a_i, r_i, s'i)\)</span> from <span class="arithmatex">\(D\)</span>    <br>
8: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(j\)</span> in minibatch do    <br>
9: <span class="arithmatex">\(\quad\quad\)</span> if episode terminates at step <span class="arithmatex">\(i+1\)</span> then      <br>
10: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i\)</span>     <br>
11: <span class="arithmatex">\(\quad\quad\)</span> else     <br>
12: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i + \gamma \max\limits{a'} \hat{Q}(s'i, a'; \mathbf{w}^-)\)</span>     <br>
13: <span class="arithmatex">\(\quad\quad\)</span> end if   <br>
14: <span class="arithmatex">\(\quad\quad\)</span> Update <span class="arithmatex">\(\mathbf{w}\)</span> using gradient descent:  <br>
<span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\Delta \mathbf{w} = \alpha \left( y_i - \hat{Q}(s_i, a_i; \mathbf{w}) \right) \nabla{\mathbf{w}} \hat{Q}(s_i, a_i; \mathbf{w})\)</span>    <br>
15: <span class="arithmatex">\(\quad\)</span> end for       <br>
16: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>   <br>
17: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(t \mod C == 0\)</span> then   <br>
18: <span class="arithmatex">\(\quad\quad\)</span> <span class="arithmatex">\(\mathbf{w}^- \leftarrow \mathbf{w}\)</span>     <br>
19: <span class="arithmatex">\(\quad\)</span> end if    <br>
20: end loop        </p>
<h2 id="reinforcement-4_model_free_control-model-free-control-mental-map">Model Free Control Mental Map<a class="headerlink" href="#reinforcement-4_model_free_control-model-free-control-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                     Model-Free Control
    Goal: Learn the Optimal Policy π* without knowing P or R
                               │
                               ▼
           Key Concept: Action-Value Function Q(s,a)
       ┌─────────────────────────────────────────────┐
       │Qπ(s,a) = Expected return by taking action a │
       │in state s and following policy π thereafter │
       └─────────────────────────────────────────────┘
                               │
                      No model → Learn Q directly
                               │
                               ▼
                   Generalized Policy Iteration
       ┌───────────────────────────┬───────────────────────────┐
       │   Policy Evaluation       │     Policy Improvement    │
       │   Learn Qπ(s,a)           │   π ← greedy w.r.t Q      │
       └───────────────────────────┴───────────────────────────┘
                               │
                               ▼
                Challenge: Exploration vs. Exploitation
       ┌──────────────────────────────────────────────────────┐
       │Greedy policy → Exploits but stops exploring          │
       │ε-greedy policy → Balances exploration &amp; exploitation │
       │GLIE condition: ε → 0 and ∞ exploration               │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                Model-Free Control Families (Tabular)
       ┌────────────────────────────┬────────────────────────────┐
       │   Monte Carlo Control      │      Temporal Difference   │
       │   (Episode-based)          │      (Step-based)          │
       └────────────────────────────┴────────────────────────────┘
                               │
          ┌────────────────────┴───────────────────┐
          ▼                                        ▼
 Monte Carlo Control:                       TD Control:
 Estimates Q from full returns          Estimates Q usingbootstrapped targets
 Uses ε-greedy policy                   Works online, faster, low variance
 Episodic only                          Works for episodic &amp; continuing
          │                                        │
    ┌─────┴─────────────┐             ┌────────────┴───────────────┐
    │ GLIE MC Control   │             │ On-Policy TD: SARSA        │
    └───────────────────┘             │ Off-Policy TD: Q-Learning  │
                                      └────────────────────────────┘
                                                    |
                                                    ▼
┌──────────────────────────────────────────────────────────────────────────────┐
| On-Policy TD — SARSA                     |  Off-Policy TD — Q-Learning       |
| Learns Qπ for the policy being followed  |  Learns Q* while following π_b    |
| Update uses next action from π           |  Update uses max action (greedy)  |
| Update Target:                           |  Update Target:                   |
|  r + γ Q(s',a')                          |  r + γ maxₐ Q(s',a)               |
└────────────────────────────┴─────────────────────────────────────────────────┘



      Value Function Approximation (Large/Continuous spaces)
       ┌──────────────────────────────────────────────────────┐
       │ Replace Q(s,a) with Q̂(s,a;w) using function approx   │
       │ Generalization across states                         │
       │ Gradient-based updates (SGD)                         │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
            Deep Q-Learning (DQN) — Stable VFA Control
       ┌──────────────────────────────────────────────────────┐
       │ Experience Replay — decorrelate samples             │
       │ Target Networks — stabilize bootstrapped targets    │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                      Final Outcome of Model-Free Control
       ┌───────────────────────────────────────────────────────┐
       │ Learn π* directly from experience without model       │
       │ Learn Q*(s,a) through MC, SARSA, or Q-Learning        │
       │ Scale to large spaces using function approximation    │
       │ DQN enables deep RL in complex environments           │
       └───────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-5_policy_gradient" heading-number="3.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-policy-gradient-methods">Chapter 5: Policy Gradient Methods<a class="headerlink" href="#reinforcement-5_policy_gradient-chapter-5-policy-gradient-methods" title="Permanent link">¶</a></h1>
<p>In previous chapters, we derived policies indirectly from value functions using greedy or ε-greedy strategies. However, value-based RL has several challenges:</p>
<ul>
<li>Does not naturally support stochastic policies  </li>
<li>Struggles in continuous action spaces  </li>
<li>Optimizing through value functions is often indirect and unstable</li>
</ul>
<p>Policy Gradient Methods directly optimize the policy itself:</p>
<div class="arithmatex">\[
\pi_\theta(a|s) = P(a \mid s; \theta)
\]</div>
<p>Our goal becomes:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta V(\theta)
\]</div>
<p>That is, learn policy parameters θ that maximize expected return.</p>
<blockquote>
<p>Value-based methods struggle in these cases because they do not directly learn the policy. Instead, they estimate action values <span class="arithmatex">\(Q(s,a)\)</span> and derive a policy using greedy or ε-greedy strategies. This makes the policy indirect and unstable. Small changes in <span class="arithmatex">\(Q(s,a)\)</span> can suddenly change the best action, making learning discontinuous and erratic — especially with function approximation like neural networks. Furthermore, value-based methods do not naturally support stochastic or continuous action spaces, since computing <span class="arithmatex">\(\arg\max_a Q(s,a)\)</span> is infeasible when actions are continuous or infinite. Policy-based methods solve this problem by directly modeling and learning the policy, such as using a softmax distribution for discrete actions or Gaussian distributions for continuous actions.</p>
</blockquote>
<h2 id="reinforcement-5_policy_gradient-value-based-vs-policy-based-rl">Value-Based vs Policy-Based RL<a class="headerlink" href="#reinforcement-5_policy_gradient-value-based-vs-policy-based-rl" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Approach</th>
<th>What is Learned?</th>
<th>Policy Type</th>
<th>Works in Continuous Actions?</th>
</tr>
</thead>
<tbody>
<tr>
<td>Value-Based</td>
<td><span class="arithmatex">\(V(s)\)</span> or <span class="arithmatex">\(Q(s,a)\)</span></td>
<td>Indirect (ε-greedy, greedy)</td>
<td>No</td>
</tr>
<tr>
<td>Policy-Based</td>
<td><span class="arithmatex">\(\pi_\theta(a/s)\)</span></td>
<td>Direct, stochastic</td>
<td>Yes</td>
</tr>
<tr>
<td>Actor-Critic</td>
<td>Both</td>
<td>Direct &amp; learned</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-5_policy_gradient-policy-optimization-objective">Policy Optimization Objective<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-optimization-objective" title="Permanent link">¶</a></h2>
<p>In policy-based reinforcement learning, the policy itself is directly parameterized as <span class="arithmatex">\(\pi_\theta(a|s)\)</span>, and our goal is to find the parameters <span class="arithmatex">\(\theta\)</span> that produce the best possible behavior. For episodic tasks starting at initial state <span class="arithmatex">\(s_0\)</span>, the quality of a policy is measured by its expected return:</p>
<div class="arithmatex">\[
V(\theta) = V_{\pi_\theta}(s_0) = \mathbb{E}_{\pi_\theta}[G_0]
\]</div>
<p>Therefore, policy optimization can be formulated as an optimization problem, where the goal is to find the policy parameters <span class="arithmatex">\(\theta\)</span> that maximize the expected return:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta V(s_0, \theta)
\]</div>
<p>This optimization does not necessarily require gradients.  We can also use gradient-free (derivative-free) optimization methods such as:</p>
<ul>
<li>Hill Climbing – Iteratively adjusts parameters in small random directions and keeps changes that improve performance.</li>
<li>Simplex / Amoeba / Nelder-Mead – Uses a geometric shape (simplex) to explore the parameter space and moves it towards higher-performing regions.</li>
<li>Genetic Algorithms – Evolves a population of candidate policies using selection, crossover, and mutation, inspired by natural evolution.</li>
<li>Cross-Entropy Method (CEM) – Samples multiple policy candidates, selects the top performers, and updates the sampling distribution towards them.</li>
<li>Covariance Matrix Adaptation (CMA) – Adapts both the mean and covariance of a Gaussian distribution to efficiently search complex, high-dimensional policy spaces.</li>
</ul>
<p>Gradient-free policy optimization methods are often excellent and simple baselines to try.  They are highly flexible, can work with any policy parameterization (including non-differentiable ones), and are easy to parallelize, as policies can be evaluated independently across multiple environments. However, these methods are typically less sample efficient because they treat each policy evaluation as a black box and ignore the temporal structure of trajectories. They do not make use of gradients, value functions, or bootstrapping.</p>
<p>To improve efficiency, we can use gradient-based optimization techniques, which exploit  the structure of the return function and update parameters using local information. Common gradient-based optimizers include:</p>
<ul>
<li>Gradient Descent</li>
<li>Conjugate Gradient</li>
<li>Quasi-Newton Methods (e.g., BFGS, L-BFGS)</li>
</ul>
<p>These methods are generally more sample efficient, especially in large or continuous state-action spaces.</p>
<h2 id="reinforcement-5_policy_gradient-policy-gradient">Policy Gradient<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-gradient" title="Permanent link">¶</a></h2>
<p>The goal is to find parameters <span class="arithmatex">\(\theta\)</span> that maximize the expected return. Policy gradient algorithms search for a local maximum of <span class="arithmatex">\(V(\theta)\)</span> by performing gradient ascent:</p>
<div class="arithmatex">\[
\Delta \theta = \alpha \nabla_\theta V(s_0, \theta)
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is the step-size (learning rate) and <span class="arithmatex">\(\nabla_\theta V(s_0, \theta)\)</span> is the policy gradient.</p>
<p>Assuming an episodic MDP with discount factor <span class="arithmatex">\(\gamma = 1\)</span>, the value of a parameterized policy <span class="arithmatex">\(\pi_\theta\)</span> starting from state <span class="arithmatex">\(s_0\)</span> is</p>
<div class="arithmatex">\[
V(s_0,\theta) 
= \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} r(s_t, a_t); \; s_0, \pi_\theta \right],
\]</div>
<p>where the expectation is taken over the states and actions visited when following <span class="arithmatex">\(\pi_\theta\)</span>. This policy value can be re-expressed in multiple ways.  </p>
<ul>
<li>
<p>First, in terms of the action-value function:
    <script type="math/tex; mode=display">
    V(s_0,\theta) = \sum_{a} \pi_\theta(a \mid s_0) \, Q(s_0,a;\theta).
    </script>
</p>
</li>
<li>
<p>Second, in terms of full trajectories. Let a state–action trajectory be:</p>
<p><span class="arithmatex">\(\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\dots,s_{T-1},a_{T-1},r_T),\)</span></p>
<p>and define</p>
<p>
<script type="math/tex; mode=display">
R(\tau) = \sum_{t=0}^{T} r_{s_t,a_t}</script>
as the sum of rewards of trajectory <span class="arithmatex">\(\tau\)</span>.  </p>
<p>Let <span class="arithmatex">\(P(\tau;\theta)\)</span> denote the probability of trajectory <span class="arithmatex">\(\tau\)</span> when starting in <span class="arithmatex">\(s_0\)</span> and following policy <span class="arithmatex">\(\pi_\theta\)</span>. Then:</p>
<div class="arithmatex">\[
V(s_0,\theta) = \sum_{\tau} P(\tau;\theta) \, R(\tau).\]</div>
</li>
</ul>
<p>In this trajectory notation, our optimization objective becomes</p>
<div class="arithmatex">\[
\theta^* 
= \arg\max_{\theta} V(s_0,\theta) 
= \arg\max_{\theta} \sum_{\tau} P(\tau;\theta) \, R(\tau).
\]</div>
<p>Taking gradient yields:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) = 
\sum_{\tau} P(\tau|\theta)R(\tau) 
\nabla_\theta \log P(\tau|\theta)
\]</div>
<p>Using sampled trajectories:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m}\sum_{i=1}^{m} 
R(\tau^{(i)})
\nabla_\theta \log P(\tau^{(i)}|\theta)
\]</div>
<p>Trajectory probability:</p>
<div class="arithmatex">\[
P(\tau|\theta) =
P(s_0)
\prod_{t=0}^{T-1}
\pi_\theta(a_t|s_t)\cdot P(s_{t+1}|s_t,a_t)
\]</div>
<p>Since dynamics are independent of <span class="arithmatex">\(\theta\)</span>:</p>
<div class="arithmatex">\[
\nabla_\theta \log P(\tau|\theta) =
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t|s_t)
\]</div>
<p>Thus:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m}\sum_{i=1}^{m}
R(\tau^{(i)})
\sum_{t=0}^{T-1} 
\nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
\]</div>
<p>The term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span> is called the score function. 
It is the gradient of the log of a parameterized probability distribution and measures how sensitive the policy’s action probability is to changes in the parameters <span class="arithmatex">\(\theta\)</span>.  It plays a central role in policy gradient methods because it allows us to estimate gradients without knowing the environment dynamics, using only samples from the policy.</p>
<h4 id="reinforcement-5_policy_gradient-softmax-policy-discrete-action-spaces">Softmax Policy (Discrete Action Spaces)<a class="headerlink" href="#reinforcement-5_policy_gradient-softmax-policy-discrete-action-spaces" title="Permanent link">¶</a></h4>
<p>In discrete action spaces, a common parameterization of the policy is the softmax policy, which assigns probabilities based on exponentiated weighted features. Each action is represented using feature vector <span class="arithmatex">\(\phi(s,a)\)</span>, and the policy is defined as:</p>
<div class="arithmatex">\[
\pi_\theta(a|s) = 
\frac{e^{\phi(s,a)^T \theta}}
     {\sum_{a'} e^{\phi(s,a')^T \theta}}
\]</div>
<p>The corresponding score function is:</p>
<div class="arithmatex">\[
\nabla_\theta \log \pi_\theta(a|s)
= \phi(s,a) - \mathbb{E}_{a' \sim \pi_\theta}[\phi(s,a')]
\]</div>
<p>This means the gradient increases the probability of the selected action's features and decreases the probability of competing actions based on their expected feature values.</p>
<h4 id="reinforcement-5_policy_gradient-gaussian-policy-continuous-action-spaces">Gaussian Policy (Continuous Action Spaces)<a class="headerlink" href="#reinforcement-5_policy_gradient-gaussian-policy-continuous-action-spaces" title="Permanent link">¶</a></h4>
<p>For continuous action spaces, the Gaussian policy is a natural choice.  The policy outputs actions by sampling from a normal distribution:</p>
<div class="arithmatex">\[
a \sim \mathcal{N}(\mu(s), \sigma^2)
\]</div>
<p>The mean is a linear function of state features:</p>
<div class="arithmatex">\[
\mu(s) = \phi(s)^T \theta
\]</div>
<p>If we assume a fixed variance <span class="arithmatex">\(\sigma^2\)</span>, the score function becomes:</p>
<div class="arithmatex">\[
\nabla_\theta \log \pi_\theta(a|s)
= \frac{(a - \mu(s))}{\sigma^2} \, \phi(s)
\]</div>
<p>This tells us that the gradient increases the likelihood of actions that are close to the mean <span class="arithmatex">\(\mu(s)\)</span> and reduces the probability of actions that deviate from it.</p>
<p>Deep neural networks (and other differentiable models) can also be used<br>
to represent <span class="arithmatex">\(\pi_\theta(a|s)\)</span>, allowing score functions to be computed automatically using backpropagation.</p>
<blockquote>
<p>Intution: 
Think of a sample trajectory <span class="arithmatex">\(\tau\)</span> as something we tried — a sequence of states, actions, and rewards collected during an episode. The return <span class="arithmatex">\(R(\tau)\)</span> tells us how good that sample was (higher return means better behavior).
The gradient term <span class="arithmatex">\(\nabla_\theta \log P(\tau|\theta)\)</span> tells us how to adjust the policy parameters <span class="arithmatex">\(\theta\)</span> to make the trajectory more or less likely.
So, when we multiply them:
<script type="math/tex; mode=display">
R(\tau) \, \nabla_\theta \log P(\tau \mid \theta)</script>
we are effectively saying:
If a trajectory was good, update the policy to make it more likely to occur again.<br>
If it was bad, update the policy to make it less likely.
This simple idea is the core of policy gradient methods.</p>
</blockquote>
<h2 id="reinforcement-5_policy_gradient-reinforce-algorithm-monte-carlo-policy-gradient">REINFORCE Algorithm (Monte Carlo Policy Gradient)<a class="headerlink" href="#reinforcement-5_policy_gradient-reinforce-algorithm-monte-carlo-policy-gradient" title="Permanent link">¶</a></h2>
<p>Update rule:</p>
<div class="arithmatex">\[
\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(a_t|s_t)\, R_t
\]</div>
<p>Algorithm:</p>
<p>1: Initialize policy parameters <span class="arithmatex">\(\theta\)</span><br>
2: loop (for each episode)<br>
3: <span class="arithmatex">\(\quad\)</span> Generate a trajectory <span class="arithmatex">\(\tau = (s_0, a_0, r_1, \dots, s_T)\)</span> using <span class="arithmatex">\(\pi_\theta\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> for each time step <span class="arithmatex">\(t\)</span> in <span class="arithmatex">\(\tau\)</span><br>
5: <span class="arithmatex">\(\quad\quad\)</span> Compute return: <span class="arithmatex">\(\qquad R_t = \sum_{k=t}^{T-1} \gamma^{\,k-t} r_{k+1}\)</span><br>
6: <span class="arithmatex">\(\quad\quad\)</span> Compute policy gradient term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span><br>
7: <span class="arithmatex">\(\quad\quad\)</span> Update policy parameters: <span class="arithmatex">\(\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t)\, R_t\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> end for<br>
9: end loop  </p>
<h2 id="reinforcement-5_policy_gradient-policy-gradient-methods-mental-map">Policy Gradient Methods — Mental Map<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-gradient-methods-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                     Policy Gradient Methods
    Goal: Learn the optimal policy π* directly (no Q or V tables)
                               │
                               ▼
         Key Concept: Parameterized Policy πθ(a|s)
       ┌─────────────────────────────────────────────┐
       │ Policy is a function with parameters θ      │
       │ πθ(a|s) gives probability of taking action a│
       │ Optimization targets J(θ)=Expected Return   │
       └─────────────────────────────────────────────┘
                               │
                    Direct Policy Optimization
                               │
                               ▼
                 Optimization Objective (J(θ))
       ┌─────────────────────────────────────────────┐
       │ θ* = argmaxθ V(θ) = argmaxθ Eπθ[G₀]         │
       │ Search in parameter space for best policy   │
       └─────────────────────────────────────────────┘
                               │
                               ▼
              Two Families of Policy Optimization
       ┌────────────────────────────┬────────────────────────────┐
       │  Gradient-Free Methods     │   Gradient-Based Methods   │
       └────────────────────────────┴────────────────────────────┘
                │                                      │
                │                                      ▼
                │                          Policy Gradient Methods
                │                                      │
                ▼                                      ▼
   No gradient needed                     Uses ∇θ log πθ(a|s) * Return
   – Hill Climbing                        – REINFORCE
   – CEM, CMA                             – Actor-Critic
   – Genetic Algorithms                   – Advantage Methods
   │                                      │
   └──── Flexible &amp; parallelizable        └──── Sample efficient
                               │
                               ▼
                   Policy Gradient Core Idea
       ┌───────────────────────────────────────────────┐
       │ Increase probability of good actions          │
       │ Decrease probability of poor actions          │
       │ Gradient term: ∇θ log πθ(a|s)(score function) │
       └───────────────────────────────────────────────┘
                               │
                               ▼
                     REINFORCE Algorithm (MC)
       ┌───────────────────────────────────────────────┐
       │ Sample full episodes (Monte Carlo)            │
       │ Compute return Gt at each time step           │
       │ Update: θ ← θ + α ∇θ log πθ(a_t|s_t) * G_t    │
       └───────────────────────────────────────────────┘
                               │
                               ▼
                      Policy Parameterization
       ┌────────────────────────────┬────────────────────────────┐
       │ Softmax Policy (Discrete)  │ Gaussian Policy(Continuous)│
       │ πθ(a|s) = exp(...)         │ a ~ N(μ(s), σ²)            │
       │ ∇θ log πθ = φ - Eφ         │ ∇θ log πθ = (a-μ)/σ² * φ   │
       └────────────────────────────┴────────────────────────────┘
                               │
                               ▼
                         Final Outcome
       ┌─────────────────────────────────────────────────┐
       │ Learn π* directly (no need for Q or V tables)   │
       │ Works naturally with stochastic &amp; continuous    │
       │ Supports neural network policy parameterization │
       │ Foundation of modern deep RL (PPO, A3C, DDPG)   │
       └─────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-6_pg2" heading-number="3.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-policy-gradient-variance-reduction-and-actor-critic">Chapter 6: Policy Gradient Variance Reduction and Actor-Critic<a class="headerlink" href="#reinforcement-6_pg2-chapter-6-policy-gradient-variance-reduction-and-actor-critic" title="Permanent link">¶</a></h1>
<p>In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy <span class="arithmatex">\(\pi_\theta(a|s)\)</span> in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:</p>
<ul>
<li>
<p>High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return <span class="arithmatex">\(G_t\)</span> depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.</p>
</li>
<li>
<p>Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.</p>
</li>
<li>
<p>Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.</p>
</li>
<li>
<p>Parameter Space vs Policy Space Mismatch: A small change in policy parameters <span class="arithmatex">\(\theta\)</span> does not always translate to a small change in the policy’s behavior. For example, in a two-action policy with probability <span class="arithmatex">\(\pi_\theta(a=1)=\sigma(\theta)\)</span> (sigmoid), a slight shift in <span class="arithmatex">\(\theta\)</span> can swing the action probabilities significantly if <span class="arithmatex">\(\theta\)</span> is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.</p>
</li>
</ul>
<p>The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor–critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.</p>
<h2 id="reinforcement-6_pg2-policy-gradient-theorem-and-reinforce">Policy Gradient Theorem and REINFORCE<a class="headerlink" href="#reinforcement-6_pg2-policy-gradient-theorem-and-reinforce" title="Permanent link">¶</a></h2>
<p>n the previous chapter, we derived an expression for the gradient of the policy objective:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m} \sum_{i=1}^{m}
R(\tau^{(i)}) 
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)}),
\]</div>
<p>where each trajectory <span class="arithmatex">\(\tau^{(i)}\)</span> is generated by the current policy <span class="arithmatex">\(\pi_\theta\)</span>.</p>
<p>This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:</p>
<ul>
<li>The return <span class="arithmatex">\(R(\tau)\)</span> depends on the entire trajectory.</li>
<li>Different trajectories can have very different returns.</li>
<li>Updates become noisy, unstable, and slow to converge.</li>
</ul>
<p>In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.</p>
<p>Goal:<br>
Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.</p>
<h3 id="reinforcement-6_pg2-reducing-variance-with-baselines">Reducing Variance with Baselines<a class="headerlink" href="#reinforcement-6_pg2-reducing-variance-with-baselines" title="Permanent link">¶</a></h3>
<p>One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action’s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return.
Mathematically, we modify the gradient as follows:</p>
<div class="arithmatex">\[
\nabla_\theta \mathbb{E}_\tau [R] \;=\;
\mathbb{E}_\tau \left[
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\;
\left(\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\right)
\right].
\]</div>
<p>where <span class="arithmatex">\(b(s_t)\)</span> is an arbitrary baseline that depends on the state <span class="arithmatex">\(s_t\)</span> (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, <span class="arithmatex">\(b(s_t)\)</span> represents a reference level for the return at state <span class="arithmatex">\(s_t\)</span>; the term <span class="arithmatex">\((G_t - b(s_t))\)</span> is asking: did the action at <span class="arithmatex">\(s_t\)</span> do better or worse than this baseline expectation?</p>
<p>A particularly good choice for the baseline is the value function under the current policy, <span class="arithmatex">\(b(s_t) \approx V_{\pi}(s_t)\)</span>. This is the expected return from state <span class="arithmatex">\(s_t\)</span> if we continue following the current policy. Using <span class="arithmatex">\(V_{\pi}(s_t)\)</span> as <span class="arithmatex">\(b(s_t)\)</span> minimizes variance because it subtracts out the expected part of <span class="arithmatex">\(G_t\)</span>, leaving only the unexpected advantage of the action <span class="arithmatex">\(a_t\)</span>.
Using a value function baseline leads to defining the advantage function:
<script type="math/tex; mode=display"> A(s_t, a_t) \;=\; Q(s_t, a_t) - V(s_t), </script>
where <span class="arithmatex">\(Q(s_t,a_t)\)</span> is the expected return for taking action <span class="arithmatex">\(a_t\)</span> in <span class="arithmatex">\(s_t\)</span> and following the policy thereafter, and <span class="arithmatex">\(V(s_t)\)</span> is the expected return from <span class="arithmatex">\(s_t\)</span> on average. The advantage <span class="arithmatex">\(A(s_t,a_t)\)</span> tells us how much better or worse the chosen action was compared to the policy’s typical action at that state. If <span class="arithmatex">\(A(s_t,a_t)\)</span> is positive, the action did better than expected; if negative, it did worse than expected. Using <span class="arithmatex">\(A(s_t,a_t)\)</span> in the gradient update focuses learning on the deviations from usual outcomes.</p>
<p>Benefits of Using a Baseline (Advantage):
1.  Variance Reduction: Subtracting <span class="arithmatex">\(V(s_t)\)</span> removes the predictable part of the return, reducing the variability of the term <span class="arithmatex">\((G_t - b(s_t))\)</span>. The policy update then depends on advantage, which typically has lower variance than raw returns.
2.  Focused Learning: The update ignores outcomes that are “as expected” and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment.
3.  Unbiased Gradient: Because <span class="arithmatex">\(b(s_t)\)</span> does not depend on the action, the expected value of <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\, b(s_t)\)</span> is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance</p>
<p>Using a baseline in practice usually means we need to estimate the value function <span class="arithmatex">\(V_{\pi}(s)\)</span> for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress <span class="arithmatex">\(b(s)\)</span> toward the observed returns <span class="arithmatex">\(G_t\)</span> to improve the baseline estimate.</p>
<p>Algorithm: Policy Gradient with Baseline (Advantage Estimation)</p>
<p>1: Initialize policy parameter <span class="arithmatex">\(\theta\)</span>, baseline <span class="arithmatex">\(b(s)\)</span><br>
2: for iteration <span class="arithmatex">\(= 1, 2, \dots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect a set of trajectories by executing the current policy <span class="arithmatex">\(\pi_\theta\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> for each trajectory <span class="arithmatex">\(\tau^{(i)}\)</span> and each timestep <span class="arithmatex">\(t\)</span> do<br>
5: <span class="arithmatex">\(\quad\quad\)</span> Compute return:<br>
<span class="arithmatex">\(\quad\quad\quad G_t^{(i)} = \sum_{t'=t}^{T-1} r_{t'}^{(i)}\)</span><br>
6: <span class="arithmatex">\(\quad\quad\)</span> Compute advantage estimate:<br>
<span class="arithmatex">\(\quad\quad\quad \hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> end for<br>
8: <span class="arithmatex">\(\quad\)</span> Re-fit baseline by minimizing:<br>
<span class="arithmatex">\(\quad\quad \sum_i \sum_t \big(b(s_t^{(i)}) - G_t^{(i)}\big)^2\)</span><br>
9: <span class="arithmatex">\(\quad\)</span> Update policy parameters using gradient estimate:<br>
<span class="arithmatex">\(\quad\quad \theta \leftarrow \theta + \alpha \sum_{i,t} \nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\, \hat{A}_t^{(i)}\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> (Plug into SGD or Adam optimizer)<br>
11: end for  </p>
<p>This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect (<span class="arithmatex">\(b(s)=V_\pi(s)\)</span>), then <span class="arithmatex">\(\hat{A}t = G_t - V\pi(s_t)\)</span> is an estimate of the advantage <span class="arithmatex">\(A(s_t,a_t)\)</span>. Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.</p>
<h2 id="reinforcement-6_pg2-actorcritic-methods">Actor–Critic Methods<a class="headerlink" href="#reinforcement-6_pg2-actorcritic-methods" title="Permanent link">¶</a></h2>
<p>Using a learned baseline brings us to the idea of actor–critic algorithms. In the policy gradient with baseline above, the policy is the "actor" and the value function baseline is a "critic" that evaluates the actor’s decisions. Actor–critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.</p>
<ul>
<li>Actor: the policy <span class="arithmatex">\(\pi_\theta(a|s)\)</span> that selects actions and is updated by gradient ascent.</li>
<li>Critic: a value function <span class="arithmatex">\(V_w(s)\)</span> (with parameters <span class="arithmatex">\(w\)</span>) that estimates the return from state <span class="arithmatex">\(s\)</span> under the current policy. The critic provides the baseline or advantage estimates used in the actor’s update.</li>
</ul>
<p>Instead of waiting for full episode returns <span class="arithmatex">\(G_t\)</span>, an actor–critic uses the critic’s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic’s current estimate of advantage <span class="arithmatex">\(A(s_t,a_t)\)</span>, and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.</p>
<p>Actor update: The policy (actor) update is similar to before, but using the critic’s advantage estimate <span class="arithmatex">\(A_t\)</span> at time <span class="arithmatex">\(t\)</span>:
<script type="math/tex; mode=display"> \theta \;\leftarrow\; \theta + \alpha\, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\; A_t. </script>
Here <span class="arithmatex">\(A_t \approx Q(s_t,a_t) - V(s_t)\)</span> is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive <span class="arithmatex">\(A_t\)</span>) and away from actions that seem worse than expected.</p>
<p>Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error <span class="arithmatex">\(\delta_t\)</span> to update <span class="arithmatex">\(w\)</span>:
<script type="math/tex; mode=display"> \delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t), </script>
which measures the discrepancy between the predicted value at <span class="arithmatex">\(s_t\)</span> and the reward plus discounted value of the next state. The critic’s parameters <span class="arithmatex">\(w\)</span> are updated by a gradient step proportional to <span class="arithmatex">\(\delta_t \nabla_w V_w(s_t)\)</span> (this is essentially a TD(0) update). In practice:
<script type="math/tex; mode=display"> w \;\leftarrow\; w + \beta\, \delta_t\, \nabla_w V_w(s_t), </script>
with <span class="arithmatex">\(\beta\)</span> a critic learning rate. This update pushes the critic’s value estimate <span class="arithmatex">\(V_w(s_t)\)</span> toward the observed reward plus the estimated value of <span class="arithmatex">\(s_{t+1}\)</span>. The TD error <span class="arithmatex">\(\delta_t\)</span> is also used as an advantage estimate for the actor: notice <span class="arithmatex">\(\delta_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)\)</span> serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.</p>
<p>Why Actor–Critic? Actor–critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor–critic algorithms tend to:
- Learn faster (lower variance updates thanks to the critic’s guidance)
- Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends)
- Naturally handle continuing (non-episodic) tasks via the critic’s ongoing value estimates
- Still allow stochastic policies and continuous actions (since the actor is explicit)</p>
<p>However, actor–critics introduce bias through the critic’s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor–critic; later algorithms address this).</p>
<p>Advantage Estimation: In an actor–critic, one often uses n-step returns or more generally <span class="arithmatex">\(\lambda\)</span>-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return <span class="arithmatex">\(R^{(3)}t = r_t + \gamma r{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3})\)</span>, and define <span class="arithmatex">\(\hat{A}_t = R^{(3)}_t - V(s_t)\)</span>. Smaller <span class="arithmatex">\(n\)</span> gives lower variance but more bias; larger <span class="arithmatex">\(n\)</span> gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.</p>
<p>In summary, the actor–critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy’s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.</p>
<h2 id="reinforcement-6_pg2-limitations-of-vanilla-policy-gradient-and-trust-region-motivation">Limitations of Vanilla Policy Gradient and Trust-Region Motivation<a class="headerlink" href="#reinforcement-6_pg2-limitations-of-vanilla-policy-gradient-and-trust-region-motivation" title="Permanent link">¶</a></h2>
<p>Despite using baselines and even actor–critic methods, vanilla policy gradient algorithms (including basic actor–critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:</p>
<ul>
<li>
<p>On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy <span class="arithmatex">\(\pi_{\text{old}}\)</span> to update the current policy <span class="arithmatex">\(\pi_{\text{new}}\)</span> introduces bias, because the gradient formula assumes data comes from <span class="arithmatex">\(\pi_{\text{new}}\)</span>. In short, standard policy gradients waste a lot of data, making them sample-inefficient.</p>
</li>
<li>
<p>Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios <span class="arithmatex">\(r_t = \frac{\pi_{\text{new}}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}\)</span>. This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if <span class="arithmatex">\(\pi_{\text{new}}\)</span> differs significantly from <span class="arithmatex">\(\pi_{\text{old}}\)</span>. A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.</p>
</li>
<li>
<p>Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile – even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not “too large” in terms of its impact on the policy’s behavior.</p>
</li>
<li>
<p>Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in <span class="arithmatex">\(\theta\)</span>-space. A small change in parameters can lead to a disproportionate change in the policy’s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes ≠ small policy changes, it’s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.</p>
</li>
</ul>
<p>These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update – in other words, to stay within a “trust region” around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback–Leibler divergence as a distance between <span class="arithmatex">\(\pi_{\text{new}}\)</span> and <span class="arithmatex">\(\pi_{\text{old}}\)</span>), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor–critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.</p></body></html></section><section class="print-page" id="reinforcement-7_gae" heading-number="3.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo">Chapter 7: Advances in Policy Optimization – GAE, TRPO, and PPO<a class="headerlink" href="#reinforcement-7_gae-chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo" title="Permanent link">¶</a></h1>
<p>In the previous chapter, we improved the foundation of policy gradients by reducing variance (using baselines) and introducing actor–critic methods. We also noted that unrestricted policy updates can be unstable and sample-inefficient. In this chapter, we present modern advances in policy optimization that build on those ideas to achieve much better performance in practice. We focus on two main developments: Generalized Advantage Estimation (GAE), which refines how we estimate advantages to balance bias and variance, and trust-region methods (specifically Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)) that ensure updates do not destabilize the policy. These techniques enable more sample-efficient, stable learning by reusing data safely and preventing large detrimental policy shifts.</p>
<h2 id="reinforcement-7_gae-generalized-advantage-estimation-gae">Generalized Advantage Estimation (GAE)<a class="headerlink" href="#reinforcement-7_gae-generalized-advantage-estimation-gae" title="Permanent link">¶</a></h2>
<p>Accurate and low-variance advantage estimates are crucial for effective policy gradient updates. Recall that the policy gradient update uses the term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\,A(s_t,a_t)\)</span> – if <span class="arithmatex">\(A(s_t,a_t)\)</span> is noisy or biased, it can severely affect learning. Advantage can be estimated via:
- Monte Carlo returns: <span class="arithmatex">\(A_t = G_t - V(s_t)\)</span> using the full return <span class="arithmatex">\(G_t\)</span> (summing all future rewards until episode end). This is an unbiased estimator of the true advantage, but it has very high variance because it includes all random future outcomes.</p>
<ul>
<li>
<p>One-step TD returns: <span class="arithmatex">\(A_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)\)</span>, using the critic’s bootstrapped estimate of the future. This one-step advantage (equivalently the TD error <span class="arithmatex">\(\delta_t\)</span>) has much lower variance (it relies on the learned value for the next state) but is biased by function approximation and by truncating the return after one step.</p>
</li>
<li>
<p>n-Step returns:We can also use intermediate approaches, for example a 2-step return <span class="arithmatex">\(R^{(2)}t = r_t + \gamma r{t+1} + \gamma^2 V(s_{t+2})\)</span> giving an advantage <span class="arithmatex">\(\hat{A}^{(2)}_t = R^{(2)}_t - V(s_t)\)</span>. In general, an n-step advantage estimator can be written as:</p>
<div class="arithmatex">\[A^{t}(n) = \sum_{i=0}^{n-1} \gamma^{i} r_{t+i+1} + \gamma^{n} V(s_{t+n}) -V(s_{t})\]</div>
<p>which blends <span class="arithmatex">\(n\)</span> actual rewards with a bootstrap at time <span class="arithmatex">\(t+n\)</span>. Smaller <span class="arithmatex">\(n\)</span> (like 1) means more bias (due to heavy reliance on <span class="arithmatex">\(V\)</span>) but low variance; larger <span class="arithmatex">\(n\)</span> (approaching the episode length) reduces bias but increases variance.</p>
</li>
</ul>
<p>The pattern becomes clearer if we express these in terms of the TD error <span class="arithmatex">\(\delta_t\)</span> (the one-step advantage at <span class="arithmatex">\(t\)</span>):</p>
<p>
<script type="math/tex; mode=display"> \delta_t \;=\; r_t + \gamma V(s_{t+1}) - V(s_t). </script>
- For a 1-step return, <span class="arithmatex">\(\hat{A}^{(1)}_t = \delta_t\)</span>.
- For a 2-step return, <span class="arithmatex">\(\hat{A}^{(2)}t = \delta_t + \gamma\,\delta\)</span>.
- For an <span class="arithmatex">\(n\)</span>-step return, <span class="arithmatex">\(\hat{A}^{(n)}t = \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{n-1}\delta_{t+n-1}\)</span></p>
<p>Each additional term <span class="arithmatex">\(\gamma^i \delta_{t+i}\)</span> extends the return by one more step of real reward before bootstrapping, increasing bias a bit (since it assumes the later <span class="arithmatex">\(\delta\)</span> terms are based on an approximate <span class="arithmatex">\(V\)</span>) but capturing more actual reward outcomes (reducing variance less).</p>
<p>Generalized Advantage Estimation (GAE) takes this idea to its logical conclusion by forming a weighted sum of all n-step advantages, with exponentially decreasing weights. Instead of picking a fixed <span class="arithmatex">\(n\)</span>, GAE uses a parameter <span class="arithmatex">\(0 \le \lambda \le 1\)</span> to blend advantages of different lengths:</p>
<div class="arithmatex">\[\hat{A}^{\text{GAE}(\gamma,\lambda)}_t \;=\; (1-\lambda)\Big(\hat{A}^{(1)}_t + \lambda\,\hat{A}^{(2)}_t + \lambda^2\,\hat{A}^{(3)}_t + \cdots\Big)\]</div>
<p>This infinite series can be shown to simplify to a very convenient form:</p>
<div class="arithmatex">\[\hat{A}_t^{\text{GAE}(\gamma,\lambda)} = \sum_{i=0}^{\infty} (\gamma \lambda)^i\delta_{t+i}\]</div>
<p>which is an exponentially-weighted sum of the future TD errors. In practice, this is implemented with a simple recursion running backward through each trajectory (since it’s a sum of discounted TD errors).</p>
<p>Key intuition: <span class="arithmatex">\(\lambda\)</span> controls the bias–variance trade-off in advantage estimation:</p>
<ul>
<li>
<p><span class="arithmatex">\(\lambda = 0\)</span> uses only the one-step TD error: <span class="arithmatex">\(\hat{A}^{\text{GAE}(0)}_t = \delta_t\)</span>. This is the lowest-variance, highest-bias estimator (similar to TD(0) advantage)[21].</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda = 1\)</span> uses an infinitely long sum of un-discounted TD errors, which in theory equals the full Monte Carlo return advantage (since all bootstrapping is deferred to the end). This is unbiased (in the limit of exact <span class="arithmatex">\(V\)</span>) but highest variance – essentially Monte Carlo estimation.</p>
</li>
<li>
<p>Intermediate <span class="arithmatex">\(0&lt;\lambda&lt;1\)</span> gives a mixture. A typical choice is <span class="arithmatex">\(\lambda = 0.95\)</span> in many applications, which provides a good balance (mostly long-horizon returns with a bit of bootstrapping to damp variance).</p>
</li>
</ul>
<p>GAE is not introducing a new kind of return; rather, it generalizes existing returns. It smoothly interpolates between TD and Monte Carlo methods. When <span class="arithmatex">\(\lambda\)</span> is low, GAE trusts the critic more (using more bootstrapped estimates); when <span class="arithmatex">\(\lambda\)</span> is high, GAE leans toward actual returns over many steps.
In modern actor–critic algorithms (including TRPO and PPO), GAE is used to compute the advantage for each state-action in a batch. A typical implementation for each iteration is:</p>
<ol>
<li>Collect trajectories using the current policy <span class="arithmatex">\(\pi_{\theta}\)</span> (e.g. run <span class="arithmatex">\(N\)</span> episodes or <span class="arithmatex">\(T\)</span> time steps of experience).</li>
<li>Compute state values <span class="arithmatex">\(V(s_t)\)</span> for each state visited (using the current value function estimate).</li>
<li>Compute TD residuals <span class="arithmatex">\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span> for each time step.</li>
<li>
<p>Apply GAE formula: going from <span class="arithmatex">\(t=T-1\)</span> down to <span class="arithmatex">\(0\)</span>, accumulate <span class="arithmatex">\(\hat{A}_t = \delta_t + \gamma \lambda, \hat{A}{t+1}\)</span>, with <span class="arithmatex">\(\hat{A}_{T} = 0\)</span>. This yields <span class="arithmatex">\(\hat{A}_t \approx \sum{i\ge0} (\gamma \lambda)^i \delta{t+i}\)</span>.</p>
</li>
<li>
<p>Use Advantages for Update: These <span class="arithmatex">\(\hat{A}_t\)</span> values serve as the advantage estimates in the policy gradient update. Simultaneously, you can compute proxy returns for the critic by adding <span class="arithmatex">\(\hat{A}_t\)</span> to the baseline <span class="arithmatex">\(V(s_t)\)</span> (i.e. <span class="arithmatex">\(\hat{R}_t = \hat{A}_t + V(s_t)\)</span>, an estimate of the actual return) and use those to update the value function parameters.</p>
</li>
</ol>
<p>The result of GAE is a much smoother, lower-variance advantage signal for the actor, without introducing too much bias. Empirically, this greatly stabilizes training: the policy doesn’t overreact to single high-return episodes, and it doesn’t ignore long-term outcomes either. GAE essentially bridges the gap between the high-variance Monte Carlo world of Chapter 5 and the low-variance TD world of Chapter 3–4, and it has become a standard component in virtually all modern policy optimization algorithms.</p>
<h2 id="reinforcement-7_gae-kl-divergence-constraints-and-surrogate-objectives">KL Divergence Constraints and Surrogate Objectives<a class="headerlink" href="#reinforcement-7_gae-kl-divergence-constraints-and-surrogate-objectives" title="Permanent link">¶</a></h2>
<p>We now turn to the question of stable policy updates. As discussed, a major issue with vanilla policy gradient is that a single update can accidentally push the policy into a disastrous region (because the gradient is computed at the current policy but we might step too far). To make updates safer, we want to constrain how much the policy changes at each step. A natural way to measure change between the old policy <span class="arithmatex">\(\pi_{\text{old}}\)</span> and a new policy <span class="arithmatex">\(\pi_{\text{new}}\)</span> is to use the Kullback–Leibler (KL) divergence. For example, we can require:</p>
<div class="arithmatex">\[\mathbb{E}_{s \sim d^{\pi_{\text{old}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\text{new}}(\cdot \mid s)\,\|\,\pi_{\text{old}}(\cdot \mid s)\bigr)
\right]
\le \delta\]</div>
<p>for some small <span class="arithmatex">\(\delta\)</span>. This means that on average over states (under the old policy’s state distribution <span class="arithmatex">\(d_{\pi_{\text{old}}}\)</span>), the new policy’s probability distribution is not too far from the old policy’s distribution. A small KL divergence ensures the policies behave similarly, limiting the “surprise” from one update.</p>
<p>But how do we optimize under such a constraint? We need an objective function that tells us whether <span class="arithmatex">\(\pi_{\text{new}}\)</span> is better than <span class="arithmatex">\(\pi_{\text{old}}\)</span>. Fortunately, theory provides a useful tool: a surrogate objective that approximates the change in performance if the policy change is small. One version, derived from the policy performance difference lemma and monotonic improvement theorem, is:</p>
<div class="arithmatex">\[L_{\pi_{\text{old}}}(\pi_{\text{new}})
=
\mathbb{E}_{s,a \sim \pi_{\text{old}}}
\left[
\frac{\pi_{\text{new}}(a \mid s)}{\pi_{\text{old}}(a \mid s)}
A_{\pi_{\text{old}}}(s,a)
\right]\]</div>
<p>This is an objective functional—it evaluates the new policy using samples from the old policy, weighting rewards by the importance ratio <span class="arithmatex">\(r(s,a) = \pi_{\text{new}}(a|s)/\pi_{\text{old}}(a|s)\)</span>. Intuitively, <span class="arithmatex">\(L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span> is asking: if the old policy visited state <span class="arithmatex">\(s\)</span> and took action <span class="arithmatex">\(a\)</span>, how good would that decision be under the new policy’s probabilities? Actions that the new policy wants to do more of (<span class="arithmatex">\(r &gt; 1\)</span>) will contribute their advantage (good or bad) proportionally more.</p>
<p>Critically, one can show that if <span class="arithmatex">\(\pi_{\text{new}}\)</span> is very close to <span class="arithmatex">\(\pi_{\text{old}}\)</span> (in KL terms), then improving this surrogate <span class="arithmatex">\(L\)</span> guarantees an improvement in the true return <span class="arithmatex">\(J(\pi)\)</span>. Specifically, there is a bound such that:</p>
<div class="arithmatex">\[J(\pi_{\text{new}})
\ge
J(\pi_{\text{old}})
+
L_{\pi_{\text{old}}}(\pi_{\text{new}})
-
C \,
\mathbb{E}_{s \sim d^{\pi_{\text{old}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\text{new}} \,\|\, \pi_{\text{old}}\bigr)[s]
\right]\]</div>
<p>for some constant <span class="arithmatex">\(C\)</span> related to horizon and policy support. When the KL divergence is small, the last term is second-order (negligible), so roughly we get <span class="arithmatex">\(J(\pi_{\text{new}}) \gtrapprox J(\pi_{\text{old}}) + L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span>. In other words, maximizing <span class="arithmatex">\(L\)</span> while keeping KL small ensures monotonic improvement: each update should not reduce true performance.</p>
<p>This insight leads directly to a constrained optimization formulation for safe policy updates:</p>
<ul>
<li>Objective: Maximize the surrogate <span class="arithmatex">\(L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span> (i.e. maximize expected advantage-weighted probability ratios).</li>
<li>Constraint: Limit the policy divergence via <span class="arithmatex">\(D_{\mathrm{KL}}(\pi_{\text{new}}\Vert \pi_{\text{old}}) \le \delta\)</span> (for some small <span class="arithmatex">\(\delta\)</span>).
Algorithms that implement this idea are called trust-region methods, because they optimize the policy within a trust region of the old policy. Next, we discuss two prominent algorithms: TRPO, which tackles the constrained problem directly (with some approximations), and PPO, which simplifies it into an easier unconstrained loss function.</li>
</ul>
<h2 id="reinforcement-7_gae-trust-region-policy-optimization">Trust Region Policy Optimization<a class="headerlink" href="#reinforcement-7_gae-trust-region-policy-optimization" title="Permanent link">¶</a></h2>
<p>Trust Region Policy Optimization (TRPO) is a seminal algorithm that explicitly embodies the constrained update approach. TRPO chooses a new policy by approximately solving:</p>
<div class="arithmatex">\[\max_{\theta_{\text{new}}} \; L_{\theta_{\text{old}}}(\theta_{\text{new}})
\quad \text{s.t.} \quad
\mathbb{E}_{s \sim d^{\pi_{\theta_{\text{old}}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\theta_{\text{new}}} \,\|\, \pi_{\theta_{\text{old}}}\bigr)
\right]
\le \delta\]</div>
<p>where <span class="arithmatex">\(L_{\theta_{\text{old}}}(\theta_{\text{new}})\)</span> is the surrogate objective defined above, and <span class="arithmatex">\(\delta\)</span> is a small trust-region threshold. In practice, solving this exactly is difficult due to the infinite-dimensional policy space. TRPO makes it tractable by using a few key ideas:</p>
<ul>
<li>
<p>Approximating the constraint via a quadratic expansion of the KL divergence (which yields a Fisher Information Matrix). This turns the problem into something like a second-order update (a natural gradient step). In fact, TRPO’s solution can be shown to correspond to a natural gradient ascent:</p>
<p><span class="arithmatex">\(\theta_{\text{new}} = \theta_{\text{old}} + \sqrt{\frac{2\delta}{g^T F^{-1} g}}\; F^{-1} g\)</span>$</p>
<p>where <span class="arithmatex">\(g = \nabla_\theta L\)</span> and <span class="arithmatex">\(F\)</span> is the Fisher matrix. This ensures the KL constraint is satisfied approximately, and is equivalent to scaling the gradient by <span class="arithmatex">\(F^{-1}\)</span>. In simpler terms, TRPO updates <span class="arithmatex">\(\theta\)</span> in a direction that accounts for the curvature of the policy space, so that the change in policy (KL) is proportional to the step size.</p>
</li>
<li>
<p>Using a line search to ensure the new policy actually improves <span class="arithmatex">\(J(\pi)\)</span>. TRPO will back off the step size if the updated policy violates the constraint or fails to achieve a performance improvement. This safeguard maintains the monotonic improvement guarantee in practice.</p>
</li>
</ul>
<p>A simplified outline of TRPO is:</p>
<ol>
<li>Collect trajectories with the current policy <span class="arithmatex">\(\pi_{\theta_{\text{old}}}\)</span>.</li>
<li>Estimate advantages <span class="arithmatex">\(\hat{A}_t\)</span> for each time step (using GAE or another method for high-quality advantage estimates).</li>
<li>Compute surrogate objective <span class="arithmatex">\(L(\theta) = \mathbb{E}[r_t(\theta), \hat{A}t]\)</span> where <span class="arithmatex">\(r_t(\theta) = \frac{\pi{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span>.</li>
<li>Approximate KL constraint: Compute the policy gradient <span class="arithmatex">\(\nabla_\theta L\)</span> and the Fisher matrix <span class="arithmatex">\(F\)</span> (via sample-based estimation of the Hessian of the KL divergence). Solve for the update direction <span class="arithmatex">\(p \approx F^{-1} \nabla_\theta L\)</span> (e.g. using conjugate gradient).</li>
<li>Line search: Scale and apply the update step <span class="arithmatex">\(\theta \leftarrow \theta + p\)</span> gradually, checking the KL and improvement. Stop when the KL constraint or improvement criterion is satisfied.</li>
</ol>
<p>TRPO’s updates are therefore conservative by design – they will only take as large a step as can be trusted not to degrade performance. TRPO was influential because it demonstrated much more stable and reliable training on complex continuous control tasks than vanilla policy gradient.</p>
<p>Strengths and Weaknesses of TRPO: TRPO offers a theoretical guarantee of non-destructive updates – under certain assumptions, each iteration is guaranteed to improve or at least not decrease performance. It uses a natural gradient approach that respects the geometry of policy space, which is more effective than an arbitrary gradient in parameter space. However, TRPO comes at a cost: it requires calculating second-order information (the Fisher matrix), and implementing the conjugate gradient solver and line search adds complexity. The algorithm can be slower per iteration and is more complex to code and tune. In practice, TRPO, while effective, proved somewhat cumbersome for large-scale problems due to these complexities.</p>
<h2 id="reinforcement-7_gae-proximal-policy-optimization">Proximal Policy Optimization<a class="headerlink" href="#reinforcement-7_gae-proximal-policy-optimization" title="Permanent link">¶</a></h2>
<p>Proximal Policy Optimization (PPO) was introduced as a simpler, more user-friendly variant of TRPO that achieves similar results with only first-order optimization. The core idea of PPO is to keep the spirit of trust-region updates (don’t move the policy too far in one go) but implement it via a relaxed objective that can be optimized with standard stochastic gradient descent.
There are two main variants of PPO:</p>
<h3 id="reinforcement-7_gae-kl-penalty-objective">KL-Penalty Objective:<a class="headerlink" href="#reinforcement-7_gae-kl-penalty-objective" title="Permanent link">¶</a></h3>
<p>One version of PPO adds the KL-divergence as a penalty to the objective rather than a hard constraint. The objective becomes:</p>
<div class="arithmatex">\[J_{\text{PPO-KL}}(\theta)
=
\mathbb{E}\!\left[ r_t(\theta)\, \hat{A}^t \right]
-
\beta \,\mathbb{E}\!\left[
D_{\mathrm{KL}}\!\left(\pi_{\theta} \,\|\, \pi_{\theta_{\text{old}}}\right)
\right]\]</div>
<p>where <span class="arithmatex">\(\beta\)</span> is a coefficient determining how strongly to penalize deviation from the old policy. If the KL divergence in an update becomes too large, <span class="arithmatex">\(\beta\)</span> can be adjusted (increased) to enforce smaller steps in subsequent updates. This approach maintains a soft notion of a trust region.</p>
<h4 id="reinforcement-7_gae-algorithm-ppo-with-kl-penalty">Algorithm (PPO with KL Penalty)<a class="headerlink" href="#reinforcement-7_gae-algorithm-ppo-with-kl-penalty" title="Permanent link">¶</a></h4>
<p>1: Input: initial policy parameters <span class="arithmatex">\(\theta_0\)</span>, initial KL penalty <span class="arithmatex">\(\beta_0\)</span>, target KL-divergence <span class="arithmatex">\(\delta\)</span><br>
2: for <span class="arithmatex">\(k = 0, 1, 2, \ldots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect set of partial trajectories <span class="arithmatex">\(\mathcal{D}_k\)</span> using policy <span class="arithmatex">\(\pi_k = \pi(\theta_k)\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Estimate advantages <span class="arithmatex">\(\hat{A}^t_k\)</span> using any advantage estimation algorithm<br>
5: <span class="arithmatex">\(\quad\)</span> Compute policy update by approximately solving<br>
<span class="arithmatex">\(\quad\quad\)</span> <span class="arithmatex">\(\theta_{k+1} = \arg\max_\theta \; L_{\theta_k}(\theta) - \beta_k \hat{D}_{KL}(\theta \,\|\, \theta_k)\)</span><br>
6: <span class="arithmatex">\(\quad\)</span> Implement this optimization with <span class="arithmatex">\(K\)</span> steps of minibatch SGD (e.g., Adam)<br>
7: <span class="arithmatex">\(\quad\)</span> Measure actual KL: <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k)\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \ge 1.5\delta\)</span> then<br>
9: <span class="arithmatex">\(\quad\quad\)</span> Increase penalty: <span class="arithmatex">\(\beta_{k+1} = 2\beta_k\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> else if <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \le \delta/1.5\)</span> then<br>
11: <span class="arithmatex">\(\quad\quad\)</span> Decrease penalty: <span class="arithmatex">\(\beta_{k+1} = \beta_k/2\)</span><br>
12: <span class="arithmatex">\(\quad\)</span> end if<br>
13: end for  </p>
<h3 id="reinforcement-7_gae-clipped-surrogate-objective-ppo-clip">Clipped Surrogate Objective (PPO-Clip):<a class="headerlink" href="#reinforcement-7_gae-clipped-surrogate-objective-ppo-clip" title="Permanent link">¶</a></h3>
<p>The more popular variant of PPO uses a clipped surrogate objective to restrict policy updates:</p>
<div class="arithmatex">\[L^\text{CLIP}(\theta)
=
\mathbb{E}_{t}\!\left[
\min\!\Big(
r_t(\theta)\,\hat{A}^t,\;
\text{clip}\!\big(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,\hat{A}^t
\Big)
\right]\]</div>
<p>where <span class="arithmatex">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span> as before, and <span class="arithmatex">\(\epsilon\)</span> is a small hyperparameter (e.g. 0.1 or 0.2) that defines the clipping range. This objective says: if the new policy’s probability ratio <span class="arithmatex">\(r_t(\theta)\)</span> stays within <span class="arithmatex">\([1-\epsilon,\,1+\epsilon]\)</span>, we use the normal surrogate <span class="arithmatex">\(r_t \hat{A}_t\)</span>. But if <span class="arithmatex">\(r_t\)</span> tries to go outside this range (meaning the policy probability for an action has changed dramatically), we clip <span class="arithmatex">\(r_t\)</span> to either <span class="arithmatex">\(1+\epsilon\)</span> or <span class="arithmatex">\(1-\epsilon\)</span> before multiplying by <span class="arithmatex">\(\hat{A}_t\)</span>. Effectively, the advantage contribution is capped once the policy deviates too much from the old policy.</p>
<p>The clipped objective is not exactly the original constrained problem, but it serves a similar purpose: it removes the incentive for the optimizer to push <span class="arithmatex">\(r_t\)</span> outside of <span class="arithmatex">\([1-\epsilon,1+\epsilon]\)</span>. If increasing <span class="arithmatex">\(|\theta|\)</span> further doesn’t increase the objective (because the min() will select the clipped term), then overly large policy changes are discouraged.</p>
<p>Why Clipping Works: Clipping is a simple heuristic, but it has proven extremely effective:</p>
<ul>
<li>
<p>It enforces a soft trust region by preventing extreme updates for any single state-action probability. The policy can still change, but not so much that any one probability ratio blows up.</p>
</li>
<li>
<p>It avoids the complexity of solving a constrained optimization or computing second-order derivatives – we can just do standard SGD on <span class="arithmatex">\(L^{CLIP}(\theta)\)</span>.</p>
</li>
<li>
<p>It keeps importance sampling ratios near 1, which means the algorithm can safely perform multiple epochs of updates on the same batch of data without the estimates drifting too far. This directly improves sample efficiency (unlike vanilla policy gradient, PPO typically updates each batch for several epochs).</p>
</li>
</ul>
<h4 id="reinforcement-7_gae-ppo-clipped-algorithm">PPO (Clipped) Algorithm<a class="headerlink" href="#reinforcement-7_gae-ppo-clipped-algorithm" title="Permanent link">¶</a></h4>
<p>1: Input: initial policy parameters <span class="arithmatex">\(\theta_0\)</span>, clipping threshold <span class="arithmatex">\(\epsilon\)</span><br>
2: for <span class="arithmatex">\(k = 0, 1, 2, \ldots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect a set of partial trajectories <span class="arithmatex">\(\mathcal{D}_k\)</span> using policy <span class="arithmatex">\(\pi_k = \pi(\theta_k)\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Estimate advantages <span class="arithmatex">\(\hat{A}^{\,t}_k\)</span> using any advantage estimation algorithm (e.g., GAE)<br>
5: <span class="arithmatex">\(\quad\)</span> Define the clipped surrogate objective<br>
<span class="arithmatex">\(\quad\quad\)</span><br>
<script type="math/tex; mode=display">
\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)
= 
\mathbb{E}_{\tau \sim \pi_{\theta_k}}
\left[
\sum_{t=0}^{T}
\min\!\left(
r_t(\theta)\,\hat{A}^t_k,\;
\operatorname{clip}\!\left(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\right)\hat{A}^t_k
\right)
\right]
</script>
6: <span class="arithmatex">\(\quad\)</span> Update policy parameters with several epochs of minibatch SGD to approximately maximize <span class="arithmatex">\(\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> Set <span class="arithmatex">\(\theta_{k+1}\)</span> to the resulting parameters<br>
8: end for  </p>
<p>In practice, PPO with clipping has become one of the most widely used RL algorithms because it strikes a good balance between performance and simplicity. It is relatively easy to implement (compared to TRPO) and has been found to be robust across many tasks and hyperparameters. While it doesn’t guarantee monotonic improvement in theory, in practice it achieves stable training behavior very similar to TRPO.</p>
<p>In modern practice, PPO is the dominant choice for policy optimization in deep RL, due to its relative simplicity and strong performance across many environments. TRPO is still important conceptually (and sometimes used in scenarios where theoretical guarantees are desired), but PPO’s convenience usually wins out.</p>
<h2 id="reinforcement-7_gae-putting-it-together-sample-efficiency-stability-and-monotonic-improvement">Putting It Together: Sample Efficiency, Stability, and Monotonic Improvement<a class="headerlink" href="#reinforcement-7_gae-putting-it-together-sample-efficiency-stability-and-monotonic-improvement" title="Permanent link">¶</a></h2>
<p>The advances covered in this chapter are often used together in state-of-the-art algorithms:</p>
<ul>
<li>
<p>Generalized Advantage Estimation (GAE) provides high-quality advantage estimates that significantly reduce variance without too much bias. This means we can get away with smaller batch sizes or fewer episodes to get a good learning signal – improving sample efficiency.</p>
</li>
<li>
<p>Trust-region update rules (TRPO/PPO) ensure that each policy update is safe and stable – the policy doesn’t change erratically, preventing the kind of catastrophic drops in reward that naive policy gradients can suffer. By keeping policy changes small (via KL constraints or clipping), these methods enable multiple updates on the same batch of data (improving data efficiency) and maintain policy monotonicity, i.e. each update is expected to improve or at least not significantly degrade performance.</p>
</li>
<li>
<p>In practice, an algorithm like PPO with GAE is an actor–critic method that uses all these ideas: an actor policy updated with a clipped surrogate objective (making updates stable), a critic to approximate <span class="arithmatex">\(V(s)\)</span> (enabling advantage estimation), GAE to compute advantages (trading off bias/variance), and typically multiple gradient epochs per batch to squeeze more learning out of each sample. This combination has proven remarkably successful in domains from simulated control tasks to games.</p>
</li>
</ul>
<p>By building on the foundational policy gradient framework and addressing its shortcomings, GAE and trust-region approaches have made deep reinforcement learning much more practical and reliable. They illustrate how theoretical insights (performance bounds, policy geometry) and practical tricks (advantage normalization, clipping) come together to yield algorithms that can solve challenging RL problems while using reasonable amounts of training data and maintaining stability throughout learning. Each component – be it advantage estimation or constrained updates – plays a role in ensuring that learning is as efficient, stable, and monotonic as possible. Together, they represent the state-of-the-art toolkit for policy optimization in reinforcement learning.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>REINFORCE</td>
<td>MC return-based policy gradient</td>
<td>Simple, unbiased</td>
<td>Very high variance</td>
</tr>
<tr>
<td>Actor–Critic</td>
<td>TD baseline value function</td>
<td>More sample-efficient</td>
<td>Requires critic</td>
</tr>
<tr>
<td>Advantage Actor–Critic</td>
<td>Uses <span class="arithmatex">\(A(s,a)\)</span> for updates</td>
<td>Best bias–variance trade</td>
<td>Needs accurate value est.</td>
</tr>
<tr>
<td>TRPO</td>
<td>Trust-region with KL constraint</td>
<td>Strong theory, stable</td>
<td>Complex, second-order</td>
</tr>
<tr>
<td>PPO</td>
<td>Clipped/penalized surrogate objective</td>
<td>Simple, stable, popular</td>
<td>Heuristic, tuning needed</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-7_gae-mental-map">Mental Map<a class="headerlink" href="#reinforcement-7_gae-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                  Advanced Policy Gradient Methods
     Goal: Fix limitations of vanilla PG (variance, stability, KL control)
                               │
                               ▼
             Core Challenges in Policy Gradient Methods
       ┌────────────────────────────────────────────────────────┐
       │ High variance (MC returns)                             │
       │ Poor sample efficiency (on-policy only)                │
       │ Sensitive to step size → catastrophic policy collapse  │
       │ Small θ change ≠ small policy change                   │
       │ Reusing old data is unstable                           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Variance Reduction (Baselines)
       ┌────────────────────────────────────────────────────────┐
       │ Introduce baseline b(s) → subtract expectation         │
       │ Keeps estimator unbiased                               │
       │ Good choice: b(s)= V(s) → yields Advantage A(s,a)      │
       │ Update based on: how much action outperformed expected │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                       Advantage Function A(s,a)
       ┌────────────────────────────────────────────────────────┐
       │ A(s,a) = Q(s,a) – V(s)                                 │
       │ Measures how much BETTER the action was vs average     │
       │ Positive → increase πθ(a|s); Negative → decrease it    │
       │ Major variance reduction – foundation of Actor–Critic  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                         Actor–Critic Framework
       ┌────────────────────────────────────────────────────────┐
       │ Actor: policy πθ(a|s)                                  │
       │ Critic: value function V(s;w) estimates baseline       │
       │ TD error δt reduces variance (bootstrapping)           │
       │ Faster, more sample-efficient than REINFORCE           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Target Estimation for the Critic
       ┌────────────────────────────┬────────────────────────────┐
       │ Monte Carlo (∞-step)       │  TD (1-step)               │
       │ + Unbiased                 │  + Low variance            │
       │ – High variance            │  – Biased                  │
       ├────────────────────────────┴────────────────────────────┤
       │ n-Step Returns: Blend of TD and MC                      │
       │ Control bias–variance by choosing n                     │
       │ Larger n → MC-like; smaller n → TD-like                 │
       └─────────────────────────────────────────────────────────┘
                               │
                               ▼
             Fundamental Problems with Vanilla Policy Gradient
       ┌────────────────────────────────────────────────────────┐
       │ Uses each batch for ONE gradient step (on-policy)      │
       │ Step size is unstable → huge performance collapse      │
       │ Small changes in θ → large unintended policy changes   │
       │ Need mechanism to limit POLICY CHANGE, not θ change    │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
            Safe Policy Improvement Theory → TRPO &amp; PPO
       ┌────────────────────────────────────────────────────────┐
       │ Policy Performance Difference Lemma                    │
       │   J(π') − J(π) = Eπ' [Aπ(s,a)]                         │
       │ KL Divergence as policy distance metric                │
       │   D_KL(π'||π) small → safe update                      │
       │ Monotonic Improvement Bound                            │
       │   Lower bound on J(π') using surrogate loss Lπ(π')     │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                   Surrogate Objective for Safe Updates
       ┌────────────────────────────────────────────────────────┐
       │ Lπ(π') = E[ (π'(a|s)/π(a|s)) * Aπ(s,a) ]               │
       │ Importance sampling + KL regularization                │
       │ Foundation of Trust-Region Policy Optimization (TRPO)  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
              Proximal Policy Optimization (PPO) – Key Ideas
       ┌────────────────────────────┬────────────────────────────┐
       │ PPO-KL Penalty             │ PPO-Clipped Objective      │
       │ Adds β·KL to loss          │ Clips ratio r_t(θ) to      │
       │ Adjust β adaptively        │ [1−ε, 1+ε] to prevent      │
       │ Prevents large updates     │ destructive policy jumps   │
       └────────────────────────────┴────────────────────────────┘
                               │
                               ▼
                         PPO Algorithm Summary
       ┌────────────────────────────────────────────────────────┐
       │ 1. Collect trajectories from old policy                │
       │ 2. Estimate advantages Â_t (GAE, TD, etc.)            │
       │ 3. Optimize clipped surrogate for many epochs          │
       │ 4. Update parameters safely                            │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                          Final Outcome (Chapter 6)
       ┌────────────────────────────────────────────────────────┐
       │ Stable and efficient policy optimization               │
       │ Reuse data safely across multiple updates              │
       │ Avoid catastrophic policy collapse                     │
       │ Foundation of modern deep RL algorithms                │
       │ (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    │
       └────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-8_imitation_learning" heading-number="3.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-8-imitation-learning">Chapter 8: Imitation Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-chapter-8-imitation-learning" title="Permanent link">¶</a></h1>
<p>In previous chapters, we focused on reinforcement learning with explicit reward signals guiding the agent's behavior. We assumed that a well-defined reward function <span class="arithmatex">\(R(s,a)\)</span> was provided as part of the MDP, and the agent’s goal was to learn a policy that maximizes cumulative reward. But what if specifying the reward is difficult or the agent cannot safely explore to learn from reward? Imitation Learning (IL) addresses these scenarios by leveraging expert demonstrations instead of explicit rewards.</p>
<p>Imitation Learning allows an agent to learn how to act by mimicking an expert’s behavior, rather than by maximizing a hand-crafted reward.</p>
<h2 id="reinforcement-8_imitation_learning-motivation-the-case-for-learning-from-demonstrations">Motivation: The Case for Learning from Demonstrations<a class="headerlink" href="#reinforcement-8_imitation_learning-motivation-the-case-for-learning-from-demonstrations" title="Permanent link">¶</a></h2>
<p>Designing a reward function that truly captures the desired behavior can be extremely challenging. A misspecified reward can lead to unintended behaviors (reward hacking) or require exhaustive tuning. Even with a good reward, some environments present sparse rewards (e.g. only a success/failure signal at the very end of an episode) – making pure trial-and-error learning inefficient. In other cases, unsafe exploration is a concern: letting an agent freely explore (as classic RL would) could be dangerous or costly (imagine a self-driving car learning by crashing to discover that crashing is bad).</p>
<p>However, in many of these settings expert behavior is available: we might have logs of human drivers driving safely, or demonstrations of a robot performing the task. Imitation Learning leverages this data. Instead of specifying what to do via a reward function, we show the agent how to do it via example trajectories. The agent's objective is then to imitate the expert as closely as possible.</p>
<p>This paradigm contrasts with reward-based RL in key ways:</p>
<ul>
<li>
<p>Reward-Based RL: The agent explores and learns by trial-and-error, guided by a numeric reward signal for feedback. It requires careful reward design and often extensive exploration.</p>
</li>
<li>
<p>Imitation Learning: The agent learns from demonstrations of the desired behavior, treating the expert’s actions as ground truth. No explicit reward is needed to train; learning is driven by matching the expert's behavior.</p>
</li>
</ul>
<p>By learning from an expert, IL can produce competent policies much faster and safer in these scenarios. It essentially sidesteps the credit assignment problem of RL (because the "right" action is directly provided by the expert) and avoids dangerous exploration. In domains like autonomous driving, robotics, or any task where a human can demonstrate the skill, IL offers a powerful shortcut to get an agent up to a reasonable performance.</p>
<h2 id="reinforcement-8_imitation_learning-imitation-learning-problem-setup">Imitation Learning Problem Setup<a class="headerlink" href="#reinforcement-8_imitation_learning-imitation-learning-problem-setup" title="Permanent link">¶</a></h2>
<p>Formally, we can describe the imitation learning scenario using the same environment structure as an MDP <span class="arithmatex">\((S, A, P, R, \gamma)\)</span> except that the reward function <span class="arithmatex">\(R\)</span> is unknown or not used. The agent still has a state space <span class="arithmatex">\(S\)</span>, an action space <span class="arithmatex">\(A\)</span>, and the environment transition dynamics <span class="arithmatex">\(P(s' \mid s, a)\)</span>. What we do have, instead of <span class="arithmatex">\(R\)</span>, is access to expert demonstrations. An expert (which could be a human or a pre-trained optimal agent) provides example trajectories:</p>
<div class="arithmatex">\[
\tau_E = (s_0, a_0, s_1, a_1, \dots , s_T)
\]</div>
<p>collected by following the expert’s policy <span class="arithmatex">\(\pi_E\)</span> in the environment. We may have a dataset <span class="arithmatex">\(D\)</span> of these expert trajectories (or simply a set of state-action pairs drawn from expert behavior). The key point is that in IL, the agent does not receive numeric rewards from the environment. Instead, success is measured by how well the agent’s behavior matches the expert’s behavior.</p>
<p>The goal of imitation learning can be stated as: find a policy <span class="arithmatex">\(\pi\)</span> for the agent that reproduces the expert's behavior (and ideally, achieves similar performance on the task). If the expert is optimal or highly skilled, we hope <span class="arithmatex">\(\pi\)</span> will achieve near-optimal results as well. This is an alternative path to finding a good policy without ever specifying a reward function explicitly or performing unguided exploration.</p>
<p>(If we imagine there was some true but unknown reward <span class="arithmatex">\(R\)</span> the expert is optimizing, then ideally <span class="arithmatex">\(\pi\)</span> should perform nearly as well as <span class="arithmatex">\(\pi_E\)</span> on that reward. IL attempts to reach that outcome via demonstrations rather than explicit reward feedback.)</p>
<h2 id="reinforcement-8_imitation_learning-3-behavioral-cloning-learning-by-supervised-imitation">3. Behavioral Cloning: Learning by Supervised Imitation<a class="headerlink" href="#reinforcement-8_imitation_learning-3-behavioral-cloning-learning-by-supervised-imitation" title="Permanent link">¶</a></h2>
<p>The most direct approach to imitation learning is Behavioral Cloning. Behavioral cloning treats imitation as a pure supervised learning problem: we train a policy to map states to the expert’s actions, using the expert demonstrations as labeled examples. In essence, the agent "clones" the expert's behavior by learning to predict the expert's action in any given state.</p>
<blockquote>
<p>BC: Learn state to action mappings using expert demonstrations.</p>
</blockquote>
<p>In practice, we parameterize a policy <span class="arithmatex">\(\pi_\theta(a\mid s)\)</span> (e.g. a neural network with parameters <span class="arithmatex">\(\theta\)</span>) and adjust <span class="arithmatex">\(\theta\)</span> so that <span class="arithmatex">\(\pi_\theta(\cdot\mid s)\)</span> is as close as possible to the expert’s action choice in state <span class="arithmatex">\(s\)</span>. We define a loss function on the dataset of state-action pairs. For example:</p>
<ul>
<li>Discrete actions: Use cross-entropy (negative log-likelihood) of the expert’s action.</li>
</ul>
<div class="arithmatex">\[L(\theta) = - \mathbb{E}_{(s,a)\sim D}\left[ \log \pi_{\theta}(a \mid s) \right]\]</div>
<ul>
<li>Continuous actions: Use mean squared error (regression loss).</li>
</ul>
<div class="arithmatex">\[L(\theta) = \mathbb{E}_{(s,a)\sim D} \left[ \left( \pi_\theta(s) - a \right)^2 \right]\]</div>
<p>Minimizing these losses drives the policy to imitate the expert decisions on the training set.</p>
<p>Training a behavioral cloning agent typically involves three steps:</p>
<ol>
<li>
<p>Collect demonstrations: Gather a dataset <span class="arithmatex">\(D = {(s_i, a_i)}\)</span> of expert state-action examples by observing the expert <span class="arithmatex">\(\pi_E\)</span> in the environment.</p>
</li>
<li>
<p>Supervised learning on <span class="arithmatex">\((s, a)\)</span> pairs: Choose a policy representation for <span class="arithmatex">\(\pi_\theta\)</span> and use the collected data to adjust <span class="arithmatex">\(\theta\)</span>. For each example <span class="arithmatex">\((s_i, a_i)\)</span>, we update <span class="arithmatex">\(\pi_\theta\)</span> to reduce the error between its prediction <span class="arithmatex">\(\pi_\theta(s_i)\)</span> and the expert’s action <span class="arithmatex">\(a_i\)</span>. (For instance, if actions are discrete, we increase the probability <span class="arithmatex">\(\pi_\theta(a_i \mid s_i)\)</span> for the expert’s action; if continuous, we move <span class="arithmatex">\(\pi_\theta(s_i)\)</span> closer to <span class="arithmatex">\(a_i\)</span> in value.)</p>
</li>
<li>
<p>Deployment: Once the policy is trained (approximating <span class="arithmatex">\(\pi_E\)</span>), we fix <span class="arithmatex">\(\theta\)</span>. The agent then acts autonomously: at each state <span class="arithmatex">\(s\)</span>, it outputs <span class="arithmatex">\(a = \pi_\theta(s)\)</span> as its action. Ideally, this learned policy will behave similarly to the expert in the environment.</p>
</li>
</ol>
<p>If the expert demonstrations are representative of the situations the agent will face, behavioral cloning can yield a policy that mimics the expert’s behavior effectively. BC has some clear advantages:</p>
<ul>
<li>
<p>Simplicity: It reduces policy learning to standard supervised learning, for which many stable algorithms and optimizations exist.</p>
</li>
<li>
<p>Offline training: The model can be trained entirely from pre-recorded expert data, without requiring interactive environment feedback. This makes it data-efficient in terms of environment interactions.</p>
</li>
<li>
<p>Safety: No random exploration is needed. The agent never tries highly suboptimal actions during training, since it always learns from demonstrated good behavior (critical in safety-sensitive domains).</p>
</li>
</ul>
<p>However, purely copying the expert also comes with important limitations.</p>
<h3 id="reinforcement-8_imitation_learning-covariate-shift-and-compounding-errors">Covariate Shift and Compounding Errors<a class="headerlink" href="#reinforcement-8_imitation_learning-covariate-shift-and-compounding-errors" title="Permanent link">¶</a></h3>
<p>The main problem with behavioral cloning is that the training distribution of states can differ from the test distribution when the agent actually runs. During training, <span class="arithmatex">\(\pi_\theta\)</span> is only exposed to states that the expert visited. But once the agent is deployed, if it ever deviates even slightly from the expert’s trajectory, it may enter states not seen in the training data. In those unfamiliar states, the policy’s predictions may be unreliable, leading to errors that cause it to drift further from expert-like behavior.</p>
<blockquote>
<p>A small mistake can snowball: once the agent strays from what the expert would do, it encounters novel situations where its learned policy might be very poor. One error leads to another, and the agent can cascade into failure because it was never taught how to recover.</p>
</blockquote>
<p>This phenomenon is known as covariate shift or distributional shift. The learner is trained on the state distribution induced by the expert policy <span class="arithmatex">\(\pi_E\)</span>, but it is testing on the state distribution induced by its own policy <span class="arithmatex">\(\pi_\theta\)</span>. Unless <span class="arithmatex">\(\pi_\theta\)</span> is perfect, these distributions will diverge over time, and the divergence can grow unchecked. In other words, the agent might handle situations similar to the expert's trajectories well, but if it finds itself in a situation the expert never encountered (often a result of a prior mistake), it has no guidance on what to do and can rapidly veer off course. This is often illustrated by the example of a self-driving car learned by BC: if it slightly misjudges a turn and drifts, it may end up in a part of the road it never saw during training, leading to more errors (compounding until possibly a crash).</p>
<p>Another limitation is that BC does not inherently guarantee optimality or improvement beyond the expert: the policy is only as good as the demonstration data. If the expert is suboptimal or the dataset doesn’t cover certain scenarios, the cloned policy will reflect those shortcomings and cannot improve by itself (since it has no feedback signal like reward to further refine its behavior). In reinforcement learning terms, BC has no notion of feedback for success or failure; it merely apes the expert, so it cannot discover better strategies or correct mistakes outside the expert's shadow.</p>
<p>Researchers have developed strategies to mitigate the covariate shift problem. One approach is Dataset Aggregation (DAgger), which is an iterative algorithm: after training an initial policy via BC, let the policy interact with the environment and observe where it makes mistakes or visits unseen states; then have the expert provide the correct actions for those states, add these state-action pairs to the training set, and retrain the policy. By repeating this process, the policy’s training distribution is gradually brought closer to the distribution it will encounter when it controls the agent. DAgger can significantly reduce compounding errors, but it requires ongoing access to an expert for feedback during training.</p>
<p>In summary, behavioral cloning is a powerful first step for imitation learning—it's straightforward and avoids many challenges of pure RL. But one must be mindful of its limitations: a blindly cloned policy can fail catastrophically when it encounters situations outside the expert’s experience. This motivates more sophisticated imitation learning methods that incorporate the dynamics of the environment and attempt to infer the intent behind expert actions, rather than just copying them. We turn to those next.</p>
<h2 id="reinforcement-8_imitation_learning-inverse-reinforcement-learning-learning-the-why">Inverse Reinforcement Learning: Learning the "Why"<a class="headerlink" href="#reinforcement-8_imitation_learning-inverse-reinforcement-learning-learning-the-why" title="Permanent link">¶</a></h2>
<p>Behavioral cloning directly learns what to do (mapping states to actions) but does not capture why those actions are desirable. Inverse Reinforcement Learning (IRL) instead asks: Given expert behavior, what underlying reward function <span class="arithmatex">\(R\)</span> could explain it? In other words, IRL attempts to reverse-engineer the expert's objectives from its observed behavior.</p>
<p>In IRL, we assume that the expert <span class="arithmatex">\(\pi_E\)</span> is (approximately) optimal for some unknown reward function <span class="arithmatex">\(R^*\)</span>. The goal is to infer a reward function <span class="arithmatex">\(\hat{R}\)</span> such that, if an agent were to optimize <span class="arithmatex">\(\hat{R}\)</span>, it would reproduce the expert’s behavior. Formally, we want <span class="arithmatex">\(\pi_E\)</span> to be the optimal policy under the learned reward:</p>
<div class="arithmatex">\[\pi_E = \arg\max_{\pi} \, V_R^{\pi}\]</div>
<p>where <span class="arithmatex">\(V^{\pi}_{\hat{R}}\)</span> is the expected return of policy <span class="arithmatex">\(\pi\)</span> under the reward function <span class="arithmatex">\(\hat{R}\)</span>. In words, the expert should have higher cumulative reward (according to <span class="arithmatex">\(\hat{R}\)</span>) than any other policy. If we can find such an <span class="arithmatex">\(\hat{R}\)</span>, we have explained the expert’s behavior in terms of incentives.</p>
<blockquote>
<p>Intuition: IRL flips the reinforcement learning problem on its head. Rather than starting with a reward and finding a policy, we start with a policy (the expert's) and try to find a reward that this policy optimizes. It's like observing an expert driver and deducing that they must be implicitly trading off goals like "reach the destination quickly" and "avoid collisions" because their driving balances speed and safety.</p>
</blockquote>
<p>One challenge is that IRL is inherently an under-defined (ill-posed) problem: many possible reward functions might make <span class="arithmatex">\(\pi_E\)</span> appear optimal. To resolve this ambiguity, IRL algorithms introduce additional criteria or regularization. For example, they might prefer the simplest reward function that explains the behavior, or in the case of maximum entropy IRL, prefer a reward that leads to the most random (maximally entropic) policy among those that match the expert's behavior – this avoids overly narrow explanations and spreads probability over possible behaviors unless forced by data.</p>
<p>Once a candidate reward function <span class="arithmatex">\(\hat{R}(s,a)\)</span> is learned through IRL, the process typically continues as follows: we plug <span class="arithmatex">\(\hat{R}\)</span> back into the environment and solve a forward RL problem (using any suitable algorithm from earlier chapters) to obtain a policy <span class="arithmatex">\(\pi_{\hat{R}}\)</span> that maximizes this recovered reward. Ideally, <span class="arithmatex">\(\pi_{\hat{R}}\)</span> will then behave similarly to the expert's policy <span class="arithmatex">\(\pi_E\)</span> (since <span class="arithmatex">\(\hat{R}\)</span> was chosen to explain <span class="arithmatex">\(\pi_E\)</span>). The end result is an agent that not only imitates the expert, but also has an explicit reward model of the task it is performing.</p>
<p>IRL is usually more complex and computationally expensive than behavioral cloning, because it often involves a nested loop: for each candidate reward function, the algorithm may need to perform an inner optimization (solving an MDP) to evaluate how well that reward explains the expert. However, IRL provides several potential benefits:</p>
<ul>
<li>
<p>It yields a reward function, which is a portable definition of the task. This inferred reward can then be reused: for example, to train new agents from scratch, to evaluate different policies, or to modify the task (by tweaking the reward) in a principled way.</p>
</li>
<li>
<p>It can generalize better to new situations. If the environment changes in dynamics or constraints, having <span class="arithmatex">\(\hat{R}\)</span> allows us to re-optimize and find a new optimal policy for the new conditions. A policy learned by pure BC might not adapt well beyond the situations it was shown, whereas a reward captures the goal and can be re-optimized.</p>
</li>
<li>
<p>It may allow the agent to exceed the demonstrator’s performance. Since IRL ultimately produces a reward function, an agent can continue to improve with further RL optimization. If the expert was suboptimal or noisy, a sufficiently good RL algorithm might find a policy that achieves an even higher reward (i.e. fine-tunes the behavior) while still aligning with the expert’s intent encoded in <span class="arithmatex">\(\hat{R}\)</span>.</p>
</li>
</ul>
<p>In summary, IRL shifts the imitation learning problem from policy regression to reward inference. It answers a fundamentally different question: instead of directly cloning actions, infer the hidden goals that the expert is pursuing. With <span class="arithmatex">\(\hat{R}\)</span> in hand, we then fall back on standard RL techniques (like those from Chapters 4–8) to derive a policy. IRL is especially appealing in scenarios where we suspect the expert’s behavior is optimizing some elegant underlying objective, and we want to uncover that objective for reuse or interpretation. The cost of IRL is the added complexity of the learning process, but the payoff is a deeper understanding of the task and potentially greater robustness and optimality of the learned policy.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-inverse-reinforcement-learning">Maximum Entropy Inverse Reinforcement Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-inverse-reinforcement-learning" title="Permanent link">¶</a></h3>
<h3 id="reinforcement-8_imitation_learning-principle-of-maximum-entropy">Principle of Maximum Entropy<a class="headerlink" href="#reinforcement-8_imitation_learning-principle-of-maximum-entropy" title="Permanent link">¶</a></h3>
<p>The entropy of a distribution <span class="arithmatex">\(p(s)\)</span> is defined as:</p>
<div class="arithmatex">\[H(p) = -\sum_{s} p(s)\log p(s)\]</div>
<p>The principle of maximum entropy states: The probability distribution that best represents our state of knowledge is the one with the largest entropy, given the constraints of precisely stated prior data. Consider all probability distributions consistent with the observed data. Select the one with maximum entropy—i.e., the least biased distribution that fits what we know while assuming nothing extra.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-applied-to-irl">Maximum Entropy Applied to IRL<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-applied-to-irl" title="Permanent link">¶</a></h3>
<p>We seek a distribution over trajectories <span class="arithmatex">\(P(\tau)\)</span> that:</p>
<ol>
<li>Has maximum entropy, and</li>
<li>Matches expert feature expectations.</li>
</ol>
<p>Formally, we maximize:</p>
<div class="arithmatex">\[\max_{P} -\sum_{\tau} P(\tau)\log P(\tau)\]</div>
<p>subject to:</p>
<div class="arithmatex">\[\sum_{\tau} P(\tau)\mu(\tau) = \frac{1}{|D|}\sum_{\tau_i \in D} \mu(\tau_i)\]</div>
<div class="arithmatex">\[\sum_{\tau} P(\tau) = 1\]</div>
<p>Here:</p>
<ul>
<li><span class="arithmatex">\(\mu(\tau)\)</span> represents feature counts for trajectory <span class="arithmatex">\(\tau\)</span></li>
<li><span class="arithmatex">\(D\)</span> is the expert demonstration set</li>
</ul>
<p>This says: among all possible distributions consistent with observed expert feature averages, choose the one with maximum uncertainty.</p>
<h3 id="reinforcement-8_imitation_learning-matching-rewards">Matching Rewards<a class="headerlink" href="#reinforcement-8_imitation_learning-matching-rewards" title="Permanent link">¶</a></h3>
<p>In linear reward IRL, we assume rewards take the form:</p>
<div class="arithmatex">\[r_\phi(\tau) = \phi^\top \mu(\tau)\]</div>
<p>We want a policy <span class="arithmatex">\(\pi\)</span> that induces a trajectory distribution <span class="arithmatex">\(P(\tau)\)</span> matching the expert’s expected reward under <span class="arithmatex">\(r_\phi\)</span>:</p>
<div class="arithmatex">\[\max_{P(\tau)} -\sum_{\tau}P(\tau)\log P(\tau)\]</div>
<p>subject to:</p>
<div class="arithmatex">\[\sum_{\tau} P(\tau)r_\phi(\tau) = \sum_{\tau} \hat{P}(\tau)r_\phi(\tau)\]</div>
<div class="arithmatex">\[\sum_{\tau}P(\tau)=1\]</div>
<p>This aligns the learner’s expected reward with the expert’s reward estimate.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-exponential-family-distributions">Maximum Entropy ⇒ Exponential Family Distributions<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-exponential-family-distributions" title="Permanent link">¶</a></h3>
<p>Using constrained optimization (Lagrangians), we obtain:</p>
<div class="arithmatex">\[\log P(\tau) = \lambda_1 r_\phi(\tau) - 1 - \lambda_0\]</div>
<p>Thus:</p>
<div class="arithmatex">\[P(\tau) \propto \exp(r_\phi(\tau))\]</div>
<p>This reveals a key result: The maximum entropy distribution consistent with constraints belongs to the exponential family.</p>
<p>That is,</p>
<div class="arithmatex">\[p(\tau|\phi) = \frac{1}{Z(\phi)}\exp(r_\phi(\tau))\]</div>
<p>where</p>
<div class="arithmatex">\[Z(\phi)=\sum_{\tau}\exp(r_\phi(\tau))\]</div>
<p>This means we can now learn <span class="arithmatex">\(\phi\)</span> by maximizing likelihood of observed expert data, because the trajectory distribution becomes a normalized exponential model.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution">Maximum Entropy Over <span class="arithmatex">\(\tau\)</span> Equals Maximum Likelihood of Observed Data Under Max Entropy (Exponential Family) Distribution<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution" title="Permanent link">¶</a></h3>
<p>Jaynes (1957) showed:
Maximizing entropy over trajectories = maximizing likelihood of data under the maximum-entropy distribution.</p>
<p>So we:</p>
<ol>
<li>Assume <span class="arithmatex">\(p(\tau|\phi)\)</span> has exponential form</li>
<li>Learn <span class="arithmatex">\(\phi\)</span> by maximizing:</li>
</ol>
<div class="arithmatex">\[\max_{\phi} \prod_{\tau \in D} p(\tau|\phi)\]</div>
<p>This allows IRL to treat expert demonstrations as data to be probabilistically explained.</p>
<h2 id="reinforcement-8_imitation_learning-maximum-entropy-inverse-rl-algorithm">Maximum Entropy Inverse RL Algorithm<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-inverse-rl-algorithm" title="Permanent link">¶</a></h2>
<p>Assuming known dynamics and linear rewards:</p>
<ol>
<li>Input: expert demonstrations <span class="arithmatex">\(\mathcal{D}\)</span></li>
<li>Initialize reward weights <span class="arithmatex">\(r_\phi\)</span></li>
<li>Compute optimal policy <span class="arithmatex">\(\pi(a|s)\)</span> given <span class="arithmatex">\(r_\phi\)</span> (via dynamic programming / value iteration)</li>
<li>Compute state visitation frequencies <span class="arithmatex">\(\rho(s|\phi,T)\)</span></li>
<li>
<p>Compute gradient on reward parameters:</p>
<p><span class="arithmatex">\(\nabla J(\phi) = \frac{1}{N}\sum_{\tau_i \in \mathcal{D}} \mu(\tau_i) - \sum_{s}\rho(s|\phi,T)\mu(s)\)</span></p>
</li>
<li>
<p>Update <span class="arithmatex">\(\phi\)</span> via gradient step</p>
</li>
<li>Repeat from Step 3</li>
</ol>
<blockquote>
<p>Maximum Entropy IRL assumes experts act stochastically but optimally.
Instead of selecting a single best policy, it finds a distribution over trajectories consistent with expert behavior.
The resulting trajectory probabilities follow:
<span class="arithmatex">\(<span class="arithmatex">\(P(\tau) \propto \exp(r_\phi(\tau))\)</span>\)</span></p>
<p>Learning becomes maximum likelihood estimation: find reward parameters <span class="arithmatex">\(\phi\)</span> that best explain expert demonstrations.</p>
</blockquote>
<h2 id="reinforcement-8_imitation_learning-apprenticeship-learning">Apprenticeship Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-apprenticeship-learning" title="Permanent link">¶</a></h2>
<p>Apprenticeship Learning usually refers to the scenario where an agent learns to perform a task by iteratively improving its policy using expert demonstrations as a reference. In many contexts, this term is used when an IRL algorithm is combined with policy learning: the agent behaves as an apprentice to the expert, gradually mastering the task. The classic formulation by Abbeel and Ng (2004) introduced apprenticeship learning via IRL, which guarantees that the learner’s policy will perform nearly as well as the expert’s, given enough demonstration data.</p>
<p>One way to think of apprenticeship learning is as follows: rather than directly cloning actions, we try to match the feature expectations of the expert. Suppose we have some features <span class="arithmatex">\(\phi(s)\)</span> of states (or state-action pairs) that capture what we care about in the task (for example, in driving, features might include lane deviation, speed, collision count, etc.). The expert will have some expected cumulative feature values <script type="math/tex"> \mathbb{E}_{\pi_E}\left[\sum_t \phi(s_t)\right] </script>. Apprenticeship learning methods aim for the learner to achieve similar feature expectations.</p>
<p>A prototypical apprenticeship learning algorithm proceeds like this:</p>
<ol>
<li>
<p>Initialize a candidate policy (it could even start random).</p>
</li>
<li>
<p>Evaluate how this policy behaves in terms of features (run it in simulation to estimate <span class="arithmatex">\(\mathbb{E}_{\pi}\left[\sum_t \phi(s_t)\right]\)</span>).</p>
</li>
<li>
<p>Compare the policy’s behavior to the expert’s behavior. Identify the biggest discrepancy in feature expectations.</p>
</li>
<li>
<p>Adjust the reward (implicitly defined as a weighted sum of features) to penalize the discrepancy. In other words, find reward weights <span class="arithmatex">\(w\)</span> such that the expert’s advantage over the apprentice in those feature dimensions is highlighted.</p>
</li>
<li>
<p>Optimize a new policy for this updated reward function (solve the MDP with the new <span class="arithmatex">\(w\)</span> to get <span class="arithmatex">\(\pi_{\text{new}}\)</span> that maximizes <span class="arithmatex">\(w \cdot \phi\)</span>).</p>
</li>
<li>
<p>Set this <span class="arithmatex">\(\pi_{\text{new}}\)</span> as the apprentice’s policy and repeat the evaluation -&gt; comparison -&gt; reward adjustment cycle.</p>
</li>
</ol>
<p>Each iteration pushes the apprentice to close the gap on the feature that most distinguishes it from the expert. After a few iterations, this process yields a policy that matches the expert on all key feature dimensions within some tolerance. At that point, the apprentice is essentially as good as the expert with respect to any reward expressible as a combination of those features.</p>
<p>The term apprenticeship learning highlights that the agent is not just mimicking blindly but is engaged in a process of improvement guided by the expert’s example. Importantly, the focus is on achieving at least the expert’s level of performance. We don’t necessarily care about identifying the exact reward the expert had; we care that our apprentice’s policy is successful. In fact, in the algorithm above, the reward weights <span class="arithmatex">\(w\)</span> found in each iteration are intermediate tools – at the end, one can take the final policy and deploy it, without needing to stick to a single explicit reward interpretation.</p>
<p>In relation to IRL, apprenticeship learning can be seen as a practical approach to use IRL for control: IRL finds a reward that explains the expert, and then the agent learns a policy for that reward; if it’s not yet good enough, adjust and repeat. Modern developments in imitation learning often follow this spirit. For example, Generative Adversarial Imitation Learning (GAIL) is a more recent technique where the agent learns a policy by trying to fool a discriminator into thinking the agent’s trajectories are from the expert – conceptually, the discriminator’s judgment provides a sort of reward signal telling the agent how "expert-like" its behavior is. This can be viewed as a form of apprenticeship learning, since the agent is iteratively tweaking its policy to become indistinguishable from the expert.</p>
<p>In summary, apprenticeship learning is about learning by iteratively comparing to an expert and closing the gap. It often uses IRL under the hood, but its end goal is the policy (the apprentice’s skill), not necessarily the reward. It underscores a key point: in imitation learning, sometimes we care more about performing as well as the expert (a direct goal), and sometimes we care about understanding the expert’s intentions (the indirect goal via IRL). Apprenticeship learning emphasizes the former.</p>
<h2 id="reinforcement-8_imitation_learning-imitation-learning-in-the-rl-landscape">Imitation Learning in the RL Landscape<a class="headerlink" href="#reinforcement-8_imitation_learning-imitation-learning-in-the-rl-landscape" title="Permanent link">¶</a></h2>
<p>Imitation learning fills an important niche in the overall reinforcement learning framework. It is especially useful when:</p>
<ol>
<li>
<p>Rewards are difficult to specify: If it's unclear how to craft a reward that captures all aspects of the desired behavior, providing demonstrations can bypass this. IL shines in complex tasks (e.g. high-level driving maneuvers, dexterous robot manipulation) where manually writing a reward function would be cumbersome or prone to error.</p>
</li>
<li>
<p>Rewards are sparse or delayed: When reward feedback is very rare or only given at the end of an episode, a pure RL agent might struggle to get enough signal to learn. An expert trajectory provides dense guidance at every time step (state-action pairs), effectively providing a shaped signal through imitation. This can jump-start learning in tasks that are otherwise too sparse for RL to crack (Chapter 4 discussed how sparse rewards make value estimation difficult – IL sidesteps that by using expert knowledge).</p>
</li>
<li>
<p>Exploration is risky or expensive: In real-world environments like robotics, autonomous driving, or healthcare, exploring with random or untrained policies can be dangerous or costly. IL allows learning a policy without the agent ever taking unguided actions in the real environment; it learns from safe, successful behaviors demonstrated by the expert. This makes it an attractive approach when safety is a hard constraint.</p>
</li>
</ol>
<p>It’s important to note that IL is not necessarily a replacement for reward-based RL, but rather a complement to it. A common practical approach is to bootstrap an agent with imitation learning and then fine-tune it with reinforcement learning. For example, one might first use behavioral cloning to teach a robot arm the basics of a task from human demonstrations, getting it into a reasonable regime of behavior; then, if a reward function is available (even a sparse one for success), use RL to further improve the policy, possibly surpassing the human expert's performance or adapting to slight changes in the task. The initial IL phase provides a good policy prior (saving time and avoiding dangerous exploration), and the subsequent RL phase lets the agent optimize and explore around that policy to refine skills.</p>
<p>On the flip side, imitation learning does require expert data. If obtaining demonstrations is hard (or if no expert exists for a brand-new task), IL might not be applicable. Moreover, if the expert demonstrations are of varying quality or contain noise, the agent will faithfully learn those imperfections unless additional measures (like filtering data or combining with RL optimization) are taken. In contrast, a pure RL approach, given a well-defined reward and enough exploration, can in principle discover superior strategies that no demonstrator provided. Thus, in practice, there is a trade-off: IL can dramatically speed up learning and improve safety given an expert, whereas RL remains the go-to when we only have a reward signal and the freedom to explore.</p>
<p>Imitation learning has become a critical part of the toolbox for solving real-world sequential decision problems. It enables success in domains that might be intractable for pure reinforcement learning by providing an external source of guidance. By learning directly from expert behavior – through methods like behavioral cloning (learning the policy directly) or inverse reinforcement learning (learning the underlying reward and then the policy) – an agent can shortcut the trial-and-error process. Of course, IL introduces its own challenges (distribution shift, reliance on demonstration coverage, potential suboptimality of the expert), but these can often be managed with algorithmic innovations (DAgger, combining IL with RL, etc.). In summary, imitation learning serves as a powerful paradigm for training agents in cases where designing rewards or allowing extensive exploration is impractical, and it often works hand-in-hand with traditional RL to achieve the best results in complex environments.</p>
<h3 id="reinforcement-8_imitation_learning-mental-map">Mental map<a class="headerlink" href="#reinforcement-8_imitation_learning-mental-map" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>                    Imitation Learning (IL)
      Goal: Learn behavior from expert demonstrations
                     instead of explicit rewards
                                │
                                ▼
             Why Imitation Learning? (Motivation)
 ┌───────────────────────────────────────────────────────────┐
 │ Hard to design rewards → reward hacking, tuning           │
 │ Sparse rewards → inefficient trial &amp; error                │
 │ Unsafe exploration (robots, driving, healthcare)          │
 │ Expert data available → demonstrations as guidance        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                   IL vs Reward-Based RL
 ┌─────────────────────────────┬──────────────────────────────┐
 │ Reward-Based RL             │ Imitation Learning           │
 │ + Explores actively         │ + Learns from expert         │
 │ + Needs reward design       │ + No explicit reward         │
 │ – Unsafe / inefficient      │ – Depends on demo quality   │
 └─────────────────────────────┴──────────────────────────────┘
                                │
                                ▼
                         IL Problem Setup
 ┌───────────────────────────────────────────────────────────┐
 │ MDP without reward function                                │
 │ Access to expert trajectories τE (s,a pairs)               │
 │ Goal → Learn policy π that mimics πE                       │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Core Method 1: Behavioral Cloning (BC)
 ┌───────────────────────────────────────────────────────────┐
 │ Treat imitation as supervised learning                    │
 │ Train πθ(s) → aE using dataset D                          │
 │ Discrete: cross-entropy loss                              │
 │ Continuous: mean squared error                            │
 │ Advantages: simple, offline, safe                         │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Key BC Problem: Covariate / Distribution Shift
 ┌───────────────────────────────────────────────────────────┐
 │ Trained only on expert states                             │
 │ When deployed, policy errors lead to unseen states        │
 │ → Poor decisions → more drift → compounding failure       │
 │ BC cannot recover or improve beyond expert                │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                    Fixing BC: DAgger (Idea)
 ┌───────────────────────────────────────────────────────────┐
 │ Let policy act, collect mistakes                          │
 │ Ask expert for correct action                             │
 │ Add to dataset and retrain                                │
 │ → brings training data closer to deployment distribution  │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
       Core Method 2: Inverse Reinforcement Learning (IRL)
 ┌───────────────────────────────────────────────────────────┐
 │ Learn the “why” behind actions → infer hidden reward R*   │
 │ Expert assumed optimal                                     │
 │ Solve inverse problem: πE ≈ optimal for R*                 │
 │ After reward recovered → run normal RL to learn policy     │
 │ Benefits: generalization, interpretability, improve expert │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                Core Method 3: Apprenticeship Learning
 ┌───────────────────────────────────────────────────────────┐
 │ Iteratively improve policy via comparing to expert        │
 │ Match feature expectations φ(s)                           │
 │ Reweights reward → optimize → evaluate → repeat           │
 │ Goal: perform at least as well as expert                  │
 │ Often implemented via IRL (e.g., GAIL conceptually)       │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Role of IL within broader RL landscape
 ┌───────────────────────────────────────────────────────────┐
 │ When IL is useful:                                        │
 │ - Reward hard to design                                   │
 │ - Unsafe or costly to explore                             │
 │ - Sparse reward tasks                                     │
 │ IL + RL hybrid: BC warm-start → RL fine-tune beyond expert│
 │ Limitations: need expert, demos may be suboptimal         │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Final Takeaway (Chapter Summary)
 ┌───────────────────────────────────────────────────────────┐
 │ IL bypasses reward engineering &amp; risky exploration         │
 │ BC learns “what,” IRL learns “why,” apprenticeship learns  │
 │ “how to get as good as expert.”                           │
 │ IL often combined with RL for best performance.           │
 └───────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-9_rlhf" heading-number="3.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment">Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment<a class="headerlink" href="#reinforcement-9_rlhf-chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment" title="Permanent link">¶</a></h1>
<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for “good” behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment – ensuring an AI’s behavior aligns with human preferences and values – by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute “score” to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p>
<p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>
<h2 id="reinforcement-9_rlhf-bradleyterry-preference-modeling-in-rlhf">Bradley–Terry Preference Modeling in RLHF<a class="headerlink" href="#reinforcement-9_rlhf-bradleyterry-preference-modeling-in-rlhf" title="Permanent link">¶</a></h2>
<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley–Terry model, a probabilistic model for noisy comparisons. </p>
<p>Consider a <span class="arithmatex">\(K\)</span>-armed bandit with actions <span class="arithmatex">\(b_1, b_2, \dots, b_K\)</span>, and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action <span class="arithmatex">\(b_i\)</span> over <span class="arithmatex">\(b_j\)</span> is modeled as:</p>
<div class="arithmatex">\[
P(b_i \succ b_j)
=
\frac{\exp(r(b_i))}{\exp(r(b_i)) + \exp(r(b_j))}
=
p_{ij}
\]</div>
<p>where <span class="arithmatex">\(r(b)\)</span> is an unobserved scalar reward associated with action <span class="arithmatex">\(b\)</span>. Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p>
<p>Assume we collect a dataset <span class="arithmatex">\(\mathcal{D}\)</span> of <span class="arithmatex">\(N\)</span> comparisons of the form <span class="arithmatex">\((b_i, b_j, \mu)\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mu(1) = 1\)</span> if the human marked <span class="arithmatex">\(b_i \succ b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0.5\)</span> if the human marked <span class="arithmatex">\(b_i = b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0\)</span> if the human marked <span class="arithmatex">\(b_j \succ b_i\)</span></li>
</ul>
<p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p>
<div class="arithmatex">\[
\mathcal{L}
=
-
\sum_{(b_i,b_j,\mu)\in\mathcal{D}}
\left[
\mu(1)\log P(b_i \succ b_j)
+
\mu(2)\log P(b_j \succ b_i)
\right]
\]</div>
<p>Optimizing this loss adjusts the reward function <span class="arithmatex">\(r(\cdot)\)</span> so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p>
<p>Once the reward model is trained using the Bradley–Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley–Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>
<h2 id="reinforcement-9_rlhf-the-rlhf-training-pipeline">The RLHF Training Pipeline<a class="headerlink" href="#reinforcement-9_rlhf-the-rlhf-training-pipeline" title="Permanent link">¶</a></h2>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../reinforcement/images/llm_rlhf.png" data-desc-position="bottom"><img alt="Alt text" src="../reinforcement/images/llm_rlhf.png"></a></p>
<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p>
<ol>
<li>
<p>Supervised Fine-Tuning (SFT) – Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p>
</li>
<li>
<p>Reward Model Training from Human Preferences – Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function <span class="arithmatex">\(r_\phi(x,y)\)</span> (parameterized by <span class="arithmatex">\(\phi\)</span>) that predicts which response is more preferable for a given input x using Bradley–Terry model.</p>
</li>
<li>
<p>Reinforcement Learning Fine-Tuning – In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy <span class="arithmatex">\(\pi_\theta(y|x)\)</span> (with parameters <span class="arithmatex">\(\theta\)</span>) is updated to maximize the expected reward <span class="arithmatex">\(r_\phi(x,y)\)</span> of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model’s score, the policy might exploit flaws in <span class="arithmatex">\(r_\phi\)</span> (a form of “reward hacking”) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback–Leibler (KL) penalty that keeps the new policy <span class="arithmatex">\(\pi_\theta\)</span> close to the reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span> (often the SFT model). In summary, the RL objective can be written as:</p>
<div class="arithmatex">\[\max_{\pi_\theta} (
\underbrace{
\mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi_\theta(y \mid x)}
}_{\text{Sample from policy}}
\left[
\underbrace{
r_\phi(x,y)
}_{\text{Want high reward}}
\right]
-
\underbrace{
\beta \, \mathbb{D}_{\mathrm{KL}}
\left[
\pi_\theta(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x)
\right]
}_{\text{Keep KL to original model small}})
\]</div>
<p>where <span class="arithmatex">\(\beta&gt;0\)</span> controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if <span class="arithmatex">\(\pi_\theta\)</span> deviates too much from the original model’s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p>
</li>
</ol>
<p>Through this pipeline – SFT, reward modeling, and RL fine-tuning – we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like <span class="arithmatex">\(\beta\)</span> and PPO clip thresholds). The process can be unstable; for instance, if <span class="arithmatex">\(\beta\)</span> is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if <span class="arithmatex">\(\beta\)</span> is too high, the policy might hardly improve at all. Researchers have described RLHF as a “complex and often unstable procedure” that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>
<h2 id="reinforcement-9_rlhf-direct-preference-optimization-rlhf-without-rl">Direct Preference Optimization: RLHF without RL?<a class="headerlink" href="#reinforcement-9_rlhf-direct-preference-optimization-rlhf-without-rl" title="Permanent link">¶</a></h2>
<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p>
<p>Recall that in the RLHF setting, our goal is to find a policy <span class="arithmatex">\(\pi^*(y|x)\)</span> that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of <span class="arithmatex">\(J(\pi)\)</span> occurs when <span class="arithmatex">\(\pi\)</span> is proportional to the reference policy times an exponential of the reward:</p>
<div class="arithmatex">\[\pi^*(y \mid x) \propto \pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{1}{\beta}\, r_\phi(x, y)\right)\]</div>
<p>This equation gives a closed-form solution for the optimal policy in terms of the reward function <span class="arithmatex">\(r_\phi\)</span>. It makes sense: actions <span class="arithmatex">\(y\)</span> that have higher human-derived reward should be taken with higher probability, but we temper this by <span class="arithmatex">\(\beta\)</span> and weight by the reference probabilities <span class="arithmatex">\(\pi_{\text{ref}}(y|x)\)</span> so that we don’t stray too far. If we were to normalize the right-hand side, we’d write:</p>
<div class="arithmatex">\[\pi^*(y \mid x)
=
\frac{
\pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{r_\phi(x,y)}{\beta}\right)
}{
\sum_{y'} \pi_{\text{ref}}(y' \mid x)\,
\exp\!\left(\frac{r_\phi(x,y')}{\beta}\right)
}\]</div>
<p>Here the denominator is a partition functionsumming over all possible responses <span class="arithmatex">\(y'\)</span> for input <span class="arithmatex">\(x\)</span>. This normalization involves a sum over the entire response space, which is astronomically large for language models – hence we cannot directly compute <span class="arithmatex">\(\pi^*(y|x)\)</span> in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p>
<p>DPO’s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses <span class="arithmatex">\(y_+\)</span> (preferred) and <span class="arithmatex">\(y_-\)</span> (dispreferred) for the same context <span class="arithmatex">\(x\)</span>, the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p>
<div class="arithmatex">\[\frac{\pi^*(y^+ \mid x)}{\pi^\ast(y^- \mid x)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^+)}{\beta}\right)}
{\pi_{\text{ref}}(y^- \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^-)}{\beta}\right)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)}
{\pi_{\text{ref}}(y^- \mid x)}
\exp\!\left(
\frac{1}{\beta}
\big[
r_\phi(x, y^+) - r_\phi(x, y^-)
\big]
\right)\]</div>
<p>Taking the log of both sides, we get a neat relationship:</p>
<div class="arithmatex">\[\frac{1}{\beta}
\big( r_\phi(x, y^{+}) - r_\phi(x, y^{-}) \big)
=
\big[ \log \pi^\ast(y^{+} \mid x) - \log \pi^\ast(y^{-} \mid x) \big]
-
\big[ \log \pi_{\text{ref}}(y^{+} \mid x) - \log \pi_{\text{ref}}(y^{-} \mid x) \big]\]</div>
<p>The term in brackets on the right is the difference in log-probabilities that the optimal policy <span class="arithmatex">\(\pi^*\)</span> assigns to the two responses (which in turn would equal the difference in our learned policy’s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if <span class="arithmatex">\(y_+\)</span> is better than <span class="arithmatex">\(y_-\)</span> by some amount of reward, then the optimal policy should tilt its probabilities in favor of <span class="arithmatex">\(y_+\)</span> by a corresponding factor.</p>
<p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts <span class="arithmatex">\(\pi_\theta\)</span> to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context <span class="arithmatex">\(x\)</span> and two candidate outputs <span class="arithmatex">\(y_+\)</span> (human-preferred) and <span class="arithmatex">\(y_-\)</span> (human-dispreferred), we want the model to assign a higher probability to <span class="arithmatex">\(y_+\)</span> than to <span class="arithmatex">\(y_-\)</span>, with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p>
<p>In practice, the DPO loss for a pair <span class="arithmatex">\((x, y_+, y_-)\)</span> is something like:</p>
<div class="arithmatex">\[\ell_{\text{DPO}}(\theta)
= - \log \sigma \!\left(
\beta\,
\big[ \log \pi_\theta(y^{+} \mid x) - \log \pi_\theta(y^{-} \mid x) \big]
\right)\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is the sigmoid function. This loss is low (i.e. good) when <span class="arithmatex">\(\log \pi_\theta(y_+|x) \gg \log \pi_\theta(y_-|x)\)</span>, meaning the model assigns much higher probability to the preferred outcome – which is what we want. If the model hasn’t yet learned the preference, the loss will be higher, and gradient descent on this loss will push <span class="arithmatex">\(\pi_\theta\)</span> to increase the probability of <span class="arithmatex">\(y_+\)</span> and decrease that of <span class="arithmatex">\(y_-\)</span>. Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy’s logits: effectively, <span class="arithmatex">\(\log \pi_\theta(y|x)\)</span> plays the role of a reward score for how good <span class="arithmatex">\(y\)</span> is, up to the scaling factor <span class="arithmatex">\(1/\beta\)</span>. In fact, the DPO derivation can be seen as combining the preference loss on <span class="arithmatex">\(r_\phi\)</span> with the <span class="arithmatex">\(\pi^*\)</span> solution formula to produce a preference loss on <span class="arithmatex">\(\pi_\theta\)</span>. The original DPO paper calls this approach “your language model is secretly a reward model” – by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p>
<p>## Mental map</p>
<p><code>text
         Reinforcement Learning from Human Feedback (RLHF)
   Goal: Align model behavior with human preferences and values
          when explicit reward design is impractical
                                │
                                ▼
           Why Dense Rewards Are Hard for Language Models
 ┌───────────────────────────────────────────────────────────┐
 │ Open-ended tasks (dialogue, summarization, reasoning)     │
 │ No clear numeric notion of “good” behavior                │
 │ Hand-crafted dense rewards → miss nuance, reward hacking  │
 │ Metrics (BLEU, ROUGE, length) poorly reflect human values │
 │ Human values are subjective, contextual, and fuzzy        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
            From Imitation Learning to Human Preferences
 ┌───────────────────────────────────────────────────────────┐
 │ Behavioral Cloning (IL): imitate demonstrations           │
 │ + Simple, safe, no reward needed                          │
 │ – Cannot exceed expert, sensitive to distribution shift   │
 │ DAgger: fixes BC but requires step-by-step human labeling │
 │ Pairwise preferences = middle ground                      │
 │ → no dense rewards, no per-step supervision               │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Pairwise Preference Feedback (Key Idea)
 ┌───────────────────────────────────────────────────────────┐
 │ Humans compare two outputs and choose the better one      │
 │ Easier than assigning numeric rewards                     │
 │ More informative than raw demonstrations                  │
 │ Scales to complex, open-ended behaviors                   │
 │ Forms basis of reward learning in RLHF                    │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
        Bradley–Terry Model: Preferences → Reward Signal
 ┌───────────────────────────────────────────────────────────┐
 │ Model noisy human comparisons probabilistically           │
 │ P(b_i ≻ b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    │
 │ r(b): latent scalar reward                                │
 │ Fit r(·) by maximizing likelihood / cross-entropy         │
 │ Preferred outputs get higher reward scores                │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
             RLHF Training Pipeline (3 Stages)
 ┌───────────────────────────────────────────────────────────┐
 │ 1. Supervised Fine-Tuning (SFT)                           │
 │    – Behavioral cloning on human-written demos            │
 │    – Produces reference policy π_ref                      │
 │                                                           │
 │ 2. Reward Model Training                                  │
 │    – Human pairwise preferences                           │
 │    – Train r_φ(x,y) via Bradley–Terry loss                │
 │                                                           │
 │ 3. RL Fine-Tuning (PPO)                                   │
 │    – Maximize reward r_φ(x,y)                             │
 │    – KL penalty keeps π_θ close to π_ref                  │
 │    – Prevents reward hacking &amp; language drift             │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              RLHF Objective (KL-Regularized RL)
 ┌───────────────────────────────────────────────────────────┐
 │ Maximize:                                                 │
 │   E[r_φ(x,y)] − β · KL(π_θ || π_ref)                      │
 │ β controls tradeoff:                                      │
 │   Low β → reward hacking / mode collapse                  │
 │   High β → little improvement over SFT                    │
 │ PPO provides stable policy updates                        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
          Limitations of Standard RLHF (PPO-based)
 ┌───────────────────────────────────────────────────────────┐
 │ Requires training multiple models                         │
 │ Many hyperparameters (β, PPO clip, value loss, etc.)      │
 │ Sampling-based RL can be unstable                         │
 │ Expensive and complex pipeline                            │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
      Direct Preference Optimization (DPO): RLHF without RL
 ┌───────────────────────────────────────────────────────────┐
 │ Solve RLHF objective in closed form                       │
 │ Optimal policy:                                           │
 │   π*(y|x) ∝ π_ref(y|x) · exp(r_φ(x,y)/β)                  |
 │ Use probability ratios → normalization cancels            │
 │ Train π_θ directly on preference pairs                    │
 │ Loss: sigmoid on log-prob difference                      │
 │ “Your LM is secretly a reward model”                      │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              DPO vs PPO-based RLHF
 ┌─────────────────────────────┬─────────────────────────────┐
 │ RLHF (PPO)                  │ DPO                         │
 │ + Explicit RL optimization  │ + Pure supervised learning  │
 │ – Complex &amp; unstable        │ – Assumes KL-optimal form   │
 │ – Many hyperparameters      │ + Simple, stable, efficient │
 │                             │ + No separate reward model  │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
              Final Takeaway (Chapter Summary)
 ┌───────────────────────────────────────────────────────────┐
 │ Dense rewards are hard for language tasks                 │
 │ Pairwise preferences provide natural human feedback       │
 │ RLHF learns rewards from preferences + optimizes policy   │
 │ DPO simplifies RLHF by removing explicit RL               │
 │ Together, they extend imitation learning toward           │
 │ scalable value alignment for modern language models       │
 └───────────────────────────────────────────────────────────┘</code></p></body></html></section><section class="print-page" id="reinforcement-10_offline_rl" heading-number="3.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-10-batch-offline-rl-policy-evaluation-optimization">Chapter 10: Batch / Offline RL Policy Evaluation &amp; Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-chapter-10-batch-offline-rl-policy-evaluation-optimization" title="Permanent link">¶</a></h1>
<p>Learning from the Past</p>
<ul>
<li>Learning from Past Human Demonstrations: Imitation Learning</li>
<li>Learning from Past Human Preferences: RLHF and DPO</li>
<li>Learning from Past Decisions and Actions: Offline RL</li>
</ul>
<h2 id="reinforcement-10_offline_rl-offline-reinforcement-learning-a-different-approach">Offline Reinforcement Learning: A Different Approach<a class="headerlink" href="#reinforcement-10_offline_rl-offline-reinforcement-learning-a-different-approach" title="Permanent link">¶</a></h2>
<p>Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.</p>
<p>In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.</p>
<blockquote>
<p>Why Can’t We Just Use Q-Learning?</p>
<ul>
<li>Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values</li>
<li>But deadly triad of bootstrapping, function approximation and off
policy, and can fail</li>
</ul>
</blockquote>
<h2 id="reinforcement-10_offline_rl-batch-policy-evaluation-estimating-the-performance-of-a-policy">Batch Policy Evaluation: Estimating the Performance of a Policy<a class="headerlink" href="#reinforcement-10_offline_rl-batch-policy-evaluation-estimating-the-performance-of-a-policy" title="Permanent link">¶</a></h2>
<p>Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.</p>
<ol>
<li>
<p>Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.</p>
<p>Specifically, it learns two main components from data: a reward function <span class="arithmatex">\(\hat{r}(s,a)\)</span> and transition dynamics <span class="arithmatex">\(\hat{P}(s' \mid s,a)\)</span>. These are learned via supervised learning on the offline dataset <span class="arithmatex">\(D\)</span> of transitions collected by some behavior policy <span class="arithmatex">\(\pi_b\)</span>. For example, <span class="arithmatex">\(\hat{r}(s,a)\)</span> can be trained by regression to predict the observed reward given state <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>, and <span class="arithmatex">\(\hat{P}(s' \mid s,a)\)</span> can be fit to predict the next-state <span class="arithmatex">\(s'\)</span> (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in <span class="arithmatex">\(D\)</span> under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to “learn the environment’s rules". Once learned, this model <span class="arithmatex">\(\hat{\mathcal{M}} = (\hat{P}, \hat{r})\)</span> serves as a proxy for the real environment, which we can use for evaluating any policy <span class="arithmatex">\(\pi\)</span> without further real experience.</p>
<p>It’s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics <span class="arithmatex">\(\hat{P}\)</span> will be reliable only in regions covered by <span class="arithmatex">\(D\)</span>; if <span class="arithmatex">\(\pi\)</span> later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).</p>
<h4 id="reinforcement-10_offline_rl-algorithmic-outline-offline-policy-evaluation-via-model">Algorithmic Outline - Offline Policy Evaluation via Model:<a class="headerlink" href="#reinforcement-10_offline_rl-algorithmic-outline-offline-policy-evaluation-via-model" title="Permanent link">¶</a></h4>
<ol>
<li>
<p>Input: offline dataset <span class="arithmatex">\(D\)</span> of transitions (from behavior <span class="arithmatex">\(\pi_b\)</span>), a policy <span class="arithmatex">\(\pi\)</span> to evaluate, discount <span class="arithmatex">\(\gamma\)</span>.</p>
</li>
<li>
<p>Model Learning: Fit <span class="arithmatex">\(\hat{P}(s'|s,a)\)</span> and <span class="arithmatex">\(\hat{r}(s,a)\)</span> using <span class="arithmatex">\(D\)</span> (e.g. maximum likelihood estimation for dynamics, regression for rewards).</p>
</li>
<li>
<p>Policy Evaluation: Initialize <span class="arithmatex">\(V(s)=0\)</span> for all states (or some initial guess).</p>
</li>
<li>
<p>Loop (Bellman backups using <span class="arithmatex">\(\hat{P},\hat{r}\)</span>): For each state <span class="arithmatex">\(s\)</span> in the state space (or a representative set of states):</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(\hat{R}^\pi(s) = \sum_a \pi(a|s)\hat{r}(s,a)\)</span>.</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(V_{\text{new}}(s) = \hat{R}^\pi(s) + \gamma \sum_{s'} \hat{P}^\pi(s'\mid s),V(s')\)</span>.</p>
</li>
<li>
<p>Update <span class="arithmatex">\(V \leftarrow V_{\text{new}}\)</span> and repeat until convergence (the changes in <span class="arithmatex">\(V\)</span> are below a threshold).</p>
</li>
<li>
<p>Output: <span class="arithmatex">\(V(s)\)</span> for states of interest (e.g. the estimated value of <span class="arithmatex">\(\pi\)</span> under the initial state distribution <span class="arithmatex">\(S_0\)</span> can be obtained by <span class="arithmatex">\(\mathbb{E}_{s_0\sim S_0}[V(s_0)]\)</span>).</p>
</li>
</ol>
</li>
<li>
<p>Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.</p>
<h3 id="reinforcement-10_offline_rl-algorithm-3-fitted-q-evaluation-fqe-pi-c">Algorithm 3 Fitted Q Evaluation: FQE <span class="arithmatex">\((\pi, c)\)</span><a class="headerlink" href="#reinforcement-10_offline_rl-algorithm-3-fitted-q-evaluation-fqe-pi-c" title="Permanent link">¶</a></h3>
<p>Input: Dataset <span class="arithmatex">\(\mathcal{D} = \{(x_i, a_i, x'_i, c_i)\}_{i=1}^n \sim \pi_D\)</span>. Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy <span class="arithmatex">\(\pi\)</span> by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state <span class="arithmatex">\(s_i\)</span>, taking action <span class="arithmatex">\(a_i\)</span>, and then following policy <span class="arithmatex">\(\pi\)</span> thereafter.At each iteration, we construct a Bellman target:</p>
<div class="arithmatex">\[\tilde{Q}^\pi(s_i, a_i)
=
c_i + \gamma V_\theta^\pi(s_{i+1})
\]</div>
<p>where</p>
<div class="arithmatex">\[
V_\theta^\pi(s_{i+1}) = Q_\theta^\pi(s_{i+1}, \pi(s_{i+1})).
\]</div>
<p>The Q-function is parameterized by <span class="arithmatex">\(\theta\)</span> (e.g., a neural network), and is learned by solving a supervised regression problem:</p>
<div class="arithmatex">\[\arg\min_\theta
\sum_i
\Big(
Q_\theta^\pi(s_i, a_i)
-
\tilde{Q}^\pi(s_i, a_i)
\Big)^2\]</div>
<p>This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy <span class="arithmatex">\(\pi\)</span> under the data distribution induced by <span class="arithmatex">\(\pi_D\)</span>.</p>
<p>Function class <span class="arithmatex">\(F\)</span> (Let's assume we use a DNN for F). Policy </p>
<p><span class="arithmatex">\(\pi\)</span> to be evaluated.
1: Initialize <span class="arithmatex">\(Q_0 \in F\)</span> randomly<br>
2: for <span class="arithmatex">\(k = 1, 2, \dots, K\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Compute target <span class="arithmatex">\(y_i = c_i + \gamma Q_{k-1}(x'_i, \pi(x'_i)) \quad \forall i\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Build training set <span class="arithmatex">\(\tilde{\mathcal{D}}_k = \{(x_i, a_i), y_i\}_{i=1}^n\)</span><br>
5: <span class="arithmatex">\(\quad\)</span> Solve a supervised learning problem:<br>
<script type="math/tex; mode=display">
Q_k = \arg\min_{f \in F} \frac{1}{n} \sum_{i=1}^n \big(f(x_i, a_i) - y_i\big)^2
</script>
<br>
6: end for<br>
Output: <span class="arithmatex">\(\hat{C}^\pi(x) = Q_K(x, \pi(x)) \quad \forall x\)</span></p>
<blockquote>
<h2 id="reinforcement-10_offline_rl-what-is-different-vs-dqn">What is different vs DQN?<a class="headerlink" href="#reinforcement-10_offline_rl-what-is-different-vs-dqn" title="Permanent link">¶</a></h2>
<p>DQN learns an optimal policy by interacting with the environment, while FQE evaluates a <em>fixed policy</em> using a <em>fixed offline dataset</em>.</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>FQE (Fitted Q Evaluation)</th>
<th>DQN (Deep Q-Network)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Goal</td>
<td>Policy evaluation</td>
<td>Policy optimization / control</td>
</tr>
<tr>
<td>Policy</td>
<td>Fixed target policy <span class="arithmatex">\(\pi\)</span></td>
<td>Implicitly learned via <span class="arithmatex">\(\max_a Q(s,a)\)</span></td>
</tr>
<tr>
<td>Data</td>
<td>Offline, fixed dataset <span class="arithmatex">\(\mathcal{D}\)</span></td>
<td>Online, collected during training</td>
</tr>
<tr>
<td>Bellman target</td>
<td><span class="arithmatex">\(c + \gamma Q(s', \pi(s'))\)</span></td>
<td><span class="arithmatex">\(r + \gamma \max_a Q(s', a)\)</span></td>
</tr>
<tr>
<td>Action at next state</td>
<td>From given policy <span class="arithmatex">\(\pi\)</span></td>
<td>Greedy over Q-values</td>
</tr>
<tr>
<td>Exploration</td>
<td>None</td>
<td>Required (e.g. <span class="arithmatex">\(\epsilon\)</span>-greedy)</td>
</tr>
<tr>
<td>Dataset changes?</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
<tr>
<td>Off-policy instability</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Convergence guarantees</td>
<td>Yes (tabular / linear)</td>
<td>No (with function approximation)</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-10_offline_rl-1-no-maximization-bias-in-fqe">1. No maximization bias in FQE<a class="headerlink" href="#reinforcement-10_offline_rl-1-no-maximization-bias-in-fqe" title="Permanent link">¶</a></h3>
<ul>
<li>DQN suffers from overestimation bias</li>
<li>FQE does pure regression, no bootstrapped max</li>
</ul>
<h3 id="reinforcement-10_offline_rl-2-stability">2. Stability<a class="headerlink" href="#reinforcement-10_offline_rl-2-stability" title="Permanent link">¶</a></h3>
<ul>
<li>FQE ≈ supervised learning  </li>
<li>DQN ≈ bootstrapped + non-stationary targets</li>
</ul>
<h3 id="reinforcement-10_offline_rl-3-offline-vs-online">3. Offline vs Online<a class="headerlink" href="#reinforcement-10_offline_rl-3-offline-vs-online" title="Permanent link">¶</a></h3>
<ul>
<li>FQE cannot improve the policy  </li>
<li>DQN must interact with environment</li>
</ul>
</blockquote>
</li>
<li>
<p>Importance Sampling:
    This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy’s value – assuming coverage (i.e. the target policy doesn’t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.</p>
</li>
</ol>
<h2 id="reinforcement-10_offline_rl-offline-policy-learning-optimization">Offline Policy Learning / Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-offline-policy-learning-optimization" title="Permanent link">¶</a></h2>
<p>Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.</p>
<ol>
<li>
<p>Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.</p>
</li>
<li>
<p>Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.</p>
</li>
<li>
<p>Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.</p>
</li>
</ol>
<h2 id="reinforcement-10_offline_rl-challenges-in-offline-policy-optimization">Challenges in Offline Policy Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-challenges-in-offline-policy-optimization" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.</p>
</li>
<li>
<p>Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.</p>
</li>
<li>
<p>Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.</p>
</li>
</ol>
<p>Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.</p>
<h2 id="reinforcement-10_offline_rl-mental-map">Mental Map<a class="headerlink" href="#reinforcement-10_offline_rl-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                 Offline / Batch Reinforcement Learning
        Goal: Learn and evaluate policies from fixed historical data
           when exploration is unsafe, expensive, or impossible
                                │
                                ▼
              Why Online RL Is Not Always Feasible
 ┌─────────────────────────────────────────────────────────────┐
 │ Exploration can be dangerous (healthcare, driving, robotics)│
 │ Data already exists from past decisions                     │
 │ Real systems cannot reset or freely experiment              │
 │ We must learn without interacting with the environment      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Offline RL vs Standard RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Standard RL:                                                │
 │  – Collect data with current policy                         │
 │  – Explore → improve → repeat                               │
 │                                                             │
 │ Offline RL:                                                 │
 │  – Fixed dataset D from behavior policy π_b                 │
 │  – No new interaction allowed                               │
 │  – Must generalize only from observed data                  │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Why “Just Use Q-Learning” Fails Offline
 ┌─────────────────────────────────────────────────────────────┐
 │ Q-learning is off-policy — but not offline-safe             │
 │ Deadly triad:                                               │
 │   • Bootstrapping                                           │
 │   • Function approximation                                  │
 │   • Off-policy learning                                     │
 │ Leads to divergence &amp; overestimation                        │
 │ Especially severe with distribution mismatch                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Offline RL Decomposed into Two Core Problems
 ┌───────────────────────────────┬─────────────────────────────┐
 │ 1. Policy Evaluation (OPE)    │ 2. Policy Optimization      │
 │    “How good is this policy?” │    “How can we improve it?” │
 │    Without running it         │    Without new data         │
 └───────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
            Batch / Offline Policy Evaluation (OPE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate V^π or J(π) using only dataset D                   │
 │ Three major approaches:                                     │
 │  1. Model-based evaluation                                  │
 │  2. Model-free evaluation (FQE)                             │
 │  3. Importance Sampling                                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        1. Model-Based Offline Policy Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Learn a model from data:                                    │
 │   • Reward model: r̂(s,a)                                    │
 │   • Transition model: P̂(s'|s,a)                             │
 │ Treat batch data as supervised learning                     │
 │ Then simulate policy π inside learned model                 │
 │ Use Bellman backups on (P̂, r̂)                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Model-Based OPE: Key Limitation
 ┌─────────────────────────────────────────────────────────────┐
 │ Model is only reliable where data exists                    │
 │ Policy visiting unseen states/actions → extrapolation error │
 │ Model hallucination → highly biased value estimates         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Evaluate a *fixed policy* π                                 │
 │ Learn Q^π(s,a) from offline data via regression             │
 │ Bellman target:                                             │
 │   y = c + γ Q(s', π(s'))                                    │
 │ Pure supervised learning loop                               │
 │ Stable compared to Q-learning / DQN                         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             FQE vs DQN (Key Insight)
 ┌─────────────────────────────┬─────────────────────────────┐
 │ DQN                         │ FQE                         │
 │ Learns optimal policy       │ Evaluates fixed policy      │
 │ Uses max over actions       │ Uses given π(s')            │
 │ Online data collection      │ Fully offline               │
 │ Overestimation bias         │ No max → more stable        │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
        3. Importance Sampling (IS) Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat OPE as statistical estimation                         │
 │ Reweight trajectories by π / π_b                            │
 │ Unbiased if coverage holds                                  │
 │ Severe variance for long horizons or policy mismatch        │
 │ Variants:                                                   │
 │   • Per-decision IS                                         │
 │   • Weighted IS                                             │
 │   • Doubly robust estimators                                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
            Offline Policy Optimization
 ┌─────────────────────────────────────────────────────────────┐
 │ Goal: improve policy using only dataset D                   │
 │ Model-free: Fitted Q Iteration (FQI)                        │
 │ Model-based: planning inside learned model                  │
 │ Core challenge: distribution shift                          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        The Central Problem: Distribution Mismatch
 ┌─────────────────────────────────────────────────────────────┐
 │ Learned policy chooses actions unseen in data               │
 │ Q-values extrapolate → overly optimistic                    │
 │ Performance collapses at deployment                         │
 │ Offline RL ≠ just off-policy RL                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Conservative / Pessimistic Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Assume unknown actions are risky                            │
 │ Penalize state-action pairs with low data support           │
 │ Prefer policies close to behavior policy                    │
 │ Examples (conceptually):                                    │
 │   • Conservative Q-Learning (CQL)                           │
 │   • Regularization toward π_b                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Key Challenges in Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Coverage / overlap requirement                              │
 │ Model misspecification                                      │
 │ Value overestimation                                        │
 │ Bias–variance tradeoffs                                     │
 │ Safety vs optimality                                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Final Takeaway (Chapter Summary)
 ┌─────────────────────────────────────────────────────────────┐
 │ Offline RL learns entirely from past experience             │
 │ Policy evaluation is foundational before optimization       │
 │ Model-based, FQE, and IS provide OPE tools                  │
 │ Main risk: distribution shift &amp; extrapolation               │
 │ Conservative methods trade performance for safety           │
 │ Offline RL is essential for real-world decision systems     │
 └─────────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-11_fast_rl" heading-number="3.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-11-data-efficient-reinforcement-learning-bandit-foundations">Chapter 11: Data-Efficient Reinforcement Learning — Bandit Foundations<a class="headerlink" href="#reinforcement-11_fast_rl-chapter-11-data-efficient-reinforcement-learning-bandit-foundations" title="Permanent link">¶</a></h1>
<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>
<h2 id="reinforcement-11_fast_rl-the-multi-armed-bandit-model">The Multi-Armed Bandit Model<a class="headerlink" href="#reinforcement-11_fast_rl-the-multi-armed-bandit-model" title="Permanent link">¶</a></h2>
<p>A multi-armed bandit is defined as a tuple <span class="arithmatex">\((\mathcal{A}, \mathcal{R})\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{A} = \{a_1, \dots, a_m\}\)</span> is a known, finite set of actions (arms),</li>
<li><span class="arithmatex">\(R_a(r) = \mathbb{P}[r \mid a]\)</span> is an unknown probability distribution over rewards for each action.</li>
<li>there is no "state".</li>
</ul>
<p>At each timestep <span class="arithmatex">\(t\)</span>, the agent:</p>
<ol>
<li>Chooses an action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span>,</li>
<li>Receives a stochastic reward <span class="arithmatex">\(r_t \sim R_{a_t}\)</span>.</li>
</ol>
<p>Goal: Maximize cumulative reward:<br>
<script type="math/tex; mode=display">
\sum_{t=1}^{T} r_t
</script>
</p>
<p>This simple model embodies the core RL challenges—particularly exploration vs. exploitation—in an isolated setting.</p>
<h3 id="reinforcement-11_fast_rl-evaluating-algorithms-regret-framework">Evaluating Algorithms: Regret Framework<a class="headerlink" href="#reinforcement-11_fast_rl-evaluating-algorithms-regret-framework" title="Permanent link">¶</a></h3>
<p>Regret: </p>
<ul>
<li><span class="arithmatex">\(Q(a) = \mathbb{E}[r \mid a]\)</span> be the expected reward for action <span class="arithmatex">\(a\)</span>,</li>
<li><span class="arithmatex">\(a^* = \arg\max_{a \in \mathcal{A}} Q(a)\)</span>,</li>
<li>Optimal Value <span class="arithmatex">\(V^* = Q(a^*)\)</span></li>
</ul>
<p>Then regret is the opportunity loss for one step:
<script type="math/tex; mode=display">
\ell_t = \mathbb{E}[V^* - Q(a_t)]
</script>
</p>
<p>Total Regret is the total opportunity loss: Total regret over <span class="arithmatex">\(T\)</span> timesteps</p>
<p>
<script type="math/tex; mode=display">
L_T = \sum_{t=1}^T \ell_t = \sum_{a \in \mathcal{A}} \mathbb{E}[N_T(a)] \cdot \Delta_a
</script>
Where:</p>
<ul>
<li><span class="arithmatex">\(N_T(a)\)</span>: Number of times arm <span class="arithmatex">\(a\)</span> is selected by time <span class="arithmatex">\(T\)</span>,</li>
<li><span class="arithmatex">\(\Delta_a = V^* - Q(a)\)</span>: Suboptimality gap.</li>
</ul>
<blockquote>
<p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>
</blockquote>
<h2 id="reinforcement-11_fast_rl-baseline-approaches-and-their-regret">Baseline Approaches and Their Regret<a class="headerlink" href="#reinforcement-11_fast_rl-baseline-approaches-and-their-regret" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-11_fast_rl-greedy-algorithm">Greedy Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-greedy-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \cdot \mathbb{1}(a_\tau = a)
\]</div>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)
\]</div>
<h4 id="reinforcement-11_fast_rl-key-insight">Key Insight:<a class="headerlink" href="#reinforcement-11_fast_rl-key-insight" title="Permanent link">¶</a></h4>
<ul>
<li>Exploits current estimates.</li>
<li>May lock onto suboptimal arms due to early bad luck.</li>
<li>Linear regret in expectation.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-example">Example:<a class="headerlink" href="#reinforcement-11_fast_rl-example" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\)</span>, and the first sample of <span class="arithmatex">\(a_1\)</span> yields 0, the greedy agent may ignore it indefinitely.</p>
<h3 id="reinforcement-11_fast_rl-varepsilon-greedy-algorithm"><span class="arithmatex">\(\varepsilon\)</span>-Greedy Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-varepsilon-greedy-algorithm" title="Permanent link">¶</a></h3>
<p>At each timestep:</p>
<ul>
<li>With probability <span class="arithmatex">\(1 - \varepsilon\)</span>: exploit (<span class="arithmatex">\(\arg\max \hat{Q}_t(a)\)</span>),</li>
<li>With probability <span class="arithmatex">\(\varepsilon\)</span>: explore uniformly at random.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-performance">Performance:<a class="headerlink" href="#reinforcement-11_fast_rl-performance" title="Permanent link">¶</a></h4>
<ul>
<li>Guarantees exploration.</li>
<li>Linear regret unless <span class="arithmatex">\(\varepsilon\)</span> decays over time.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-decaying-varepsilon-greedy">Decaying <span class="arithmatex">\(\varepsilon\)</span>-Greedy<a class="headerlink" href="#reinforcement-11_fast_rl-decaying-varepsilon-greedy" title="Permanent link">¶</a></h3>
<p>Allows <span class="arithmatex">\(\varepsilon_t \to 0\)</span> as <span class="arithmatex">\(t \to \infty\)</span>, enabling convergence.</p>
<h2 id="reinforcement-11_fast_rl-optimism-in-the-face-of-uncertainty">Optimism in the Face of Uncertainty<a class="headerlink" href="#reinforcement-11_fast_rl-optimism-in-the-face-of-uncertainty" title="Permanent link">¶</a></h2>
<p>Prefer actions with uncertain but potentially high value:</p>
<p>Why? Two possible outcomes:</p>
<ol>
<li>
<p>Getting a high reward:    If the arm really has a high mean reward.</p>
</li>
<li>
<p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p>
</li>
</ol>
<p>Algorithm: </p>
<ul>
<li>
<p>Estimate an upper confidence bound <span class="arithmatex">\(U_t(a)\)</span> for each action value, such that   <span class="arithmatex">\(Q(a) \le U_t(a)\)</span> with high probability.</p>
</li>
<li>
<p>This depends on the number of times <span class="arithmatex">\(N_t(a)\)</span> action <span class="arithmatex">\(a\)</span> has been selected.</p>
</li>
<li>
<p>Select the action maximizing the Upper Confidence Bound (UCB):</p>
</li>
</ul>
<div class="arithmatex">\[a_t = \arg\max_{a \in \mathcal{A}} \left[ U_t(a) \right]\]</div>
<blockquote>
<p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards <span class="arithmatex">\(X_i \in [0,1]\)</span>,
<script type="math/tex; mode=display">
\mathbb{P}\!\left[ \mathbb{E}[X] > \bar{X}_n + u \right]
\;\le\; \exp(-2 n u^2).
</script>
</p>
<p>Setting the right-hand side equal to <span class="arithmatex">\(\delta\)</span> and solving for <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
u = \sqrt{\frac{\log(1/\delta)}{2n}}.
</script>
Here, <span class="arithmatex">\(\delta\)</span> is the failure probability, and the confidence interval
holds with probability at least <span class="arithmatex">\(1 - \delta\)</span>.
This means that, with probability at least <span class="arithmatex">\(1 - \delta\)</span>,
<script type="math/tex; mode=display">
\bar{X}_n - u \;\le\; \mathbb{E}[X] \;\le\; \bar{X}_n + u.
</script>
</p>
</blockquote>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \left[ \hat{Q}_t(a) + \text{UCB}_t(a) \right]
\]</div>
<h3 id="reinforcement-11_fast_rl-ucb1-algorithm">UCB1 Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-ucb1-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\text{UCB}_t(a) = \hat{Q}_t(a) + \sqrt{\frac{2 \log \frac{1}{\delta} }{N_t(a)}}
\]</div>
<ul>
<li>where <span class="arithmatex">\(\hat{Q}_t(a)\)</span> is empirical average</li>
<li><span class="arithmatex">\(N_t(a)\)</span> is number of samples of <span class="arithmatex">\(a\)</span> after <span class="arithmatex">\(t\)</span> timesteps.</li>
<li>Provable sublinear regret.</li>
<li>Balances estimated value and exploration bonus.</li>
</ul>
<p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p>
<p>1: Initialize for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span>:  <span class="arithmatex">\(\quad N(a) \leftarrow 0,\;\; \hat{Q}(a) \leftarrow 0\)</span>
2: Warm start (sample each arm once):<br>
3: for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span> do<br>
4: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a\)</span>, observe reward <span class="arithmatex">\(r \in [0,1]\)</span><br>
5: <span class="arithmatex">\(\quad N(a) \leftarrow 1\)</span><br>
6: <span class="arithmatex">\(\quad \hat{Q}(a) \leftarrow r\)</span><br>
7: end for<br>
8: Set <span class="arithmatex">\(t \leftarrow |\mathcal{A}|\)</span></p>
<p>9: for <span class="arithmatex">\(t = |\mathcal{A}|+1, |\mathcal{A}|+2, \dots\)</span> do<br>
10: <span class="arithmatex">\(\quad\)</span> Compute UCB for each arm: <span class="arithmatex">\(\quad \mathrm{UCB}_t(a) = \hat{Q}(a) + \sqrt{\frac{2\log t}{N(a)}}\)</span></p>
<p>11: <span class="arithmatex">\(\quad\)</span> Select action:<span class="arithmatex">\(\quad a_t \leftarrow \arg\max_{a \in \mathcal{A}} \mathrm{UCB}_t(a)\)</span></p>
<p>12: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span></p>
<p>13: <span class="arithmatex">\(\quad\)</span> Update count: <span class="arithmatex">\(\quad N(a_t) \leftarrow N(a_t) + 1\)</span></p>
<p>14: <span class="arithmatex">\(\quad\)</span> Update empirical mean (incremental):<br>
<script type="math/tex; mode=display">
\quad \hat{Q}(a_t) \leftarrow \hat{Q}(a_t) + \frac{1}{N(a_t)}\Big(r_t - \hat{Q}(a_t)\Big)
</script>
</p>
<p>15: end for</p>
<h2 id="reinforcement-11_fast_rl-119-optimistic-initialization-in-greedy-bandit-algorithms">11.9 Optimistic Initialization in Greedy Bandit Algorithms<a class="headerlink" href="#reinforcement-11_fast_rl-119-optimistic-initialization-in-greedy-bandit-algorithms" title="Permanent link">¶</a></h2>
<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>
<h3 id="reinforcement-11_fast_rl-motivation">Motivation<a class="headerlink" href="#reinforcement-11_fast_rl-motivation" title="Permanent link">¶</a></h3>
<p>Greedy algorithms, by default, select actions with the highest estimated value:</p>
<div class="arithmatex">\[
a_t = \arg\max_a \hat{Q}_t(a)
\]</div>
<p>If these <span class="arithmatex">\(\hat{Q}_t(a)\)</span> estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>
<h3 id="reinforcement-11_fast_rl-algorithmic-details">Algorithmic Details<a class="headerlink" href="#reinforcement-11_fast_rl-algorithmic-details" title="Permanent link">¶</a></h3>
<p>We initialize:</p>
<ul>
<li><span class="arithmatex">\(\hat{Q}_0(a) = Q_{\text{init}}\)</span> for all <span class="arithmatex">\(a \in \mathcal{A}\)</span>, where <span class="arithmatex">\(Q_{\text{init}}\)</span> is set higher than any reasonable expected reward (e.g., <span class="arithmatex">\(Q_{\text{init}} = 1\)</span> if rewards are bounded in <span class="arithmatex">\([0, 1]\)</span>).</li>
<li><span class="arithmatex">\(N(a) = 1\)</span> to ensure initial update is well-defined.</li>
</ul>
<p>Then we update action values using an incremental Monte Carlo estimate:</p>
<div class="arithmatex">\[
\hat{Q}_{t}(a_t) = \hat{Q}_{t-1}(a_t) + \frac{1}{N_t(a_t)} \left( r_t - \hat{Q}_{t-1}(a_t) \right)
\]</div>
<p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p>
<ul>
<li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li>
<li>Simple to implement: No need for tuning <span class="arithmatex">\(\varepsilon\)</span> or computing uncertainty estimates.</li>
<li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-key-design-considerations">Key Design Considerations<a class="headerlink" href="#reinforcement-11_fast_rl-key-design-considerations" title="Permanent link">¶</a></h4>
<ul>
<li>How optimistic is optimistic enough?<br>
  If <span class="arithmatex">\(Q_{\text{init}}\)</span> is not much larger than the true values, the agent may not explore effectively.</li>
<li>What if <span class="arithmatex">\(Q_{\text{init}}\)</span> is too high?<br>
  Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-function-approximation">Function Approximation<a class="headerlink" href="#reinforcement-11_fast_rl-function-approximation" title="Permanent link">¶</a></h4>
<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>
<h2 id="reinforcement-11_fast_rl-1110-theoretical-frameworks-regret-and-pac">11.10 Theoretical Frameworks: Regret and PAC<a class="headerlink" href="#reinforcement-11_fast_rl-1110-theoretical-frameworks-regret-and-pac" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-11_fast_rl-regret-based-evaluation">Regret-Based Evaluation<a class="headerlink" href="#reinforcement-11_fast_rl-regret-based-evaluation" title="Permanent link">¶</a></h3>
<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p>
<ul>
<li>Many small mistakes (frequent near-optimal actions),</li>
<li>A few large mistakes (infrequent but very suboptimal actions).</li>
</ul>
<p>Minimizing regret growth with <span class="arithmatex">\(T\)</span> is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>
<h3 id="reinforcement-11_fast_rl-probably-approximately-correct-pac-framework">Probably Approximately Correct (PAC) Framework<a class="headerlink" href="#reinforcement-11_fast_rl-probably-approximately-correct-pac-framework" title="Permanent link">¶</a></h3>
<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p>
<p>An algorithm is <span class="arithmatex">\((\varepsilon, \delta)\)</span>-PAC if, on each time step <span class="arithmatex">\(t\)</span>, it chooses an action <span class="arithmatex">\(a_t\)</span> such that:</p>
<div class="arithmatex">\[
Q(a_t) \ge Q(a^*) - \varepsilon \quad \text{with probability at least } 1 - \delta
\]</div>
<p>on all but a polynomial number of time steps (in <span class="arithmatex">\(|\mathcal{A}|\)</span>, <span class="arithmatex">\(1/\varepsilon\)</span>, <span class="arithmatex">\(1/\delta\)</span>, etc). This ensures:</p>
<ul>
<li>The agent almost always behaves nearly optimally,</li>
<li>With high probability, after a reasonable amount of time.</li>
</ul>
<p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>
<h2 id="reinforcement-11_fast_rl-comparing-exploration-strategies">Comparing Exploration Strategies<a class="headerlink" href="#reinforcement-11_fast_rl-comparing-exploration-strategies" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Regret Behavior</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Greedy</td>
<td>Linear</td>
<td>No exploration mechanism</td>
</tr>
<tr>
<td>Constant <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Linear</td>
<td>Fixed chance of exploring</td>
</tr>
<tr>
<td>Decaying <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Sublinear (if tuned)</td>
<td>Requires prior knowledge of reward gaps</td>
</tr>
<tr>
<td>Optimistic Initialization</td>
<td>Sublinear (if optimistic enough)</td>
<td>Simple, effective in tabular settings</td>
</tr>
</tbody>
</table>
<p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>
<h2 id="reinforcement-11_fast_rl-bayesian-bandits">Bayesian Bandits<a class="headerlink" href="#reinforcement-11_fast_rl-bayesian-bandits" title="Permanent link">¶</a></h2>
<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in <span class="arithmatex">\([0,1]\)</span>). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>
<h3 id="reinforcement-11_fast_rl-key-idea-maintain-beliefs-over-arm-reward-distributions">Key Idea: Maintain Beliefs Over Arm Reward Distributions<a class="headerlink" href="#reinforcement-11_fast_rl-key-idea-maintain-beliefs-over-arm-reward-distributions" title="Permanent link">¶</a></h3>
<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter <span class="arithmatex">\(\\phi_i\)</span> for arm <span class="arithmatex">\(i\)</span>. Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of <span class="arithmatex">\(\\phi_i\)</span>, representing our uncertainty.</p>
<h4 id="reinforcement-11_fast_rl-prior-and-posterior">Prior and Posterior<a class="headerlink" href="#reinforcement-11_fast_rl-prior-and-posterior" title="Permanent link">¶</a></h4>
<ul>
<li>Prior: Our initial belief about <span class="arithmatex">\(\\phi_i\)</span> is encoded in a probability distribution <span class="arithmatex">\(p(\\phi_i)\)</span>.</li>
<li>Data: After pulling arm <span class="arithmatex">\(i\)</span> and observing reward <span class="arithmatex">\(r_{i1}\)</span>, we update our belief.</li>
<li>Posterior: The new belief is computed using Bayes' rule:</li>
</ul>
<div class="arithmatex">\[p(\phi_i \mid r_{i1}) =
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
p(r_{i1})
}
=
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
\int p(r_{i1} \mid \phi_i)\, p(\phi_i)\, d\phi_i
}\]</div>
<p>This posterior becomes the new prior for future updates as more data arrives.</p>
<h3 id="reinforcement-11_fast_rl-practical-considerations">Practical Considerations<a class="headerlink" href="#reinforcement-11_fast_rl-practical-considerations" title="Permanent link">¶</a></h3>
<p>Computing the posterior <span class="arithmatex">\(p(\phi_i \mid D)\)</span> (where <span class="arithmatex">\(D\)</span> is the observed data for arm <span class="arithmatex">\(i\)</span>) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p>
<ul>
<li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li>
<li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li>
</ul>
<h3 id="reinforcement-11_fast_rl-why-use-bayesian-bandits">Why Use Bayesian Bandits?<a class="headerlink" href="#reinforcement-11_fast_rl-why-use-bayesian-bandits" title="Permanent link">¶</a></h3>
<ul>
<li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li>
<li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li>
<li>Captures uncertainty in a principled and statistically coherent manner.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-summary">Summary<a class="headerlink" href="#reinforcement-11_fast_rl-summary" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian bandits treat the reward-generating parameters <span class="arithmatex">\(\phi_i\)</span> as random variables.</li>
<li>We maintain a posterior belief <span class="arithmatex">\(p(\phi_i \mid D)\)</span> using Bayes' rule.</li>
<li>When conjugate priors are used, analytical updates are possible.</li>
<li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-thompson-sampling">Thompson Sampling:<a class="headerlink" href="#reinforcement-11_fast_rl-thompson-sampling" title="Permanent link">¶</a></h3>
<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p>
<p>This approach is also known as <em>probability matching</em>: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn’t deterministically select the arm with the highest mean—it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p>
<p>Algorithm: Thompson Sampling:</p>
<p>1: Initialize prior over each arm <span class="arithmatex">\(a\)</span>, <span class="arithmatex">\(p(\mathcal{R}_a)\)</span><br>
2: for iteration <span class="arithmatex">\(= 1, 2, \dots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> For each arm <span class="arithmatex">\(a\)</span> sample a reward distribution <span class="arithmatex">\(\mathcal{R}_a\)</span> from posterior<br>
4: <span class="arithmatex">\(\quad\)</span> Compute action-value function <span class="arithmatex">\(Q(a) = \mathbb{E}[\mathcal{R}_a]\)</span><br>
5: <span class="arithmatex">\(\quad a_t \equiv \arg\max_{a \in \mathcal{A}} Q(a)\)</span><br>
6: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> Update posterior <span class="arithmatex">\(p(\mathcal{R}_a)\)</span> using Bayes Rule<br>
8: end for  </p>
<h3 id="reinforcement-11_fast_rl-contextual-bandits">Contextual Bandits<a class="headerlink" href="#reinforcement-11_fast_rl-contextual-bandits" title="Permanent link">¶</a></h3>
<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context <span class="arithmatex">\(x_t\)</span> drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p>
<p>In this setting, the goal is to learn a context-dependent policy <span class="arithmatex">\(\pi(a \mid x)\)</span> that maps the observed context <span class="arithmatex">\(x_t\)</span> to a suitable arm <span class="arithmatex">\(a_t\)</span>, maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p>
<p>Formally, the interaction at each time step <span class="arithmatex">\(t\)</span> is:</p>
<ol>
<li>Observe context <span class="arithmatex">\(x_t \in \mathcal{X}\)</span></li>
<li>Choose action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span> based on policy <span class="arithmatex">\(\pi(a \mid x_t)\)</span></li>
<li>Receive reward <span class="arithmatex">\(r_t(a_t, x_t)\)</span></li>
</ol>
<p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p>
<div class="arithmatex">\[
\pi^*(x) = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a, x)]
\]</div>
<p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>
<h2 id="reinforcement-11_fast_rl-mental-map">Mental Map<a class="headerlink" href="#reinforcement-11_fast_rl-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                Bandits: Foundations of Data-Efficient RL
     Goal: Understand exploration-exploitation in simplest setting
           Learn to act with minimal data through principled tradeoffs
                                │
                                ▼
               What Are Multi-Armed Bandits (MAB)?
 ┌─────────────────────────────────────────────────────────────┐
 │ Single-state (stateless) decision problems                  │
 │ Fixed set of actions (arms)                                 │
 │ Unknown reward distribution per arm                         │
 │ Choose an action, receive reward, repeat                    │
 │ No transition dynamics — unlike full RL                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Core Objective: Maximize Reward
 ┌─────────────────────────────────────────────────────────────┐
 │ Maximize total reward = minimize regret                     │
 │ Regret = missed opportunity vs optimal action               │
 │ Total regret used to evaluate algorithm efficiency          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                  Basic Bandit Algorithms
 ┌─────────────────────────────────────────────────────────────┐
 │ Greedy: exploit current best estimates (linear regret)      │
 │ ε-Greedy: random exploration with fixed ε                   │
 │ Decaying ε-Greedy: reduces ε over time                      │
 │ Optimistic Initialization: set high initial Q̂ values        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Principle: Optimism in the Face of Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat unvisited arms as potentially good                    │
 │ Upper Confidence Bound (UCB) algorithms                     │
 │ Tradeoff: mean reward + exploration bonus                   │
 │ Guarantees sublinear regret                                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Algorithmic Realization: UCB1
 ┌─────────────────────────────────────────────────────────────┐
 │ UCB_t(a) = Q̂_t(a) + √(2 log t / N_t(a))                     │
 │ Encourages pulling uncertain arms early                     │
 │ Regret ≈ O(√(T log T))                                      │
 │ Theoretically grounded and simple to implement              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Theoretical Frameworks: Regret vs PAC
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap from always acting optimally         │
 │ PAC: guarantees near-optimal behavior with high probability │
 │ Regret cares about sum of mistakes; PAC focuses on steps    │
 │ Both evaluate quality and efficiency of learning            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Bayesian Bandits and Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat arm rewards as random variables                       │
 │ Use prior + observed data → posterior via Bayes rule        │
 │ Conjugate priors simplify computation                       │
 │ Enable principled uncertainty reasoning                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Thompson Sampling (Bayesian)
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample reward distribution from posterior per arm           │
 │ Pull arm with highest sampled reward                        │
 │ Probabilistic optimism: match probability of being best     │
 │ Natural exploration and strong empirical performance        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Probability Matching Perspective
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ sample optimal arm w/ correct frequency │
 │ Avoids hard-coded uncertainty bonuses                       │
 │ Simpler and often better in practice                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                       Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Input context x_t at each timestep                          │
 │ Reward distribution depends on (action, context)            │
 │ Learn policy π(a | x): context-aware decision making        │
 │ Real-world applications: ads, medicine, personalization     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Summary: Bandits as Foundation for Efficient RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Bandits isolate the exploration-exploitation tradeoff       │
 │ Simpler than full RL, but deeply insightful                 │
 │ Concepts generalize to value estimation, uncertainty        │
 │ Key tools: regret, PAC bounds, posterior reasoning          │
 └─────────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-12_fast_mdps" heading-number="3.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-12-fast-reinforcement-learning-in-mdps-and-generalization">Chapter 12: Fast Reinforcement Learning in MDPs and Generalization<a class="headerlink" href="#reinforcement-12_fast_mdps-chapter-12-fast-reinforcement-learning-in-mdps-and-generalization" title="Permanent link">¶</a></h1>
<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p>
<ul>
<li>Bandits: Single-step decision-making problems.</li>
<li>MDPs: Sequential decision-making with transition dynamics.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-evaluation-frameworks">Evaluation Frameworks<a class="headerlink" href="#reinforcement-12_fast_mdps-evaluation-frameworks" title="Permanent link">¶</a></h3>
<p>To assess learning efficiency, we use:</p>
<ul>
<li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li>
<li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li>
<li>PAC (Probably Approximately Correct): Number of steps when the policy is not <span class="arithmatex">\(\epsilon\)</span>-optimal is bounded with high probability.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-exploration-approaches">Exploration Approaches<a class="headerlink" href="#reinforcement-12_fast_mdps-exploration-approaches" title="Permanent link">¶</a></h3>
<ul>
<li>Optimism under uncertainty (e.g., UCB)</li>
<li>Probability matching (e.g., Thompson Sampling)</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-pac-framework-for-mdps">PAC Framework for MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-pac-framework-for-mdps" title="Permanent link">¶</a></h2>
<p>A reinforcement learning algorithm <span class="arithmatex">\(A\)</span> is PAC if with probability at least <span class="arithmatex">\(1 - \delta\)</span>, it selects an <span class="arithmatex">\(\epsilon\)</span>-optimal action on all but a bounded number of time steps <span class="arithmatex">\(N\)</span>, where:</p>
<div class="arithmatex">\[
N = \text{poly} \left( |S|, |A|, \frac{1}{1 - \gamma}, \frac{1}{\epsilon}, \frac{1}{\delta} \right)
\]</div>
<h2 id="reinforcement-12_fast_mdps-mbie-eb-model-based-interval-estimation-with-exploration-bonus">MBIE-EB: Model-Based Interval Estimation with Exploration Bonus<a class="headerlink" href="#reinforcement-12_fast_mdps-mbie-eb-model-based-interval-estimation-with-exploration-bonus" title="Permanent link">¶</a></h2>
<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function <span class="arithmatex">\(Q(s, a)\)</span>.</p>
<p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p>
<ul>
<li>
<p>It estimates <span class="arithmatex">\(\hat{R}(s, a)\)</span> and <span class="arithmatex">\(\hat{T}(s' \mid s, a)\)</span> from data using empirical counts.</p>
</li>
<li>
<p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p>
</li>
</ul>
<p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p>
<p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p>
<p>Algorithm:</p>
<p>1: Given <span class="arithmatex">\(\epsilon\)</span>, <span class="arithmatex">\(\delta\)</span>, <span class="arithmatex">\(m\)</span><br>
2: <span class="arithmatex">\(\beta = \dfrac{1}{1-\gamma}\sqrt{0.5 \ln \!\left(\dfrac{2|S||A|m}{\delta}\right)}\)</span><br>
3: <span class="arithmatex">\(n_{sas}(s,a,s') = 0\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A, s' \in S\)</span><br>
4: <span class="arithmatex">\(rc(s,a) = 0\)</span>, <span class="arithmatex">\(n_{sa}(s,a) = 0\)</span>, <span class="arithmatex">\(\hat{Q}(s,a) = \dfrac{1}{1-\gamma}\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A\)</span><br>
5: <span class="arithmatex">\(t = 0\)</span>, <span class="arithmatex">\(s_t = s_{\text{init}}\)</span><br>
6: loop<br>
7: <span class="arithmatex">\(\quad a_t = \arg\max_{a \in A} \hat{Q}(s_t, a)\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and state <span class="arithmatex">\(s_{t+1}\)</span><br>
9: <span class="arithmatex">\(\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\)</span>,<br>
<span class="arithmatex">\(\quad\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\)</span><br>
10: <span class="arithmatex">\(\quad rc(s_t,a_t) = \dfrac{rc(s_t,a_t)\big(n_{sa}(s_t,a_t)-1\big) + r_t}{n_{sa}(s_t,a_t)}\)</span><br>
11: <span class="arithmatex">\(\quad \hat{R}(s_t,a_t) = rc(s_t,a_t)\)</span> and<br>
<span class="arithmatex">\(\quad\quad \hat{T}(s' \mid s_t,a_t) = \dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\)</span>, <span class="arithmatex">\(\forall s' \in S\)</span><br>
12: <span class="arithmatex">\(\quad\)</span> while not converged do<br>
13: <span class="arithmatex">\(\quad\quad \hat{Q}(s,a) = \hat{R}(s,a) + \gamma \sum_{s'} \hat{T}(s' \mid s,a)\max_{a'} \hat{Q}(s',a') + \dfrac{\beta}{\sqrt{n_{sa}(s,a)}}\)</span>,<br>
<span class="arithmatex">\(\quad\quad\quad \forall s \in S, a \in A\)</span><br>
14: <span class="arithmatex">\(\quad\)</span> end while<br>
15: end loop</p>
<h2 id="reinforcement-12_fast_mdps-bayesian-model-based-reinforcement-learning">Bayesian Model-Based Reinforcement Learning<a class="headerlink" href="#reinforcement-12_fast_mdps-bayesian-model-based-reinforcement-learning" title="Permanent link">¶</a></h2>
<p>Bayesian RL methods maintain a posterior over MDP models <span class="arithmatex">\((P, R)\)</span> and sample plausible environments from the posterior to plan and act.</p>
<p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p>
<p>Formally, the Thompson sampling policy is:</p>
<div class="arithmatex">\[
\pi(s, a \mid h_t) = \mathbb{P}\left(Q(s, a) \ge Q(s, a'),\; \forall a' \ne a \;\middle|\; h_t \right)
= \mathbb{E}_{\mathcal{P}, \mathcal{R} \mid h_t} \left[ \mathbb{1}\left(a = \arg\max_{a \in \mathcal{A}} Q(s, a)\right) \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(h_t\)</span> is the history up to time <span class="arithmatex">\(t\)</span> (including all observed transitions and rewards),
- <span class="arithmatex">\(\mathcal{P}, \mathcal{R}\)</span> are the transition and reward functions respectively,
- The expectation is taken over the posterior belief on the MDP <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span>.</p>
<h3 id="reinforcement-12_fast_mdps-thompson-sampling-algorithm-in-mdps">Thompson Sampling Algorithm in MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-thompson-sampling-algorithm-in-mdps" title="Permanent link">¶</a></h3>
<ol>
<li>Maintain a posterior <span class="arithmatex">\(p(\mathcal{P}, \mathcal{R} \mid h_t)\)</span> over the transition and reward models based on all observed data.</li>
<li>Sample a model <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span> from the posterior distribution.</li>
<li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function <span class="arithmatex">\(Q^*(s, a)\)</span>.</li>
<li>Select the action according to the optimal action in the sampled model:
   <script type="math/tex; mode=display">
   a_t = \arg\max_{a \in \mathcal{A}} Q^*(s_t, a)
   </script>
</li>
</ol>
<h3 id="reinforcement-12_fast_mdps-algorithm-thompson-sampling-for-mdps">Algorithm: Thompson Sampling for MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-algorithm-thompson-sampling-for-mdps" title="Permanent link">¶</a></h3>
<p>1: Initialize prior over dynamics and reward models for each <span class="arithmatex">\((s, a)\)</span>:  <span class="arithmatex">\(\quad p(\mathcal{T}(s' \mid s, a)), \quad p(\mathcal{R}(s, a))\)</span><br>
2: Initialize initial state <span class="arithmatex">\(s_0\)</span><br>
3: for <span class="arithmatex">\(k = 1\)</span> to <span class="arithmatex">\(K\)</span> episodes do<br>
4: <span class="arithmatex">\(\quad\)</span> Sample an MDP <span class="arithmatex">\(\mathcal{M}\)</span>:<br>
5: <span class="arithmatex">\(\quad\quad\)</span> for each <span class="arithmatex">\((s, a)\)</span> pair do<br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample transition model <span class="arithmatex">\(\mathcal{T}(s' \mid s, a)\)</span> from posterior<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample reward model <span class="arithmatex">\(\mathcal{R}(s, a)\)</span> from posterior<br>
8: <span class="arithmatex">\(\quad\quad\)</span> end for<br>
9: <span class="arithmatex">\(\quad\)</span> Compute optimal value function <span class="arithmatex">\(Q_{\mathcal{M}}^*\)</span> for sampled MDP <span class="arithmatex">\(\mathcal{M}\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1\)</span> to <span class="arithmatex">\(H\)</span> do<br>
11: <span class="arithmatex">\(\quad\quad a_t = \arg\max_{a \in \mathcal{A}} Q_{\mathcal{M}}^*(s_t, a)\)</span><br>
12: <span class="arithmatex">\(\quad\quad\)</span> Take action <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span><br>
13: <span class="arithmatex">\(\quad\)</span> end for<br>
14: <span class="arithmatex">\(\quad\)</span> Update posteriors: <span class="arithmatex">\(\quad\quad p(\mathcal{R}_{s_t, a_t} \mid r_t), \quad p(\mathcal{T}(s' \mid s_t, a_t) \mid s_{t+1})\)</span> using Bayes Rule<br>
15: end for</p>
<h2 id="reinforcement-12_fast_mdps-key-characteristics">Key Characteristics<a class="headerlink" href="#reinforcement-12_fast_mdps-key-characteristics" title="Permanent link">¶</a></h2>
<ul>
<li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li>
<li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li>
<li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li>
</ul>
<blockquote>
<p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>
</blockquote>
<h2 id="reinforcement-12_fast_mdps-generalization-in-contextual-bandits">Generalization in Contextual Bandits<a class="headerlink" href="#reinforcement-12_fast_mdps-generalization-in-contextual-bandits" title="Permanent link">¶</a></h2>
<p>Contextual bandits generalize standard bandits by associating a context or state <span class="arithmatex">\(s\)</span> with each decision:</p>
<ul>
<li>Reward depends on both context and action: <span class="arithmatex">\(r \sim P[r | s,a]\)</span></li>
<li>Often model reward as linear: <span class="arithmatex">\(r = \theta^\top \phi(s,a) + \epsilon\)</span>, with <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span></li>
</ul>
<h3 id="reinforcement-12_fast_mdps-benefits-of-generalization">Benefits of Generalization<a class="headerlink" href="#reinforcement-12_fast_mdps-benefits-of-generalization" title="Permanent link">¶</a></h3>
<ul>
<li>Allows learning across states/actions</li>
<li>Enables sample-efficient exploration in large state/action spaces</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-strategic-exploration-in-deep-rl">Strategic Exploration in Deep RL<a class="headerlink" href="#reinforcement-12_fast_mdps-strategic-exploration-in-deep-rl" title="Permanent link">¶</a></h2>
<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>
<h3 id="reinforcement-12_fast_mdps-optimistic-q-learning-with-function-approximation">Optimistic Q-Learning with Function Approximation<a class="headerlink" href="#reinforcement-12_fast_mdps-optimistic-q-learning-with-function-approximation" title="Permanent link">¶</a></h3>
<p>Modified Q-learning update:</p>
<div class="arithmatex">\[
\Delta w = \alpha \left( r + r_{\text{bonus}}(s,a) + \gamma \max_{a'} Q(s', a'; w) - Q(s,a;w) \right) \nabla_w Q(s,a;w)
\]</div>
<p>Bonus <span class="arithmatex">\(r_{\text{bonus}}\)</span> reflects novelty or epistemic uncertainty.</p>
<h3 id="reinforcement-12_fast_mdps-count-based-and-density-based-exploration">Count-Based and Density-Based Exploration<a class="headerlink" href="#reinforcement-12_fast_mdps-count-based-and-density-based-exploration" title="Permanent link">¶</a></h3>
<ul>
<li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li>
<li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li>
<li>Tang et al. (2017) use hashing-based counts.</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-thompson-sampling-for-deep-rl">Thompson Sampling for Deep RL<a class="headerlink" href="#reinforcement-12_fast_mdps-thompson-sampling-for-deep-rl" title="Permanent link">¶</a></h2>
<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>
<h3 id="reinforcement-12_fast_mdps-bootstrapped-dqn">Bootstrapped DQN<a class="headerlink" href="#reinforcement-12_fast_mdps-bootstrapped-dqn" title="Permanent link">¶</a></h3>
<ul>
<li>Train multiple Q-networks on bootstrapped datasets.</li>
<li>Select one head randomly at each episode for exploration.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-bayesian-deep-q-networks">Bayesian Deep Q-Networks<a class="headerlink" href="#reinforcement-12_fast_mdps-bayesian-deep-q-networks" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian linear regression on final layer</li>
<li>Posterior used to sample Q-values, enabling optimism</li>
<li>Outperforms naive bootstrapped DQNs in some settings</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-mental-map">Mental Map<a class="headerlink" href="#reinforcement-12_fast_mdps-mental-map" title="Permanent link">¶</a></h2>
<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization
  Goal: Learn near-optimal policies in MDPs with limited data
    Extend bandit exploration ideas to sequential decision making
                            │
                            ▼
             Why MDPs Are Harder Than Bandits
</code></pre>
<p>┌─────────────────────────────────────────────────────────────┐
 │ MDPs involve sequential decisions with transitions           │
 │ Agent must explore over states and transitions              │
 │ Exploration affects future knowledge &amp; rewards              │
 │ Sample inefficiency is a major practical bottleneck         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Evaluation Frameworks for RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap vs optimal policy over time          │
 │ PAC (Probably Approximately Correct):                       │
 │   Guarantees ε-optimality with high probability             │
 │ Bayesian Regret: expected regret under prior over MDPs      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              PAC Learning in MDPs: Formal Guarantee
 ┌─────────────────────────────────────────────────────────────┐
 │ Algorithm is PAC if all but N steps are ε-optimal           │
 │ N = poly(|S|, |A|, 1/(1-γ), 1/ε, 1/δ)                        │
 │ Ensures high-probability performance bounds                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Optimism: MBIE-EB Algorithm (Model-Based)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate reward + transitions from data                     │
 │ Add bonus to Q-values: encourages actions with high uncertainty │
 │ Optimistic model induces exploration                        │
 │ Dynamic programming over Q̂ + bonus → exploration policy     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Algorithmic Principle: Optimism Under Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Add uncertainty-driven bonus to reward or Q-value           │
 │ Drives exploration to unknown regions                       │
 │ Simple but effective in tabular MDPs                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian RL and Posterior Sampling
 ┌─────────────────────────────────────────────────────────────┐
 │ Maintain belief (posterior) over MDP model (P, R)           │
 │ Sample MDP from posterior → plan optimally in sampled MDP   │
 │ Leads to probability matching via Thompson Sampling         │
 │ Posterior concentrates with data → convergence to optimal   │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Algorithm: Thompson Sampling in Model-Based RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample dynamics + rewards from posterior                    │
 │ Solve sampled MDP for optimal Q<em>                            │
 │ Act according to Q</em> in sample MDP                           │
 │ Update posterior using Bayes rule after each step           │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Exploration via Posterior Variance (Bayes)
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ Probability Matching                    │
 │ Probabilistically favors optimal but uncertain policies     │
 │ Elegant &amp; adaptive exploration                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Generalization via Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Rewards depend on both context and action                   │
 │ Learn generalizable function: Q(s,a) or π(a|s)              │
 │ Enables learning across states / actions                    │
 │ Use linear models or embeddings: φ(s,a)                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
         Exploration + Generalization in Deep RL Settings
 ┌─────────────────────────────────────────────────────────────┐
 │ Optimistic Q-learning: add r_bonus(s,a) in TD target        │
 │ r_bonus from novelty, density models, or uncertainty        │
 │ Count-based, hashing, or learned density bonuses            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian Deep RL: Posterior Approximation
 ┌─────────────────────────────────────────────────────────────┐
 │ Bootstrapped DQN: ensemble of Q-networks for exploration    │
 │ Bayesian DQN: sample from approximate Q-posteriors          │
 │ Enables implicit Thompson-like behavior                     │
 │ Scales to high-dimensional state/action spaces              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                         Chapter Summary
 ┌─────────────────────────────────────────────────────────────┐
 │ Strategic exploration = key to fast learning in MDPs        │
 │ Optimism (MBIE-EB) and Bayesian methods (Thompson)          │
 │ PAC and Bayesian regret are key evaluation tools            │
 │ Generalization (via features or deep nets) enables scaling  │
 │ Thompson Sampling and bootstrapped approximations bridge gap│
 │ Between tabular and high-dimensional RL                     │
 └─────────────────────────────────────────────────────────────┘
````</p></body></html></section><section class="print-page" id="reinforcement-13_montecarlo" heading-number="3.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-13-monte-carlo-tree-search">Chapter 13: Monte Carlo Tree Search<a class="headerlink" href="#reinforcement-13_montecarlo-chapter-13-monte-carlo-tree-search" title="Permanent link">¶</a></h1>
<p>Monte Carlo Tree Search (MCTS) is a powerful planning algorithm that uses simulation-based search to select actions in complex decision-making problems. It is especially effective in large or unknown environments where exact planning is infeasible. MCTS balances exploration and exploitation through sampling and is the backbone of major AI breakthroughs like AlphaGo and AlphaZero.</p>
<h2 id="reinforcement-13_montecarlo-131-motivation">13.1 Motivation<a class="headerlink" href="#reinforcement-13_montecarlo-131-motivation" title="Permanent link">¶</a></h2>
<p>In classical reinforcement learning (RL), agents often compute policies over the <em>entire</em> state space. MCTS takes a different approach: it performs local search from the current state, using simulated episodes to estimate action values and make near-optimal decisions <em>on the fly</em>.</p>
<p>This method is particularly useful in:</p>
<ul>
<li>Large state/action spaces</li>
<li>Games with high branching factor (e.g., Go, Chess)</li>
<li>Black-box or simulator-only environments</li>
</ul>
<h2 id="reinforcement-13_montecarlo-132-monte-carlo-search">13.2 Monte Carlo Search<a class="headerlink" href="#reinforcement-13_montecarlo-132-monte-carlo-search" title="Permanent link">¶</a></h2>
<p>A simple Monte Carlo search uses a model <span class="arithmatex">\(\mathcal{M}\)</span> (dynamics and resward model) and a rollout policy <span class="arithmatex">\(\pi\)</span> to simulate <span class="arithmatex">\(K\)</span> trajectories for each action <span class="arithmatex">\(a\)</span> from the current state <span class="arithmatex">\(s_t\)</span>:</p>
<ol>
<li>Simulate episodes <span class="arithmatex">\(\{s_t, a, r_{t+1}^{(k)}, \ldots, s_T^{(k)}\}\)</span> from <span class="arithmatex">\(\mathcal{M}, \pi\)</span>.</li>
<li>Estimate <span class="arithmatex">\(Q(s_t, a)\)</span> via sample average:</li>
</ol>
<div class="arithmatex">\[
Q(s_t, a) = \frac{1}{K} \sum_{k=1}^K G_t^{(k)} \rightarrow q^\pi(s_t, a)
\]</div>
<ol>
<li>Select the best action:</li>
</ol>
<div class="arithmatex">\[
a_t = \arg\max_a Q(s_t, a)
\]</div>
<p>This performs one-step policy improvement, but does not build deeper search trees.</p>
<h2 id="reinforcement-13_montecarlo-133-expectimax-search">13.3 Expectimax Search<a class="headerlink" href="#reinforcement-13_montecarlo-133-expectimax-search" title="Permanent link">¶</a></h2>
<p>To go beyond single-step rollouts, expectimax trees compute <span class="arithmatex">\(Q^*(s, a)\)</span> recursively using the model:</p>
<ul>
<li>Each node expands by looking ahead using the transition model.</li>
<li>Combines maximization (over actions) and expectation (over next states).</li>
<li>Forward search avoids solving the entire MDP and focuses only on the subtree starting at <span class="arithmatex">\(s_t\)</span>.</li>
</ul>
<p>However, the number of nodes grows exponentially with horizon <span class="arithmatex">\(H\)</span>: <span class="arithmatex">\(O(|S||A|)^H\)</span>.</p>
<h2 id="reinforcement-13_montecarlo-134-monte-carlo-tree-search-mcts">13.4 Monte Carlo Tree Search (MCTS)<a class="headerlink" href="#reinforcement-13_montecarlo-134-monte-carlo-tree-search-mcts" title="Permanent link">¶</a></h2>
<p>MCTS improves on expectimax by sampling rather than fully expanding the tree:</p>
<ol>
<li>Build a tree rooted at current state <span class="arithmatex">\(s_t\)</span>.</li>
<li>Perform <span class="arithmatex">\(K\)</span> simulations to expand and update parts of the tree.</li>
<li>Estimate <span class="arithmatex">\(Q(s, a)\)</span> using sampled returns.</li>
<li>Select the best action at the root:</li>
</ol>
<div class="arithmatex">\[
a_t = \arg\max_a Q(s_t, a)
\]</div>
<h2 id="reinforcement-13_montecarlo-135-upper-confidence-tree-uct">13.5 Upper Confidence Tree (UCT)<a class="headerlink" href="#reinforcement-13_montecarlo-135-upper-confidence-tree-uct" title="Permanent link">¶</a></h2>
<p>A key challenge in MCTS is deciding which action to simulate at each tree node. UCT addresses this by treating each decision as a multi-armed bandit problem and using an Upper Confidence Bound:</p>
<div class="arithmatex">\[
Q(s, a, i) = \underbrace{\frac{1}{N(i, a)} \sum_{k=1}^{N(i,a)} G_k(i,a)}_{\text{Mean Return}} + \underbrace{c \sqrt{\frac{\log N(i)}{N(i, a)}}}_{\text{Exploration Bonus}}
\]</div>
<ul>
<li><span class="arithmatex">\(N(i, a)\)</span>: number of times action <span class="arithmatex">\(a\)</span> taken at node <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(N(i)\)</span>: total visits to node <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(c\)</span>: exploration constant</li>
<li><span class="arithmatex">\(G_k(i, a)\)</span>: return from simulation <span class="arithmatex">\(k\)</span> for <span class="arithmatex">\((i, a)\)</span></li>
</ul>
<p>Action selection:</p>
<div class="arithmatex">\[
a_k^i = \arg\max_a Q(s, a, i)
\]</div>
<p>This balances exploitation of known good actions and exploration of uncertain ones.</p>
<h2 id="reinforcement-13_montecarlo-136-advantages-of-mcts">13.6 Advantages of MCTS<a class="headerlink" href="#reinforcement-13_montecarlo-136-advantages-of-mcts" title="Permanent link">¶</a></h2>
<ul>
<li>Anytime: Can stop search at any time and use the best estimates so far.</li>
<li>Model-based or black-box: Only needs sample access to the environment.</li>
<li>Best-first: Focuses computation on promising actions.</li>
<li>Scalable: Avoids full enumeration of action/state spaces.</li>
<li>Parallelizable: Independent simulations can be run in parallel.</li>
</ul>
<h2 id="reinforcement-13_montecarlo-137-alphazero-and-deep-mcts">13.7 AlphaZero and Deep MCTS<a class="headerlink" href="#reinforcement-13_montecarlo-137-alphazero-and-deep-mcts" title="Permanent link">¶</a></h2>
<p>AlphaZero revolutionized game-playing AI by combining deep learning with MCTS. Key ideas:</p>
<h3 id="reinforcement-13_montecarlo-policy-and-value-networks">Policy and Value Networks<a class="headerlink" href="#reinforcement-13_montecarlo-policy-and-value-networks" title="Permanent link">¶</a></h3>
<p>A neural network <span class="arithmatex">\(f_\theta(s)\)</span> outputs:</p>
<ul>
<li><span class="arithmatex">\(P\)</span>: action probabilities</li>
<li><span class="arithmatex">\(V\)</span>: value estimate</li>
</ul>
<div class="arithmatex">\[
(p, v) = f_\theta(s)
\]</div>
<h3 id="reinforcement-13_montecarlo-alphazero-mcts-steps">AlphaZero MCTS Steps<a class="headerlink" href="#reinforcement-13_montecarlo-alphazero-mcts-steps" title="Permanent link">¶</a></h3>
<ol>
<li>Select: Traverse tree using <span class="arithmatex">\(Q + U\)</span> to choose child nodes.</li>
<li>Expand: Add a new node, initialized with <span class="arithmatex">\(P\)</span> from <span class="arithmatex">\(f_\theta\)</span>.</li>
<li>Evaluate: Use <span class="arithmatex">\(v\)</span> from the network as the value of the leaf.</li>
<li>Backup: Propagate value estimates up the tree.</li>
<li>Repeat: Perform many rollouts to refine the tree.</li>
</ol>
<h3 id="reinforcement-13_montecarlo-root-action-selection">Root Action Selection<a class="headerlink" href="#reinforcement-13_montecarlo-root-action-selection" title="Permanent link">¶</a></h3>
<p>At the root, use visit counts <span class="arithmatex">\(N(s,a)\)</span> to compute the improved policy:</p>
<div class="arithmatex">\[
\pi(s, a) \propto N(s, a)^{1/\tau}
\]</div>
<p>where <span class="arithmatex">\(\tau\)</span> controls exploration vs exploitation.</p>
<h2 id="reinforcement-13_montecarlo-138-self-play-and-training">13.8 Self-Play and Training<a class="headerlink" href="#reinforcement-13_montecarlo-138-self-play-and-training" title="Permanent link">¶</a></h2>
<p>AlphaZero uses self-play to generate training data:</p>
<ol>
<li>Play full games using MCTS.</li>
<li>Record <span class="arithmatex">\((s, \pi, z)\)</span> tuples where:</li>
<li><span class="arithmatex">\(s\)</span>: game state</li>
<li><span class="arithmatex">\(\pi\)</span>: improved policy from MCTS</li>
<li><span class="arithmatex">\(z\)</span>: final game outcome</li>
<li>Train <span class="arithmatex">\(f_\theta\)</span> to minimize combined loss:</li>
</ol>
<div class="arithmatex">\[
\mathcal{L} = (z - v)^2 - \pi^\top \log p + \lambda \|\theta\|^2
\]</div>
<p>This allows continual improvement without human supervision.</p>
<h2 id="reinforcement-13_montecarlo-139-evaluation-and-impact">13.9 Evaluation and Impact<a class="headerlink" href="#reinforcement-13_montecarlo-139-evaluation-and-impact" title="Permanent link">¶</a></h2>
<ul>
<li>MCTS dramatically improves performance over raw policy/value networks.</li>
<li>Essential to surpassing human performance in Go, Chess, and Shogi.</li>
<li>Eliminates the need for human expert data.</li>
</ul>
<p>Insights:</p>
<ul>
<li>UCT enables principled tree search with exploration.</li>
<li>Neural nets guide and accelerate MCTS.</li>
<li>MCTS can be used in any environment where lookahead is possible.</li>
</ul>
<h2 id="reinforcement-13_montecarlo-1310-summary">13.10 Summary<a class="headerlink" href="#reinforcement-13_montecarlo-1310-summary" title="Permanent link">¶</a></h2>
<ul>
<li>MCTS uses simulation-based planning with a growing search tree.</li>
<li>UCT adds upper confidence bounds to balance exploration/exploitation.</li>
<li>AlphaZero combines MCTS with deep learning for superhuman performance.</li>
<li>Self-play enables autonomous training without labeled data.</li>
</ul>
<p>MCTS represents a powerful bridge between planning and learning, enabling agents to make strong decisions under uncertainty in complex domains.</p></body></html></section><section class="print-page" id="reinforcement-14_final" heading-number="3.14"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-14-reinforcement-learning-review-and-wrap-up">Chapter 14: Reinforcement Learning – Review and Wrap-Up<a class="headerlink" href="#reinforcement-14_final-chapter-14-reinforcement-learning-review-and-wrap-up" title="Permanent link">¶</a></h1>
<p>In this final chapter, we recap the journey of reinforcement learning (RL) from its foundational ideas in multi-armed bandits through to the cutting-edge of deep RL. Along the way we will revisit key algorithmic concepts – including Upper Confidence Bounds (UCB), Thompson Sampling, Model-Based Interval Estimation with Exploration Bonus (MBIE-EB), and Monte Carlo Tree Search (MCTS) – and highlight how different approaches to exploration (optimism vs. probability matching) have shaped the field. We will also emphasize the theoretical foundations of RL (regret minimization, PAC guarantees, Bayesian methods) and illustrate how these principles connect to real-world successes like AlphaTensor and ChatGPT. Throughout, the aim is to provide a high-level summary and synthesis, reinforcing the insights gained across previous chapters.</p>
<h2 id="reinforcement-14_final-recap-from-bandits-to-deep-reinforcement-learning">Recap: From Bandits to Deep Reinforcement Learning<a class="headerlink" href="#reinforcement-14_final-recap-from-bandits-to-deep-reinforcement-learning" title="Permanent link">¶</a></h2>
<p>Reinforcement learning can be defined as learning through experience (data) to make good decisions under uncertainty. In an RL problem, an agent interacts with an environment, observes states <span class="arithmatex">\(s\)</span>, takes actions <span class="arithmatex">\(a\)</span>, and receives rewards <span class="arithmatex">\(r\)</span>, with the goal of learning a policy <span class="arithmatex">\(\pi(a|s)\)</span> that maximizes future expected reward. Several core features distinguish RL from other learning paradigms:</p>
<ul>
<li>
<p>Optimization of Long-Term Reward: The agent seeks to maximize cumulative reward, accounting for delayed consequences of actions.</p>
</li>
<li>
<p>Trial-and-Error Learning: The agent learns by exploring different actions and observing outcomes, balancing exploration vs. exploitation.</p>
</li>
<li>
<p>Generalization: The agent must generalize from limited experience to new situations (often via function approximation in large state spaces).</p>
</li>
<li>
<p>Data Distribution Shift: Unlike supervised learning, the agent’s own actions affect the data it collects and the states it visits, creating a feedback loop in the learning process.</p>
</li>
</ul>
<p>We began our journey with multi-armed bandits, the simplest RL setting. In a bandit problem there is a single state (no state transitions); each action (arm) yields a reward drawn from an unknown distribution, and the goal is to maximize reward over repeated plays. A bandit is essentially a stateless decision problem – the next situation does not depend on the previous action. This contrasts with the general Markov Decision Process (MDP) setting, where each action can change the state and influence future rewards and decisions. Bandits capture the essence of exploration-exploitation without the complication of state transitions, making them a perfect starting point.</p>
<p>From bandits we progressed to MDPs and multi-step RL problems, which introduce state dynamics and temporal credit assignment. We studied model-free methods (like Q-learning and policy gradient) and model-based methods (like planning with known models or learned models), as well as combinations thereof. As tasks grew more complex, we incorporated function approximation (e.g. using deep neural networks) to handle large or continuous state spaces. This led us into the realm of deep reinforcement learning, where algorithms like DQN and policy optimization methods (PPO, etc.) leverage deep networks as powerful function approximators. While function approximation enables scaling to complex domains, it also introduced new challenges such as stability of learning (e.g. off-policy learning instability, need for techniques like experience replay, target networks, or trust region methods). In parallel, we discussed how off-policy learning and exploration in large domains remain critical challenges, and saw approaches to address these (from clipped policy optimization (PPO) for stability, to imitation learning like DAGGER to incorporate expert knowledge, to pessimistic value adjustments for safer offline learning).</p>
<p>Throughout this journey, a unifying theme has been the exploration-exploitation dilemma and the development of algorithms to efficiently learn optimal strategies. In the following sections, we summarize some key algorithmic ideas for exploration and discuss how they exemplify different strategies to address this core challenge.</p>
<h2 id="reinforcement-14_final-key-algorithmic-ideas-in-exploration-and-planning">Key Algorithmic Ideas in Exploration and Planning<a class="headerlink" href="#reinforcement-14_final-key-algorithmic-ideas-in-exploration-and-planning" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-14_final-optimistic-exploration-upper-confidence-bounds-ucb">Optimistic Exploration: Upper Confidence Bounds (UCB)<a class="headerlink" href="#reinforcement-14_final-optimistic-exploration-upper-confidence-bounds-ucb" title="Permanent link">¶</a></h3>
<p>A foundational idea for efficient exploration is optimism in the face of uncertainty. The principle is simple: assume the best about untried actions so that the agent is driven to explore them. The Upper Confidence Bound (UCB) algorithm is a classic realization of this idea for multi-armed bandits. UCB maintains an estimate <span class="arithmatex">\(\hat{Q}_t(a)\)</span> for the mean reward of each arm <span class="arithmatex">\(a\)</span> and an uncertainty interval (confidence bound) around that estimate. At each time <span class="arithmatex">\(t\)</span>, it selects the action maximizing an upper-confidence estimate of the reward:</p>
<div class="arithmatex">\[
a_t = \arg\max_{a \in A} \left[ \hat{Q}_t(a) + c \frac{\ln t}{N_t(a)} \right],
\]</div>
<p>where <span class="arithmatex">\(N_t(a)\)</span> is the number of times action <span class="arithmatex">\(a\)</span> has been taken up to time <span class="arithmatex">\(t\)</span>, and <span class="arithmatex">\(c\)</span> is a constant (e.g. <span class="arithmatex">\(c=\sqrt{2}\)</span> for the UCB1 algorithm).</p>
<p>This selection rule balances exploitation (the <span class="arithmatex">\(\hat{Q}_t(a)\)</span> term) with exploration (the bonus term that is large for rarely-selected actions). Intuitively, UCB explores actions with high potential payoffs or high uncertainty. This approach yields strong theoretical guarantees: for instance, UCB1 achieves sublinear regret on the order of <span class="arithmatex">\(O(\ln T)\)</span> for bandits, meaning the gap between the accumulated reward of UCB and that of an oracle choosing the best arm at each play grows only logarithmically with time. Optimistic algorithms like UCB are attractive because they are simple and provide worst-case performance guarantees (they will eventually try everything enough to near-certainty). Variants of UCB and optimism-driven exploration have been extended beyond bandits, for example to MDPs via exploration bonus terms.</p>
<h3 id="reinforcement-14_final-probability-matching-thompson-sampling">Probability Matching: Thompson Sampling<a class="headerlink" href="#reinforcement-14_final-probability-matching-thompson-sampling" title="Permanent link">¶</a></h3>
<p>An alternative approach to exploration comes from a Bayesian perspective. Instead of confidence bounds, the agent maintains a posterior distribution over the reward parameters of each action and samples an action according to the probability it is optimal. This strategy is known as Thompson Sampling (or probability matching). In the multi-armed bandit setting, Thompson Sampling can be implemented by assuming a prior for each arm’s mean reward, updating it with observed rewards, and then at each step sampling a value <span class="arithmatex">\(\tilde{\theta}_a\)</span> from the posterior of each arm’s mean. The agent then plays the arm with the highest sampled value. By randomly exploring according to its uncertainty, Thompson Sampling naturally balances exploration and exploitation in a Bayesian-optimal way for certain problems.</p>
<p>For example, if rewards are Bernoulli and a Beta prior is used for each arm’s success probability, Thompson Sampling draws a sample from each arm’s Beta posterior and picks the arm with the largest sample. This probability matching tends to allocate more trials to arms that are likely to be best, yet still occasionally tries others proportional to uncertainty. Empirically, Thompson Sampling often performs exceptionally well, sometimes even outperforming UCB in practice, and it has a Bayesian regret that is optimal in certain settings. The caveat is that analyzing Thompson Sampling’s worst-case performance is more complex; however, theoretical advances have shown Thompson Sampling achieves <span class="arithmatex">\(O(\ln T)\)</span> regret for many bandit problems as well. A key appeal of Thompson Sampling is its flexibility – it can be applied to complex problems if one can sample from a posterior (or an approximate posterior) of the model’s parameters. In modern RL, variants of Thompson Sampling inspire approaches like Bootstrapped DQN (which maintains an ensemble of value networks to generate randomized Q-value estimates for exploration).</p>
<h3 id="reinforcement-14_final-pac-mdp-algorithms-and-exploration-bonuses-mbie-eb">PAC-MDP Algorithms and Exploration Bonuses (MBIE-EB)<a class="headerlink" href="#reinforcement-14_final-pac-mdp-algorithms-and-exploration-bonuses-mbie-eb" title="Permanent link">¶</a></h3>
<p>In full reinforcement learning problems (MDPs), the exploration challenge becomes more intricate due to state transitions. PAC-MDP algorithms provide a framework for efficient exploration with theoretical guarantees. PAC stands for “Probably Approximately Correct,” meaning these algorithms guarantee that with high probability (<span class="arithmatex">\(1-\delta\)</span>) the agent will behave near-optimally (within <span class="arithmatex">\(\varepsilon\)</span> of the optimal return) after a certain number of time steps that is polynomial in relevant problem parameters. In other words, a PAC-MDP algorithm will make only a finite (polynomial) number of suboptimal decisions before it effectively converges to an <span class="arithmatex">\(\varepsilon\)</span>-optimal policy.</p>
<p>One representative PAC-MDP approach is Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) by Strehl and Littman (2008). This algorithm uses an optimistic model-based strategy: it learns an estimated MDP (transition probabilities <span class="arithmatex">\(\hat{T}\)</span> and rewards <span class="arithmatex">\(\hat{R}\)</span>) from experience and uses dynamic programming to compute a value function <span class="arithmatex">\(\tilde{Q}(s,a)\)</span> for that estimated model. Critically, MBIE-EB adds an exploration bonus term to reward or value updates for state-action pairs that have been infrequently visited. For example, the update might be:</p>
<div class="arithmatex">\[
\tilde{Q}(s,a) \leftarrow \hat{R}(s,a) + \gamma \sum_{s'} \hat{T}(s'|s,a) \max_{a'} \tilde{Q}(s',a') + \beta \frac{1}{\sqrt{N(s,a)}},
\]</div>
<p>where <span class="arithmatex">\(N(s,a)\)</span> counts visits to <span class="arithmatex">\((s,a)\)</span> and <span class="arithmatex">\(\beta\)</span> is a bonus scale derived from PAC confidence bounds. The <span class="arithmatex">\(\sqrt{1/N(s,a)}\)</span> bonus term is large for rarely tried state-action pairs, injecting optimism that encourages the agent to explore them. MBIE-EB selects actions according to the optimistic <span class="arithmatex">\(\tilde{Q}\)</span> values (i.e. optimism under uncertainty in an MDP context). Strehl and Littman proved that MBIE-EB is PAC-MDP: with probability <span class="arithmatex">\(1-\delta\)</span>, after a number of steps polynomial in <span class="arithmatex">\(|S|, |A|, 1/\varepsilon, 1/\delta\)</span>, etc., the algorithm’s policy is <span class="arithmatex">\(\varepsilon\)</span>-optimal. PAC algorithms like MBIE-EB (and related methods like R-MAX and UCRL) guarantee efficient exploration in theory, though they can be computationally demanding in practice for large domains. They illustrate how theoretical foundations (confidence intervals and PAC guarantees) directly inform algorithm design.</p>
<h3 id="reinforcement-14_final-monte-carlo-tree-search-mcts-for-planning">Monte Carlo Tree Search (MCTS) for Planning<a class="headerlink" href="#reinforcement-14_final-monte-carlo-tree-search-mcts-for-planning" title="Permanent link">¶</a></h3>
<p>So far we have discussed exploration in the context of learning unknown values or models. Another key idea in the RL toolkit is planning using simulation, particularly via Monte Carlo Tree Search (MCTS). MCTS is a family of simulation-based search algorithms that became famous through their use in game-playing AI (e.g. AlphaGo and AlphaZero). The idea is to build a partial search tree from the current state by simulating many random play-outs (rollouts) and using the results to gradually refine value estimates for states and actions.</p>
<p>One of the most widely used MCTS algorithms is UCT (Upper Confidence Trees), which blends the UCB idea with tree search. In each simulation (from root state until a terminal state or depth limit), UCT traverses the tree by choosing actions that maximize an upper confidence bound: at a state (tree node) <span class="arithmatex">\(s\)</span>, it selects the action <span class="arithmatex">\(a\)</span> that maximizes</p>
<div class="arithmatex">\[
\frac{w_{s,a}}{n_{s,a}} + c \sqrt{\frac{\ln N_s}{n_{s,a}}},
\]</div>
<p>where <span class="arithmatex">\(w_{s,a}\)</span> is the total reward accrued from past simulations taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>, <span class="arithmatex">\(n_{s,a}\)</span> is the number of simulations that took that action, and <span class="arithmatex">\(N_s = \sum_a n_{s,a}\)</span> is the total simulations from state <span class="arithmatex">\(s\)</span>. This formula is essentially the UCB1 formula extended to tree nodes: the first term is exploitation (the empirical mean reward), and the second is an exploration bonus that is higher for seldom-tried actions. By using this rule at each step of simulation (Selection phase), MCTS efficiently explores the game tree, focusing on promising moves while still trying less-visited moves once in a while. After selection, a random Simulation (rollout) is played out to the end, and the outcome is backpropagated to update <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(n\)</span> along the path. Repeating thousands or millions of simulations yields increasingly accurate value estimates for the root state and preferred actions.</p>
<p>MCTS does not learn parameters from data in the traditional sense; rather it is a planning method that can be applied if we have a generative model of the environment (e.g. a simulator or game rules). However, it connects to our theme as another approach to balancing exploration and exploitation via UCB-like algorithms. In practice, MCTS can be combined with learning. Notably, AlphaGo and AlphaZero combined deep neural networks (for state evaluation and policy guidance) with Monte Carlo Tree Search to achieve superhuman performance in Go, chess, and shogi. In those systems, the neural network’s value estimates guide the rollout, and MCTS provides a powerful lookahead search that complements the learned policy. This combination dramatically improves data efficiency – for example, AlphaZero uses MCTS to effectively explore the game space instead of needing an exorbitant amount of self-play games, and the knowledge gained from MCTS is distilled back into the network through training. MCTS exemplifies how models and planning can be leveraged in RL: if a model of the environment is available (or learned), one can simulate experience to aid decision-making without direct real-world trial-and-error for every decision. This is crucial in domains where real experiments are costly or limited.</p>
<p>Computational vs Data Efficiency: It is worth noting that methods like MCTS (and exhaustive exploration algorithms) tend to be computationally intensive – they trade computation for reduced real-world data needs. We often face a trade-off: algorithms that are very data-efficient (using fewer environment interactions) are often computationally expensive, whereas simpler algorithms that learn quickly in computation might require more data. In some domains (like games or simulated environments), we can afford massive computation, effectively converting computation into simulated “data” for learning. In others (like physical systems or online user interactions), data is scarce or expensive, so sample-efficient algorithms (even if computationally heavy) are preferred. This trade-off has been a recurring consideration as we moved from bandits to deep RL.</p>
<h2 id="reinforcement-14_final-exploration-paradigms-optimism-vs-probability-matching">Exploration Paradigms: Optimism vs. Probability Matching<a class="headerlink" href="#reinforcement-14_final-exploration-paradigms-optimism-vs-probability-matching" title="Permanent link">¶</a></h2>
<p>We have seen two major paradigms for addressing the exploration-exploitation challenge:</p>
<ul>
<li>
<p>Optimism in the face of uncertainty: The agent behaves as if the environment is as rewarding as plausibly possible, given the data. This leads to algorithms like UCB, optimistic initial values, exploration bonuses (e.g. MBIE-EB, optimistic Q-learning), and UCT in MCTS. Optimistic methods systematically encourage trying actions that could be best. They often come with strong theoretical guarantees (UCB’s regret bound, PAC-MDP bounds, etc.) because they ensure sufficient exploration of each alternative. Optimism tends to be a more worst-case (frequentist) approach: it doesn’t assume a prior, just relies on confidence intervals that hold with high probability for any reward distribution.</p>
</li>
<li>
<p>Probability matching (Thompson Sampling and Bayesian methods): The agent maintains a belief (probability distribution) about the environment’s parameters and randomizes its actions according to this belief. Effectively, it samples a hypothesis for the true model and then exploits that hypothesis (e.g., play the best action for that sampled model). Over time, the belief is updated with Bayes’ rule as more data comes in, so the sampling naturally shifts toward optimal actions. This approach is more Bayesian in spirit: it assumes a prior distribution and seeks to maximize performance on average with respect to that prior (i.e., good Bayesian regret). Probability matching can be very effective in practice and can incorporate prior knowledge elegantly. The downside is that providing theoretical guarantees in the worst-case sense can be challenging – the guarantees are often Bayesian (in expectation over the prior) rather than uniform for all environments. Recent theoretical work, however, has shown that even without a perfect prior, Thompson Sampling performs near-optimally in many settings, and there are ways to bound its regret. In terms of implementation complexity, Thompson Sampling may require the ability to sample from posterior distributions, which can be non-trivial in large-scale problems (though approximate methods exist). Optimistic methods, on the other hand, require confidence bound calculations, which for simple tabular cases are straightforward, but for complex function approximation can be difficult (leading to research on exploration bonuses using predictive models or uncertainty estimates).</p>
</li>
</ul>
<p>In summary, optimism vs. probability matching represents two different philosophies for exploration. Optimistic algorithms behave more deterministically (always picking the current optimistic-best option), ensuring systematic coverage of possibilities, while Thompson-style algorithms inject randomized exploration in proportion to uncertainty. Interestingly, human decision-making experiments suggest people may combine elements of both strategies – not purely optimistic nor purely Thompson. Both paradigms have influenced modern RL: for example, exploration bonuses (optimism) are commonly used in deep RL (e.g. with bonus rewards from prediction error or curiosity), and Bayesian RL approaches (like posterior sampling for MDPs) are gaining traction for problems where a reasonable prior is available or an ensemble can approximate uncertainty.</p>
<h3 id="reinforcement-14_final-theoretical-foundations-regret-pac-and-bayesian-optimality">Theoretical Foundations: Regret, PAC, and Bayesian Optimality<a class="headerlink" href="#reinforcement-14_final-theoretical-foundations-regret-pac-and-bayesian-optimality" title="Permanent link">¶</a></h3>
<p>Understanding how well an RL algorithm performs relative to an ideal standard is a major theme in RL theory. We revisited two main frameworks for this: regret analysis and PAC (sample complexity) analysis, along with the Bayesian viewpoint.</p>
<ul>
<li>Regret: Regret measures the opportunity loss from not acting optimally at each time step. Formally, in a bandit with optimal expected reward <span class="arithmatex">\(\mu^*\)</span>, the regret after <span class="arithmatex">\(T\)</span> plays is</li>
</ul>
<div class="arithmatex">\[
R(T) = T\mu^* - \sum_{t=1}^T r_t,
\]</div>
<p>i.e. the difference between the reward that would be obtained by always executing the optimal arm and the reward actually obtained. Sublinear regret (e.g. <span class="arithmatex">\(R(T) = o(T)\)</span>) implies the algorithm eventually learns the optimal policy (average regret <span class="arithmatex">\(\to 0\)</span> as <span class="arithmatex">\(T\)</span> grows). We saw that <span class="arithmatex">\(\varepsilon\)</span>-greedy exploration can lead to linear regret in the worst case (always pulling some suboptimal arm a constant fraction of the time yields <span class="arithmatex">\(R(T) \sim \Omega(T)\)</span>). In contrast, UCB1 achieves <span class="arithmatex">\(R(T) = O(\ln T)\)</span>, which is asymptotically optimal up to constant factors (matching the Lai &amp; Robbins lower bound for bandits that <span class="arithmatex">\(R(T) \ge \Omega(\ln T)\)</span> for any algorithm). Regret analysis can be extended to MDPs (though it becomes more complex). For example, algorithms like UCRL2 (an optimistic tabular RL algorithm) have regret bounds on the order of <span class="arithmatex">\(\tilde{O}(\sqrt{T})\)</span> in an MDP (reflecting the harder challenge of states) under certain assumptions. Regret is a worst-case, online metric – it asks how well we do even against an adversarially chosen problem (or in the unknown actual environment) without assumptions of a prior, focusing on long-term performance.</p>
<ul>
<li>
<p>PAC (Probably Approximately Correct) guarantees: PAC analysis focuses on sample complexity: how many time steps or episodes are required for the algorithm to achieve near-optimal performance with high probability. A PAC guarantee typically states: for any <span class="arithmatex">\(\varepsilon, \delta\)</span>, there exists <span class="arithmatex">\(N(\varepsilon,\delta)\)</span> (poly in relevant parameters) such that with probability at least <span class="arithmatex">\(1-\delta\)</span>, the algorithm’s policy is <span class="arithmatex">\(\varepsilon\)</span>-optimal after <span class="arithmatex">\(N\)</span> steps (or, equivalently, all but at most <span class="arithmatex">\(N\)</span> of the steps are <span class="arithmatex">\(\varepsilon\)</span>-suboptimal). This is a finite-sample guarantee, giving confidence that the learning will not take too long. We discussed that algorithms like MBIE-EB and R-MAX are PAC-MDP: for a given accuracy <span class="arithmatex">\(\varepsilon\)</span> and confidence <span class="arithmatex">\(1-\delta\)</span>, their sample complexity (number of suboptimal actions) is bounded by a polynomial in <span class="arithmatex">\(|S|, |A|, 1/\varepsilon, 1/\delta, 1/(1-\gamma)\)</span>, etc. PAC analysis is particularly useful when we care about guarantees in a learning phase before near-optimal performance is reached (important in safety-critical or costly domains where we need to know learning will be efficient with high probability). While regret goes to zero only asymptotically, PAC gives an explicit bound on how long it takes to be good. Often, achieving PAC guarantees in large-scale problems requires simplifying assumptions or limited function approximation classes, as general function approximation PAC results are quite difficult.</p>
</li>
<li>
<p>Bayesian approaches and Bayes-optimality: In a Bayesian formulation, we assume a prior distribution over environments (bandit reward distributions or MDP dynamics). We can then consider the Bayes-optimal policy, which is the policy that maximizes expected cumulative reward with respect to this prior. This leads to the concept of Bayesian regret – the expected regret under the prior. A Bayes-optimal algorithm minimizes Bayesian regret and, by definition, will outperform any other algorithm on average if the prior is correct. One famous result in this vein is the Gittins Index for multi-armed bandits, which gives an optimal solution when each arm has independent known priors (casting the problem as a Markov process and solving it via dynamic programming). However, computing Bayes-optimal solutions for general RL (especially with state) is usually intractable – it involves solving a POMDP (partially observable MDP) where the hidden state is the true environment parameters. Thompson Sampling can be interpreted as an approximation to the Bayes-optimal policy that is much easier to implement. It has low Bayesian regret and in some cases can be shown to be asymptotically Bayes-optimal. The Bayesian view is powerful because it allows incorporation of prior knowledge and gives a normative standard (what should we do if we know what we don’t know, in distribution). But its limitation is the computational difficulty and the dependence on having a reasonable prior. In practice, algorithms inspired by Bayesian ideas (like ensemble sampling or posterior sampling for reinforcement learning) try to capture some of the benefit without solving the full Bayes-optimal policy.</p>
</li>
</ul>
<p>These theoretical frameworks complement each other. Regret and PAC analyses give worst-case performance assurances (no matter what the true environment is, within assumptions) and often inspire optimistic algorithms. Bayesian analysis aims for average-case optimality given prior knowledge and often inspires probability matching or adaptive algorithms. As an RL practitioner or researcher, understanding these foundations helps in choosing and designing algorithms appropriate for the problem at hand – whether one prioritizes guaranteed efficiency, practical performance with prior info, or a mix of both.</p>
<h2 id="reinforcement-14_final-from-theory-to-practice-real-world-applications-and-achievements">From Theory to Practice: Real-World Applications and Achievements<a class="headerlink" href="#reinforcement-14_final-from-theory-to-practice-real-world-applications-and-achievements" title="Permanent link">¶</a></h2>
<p>One of the most exciting aspects of the recent decade in RL is seeing theoretical ideas translate into real-world (or at least real-problem) successes. In this section, we connect some of the classic algorithms and concepts to notable applications:</p>
<ul>
<li>
<p>Game Mastery and Planning – AlphaGo, AlphaZero, AlphaTensor: Starting with games, AlphaGo famously combined deep neural networks with MCTS (using UCT) and was trained with reinforcement learning to defeat human Go champions. Its successor AlphaZero took this further by learning from scratch (self-play) for multiple games, using Monte Carlo Tree Search guided by a learned value/policy network. The blend of planning (MCTS) and learning (deep RL) that AlphaZero employs is a direct embodiment of concepts we covered: it uses optimistic simulations (MCTS uses UCB in the tree) and improves data efficiency by leveraging a model (the game simulator) for exploration. The success of AlphaZero demonstrates the power of combining model-based search with model-free function approximation. Recently, these ideas have even extended to domains beyond traditional games. AlphaTensor (DeepMind, 2022) is a system that treated the discovery of new matrix multiplication algorithms as a single-player game, and it applied a variant of AlphaZero’s RL approach to find faster algorithms for matrix multiply. The AlphaTensor agent was trained via self-play reinforcement learning to manipulate tensor representations of matrix multiplication and achieved a breakthrough: it discovered matrix multiplication algorithms that surpass the decades-old human benchmarks in efficiency. This is a striking example of RL not just playing games but discovering algorithms – essentially using reward signals to guide a search through the space of mathematical formulas. It showcases how MCTS (for planning) and deep RL can work together on combinatorial optimization problems: the agent expands a search tree of partial solutions, guided by value networks and an exploration policy, very much like how it would approach a board game. AlphaTensor’s success underscores the generality of RL methods and how ideas like optimism (self-play explores new moves) and guided search can yield new discoveries.</p>
</li>
<li>
<p>Natural Language and Human Feedback – ChatGPT: A more recent and widely impactful application of reinforcement learning is in natural language processing – specifically, training large language models to better align with human intentions. ChatGPT (OpenAI, 2022) is a prime example, where RL was used to fine-tune a pretrained language model using human feedback. The technique, known as Reinforcement Learning from Human Feedback (RLHF), involves first collecting human preference data on model outputs and then training a reward model that predicts human preference. The language model (policy) is then optimized (via a policy gradient method like PPO) to maximize the reward model’s score, i.e. to produce answers humans would rate highly. This is essentially an RL loop on top of the language model, treating the task of generating helpful, correct responses as an MDP (or episodic decision problem) and using the learned reward function as the reward signal. The result, ChatGPT, is notably more aligned with user expectations than its predecessor models. In our context, ChatGPT’s training illustrates several RL ideas in action: offline data (pretraining on text) combined with online RL fine-tuning, and the critical role of a well-shaped reward function for alignment. It also highlights exploration in a different sense – exploring the space of possible answers to find those that yield high reward according to human feedback. The success of ChatGPT demonstrates that RL is not limited to games or robotics; it can be scaled to very high-dimensional action spaces (like generating entire paragraphs of text) when guided by human-informed rewards. From a theoretical lens, one can view RLHF as optimizing an objective that marries the model’s knowledge (from supervised training) with a policy optimization under a learned reward. While classical exploration algorithms (UCB, Thompson) are not directly apparent in ChatGPT’s training (since the “exploration” comes from the model generating varied outputs and the policy optimization process), the high-level principle remains: use feedback signals to iteratively refine behavior.</p>
</li>
<li>
<p>Scientific and Industrial Applications: Beyond these headline examples, RL is increasingly applied in scientific and industrial domains. The course of our study touched on a few, such as:</p>
</li>
</ul>
<p>Controlling nuclear fusion plasmas: Researchers applied deep RL to control the magnetic coils in a tokamak reactor to sustain plasma configurations. This is a complex continuous control problem with safety constraints, where function approximation and careful exploration (largely in simulations before real experiments) were key.</p>
<p>Optimizing public health interventions: An RL approach was used to design efficient COVID-19 border testing policies. Framing the problem as a sequential decision task (who to test and when) and using RL to maximize some health outcome or efficiency metric allowed automating policy design that adapted to data.</p>
<p>Robotics and Autonomous Systems: Many advances in robotics have come from RL algorithms that allow robots to learn locomotion, manipulation, or flight. Often these use deep RL and sometimes simulation-to-reality transfer. The exploration techniques we learned (like curiosity-driven bonuses or domain randomization) help address the challenge of learning in these complex environments.</p>
<p>Recommender Systems and Online Decision Making: Multi-armed bandit algorithms (including Thompson Sampling and UCB) are widely used in industry for things like A/B testing, website optimization, and personalized recommendations. For example, serving personalized content can be seen as a bandit problem where each content choice is an arm and click-through or engagement is the reward. Companies employ bandit algorithms to balance exploration of new content with exploitation of known user preferences, often in a context of contextual bandits (where the state or context is user features). The theoretical guarantees of bandit algorithms give confidence in their performance, and their simplicity makes them practical at scale.</p>
<p>In all these cases, the fundamental concepts from this course appear and validate themselves: whether it’s optimism guiding AlphaZero’s search, or Thompson Sampling driving an online recommendation strategy, or policy gradients tuning ChatGPT using human rewards, the same core ideas of reinforcement learning apply. Modern applications often hybridize approaches – for instance, using model-based simulations (AlphaTensor, AlphaZero), or combining learning from offline data with online exploration (ChatGPT’s RLHF, or robotics). This underscores the importance of mastering the basics: understanding value functions, policy optimization, exploration mechanisms, and theoretical limits has direct relevance even as we push RL into new territory.</p>
<h2 id="reinforcement-14_final-final-takeaways">Final Takeaways<a class="headerlink" href="#reinforcement-14_final-final-takeaways" title="Permanent link">¶</a></h2>
<p>In closing, we synthesize a few key insights and lessons from the full RL journey:</p>
<ul>
<li>
<p>Reinforcement Learning Unifies Many Themes: We saw that RL problems range from simple bandits to complex high-dimensional control, but they share the need for sequential decision making under uncertainty. Concepts like state, action, reward, policy, value function, model form a common language to describe problems as diverse as games, robotics, and recommendation systems. Recognizing an appropriate RL formulation (MDP, bandit, etc.) for a given real-world problem is the first step to applying these methods.</p>
</li>
<li>
<p>Exploration vs. Exploitation is Fundamental: The trade-off between trying new actions and leveraging known good actions underpins all of RL. We examined different strategies:</p>
<ul>
<li>
<p>Heuristics like <span class="arithmatex">\(\epsilon\)</span>-greedy (simple but can be suboptimal),</p>
</li>
<li>
<p>Optimistic algorithms (UCB, optimism in value iteration,  exploration bonuses) which ensure systematic exploration using confidence bounds,</p>
</li>
<li>
<p>Probabilistic approaches (Thompson Sampling, randomized value functions) which inject randomness based on uncertainty.</p>
</li>
</ul>
</li>
</ul>
<p>Each approach has its advantages – optimism often yields strong guarantees and is conceptually straightforward, while Thompson Sampling often gives excellent practical performance and naturally incorporates prior knowledge. In large-scale problems, clever exploration bonuses (intrinsic rewards for novelty) and approximate uncertainty estimates are key to maintaining exploration. The central lesson is that successful RL requires deliberate exploration strategies; naive exploration can lead to poor sample efficiency or getting stuck in suboptimal behaviors.</p>
<ul>
<li>
<p>Theoretical Foundations Guide Algorithm Design: Concepts like regret and PAC provide ways to formally measure learning efficiency. They not only help us compare algorithms (e.g. which has lower regret or better sample complexity) but have directly inspired algorithmic techniques (like UCB from the idea of minimizing regret, or PAC-inspired algorithms like MBIE-EB and R-MAX designed to guarantee learning within polynomial time). Meanwhile, the Bayesian perspective offers a gold-standard for optimal decision-making given prior info, even if it’s often computationally intractable – it guides us toward algorithms that perform well on average and informs approaches like posterior sampling. As RL practitioners, we should remember:</p>
<ul>
<li>
<p>Regret minimization focuses on not wasting too many opportunities – it’s about learning as fast as possible in an online sense.</p>
</li>
<li>
<p>PAC guarantees focus on bounding the learning time with high confidence – giving safety that an algorithm won’t do too poorly for too long.</p>
</li>
<li>
<p>Bayesian optimality focuses on using prior knowledge efficiently – it’s about doing the best given what you (probabilistically) know.</p>
</li>
</ul>
</li>
</ul>
<p>All three perspectives are important; balancing them or choosing the right one depends on the application (e.g., in a one-off A/B test you might care about regret, in a lifelong robot learning you care about sample efficiency with high probability, and in a personalized system you might incorporate Bayesian priors about users).</p>
<ul>
<li>
<p>Function Approximation and Deep RL Open New Possibilities (and Challenges): The leap from tabular or small-scale problems to real-world complexity required using function approximation (especially deep neural networks). This enabled RL to handle images, continuous states, and enormous state spaces – as seen in Atari games, Go, and continuous control benchmarks. The success of deep RL (DQN, policy gradient methods, etc.) comes from blending RL algorithms with powerful representation learning. However, it also brought challenges like stability of training, overfitting, exploration in high dimensions, and reproducibility issues. Key techniques to mitigate these include experience replay, target networks, regularization, large-scale parallel training, and reward shaping. The takeaway is that theoretical convergence guarantees often break down with function approximation, so a lot of practical know-how and experimentation is needed. Yet, the core ideas (Bellman equations, policy improvement, etc.) still apply – just approximate. The field is actively developing better theories for RL with function approximation (e.g. understanding generalization, error propagation) and techniques for more reliable training.</p>
</li>
<li>
<p>Real-World Impact and Ongoing Research: Reinforcement learning has graduated from textbook problems to impacting real-world systems. Its principles have powered superhuman game AIs, improved scientific research (e.g. algorithm discovery, experiment design), enhanced language models, and optimized business decisions. At the same time, truly robust and general-purpose RL is still an open challenge. Issues of stability, efficiency, and safety remain – for instance:</p>
<ul>
<li>Developing algorithms that work out-of-the-box with minimal tuning for any problem (robustness).</li>
<li>Improving data efficiency so that RL can be applied with limited real-world interactions (e.g., via model-based methods, better exploration, or transfer learning).</li>
<li>Integrating learning and planning seamlessly, and handling settings that mix offline data with online exploration.</li>
<li>Expanding the RL framework to account for multiple objectives, collaboration or competition (multi-agent RL), and richer feedback modalities beyond scalar rewards.</li>
</ul>
</li>
</ul>
<p>These are active research directions. The skills and concepts acquired – from understanding theoretical bounds to implementing algorithms – equip us to tackle these frontiers.</p>
<p>In summary, the journey from multi-armed bandits to deep reinforcement learning has taught us not only a catalogue of algorithms, but a way of thinking about sequential decision problems. We learned how to measure learning efficiency and why exploration is hard yet critical. We saw simple ideas like optimism and probability matching scale up to complex systems that play Go or converse in English. As you move forward from this textbook, remember the foundational principles: reward is your guide, value estimation is your tool, policy is your output, and exploration is your catalyst. With these in mind, you are well-prepared to both apply RL to challenging problems and to contribute to the advancing frontier of reinforcement learning research.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-4' heading-number='4'>
                        <h1>Deep Learning<a class='headerlink' href='#section-4' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="deeplearning-1_mlp" heading-number="4.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="an-introduction-to-neural-networks">An Introduction to Neural Networks<a class="headerlink" href="#deeplearning-1_mlp-an-introduction-to-neural-networks" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-1_mlp-1-neural-networks-as-computation-graphs">1. Neural Networks as Computation Graphs<a class="headerlink" href="#deeplearning-1_mlp-1-neural-networks-as-computation-graphs" title="Permanent link">¶</a></h2>
<p>Modern neural networks are best understood as differentiable computation graphs. 
They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p>
<p>Each node in this graph corresponds to a function:</p>
<div class="arithmatex">\[z_i = f_i(x_1, \dots, x_k)\]</div>
<p>and the entire network defines a composite function:</p>
<div class="arithmatex">\[f_\theta(x) = f_L \circ f_{L-1} \circ \dots \circ f_1(x)\]</div>
<p>where <span class="arithmatex">\(\theta = \{W_i, b_i\}\)</span> denotes all learnable parameters.</p>
<h3 id="deeplearning-1_mlp-formal-structure">Formal Structure<a class="headerlink" href="#deeplearning-1_mlp-formal-structure" title="Permanent link">¶</a></h3>
<p>For a Multilayer Perceptron (MLP):</p>
<div class="arithmatex">\[h_0 = x, \quad h_i = \sigma(W_i h_{i-1} + b_i), \quad i=1,\dots,L-1, \quad \hat{y} = W_L h_{L-1} + b_L\]</div>
<p>with: <span class="arithmatex">\(W_i \in \mathbb{R}^{d_i \times d_{i-1}}, \quad b_i \in \mathbb{R}^{d_i}\)</span></p>
<p>Each layer is a small differentiable function. When we connect them, we form a composite map — the fundamental abstraction underlying <em>autodiff</em>, <em>backprop</em>, and <em>learning</em>.</p>
<p>Key property: Because every node in the graph is differentiable, the entire function <span class="arithmatex">\(f_\theta(x)\)</span> is differentiable with respect to both input <span class="arithmatex">\(x\)</span> and parameters <span class="arithmatex">\(\theta\)</span>.</p>
<p>Graphically, the network is a directed acyclic graph (DAG):</p>
<ul>
<li>Edges: carry tensor values.</li>
<li>Nodes: represent differentiable functions.</li>
<li>Forward pass: evaluates node outputs.</li>
<li>Backward pass: propagates sensitivities (gradients) backward.</li>
</ul>
<blockquote>
<p>This graph abstraction unifies all architectures — CNNs, RNNs, Transformers, Diffusion Models — as differentiable computation graphs.</p>
</blockquote>
<h2 id="deeplearning-1_mlp-2-gradients-jacobians-and-differentiation">2. Gradients, Jacobians, and Differentiation<a class="headerlink" href="#deeplearning-1_mlp-2-gradients-jacobians-and-differentiation" title="Permanent link">¶</a></h2>
<p>For any function <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian matrix <span class="arithmatex">\(J_f(x)\)</span> encodes local derivatives:</p>
<div class="arithmatex">\[[J_f(x)]_{ij} = \frac{\partial f_i}{\partial x_j}\]</div>
<p>In neural networks, we often deal with a scalar loss function:</p>
<div class="arithmatex">\[L(\theta) = \ell(f_\theta(x), y)\]</div>
<p>and want: </p>
<div class="arithmatex">\[\nabla_\theta L = \frac{\partial L}{\partial \theta}\]</div>
<p>However, computing full Jacobians is computationally infeasible — for a network with millions of parameters, explicit Jacobians would have trillions of entries.<br>
Instead, automatic differentiation (autodiff) computes vector–Jacobian products efficiently.</p>
<p>For scalar loss <span class="arithmatex">\(L\)</span>: <span class="arithmatex">\(\nabla_\theta L = J_{f_\theta}(x)^T \nabla_{f_\theta} L\)</span></p>
<p>where <span class="arithmatex">\(J_{f_\theta}(x)\)</span> is the Jacobian of the output w.r.t. parameters.</p>
<p>This operation can be done efficiently in reverse-mode autodiff — the heart of backpropagation.</p>
<hr>
<h2 id="deeplearning-1_mlp-3-forward-and-backward-passes">3. Forward and Backward Passes<a class="headerlink" href="#deeplearning-1_mlp-3-forward-and-backward-passes" title="Permanent link">¶</a></h2>
<h4 id="deeplearning-1_mlp-forward-pass">Forward Pass<a class="headerlink" href="#deeplearning-1_mlp-forward-pass" title="Permanent link">¶</a></h4>
<p>Given input <span class="arithmatex">\(x\)</span> and parameters <span class="arithmatex">\(\theta\)</span>:</p>
<ol>
<li>Compute layer outputs sequentially: <span class="arithmatex">\(h_i = \sigma(W_i h_{i-1} + b_i)\)</span></li>
<li>Compute loss <span class="arithmatex">\(L = \ell(f_\theta(x), y)\)</span></li>
<li>Store intermediate activations <span class="arithmatex">\(h_i\)</span> for reuse during backpropagation.</li>
</ol>
<p>This pass evaluates the function <span class="arithmatex">\(L(\theta)\)</span>.</p>
<h4 id="deeplearning-1_mlp-backward-pass">Backward Pass<a class="headerlink" href="#deeplearning-1_mlp-backward-pass" title="Permanent link">¶</a></h4>
<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p>
<p><span class="arithmatex">\(\frac{\partial L}{\partial \theta_i} = 
\frac{\partial L}{\partial h_L}
\frac{\partial h_L}{\partial h_{L-1}}
\dots
\frac{\partial h_{i+1}}{\partial \theta_i}\)</span></p>
<p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p>
<p>Reverse-mode autodiff (backprop) algorithm:</p>
<ol>
<li>Initialize <span class="arithmatex">\(\bar{h}_L = \frac{\partial L}{\partial h_L} = 1\)</span>.</li>
<li>For each layer <span class="arithmatex">\(l = L, L-1, \dots, 1\)</span>:</li>
<li>Compute local derivative <span class="arithmatex">\(\frac{\partial h_l}{\partial h_{l-1}}\)</span></li>
<li>Accumulate gradient:<br>
<span class="arithmatex">\(\bar{h}_{l-1} = \bar{h}_l \frac{\partial h_l}{\partial h_{l-1}}\)</span></li>
<li>Compute parameter gradients:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial W_l} = \bar{h}_l (h_{l-1})^T\)</span></li>
<li>Return all <span class="arithmatex">\(\nabla_\theta L\)</span>.</li>
</ol>
<p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>
<h2 id="deeplearning-1_mlp-4-chain-rule-backpropagation-and-automatic-differentiation">4. Chain Rule, Backpropagation, and Automatic Differentiation<a class="headerlink" href="#deeplearning-1_mlp-4-chain-rule-backpropagation-and-automatic-differentiation" title="Permanent link">¶</a></h2>
<p>The chain rule underpins all gradient computation.<br>
For scalar functions:</p>
<p><span class="arithmatex">\(\frac{dL}{dx} = \frac{dL}{dz} \frac{dz}{dx}\)</span></p>
<p>and recursively for multivariate functions:</p>
<p><span class="arithmatex">\(\nabla_x L = J_{z}(x)^T \nabla_z L\)</span></p>
<p>Autodiff implements this automatically, performing either:</p>
<ul>
<li>Forward-mode AD: propagates derivatives forward, efficient when #inputs ≪ #outputs.</li>
<li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs ≪ #inputs (our case).</li>
</ul>
<p>Reverse-mode AD ≡ backpropagation.</p>
<p>Computational Complexity:
- Cost ≈ 2× forward pass (one forward, one backward).
- Memory ≈ size of stored activations.</p>
<p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p>
<p><span class="arithmatex">\(\min_\theta L(\theta)\)</span></p>
<p>where <span class="arithmatex">\(L\)</span> is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms.
s</p>
<h2 id="deeplearning-1_mlp-5-from-gradients-to-optimization">5. From Gradients to Optimization<a class="headerlink" href="#deeplearning-1_mlp-5-from-gradients-to-optimization" title="Permanent link">¶</a></h2>
<p>The Learning Problem - Training a neural network means solving:</p>
<p><span class="arithmatex">\(\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}} [\,\ell(f_\theta(x), y)\,]\)</span></p>
<p>Since the true data distribution <span class="arithmatex">\(\mathcal{D}\)</span> is unknown, we use empirical risk minimization (ERM):</p>
<p><span class="arithmatex">\(\min_\theta \frac{1}{N} \sum_{i=1}^N \ell(f_\theta(x_i), y_i)\)</span></p>
<p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods — powered by backpropagation — reliably find good solutions.</p>
<h3 id="deeplearning-1_mlp-first-order-optimization-algorithms">First-Order Optimization Algorithms<a class="headerlink" href="#deeplearning-1_mlp-first-order-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>All modern deep learning optimization relies on gradients:</p>
<p><span class="arithmatex">\(\nabla_\theta L = \frac{\partial L}{\partial \theta}\)</span></p>
<p>The basic rule: update parameters in the direction of <em>negative gradient</em>:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \nabla_\theta L_t\)</span></p>
<p>where <span class="arithmatex">\(\eta\)</span> is the learning rate.</p>
<h4 id="deeplearning-1_mlp-stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)<a class="headerlink" href="#deeplearning-1_mlp-stochastic-gradient-descent-sgd" title="Permanent link">¶</a></h4>
<p>We use mini-batches instead of full data:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \nabla_\theta \frac{1}{|B_t|}\sum_{i \in B_t} \ell(f_\theta(x_i), y_i)\)</span></p>
<ul>
<li>Cheap per-step computation.</li>
<li>Introduces <em>gradient noise</em>, which helps escape shallow minima and saddle points.</li>
</ul>
<h4 id="deeplearning-1_mlp-momentum">Momentum<a class="headerlink" href="#deeplearning-1_mlp-momentum" title="Permanent link">¶</a></h4>
<p>Accelerates learning by accumulating a velocity vector:</p>
<p><span class="arithmatex">\(v_{t+1} = \mu v_t - \eta \nabla_\theta L_t, \quad \theta_{t+1} = \theta_t + v_{t+1}\)</span></p>
<p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>
<h4 id="deeplearning-1_mlp-adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)<a class="headerlink" href="#deeplearning-1_mlp-adam-adaptive-moment-estimation" title="Permanent link">¶</a></h4>
<p>Maintains exponentially weighted averages of gradients and squared gradients:</p>
<p><span class="arithmatex">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1)\nabla_\theta L_t\)</span></p>
<p><span class="arithmatex">\(v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta L_t)^2\)</span></p>
<p>Bias-corrected updates:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\)</span></p>
<p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>
<h4 id="deeplearning-1_mlp-second-order-and-curvature-aware-methods">Second-Order and Curvature-Aware Methods<a class="headerlink" href="#deeplearning-1_mlp-second-order-and-curvature-aware-methods" title="Permanent link">¶</a></h4>
<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p>
<p><span class="arithmatex">\(H = \frac{\partial^2 L}{\partial \theta^2}\)</span></p>
<p>Newton’s update:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - H^{-1}\nabla_\theta L\)</span></p>
<p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets.<br>
Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>
<h3 id="deeplearning-1_mlp-optimization-landscape-and-gradient-flow">Optimization Landscape and Gradient Flow<a class="headerlink" href="#deeplearning-1_mlp-optimization-landscape-and-gradient-flow" title="Permanent link">¶</a></h3>
<p>Although neural network loss surfaces are highly nonconvex, they possess <em>favorable geometry</em>:</p>
<ul>
<li>Most critical points are saddle points, not local minima.</li>
<li>Wide, flat minima generalize better (implicit regularization of SGD).</li>
<li>Gradient noise helps explore valleys in high-dimensional space.</li>
</ul>
<p>Gradient flow (continuous limit of SGD):</p>
<p><span class="arithmatex">\(\frac{d\theta(t)}{dt} = - \nabla_\theta L(\theta(t))\)</span></p>
<p>describes a trajectory in parameter space governed by the vector field of gradients.</p>
<p>The optimization algorithm defines the <em>dynamics</em> of this flow (e.g., momentum adds inertia).</p>
<h2 id="deeplearning-1_mlp-6-what-mlps-cant-do">6. What MLPs Can’t Do?<a class="headerlink" href="#deeplearning-1_mlp-6-what-mlps-cant-do" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-1_mlp-a-multiplicative-interactions">(a) Multiplicative Interactions<a class="headerlink" href="#deeplearning-1_mlp-a-multiplicative-interactions" title="Permanent link">¶</a></h3>
<p>MLPs compute sums of weighted activations — inherently <em>additive</em> operations:</p>
<p><span class="arithmatex">\(h = \sigma(Wx + b)\)</span></p>
<p>They cannot naturally represent multiplicative relationships (like <span class="arithmatex">\(x_1 x_2\)</span>) unless approximated via nonlinear stacking, which is inefficient.</p>
<p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>
<h3 id="deeplearning-1_mlp-b-attention-and-dynamic-routing">(b) Attention and Dynamic Routing<a class="headerlink" href="#deeplearning-1_mlp-b-attention-and-dynamic-routing" title="Permanent link">¶</a></h3>
<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p>
<p><span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V\)</span></p>
<p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>
<h3 id="deeplearning-1_mlp-c-metric-learning-and-inductive-bias">(c) Metric Learning and Inductive Bias<a class="headerlink" href="#deeplearning-1_mlp-c-metric-learning-and-inductive-bias" title="Permanent link">¶</a></h3>
<p>MLPs lack structural priors about similarity or geometry.<br>
Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p>
<p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient — the landscape becomes smoother and gradients more informative.</p>
<h2 id="deeplearning-1_mlp-7-beyond-backprop-curvature-generalization-and-geometry">7. Beyond Backprop: Curvature, Generalization, and Geometry<a class="headerlink" href="#deeplearning-1_mlp-7-beyond-backprop-curvature-generalization-and-geometry" title="Permanent link">¶</a></h2>
<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>
<h3 id="deeplearning-1_mlp-natural-gradient">Natural Gradient<a class="headerlink" href="#deeplearning-1_mlp-natural-gradient" title="Permanent link">¶</a></h3>
<p>Instead of minimizing loss directly in parameter space, we minimize it in <em>function space</em>:</p>
<p><span class="arithmatex">\(\Delta \theta = - \eta F^{-1} \nabla_\theta L\)</span></p>
<p>where <span class="arithmatex">\(F\)</span> is the Fisher information matrix:</p>
<p><span class="arithmatex">\(F = \mathbb{E}\left[\nabla_\theta \log p_\theta(x) \nabla_\theta \log p_\theta(x)^T\right]\)</span></p>
<p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>
<h3 id="deeplearning-1_mlp-implicit-bias-of-gradient-descent">Implicit Bias of Gradient Descent<a class="headerlink" href="#deeplearning-1_mlp-implicit-bias-of-gradient-descent" title="Permanent link">¶</a></h3>
<p>Even in overparameterized models, gradient descent tends to find <em>low-norm</em> or <em>flat</em> minima that generalize better — a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>
<h3 id="deeplearning-1_mlp-optimization-as-inference">Optimization as Inference<a class="headerlink" href="#deeplearning-1_mlp-optimization-as-inference" title="Permanent link">¶</a></h3>
<p>Many modern perspectives view training as approximate inference:</p>
<p><span class="arithmatex">\(p(\theta | D) \propto e^{-L(\theta)/T}\)</span></p>
<p>Gradient descent samples from this energy landscape as <span class="arithmatex">\(T \to 0\)</span>; stochastic variants like SGD approximate Bayesian inference under certain limits.</p></body></html></section><section class="print-page" id="deeplearning-2_convnets" heading-number="4.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-convolutional-neural-networks-cnns">Chapter 2: Convolutional Neural Networks (CNNs)<a class="headerlink" href="#deeplearning-2_convnets-chapter-2-convolutional-neural-networks-cnns" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-2_convnets-1-core-principles-locality-and-translation-invariance">1. Core Principles: Locality and Translation Invariance<a class="headerlink" href="#deeplearning-2_convnets-1-core-principles-locality-and-translation-invariance" title="Permanent link">¶</a></h2>
<p>Before understanding convolutional networks, it’s crucial to grasp why they exist — the structural priors they impose on data.</p>
<h3 id="deeplearning-2_convnets-11-locality">1.1 Locality<a class="headerlink" href="#deeplearning-2_convnets-11-locality" title="Permanent link">¶</a></h3>
<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p>
<p>For example:</p>
<ul>
<li>Adjacent pixels in an image often belong to the same object or texture.</li>
<li>Neighboring audio samples belong to the same phoneme.</li>
<li>Nearby words in a sentence influence each other’s meaning.</li>
</ul>
<p>MLPs treat every input dimension as <em>independent</em>, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, <em>local region</em> of the input, called its receptive field.</p>
<p>Formally, for an input <span class="arithmatex">\(x \in \mathbb{R}^{H \times W}\)</span>, a neuron at position <span class="arithmatex">\((i,j)\)</span> in a CNN depends only on values in a small window <span class="arithmatex">\(\Omega(i,j)\)</span>:
<script type="math/tex; mode=display">
h_{i,j} = \sigma\!\left(\sum_{(m,n)\in \Omega(i,j)} W_{m,n} \, x_{i+m, j+n} + b\right)
</script>
This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>
<hr>
<h3 id="deeplearning-2_convnets-12-translation-invariance">1.2 Translation Invariance<a class="headerlink" href="#deeplearning-2_convnets-12-translation-invariance" title="Permanent link">¶</a></h3>
<p>Natural patterns are repeatable across locations — the same feature (e.g., an edge, a cat’s ear) can appear <em>anywhere</em> in the image.</p>
<p>An MLP would need to learn a separate detector for each position.<br>
CNNs overcome this through weight sharing: the same filter <span class="arithmatex">\(W\)</span> is applied across all spatial positions.</p>
<p>Mathematically:
<script type="math/tex; mode=display">
(I * W)(i,j) = \sum_{m,n} I(i+m,j+n)\, W(m,n)
</script>
</p>
<p>This operation — convolution — ensures translation equivariance:
<script type="math/tex; mode=display">
f(T_\Delta I) = T_\Delta f(I)
</script>
meaning if the input shifts by <span class="arithmatex">\(\Delta\)</span>, the output shifts by the same amount.<br>
After pooling, this becomes translation invariance, i.e. the output doesn’t change under small shifts.</p>
<p>These two properties — locality and translation invariance — are the foundation of convolutional architectures.</p>
<h2 id="deeplearning-2_convnets-2-motivation-why-convolutions">2. Motivation: Why Convolutions?<a class="headerlink" href="#deeplearning-2_convnets-2-motivation-why-convolutions" title="Permanent link">¶</a></h2>
<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos.<br>
An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p>
<p>Example:<br>
For a 256×256 RGB image (≈200K input features), even one hidden layer with 1,000 neurons requires:
<span class="arithmatex">\(<span class="arithmatex">\((256 \times 256 \times 3) \times 1000 = 196\,\text{million weights}.\)</span>\)</span></p>
<p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p>
<p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>
<h2 id="deeplearning-2_convnets-3-the-convolution-operation">3. The Convolution Operation<a class="headerlink" href="#deeplearning-2_convnets-3-the-convolution-operation" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-2_convnets-31-discrete-convolution">3.1 Discrete Convolution<a class="headerlink" href="#deeplearning-2_convnets-31-discrete-convolution" title="Permanent link">¶</a></h3>
<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p>
<p>For 2D inputs (e.g. images):</p>
<div class="arithmatex">\[
S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i+m, j+n) K(m,n)
\]</div>
<ul>
<li><span class="arithmatex">\(I\)</span> — input (image)</li>
<li><span class="arithmatex">\(K\)</span> — kernel (filter)</li>
<li><span class="arithmatex">\(S\)</span> — output feature map</li>
</ul>
<p>Each filter detects a specific local pattern (edges, corners, textures).</p>
<h3 id="deeplearning-2_convnets-32-convolution-in-neural-networks">3.2 Convolution in Neural Networks<a class="headerlink" href="#deeplearning-2_convnets-32-convolution-in-neural-networks" title="Permanent link">¶</a></h3>
<p>In CNNs, the convolution becomes a learnable operation:</p>
<div class="arithmatex">\[
h_{i,j,k} = \sigma\left( \sum_{c=1}^{C_\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \right)
\]</div>
<ul>
<li><span class="arithmatex">\(x_c\)</span>: input channel <span class="arithmatex">\(c\)</span> (e.g. R, G, B)</li>
<li><span class="arithmatex">\(W_{k,c}\)</span>: kernel for output channel <span class="arithmatex">\(k\)</span> and input channel <span class="arithmatex">\(c\)</span></li>
<li><span class="arithmatex">\(b_k\)</span>: bias for output channel <span class="arithmatex">\(k\)</span></li>
<li><span class="arithmatex">\(\sigma\)</span>: nonlinearity (ReLU, etc.)</li>
</ul>
<p>This produces <span class="arithmatex">\(C_\text{out}\)</span> feature maps, each representing a learned spatial pattern.</p>
<p>Weight sharing drastically reduces parameters:<br>
Each kernel might be <span class="arithmatex">\(3 \times 3\)</span> or <span class="arithmatex">\(5 \times 5\)</span> — independent of image size.</p>
<h2 id="deeplearning-2_convnets-4-building-blocks-of-cnns">4. Building Blocks of CNNs<a class="headerlink" href="#deeplearning-2_convnets-4-building-blocks-of-cnns" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-2_convnets-41-convolutional-layer">4.1 Convolutional Layer<a class="headerlink" href="#deeplearning-2_convnets-41-convolutional-layer" title="Permanent link">¶</a></h3>
<p>Performs learnable filtering and produces feature maps.</p>
<p>If input has shape <span class="arithmatex">\((H, W, C_\text{in})\)</span>:
- Kernel: <span class="arithmatex">\((k_H, k_W, C_\text{in}, C_\text{out})\)</span>
- Output: <span class="arithmatex">\((H', W', C_\text{out})\)</span></p>
<h3 id="deeplearning-2_convnets-42-nonlinear-activation">4.2 Nonlinear Activation<a class="headerlink" href="#deeplearning-2_convnets-42-nonlinear-activation" title="Permanent link">¶</a></h3>
<p>After convolution, apply nonlinearity (commonly ReLU):
<script type="math/tex; mode=display">
\text{ReLU}(x) = \max(0, x)
</script>
</p>
<h3 id="deeplearning-2_convnets-43-pooling-layer">4.3 Pooling Layer<a class="headerlink" href="#deeplearning-2_convnets-43-pooling-layer" title="Permanent link">¶</a></h3>
<p>Reduces spatial dimensions and increases invariance.</p>
<ul>
<li>Max pooling: selects the largest value in a patch.</li>
<li>Average pooling: takes mean value.</li>
</ul>
<p>Formally:
<script type="math/tex; mode=display">
y_{i,j} = \max_{(m,n)\in \Omega(i,j)} h_{m,n}
</script>
</p>
<p>Pooling introduces translation invariance — small shifts in input don’t drastically change outputs.</p>
<h3 id="deeplearning-2_convnets-44-flatten-fully-connected-layers">4.4 Flatten + Fully Connected Layers<a class="headerlink" href="#deeplearning-2_convnets-44-flatten-fully-connected-layers" title="Permanent link">¶</a></h3>
<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>
<h2 id="deeplearning-2_convnets-5-cnn-architecture-as-a-computation-graph">5. CNN Architecture as a Computation Graph<a class="headerlink" href="#deeplearning-2_convnets-5-cnn-architecture-as-a-computation-graph" title="Permanent link">¶</a></h2>
<p>A typical CNN defines a differentiable map:</p>
<div class="arithmatex">\[
f_\theta(x) = W_L (\text{Flatten}(h_{L-1})) + b_L
\]</div>
<p>where each layer <span class="arithmatex">\(h_l\)</span> is defined recursively as:</p>
<div class="arithmatex">\[
h_l = \sigma(\text{Conv}(h_{l-1}; W_l) + b_l), \quad l = 1, \dots, L-1
\]</div>
<p>Here, <code>Conv</code> represents the convolution operation.</p>
<p>Each layer is spatially local, translation-equivariant, and differentiable — meaning backpropagation works seamlessly, just as in MLPs.</p>
<h2 id="deeplearning-2_convnets-6-backpropagation-through-convolutions">6. Backpropagation Through Convolutions<a class="headerlink" href="#deeplearning-2_convnets-6-backpropagation-through-convolutions" title="Permanent link">¶</a></h2>
<p>The gradient computation is a direct extension of the chain rule.</p>
<h3 id="deeplearning-2_convnets-61-forward-pass">6.1 Forward Pass<a class="headerlink" href="#deeplearning-2_convnets-61-forward-pass" title="Permanent link">¶</a></h3>
<p>Compute:
<script type="math/tex; mode=display">
y = W * x + b
</script>
</p>
<h3 id="deeplearning-2_convnets-62-backward-pass">6.2 Backward Pass<a class="headerlink" href="#deeplearning-2_convnets-62-backward-pass" title="Permanent link">¶</a></h3>
<p>We need:
- Gradient w.r.t. weights:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial W} = x * \frac{\partial L}{\partial y}\)</span>
- Gradient w.r.t. input:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial x} = \text{flip}(W) * \frac{\partial L}{\partial y}\)</span></p>
<p>The flipping arises from the mathematical property of convolution.<br>
Modern frameworks handle this efficiently via <em>convolution transpose</em> operations.</p>
<p>Optimization viewpoint:<br>
Convolution layers remain linear in their weights — the nonlinearity and local parameter sharing define their expressive power.</p>
<h2 id="deeplearning-2_convnets-7-inductive-biases-in-cnns">7. Inductive Biases in CNNs<a class="headerlink" href="#deeplearning-2_convnets-7-inductive-biases-in-cnns" title="Permanent link">¶</a></h2>
<p>Convolutional architectures embed <em>strong inductive biases</em>:</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Mathematical Mechanism</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local connectivity</td>
<td>Small kernels (3×3, 5×5)</td>
<td>Exploits spatial locality</td>
</tr>
<tr>
<td>Weight sharing</td>
<td>Same filter across space</td>
<td>Reduces parameters drastically</td>
</tr>
<tr>
<td>Translation equivariance</td>
<td>Convolution operation</td>
<td>Same pattern detection anywhere</td>
</tr>
<tr>
<td>Pooling invariance</td>
<td>Spatial downsampling</td>
<td>Robust to small shifts/noise</td>
</tr>
</tbody>
</table>
<p>These biases make CNNs data-efficient and easy to train — especially compared to fully connected networks on images.</p>
<h2 id="deeplearning-2_convnets-8-optimization-and-training-dynamics">8. Optimization and Training Dynamics<a class="headerlink" href="#deeplearning-2_convnets-8-optimization-and-training-dynamics" title="Permanent link">¶</a></h2>
<p>Training CNNs is similar to MLPs — we use gradient-based optimizers (SGD, Adam, etc.) — but with different landscape geometry:</p>
<ul>
<li>Parameter sharing makes the loss smoother (less overfitting).</li>
<li>Batch normalization stabilizes gradient flow:
  <script type="math/tex; mode=display">
  \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
  </script>
</li>
<li>Regularization via dropout or weight decay improves generalization.</li>
<li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li>
</ul>
<p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>
<h2 id="deeplearning-2_convnets-9-cnn-architectures-through-history">9. CNN Architectures Through History<a class="headerlink" href="#deeplearning-2_convnets-9-cnn-architectures-through-history" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Year</th>
<th>Key Innovation</th>
<th>Depth</th>
<th>Inductive Bias</th>
</tr>
</thead>
<tbody>
<tr>
<td>LeNet-5</td>
<td>1998</td>
<td>First practical CNN for handwritten digits</td>
<td>7 layers</td>
<td>Local receptive fields</td>
</tr>
<tr>
<td>AlexNet</td>
<td>2012</td>
<td>GPU training, ReLU, dropout</td>
<td>8 layers</td>
<td>Data augmentation</td>
</tr>
<tr>
<td>VGG</td>
<td>2014</td>
<td>Deep stacks of small 3×3 filters</td>
<td>19 layers</td>
<td>Uniform architecture</td>
</tr>
<tr>
<td>ResNet</td>
<td>2015</td>
<td>Skip connections for gradient flow</td>
<td>152 layers</td>
<td>Identity mapping</td>
</tr>
<tr>
<td>DenseNet</td>
<td>2016</td>
<td>Feature reuse via dense connectivity</td>
<td>201 layers</td>
<td>Multi-scale learning</td>
</tr>
<tr>
<td>EfficientNet</td>
<td>2019</td>
<td>Compound scaling</td>
<td>variable</td>
<td>Optimized parameter scaling</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-2_convnets-10-cnns-and-the-optimization-landscape">10. CNNs and the Optimization Landscape<a class="headerlink" href="#deeplearning-2_convnets-10-cnns-and-the-optimization-landscape" title="Permanent link">¶</a></h2>
<p>CNNs reshape the optimization problem compared to MLPs:</p>
<ul>
<li>Reduced parameter redundancy → fewer degenerate directions in gradient space.</li>
<li>Structured weight sharing → smoother loss surface, fewer sharp minima.</li>
<li>Skip connections (ResNets) introduce <em>identity mappings</em>, improving conditioning of the Jacobian and preventing vanishing gradients.</li>
</ul>
<p>In optimization terms, CNNs are better-conditioned models of the input–output mapping.</p>
<h2 id="deeplearning-2_convnets-11-beyond-classical-cnns">11. Beyond Classical CNNs<a class="headerlink" href="#deeplearning-2_convnets-11-beyond-classical-cnns" title="Permanent link">¶</a></h2>
<p>Modern vision architectures have evolved:
- Residual Networks (ResNets): skip connections allow training very deep models.
- Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count.
- Dilated Convolutions: expand receptive field without extra parameters.
- Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>
<h2 id="deeplearning-2_convnets-12-mathematical-summary">12. Mathematical Summary<a class="headerlink" href="#deeplearning-2_convnets-12-mathematical-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Convolution</td>
<td><span class="arithmatex">\((I * K)(i,j) = \sum_m \sum_n I(i+m,j+n) K(m,n)\)</span></td>
<td>Weighted local sum</td>
</tr>
<tr>
<td>CNN Layer</td>
<td><span class="arithmatex">\(h = \sigma(W * x + b)\)</span></td>
<td>Convolution + nonlinearity</td>
</tr>
<tr>
<td>Pooling</td>
<td><span class="arithmatex">\(y_{i,j} = \max_{(m,n)\in \Omega(i,j)} h_{m,n}\)</span></td>
<td>Downsampling</td>
</tr>
<tr>
<td>Gradient wrt weights</td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial W} = x * \frac{\partial L}{\partial y}\)</span></td>
<td>Backprop step</td>
</tr>
<tr>
<td>Gradient wrt input</td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial x} = \text{flip}(W) * \frac{\partial L}{\partial y}\)</span></td>
<td>Sensitivity propagation</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-2_convnets-13-intuitive-summary">13. Intuitive Summary<a class="headerlink" href="#deeplearning-2_convnets-13-intuitive-summary" title="Permanent link">¶</a></h2>
<p>Convolutional networks are:
- Local → they process neighborhoods of data.
- Hierarchical → deeper layers build on lower-level features.
- Translation-equivariant → same pattern anywhere is treated the same.
- Efficient → far fewer parameters than MLPs.</p>
<p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p></body></html></section><section class="print-page" id="deeplearning-3_sequence_data" heading-number="4.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-modeling-sequence-data-in-deep-learning">Chapter 3: Modeling Sequence Data in Deep Learning<a class="headerlink" href="#deeplearning-3_sequence_data-chapter-3-modeling-sequence-data-in-deep-learning" title="Permanent link">¶</a></h1>
<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length <span class="arithmatex">\(T\)</span> can be written as <span class="arithmatex">\((x_1,x_2,\dots,x_T)\)</span>, where each element <span class="arithmatex">\(x_t\)</span> is indexed by its position in the sequence. Elements can repeat (e.g. the word “the” may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p>
<p>Sequences are collection of elements where:</p>
<ul>
<li>Elements can be repeated.</li>
<li>Order matters.</li>
<li>Of variable length.</li>
</ul>
<h2 id="deeplearning-3_sequence_data-limitations-of-traditional-supervised-models">Limitations of Traditional Supervised Models:<a class="headerlink" href="#deeplearning-3_sequence_data-limitations-of-traditional-supervised-models" title="Permanent link">¶</a></h2>
<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data – by, say, collapsing the sequence into a fixed-size feature vector – ignores the important temporal or sequential structure. As one summary notes, “feedforward neural networks are severely limited when it comes to sequential data”. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p>
<p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>
<h2 id="deeplearning-3_sequence_data-the-simplest-assumption-independent-words-bag-of-words">The Simplest Assumption: Independent Words (Bag-of-Words)<a class="headerlink" href="#deeplearning-3_sequence_data-the-simplest-assumption-independent-words-bag-of-words" title="Permanent link">¶</a></h2>
<p>A naïve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word’s occurrence, treating all words as “independent features.” This ignores sequence structure: “the order of words in the original documents is irrelevant”. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: “bag-of-words assumes words are independent of one another”. In reality, words co-occur in context (“peanut butter” versus “peanut giraffe”) – bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>
<h2 id="deeplearning-3_sequence_data-n-gram-models-and-fixed-context-assumptions">N-gram Models and Fixed-Context Assumptions<a class="headerlink" href="#deeplearning-3_sequence_data-n-gram-models-and-fixed-context-assumptions" title="Permanent link">¶</a></h2>
<p>To go beyond complete independence, one can incorporate local context by using <span class="arithmatex">\(n\)</span>-gram models. An <span class="arithmatex">\(n\)</span>-gram model makes the (Markov) assumption that the probability of each element depends only on the previous <span class="arithmatex">\(n-1\)</span> elements. For language, a bigram model (2-gram) assumes <span class="arithmatex">\(P(w_t\mid w_{t-1})\)</span>, a trigram (3-gram) uses <span class="arithmatex">\(P(w_t\mid w_{t-2},w_{t-1})\)</span>, etc. In general, the chain rule with an <span class="arithmatex">\(N\)</span>-gram approximation is</p>
<div class="arithmatex">\[
P(x_1, \ldots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_{t-N+1}, \ldots, x_{t-1}) \, .
\]</div>
<p>This preserves some order information: the window of the last <span class="arithmatex">\(N-1\)</span> items is used to predict the next. However, <span class="arithmatex">\(n\)</span>-gram models have well-known downsides:</p>
<ul>
<li>
<p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language “cannot reason about context beyond the immediate <span class="arithmatex">\(n\)</span>-gram window”, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an <span class="arithmatex">\(n\)</span>-gram.</p>
</li>
<li>
<p>Data sparsity and scalability: The number of possible <span class="arithmatex">\(n\)</span>-grams grows exponentially with vocabulary size <span class="arithmatex">\(V\)</span>. For a vocabulary of size <span class="arithmatex">\(V\)</span>, there are <span class="arithmatex">\(V^N\)</span> possible <span class="arithmatex">\(N\)</span>-grams. Jurafsky &amp; Martin observe that even for Shakespeare’s corpus (<span class="arithmatex">\(V\approx 29{,}066\)</span>), there are <span class="arithmatex">\(V^2\approx8.4\times 10^8\)</span> possible bigrams and <span class="arithmatex">\(V^4\approx 7\times 10^{17}\)</span> possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough <span class="arithmatex">\(n\)</span>-gram counts, and storing these tables is impractical for large <span class="arithmatex">\(N\)</span> or <span class="arithmatex">\(V\)</span>. In practice, language models become “ridiculously sparse” and unwieldy.</p>
</li>
<li>
<p>No parametrization (non-differentiable): Traditional <span class="arithmatex">\(n\)</span>-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p>
</li>
</ul>
<p>In summary, while <span class="arithmatex">\(n\)</span>-grams preserve local order up to length <span class="arithmatex">\(N\)</span>, they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>
<h2 id="deeplearning-3_sequence_data-learnable-context-models-vectorization-and-neural-nets">Learnable Context Models: Vectorization and Neural Nets<a class="headerlink" href="#deeplearning-3_sequence_data-learnable-context-models-vectorization-and-neural-nets" title="Permanent link">¶</a></h2>
<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p>
<ul>
<li>
<p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p>
</li>
<li>
<p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p>
</li>
<li>
<p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can “operate in variable length of sequences”.</p>
</li>
<li>
<p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p>
</li>
<li>
<p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p>
</li>
</ul>
<p>For example, one could take the last few words, map each to an embedding <span class="arithmatex">\(\mathbf{x}{t-N+1},\dots,\mathbf{x}{t-1}\)</span>, concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word’s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (<span class="arithmatex">\(N-1\)</span>) and uses a separate weight for each position, so it’s not efficient or variable-length. </p>
<p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector <span class="arithmatex">\(\mathbf{h}_t\)</span> that evolves as the sequence is read. Such a context-vector “acts as memory” summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can “learn” how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>
<h2 id="deeplearning-3_sequence_data-recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)<a class="headerlink" href="#deeplearning-3_sequence_data-recurrent-neural-networks-rnns" title="Permanent link">¶</a></h2>
<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) – models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step <span class="arithmatex">\(t\)</span>, the RNN takes the current input <span class="arithmatex">\(\mathbf{x}t\)</span> and the previous hidden state <span class="arithmatex">\(\mathbf{h}{t-1}\)</span> and computes a new hidden state <span class="arithmatex">\(\mathbf{h}_t\)</span>. The simplest RNN update is:</p>
<div class="arithmatex">\[
h_t = \phi(W_h h_{t-1} + W_x x_t + b) \, .
\]</div>
<p>where <span class="arithmatex">\(\phi\)</span> is a nonlinear activation (often <span class="arithmatex">\(\tanh\)</span>) and <span class="arithmatex">\(W_h,W_x\)</span> are weight matrices. The same weight matrices <span class="arithmatex">\(W_h,W_x\)</span> are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p>
<p>Intuitively, the RNN’s hidden state <span class="arithmatex">\(\mathbf{h}_t\)</span> “remembers” the information from all prior inputs up to time <span class="arithmatex">\(t\)</span>. The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p>
<div class="arithmatex">\[
y_t = \mathrm{softmax}(W_y h_t + b_y) \, .
\]</div>
<p>so that <span class="arithmatex">\(P(x_{t+1}=w \mid \mathbf{h}_t)\)</span> is given by the corresponding component of <span class="arithmatex">\(\mathbf{y}_t\)</span>. In language modeling, for instance, <span class="arithmatex">\(y_t\)</span> gives a probability for each word in the vocabulary. As described in practice, “RNNs predict the output from the last hidden state along with output parameter <span class="arithmatex">\(W_y\)</span>; a softmax function to ensure the probability over all possible words”. </p>
<p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an “MLP with shared weights across time.” At time <span class="arithmatex">\(t\)</span>, the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>
<h2 id="deeplearning-3_sequence_data-unrolling-and-backpropagation-through-time-bptt">Unrolling and Backpropagation Through Time (BPTT)<a class="headerlink" href="#deeplearning-3_sequence_data-unrolling-and-backpropagation-through-time-bptt" title="Permanent link">¶</a></h2>
<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across <span class="arithmatex">\(T\)</span> time steps, creating a deep feedforward network of depth <span class="arithmatex">\(T\)</span> (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, “the network needs to be expanded, or unfolded, so that the parameters could be differentiated ... – hence backpropagation through time (BPTT)”. In practice, each weight matrix <span class="arithmatex">\(W\)</span> receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if <span class="arithmatex">\(L = -\sum_t \log P(x_t\mid \mathbf{h}_{t-1})\)</span> is the loss, then for each <span class="arithmatex">\(W\)</span> we compute</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial W} = \sum_{t} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W} \, .
\]</div>
<p>taking into account the influence of <span class="arithmatex">\(W\)</span> at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>
<h2 id="deeplearning-3_sequence_data-vanishing-and-exploding-gradients">Vanishing and Exploding Gradients<a class="headerlink" href="#deeplearning-3_sequence_data-vanishing-and-exploding-gradients" title="Permanent link">¶</a></h2>
<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative <span class="arithmatex">\(\partial \mathbf{h}t/\partial \mathbf{h}{t-1}\)</span> involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius <span class="arithmatex">\(&lt;1\)</span> causes the gradients to shrink exponentially (vanishing), while if the spectral radius is <span class="arithmatex">\(&gt;1\)</span> they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues <span class="arithmatex">\(&gt;1\)</span>), whereas the vanishing gradient problem occurs when long-term components of the gradient go “exponentially fast to norm 0”. Formally, for a linearized RNN one can show that if the largest eigenvalue <span class="arithmatex">\(\lambda_{\max}\)</span> of the recurrent weight matrix satisfies <span class="arithmatex">\(|\lambda_{\max}|&lt;1\)</span>, long-term gradients vanish as <span class="arithmatex">\(t\to\infty\)</span>, and if <span class="arithmatex">\(|\lambda_{\max}|&gt;1\)</span> they explode. </p>
<p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: “when long term components go to zero, the model cannot learn correlation between distant events.” In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>
<h2 id="deeplearning-3_sequence_data-gated-architectures-lstm-and-gru">Gated Architectures: LSTM and GRU<a class="headerlink" href="#deeplearning-3_sequence_data-gated-architectures-lstm-and-gru" title="Permanent link">¶</a></h2>
<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable “gates” that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state <span class="arithmatex">\(\mathbf{C}_t\)</span> and three gates: input (<span class="arithmatex">\(\mathbf{i}_t\)</span>), forget (<span class="arithmatex">\(\mathbf{f}_t\)</span>), and output (<span class="arithmatex">\(\mathbf{o}t\)</span>) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time <span class="arithmatex">\(t\)</span> with input <span class="arithmatex">\(\mathbf{x}t\)</span> and previous hidden <span class="arithmatex">\(\mathbf{h}{t-1}\)</span> and cell <span class="arithmatex">\(\mathbf{C}{t-1}\)</span>, the gates and cell update are given by (all operations are elementwise):</p>
<p>​<script type="math/tex; mode=display">
\begin{aligned}
i_t &= \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i), \\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f), \\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o), \\
\tilde{C}_t &= \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \, .
\end{aligned}
</script>
</p>
<p>The new cell state <span class="arithmatex">\(\mathbf{C}_t\)</span> is then updated by combining the old state and the candidate:</p>
<div class="arithmatex">\[
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \, .
\]</div>
<p>where <span class="arithmatex">\(\odot\)</span> denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p>
<div class="arithmatex">\[
h_t = o_t \odot \tanh(C_t) \,
\]</div>
<p>The intuition is that the forget gate <span class="arithmatex">\(\mathbf{f_t}\)</span> can reset or retain the old memory <span class="arithmatex">\(\mathbf{C}_{t-1}\)</span>, the input gate <span class="arithmatex">\(\mathbf{i}_t\)</span> controls how much new information <span class="arithmatex">\(\tilde{\mathbf{C}}_t\)</span> to write, and the output gate <span class="arithmatex">\(\mathbf{o}_t\)</span> controls how much of the cell state to expose as <span class="arithmatex">\(\mathbf{h}_t\)</span>. By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs “alleviate the vanishing gradient problem,” making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p>
<p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>
<h2 id="deeplearning-3_sequence_data-optimization-challenges-and-solutions">Optimization Challenges and Solutions<a class="headerlink" href="#deeplearning-3_sequence_data-optimization-challenges-and-solutions" title="Permanent link">¶</a></h2>
<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p>
<ul>
<li>
<p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping “solves the exploding gradients problem” by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p>
</li>
<li>
<p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing <span class="arithmatex">\(W_h\)</span> as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, “Orthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,” ensuring stable gradient propagation. In practice, some implementations initialize <span class="arithmatex">\(W_h\)</span> to random orthogonal (or unitary) matrices to encourage long memory.</p>
</li>
<li>
<p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p>
</li>
<li>
<p>Regularization: Some works add penalties to encourage <span class="arithmatex">\(W_h\)</span> to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p>
</li>
</ul>
<p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models – especially gated RNNs – provide a powerful framework: mathematically, they define hidden states <span class="arithmatex">\(\mathbf{h}_t\)</span> updated by <span class="arithmatex">\(\mathbf{h}t = f(\mathbf{h}{t-1},\mathbf{x}_t)\)</span> with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p></body></html></section><section class="print-page" id="deeplearning-4_nlp" heading-number="4.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="deep-learning-for-natural-language-processing">Deep Learning for Natural Language Processing<a class="headerlink" href="#deeplearning-4_nlp-deep-learning-for-natural-language-processing" title="Permanent link">¶</a></h1>
<ul>
<li>Natural language is context-dependent, compositional, and ambiguous.</li>
<li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation — ideal for modeling contextual relationships.</li>
<li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li>
</ul>
<h3 id="deeplearning-4_nlp-key-challenges-of-language">Key Challenges of Language<a class="headerlink" href="#deeplearning-4_nlp-key-challenges-of-language" title="Permanent link">¶</a></h3>
<p>Human language presents a unique set of challenges for computational models.<br>
Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p>
<ul>
<li>
<p>Words are not discrete symbols.<br>
  The same word can have several related senses depending on context — for example:<br>
<code>face₁</code> (human face), <code>face₂</code> (clock face), <code>face₃</code> (to confront), and <code>face₄</code> (a person or presence).<br>
  Treating these as independent dictionary entries loses the shared semantic structure between them.<br>
  A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p>
</li>
<li>
<p>Need for distributed representations.<br>
  Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).<br>
  This allows similarity, analogy, and composition to emerge geometrically — for instance, <code>king - man + woman ≈ queen</code>.</p>
</li>
<li>
<p>Disambiguation depends on context.<br>
  The meaning of a word or phrase is determined by its linguistic surroundings.<br>
  For example, in “The man who ate the pepper sneezed,” the subject of <em>sneezed</em> is determined by a non-adjacent clause (<em>the man</em>), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p>
</li>
<li>
<p>Non-local dependencies.<br>
  Natural language contains relationships between words that may be far apart in sequence.<br>
  Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.<br>
  Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p>
</li>
<li>
<p>Compositionality.<br>
  The meaning of larger expressions arises from the meanings of their parts and how they are combined.<br>
  However, this combination is not purely linear.<br>
  For example, <code>carnivorous plant</code> is not simply the sum of <em>carnivore</em> and <em>plant</em> — its interpretation depends on how the features interact (<em>a plant that eats insects</em>).<br>
  Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p>
</li>
</ul>
<p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically.<br>
Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>
<h2 id="deeplearning-4_nlp-the-transformer-architecture">The Transformer Architecture<a class="headerlink" href="#deeplearning-4_nlp-the-transformer-architecture" title="Permanent link">¶</a></h2>
<ul>
<li>Sequence models (RNNs, LSTMs) process tokens sequentially — limiting parallelism and long-range context.</li>
<li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li>
</ul>
<h3 id="deeplearning-4_nlp-core-mechanism-self-attention">Core Mechanism: Self-Attention<a class="headerlink" href="#deeplearning-4_nlp-core-mechanism-self-attention" title="Permanent link">¶</a></h3>
<p>Given token embeddings <script type="math/tex"> e_i \in \mathbb{R}^d </script>:</p>
<div class="arithmatex">\[
q_i = e_i W^Q, \quad k_i = e_i W^K, \quad v_i = e_i W^V
\]</div>
<p>Attention weights:</p>
<div class="arithmatex">\[
\alpha_{ij} = \mathrm{softmax}_j \left( \frac{q_i k_j^\top}{\sqrt{d}} \right)
\]</div>
<p>Output:</p>
<div class="arithmatex">\[
z_i = \sum_j \alpha_{ij} v_j
\]</div>
<p>Each token’s new representation <script type="math/tex"> z_i </script> is a contextual blend of all others.<br>
Captures semantic and syntactic relations without explicit recurrence.</p>
<h3 id="deeplearning-4_nlp-multi-head-attention">Multi-Head Attention<a class="headerlink" href="#deeplearning-4_nlp-multi-head-attention" title="Permanent link">¶</a></h3>
<p>Use multiple projections <span class="arithmatex">\((W^Q_h, W^K_h, W^V_h)\)</span> → multiple “heads.”<br>
Each head focuses on different relations (e.g. subject–verb, modifier–noun).<br>
Outputs are concatenated and projected back to dimension <span class="arithmatex">\(d\)</span>:</p>
<div class="arithmatex">\[
\text{MHA}(E) = [Z_1; Z_2; \dots; Z_H] W^O
\]</div>
<h3 id="deeplearning-4_nlp-position-encoding">Position Encoding<a class="headerlink" href="#deeplearning-4_nlp-position-encoding" title="Permanent link">¶</a></h3>
<p>Since attention is permutation-invariant, Transformers add position information:</p>
<div class="arithmatex">\[
\text{PE}_{(pos,2i)} = \sin(pos / 10000^{2i/d}), \quad
\text{PE}_{(pos,2i+1)} = \cos(pos / 10000^{2i/d})
\]</div>
<p>→ These sinusoidal signals are added to embeddings to encode word order.</p>
<h3 id="deeplearning-4_nlp-full-transformer-block">Full Transformer Block<a class="headerlink" href="#deeplearning-4_nlp-full-transformer-block" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>Input
  ↓
Multi-Head Self-Attention
  ↓
+ Skip Connection
  ↓
Layer Normalization
  ↓
Feedforward Network (ReLU)
  ↓
+ Skip Connection
  ↓
Layer Normalization
  ↓
Output
</code></pre></div>
<p>Skip connections enable gradient flow and top-down influence.<br>
Stacking <span class="arithmatex">\(N\)</span> blocks yields hierarchical contextualization of meaning.</p>
<h3 id="deeplearning-4_nlp-intuition">Intuition<a class="headerlink" href="#deeplearning-4_nlp-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>Self-attention handles non-local relations.</li>
<li>Multi-head captures multiple semantic dimensions simultaneously.</li>
<li>Stacked layers build abstraction — from word-level to phrase- and discourse-level features.</li>
</ul>
<h2 id="deeplearning-4_nlp-unsupervised-learning-and-bert">Unsupervised Learning and BERT<a class="headerlink" href="#deeplearning-4_nlp-unsupervised-learning-and-bert" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-4_nlp-the-need-for-contextualized-representations">The Need for Contextualized Representations<a class="headerlink" href="#deeplearning-4_nlp-the-need-for-contextualized-representations" title="Permanent link">¶</a></h3>
<ul>
<li>Word embeddings like Word2Vec are static: one vector per word.</li>
<li>Language understanding requires contextual embeddings: “bank” (river vs. finance).</li>
<li>Transformers enable bidirectional context — understanding a word from both sides.</li>
</ul>
<h3 id="deeplearning-4_nlp-bert-pretraining-objectives">BERT Pretraining Objectives<a class="headerlink" href="#deeplearning-4_nlp-bert-pretraining-objectives" title="Permanent link">¶</a></h3>
<ol>
<li>Masked Language Modeling (MLM)<br>
Randomly mask 15% of tokens, predict them:</li>
</ol>
<div class="arithmatex">\[
\text{Loss}_{MLM} = - \sum_{i \in M} \log P(w_i | \text{context})
\]</div>
<p>Encourages bidirectional encoding of meaning.</p>
<ol>
<li>Next Sentence Prediction (NSP)<br>
Model predicts if sentence B follows sentence A.<br>
Builds discourse-level coherence and world knowledge.</li>
</ol>
<h3 id="deeplearning-4_nlp-architecture">Architecture<a class="headerlink" href="#deeplearning-4_nlp-architecture" title="Permanent link">¶</a></h3>
<ul>
<li>Deep bidirectional Transformer encoder.</li>
<li>Uses special tokens:</li>
<li><code>[CLS]</code> – sentence-level classification embedding</li>
<li><code>[SEP]</code> – separates segments</li>
<li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li>
<li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li>
</ul>
<h3 id="deeplearning-4_nlp-significance">Significance<a class="headerlink" href="#deeplearning-4_nlp-significance" title="Permanent link">¶</a></h3>
<p>BERT shows self-supervised pretraining → transfer learning pipeline:</p>
<div class="highlight"><pre><span></span><code>Pretrain (unsupervised)
   ↓
Fine-tune (supervised)
   ↓
Task-specific adaptation
</code></pre></div>
<p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data.<br>
Learns semantic similarity, coreference, and discourse relations implicitly.</p>
<h2 id="deeplearning-4_nlp-grounded-and-embodied-language-learning">Grounded and Embodied Language Learning<a class="headerlink" href="#deeplearning-4_nlp-grounded-and-embodied-language-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-4_nlp-motivation">Motivation<a class="headerlink" href="#deeplearning-4_nlp-motivation" title="Permanent link">¶</a></h3>
<ul>
<li>Language understanding ultimately involves relating words to the world.</li>
<li>Humans learn language in context — perception, action, and social interaction.</li>
<li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li>
</ul>
<h3 id="deeplearning-4_nlp-grounded-agents">Grounded Agents<a class="headerlink" href="#deeplearning-4_nlp-grounded-agents" title="Permanent link">¶</a></h3>
<ul>
<li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li>
<li>Train via predictive modeling — anticipate sensory outcomes from language-conditioned actions.</li>
<li>Enables semantic grounding: linking word “red” to visual color, “pick up” to motor command.</li>
</ul>
<h3 id="deeplearning-4_nlp-predictive-and-self-supervised-paradigms">Predictive and Self-Supervised Paradigms<a class="headerlink" href="#deeplearning-4_nlp-predictive-and-self-supervised-paradigms" title="Permanent link">¶</a></h3>
<p>Agents learn representations by predicting future sensory or linguistic states:</p>
<div class="arithmatex">\[
\min_\theta \mathbb{E} [ \| f_\theta(s_t, a_t) - s_{t+1} \|^2 ]
\]</div>
<p>→ Connects to world models and predictive coding principles in neuroscience.<br>
The agent’s internal model encodes both linguistic meaning and causal structure of the environment.</p>
<h2 id="deeplearning-4_nlp-insights-from-deepmind-work">Insights from DeepMind Work<a class="headerlink" href="#deeplearning-4_nlp-insights-from-deepmind-work" title="Permanent link">¶</a></h2>
<ul>
<li>Embodied agents trained in simulated environments exhibit:</li>
<li>Systematic generalization (e.g., learning “pick up red object” → generalize to unseen colors).</li>
<li>Question answering and instruction following grounded in perception.</li>
<li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li>
</ul>
<h3 id="deeplearning-4_nlp-conceptual-shift">Conceptual Shift<a class="headerlink" href="#deeplearning-4_nlp-conceptual-shift" title="Permanent link">¶</a></h3>
<p>From pipeline → integrated model:</p>
<table>
<thead>
<tr>
<th>Classic Pipeline</th>
<th>Embodied / Interactive Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Letters → Words → Syntax → Meaning → Action</td>
<td>Multimodal loops: Perception ↔ Action ↔ Language ↔ Prediction</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-4_nlp-conceptual-map-from-representation-to-understanding">Conceptual Map: From Representation to Understanding<a class="headerlink" href="#deeplearning-4_nlp-conceptual-map-from-representation-to-understanding" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>Word Input
   ↓
Distributed Representations (embedding)
   ↓
Self-Attention Mechanism
   ↓
Multi-Head Parallel Processing
   ↓
Hierarchical Transformer Layers
   ↓
Contextualized Embeddings (BERT)
   ↓
Transfer Learning to Tasks
   ↓
Embodied Agents (Grounded Semantics)
   ↓
Language Understanding as Prediction + Interaction
</code></pre></div>
<h3 id="deeplearning-4_nlp-key-transitions">Key Transitions<a class="headerlink" href="#deeplearning-4_nlp-key-transitions" title="Permanent link">¶</a></h3>
<p>Symbol → Vector: Continuous representations enable learning of semantic gradients.</p>
<p>Sequence → Attention: Parallel context integration replaces recurrence.</p>
<p>Text → Context: Pretraining captures knowledge without explicit supervision.</p>
<p>Language → World: Grounding links linguistic representations to sensory and causal models.</p>
<h3 id="deeplearning-4_nlp-unifying-principle">Unifying Principle<a class="headerlink" href="#deeplearning-4_nlp-unifying-principle" title="Permanent link">¶</a></h3>
<p>Deep language understanding = predictive modeling of structured context
across both linguistic and environmental domains.</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Core Idea</th>
<th>Model / Mechanism</th>
</tr>
</thead>
<tbody>
<tr>
<td>Distributed representations</td>
<td>Meanings as patterns, not symbols</td>
<td>Embeddings</td>
</tr>
<tr>
<td>Context dependence</td>
<td>Sense resolution via interaction</td>
<td>Self-attention</td>
</tr>
<tr>
<td>Parallelism</td>
<td>All words attend to all others</td>
<td>Transformer</td>
</tr>
<tr>
<td>Bidirectionality</td>
<td>Context from both sides</td>
<td>BERT encoder</td>
</tr>
<tr>
<td>Transfer learning</td>
<td>Self-supervised → supervised</td>
<td>Fine-tuning</td>
</tr>
<tr>
<td>Grounding</td>
<td>Language tied to perception/action</td>
<td>Embodied agents</td>
</tr>
<tr>
<td>Predictive learning</td>
<td>Understanding as anticipation</td>
<td>World models</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="deeplearning-5_attention" heading-number="4.5"><h1 id="deeplearning-5_attention-deeplearning-5_attention">5. Transformers and Attention Mechanisms</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-attention-memory-and-cognition">1. Attention, Memory, and Cognition<a class="headerlink" href="#deeplearning-5_attention-1-attention-memory-and-cognition" title="Permanent link">¶</a></h2>
<ul>
<li>Attention = ability to focus on relevant signals and ignore distractions.  </li>
<li>Enables selective processing (e.g. cocktail party effect).  </li>
<li>
<p>Allows focusing on one thought or event at a time.</p>
</li>
<li>
<p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p>
</li>
<li>
<p>Together, they form the basis of cognition — controlling what to process, store, and recall.</p>
</li>
<li>
<p>Neural networks can model aspects of this by learning <em>what to attend to</em> and <em>what to remember</em>.</p>
</li>
<li>
<p>Goal of attention in DL:<br>
  Reduce complexity by focusing computation on the most informative parts of data or internal state.</p>
</li>
</ul>
<h2 id="deeplearning-5_attention-2-implicit-attention-in-neural-networks">2. Implicit Attention in Neural Networks<a class="headerlink" href="#deeplearning-5_attention-2-implicit-attention-in-neural-networks" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Neural networks are parametric nonlinear functions <span class="arithmatex">\(y = f_\theta(x)\)</span> mapping inputs to outputs.<br>
  They naturally exhibit <em>implicit attention</em>: certain input dimensions influence outputs more.</p>
</li>
<li>
<p>The Jacobian <span class="arithmatex">\(J = \frac{\partial y}{\partial x}\)</span> quantifies this sensitivity — shows which input parts the model “pays attention” to.</p>
</li>
<li>
<p>Example:<br>
  In deep RL, sensitivity maps reveal focus on <em>state-value</em> vs <em>action-advantage</em> components.</p>
</li>
<li>
<p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p>
</li>
<li>Hidden state <span class="arithmatex">\(h_t\)</span> stores past info.  </li>
<li>The <em>sequential Jacobian</em> <span class="arithmatex">\(\frac{\partial y_t}{\partial x_{t-k}}\)</span> shows which past inputs are remembered.  </li>
<li>
<p>Implicitly attends to relevant time steps (memory through recurrence).</p>
</li>
<li>
<p>In tasks like machine translation, implicit attention lets models reorder tokens:</p>
<blockquote>
<p>“to reach” → “zu erreichen”</p>
</blockquote>
</li>
</ul>
<h2 id="deeplearning-5_attention-3-explicit-hard-attention">3. Explicit (Hard) Attention<a class="headerlink" href="#deeplearning-5_attention-3-explicit-hard-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Explicit attention introduces a separate <em>attention mechanism</em> that decides where to look or what to read.<br>
  It restricts the data fed to the main network.</li>
</ul>
<h3 id="deeplearning-5_attention-why-explicit-attention">Why explicit attention?<a class="headerlink" href="#deeplearning-5_attention-why-explicit-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Efficiency: processes only selected parts of input.  </li>
<li>Scalability: works on large or variable-size data.  </li>
<li>Sequential processing: e.g. moving “gaze” across static images.  </li>
<li>Interpretability: easier to visualize focus regions.</li>
</ul>
<h3 id="deeplearning-5_attention-model-structure">Model structure<a class="headerlink" href="#deeplearning-5_attention-model-structure" title="Permanent link">¶</a></h3>
<ul>
<li>Network outputs attention parameters <span class="arithmatex">\(a\)</span> that define a <em>glimpse distribution</em> <span class="arithmatex">\(p(g|a)\)</span> over possible data regions.  </li>
<li>A glimpse <span class="arithmatex">\(g\)</span> (subset or window of data) is sampled and passed back as input.  </li>
<li>System becomes recurrent, even if the base network is not.</li>
</ul>
<h3 id="deeplearning-5_attention-training-non-differentiable">Training (non-differentiable)<a class="headerlink" href="#deeplearning-5_attention-training-non-differentiable" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>When glimpse selection is discrete or stochastic, use REINFORCE:
  <script type="math/tex; mode=display">
  \nabla_\theta \mathbb{E}[R] = \mathbb{E}[(R - b) \nabla_\theta \log \pi_\theta(g)]
  </script>
  where <span class="arithmatex">\(R\)</span> is the reward (e.g. task loss) and <span class="arithmatex">\(b\)</span> a baseline for variance reduction.</p>
</li>
<li>
<p>Thus, attention acts as a policy <span class="arithmatex">\(\pi_\theta(g)\)</span> over glimpses.</p>
</li>
</ul>
<h3 id="deeplearning-5_attention-examples">Examples<a class="headerlink" href="#deeplearning-5_attention-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li>
<li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li>
</ul>
<h2 id="deeplearning-5_attention-4-soft-attention">4. Soft Attention<a class="headerlink" href="#deeplearning-5_attention-4-soft-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Hard attention samples discrete glimpses → non-differentiable → needs RL.  </li>
<li>Soft attention computes a <em>weighted average</em> over all glimpses → differentiable → trainable by backprop.</li>
</ul>
<h3 id="deeplearning-5_attention-basic-idea">Basic idea<a class="headerlink" href="#deeplearning-5_attention-basic-idea" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Attention parameters <span class="arithmatex">\(a\)</span> define weights <span class="arithmatex">\(w_i\)</span> over input features <span class="arithmatex">\(v_i\)</span>:
  <script type="math/tex; mode=display">
  v = \sum_i w_i v_i, \quad \sum_i w_i = 1
  </script>
  The readout <span class="arithmatex">\(v\)</span> is a smooth combination of inputs.</p>
</li>
<li>
<p>Replaces sampling by <em>expectation</em> → continuous, differentiable.</p>
</li>
</ul>
<h3 id="deeplearning-5_attention-benefits">Benefits<a class="headerlink" href="#deeplearning-5_attention-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>Trained end-to-end with gradients.  </li>
<li>Easier and more stable than hard attention.  </li>
<li>Allows <em>focus distribution</em> rather than a single point.</li>
</ul>
<h3 id="deeplearning-5_attention-variants">Variants<a class="headerlink" href="#deeplearning-5_attention-variants" title="Permanent link">¶</a></h3>
<ul>
<li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li>
<li>Content-based attention: focuses by similarity of key <span class="arithmatex">\(k\)</span> to data vectors <span class="arithmatex">\(x_i\)</span> via score <span class="arithmatex">\(S(k, x_i)\)</span>, usually normalized by softmax:
  <script type="math/tex; mode=display">
  w_i = \frac{\exp(S(k, x_i))}{\sum_j \exp(S(k, x_j))}
  </script>
</li>
</ul>
<h3 id="deeplearning-5_attention-applications">Applications<a class="headerlink" href="#deeplearning-5_attention-applications" title="Permanent link">¶</a></h3>
<ul>
<li>Handwriting synthesis: RNN learns soft “window” over text sequence.  </li>
<li>Neural Machine Translation: associative attention aligns words between languages.  </li>
<li>
<p>DRAW model: uses Gaussian filters to read/write parts of an image.</p>
</li>
<li>
<p>Soft attention = <em>data-dependent dynamic weighting</em> (similar to convolution with adaptive filters).</p>
</li>
</ul>
<h2 id="deeplearning-5_attention-5-introspective-attention-and-memory">5. Introspective Attention and Memory<a class="headerlink" href="#deeplearning-5_attention-5-introspective-attention-and-memory" title="Permanent link">¶</a></h2>
<ul>
<li>So far: attention over external data.  </li>
<li>Now: attention over internal state or memory → “introspective attention.”  </li>
<li>Lets the network <em>read</em> or <em>write</em> selectively to memory locations.  </li>
<li>Enables reasoning, recall, and algorithmic behavior.</li>
</ul>
<h3 id="deeplearning-5_attention-neural-turing-machine-ntm">Neural Turing Machine (NTM)<a class="headerlink" href="#deeplearning-5_attention-neural-turing-machine-ntm" title="Permanent link">¶</a></h3>
<ul>
<li>Adds a differentiable memory matrix <span class="arithmatex">\(M \in \mathbb{R}^{N \times W}\)</span>.  </li>
<li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li>
</ul>
<p>Operations
- Write: modify selected rows in <span class="arithmatex">\(M\)</span> using attention weights <span class="arithmatex">\(w_t\)</span>.<br>
- Read: output weighted sum of memory slots:
  <script type="math/tex; mode=display">
  r_t = \sum_i w_{t,i} M_i
  </script>
- Addressing modes:
  - <em>Content-based</em>: match key vector <span class="arithmatex">\(k_t\)</span> to memory contents (via cosine similarity).<br>
  - <em>Location-based</em>: shift attention by relative position.</p>
<p>Training: fully differentiable — end-to-end via backprop.</p>
<p>Example task: copying sequences of variable length — learns algorithmic generalization.</p>
<h3 id="deeplearning-5_attention-differentiable-neural-computer-dnc">Differentiable Neural Computer (DNC)<a class="headerlink" href="#deeplearning-5_attention-differentiable-neural-computer-dnc" title="Permanent link">¶</a></h3>
<ul>
<li>Successor to NTM with richer memory access:</li>
<li>Tracks temporal links between writes.  </li>
<li>Supports dynamic memory allocation.  </li>
<li>Improves stability and scalability.</li>
</ul>
<p>Application: synthetic QA tasks (bAbI dataset) — answers questions requiring multiple supporting facts and temporal reasoning.</p>
<p>Key insight:<br>
Attention provides <em>selective access</em> to memory, acting like “addressing” in a differentiable data structure.</p>
<h2 id="deeplearning-5_attention-6-transformers-and-self-attention">6. Transformers and Self-Attention<a class="headerlink" href="#deeplearning-5_attention-6-transformers-and-self-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Transformers: remove recurrence and convolution entirely — rely only on attention.</li>
</ul>
<h3 id="deeplearning-5_attention-self-attention">Self-Attention<a class="headerlink" href="#deeplearning-5_attention-self-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Each token attends to all others in the sequence:
  <script type="math/tex; mode=display">
  \text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
  </script>
  where:</li>
<li><span class="arithmatex">\(Q, K, V\)</span> are query, key, and value matrices (learned linear projections of input embeddings).</li>
<li>Produces context-aware representations for all tokens in parallel.</li>
</ul>
<h3 id="deeplearning-5_attention-multi-head-attention">Multi-Head Attention<a class="headerlink" href="#deeplearning-5_attention-multi-head-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Multiple attention “heads” (<span class="arithmatex">\(H\)</span>) learn different relationships:
  <script type="math/tex; mode=display">
  \text{MultiHead}(Q,K,V) = [h_1; h_2; \dots; h_H] W^O
  </script>
  Each head captures a distinct pattern (syntax, semantics, position, etc.).</li>
</ul>
<h3 id="deeplearning-5_attention-transformer-block">Transformer Block<a class="headerlink" href="#deeplearning-5_attention-transformer-block" title="Permanent link">¶</a></h3>
<ul>
<li>Structure:</li>
<li>Multi-head self-attention  </li>
<li>Add &amp; LayerNorm  </li>
<li>Feedforward (ReLU + linear)  </li>
<li>Add &amp; LayerNorm  </li>
<li>Skip connections improve gradient flow and allow top-down signal mixing.</li>
</ul>
<h3 id="deeplearning-5_attention-positional-encoding">Positional Encoding<a class="headerlink" href="#deeplearning-5_attention-positional-encoding" title="Permanent link">¶</a></h3>
<ul>
<li>Since model is permutation-invariant, inject position information:
  <script type="math/tex; mode=display">
  \text{PE}_{(pos,2i)} = \sin(pos / 10000^{2i/d})
  </script>
<script type="math/tex; mode=display">
  \text{PE}_{(pos,2i+1)} = \cos(pos / 10000^{2i/d})
  </script>
  Added to input embeddings.</li>
</ul>
<h3 id="deeplearning-5_attention-intuition">Intuition<a class="headerlink" href="#deeplearning-5_attention-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>Self-attention generalizes RNN memory:</li>
<li>Recurrent → sequential access  </li>
<li>Transformer → <em>direct pairwise access</em> between all tokens.</li>
<li>Enables long-range dependencies and parallelization.</li>
</ul>
<h3 id="deeplearning-5_attention-key-result">Key result<a class="headerlink" href="#deeplearning-5_attention-key-result" title="Permanent link">¶</a></h3>
<ul>
<li>Attention-only models achieve SOTA in translation and NLP tasks.  </li>
<li>Forms basis for BERT, GPT, and modern large language models.</li>
</ul>
<h2 id="deeplearning-5_attention-7-adaptive-computation-time-act-and-summary">7. Adaptive Computation Time (ACT) and Summary<a class="headerlink" href="#deeplearning-5_attention-7-adaptive-computation-time-act-and-summary" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-5_attention-adaptive-computation-time-act">Adaptive Computation Time (ACT)<a class="headerlink" href="#deeplearning-5_attention-adaptive-computation-time-act" title="Permanent link">¶</a></h3>
<ul>
<li>Proposed by Graves (2016): allows networks to “ponder” variable amounts of time per input.  </li>
<li>Each step computes a halting probability <span class="arithmatex">\(p_t\)</span>; total halt when <span class="arithmatex">\(\sum_t p_t = 1\)</span>.</li>
<li>Output is a weighted sum of intermediate states:
  <script type="math/tex; mode=display">
  y = \sum_t p_t h_t
  </script>
</li>
<li>Encourages efficient use of computation — more steps for harder inputs, fewer for easy ones.</li>
<li>Regularized by a <em>time penalty</em> to avoid overthinking.</li>
</ul>
<h3 id="deeplearning-5_attention-universal-transformers">Universal Transformers<a class="headerlink" href="#deeplearning-5_attention-universal-transformers" title="Permanent link">¶</a></h3>
<ul>
<li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li>
<li>Shares parameters across layers — like an RNN unrolled over depth.</li>
<li>Combine parallel self-attention + iterative refinement + ACT.</li>
<li>Achieves better generalization and adaptive reasoning on sequence tasks.</li>
</ul>
<h2 id="deeplearning-5_attention-summary">Summary<a class="headerlink" href="#deeplearning-5_attention-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Attention = selective processing of relevant information.  </li>
<li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li>
<li>Explicit attention can be hard (sampled) or soft (differentiable).  </li>
<li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li>
<li>Transformers unify attention as the core mechanism — fully parallel, context-rich.  </li>
<li>Adaptive computation gives flexibility in processing time and complexity.</li>
</ul>
<p>Takeaway:<br>
Selective attention and memory — biological inspirations — are now core architectural principles driving modern deep learning.</p></body></html></section><section class="print-page" id="deeplearning-6_gans" heading-number="4.6"><h1 id="deeplearning-6_gans-deeplearning-6_gans">6. Generative Models and GANs</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-overview-generative-models">1. Overview: Generative Models<a class="headerlink" href="#deeplearning-6_gans-1-overview-generative-models" title="Permanent link">¶</a></h2>
<ul>
<li>Goal: learn a model of the true data distribution <span class="arithmatex">\(p^*(x)\)</span> from samples.</li>
</ul>
<h3 id="deeplearning-6_gans-types-of-generative-models">Types of Generative Models<a class="headerlink" href="#deeplearning-6_gans-types-of-generative-models" title="Permanent link">¶</a></h3>
<ol>
<li>Explicit likelihood models – define tractable <span class="arithmatex">\(p_\theta(x)\)</span></li>
<li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li>
<li>
<p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p>
</li>
<li>
<p>Implicit models – define <em>sampling procedure</em>, not explicit <span class="arithmatex">\(p_\theta(x)\)</span>  </p>
</li>
<li>Examples: GANs, Moment Matching Networks.</li>
</ol>
<h2 id="deeplearning-6_gans-11-the-gan-idea">1.1 The GAN Idea<a class="headerlink" href="#deeplearning-6_gans-11-the-gan-idea" title="Permanent link">¶</a></h2>
<ul>
<li>Two-player minimax game:</li>
<li>Generator (G): maps noise <span class="arithmatex">\(z \sim p(z)\)</span> to data space <span class="arithmatex">\(G(z)\)</span>.</li>
<li>
<p>Discriminator (D): classifies samples as <em>real</em> (from <span class="arithmatex">\(p^*(x)\)</span>) or <em>fake</em> (<span class="arithmatex">\(G(z)\)</span>).</p>
</li>
<li>
<p>Objectives:
  <script type="math/tex; mode=display">
  \min_G \max_D \; \mathbb{E}_{x\sim p^*(x)}[\log D(x)] +
  \mathbb{E}_{z\sim p(z)}[\log(1 - D(G(z)))]
  </script>
</p>
</li>
<li>
<p>Interpretation:</p>
</li>
<li><span class="arithmatex">\(D\)</span> learns to distinguish real from fake.</li>
<li><span class="arithmatex">\(G\)</span> learns to fool <span class="arithmatex">\(D\)</span>.</li>
<li>Training reaches equilibrium when <span class="arithmatex">\(p_G(x) = p^*(x)\)</span>.</li>
</ul>
<h2 id="deeplearning-6_gans-12-alternative-view-teacherstudent-analogy">1.2 Alternative View — Teacher–Student Analogy<a class="headerlink" href="#deeplearning-6_gans-12-alternative-view-teacherstudent-analogy" title="Permanent link">¶</a></h2>
<ul>
<li>Teacher (D): distinguishes real vs fake, providing feedback.</li>
<li>Student (G): improves by making fake data look real.</li>
<li>Cooperative interpretation of the adversarial process.</li>
</ul>
<h2 id="deeplearning-6_gans-13-gans-as-a-game">1.3 GANs as a Game<a class="headerlink" href="#deeplearning-6_gans-13-gans-as-a-game" title="Permanent link">¶</a></h2>
<ul>
<li>Zero-sum, bi-level optimization → strong connection to game theory.</li>
<li>GAN equilibrium = Nash equilibrium between <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.</li>
<li>Training alternates between optimizing <span class="arithmatex">\(D\)</span> and <span class="arithmatex">\(G\)</span>.</li>
</ul>
<p>Key Intuition:<br>
GANs learn by <em>competition</em> between a generator and discriminator rather than direct likelihood maximization.</p>
<h2 id="deeplearning-6_gans-2-gan-objective-as-divergence-minimization">2. GAN Objective as Divergence Minimization<a class="headerlink" href="#deeplearning-6_gans-2-gan-objective-as-divergence-minimization" title="Permanent link">¶</a></h2>
<ul>
<li>Generative modeling often aims to minimize a distance or divergence between
  the true data distribution <span class="arithmatex">\(p^*(x)\)</span> and model distribution <span class="arithmatex">\(p_G(x)\)</span>.</li>
</ul>
<h3 id="deeplearning-6_gans-21-kl-and-related-divergences">2.1 KL and Related Divergences<a class="headerlink" href="#deeplearning-6_gans-21-kl-and-related-divergences" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Maximum Likelihood Estimation (MLE):
  <script type="math/tex; mode=display">
  \min_\theta D_{\text{KL}}(p^*(x) \| p_\theta(x))
  </script>
  → drives <span class="arithmatex">\(p_\theta\)</span> to assign high probability to observed data.</p>
</li>
<li>
<p>But: implicit models (like GANs) don’t have explicit likelihoods, so MLE can’t be used directly.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-22-gan-as-jensenshannon-js-divergence-minimization">2.2 GAN as Jensen–Shannon (JS) Divergence Minimization<a class="headerlink" href="#deeplearning-6_gans-22-gan-as-jensenshannon-js-divergence-minimization" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>If discriminator <span class="arithmatex">\(D\)</span> is optimal:
  <script type="math/tex; mode=display">
  D^*(x) = \frac{p^*(x)}{p^*(x) + p_G(x)}
  </script>
  Plugging into the GAN loss shows that the generator minimizes:
  <script type="math/tex; mode=display">
  D_{\text{JS}}(p^*(x) \| p_G(x))
  </script>
  → GAN ≈ JS divergence minimization.</p>
</li>
<li>
<p>However, this relies on an <em>optimal discriminator</em> — not true in practice.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-23-limitations-of-kl-js-divergences">2.3 Limitations of KL / JS Divergences<a class="headerlink" href="#deeplearning-6_gans-23-limitations-of-kl-js-divergences" title="Permanent link">¶</a></h2>
<ul>
<li>If <span class="arithmatex">\(p_G\)</span> and <span class="arithmatex">\(p^*\)</span> have non-overlapping support,<br>
  → no useful gradient signal (zero gradient problem).</li>
<li>The density ratio <span class="arithmatex">\(\frac{p^*(x)}{p_G(x)}\)</span> becomes infinite where <span class="arithmatex">\(p_G=0\)</span>.</li>
<li>Thus, GANs can fail to learn when supports are disjoint.</li>
</ul>
<h2 id="deeplearning-6_gans-24-alternative-distances-divergences">2.4 Alternative Distances &amp; Divergences<a class="headerlink" href="#deeplearning-6_gans-24-alternative-distances-divergences" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-wasserstein-distance-earth-movers">(a) Wasserstein Distance (Earth Mover’s)<a class="headerlink" href="#deeplearning-6_gans-a-wasserstein-distance-earth-movers" title="Permanent link">¶</a></h3>
<ul>
<li>Measures minimal “cost” of moving probability mass:
  <script type="math/tex; mode=display">
  W(p^*, p_G) = \inf_{\gamma \in \Pi(p^*, p_G)} \mathbb{E}_{(x,y)\sim \gamma}[\|x - y\|]
  </script>
</li>
<li>Provides smooth, non-vanishing gradients even when supports don’t overlap.</li>
<li>WGAN: enforce 1-Lipschitz <span class="arithmatex">\(D\)</span> via:</li>
<li>weight clipping,</li>
<li>gradient penalty (WGAN-GP),</li>
<li>spectral normalization.</li>
</ul>
<h3 id="deeplearning-6_gans-b-mmd-maximum-mean-discrepancy">(b) MMD (Maximum Mean Discrepancy)<a class="headerlink" href="#deeplearning-6_gans-b-mmd-maximum-mean-discrepancy" title="Permanent link">¶</a></h3>
<ul>
<li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):
  <script type="math/tex; mode=display">
  \text{MMD}^2(p, q) = \|\mathbb{E}_p[\phi(x)] - \mathbb{E}_q[\phi(x)]\|^2
  </script>
</li>
<li>MMD-GAN: learns kernel features <span class="arithmatex">\(\phi\)</span> jointly with <span class="arithmatex">\(D\)</span>.</li>
</ul>
<h3 id="deeplearning-6_gans-c-f-divergences">(c) f-divergences<a class="headerlink" href="#deeplearning-6_gans-c-f-divergences" title="Permanent link">¶</a></h3>
<ul>
<li>General framework using convex functions <span class="arithmatex">\(f\)</span>:
  <script type="math/tex; mode=display">
  D_f(p \| q) = \mathbb{E}_q[f\!\left(\frac{p(x)}{q(x)}\right)]
  </script>
</li>
<li>GAN training derived via variational lower bound on <span class="arithmatex">\(D_f\)</span>.</li>
</ul>
<h2 id="deeplearning-6_gans-25-practical-view">2.5 Practical View<a class="headerlink" href="#deeplearning-6_gans-25-practical-view" title="Permanent link">¶</a></h2>
<ul>
<li>GANs are not pure divergence minimizers in practice:</li>
<li><span class="arithmatex">\(D\)</span> not optimal → approximate divergence.</li>
<li>Neural discriminator learns a smooth approximation to density ratio.</li>
<li>Provides <em>useful gradients</em> even when the true divergence would fail.</li>
</ul>
<h2 id="deeplearning-6_gans-26-summary-table">2.6 Summary Table<a class="headerlink" href="#deeplearning-6_gans-26-summary-table" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Perspective</th>
<th>Example</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>KL Divergence</td>
<td>MLE, VAEs</td>
<td>Explicit likelihoods</td>
</tr>
<tr>
<td>JS Divergence</td>
<td>Original GAN</td>
<td>Adversarial training</td>
</tr>
<tr>
<td>Wasserstein</td>
<td>WGAN</td>
<td>Smooth gradients</td>
</tr>
<tr>
<td>MMD</td>
<td>MMD-GAN</td>
<td>Kernel mean embedding</td>
</tr>
<tr>
<td>f-divergence</td>
<td>f-GAN</td>
<td>Variational bound family</td>
</tr>
</tbody>
</table>
<p>Insight:<br>
GANs can be viewed as learning a <em>neural divergence measure</em> that provides a stable, informative training signal.</p>
<h2 id="deeplearning-6_gans-3-evaluating-gans">3. Evaluating GANs<a class="headerlink" href="#deeplearning-6_gans-3-evaluating-gans" title="Permanent link">¶</a></h2>
<ul>
<li>Evaluating generative models is difficult — no single metric captures all aspects.</li>
<li>Must assess:</li>
<li>Sample quality (fidelity, realism)</li>
<li>Diversity / generalization</li>
<li>Representation learning (usefulness of learned features)</li>
</ul>
<h2 id="deeplearning-6_gans-31-why-not-log-likelihood">3.1 Why Not Log-Likelihood?<a class="headerlink" href="#deeplearning-6_gans-31-why-not-log-likelihood" title="Permanent link">¶</a></h2>
<ul>
<li>GANs are implicit models — no tractable <span class="arithmatex">\(p(x)\)</span>.</li>
<li>Estimating log-likelihood is expensive and unreliable.</li>
<li>Hence: use feature-based or classifier-based proxies.</li>
</ul>
<h2 id="deeplearning-6_gans-32-inception-score-is">3.2 Inception Score (IS)<a class="headerlink" href="#deeplearning-6_gans-32-inception-score-is" title="Permanent link">¶</a></h2>
<ul>
<li>Uses a pretrained Inception v3 classifier.</li>
<li>Compares predicted label distributions of generated samples.</li>
</ul>
<p>Formula:
<script type="math/tex; mode=display">
\text{IS} = \exp\!\left( \mathbb{E}_{x \sim G} [ D_{KL}(p(y|x) \| p(y)) ] \right)
</script>
</p>
<p>Intuition:
- High-quality images → confident predictions (<span class="arithmatex">\(p(y|x)\)</span> low entropy).<br>
- Diverse images → marginal label distribution <span class="arithmatex">\(p(y)\)</span> high entropy.</p>
<p>Properties:
- Measures <em>sample quality</em> and <em>diversity</em>.
- Correlates with human judgment.
- Fails to capture intra-class variation or features beyond ImageNet classes.</p>
<p>Higher is better.</p>
<h2 id="deeplearning-6_gans-33-frechet-inception-distance-fid">3.3 Fréchet Inception Distance (FID)<a class="headerlink" href="#deeplearning-6_gans-33-frechet-inception-distance-fid" title="Permanent link">¶</a></h2>
<ul>
<li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li>
</ul>
<p>Formula:
<script type="math/tex; mode=display">
\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
</script>
</p>
<p>where <span class="arithmatex">\((\mu_r, \Sigma_r)\)</span> and <span class="arithmatex">\((\mu_g, \Sigma_g)\)</span> are mean and covariance of real and generated data features.</p>
<p>Properties:
- Sensitive to mode dropping and artifacts.
- Correlates strongly with human evaluation.
- Lower is better.
- Biased for small sample sizes → use KID (Kernel Inception Distance) for correction.</p>
<h2 id="deeplearning-6_gans-34-overfitting-check-nearest-neighbours">3.4 Overfitting Check — Nearest Neighbours<a class="headerlink" href="#deeplearning-6_gans-34-overfitting-check-nearest-neighbours" title="Permanent link">¶</a></h2>
<ul>
<li>Compute nearest real images to generated samples in pretrained feature space.</li>
<li>Helps detect memorization (copying training images).</li>
</ul>
<h2 id="deeplearning-6_gans-35-evaluation-depends-on-goal">3.5 Evaluation Depends on Goal<a class="headerlink" href="#deeplearning-6_gans-35-evaluation-depends-on-goal" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Goal</th>
<th>Metric Example</th>
<th>Measures</th>
</tr>
</thead>
<tbody>
<tr>
<td>Image quality</td>
<td>FID, IS</td>
<td>Fidelity &amp; diversity</td>
</tr>
<tr>
<td>Representation learning</td>
<td>Linear probe accuracy</td>
<td>Feature usefulness</td>
</tr>
<tr>
<td>Data generation</td>
<td>Human evaluation</td>
<td>Perceptual quality</td>
</tr>
<tr>
<td>RL / control</td>
<td>Policy reward</td>
<td>Functional realism</td>
</tr>
</tbody>
</table>
<p>Key Takeaway:<br>
Use <em>multiple complementary metrics</em> — quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>
<h2 id="deeplearning-6_gans-4-the-gan-zoo">4. The GAN Zoo<a class="headerlink" href="#deeplearning-6_gans-4-the-gan-zoo" title="Permanent link">¶</a></h2>
<blockquote>
<p>GANs have evolved rapidly — from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>
</blockquote>
<h2 id="deeplearning-6_gans-41-the-original-gan">4.1 The Original GAN<a class="headerlink" href="#deeplearning-6_gans-41-the-original-gan" title="Permanent link">¶</a></h2>
<ul>
<li>First formulation of adversarial training.</li>
<li>Architecture: simple multilayer perceptrons (MLPs).</li>
<li>Trained on small images (e.g. 32×32).  </li>
<li>Ignored spatial structure (flattened pixels).  </li>
<li>Introduced the minimax objective still used today.</li>
</ul>
<h2 id="deeplearning-6_gans-42-conditional-gan">4.2 Conditional GAN<a class="headerlink" href="#deeplearning-6_gans-42-conditional-gan" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Adds <em>conditioning information</em> <span class="arithmatex">\(y\)</span> (e.g. class label or input image).<br>
<script type="math/tex; mode=display">
  \min_G \max_D \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{z,y}[\log(1 - D(G(z,y),y))]
  </script>
</p>
</li>
<li>
<p>Enables controlled generation — specify category or domain.<br>
  Examples:</p>
</li>
<li>Class-conditional image synthesis (e.g., "generate a dog").  </li>
<li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li>
</ul>
<h2 id="deeplearning-6_gans-43-laplacian-gan">4.3 Laplacian GAN<a class="headerlink" href="#deeplearning-6_gans-43-laplacian-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Generates images progressively, starting from low resolution.  </li>
<li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li>
<li>Fully convolutional — can produce arbitrarily large outputs.</li>
<li>Improves high-res synthesis through multi-scale structure.</li>
</ul>
<h2 id="deeplearning-6_gans-44-deep-convolutional-gan">4.4 Deep Convolutional GAN<a class="headerlink" href="#deeplearning-6_gans-44-deep-convolutional-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Replaces MLPs with deep convnets for both <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.</li>
<li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li>
<li>Enables smooth interpolation in latent space:</li>
<li><span class="arithmatex">\(G(z_1)\)</span> → <span class="arithmatex">\(G(\frac{1}{2}(z_1 + z_2))\)</span> → <span class="arithmatex">\(G(z_2)\)</span> produces semantically meaningful transitions.</li>
<li>Latent space exhibits semantic arithmetic (e.g. “man + glasses – woman”).</li>
</ul>
<h2 id="deeplearning-6_gans-45-spectrally-normalized-gan">4.5 Spectrally Normalized GAN<a class="headerlink" href="#deeplearning-6_gans-45-spectrally-normalized-gan" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Enforces 1-Lipschitz constraint on <span class="arithmatex">\(D\)</span> via spectral normalization:
  <script type="math/tex; mode=display">
  \bar{W} = \frac{W}{\sigma_{\max}(W)}
  </script>
  where <span class="arithmatex">\(\sigma_{\max}(W)\)</span> is the largest singular value.</p>
</li>
<li>
<p>Stabilizes training and improves generalization.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-46-projection-discriminator">4.6 Projection Discriminator<a class="headerlink" href="#deeplearning-6_gans-46-projection-discriminator" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Adds class embedding projection inside <span class="arithmatex">\(D\)</span>:<br>
<script type="math/tex; mode=display">
  D(x, y) = h(x)^\top v_y + b_y
  </script>
  where <span class="arithmatex">\(v_y\)</span> is the embedding for class <span class="arithmatex">\(y\)</span>.</p>
</li>
<li>
<p>Theoretically consistent probabilistic discriminator formulation.  </p>
</li>
<li>Strong empirical results on class-conditional image synthesis.</li>
</ul>
<h2 id="deeplearning-6_gans-47-self-attention-gan">4.7 Self-Attention GAN<a class="headerlink" href="#deeplearning-6_gans-47-self-attention-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Introduces self-attention layers to capture long-range dependencies.  </li>
<li>Improves global structure and coherence in generated images.</li>
<li>Inspired by Transformer attention.</li>
</ul>
<h2 id="deeplearning-6_gans-48-biggan">4.8 BigGAN<a class="headerlink" href="#deeplearning-6_gans-48-biggan" title="Permanent link">¶</a></h2>
<ul>
<li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li>
<li>Key ingredients:</li>
<li>Hinge loss for <span class="arithmatex">\(D\)</span>  </li>
<li>Spectral normalization  </li>
<li>Self-attention  </li>
<li>Projection discriminator  </li>
<li>Orthogonal regularization  </li>
<li>Skip connections from noise  </li>
<li>Shared class embeddings  </li>
<li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li>
</ul>
<h2 id="deeplearning-6_gans-49-logan">4.9 LOGAN<a class="headerlink" href="#deeplearning-6_gans-49-logan" title="Permanent link">¶</a></h2>
<ul>
<li>Introduces latent optimization — optimize <span class="arithmatex">\(z\)</span> via gradient updates to improve adversarial dynamics.  </li>
<li>Uses natural gradient descent in latent space.  </li>
<li>Yields higher FID/IS improvements over BigGAN.</li>
</ul>
<h2 id="deeplearning-6_gans-410-progressive-gan">4.10 Progressive GAN<a class="headerlink" href="#deeplearning-6_gans-410-progressive-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Trains from low to high resolution (4×4 → 8×8 → 16×16 …).  </li>
<li>Each stage adds new layers to <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.  </li>
<li>Dramatically improves stability and image quality (especially faces).</li>
</ul>
<h2 id="deeplearning-6_gans-411-stylegan">4.11 StyleGAN<a class="headerlink" href="#deeplearning-6_gans-411-stylegan" title="Permanent link">¶</a></h2>
<ul>
<li>Adds style-based generator architecture:</li>
<li>Latent vector <span class="arithmatex">\(z\)</span> transformed by MLP to intermediate <span class="arithmatex">\(w\)</span>.</li>
<li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li>
<li>
<p>Injects per-pixel noise for local details.</p>
</li>
<li>
<p>Learns disentangled representations — global attributes (style) vs local (texture).</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-412-takeaways">4.12 Takeaways<a class="headerlink" href="#deeplearning-6_gans-412-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>GAN progress driven by:</li>
<li>Better architectures (Conv, Attention, Progressive, Style-based)</li>
<li>Normalization &amp; regularization</li>
<li>Stability techniques</li>
<li>Large-scale training</li>
</ul>
<p>Trend:<br>
From small MLPs → Conv architectures → Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>
<h2 id="deeplearning-6_gans-5-representation-learning-with-gans">5. Representation Learning with GANs<a class="headerlink" href="#deeplearning-6_gans-5-representation-learning-with-gans" title="Permanent link">¶</a></h2>
<blockquote>
<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>
</blockquote>
<h2 id="deeplearning-6_gans-51-motivation">5.1 Motivation<a class="headerlink" href="#deeplearning-6_gans-51-motivation" title="Permanent link">¶</a></h2>
<ul>
<li>GANs implicitly learn latent spaces that capture high-level semantics.</li>
<li>Exploring or constraining this latent space enables unsupervised representation learning.</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-52-evidence-from-dcgan">5.2 Evidence from DCGAN<a class="headerlink" href="#deeplearning-6_gans-52-evidence-from-dcgan" title="Permanent link">¶</a></h2>
<ul>
<li>DCGAN latent vectors encode meaningful directions:</li>
<li>Smooth interpolation between points → semantic transformations.</li>
<li>Linear arithmetic in latent space (e.g., <em>smiling woman – woman + man → smiling man</em>).</li>
<li>Suggests disentangled feature representations emerge naturally.</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-53-infogan">5.3 InfoGAN<a class="headerlink" href="#deeplearning-6_gans-53-infogan" title="Permanent link">¶</a></h2>
<ul>
<li>Extends GAN with information maximization objective:</li>
<li>Encourages some latent codes <span class="arithmatex">\(c\)</span> to be <em>interpretable</em> and <em>disentangled</em>.</li>
</ul>
<p>Objective:
<script type="math/tex; mode=display">
\min_G \max_D V(D,G) - \lambda I(c; G(z, c))
</script>
where <span class="arithmatex">\(I(c; G(z, c))\)</span> is mutual information between latent code and generated output.</p>
<ul>
<li>Adds an auxiliary network to infer <span class="arithmatex">\(c\)</span> from <span class="arithmatex">\(G(z, c)\)</span>.</li>
<li>Learns to associate:</li>
<li>Discrete codes → categories (digits, shapes)</li>
<li>Continuous codes → attributes (rotation, scale)</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-54-ali-bigan">5.4 ALI / BiGAN<a class="headerlink" href="#deeplearning-6_gans-54-ali-bigan" title="Permanent link">¶</a></h2>
<ul>
<li>Adds an encoder <span class="arithmatex">\(E(x)\)</span> mapping real data to latent space.</li>
<li>
<p>Joint discriminator distinguishes pairs:
  <script type="math/tex; mode=display">
  (x, E(x)) \quad \text{vs.} \quad (G(z), z)
  </script>
</p>
</li>
<li>
<p>At equilibrium:</p>
</li>
<li>
<p><span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(G\)</span> become approximate inverses:</p>
<ul>
<li><span class="arithmatex">\(x \approx G(E(x))\)</span></li>
<li><span class="arithmatex">\(z \approx E(G(z))\)</span></li>
</ul>
</li>
<li>
<p>Enables inference and representation learning simultaneously.</p>
</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-55-bigbigan">5.5 BigBiGAN<a class="headerlink" href="#deeplearning-6_gans-55-bigbigan" title="Permanent link">¶</a></h2>
<ul>
<li>Scales BiGAN to BigGAN architecture.</li>
<li>Uses large-scale encoders (<span class="arithmatex">\(E\)</span>) with ResNet blocks.</li>
<li>Learns strong unsupervised representations competitive with self-supervised models.</li>
</ul>
<p>Observations:
- Reconstructions <span class="arithmatex">\(G(E(x))\)</span> preserve semantic content, not exact pixels.
- Encoder features yield high ImageNet classification accuracy after linear probing.</p>
<hr>
<h2 id="deeplearning-6_gans-56-summary">5.6 Summary<a class="headerlink" href="#deeplearning-6_gans-56-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Key Idea</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr>
<td>DCGAN</td>
<td>Implicitly semantic latent space</td>
<td>Interpolations meaningful</td>
</tr>
<tr>
<td>InfoGAN</td>
<td>Maximize info between codes and outputs</td>
<td>Disentangled features</td>
</tr>
<tr>
<td>BiGAN / ALI</td>
<td>Add encoder, joint training</td>
<td>Bidirectional mapping</td>
</tr>
<tr>
<td>BigBiGAN</td>
<td>Large-scale BiGAN</td>
<td>Competitive unsupervised features</td>
</tr>
</tbody>
</table>
<p>Key Insight:<br>
GANs not only <em>generate</em>, but also <em>encode</em> — their latent structure can act as a rich, learned representation space.</p>
<h2 id="deeplearning-6_gans-6-gans-for-other-modalities-and-problems">6. GANs for Other Modalities and Problems<a class="headerlink" href="#deeplearning-6_gans-6-gans-for-other-modalities-and-problems" title="Permanent link">¶</a></h2>
<blockquote>
<p>GANs extend far beyond images — used for translation, audio, video, RL, and even art.</p>
</blockquote>
<hr>
<h2 id="deeplearning-6_gans-61-image-to-image-translation">6.1 Image-to-Image Translation<a class="headerlink" href="#deeplearning-6_gans-61-image-to-image-translation" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-pix2pix">(a) Pix2Pix<a class="headerlink" href="#deeplearning-6_gans-a-pix2pix" title="Permanent link">¶</a></h3>
<ul>
<li>Conditional GAN trained on <em>paired</em> datasets <span class="arithmatex">\((x, y)\)</span>.</li>
<li>Learns deterministic mapping between domains (e.g., edges → photos).</li>
<li>Loss combines adversarial term + L1 reconstruction:
  <script type="math/tex; mode=display">
  \mathcal{L}_{\text{Pix2Pix}} = \mathcal{L}_{\text{GAN}}(G,D) + \lambda \|y - G(x)\|_1
  </script>
</li>
</ul>
<h3 id="deeplearning-6_gans-b-cyclegan">(b) CycleGAN<a class="headerlink" href="#deeplearning-6_gans-b-cyclegan" title="Permanent link">¶</a></h3>
<ul>
<li>Unpaired domain translation — no 1:1 correspondence.</li>
<li>Uses cycle consistency:</li>
<li><span class="arithmatex">\(x \in A \to G_B(x) \to F_A(G_B(x)) \approx x\)</span></li>
<li>Enforces invertibility between domains.</li>
<li>Enables tasks like <em>horse ↔ zebra</em>, <em>summer ↔ winter</em>.</li>
</ul>
<h2 id="deeplearning-6_gans-62-audio-synthesis">6.2 Audio Synthesis<a class="headerlink" href="#deeplearning-6_gans-62-audio-synthesis" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-wavegan">(a) WaveGAN<a class="headerlink" href="#deeplearning-6_gans-a-wavegan" title="Permanent link">¶</a></h3>
<ul>
<li>Adapts convolutional GANs to 1D waveforms.</li>
<li>Fully unsupervised raw-audio synthesis.</li>
</ul>
<h3 id="deeplearning-6_gans-b-melgan">(b) MelGAN<a class="headerlink" href="#deeplearning-6_gans-b-melgan" title="Permanent link">¶</a></h3>
<ul>
<li>Conditional GAN trained to generate mel-spectrogram waveforms.</li>
<li>Used in text-to-speech (GAN-TTS).</li>
</ul>
<h3 id="deeplearning-6_gans-c-gan-tts">(c) GAN-TTS<a class="headerlink" href="#deeplearning-6_gans-c-gan-tts" title="Permanent link">¶</a></h3>
<ul>
<li>High-fidelity speech synthesis model.</li>
<li>Achieves human-like audio quality via adversarial losses.</li>
</ul>
<h2 id="deeplearning-6_gans-63-video-synthesis-prediction">6.3 Video Synthesis &amp; Prediction<a class="headerlink" href="#deeplearning-6_gans-63-video-synthesis-prediction" title="Permanent link">¶</a></h2>
<ul>
<li>GANs extended to spatiotemporal data:</li>
<li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li>
<li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li>
<li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li>
</ul>
<h2 id="deeplearning-6_gans-64-gans-in-reinforcement-learning-imitation-control">6.4 GANs in Reinforcement Learning (Imitation &amp; Control)<a class="headerlink" href="#deeplearning-6_gans-64-gans-in-reinforcement-learning-imitation-control" title="Permanent link">¶</a></h2>
<ul>
<li>GAIL (Ho &amp; Ermon, 2016): <em>Generative Adversarial Imitation Learning</em>  </li>
<li>Discriminator distinguishes expert vs policy trajectories.</li>
<li>Generator = policy network optimizing to mimic experts.</li>
</ul>
<h2 id="deeplearning-6_gans-65-creative-applied-uses">6.5 Creative &amp; Applied Uses<a class="headerlink" href="#deeplearning-6_gans-65-creative-applied-uses" title="Permanent link">¶</a></h2>
<ul>
<li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li>
<li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li>
<li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li>
<li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li>
<li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li>
</ul>
<h2 id="deeplearning-6_gans-66-summary">6.6 Summary<a class="headerlink" href="#deeplearning-6_gans-66-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Example</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paired image translation</td>
<td>Pix2Pix</td>
<td>Conditional GAN + L1 loss</td>
</tr>
<tr>
<td>Unpaired translation</td>
<td>CycleGAN</td>
<td>Cycle consistency</td>
</tr>
<tr>
<td>Audio</td>
<td>MelGAN, WaveGAN</td>
<td>Conditional waveform generation</td>
</tr>
<tr>
<td>Video</td>
<td>DVD-GAN, TGAN-v2</td>
<td>Temporal adversarial modeling</td>
</tr>
<tr>
<td>RL / Imitation</td>
<td>GAIL</td>
<td>Adversarial trajectory matching</td>
</tr>
<tr>
<td>Art / Creativity</td>
<td>GauGAN, SPIRAL</td>
<td>Adversarial synthesis and style transfer</td>
</tr>
</tbody>
</table>
<p>Insight:<br>
Adversarial learning generalizes across domains — GANs serve as a <em>universal generator–critic framework</em> for structured data.</p></body></html></section><section class="print-page" id="deeplearning-7_unsuper" heading-number="4.7"><h1 id="deeplearning-7_unsuper-deeplearning-7_unsuper">7. Unsupervised and Self-Supervised Learning</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-what-is-unsupervised-learning">1. What is Unsupervised Learning?<a class="headerlink" href="#deeplearning-7_unsuper-1-what-is-unsupervised-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-definition">Definition<a class="headerlink" href="#deeplearning-7_unsuper-definition" title="Permanent link">¶</a></h3>
<ul>
<li>Goal: discover structure in data without explicit labels or rewards.  </li>
<li>Learns a compact, informative representation of input data.</li>
</ul>
<table>
<thead>
<tr>
<th>Learning Type</th>
<th>Goal</th>
<th>Supervision</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised</td>
<td>Map inputs → labels</td>
<td>Requires labeled data</td>
</tr>
<tr>
<td>Reinforcement</td>
<td>Learn actions maximizing future reward</td>
<td>Requires reward signal</td>
</tr>
<tr>
<td>Unsupervised</td>
<td>Find hidden structure</td>
<td>No labels or rewards</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-core-ideas">Core Ideas<a class="headerlink" href="#deeplearning-7_unsuper-core-ideas" title="Permanent link">¶</a></h3>
<ul>
<li>Model latent structure or relationships between observations.  </li>
<li>Examples:</li>
<li>Clustering: group similar data points.  </li>
<li>Dimensionality reduction: project data to low-dimensional latent space.  </li>
<li>Manifold learning / disentangling: uncover independent factors of variation.</li>
</ul>
<h3 id="deeplearning-7_unsuper-evaluation-challenges">Evaluation Challenges<a class="headerlink" href="#deeplearning-7_unsuper-evaluation-challenges" title="Permanent link">¶</a></h3>
<p>How do we know if unsupervised learning worked?</p>
<ul>
<li>Ambiguity of structure: multiple valid clusterings possible.<br>
  e.g., cluster by <em>leg count</em>, <em>arm number</em>, or <em>height</em> in robot dataset.</li>
<li>Metrics depend on downstream use:<br>
  useful representations should improve data efficiency, generalization, or transfer.</li>
</ul>
<h3 id="deeplearning-7_unsuper-classic-methods">Classic Methods<a class="headerlink" href="#deeplearning-7_unsuper-classic-methods" title="Permanent link">¶</a></h3>
<ul>
<li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li>
<li>ICA (Independent Component Analysis): separates statistically independent components.  </li>
<li>Modern goal: move beyond orthogonality → learn <em>disentangled</em> factors.</li>
</ul>
<h3 id="deeplearning-7_unsuper-summary">Summary<a class="headerlink" href="#deeplearning-7_unsuper-summary" title="Permanent link">¶</a></h3>
<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself — forming the foundation for <em>representation learning</em>.</p>
<h2 id="deeplearning-7_unsuper-2-why-is-unsupervised-learning-important">2. Why is Unsupervised Learning Important?<a class="headerlink" href="#deeplearning-7_unsuper-2-why-is-unsupervised-learning-important" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-21-historical-context-of-representation-learning">2.1 Historical Context of Representation Learning<a class="headerlink" href="#deeplearning-7_unsuper-21-historical-context-of-representation-learning" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Era</th>
<th>Key Milestone</th>
<th>Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td>1950s–2000s</td>
<td>Arthur Samuel (1959): <em>Machine Learning</em> coined</td>
<td>Feature engineering, clustering</td>
</tr>
<tr>
<td>2000s</td>
<td>Kernel methods (Hofmann et al., 2008)</td>
<td>Hand-crafted similarity functions</td>
</tr>
<tr>
<td>2006</td>
<td>Hinton &amp; Salakhutdinov: <em>RBMs &amp; Autoencoders</em></td>
<td>Layer-wise unsupervised pretraining</td>
</tr>
<tr>
<td>2012</td>
<td>Krizhevsky et al.: <em>AlexNet</em></td>
<td>End-to-end supervised learning dominates</td>
</tr>
</tbody>
</table>
<ul>
<li>Progress came from more data, deeper models, and better hardware — but not necessarily more efficient learning.</li>
</ul>
<h3 id="deeplearning-7_unsuper-22-limitations-of-purely-supervised-learning">2.2 Limitations of Purely Supervised Learning<a class="headerlink" href="#deeplearning-7_unsuper-22-limitations-of-purely-supervised-learning" title="Permanent link">¶</a></h3>
<p>Supervised models are:
- Data inefficient — need millions of labeled samples.
- Brittle — vulnerable to adversarial perturbations.
- Poor at transfer — struggle with new domains or tasks.
- Lack common sense — limited abstraction and reasoning.</p>
<h3 id="deeplearning-7_unsuper-23-evidence-of-current-gaps">2.3 Evidence of Current Gaps<a class="headerlink" href="#deeplearning-7_unsuper-23-evidence-of-current-gaps" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Example</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data efficiency</td>
<td>Learning from few examples</td>
<td>Lake et al. (2017)</td>
</tr>
<tr>
<td>Robustness</td>
<td>Adversarial examples, brittle decisions</td>
<td>Goodfellow et al. (2015)</td>
</tr>
<tr>
<td>Generalization</td>
<td>CoinRun, DMLab-30</td>
<td>Cobbe (2018), DeepMind</td>
</tr>
<tr>
<td>Transfer</td>
<td>Schema Networks</td>
<td>Kansky et al. (2017)</td>
</tr>
<tr>
<td>Common sense</td>
<td>Conceptual reasoning</td>
<td>Lake et al. (2015)</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-24-why-unsupervised-learning-matters">2.4 Why Unsupervised Learning Matters<a class="headerlink" href="#deeplearning-7_unsuper-24-why-unsupervised-learning-matters" title="Permanent link">¶</a></h3>
<ul>
<li>Enables data-efficient adaptation to new tasks.</li>
<li>Provides robust, generalizable features.</li>
<li>Promotes transfer learning by separating invariant factors.</li>
<li>Encourages abstract reasoning and causal understanding.</li>
</ul>
<h3 id="deeplearning-7_unsuper-25-towards-general-ai">2.5 Towards General AI<a class="headerlink" href="#deeplearning-7_unsuper-25-towards-general-ai" title="Permanent link">¶</a></h3>
<p>Unsupervised learning provides shared representations enabling:
- Rapid multi-task adaptation.<br>
- Reuse across vision, language, and control.<br>
- Reduced supervision in real-world learning.</p>
<p>Summary:<br>
Unsupervised representation learning addresses the core limits of current AI — aiming for <em>data efficiency, robustness, generalization, transfer,</em> and <em>common sense</em>.</p>
<h2 id="deeplearning-7_unsuper-3-what-makes-a-good-representation">3. What Makes a Good Representation?<a class="headerlink" href="#deeplearning-7_unsuper-3-what-makes-a-good-representation" title="Permanent link">¶</a></h2>
<blockquote>
<p>A representation is an internal model of the world — an abstraction that makes reasoning and prediction efficient.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-31-what-is-a-representation">3.1 What is a Representation?<a class="headerlink" href="#deeplearning-7_unsuper-31-what-is-a-representation" title="Permanent link">¶</a></h3>
<blockquote>
<p>“A formal system for making explicit certain entities or types of information, together with a specification of how the system does this.”</p>
</blockquote>
<ul>
<li>Represents <em>information about the world</em> in a way useful for computation.  </li>
<li>Not about a single feature, but the geometry or manifold shape in representational space.</li>
</ul>
<h3 id="deeplearning-7_unsuper-32-why-representation-form-matters">3.2 Why Representation Form Matters<a class="headerlink" href="#deeplearning-7_unsuper-32-why-representation-form-matters" title="Permanent link">¶</a></h3>
<ul>
<li>Determines which computations are easy.  </li>
<li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li>
</ul>
<h3 id="deeplearning-7_unsuper-33-desirable-properties">3.3 Desirable Properties<a class="headerlink" href="#deeplearning-7_unsuper-33-desirable-properties" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Intuition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Untangling</td>
<td>Simplifies complex input manifolds</td>
<td>Enables linear decoding</td>
</tr>
<tr>
<td>Attention</td>
<td>Allows selective focus on relevant factors</td>
<td>Supports task-specific filtering</td>
</tr>
<tr>
<td>Clustering</td>
<td>Groups similar experiences together</td>
<td>Facilitates generalization</td>
</tr>
<tr>
<td>Latent Information</td>
<td>Encodes hidden or inferred causes</td>
<td>Predicts unobserved aspects</td>
</tr>
<tr>
<td>Compositionality</td>
<td>Builds complex concepts from simple parts</td>
<td>Enables open-ended reasoning</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-34-information-bottleneck-principle">3.4 Information Bottleneck Principle<a class="headerlink" href="#deeplearning-7_unsuper-34-information-bottleneck-principle" title="Permanent link">¶</a></h3>
<ul>
<li>Good representations compress inputs while preserving information about outputs.
  <script type="math/tex; mode=display">
  \max I(Z; Y) - \beta I(Z; X)
  </script>
</li>
<li>Encourages minimal sufficient representations — compact yet predictive.</li>
</ul>
<h2 id="deeplearning-7_unsuper-4-evaluating-the-merit-of-a-representation">4. Evaluating the Merit of a Representation<a class="headerlink" href="#deeplearning-7_unsuper-4-evaluating-the-merit-of-a-representation" title="Permanent link">¶</a></h2>
<blockquote>
<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-41-the-evaluation-challenge">4.1 The Evaluation Challenge<a class="headerlink" href="#deeplearning-7_unsuper-41-the-evaluation-challenge" title="Permanent link">¶</a></h3>
<ul>
<li>No single metric defines a “good” representation.</li>
<li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li>
</ul>
<p>Representations should enable:
- Data efficiency — learn new tasks from few examples.<br>
- Robustness — resist noise or perturbations.<br>
- Generalization — perform well on new data.<br>
- Transfer — reuse knowledge in new settings.<br>
- Common sense — support reasoning and abstraction.</p>
<h3 id="deeplearning-7_unsuper-42-example-evaluating-representations-via-symmetries">4.2 Example: Evaluating Representations via Symmetries<a class="headerlink" href="#deeplearning-7_unsuper-42-example-evaluating-representations-via-symmetries" title="Permanent link">¶</a></h3>
<p>Let:
- <span class="arithmatex">\(W\)</span> = world space<br>
- <span class="arithmatex">\(Z\)</span> = representational space<br>
- <span class="arithmatex">\(G = G_x \times G_y \times G_c\)</span> = group of transformations (e.g., position, color)</p>
<p>A good representation <span class="arithmatex">\(f: W \rightarrow Z\)</span> should satisfy:
<script type="math/tex; mode=display">
f(g \cdot w) = g' \cdot f(w), \quad \forall g \in G, w \in W
</script>
</p>
<p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space → equivariance.</p>
<h3 id="deeplearning-7_unsuper-43-desirable-evaluation-criteria">4.3 Desirable Evaluation Criteria<a class="headerlink" href="#deeplearning-7_unsuper-43-desirable-evaluation-criteria" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Criterion</th>
<th>Desired Property</th>
<th>Example / Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>Equivariance</td>
<td>Transformations map consistently</td>
<td>Translation → shift in latent</td>
</tr>
<tr>
<td>Compositionality</td>
<td>Combine factors to form new concepts</td>
<td>Modular latent factors</td>
</tr>
<tr>
<td>Metric structure</td>
<td>Smooth distances reflect similarity</td>
<td><span class="arithmatex">\(L_2\)</span>, cosine</td>
</tr>
<tr>
<td>Attention</td>
<td>Selectively focus on task-relevant parts</td>
<td>Masking or gating mechanisms</td>
</tr>
<tr>
<td>Symmetries</td>
<td>Invariance to irrelevant transformations</td>
<td>Rotation, scale invariance</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-44-downstream-evaluation-tasks">4.4 Downstream Evaluation Tasks<a class="headerlink" href="#deeplearning-7_unsuper-44-downstream-evaluation-tasks" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Evaluation Setting</th>
<th>Example Task</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Perception / Control</td>
<td>Predict object color or position</td>
<td>Gens &amp; Domingos, <em>Deep Symmetry Networks</em> (2014)</td>
</tr>
<tr>
<td>Robustness</td>
<td>Classify images under adversarial noise</td>
<td>Gowal et al., 2019</td>
</tr>
<tr>
<td>Sequential Attention</td>
<td>Learn task-focused vision</td>
<td>Zoran et al., 2020</td>
</tr>
<tr>
<td>Transfer / RL</td>
<td>Zero-shot navigation (DARLA)</td>
<td>Higgins et al., ICML 2017</td>
</tr>
<tr>
<td>Lifelong Learning</td>
<td>Maintain latent structure over domains</td>
<td>Achille et al., NeurIPS 2018</td>
</tr>
<tr>
<td>Reasoning / Imagination</td>
<td>Compositional concept inference</td>
<td>Lake et al., <em>Science</em> 2015; Higgins et al., <em>ICLR</em> 2018</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-45-why-evaluation-matters">4.5 Why Evaluation Matters<a class="headerlink" href="#deeplearning-7_unsuper-45-why-evaluation-matters" title="Permanent link">¶</a></h3>
<p>A good representation supports simple mappings to downstream tasks:
- Linear classifiers for vision tasks (e.g., color or position recognition).<br>
- Efficient policy learning in RL with fewer samples.<br>
- Abstract reasoning and imagination — <em>“If rainbow elephants live in big cities, can we expect one in London?”</em></p>
<h2 id="deeplearning-7_unsuper-5-representation-learning-techniques">5. Representation Learning Techniques<a class="headerlink" href="#deeplearning-7_unsuper-5-representation-learning-techniques" title="Permanent link">¶</a></h2>
<blockquote>
<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches — all aiming to extract structure from data without labels.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-51-categories-of-methods">5.1 Categories of Methods<a class="headerlink" href="#deeplearning-7_unsuper-51-categories-of-methods" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>Core Idea</th>
<th>Typical Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generative Modeling</td>
<td>Learn <span class="arithmatex">\(p(x)\)</span> or a model that can <em>reconstruct</em> data</td>
<td>VAE, β-VAE, MONet, GQN, GANs</td>
</tr>
<tr>
<td>Contrastive Learning</td>
<td>Learn by <em>discriminating</em> similar vs dissimilar samples</td>
<td>CPC, SimCLR, word2vec</td>
</tr>
<tr>
<td>Self-Supervised Learning</td>
<td>Design <em>pretext tasks</em> that predict missing or reordered parts</td>
<td>BERT, Colorization, Context Prediction</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-7_unsuper-52-generative-modeling">5.2 Generative Modeling<a class="headerlink" href="#deeplearning-7_unsuper-52-generative-modeling" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-521-motivation">5.2.1 Motivation<a class="headerlink" href="#deeplearning-7_unsuper-521-motivation" title="Permanent link">¶</a></h3>
<ul>
<li>Goal: learn the underlying data distribution <span class="arithmatex">\(p(x)\)</span> to reveal hidden structure and causal factors.  </li>
<li>Unsupervised generative modeling captures common regularities in data — enabling representation learning, synthesis, and reasoning.  </li>
<li>Instead of directly memorizing examples, the model learns a probabilistic process that could have <em>generated</em> them.</li>
</ul>
<blockquote>
<p>Generative models explain the data by learning <em>how it might have arisen.</em></p>
</blockquote>
<h3 id="deeplearning-7_unsuper-522-from-maximum-likelihood-to-latent-variable-models">5.2.2 From Maximum Likelihood to Latent Variable Models<a class="headerlink" href="#deeplearning-7_unsuper-522-from-maximum-likelihood-to-latent-variable-models" title="Permanent link">¶</a></h3>
<h4 id="deeplearning-7_unsuper-maximum-likelihood-principle">Maximum Likelihood Principle<a class="headerlink" href="#deeplearning-7_unsuper-maximum-likelihood-principle" title="Permanent link">¶</a></h4>
<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:
<script type="math/tex; mode=display">
\mathbb{E}_{p^*(x)}[\log p_\theta(x)]
</script>
where <span class="arithmatex">\(p^*(x)\)</span> is the true data distribution and <span class="arithmatex">\(p_\theta(x)\)</span> is the model.</p>
<h4 id="deeplearning-7_unsuper-latent-variable-formulation">Latent Variable Formulation<a class="headerlink" href="#deeplearning-7_unsuper-latent-variable-formulation" title="Permanent link">¶</a></h4>
<ul>
<li>Assume data arises from hidden (latent) variables <span class="arithmatex">\(z\)</span>:
  <script type="math/tex; mode=display">
  \log p_\theta(x) = \log \int p_\theta(x|z)\,p(z)\,dz
  </script>
</li>
<li>Here:</li>
<li><span class="arithmatex">\(p(z)\)</span> — prior over latent variables (e.g., <span class="arithmatex">\(\mathcal{N}(0, I)\)</span>)  </li>
<li><span class="arithmatex">\(p_\theta(x|z)\)</span> — likelihood or <em>decoder</em> mapping latent codes to data</li>
</ul>
<p>This defines a latent variable model:<br>
the data-generating process maps from a <em>low-dimensional latent space</em> to the observed space.</p>
<h3 id="deeplearning-7_unsuper-523-inference-in-latent-variable-models">5.2.3 Inference in Latent Variable Models<a class="headerlink" href="#deeplearning-7_unsuper-523-inference-in-latent-variable-models" title="Permanent link">¶</a></h3>
<p>Goal: infer the posterior
<script type="math/tex; mode=display">
p(z|x) = \frac{p_\theta(x|z)p(z)}{p_\theta(x)}
</script>
to identify which latent factors <span class="arithmatex">\(z\)</span> most likely generated observation <span class="arithmatex">\(x\)</span>.</p>
<ul>
<li>Intuition:<br>
  Recover the underlying causes that explain the data — along with uncertainty estimates.</li>
<li>Problem:<br>
  Computing <span class="arithmatex">\(p(z|x)\)</span> is often intractable, since <span class="arithmatex">\(p_\theta(x)\)</span> involves integrating over all <span class="arithmatex">\(z\)</span>.<br>
  → We must approximate inference using neural networks.</li>
</ul>
<p>Thus, generative models combine:</p>
<ul>
<li>Generation: <span class="arithmatex">\(z \rightarrow x\)</span> (decode latent causes into data)</li>
<li>Inference: <span class="arithmatex">\(x \rightarrow z\)</span> (encode data into latent causes)</li>
</ul>
<h3 id="deeplearning-7_unsuper-524-variational-autoencoders-vaes">5.2.4 Variational Autoencoders (VAEs)<a class="headerlink" href="#deeplearning-7_unsuper-524-variational-autoencoders-vaes" title="Permanent link">¶</a></h3>
<p>To make inference tractable, VAEs introduce an approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span> and optimize a variational bound on the likelihood:</p>
<h4 id="deeplearning-7_unsuper-evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)<a class="headerlink" href="#deeplearning-7_unsuper-evidence-lower-bound-elbo" title="Permanent link">¶</a></h4>
<div class="arithmatex">\[
\log p_\theta(x)
\ge 
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
- D_{KL}[q_\phi(z|x)\,||\,p(z)]
\]</div>
<h4 id="deeplearning-7_unsuper-terms">Terms<a class="headerlink" href="#deeplearning-7_unsuper-terms" title="Permanent link">¶</a></h4>
<ol>
<li>
<p>Reconstruction term<br>
   Encourages the model to faithfully reproduce the input from its latent code.</p>
</li>
<li>
<p>KL divergence term<br>
   Regularizes the latent posterior to match the prior — ensuring smoothness and preventing overfitting.</p>
</li>
</ol>
<h4 id="deeplearning-7_unsuper-neural-implementation">Neural Implementation<a class="headerlink" href="#deeplearning-7_unsuper-neural-implementation" title="Permanent link">¶</a></h4>
<ul>
<li>Encoder <span class="arithmatex">\(q_\phi(z|x)\)</span>: approximates inference (maps data → latent code).  </li>
<li>Decoder <span class="arithmatex">\(p_\theta(x|z)\)</span>: generates data from the latent space (latent → data).  </li>
<li>Both are parameterized by deep neural networks.</li>
</ul>
<p>Reparameterization trick (Kingma &amp; Welling, 2014):<br>
<script type="math/tex; mode=display">
z = \mu_\phi(x) + \sigma_\phi(x)\odot\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
</script>
enables backpropagation through stochastic latent sampling.</p>
<h4 id="deeplearning-7_unsuper-why-vaes-matter">Why VAEs Matter<a class="headerlink" href="#deeplearning-7_unsuper-why-vaes-matter" title="Permanent link">¶</a></h4>
<ul>
<li>Provide continuous, structured latent spaces capturing generative factors.  </li>
<li>Support smooth interpolation and semantic manipulation.  </li>
<li>Foundation for disentangled and interpretable representation learning (e.g., β-VAE).  </li>
<li>Bridge probabilistic modeling with deep learning.</li>
</ul>
<blockquote>
<p>VAEs turn probabilistic inference into a scalable neural optimization problem — the cornerstone of modern generative representation learning.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-524-vae">5.2.4 β-VAE<a class="headerlink" href="#deeplearning-7_unsuper-524-vae" title="Permanent link">¶</a></h3>
<ul>
<li>Adds weight β to KL term:
  <script type="math/tex; mode=display">
  \mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta D_{KL}[q(z|x)||p(z)]
  </script>
</li>
<li>Encourages disentangled latent factors (position, shape, rotation, color).</li>
<li>
<p>Provides interpretable, semantically meaningful representations.</p>
</li>
<li>
<p>DARLA (Higgins et al., 2017): β-VAE for reinforcement learning → improved transfer and sim2real generalization.</p>
</li>
</ul>
<h3 id="deeplearning-7_unsuper-525-sequential-and-layered-models">5.2.5 Sequential and Layered Models<a class="headerlink" href="#deeplearning-7_unsuper-525-sequential-and-layered-models" title="Permanent link">¶</a></h3>
<p>ConvDRAW (Gregor et al., 2016)<br>
- Sequential VAE with recurrent refinement.<br>
- Models temporal and spatial dependencies.</p>
<p>MONet (Burgess et al., 2019)<br>
- Attention-based scene decomposition.<br>
- Each latent corresponds to one object → compositional representations.<br>
- Enables object-centric reasoning and RL transfer.</p>
<p>GQN (Eslami et al., 2018)<br>
- <em>Generative Query Networks</em>: learn neural scene representations.<br>
- Given partial observations, predict unseen viewpoints (3D reasoning).</p>
<p>VQ-VAE (van den Oord et al., 2017)<br>
- Learns discrete latent variables via vector quantization.<br>
- Enables hierarchical or symbolic structure.<br>
- Useful for speech, images, and video.
-</p>
<h3 id="deeplearning-7_unsuper-526-gans-goodfellow-et-al-2014">5.2.6 GANs (Goodfellow et al., 2014)<a class="headerlink" href="#deeplearning-7_unsuper-526-gans-goodfellow-et-al-2014" title="Permanent link">¶</a></h3>
<ul>
<li>Implicit generative models — learn by adversarial game:</li>
<li>Generator creates samples.</li>
<li>Discriminator provides learning signal (no reconstruction loss).</li>
<li>BigBiGAN (Donahue et al., 2019):</li>
<li>Adds encoder for inference.</li>
<li>Learns rich, high-level representations → SOTA semi-supervised performance on ImageNet.</li>
</ul>
<h3 id="deeplearning-7_unsuper-527-large-scale-generative-models">5.2.7 Large-Scale Generative Models<a class="headerlink" href="#deeplearning-7_unsuper-527-large-scale-generative-models" title="Permanent link">¶</a></h3>
<ul>
<li>GPT (Radford et al., 2019):  </li>
<li>Large transformer trained via language modeling.</li>
<li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li>
</ul>
<h2 id="deeplearning-7_unsuper-53-contrastive-learning">5.3 Contrastive Learning<a class="headerlink" href="#deeplearning-7_unsuper-53-contrastive-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-core-idea">Core Idea<a class="headerlink" href="#deeplearning-7_unsuper-core-idea" title="Permanent link">¶</a></h3>
<ul>
<li>No need to model <span class="arithmatex">\(p(x)\)</span> explicitly.</li>
<li>Learn representations that maximize mutual information between related samples.</li>
</ul>
<h3 id="deeplearning-7_unsuper-531-word2vec-mikolov-et-al-2013">5.3.1 word2vec (Mikolov et al., 2013)<a class="headerlink" href="#deeplearning-7_unsuper-531-word2vec-mikolov-et-al-2013" title="Permanent link">¶</a></h3>
<ul>
<li>Predict context words given a target word.  </li>
<li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li>
<li>Learns semantic embeddings; supports few-shot translation.</li>
</ul>
<h3 id="deeplearning-7_unsuper-532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018">5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)<a class="headerlink" href="#deeplearning-7_unsuper-532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018" title="Permanent link">¶</a></h3>
<ul>
<li>Maximize mutual information between current representation and future observations.  </li>
<li>Trains a classifier to distinguish real future samples from negatives.  </li>
<li>
<p>Learns features useful across modalities (vision, speech).</p>
</li>
<li>
<p>Data-efficient Image Recognition (Hénaff et al., 2019):<br>
  contrastive features outperform pixel-level training in low-data regimes.</p>
</li>
</ul>
<h3 id="deeplearning-7_unsuper-533-simclr-chen-et-al-2020">5.3.3 SimCLR (Chen et al., 2020)<a class="headerlink" href="#deeplearning-7_unsuper-533-simclr-chen-et-al-2020" title="Permanent link">¶</a></h3>
<ul>
<li>Simple, scalable contrastive framework:</li>
<li>Generate two augmented views of the same image.</li>
<li>Maximize agreement via contrastive loss (NT-Xent).</li>
<li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li>
<li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li>
</ul>
<h2 id="deeplearning-7_unsuper-54-self-supervised-learning">5.4 Self-Supervised Learning<a class="headerlink" href="#deeplearning-7_unsuper-54-self-supervised-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-idea">Idea<a class="headerlink" href="#deeplearning-7_unsuper-idea" title="Permanent link">¶</a></h3>
<ul>
<li>Design <em>pretext tasks</em> that use natural structure in data as supervision.  </li>
<li>Representations are deterministic and transferable to new tasks.</li>
</ul>
<h3 id="deeplearning-7_unsuper-541-examples">5.4.1 Examples<a class="headerlink" href="#deeplearning-7_unsuper-541-examples" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Description</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Colorization</td>
<td>Predict color from grayscale image</td>
<td>Zhang et al., 2016</td>
</tr>
<tr>
<td>Context Prediction</td>
<td>Predict position of image patches</td>
<td>Doersch et al., 2015</td>
</tr>
<tr>
<td>Sequence Sorting</td>
<td>Predict correct frame order in videos</td>
<td>Lee et al., 2017</td>
</tr>
<tr>
<td>BERT (Devlin et al., 2019)</td>
<td>Masked language modeling + next sentence prediction</td>
<td>Revolutionized NLP</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-542-key-benefits">5.4.2 Key Benefits<a class="headerlink" href="#deeplearning-7_unsuper-542-key-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>Requires no labels — just structure in data.  </li>
<li>Produces general features useful for:</li>
<li>Semi-supervised classification  </li>
<li>Transfer learning  </li>
<li>Downstream reasoning tasks</li>
</ul>
<h3 id="deeplearning-7_unsuper-55-design-principles">5.5 Design Principles<a class="headerlink" href="#deeplearning-7_unsuper-55-design-principles" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Consideration</th>
<th>Desired Property</th>
</tr>
</thead>
<tbody>
<tr>
<td>Modality</td>
<td>Align architecture with data type (image, text, audio)</td>
</tr>
<tr>
<td>Task Design</td>
<td>Choose pretext that aligns with useful features</td>
</tr>
<tr>
<td>Consistency</td>
<td>Maintain temporal/spatial coherence</td>
</tr>
<tr>
<td>Discrete + Continuous Latents</td>
<td>Enable symbolic and continuous reasoning</td>
</tr>
<tr>
<td>Adaptivity</td>
<td>Representations should evolve with experience</td>
</tr>
</tbody>
</table>
<p>Summary:<br>
Unsupervised representation learning uses <em>three complementary lenses</em>:
- Generative → model what the world looks like.<br>
- Contrastive → learn what is similar or different.<br>
- Self-supervised → create pseudo-tasks that reveal structure.<br>
Together, they aim for data-efficient, transferable, and interpretable representations.</p></body></html></section><section class="print-page" id="deeplearning-8_latentvariables" heading-number="4.8"><h1 id="deeplearning-8_latentvariables-deeplearning-8_latentvariables">8. Latent Variable Models</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-generative-modelling">1. Generative Modelling<a class="headerlink" href="#deeplearning-8_latentvariables-1-generative-modelling" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-11-what-are-generative-models">1.1 What Are Generative Models?<a class="headerlink" href="#deeplearning-8_latentvariables-11-what-are-generative-models" title="Permanent link">¶</a></h3>
<ul>
<li>Probabilistic models of high-dimensional data.</li>
<li>Describe how observations are generated from underlying processes.</li>
<li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-12-why-they-matter">1.2 Why They Matter<a class="headerlink" href="#deeplearning-8_latentvariables-12-why-they-matter" title="Permanent link">¶</a></h3>
<p>Generative models can:
- Estimate data density (detect outliers, anomalies).<br>
- Enable compression (encode → decode).<br>
- Map between domains (e.g., translation, text-to-speech).<br>
- Support model-based RL (predict future states).<br>
- Learn representations from raw data.<br>
- Improve understanding of data structure.</p>
<h2 id="deeplearning-8_latentvariables-13-types-of-generative-models-in-deep-learning">1.3 Types of Generative Models in Deep Learning<a class="headerlink" href="#deeplearning-8_latentvariables-13-types-of-generative-models-in-deep-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-a-autoregressive-models">(a) Autoregressive Models<a class="headerlink" href="#deeplearning-8_latentvariables-a-autoregressive-models" title="Permanent link">¶</a></h3>
<p>Model joint distribution via chain rule:
<script type="math/tex; mode=display">
p(x) = \prod_{i=1}^D p(x_i \mid x_{<i})
</script>
</p>
<p>Trained with maximum likelihood</p>
<p>Examples:</p>
<ul>
<li>RNN/Transformer LMs  </li>
<li>NADE  </li>
<li>PixelCNN / WaveNet</li>
</ul>
<p>Pros:</p>
<ul>
<li>Easy training (max. likelihood).</li>
<li>No sampling during training.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Slow generation (sequential).</li>
<li>Often capture local structure better than global structure.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-b-latent-variable-models">(b) Latent Variable Models<a class="headerlink" href="#deeplearning-8_latentvariables-b-latent-variable-models" title="Permanent link">¶</a></h3>
<p>Introduce an unobserved latent variable <span class="arithmatex">\(z\)</span>:</p>
<ul>
<li>Prior: <span class="arithmatex">\(p(z)\)</span>  </li>
<li>Likelihood: <span class="arithmatex">\(p_\theta(x\mid z)\)</span>  </li>
</ul>
<p>Joint:
<script type="math/tex; mode=display">
p_\theta(x, z) = p_\theta(x\mid z)\,p(z)
</script>
</p>
<p>Pros</p>
<ul>
<li>Flexible &amp; interpretable  </li>
<li>Natural for representation learning  </li>
<li>Fast generation  </li>
</ul>
<p>Cons
- Require approximate inference unless specially designed (e.g., invertible models).</p>
<h3 id="deeplearning-8_latentvariables-c-implicit-models-gans">(c) Implicit Models (GANs)<a class="headerlink" href="#deeplearning-8_latentvariables-c-implicit-models-gans" title="Permanent link">¶</a></h3>
<ul>
<li>Define a generator <span class="arithmatex">\(G(z)\)</span> with no explicit likelihood.</li>
<li>Trained adversarially using a discriminator.</li>
</ul>
<p>Pros</p>
<ul>
<li>Extremely realistic samples  </li>
<li>Fast sampling  </li>
</ul>
<p>Cons</p>
<ul>
<li>Cannot evaluate <span class="arithmatex">\(p(x)\)</span>  </li>
<li>Mode collapse  </li>
<li>Training instability  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-2-latent-variable-models-inference">2. Latent Variable Models &amp; Inference<a class="headerlink" href="#deeplearning-8_latentvariables-2-latent-variable-models-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-21-what-is-a-latent-variable-model-lvm">2.1 What is a Latent Variable Model (LVM)?<a class="headerlink" href="#deeplearning-8_latentvariables-21-what-is-a-latent-variable-model-lvm" title="Permanent link">¶</a></h3>
<p>A latent variable model introduces an unobserved variable <span class="arithmatex">\(z\)</span> that explains the observed data <span class="arithmatex">\(x\)</span>.</p>
<p>Model components:</p>
<ul>
<li>Prior over latent variables:  </li>
</ul>
<p>
<script type="math/tex; mode=display">
  p(z)
  </script>
</p>
<ul>
<li>Likelihood / decoder mapping latent → observation:  </li>
</ul>
<p>
<script type="math/tex; mode=display">
  p_\theta(x \mid z)
  </script>
</p>
<p>Joint distribution:</p>
<div class="arithmatex">\[
p_\theta(x, z) = p_\theta(x \mid z)\,p(z)
\]</div>
<p>Marginal likelihood (what we want to maximize when training):</p>
<div class="arithmatex">\[
p_\theta(x) = \int p_\theta(x \mid z)\,p(z)\,dz
\]</div>
<h3 id="deeplearning-8_latentvariables-22-intuition-latents-as-explanations">2.2 Intuition: Latents as “Explanations”<a class="headerlink" href="#deeplearning-8_latentvariables-22-intuition-latents-as-explanations" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>A particular value of <span class="arithmatex">\(z\)</span> is a hypothesis about hidden causes that produced <span class="arithmatex">\(x\)</span>.</p>
</li>
<li>
<p>Generation = sample latent → map it to data:
  <script type="math/tex; mode=display">
  z \sim p(z), \quad x \sim p_\theta(x\mid z)
  </script>
</p>
</li>
</ul>
<p>Most of the article focuses on the inverse of this:<br>
recovering <span class="arithmatex">\(z\)</span> from <span class="arithmatex">\(x\)</span>.</p>
<h2 id="deeplearning-8_latentvariables-23-what-is-inference">2.3 What Is Inference?<a class="headerlink" href="#deeplearning-8_latentvariables-23-what-is-inference" title="Permanent link">¶</a></h2>
<p>Inference means computing the posterior:
<script type="math/tex; mode=display">
p_\theta(z \mid x) = \frac{p_\theta(x\mid z)\,p(z)}{p_\theta(x)}
</script>
</p>
<p>Why it matters:</p>
<ul>
<li>Explains the observation (which latents likely produced it?)</li>
<li>Needed inside maximum-likelihood training<br>
  (the gradient depends on the posterior!)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-24-inference-requires-the-marginal-likelihood">2.4 Inference Requires the Marginal Likelihood<a class="headerlink" href="#deeplearning-8_latentvariables-24-inference-requires-the-marginal-likelihood" title="Permanent link">¶</a></h2>
<p>To compute the posterior, we need:
<script type="math/tex; mode=display">
p_\theta(x) = \int p_\theta(x \mid z)p(z)\,dz
</script>
This integral is often intractable.</p>
<p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>
<h2 id="deeplearning-8_latentvariables-25-example-mixture-of-gaussians">2.5 Example: Mixture of Gaussians<a class="headerlink" href="#deeplearning-8_latentvariables-25-example-mixture-of-gaussians" title="Permanent link">¶</a></h2>
<p>Model:</p>
<ul>
<li>Choose cluster <span class="arithmatex">\(k\)</span>  </li>
<li>Sample <span class="arithmatex">\(x\)</span> from Gaussian for that cluster</li>
</ul>
<p>Posterior:
<script type="math/tex; mode=display">
p(k \mid x)
= 
\frac{\pi_k\,\mathcal{N}(x \mid \mu_k, \Sigma_k)}
{\sum_j \pi_j \mathcal{N}(x \mid \mu_j, \Sigma_j)}
</script>
</p>
<p>This model is tractable because:</p>
<ul>
<li>Finite number of discrete states  </li>
<li>Closed-form posterior</li>
</ul>
<h2 id="deeplearning-8_latentvariables-26-the-need-for-inference-in-learning">2.6 The Need for Inference in Learning<a class="headerlink" href="#deeplearning-8_latentvariables-26-the-need-for-inference-in-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-maximum-likelihood-as-the-core-training-principle">Maximum Likelihood as the Core Training Principle<a class="headerlink" href="#deeplearning-8_latentvariables-maximum-likelihood-as-the-core-training-principle" title="Permanent link">¶</a></h3>
<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters <span class="arithmatex">\(\theta\)</span> that make the observed training data as probable as possible:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta \sum_{i} \log p_\theta(x^{(i)})
\]</div>
<p>For latent variable models, the marginal likelihood is:
<script type="math/tex; mode=display">
\log p_\theta(x) = \log \int p_\theta(x, z)\,dz
</script>
This integral is rarely tractable, which makes direct maximization difficult.</p>
<h3 id="deeplearning-8_latentvariables-why-optimization-is-hard-in-latent-variable-models">Why Optimization Is Hard in Latent Variable Models<a class="headerlink" href="#deeplearning-8_latentvariables-why-optimization-is-hard-in-latent-variable-models" title="Permanent link">¶</a></h3>
<ul>
<li>The log-likelihood involves an integral (or sum) over the latent variables <span class="arithmatex">\(z\)</span>.  </li>
<li>Because this integral usually has no closed form, we must use iterative optimization methods.</li>
</ul>
<p>Common approaches:</p>
<ol>
<li>Gradient-based optimization (e.g., gradient descent)</li>
<li>Expectation-Maximization (EM)</li>
</ol>
<p>Below we explain why inference (computing the posterior <span class="arithmatex">\(p_\theta(z \mid x)\)</span>) is essential for both.</p>
<h2 id="deeplearning-8_latentvariables-261-gradient-based-learning-requires-the-posterior">2.6.1 Gradient-Based Learning Requires the Posterior<a class="headerlink" href="#deeplearning-8_latentvariables-261-gradient-based-learning-requires-the-posterior" title="Permanent link">¶</a></h2>
<p>Using the identity:</p>
<div class="arithmatex">\[
\nabla_\theta \log p_\theta(x)
=
\mathbb{E}_{p_\theta(z\mid x)}[\nabla_\theta \log p_\theta(x, z)]
\]</div>
<blockquote>
<p>Differentiate the log-marginal</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{\nabla_\theta p_\theta(x)}{p_\theta(x)}\]</div>
<p>Using: <span class="arithmatex">\(p_\theta(x)=\int p_\theta(x,z)\,dz,\)</span></p>
<p>differentiate under the integral:</p>
<div class="arithmatex">\[\nabla_\theta p_\theta(x)
= \nabla_\theta \int p_\theta(x,z)\,dz
= \int \nabla_\theta p_\theta(x,z)\,dz\]</div>
<p>Combine:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{1}{p_\theta(x)} \int \nabla_\theta p_\theta(x,z)\,dz\]</div>
<p>Apply the log-derivative identity</p>
<p>The identity:</p>
<div class="arithmatex">\[\nabla_\theta p_\theta(x,z)
= p_\theta(x,z)\,\nabla_\theta \log p_\theta(x,z)\]</div>
<p>Substitute:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{1}{p_\theta(x)}
\int p_\theta(x,z)\,\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>Recognize the posterior</p>
<p>Bayes’ rule:</p>
<div class="arithmatex">\[p_\theta(z\mid x)
= \frac{p_\theta(x,z)}{p_\theta(x)}\]</div>
<p>Substitute into the integral:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\int
\frac{p_\theta(x,z)}{p_\theta(x)}
\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>This becomes:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\int p_\theta(z\mid x)\,\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>Write as an expectation</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\mathbb{E}_{p_\theta(z\mid x)}
\left[
\nabla_\theta \log p_\theta(x,z)
\right]\]</div>
<p>To compute the gradient of the marginal likelihood, we must take an expectation under the <em>posterior</em>  <span class="arithmatex">\(p_\theta(z\mid x)\)</span>.</p>
</blockquote>
<p>So:</p>
<ul>
<li>We cannot compute <span class="arithmatex">\(\nabla_\theta \log p_\theta(x)\)</span> without knowing the posterior.</li>
<li>Inference becomes part of every gradient step.</li>
<li>If inference is intractable → gradient is intractable.</li>
</ul>
<p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>
<h2 id="deeplearning-8_latentvariables-262-expectation-maximization-em-also-requires-inference">2.6.2 Expectation-Maximization (EM) Also Requires Inference<a class="headerlink" href="#deeplearning-8_latentvariables-262-expectation-maximization-em-also-requires-inference" title="Permanent link">¶</a></h2>
<p>EM is an alternative to gradient descent for maximizing likelihood.</p>
<h3 id="deeplearning-8_latentvariables-e-step">E-step:<a class="headerlink" href="#deeplearning-8_latentvariables-e-step" title="Permanent link">¶</a></h3>
<p>Compute (or approximate) the posterior:
<script type="math/tex; mode=display">
q(z) \approx p_\theta(z \mid x)
</script>
This assigns responsibilities to each latent configuration.</p>
<h3 id="deeplearning-8_latentvariables-m-step">M-step:<a class="headerlink" href="#deeplearning-8_latentvariables-m-step" title="Permanent link">¶</a></h3>
<p>Update parameters by maximizing the expected complete-data log-likelihood:
<script type="math/tex; mode=display">
\theta^{(t+1)} = \arg\max_\theta \mathbb{E}_{q(z)}[\log p_\theta(x, z)]
</script>
</p>
<p>Thus, the E-step directly requires inference.</p>
<h2 id="deeplearning-8_latentvariables-27-why-exact-inference-is-hard">2.7 Why Exact Inference Is Hard<a class="headerlink" href="#deeplearning-8_latentvariables-27-why-exact-inference-is-hard" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-continuous-latents">Continuous latents:<a class="headerlink" href="#deeplearning-8_latentvariables-continuous-latents" title="Permanent link">¶</a></h3>
<ul>
<li>Require multidimensional integration over nonlinear likelihoods.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-discrete-latents">Discrete latents:<a class="headerlink" href="#deeplearning-8_latentvariables-discrete-latents" title="Permanent link">¶</a></h3>
<ul>
<li>Require summing over exponentially many configurations.</li>
</ul>
<p>Only a few cases allow closed-form inference:</p>
<ul>
<li>Mixture models  </li>
<li>Linear Gaussian systems  </li>
<li>Invertible / flow-based models (covered next)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-28-two-strategies-to-handle-intractability">2.8 Two Strategies to Handle Intractability<a class="headerlink" href="#deeplearning-8_latentvariables-28-two-strategies-to-handle-intractability" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-1-design-tractable-models">1. Design tractable models<a class="headerlink" href="#deeplearning-8_latentvariables-1-design-tractable-models" title="Permanent link">¶</a></h3>
<ul>
<li>Invertible models (normalizing flows)</li>
<li>Autoregressive latent structures<br>
Pros: exact inference<br>
Cons: restricted model class</li>
</ul>
<h3 id="deeplearning-8_latentvariables-2-approximate-inference">2. Approximate inference<a class="headerlink" href="#deeplearning-8_latentvariables-2-approximate-inference" title="Permanent link">¶</a></h3>
<ul>
<li>Use approximations to posterior <span class="arithmatex">\(p(z \mid x)\)</span>  </li>
<li>Variational Inference or MCMC<br>
Pros: flexible, expressive models<br>
Cons: introduces approximation error</li>
</ul>
<h2 id="deeplearning-8_latentvariables-3-invertible-models-exact-inference">3. Invertible Models &amp; Exact Inference<a class="headerlink" href="#deeplearning-8_latentvariables-3-invertible-models-exact-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-31-what-are-invertible-models">3.1 What Are Invertible Models?<a class="headerlink" href="#deeplearning-8_latentvariables-31-what-are-invertible-models" title="Permanent link">¶</a></h3>
<p>Invertible models (also called normalizing flows) are latent variable models where:</p>
<ul>
<li>The latent variable <span class="arithmatex">\(z\)</span> and data <span class="arithmatex">\(x\)</span> have the same dimensionality</li>
<li>There exists an invertible, differentiable mapping<br>
<script type="math/tex; mode=display">
  x = f_\theta(z)
  </script>
</li>
<li>Because <span class="arithmatex">\(f_\theta\)</span> is invertible:
  <script type="math/tex; mode=display">
  z = f_\theta^{-1}(x)
  </script>
</li>
</ul>
<p>Key property:<br>
Inference is exact and trivial — simply apply the inverse function.</p>
<h2 id="deeplearning-8_latentvariables-32-generative-process">3.2 Generative Process<a class="headerlink" href="#deeplearning-8_latentvariables-32-generative-process" title="Permanent link">¶</a></h2>
<p>To generate a sample:</p>
<ol>
<li>Sample <span class="arithmatex">\(z \sim p(z)\)</span> (usually a simple prior like <span class="arithmatex">\(\mathcal{N}(0, I)\)</span>)</li>
<li>Transform via<br>
<script type="math/tex; mode=display">
   x = f_\theta(z)
   </script>
</li>
</ol>
<p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>
<h2 id="deeplearning-8_latentvariables-33-why-are-invertible-models-attractive">3.3 Why Are Invertible Models Attractive?<a class="headerlink" href="#deeplearning-8_latentvariables-33-why-are-invertible-models-attractive" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Exact inference:<br>
<script type="math/tex; mode=display">
  p_\theta(z \mid x)
  </script>
<br>
  is computed by a single function evaluation (no approximation needed).</p>
</li>
<li>
<p>Exact likelihood:<br>
  Can compute <span class="arithmatex">\(\log p_\theta(x)\)</span> exactly using the change-of-variables formula.</p>
</li>
</ul>
<h2 id="deeplearning-8_latentvariables-34-change-of-variables-for-likelihood">3.4 Change of Variables for Likelihood<a class="headerlink" href="#deeplearning-8_latentvariables-34-change-of-variables-for-likelihood" title="Permanent link">¶</a></h2>
<p>Given an invertible mapping <span class="arithmatex">\(x = f_\theta(z)\)</span>:</p>
<div class="arithmatex">\[
p_\theta(x) = p(z) \left| \det \left( \frac{\partial f_\theta^{-1}(x)}{\partial x} \right) \right|
\]</div>
<p>Equivalently, using <span class="arithmatex">\(z = f_\theta^{-1}(x)\)</span>:</p>
<div class="arithmatex">\[
\log p_\theta(x)
=
\log p(z)
+
\log \left| \det J_{f_\theta^{-1}}(x) \right|
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(J_{f_\theta^{-1}}\)</span> is the Jacobian matrix of the inverse map  </li>
<li>The determinant accounts for volume change introduced by transformation</li>
</ul>
<h2 id="deeplearning-8_latentvariables-35-example-independent-component-analysis-ica">3.5 Example: Independent Component Analysis (ICA)<a class="headerlink" href="#deeplearning-8_latentvariables-35-example-independent-component-analysis-ica" title="Permanent link">¶</a></h2>
<p>ICA is the simplest invertible model:</p>
<ul>
<li>Latent prior:  factorial prior
  <script type="math/tex; mode=display">
  p(z) = \prod_i p(z_i)
  </script>
  with non-Gaussian heavy-tailed components</li>
<li>Linear invertible mixing:<br>
<script type="math/tex; mode=display">
  x = A z
  </script>
</li>
</ul>
<p>Inference:
<script type="math/tex; mode=display">
z = A^{-1} x
</script>
</p>
<p>ICA recovers independent sources that explain the observed signal.</p>
<h2 id="deeplearning-8_latentvariables-36-building-complex-invertible-models">3.6 Building Complex Invertible Models<a class="headerlink" href="#deeplearning-8_latentvariables-36-building-complex-invertible-models" title="Permanent link">¶</a></h2>
<p>Modern flows build <span class="arithmatex">\(f_\theta\)</span> by composing many simple invertible layers:</p>
<div class="arithmatex">\[
f_\theta = f_K \circ f_{K-1} \circ \dots \circ f_1
\]</div>
<p>Composition of invertible functions is invertible.</p>
<p>Building blocks:</p>
<ul>
<li>Linear transforms</li>
<li>Autoregressive flows (IAF, MAF)</li>
<li>Coupling layers (RealNVP, Glow)</li>
<li>Residual flows</li>
<li>Sylvester flows</li>
</ul>
<p>Design goal:</p>
<blockquote>
<p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>
</blockquote>
<h2 id="deeplearning-8_latentvariables-37-advantages-limitations">3.7 Advantages &amp; Limitations<a class="headerlink" href="#deeplearning-8_latentvariables-37-advantages-limitations" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-advantages">Advantages<a class="headerlink" href="#deeplearning-8_latentvariables-advantages" title="Permanent link">¶</a></h3>
<ul>
<li>Exact inference  </li>
<li>Exact log-likelihood  </li>
<li>Fast, parallel sampling  </li>
<li>Useful as components in larger probabilistic models</li>
</ul>
<h3 id="deeplearning-8_latentvariables-limitations">Limitations<a class="headerlink" href="#deeplearning-8_latentvariables-limitations" title="Permanent link">¶</a></h3>
<ul>
<li>Latent and data dimensions must match  </li>
<li>Latents must be continuous  </li>
<li>Observations must be continuous or quantized  </li>
<li>Very deep flows require large memory  </li>
<li>Hard to encode strong structure or sparsity  </li>
</ul>
<blockquote>
<p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>
</blockquote>
<h2 id="deeplearning-8_latentvariables-mar">Mar<a class="headerlink" href="#deeplearning-8_latentvariables-mar" title="Permanent link">¶</a></h2>
<h2 id="deeplearning-8_latentvariables-4-variational-inference-vi">4. Variational Inference (VI)<a class="headerlink" href="#deeplearning-8_latentvariables-4-variational-inference-vi" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-41-why-variational-inference">4.1 Why Variational Inference?<a class="headerlink" href="#deeplearning-8_latentvariables-41-why-variational-inference" title="Permanent link">¶</a></h3>
<p>In many latent variable models, the true posterior
<script type="math/tex; mode=display">
p_\theta(z \mid x)
</script>
is intractable because computing
<script type="math/tex; mode=display">
p_\theta(x) = \int p_\theta(x, z)\,dz
</script>
is impossible in closed form.</p>
<p>We still need the posterior for:</p>
<ul>
<li>Inference (explaining the observation)</li>
<li>Learning (MLE gradient depends on it)</li>
<li>EM algorithm E-step</li>
</ul>
<h4 id="deeplearning-8_latentvariables-approximate-inference">Approximate Inference<a class="headerlink" href="#deeplearning-8_latentvariables-approximate-inference" title="Permanent link">¶</a></h4>
<p>There are two major classes of approaches to approximate inference:</p>
<h4 id="deeplearning-8_latentvariables-markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)<a class="headerlink" href="#deeplearning-8_latentvariables-markov-chain-monte-carlo-mcmc" title="Permanent link">¶</a></h4>
<p>Generate samples from the exact posterior using a Markov chain.</p>
<ul>
<li>Very general; exact in the limit of infinite time / computation  </li>
<li>Computationally expensive  </li>
<li>Convergence is hard to diagnose  </li>
</ul>
<h4 id="deeplearning-8_latentvariables-2-variational-inference-vi">2. Variational Inference (VI)<a class="headerlink" href="#deeplearning-8_latentvariables-2-variational-inference-vi" title="Permanent link">¶</a></h4>
<p>Approximate the posterior with a tractable distribution<br>
(e.g., fully factorized, mixture, or autoregressive).</p>
<ul>
<li>Fairly efficient — inference reduces to optimization of distribution parameters  </li>
<li>Fast at test time (single forward pass of the inference network)  </li>
<li>Cannot easily trade computation for accuracy (unlike MCMC)  </li>
</ul>
<p>MCMC = flexible, asymptotically exact, but slow.<br>
VI = fast and scalable, but biased due to restricted approximating family.</p>
<h2 id="deeplearning-8_latentvariables-42-core-idea-of-variational-inference">4.2 Core Idea of Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-42-core-idea-of-variational-inference" title="Permanent link">¶</a></h2>
<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing.
Approximate the posterior with a simpler distribution:</p>
<div class="arithmatex">\[
q_\phi(z \mid x) \approx p_\theta(z \mid x)
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(q_\phi\)</span> is the variational posterior</li>
<li><span class="arithmatex">\(\phi\)</span> are variational parameters (learned)</li>
</ul>
<p>Requirements:</p>
<ol>
<li>We can sample from <span class="arithmatex">\(q_\phi(z \mid x)\)</span>  </li>
<li>We can compute <span class="arithmatex">\(\log q_\phi(z \mid x)\)</span> and its gradient wrt <span class="arithmatex">\(\phi\)</span>  </li>
</ol>
<p>Common choice: mean-field approximation</p>
<div class="arithmatex">\[
q_\phi(z \mid x) = \prod_i q_\phi(z_i \mid x)
\]</div>
<h2 id="deeplearning-8_latentvariables-43-training-with-variational-inference">4.3 Training with Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-43-training-with-variational-inference" title="Permanent link">¶</a></h2>
<p>Goal: maximize the marginal likelihood</p>
<div class="arithmatex">\[
\log p_\theta(x)
\]</div>
<p>Since it's intractable, VI uses a lower bound on this quantity.</p>
<h3 id="deeplearning-8_latentvariables-variational-lower-bound-elbo">Variational Lower Bound (ELBO)<a class="headerlink" href="#deeplearning-8_latentvariables-variational-lower-bound-elbo" title="Permanent link">¶</a></h3>
<p>Using Jensen’s inequality:</p>
<div class="arithmatex">\[
\log p_\theta(x)
\ge 
\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x, z)]
-
\mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)]
\]</div>
<p>This is the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\text{ELBO}(\theta, \phi)
=
\mathbb{E}_{q_\phi}\!\left[\log p_\theta(x, z)\right]
-
\mathbb{E}_{q_\phi}\!\left[\log q_\phi(z \mid x)\right]
\]</div>
<p>We maximize ELBO w.r.t both <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(\phi\)</span>.</p>
<h2 id="deeplearning-8_latentvariables-44-kl-interpretation-variational-gap">4.4 KL Interpretation (Variational Gap)<a class="headerlink" href="#deeplearning-8_latentvariables-44-kl-interpretation-variational-gap" title="Permanent link">¶</a></h2>
<p>Rewrite ELBO:</p>
<div class="arithmatex">\[
\log p_\theta(x)
=
\text{ELBO}(\theta, \phi)
+
D_{\text{KL}}(q_\phi(z \mid x) \,\|\, p_\theta(z \mid x))
\]</div>
<p>Thus:</p>
<ul>
<li>
<p>Maximizing ELBO wrt <span class="arithmatex">\(\phi\)</span><br>
  → minimizes the KL divergence between <span class="arithmatex">\(q_\phi\)</span> and the true posterior.</p>
</li>
<li>
<p>The variational gap is<br>
<script type="math/tex; mode=display">
  D_{\text{KL}}(q_\phi(z\mid x) || p_\theta(z\mid x))
  </script>
</p>
</li>
</ul>
<p>If <span class="arithmatex">\(q_\phi\)</span> is expressive enough:
<script type="math/tex; mode=display">
q_\phi(z\mid x) = p_\theta(z\mid x)
\quad \Rightarrow \quad
\text{gap} = 0
</script>
</p>
<h2 id="deeplearning-8_latentvariables-45-what-happens-when-updating-each-parameter-set">4.5 What Happens When Updating Each Parameter Set?<a class="headerlink" href="#deeplearning-8_latentvariables-45-what-happens-when-updating-each-parameter-set" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-updating-variational-parameters-phi">Updating variational parameters <span class="arithmatex">\(\phi\)</span>:<a class="headerlink" href="#deeplearning-8_latentvariables-updating-variational-parameters-phi" title="Permanent link">¶</a></h3>
<ul>
<li>Minimizes the variational gap  </li>
<li>Makes <span class="arithmatex">\(q_\phi(z \mid x)\)</span> closer to the true posterior  </li>
<li>Does not affect the model directly</li>
</ul>
<h3 id="deeplearning-8_latentvariables-updating-model-parameters-theta">Updating model parameters <span class="arithmatex">\(\theta\)</span>:<a class="headerlink" href="#deeplearning-8_latentvariables-updating-model-parameters-theta" title="Permanent link">¶</a></h3>
<ul>
<li>Increases <span class="arithmatex">\(\log p_\theta(x)\)</span> (good)</li>
<li>BUT often also reduces the gap by making the posterior simpler<br>
  → Risk: posterior collapse / variational pruning</li>
</ul>
<p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>
<h2 id="deeplearning-8_latentvariables-46-variational-pruning-posterior-collapse">4.6 Variational Pruning (Posterior Collapse)<a class="headerlink" href="#deeplearning-8_latentvariables-46-variational-pruning-posterior-collapse" title="Permanent link">¶</a></h2>
<p>Because VI pushes <span class="arithmatex">\(p_\theta(z \mid x)\)</span> towards <span class="arithmatex">\(q_\phi(z\mid x)\)</span>, the model may choose to ignore some latent dimensions:</p>
<div class="arithmatex">\[
p_\theta(z_i \mid x) = p(z_i)
\]</div>
<p>Meaning the latent variable carries no information about <span class="arithmatex">\(x\)</span>.</p>
<p>Pros:</p>
<ul>
<li>Automatically learns effective latent dimensionality</li>
</ul>
<p>Cons:</p>
<ul>
<li>Prevents fully utilizing the latent capacity  </li>
<li>Common issue in VAEs (particularly with strong decoders)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-47-choosing-the-variational-posterior-family">4.7 Choosing the Variational Posterior Family<a class="headerlink" href="#deeplearning-8_latentvariables-47-choosing-the-variational-posterior-family" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-simple-mean-field-gaussian">Simple: Mean-field Gaussian<a class="headerlink" href="#deeplearning-8_latentvariables-simple-mean-field-gaussian" title="Permanent link">¶</a></h3>
<ul>
<li>Fast</li>
<li>Easy to optimize</li>
<li>But limited expressivity</li>
</ul>
<h3 id="deeplearning-8_latentvariables-more-expressive-options">More expressive options:<a class="headerlink" href="#deeplearning-8_latentvariables-more-expressive-options" title="Permanent link">¶</a></h3>
<ul>
<li>Mixture posteriors</li>
<li>Gaussians with full covariance</li>
<li>Autoregressive posteriors</li>
<li>Normalizing-flow posteriors</li>
</ul>
<p>Trade-off: accuracy vs speed.</p>
<h2 id="deeplearning-8_latentvariables-48-amortized-variational-inference">4.8 Amortized Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-48-amortized-variational-inference" title="Permanent link">¶</a></h2>
<p>Classic VI:</p>
<ul>
<li>Each datapoint <span class="arithmatex">\(x\)</span> has its own variational parameters  </li>
<li>Requires iterative optimization per datapoint  </li>
<li>Too slow for deep learning</li>
</ul>
<p>Amortized VI:</p>
<ul>
<li>Use an inference network (encoder)
  <script type="math/tex; mode=display">
  \phi(x) \mapsto \text{parameters of } q_\phi(z\mid x)
  </script>
</li>
<li>Fast inference  </li>
<li>Works with SGD  </li>
<li>Introduced in Helmholtz Machines  </li>
<li>Popularized by Variational Autoencoders</li>
</ul>
<h2 id="deeplearning-8_latentvariables-49-variational-vs-exact-inference">4.9 Variational vs Exact Inference<a class="headerlink" href="#deeplearning-8_latentvariables-49-variational-vs-exact-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-advantages-of-vi">Advantages of VI<a class="headerlink" href="#deeplearning-8_latentvariables-advantages-of-vi" title="Permanent link">¶</a></h3>
<ul>
<li>Scalable to modern deep models  </li>
<li>Fast inference  </li>
<li>Enables flexible model design  </li>
</ul>
<h3 id="deeplearning-8_latentvariables-disadvantages">Disadvantages<a class="headerlink" href="#deeplearning-8_latentvariables-disadvantages" title="Permanent link">¶</a></h3>
<ul>
<li>Approximation bias  </li>
<li>Posterior may be oversimplified  </li>
<li>Can limit expressiveness of the full model  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-410-summary-of-section-4">4.10 Summary of Section 4<a class="headerlink" href="#deeplearning-8_latentvariables-410-summary-of-section-4" title="Permanent link">¶</a></h2>
<ul>
<li>Variational inference approximates the true posterior with a tractable distribution.  </li>
<li>ELBO gives a trainable lower bound on the marginal likelihood.  </li>
<li>VI converts inference into optimization.  </li>
<li>Amortized VI enables neural inference (encoders).  </li>
<li>Variational pruning can arise naturally and must be managed.  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-5-gradient-estimation-in-variational-inference">5. Gradient Estimation in Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-5-gradient-estimation-in-variational-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-51-why-do-we-need-gradient-estimators">5.1 Why Do We Need Gradient Estimators?<a class="headerlink" href="#deeplearning-8_latentvariables-51-why-do-we-need-gradient-estimators" title="Permanent link">¶</a></h3>
<p>To train a latent variable model with variational inference, we maximize the ELBO:</p>
<div class="arithmatex">\[
\text{ELBO}(\theta, \phi)
=
\mathbb{E}_{q_\phi(z\mid x)}\Big[ \log p_\theta(x, z) - \log q_\phi(z\mid x) \Big]
\]</div>
<p>We need gradients with respect to:</p>
<ol>
<li>Model parameters <span class="arithmatex">\(\theta\)</span></li>
<li>Variational parameters <span class="arithmatex">\(\phi\)</span></li>
</ol>
<p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-52-gradients-wrt-model-parameters-theta">5.2 Gradients w.r.t. Model Parameters (<span class="arithmatex">\(\theta\)</span>)<a class="headerlink" href="#deeplearning-8_latentvariables-52-gradients-wrt-model-parameters-theta" title="Permanent link">¶</a></h2>
<p>This part is easy.</p>
<p>Because <span class="arithmatex">\(q_\phi(z\mid x)\)</span> does not depend on <span class="arithmatex">\(\theta\)</span>:</p>
<div class="arithmatex">\[
\nabla_\theta \text{ELBO}
=
\mathbb{E}_{q_\phi(z\mid x)} \big[ \nabla_\theta \log p_\theta(x, z) \big]
\]</div>
<p>We estimate this using samples:</p>
<ol>
<li>Draw <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span>  </li>
<li>Compute <span class="arithmatex">\(\nabla_\theta \log p_\theta(x,z)\)</span>  </li>
<li>Average across samples</li>
</ol>
<p>No special techniques required.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-53-gradients-wrt-variational-parameters-phi">5.3 Gradients w.r.t. Variational Parameters (<span class="arithmatex">\(\phi\)</span>)<a class="headerlink" href="#deeplearning-8_latentvariables-53-gradients-wrt-variational-parameters-phi" title="Permanent link">¶</a></h2>
<p>This is more difficult.</p>
<p>We want:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z\mid x)}[f(z)]
\]</div>
<p>But <span class="arithmatex">\(q_\phi(z\mid x)\)</span> depends on <span class="arithmatex">\(\phi\)</span>.<br>
Two main strategies exist to handle this dependence:</p>
<hr>
<h1 id="deeplearning-8_latentvariables-54-two-families-of-gradient-estimators">5.4 Two Families of Gradient Estimators<a class="headerlink" href="#deeplearning-8_latentvariables-54-two-families-of-gradient-estimators" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-8_latentvariables-1-likelihood-ratio-reinforce-estimator">🔷 1. Likelihood-Ratio / REINFORCE Estimator<a class="headerlink" href="#deeplearning-8_latentvariables-1-likelihood-ratio-reinforce-estimator" title="Permanent link">¶</a></h2>
<p>Uses the identity:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)]
=
\mathbb{E}_{q_\phi(z)}[f(z)\,\nabla_\phi \log q_\phi(z)]
\]</div>
<p>This allows gradients for:
- Discrete latent variables<br>
- Non-differentiable <span class="arithmatex">\(f(z)\)</span><br>
- Any distribution where we can compute <span class="arithmatex">\(\log q_\phi(z)\)</span></p>
<p>Pros
- Very general<br>
- Works for discrete and continuous latents  </p>
<p>Cons
- High variance<br>
- Requires variance reduction (baselines, control variates)</p>
<p>This is the same gradient estimator used in policy gradients in RL.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-2-reparameterization-pathwise-estimator">🔷 2. Reparameterization / Pathwise Estimator<a class="headerlink" href="#deeplearning-8_latentvariables-2-reparameterization-pathwise-estimator" title="Permanent link">¶</a></h2>
<p>Instead of sampling <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span> directly,
write it as a differentiable transformation of noise:</p>
<div class="arithmatex">\[
z = g_\phi(\epsilon, x), \quad \epsilon \sim p(\epsilon)
\]</div>
<p>Then:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z\mid x)}[f(z)]
=
\mathbb{E}_{\epsilon \sim p(\epsilon)}
\big[ \nabla_\phi f(g_\phi(\epsilon, x)) \big]
\]</div>
<p>This pushes the dependence on <span class="arithmatex">\(\phi\)</span> inside a differentiable function.</p>
<h3 id="deeplearning-8_latentvariables-example-gaussian-posterior">Example: Gaussian posterior<a class="headerlink" href="#deeplearning-8_latentvariables-example-gaussian-posterior" title="Permanent link">¶</a></h3>
<p>If
<script type="math/tex; mode=display">
q_\phi(z\mid x) = \mathcal{N}(z\mid \mu_\phi(x), \sigma_\phi(x)^2),
</script>
then:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon, \quad \epsilon\sim \mathcal{N}(0,1)
\]</div>
<p>Pros
- Low variance<br>
- Enables stable VAE training  </p>
<p>Cons
- Only works for continuous latent variables<br>
- Requires differentiable sampling procedure</p>
<hr>
<h2 id="deeplearning-8_latentvariables-55-comparison-table">5.5 Comparison Table<a class="headerlink" href="#deeplearning-8_latentvariables-55-comparison-table" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>REINFORCE</th>
<th>Reparameterization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Works for discrete latent variables</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Works for continuous latent variables</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Low-variance gradients</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Requires differentiable sampling</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Used in VAEs</td>
<td>sometimes</td>
<td>always</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="deeplearning-8_latentvariables-56-practical-notes">5.6 Practical Notes<a class="headerlink" href="#deeplearning-8_latentvariables-56-practical-notes" title="Permanent link">¶</a></h2>
<ul>
<li>Modern VAEs always use the reparameterization trick.  </li>
<li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li>
<li>Discrete VAEs use:</li>
<li>Gumbel-Softmax  </li>
<li>NVIL / REINFORCE with baselines  </li>
<li>VIMCO  </li>
</ul>
<hr>
<h2 id="deeplearning-8_latentvariables-57-summary-of-section-5">5.7 Summary of Section 5<a class="headerlink" href="#deeplearning-8_latentvariables-57-summary-of-section-5" title="Permanent link">¶</a></h2>
<ul>
<li>Gradient estimation is essential for training VI models.  </li>
<li><span class="arithmatex">\(\nabla_\theta\)</span> is easy: just sample from the variational posterior.  </li>
<li><span class="arithmatex">\(\nabla_\phi\)</span> is hard because sampling depends on parameters.  </li>
<li>Two estimators solve this:</li>
<li>Likelihood-ratio (REINFORCE)  </li>
<li>Reparameterization trick  </li>
<li>Reparameterization yields low-variance gradients and powers modern VAEs.</li>
</ul>
<h2 id="deeplearning-8_latentvariables-6-variational-autoencoders-vaes">6. Variational Autoencoders (VAEs)<a class="headerlink" href="#deeplearning-8_latentvariables-6-variational-autoencoders-vaes" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-61-what-is-a-vae">6.1 What Is a VAE?<a class="headerlink" href="#deeplearning-8_latentvariables-61-what-is-a-vae" title="Permanent link">¶</a></h3>
<p>A VAE is a latent variable generative model with:</p>
<ul>
<li>Continuous latent variables <span class="arithmatex">\(z\)</span></li>
<li>Neural networks for both:</li>
<li>Encoder (variational posterior) <span class="arithmatex">\(q_\phi(z \mid x)\)</span>  </li>
<li>Decoder (likelihood) <span class="arithmatex">\(p_\theta(x \mid z)\)</span></li>
<li>Training through amortized variational inference  </li>
<li>Gradients computed using the reparameterization trick</li>
</ul>
<p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-62-vae-model-components">6.2 VAE Model Components<a class="headerlink" href="#deeplearning-8_latentvariables-62-vae-model-components" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-prior">Prior<a class="headerlink" href="#deeplearning-8_latentvariables-prior" title="Permanent link">¶</a></h3>
<p>Usually a factorized standard Gaussian:
<script type="math/tex; mode=display">
p(z) = \mathcal{N}(0, I)
</script>
</p>
<h3 id="deeplearning-8_latentvariables-likelihood-decoder">Likelihood / Decoder<a class="headerlink" href="#deeplearning-8_latentvariables-likelihood-decoder" title="Permanent link">¶</a></h3>
<p>Maps latents to a distribution over observations.</p>
<p>For binary data:
<script type="math/tex; mode=display">
p_\theta(x \mid z) = \text{Bernoulli}(x; f_\theta(z))
</script>
</p>
<p>For real-valued data:
<script type="math/tex; mode=display">
p_\theta(x \mid z) = \mathcal{N}(x; \mu_\theta(z), \sigma^2 I)
</script>
</p>
<h3 id="deeplearning-8_latentvariables-variational-posterior-encoder">Variational Posterior / Encoder<a class="headerlink" href="#deeplearning-8_latentvariables-variational-posterior-encoder" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x))
\]</div>
<p>All of these functions (encoder &amp; decoder) can be implemented with:
- MLPs<br>
- ConvNets<br>
- ResNets<br>
- Transformers<br>
depending on the domain.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-63-training-objective-the-elbo">6.3 Training Objective: The ELBO<a class="headerlink" href="#deeplearning-8_latentvariables-63-training-objective-the-elbo" title="Permanent link">¶</a></h2>
<p>VAEs maximize the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\mathcal{L}(x)
=
\mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
-
D_{\text{KL}}\!\Big(q_\phi(z\mid x)\,\|\, p(z)\Big)
\]</div>
<p>Interpretation:</p>
<ol>
<li>
<p>Reconstruction Term<br>
   Measures how well the model predicts <span class="arithmatex">\(x\)</span> from <span class="arithmatex">\(z\)</span>.<br>
   Encourages informative latents.</p>
</li>
<li>
<p>KL Regularization Term<br>
   Encourages <span class="arithmatex">\(q_\phi(z\mid x)\)</span> to stay close to the prior <span class="arithmatex">\(p(z)\)</span>.<br>
   Prevents overfitting and encourages smooth latent spaces.</p>
</li>
</ol>
<p>The KL term often has closed-form for Gaussian distributions.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-64-reparameterization-trick-key-to-vaes">6.4 Reparameterization Trick (Key to VAEs)<a class="headerlink" href="#deeplearning-8_latentvariables-64-reparameterization-trick-key-to-vaes" title="Permanent link">¶</a></h2>
<p>Direct backprop through a sample <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span> is impossible.</p>
<p>Solution: rewrite sampling as a differentiable transformation of noise:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon,
\quad
\epsilon \sim \mathcal{N}(0, I)
\]</div>
<p>This allows gradient flow through <span class="arithmatex">\(z\)</span> and makes VAE training practical.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-65-vae-as-a-framework">6.5 VAE as a Framework<a class="headerlink" href="#deeplearning-8_latentvariables-65-vae-as-a-framework" title="Permanent link">¶</a></h2>
<p>The term “VAE” now refers to a broad family of models:
- Continuous latent variables
- Amortized inference
- Reparameterization-based gradients
- Trained by maximizing ELBO (or its variants)</p>
<p>Modern VAEs extend the basic version in many ways:
- Multiple latent layers<br>
- More expressive posteriors (flows, mixtures)<br>
- More expressive priors (hierarchical, autoregressive)<br>
- More expressive decoders (ResNets, autoregressive PixelCNN decoders)<br>
- Iterative inference networks<br>
- Variance reduction techniques</p>
<p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-66-summary-of-section-6">6.6 Summary of Section 6<a class="headerlink" href="#deeplearning-8_latentvariables-66-summary-of-section-6" title="Permanent link">¶</a></h2>
<ul>
<li>VAEs are tractable generative models with continuous latent variables.</li>
<li>They pair:</li>
<li>a decoder <span class="arithmatex">\(p_\theta(x\mid z)\)</span> and  </li>
<li>an encoder <span class="arithmatex">\(q_\phi(z\mid x)\)</span>
  using amortized VI.</li>
<li>Training uses ELBO + reparameterization trick.</li>
<li>VAEs balance reconstruction quality with regularized latent structure.</li>
<li>The VAE framework is highly extensible and central to modern deep generative modeling.</li>
</ul></body></html></section></section>
                    <section class='print-page md-section' id='section-5' heading-number='5'>
                        <h1>Information Theory<a class='headerlink' href='#section-5' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="informationtheory-1_intro_to_infotheory" heading-number="5.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-to-information-theory-for-machine-learning">Chapter 1 — Introduction to Information Theory (for Machine Learning)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-chapter-1-introduction-to-information-theory-for-machine-learning" title="Permanent link">¶</a></h1>
<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p>
<ul>
<li>loss functions (cross-entropy, NLL)</li>
<li>representation learning and contrastive learning</li>
<li>variational inference and VAEs</li>
<li>generative modeling (GANs, flows, diffusion)</li>
<li>reinforcement learning (entropy bonuses, policy KL constraints)</li>
<li>model capacity, generalization, and bottlenecks</li>
</ul>
<p>This chapter introduces the core motivations and conceptual tools.</p>
<h2 id="informationtheory-1_intro_to_infotheory-1-why-information-theory-matters-for-ml">1. Why Information Theory Matters for ML<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-1-why-information-theory-matters-for-ml" title="Permanent link">¶</a></h2>
<p>Information theory answers questions fundamental to ML:</p>
<ul>
<li>How much uncertainty does a model reduce?</li>
<li>How do we quantify the difference between two probability distributions?</li>
<li>How do we measure dependence between variables?</li>
<li>What is the maximum information a neural network layer can transmit?</li>
<li>How do we formalize compression and generalization?</li>
</ul>
<p>In ML, information theory is not abstract mathematics —<br>
it provides the <em>language</em> for describing learning itself:</p>
<blockquote>
<p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p>
</blockquote>
<p>This viewpoint unifies:</p>
<ul>
<li>Maximum likelihood  </li>
<li>Variational inference  </li>
<li>Contrastive learning  </li>
<li>GAN objectives  </li>
<li>Representation learning  </li>
<li>Reinforcement learning signal shaping  </li>
</ul>
<h2 id="informationtheory-1_intro_to_infotheory-2-the-communication-view-shannons-formulation">2. The Communication View (Shannon’s Formulation)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-2-the-communication-view-shannons-formulation" title="Permanent link">¶</a></h2>
<p>A classical communication system consists of:</p>
<ol>
<li>
<p>Source:    Generates data (symbols, images, text, states).</p>
</li>
<li>
<p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p>
</li>
<li>
<p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p>
</li>
<li>
<p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p>
</li>
<li>
<p>Receiver:   Obtains the final predictions or reconstructions.</p>
</li>
</ol>
<p>Information theory studies:</p>
<ul>
<li>Limits of efficient communication  </li>
<li>Optimal encoding and representation  </li>
<li>Tradeoffs between compression and fidelity  </li>
<li>Effect of noise on learnability</li>
</ul>
<h2 id="informationtheory-1_intro_to_infotheory-3-the-uncertainty-view-shannonbayesian-perspective">3. The Uncertainty View (Shannon–Bayesian Perspective)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-3-the-uncertainty-view-shannonbayesian-perspective" title="Permanent link">¶</a></h2>
<p>Information theory also quantifies <em>uncertainty</em>:</p>
<ul>
<li>More uncertainty → more information needed  </li>
<li>Less uncertainty → easier prediction and compression  </li>
</ul>
<p>Key idea: Information is the reduction of uncertainty.</p>
<p>In ML:</p>
<ul>
<li>Entropy measures label uncertainty  </li>
<li>Cross-entropy measures model fit  </li>
<li>KL divergence measures mismatch  </li>
<li>Mutual information measures representation quality  </li>
<li>ELBO measures how well a generative model explains data  </li>
</ul>
<p>Thus, learning and compression are mathematically the same problem.</p>
<h2 id="informationtheory-1_intro_to_infotheory-4-machine-learning-as-communication">4. Machine Learning as Communication<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-4-machine-learning-as-communication" title="Permanent link">¶</a></h2>
<p>Modern ML pipelines resemble a communication system:</p>
<h3 id="informationtheory-1_intro_to_infotheory-data-encoder-latent-representation-decoder-output">Data → Encoder → Latent Representation → Decoder → Output<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-data-encoder-latent-representation-decoder-output" title="Permanent link">¶</a></h3>
<p>Examples:</p>
<ul>
<li>Autoencoders / VAEs: compress <span class="arithmatex">\(x\)</span> into <span class="arithmatex">\(z\)</span>, then reconstruct</li>
<li>Transformers: compress sequences into features, decode predictions</li>
<li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li>
<li>GANs: learn generator distributions close to data distribution</li>
<li>RL agents: compress sensory input into state representations</li>
</ul>
<p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>
<h3 id="informationtheory-1_intro_to_infotheory-5-roadmap-for-this-web-book">5. Roadmap for This Web-book<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-5-roadmap-for-this-web-book" title="Permanent link">¶</a></h3>
<p>This web-book is structured to build information theory specifically for ML:</p>
<ol>
<li>
<p>Entropy &amp; Self-Information<br>
   Foundations of uncertainty, coding length, and compression.</p>
</li>
<li>
<p>Cross-Entropy &amp; Negative Log-Likelihood<br>
   Core ML loss; the bridge between probability and training objectives.</p>
</li>
<li>
<p>KL Divergence &amp; f-Divergences<br>
   Quantifying model mismatch, VI, GAN divergences.</p>
</li>
<li>
<p>Jensen–Shannon &amp; Wasserstein Distances<br>
   GAN stability, geometric learning, distribution metrics.</p>
</li>
<li>
<p>Mutual Information &amp; Estimation Bounds<br>
   Representation learning, contrastive learning, InfoNCE.</p>
</li>
<li>
<p>Variational Inference &amp; ELBO<br>
   VAEs, Bayesian deep learning, posterior approximations.</p>
</li>
<li>
<p>Information Bottleneck &amp; Representation Theory<br>
   Why deep networks compress, and how representations generalize.</p>
</li>
<li>
<p>Summary &amp; Concept Map<br>
   Unifying view of entropy → KL → MI → VI → representation learning.</p>
</li>
</ol></body></html></section><section class="print-page" id="informationtheory-2_entropy" heading-number="5.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-entropy-self-information-cross-entropy-information-measures">Chapter 2 — Entropy, Self-Information, Cross-Entropy &amp; Information Measures<a class="headerlink" href="#informationtheory-2_entropy-chapter-2-entropy-self-information-cross-entropy-information-measures" title="Permanent link">¶</a></h1>
<h2 id="informationtheory-2_entropy-1-self-information-surprisal">1. Self-Information (Surprisal)<a class="headerlink" href="#informationtheory-2_entropy-1-self-information-surprisal" title="Permanent link">¶</a></h2>
<p>Self-information quantifies the <em>surprise</em> of observing an event.</p>
<p>For an event with probability <span class="arithmatex">\(p(x)\)</span>:</p>
<div class="arithmatex">\[
I(x) = - \log_2 p(x)
\]</div>
<p>Why the log?</p>
<ul>
<li>Additivity of independent events  </li>
<li>Probability → information monotonicity  </li>
<li>Log base 2 → units in bits  </li>
<li>Log-likelihoods become additive → ML becomes convex (in many models)</li>
</ul>
<p>Interpretations:</p>
<ul>
<li>Unlikely events carry more information  </li>
<li>Certain events carry zero information  </li>
<li>Foundation of cross-entropy and negative log-likelihood  </li>
</ul>
<p>In ML:  </p>
<ul>
<li>The loss used in classification is simply the surprisal of the correct class.</li>
</ul>
<h2 id="informationtheory-2_entropy-2-entropy-expected-uncertainty">2. Entropy — Expected Uncertainty<a class="headerlink" href="#informationtheory-2_entropy-2-entropy-expected-uncertainty" title="Permanent link">¶</a></h2>
<p>Entropy is the expected self-information:</p>
<div class="arithmatex">\[
H(X) = -\sum_x p(x) \log p(x)
\]</div>
<p>Entropy measures:</p>
<ul>
<li>Uncertainty  </li>
<li>Randomness  </li>
<li>Compressibility  </li>
<li>Difficulty of prediction  </li>
</ul>
<h3 id="informationtheory-2_entropy-key-properties">Key properties:<a class="headerlink" href="#informationtheory-2_entropy-key-properties" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(H(X) = 0\)</span> if a variable is deterministic  </li>
<li>Maximum when distribution is uniform  </li>
<li>Upper bound on achievable compression (Shannon)</li>
</ul>
<h3 id="informationtheory-2_entropy-ml-interpretation">ML Interpretation:<a class="headerlink" href="#informationtheory-2_entropy-ml-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>High entropy labels → noisy dataset → harder learning</li>
<li>Activation entropy reflects network expressiveness</li>
<li>Entropy of output distribution measures model confidence</li>
<li>Entropy regularization improves exploration in RL</li>
</ul>
<h2 id="informationtheory-2_entropy-3-differential-entropy-continuous-entropy">3. Differential Entropy (Continuous Entropy)<a class="headerlink" href="#informationtheory-2_entropy-3-differential-entropy-continuous-entropy" title="Permanent link">¶</a></h2>
<p>For continuous variables:</p>
<div class="arithmatex">\[
h(X) = -\int p(x) \log p(x)\,dx
\]</div>
<p>Important differences:</p>
<ul>
<li>Can be negative</li>
<li>Not invariant under reparameterization</li>
<li>Not comparable between different coordinate systems</li>
</ul>
<h3 id="informationtheory-2_entropy-why-it-matters-in-ml">Why it matters in ML:<a class="headerlink" href="#informationtheory-2_entropy-why-it-matters-in-ml" title="Permanent link">¶</a></h3>
<ul>
<li>VAEs use continuous latent variables <span class="arithmatex">\(z\)</span></li>
<li>Flows and diffusion models use continuous densities</li>
<li>Score-based models estimate gradients of log-densities, not densities directly</li>
</ul>
<p>Differential entropy is not the same thing as Shannon entropy — a common source of confusion.</p>
<h2 id="informationtheory-2_entropy-4-joint-conditional-and-total-entropy">4. Joint, Conditional, and Total Entropy<a class="headerlink" href="#informationtheory-2_entropy-4-joint-conditional-and-total-entropy" title="Permanent link">¶</a></h2>
<h3 id="informationtheory-2_entropy-joint-entropy">Joint entropy:<a class="headerlink" href="#informationtheory-2_entropy-joint-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(X,Y) = -\sum_{x,y} p(x,y)\log p(x,y)
\]</div>
<h3 id="informationtheory-2_entropy-conditional-entropy">Conditional entropy:<a class="headerlink" href="#informationtheory-2_entropy-conditional-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(Y|X) = -\sum_{x,y} p(x,y)\log p(y|x)
\]</div>
<p>Interpretation:</p>
<ul>
<li>Average residual uncertainty in <span class="arithmatex">\(Y\)</span> after observing <span class="arithmatex">\(X\)</span></li>
</ul>
<h3 id="informationtheory-2_entropy-chain-rule-of-entropy">Chain rule of entropy:<a class="headerlink" href="#informationtheory-2_entropy-chain-rule-of-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(X,Y) = H(X) + H(Y|X)
\]</div>
<p>This rule is foundational for:</p>
<ul>
<li>Autoregressive modeling  </li>
<li>Sequence modeling  </li>
<li>Transformers (predictive factorization)  </li>
<li>Bayesian networks  </li>
</ul>
<h2 id="informationtheory-2_entropy-5-cross-entropy-coding-p-using-q">5. Cross-Entropy — Coding <span class="arithmatex">\(p\)</span> Using <span class="arithmatex">\(q\)</span><a class="headerlink" href="#informationtheory-2_entropy-5-cross-entropy-coding-p-using-q" title="Permanent link">¶</a></h2>
<p>Cross-entropy is the expected surprise under model <span class="arithmatex">\(q\)</span>:</p>
<div class="arithmatex">\[
H(p, q) = -\sum_x p(x)\log q(x)
\]</div>
<h3 id="informationtheory-2_entropy-crucial-identity">Crucial identity:<a class="headerlink" href="#informationtheory-2_entropy-crucial-identity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(p, q) = H(p) + D_{\text{KL}}(p\|q)
\]</div>
<p>Meaning:</p>
<ul>
<li>True entropy + penalty for using the wrong distribution</li>
<li>Cross-entropy ≥ entropy</li>
</ul>
<h3 id="informationtheory-2_entropy-ml-interpretation_1">ML Interpretation:<a class="headerlink" href="#informationtheory-2_entropy-ml-interpretation_1" title="Permanent link">¶</a></h3>
<p>Cross-entropy = Negative Log Likelihood:</p>
<div class="arithmatex">\[
\mathcal{L} = - \log q(y_{\text{true}})
\]</div>
<p>This powers:</p>
<ul>
<li>Softmax classifiers  </li>
<li>Logistic regression  </li>
<li>Transformers (next-token prediction)  </li>
<li>Language models (autoregressive LM)  </li>
<li>Image segmentation (pixel-wise CE)  </li>
</ul>
<p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>
<h2 id="informationtheory-2_entropy-6-perplexity-entropy-in-language-modeling">6. Perplexity — Entropy in Language Modeling<a class="headerlink" href="#informationtheory-2_entropy-6-perplexity-entropy-in-language-modeling" title="Permanent link">¶</a></h2>
<p>Perplexity is:</p>
<div class="arithmatex">\[
\text{PPL} = 2^{H}
\]</div>
<p>Interpretation:</p>
<ul>
<li>The “effective vocabulary size” the model thinks it must guess from</li>
<li>Lower perplexity = better language model</li>
</ul>
<p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>
<h2 id="informationtheory-2_entropy-7-mutual-information-information-shared-between-variables">7. Mutual Information — Information Shared Between Variables<a class="headerlink" href="#informationtheory-2_entropy-7-mutual-information-information-shared-between-variables" title="Permanent link">¶</a></h2>
<div class="arithmatex">\[
I(X;Y) = D_{\text{KL}}(p(x,y)\|p(x)p(y))
\]</div>
<p>MI measures:</p>
<ul>
<li>How much knowing <span class="arithmatex">\(X\)</span> tells us about <span class="arithmatex">\(Y\)</span></li>
<li>Reduction in entropy of one variable after observing the other</li>
</ul>
<h3 id="informationtheory-2_entropy-equivalent-forms">Equivalent forms:<a class="headerlink" href="#informationtheory-2_entropy-equivalent-forms" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
I(X;Y) = H(X) - H(X|Y)
\]</div>
<div class="arithmatex">\[
I(X;Y) = H(X) + H(Y) - H(X,Y)
\]</div>
<p>MI links entropy and KL divergence into a unified measure of dependence.</p>
<h3 id="informationtheory-2_entropy-why-mi-is-critical-in-ml">Why MI is critical in ML:<a class="headerlink" href="#informationtheory-2_entropy-why-mi-is-critical-in-ml" title="Permanent link">¶</a></h3>
<ul>
<li>Representation learning (maximize MI with labels)</li>
<li>Contrastive learning (InfoNCE is a lower bound to MI)</li>
<li>InfoGAN (maximize MI between latent code and output)</li>
<li>Feature selection (choose features with highest MI to labels)</li>
<li>Stochastic encoders control MI with constraints</li>
</ul>
<h2 id="informationtheory-2_entropy-8-the-data-processing-inequality-dpi">8. The Data Processing Inequality (DPI)<a class="headerlink" href="#informationtheory-2_entropy-8-the-data-processing-inequality-dpi" title="Permanent link">¶</a></h2>
<p>If:</p>
<div class="arithmatex">\[
X \rightarrow Z \rightarrow Y
\]</div>
<p>is a Markov chain, then:</p>
<div class="arithmatex">\[
I(X;Y) \le I(X;Z)
\]</div>
<p>Meaning:</p>
<ul>
<li>Processing or compressing data cannot add information</li>
<li>Neural networks cannot create information about the input<br>
  — they can only discard or transform it</li>
</ul>
<p>ML relevance:</p>
<ul>
<li>Explains why deeper layers become more task-specialized  </li>
<li>Supports the Information Bottleneck theory in deep learning  </li>
<li>Ensures that any learned representation is bounded by input information  </li>
<li>Helps analyze generalization and compression in deep nets</li>
</ul>
<h2 id="informationtheory-2_entropy-9-entropy-in-neural-networks">9. Entropy in Neural Networks<a class="headerlink" href="#informationtheory-2_entropy-9-entropy-in-neural-networks" title="Permanent link">¶</a></h2>
<p>Entropy plays multiple roles in deep learning:</p>
<h3 id="informationtheory-2_entropy-output-entropy">Output entropy<a class="headerlink" href="#informationtheory-2_entropy-output-entropy" title="Permanent link">¶</a></h3>
<p>Low entropy → confident predictions<br>
High entropy → uncertainty</p>
<h3 id="informationtheory-2_entropy-entropy-of-hidden-representations">Entropy of hidden representations<a class="headerlink" href="#informationtheory-2_entropy-entropy-of-hidden-representations" title="Permanent link">¶</a></h3>
<ul>
<li>Early layers: reduce entropy (denoising)  </li>
<li>Deep layers: compress irrelevant information  </li>
<li>Good representations retain low entropy but high MI with labels</li>
</ul>
<h3 id="informationtheory-2_entropy-entropy-regularization-in-rl">Entropy regularization in RL<a class="headerlink" href="#informationtheory-2_entropy-entropy-regularization-in-rl" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
J(\pi) += \beta H(\pi(\cdot|s))
</script>
encourages exploration.</p>
<h3 id="informationtheory-2_entropy-dropout-increases-entropy">Dropout increases entropy<a class="headerlink" href="#informationtheory-2_entropy-dropout-increases-entropy" title="Permanent link">¶</a></h3>
<p>forcing models to encode more robust representations.</p></body></html></section><section class="print-page" id="informationtheory-3_kl" heading-number="5.3"><h1 id="informationtheory-3_kl-informationtheory-3_kl">3. Kullback-Leibler Divergence</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Chapter 3 — KL Divergence, f-Divergences, Jensen–Shannon Divergence, and Wasserstein Distance</p>
<p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>
<h2 id="informationtheory-3_kl-1-kl-divergence-measuring-distribution-mismatch">1. KL Divergence: Measuring Distribution Mismatch<a class="headerlink" href="#informationtheory-3_kl-1-kl-divergence-measuring-distribution-mismatch" title="Permanent link">¶</a></h2>
<p>The Kullback–Leibler (KL) divergence measures how different two probability distributions are. For distributions <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(p\|q)
= \sum_x p(x)\log\frac{p(x)}{q(x)}.
\]</div>
<p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from <span class="arithmatex">\(p\)</span> using a code optimized for <span class="arithmatex">\(q\)</span>. If <span class="arithmatex">\(q\)</span> assigns very low probability to events that occur frequently under <span class="arithmatex">\(p\)</span>, the KL divergence becomes large.</p>
<h3 id="informationtheory-3_kl-key-properties-of-kl-divergence">Key properties of KL divergence<a class="headerlink" href="#informationtheory-3_kl-key-properties-of-kl-divergence" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Non-negative<br>
<script type="math/tex; mode=display">
   D_{\text{KL}}(p\|q) \ge 0.
   </script>
</p>
</li>
<li>
<p>Zero only when the two distributions are identical.</p>
</li>
<li>
<p>Asymmetric<br>
<script type="math/tex; mode=display">
   D_{\text{KL}}(p\|q) \ne D_{\text{KL}}(q\|p).
   </script>
</p>
</li>
<li>
<p>Not a true metric, since it fails the triangle inequality.</p>
</li>
<li>
<p>Can be infinite when <span class="arithmatex">\(p(x) &gt; 0\)</span> but <span class="arithmatex">\(q(x) = 0\)</span>.<br>
   This is a crucial issue in generative modeling, where such mismatches occur frequently.</p>
</li>
</ol>
<h2 id="informationtheory-3_kl-2-kl-divergence-in-machine-learning">2. KL Divergence in Machine Learning<a class="headerlink" href="#informationtheory-3_kl-2-kl-divergence-in-machine-learning" title="Permanent link">¶</a></h2>
<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>
<h3 id="informationtheory-3_kl-21-maximum-likelihood-as-forward-kl-minimization">2.1 Maximum likelihood as forward KL minimization<a class="headerlink" href="#informationtheory-3_kl-21-maximum-likelihood-as-forward-kl-minimization" title="Permanent link">¶</a></h3>
<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p>
<div class="arithmatex">\[
\theta^*
= \arg\min_\theta D_{\text{KL}}(p_{\text{data}} \,\|\, q_\theta).
\]</div>
<p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p>
<p>This produces mode-covering behavior, which is characteristic of:</p>
<ul>
<li>normalizing flows  </li>
<li>autoregressive models  </li>
<li>density estimation models trained via log-likelihood  </li>
</ul>
<h3 id="informationtheory-3_kl-22-kl-divergence-in-variational-inference-vi">2.2 KL divergence in variational inference (VI)<a class="headerlink" href="#informationtheory-3_kl-22-kl-divergence-in-variational-inference-vi" title="Permanent link">¶</a></h3>
<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior <span class="arithmatex">\(q(z|x)\)</span> and the true posterior <span class="arithmatex">\(p(z|x)\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Since the true posterior is typically intractable, VAEs approximate this with:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z)).
\]</div>
<p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p>
<p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative “safe” solution.</p>
<h3 id="informationtheory-3_kl-23-kl-divergence-in-reinforcement-learning">2.3 KL divergence in reinforcement learning<a class="headerlink" href="#informationtheory-3_kl-23-kl-divergence-in-reinforcement-learning" title="Permanent link">¶</a></h3>
<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p>
<div class="arithmatex">\[
D_{\text{KL}}(\pi_{\text{old}} \,\|\, \pi_{\text{new}}).
\]</div>
<p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>
<h3 id="informationtheory-3_kl-24-kl-divergence-in-distillation-and-compression">2.4 KL divergence in distillation and compression<a class="headerlink" href="#informationtheory-3_kl-24-kl-divergence-in-distillation-and-compression" title="Permanent link">¶</a></h3>
<p>KL divergence compares two probability distributions directly and is used for:</p>
<ul>
<li>teacher–student distillation  </li>
<li>compressing large models into smaller ones  </li>
<li>aligning probability distributions across layers  </li>
<li>calibrating output probabilities  </li>
</ul>
<p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>
<h2 id="informationtheory-3_kl-3-understanding-kl-behavior-mode-covering-vs-mode-seeking">3. Understanding KL Behavior: Mode Covering vs. Mode Seeking<a class="headerlink" href="#informationtheory-3_kl-3-understanding-kl-behavior-mode-covering-vs-mode-seeking" title="Permanent link">¶</a></h2>
<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>
<h3 id="informationtheory-3_kl-forward-kl-d_textklpq">Forward KL: <span class="arithmatex">\(D_{\text{KL}}(p\|q)\)</span><a class="headerlink" href="#informationtheory-3_kl-forward-kl-d_textklpq" title="Permanent link">¶</a></h3>
<p><em>(Used in maximum likelihood, flows → mode covering)</em></p>
<p>Forward KL asks whether the model <span class="arithmatex">\(q\)</span> assigns sufficient probability wherever the data distribution <span class="arithmatex">\(p\)</span> has mass:</p>
<blockquote>
<p>“Does the model assign enough probability to every place where the data occurs?”</p>
</blockquote>
<p>If <span class="arithmatex">\(q\)</span> misses even a small region where <span class="arithmatex">\(p\)</span> has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p>
<p>Result: <strong>mode covering</strong><br>
The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p>
<p>Flows and MLE-based models display this behavior.</p>
<h3 id="informationtheory-3_kl-reverse-kl-d_textklqp">Reverse KL: <span class="arithmatex">\(D_{\text{KL}}(q\|p)\)</span><a class="headerlink" href="#informationtheory-3_kl-reverse-kl-d_textklqp" title="Permanent link">¶</a></h3>
<p><em>(Used in VI, VAEs, GAN-like behavior → mode seeking)</em></p>
<p>Reverse KL asks the opposite question:</p>
<blockquote>
<p>“Is the model placing probability in places where the data distribution is very small or zero?”</p>
</blockquote>
<p>Reverse KL heavily penalizes placing mass in low-density regions of <span class="arithmatex">\(p\)</span>, making the model conservative.</p>
<p>Result: <strong>mode seeking</strong><br>
The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p>
<p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>
<h2 id="informationtheory-3_kl-4-f-divergences-a-unified-family-of-divergences">4. f-Divergences: A Unified Family of Divergences<a class="headerlink" href="#informationtheory-3_kl-4-f-divergences-a-unified-family-of-divergences" title="Permanent link">¶</a></h2>
<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function <span class="arithmatex">\(f\)</span>:</p>
<div class="arithmatex">\[
D_f(p\|q) = \sum_x q(x)\, f\!\left(\frac{p(x)}{q(x)}\right).
\]</div>
<h2 id="informationtheory-3_kl-5-jensenshannon-divergence-the-original-gan-divergence">5. Jensen–Shannon Divergence: The Original GAN Divergence<a class="headerlink" href="#informationtheory-3_kl-5-jensenshannon-divergence-the-original-gan-divergence" title="Permanent link">¶</a></h2>
<p>The Jensen–Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p>
<div class="arithmatex">\[
\text{JS}(p\|q)
= \frac12 D_{\text{KL}}(p\|m)
+ \frac12 D_{\text{KL}}(q\|m)
\]</div>
<p>where the mixture is:</p>
<div class="arithmatex">\[
m = \frac12(p+q).
\]</div>
<p>JS divergence is symmetric and always lies between 0 and <span class="arithmatex">\(\log 2\)</span>.</p>
<h3 id="informationtheory-3_kl-why-js-appears-in-gans">Why JS appears in GANs<a class="headerlink" href="#informationtheory-3_kl-why-js-appears-in-gans" title="Permanent link">¶</a></h3>
<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p>
<div class="arithmatex">\[
\text{JS}(p\|q) - \log 2.
\]</div>
<p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>
<h2 id="informationtheory-3_kl-6-why-js-divergence-causes-gan-instability">6. Why JS Divergence Causes GAN Instability<a class="headerlink" href="#informationtheory-3_kl-6-why-js-divergence-causes-gan-instability" title="Permanent link">¶</a></h2>
<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> are disjoint:</p>
<div class="arithmatex">\[
\text{JS}(p\|q) = \log 2.
\]</div>
<p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p>
<p>Consequences:</p>
<ol>
<li>The discriminator immediately becomes perfect.  </li>
<li>The generator stops receiving meaningful gradients.  </li>
<li>Training often collapses, oscillates, or diverges.  </li>
</ol>
<p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>
<h2 id="informationtheory-3_kl-7-total-variation-and-hellinger-distances">7. Total Variation and Hellinger Distances<a class="headerlink" href="#informationtheory-3_kl-7-total-variation-and-hellinger-distances" title="Permanent link">¶</a></h2>
<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>
<h3 id="informationtheory-3_kl-71-total-variation-tv-distance">7.1 Total Variation (TV) Distance<a class="headerlink" href="#informationtheory-3_kl-71-total-variation-tv-distance" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\text{TV}(p,q) = \frac12\sum_x |p(x)-q(x)|.
\]</div>
<p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform <span class="arithmatex">\(p\)</span> into <span class="arithmatex">\(q\)</span>.</p>
<p>Applications in ML:</p>
<ul>
<li>Robustness under distribution shift  </li>
<li>Generalization bounds (PAC-Bayes)  </li>
<li>Fairness and safety  </li>
</ul>
<h3 id="informationtheory-3_kl-72-hellinger-distance">7.2 Hellinger Distance<a class="headerlink" href="#informationtheory-3_kl-72-hellinger-distance" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H^2(p,q)
= \frac12 \sum_x\left(\sqrt{p(x)} - \sqrt{q(x)}\right)^2.
\]</div>
<p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p>
<p>Uses in ML include:</p>
<ul>
<li>Robust statistics  </li>
<li>Domain adaptation  </li>
<li>Generalization theory  </li>
<li>Some GAN formulations  </li>
</ul>
<h2 id="informationtheory-3_kl-8-wasserstein-distance-geometry-of-probability-distributions">8. Wasserstein Distance: Geometry of Probability Distributions<a class="headerlink" href="#informationtheory-3_kl-8-wasserstein-distance-geometry-of-probability-distributions" title="Permanent link">¶</a></h2>
<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>
<h3 id="informationtheory-3_kl-81-primal-form-earth-mover-interpretation">8.1 Primal form (Earth Mover interpretation)<a class="headerlink" href="#informationtheory-3_kl-81-primal-form-earth-mover-interpretation" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
W(p,q)
= \inf_{\gamma \in \Gamma(p,q)}
\mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|].
\]</div>
<p>It seeks the transport plan <span class="arithmatex">\(\gamma\)</span> requiring the least expected effort to turn <span class="arithmatex">\(p\)</span> into <span class="arithmatex">\(q\)</span>.</p>
<h3 id="informationtheory-3_kl-82-dual-form-used-in-wgan">8.2 Dual form (used in WGAN)<a class="headerlink" href="#informationtheory-3_kl-82-dual-form-used-in-wgan" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
W(p,q)
= \sup_{\|f\|_L\le 1}
\left(\mathbb{E}_p[f(x)]
     - \mathbb{E}_q[f(x)]\right).
\]</div>
<p>GANs implement <span class="arithmatex">\(f\)</span> as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>
<h3 id="informationtheory-3_kl-83-why-wasserstein-solves-gan-instability">8.3 Why Wasserstein solves GAN instability<a class="headerlink" href="#informationtheory-3_kl-83-why-wasserstein-solves-gan-instability" title="Permanent link">¶</a></h3>
<p>Wasserstein distance has several advantages:</p>
<ul>
<li>Provides informative gradients even with no overlap  </li>
<li>Reflects the actual geometry of the data space  </li>
<li>Avoids the saturation and vanishing gradients of JS divergence  </li>
<li>Works reliably in high-dimensional spaces  </li>
</ul>
<p>These properties make Wasserstein GANs far more stable than classical GANs.</p>
<h3 id="informationtheory-3_kl-84-wgan-gp-gradient-penalty">8.4 WGAN-GP: Gradient Penalty<a class="headerlink" href="#informationtheory-3_kl-84-wgan-gp-gradient-penalty" title="Permanent link">¶</a></h3>
<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p>
<div class="arithmatex">\[
\lambda(\|\nabla_x f(x)\|_2 - 1)^2.
\]</div>
<p>This produces smoother and more stable training compared to weight clipping.</p>
<h2 id="informationtheory-3_kl-9-divergence-versus-distance">9. Divergence versus Distance<a class="headerlink" href="#informationtheory-3_kl-9-divergence-versus-distance" title="Permanent link">¶</a></h2>
<p>Divergences such as KL and JS:</p>
<ul>
<li>may be infinite  </li>
<li>are asymmetric  </li>
<li>do not behave well when distributions have disjoint support  </li>
</ul>
<p>Distances such as Wasserstein, TV, and Hellinger:</p>
<ul>
<li>are symmetric  </li>
<li>obey triangle inequality  </li>
<li>remain meaningful under distribution shift  </li>
</ul>
<p>In machine learning:</p>
<ul>
<li>Divergences are useful for inference and likelihood  </li>
<li>Distances are useful for generative modeling and geometry  </li>
</ul>
<h2 id="informationtheory-3_kl-10-why-divergences-fail-in-high-dimensions">10. Why Divergences Fail in High Dimensions<a class="headerlink" href="#informationtheory-3_kl-10-why-divergences-fail-in-high-dimensions" title="Permanent link">¶</a></h2>
<p>In high-dimensional spaces:</p>
<ul>
<li>Real and generated samples rarely overlap  </li>
<li>KL divergence often becomes infinite  </li>
<li>JS divergence becomes flat  </li>
<li>Gradients vanish  </li>
</ul>
<p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p>
<hr>
<p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p>
<p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen–Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p>
<p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p></body></html></section><section class="print-page" id="informationtheory-4_bayes" heading-number="5.4"><h1 id="informationtheory-4_bayes-informationtheory-4_bayes">4. Bayesian Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p>
<p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>
<h2 id="informationtheory-4_bayes-1-bayes-rule">1. Bayes’ Rule<a class="headerlink" href="#informationtheory-4_bayes-1-bayes-rule" title="Permanent link">¶</a></h2>
<p>Bayes’ theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable <span class="arithmatex">\(z\)</span> and an observed variable <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x|z)\,p(z)}{p(x)}.
\]</div>
<p>Each term has a clear interpretation.</p>
<ul>
<li><span class="arithmatex">\(p(z)\)</span>: prior belief about the unknown variable  </li>
<li><span class="arithmatex">\(p(x|z)\)</span>: likelihood of observing <span class="arithmatex">\(x\)</span> given <span class="arithmatex">\(z\)</span>  </li>
<li><span class="arithmatex">\(p(x)\)</span>: marginal likelihood or evidence  </li>
<li><span class="arithmatex">\(p(z|x)\)</span>: posterior distribution after observing data  </li>
</ul>
<p>Bayesian inference is the task of computing <span class="arithmatex">\(p(z|x)\)</span>.</p>
<h2 id="informationtheory-4_bayes-2-priors-encoding-assumptions-about-hidden-variables">2. Priors: Encoding Assumptions About Hidden Variables<a class="headerlink" href="#informationtheory-4_bayes-2-priors-encoding-assumptions-about-hidden-variables" title="Permanent link">¶</a></h2>
<p>The prior <span class="arithmatex">\(p(z)\)</span> expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>
<h3 id="informationtheory-4_bayes-21-regularization">2.1 Regularization<a class="headerlink" href="#informationtheory-4_bayes-21-regularization" title="Permanent link">¶</a></h3>
<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields <span class="arithmatex">\(L_2\)</span> regularization.</p>
<h3 id="informationtheory-4_bayes-22-structural-assumptions">2.2 Structural assumptions<a class="headerlink" href="#informationtheory-4_bayes-22-structural-assumptions" title="Permanent link">¶</a></h3>
<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>
<h3 id="informationtheory-4_bayes-23-uncertainty">2.3 Uncertainty<a class="headerlink" href="#informationtheory-4_bayes-23-uncertainty" title="Permanent link">¶</a></h3>
<p>The prior makes explicit that before observing data, we do not know the true value of <span class="arithmatex">\(z\)</span>.</p>
<h3 id="informationtheory-4_bayes-24-generative-modeling">2.4 Generative modeling<a class="headerlink" href="#informationtheory-4_bayes-24-generative-modeling" title="Permanent link">¶</a></h3>
<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>
<h2 id="informationtheory-4_bayes-3-likelihood-connecting-latent-variables-to-observed-data">3. Likelihood: Connecting Latent Variables to Observed Data<a class="headerlink" href="#informationtheory-4_bayes-3-likelihood-connecting-latent-variables-to-observed-data" title="Permanent link">¶</a></h2>
<p>The likelihood <span class="arithmatex">\(p(x|z)\)</span> describes how the data are generated from latent causes. In many generative models:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> represents latent structure  </li>
<li><span class="arithmatex">\(x\)</span> represents an image, time series, or text  </li>
<li><span class="arithmatex">\(p(x|z)\)</span> is parameterized by a neural network decoder  </li>
</ul>
<p>The likelihood term encourages the latent variable <span class="arithmatex">\(z\)</span> to explain the observed data.</p>
<h2 id="informationtheory-4_bayes-4-the-posterior-what-we-really-want-to-compute">4. The Posterior: What We Really Want to Compute<a class="headerlink" href="#informationtheory-4_bayes-4-the-posterior-what-we-really-want-to-compute" title="Permanent link">¶</a></h2>
<p>The goal of Bayesian inference is the posterior:</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x|z)p(z)}{p(x)}.
\]</div>
<p>The posterior expresses how our belief about <span class="arithmatex">\(z\)</span> changes after seeing <span class="arithmatex">\(x\)</span>. It incorporates both:</p>
<ul>
<li>prior knowledge  </li>
<li>evidence from data  </li>
</ul>
<p>Unfortunately, computing this posterior is usually intractable.</p>
<h2 id="informationtheory-4_bayes-5-why-exact-inference-is-hard">5. Why Exact Inference Is Hard<a class="headerlink" href="#informationtheory-4_bayes-5-why-exact-inference-is-hard" title="Permanent link">¶</a></h2>
<p>The denominator in Bayes’ rule is the marginal likelihood:</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz.
\]</div>
<p>This integral is often impossible to evaluate directly because:</p>
<ul>
<li>the latent space <span class="arithmatex">\(z\)</span> can be high-dimensional  </li>
<li>the joint distribution <span class="arithmatex">\(p(x,z)\)</span> may involve a complex neural network  </li>
<li>the integral has no analytic form  </li>
</ul>
<p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>
<h2 id="informationtheory-4_bayes-6-maximum-a-posteriori-map-vs-full-bayesian-inference">6. Maximum a Posteriori (MAP) vs Full Bayesian Inference<a class="headerlink" href="#informationtheory-4_bayes-6-maximum-a-posteriori-map-vs-full-bayesian-inference" title="Permanent link">¶</a></h2>
<p>There are two kinds of Bayesian computation.</p>
<h3 id="informationtheory-4_bayes-61-map-estimation">6.1 MAP estimation<a class="headerlink" href="#informationtheory-4_bayes-61-map-estimation" title="Permanent link">¶</a></h3>
<p>MAP finds the <em>single most likely</em> value of <span class="arithmatex">\(z\)</span>:</p>
<div class="arithmatex">\[
z_{\text{MAP}}
= \arg\max_z\, p(z|x).
\]</div>
<p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>
<h3 id="informationtheory-4_bayes-62-full-posterior-inference">6.2 Full posterior inference<a class="headerlink" href="#informationtheory-4_bayes-62-full-posterior-inference" title="Permanent link">¶</a></h3>
<p>The full posterior <span class="arithmatex">\(p(z|x)\)</span> describes a <em>distribution</em> over possible values of <span class="arithmatex">\(z\)</span>, reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>
<h2 id="informationtheory-4_bayes-7-bayesian-latent-variable-models">7. Bayesian Latent-Variable Models<a class="headerlink" href="#informationtheory-4_bayes-7-bayesian-latent-variable-models" title="Permanent link">¶</a></h2>
<p>Many generative models are Bayesian latent-variable models with:</p>
<ol>
<li>
<p>a prior over latent variables<br>
<script type="math/tex; mode=display">
   z \sim p(z)
   </script>
</p>
</li>
<li>
<p>a conditional likelihood<br>
<script type="math/tex; mode=display">
   x \sim p(x|z)
   </script>
</p>
</li>
<li>
<p>a posterior<br>
<script type="math/tex; mode=display">
   p(z|x)
   </script>
</p>
</li>
</ol>
<p>Examples include:</p>
<ul>
<li>VAEs  </li>
<li>mixture models  </li>
<li>topic models  </li>
<li>probabilistic PCA  </li>
<li>diffusion models (in a specific sense)  </li>
</ul>
<p>Bayesian inference is the foundation of these models.</p>
<h2 id="informationtheory-4_bayes-8-the-evidence-and-its-importance">8. The Evidence and Its Importance<a class="headerlink" href="#informationtheory-4_bayes-8-the-evidence-and-its-importance" title="Permanent link">¶</a></h2>
<p>The marginal likelihood, also called the evidence:</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz
\]</div>
<p>plays several roles:</p>
<ul>
<li>It normalizes the posterior.  </li>
<li>It evaluates how well a model explains data.  </li>
<li>It is used in Bayesian model comparison.  </li>
<li>Its logarithm appears in training objectives for VAEs and diffusion models.</li>
</ul>
<p>Maximizing evidence corresponds to learning a model that explains the data well.</p>
<h2 id="informationtheory-4_bayes-9-bayesian-interpretation-of-kl-divergence">9. Bayesian Interpretation of KL Divergence<a class="headerlink" href="#informationtheory-4_bayes-9-bayesian-interpretation-of-kl-divergence" title="Permanent link">¶</a></h2>
<p>KL divergence naturally appears when comparing an approximate posterior <span class="arithmatex">\(q(z|x)\)</span> with the true posterior <span class="arithmatex">\(p(z|x)\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Minimizing this KL divergence means making the approximation <span class="arithmatex">\(q\)</span> as close as possible to the exact posterior.</p>
<p>This forms the basis of variational inference.</p>
<hr>
<h2 id="informationtheory-4_bayes-10-why-we-need-variational-inference">10. Why We Need Variational Inference<a class="headerlink" href="#informationtheory-4_bayes-10-why-we-need-variational-inference" title="Permanent link">¶</a></h2>
<p>Because the true posterior is intractable, we introduce a simpler distribution <span class="arithmatex">\(q(z|x)\)</span> and optimize it to approximate <span class="arithmatex">\(p(z|x)\)</span>.</p>
<p>We cannot compute:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x))
\]</div>
<p>directly, because <span class="arithmatex">\(p(z|x)\)</span> depends on <span class="arithmatex">\(p(x)\)</span>, which is the intractable integral.</p>
<p>Variational inference resolves this by rewriting <span class="arithmatex">\(\log p(x)\)</span> and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p>
<p>This is the topic of the next chapter.</p>
<hr>
<p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes’ rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p>
<p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p></body></html></section><section class="print-page" id="informationtheory-5_mc_intro" heading-number="5.5"><h1 id="informationtheory-5_mc_intro-informationtheory-5_mc_intro">5. Probability toolbox</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}, \qquad 
p(x) = \int p(x,z)\,dz.
\]</div>
<p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p>
<p>This section introduces four major families of methods:</p>
<ol>
<li>complete enumeration  </li>
<li>Laplace approximation  </li>
<li>Monte Carlo methods  </li>
<li>variational methods  </li>
</ol>
<p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>
<h2 id="informationtheory-5_mc_intro-1-complete-enumeration">1. Complete Enumeration<a class="headerlink" href="#informationtheory-5_mc_intro-1-complete-enumeration" title="Permanent link">¶</a></h2>
<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p>
<div class="arithmatex">\[
p(x) = \sum_z p(x,z)
\quad \text{or} \quad
p(x) = \int p(x,z)\,dz.
\]</div>
<p>This is feasible only when:</p>
<ul>
<li>the latent variable is low dimensional  </li>
<li>the domain is small or discrete  </li>
<li>the joint distribution has a simple closed form  </li>
</ul>
<p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>
<h2 id="informationtheory-5_mc_intro-2-laplace-approximation">2. Laplace Approximation<a class="headerlink" href="#informationtheory-5_mc_intro-2-laplace-approximation" title="Permanent link">¶</a></h2>
<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p>
<p>Given a posterior</p>
<div class="arithmatex">\[
p(z|x) \propto p(x,z),
\]</div>
<p>the Laplace approximation fits a Gaussian distribution</p>
<div class="arithmatex">\[
q(z|x) \approx \mathcal{N}(z_{\text{MAP}}, H^{-1}),
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z_{\text{MAP}}\)</span> is the mode of <span class="arithmatex">\(p(z|x)\)</span>  </li>
<li><span class="arithmatex">\(H\)</span> is the Hessian of <span class="arithmatex">\(-\log p(z|x)\)</span> at the mode  </li>
</ul>
<p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>
<h2 id="informationtheory-5_mc_intro-3-monte-carlo-methods">3. Monte Carlo Methods<a class="headerlink" href="#informationtheory-5_mc_intro-3-monte-carlo-methods" title="Permanent link">¶</a></h2>
<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z|x)}[f(z)] 
\approx \frac{1}{N}\sum_{i=1}^N f(z_i),
\qquad z_i \sim p(z|x).
\]</div>
<p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p>
<p>Sampling strategies fall into two groups:</p>
<ul>
<li>independent sampling  </li>
<li>Markov chain–based sampling (MCMC)  </li>
</ul>
<p>The next chapter explains Monte Carlo and sampling methods in detail.</p>
<h2 id="informationtheory-5_mc_intro-4-variational-methods">4. Variational Methods<a class="headerlink" href="#informationtheory-5_mc_intro-4-variational-methods" title="Permanent link">¶</a></h2>
<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from <span class="arithmatex">\(p(z|x)\)</span>, we introduce a distribution <span class="arithmatex">\(q(z|x)\)</span> and optimize it to be close to the true posterior. The objective is to minimize</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Because <span class="arithmatex">\(p(z|x)\)</span> is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\log p(x)
=
\mathcal{L}(x) + D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>
<h2 id="informationtheory-5_mc_intro-summary">Summary<a class="headerlink" href="#informationtheory-5_mc_intro-summary" title="Permanent link">¶</a></h2>
<p>Approximate inference methods can be understood as four major strategies:</p>
<ul>
<li>complete enumeration: exact but rarely feasible  </li>
<li>Laplace approximation: fast Gaussian approximation near the mode  </li>
<li>Monte Carlo methods: sampling-based numerical estimation  </li>
<li>variational methods: optimization-based posterior approximation  </li>
</ul>
<p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p></body></html></section><section class="print-page" id="informationtheory-6_mc" heading-number="5.6"><h1 id="informationtheory-6_mc-informationtheory-6_mc">6. Monte Carlo Methods</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p>
<p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>
<h2 id="informationtheory-6_mc-1-the-goal-of-sampling">1. The Goal of Sampling<a class="headerlink" href="#informationtheory-6_mc-1-the-goal-of-sampling" title="Permanent link">¶</a></h2>
<p>Many problems require computing expectations of the form</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)] = \int f(z)\,p(z)\,dz,
\]</div>
<p>or evaluating posterior quantities such as</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p>
<p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>
<h2 id="informationtheory-6_mc-2-independent-sampling">2. Independent Sampling<a class="headerlink" href="#informationtheory-6_mc-2-independent-sampling" title="Permanent link">¶</a></h2>
<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p>
<p>These methods work best when:</p>
<ul>
<li>sampling directly from <span class="arithmatex">\(p(z)\)</span> is tractable  </li>
<li>the distribution is low-dimensional  </li>
<li>the support is simple (e.g., Gaussian, uniform)  </li>
</ul>
<h3 id="informationtheory-6_mc-21-direct-sampling">2.1 Direct Sampling<a class="headerlink" href="#informationtheory-6_mc-21-direct-sampling" title="Permanent link">¶</a></h3>
<p>When the distribution has an invertible CDF <span class="arithmatex">\(F(z)\)</span>:</p>
<ol>
<li>sample <span class="arithmatex">\(u \sim \text{Uniform}(0,1)\)</span>  </li>
<li>compute <span class="arithmatex">\(z = F^{-1}(u)\)</span>  </li>
</ol>
<p>This yields exact samples. It is commonly used in:</p>
<ul>
<li>uniform sampling  </li>
<li>exponential distributions  </li>
<li>simple discrete distributions  </li>
</ul>
<h3 id="informationtheory-6_mc-22-importance-sampling">2.2 Importance Sampling<a class="headerlink" href="#informationtheory-6_mc-22-importance-sampling" title="Permanent link">¶</a></h3>
<p>When sampling from <span class="arithmatex">\(p(z)\)</span> is difficult but evaluating <span class="arithmatex">\(p(z)\)</span> is easy, importance sampling draws samples from a proposal distribution <span class="arithmatex">\(q(z)\)</span> and reweights them:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)]
=
\mathbb{E}_{q(z)}\left[f(z)\frac{p(z)}{q(z)}\right].
\]</div>
<p>The weights</p>
<p>
<script type="math/tex; mode=display">(z)=
\frac{q(z)}{p(z)}
</script>  ​</p>
<p>correct for the fact that samples were drawn from <span class="arithmatex">\(q\)</span> instead of <span class="arithmatex">\(p\)</span>.</p>
<p>The main challenge is weight variability. If <span class="arithmatex">\(q(z)\)</span> does not closely match <span class="arithmatex">\(p(z)\)</span>, the ratio <span class="arithmatex">\(p(z)/q(z)\)</span> becomes extremely uneven: most samples have tiny weights, while a few samples have very large weights. This produces high-variance estimates because the estimator becomes dominated by a handful of rare but extremely influential samples. In high-dimensional spaces, designing a proposal <span class="arithmatex">\(q(z)\)</span> that covers the important regions of <span class="arithmatex">\(p(z)\)</span> is especially difficult, making importance sampling unreliable unless the proposal distribution is carefully chosen.</p>
<h3 id="informationtheory-6_mc-23-rejection-sampling">2.3 Rejection Sampling<a class="headerlink" href="#informationtheory-6_mc-23-rejection-sampling" title="Permanent link">¶</a></h3>
<p>Rejection sampling draws exact samples from a target distribution <span class="arithmatex">\(p(z)\)</span> by using a simpler proposal distribution <span class="arithmatex">\(q(z)\)</span> and accepting or rejecting candidate samples based on how well <span class="arithmatex">\(q\)</span> covers <span class="arithmatex">\(p\)</span>. The method requires a constant <span class="arithmatex">\(M\)</span> such that
<script type="math/tex; mode=display">
p(z) \le M q(z) \quad \text{for all } z.
</script>
</p>
<p>This condition ensures that <span class="arithmatex">\(Mq(z)\)</span> forms an envelope that completely contains <span class="arithmatex">\(p(z)\)</span>.</p>
<p>Procedure:</p>
<ol>
<li>sample <span class="arithmatex">\(z \sim q(z)\)</span>  </li>
<li>accept with probability <span class="arithmatex">\(\frac{p(z)}{M q(z)}\)</span>  </li>
</ol>
<p>If accepted, <span class="arithmatex">\(z\)</span> is guaranteed to be an exact draw from <span class="arithmatex">\(p\)</span>.</p>
<p>Rejection sampling is conceptually simple and does not distort the target distribution, but it becomes inefficient in many settings. When <span class="arithmatex">\(q(z)\)</span> is a poor match for <span class="arithmatex">\(p(z)\)</span>, the constant <span class="arithmatex">\(M\)</span> must be large, which means that most samples are rejected. In high-dimensional spaces, the mismatch between <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> typically worsens exponentially, making the acceptance probability extremely small. As a result, rejection sampling is rarely practical for modern high-dimensional machine-learning models, although it remains useful in low-dimensional problems or when <span class="arithmatex">\(q\)</span> can be chosen to closely match <span class="arithmatex">\(p\)</span>.</p>
<h2 id="informationtheory-6_mc-3-monte-carlo-estimation">3. Monte Carlo Estimation<a class="headerlink" href="#informationtheory-6_mc-3-monte-carlo-estimation" title="Permanent link">¶</a></h2>
<p>Monte Carlo approximates expectations by:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)] \approx \frac{1}{N}\sum_{i=1}^N f(z_i),
\quad z_i \sim p(z).
\]</div>
<p>Key properties:</p>
<ul>
<li>error scales as <span class="arithmatex">\(\mathcal{O}(1/\sqrt{N})\)</span>  </li>
<li>works in high dimensions  </li>
<li>accuracy depends on sampling quality  </li>
</ul>
<p>Monte Carlo is the backbone of almost all probabilistic computation.</p>
<h2 id="informationtheory-6_mc-4-markov-chain-monte-carlo-mcmc">4. Markov Chain Monte Carlo (MCMC)<a class="headerlink" href="#informationtheory-6_mc-4-markov-chain-monte-carlo-mcmc" title="Permanent link">¶</a></h2>
<p>When sampling directly from <span class="arithmatex">\(p(z)\)</span> is hard, Markov Chain Monte Carlo constructs a Markov chain</p>
<div class="arithmatex">\[
z_1 \to z_2 \to z_3 \to \cdots
\]</div>
<p>whose stationary distribution is <span class="arithmatex">\(p(z)\)</span>.</p>
<p>After a burn-in period, samples approximate <span class="arithmatex">\(p(z)\)</span> even if individual states are dependent.</p>
<p>MCMC is widely applicable because it does not require the normalization constant of <span class="arithmatex">\(p(z)\)</span>:</p>
<div class="arithmatex">\[
p(z|x) \propto p(x,z).
\]</div>
<h3 id="informationtheory-6_mc-41-metropolishastings-algorithm">4.1 Metropolis–Hastings Algorithm<a class="headerlink" href="#informationtheory-6_mc-41-metropolishastings-algorithm" title="Permanent link">¶</a></h3>
<p>The Metropolis–Hastings (MH) algorithm constructs a Markov chain whose stationary distribution is the target distribution <span class="arithmatex">\(p(z)\)</span>, even when <span class="arithmatex">\(p(z)\)</span> is known only up to a proportionality constant. This makes MH suitable for Bayesian inference, where the posterior is often available only in unnormalized form:</p>
<div class="arithmatex">\[p(z \mid x) \propto p(x, z)\]</div>
<p>MH works by proposing a new point based on the current state and then accepting or rejecting it according to how well it aligns with the target distribution.</p>
<p>Given a current sample <span class="arithmatex">\(z\)</span>, the algorithm proceeds as follows:</p>
<ol>
<li>
<p>propose a new sample <span class="arithmatex">\(z'\)</span> using a proposal distribution <span class="arithmatex">\(z' \sim q(z' \mid z)\)</span>.</p>
</li>
<li>
<p>compute the acceptance probability
     <script type="math/tex; mode=display">
     \alpha = \min\left(1, \frac{p(z')\,q(z|z')}
     {p(z)\,q(z'|z)}
     \right).
     </script>
</p>
</li>
<li>
<p>accept the proposal with probability <span class="arithmatex">\(\alpha(z,z')\)</span>
otherwise remain at the current state If accepted, set <span class="arithmatex">\(z_{t+1} = z'\)</span>, otherwise keep <span class="arithmatex">\(z_{t+1} = z\)</span>.</p>
</li>
</ol>
<p>This simple rule ensures that the Markov chain satisfies detailed balance and converges to the desired distribution <span class="arithmatex">\(p(z)\)</span>.</p>
<p>Metropolis–Hastings is flexible and works with virtually any distribution from which we can evaluate <span class="arithmatex">\(p(z)\)</span> up to a constant. However, its efficiency depends strongly on the proposal distribution. If the proposal steps are too small, the chain performs a random walk and mixes slowly. If the steps are too large, most proposals are rejected. Choosing or adapting the proposal distribution is therefore crucial for performance, especially in high-dimensional settings.</p>
<h3 id="informationtheory-6_mc-42-gibbs-sampling">4.2 Gibbs Sampling<a class="headerlink" href="#informationtheory-6_mc-42-gibbs-sampling" title="Permanent link">¶</a></h3>
<p>Gibbs sampling is a special case of MCMC designed for multivariate distributions where sampling from the full conditional distributions is easy. Instead of proposing a new state and accepting or rejecting it, Gibbs sampling updates one variable at a time by drawing directly from its exact conditional distribution.</p>
<p>For a latent vector:</p>
<div class="arithmatex">\[z = (z_1, z_2, \dots, z_d)\]</div>
<p>a Gibbs update for coordinate <span class="arithmatex">\(i\)</span> samples:</p>
<div class="arithmatex">\[
z_i \sim p(z_i \mid z_{-i}).
\]</div>
<p>where <span class="arithmatex">\(z_{-i}\)</span> denotes all components except <span class="arithmatex">\(z_i\)</span>.</p>
<p>By cycling through all coordinates repeatedly, the Markov chain eventually converges to the target joint distribution <span class="arithmatex">\(p(z)\)</span>.</p>
<p>The key requirement is that each full conditional distribution</p>
<div class="arithmatex">\[p(z_i \mid z_{-i})\]</div>
<p>must be analytically tractable and easy to sample from. When this holds, Gibbs sampling is simple to implement and avoids the accept–reject step of Metropolis–Hastings.</p>
<p>However, Gibbs sampling can mix slowly when variables are strongly correlated, since updating one coordinate at a time may explore the space inefficiently. Gibbs sampling is widely used in models where conditional distributions are naturally available, including:</p>
<ul>
<li>topic models such as Latent Dirichlet Allocation (LDA)</li>
<li>hidden Markov models</li>
<li>Gaussian graphical models</li>
<li>Bayesian networks with conjugate priors</li>
</ul>
<h3 id="informationtheory-6_mc-43-slice-sampling">4.3 Slice Sampling<a class="headerlink" href="#informationtheory-6_mc-43-slice-sampling" title="Permanent link">¶</a></h3>
<p>Slice sampling is an MCMC method that avoids choosing a proposal distribution by sampling uniformly from the region (the slice) where the probability density is above a randomly chosen threshold. Given the current point <span class="arithmatex">\(z\)</span>, slice sampling introduces an auxiliary variable <span class="arithmatex">\(u\)</span>:</p>
<ol>
<li>
<p>Draw a height</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(u \sim \text{Uniform}(0,\, p(z))\)</span>\)</span></p>
</li>
<li>
<p>Define the horizontal slice</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(S = \{ z' : p(z') &gt; u \}\)</span>\)</span></p>
</li>
<li>
<p>Sample the next state</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(z' \sim \text{Uniform}(S)\)</span>\)</span></p>
</li>
</ol>
<p>This procedure constructs a Markov chain whose stationary distribution is <span class="arithmatex">\(p(z)\)</span>. Intuitively, the algorithm first chooses a horizontal level <span class="arithmatex">\(u\)</span> below the current density value and then samples uniformly from the region of the density that lies above this level.</p>
<p>Slice sampling adapts naturally to the local geometry of the target distribution: narrow peaks produce narrow slices, and broad regions produce wide slices, without requiring manual tuning of proposal scales. In practice, slice sampling often mixes better than basic random-walk proposals, especially when the target density varies in amplitude across different regions.</p>
<p>However, slice sampling requires an efficient way to identify or approximate the slice region, which may be challenging in high-dimensional or multimodal settings.</p>
<h2 id="informationtheory-6_mc-5-reducing-random-walk-behaviour">5. Reducing Random-Walk Behaviour<a class="headerlink" href="#informationtheory-6_mc-5-reducing-random-walk-behaviour" title="Permanent link">¶</a></h2>
<p>Basic MCMC algorithms such as Metropolis–Hastings often move in small, local steps and therefore explore the state space slowly. This random-walk behaviour leads to poor mixing and long autocorrelation times, especially in high-dimensional or strongly correlated distributions.</p>
<p>Several advanced MCMC techniques attempt to reduce random-walk dynamics by proposing more informed or distant moves.</p>
<h3 id="informationtheory-6_mc-51-hamiltonian-monte-carlo-hmc">5.1 Hamiltonian Monte Carlo (HMC)<a class="headerlink" href="#informationtheory-6_mc-51-hamiltonian-monte-carlo-hmc" title="Permanent link">¶</a></h3>
<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving smoothly through the probability landscape.</p>
<p>Let <span class="arithmatex">\(z\)</span> denote the position (the latent variable) and let <span class="arithmatex">\(r\)</span> denote an auxiliary momentum variable. The joint density of <span class="arithmatex">\((z, r)\)</span> is defined through a Hamiltonian:</p>
<div class="arithmatex">\[
H(z, r) = -\log p(z) + \frac{1}{2} r^\top r.
\]</div>
<p>The first term acts like a potential energy, and the second acts like kinetic energy. The total Hamiltonian is approximately conserved under Hamiltonian dynamics governed by:</p>
<div class="arithmatex">\[
\frac{dz}{dt} = r, \qquad
\frac{dr}{dt} = \nabla_z \log p(z).
\]</div>
<p>Following these dynamics moves the system along continuous trajectories that remain mostly in high-probability regions, allowing the sampler to travel long distances without being rejected.</p>
<h4 id="informationtheory-6_mc-leapfrog-integration-and-step-size">Leapfrog Integration and Step Size<a class="headerlink" href="#informationtheory-6_mc-leapfrog-integration-and-step-size" title="Permanent link">¶</a></h4>
<p>Exact Hamiltonian dynamics cannot be simulated analytically, so HMC uses a numerical integrator, typically the leapfrog method. Leapfrog integration updates position and momentum in small steps of size <span class="arithmatex">\(\epsilon\)</span>:</p>
<ol>
<li>
<p>half-step momentum update<br>
<script type="math/tex; mode=display">
   r_{t+\tfrac{1}{2}} = r_t + \frac{\epsilon}{2}\,\nabla_z \log p(z_t)
   </script>
</p>
</li>
<li>
<p>full-step position update<br>
<script type="math/tex; mode=display">
   z_{t+1} = z_t + \epsilon\, r_{t+\tfrac{1}{2}}
   </script>
</p>
</li>
<li>
<p>half-step momentum update<br>
<script type="math/tex; mode=display">
   r_{t+1} = r_{t+\tfrac{1}{2}} + \frac{\epsilon}{2}\,\nabla_z \log p(z_{t+1})
   </script>
</p>
</li>
</ol>
<p>These updates are repeated <span class="arithmatex">\(L\)</span> times, producing a proposal <span class="arithmatex">\((z', r')\)</span> after a simulated trajectory of length <span class="arithmatex">\(L \epsilon\)</span>.</p>
<p>The step size <span class="arithmatex">\(\epsilon\)</span> strongly influences performance:</p>
<ul>
<li>if <span class="arithmatex">\(\epsilon\)</span> is too large, numerical integration becomes inaccurate and proposals are rejected  </li>
<li>if <span class="arithmatex">\(\epsilon\)</span> is too small, trajectories progress slowly and exploration becomes inefficient  </li>
</ul>
<p>Adaptive schemes such as dual averaging automatically tune <span class="arithmatex">\(\epsilon\)</span>.</p>
<h4 id="informationtheory-6_mc-acceptance-step">Acceptance Step<a class="headerlink" href="#informationtheory-6_mc-acceptance-step" title="Permanent link">¶</a></h4>
<p>Although leapfrog integration nearly preserves energy, numerical error accumulates. Therefore HMC applies a Metropolis acceptance step:</p>
<div class="arithmatex">\[
\alpha = \min\left(1,\,
\exp\big(-H(z',r') + H(z,r)\big)
\right).
\]</div>
<p>Because leapfrog integration is reversible and volume-preserving, acceptance rates remain high even for long trajectories.</p>
<h4 id="informationtheory-6_mc-choosing-the-trajectory-length">Choosing the Trajectory Length<a class="headerlink" href="#informationtheory-6_mc-choosing-the-trajectory-length" title="Permanent link">¶</a></h4>
<p>The number of leapfrog steps <span class="arithmatex">\(L\)</span> (or total integration time <span class="arithmatex">\(L\epsilon\)</span>) affects how far the sampler travels:</p>
<ul>
<li>small <span class="arithmatex">\(L\)</span> results in short trajectories, similar to random-walk proposals  </li>
<li>large <span class="arithmatex">\(L\)</span> explores further but may waste computation or return near the starting point  </li>
</ul>
<p>The No-U-Turn Sampler (NUTS) automatically selects an appropriate integration length and forms the basis of modern HMC implementations such as Stan.</p>
<h4 id="informationtheory-6_mc-summary-of-advantages">Summary of Advantages<a class="headerlink" href="#informationtheory-6_mc-summary-of-advantages" title="Permanent link">¶</a></h4>
<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving through the probability landscape.</p>
<p>Hamiltonian Monte Carlo offers several advantages:</p>
<ul>
<li>efficient exploration in high-dimensional or correlated distributions  </li>
<li>large, directed moves that avoid random-walk behaviour  </li>
<li>low autocorrelation between samples  </li>
<li>high acceptance rates due to approximate energy conservation  </li>
<li>uses gradients of <span class="arithmatex">\(\log p(z)\)</span> to guide proposals  </li>
</ul>
<p>HMC is widely used in probabilistic programming frameworks such as Stan, PyMC, and NumPyro, largely because of its scalability and efficiency in challenging Bayesian inference problems.</p>
<h3 id="informationtheory-6_mc-52-overrelaxation">5.2 Overrelaxation<a class="headerlink" href="#informationtheory-6_mc-52-overrelaxation" title="Permanent link">¶</a></h3>
<p>Overrelaxation modifies proposals so that successive samples are negatively correlated. Instead of randomly perturbing the current state, overrelaxation proposes a point on the opposite side of the conditional mean.</p>
<p>Intuitively, if the current sample lies above the mean, the overrelaxation proposal nudges it below the mean, and vice versa. This helps the chain avoid local sticking and oscillation.</p>
<p>Overrelaxation is most effective when the conditional distribution is approximately Gaussian or when the model exhibits strong linear structure.</p>
<h2 id="informationtheory-6_mc-6-sensitivity-to-step-size">6. Sensitivity to Step Size<a class="headerlink" href="#informationtheory-6_mc-6-sensitivity-to-step-size" title="Permanent link">¶</a></h2>
<p>The efficiency of MCMC algorithms depends critically on the choice of step size (or proposal scale):</p>
<ul>
<li>If the step size is too small, the chain takes tiny moves and mixes slowly.  </li>
<li>If the step size is too large, most proposals are rejected.</li>
</ul>
<p>Finding an appropriate step size is essential for balancing exploration and acceptance.  </p>
<p>For random-walk Metropolis–Hastings:</p>
<ul>
<li>acceptance rates near 0.2–0.4 often work well in high dimensions  </li>
<li>smaller dimensions tolerate larger acceptance rates</li>
</ul>
<p>For HMC, step size affects both the numerical integration quality and the acceptance probability. Too large a step size causes integration error and rejections; too small a step size results in slow exploration.</p>
<p>Adaptive MCMC methods automatically tune the step size to achieve target acceptance rates.</p>
<h2 id="informationtheory-6_mc-7-when-to-stop-convergence-and-diagnostics">7. When to Stop: Convergence and Diagnostics<a class="headerlink" href="#informationtheory-6_mc-7-when-to-stop-convergence-and-diagnostics" title="Permanent link">¶</a></h2>
<p>Running an MCMC chain forever is impossible, so practical inference requires diagnosing convergence.</p>
<p>Several indicators are commonly used:</p>
<h3 id="informationtheory-6_mc-71-burn-in">7.1 Burn-in<a class="headerlink" href="#informationtheory-6_mc-71-burn-in" title="Permanent link">¶</a></h3>
<p>The initial part of the chain (the burn-in period) may not represent the target distribution. These early samples are discarded until the chain reaches a stable region.</p>
<h3 id="informationtheory-6_mc-72-autocorrelation">7.2 Autocorrelation<a class="headerlink" href="#informationtheory-6_mc-72-autocorrelation" title="Permanent link">¶</a></h3>
<p>High autocorrelation indicates slow mixing. Effective sample size (ESS) measures the number of independent samples equivalent to the correlated MCMC draws.</p>
<h3 id="informationtheory-6_mc-73-multiple-chains">7.3 Multiple chains<a class="headerlink" href="#informationtheory-6_mc-73-multiple-chains" title="Permanent link">¶</a></h3>
<p>Running several independent chains allows comparison. If chains converge to the same region, the sampler is more likely to have reached equilibrium.</p>
<h3 id="informationtheory-6_mc-74-gelmanrubin-statistic-r-hat">7.4 Gelman–Rubin statistic (R-hat)<a class="headerlink" href="#informationtheory-6_mc-74-gelmanrubin-statistic-r-hat" title="Permanent link">¶</a></h3>
<p>R-hat compares within-chain and between-chain variance. Values close to 1 indicate convergence.</p>
<h3 id="informationtheory-6_mc-75-visual-inspection">7.5 Visual inspection<a class="headerlink" href="#informationtheory-6_mc-75-visual-inspection" title="Permanent link">¶</a></h3>
<p>Trace plots, autocorrelation plots, and histograms provide qualitative insight into mixing and stability.</p>
<p>There is no single perfect test, but combining multiple diagnostics provides reasonable confidence that the Markov chain has approximated the target distribution.</p>
<hr>
<p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p>
<p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p>
<p>Random-walk behaviour limits the efficiency of basic MCMC methods. Hamiltonian Monte Carlo reduces this by exploiting gradient information and simulating Hamiltonian dynamics, while overrelaxation introduces negative correlation to speed up mixing. Step size must be chosen carefully to balance exploration and acceptance. Convergence diagnostics such as burn-in, effective sample size, and R-hat help determine when to stop sampling and assess the quality of the generated samples.</p></body></html></section><section class="print-page" id="informationtheory-7a_vi_intro" heading-number="5.7"><h1 id="informationtheory-7a_vi_intro-informationtheory-7a_vi_intro">8. Optimization-Based Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p>
<p>This chapter introduces three optimization-based inference strategies:</p>
<ol>
<li>Maximum a posteriori (MAP) estimation  </li>
<li>Expectation–Maximization (EM)  </li>
<li>Variational inference (VI), in its simplest introductory form  </li>
</ol>
<p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>
<h2 id="informationtheory-7a_vi_intro-1-motivation-for-optimization-based-inference">1. Motivation for Optimization-Based Inference<a class="headerlink" href="#informationtheory-7a_vi_intro-1-motivation-for-optimization-based-inference" title="Permanent link">¶</a></h2>
<p>Bayesian inference requires the posterior</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>The challenge lies in computing the marginal likelihood</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz,
\]</div>
<p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p>
<ul>
<li>high-dimensional latent spaces  </li>
<li>multimodal posteriors  </li>
<li>large datasets  </li>
<li>models requiring gradient-based learning  </li>
</ul>
<p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>
<h2 id="informationtheory-7a_vi_intro-2-maximum-a-posteriori-map-estimation">2. Maximum A Posteriori (MAP) Estimation<a class="headerlink" href="#informationtheory-7a_vi_intro-2-maximum-a-posteriori-map-estimation" title="Permanent link">¶</a></h2>
<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes’ rule:</p>
<div class="arithmatex">\[
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)},
\]</div>
<p>MAP chooses the mode of the posterior:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= \arg\max_\theta p(\theta|x).
\]</div>
<p>Since <span class="arithmatex">\(p(x)\)</span> does not depend on <span class="arithmatex">\(\theta\)</span>, this is equivalent to:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= \arg\max_\theta \big[ \log p(x|\theta) + \log p(\theta) \big].
\]</div>
<p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through <span class="arithmatex">\(p(\theta)\)</span>. However, it returns only a point estimate and does not capture uncertainty.</p>
<p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-3-expectationmaximization-em">3. Expectation–Maximization (EM)<a class="headerlink" href="#informationtheory-7a_vi_intro-3-expectationmaximization-em" title="Permanent link">¶</a></h2>
<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p>
<div class="arithmatex">\[
\log p_\theta(x) 
= \log \sum_z p_\theta(x,z).
\]</div>
<p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>
<h3 id="informationtheory-7a_vi_intro-e-step">E-step<a class="headerlink" href="#informationtheory-7a_vi_intro-e-step" title="Permanent link">¶</a></h3>
<p>Compute the posterior over latent variables under the current parameters:</p>
<div class="arithmatex">\[
q(z) = p_\theta(z|x).
\]</div>
<h3 id="informationtheory-7a_vi_intro-m-step">M-step<a class="headerlink" href="#informationtheory-7a_vi_intro-m-step" title="Permanent link">¶</a></h3>
<p>Maximize the expected complete-data log-likelihood:</p>
<div class="arithmatex">\[
\theta \leftarrow 
\arg\max_\theta 
\mathbb{E}_{q(z)}[\log p_\theta(x,z)].
\]</div>
<p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p>
<ul>
<li>mixture of Gaussians  </li>
<li>hidden Markov models  </li>
<li>probabilistic PCA  </li>
<li>clustering and density estimation  </li>
</ul>
<p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior <span class="arithmatex">\(q(z) = p_\theta(z|x)\)</span>.</p>
<h2 id="informationtheory-7a_vi_intro-4-em-and-map-map-em">4. EM and MAP: MAP-EM<a class="headerlink" href="#informationtheory-7a_vi_intro-4-em-and-map-map-em" title="Permanent link">¶</a></h2>
<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= 
\arg\max_\theta 
\left[
\mathbb{E}_{p(z|x,\theta)}[\log p(x,z|\theta)]
+ \log p(\theta)
\right].
\]</div>
<p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>
<h2 id="informationtheory-7a_vi_intro-5-limitations-of-map-and-em">5. Limitations of MAP and EM<a class="headerlink" href="#informationtheory-7a_vi_intro-5-limitations-of-map-and-em" title="Permanent link">¶</a></h2>
<p>Both MAP and EM have limitations that motivate more general methods:</p>
<ol>
<li>MAP returns only a point estimate and discards posterior uncertainty.  </li>
<li>EM requires exact posterior computation in the E-step:
   <script type="math/tex; mode=display">
   q(z) = p_\theta(z|x),
   </script>
   which is often intractable.  </li>
<li>EM struggles with:</li>
<li>multimodal posteriors  </li>
<li>high-dimensional latent spaces  </li>
<li>arbitrary likelihood forms  </li>
</ol>
<p>These limitations lead naturally to variational inference.</p>
<h2 id="informationtheory-7a_vi_intro-6-a-brief-introduction-to-variational-inference-vi">6. A Brief Introduction to Variational Inference (VI)<a class="headerlink" href="#informationtheory-7a_vi_intro-6-a-brief-introduction-to-variational-inference-vi" title="Permanent link">¶</a></h2>
<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p>
<div class="arithmatex">\[
q(z) = p_\theta(z|x),
\]</div>
<p>VI chooses a family of distributions</p>
<div class="arithmatex">\[
q_\phi(z|x) \in \mathcal{Q}
\]</div>
<p>and optimizes it to be close to the true posterior. The objective is:</p>
<div class="arithmatex">\[
\phi^* = 
\arg\min_\phi D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Because <span class="arithmatex">\(p(z|x)\)</span> contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\log p(x)
=
\mathcal{L}(x;\phi,\theta)
+
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p>
<p>VI:</p>
<ul>
<li>generalizes MAP (when <span class="arithmatex">\(q\)</span> is a delta function)  </li>
<li>generalizes EM (when <span class="arithmatex">\(q = p_\theta(z|x)\)</span>)  </li>
<li>supports flexible approximations  </li>
<li>scales to large datasets  </li>
<li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li>
</ul>
<p>The next chapter explores variational inference in detail.</p>
<hr>
<p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p>
<p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p>
<p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p></body></html></section><section class="print-page" id="informationtheory-7b_vi" heading-number="5.8"><h1 id="informationtheory-7b_vi-informationtheory-7b_vi">7. Variatonal Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Variational inference (VI) provides a general framework for approximating difficult probability distributions with simpler, tractable ones. Many modern machine-learning models rely on VI, including variational autoencoders (VAEs), Bayesian neural networks, latent-variable models, and diffusion models. VI offers a scalable alternative to sampling-based inference and converts the inference problem into an optimization problem.</p>
<h2 id="informationtheory-7b_vi-1-the-problem-of-inference">1. The Problem of Inference<a class="headerlink" href="#informationtheory-7b_vi-1-the-problem-of-inference" title="Permanent link">¶</a></h2>
<p>Many probabilistic models introduce hidden variables to explain observations. Examples include:</p>
<ul>
<li>latent variables <span class="arithmatex">\(z\)</span> in VAEs  </li>
<li>weight distributions in Bayesian neural networks  </li>
<li>cluster indicators in mixture models  </li>
<li>hidden states in topic models and HMMs  </li>
</ul>
<p>The goal is to compute the posterior distribution</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>The difficulty lies in the marginal likelihood</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz,
\]</div>
<p>which is often intractable in high-dimensional or complex models. Exact Bayesian inference becomes impossible, which motivates approximate methods. Variational inference addresses this challenge.</p>
<h2 id="informationtheory-7b_vi-2-the-idea-of-variational-inference">2. The Idea of Variational Inference<a class="headerlink" href="#informationtheory-7b_vi-2-the-idea-of-variational-inference" title="Permanent link">¶</a></h2>
<p>Variational inference replaces the intractable posterior with a tractable approximation. Instead of trying to compute <span class="arithmatex">\(p(z|x)\)</span> exactly, VI introduces a family of simpler distributions</p>
<div class="arithmatex">\[
q_\phi(z|x) \in \mathcal{Q},
\]</div>
<p>and chooses the member that is closest to the true posterior. Closeness is measured using the KL divergence:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>The goal is:</p>
<div class="arithmatex">\[
\phi^* = \arg\min_\phi
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>However, the KL depends on <span class="arithmatex">\(p(z|x)\)</span>, which is unknown, making direct minimization impossible. The key insight is that the KL can be rewritten in terms of computable quantities, leading to the Evidence Lower Bound (ELBO).</p>
<h2 id="informationtheory-7b_vi-3-deriving-the-elbo">3. Deriving the ELBO<a class="headerlink" href="#informationtheory-7b_vi-3-deriving-the-elbo" title="Permanent link">¶</a></h2>
<p>We start from the marginal likelihood:</p>
<div class="arithmatex">\[
\log p(x) = \log \int p(x,z)\,dz.
\]</div>
<p>We multiply and divide by <span class="arithmatex">\(q_\phi(z|x)\)</span>:</p>
<div class="arithmatex">\[
\log p(x)
=
\log \int q_\phi(z|x)\,
\frac{p(x,z)}{q_\phi(z|x)}\,dz.
\]</div>
<p>Applying Jensen’s inequality yields:</p>
<div class="arithmatex">\[
\log p(x)
\ge 
\mathbb{E}_{q_\phi(z|x)}
\left[
\log \frac{p(x,z)}{q_\phi(z|x)}
\right].
\]</div>
<p>This expression is the ELBO:</p>
<div class="arithmatex">\[
\mathcal{L}(x;\phi,\theta)
=
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z)]
-
\mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)].
\]</div>
<p>A useful identity reveals:</p>
<div class="arithmatex">\[
\log p(x)
=
\mathcal{L}(x;\phi,\theta)
+
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Since KL divergence is non-negative:</p>
<div class="arithmatex">\[
\mathcal{L}(x;\phi,\theta) \le \log p(x).
\]</div>
<p>Maximizing the ELBO is equivalent to minimizing the KL divergence between <span class="arithmatex">\(q_\phi(z|x)\)</span> and the true posterior.</p>
<h2 id="informationtheory-7b_vi-4-interpreting-the-elbo">4. Interpreting the ELBO<a class="headerlink" href="#informationtheory-7b_vi-4-interpreting-the-elbo" title="Permanent link">¶</a></h2>
<p>The ELBO can be decomposed into two terms that have clear interpretations. Writing</p>
<div class="arithmatex">\[
p(x,z) = p_\theta(x|z)p(z),
\]</div>
<p>and substituting into the ELBO gives:</p>
<div class="arithmatex">\[
\mathcal{L}(x;\phi,\theta)
=
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
-
D_{\text{KL}}(q_\phi(z|x)\|p(z)).
\]</div>
<h3 id="informationtheory-7b_vi-reconstruction-term">Reconstruction term<a class="headerlink" href="#informationtheory-7b_vi-reconstruction-term" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)].
\]</div>
<p>This term ensures that <span class="arithmatex">\(z\)</span> captures enough information to generate or reconstruct the observed data. It corresponds to likelihood or reconstruction accuracy.</p>
<h3 id="informationtheory-7b_vi-regularization-term">Regularization term<a class="headerlink" href="#informationtheory-7b_vi-regularization-term" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z)).
\]</div>
<p>This term ensures that the posterior approximation does not drift too far from the prior. In VAEs, the prior is usually a Gaussian, making the latent space smooth and structured.</p>
<p>The ELBO therefore expresses a balance:</p>
<ul>
<li>the first term rewards informative latent variables  </li>
<li>the second term penalizes overly complex or irregular latent distributions  </li>
</ul>
<h2 id="informationtheory-7b_vi-5-why-vi-uses-reverse-kl">5. Why VI Uses Reverse KL<a class="headerlink" href="#informationtheory-7b_vi-5-why-vi-uses-reverse-kl" title="Permanent link">¶</a></h2>
<p>Variational inference minimizes</p>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)),
\]</div>
<p>which is reverse KL. Reverse KL has important behavioral properties:</p>
<ul>
<li>it heavily penalizes assigning probability mass where the true posterior is low  </li>
<li>it allows <span class="arithmatex">\(q\)</span> to ignore some modes of <span class="arithmatex">\(p(z|x)\)</span>  </li>
<li>it prefers tight, conservative approximations  </li>
</ul>
<p>As a result:</p>
<ul>
<li>VI tends to be mode seeking  </li>
<li>it focuses on a single high-density region  </li>
<li>it can miss multimodal structure of the true posterior  </li>
</ul>
<p>This behavior explains why VAEs sometimes produce smooth or blurry samples: the latent space favors safe, central modes.</p>
<h2 id="informationtheory-7b_vi-6-variational-autoencoders-vaes">6. Variational Autoencoders (VAEs)<a class="headerlink" href="#informationtheory-7b_vi-6-variational-autoencoders-vaes" title="Permanent link">¶</a></h2>
<p>A VAE applies variational inference to a deep latent-variable model. It introduces:</p>
<ol>
<li>a latent prior<br>
<script type="math/tex; mode=display">
   z \sim p(z)
   </script>
</li>
<li>a decoder (generative model)<br>
<script type="math/tex; mode=display">
   x \sim p_\theta(x|z)
   </script>
</li>
<li>an encoder (variational posterior)<br>
<script type="math/tex; mode=display">
   q_\phi(z|x) \approx p(z|x)
   </script>
</li>
</ol>
<p>The encoder and decoder are neural networks, trained jointly by maximizing the ELBO over all data points.</p>
<h3 id="informationtheory-7b_vi-61-generative-model">6.1 Generative model<a class="headerlink" href="#informationtheory-7b_vi-61-generative-model" title="Permanent link">¶</a></h3>
<p>Given <span class="arithmatex">\(z\)</span> sampled from the prior, the decoder produces a distribution over possible <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
p_\theta(x|z).
\]</div>
<h3 id="informationtheory-7b_vi-62-inference-model">6.2 Inference model<a class="headerlink" href="#informationtheory-7b_vi-62-inference-model" title="Permanent link">¶</a></h3>
<p>The encoder produces the parameters of the approximate posterior:</p>
<div class="arithmatex">\[
q_\phi(z|x) = \mathcal{N}(z\mid \mu_\phi(x), \sigma^2_\phi(x)).
\]</div>
<p>This is the distribution used inside the ELBO.</p>
<h3 id="informationtheory-7b_vi-63-vae-training-objective">6.3 VAE training objective<a class="headerlink" href="#informationtheory-7b_vi-63-vae-training-objective" title="Permanent link">¶</a></h3>
<p>The objective for each data point is:</p>
<div class="arithmatex">\[
\mathcal{L}(x)
=
\mathbb{E}_{q_\phi(z|x)}
[\log p_\theta(x|z)]
-
D_{\text{KL}}(q_\phi(z|x)\|p(z)).
\]</div>
<p>The first term encourages correct reconstruction; the second keeps latent codes regularized.</p>
<h2 id="informationtheory-7b_vi-7-the-reparameterization-trick">7. The Reparameterization Trick<a class="headerlink" href="#informationtheory-7b_vi-7-the-reparameterization-trick" title="Permanent link">¶</a></h2>
<p>The expectation in the ELBO involves sampling from <span class="arithmatex">\(q_\phi(z|x)\)</span>. To differentiate through this sampling step, VAEs use the reparameterization:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\odot\epsilon,
\qquad \epsilon \sim \mathcal{N}(0,I).
\]</div>
<p>This expresses sampling as a deterministic transformation of noise, allowing gradients to flow through the encoder.</p>
<p>This trick is central to making VI scalable and efficient in deep learning.</p>
<h2 id="informationtheory-7b_vi-8-consequences-of-reverse-kl-in-vaes">8. Consequences of Reverse KL in VAEs<a class="headerlink" href="#informationtheory-7b_vi-8-consequences-of-reverse-kl-in-vaes" title="Permanent link">¶</a></h2>
<p>The reverse KL term shapes the behavior of the VAE:</p>
<ul>
<li>it encourages smooth, overlapping latent regions  </li>
<li>it prefers safe latent representations  </li>
<li>it explains why VAEs sometimes produce blurry or conservative samples  </li>
<li>it stabilizes training  </li>
<li>it produces well-structured latent spaces  </li>
</ul>
<p>Extensions such as the <span class="arithmatex">\(\beta\)</span>-VAE, hierarchical VAEs, and flows inside the encoder allow for more expressive or disentangled representations.</p>
<h2 id="informationtheory-7b_vi-9-variational-inference-beyond-vaes">9. Variational Inference Beyond VAEs<a class="headerlink" href="#informationtheory-7b_vi-9-variational-inference-beyond-vaes" title="Permanent link">¶</a></h2>
<p>VI provides a general-purpose framework for approximate inference in many settings.</p>
<h3 id="informationtheory-7b_vi-bayesian-neural-networks">Bayesian neural networks<a class="headerlink" href="#informationtheory-7b_vi-bayesian-neural-networks" title="Permanent link">¶</a></h3>
<p>Posterior distributions over weights are approximated by variational distributions:</p>
<div class="arithmatex">\[
q(w)\approx p(w|D).
\]</div>
<h3 id="informationtheory-7b_vi-diffusion-models">Diffusion models<a class="headerlink" href="#informationtheory-7b_vi-diffusion-models" title="Permanent link">¶</a></h3>
<p>The training objective resembles a variational bound on the data likelihood, using KL divergences between transition kernels.</p>
<h3 id="informationtheory-7b_vi-normalizing-flows-for-vi">Normalizing flows for VI<a class="headerlink" href="#informationtheory-7b_vi-normalizing-flows-for-vi" title="Permanent link">¶</a></h3>
<p>Flows can produce more expressive variational posteriors than simple Gaussians.</p>
<h3 id="informationtheory-7b_vi-reinforcement-learning">Reinforcement learning<a class="headerlink" href="#informationtheory-7b_vi-reinforcement-learning" title="Permanent link">¶</a></h3>
<p>Entropy-regularized RL and soft Q-learning can be interpreted through variational principles.</p>
<p>VI therefore offers a unifying viewpoint across deep generative models, Bayesian inference, and probabilistic deep learning.</p>
<hr>
<p>Variational inference replaces an intractable posterior distribution with a tractable approximation and optimizes this approximation by maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, capturing the trade-off between accuracy and complexity. VAEs are an important application of VI, using neural networks to parameterize both the generative model and the approximate posterior. Reverse KL explains the conservative behavior of VI-based models. Variational inference provides a flexible approach for approximate Bayesian inference and underlies many modern generative and representation-learning techniques.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-6' heading-number='6'>
                        <h1>Cheat Sheets<a class='headerlink' href='#section-6' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="cheatsheets-20a_cheatsheet" heading-number="6.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="comprehensive-optimization-algorithm-cheat-sheet">Comprehensive Optimization Algorithm Cheat Sheet<a class="headerlink" href="#cheatsheets-20a_cheatsheet-comprehensive-optimization-algorithm-cheat-sheet" title="Permanent link">¶</a></h1>
<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search.<br>
It balances <strong>theoretical precision</strong> with <strong>practical intuition</strong>—from gradient-based solvers to black-box evolutionary methods.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-how-to-read-this-table">🧭 How to Read This Table<a class="headerlink" href="#cheatsheets-20a_cheatsheet-how-to-read-this-table" title="Permanent link">¶</a></h2>
<p>Each method lists:
- <strong>Problem Type</strong> — the class of objectives it applies to.
- <strong>Assumptions</strong> — smoothness, convexity, or structural conditions.
- <strong>Core Update Rule</strong> — canonical iteration.
- <strong>Scalability</strong> — computational feasibility.
- <strong>Per-Iteration Cost</strong> — approximate computational complexity.
- <strong>Applications</strong> — typical ML or engineering use cases.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-first-order-methods">🚀 First-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-first-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent (GD)</td>
<td>Unconstrained smooth (convex/nonconvex)</td>
<td>Differentiable; <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Nesterov’s Accelerated GD</td>
<td>Smooth convex (fast rate)</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(y_k = x_k + \frac{k-1}{k+2}(x_k - x_{k-1})\)</span>; <span class="arithmatex">\(x_{k+1} = y_k - \eta \nabla f(y_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Accelerated convex models</td>
</tr>
<tr>
<td>(Polyak) Heavy-Ball Momentum</td>
<td>Unconstrained smooth</td>
<td>Differentiable, <span class="arithmatex">\(\beta \in (0,1)\)</span></td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k) + \beta(x_k - x_{k-1})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Deep networks, convex smooth losses</td>
</tr>
<tr>
<td>Conjugate Gradient (CG)</td>
<td>Quadratic or linear systems <span class="arithmatex">\(Ax=b\)</span></td>
<td><span class="arithmatex">\(A\)</span> symmetric positive definite</td>
<td><span class="arithmatex">\(p_{k+1}=r_{k+1}+\beta_k p_k\)</span>, <span class="arithmatex">\(x_{k+1}=x_k+\alpha_k p_k\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Large-scale least squares, implicit Newton steps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td>Non-Euclidean geometry</td>
<td>Convex; mirror map <span class="arithmatex">\(\psi\)</span> strongly convex</td>
<td><span class="arithmatex">\(x_{k+1} = \nabla \psi^*(\nabla \psi(x_k) - \eta \nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Probability simplex, online learning</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Conjugate Gradient (CG)</em> bridges first- and second-order methods: it achieves exact convergence in at most <span class="arithmatex">\(d\)</span> steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-second-order-methods">⚙️ Second-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-second-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Newton’s Method</td>
<td>Smooth convex</td>
<td>Twice differentiable; <span class="arithmatex">\(\nabla^2 f(x)\)</span> PD</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1}\nabla f(x_k)\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>Logistic regression (IRLS), convex solvers</td>
</tr>
<tr>
<td>BFGS / L-BFGS</td>
<td>Smooth convex</td>
<td>Differentiable, approximate Hessian</td>
<td>Solve <span class="arithmatex">\(B_k p_k=-\nabla f(x_k)\)</span>; update <span class="arithmatex">\(B_k\)</span> via secant rule</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>GLMs, medium ML models</td>
</tr>
<tr>
<td>Trust-Region</td>
<td>Smooth convex/nonconvex</td>
<td>Twice differentiable</td>
<td><span class="arithmatex">\(\min_p \tfrac{1}{2}p^\top \nabla^2 f(x_k)p + \nabla f(x_k)^\top p\)</span> s.t. <span class="arithmatex">\(\|p\|\le\Delta_k\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>TRPO, physics-based ML</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods">🧮 Proximal, Projected &amp; Splitting Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA)</td>
<td>Composite <span class="arithmatex">\(f=g+h\)</span></td>
<td><span class="arithmatex">\(g\)</span> smooth, <span class="arithmatex">\(h\)</span> convex</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\alpha h}(x_k-\alpha\nabla g(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>LASSO, sparse recovery</td>
</tr>
<tr>
<td>FISTA</td>
<td>Same as ISTA</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(g\)</span></td>
<td>Like ISTA with momentum</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Compressed sensing</td>
</tr>
<tr>
<td>Projected Gradient (PG)</td>
<td>Convex constrained</td>
<td><span class="arithmatex">\(f\)</span> smooth; easy projection</td>
<td><span class="arithmatex">\(x_{k+1}=\Pi_C(x_k-\eta\nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> + projection</td>
<td>Box/simplex constraints</td>
</tr>
<tr>
<td>ADMM</td>
<td>Separable convex + linear constraints</td>
<td><span class="arithmatex">\(f,g\)</span> convex</td>
<td>Alternating minimization + dual update</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> per block</td>
<td>Distributed ML, consensus</td>
</tr>
<tr>
<td>Majorization–Minimization (MM)</td>
<td>Convex/nonconvex</td>
<td>$g(x</td>
<td>x_k)\ge f(x)$</td>
<td>$x_{k+1}=\arg\min g(x</td>
<td>x_k)$</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-coordinate-block-methods">🧩 Coordinate &amp; Block Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-coordinate-block-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent (CD)</td>
<td>Separable convex</td>
<td>Convex, differentiable</td>
<td>Update one coordinate: <span class="arithmatex">\(x_{i}^{k+1}=x_i^k-\eta\partial_i f(x^k)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(d)\)</span></td>
<td>LASSO, SVM duals</td>
</tr>
<tr>
<td>Block Coordinate Descent (BCD)</td>
<td>Block separable</td>
<td>Convex per block</td>
<td>Minimize over <span class="arithmatex">\(x^{(j)}\)</span> while fixing others</td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd_j)\)</span></td>
<td>Matrix factorization, alternating minimization</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods">🎲 Stochastic &amp; Mini-Batch Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stochastic Gradient Descent (SGD)</td>
<td>Large-scale / streaming</td>
<td>Unbiased stochastic gradients</td>
<td><span class="arithmatex">\(x_{k+1}=x_k-\eta_t\nabla f_{i_k}(x_k)\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Deep learning, online learning</td>
</tr>
<tr>
<td>Variance-Reduced (SVRG/SAGA/SARAH)</td>
<td>Finite-sum convex</td>
<td>Smooth, strongly convex</td>
<td><span class="arithmatex">\(v_k=\nabla f_{i_k}(x_k)-\nabla f_{i_k}(\tilde{x})+\nabla f(\tilde{x})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Logistic regression, GLMs</td>
</tr>
<tr>
<td>Adaptive SGD (Adam/RMSProp/Adagrad)</td>
<td>Nonconvex stochastic</td>
<td>Bounded variance</td>
<td><span class="arithmatex">\(m_k=\beta_1m_{k-1}+(1-\beta_1)g_k\)</span>, <span class="arithmatex">\(v_k=\beta_2v_{k-1}+(1-\beta_2)g_k^2\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Neural networks</td>
</tr>
<tr>
<td>Proximal Stochastic (Prox-SGD / Prox-SAGA)</td>
<td>Nonsmooth stochastic</td>
<td><span class="arithmatex">\(f=g+h\)</span> with prox of <span class="arithmatex">\(h\)</span> known</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\eta h}(x_k-\eta\widehat{\nabla g}(x_k))\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Sparse online learning</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-interior-point-augmented-methods">🧱 Interior-Point &amp; Augmented Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-interior-point-augmented-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interior-Point</td>
<td>Convex with inequalities</td>
<td>Slater’s condition, self-concordant barrier</td>
<td>Solve <span class="arithmatex">\(\min f_0(x)-\tfrac{1}{t}\sum_i\log(-g_i(x))\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>LP, QP, SDP</td>
</tr>
<tr>
<td>Augmented Lagrangian (ALM)</td>
<td>Constrained convex</td>
<td><span class="arithmatex">\(f,g\)</span> convex; equality constraints</td>
<td><span class="arithmatex">\(L_\rho(x,\lambda)=f(x)+\lambda^T g(x)+\tfrac{\rho}{2}\|g(x)\|^2\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Penalty methods, PDEs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization">🌐 Derivative-Free &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Idea</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nelder–Mead Simplex</td>
<td>Low-dimensional, smooth or noisy</td>
<td>No gradients; continuous <span class="arithmatex">\(f\)</span></td>
<td>Maintain simplex of <span class="arithmatex">\(d+1\)</span> points; reflect–expand–contract–shrink operations</td>
<td>Small</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>Parameter tuning, physics models</td>
</tr>
<tr>
<td>Simulated Annealing</td>
<td>Nonconvex, global</td>
<td>Stochastic exploration via temperature</td>
<td>Random perturbations accepted w.p. <span class="arithmatex">\(\exp(-\Delta f/T)\)</span>; <span class="arithmatex">\(T\downarrow\)</span></td>
<td>Medium</td>
<td>High (many samples)</td>
<td>Hyperparameter tuning, design optimization</td>
</tr>
<tr>
<td>Multi-start Local Search</td>
<td>Nonconvex</td>
<td>None; relies on restart diversity</td>
<td>Run local solver from multiple random inits, pick best result</td>
<td>Medium</td>
<td><span class="arithmatex">\(k\times\)</span> local solver</td>
<td>Avoids local minima; cheap global heuristic</td>
</tr>
<tr>
<td>Evolutionary Algorithms (EA)</td>
<td>Black-box, global</td>
<td>Population-based; fitness function only</td>
<td>Mutate, select, recombine candidates</td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span> per gen</td>
<td>Global optimization, control, AutoML</td>
</tr>
<tr>
<td>Genetic Algorithms (GA)</td>
<td>Combinatorial / continuous</td>
<td>Chromosomal encoding of solutions</td>
<td>Apply selection, crossover, mutation; evolve over generations</td>
<td>Medium–Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Feature selection, neural architecture search</td>
</tr>
<tr>
<td>Evolution Strategies (ES)</td>
<td>Continuous, black-box</td>
<td>Gaussian mutation around mean</td>
<td><span class="arithmatex">\(\theta_{k+1} = \theta_k + \eta \sum_i w_i \epsilon_i f(\theta_k+\sigma \epsilon_i)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Reinforcement learning, black-box control</td>
</tr>
<tr>
<td>Derivative-Free Optimization (DFO)</td>
<td>Black-box, noisy <span class="arithmatex">\(f\)</span></td>
<td>Only function values available</td>
<td>Gradient estimated via random perturbations: <span class="arithmatex">\(g\approx\frac{f(x+hu)-f(x)}{h}u\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d)\)</span>–<span class="arithmatex">\(O(d^2)\)</span></td>
<td>Robotics, policy search, design</td>
</tr>
<tr>
<td>Black-Box Optimization Framework</td>
<td>General</td>
<td>No analytical gradients; often stochastic</td>
<td>Unified term covering EA, GA, ES, and DFO</td>
<td>Medium–Large</td>
<td>varies</td>
<td>Hyperparameter search, AutoML, reinforcement learning</td>
</tr>
<tr>
<td>Numerical Encodings</td>
<td>Used in GA/EA</td>
<td>Represents variables in binary, integer, or floating-point form</td>
<td>Choice of encoding impacts mutation/crossover behavior</td>
<td>N/A</td>
<td>negligible</td>
<td>Optimization of mixed or discrete variables</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-convergence-complexity-snapshot">📈 Convergence &amp; Complexity Snapshot<a class="headerlink" href="#cheatsheets-20a_cheatsheet-convergence-complexity-snapshot" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method Type</th>
<th>Convergence (Convex)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Subgradient</td>
<td><span class="arithmatex">\(O(1/\sqrt{k})\)</span></td>
<td>Nonsmooth convex</td>
</tr>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Smooth convex</td>
</tr>
<tr>
<td>Accelerated Gradient</td>
<td><span class="arithmatex">\(O(1/k^2)\)</span></td>
<td>Optimal first-order</td>
</tr>
<tr>
<td>Newton / Quasi-Newton</td>
<td>Quadratic / Superlinear</td>
<td>Local only</td>
</tr>
<tr>
<td>Strongly Convex</td>
<td><span class="arithmatex">\((1-\mu/L)^k\)</span></td>
<td>Linear rate</td>
</tr>
<tr>
<td>Variance-Reduced</td>
<td>Linear (strongly convex)</td>
<td>Finite-sum optimization</td>
</tr>
<tr>
<td>ADMM / Proximal</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Composite convex</td>
</tr>
<tr>
<td>Interior-Point</td>
<td>Polynomial time</td>
<td>High-accuracy convex</td>
</tr>
<tr>
<td>Derivative-Free / Heuristics</td>
<td>No formal bound</td>
<td>Empirical convergence only</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-practitioner-summary">🧠 Practitioner Summary<a class="headerlink" href="#cheatsheets-20a_cheatsheet-practitioner-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradients available, smooth convex</td>
<td>Gradient Descent, Nesterov</td>
</tr>
<tr>
<td>Curvature matters, moderate scale</td>
<td>Newton, BFGS, Conjugate Gradient</td>
</tr>
<tr>
<td>Nonsmooth regularizer</td>
<td>Proximal Gradient, ADMM</td>
</tr>
<tr>
<td>Simple constraints</td>
<td>Projected Gradient</td>
</tr>
<tr>
<td>Large-scale / streaming</td>
<td>SGD, Adam, RMSProp</td>
</tr>
<tr>
<td>Finite-sum convex</td>
<td>SVRG, SAGA</td>
</tr>
<tr>
<td>Online / adaptive</td>
<td>Mirror Descent, FTRL</td>
</tr>
<tr>
<td>No gradients (black-box)</td>
<td>DFO, Nelder–Mead, ES, GA</td>
</tr>
<tr>
<td>Global nonconvex search</td>
<td>Simulated Annealing, Multi-starts, Evolutionary Algorithms</td>
</tr>
<tr>
<td>Distributed / separable</td>
<td>ADMM, ALM</td>
</tr>
<tr>
<td>High-precision convex programs</td>
<td>Interior-Point, Trust-Region</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization">🧩 Notes on Global &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Conjugate Gradient</strong>: memory-efficient quasi-second-order method for large convex quadratics.  </li>
<li><strong>Nelder–Mead</strong>: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li>
<li><strong>Simulated Annealing</strong>: probabilistic global search inspired by thermodynamics.  </li>
<li><strong>Multi-Starts</strong>: pragmatic global exploration by repeated local optimization.  </li>
<li><strong>Evolutionary / Genetic / ES</strong>: population-based global heuristics; robust to noise and discontinuity.  </li>
<li><strong>Derivative-Free Optimization (DFO)</strong>: umbrella for random, surrogate-based, or adaptive black-box methods.  </li>
<li><strong>Numerical Encoding</strong>: crucial in discrete search—how real or binary variables are represented determines performance.</li>
</ul>
<hr>
<blockquote>
<p><strong>Summary Insight:</strong><br>
- Convex + differentiable → use gradient-based or Newton-type methods.<br>
- Convex + nonsmooth → use proximal, ADMM, or coordinate descent.<br>
- Large-scale or stochastic → use SGD or adaptive variants.<br>
- No gradients or nonconvex → use derivative-free or evolutionary methods.<br>
- The structure of the objective, not its size alone, determines the optimal solver family.</p>
</blockquote></body></html></section></section>
                    <section class='print-page md-section' id='section-7' heading-number='7'>
                        <h1>Appendices<a class='headerlink' href='#section-7' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="appendices-120_ineqaulities" heading-number="7.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-a-common-inequalities-and-identities">Appendix A: Common Inequalities and Identities<a class="headerlink" href="#appendices-120_ineqaulities-appendix-a-common-inequalities-and-identities" title="Permanent link">¶</a></h1>
<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the “algebraic tools” you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemaréchal, 2001).</p>
<h2 id="appendices-120_ineqaulities-a1-cauchyschwarz-inequality">A.1 Cauchy–Schwarz inequality<a class="headerlink" href="#appendices-120_ineqaulities-a1-cauchyschwarz-inequality" title="Permanent link">¶</a></h2>
<p>For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>,
<script type="math/tex; mode=display">
|x^\top y| \le \|x\|_2 \, \|y\|_2.
</script>
</p>
<p>Equality holds if and only if <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent.</p>
<p>Consequences:</p>
<ul>
<li>Defines the notion of angle between vectors.</li>
<li>Justifies dual norms.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a2-jensens-inequality">A.2 Jensen’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a2-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex, and let <span class="arithmatex">\(X\)</span> be a random variable. Then
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>In finite form: for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum_i \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<p>Jensen’s inequality is equivalent to convexity: it says “the function at the average is no more than the average of the function values.” It is used constantly to prove convexity of expectations and log-sum-exp.</p>
<h2 id="appendices-120_ineqaulities-a3-amgm-inequality">A.3 AM–GM inequality<a class="headerlink" href="#appendices-120_ineqaulities-a3-amgm-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(x_1,\dots,x_n \ge 0\)</span>,
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^n x_i
\ge
\left(\prod_{i=1}^n x_i \right)^{1/n}.
</script>
</p>
<p>This can be proved using Jensen’s inequality with <span class="arithmatex">\(f(t) = \log t\)</span>, which is concave. AM–GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>
<h2 id="appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz">A.4 Hölder’s inequality (generalised Cauchy–Schwarz)<a class="headerlink" href="#appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(p,q \ge 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> (conjugate exponents),
<script type="math/tex; mode=display">
\sum_{i=1}^n |x_i y_i|
\le
\left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
\left( \sum_{i=1}^n |y_i|^q \right)^{1/q}.
</script>
</p>
<ul>
<li>When <span class="arithmatex">\(p=q=2\)</span>, Hölder becomes Cauchy–Schwarz.</li>
<li>Hölder underlies dual norms: the dual of <span class="arithmatex">\(\ell_p\)</span> is <span class="arithmatex">\(\ell_q\)</span>.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a5-youngs-inequality">A.5 Young’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a5-youngs-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(a,b \ge 0\)</span> and <span class="arithmatex">\(p,q &gt; 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>,
<script type="math/tex; mode=display">
ab \le \frac{a^p}{p} + \frac{b^q}{q}.
</script>
</p>
<p>This is useful in bounding cross terms in convergence proofs.</p>
<h2 id="appendices-120_ineqaulities-a6-fenchels-inequality">A.6 Fenchel’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a6-fenchels-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be a convex function and let <span class="arithmatex">\(f^*\)</span> be its convex conjugate:
<script type="math/tex; mode=display">
f^*(y) = \sup_x (y^\top x - f(x)).
</script>
</p>
<p>Then for all <span class="arithmatex">\(x,y\)</span>,
<script type="math/tex; mode=display">
f(x) + f^*(y) \ge y^\top x.
</script>
</p>
<p>Fenchel’s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel’s inequality.</p>
<h2 id="appendices-120_ineqaulities-a7-supporting-hyperplane-inequality">A.7 Supporting hyperplane inequality<a class="headerlink" href="#appendices-120_ineqaulities-a7-supporting-hyperplane-inequality" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is convex, then for any <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\(g \in \partial f(x)\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y-x)
\quad \text{for all } y.
</script>
</p>
<p>This can be viewed as “<span class="arithmatex">\(f\)</span> lies above all its tangent hyperplanes,” even when it’s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>
<h2 id="appendices-120_ineqaulities-a8-summary">A.8 Summary<a class="headerlink" href="#appendices-120_ineqaulities-a8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Cauchy–Schwarz and Hölder bound inner products.</li>
<li>Jensen shows convexity and expectation interact cleanly.</li>
<li>Fenchel’s inequality is the algebra of duality.</li>
<li>Supporting hyperplane inequality is the geometry of convexity.</li>
</ul>
<p>These inequalities are used implicitly all over convex optimisation.</p></body></html></section><section class="print-page" id="appendices-130_projections" heading-number="7.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about “dropping perpendiculars” to a subspace or convex set.</p>
<p>Projection onto a subspace: Let <span class="arithmatex">\(W \subseteq \mathbb{R}^n\)</span> be a subspace (e.g. defined by a set of linear equations <span class="arithmatex">\(Ax=0\)</span> or spanned by some basis vectors). The orthogonal projection of any <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> onto <span class="arithmatex">\(W\)</span> is the unique point <span class="arithmatex">\(P_W(x) \in W\)</span> such that <span class="arithmatex">\(x - P_W(x)\)</span> is orthogonal to <span class="arithmatex">\(W\)</span>. If <span class="arithmatex">\({q_1,\dots,q_k}\)</span> is an orthonormal basis of <span class="arithmatex">\(W\)</span>, then
​
<script type="math/tex; mode=display">
P_W(x) = \sum_{i=1}^k \langle x, q_i \rangle \, q_i
</script>
</p>
<p>as mentioned earlier. This <span class="arithmatex">\(P_W(x)\)</span> minimizes the distance <span class="arithmatex">\(|x - y|2\)</span> over all <span class="arithmatex">\(y\in W\)</span>. The residual <span class="arithmatex">\(r = x - P_W(x)\)</span> is orthogonal to every direction in <span class="arithmatex">\(W\)</span>. For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In <span class="arithmatex">\(\mathbb{R}^n\)</span>, <span class="arithmatex">\(P_W\)</span> is an <span class="arithmatex">\(n \times n\)</span> matrix (if <span class="arithmatex">\(W\)</span> is <span class="arithmatex">\(k\)</span>-dimensional, <span class="arithmatex">\(P_W\)</span> has rank <span class="arithmatex">\(k\)</span>) that satisfies <span class="arithmatex">\(P_W^2 = P_W\)</span> (idempotent) and <span class="arithmatex">\(P_W = P_W^T\)</span> (symmetric). In optimization, if we are constrained to <span class="arithmatex">\(W\)</span>, a projected gradient step does <span class="arithmatex">\(x_{k+1} = P_W(x_k - \alpha \nabla f(x_k))\)</span> to ensure <span class="arithmatex">\(x_{k+1} \in W\)</span>.</p>
<p>Projection onto a convex set: More generally, for a closed convex set <span class="arithmatex">\(C \subset \mathbb{R}^n\)</span>, the projection <span class="arithmatex">\(\operatorname{proj}_C(x)\)</span> is defined as the unique point in <span class="arithmatex">\(C\)</span> closest to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
P_C(x) = \arg\min_{y \in C} \|x - y\|_2
\]</div>
<p>For convex <span class="arithmatex">\(C\)</span>, this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box <span class="arithmatex">\([l,u]\)</span> just clips each coordinate between <span class="arithmatex">\(l\)</span> and <span class="arithmatex">\(u\)</span>). Some properties of convex projections: </p>
<ul>
<li>
<p><span class="arithmatex">\(P_C(x)\)</span> lies on the boundary of <span class="arithmatex">\(C\)</span> along the direction of <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(x \notin C\)</span>. </p>
</li>
<li>
<p>The first-order optimality condition for the minimization above says <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the tangent of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>, or equivalently <span class="arithmatex">\(\langle x - P_C(x), y - P_C(x)\rangle \le 0\)</span> for all <span class="arithmatex">\(y \in C\)</span>. This means the line from <span class="arithmatex">\(P_C(x)\)</span> to <span class="arithmatex">\(x\)</span> forms a supporting hyperplane to <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>. </p>
</li>
<li>Also, projections are firmly non-expansive: <span class="arithmatex">\(|P_C(x)-P_C(y)|^2 \le \langle P_C(x)-P_C(y), x-y \rangle \le |x-y|^2\)</span>. Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li>
</ul>
<p>Examples:</p>
<ul>
<li>
<p>Projection onto an affine set <span class="arithmatex">\(Ax=b\)</span> (assuming it’s consistent) can be derived via normal equations: one finds a correction <span class="arithmatex">\(\delta x\)</span> in the row space of <span class="arithmatex">\(A^T\)</span> such that <span class="arithmatex">\(A(x+\delta x)=b\)</span>. The solution is <span class="arithmatex">\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\)</span> (for full row rank <span class="arithmatex">\(A\)</span>).</p>
</li>
<li>
<p>Projection onto the nonnegative orthant <span class="arithmatex">\({x: x_i\ge0}\)</span> just sets <span class="arithmatex">\(x_i^- = \min(x_i,0)\)</span> to zero (i.e. <span class="arithmatex">\([x]^+ = \max{x,0}\)</span> componentwise). This is used in nonnegativity constraints.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\({|x|_2 \le \alpha}\)</span> scales <span class="arithmatex">\(x\)</span> down to have length <span class="arithmatex">\(\alpha\)</span> if <span class="arithmatex">\(|x|&gt;\alpha\)</span>, or does nothing if <span class="arithmatex">\(|x|\le\alpha\)</span>.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_1\)</span> ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal <span class="arithmatex">\(\alpha\)</span>.</p>
</li>
</ul>
<p>Why projections matter in optimization: Many convex optimization problems involve constraints <span class="arithmatex">\(x \in C\)</span> where <span class="arithmatex">\(C\)</span> is convex. If we can compute <span class="arithmatex">\(P_C(x)\)</span> easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to <span class="arithmatex">\(C\)</span>, we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the feasible region at <span class="arithmatex">\(P_C(x)\)</span> connects to KKT conditions: at optimum <span class="arithmatex">\(\hat{x}\)</span> with <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(\hat{x}))\)</span>, the vector <span class="arithmatex">\(-\nabla f(\hat{x})\)</span> must lie in the normal cone of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(\hat{x}\)</span>, meaning the gradient is “balanced” by the constraint boundary — this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(x^*))\)</span> for some step <span class="arithmatex">\(\alpha\)</span>, i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p>
<p>Orthogonal decomposition: Any vector <span class="arithmatex">\(x\)</span> can be uniquely decomposed relative to a subspace <span class="arithmatex">\(W\)</span> as <span class="arithmatex">\(x = P_W(x) + r\)</span> with <span class="arithmatex">\(r \perp W\)</span>. Moreover, <span class="arithmatex">\(|x|^2 = |P_W(x)|^2 + |r|^2\)</span> (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint <span class="arithmatex">\(x\in W\)</span>, any descent direction <span class="arithmatex">\(d\)</span> can be split into a part tangent to <span class="arithmatex">\(W\)</span> (which actually moves within <span class="arithmatex">\(W\)</span>) and a part normal to <span class="arithmatex">\(W\)</span> (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient <span class="arithmatex">\(\nabla f(x^)\)</span> being orthogonal to the feasible region means <span class="arithmatex">\(\nabla f(x^)\)</span> lies entirely in the normal subspace <span class="arithmatex">\(W^\perp\)</span> — no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: <span class="arithmatex">\(\nabla f(x^)\)</span> is in the row space of <span class="arithmatex">\(A\)</span> if <span class="arithmatex">\(Ax^=b\)</span> are active constraints, which leads to <span class="arithmatex">\(\nabla f(x^*) = A^T \lambda\)</span> for some <span class="arithmatex">\(\lambda\)</span> (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p>
<p>Projection algorithms: The simplicity or difficulty of computing <span class="arithmatex">\(P_C(x)\)</span> often determines if we can solve a problem efficiently. If <span class="arithmatex">\(C\)</span> is something like a polyhedron given by linear inequalities, <span class="arithmatex">\(P_C\)</span> might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing <span class="arithmatex">\(\operatorname{prox}{\gamma g}(x) = \arg\min_y {g(y) + \frac{1}{2\gamma}|y-x|^2}\)</span>, which for indicator functions of set <span class="arithmatex">\(C\)</span> yields <span class="arithmatex">\(\operatorname{prox}{\delta_C}(x) = P_C(x)\)</span>. Thus projection is a special proximal operator (one for constraints).</p>
<p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in <span class="arithmatex">\(C_1 \cap C_2\)</span> by <span class="arithmatex">\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\)</span>), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections — that the closest point condition yields orthogonality conditions and that projections do not expand distances — is crucial for understanding how constraint-handling algorithms converge.</p></body></html></section><section class="print-page" id="appendices-140_support" heading-number="7.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-b-support-functions-and-dual-geometry-advanced">Appendix B: Support Functions and Dual Geometry (Advanced)<a class="headerlink" href="#appendices-140_support-appendix-b-support-functions-and-dual-geometry-advanced" title="Permanent link">¶</a></h1>
<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>
<hr>
<h2 id="appendices-140_support-b1-support-functions">B.1 Support functions<a class="headerlink" href="#appendices-140_support-b1-support-functions" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be a nonempty set. The support function of <span class="arithmatex">\(C\)</span> is
<script type="math/tex; mode=display">
\sigma_C(y) = \sup_{x \in C} y^\top x.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For a given direction <span class="arithmatex">\(y\)</span>, <span class="arithmatex">\(\sigma_C(y)\)</span> tells you how far you can go in that direction while staying in <span class="arithmatex">\(C\)</span>.</li>
<li>It is the value of the linear maximisation problem
  <script type="math/tex; mode=display">
  \max_{x \in C} y^\top x.
  </script>
</li>
</ul>
<p>Key facts:</p>
<ol>
<li><span class="arithmatex">\(\sigma_C\)</span> is always convex, even if <span class="arithmatex">\(C\)</span> is not convex.</li>
<li>If <span class="arithmatex">\(C\)</span> is convex and closed, <span class="arithmatex">\(\sigma_C\)</span> essentially characterises <span class="arithmatex">\(C\)</span>.<br>
   In particular, <span class="arithmatex">\(C\)</span> can be recovered as the intersection of halfspaces
   <script type="math/tex; mode=display">
   x^\top y \le \sigma_C(y)\quad \text{for all } y.
   </script>
</li>
</ol>
<p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>
<hr>
<h2 id="appendices-140_support-b2-support-functions-and-dual-norms">B.2 Support functions and dual norms<a class="headerlink" href="#appendices-140_support-b2-support-functions-and-dual-norms" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(C\)</span> is the unit ball of a norm <span class="arithmatex">\(\|\cdot\|\)</span>, i.e.
<script type="math/tex; mode=display">
C = \{ x : \|x\| \le 1 \},
</script>
then
<script type="math/tex; mode=display">
\sigma_C(y)
=
\sup_{\|x\|\le 1} y^\top x
=
\|y\|_*,
</script>
the dual norm of <span class="arithmatex">\(\|\cdot\|\)</span>.</p>
<p>Example:</p>
<ul>
<li>For <span class="arithmatex">\(\ell_2\)</span>, <span class="arithmatex">\(\|\cdot\|_2\)</span> is self-dual, so <span class="arithmatex">\(\|y\|_2^* = \|y\|_2\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_1\)</span>, the dual norm is <span class="arithmatex">\(\ell_\infty\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_\infty\)</span>, the dual norm is <span class="arithmatex">\(\ell_1\)</span>.</li>
</ul>
<p>This shows that dual norms are just support functions of norm balls.</p>
<hr>
<h2 id="appendices-140_support-b3-indicator-functions-and-conjugates">B.3 Indicator functions and conjugates<a class="headerlink" href="#appendices-140_support-b3-indicator-functions-and-conjugates" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a set <span class="arithmatex">\(C\)</span>:
<script type="math/tex; mode=display">
\delta_C(x) =
\begin{cases}
0 & x \in C, \\
+\infty & x \notin C.
\end{cases}
</script>
</p>
<p>Its convex conjugate is
<script type="math/tex; mode=display">
\delta_C^*(y)
=
\sup_x (y^\top x - \delta_C(x))
=
\sup_{x \in C} y^\top x
=
\sigma_C(y).
</script>
</p>
<p>Thus,</p>
<blockquote>
<p>The support function <span class="arithmatex">\(\sigma_C\)</span> is the convex conjugate of the indicator of <span class="arithmatex">\(C\)</span>.</p>
</blockquote>
<p>This is extremely important conceptually:</p>
<ul>
<li>Conjugates turn sets into functions.</li>
<li>Duality in optimisation is often conjugacy in disguise.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b4-hyperplane-separation-revisited">B.4 Hyperplane separation revisited<a class="headerlink" href="#appendices-140_support-b4-hyperplane-separation-revisited" title="Permanent link">¶</a></h2>
<p>Recall: if <span class="arithmatex">\(C\)</span> is closed and convex, then at any boundary point <span class="arithmatex">\(x_0 \in C\)</span> there is a supporting hyperplane
<script type="math/tex; mode=display">
a^\top x \le a^\top x_0
\quad \text{for all } x \in C.
</script>
</p>
<p>This <span class="arithmatex">\(a\)</span> is exactly the kind of vector we would use in a support function evaluation. In fact, <span class="arithmatex">\(a^\top x_0 = \sigma_C(a)\)</span> if <span class="arithmatex">\(x_0\)</span> is an extreme point (or exposed point) in direction <span class="arithmatex">\(a\)</span>.</p>
<p>Geometric interpretation:</p>
<ul>
<li>Lagrange multipliers in the dual problem play the role of these <span class="arithmatex">\(a\)</span>’s.</li>
<li>They identify supporting hyperplanes that “witness” optimality.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b5-duality-as-support">B.5 Duality as support<a class="headerlink" href="#appendices-140_support-b5-duality-as-support" title="Permanent link">¶</a></h2>
<p>Consider the (convex) primal problem
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & x \in C,
\end{array}
</script>
where <span class="arithmatex">\(C\)</span> is a convex feasible set.</p>
<p>We can rewrite the problem as minimising
<script type="math/tex; mode=display">
f(x) + \delta_C(x).
</script>
</p>
<p>The convex conjugate of <span class="arithmatex">\(f + \delta_C\)</span> is
<script type="math/tex; mode=display">
(f + \delta_C)^*(y)
=
\inf_{u+v=y} \left( f^*(u) + \delta_C^*(v) \right)
=
\inf_{u+v=y} \left( f^*(u) + \sigma_C(v) \right).
</script>
</p>
<p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on <span class="arithmatex">\(f(x)\)</span> over <span class="arithmatex">\(x \in C\)</span> using conjugates and support functions (Rockafellar, 1970).</p>
<p>This view makes precise the slogan:</p>
<blockquote>
<p>“Dual variables are hyperplanes that support the feasible set and the objective from below.”</p>
</blockquote>
<hr>
<h2 id="appendices-140_support-b6-geometry-of-kkt-and-multipliers">B.6 Geometry of KKT and multipliers<a class="headerlink" href="#appendices-140_support-b6-geometry-of-kkt-and-multipliers" title="Permanent link">¶</a></h2>
<p>At the optimal point <span class="arithmatex">\(x^*\)</span> of a convex problem, there is typically a hyperplane that supports the feasible set at <span class="arithmatex">\(x^*\)</span> and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p>
<ul>
<li>The multipliers form a certificate that <span class="arithmatex">\(x^*\)</span> cannot be improved without violating feasibility.</li>
<li>The dual problem is the search for the “best” such certificate.</li>
</ul>
<p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater’s condition (Boyd and Vandenberghe, 2004).</p>
<hr>
<h2 id="appendices-140_support-b7-why-this-matters">B.7 Why this matters<a class="headerlink" href="#appendices-140_support-b7-why-this-matters" title="Permanent link">¶</a></h2>
<p>This geometric point of view is not just pretty:</p>
<ul>
<li>It explains why strong duality holds.</li>
<li>It explains what <span class="arithmatex">\(\mu_i^*\)</span> and <span class="arithmatex">\(\lambda_j^*\)</span> “mean.”</li>
<li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li>
</ul>
<!-- # F.2 Support Functions and Dual Geometry

Support functions are one of the most elegant bridges between convex sets and linear optimization. For any convex set, they describe its extent in a given direction — and thus appear naturally in:

- Duality theory and convex conjugates (see Section D.1)  
- Norm analysis and dual norms (Section A.4 and A.5)  
- Subgradient calculations and optimality conditions (Section A.7 and Section D.3)  
- Projection and cutting-plane algorithms in high-dimensional optimization  

Geometrically, a support function tells you:  
> *How far can I go in direction $y$ and still remain inside the set $C$?*

---

## Definition and Geometry

Let $C \subseteq \mathbb{R}^n$ be a nonempty convex set. The support function $\sigma_C : \mathbb{R}^n \to \mathbb{R}$ is defined as:

$$
\sigma_C(y) = \sup_{x \in C} \langle y, x \rangle
$$

- $y$ is the direction vector.  
- $\langle y, x \rangle$ is the inner product (see Section A.2).  
- $\sigma_C(y)$ gives the maximum projection of $C$ along direction $y$.

It corresponds to the furthest point of $C$ in direction $y$, and hence defines a supporting hyperplane to the set.

---

## Key Properties

- Positive Homogeneity:  
  $$
  \sigma_C(\alpha y) = \alpha \sigma_C(y) \quad \text{for } \alpha \ge 0
  $$
- Convexity:  
  $$
  \sigma_C(y_1 + y_2) \le \sigma_C(y_1) + \sigma_C(y_2)
  $$
- Attainment: If $C$ is closed and bounded (compact), the supremum is attained — the max is reached at some $x^\star \in C$.  
- Set Representation:  
  Every closed convex set can be recovered from its support function:
  $$
  C = \{ x \in \mathbb{R}^n \mid \langle y, x \rangle \le \sigma_C(y) \quad \forall y \in \mathbb{R}^n \}
  $$

---

## Computation and Intuition

To compute $\sigma_C(y)$:

1. Specify the convex set $C$ (e.g., a ball, polytope, or feasible region).
2. Fix the direction $y \in \mathbb{R}^n$.
3. Maximize the dot product $\langle y, x \rangle$ over $x \in C$.

This is a linear program over $C$.

### Links to Optimization:
- In duality theory (Section D.1), linear functionals $\langle y, x \rangle$ are used to lower-bound convex functions — support functions arise naturally.
- For constraint sets defined by indicator functions (Section A.8), the support function is their convex conjugate:
  $$
  \sigma_C = \delta_C^*
  $$

---

## Examples

### Example 1: $\ell_2$ Unit Ball

Let $C = \{ x \mid \|x\|_2 \le 1 \}$

Then the support function is:

$$
\sigma_C(y) = \sup_{\|x\|_2 \le 1} \langle y, x \rangle = \|y\|_2
$$

Interpretation: the farthest point in direction $y$ lies on the boundary and aligns with $y$.

👉 This reveals that the support function of a norm ball gives the dual norm — see Section A.4.

---

### Example 2: $\ell_1$ Unit Ball

Let $C = \{ x \mid \|x\|_1 \le 1 \}$

Then:

$$
\sigma_C(y) = \|y\|_\infty
$$

Intuition: in direction $y$, the maximal point in $C$ aligns with the coordinate having largest magnitude.

This dual norm relationship is fundamental in sparsity-inducing optimization (e.g., LASSO in Section F.1).

---

### Example 3: Polytope (Convex Hull)

Let $C = \text{conv}\{v_1, \dots, v_m\}$

Then:

$$
\sigma_C(y) = \max_{i=1,\dots,m} \langle y, v_i \rangle
$$

Interpretation: the maximum projection occurs at one of the vertices of the polytope.

In LP problems (see Section C.3), this is how extreme points determine optimal solutions.

---

## Applications in Optimization

### 🔄 Duality and Convex Conjugates

The support function is the Fenchel conjugate of an indicator function:

$$
\sigma_C(y) = \delta_C^*(y)
$$

This connection underpins many dual optimization frameworks, including saddle-point methods, dual norms, and variational formulations.

---

### 📐 Dual Norms

For any norm $\|\cdot\|$, the support function of its unit ball gives the dual norm:

$$
\sigma_{B}(y) = \sup_{\|x\| \le 1} \langle y, x \rangle = \|y\|_*
$$

See Section A.5 for the definition of dual norms, and Section C.1 for how they affect step sizes and convergence geometry.

---

### 📏 Geometric Use Cases

- Compute distances to sets via duality.  
- Generate separating hyperplanes for convex sets.  
- Implement projection algorithms (e.g., mirror descent in Section K.1).  
- Construct robust constraints and worst-case bounds in uncertainty modeling (see Section E.2).

---

## Summary and Takeaways

- The support function $\sigma_C(y)$ measures how far a convex set extends in direction $y$.
- It is always convex, positively homogeneous, and subadditive.
- Support functions appear in:
  - Duality theory via convex conjugates  
  - Norm analysis via dual norms  
  - Subgradients and projections  
  - Constraint representations and recovery of convex sets
- For norm balls, $\sigma_C$ gives the dual norm.  
- For polytopes, $\sigma_C$ is the max over vertices.  
- For machine learning, support functions help model constraints, regularization penalties, and geometric algorithms.

Mental model:  
Think of a support function as a “radar scan” — it tells you the furthest point of a convex set in any given direction.
 --></body></html></section><section class="print-page" id="appendices-160_conjugates" heading-number="7.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-d-convex-conjugates-and-fenchel-duality">Appendix D: Convex Conjugates and Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-appendix-d-convex-conjugates-and-fenchel-duality" title="Permanent link">¶</a></h1>
<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis.<br>
They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p>
<ul>
<li>Convex conjugates convert a function into its “slope-space” representation — capturing its tightest linear overestimates.  </li>
<li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li>
</ul>
<p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>
<h2 id="appendices-160_conjugates-d1-intuitive-picture">D.1 Intuitive Picture<a class="headerlink" href="#appendices-160_conjugates-d1-intuitive-picture" title="Permanent link">¶</a></h2>
<p>Imagine a convex function <span class="arithmatex">\(f(x)\)</span> drawn as a bowl in space.<br>
Each point <span class="arithmatex">\(y\)</span> defines a line (or hyperplane) of slope <span class="arithmatex">\(y\)</span>:
<script type="math/tex; mode=display">
x \mapsto \langle y, x \rangle - b.
</script>
The convex conjugate <span class="arithmatex">\(f^*(y)\)</span> is the smallest height <span class="arithmatex">\(b\)</span> such that this line always stays above <span class="arithmatex">\(f(x)\)</span>.<br>
In other words:</p>
<blockquote>
<p><span class="arithmatex">\(f^*(y)\)</span> measures the tightest linear overestimate of <span class="arithmatex">\(f\)</span> in direction <span class="arithmatex">\(y\)</span>.</p>
</blockquote>
<p>So <span class="arithmatex">\(f^*\)</span> encodes how “steep” <span class="arithmatex">\(f\)</span> can be in every direction — it transforms the geometry of <span class="arithmatex">\(f\)</span> into a new convex function on slope-space.</p>
<h2 id="appendices-160_conjugates-d2-definition-and-key-properties">D.2 Definition and Key Properties<a class="headerlink" href="#appendices-160_conjugates-d2-definition-and-key-properties" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\cup\{+\infty\}\)</span> be a proper convex function.<br>
Its convex (Fenchel) conjugate is
<script type="math/tex; mode=display">
f^*(y) = \sup_{x \in \mathbb{R}^n} \big( \langle y, x \rangle - f(x) \big).
</script>
</p>
<p>Interpretation
- <span class="arithmatex">\(y\)</span>: a slope or linear functional.
- The supremum seeks the largest gap between the linear function <span class="arithmatex">\(\langle y,x\rangle\)</span> and the graph of <span class="arithmatex">\(f\)</span>.
- <span class="arithmatex">\(f^*(y)\)</span> is always convex, even if <span class="arithmatex">\(f\)</span> isn’t strictly convex.</p>
<h3 id="appendices-160_conjugates-fundamental-identities">Fundamental Identities<a class="headerlink" href="#appendices-160_conjugates-fundamental-identities" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Fenchel–Young inequality
   <script type="math/tex; mode=display">
   \langle y,x\rangle \le f(x) + f^*(y),
   </script>
   with equality iff <span class="arithmatex">\(y \in \partial f(x)\)</span>.</p>
</li>
<li>
<p>Biconjugation
   <script type="math/tex; mode=display">
   f^{} = f \quad \text{if <span class="arithmatex">\(f\)</span> is proper, convex, and lower semicontinuous.}
   </script>
   This tells us the conjugate transform loses no information for convex functions.</p>
</li>
<li>
<p>Order reversal
   <span class="arithmatex">\(f \le g \;\Rightarrow\; f^* \ge g^*\)</span>.</p>
</li>
<li>
<p>Scaling and shift</p>
</li>
<li><span class="arithmatex">\((f + a)^*(y) = f^*(y) - a\)</span>,</li>
<li><span class="arithmatex">\((\alpha f)^*(y) = \alpha f^*(y/\alpha)\)</span> for <span class="arithmatex">\(\alpha&gt;0.\)</span></li>
</ol>
<hr>
<h2 id="appendices-160_conjugates-d3-canonical-examples">D.3 Canonical Examples<a class="headerlink" href="#appendices-160_conjugates-d3-canonical-examples" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Function <span class="arithmatex">\(f(x)\)</span></th>
<th>Conjugate <span class="arithmatex">\(f^*(y)\)</span></th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\( \tfrac{1}{2}\|x\|_2^2 \)</span></td>
<td><span class="arithmatex">\( \tfrac{1}{2}\|y\|_2^2 \)</span></td>
<td>Self-conjugate quadratic</td>
</tr>
<tr>
<td><span class="arithmatex">\( \|x\|_1 \)</span></td>
<td><span class="arithmatex">\( \delta_{\{\|y\|_\infty \le 1\}}(y) \)</span></td>
<td>Dual norm indicator</td>
</tr>
<tr>
<td><span class="arithmatex">\( \delta_C(x) \)</span></td>
<td><span class="arithmatex">\( \sigma_C(y)=\sup_{x\in C}\langle y,x\rangle \)</span></td>
<td>Support function of set <span class="arithmatex">\(C\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\( e^x \)</span></td>
<td><span class="arithmatex">\( y\log y - y,\, y&gt;0 \)</span></td>
<td>Appears in entropy and KL-divergence</td>
</tr>
</tbody>
</table>
<p>These examples illustrate how conjugation connects:
- Norms ↔ dual norms,<br>
- Sets ↔ support functions,<br>
- Exponentials ↔ entropy,<br>
- Quadratics ↔ themselves.</p>
<h2 id="appendices-160_conjugates-d4-geometric-interpretation">D.4 Geometric Interpretation<a class="headerlink" href="#appendices-160_conjugates-d4-geometric-interpretation" title="Permanent link">¶</a></h2>
<ul>
<li>Each point on <span class="arithmatex">\(f\)</span> has a tangent hyperplane whose slope is a subgradient.  </li>
<li>The collection of all such hyperplanes forms the epigraph of <span class="arithmatex">\(f^*\)</span>.  </li>
<li>The transformation <span class="arithmatex">\(f \mapsto f^*\)</span> swaps the roles of “position” and “slope”:<br>
  convex geometry ↔ supporting hyperplanes.</li>
</ul>
<p>Visually:<br>
- <span class="arithmatex">\(f\)</span> describes a bowl in <span class="arithmatex">\((x,t)\)</span>-space.<br>
- <span class="arithmatex">\(f^*\)</span> describes the envelope of tangent planes to that bowl.</p>
<h2 id="appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality">D.5 From Conjugates to Duality — Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality" title="Permanent link">¶</a></h2>
<p>Many convex optimization problems can be written as
<script type="math/tex; mode=display">
\min_x \; f(x) + g(Ax),
</script>
where <span class="arithmatex">\(f,g\)</span> are convex and <span class="arithmatex">\(A\)</span> is linear.<br>
Fenchel duality uses conjugates to build a dual problem in terms of <span class="arithmatex">\(f^*\)</span> and <span class="arithmatex">\(g^*\)</span>.</p>
<h3 id="appendices-160_conjugates-the-fenchel-dual-problem">The Fenchel Dual Problem<a class="headerlink" href="#appendices-160_conjugates-the-fenchel-dual-problem" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\max_y \; -f^*(A^\top y) - g^*(-y).
\]</div>
<p>Interpretation
- <span class="arithmatex">\(y\)</span> is the dual variable (similar to Lagrange multipliers).<br>
- The dual objective collects the best linear lower bounds on the primal cost.</p>
<h2 id="appendices-160_conjugates-d6-weak-and-strong-duality">D.6 Weak and Strong Duality<a class="headerlink" href="#appendices-160_conjugates-d6-weak-and-strong-duality" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Weak duality: For any <span class="arithmatex">\(x,y\)</span>,
  <script type="math/tex; mode=display">
  f(x)+g(Ax) \ge -f^*(A^\top y) - g^*(-y).
  </script>
  So the dual value always underestimates the primal value.</p>
</li>
<li>
<p>Strong duality:<br>
  If <span class="arithmatex">\(f,g\)</span> are closed convex and a mild constraint qualification holds (e.g. Slater’s condition — existence of strictly feasible <span class="arithmatex">\(x\)</span>), then
  <script type="math/tex; mode=display">
  \min_x [f(x)+g(Ax)] = \max_y [-f^*(A^\top y) - g^*(-y)].
  </script>
</p>
</li>
</ul>
<p>At the optimum:
<script type="math/tex; mode=display">
A^\top y^* \in \partial f(x^*), 
\qquad
-y^* \in \partial g(Ax^*).
</script>
These are the Fenchel–KKT conditions, directly linking primal and dual subgradients.</p>
<h2 id="appendices-160_conjugates-d7-illustrative-examples">D.7 Illustrative Examples<a class="headerlink" href="#appendices-160_conjugates-d7-illustrative-examples" title="Permanent link">¶</a></h2>
<h3 id="appendices-160_conjugates-a-linear-programming">(a) Linear Programming<a class="headerlink" href="#appendices-160_conjugates-a-linear-programming" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_{x \ge 0} c^\top x \quad \text{s.t. } Ax = b.
</script>
</p>
<p>Take<br>
<span class="arithmatex">\(f(x) = c^\top x + \delta_{\{x\ge0\}}(x)\)</span>,<br>
<span class="arithmatex">\(g(z)=\delta_{\{z=b\}}(z)\)</span>.</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y) = \delta_{\{y \le c\}}(y),
\qquad
g^*(y) = b^\top y.
</script>
</p>
<p>Dual:
<script type="math/tex; mode=display">
\max_y \; b^\top y \quad \text{s.t. } A^\top y \le c,
</script>
which is the standard LP dual.</p>
<h3 id="appendices-160_conjugates-b-quadratic-set-constraint">(b) Quadratic + Set Constraint<a class="headerlink" href="#appendices-160_conjugates-b-quadratic-set-constraint" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_x \tfrac{1}{2}\|x\|_2^2 + \delta_C(x).
</script>
</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y)=\tfrac{1}{2}\|y\|_2^2, \qquad g^*(y)=\sigma_C(y),
</script>
so the dual is
<script type="math/tex; mode=display">
\max_y -\tfrac{1}{2}\|y\|_2^2 - \sigma_C(y).
</script>
Optimality gives <span class="arithmatex">\(x^*=y^*\)</span>, the projection condition in Euclidean geometry.</p>
<h2 id="appendices-160_conjugates-d8-practical-significance">D.8 Practical Significance<a class="headerlink" href="#appendices-160_conjugates-d8-practical-significance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Area</th>
<th>How Fenchel Duality Appears</th>
</tr>
</thead>
<tbody>
<tr>
<td>Optimization theory</td>
<td>Derives general dual problems beyond inequality constraints.</td>
</tr>
<tr>
<td>Algorithm design</td>
<td>Basis for primal–dual and splitting methods (ADMM, Chambolle–Pock, Mirror Descent).</td>
</tr>
<tr>
<td>Geometry</td>
<td>Dual problem finds the “best supporting hyperplane” to the primal epigraph.</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Loss–regularizer pairs (hinge ↔ clipped loss, logistic ↔ log-sum-exp) often form conjugate pairs.</td>
</tr>
<tr>
<td>Proximal operators</td>
<td>Linked via Moreau identity:  <span class="arithmatex">\(\mathrm{prox}_{f^*}(y) = y - \mathrm{prox}_f(y)\)</span>.</td>
</tr>
</tbody>
</table>
<h2 id="appendices-160_conjugates-d9-conceptual-unification">D.9 Conceptual Unification<a class="headerlink" href="#appendices-160_conjugates-d9-conceptual-unification" title="Permanent link">¶</a></h2>
<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p>
<ul>
<li>From geometry: support functions, projections, subgradients (Appendices B–C).  </li>
<li>From analysis: inequalities like Fenchel’s and Jensen’s (Appendix A).  </li>
<li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7–8).  </li>
<li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9–10).</li>
</ul>
<p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>
<h2 id="appendices-160_conjugates-d10-summary-and-takeaways">D.10 Summary and Takeaways<a class="headerlink" href="#appendices-160_conjugates-d10-summary-and-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>The convex conjugate <span class="arithmatex">\(f^*\)</span> expresses <span class="arithmatex">\(f\)</span> through its linear support planes.  </li>
<li>The Fenchel–Young inequality connects primal variables and dual slopes.  </li>
<li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li>
<li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li>
<li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li>
</ul>
<hr>
<p>Further Reading</p>
<ul>
<li>Rockafellar, R. T. (1970). <em>Convex Analysis</em>. Princeton UP.  </li>
<li>Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex Optimization</em>, Chs. 3 &amp; 5.  </li>
<li>Bauschke, H. H., &amp; Combettes, P. L. (2017). <em>Convex Analysis and Monotone Operator Theory</em>.  </li>
<li>Hiriart-Urruty, J.-B., &amp; Lemaréchal, C. (2001). <em>Fundamentals of Convex Analysis</em>.  </li>
</ul></body></html></section><section class="print-page" id="appendices-170_probability" heading-number="7.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-e-convexity-in-probability-and-statistics">Appendix E : Convexity in Probability and Statistics<a class="headerlink" href="#appendices-170_probability-appendix-e-convexity-in-probability-and-statistics" title="Permanent link">¶</a></h1>
<p>Convex analysis is not just geometry and optimization — it is deeply woven into probability, statistics, and information theory.<br>
Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p>
<p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>
<h2 id="appendices-170_probability-e1-convexity-of-expectations">E.1 Convexity of Expectations<a class="headerlink" href="#appendices-170_probability-e1-convexity-of-expectations" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f:\mathbb{R}^n\!\to\!\mathbb{R}\)</span> be convex and <span class="arithmatex">\(X\)</span> a random vector.<br>
Then by Jensen’s inequality (Appendix A):</p>
<div class="arithmatex">\[
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
\]</div>
<h3 id="appendices-170_probability-consequences">Consequences<a class="headerlink" href="#appendices-170_probability-consequences" title="Permanent link">¶</a></h3>
<ul>
<li>Expectations preserve convexity:<br>
  if each <span class="arithmatex">\(f(\cdot,\xi)\)</span> is convex, then <span class="arithmatex">\(F(x)=\mathbb{E}_\xi[f(x,\xi)]\)</span> is convex.</li>
<li>Stochastic objectives in ML — e.g. expected loss <span class="arithmatex">\(\mathbb{E}_{(a,b)}[\ell(a^\top x,b)]\)</span> — are convex when the sample-wise loss is convex.</li>
</ul>
<p>Hence almost all <em>empirical risk minimization</em> problems are discrete approximations of convex expectations.</p>
<h2 id="appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions">E.2 Convexity of Log-Partition and Moment-Generating Functions<a class="headerlink" href="#appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions" title="Permanent link">¶</a></h2>
<p>For a random variable <span class="arithmatex">\(X\)</span>, the moment-generating function (MGF) and cumulant-generating function (CGF) are</p>
<div class="arithmatex">\[
M_X(t)=\mathbb{E}[e^{tX}], \qquad
K_X(t)=\log M_X(t).
\]</div>
<p>Fact: <span class="arithmatex">\(K_X(t)\)</span> is always convex in <span class="arithmatex">\(t\)</span>.</p>
<p>Reason: <span class="arithmatex">\(K_X''(t)=\mathrm{Var}_t(X)\ge0\)</span>;<br>
variance is nonnegative.  </p>
<h3 id="appendices-170_probability-implications">Implications<a class="headerlink" href="#appendices-170_probability-implications" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(K_X(t)\)</span> acts as a convex “potential” controlling exponential families.</li>
<li>The log-partition function in statistics,
  <script type="math/tex; mode=display">
  A(\theta)=\log \int e^{\langle \theta,T(x)\rangle}\,h(x)\,dx,
  </script>
  is convex in <span class="arithmatex">\(\theta\)</span> (strictly convex for full exponential families).</li>
<li>Its gradient gives the mean parameter: <span class="arithmatex">\(\nabla A(\theta)=\mathbb{E}_\theta[T(X)]\)</span>.</li>
</ul>
<p>Thus convexity of <span class="arithmatex">\(A\)</span> guarantees a one-to-one mapping between natural and mean parameters — a foundation of exponential-family inference.</p>
<h2 id="appendices-170_probability-e3-exponential-families-and-dual-convexity">E.3 Exponential Families and Dual Convexity<a class="headerlink" href="#appendices-170_probability-e3-exponential-families-and-dual-convexity" title="Permanent link">¶</a></h2>
<p>An exponential-family density has the form
<script type="math/tex; mode=display">
p_\theta(x)=\exp\big(\langle\theta,T(x)\rangle-A(\theta)\big)h(x).
</script>
</p>
<p>Properties:</p>
<ol>
<li><span class="arithmatex">\(A(\theta)\)</span> is convex, smooth, and serves as a potential function.</li>
<li>Its convex conjugate <span class="arithmatex">\(A^*(\mu)\)</span> defines the entropy of the family:
   <script type="math/tex; mode=display">
   A^*(\mu)=\sup_\theta(\langle\mu,\theta\rangle-A(\theta)) = -H(p_\mu),
   </script>
   where <span class="arithmatex">\(H\)</span> is the Shannon entropy of the distribution with mean <span class="arithmatex">\(\mu\)</span>.</li>
</ol>
<p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>
<h2 id="appendices-170_probability-e4-convex-divergences-and-information-measures">E.4 Convex Divergences and Information Measures<a class="headerlink" href="#appendices-170_probability-e4-convex-divergences-and-information-measures" title="Permanent link">¶</a></h2>
<h3 id="appendices-170_probability-a-kullbackleibler-kl-divergence">(a) Kullback–Leibler (KL) Divergence<a class="headerlink" href="#appendices-170_probability-a-kullbackleibler-kl-divergence" title="Permanent link">¶</a></h3>
<p>For densities <span class="arithmatex">\(p,q\)</span>,
<script type="math/tex; mode=display">
D_{\mathrm{KL}}(p\|q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx.
</script>
</p>
<ul>
<li><span class="arithmatex">\(D_{\mathrm{KL}}\)</span> is jointly convex in <span class="arithmatex">\((p,q)\)</span>.  </li>
<li>Proof: the function <span class="arithmatex">\((u,v)\mapsto u\log(u/v)\)</span> is convex on <span class="arithmatex">\(\mathbb{R}_+^2\)</span>.  </li>
<li>Consequently, mixtures of distributions cannot increase KL divergence — a key fact in variational inference and EM.</li>
</ul>
<h3 id="appendices-170_probability-b-bregman-divergences">(b) Bregman Divergences<a class="headerlink" href="#appendices-170_probability-b-bregman-divergences" title="Permanent link">¶</a></h3>
<p>Given a differentiable convex <span class="arithmatex">\(\phi\)</span>, define
<script type="math/tex; mode=display">
D_\phi(x\|y)=\phi(x)-\phi(y)-\langle\nabla\phi(y),x-y\rangle.
</script>
KL divergence is a Bregman divergence for <span class="arithmatex">\(\phi(p)=\sum_i p_i\log p_i\)</span>.<br>
Thus information-theoretic distances are <em>geometric shadows</em> of convex functions.</p>
<h3 id="appendices-170_probability-c-f-divergences">(c) f-Divergences<a class="headerlink" href="#appendices-170_probability-c-f-divergences" title="Permanent link">¶</a></h3>
<p>A general convex generator <span class="arithmatex">\(f\)</span> with <span class="arithmatex">\(f(1)=0\)</span> yields
<script type="math/tex; mode=display">
D_f(p\|q)=\int q(x)\,f\!\left(\frac{p(x)}{q(x)}\right)dx.
</script>
Convexity of <span class="arithmatex">\(f\)</span> ⇒ convexity of <span class="arithmatex">\(D_f\)</span>.<br>
Common choices recover KL, χ², Hellinger, and Jensen–Shannon divergences.</p>
<h2 id="appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning">E.5 Convex Loss Functions in Statistics and Machine Learning<a class="headerlink" href="#appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning" title="Permanent link">¶</a></h2>
<p>Convexity ensures estimators are globally optimal and algorithms converge.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Loss / Negative Log-Likelihood</th>
<th>Convexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian noise</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2\)</span></td>
<td>quadratic, strongly convex</td>
</tr>
<tr>
<td>Laplace noise</td>
<td><span class="arithmatex">\(\|Ax-b\|_1\)</span></td>
<td>convex, nonsmooth</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\log(1+e^{-y a^\top x})\)</span></td>
<td>convex, smooth</td>
</tr>
<tr>
<td>Poisson regression</td>
<td><span class="arithmatex">\(e^{a^\top x}-y a^\top x\)</span></td>
<td>convex, exponential</td>
</tr>
<tr>
<td>Huber loss</td>
<td>piecewise quadratic/linear</td>
<td>convex, robust</td>
</tr>
</tbody>
</table>
<p>Convexity of the negative log-likelihood follows from convexity of the log-partition function <span class="arithmatex">\(A(\theta)\)</span> in exponential families.</p>
<h2 id="appendices-170_probability-e6-convexity-and-bayesian-inference">E.6 Convexity and Bayesian Inference<a class="headerlink" href="#appendices-170_probability-e6-convexity-and-bayesian-inference" title="Permanent link">¶</a></h2>
<p>In Bayesian inference, convexity appears in:</p>
<ul>
<li>
<p>Log-concave posteriors:<br>
  If the likelihood and prior are log-concave, the posterior <span class="arithmatex">\(p(x|y)\propto \exp(-f(x))\)</span> is also log-concave ⇒<br>
<span class="arithmatex">\(\log p(x|y)\)</span> concave, <span class="arithmatex">\(f(x)\)</span> convex.</p>
</li>
<li>
<p>MAP estimation:<br>
  Maximizing <span class="arithmatex">\(\log p(x|y)\)</span> ≡ minimizing a convex function when <span class="arithmatex">\(p(x|y)\)</span> is log-concave ⇒ global optimum guaranteed.</p>
</li>
<li>
<p>Variational inference:<br>
  The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).<br>
  Optimizing it is equivalent to minimizing a convex divergence.</p>
</li>
</ul>
<p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>
<h2 id="appendices-170_probability-e7-statistical-risk-and-convex-surrogates">E.7 Statistical Risk and Convex Surrogates<a class="headerlink" href="#appendices-170_probability-e7-statistical-risk-and-convex-surrogates" title="Permanent link">¶</a></h2>
<p>Convex surrogate losses replace nonconvex 0–1 loss with convex approximations:</p>
<ul>
<li>Hinge loss (<span class="arithmatex">\(\max(0,1-y a^\top x)\)</span>) → support-vector machines.  </li>
<li>Logistic loss → probabilistic classification (cross-entropy).  </li>
<li>Exponential loss → AdaBoost.</li>
</ul>
<p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p></body></html></section><section class="print-page" id="appendices-180_subgradient_methods" heading-number="7.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-f-subgradient-method-derivation-geometry-and-convergence">Appendix F: Subgradient Method: Derivation, Geometry, and Convergence<a class="headerlink" href="#appendices-180_subgradient_methods-appendix-f-subgradient-method-derivation-geometry-and-convergence" title="Permanent link">¶</a></h1>
<p>This appendix presents the subgradient method—the fundamental algorithm for minimizing nonsmooth convex functions.<br>
It generalizes gradient descent to functions such as the <span class="arithmatex">\(\ell_1\)</span> norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>
<h2 id="appendices-180_subgradient_methods-f1-problem-setup">F.1 Problem Setup<a class="headerlink" href="#appendices-180_subgradient_methods-f1-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is convex but possibly nondifferentiable and <span class="arithmatex">\(\mathcal{X}\)</span> is a convex feasible set.</p>
<h2 id="appendices-180_subgradient_methods-f2-subgradients-and-geometry">F.2 Subgradients and Geometry<a class="headerlink" href="#appendices-180_subgradient_methods-f2-subgradients-and-geometry" title="Permanent link">¶</a></h2>
<p>A subgradient <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span> satisfies</p>
<div class="arithmatex">\[
f(y) \ge f(x_t) + \langle g_t,\, y - x_t \rangle, \quad \forall y \in \mathcal{X}.
\]</div>
<ul>
<li>If <span class="arithmatex">\(f\)</span> is differentiable, <span class="arithmatex">\(\partial f(x_t) = \{\nabla f(x_t)\}\)</span>.  </li>
<li>At a nonsmooth point (e.g. <span class="arithmatex">\(|x|\)</span> at <span class="arithmatex">\(x=0\)</span>), <span class="arithmatex">\(\partial f(x_t)\)</span> is a set of supporting slopes.  </li>
<li>Each subgradient defines a supporting hyperplane below the graph of <span class="arithmatex">\(f\)</span>.</li>
</ul>
<p>Hence a subgradient gives a descent direction even when <span class="arithmatex">\(f\)</span> lacks a unique gradient.</p>
<h2 id="appendices-180_subgradient_methods-f3-update-rule-and-projection-view">F.3 Update Rule and Projection View<a class="headerlink" href="#appendices-180_subgradient_methods-f3-update-rule-and-projection-view" title="Permanent link">¶</a></h2>
<p>The projected subgradient step is</p>
<div class="arithmatex">\[
x_{t+1} = \Pi_{\mathcal{X}}\!\big(x_t - \eta_t g_t\big),
\]</div>
<p>where
- <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span>,<br>
- <span class="arithmatex">\(\eta_t&gt;0\)</span> is the step size,<br>
- <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> projects onto <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<p>If <span class="arithmatex">\(\mathcal{X} = \mathbb{R}^n\)</span>, projection disappears:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta_t g_t.
</script>
</p>
<p>Geometric view: move in a subgradient direction, then project back to feasibility.<br>
The method “slides” along the edges of <span class="arithmatex">\(f\)</span>’s epigraph.</p>
<h2 id="appendices-180_subgradient_methods-f4-distance-analysis">F.4 Distance Analysis<a class="headerlink" href="#appendices-180_subgradient_methods-f4-distance-analysis" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x^\star\)</span> be an optimal solution. Expanding the squared distance:</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
= \|x_t - x^\star\|^2
- 2\eta_t\langle g_t, x_t - x^\star\rangle
+ \eta_t^2 \|g_t\|^2.
\]</div>
<p>By convexity,
<script type="math/tex; mode=display">
f(x_t) - f(x^\star) \le \langle g_t, x_t - x^\star\rangle.
</script>
</p>
<p>Substitute to get</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
\le
\|x_t - x^\star\|^2
- 2\eta_t\big(f(x_t)-f(x^\star)\big)
+ \eta_t^2 \|g_t\|^2.
\]</div>
<h2 id="appendices-180_subgradient_methods-f5-bounding-suboptimality">F.5 Bounding Suboptimality<a class="headerlink" href="#appendices-180_subgradient_methods-f5-bounding-suboptimality" title="Permanent link">¶</a></h2>
<p>Rearranging:</p>
<div class="arithmatex">\[
f(x_t)-f(x^\star)
\le
\frac{\|x_t-x^\star\|^2 - \|x_{t+1}-x^\star\|^2}{2\eta_t}
+ \frac{\eta_t}{2}\|g_t\|^2.
\]</div>
<p>This shows a trade-off:</p>
<ul>
<li>Large <span class="arithmatex">\(\eta_t\)</span> → faster steps but higher error term.  </li>
<li>Small <span class="arithmatex">\(\eta_t\)</span> → more precise but slower progress.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f6-convergence-rate">F.6 Convergence Rate<a class="headerlink" href="#appendices-180_subgradient_methods-f6-convergence-rate" title="Permanent link">¶</a></h2>
<p>Assume <span class="arithmatex">\(\|g_t\| \le G\)</span>. Summing over <span class="arithmatex">\(t=0,\dots,T-1\)</span>:</p>
<div class="arithmatex">\[
\sum_{t=0}^{T-1}\!\big(f(x_t)-f(x^\star)\big)
\le
\frac{\|x_0-x^\star\|^2}{2\eta}
+ \frac{\eta G^2 T}{2}.
\]</div>
<p>Define <span class="arithmatex">\(\bar{x}_T = \tfrac{1}{T}\sum_{t=0}^{T-1} x_t\)</span>.<br>
By convexity of <span class="arithmatex">\(f\)</span>,</p>
<div class="arithmatex">\[
f(\bar{x}_T)-f(x^\star)
\le
\frac{\|x_0-x^\star\|^2}{2\eta T}
+ \frac{\eta G^2}{2}.
\]</div>
<p>Choosing <span class="arithmatex">\(\eta_t = \tfrac{R}{G\sqrt{T}}\)</span> with <span class="arithmatex">\(R=\|x_0-x^\star\|\)</span> yields</p>
<p>
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^\star)
\le
\frac{RG}{\sqrt{T}},
</script>
i.e. a sublinear rate <span class="arithmatex">\(O(1/\sqrt{T})\)</span>.</p>
<h2 id="appendices-180_subgradient_methods-f7-interpretation-and-practice">F.7 Interpretation and Practice<a class="headerlink" href="#appendices-180_subgradient_methods-f7-interpretation-and-practice" title="Permanent link">¶</a></h2>
<ul>
<li>Works for any convex function, smooth or not.  </li>
<li>Converges slower than smooth-gradient methods (<span class="arithmatex">\(O(1/T)\)</span> or linear), but applies more generally.  </li>
<li>Step size schedule is crucial:<br>
<span class="arithmatex">\(\eta_t \!\downarrow 0\)</span> for convergence, or fixed <span class="arithmatex">\(\eta\)</span> for steady error.  </li>
<li>Averaging <span class="arithmatex">\(\bar{x}_T\)</span> improves stability.</li>
</ul>
<h3 id="appendices-180_subgradient_methods-typical-ml-uses">Typical ML Uses<a class="headerlink" href="#appendices-180_subgradient_methods-typical-ml-uses" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Objective</th>
<th>Nonsmooth Term</th>
</tr>
</thead>
<tbody>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td><span class="arithmatex">\(\ell_1\)</span> penalty</td>
</tr>
<tr>
<td>SVM</td>
<td><span class="arithmatex">\(\tfrac12\|w\|^2 + C\sum_i \max(0,1-y_i w^\top x_i)\)</span></td>
<td>hinge loss</td>
</tr>
<tr>
<td>Robust regression</td>
<td>$\sum_i</td>
<td>a_i^\top x - b_i</td>
</tr>
<tr>
<td>Neural nets</td>
<td><span class="arithmatex">\(\|w\|_1\)</span> or ReLU activations</td>
<td>piecewise linear</td>
</tr>
</tbody>
</table>
<h2 id="appendices-180_subgradient_methods-f8-beyond-basic-subgradients">F.8 Beyond Basic Subgradients<a class="headerlink" href="#appendices-180_subgradient_methods-f8-beyond-basic-subgradients" title="Permanent link">¶</a></h2>
<p>Many advanced methods refine or accelerate the basic idea:</p>
<ul>
<li>Stochastic subgradients: sample-based updates for large-scale ML.  </li>
<li>Mirror descent: adapt geometry via Bregman divergences.  </li>
<li>Proximal methods: replace step with proximal operator (see Appendix B).  </li>
<li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f9-summary">F.9 Summary<a class="headerlink" href="#appendices-180_subgradient_methods-f9-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Subgradients generalize gradients to nondifferentiable convex functions.  </li>
<li>The projected subgradient method provides a universal, robust minimization algorithm.  </li>
<li>Achieves <span class="arithmatex">\(O(1/\sqrt{T})\)</span> convergence under bounded subgradients.  </li>
<li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9–10.</li>
</ul></body></html></section><section class="print-page" id="appendices-190_proximal" heading-number="7.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization">Appendix G | Projections and Proximal Operators in Constrained Convex Optimization<a class="headerlink" href="#appendices-190_proximal-appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization" title="Permanent link">¶</a></h1>
<p>Many convex optimization problems involve constraints or nonsmooth penalties.<br>
This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>
<h2 id="appendices-190_proximal-g1-problem-setup">G.1 Problem Setup<a class="headerlink" href="#appendices-190_proximal-g1-problem-setup" title="Permanent link">¶</a></h2>
<p>We wish to minimize a convex, differentiable function <span class="arithmatex">\( f(x) \)</span> subject to a convex feasible set <span class="arithmatex">\( \mathcal{X} \subseteq \mathbb{R}^n \)</span>:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x).
\]</div>
<p>A plain gradient step,</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t),
\]</div>
<p>may leave <span class="arithmatex">\( x_{t+1} \notin \mathcal{X} \)</span>.<br>
We fix this by projecting the iterate back into the feasible region.</p>
<h2 id="appendices-190_proximal-g2-projection-operator">G.2 Projection Operator<a class="headerlink" href="#appendices-190_proximal-g2-projection-operator" title="Permanent link">¶</a></h2>
<p>The projection of a point <span class="arithmatex">\(y\)</span> onto a convex set <span class="arithmatex">\(\mathcal{X}\)</span> is</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2.
\]</div>
<p>Hence, the projected gradient descent update is</p>
<div class="arithmatex">\[
x_{t+1} = \text{Proj}_{\mathcal{X}}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<h3 id="appendices-190_proximal-geometric-insight">Geometric Insight<a class="headerlink" href="#appendices-190_proximal-geometric-insight" title="Permanent link">¶</a></h3>
<ul>
<li>Take a descent step possibly outside the feasible set.  </li>
<li>Project back to the closest feasible point.  </li>
<li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li>
</ul>
<p>Example — Euclidean ball:<br>
If <span class="arithmatex">\( \mathcal{X} = \{x : \|x\|_2 \le 1\} \)</span>, then</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y) = \frac{y}{\max(1, \|y\|_2)}.
\]</div>
<ul>
<li>Inside the ball → unchanged.  </li>
<li>Outside → scaled back to the boundary.</li>
</ul>
<hr>
<h2 id="appendices-190_proximal-g3-from-projections-to-proximal-operators">G.3 From Projections to Proximal Operators<a class="headerlink" href="#appendices-190_proximal-g3-from-projections-to-proximal-operators" title="Permanent link">¶</a></h2>
<p>Projections handle explicit constraints, but many problems use implicit penalties — e.g. sparsity (<span class="arithmatex">\(\|x\|_1\)</span>), total variation, or nonnegativity penalties.</p>
<p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>
<h3 id="appendices-190_proximal-definition">Definition<a class="headerlink" href="#appendices-190_proximal-definition" title="Permanent link">¶</a></h3>
<p>For a convex (possibly nondifferentiable) function <span class="arithmatex">\( g(x) \)</span>,</p>
<p>
<script type="math/tex; mode=display">
\text{prox}_{\lambda g}(y)
= \arg\min_x \Big( g(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big),
</script>
where <span class="arithmatex">\( \lambda &gt; 0 \)</span> balances regularization vs. proximity.</p>
<h3 id="appendices-190_proximal-interpretation">Interpretation<a class="headerlink" href="#appendices-190_proximal-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>The quadratic term <span class="arithmatex">\( \tfrac{1}{2\lambda}\|x - y\|^2 \)</span> keeps <span class="arithmatex">\(x\)</span> close to <span class="arithmatex">\(y\)</span>.  </li>
<li>The function <span class="arithmatex">\( g(x) \)</span> encourages structure (sparsity, smoothness, feasibility).  </li>
<li>Small <span class="arithmatex">\(\lambda\)</span>: conservative correction; large <span class="arithmatex">\(\lambda\)</span>: stronger regularization.</li>
</ul>
<p>The proximal step acts as a soft correction after a gradient step.</p>
<hr>
<h2 id="appendices-190_proximal-g4-projection-as-a-special-case">G.4 Projection as a Special Case<a class="headerlink" href="#appendices-190_proximal-g4-projection-as-a-special-case" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<div class="arithmatex">\[
I_{\mathcal{X}}(x) =
\begin{cases}
0, &amp; x \in \mathcal{X}, \\[4pt]
+\infty, &amp; x \notin \mathcal{X}.
\end{cases}
\]</div>
<p>Substitute <span class="arithmatex">\(g(x)=I_{\mathcal{X}}(x)\)</span> into the proximal definition:</p>
<div class="arithmatex">\[
\text{prox}_{\lambda I_{\mathcal{X}}}(y)
= \arg\min_x \Big( I_{\mathcal{X}}(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2
= \text{Proj}_{\mathcal{X}}(y).
\]</div>
<p>✅ Projection is just a proximal operator for an indicator function.</p>
<h2 id="appendices-190_proximal-g5-proximal-gradient-method">G.5 Proximal Gradient Method<a class="headerlink" href="#appendices-190_proximal-g5-proximal-gradient-method" title="Permanent link">¶</a></h2>
<p>When minimizing a composite convex objective
<script type="math/tex; mode=display">
\min_x \; f(x) + g(x),
</script>
where <span class="arithmatex">\(f\)</span> is smooth and <span class="arithmatex">\(g\)</span> convex (possibly nonsmooth), the proximal gradient method updates:</p>
<div class="arithmatex">\[
x_{t+1} = \text{prox}_{\eta g}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<ul>
<li>The gradient step reduces the smooth part <span class="arithmatex">\(f(x)\)</span>.  </li>
<li>The proximal step enforces structure via <span class="arithmatex">\(g(x)\)</span>.<br>
This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li>
</ul>
<h2 id="appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm">G.6 Example: Proximal Operator of the <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm" title="Permanent link">¶</a></h2>
<p>We seek</p>
<div class="arithmatex">\[
\text{prox}_{\lambda \|\cdot\|_1}(y)
= \arg\min_x \left( \lambda\|x\|_1 + \tfrac{1}{2}\|x - y\|^2 \right).
\]</div>
<h3 id="appendices-190_proximal-step-1-coordinate-separation">Step 1. Coordinate Separation<a class="headerlink" href="#appendices-190_proximal-step-1-coordinate-separation" title="Permanent link">¶</a></h3>
<p>The problem is separable across coordinates:
<script type="math/tex; mode=display">
\min_x \sum_i \Big(\lambda |x_i| + \tfrac{1}{2}(x_i - y_i)^2\Big),
</script>
so each coordinate solves
<script type="math/tex; mode=display">
\min_x \phi(x) = \lambda|x| + \tfrac{1}{2}(x - y)^2.
</script>
</p>
<h3 id="appendices-190_proximal-step-2-subgradient-optimality">Step 2. Subgradient Optimality<a class="headerlink" href="#appendices-190_proximal-step-2-subgradient-optimality" title="Permanent link">¶</a></h3>
<p>Optimality condition:
<script type="math/tex; mode=display">
0 \in \partial\phi(x^\star) = \lambda \partial|x^\star| + (x^\star - y).
</script>
Thus,
<script type="math/tex; mode=display">
x^\star = y - \lambda s, \quad s \in \partial |x^\star|.
</script>
</p>
<h3 id="appendices-190_proximal-step-3-case-analysis">Step 3. Case Analysis<a class="headerlink" href="#appendices-190_proximal-step-3-case-analysis" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Case</th>
<th>Condition</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x^\star&gt;0\)</span></td>
<td><span class="arithmatex">\(y&gt;\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y - \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star&lt;0\)</span></td>
<td><span class="arithmatex">\(y&lt;-\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y + \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star=0\)</span></td>
<td>(</td>
<td>y</td>
</tr>
</tbody>
</table>
<h3 id="appendices-190_proximal-step-4-compact-form">Step 4. Compact Form<a class="headerlink" href="#appendices-190_proximal-step-4-compact-form" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\boxed{
\text{prox}_{\lambda|\cdot|}(y)
= \text{sign}(y) \cdot \max(|y| - \lambda,\, 0)
}
\]</div>
<p>This is the soft-thresholding operator.</p>
<h3 id="appendices-190_proximal-step-5-vector-case">Step 5. Vector Case<a class="headerlink" href="#appendices-190_proximal-step-5-vector-case" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(y \in \mathbb{R}^n\)</span>,</p>
<div class="arithmatex">\[
\big(\text{prox}_{\lambda\|\cdot\|_1}(y)\big)_i
= \text{sign}(y_i)\cdot\max(|y_i| - \lambda, 0).
\]</div>
<p>Each coordinate is independently shrunk toward zero — producing sparse solutions.</p>
<h3 id="appendices-190_proximal-step-6-interpretation">Step 6. Interpretation<a class="headerlink" href="#appendices-190_proximal-step-6-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>Coordinates with <span class="arithmatex">\(|y_i| \le \lambda\)</span> → set to zero (promotes sparsity).  </li>
<li>Coordinates with <span class="arithmatex">\(|y_i| &gt; \lambda\)</span> → shrink by <span class="arithmatex">\(\lambda\)</span>.  </li>
<li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li>
</ul>
<h2 id="appendices-190_proximal-g7-geometry-and-connection-to-algorithms">G.7 Geometry and Connection to Algorithms<a class="headerlink" href="#appendices-190_proximal-g7-geometry-and-connection-to-algorithms" title="Permanent link">¶</a></h2>
<ul>
<li>Projection = nearest feasible point → handles <em>hard constraints</em>.  </li>
<li>Proximal operator = nearest structured point → handles <em>soft regularization</em>.  </li>
<li>Proximal gradient = combines both, yielding algorithms like:</li>
<li>ISTA / FISTA (sparse recovery, LASSO),</li>
<li>Projected gradient (feasibility),</li>
<li>ADMM (splitting into subproblems).</li>
</ul>
<p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>
<h2 id="appendices-190_proximal-g8-summary">G.8 Summary<a class="headerlink" href="#appendices-190_proximal-g8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li>
<li>Projection is a special case of the proximal operator for an indicator function.  </li>
<li>Proximal mappings handle nonsmooth regularizers (e.g., <span class="arithmatex">\(\ell_1\)</span>-norm).  </li>
<li>The proximal gradient method unifies constrained and regularized optimization.  </li>
<li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li>
</ul></body></html></section><section class="print-page" id="appendices-200_mirror" heading-number="7.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-h-mirror-descent-and-bregman-geometry">Appendix H: Mirror Descent and Bregman Geometry<a class="headerlink" href="#appendices-200_mirror-appendix-h-mirror-descent-and-bregman-geometry" title="Permanent link">¶</a></h1>
<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry.<br>
In many structured domains—such as probability simplices or sparse models—Euclidean updates can destroy problem structure or cause instability.  </p>
<p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence.<br>
It performs gradient-like updates in a dual space, respecting the <em>intrinsic geometry</em> of the domain.</p>
<h2 id="appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd">H.1 Motivation and Limitations of Euclidean GD<a class="headerlink" href="#appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd" title="Permanent link">¶</a></h2>
<p>Standard GD update:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta \nabla f(x_t)
</script>
assumes Euclidean distance
<script type="math/tex; mode=display">
\|x-y\|_2 = \sqrt{\sum_i (x_i - y_i)^2}.
</script>
</p>
<p>This works well in <span class="arithmatex">\(\mathbb{R}^n\)</span> without structure, but fails to respect constraints or sparsity.</p>
<p>In practice:</p>
<ul>
<li>Many parameters are nonnegative or normalized (probabilities, weights).  </li>
<li>Euclidean steps can violate constraints or zero out coordinates.  </li>
<li>The “flat” <span class="arithmatex">\(\ell_2\)</span> geometry treats all directions equally.</li>
</ul>
<blockquote>
<p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the <em>metric</em> via a mirror map.</p>
</blockquote>
<h2 id="appendices-200_mirror-h2-geometry-in-optimization">H.2 Geometry in Optimization<a class="headerlink" href="#appendices-200_mirror-h2-geometry-in-optimization" title="Permanent link">¶</a></h2>
<p>The “steepest descent” direction depends on the notion of distance.<br>
GD implicitly minimizes a <em>linearized loss</em> plus a Euclidean proximity term.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Natural Constraint</th>
<th>Appropriate Geometry</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability vectors</td>
<td><span class="arithmatex">\(x_i\ge0, \sum_i x_i=1\)</span></td>
<td>KL / entropy geometry</td>
</tr>
<tr>
<td>Sparse models</td>
<td><span class="arithmatex">\(\|x\|_1\)</span>-structured</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
</tr>
<tr>
<td>Online learning</td>
<td>multiplicative updates</td>
<td>log-space geometry</td>
</tr>
</tbody>
</table>
<p>Using Euclidean projections in these domains can cause:</p>
<ul>
<li>abrupt projection onto boundaries,</li>
<li>loss of positivity or sparsity,</li>
<li>geometric inconsistency.</li>
</ul>
<h2 id="appendices-200_mirror-h3-mirror-descent-framework">H.3 Mirror Descent Framework<a class="headerlink" href="#appendices-200_mirror-h3-mirror-descent-framework" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(\psi(x)\)</span> be a mirror map — a strictly convex, differentiable potential encoding the geometry.</p>
<p>Define the dual coordinate:
<script type="math/tex; mode=display">
u = \nabla \psi(x),
</script>
and its inverse mapping through the convex conjugate <span class="arithmatex">\(\psi^*\)</span>:
<script type="math/tex; mode=display">
x = \nabla \psi^*(u).
</script>
</p>
<h3 id="appendices-200_mirror-bregman-divergence">Bregman Divergence<a class="headerlink" href="#appendices-200_mirror-bregman-divergence" title="Permanent link">¶</a></h3>
<p>The geometry is quantified by the Bregman divergence:
<script type="math/tex; mode=display">
D_\psi(x \| y)
= \psi(x) - \psi(y) - \langle \nabla\psi(y), x - y \rangle.
</script>
</p>
<ul>
<li>Measures how nonlinear <span class="arithmatex">\(\psi\)</span> is between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\tfrac12\|x\|_2^2\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes <span class="arithmatex">\(\tfrac12\|x-y\|_2^2\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes KL divergence.</li>
</ul>
<h2 id="appendices-200_mirror-h4-mirror-descent-update-rule">H.4 Mirror Descent Update Rule<a class="headerlink" href="#appendices-200_mirror-h4-mirror-descent-update-rule" title="Permanent link">¶</a></h2>
<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:
<script type="math/tex; mode=display">
x_{t+1}
= \arg\min_{x\in\mathcal{X}}
\Big\{ \langle \nabla f(x_t), x - x_t\rangle
+ \tfrac{1}{\eta} D_\psi(x \| x_t) \Big\}.
</script>
</p>
<p>Equivalent dual-space form:
<script type="math/tex; mode=display">
\begin{aligned}
u_t &= \nabla \psi(x_t),\\
u_{t+1} &= u_t - \eta \nabla f(x_t),\\
x_{t+1} &= \nabla \psi^*(u_{t+1}).
\end{aligned}
</script>
</p>
<p>✅ MD is gradient descent in dual coordinates, where distances are measured by <span class="arithmatex">\(D_\psi\)</span> instead of <span class="arithmatex">\(\|x-y\|_2\)</span>.</p>
<h2 id="appendices-200_mirror-h5-comparing-gd-projected-gd-and-md">H.5 Comparing GD, Projected GD, and MD<a class="headerlink" href="#appendices-200_mirror-h5-comparing-gd-projected-gd-and-md" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Update Rule</th>
<th>Geometry</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(x - \eta\nabla f\)</span></td>
<td>Euclidean</td>
<td>may leave feasible set</td>
</tr>
<tr>
<td>Projected GD</td>
<td><span class="arithmatex">\(\text{Proj}(x - \eta\nabla f)\)</span></td>
<td>Euclidean + projection</td>
<td>can cause discontinuous jumps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td><span class="arithmatex">\(\arg\min_x \langle\nabla f, x - x_t\rangle + \frac{1}{\eta}D_\psi(x\|x_t)\)</span></td>
<td>Bregman</td>
<td>smooth, structure-preserving</td>
</tr>
</tbody>
</table>
<h2 id="appendices-200_mirror-h6-simplex-example-kl-geometry">H.6 Simplex Example (KL Geometry)<a class="headerlink" href="#appendices-200_mirror-h6-simplex-example-kl-geometry" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x\in\Delta^2=\{x\ge0, x_1+x_2=1\}\)</span>, objective <span class="arithmatex">\(f(x)=x_1^2+2x_2\)</span>, <span class="arithmatex">\(\eta=0.3\)</span>.</p>
<h3 id="appendices-200_mirror-euclidean-gd-projection">Euclidean GD + Projection<a class="headerlink" href="#appendices-200_mirror-euclidean-gd-projection" title="Permanent link">¶</a></h3>
<ol>
<li><span class="arithmatex">\(\nabla f=(2x_1,2)=(1,2)\)</span>,  </li>
<li><span class="arithmatex">\(y=x-\eta\nabla f=(0.2,-0.1)\)</span>,  </li>
<li>Project → <span class="arithmatex">\(x_{new}=(1,0)\)</span>.</li>
</ol>
<p>→ Projection kills one coordinate ⇒ lost smoothness.</p>
<h3 id="appendices-200_mirror-mirror-descent-with-negative-entropy">Mirror Descent with Negative Entropy<a class="headerlink" href="#appendices-200_mirror-mirror-descent-with-negative-entropy" title="Permanent link">¶</a></h3>
<p>Mirror map <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>.<br>
Update:
<script type="math/tex; mode=display">
x_i^{new}\propto x_i\exp(-\eta\nabla_i f(x)),
\quad \text{then normalize.}
</script>
Gives <span class="arithmatex">\(x\approx(0.57,0.43)\)</span> — smooth, positive, stays in simplex.</p>
<blockquote>
<p>MD follows the manifold of the simplex naturally—no harsh projection.</p>
</blockquote>
<h2 id="appendices-200_mirror-h7-choosing-the-mirror-map">H.7 Choosing the Mirror Map<a class="headerlink" href="#appendices-200_mirror-h7-choosing-the-mirror-map" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Mirror Map <span class="arithmatex">\(\psi(x)\)</span></th>
<th>Bregman Divergence <span class="arithmatex">\(D_\psi\)</span></th>
<th>Typical Domain / Application</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\tfrac12\|x\|_2^2\)</span></td>
<td>Euclidean distance</td>
<td>unconstrained <span class="arithmatex">\(\mathbb{R}^n\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(\sum_i x_i\log x_i\)</span></td>
<td>KL divergence</td>
<td>simplex, probabilities</td>
</tr>
<tr>
<td><span class="arithmatex">\(\|x\|_1\)</span> or variants</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
<td>sparse models</td>
</tr>
<tr>
<td>log-barrier <span class="arithmatex">\(\sum_i -\log x_i\)</span></td>
<td>barrier divergence</td>
<td>positive orthant</td>
</tr>
</tbody>
</table>
<p>Mirror maps act as design choices defining the optimization geometry.</p>
<h2 id="appendices-200_mirror-h8-practical-remarks">H.8 Practical Remarks<a class="headerlink" href="#appendices-200_mirror-h8-practical-remarks" title="Permanent link">¶</a></h2>
<p>When to prefer Mirror Descent:</p>
<ul>
<li>Structured domains (simplex, positive vectors, sparse spaces)</li>
<li>Smooth, structure-preserving updates desired</li>
<li>Avoiding discontinuous projections</li>
</ul>
<p>Computational notes:</p>
<ul>
<li>Some <span class="arithmatex">\(\psi\)</span> yield closed-form updates (e.g. multiplicative weights).  </li>
<li>Works with adaptive or momentum step-size schemes.  </li>
<li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li>
</ul>
<hr>
<h2 id="appendices-200_mirror-h9-convergence-at-a-glance">H.9 Convergence at a Glance<a class="headerlink" href="#appendices-200_mirror-h9-convergence-at-a-glance" title="Permanent link">¶</a></h2>
<p>For convex <span class="arithmatex">\(f\)</span> with bounded gradients <span class="arithmatex">\(\|\nabla f\|\le G\)</span> and strong convex mirror map <span class="arithmatex">\(\psi\)</span>,
Mirror Descent achieves the same sublinear rate as projected subgradient methods:
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^*)
\le O\!\left(\frac{1}{\sqrt{T}}\right),
</script>
but with improved <em>geometry-adapted</em> constants that exploit curvature of <span class="arithmatex">\(\psi\)</span>.</p></body></html></section><section class="print-page" id="appendices-300_matrixfactorization" heading-number="7.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="numerical-linear-algebra-for-convex-optimization">Numerical Linear Algebra for Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-numerical-linear-algebra-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>Numerical linear algebra is the computational foundation of convex optimization.
Every modern optimization algorithm — from Newton’s method to interior-point or proximal algorithms — ultimately requires solving a structured linear system:
<script type="math/tex; mode=display">
H x = b,
</script>
where <span class="arithmatex">\(H\)</span> may represent a Hessian, a normal equations matrix, or a KKT (Karush–Kuhn–Tucker) system.</p>
<p>In practice, we never compute <span class="arithmatex">\(H^{-1}\)</span> directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>
<h2 id="appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization">1. Why Linear Algebra Matters in Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization" title="Permanent link">¶</a></h2>
<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p>
<ul>
<li>
<p>Newton’s method:
  <script type="math/tex; mode=display">
  \nabla^2 f(x_k), \\ \Delta x = -\nabla f(x_k)
  </script>
</p>
</li>
<li>
<p>Interior-point methods (KKT systems):</p>
</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
H &amp; A^T \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\[3pt] \Delta \lambda
\end{bmatrix}
=
\begin{bmatrix}
-r_d \\[3pt] -r_p
\end{bmatrix}
\]</div>
<ul>
<li>Least-squares problems: <span class="arithmatex">\(A^T A x = A^T b\)</span></li>
</ul>
<p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>
<h2 id="appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox">2. The Matrix Factorization Toolbox<a class="headerlink" href="#appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox" title="Permanent link">¶</a></h2>
<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure.
They enable efficient triangular solves instead of direct inversion.</p>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Applies To</th>
<th>Form</th>
<th>Common Use</th>
<th>Key Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td>Any nonsingular matrix</td>
<td><span class="arithmatex">\(A = L U\)</span></td>
<td>General linear systems</td>
<td>Requires pivoting for stability</td>
</tr>
<tr>
<td>QR</td>
<td>Any (rectangular) matrix</td>
<td><span class="arithmatex">\(A = Q R\)</span></td>
<td>Least-squares</td>
<td>Orthogonal, stable</td>
</tr>
<tr>
<td>Cholesky</td>
<td>Symmetric positive definite</td>
<td><span class="arithmatex">\(A = L L^T\)</span></td>
<td>SPD systems, normal equations</td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td><span class="arithmatex">\(LDL^T\)</span></td>
<td>Symmetric indefinite</td>
<td><span class="arithmatex">\(A = L D L^T\)</span></td>
<td>KKT systems</td>
<td>Handles indefiniteness</td>
</tr>
<tr>
<td>Eigen</td>
<td>Symmetric/Hermitian</td>
<td><span class="arithmatex">\(A = Q \Lambda Q^T\)</span></td>
<td>Curvature, convexity checks</td>
<td>Diagonalizes <span class="arithmatex">\(A\)</span></td>
</tr>
<tr>
<td>SVD</td>
<td>Any matrix</td>
<td><span class="arithmatex">\(A = U \Sigma V^T\)</span></td>
<td>Rank, conditioning, pseudoinverse</td>
<td>Most stable, expensive</td>
</tr>
</tbody>
</table>
<p>Each factorization corresponds to a <em>numerically preferred strategy</em> for certain classes of problems.</p>
<h2 id="appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse">3. LU Factorization — <em>The General-Purpose Workhorse</em><a class="headerlink" href="#appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = P L U
</script>
where <span class="arithmatex">\(P\)</span> is a permutation matrix ensuring stability.</p>
<ul>
<li>Used for: General linear systems, nonsymmetric matrices.</li>
<li>Cost: <span class="arithmatex">\(\approx \tfrac{2}{3}n^3\)</span> (dense).</li>
<li>Stability: Requires partial pivoting (<span class="arithmatex">\(PA=LU\)</span>) to avoid numerical blow-up.</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving KKT systems in linear programming (LP simplex tableau).</li>
<li>Small dense systems with no symmetry or SPD property.</li>
</ul>
<p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or <span class="arithmatex">\(LDL^T\)</span>.</p>
<h2 id="appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable">4. QR Factorization — <em>Orthogonal and Stable</em><a class="headerlink" href="#appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = Q R, \quad Q^T Q = I, \ R \text{ upper triangular.}
</script>
</p>
<ul>
<li>Used for: Least-squares problems
  <script type="math/tex; mode=display">
  \min_x |A x - b|_2^2.
  </script>
  Instead of forming normal equations (<span class="arithmatex">\(A^T A x = A^T b\)</span>), we solve:
  <script type="math/tex; mode=display">
  R x = Q^T b.
  </script>
</li>
<li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Linear regression via least squares.</li>
<li>ADMM and proximal steps with overdetermined systems.</li>
<li>Orthogonal projections in signal processing.</li>
</ul>
<p>Variants:</p>
<ul>
<li>Householder QR: numerically robust, used in LAPACK.</li>
<li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems">5. Cholesky Factorization — <em>Fastest for SPD Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L L^T, \quad L \text{ lower triangular.}
</script>
Applicable when <span class="arithmatex">\(A\)</span> is symmetric positive definite (SPD) — common in convex problems.</p>
<p>Why it’s central:
Convexity ensures <span class="arithmatex">\(A \succeq 0\)</span>.
For strictly convex problems, <span class="arithmatex">\(A \succ 0\)</span> and Cholesky is the most efficient and stable method.</p>
<p>Cost: <span class="arithmatex">\(\tfrac{1}{3}n^3\)</span> operations — half of LU.</p>
<p>Example use cases:</p>
<ul>
<li>Newton’s method on unconstrained convex functions.</li>
<li>Solving normal equations <span class="arithmatex">\(A^T A x = A^T b\)</span>.</li>
<li>QP subproblems and ridge regression.</li>
</ul>
<p>Implementation detail:
No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>
<h2 id="appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems">6. LDLᵀ Factorization — <em>For Indefinite Symmetric Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L D L^T,
</script>
where <span class="arithmatex">\(D\)</span> is block diagonal (1×1 or 2×2 blocks), and <span class="arithmatex">\(L\)</span> is unit lower triangular.</p>
<p>Used when <span class="arithmatex">\(A\)</span> is symmetric but not SPD (e.g., KKT systems).</p>
<p>Example use cases:</p>
<ul>
<li>
<p>Interior-point methods for QPs and SDPs:
  <script type="math/tex; mode=display">
  \begin{bmatrix}
  Q & A^T \\ A & 0
  \end{bmatrix}
  \begin{bmatrix}
  \Delta x \\ \Delta \lambda
  \end{bmatrix} =
  \begin{bmatrix}
  r_1 \\ r_2
  \end{bmatrix}.
  </script>
</p>
</li>
<li>
<p>Equality-constrained least-squares.</p>
</li>
<li>Sparse symmetric indefinite systems in primal-dual algorithms.</li>
</ul>
<p>Algorithmic note:
Uses Bunch–Kaufman pivoting to maintain numerical stability.
In practice, LDLᵀ is used with sparse reordering and partial elimination.</p>
<h2 id="appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement">7. Block Systems and the Schur Complement<a class="headerlink" href="#appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement" title="Permanent link">¶</a></h2>
<p>Many KKT or structured systems naturally appear in block form:
<script type="math/tex; mode=display">
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix} =
\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}.
</script>
</p>
<p>Assuming <span class="arithmatex">\(A_{11}\)</span> is invertible:</p>
<ol>
<li>Eliminate <span class="arithmatex">\(x_1\)</span>:
   <script type="math/tex; mode=display">
   x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)
   </script>
</li>
<li>Substitute into the second block:
   <script type="math/tex; mode=display">
   (A_{22} - A_{21}A_{11}^{-1}A_{12})x_2 = b_2 - A_{21}A_{11}^{-1}b_1
   </script>
</li>
</ol>
<p>The matrix
<script type="math/tex; mode=display">
S = A_{22} - A_{21}A_{11}^{-1}A_{12}
</script>
is the Schur complement of <span class="arithmatex">\(A_{11}\)</span> in <span class="arithmatex">\(A\)</span>.</p>
<h3 id="appendices-300_matrixfactorization-schur-complement-in-optimization">Schur Complement in Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-schur-complement-in-optimization" title="Permanent link">¶</a></h3>
<ul>
<li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li>
<li>Preserves symmetry and often positive definiteness.</li>
<li>Foundation of block elimination and reduced Hessian methods.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Interior-point Newton systems (eliminate <span class="arithmatex">\(\Delta x\)</span> to get a system in <span class="arithmatex">\(\Delta \lambda\)</span>).</li>
<li>Partial elimination in sequential quadratic programming (SQP).</li>
<li>Covariance conditioning and Gaussian marginalization.</li>
</ul>
<p>Numerical caution: Never form <span class="arithmatex">\(A_{11}^{-1}\)</span> explicitly — use triangular solves via Cholesky or LU.</p>
<h2 id="appendices-300_matrixfactorization-8-block-elimination-algorithm">8. Block Elimination Algorithm<a class="headerlink" href="#appendices-300_matrixfactorization-8-block-elimination-algorithm" title="Permanent link">¶</a></h2>
<p>Given a nonsingular <span class="arithmatex">\(A_{11}\)</span>:</p>
<ol>
<li>Compute <span class="arithmatex">\(A_{11}^{-1}A_{12}\)</span> and <span class="arithmatex">\(A_{11}^{-1}b_1\)</span> by solving triangular systems.</li>
<li>Form <span class="arithmatex">\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\)</span>, <span class="arithmatex">\(\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\)</span>.</li>
<li>Solve <span class="arithmatex">\(Sx_2 = \tilde{b}\)</span>.</li>
<li>Recover <span class="arithmatex">\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\)</span>.</li>
</ol>
<p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p>
<p>Example use case:</p>
<ul>
<li>Partitioned least-squares with fixed and variable parameters.</li>
<li>Constrained optimization where some variables can be analytically eliminated.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices">9. Structured Plus Low-Rank Matrices<a class="headerlink" href="#appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices" title="Permanent link">¶</a></h2>
<p>Suppose we need to solve:
<script type="math/tex; mode=display">
(A + BC)x = b,
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(A \in \mathbb{R}^{n \times n}\)</span> is structured or easily invertible (e.g., diagonal or sparse),</li>
<li><span class="arithmatex">\(B \in \mathbb{R}^{n \times p}\)</span>, <span class="arithmatex">\(C \in \mathbb{R}^{p \times n}\)</span> are low rank.</li>
</ul>
<p>This situation arises when updating an existing system with a small modification.</p>
<h3 id="appendices-300_matrixfactorization-block-reformulation">Block Reformulation<a class="headerlink" href="#appendices-300_matrixfactorization-block-reformulation" title="Permanent link">¶</a></h3>
<p>Introduce <span class="arithmatex">\(y = Cx\)</span>, yielding:</p>
<p>$$
<script type="math/tex; mode=display">\begin{bmatrix}
A & B \\ C & -I
\end{bmatrix}</script>
<script type="math/tex; mode=display">\begin{bmatrix}
x \\ y
\end{bmatrix}</script>
=</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}
b \\ 0
\end{bmatrix}</script>.
$$</p>
<p>Block elimination gives:
<script type="math/tex; mode=display">
(I + C A^{-1} B)y = C A^{-1} b,
\quad
x = A^{-1}(b - By).
</script>
</p>
<h3 id="appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity">Matrix Inversion Lemma (Woodbury Identity)<a class="headerlink" href="#appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(A + BC\)</span> are nonsingular:
<script type="math/tex; mode=display">
(A + BC)^{-1} = A^{-1} - A^{-1}B(I + C A^{-1}B)^{-1}C A^{-1}.
</script>
</p>
<p>Example use cases:</p>
<ul>
<li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li>
<li>Ridge regression / kernel methods: low-rank updates to <span class="arithmatex">\((X^T X + \lambda I)^{-1}\)</span>.</li>
<li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li>
</ul>
<p>Numerical note: Avoid explicit inversion; use solves with <span class="arithmatex">\(A\)</span> and small dense matrices.</p>
<h2 id="appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity">10. Conditioning, Stability, and Sparsity<a class="headerlink" href="#appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-conditioning">Conditioning<a class="headerlink" href="#appendices-300_matrixfactorization-conditioning" title="Permanent link">¶</a></h3>
<ul>
<li>Condition number: <span class="arithmatex">\(\kappa(A) = |A||A^{-1}|\)</span> measures sensitivity to perturbations.</li>
<li>High <span class="arithmatex">\(\kappa(A)\)</span> ⇒ round-off errors amplified ⇒ ill-conditioning.</li>
<li>Regularization (adding <span class="arithmatex">\(\lambda I\)</span>) improves numerical stability.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-stability">Stability<a class="headerlink" href="#appendices-300_matrixfactorization-stability" title="Permanent link">¶</a></h3>
<ul>
<li>Orthogonal transformations (QR, SVD) are backward stable.</li>
<li>LU needs partial pivoting.</li>
<li>LDLᵀ needs symmetric pivoting (Bunch–Kaufman).</li>
<li>Cholesky is stable for SPD matrices.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-sparsity-and-fill-in">Sparsity and Fill-In<a class="headerlink" href="#appendices-300_matrixfactorization-sparsity-and-fill-in" title="Permanent link">¶</a></h3>
<ul>
<li>Large convex solvers exploit sparse Cholesky / LDLᵀ.</li>
<li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li>
<li>Symbolic factorization determines the pattern before numeric factorization.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning">11. Iterative Solvers and Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning" title="Permanent link">¶</a></h2>
<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>
<h3 id="appendices-300_matrixfactorization-common-iterative-methods">Common Iterative Methods<a class="headerlink" href="#appendices-300_matrixfactorization-common-iterative-methods" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>For</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>CG</td>
<td>SPD systems</td>
<td>Uses matrix–vector products; converges in ≤ n steps</td>
</tr>
<tr>
<td>MINRES / SYMMLQ</td>
<td>Symmetric indefinite</td>
<td>Handles KKT and saddle-point systems</td>
</tr>
<tr>
<td>GMRES / BiCGSTAB</td>
<td>Nonsymmetric</td>
<td>General-purpose Krylov solvers</td>
</tr>
</tbody>
</table>
<h3 id="appendices-300_matrixfactorization-preconditioning">Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-preconditioning" title="Permanent link">¶</a></h3>
<p>Preconditioners <span class="arithmatex">\(M \approx A^{-1}\)</span> improve convergence:</p>
<ul>
<li>Jacobi (diagonal): <span class="arithmatex">\(M = \text{diag}(A)^{-1}\)</span></li>
<li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li>
<li>Block preconditioners: use Schur complement approximations for KKT systems</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li>
<li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions">12. Eigenvalue and SVD Decompositions<a class="headerlink" href="#appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-eigenvalue-decomposition">Eigenvalue Decomposition<a class="headerlink" href="#appendices-300_matrixfactorization-eigenvalue-decomposition" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^T, \quad Q^T Q = I.
</script>
Reveals curvature, stability, and definiteness:</p>
<ul>
<li>Convexity ⇔ <span class="arithmatex">\(\Lambda \ge 0\)</span>.</li>
<li>Used in semidefinite programming (SDP) and spectral analysis.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#appendices-300_matrixfactorization-singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = U \Sigma V^T,
</script>
with <span class="arithmatex">\(\Sigma = \text{diag}(\sigma_i) \ge 0\)</span>.</p>
<p>Applications:</p>
<ul>
<li>Rank and condition number estimation (<span class="arithmatex">\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span>).</li>
<li>Low-rank approximation (<span class="arithmatex">\(A_k = U_k \Sigma_k V_k^T\)</span>).</li>
<li>Pseudoinverse: <span class="arithmatex">\(A^+ = V \Sigma^{-1} U^T\)</span>.</li>
<li>Convex relaxations: nuclear-norm minimization (matrix completion).</li>
</ul>
<h2 id="appendices-300_matrixfactorization-13-computational-complexity-summary">13. Computational Complexity Summary<a class="headerlink" href="#appendices-300_matrixfactorization-13-computational-complexity-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Dense Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td><span class="arithmatex">\(\frac{2}{3}n^3\)</span></td>
<td>Needs pivoting</td>
</tr>
<tr>
<td>Cholesky</td>
<td><span class="arithmatex">\(\frac{1}{3}n^3\)</span></td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td>QR</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>Stable, more memory</td>
</tr>
<tr>
<td>LDLᵀ</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>For indefinite</td>
</tr>
<tr>
<td>SVD</td>
<td><span class="arithmatex">\(\approx \frac{4}{3}n^3\)</span></td>
<td>Most accurate</td>
</tr>
<tr>
<td>CG / MINRES</td>
<td>Variable</td>
<td>Depends on condition number and preconditioning</td>
</tr>
</tbody>
</table>
<p>Sparse systems reduce cost to roughly <span class="arithmatex">\(O(n^{1.5})\)</span>–<span class="arithmatex">\(O(n^2)\)</span> depending on fill-in.</p>
<h2 id="appendices-300_matrixfactorization-14-example-applications-overview">14. Example Applications Overview<a class="headerlink" href="#appendices-300_matrixfactorization-14-example-applications-overview" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Matrix</th>
<th>Solver / Factorization</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unconstrained Newton step</td>
<td>SPD Hessian</td>
<td>Cholesky</td>
<td>Convex quadratic, ridge regression</td>
</tr>
<tr>
<td>Equality-constrained QP</td>
<td>Symmetric indefinite KKT</td>
<td>LDLᵀ</td>
<td>Interior-point QP solver</td>
</tr>
<tr>
<td>Overdetermined LS</td>
<td>Rectangular <span class="arithmatex">\(A\)</span></td>
<td>QR</td>
<td>Linear regression, ADMM subproblem</td>
</tr>
<tr>
<td>KKT block system</td>
<td>Block-symmetric</td>
<td>Schur complement</td>
<td>Primal-dual method</td>
</tr>
<tr>
<td>Low-rank correction</td>
<td><span class="arithmatex">\(A + U U^T\)</span></td>
<td>Woodbury</td>
<td>Kalman filter, online update</td>
</tr>
<tr>
<td>Rank-deficient system</td>
<td>Any</td>
<td>SVD</td>
<td>Matrix completion, regularization</td>
</tr>
<tr>
<td>Large-scale Hessian</td>
<td>SPD</td>
<td>CG + preconditioner</td>
<td>Logistic regression, large ML models</td>
</tr>
</tbody>
</table></body></html></section></section></div><style>.print-site-enumerate-headings #index > h1:before { content: '1 ' }

                .print-site-enumerate-headings #index h2:before { content: '1.' counter(counter-index-2) ' ' }
                .print-site-enumerate-headings #index h2 {  counter-reset: counter-index-3 ;  counter-increment: counter-index-2 }
            
                .print-site-enumerate-headings #index h3:before { content: '1.' counter(counter-index-2) '.' counter(counter-index-3) ' ' }
                .print-site-enumerate-headings #index h3 {  counter-increment: counter-index-3 }
            
.print-site-enumerate-headings #section-2 > h1:before { content: '2 ' }
.print-site-enumerate-headings #convex-11_intro > h1:before { content: '2.1 ' }

                .print-site-enumerate-headings #convex-11_intro h2:before { content: '2.1.' counter(counter-convex-11_intro-2) ' ' }
                .print-site-enumerate-headings #convex-11_intro h2 {  counter-increment: counter-convex-11_intro-2 }
            
.print-site-enumerate-headings #convex-12_vector > h1:before { content: '2.2 ' }

                .print-site-enumerate-headings #convex-12_vector h2:before { content: '2.2.' counter(counter-convex-12_vector-2) ' ' }
                .print-site-enumerate-headings #convex-12_vector h2 {  counter-increment: counter-convex-12_vector-2 }
            
.print-site-enumerate-headings #convex-13_calculus > h1:before { content: '2.3 ' }

                .print-site-enumerate-headings #convex-13_calculus h2:before { content: '2.3.' counter(counter-convex-13_calculus-2) ' ' }
                .print-site-enumerate-headings #convex-13_calculus h2 {  counter-increment: counter-convex-13_calculus-2 }
            
.print-site-enumerate-headings #convex-14_convexsets > h1:before { content: '2.4 ' }

                .print-site-enumerate-headings #convex-14_convexsets h2:before { content: '2.4.' counter(counter-convex-14_convexsets-2) ' ' }
                .print-site-enumerate-headings #convex-14_convexsets h2 {  counter-increment: counter-convex-14_convexsets-2 }
            
.print-site-enumerate-headings #convex-15_convexfunctions > h1:before { content: '2.5 ' }

                .print-site-enumerate-headings #convex-15_convexfunctions h2:before { content: '2.5.' counter(counter-convex-15_convexfunctions-2) ' ' }
                .print-site-enumerate-headings #convex-15_convexfunctions h2 {  counter-increment: counter-convex-15_convexfunctions-2 }
            
.print-site-enumerate-headings #convex-16_subgradients > h1:before { content: '2.6 ' }

                .print-site-enumerate-headings #convex-16_subgradients h2:before { content: '2.6.' counter(counter-convex-16_subgradients-2) ' ' }
                .print-site-enumerate-headings #convex-16_subgradients h2 {  counter-increment: counter-convex-16_subgradients-2 }
            
.print-site-enumerate-headings #convex-16a_optimality_conditions > h1:before { content: '2.7 ' }

                .print-site-enumerate-headings #convex-16a_optimality_conditions h2:before { content: '2.7.' counter(counter-convex-16a_optimality_conditions-2) ' ' }
                .print-site-enumerate-headings #convex-16a_optimality_conditions h2 {  counter-increment: counter-convex-16a_optimality_conditions-2 }
            
.print-site-enumerate-headings #convex-17_kkt > h1:before { content: '2.8 ' }

                .print-site-enumerate-headings #convex-17_kkt h2:before { content: '2.8.' counter(counter-convex-17_kkt-2) ' ' }
                .print-site-enumerate-headings #convex-17_kkt h2 {  counter-increment: counter-convex-17_kkt-2 }
            
.print-site-enumerate-headings #convex-18_duality > h1:before { content: '2.9 ' }

                .print-site-enumerate-headings #convex-18_duality h2:before { content: '2.9.' counter(counter-convex-18_duality-2) ' ' }
                .print-site-enumerate-headings #convex-18_duality h2 {  counter-increment: counter-convex-18_duality-2 }
            
.print-site-enumerate-headings #convex-18a_pareto > h1:before { content: '2.10 ' }

                .print-site-enumerate-headings #convex-18a_pareto h2:before { content: '2.10.' counter(counter-convex-18a_pareto-2) ' ' }
                .print-site-enumerate-headings #convex-18a_pareto h2 {  counter-increment: counter-convex-18a_pareto-2 }
            
.print-site-enumerate-headings #convex-18b_regularization > h1:before { content: '2.11 ' }

                .print-site-enumerate-headings #convex-18b_regularization h2:before { content: '2.11.' counter(counter-convex-18b_regularization-2) ' ' }
                .print-site-enumerate-headings #convex-18b_regularization h2 {  counter-increment: counter-convex-18b_regularization-2 }
            
.print-site-enumerate-headings #convex-19_optimizationalgo > h1:before { content: '2.12 ' }

                .print-site-enumerate-headings #convex-19_optimizationalgo h2:before { content: '2.12.' counter(counter-convex-19_optimizationalgo-2) ' ' }
                .print-site-enumerate-headings #convex-19_optimizationalgo h2 {  counter-increment: counter-convex-19_optimizationalgo-2 }
            
.print-site-enumerate-headings #convex-19a_optimization_constraints > h1:before { content: '2.13 ' }

                .print-site-enumerate-headings #convex-19a_optimization_constraints h2:before { content: '2.13.' counter(counter-convex-19a_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19a_optimization_constraints h2 {  counter-increment: counter-convex-19a_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-19b_optimization_constraints > h1:before { content: '2.14 ' }

                .print-site-enumerate-headings #convex-19b_optimization_constraints h2:before { content: '2.14.' counter(counter-convex-19b_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19b_optimization_constraints h2 {  counter-increment: counter-convex-19b_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-20_advanced > h1:before { content: '2.15 ' }

                .print-site-enumerate-headings #convex-20_advanced h2:before { content: '2.15.' counter(counter-convex-20_advanced-2) ' ' }
                .print-site-enumerate-headings #convex-20_advanced h2 {  counter-increment: counter-convex-20_advanced-2 }
            
.print-site-enumerate-headings #convex-21_models > h1:before { content: '2.16 ' }

                .print-site-enumerate-headings #convex-21_models h2:before { content: '2.16.' counter(counter-convex-21_models-2) ' ' }
                .print-site-enumerate-headings #convex-21_models h2 {  counter-increment: counter-convex-21_models-2 }
            
.print-site-enumerate-headings #convex-30_canonical_problems > h1:before { content: '2.17 ' }

                .print-site-enumerate-headings #convex-30_canonical_problems h2:before { content: '2.17.' counter(counter-convex-30_canonical_problems-2) ' ' }
                .print-site-enumerate-headings #convex-30_canonical_problems h2 {  counter-increment: counter-convex-30_canonical_problems-2 }
            
.print-site-enumerate-headings #convex-35_modern > h1:before { content: '2.18 ' }

                .print-site-enumerate-headings #convex-35_modern h2:before { content: '2.18.' counter(counter-convex-35_modern-2) ' ' }
                .print-site-enumerate-headings #convex-35_modern h2 {  counter-increment: counter-convex-35_modern-2 }
            
.print-site-enumerate-headings #convex-40_nonconvex > h1:before { content: '2.19 ' }

                .print-site-enumerate-headings #convex-40_nonconvex h2:before { content: '2.19.' counter(counter-convex-40_nonconvex-2) ' ' }
                .print-site-enumerate-headings #convex-40_nonconvex h2 {  counter-increment: counter-convex-40_nonconvex-2 }
            
.print-site-enumerate-headings #convex-42_derivativefree > h1:before { content: '2.20 ' }

                .print-site-enumerate-headings #convex-42_derivativefree h2:before { content: '2.20.' counter(counter-convex-42_derivativefree-2) ' ' }
                .print-site-enumerate-headings #convex-42_derivativefree h2 {  counter-increment: counter-convex-42_derivativefree-2 }
            
.print-site-enumerate-headings #convex-44_metaheuristic > h1:before { content: '2.21 ' }

                .print-site-enumerate-headings #convex-44_metaheuristic h2:before { content: '2.21.' counter(counter-convex-44_metaheuristic-2) ' ' }
                .print-site-enumerate-headings #convex-44_metaheuristic h2 {  counter-increment: counter-convex-44_metaheuristic-2 }
            
.print-site-enumerate-headings #convex-48_advanced_combinatorial > h1:before { content: '2.22 ' }

                .print-site-enumerate-headings #convex-48_advanced_combinatorial h2:before { content: '2.22.' counter(counter-convex-48_advanced_combinatorial-2) ' ' }
                .print-site-enumerate-headings #convex-48_advanced_combinatorial h2 {  counter-increment: counter-convex-48_advanced_combinatorial-2 }
            
.print-site-enumerate-headings #convex-50_future > h1:before { content: '2.23 ' }

                .print-site-enumerate-headings #convex-50_future h2:before { content: '2.23.' counter(counter-convex-50_future-2) ' ' }
                .print-site-enumerate-headings #convex-50_future h2 {  counter-increment: counter-convex-50_future-2 }
            
.print-site-enumerate-headings #section-3 > h1:before { content: '3 ' }
.print-site-enumerate-headings #reinforcement-1_intro > h1:before { content: '3.1 ' }

                .print-site-enumerate-headings #reinforcement-1_intro h2:before { content: '3.1.' counter(counter-reinforcement-1_intro-2) ' ' }
                .print-site-enumerate-headings #reinforcement-1_intro h2 {  counter-increment: counter-reinforcement-1_intro-2 }
            
.print-site-enumerate-headings #reinforcement-2_mdp > h1:before { content: '3.2 ' }

                .print-site-enumerate-headings #reinforcement-2_mdp h2:before { content: '3.2.' counter(counter-reinforcement-2_mdp-2) ' ' }
                .print-site-enumerate-headings #reinforcement-2_mdp h2 {  counter-increment: counter-reinforcement-2_mdp-2 }
            
.print-site-enumerate-headings #reinforcement-3_modelfree > h1:before { content: '3.3 ' }

                .print-site-enumerate-headings #reinforcement-3_modelfree h2:before { content: '3.3.' counter(counter-reinforcement-3_modelfree-2) ' ' }
                .print-site-enumerate-headings #reinforcement-3_modelfree h2 {  counter-increment: counter-reinforcement-3_modelfree-2 }
            
.print-site-enumerate-headings #reinforcement-4_model_free_control > h1:before { content: '3.4 ' }

                .print-site-enumerate-headings #reinforcement-4_model_free_control h2:before { content: '3.4.' counter(counter-reinforcement-4_model_free_control-2) ' ' }
                .print-site-enumerate-headings #reinforcement-4_model_free_control h2 {  counter-increment: counter-reinforcement-4_model_free_control-2 }
            
.print-site-enumerate-headings #reinforcement-5_policy_gradient > h1:before { content: '3.5 ' }

                .print-site-enumerate-headings #reinforcement-5_policy_gradient h2:before { content: '3.5.' counter(counter-reinforcement-5_policy_gradient-2) ' ' }
                .print-site-enumerate-headings #reinforcement-5_policy_gradient h2 {  counter-increment: counter-reinforcement-5_policy_gradient-2 }
            
.print-site-enumerate-headings #reinforcement-6_pg2 > h1:before { content: '3.6 ' }

                .print-site-enumerate-headings #reinforcement-6_pg2 h2:before { content: '3.6.' counter(counter-reinforcement-6_pg2-2) ' ' }
                .print-site-enumerate-headings #reinforcement-6_pg2 h2 {  counter-increment: counter-reinforcement-6_pg2-2 }
            
.print-site-enumerate-headings #reinforcement-7_gae > h1:before { content: '3.7 ' }

                .print-site-enumerate-headings #reinforcement-7_gae h2:before { content: '3.7.' counter(counter-reinforcement-7_gae-2) ' ' }
                .print-site-enumerate-headings #reinforcement-7_gae h2 {  counter-increment: counter-reinforcement-7_gae-2 }
            
.print-site-enumerate-headings #reinforcement-8_imitation_learning > h1:before { content: '3.8 ' }

                .print-site-enumerate-headings #reinforcement-8_imitation_learning h2:before { content: '3.8.' counter(counter-reinforcement-8_imitation_learning-2) ' ' }
                .print-site-enumerate-headings #reinforcement-8_imitation_learning h2 {  counter-increment: counter-reinforcement-8_imitation_learning-2 }
            
.print-site-enumerate-headings #reinforcement-9_rlhf > h1:before { content: '3.9 ' }

                .print-site-enumerate-headings #reinforcement-9_rlhf h2:before { content: '3.9.' counter(counter-reinforcement-9_rlhf-2) ' ' }
                .print-site-enumerate-headings #reinforcement-9_rlhf h2 {  counter-increment: counter-reinforcement-9_rlhf-2 }
            
.print-site-enumerate-headings #reinforcement-10_offline_rl > h1:before { content: '3.10 ' }

                .print-site-enumerate-headings #reinforcement-10_offline_rl h2:before { content: '3.10.' counter(counter-reinforcement-10_offline_rl-2) ' ' }
                .print-site-enumerate-headings #reinforcement-10_offline_rl h2 {  counter-increment: counter-reinforcement-10_offline_rl-2 }
            
.print-site-enumerate-headings #reinforcement-11_fast_rl > h1:before { content: '3.11 ' }

                .print-site-enumerate-headings #reinforcement-11_fast_rl h2:before { content: '3.11.' counter(counter-reinforcement-11_fast_rl-2) ' ' }
                .print-site-enumerate-headings #reinforcement-11_fast_rl h2 {  counter-increment: counter-reinforcement-11_fast_rl-2 }
            
.print-site-enumerate-headings #reinforcement-12_fast_mdps > h1:before { content: '3.12 ' }

                .print-site-enumerate-headings #reinforcement-12_fast_mdps h2:before { content: '3.12.' counter(counter-reinforcement-12_fast_mdps-2) ' ' }
                .print-site-enumerate-headings #reinforcement-12_fast_mdps h2 {  counter-increment: counter-reinforcement-12_fast_mdps-2 }
            
.print-site-enumerate-headings #reinforcement-13_montecarlo > h1:before { content: '3.13 ' }

                .print-site-enumerate-headings #reinforcement-13_montecarlo h2:before { content: '3.13.' counter(counter-reinforcement-13_montecarlo-2) ' ' }
                .print-site-enumerate-headings #reinforcement-13_montecarlo h2 {  counter-increment: counter-reinforcement-13_montecarlo-2 }
            
.print-site-enumerate-headings #reinforcement-14_final > h1:before { content: '3.14 ' }

                .print-site-enumerate-headings #reinforcement-14_final h2:before { content: '3.14.' counter(counter-reinforcement-14_final-2) ' ' }
                .print-site-enumerate-headings #reinforcement-14_final h2 {  counter-increment: counter-reinforcement-14_final-2 }
            
.print-site-enumerate-headings #section-4 > h1:before { content: '4 ' }
.print-site-enumerate-headings #deeplearning-1_mlp > h1:before { content: '4.1 ' }

                .print-site-enumerate-headings #deeplearning-1_mlp h2:before { content: '4.1.' counter(counter-deeplearning-1_mlp-2) ' ' }
                .print-site-enumerate-headings #deeplearning-1_mlp h2 {  counter-increment: counter-deeplearning-1_mlp-2 }
            
.print-site-enumerate-headings #deeplearning-2_convnets > h1:before { content: '4.2 ' }

                .print-site-enumerate-headings #deeplearning-2_convnets h2:before { content: '4.2.' counter(counter-deeplearning-2_convnets-2) ' ' }
                .print-site-enumerate-headings #deeplearning-2_convnets h2 {  counter-increment: counter-deeplearning-2_convnets-2 }
            
.print-site-enumerate-headings #deeplearning-3_sequence_data > h1:before { content: '4.3 ' }

                .print-site-enumerate-headings #deeplearning-3_sequence_data h2:before { content: '4.3.' counter(counter-deeplearning-3_sequence_data-2) ' ' }
                .print-site-enumerate-headings #deeplearning-3_sequence_data h2 {  counter-increment: counter-deeplearning-3_sequence_data-2 }
            
.print-site-enumerate-headings #deeplearning-4_nlp > h1:before { content: '4.4 ' }

                .print-site-enumerate-headings #deeplearning-4_nlp h2:before { content: '4.4.' counter(counter-deeplearning-4_nlp-2) ' ' }
                .print-site-enumerate-headings #deeplearning-4_nlp h2 {  counter-increment: counter-deeplearning-4_nlp-2 }
            
.print-site-enumerate-headings #deeplearning-5_attention > h1:before { content: '4.5 ' }

                .print-site-enumerate-headings #deeplearning-5_attention h2:before { content: '4.5.' counter(counter-deeplearning-5_attention-2) ' ' }
                .print-site-enumerate-headings #deeplearning-5_attention h2 {  counter-increment: counter-deeplearning-5_attention-2 }
            
.print-site-enumerate-headings #deeplearning-6_gans > h1:before { content: '4.6 ' }

                .print-site-enumerate-headings #deeplearning-6_gans h2:before { content: '4.6.' counter(counter-deeplearning-6_gans-2) ' ' }
                .print-site-enumerate-headings #deeplearning-6_gans h2 {  counter-increment: counter-deeplearning-6_gans-2 }
            
.print-site-enumerate-headings #deeplearning-7_unsuper > h1:before { content: '4.7 ' }

                .print-site-enumerate-headings #deeplearning-7_unsuper h2:before { content: '4.7.' counter(counter-deeplearning-7_unsuper-2) ' ' }
                .print-site-enumerate-headings #deeplearning-7_unsuper h2 {  counter-increment: counter-deeplearning-7_unsuper-2 }
            
.print-site-enumerate-headings #deeplearning-8_latentvariables > h1:before { content: '4.8 ' }

                .print-site-enumerate-headings #deeplearning-8_latentvariables h2:before { content: '4.8.' counter(counter-deeplearning-8_latentvariables-2) ' ' }
                .print-site-enumerate-headings #deeplearning-8_latentvariables h2 {  counter-increment: counter-deeplearning-8_latentvariables-2 }
            
.print-site-enumerate-headings #section-5 > h1:before { content: '5 ' }
.print-site-enumerate-headings #informationtheory-1_intro_to_infotheory > h1:before { content: '5.1 ' }

                .print-site-enumerate-headings #informationtheory-1_intro_to_infotheory h2:before { content: '5.1.' counter(counter-informationtheory-1_intro_to_infotheory-2) ' ' }
                .print-site-enumerate-headings #informationtheory-1_intro_to_infotheory h2 {  counter-increment: counter-informationtheory-1_intro_to_infotheory-2 }
            
.print-site-enumerate-headings #informationtheory-2_entropy > h1:before { content: '5.2 ' }

                .print-site-enumerate-headings #informationtheory-2_entropy h2:before { content: '5.2.' counter(counter-informationtheory-2_entropy-2) ' ' }
                .print-site-enumerate-headings #informationtheory-2_entropy h2 {  counter-increment: counter-informationtheory-2_entropy-2 }
            
.print-site-enumerate-headings #informationtheory-3_kl > h1:before { content: '5.3 ' }

                .print-site-enumerate-headings #informationtheory-3_kl h2:before { content: '5.3.' counter(counter-informationtheory-3_kl-2) ' ' }
                .print-site-enumerate-headings #informationtheory-3_kl h2 {  counter-increment: counter-informationtheory-3_kl-2 }
            
.print-site-enumerate-headings #informationtheory-4_bayes > h1:before { content: '5.4 ' }

                .print-site-enumerate-headings #informationtheory-4_bayes h2:before { content: '5.4.' counter(counter-informationtheory-4_bayes-2) ' ' }
                .print-site-enumerate-headings #informationtheory-4_bayes h2 {  counter-increment: counter-informationtheory-4_bayes-2 }
            
.print-site-enumerate-headings #informationtheory-5_mc_intro > h1:before { content: '5.5 ' }

                .print-site-enumerate-headings #informationtheory-5_mc_intro h2:before { content: '5.5.' counter(counter-informationtheory-5_mc_intro-2) ' ' }
                .print-site-enumerate-headings #informationtheory-5_mc_intro h2 {  counter-increment: counter-informationtheory-5_mc_intro-2 }
            
.print-site-enumerate-headings #informationtheory-6_mc > h1:before { content: '5.6 ' }

                .print-site-enumerate-headings #informationtheory-6_mc h2:before { content: '5.6.' counter(counter-informationtheory-6_mc-2) ' ' }
                .print-site-enumerate-headings #informationtheory-6_mc h2 {  counter-increment: counter-informationtheory-6_mc-2 }
            
.print-site-enumerate-headings #informationtheory-7a_vi_intro > h1:before { content: '5.7 ' }

                .print-site-enumerate-headings #informationtheory-7a_vi_intro h2:before { content: '5.7.' counter(counter-informationtheory-7a_vi_intro-2) ' ' }
                .print-site-enumerate-headings #informationtheory-7a_vi_intro h2 {  counter-increment: counter-informationtheory-7a_vi_intro-2 }
            
.print-site-enumerate-headings #informationtheory-7b_vi > h1:before { content: '5.8 ' }

                .print-site-enumerate-headings #informationtheory-7b_vi h2:before { content: '5.8.' counter(counter-informationtheory-7b_vi-2) ' ' }
                .print-site-enumerate-headings #informationtheory-7b_vi h2 {  counter-increment: counter-informationtheory-7b_vi-2 }
            
.print-site-enumerate-headings #section-6 > h1:before { content: '6 ' }
.print-site-enumerate-headings #cheatsheets-20a_cheatsheet > h1:before { content: '6.1 ' }

                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2:before { content: '6.1.' counter(counter-cheatsheets-20a_cheatsheet-2) ' ' }
                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2 {  counter-increment: counter-cheatsheets-20a_cheatsheet-2 }
            
.print-site-enumerate-headings #section-7 > h1:before { content: '7 ' }
.print-site-enumerate-headings #appendices-120_ineqaulities > h1:before { content: '7.1 ' }

                .print-site-enumerate-headings #appendices-120_ineqaulities h2:before { content: '7.1.' counter(counter-appendices-120_ineqaulities-2) ' ' }
                .print-site-enumerate-headings #appendices-120_ineqaulities h2 {  counter-increment: counter-appendices-120_ineqaulities-2 }
            
.print-site-enumerate-headings #appendices-130_projections > h1:before { content: '7.2 ' }

                .print-site-enumerate-headings #appendices-130_projections h2:before { content: '7.2.' counter(counter-appendices-130_projections-2) ' ' }
                .print-site-enumerate-headings #appendices-130_projections h2 {  counter-increment: counter-appendices-130_projections-2 }
            
.print-site-enumerate-headings #appendices-140_support > h1:before { content: '7.3 ' }

                .print-site-enumerate-headings #appendices-140_support h2:before { content: '7.3.' counter(counter-appendices-140_support-2) ' ' }
                .print-site-enumerate-headings #appendices-140_support h2 {  counter-increment: counter-appendices-140_support-2 }
            
.print-site-enumerate-headings #appendices-160_conjugates > h1:before { content: '7.4 ' }

                .print-site-enumerate-headings #appendices-160_conjugates h2:before { content: '7.4.' counter(counter-appendices-160_conjugates-2) ' ' }
                .print-site-enumerate-headings #appendices-160_conjugates h2 {  counter-increment: counter-appendices-160_conjugates-2 }
            
.print-site-enumerate-headings #appendices-170_probability > h1:before { content: '7.5 ' }

                .print-site-enumerate-headings #appendices-170_probability h2:before { content: '7.5.' counter(counter-appendices-170_probability-2) ' ' }
                .print-site-enumerate-headings #appendices-170_probability h2 {  counter-increment: counter-appendices-170_probability-2 }
            
.print-site-enumerate-headings #appendices-180_subgradient_methods > h1:before { content: '7.6 ' }

                .print-site-enumerate-headings #appendices-180_subgradient_methods h2:before { content: '7.6.' counter(counter-appendices-180_subgradient_methods-2) ' ' }
                .print-site-enumerate-headings #appendices-180_subgradient_methods h2 {  counter-increment: counter-appendices-180_subgradient_methods-2 }
            
.print-site-enumerate-headings #appendices-190_proximal > h1:before { content: '7.7 ' }

                .print-site-enumerate-headings #appendices-190_proximal h2:before { content: '7.7.' counter(counter-appendices-190_proximal-2) ' ' }
                .print-site-enumerate-headings #appendices-190_proximal h2 {  counter-increment: counter-appendices-190_proximal-2 }
            
.print-site-enumerate-headings #appendices-200_mirror > h1:before { content: '7.8 ' }

                .print-site-enumerate-headings #appendices-200_mirror h2:before { content: '7.8.' counter(counter-appendices-200_mirror-2) ' ' }
                .print-site-enumerate-headings #appendices-200_mirror h2 {  counter-increment: counter-appendices-200_mirror-2 }
            
.print-site-enumerate-headings #appendices-300_matrixfactorization > h1:before { content: '7.9 ' }

                .print-site-enumerate-headings #appendices-300_matrixfactorization h2:before { content: '7.9.' counter(counter-appendices-300_matrixfactorization-2) ' ' }
                .print-site-enumerate-headings #appendices-300_matrixfactorization h2 {  counter-increment: counter-appendices-300_matrixfactorization-2 }
            </style>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "/convex_optimization/", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>