
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Print Site - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("/convex_optimization/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#index" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Site
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16a_optimality_conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. First-Order Optimality Conditions in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18a_pareto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Pareto Optimality and Multi-Objective Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18b_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Regularized Approximation – Balancing Fit and Complexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19a_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Optimization Algorithms for Equality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19b_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Optimization Algorithms for Inequality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/30_canonical_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Canonical Problems in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization Beyond Convexity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Optimization Beyond Convexity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nonconvex/41_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nonconvex/42_meta/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Metaheuristic Optimization Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nonconvex/43_hybrid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Hybrid and Modern Optimization Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/1_mlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/2_convnets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Convolutional Neural Networks (CNNs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/3_sequence_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Sequence Data and Recurrent Neural Networks (RNNs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/4_nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Natural Language Processing (NLP) with Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/5_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Transformers and Attention Mechanisms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/6_gans/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Generative Models and GANs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/7_unsuper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Unsupervised and Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/8_latentvariables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Latent Variable Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deeplearning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Optimization Algorithms for Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Regularization Techniques in Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/1_intro.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Advanced Topics in Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Distributed Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Distributed Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distributedsystems/0_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distributedsystems/1_mapreduce/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. MapReduce
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distributedsystems/2_threads/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Threads and RPC
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Information Theory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Information Theory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/1_intro_to_infotheory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/2_entropy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Entropy, Self-Information & Cross-Entropy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/3_KL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Kullback-Leibler Divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/4_bayes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Bayesian Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/5_mc_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Probability toolbox
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/6_mc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Monte Carlo Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/7a_vi_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization-Based Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/7b_vi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Variatonal Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informationtheory/8_representation.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Representation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Cheat Sheets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Cheat Sheets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheatsheets/20a_cheatsheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algos - Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Appendices
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Appendices
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/160_conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix D - Convex Conjugates and Fenchel Duality (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/170_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix E - Convexity in Probability and Statistics (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/180_subgradient_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix F - Subgradient Method and Variants (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/190_proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix G - Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/200_mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix H - Mirror Descent and Bregman Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/300_matrixfactorization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix I - Matrix Factorization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Convex Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Convex Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convex-11_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction and Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-12_vector" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Linear Algebra Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-13_calculus" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Multivariable Calculus for Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-14_convexsets" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Convex Sets and Geometric Fundamentals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-15_convexfunctions" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Convex Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16_subgradients" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Nonsmooth Convex Optimization – Subgradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16a_optimality_conditions" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. First-Order Optimality Conditions in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-17_kkt" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Optimization Principles – From Gradient Descent to KKT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18_duality" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Lagrange Duality Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18a_pareto" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Pareto Optimality and Multi-Objective Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18b_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Regularized Approximation – Balancing Fit and Complexity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19_optimizationalgo" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Algorithms for Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19a_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Optimization Algorithms for Equality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19b_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Optimization Algorithms for Inequality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-20_advanced" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Advanced Large-Scale and Structured Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-21_models" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Modelling Patterns and Algorithm Selection in Practice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-30_canonical_problems" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Canonical Problems in Convex Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-3" class="md-nav__link">
    <span class="md-ellipsis">
      3 Optimization Beyond Convexity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 Optimization Beyond Convexity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nonconvex-41_intro" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 1. Introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonconvex-42_meta" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 2. Metaheuristic Optimization Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonconvex-43_hybrid" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 3. Hybrid and Modern Optimization Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-4" class="md-nav__link">
    <span class="md-ellipsis">
      4 Deep Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Deep Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deeplearning-1_mlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 1. Introduction to Deep Learning Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-2_convnets" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 2. Convolutional Neural Networks (CNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-3_sequence_data" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 3. Sequence Data and Recurrent Neural Networks (RNNs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-4_nlp" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 4. Natural Language Processing (NLP) with Deep Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-5_attention" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 5. Transformers and Attention Mechanisms
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-6_gans" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 6. Generative Models and GANs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-7_unsuper" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 7. Unsupervised and Self-Supervised Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplearning-8_latentvariables" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 8. Latent Variable Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-5" class="md-nav__link">
    <span class="md-ellipsis">
      5 Distributed Systems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 Distributed Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#distributedsystems-0_intro" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedsystems-1_mapreduce" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 1. MapReduce
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedsystems-2_threads" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 2. Threads and RPC
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-6" class="md-nav__link">
    <span class="md-ellipsis">
      6 Information Theory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 Information Theory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#informationtheory-1_intro_to_infotheory" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 1. Introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-2_entropy" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 2. Entropy, Self-Information & Cross-Entropy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-3_kl" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 3. Kullback-Leibler Divergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-4_bayes" class="md-nav__link">
    <span class="md-ellipsis">
      6.4 4. Bayesian Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-5_mc_intro" class="md-nav__link">
    <span class="md-ellipsis">
      6.5 5. Probability toolbox
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-6_mc" class="md-nav__link">
    <span class="md-ellipsis">
      6.6 6. Monte Carlo Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7a_vi_intro" class="md-nav__link">
    <span class="md-ellipsis">
      6.7 8. Optimization-Based Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#informationtheory-7b_vi" class="md-nav__link">
    <span class="md-ellipsis">
      6.8 7. Variatonal Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-7" class="md-nav__link">
    <span class="md-ellipsis">
      7 Cheat Sheets
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7 Cheat Sheets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cheatsheets-20a_cheatsheet" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 Optimization Algos - Cheat Sheet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-8" class="md-nav__link">
    <span class="md-ellipsis">
      8 Appendices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8 Appendices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendices-120_ineqaulities" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Appendix A - Common Inequalities and Identities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-130_projections" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Appendix B - Projection and Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-140_support" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Appendix C - Support Functions and Dual Geometry (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-160_conjugates" class="md-nav__link">
    <span class="md-ellipsis">
      8.4 Appendix D - Convex Conjugates and Fenchel Duality (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-170_probability" class="md-nav__link">
    <span class="md-ellipsis">
      8.5 Appendix E - Convexity in Probability and Statistics (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-180_subgradient_methods" class="md-nav__link">
    <span class="md-ellipsis">
      8.6 Appendix F - Subgradient Method and Variants (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-190_proximal" class="md-nav__link">
    <span class="md-ellipsis">
      8.7 Appendix G - Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-200_mirror" class="md-nav__link">
    <span class="md-ellipsis">
      8.8 Appendix H - Mirror Descent and Bregman Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-300_matrixfactorization" class="md-nav__link">
    <span class="md-ellipsis">
      8.9 Appendix I - Matrix Factorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index" heading-number="1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="mathematics-for-machine-learning">Mathematics for Machine Learning<a class="headerlink" href="#index-mathematics-for-machine-learning" title="Permanent link">¶</a></h1>
<p>Welcome to <em>Mathematics for Machine Learning</em>, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p>
<p>This digital book provides a unified, intuition-driven exploration of key mathematical tools — from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>
<h2 id="index-motivation">Motivation<a class="headerlink" href="#index-motivation" title="Permanent link">¶</a></h2>
<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p>
<ul>
<li>Concise derivations of essential results</li>
<li>Geometric intuition and figures where helpful</li>
<li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li>
<li>Appendices that extend into more advanced or specialized topics</li>
</ul>
<p>Whether you’re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p></body></html></section>
                    <section class='print-page md-section' id='section-2' heading-number='2'>
                        <h1>Convex Optimization<a class='headerlink' href='#section-2' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="convex-11_intro" heading-number="2.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-and-overview">Chapter 1  Introduction and Overview<a class="headerlink" href="#convex-11_intro-chapter-1-introduction-and-overview" title="Permanent link">¶</a></h1>
<p>Optimization is the mathematical foundation of nearly all modern machine learning, signal processing, and control systems.<br>
Every learning algorithm — from linear regression to deep neural networks — is ultimately an optimization procedure: it adjusts model parameters to minimize a loss or maximize a performance criterion based on observed data.</p>
<p>Convex optimization is a special and profoundly important subset of optimization.<br>
It provides structure, guarantees, and tractability that general nonlinear optimization often lacks.<br>
When the objective and constraints are convex, we obtain three fundamental advantages:</p>
<ol>
<li>
<p>Global optimality:<br>
   Any local minimum is also a global minimum — eliminating the risk of getting trapped in suboptimal solutions.</p>
</li>
<li>
<p>Algorithmic stability and efficiency: 
   Convex problems admit well-understood convergence behavior and can be solved reliably by gradient, Newton, or interior-point methods.</p>
</li>
<li>
<p>Theoretical guarantees and interpretability:
   Duality theory and KKT (Karush-Kuhn-Tucker) conditions provide verifiable optimality certificates and often lend economic or geometric meaning to solutions.</p>
</li>
</ol>
<p>These properties make convex optimization the “language of guarantees” in machine learning.<br>
While deep learning and other modern methods are largely nonconvex, many of their building blocks — such as linear models, regularizers, and convex losses — originate from convex analysis.<br>
Understanding convex optimization equips us with the principles that ensure robustness, efficiency, and insight across all areas of data-driven modeling.</p>
<blockquote>
<p>Convexity ⇒ Robustness.<br>
Convex problems are stable: small perturbations to inputs cause proportionally small shifts in the solution.<br>
Many difficult non-convex problems are attacked by constructing convex relaxations, whose solutions yield bounds or high-quality approximations.</p>
</blockquote>
<p>This web-book is written for ML practitioners who want to understand <em>why</em> convex optimization works,  and <em>how</em> to use its geometry, duality, and algorithms to build and tune models in practice.</p>
<h2 id="convex-11_intro-11-motivation-optimization-in-machine-learning">1.1 Motivation: Optimization in Machine Learning<a class="headerlink" href="#convex-11_intro-11-motivation-optimization-in-machine-learning" title="Permanent link">¶</a></h2>
<p>Most supervised learning problems can be viewed as minimizing a regularized empirical risk:</p>
<div class="arithmatex">\[
\min_x \; \frac{1}{N}\sum_{i=1}^{N} \ell(a_i^\top x, b_i) + \lambda R(x)
\quad \text{s.t. } x \in \mathcal{X}.
\]</div>
<p>Here:</p>
<ul>
<li><span class="arithmatex">\(\ell(\cdot,\cdot)\)</span> is a loss function measuring fit to data,  </li>
<li><span class="arithmatex">\(R(x)\)</span> is a regularizer controlling complexity or promoting structure,  </li>
<li><span class="arithmatex">\(\mathcal{X}\)</span> encodes simple constraints (box, simplex, or norm ball).</li>
</ul>
<p>Many of these objectives — least squares, logistic loss, hinge loss, <span class="arithmatex">\(\ell_1\)</span> or <span class="arithmatex">\(\ell_2\)</span> regularizers — are convex. That convexity is what makes them <em>reliably solvable</em> at scale.</p>
<blockquote>
<p>Key idea: Convex optimization is the quiet engine under the hood of most practical ML methods, even in settings that appear nonconvex.</p>
</blockquote>
<h2 id="convex-11_intro-12-convex-sets-and-convex-functions-first-intuition">1.2 Convex Sets and Convex Functions — First Intuition<a class="headerlink" href="#convex-11_intro-12-convex-sets-and-convex-functions-first-intuition" title="Permanent link">¶</a></h2>
<p>A set <span class="arithmatex">\(\mathcal{C}\subseteq\mathbb{R}^n\)</span> is convex if for all <span class="arithmatex">\(x,y\in\mathcal{C}\)</span> and any <span class="arithmatex">\(\theta\in[0,1]\)</span>,
<script type="math/tex; mode=display">
\theta x + (1-\theta)y \in \mathcal{C}.
</script>
This means the line segment joining any two points in <span class="arithmatex">\(\mathcal{C}\)</span> stays inside <span class="arithmatex">\(\mathcal{C}\)</span>.</p>
<p>A function <span class="arithmatex">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> is convex if its epigraph is a convex set,<br>
or equivalently if for all <span class="arithmatex">\(x,y\)</span> and <span class="arithmatex">\(\theta\in[0,1]\)</span>,
<script type="math/tex; mode=display">
f(\theta x + (1-\theta)y)
\le \theta f(x) + (1-\theta) f(y).
</script>
</p>
<p>Intuitively, the graph of <span class="arithmatex">\(f\)</span> lies below the chord connecting any two points — it curves upward but never downward.</p>
<blockquote>
<p>Clarification: Affine functions (linear + constant) are both convex and concave.<br>
They define flat surfaces — neither bowl-shaped nor peaked.</p>
</blockquote>
<h2 id="convex-11_intro-13-why-convex-optimization-still-matters-in-ml">1.3 Why Convex Optimization Still Matters in ML<a class="headerlink" href="#convex-11_intro-13-why-convex-optimization-still-matters-in-ml" title="Permanent link">¶</a></h2>
<p>Convex optimization remains vital in ML for three reasons:</p>
<ol>
<li>
<p>Convex surrogates<br>
   Losses such as logistic, hinge, or Huber are convex approximations to difficult nonconvex objectives (like 0–1 loss). They make training tractable while preserving predictive performance.</p>
</li>
<li>
<p>Convex subproblems in nonconvex training<br>
   Even deep learning routinely solves convex inner loops: least-squares layers, proximal updates, line searches, or trust-region substeps.</p>
</li>
<li>
<p>Implicit bias and geometry<br>
   Gradient descent on convex models (e.g., least squares) naturally converges to the <em>minimum-norm</em> solution — a property used to analyze implicit regularization in overparameterized regimes.</p>
</li>
</ol>
<p>Convex optimization provides both the tools and the theory for understanding why first-order methods generalize and converge.</p>
<h2 id="convex-11_intro-14-from-global-optima-to-algorithms">1.4 From Global Optima to Algorithms<a class="headerlink" href="#convex-11_intro-14-from-global-optima-to-algorithms" title="Permanent link">¶</a></h2>
<p>Convexity eliminates local traps. For a differentiable convex <span class="arithmatex">\(f\)</span> on a convex domain <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<div class="arithmatex">\[
\nabla f(x^\star) = 0 \;\Rightarrow\; x^\star \text{ is a global minimizer.}
\]</div>
<p>There are no local minima or saddle points distinct from the global solution.  For nondifferentiable convex <span class="arithmatex">\(f\)</span>, the same holds with subgradients:
<span class="arithmatex">\(0\in\partial f(x^\star)\)</span>.</p>
<blockquote>
<p>Practical meaning: You can trust gradient-based methods to find the best possible solution —<br>
not just a good one — if the problem is convex.</p>
</blockquote>
<h2 id="convex-11_intro-15-canonical-convex-ml-problems-at-a-glance">1.5 Canonical Convex ML Problems at a Glance<a class="headerlink" href="#convex-11_intro-15-canonical-convex-ml-problems-at-a-glance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Objective</th>
<th>Typical Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td>Least squares</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Gradient descent, CG</td>
</tr>
<tr>
<td>Ridge regression</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_2^2\)</span></td>
<td>Closed form / GD</td>
</tr>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td>Prox-gradient (ISTA/FISTA)</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\sum_i \log(1+\exp(-y_i a_i^\top x)) + \lambda\|x\|_2^2\)</span></td>
<td>Newton, SGD</td>
</tr>
<tr>
<td>SVM (hinge loss)</td>
<td><span class="arithmatex">\(\tfrac{1}{2}\|x\|^2 + C\sum_i \max(0,1-y_i a_i^\top x)\)</span></td>
<td>Subgradient, SMO</td>
</tr>
<tr>
<td>Robust regression</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Linear programming</td>
</tr>
<tr>
<td>Elastic Net</td>
<td><span class="arithmatex">\(\|A x-b\|_2^2+\lambda_1\|x\|_1+\lambda_2\|x\|_2^2\)</span></td>
<td>Coordinate descent</td>
</tr>
</tbody>
</table>
<p>These patterns appear repeatedly in later chapters and unify much of convex ML.</p>
<h2 id="convex-11_intro-16-web-book-roadmap-and-how-to-use-it">1.6 Web-Book Roadmap and How to Use It<a class="headerlink" href="#convex-11_intro-16-web-book-roadmap-and-how-to-use-it" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Question</th>
<th>Where to Look</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>What makes a function or set convex?</td>
<td>Ch. 2 – 5</td>
<td>Geometry &amp; calculus of convexity</td>
</tr>
<tr>
<td>How do gradients, subgradients, and KKT conditions certify optimality?</td>
<td>Ch. 6 – 9</td>
<td>Optimality &amp; duality</td>
</tr>
<tr>
<td>How are convex problems actually solved?</td>
<td>Ch. 10 – 14</td>
<td>First-order, second-order, interior-point methods</td>
</tr>
<tr>
<td>How do I pick a solver for my ML model?</td>
<td>Ch. 15 – 17</td>
<td>Large-scale, structured, and modeling patterns</td>
</tr>
</tbody>
</table>
<h2 id="convex-11_intro-17-next-steps">1.7 Next Steps<a class="headerlink" href="#convex-11_intro-17-next-steps" title="Permanent link">¶</a></h2>
<p>In the next chapters we move from intuition to structure:</p>
<blockquote>
<p>Geometry first (convex sets, functions),<br>
then calculus (gradients, subgradients),<br>
then optimality and algorithms.</p>
</blockquote>
<p>This geometric foundation will make every later algorithm — gradient descent, Newton, or interior-point — feel natural and inevitable.</p></body></html></section><section class="print-page" id="convex-12_vector" heading-number="2.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-linear-algebra-foundations">Chapter 2: Linear Algebra Foundations<a class="headerlink" href="#convex-12_vector-chapter-2-linear-algebra-foundations" title="Permanent link">¶</a></h1>
<p>Linear algebra is the geometry of optimization. A simple motivating example: fitting a linear model <span class="arithmatex">\(x\)</span> to data <span class="arithmatex">\((A,b)\)</span> solves:</p>
<div class="arithmatex">\[
\min_x \ \|A x - b\|_2^2.
\]</div>
<p>We will see that this is a projection of <span class="arithmatex">\(b\)</span> onto the column space of <span class="arithmatex">\(A\)</span>, and that ideas like rank, nullspace, orthogonality, and conditioning directly control optimization algorithms and convergence.</p>
<p>This chapter builds the geometric toolkit used throughout convex optimization and machine learning.</p>
<h2 id="convex-12_vector-21-vector-spaces-subspaces-and-affine-sets">2.1 Vector spaces, subspaces, and affine sets<a class="headerlink" href="#convex-12_vector-21-vector-spaces-subspaces-and-affine-sets" title="Permanent link">¶</a></h2>
<p>A vector space over <span class="arithmatex">\(\mathbb{R}\)</span> is a set <span class="arithmatex">\(V\)</span> equipped with addition and scalar multiplication satisfying the usual axioms: closure, associativity, commutativity of addition, distributivity, existence of an additive identity and additive inverses, and compatibility with scalar multiplication.</p>
<p>A subspace of a vector space <span class="arithmatex">\(V\)</span> is a subset <span class="arithmatex">\(S \subseteq V\)</span> that<br>
1. contains the zero vector,<br>
2. is closed under addition, and<br>
3. is closed under scalar multiplication.  </p>
<h3 id="convex-12_vector-affine-sets">Affine Sets<a class="headerlink" href="#convex-12_vector-affine-sets" title="Permanent link">¶</a></h3>
<p>A set <span class="arithmatex">\(A \subseteq V\)</span> is affine if for any <span class="arithmatex">\(x, y \in A\)</span> and any <span class="arithmatex">\(\theta \in \mathbb{R}\)</span>,<br>
<script type="math/tex; mode=display">
\theta x + (1 - \theta) y \in A.
</script>
</p>
<p>That is, the entire line passing through any two points in <span class="arithmatex">\(A\)</span> lies within <span class="arithmatex">\(A\)</span>.</p>
<blockquote>
<p>By contrast, a convex set only requires this property for <span class="arithmatex">\(\theta \in [0,1]\)</span>, meaning only the line segment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> must lie within the set.</p>
</blockquote>
<p>Every affine set can be written as<br>
<script type="math/tex; mode=display">
A = x_0 + S = \{\, x_0 + s : s \in S \,\},
</script>
where <span class="arithmatex">\(S\)</span> is a subspace of <span class="arithmatex">\(V\)</span>. Affine sets arise as the solution sets to linear equality constraints <span class="arithmatex">\(A x = b\)</span>.</p>
<h3 id="convex-12_vector-affine-transformations">Affine Transformations<a class="headerlink" href="#convex-12_vector-affine-transformations" title="Permanent link">¶</a></h3>
<p>An affine transformation (or affine map) is a function <span class="arithmatex">\(f : V \to W\)</span> that can be written as<br>
<script type="math/tex; mode=display">
f(x) = A x + b,
</script>
where <span class="arithmatex">\(A\)</span> is a linear map and <span class="arithmatex">\(b\)</span> is a fixed vector.<br>
Affine transformations preserve both affinity and convexity:
if <span class="arithmatex">\(C\)</span> is convex, then <span class="arithmatex">\(A C + b\)</span> is also convex.</p>
<h2 id="convex-12_vector-22-linear-combinations-span-basis-dimension">2.2 Linear combinations, span, basis, dimension<a class="headerlink" href="#convex-12_vector-22-linear-combinations-span-basis-dimension" title="Permanent link">¶</a></h2>
<p>Given vectors <span class="arithmatex">\(v_1,\dots,v_k\)</span>, any vector of the form
<script type="math/tex; mode=display">
\alpha_1 v_1 + \cdots + \alpha_k v_k
</script>
is a linear combination. The set of all linear combinations is called the span:
<script type="math/tex; mode=display">
\mathrm{span}\{v_1,\dots,v_k\} = \left\{ \sum_{i=1}^k \alpha_i v_i : \alpha_i \in \mathbb{R} \right\}.
</script>
</p>
<p>A set of vectors is linearly independent if no vector can be written as a combination of others.  A basis of a space <span class="arithmatex">\(V\)</span> is a linearly independent set whose span equals <span class="arithmatex">\(V\)</span>. The number of basis vectors is the dimension <span class="arithmatex">\(\dim(V)\)</span>.</p>
<p>Rank and nullity facts:</p>
<ul>
<li>The column space of <span class="arithmatex">\(A\)</span> is the span of its columns. Its dimension is <span class="arithmatex">\(\mathrm{rank}(A)\)</span>.</li>
<li>The nullspace of <span class="arithmatex">\(A\)</span> is <span class="arithmatex">\(\{ x : Ax = 0 \}\)</span>.</li>
<li>The rank-nullity theorem states:
<script type="math/tex; mode=display">
\mathrm{rank}(A) + \mathrm{nullity}(A) = n,
</script>
where <span class="arithmatex">\(n\)</span> is the number of columns of <span class="arithmatex">\(A\)</span>.</li>
</ul>
<blockquote>
<h4 id="convex-12_vector-column-space">Column Space:<a class="headerlink" href="#convex-12_vector-column-space" title="Permanent link">¶</a></h4>
<p>The column space of a matrix <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( C(A) \)</span>, is the set of all possible output vectors <span class="arithmatex">\( b \)</span> that can be written as <span class="arithmatex">\( Ax \)</span> for some <span class="arithmatex">\( x \)</span>. In other words, it contains all vectors that the matrix can “reach” through linear combinations of its columns. The question “Does the system <span class="arithmatex">\( Ax = b \)</span> have a solution?” is equivalent to asking whether <span class="arithmatex">\( b \in C(A) \)</span>. If <span class="arithmatex">\( b \)</span> lies in the column space, a solution exists; otherwise, it does not.</p>
<h4 id="convex-12_vector-null-space">Null Space:<a class="headerlink" href="#convex-12_vector-null-space" title="Permanent link">¶</a></h4>
<p>The null space (or kernel) of <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( N(A) \)</span>, is the set of all input vectors <span class="arithmatex">\( x \)</span> that are mapped to zero:  <span class="arithmatex">\( N(A) = \{ x : Ax = 0 \} \)</span>.<br>
It answers a different question: <em>If a solution to <span class="arithmatex">\( Ax = b \)</span> exists, is it unique?</em><br>
If the null space contains only the zero vector (<span class="arithmatex">\( \mathrm{nullity}(A) = 0 \)</span>), the solution is unique. But if <span class="arithmatex">\( N(A) \)</span> contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p>
<h4 id="convex-12_vector-multicollinearity">Multicollinearity:<a class="headerlink" href="#convex-12_vector-multicollinearity" title="Permanent link">¶</a></h4>
<p>When one feature in the data matrix <span class="arithmatex">\( A \)</span> is a linear combination of others—for example, <span class="arithmatex">\( \text{feature}_3 = 2 \times \text{feature}_1 + \text{feature}_2 \)</span>—the columns of <span class="arithmatex">\( A \)</span> become linearly dependent. This creates a nonzero vector in the null space of <span class="arithmatex">\( A \)</span>, meaning multiple weight vectors <span class="arithmatex">\( x \)</span> can produce the same predictions. The model is then <em>unidentifiable</em> (Underdetermined – the number of unknowns (parameters) exceeds the number of independent equations (information)), and <span class="arithmatex">\( A^\top A \)</span> becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p>
<blockquote>
<p>Regularization introduces an additional constraint or penalty that selects a <em>single, stable</em> solution from among the infinite possibilities.</p>
<ul>
<li>
<p>Ridge regression (L2 regularization) adds a penalty on the norm of <span class="arithmatex">\(x\)</span>:
  <script type="math/tex; mode=display">
  \min_x \|A x - b\|_2^2 + \lambda \|x\|_2^2,
  </script>
  which modifies the normal equations to
  <script type="math/tex; mode=display">
  (A^\top A + \lambda I)x = A^\top b.
  </script>
  The added term <span class="arithmatex">\(\lambda I\)</span> ensures invertibility and numerical stability.</p>
</li>
<li>
<p>Lasso regression (L1 regularization) instead penalizes <span class="arithmatex">\(\|x\|_1\)</span>, promoting sparsity by driving some coefficients exactly to zero.</p>
</li>
</ul>
<p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution—favoring smaller or sparser coefficient vectors—and making the regression problem well-posed even when <span class="arithmatex">\(A\)</span> is rank-deficient.</p>
</blockquote>
<h4 id="convex-12_vector-feasible-directions">Feasible Directions:<a class="headerlink" href="#convex-12_vector-feasible-directions" title="Permanent link">¶</a></h4>
<p>In a constrained optimization problem of the form <span class="arithmatex">\( Ax = b \)</span>, the null space of <span class="arithmatex">\( A \)</span> characterizes the directions along which one can move without violating the constraints. If <span class="arithmatex">\( d \in N(A) \)</span>, then moving from a feasible point <span class="arithmatex">\( x \)</span> to <span class="arithmatex">\( x + d \)</span> preserves feasibility, since  <span class="arithmatex">\( A(x + d) = Ax + Ad = b + 0 = b \)</span>. Thus, the null space defines the <em>space of free movement</em>—directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p>
<h4 id="convex-12_vector-row-space">Row Space:<a class="headerlink" href="#convex-12_vector-row-space" title="Permanent link">¶</a></h4>
<p>The row space of <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( R(A) \)</span>, is the span of the rows of <span class="arithmatex">\( A \)</span> (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to <span class="arithmatex">\( \mathrm{rank}(A) \)</span>. The row space is orthogonal to the null space of <span class="arithmatex">\( A \)</span>:  <span class="arithmatex">\( R(A) \perp N(A) \)</span>.<br>
In optimization, the row space corresponds to the set of active constraints or the directions along which changes in <span class="arithmatex">\( x \)</span> affect the constraints.</p>
<h4 id="convex-12_vector-left-null-space">Left Null Space:<a class="headerlink" href="#convex-12_vector-left-null-space" title="Permanent link">¶</a></h4>
<p>The left null space, denoted <span class="arithmatex">\( N(A^\top) \)</span>, is the set of all vectors <span class="arithmatex">\( y \)</span> such that <span class="arithmatex">\( A^\top y = 0 \)</span>. These vectors are orthogonal to the columns of <span class="arithmatex">\( A \)</span>, and therefore orthogonal to the column space itself. In least squares problems, <span class="arithmatex">\( N(A^\top) \)</span> represents residual directions—components of <span class="arithmatex">\( b \)</span> that cannot be explained by the model <span class="arithmatex">\( Ax = b \)</span>.</p>
<h4 id="convex-12_vector-projection-interpretation-least-squares">Projection Interpretation (Least Squares):<a class="headerlink" href="#convex-12_vector-projection-interpretation-least-squares" title="Permanent link">¶</a></h4>
<p>When <span class="arithmatex">\( Ax = b \)</span> has no exact solution (as in overdetermined systems), the least squares solution finds <span class="arithmatex">\( x \)</span> such that <span class="arithmatex">\( Ax \)</span> is the projection of <span class="arithmatex">\( b \)</span> onto the column space of <span class="arithmatex">\( A \)</span>:<br>
<script type="math/tex; mode=display">
x^* = (A^\top A)^{-1} A^\top b,
</script>
and the residual<br>
<script type="math/tex; mode=display">
r = b - Ax^*
</script>
lies in the left null space <span class="arithmatex">\( N(A^\top) \)</span>.<br>
This provides a geometric view: the solution projects <span class="arithmatex">\( b \)</span> onto the closest point in the subspace that <span class="arithmatex">\( A \)</span> can reach.</p>
<h4 id="convex-12_vector-ranknullity-relationship">Rank–Nullity Relationship:<a class="headerlink" href="#convex-12_vector-ranknullity-relationship" title="Permanent link">¶</a></h4>
<p>The rank of <span class="arithmatex">\( A \)</span> is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank–Nullity Theorem:
<script type="math/tex; mode=display">
\mathrm{rank}(A) + \mathrm{nullity}(A) = n,
</script>
where <span class="arithmatex">\( n \)</span> is the number of columns of <span class="arithmatex">\( A \)</span>.<br>
This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in <span class="arithmatex">\( x \)</span>.</p>
<p>Geometric Interpretation:  </p>
<ul>
<li>The column space represents all <em>reachable outputs</em>.  </li>
<li>The null space represents all <em>indistinguishable inputs</em> that map to zero.  </li>
<li>The row space represents all <em>independent constraints</em> imposed by <span class="arithmatex">\( A \)</span>.  </li>
<li>The left null space captures <em>inconsistencies</em> or residual directions that cannot be explained by the model.  </li>
</ul>
<p>Together, these four subspaces define the complete geometry of the linear map <span class="arithmatex">\( A: \mathbb{R}^n \to \mathbb{R}^m \)</span>.</p>
</blockquote>
<h2 id="convex-12_vector-23-inner-products-and-orthogonality">2.3 Inner products and orthogonality<a class="headerlink" href="#convex-12_vector-23-inner-products-and-orthogonality" title="Permanent link">¶</a></h2>
<p>An inner product on <span class="arithmatex">\(\mathbb{R}^n\)</span> is a map <span class="arithmatex">\(\langle \cdot,\cdot\rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\)</span> such that for all <span class="arithmatex">\(x,y,z\)</span> and all scalars <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\langle x,y \rangle = \langle y,x\rangle\)</span> (symmetry),</li>
<li><span class="arithmatex">\(\langle x+y,z \rangle = \langle x,z \rangle + \langle y,z\rangle\)</span> (linearity in first argument),</li>
<li><span class="arithmatex">\(\langle \alpha x, y\rangle = \alpha \langle x, y\rangle\)</span>,</li>
<li><span class="arithmatex">\(\langle x, x\rangle \ge 0\)</span> with equality iff <span class="arithmatex">\(x=0\)</span> (positive definiteness).</li>
</ol>
<p>The inner product induces:</p>
<ul>
<li>length (norm): <span class="arithmatex">\(\|x\|_2 = \sqrt{\langle x,x\rangle}\)</span>,</li>
<li>angle: <script type="math/tex">
\cos \theta = \frac{\langle x,y\rangle}{\|x\|\|y\|}~.
</script>
</li>
</ul>
<p>Two vectors are orthogonal if <span class="arithmatex">\(\langle x,y\rangle = 0\)</span>. A set of vectors <span class="arithmatex">\(\{v_i\}\)</span> is orthonormal if each <span class="arithmatex">\(\|v_i\| = 1\)</span> and <span class="arithmatex">\(\langle v_i, v_j\rangle = 0\)</span> for <span class="arithmatex">\(i\ne j\)</span>.</p>
<blockquote>
<p>More generally, an inner product endows <span class="arithmatex">\(V\)</span> with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p>
<p>Geometry from the inner product: An inner product induces a norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x \rangle}\)</span> and a notion of distance <span class="arithmatex">\(d(x,y) = \|x-y\|\)</span>. It also defines angles: <span class="arithmatex">\(\langle x,y \rangle = 0\)</span> means <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: <span class="arithmatex">\(\|x+y\|^2 + \|x-y\|^2 = 2\|x\|^2 + 2\|y\|^2\)</span>.  </p>
</blockquote>
<p>The Cauchy–Schwarz inequality: For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>:
<script type="math/tex; mode=display">
|\langle x,y\rangle| \le \|x\|\|y\|~,
</script>
with equality iff <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent Geometrically, it means the absolute inner product is maximized when <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> point in the same or opposite direction. </p>
<p>Examples of inner products:</p>
<ul>
<li>
<p>Standard (Euclidean) inner product: <span class="arithmatex">\(\langle x,y\rangle = x^\top y = \sum_i x_i y_i\)</span>. This underlies most optimization algorithms on <span class="arithmatex">\(\mathbb{R}^n\)</span>, where <span class="arithmatex">\(\nabla f(x)\)</span> is defined via this inner product (so that <span class="arithmatex">\(\langle \nabla f(x), h\rangle\)</span> gives the directional derivative in direction <span class="arithmatex">\(h\)</span>).  </p>
</li>
<li>
<p>Weighted inner product: <span class="arithmatex">\(\langle x,y\rangle_W = x^\top W y\)</span> for some symmetric positive-definite matrix <span class="arithmatex">\(W\)</span>. Here <span class="arithmatex">\(\|x\|_W = \sqrt{x^\top W x}\)</span> is a weighted length. Such inner products appear in preconditioning: by choosing <span class="arithmatex">\(W\)</span> cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses <span class="arithmatex">\(W = \Sigma^{-1}\)</span> for covariance <span class="arithmatex">\(\Sigma\)</span>).  </p>
</li>
<li>
<p>Function space inner product: <span class="arithmatex">\(\langle f, g \rangle = \int_a^b f(t)\,g(t)\,dt\)</span>. This turns the space of square-integrable functions on <span class="arithmatex">\([a,b]\)</span> into an inner product space (a Hilbert space, <span class="arithmatex">\(L^2[a,b]\)</span>). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p>
</li>
</ul>
<blockquote>
<p>Any vector space with an inner product has an orthonormal basis (via the Gram–Schmidt process). Gram–Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as <span class="arithmatex">\(A = QR\)</span> where <span class="arithmatex">\(Q\)</span> has orthonormal columns and <span class="arithmatex">\(R\)</span> is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve <span class="arithmatex">\(Ax=b\)</span> and to analyze subspaces. For example, for an overdetermined system (<span class="arithmatex">\(m&gt;n\)</span> i.e. more equations than unknowns), <span class="arithmatex">\(Ax=b\)</span> has a least-squares solution <span class="arithmatex">\(x = R^{-1}(Q^\top b)\)</span>, and for underdetermined (<span class="arithmatex">\(m&lt;n\)</span>), <span class="arithmatex">\(Ax=b\)</span> has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p>
</blockquote>
<p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p>
<ul>
<li>
<p>Gradients: The gradient <span class="arithmatex">\(\nabla f(x)\)</span> is defined as the vector satisfying <span class="arithmatex">\(f(x+h)\approx f(x) + \langle \nabla f(x), h\rangle\)</span>. Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction <span class="arithmatex">\(-\nabla f(x)\)</span> because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix <span class="arithmatex">\(W\)</span>), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p>
</li>
<li>
<p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints <span class="arithmatex">\(Ax=b\)</span> (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(\mathrm{range}(A)\)</span>) and quadratic programs (where each iteration might involve a projection).  </p>
</li>
<li>
<p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p>
</li>
<li>
<p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix <span class="arithmatex">\(G_{ij} = \langle x_i, x_j\rangle\)</span> for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: <span class="arithmatex">\(X^\top X\)</span> is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix’s condition number and thus algorithm performance.</p>
</li>
</ul>
<h2 id="convex-12_vector-24-norms-and-distances">2.4 Norms and distances<a class="headerlink" href="#convex-12_vector-24-norms-and-distances" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(\|\cdot\|: \mathbb{R}^n \to \mathbb{R}\)</span> is a norm if for all <span class="arithmatex">\(x,y\)</span> and scalar <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\|x\| \ge 0\)</span> and <span class="arithmatex">\(\|x\| = 0 \iff x=0\)</span>,</li>
<li><span class="arithmatex">\(\|\alpha x\| = |\alpha|\|x\|\)</span> (absolute homogeneity),</li>
<li><span class="arithmatex">\(\|x+y\| \le \|x\| + \|y\|\)</span> (triangle inequality).</li>
</ol>
<p>If the vector space has an inner product, the norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x\rangle}\)</span> is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry.<br>
Common examples on <span class="arithmatex">\(\mathbb{R}^n\)</span>:  </p>
<ul>
<li>
<p><span class="arithmatex">\(\ell_2\)</span> norm (Euclidean): <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span>, the usual length in space.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>, measuring taxicab distance. In <span class="arithmatex">\(\mathbb{R}^2\)</span>, its unit ball is a diamond.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span>, measuring the largest coordinate magnitude. Its unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a square.  </p>
</li>
<li>
<p>General <span class="arithmatex">\(\ell_p\)</span> norm: <span class="arithmatex">\(\|x\|_p = \left(\sum_i |x_i|^p\right)^{1/p}\)</span> for <span class="arithmatex">\(p\ge1\)</span>. This interpolates between <span class="arithmatex">\(\ell_1\)</span> and <span class="arithmatex">\(\ell_2\)</span>, and approaches <span class="arithmatex">\(\ell_\infty\)</span> as <span class="arithmatex">\(p\to\infty\)</span>. All <span class="arithmatex">\(\ell_p\)</span> norms are convex and satisfy the norm axioms.  </p>
</li>
</ul>
<p>Every norm induces a metric (distance) <span class="arithmatex">\(d(x,y) = |x-y|\)</span> on the space. Norms thus define the shape of “balls” (sets <span class="arithmatex">\({x: |x|\le \text{constant}}\)</span>) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm’s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../convex/images/norms.png" data-desc-position="bottom"><img alt="Alt text" src="../convex/images/norms.png" style="float:right; margin-right:15px; width:400px;"></a></p>
<p>Unit-ball geometry: The shape of the unit ball <span class="arithmatex">\({x: |x| \le 1}\)</span> reveals how a norm treats different directions. For example, the <span class="arithmatex">\(\ell_2\)</span> unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a perfect circle, treating all directions uniformly, whereas the <span class="arithmatex">\(\ell_1\)</span> unit ball is a diamond with corners along the axes, indicating that <span class="arithmatex">\(\ell_1\)</span> treats the coordinate axes as special (those are “cheaper” directions since the ball extends further along axes, touching them at <span class="arithmatex">\((\pm1,0)\)</span> and <span class="arithmatex">\((0,\pm1)\)</span>). The <span class="arithmatex">\(\ell_\infty\)</span> unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (<span class="arithmatex">\(\ell_1\)</span>), green circle (<span class="arithmatex">\(\ell_2\)</span>), and blue square (<span class="arithmatex">\(\ell_\infty\)</span>) in <span class="arithmatex">\(\mathbb{R}^2\)</span> . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an <span class="arithmatex">\(\ell_1\)</span> norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an <span class="arithmatex">\(\ell_2\)</span> ball encourages more evenly-distributed changes. An <span class="arithmatex">\(\ell_\infty\)</span> constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p>
<p>Dual norms: Each norm <span class="arithmatex">\(\|\cdot\|\)</span> has a dual norm <span class="arithmatex">\(\|\cdot\|_*\)</span> defined by
<script type="math/tex; mode=display">
\|y\|_* = \sup_{\|x\|\le 1} x^\top y~.
</script>
For example, the dual of <span class="arithmatex">\(\ell_1\)</span> is <span class="arithmatex">\(\ell_\infty\)</span>, and the dual of <span class="arithmatex">\(\ell_2\)</span> is itself.</p>
<blockquote>
<p>Imagine the vector <span class="arithmatex">\(x\)</span> lives inside the original norm ball (<span class="arithmatex">\(\|x\| \le 1\)</span>). The term <span class="arithmatex">\(x^\top y\)</span> is the dot product, which measures the alignment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>. The dual norm <span class="arithmatex">\(\|y\|_*\)</span> is the maximum possible value you can get by taking the dot product of <span class="arithmatex">\(y\)</span> with any vector <span class="arithmatex">\(x\)</span> that fits inside the original norm ball.If the dual norm <span class="arithmatex">\(\|y\|_*\)</span> is large, it means <span class="arithmatex">\(y\)</span> is strongly aligned with a direction <span class="arithmatex">\(x\)</span> that is "small" (size <span class="arithmatex">\(\le 1\)</span>) according to the original norm.If the dual norm is small, <span class="arithmatex">\(y\)</span> must be poorly aligned with all vectors <span class="arithmatex">\(x\)</span> in the ball.</p>
</blockquote>
<p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use <span class="arithmatex">\(\ell_\infty\)</span> (since one coordinate move at a time is like a step in <span class="arithmatex">\(\ell_\infty\)</span> unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using <span class="arithmatex">\(\ell_1\)</span> norm for sparse problems). The norm also figures in complexity bounds: an algorithm’s convergence rate may depend on the diameter of the feasible set in the chosen norm, <span class="arithmatex">\(D = \max_{\text{feasible}}|x - x^*|\)</span>. For instance, in subgradient methods, having a smaller <span class="arithmatex">\(\ell_2\)</span> diameter or <span class="arithmatex">\(\ell_1\)</span> diameter can improve bounds. Moreover, when constraints are given by norms (like <span class="arithmatex">\(|x|_1 \le t\)</span>), projections and proximal operators with respect to that norm become subroutines in algorithms.</p>
<p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (<span class="arithmatex">\(|x_k - x^*|\)</span>), how to constrain solutions (<span class="arithmatex">\(|x| \le R\)</span>), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>
<h2 id="convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices">2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices<a class="headerlink" href="#convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{n\times n}\)</span> is linear, a nonzero <span class="arithmatex">\(v\)</span> is an eigenvector with eigenvalue <span class="arithmatex">\(\lambda\)</span> if</p>
<div class="arithmatex">\[
Av = \lambda v~.
\]</div>
<p>When <span class="arithmatex">\(A\)</span> is symmetric (<span class="arithmatex">\(A = A^\top\)</span>), it has:</p>
<ul>
<li>real eigenvalues,</li>
<li>an orthonormal eigenbasis,</li>
<li>a spectral decomposition</li>
</ul>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^\top,
</script>
where <span class="arithmatex">\(Q\)</span> is orthonormal and <span class="arithmatex">\(\Lambda\)</span> is diagonal.</p>
<p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along <span class="arithmatex">\(n\)</span> orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by <span class="arithmatex">\(\lambda_i\)</span>.</p>
<blockquote>
<p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p>
<p>In optimization, the Hessian matrix of a multivariate function <span class="arithmatex">\(f(x)\)</span> is symmetric. Its eigenvalues <span class="arithmatex">\(\lambda_i(\nabla^2 f(x))\)</span> tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there’s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p>
</blockquote>
<p>Positive semidefinite matrices: A symmetric matrix <span class="arithmatex">\(Q\)</span> is positive semidefinite (PSD) if</p>
<div class="arithmatex">\[
x^\top Q x \ge 0 \quad \text{for all } x~.
\]</div>
<p>If <span class="arithmatex">\(x^\top Q x &gt; 0\)</span> for all <span class="arithmatex">\(x\ne 0\)</span>, then <span class="arithmatex">\(Q\)</span> is positive definite (PD).</p>
<p>Why this matters: if <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top Q x + c^\top x + d\)</span>, then</p>
<div class="arithmatex">\[
\nabla^2 f(x) = Q~.
\]</div>
<p>So <span class="arithmatex">\(f\)</span> is convex iff <span class="arithmatex">\(Q\)</span> is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p>
<blockquote>
<p>Implications of definiteness: If <span class="arithmatex">\(A \succ 0\)</span>, the quadratic function <span class="arithmatex">\(x^T A x\)</span> is strictly convex and has a unique minimizer at <span class="arithmatex">\(x=0\)</span>. If <span class="arithmatex">\(A \succeq 0\)</span>, <span class="arithmatex">\(x^T A x\)</span> is convex but could be flat in some directions (if some <span class="arithmatex">\(\lambda_i = 0\)</span>, those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> means <span class="arithmatex">\(f\)</span> has a unique local (and global, if domain convex) minimum at that <span class="arithmatex">\(x\)</span> (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater’s condition for strong duality.</p>
<p>Condition number and convergence: For iterative methods on convex quadratics <span class="arithmatex">\(f(x) = \frac{1}{2}x^T Q x - b^T x\)</span>, the eigenvalues of <span class="arithmatex">\(Q\)</span> dictate convergence speed. Gradient descent’s error after <span class="arithmatex">\(k\)</span> steps satisfies roughly <span class="arithmatex">\(|x_k - x^*| \le (\frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}})^k |x_0 - x^*|\)</span> (for normalized step). So the ratio <span class="arithmatex">\(\frac{\lambda_{\max}}{\lambda_{\min}} = \kappa(Q)\)</span> appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton’s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to <span class="arithmatex">\(\kappa\)</span> (locally). This explains why second-order methods shine on ill-conditioned problems: they “whiten” the curvature by dividing by eigenvalues.</p>
<p>Optimization interpretation of eigenvectors: The eigenvectors of <span class="arithmatex">\(\nabla^2 f(x^*)\)</span> at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton’s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p>
</blockquote>
<h2 id="convex-12_vector-26-orthogonal-projections-and-least-squares">2.6 Orthogonal projections and least squares<a class="headerlink" href="#convex-12_vector-26-orthogonal-projections-and-least-squares" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(S\)</span> be a subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span>. The orthogonal projection of a vector <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is the unique vector <span class="arithmatex">\(p \in S\)</span> minimising <span class="arithmatex">\(\|b - p\|_2\)</span>. Geometrically, <span class="arithmatex">\(p\)</span> is the closest point in <span class="arithmatex">\(S\)</span> to <span class="arithmatex">\(b\)</span>.</p>
<p>If <span class="arithmatex">\(S = \mathrm{span}\{a_1,\dots,a_k\}\)</span> and <span class="arithmatex">\(A = [a_1~\cdots~a_k]\)</span>, then projecting <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is equivalent to solving the least-squares problem</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2~.
\]</div>
<p>The solution <span class="arithmatex">\(x^*\)</span> satisfies the normal equations</p>
<div class="arithmatex">\[
A^\top A x^* = A^\top b~.
\]</div>
<p>This is our first real convex optimisation problem:</p>
<ul>
<li>the objective <span class="arithmatex">\(\|Ax-b\|_2^2\)</span> is convex,</li>
<li>there are no constraints,</li>
<li>we can solve it in closed form.</li>
</ul>
<h2 id="convex-12_vector-27-advanced-concepts">2.7 Advanced Concepts<a class="headerlink" href="#convex-12_vector-27-advanced-concepts" title="Permanent link">¶</a></h2>
<p>Operator norm: Given a matrix (linear map) <span class="arithmatex">\(A: \mathbb{R}^n \to \mathbb{R}^m\)</span> and given a choice of vector norms on input and output, one can define the induced operator norm. If we use <span class="arithmatex">\(|\cdot|_p\)</span> on <span class="arithmatex">\(\mathbb{R}^n\)</span> and <span class="arithmatex">\(|\cdot|_q\)</span> on <span class="arithmatex">\(\mathbb{R}^m\)</span>, the operator norm is</p>
<div class="arithmatex">\[
\|A\|_{p \to q}
= \sup_{x \ne 0} \frac{\|Ax\|_q}{\|x\|_p}
= \sup_{\|x\|_p \le 1} \|Ax\|_q
\]</div>
<p>This gives the maximum factor by which <span class="arithmatex">\(A\)</span> can stretch a vector (measuring <span class="arithmatex">\(x\)</span> in norm <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(Ax\)</span> in norm <span class="arithmatex">\(q\)</span>).pecial cases are common: with <span class="arithmatex">\(p = q = 2\)</span>, <span class="arithmatex">\(|A|_{2 \to 2}\)</span> (often just written <span class="arithmatex">\(|A|_2\)</span>) is the spectral norm, which equals the largest singular value of <span class="arithmatex">\(A\)</span> (more on singular values below).
If <span class="arithmatex">\(p = q = 1\)</span>, <span class="arithmatex">\(|A|_{1 \to 1}\)</span> is the maximum absolute column sum of <span class="arithmatex">\(A\)</span>.
If <span class="arithmatex">\(p = q = \infty\)</span>, <span class="arithmatex">\(|A|{\infty \to \infty}\)</span> is the maximum absolute row sum.</p>
<p>Operator norms tell us the worst-case amplification of signals by <span class="arithmatex">\(A\)</span>. In gradient descent on <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top A x - b^\top x\)</span> (a quadratic form), the step size must be <span class="arithmatex">\(\le \tfrac{2}{|A|_2}\)</span> for convergence; here <span class="arithmatex">\(|A|_2 = \lambda_{\max}(A)\)</span> if <span class="arithmatex">\(A\)</span> is symmetric (it’s related to Hessian eigenvalues, Chapter 5). In general, controlling <span class="arithmatex">\(|A|\)</span> controls stability: if <span class="arithmatex">\(|A| &lt; 1\)</span>, the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p>
<p>Singular Value Decomposition (SVD): Any matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as</p>
<div class="arithmatex">\[
A = U \Sigma V^\top
\]</div>
<p>where <span class="arithmatex">\(U \in \mathbb{R}^{m\times m}\)</span> and <span class="arithmatex">\(V \in \mathbb{R}^{n\times n}\)</span> are orthogonal matrices (their columns are orthonormal bases of <span class="arithmatex">\(\mathbb{R}^m\)</span> and <span class="arithmatex">\(\mathbb{R}^n\)</span>, respectively), and <span class="arithmatex">\(\Sigma\)</span> is an <span class="arithmatex">\(m\times n\)</span> diagonal matrix with nonnegative entries <span class="arithmatex">\(\sigma_1 \ge \sigma_2 \ge \cdots \ge 0\)</span> on the diagonal. The <span class="arithmatex">\(\sigma_i\)</span> are the singular values of <span class="arithmatex">\(A\)</span>. Geometrically, <span class="arithmatex">\(A\)</span> sends the unit ball in <span class="arithmatex">\(\mathbb{R}^n\)</span> to an ellipsoid in <span class="arithmatex">\(\mathbb{R}^m\)</span> whose principal semi-axes lengths are the singular values and directions are the columns of <span class="arithmatex">\(V\)</span> (mapped to columns of <span class="arithmatex">\(U\)</span>). The largest singular value <span class="arithmatex">\(\sigma_{\max} = |A|_2\)</span> is the spectral norm. The smallest (if <span class="arithmatex">\(n \le m\)</span>, <span class="arithmatex">\(\sigma{\min}\)</span> of those <span class="arithmatex">\(n\)</span>) indicates how <span class="arithmatex">\(A\)</span> contracts the least – if <span class="arithmatex">\(\sigma_{\min} = 0\)</span>, <span class="arithmatex">\(A\)</span> is rank-deficient.</p>
<p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number <span class="arithmatex">\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span> (when <span class="arithmatex">\(\sigma_{\min}&gt;0\)</span>), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in <span class="arithmatex">\(x\)</span>-space hardly change <span class="arithmatex">\(Ax\)</span> (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means <span class="arithmatex">\(A\)</span> is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of <span class="arithmatex">\(A\)</span> (Eckart–Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p>
<p>Low-rank structure: The rank of <span class="arithmatex">\(A\)</span> equals the number of nonzero singular values. If <span class="arithmatex">\(A\)</span> has rank <span class="arithmatex">\(r \ll \min(n,m)\)</span>, it means <span class="arithmatex">\(A\)</span> effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem’s data matrix <span class="arithmatex">\(A\)</span> is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p>
<p>Operator norm in optimization: Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem <span class="arithmatex">\(f(x) = \frac{1}{2}x^TQx - b^Tx\)</span>, the Hessian is <span class="arithmatex">\(Q\)</span> and gradient descent converges if <span class="arithmatex">\(\alpha &lt; 2/\lambda_{\max}(Q)\)</span>. Preconditioning aims to transform <span class="arithmatex">\(Q\)</span> into one with a smaller condition number by multiplying by some <span class="arithmatex">\(P\)</span> (like using <span class="arithmatex">\(P^{-1}Q\)</span>) — effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex <span class="arithmatex">\(f\)</span>, the Lipschitz constant of <span class="arithmatex">\(\nabla f\)</span> (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p>
<p>Summary of spectral properties:</p>
<ul>
<li>
<p>The spectral norm <span class="arithmatex">\(|A|_2 = \sigma_{\max}(A)\)</span> quantifies the largest stretching. It determines stability and step sizes.</p>
</li>
<li>
<p>The smallest singular value <span class="arithmatex">\(\sigma_{\min}\)</span> (if <span class="arithmatex">\(A\)</span> is tall full-rank) tells if <span class="arithmatex">\(A\)</span> is invertible and how sensitive the inverse is. If <span class="arithmatex">\(\sigma_{\min}\)</span> is tiny, small changes in output cause huge changes in solving <span class="arithmatex">\(Ax=b\)</span>.</p>
</li>
<li>
<p>The condition number <span class="arithmatex">\(\kappa = \sigma_{\max}/\sigma_{\min}\)</span> is a figure of merit for algorithms: gradient descent iterations needed often scale with <span class="arithmatex">\(\kappa\)</span> (worse conditioning = slower). Regularization like adding <span class="arithmatex">\(\mu I\)</span> increases <span class="arithmatex">\(\sigma_{\min}\)</span>, thereby reducing <span class="arithmatex">\(\kappa\)</span> and accelerating convergence (at the expense of bias).</p>
</li>
<li>
<p>Nuclear norm (sum of singular values) and spectral norm often appear in optimization as convex surrogates for rank and as constraints to limit the operator’s impact.</p>
</li>
</ul>
<p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p></body></html></section><section class="print-page" id="convex-13_calculus" heading-number="2.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-multivariable-calculus-for-optimization">Chapter 3: Multivariable Calculus for Optimization<a class="headerlink" href="#convex-13_calculus-chapter-3-multivariable-calculus-for-optimization" title="Permanent link">¶</a></h1>
<p>Optimization seeks to find points that minimize or maximize a real-valued function. To analyze and solve such problems, we rely on tools from multivariable calculus — gradients, Jacobians, Hessians, and Taylor expansions — which describe how a function changes locally.</p>
<p>This chapter provides the differential calculus foundation essential for convex analysis and gradient-based learning algorithms.  It links geometric intuition and analytical tools that underlie optimization methods such as gradient descent, Newton’s method, and backpropagation.</p>
<h2 id="convex-13_calculus-31-gradients-and-directional-derivatives">3.1 Gradients and Directional Derivatives<a class="headerlink" href="#convex-13_calculus-31-gradients-and-directional-derivatives" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f:\mathbb{R}^n\to\mathbb{R}\)</span>. Differentiability at <span class="arithmatex">\(x\)</span> means there exists a linear map (the gradient) such that</p>
<div class="arithmatex">\[
f(x+h)=f(x)+\nabla f(x)^\top h+o(\|h\|).
\]</div>
<p>Equivalently, for every direction <span class="arithmatex">\(v\in\mathbb{R}^n\)</span>, the directional derivative exists and matches the gradient pairing:</p>
<div class="arithmatex">\[
D_v f(x)=\lim_{t \rightarrow 0}\frac{f(x+tv)-f(x)}{t}=\nabla f(x)^\top v.
\]</div>
<p>This shows that <span class="arithmatex">\(\nabla f(x)\)</span> is the unique vector giving the best linear approximation of <span class="arithmatex">\(f\)</span> near <span class="arithmatex">\(x\)</span>.<br>
Among all unit directions <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
D_u f(x) = \langle \nabla f(x), u \rangle
</script>
is maximized when <span class="arithmatex">\(u\)</span> aligns with <span class="arithmatex">\(\nabla f(x)\)</span> — the direction of steepest ascent.<br>
The opposite direction, <span class="arithmatex">\(- \nabla f(x)\)</span>, gives the steepest descent.</p>
<blockquote>
<p>A level set of a differentiable function <span class="arithmatex">\(f\)</span> is
<script type="math/tex; mode=display">
L_c = \{\, x \in \mathbb{R}^n : f(x) = c \,\}.
</script>
</p>
<p>At any point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x) \ne 0\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to the level set <span class="arithmatex">\(L_{f(x)}\)</span>. Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them — in the direction of the steepest ascent of <span class="arithmatex">\(f\)</span>. If we wish to decrease <span class="arithmatex">\(f\)</span>, we move roughly in the opposite direction, <span class="arithmatex">\(-\nabla f(x)\)</span> (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>
</blockquote>
<h2 id="convex-13_calculus-32-jacobians">3.2  Jacobians<a class="headerlink" href="#convex-13_calculus-32-jacobians" title="Permanent link">¶</a></h2>
<p>When dealing with optimization or learning, functions rarely map one number to another. They often map many inputs to many outputs — for example, a neural network layer, a physical model, or a vector-valued transformation. The Jacobian matrix captures <em>how each output reacts to each input</em> — the complete local sensitivity of a system.</p>
<p>Derivative to Gradient:  For a scalar function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, the derivative generalizes to the gradient:</p>
<div class="arithmatex">\[
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x) \\
\vdots \\
\frac{\partial f}{\partial x_n}(x)
\end{bmatrix}.
\]</div>
<ul>
<li>Each component <span class="arithmatex">\(\frac{\partial f}{\partial x_i}\)</span> tells how <span class="arithmatex">\(f\)</span> changes as we vary <span class="arithmatex">\(x_i\)</span> alone.  </li>
<li>Collectively, <span class="arithmatex">\(\nabla f(x)\)</span> forms the vector of steepest ascent, pointing toward the direction of maximal increase.  </li>
<li>The magnitude <span class="arithmatex">\(\|\nabla f(x)\|\)</span> measures how sharply <span class="arithmatex">\(f\)</span> rises.</li>
</ul>
<p>From Gradient to Jacobian — Many Inputs, Many Outputs: Now let <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span> be a vector-valued function:</p>
<div class="arithmatex">\[
F(x) =
\begin{bmatrix}
F_1(x) \\[4pt]
F_2(x) \\[4pt]
\vdots \\[4pt]
F_m(x)
\end{bmatrix}.
\]</div>
<p>Each <span class="arithmatex">\(F_i(x)\)</span> is a scalar function with its own gradient <span class="arithmatex">\(\nabla F_i(x)^\top\)</span>.<br>
Stacking these row vectors gives the Jacobian matrix:
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>For <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian is the <span class="arithmatex">\(m \times n\)</span> matrix
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \dots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \dots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>It represents the best linear map approximating <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x) \, h.
</script>
</p>
<p>Just as a tangent line approximates a scalar curve, the Jacobian defines the tangent linear map that approximates <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x) \, h.
</script>
</p>
<ul>
<li>The small displacement <span class="arithmatex">\(h\)</span> in input space is transformed linearly into an output change <span class="arithmatex">\(J_F(x)h\)</span>.  </li>
<li>Thus, <span class="arithmatex">\(J_F(x)\)</span> acts like the <em>microscopic blueprint</em> of <span class="arithmatex">\(F\)</span> around <span class="arithmatex">\(x\)</span>.  </li>
<li>Locally, <span class="arithmatex">\(F\)</span> behaves like a matrix transformation — stretching, rotating, or skewing space.</li>
</ul>
<table>
<thead>
<tr>
<th>Part of <span class="arithmatex">\(J_F(x)\)</span></th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row <span class="arithmatex">\(i\)</span></td>
<td>Gradient of the <span class="arithmatex">\(i\)</span>-th output <span class="arithmatex">\(F_i(x)\)</span> — how that output changes with each input variable.</td>
</tr>
<tr>
<td>Column <span class="arithmatex">\(j\)</span></td>
<td>Sensitivity of all outputs to input <span class="arithmatex">\(x_j\)</span> — how changing <span class="arithmatex">\(x_j\)</span> influences the entire output vector.</td>
</tr>
<tr>
<td>Determinant (if <span class="arithmatex">\(m=n\)</span>)</td>
<td>Local volume scaling — how much <span class="arithmatex">\(F\)</span> expands or compresses space near <span class="arithmatex">\(x\)</span>.</td>
</tr>
<tr>
<td>Rank of <span class="arithmatex">\(J_F(x)\)</span></td>
<td>Local dimensionality of the image of <span class="arithmatex">\(F\)</span> — tells if directions are lost or preserved.</td>
</tr>
</tbody>
</table>
<h2 id="convex-13_calculus-33-the-hessian-and-curvature">3.3 The Hessian and Curvature<a class="headerlink" href="#convex-13_calculus-33-the-hessian-and-curvature" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is twice differentiable, its Hessian is</p>
<div class="arithmatex">\[
\nabla^2 f(x) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}.
\]</div>
<p>The Hessian encodes curvature information:
- <span class="arithmatex">\(\nabla^2 f(x) \succeq 0\)</span> (positive semidefinite) ⟹ <span class="arithmatex">\(f\)</span> is convex near <span class="arithmatex">\(x\)</span>.<br>
- <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> ⟹ <span class="arithmatex">\(f\)</span> is strictly convex.<br>
- Negative eigenvalues ⟹ directions of local decrease.</p>
<p>Example – quadratic function: <span class="arithmatex">\(f(x) = \frac{1}{2}x^TQx - b^T x\)</span>. Here <span class="arithmatex">\(\nabla f(x) = Qx - b\)</span> (linear), and <span class="arithmatex">\(\nabla^2 f(x) = Q\)</span>. Solving <span class="arithmatex">\(\nabla f=0\)</span> yields <span class="arithmatex">\(Qx=b\)</span>, so if <span class="arithmatex">\(Q \succ 0\)</span> the unique minimizer is <span class="arithmatex">\(x^* = Q^{-1}b\)</span>. The Hessian being <span class="arithmatex">\(Q \succ 0\)</span> confirms convexity. If <span class="arithmatex">\(Q\)</span> has large eigenvalues, gradient <span class="arithmatex">\(Qx - b\)</span> changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of <span class="arithmatex">\(Q\)</span> controls difficulty of minimizing <span class="arithmatex">\(f\)</span>.</p>
<blockquote>
<p>Eigenvalues of the Hessian describe curvature along principal directions. Large eigenvalues correspond to steep curvature; small ones correspond to flat regions. Understanding this curvature is essential in designing stable optimization algorithms.</p>
</blockquote>
<h2 id="convex-13_calculus-34-taylor-approximation">3.4 Taylor approximation<a class="headerlink" href="#convex-13_calculus-34-taylor-approximation" title="Permanent link">¶</a></h2>
<p>For differentiable <span class="arithmatex">\(f\)</span>, we have the first-order Taylor expansion around <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
f(x + d) \approx f(x) + \nabla f(x)^\top d~.
</script>
</p>
<p>The gradient gives the best local linear approximation, predicting how <span class="arithmatex">\(f\)</span> changes for a small move <span class="arithmatex">\(d\)</span>. This is the foundation of first-order optimization methods such as gradient descent.</p>
<p>If f is twice differentiable, we have the second-order expansion
<script type="math/tex; mode=display">
f(x + d) \approx f(x)
+ \nabla f(x)^\top d
+ \frac{1}{2} d^\top \nabla^2 f(x) d~.
</script>
</p>
<p>If <span class="arithmatex">\(\nabla^2 f(x)\)</span> is positive semidefinite, the quadratic term is always <span class="arithmatex">\(\ge 0\)</span>. Locally, <span class="arithmatex">\(x\)</span> is in a “bowl”. If <span class="arithmatex">\(\nabla^2 f(x)\)</span> is indefinite, the landscape can curve up in some directions and down in others — typical of saddle points.</p>
<p>This quadratic model is the foundation of Newton’s method and trust-region algorithms:  the linear term drives direction; the quadratic term refines curvature.</p>
<h2 id="convex-13_calculus-35-convexity-and-the-hessian">3.5 Convexity and the Hessian<a class="headerlink" href="#convex-13_calculus-35-convexity-and-the-hessian" title="Permanent link">¶</a></h2>
<p>A twice-differentiable function <span class="arithmatex">\(f\)</span> is convex on a convex set if and only if</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0 \quad \forall x \text{ in the domain.}
\]</div>
<p>The Hessian describes how the gradient changes.</p>
<h2 id="convex-13_calculus-36-first-and-second-order-optimality-conditions">3.6 First and Second-Order Optimality Conditions<a class="headerlink" href="#convex-13_calculus-36-first-and-second-order-optimality-conditions" title="Permanent link">¶</a></h2>
<p>Suppose we want to solve an unconstrained optimization problem:
<script type="math/tex; mode=display">
\min_x f(x).
</script>
</p>
<p>A point <span class="arithmatex">\(x^\star\)</span> is called a critical point if
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0.
</script>
</p>
<h3 id="convex-13_calculus-first-order-necessary-condition">First-Order Necessary Condition<a class="headerlink" href="#convex-13_calculus-first-order-necessary-condition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable and <span class="arithmatex">\(x^\star\)</span> is a local minimizer, then necessarily
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0.
</script>
</p>
<p>Intuitively, this means the slope in every direction must vanish — there is no infinitesimal move that decreases <span class="arithmatex">\(f\)</span> further.</p>
<h3 id="convex-13_calculus-second-order-necessary-condition">Second-Order Necessary Condition<a class="headerlink" href="#convex-13_calculus-second-order-necessary-condition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is twice differentiable and <span class="arithmatex">\(x^\star\)</span> is a local minimizer, then
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0,
\quad
\nabla^2 f(x^\star) \succeq 0,
</script>
</p>
<p>meaning the Hessian is positive semidefinite (PSD). The function curves upward (or flat) in all local directions.</p>
<h3 id="convex-13_calculus-second-order-sufficient-condition">Second-Order Sufficient Condition<a class="headerlink" href="#convex-13_calculus-second-order-sufficient-condition" title="Permanent link">¶</a></h3>
<p>If
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0,
\quad
\nabla^2 f(x^\star) \succ 0,
</script>
i.e. the Hessian is positive definite (PD),<br>
then <span class="arithmatex">\(x^\star\)</span> is a strict local minimizer — the point lies at the bottom of a strictly convex bowl.</p>
<p>The gradient gives the best local linear approximation, predicting how f changes for a small move d.  This is the foundation of first-order optimization methods such as gradient descent. If <span class="arithmatex">\(\nabla f(x^\star)\)</span> has both positive and negative eigenvalues, <span class="arithmatex">\(x^\star\)</span> is a saddle point — neither a minimum nor a maximum.</p>
<p>Convexity makes everything simpler.If <span class="arithmatex">\(f\)</span> is convex, then <em>any</em> point <span class="arithmatex">\(x^\star\)</span> satisfying <span class="arithmatex">\(\nabla f(x^\star) = 0\)</span>  is not only a local minimizer — it is a global minimizer.</p>
<h2 id="convex-13_calculus-37-lipschitz-continuity-and-smoothness">3.7 Lipschitz Continuity and Smoothness<a class="headerlink" href="#convex-13_calculus-37-lipschitz-continuity-and-smoothness" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(f\)</span> has a Lipschitz continuous gradient with constant <span class="arithmatex">\(L &gt; 0\)</span> if
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\|, \quad \forall x,y.
</script>
</p>
<p>Such a function is called L-smooth. This condition bounds how quickly the gradient can change, ensuring the function is not excessively curved.</p>
<p>This property implies the Descent Lemma:
<script type="math/tex; mode=display">
f(y) \le f(x) + \nabla f(x)^\top (y - x) + \tfrac{L}{2} \|y - x\|^2.
</script>
</p>
<p>Smoothness bounds how fast <span class="arithmatex">\(f\)</span> can curve.<br>
In gradient descent, choosing a step size <span class="arithmatex">\(\eta \le 1/L\)</span> guarantees convergence for convex functions.</p>
<blockquote>
<p>In ML training, <span class="arithmatex">\(L\)</span> controls how “aggressive” the learning rate can be — smoother losses allow larger steps.</p>
</blockquote>
<h2 id="convex-13_calculus-38-strong-convexity-functions-with-guaranteed-curvature">3.8 Strong Convexity — Functions with Guaranteed Curvature<a class="headerlink" href="#convex-13_calculus-38-strong-convexity-functions-with-guaranteed-curvature" title="Permanent link">¶</a></h2>
<p>A differentiable function <span class="arithmatex">\(f\)</span> is said to be <span class="arithmatex">\(\mu\)</span>-strongly convex if for some <span class="arithmatex">\(\mu &gt; 0\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + \langle \nabla f(x),\, y - x \rangle + \frac{\mu}{2}\|y - x\|^2,
\quad \forall\, x, y.
</script>
</p>
<p>This inequality means f always lies above its tangent plane by at least a quadratic term of curvature μ.<br>
Strong convexity ensures a minimum curvature: f grows at least as fast as a parabola away from its minimizer.</p>
<h2 id="convex-13_calculus-39-subgradients-and-nonsmooth-extensions">3.9 Subgradients and Nonsmooth Extensions<a class="headerlink" href="#convex-13_calculus-39-subgradients-and-nonsmooth-extensions" title="Permanent link">¶</a></h2>
<p>Many useful convex functions are nonsmooth — e.g., hinge loss, <span class="arithmatex">\(\ell_1\)</span> norm, ReLU.<br>
They lack a gradient at certain points but admit a subgradient.</p>
<p>A vector <span class="arithmatex">\(g\)</span> is a subgradient of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> if
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y - x), \quad \forall y.
</script>
</p>
<p>The set of all such vectors is the subdifferential <span class="arithmatex">\(\partial f(x)\)</span>.  </p>
<p>Examples:
- <span class="arithmatex">\(f(x) = \|x\|_1\)</span> has
  <script type="math/tex; mode=display">
  (\partial f(x))_i =
  \begin{cases}
  \operatorname{sign}(x_i), & x_i \ne 0, \\
  [-1, 1], & x_i = 0.
  \end{cases}
  </script>
- For <span class="arithmatex">\(f(x) = \max_i x_i\)</span>, any unit vector supported on the active index is a subgradient.</p>
<p>Subgradients generalize the gradient concept, allowing optimization even when derivatives do not exist.<br>
They are the backbone of nonsmooth convex optimization and proximal methods.  In machine learning, they make it possible to minimize losses such as the hinge or absolute deviation, where gradients are undefined at corners.</p></body></html></section><section class="print-page" id="convex-14_convexsets" heading-number="2.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-4-convex-sets-and-geometric-fundamentals">Chapter 4: Convex Sets and Geometric Fundamentals<a class="headerlink" href="#convex-14_convexsets-chapter-4-convex-sets-and-geometric-fundamentals" title="Permanent link">¶</a></h1>
<p>Optimisation problems almost always include constraints. The feasible region, the set of points allowed by those constraints, is often a convex set. This chapter builds geometric intuition for convex sets, affine sets, hyperplanes, polyhedra, and supporting hyperplanes.</p>
<h2 id="convex-14_convexsets-41-convex-sets">4.1 Convex sets<a class="headerlink" href="#convex-14_convexsets-41-convex-sets" title="Permanent link">¶</a></h2>
<p>A set <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> is convex if for any <span class="arithmatex">\(x,y \in C\)</span> and any <span class="arithmatex">\(\theta \in [0,1]\)</span>,
<script type="math/tex; mode=display">
\theta x + (1-\theta) y \in C~.
</script>
</p>
<blockquote>
<p>For any two feasible points, the entire line segment between them is also feasible. No “indentations.”</p>
</blockquote>
<h3 id="convex-14_convexsets-examples">Examples<a class="headerlink" href="#convex-14_convexsets-examples" title="Permanent link">¶</a></h3>
<ul>
<li>An affine subspace: <span class="arithmatex">\(\{ x : Ax = b \}\)</span>.</li>
<li>A halfspace: <span class="arithmatex">\(\{ x : a^\top x \le b \}\)</span>.</li>
<li>An <span class="arithmatex">\(\ell_2\)</span> ball: <span class="arithmatex">\(\{ x : \|x\|_2 \le r \}\)</span>.</li>
<li>An <span class="arithmatex">\(\ell_\infty\)</span> ball: <span class="arithmatex">\(\{ x : \|x\|_\infty \le r \}\)</span>, which is a box.</li>
<li>The probability simplex: <span class="arithmatex">\(\{ x \in \mathbb{R}^n : x \ge 0, \sum_i x_i = 1 \}\)</span>.</li>
</ul>
<p>Convexity means “no caves or holes” — you can mix any two feasible points and stay feasible.</p>
<p>Geometrically:  </p>
<ul>
<li>Convex sets are closed under averaging.  </li>
<li>Optimization over convex sets is stable: small perturbations to data do not create new local minima.  </li>
<li>Convex sets preserve feasibility under stochastic mixing — a key property in probabilistic ML models.</li>
</ul>
<blockquote>
<p>A set that is not convex: a crescent shape or annulus. The defining failure is: there exist <span class="arithmatex">\(x,y\)</span> in the set such that some convex combination leaves the set.</p>
</blockquote>
<h2 id="convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces">4.2 Affine sets, hyperplanes, and halfspaces<a class="headerlink" href="#convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces" title="Permanent link">¶</a></h2>
<p>An affine set is a translate of a subspace:
<script type="math/tex; mode=display">
\{ x_0 + v : v \in S \},
</script>
where <span class="arithmatex">\(S\)</span> is a subspace. Affine sets are convex.</p>
<p>A hyperplane in <span class="arithmatex">\(\mathbb{R}^n\)</span> is a set of the form
<script type="math/tex; mode=display">
H = \{ x : a^\top x = b \}
</script>
for some nonzero <span class="arithmatex">\(a \in \mathbb{R}^n\)</span>.</p>
<p>A halfspace is
<script type="math/tex; mode=display">
\{ x : a^\top x \le b \}.
</script>
</p>
<blockquote>
<p>Every affine set is convex, and every hyperplane is both affine and convex.</p>
<p>Halfspaces are convex; intersections of halfspaces are convex. Linear inequality constraints define intersections of halfspaces, and therefore give convex feasible regions.</p>
</blockquote>
<h2 id="convex-14_convexsets-43-convex-combinations-convex-hulls">4.3 Convex combinations, convex hulls<a class="headerlink" href="#convex-14_convexsets-43-convex-combinations-convex-hulls" title="Permanent link">¶</a></h2>
<p>A convex combination of points <span class="arithmatex">\(x_1,\dots,x_k\)</span> is
<script type="math/tex; mode=display">
\sum_{i=1}^k \theta_i x_i
\quad\text{with}\quad
\theta_i \ge 0,\ \sum_{i=1}^k \theta_i = 1.
</script>
</p>
<p>The convex hull of a set <span class="arithmatex">\(S\)</span> is the set of all convex combinations of finitely many points of <span class="arithmatex">\(S\)</span>. It is the “smallest” convex set containing <span class="arithmatex">\(S\)</span>.</p>
<p>Convex hulls matter because:</p>
<ul>
<li>Polytopes (bounded polyhedra) can be described as convex hulls of finitely many points (their vertices).</li>
<li>Many relaxations in optimisation replace a complicated nonconvex feasible set by its convex hull.</li>
</ul>
<p>Convex hulls appear everywhere:</p>
<ul>
<li>In machine learning, convex combinations define <em>mixtures</em> (e.g., mixture of experts, convex combination of base classifiers).</li>
<li>In optimization, many relaxations approximate hard discrete sets by their convex hull — making problems solvable with convex tools.</li>
</ul>
<p>Geometric intuition: the convex hull of points is like wrapping a rubber band around them.</p>
<h2 id="convex-14_convexsets-44-polyhedra-and-polytopes">4.4 Polyhedra and polytopes<a class="headerlink" href="#convex-14_convexsets-44-polyhedra-and-polytopes" title="Permanent link">¶</a></h2>
<p>A polyhedron is an intersection of finitely many halfspaces:
<script type="math/tex; mode=display">
P = \{ x : Ax \le b \}.
</script>
Polyhedra are convex and cane be unbounded. If <span class="arithmatex">\(P\)</span> is also bounded, it is called a polytope.</p>
<p>In linear programming, we minimise a linear objective <span class="arithmatex">\(c^\top x\)</span> over a polyhedron. The optimal solution, if it exists, is always attained at an extreme point (vertex) of the feasible polyhedron.</p>
<p>Linear programs (LPs) minimize <span class="arithmatex">\(c^\top x\)</span> over polyhedra. Since the objective is linear, the optimum lies at a vertex (extreme point) of the feasible region.</p>
<p>In ML:</p>
<ul>
<li>LPs describe support vector machines (linear margin constraints).  </li>
<li>Polyhedral feasible regions define sparsity-inducing <span class="arithmatex">\(\ell_1\)</span> regularizers (via linear inequalities).</li>
</ul>
<h2 id="convex-14_convexsets-45-extreme-points">4.5 Extreme points<a class="headerlink" href="#convex-14_convexsets-45-extreme-points" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(C\)</span> be a convex set. A point <span class="arithmatex">\(x \in C\)</span> is an extreme point if it cannot be expressed as a strict convex combination of two other distinct points in <span class="arithmatex">\(C\)</span>. Formally, <span class="arithmatex">\(x\)</span> is extreme in <span class="arithmatex">\(C\)</span> if whenever
<script type="math/tex; mode=display">
x = \theta y + (1-\theta) z,
\quad
0<\theta<1,
\quad
y,z \in C,
</script>
then <span class="arithmatex">\(y = z = x\)</span>.</p>
<p>Geometric meaning:</p>
<ul>
<li>Extreme points are the “corners.”</li>
<li>In a polytope, extreme points are precisely the vertices.</li>
</ul>
<p>This is why linear programming solutions are found at vertices.</p>
<h2 id="convex-14_convexsets-46-cones">4.6 Cones<a class="headerlink" href="#convex-14_convexsets-46-cones" title="Permanent link">¶</a></h2>
<p>A set <span class="arithmatex">\(K\)</span> is a cone if for any <span class="arithmatex">\(x \in K\)</span> and <span class="arithmatex">\(\alpha \ge 0\)</span>, <span class="arithmatex">\(\alpha x \in K\)</span>. A cone is convex if additionally <span class="arithmatex">\(x,y\in K\)</span> implies <span class="arithmatex">\(x+y \in K\)</span>. Convex cones are important (e.g. nonnegative orthant, PSD matrices cone) because many optimization problems can be cast as cone programs. Cones have extreme rays instead of points (directions that generate edges of the cone). For instance, the extreme rays of the positive orthant in <span class="arithmatex">\(\mathbb{R}^n\)</span> are the coordinate axes (each axis direction can’t be formed by positive combos of others).</p>
<ul>
<li>Cones are closed under nonnegative scaling, but not necessarily addition.  </li>
<li>Conic hull (convex cone): Collection of all conic combinations of points in <span class="arithmatex">\(S\)</span>.</li>
<li>A cone is not necessarily a subspace (negative multiples may not be included).  </li>
<li>A convex cone is closed under addition and nonnegative scaling.  </li>
<li>
<p>Polar Cones: Given a cone <span class="arithmatex">\(K \subseteq \mathbb{R}^n\)</span>, the polar cone is:</p>
<div class="arithmatex">\[
K^\circ = \{ y \in \mathbb{R}^n \mid \langle y, x \rangle \le 0, \; \forall x \in K \}.
\]</div>
<ul>
<li>Intuition: polar cone vectors form non-acute angles with every vector in <span class="arithmatex">\(K\)</span>.  </li>
<li>Properties:  <ul>
<li>Always a closed convex cone.  </li>
<li>If <span class="arithmatex">\(K\)</span> is a subspace, <span class="arithmatex">\(K^\circ\)</span> is the orthogonal complement.  </li>
<li>Duality: <span class="arithmatex">\((K^\circ)^\circ = K\)</span> for closed convex cones.  </li>
</ul>
</li>
</ul>
</li>
<li>
<p>Tangent Cone: For a set <span class="arithmatex">\(C\)</span> and point <span class="arithmatex">\(x \in C\)</span>, the tangent cone <span class="arithmatex">\(T_C(x)\)</span> contains all directions in which one can “move infinitesimally” while remaining in <span class="arithmatex">\(C\)</span>:</p>
<div class="arithmatex">\[
T_C(x) = \Big\{ d \in \mathbb{R}^n \;\Big|\; \exists t_k \downarrow 0, \; x_k \in C, \; x_k \to x, \; \frac{x_k - x}{t_k} \to d \Big\}.
\]</div>
<ul>
<li>Interior point: <span class="arithmatex">\(T_C(x) = \mathbb{R}^n\)</span>.  </li>
<li>Boundary point: <span class="arithmatex">\(T_C(x)\)</span> restricts movement to directions staying inside <span class="arithmatex">\(C\)</span>. </li>
<li>Tangent cones define feasible directions for projected gradient steps or constrained optimization.</li>
</ul>
</li>
<li>
<p>Normal Cone: For a convex set <span class="arithmatex">\(C\)</span> at point <span class="arithmatex">\(x \in C\)</span>:
    <script type="math/tex; mode=display">
    N_C(x) = \{ v \in \mathbb{R}^n \mid \langle v, y - x \rangle \le 0, \; \forall y \in C \}.
    </script>
</p>
<ul>
<li>Each <span class="arithmatex">\(v \in N_C(x)\)</span> defines a supporting hyperplane at <span class="arithmatex">\(x\)</span>.  </li>
<li>Relation: <span class="arithmatex">\(N_C(x) = \big(T_C(x)\big)^\circ\)</span> — polar of tangent cone.  </li>
<li>Interior point: <span class="arithmatex">\(N_C(x) = \{0\}\)</span>.  </li>
<li>Boundary/corner: <span class="arithmatex">\(N_C(x)\)</span> is a cone of outward normals.- Appears in first-order optimality conditions:
<script type="math/tex; mode=display">
0 \in \partial f(x^) + N_C(x^),
</script>
where the subgradient of <span class="arithmatex">\(f\)</span> is balanced by the “push-back” of constraints.</li>
</ul>
</li>
</ul>
<h2 id="convex-14_convexsets-47-supporting-hyperplanes-and-separation">4.7 Supporting hyperplanes and separation<a class="headerlink" href="#convex-14_convexsets-47-supporting-hyperplanes-and-separation" title="Permanent link">¶</a></h2>
<p>Convex sets can be “touched” or “separated” by hyperplanes.</p>
<h3 id="convex-14_convexsets-supporting-hyperplane-theorem">Supporting hyperplane theorem<a class="headerlink" href="#convex-14_convexsets-supporting-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be a nonempty closed convex set, and let <span class="arithmatex">\(x_0\)</span> be a boundary point of <span class="arithmatex">\(C\)</span>. Then there exists a nonzero <span class="arithmatex">\(a\)</span> such that
<script type="math/tex; mode=display">
a^\top x \le a^\top x_0 \quad \text{for all } x \in C~.
</script>
In words, there is a hyperplane <span class="arithmatex">\(a^\top x = a^\top x_0\)</span> that “supports” <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(x_0\)</span>: it touches <span class="arithmatex">\(C\)</span> but does not cut through it.</p>
<h3 id="convex-14_convexsets-separating-hyperplane-theorem">Separating hyperplane theorem<a class="headerlink" href="#convex-14_convexsets-separating-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(C\)</span> and <span class="arithmatex">\(D\)</span> are two disjoint nonempty convex sets, then there exists a hyperplane that separates them: some nonzero <span class="arithmatex">\(a\)</span> and scalar <span class="arithmatex">\(b\)</span> such that
<script type="math/tex; mode=display">
a^\top x \le b \quad \forall x \in C,
\qquad
a^\top y \ge b \quad \forall y \in D~.
</script>
</p>
<p>Why do we care?</p>
<ul>
<li>These theorems are the geometric heart of duality.</li>
<li>KKT conditions can be interpreted as existence of a supporting hyperplane that is simultaneously aligned with objective and constraints.</li>
<li>Subgradients of convex functions correspond to supporting hyperplanes of epigraphs.</li>
</ul>
<h2 id="convex-14_convexsets-47-feasible-regions-in-convex-optimisation">4.7 Feasible regions in convex optimisation<a class="headerlink" href="#convex-14_convexsets-47-feasible-regions-in-convex-optimisation" title="Permanent link">¶</a></h2>
<p>In convex optimisation, the feasible set is typically something like
<script type="math/tex; mode=display">
\{ x : g_i(x) \le 0,\ i=1,\dots,m,\ h_j(x)=0,\ j=1,\dots,p \}.
</script>
</p>
<ul>
<li>If each <span class="arithmatex">\(g_i\)</span> is convex and each <span class="arithmatex">\(h_j\)</span> is affine, then the feasible set is convex.</li>
<li>If <span class="arithmatex">\(f\)</span> is also convex, then the entire problem is a convex optimisation problem.</li>
</ul></body></html></section><section class="print-page" id="convex-15_convexfunctions" heading-number="2.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-convex-functions">Chapter 5: Convex Functions<a class="headerlink" href="#convex-15_convexfunctions-chapter-5-convex-functions" title="Permanent link">¶</a></h1>
<p>Convex functions are the objectives we minimise. Understanding them is essential, because convexity of the objective is what turns an optimisation problem from "possibly impossible" to "provably solvable".</p>
<h2 id="convex-15_convexfunctions-51-definitions-of-convexity">5.1 Definitions of convexity<a class="headerlink" href="#convex-15_convexfunctions-51-definitions-of-convexity" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is convex if for all <span class="arithmatex">\(x,y\)</span> in its domain and all <span class="arithmatex">\(\theta \in [0,1]\)</span>,</p>
<div class="arithmatex">\[
f(\theta x + (1-\theta) y)
\le
\theta f(x) + (1-\theta) f(y)~.
\]</div>
<p>If the inequality is strict whenever <span class="arithmatex">\(x \ne y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>, then <span class="arithmatex">\(f\)</span> is strictly convex.</p>
<p>Geometrically, the chord between two points on the graph of f lies above the graph itself.<br>
This means that f never “bends downward” — it has a single valley rather than multiple dips.<br>
Equivalently, the <strong>epigraph</strong> of f,</p>
<div class="arithmatex">\[
\mathrm{epi}(f) = \{ (x, t) \in \mathbb{R}^n \times \mathbb{R} : f(x) \le t \}.
\]</div>
<p>is a convex set.</p>
<h2 id="convex-15_convexfunctions-52-first-order-characterisation">5.2 First-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-52-first-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, then <span class="arithmatex">\(f\)</span> is convex if and only if</p>
<div class="arithmatex">\[
f(y) \ge f(x) + \nabla f(x)^\top (y - x)
\quad \text{for all } x,y.
\]</div>
<p>Interpretation:</p>
<ul>
<li>The first-order Taylor approximation is always a global underestimator.</li>
<li>The gradient at <span class="arithmatex">\(x\)</span> defines a supporting hyperplane to the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((x, f(x))\)</span>.</li>
</ul>
<p>This inequality is sometimes called the first-order condition for convexity.</p>
<blockquote>
<p>This is a powerful characterization: it says the tangent hyperplane at any point <span class="arithmatex">\(x\)</span> lies below the graph of <span class="arithmatex">\(f\)</span> everywhere. In other words, the gradient at <span class="arithmatex">\(x\)</span> provides a global underestimator of <span class="arithmatex">\(f\)</span> (supporting hyperplane to epigraph). Geometrically, this means no tangent line ever goes above the function. </p>
<p>For a convex differentiable <span class="arithmatex">\(f\)</span>, we have <span class="arithmatex">\(f(y) - f(x) \ge \nabla f(x)^T (y-x)\)</span>, so moving from <span class="arithmatex">\(x\)</span> in any direction, the actual increase in <span class="arithmatex">\(f\)</span> is at least as large as the linear prediction by <span class="arithmatex">\(\nabla f(x)\)</span> (since the function bends upward or straight). At optimum <span class="arithmatex">\(\hat{x}\)</span>, a necessary and sufficient condition (for convex differentiable <span class="arithmatex">\(f\)</span>) is <span class="arithmatex">\(\nabla f(\hat{x}) = 0\)</span>. This ties to optimality: <span class="arithmatex">\(\nabla f(\hat{x})=0\)</span> means <span class="arithmatex">\(f(y)\ge f(\hat{x}) + \nabla f(\hat{x})^T (y-\hat{x}) = f(\hat{x})\)</span> for all <span class="arithmatex">\(y\)</span>, so <span class="arithmatex">\(\hat{x}\)</span> is global minimizer.</p>
<p>If <span class="arithmatex">\(f\)</span> is not differentiable, a similar condition holds with subgradients (see next chapter): <span class="arithmatex">\(f\)</span> is convex iff for all <span class="arithmatex">\(x,y\)</span> there exists a (sub)gradient <span class="arithmatex">\(g \in \partial f(x)\)</span> such that <span class="arithmatex">\(f(y) \ge f(x) + g^T(y-x)\)</span>. The set of all subgradients <span class="arithmatex">\(\partial f(x)\)</span> is a convex set (the subdifferential). At optimum, <span class="arithmatex">\(0 \in \partial f(\hat{x})\)</span> is the condition. </p>
</blockquote>
<h2 id="convex-15_convexfunctions-53-second-order-characterisation">5.3 Second-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-53-second-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is twice continuously differentiable, then <span class="arithmatex">\(f\)</span> is convex if and only if its Hessian is positive semidefinite everywhere:</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0
\quad \text{for all } x~.
\]</div>
<p>If <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> for all <span class="arithmatex">\(x\)</span>, then <span class="arithmatex">\(f\)</span> is strictly convex.</p>
<h2 id="convex-15_convexfunctions-54-examples-of-convex-functions">5.4 Examples of convex functions<a class="headerlink" href="#convex-15_convexfunctions-54-examples-of-convex-functions" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Affine functions:<br>
<span class="arithmatex">\(f(x) = a^\top x + b\)</span>.<br>
   Always convex (and concave).</p>
</li>
<li>
<p>Quadratic functions with PSD Hessian:<br>
<span class="arithmatex">\(f(x) = \tfrac12 x^\top Q x + c^\top x + d\)</span>,<br>
   where <span class="arithmatex">\(Q \succeq 0\)</span> (symmetric positive semidefinite).<br>
   Convex because <span class="arithmatex">\(\nabla^2 f(x) = Q \succeq 0\)</span>.</p>
</li>
<li>
<p>Norms:<br>
<span class="arithmatex">\(f(x) = \|x\|\)</span> for any norm.<br>
   All norms are convex.</p>
</li>
<li>
<p>Maximum of affine functions:<br>
<span class="arithmatex">\(f(x) = \max_i (a_i^\top x + b_i)\)</span>.<br>
   Convex because it is the pointwise maximum of convex functions.</p>
</li>
<li>
<p>Log-sum-exp function:<br>
<span class="arithmatex">\(f(x) = \log \left( \sum_{i=1}^k \exp(a_i^\top x + b_i) \right)\)</span>.<br>
   This function is convex and is ubiquitous in statistics and machine learning (softmax, logistic regression). The convexity follows from Jensen’s inequality and properties of the exponential (Boyd and Vandenberghe, 2004).</p>
</li>
</ol>
<h2 id="convex-15_convexfunctions-55-jensens-inequality">5.5 Jensen’s inequality<a class="headerlink" href="#convex-15_convexfunctions-55-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex, and let <span class="arithmatex">\(X\)</span> be a random variable taking values in the domain of <span class="arithmatex">\(f\)</span>. Then
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]~.
</script>
</p>
<blockquote>
<p>the function value at the mean is always less than or equal to the mean of function values.</p>
</blockquote>
<p>This is Jensen’s inequality. It generalises the definition of convexity from two-point averages to arbitrary expectations. As a special case, for scalars <span class="arithmatex">\(x_1,\dots,x_n\)</span> and weights <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum_i \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<blockquote>
<p>Jensen’s inequality has many uses: in machine learning, it justifies algorithms like EM (which use the inequality to create surrogate objectives), and it provides bounds like <span class="arithmatex">\(\log(\mathbb{E}[e^X]) \ge \mathbb{E}[X]\)</span> (by convexity of <span class="arithmatex">\(\log\)</span> or <span class="arithmatex">\(e^x\)</span>). As a simple example, taking <span class="arithmatex">\(f(x)=x^2\)</span> and <span class="arithmatex">\(X\)</span> uniform in <span class="arithmatex">\({-1,1}\)</span>, Jensen says <span class="arithmatex">\((\mathbb{E}[X])^2 = 0^2 \le \mathbb{E}[X^2] = 1\)</span>, which is true. Or <span class="arithmatex">\(f(x)=\frac{1}{x}\)</span> convex on <span class="arithmatex">\((0,\infty)\)</span> implies <span class="arithmatex">\(\frac{1}{\mathbb{E}[X]} \le \mathbb{E}[\frac{1}{X}]\)</span> for positive <span class="arithmatex">\(X\)</span>. In optimization, Jensen’s inequality often helps in proving convexity of expectations: if you mix some distributions or uncertain inputs, the expected loss is convex if the loss function is convex.</p>
</blockquote>
<h2 id="convex-15_convexfunctions-56-operations-that-preserve-convexity">5.6 Operations that preserve convexity<a class="headerlink" href="#convex-15_convexfunctions-56-operations-that-preserve-convexity" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are convex, then:</p>
<ul>
<li><span class="arithmatex">\(f + g\)</span> is convex.</li>
<li><span class="arithmatex">\(\alpha f\)</span> is convex for any <span class="arithmatex">\(\alpha \ge 0\)</span>.</li>
<li><span class="arithmatex">\(\max\{f,g\}\)</span> is convex.</li>
<li>Composition with an affine map preserves convexity:<br>
  If <span class="arithmatex">\(A\)</span> is a matrix and <span class="arithmatex">\(b\)</span> a vector, then <span class="arithmatex">\(x \mapsto f(Ax + b)\)</span> is convex.</li>
</ul>
<p>If <span class="arithmatex">\(f\)</span> is convex and nondecreasing in each argument, and each <span class="arithmatex">\(g_i\)</span> is convex, then the composition <span class="arithmatex">\(x \mapsto f(g_1(x), \dots, g_k(x))\)</span> is convex. This helps you build new convex functions from known ones.</p>
<h2 id="convex-15_convexfunctions-57-level-sets-of-convex-functions">5.7 Level sets of convex functions<a class="headerlink" href="#convex-15_convexfunctions-57-level-sets-of-convex-functions" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(\alpha \in \mathbb{R}\)</span>, define the sublevel set
<script type="math/tex; mode=display">
\{ x : f(x) \le \alpha \}.
</script>
</p>
<blockquote>
<p>Geometrically, these are the “contour slices” of a convex bowl — cross-sections below certain heights.  </p>
</blockquote>
<p>If <span class="arithmatex">\(f\)</span> is convex, then every sublevel set is convex. This is crucial: constraints of the form <span class="arithmatex">\(f(x) \le \alpha\)</span> are convex constraints.</p>
<p>For example, the set
<script type="math/tex; mode=display">
\{ x : \|Ax - b\|_2 \le \epsilon \}
</script>
is convex because <span class="arithmatex">\(x \mapsto \|Ax-b\|_2\)</span> is convex.</p>
<h2 id="convex-15_convexfunctions-58-strict-and-strong-convexity">5.8 Strict and strong convexity<a class="headerlink" href="#convex-15_convexfunctions-58-strict-and-strong-convexity" title="Permanent link">¶</a></h2>
<ul>
<li><span class="arithmatex">\(f\)</span> is strictly convex if
<script type="math/tex; mode=display">
f(\theta x + (1-\theta) y) < \theta f(x) + (1-\theta) f(y)
</script>
for all <span class="arithmatex">\(x \ne y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>.</li>
</ul>
<blockquote>
<p>Strict convexity ensures uniqueness of the minimizer: there can be only one bottom to the bowl.</p>
</blockquote>
<ul>
<li><span class="arithmatex">\(f\)</span> is strongly convex with parameter <span class="arithmatex">\(m&gt;0\)</span> if
<script type="math/tex; mode=display">
f(y) \ge f(x) + \nabla f(x)^\top (y-x) + \frac{m}{2} \|y-x\|_2^2.
</script>
</li>
</ul>
<p>Strong convexity implies a unique minimiser and gives quantitative convergence rates for gradient methods.</p></body></html></section><section class="print-page" id="convex-16_subgradients" heading-number="2.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-nonsmooth-convex-optimization-subgradients">Chapter 6: Nonsmooth Convex Optimization – Subgradients<a class="headerlink" href="#convex-16_subgradients-chapter-6-nonsmooth-convex-optimization-subgradients" title="Permanent link">¶</a></h1>
<p>Many of the most important convex functions are not differentiable everywhere:</p>
<ul>
<li><span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span> has corners at <span class="arithmatex">\(x_i = 0\)</span>,</li>
<li><span class="arithmatex">\(f(x) = \max\{a_1^\top x + b_1, \dots, a_k^\top x + b_k\}\)</span> is piecewise affine,</li>
<li>the hinge loss <span class="arithmatex">\(\max\{0, 1 - y w^\top x\}\)</span> (used in SVMs) is not smooth at the kink.</li>
</ul>
<p>For a convex but nonsmooth <span class="arithmatex">\(f\)</span>, the usual condition “<span class="arithmatex">\(\nabla f(x^*) = 0\)</span>” may not make sense, because <span class="arithmatex">\(\nabla f(x^*)\)</span> may not exist. But geometrically, convex functions still have supporting hyperplanes at every point. That is the key.</p>
<h2 id="convex-16_subgradients-61-subgradients-and-the-subdifferential">6.1 Subgradients and the subdifferential<a class="headerlink" href="#convex-16_subgradients-61-subgradients-and-the-subdifferential" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> be convex. A vector <span class="arithmatex">\(g \in \mathbb{R}^n\)</span> is called a subgradient of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> if, for all <span class="arithmatex">\(y\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y - x).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The affine function <span class="arithmatex">\(y \mapsto f(x) + g^\top (y-x)\)</span> is a global underestimator of <span class="arithmatex">\(f\)</span>.</li>
<li><span class="arithmatex">\(g\)</span> defines a supporting hyperplane to the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((x,f(x))\)</span>.</li>
</ul>
<p>The set of all subgradients of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> is called the subdifferential of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
\partial f(x) = \{ g : f(y) \ge f(x) + g^\top (y-x) \ \forall y \}.
</script>
</p>
<p>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then
<script type="math/tex; mode=display">
\partial f(x) = \{ \nabla f(x) \}.
</script>
</p>
<p>If <span class="arithmatex">\(f\)</span> is not differentiable at <span class="arithmatex">\(x\)</span>, <span class="arithmatex">\(\partial f(x)\)</span> is typically a nonempty convex set.</p>
<p>Geometrically, each <span class="arithmatex">\(g \in \partial f(x)\)</span> defines a <em>supporting hyperplane</em> touching the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((x, f(x))\)</span>.  At smooth points, there is a single supporting hyperplane (the tangent); at corners, there are many possible supporting hyperplanes, forming a convex set of slopes.</p>
<p>Intuitively:</p>
<ul>
<li>Gradients describe “tilt” at smooth points.</li>
<li>Subgradients describe all possible valid tilts at kinks.</li>
</ul>
<h2 id="convex-16_subgradients-62-examples">6.2 Examples<a class="headerlink" href="#convex-16_subgradients-62-examples" title="Permanent link">¶</a></h2>
<h3 id="convex-16_subgradients-absolute-value-in-1d">Absolute value in 1D<a class="headerlink" href="#convex-16_subgradients-absolute-value-in-1d" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(f(t) = |t|\)</span>.</p>
<ul>
<li>For <span class="arithmatex">\(t&gt;0\)</span>, <span class="arithmatex">\(\partial f(t) = \{1\}\)</span>.</li>
<li>For <span class="arithmatex">\(t&lt;0\)</span>, <span class="arithmatex">\(\partial f(t) = \{-1\}\)</span>.</li>
<li>For <span class="arithmatex">\(t=0\)</span>, 
  <script type="math/tex; mode=display">
  \partial f(0) = [-1, 1].
  </script>
</li>
</ul>
<p>At the kink, any slope between <span class="arithmatex">\(-1\)</span> and <span class="arithmatex">\(1\)</span> is a valid supporting line from below.</p>
<h3 id="convex-16_subgradients-ell_1-norm"><span class="arithmatex">\(\ell_1\)</span> norm<a class="headerlink" href="#convex-16_subgradients-ell_1-norm" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(f(x) = \|x\|_1 = \sum_i |x_i|\)</span>, we have
<script type="math/tex; mode=display">
\partial \|x\|_1 = \{ g \in \mathbb{R}^n : g_i \in \partial |x_i| \}.
</script>
So</p>
<ul>
<li>if <span class="arithmatex">\(x_i &gt; 0\)</span>, then <span class="arithmatex">\(g_i = 1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i &lt; 0\)</span>, then <span class="arithmatex">\(g_i = -1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i = 0\)</span>, then <span class="arithmatex">\(g_i \in [-1,1]\)</span>.</li>
</ul>
<p>This is exactly what shows up in LASSO optimality conditions in statistics.</p>
<h3 id="convex-16_subgradients-pointwise-max-of-affine-functions">Pointwise max of affine functions<a class="headerlink" href="#convex-16_subgradients-pointwise-max-of-affine-functions" title="Permanent link">¶</a></h3>
<p>Let
<script type="math/tex; mode=display">
f(x) = \max_{i=1,\dots,k} (a_i^\top x + b_i).
</script>
</p>
<p>If a single index <span class="arithmatex">\(i^*\)</span> achieves the max at <span class="arithmatex">\(x\)</span>, then
<script type="math/tex; mode=display">
\partial f(x) = \{ a_{i^*} \}.
</script>
</p>
<p>If multiple <span class="arithmatex">\(i\)</span> are tied at the max, then
<script type="math/tex; mode=display">
\partial f(x) = \mathrm{conv}\{ a_i : i \text{ active at } x \},
</script>
the convex hull of all active slopes.</p>
<h2 id="convex-16_subgradients-63-subgradient-optimality-condition">6.3 Subgradient optimality condition<a class="headerlink" href="#convex-16_subgradients-63-subgradient-optimality-condition" title="Permanent link">¶</a></h2>
<p>Suppose we want to solve the unconstrained convex minimisation problem</p>
<div class="arithmatex">\[
\min_x f(x),
\]</div>
<p>Then a point <span class="arithmatex">\(x^*\)</span> is optimal if and only if</p>
<div class="arithmatex">\[
0 \in \partial f(x^*).
\]</div>
<p>The condition <span class="arithmatex">\(0\in\partial f(x^*)\)</span> means no subgradient points in a direction that can reduce <span class="arithmatex">\(f\)</span>.  In optimization geometry, this corresponds to the supporting hyperplane being <em>horizontal</em> at <span class="arithmatex">\(x^*\)</span> — the flat bottom of the convex bowl. In constrained problems, this generalizes to <span class="arithmatex">\(0\in\partial f(x^)+A^\top\lambda^\)</span>, the KKT stationarity condition (see Chapter 8).</p>
<h2 id="convex-16_subgradients-64-subgradient-calculus-useful-rules">6.4 Subgradient calculus (useful rules)<a class="headerlink" href="#convex-16_subgradients-64-subgradient-calculus-useful-rules" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are convex:</p>
<ul>
<li>
<p><span class="arithmatex">\(\partial (f+g)(x) \subseteq \partial f(x) + \partial g(x)\)</span>, i.e.
  <script type="math/tex; mode=display">
  \partial (f+g)(x)
  \subseteq
  \{ u+v : u \in \partial f(x),\ v \in \partial g(x) \}.
  </script>
</p>
</li>
<li>
<p>If <span class="arithmatex">\(A\)</span> is a matrix and <span class="arithmatex">\(h(x) = f(Ax)\)</span>, then
  <script type="math/tex; mode=display">
  \partial h(x) = A^\top \partial f(Ax).
  </script>
</p>
</li>
<li>
<p>If <span class="arithmatex">\(f(x) = \max_i f_i(x)\)</span> and each <span class="arithmatex">\(f_i\)</span> is convex, then
  <script type="math/tex; mode=display">
  \partial f(x) = \mathrm{conv}\{ \partial f_i(x) : i \text{ active at } x \}.
  </script>
</p>
</li>
</ul>
<p>These rules make it possible to compute subgradients of complicated nonsmooth objectives.</p>
<h2 id="convex-16_subgradients-65-subgradient-methods">6.5 Subgradient Methods<a class="headerlink" href="#convex-16_subgradients-65-subgradient-methods" title="Permanent link">¶</a></h2>
<p>Even though nonsmooth functions lack gradients, we can still minimize them using subgradient descent. Given a subgradient <span class="arithmatex">\(g_k \in \partial f(x_k)\)</span>, the iteration</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha_k g_k
\]</div>
<p>moves in the direction of the negative subgradient. Unlike in smooth optimization, the step sizes <span class="arithmatex">\(\alpha_k\)</span> typically decrease with <span class="arithmatex">\(k\)</span> (for example, <span class="arithmatex">\(\alpha_k = c / \sqrt{k}\)</span>) to guarantee convergence. Subgradient descent converges to the global minimum for convex <span class="arithmatex">\(f\)</span>, though at a slower rate than smooth gradient descent. While smooth convex functions enjoy <span class="arithmatex">\(\mathcal{O}(1/k^2)\)</span> or linear convergence under strong convexity, nonsmooth convex functions converge at rate <span class="arithmatex">\(\mathcal{O}(1/\sqrt{k})\)</span>. In practice, many machine learning algorithms—such as SVM training with hinge loss, <span class="arithmatex">\(\ell_1\)</span>-regularized models, and even certain deep learning optimizers—operate as subgradient methods in disguise. Their stability and robustness stem from convexity rather than smoothness.</p>
<h2 id="convex-16_subgradients-66-proximal-and-smoothed-alternatives">6.6  Proximal and Smoothed Alternatives<a class="headerlink" href="#convex-16_subgradients-66-proximal-and-smoothed-alternatives" title="Permanent link">¶</a></h2>
<p>When the subgradient method converges too slowly, we often smooth or separate the nonsmooth term.</p>
<ul>
<li>
<p>Proximal methods (Chapter 12) compute <span class="arithmatex">\(\mathrm{prox}_{\alpha f}(y)=\arg\min_x f(x)+\tfrac1{2\alpha}\|x-y\|^2\)</span> — this yields faster and more stable updates.</p>
</li>
<li>
<p>Smoothed approximations replace <span class="arithmatex">\(\max\)</span> or <span class="arithmatex">\(|t|\)</span> by smooth surrogates (e.g. Huber loss, softplus) retaining convexity but enabling gradient-based solvers.</p>
</li>
</ul>
<p>These bridges connect subgradient theory with modern first-order algorithms.</p></body></html></section><section class="print-page" id="convex-16a_optimality_conditions" heading-number="2.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-first-order-and-geometric-optimality-conditions">Chapter 7: First-Order and Geometric Optimality Conditions<a class="headerlink" href="#convex-16a_optimality_conditions-chapter-7-first-order-and-geometric-optimality-conditions" title="Permanent link">¶</a></h1>
<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions provide precise geometric and analytic criteria for such points to be optimal. They generalize the idea of “zero gradient” to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p>
<p>These conditions form the conceptual bridge between unconstrained minimization and the Karush–Kuhn–Tucker (KKT) theory developed in the next chapter.</p>
<h3 id="convex-16a_optimality_conditions-71-understanding-nth-order-optimality-conditions">7.1  Understanding “nth-Order” Optimality Conditions<a class="headerlink" href="#convex-16a_optimality_conditions-71-understanding-nth-order-optimality-conditions" title="Permanent link">¶</a></h3>
<p>For a differentiable function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, the “order’’ of an optimality condition refers to how many derivatives (or generalized derivatives) we inspect around a candidate minimizer <span class="arithmatex">\(x^*\)</span>:</p>
<table>
<thead>
<tr>
<th>Order</th>
<th>Uses</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-order</td>
<td><span class="arithmatex">\(\nabla f(x^*)\)</span> (or subgradients)</td>
<td>Checks whether any local descent direction exists</td>
</tr>
<tr>
<td>Second-order</td>
<td>Hessian <span class="arithmatex">\(\nabla^2 f(x^*)\)</span></td>
<td>Examines curvature to ensure the point is bowl-shaped (no local maxima or saddle)</td>
</tr>
<tr>
<td>Third and higher</td>
<td>Higher derivatives</td>
<td>Rarely used; detect flat or degenerate cases when curvature vanishes</td>
</tr>
</tbody>
</table>
<p>In general optimization, these successive tests ensure a point is truly a <em>local</em> minimizer. However, in convex optimization, this hierarchy collapses beautifully:</p>
<h3 id="convex-16a_optimality_conditions-why-only-first-order-conditions-matter-for-convex-functions">Why Only First-Order Conditions Matter for Convex Functions<a class="headerlink" href="#convex-16a_optimality_conditions-why-only-first-order-conditions-matter-for-convex-functions" title="Permanent link">¶</a></h3>
<p>A convex function already has non-negative curvature everywhere — its Hessian is automatically positive semidefinite wherever it exists:</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0, \quad \forall x.
\]</div>
<p>Therefore, once the first-order condition is satisfied, no direction can decrease <span class="arithmatex">\(f\)</span> — not locally, but globally.  Convexity guarantees that the landscape is bowl-shaped everywhere, so a stationary point is the unique global minimum. In contrast, nonconvex functions can have zero gradient points that are maxima or saddles; second- or higher-order checks are needed to tell them apart. Hence, convex optimization requires only the first-order condition — it captures both necessity and sufficiency for global optimality. This remarkable simplification is one of the reasons convex analysis is so powerful and elegant.</p>
<h2 id="convex-16a_optimality_conditions-72-motivation">7.2 Motivation<a class="headerlink" href="#convex-16a_optimality_conditions-72-motivation" title="Permanent link">¶</a></h2>
<p>Consider minimizing a convex function <span class="arithmatex">\(f\)</span> over a convex set <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x).
\]</div>
<p>Even for simple convex objectives, we need a way to check when a point <span class="arithmatex">\(\hat{x}\)</span> is optimal. In unconstrained problems, this means no direction can reduce <span class="arithmatex">\(f\)</span>.<br>
With constraints, we only consider feasible directions — those that stay inside <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<p>In both cases, optimality can be understood as an <em>equilibrium condition</em>:<br>
the gradient (or subgradient) of <span class="arithmatex">\(f\)</span> is balanced by the “forces’’ from the constraints. These equilibrium conditions are the first-order optimality conditions.</p>
<p>In machine learning, this reasoning appears everywhere — verifying when a trained model reaches stationarity (zero gradient) or when a sparsity constraint (like <span class="arithmatex">\(\ell_1\)</span> regularization) is active.</p>
<h2 id="convex-16a_optimality_conditions-73-unconstrained-convex-problems">7.3 Unconstrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-73-unconstrained-convex-problems" title="Permanent link">¶</a></h2>
<p>For the unconstrained problem</p>
<div class="arithmatex">\[
\min_x f(x),
\]</div>
<p>the first-order optimality condition is simple:</p>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, <span class="arithmatex">\(\hat{x}\)</span> is optimal if and only if</p>
<div class="arithmatex">\[
\nabla f(\hat{x}) = 0.
\]</div>
<p>If <span class="arithmatex">\(f\)</span> is convex but possibly nonsmooth, the gradient is replaced by the subdifferential:</p>
<div class="arithmatex">\[
0 \in \partial f(\hat{x}).
\]</div>
<p>Intuitively, this means that the origin lies inside the set of all subgradients at <span class="arithmatex">\(\hat{x}\)</span>.  Geometrically, every supporting hyperplane to the graph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((\hat{x}, f(\hat{x}))\)</span> has zero slope — the function cannot be decreased by moving in any direction.</p>
<p>For smooth functions, <span class="arithmatex">\(\nabla f(\hat{x}) = 0\)</span> means that the tangent plane is horizontal. For nonsmooth functions, <span class="arithmatex">\(0 \in \partial f(\hat{x})\)</span> means there exists at least one horizontal supporting plane among all possible tangents.</p>
<h2 id="convex-16a_optimality_conditions-74-constrained-convex-problems">7.4 Constrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-74-constrained-convex-problems" title="Permanent link">¶</a></h2>
<p>Now consider minimizing <span class="arithmatex">\(f(x)\)</span> over a closed convex set <span class="arithmatex">\(\mathcal{X} \subseteq \mathbb{R}^n\)</span>:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x).
\]</div>
<p>If <span class="arithmatex">\(\hat{x} \in \operatorname{int}(\mathcal{X})\)</span> (the interior), the situation is identical to the unconstrained case:
no boundary prevents motion, so</p>
<div class="arithmatex">\[
0 \in \partial f(\hat{x}).
\]</div>
<p>However, if <span class="arithmatex">\(\hat{x}\)</span> lies on the boundary of <span class="arithmatex">\(\mathcal{X}\)</span>, we must restrict movement to feasible directions — those that stay within the set.<br>
At such points, not all directions are allowed, and the gradient (or subgradient) may point outward from the feasible region.</p>
<h3 id="convex-16a_optimality_conditions-tangent-and-normal-cones">Tangent and Normal Cones<a class="headerlink" href="#convex-16a_optimality_conditions-tangent-and-normal-cones" title="Permanent link">¶</a></h3>
<p>At a point <span class="arithmatex">\(x \in \mathcal{X}\)</span>, define the tangent cone <span class="arithmatex">\(T_x(\mathcal{X})\)</span> as the set of feasible directions:</p>
<div class="arithmatex">\[
T_x(\mathcal{X}) = \{\, d : \exists\, t_k \downarrow 0,\; x + t_k d \in \mathcal{X} \,\}.
\]</div>
<p>It captures all directions in which one can move infinitesimally without leaving <span class="arithmatex">\(\mathcal{X}\)</span>.  </p>
<p>The normal cone is its polar:</p>
<div class="arithmatex">\[
N_x(\mathcal{X}) = \{\, v : v^\top d \le 0,\; \forall d \in T_x(\mathcal{X}) \,\}.
\]</div>
<p>The normal cone consists of vectors pointing <em>outward</em> from <span class="arithmatex">\(\mathcal{X}\)</span> — the directions orthogonal (or opposing) to every feasible direction.</p>
<p>Geometrically, at an optimal boundary point, the gradient of <span class="arithmatex">\(f\)</span> must lie inside the normal cone:<br>
if you try to move within <span class="arithmatex">\(\mathcal{X}\)</span> (inside the tangent cone), <span class="arithmatex">\(f\)</span> cannot decrease further.</p>
<h3 id="convex-16a_optimality_conditions-first-order-condition-with-constraints">First-Order Condition with Constraints<a class="headerlink" href="#convex-16a_optimality_conditions-first-order-condition-with-constraints" title="Permanent link">¶</a></h3>
<p>For constrained convex optimization, the unified first-order condition is</p>
<div class="arithmatex">\[
0 \in \partial f(\hat{x}) + N_{\mathcal{X}}(\hat{x}).
\]</div>
<p>This means that there exists a subgradient <span class="arithmatex">\(g \in \partial f(\hat{x})\)</span> and a normal vector <span class="arithmatex">\(v \in N_{\mathcal{X}}(\hat{x})\)</span> such that <span class="arithmatex">\(g + v = 0\)</span>.<br>
Equivalently, the objective’s slope is exactly counterbalanced by the constraint’s normal pressure.</p>
<p>If <span class="arithmatex">\(\hat{x}\)</span> lies in the interior of <span class="arithmatex">\(\mathcal{X}\)</span>, <span class="arithmatex">\(N_{\mathcal{X}}(\hat{x}) = \{0\}\)</span>, so the condition reduces to the unconstrained one (<span class="arithmatex">\(0 \in \partial f(\hat{x})\)</span>).<br>
If <span class="arithmatex">\(\hat{x}\)</span> is on the boundary, the constraint pushes back against the descent direction.</p>
<h3 id="convex-16a_optimality_conditions-geometric-interpretation">Geometric Interpretation<a class="headerlink" href="#convex-16a_optimality_conditions-geometric-interpretation" title="Permanent link">¶</a></h3>
<p>At optimality:</p>
<ul>
<li>The gradient (or a subgradient) points into the normal cone of the feasible set.  </li>
<li>The tangent cone defines all directions along which the function cannot decrease.  </li>
<li>The inclusion <span class="arithmatex">\(0 \in \partial f(\hat{x}) + N_{\mathcal{X}}(\hat{x})\)</span> encodes equilibrium between descent forces and boundary constraints.</li>
</ul>
<p>This picture generalizes the intuitive idea from single-variable calculus:<br>
the derivative changes sign at a minimum, while with constraints, the derivative at the boundary is balanced by the constraint’s barrier.</p></body></html></section><section class="print-page" id="convex-17_kkt" heading-number="2.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-8-lagrange-multipliers-and-kkt-framework">Chapter 8: Lagrange Multipliers and KKT Framework<a class="headerlink" href="#convex-17_kkt-chapter-8-lagrange-multipliers-and-kkt-framework" title="Permanent link">¶</a></h1>
<p>At this point we understand:</p>
<ul>
<li>how to recognise convex functions,</li>
<li>how to talk about feasible sets,</li>
<li>how to describe optimality with gradients or subgradients.</li>
</ul>
<p>This chapter unifies these ideas. We begin with unconstrained optimization and the gradient descent principle, then extend to equality and inequality constraints — culminating in the Karush–Kuhn–Tucker (KKT) conditions, the cornerstone of constrained convex optimization.</p>
<p>In constrained convex optimization, the gradient cannot vanish freely—it must be counteracted by constraint forces. Lagrange multipliers quantify these forces. The Karush–Kuhn–Tucker (KKT) conditions express this balance algebraically.</p>
<h2 id="convex-17_kkt-81-unconstrained-convex-minimisation">8.1 Unconstrained convex minimisation<a class="headerlink" href="#convex-17_kkt-81-unconstrained-convex-minimisation" title="Permanent link">¶</a></h2>
<p>Consider
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and differentiable.</p>
<p>Gradient descent is the iterative method:
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} - \alpha_k \nabla f(x^{(k)}),
</script>
for some step size <span class="arithmatex">\(\alpha_k &gt; 0\)</span>.</p>
<p>Intuition:</p>
<ul>
<li>Move opposite the gradient to reduce <span class="arithmatex">\(f\)</span>.</li>
<li>Under suitable conditions on step size (e.g. Lipschitz gradient), this converges to a global minimiser if <span class="arithmatex">\(f\)</span> is convex.</li>
</ul>
<p>If <span class="arithmatex">\(f\)</span> is strongly convex, we get uniqueness of the minimiser and faster convergence.</p>
<blockquote>
<p>In machine learning, this is the foundation of training via backpropagation — each step reduces the loss by following the negative gradient of the cost function with respect to model parameters.</p>
</blockquote>
<h2 id="convex-17_kkt-82-equality-constrained-optimisation-and-lagrange-multipliers">8.2 Equality-constrained optimisation and Lagrange multipliers<a class="headerlink" href="#convex-17_kkt-82-equality-constrained-optimisation-and-lagrange-multipliers" title="Permanent link">¶</a></h2>
<p>Now suppose we add equality constraints:</p>
<p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & h_j(x) = 0,\quad j=1,\dots,p,
\end{array}
</script>
where <span class="arithmatex">\(f\)</span> and each <span class="arithmatex">\(h_j\)</span> are differentiable.</p>
<p>We define the Lagrangian
<script type="math/tex; mode=display">
L(x,\lambda)
=
f(x) + \sum_{j=1}^p \lambda_j h_j(x),
</script>
where <span class="arithmatex">\(\lambda_j\)</span> are the Lagrange multipliers.</p>
<p>A necessary condition for <span class="arithmatex">\(x^*\)</span> to be optimal (under suitable regularity assumptions) is:</p>
<ol>
<li>Stationarity:
   <script type="math/tex; mode=display">
   \nabla_x L(x^*, \lambda^*) = 0
   \quad \Longleftrightarrow \quad
   \nabla f(x^*) + \sum_j \lambda_j^* \nabla h_j(x^*) = 0.
   </script>
</li>
<li>Primal feasibility:
   Primal feasibility simply means that the point <span class="arithmatex">\(x^*\)</span> satisfies all the original constraints of the optimization problem.</li>
</ol>
<p>
<script type="math/tex; mode=display">
   h_j(x^*) = 0 \quad \text{for all } j.
   </script>
</p>
<p>Geometrically, stationarity says: At an equality-constrained optimum, the gradient of <span class="arithmatex">\(f\)</span> is orthogonal to the feasible set — it points in a direction that cannot reduce <span class="arithmatex">\(f\)</span> without violating a constraint.</p>
<h2 id="convex-17_kkt-83-inequality-constraints-and-kkt">8.3 Inequality constraints and KKT<a class="headerlink" href="#convex-17_kkt-83-inequality-constraints-and-kkt" title="Permanent link">¶</a></h2>
<p>Now consider the general convex problem:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & g_i(x) \le 0,\quad i=1,\dots,m, \\
& h_j(x) = 0,\quad j=1,\dots,p.
\end{array}
</script>
</p>
<p>We form the Lagrangian
<script type="math/tex; mode=display">
L(x,\lambda,\mu)
=
f(x)
+ \sum_{j=1}^p \lambda_j h_j(x)
+ \sum_{i=1}^m \mu_i g_i(x),
</script>
with dual varibales <span class="arithmatex">\(\lambda \in \mathbb{R}^p\)</span> (unrestricted) and <span class="arithmatex">\(\mu \in \mathbb{R}^m\)</span> with <span class="arithmatex">\(\mu_i \ge 0\)</span>.</p>
<p>The Karush–Kuhn–Tucker (KKT) conditions consist of:</p>
<ol>
<li>
<p>Primal feasibility:
   <script type="math/tex; mode=display">
   g_i(x^*) \le 0,\quad i=1,\dots,m,
   \qquad
   h_j(x^*) = 0,\quad j=1,\dots,p.
   </script>
</p>
</li>
<li>
<p>Dual feasibility:
   <script type="math/tex; mode=display">
   \mu_i^* \ge 0,\quad i=1,\dots,m.
   </script>
</p>
</li>
</ol>
<blockquote>
<p>Dual feasibility says the “penalty coefficients” for inequality constraints can only push you inward (not reward you for violating constraints).</p>
</blockquote>
<ol>
<li>
<p>Stationarity:</p>
<p><span class="arithmatex">\(\nabla f(x^*) 
  + \sum_{j=1}^p \lambda_j^* \nabla h_j(x^*)
  + \sum_{i=1}^m \mu_i^* \nabla g_i(x^*)
  = 0\)</span></p>
</li>
<li>
<p>Complementary slackness:</p>
</li>
</ol>
<p>
<script type="math/tex; mode=display">
   \mu_i^* g_i(x^*) = 0
   \quad \text{for all } i.
   </script>
</p>
<p>Complementary slackness means:</p>
<ul>
<li>If a constraint <span class="arithmatex">\(g_i(x) \le 0\)</span> is strictly inactive at <span class="arithmatex">\(x^*\)</span> (i.e. <span class="arithmatex">\(g_i(x^*) &lt; 0\)</span>), then <span class="arithmatex">\(\mu_i^* = 0\)</span>.</li>
<li>If <span class="arithmatex">\(\mu_i^* &gt; 0\)</span>, then the constraint is tight: <span class="arithmatex">\(g_i(x^*) = 0\)</span>.</li>
</ul>
<p>This matches geometric intuition: only active constraints can “push back” on the optimiser.</p>
<h2 id="convex-17_kkt-84-slaters-condition-ensuring-strong-duality">8.4 Slater’s Condition – Ensuring Strong Duality<a class="headerlink" href="#convex-17_kkt-84-slaters-condition-ensuring-strong-duality" title="Permanent link">¶</a></h2>
<p>For the KKT conditions to not only hold but also guarantee optimality and zero duality gap, the problem must satisfy a regularity condition known as Slater’s condition.</p>
<h3 id="convex-17_kkt-definition">Definition<a class="headerlink" href="#convex-17_kkt-definition" title="Permanent link">¶</a></h3>
<p>For the convex problem above, if all <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g_i\)</span> are convex and all <span class="arithmatex">\(h_j\)</span> are affine, then Slater’s condition holds if there exists at least one strictly feasible point:</p>
<div class="arithmatex">\[
\exists\, x^{\text{slater}} \text{ such that } 
h_j(x^{\text{slater}}) = 0, \ \forall j, 
\quad \text{and} \quad 
g_i(x^{\text{slater}}) &lt; 0, \ \forall i.
\]</div>
<p>This means there is some point that satisfies the equalities exactly and the inequalities strictly — i.e., a point inside the feasible region, not merely on its boundary.</p>
<h3 id="convex-17_kkt-dual-problems-and-the-duality-gap">Dual Problems and the Duality Gap<a class="headerlink" href="#convex-17_kkt-dual-problems-and-the-duality-gap" title="Permanent link">¶</a></h3>
<p>Every constrained problem (the primal) has a dual:
<script type="math/tex; mode=display">
p^* = \min_x f(x), \qquad
d^* = \max_{\lambda,\mu\ge0} g(\lambda,\mu),
</script>
where <span class="arithmatex">\(g(\lambda,\mu)\)</span> is the dual function obtained from the Lagrangian.</p>
<p>By weak duality, for all feasible <span class="arithmatex">\(x,(\lambda,\mu)\)</span>,
<script type="math/tex; mode=display">
d^* \le p^*.
</script>
The difference<br>
<script type="math/tex; mode=display">
\text{duality gap} = p^* - d^*
</script>
measures how far apart the primal and dual optima are.</p>
<ul>
<li>If <span class="arithmatex">\(p^*&gt;d^*\)</span>, the gap is positive (weak duality only).  </li>
<li>If <span class="arithmatex">\(p^*=d^*\)</span>, we have strong duality — the primal and dual attain the same optimum.</li>
</ul>
<h3 id="convex-17_kkt-what-slaters-condition-guarantees">What Slater’s Condition Guarantees<a class="headerlink" href="#convex-17_kkt-what-slaters-condition-guarantees" title="Permanent link">¶</a></h3>
<p>For convex problems, Slater’s condition ensures:
1. Strong duality: <span class="arithmatex">\(p^*=d^*\)</span> (duality gap = 0).<br>
2. Dual attainment: finite <span class="arithmatex">\((\lambda^*,\mu^*)\)</span> exist.<br>
3. KKT conditions are necessary and sufficient for optimality.</p>
<p>Intuitively, Slater’s condition says the feasible region has “breathing room’’; it’s not pinched to the boundary.<br>
Then the dual hyperplanes can exactly touch the primal surface — they <em>kiss</em> at the optimum, eliminating the gap.</p>
<h3 id="convex-17_kkt-examples">Examples<a class="headerlink" href="#convex-17_kkt-examples" title="Permanent link">¶</a></h3>
<p>(a) Condition Holds → Zero Gap
<script type="math/tex; mode=display">
\min_x x^2 \quad \text{s.t. } x \ge 1.
</script>
Strictly feasible point <span class="arithmatex">\(x=2\)</span> satisfies <span class="arithmatex">\(g(x)=-1&lt;0\)</span>.<br>
Hence <span class="arithmatex">\(p^*=d^*=1\)</span> — no duality gap.</p>
<p>(b) Condition Fails → Positive Gap
<script type="math/tex; mode=display">
\min_x x \quad \text{s.t. } x^2 \le 0.
</script>
Feasible set <span class="arithmatex">\(\{0\}\)</span> has no interior; no strictly feasible point.<br>
Dual cannot attain equality: <span class="arithmatex">\(p^*&gt;d^*\)</span>.</p>
<h2 id="convex-17_kkt-85-geometric-and-physical-interpretation">8.5 Geometric and Physical Interpretation<a class="headerlink" href="#convex-17_kkt-85-geometric-and-physical-interpretation" title="Permanent link">¶</a></h2>
<ul>
<li>The gradient of the objective is balanced by the weighted gradients of the active constraints.  </li>
<li>Each multiplier <span class="arithmatex">\(\mu_i\)</span> or <span class="arithmatex">\(\lambda_j\)</span> acts like a <em>tension</em> or <em>shadow price</em> that enforces feasibility.</li>
<li>The KKT system generalizes the condition <span class="arithmatex">\(\nabla f(x^*) = 0\)</span>:</li>
<li>In the unconstrained case, there are no forces — pure gradient equilibrium.</li>
<li>With constraints, these forces push the solution back into the feasible region.</li>
</ul>
<blockquote>
<p>Physically, you can imagine optimization as minimizing potential energy subject to rigid walls (constraints).<br>
At equilibrium, the total force — gradient of <span class="arithmatex">\(f\)</span> plus constraint reactions — equals zero.
Convexity ensures the landscape is bowl-shaped.<br>
Slater’s condition ensures the bowl has interior volume so that primal and dual solutions coincide.<br>
Together they make the KKT framework both elegant and powerful — the foundation upon which Lagrange duality theory is built.</p>
</blockquote></body></html></section><section class="print-page" id="convex-18_duality" heading-number="2.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-9-lagrange-duality-theory">Chapter 9: Lagrange Duality Theory<a class="headerlink" href="#convex-18_duality-chapter-9-lagrange-duality-theory" title="Permanent link">¶</a></h1>
<p>Duality is one of the most beautiful and useful ideas in convex optimisation. Every constrained optimisation problem (the primal) has an associated dual problem. The dual problem:</p>
<ul>
<li>provides a lower bound on the optimal primal value,</li>
<li>often has structure that is easier to analyse,</li>
<li>gives certificates of optimality,</li>
<li>interprets multipliers as “prices” of constraints.</li>
</ul>
<p>In convex optimisation, under mild assumptions, the primal and dual optimal values are equal.</p>
<h2 id="convex-18_duality-91-the-primal-problem">9.1 The primal problem<a class="headerlink" href="#convex-18_duality-91-the-primal-problem" title="Permanent link">¶</a></h2>
<p>We consider the general problem:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & g_i(x) \le 0,\quad i=1,\dots,m, \\
& h_j(x) = 0,\quad j=1,\dots,p.
\end{array}
</script>
</p>
<p>Assume <span class="arithmatex">\(f\)</span> and the <span class="arithmatex">\(g_i\)</span> are convex, and <span class="arithmatex">\(h_j\)</span> are affine. This is a convex optimisation problem.</p>
<p>We call <span class="arithmatex">\(f^\star\)</span> the optimal value:
<script type="math/tex; mode=display">
f^\star = \inf \{ f(x) : g_i(x) \le 0,\ h_j(x) = 0 \}.
</script>
</p>
<p>Here, <em>infimum</em> means the smallest value <span class="arithmatex">\(f(x)\)</span> can approach — even if it is not exactly attained.</p>
<h2 id="convex-18_duality-92-why-duality">9.2 Why Duality?<a class="headerlink" href="#convex-18_duality-92-why-duality" title="Permanent link">¶</a></h2>
<p>Before any equations, let’s understand the high-level idea.</p>
<p>A constrained optimization problem can be viewed as a <em>trade-off</em>:</p>
<blockquote>
<p>Minimize <span class="arithmatex">\(f(x)\)</span> while paying penalties for violating constraints.</p>
</blockquote>
<p>If we allow violations but penalize them proportionally, we get a relaxed problem.<br>
How high should these penalties be? That’s what the dual variables <span class="arithmatex">\(\mu_i, \lambda_j\)</span> represent.</p>
<ul>
<li><span class="arithmatex">\(\mu_i\)</span> (for inequalities): how costly it is to violate constraint <span class="arithmatex">\(g_i(x) \le 0\)</span>.  </li>
<li><span class="arithmatex">\(\lambda_j\)</span> (for equalities): how much the objective changes when equality constraint <span class="arithmatex">\(h_j(x)=0\)</span> is relaxed.</li>
</ul>
<p>Thus, duality converts <em>constraints</em> into <em>prices</em> or <em>forces</em> that shape the optimization landscape.</p>
<h2 id="convex-18_duality-92-the-lagrangian">9.2 The Lagrangian<a class="headerlink" href="#convex-18_duality-92-the-lagrangian" title="Permanent link">¶</a></h2>
<p>The Lagrangian function incorporates both the objective and constraints:
<script type="math/tex; mode=display">
L(x, \lambda, \mu)
= f(x)
+ \sum_{i=1}^m \mu_i g_i(x)
+ \sum_{j=1}^p \lambda_j h_j(x),
</script>
with dual variables <span class="arithmatex">\(\mu \in \mathbb{R}^m\)</span>, <span class="arithmatex">\(\lambda \in \mathbb{R}^p\)</span>.</p>
<ul>
<li><span class="arithmatex">\(\mu_i \ge 0\)</span> are multipliers for inequality constraints,</li>
<li><span class="arithmatex">\(\lambda_j\)</span> are free (can be any sign) for equality constraints.</li>
</ul>
<p>If <span class="arithmatex">\(\mu_i &gt; 0\)</span>, violating the <span class="arithmatex">\(i\)</span>th constraint is penalized heavily; if <span class="arithmatex">\(\mu_i = 0\)</span>, that constraint is inactive.</p>
<h2 id="convex-18_duality-94-the-dual-function-lower-bounds-from-penalties">9.4 The Dual Function – Lower Bounds from Penalties<a class="headerlink" href="#convex-18_duality-94-the-dual-function-lower-bounds-from-penalties" title="Permanent link">¶</a></h2>
<p>For fixed <span class="arithmatex">\((\lambda, \mu)\)</span>, we define the dual function:
<script type="math/tex; mode=display">
\theta(\lambda, \mu) = \inf_x L(x, \lambda, \mu).
</script>
</p>
<p>Intuitively:</p>
<ul>
<li>We pick penalties <span class="arithmatex">\((\lambda, \mu)\)</span> for constraint violations.</li>
<li>We minimize <span class="arithmatex">\(L\)</span> with respect to <span class="arithmatex">\(x\)</span> — allowing constraint violations but paying for them.</li>
<li>The resulting value <span class="arithmatex">\(\theta(\lambda, \mu)\)</span> is a <em>lower bound</em> on the original problem’s optimum <span class="arithmatex">\(f^*\)</span>.</li>
</ul>
<p>Formally, for any feasible <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\(\mu \ge 0\)</span>,
<script type="math/tex; mode=display">
L(x, \lambda, \mu)
= f(x) + \sum_i \mu_i g_i(x) + \sum_j \lambda_j h_j(x)
\le f(x),
</script>
since <span class="arithmatex">\(g_i(x) \le 0\)</span> and <span class="arithmatex">\(\mu_i \ge 0\)</span>.<br>
Taking the infimum over all <span class="arithmatex">\(x\)</span> gives:
<script type="math/tex; mode=display">
\theta(\lambda, \mu) \le f^*.
</script>
</p>
<p>This property is known as weak duality:</p>
<blockquote>
<p>The dual function provides lower bounds on the primal optimum.</p>
</blockquote>
<h3 id="convex-18_duality-properties-of-the-dual-function">Properties of the Dual Function<a class="headerlink" href="#convex-18_duality-properties-of-the-dual-function" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\theta(\lambda, \mu)\)</span> is concave, even if <span class="arithmatex">\(f\)</span> itself is not convex.<br>
  (Infimum of affine functions in <span class="arithmatex">\((\lambda, \mu)\)</span> is concave.)</li>
<li>It may take the value <span class="arithmatex">\(-\infty\)</span> if the Lagrangian is unbounded below.</li>
</ul>
<p>The dual function defines a new optimization problem — the dual problem — where we maximize this lower bound.</p>
<h2 id="convex-18_duality-95-the-dual-problem">9.5 The Dual Problem<a class="headerlink" href="#convex-18_duality-95-the-dual-problem" title="Permanent link">¶</a></h2>
<p>We now <em>maximize</em> the dual function subject to <span class="arithmatex">\(\mu \ge 0\)</span>:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{maximize}_{\lambda, \mu} & \theta(\lambda, \mu) \\
\text{subject to} & \mu \ge 0.
\end{array}
</script>
</p>
<p>This is the Lagrange dual problem.</p>
<p>Let <span class="arithmatex">\(d^*\)</span> denote the optimal dual value.<br>
From weak duality, we always have:
<script type="math/tex; mode=display">
d^* \le f^*.
</script>
</p>
<p>The dual problem is always a concave maximization — equivalently, a convex optimization problem in the variables <span class="arithmatex">\((\lambda, \mu)\)</span>.</p>
<h2 id="convex-18_duality-96-strong-duality-and-the-zero-duality-gap">9.6 Strong Duality and the Zero Duality Gap<a class="headerlink" href="#convex-18_duality-96-strong-duality-and-the-zero-duality-gap" title="Permanent link">¶</a></h2>
<p>When <span class="arithmatex">\(d^* = f^*\)</span>, we say strong duality holds.<br>
The difference <span class="arithmatex">\(f^* - d^*\)</span> is called the duality gap.</p>
<ul>
<li>Weak duality: <span class="arithmatex">\(d^* \le f^*\)</span> (always true).  </li>
<li>Strong duality: <span class="arithmatex">\(d^* = f^*\)</span> (no gap).</li>
</ul>
<p>Strong duality means the dual gives <em>exactly the same value</em> as the primal — and optimal multipliers <span class="arithmatex">\((\lambda^*, \mu^*)\)</span> exist.</p>
<p>For convex problems, this beautiful property holds under Slater’s condition (see Chapter 8):</p>
<blockquote>
<p>If there exists a strictly feasible point <span class="arithmatex">\(\tilde{x}\)</span> such that<br>
<span class="arithmatex">\(g_i(\tilde{x}) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>, and <span class="arithmatex">\(h_j(\tilde{x}) = 0\)</span> for all <span class="arithmatex">\(j\)</span>,<br>
then strong duality holds — the duality gap is zero.</p>
</blockquote>
<p>Consequences:</p>
<ul>
<li><span class="arithmatex">\(f^* = d^*\)</span> (zero gap).  </li>
<li>Dual variables <span class="arithmatex">\((\lambda^*, \mu^*)\)</span> exist and are finite.  </li>
<li>The KKT conditions are both necessary and sufficient for optimality.</li>
</ul>
<h2 id="convex-18_duality-97-duality-and-the-kkt-conditions">9.7 Duality and the KKT Conditions<a class="headerlink" href="#convex-18_duality-97-duality-and-the-kkt-conditions" title="Permanent link">¶</a></h2>
<p>The KKT conditions (from Chapter 8) are the points where:</p>
<ol>
<li>The primal is feasible (<span class="arithmatex">\(g_i(x^*) \le 0\)</span>, <span class="arithmatex">\(h_j(x^*) = 0\)</span>),</li>
<li>The dual is feasible (<span class="arithmatex">\(\mu_i^* \ge 0\)</span>),</li>
<li>The gradients balance (stationarity):
   <script type="math/tex; mode=display">
   \nabla f(x^*) + \sum_i \mu_i^* \nabla g_i(x^*) + \sum_j \lambda_j^* \nabla h_j(x^*) = 0,
   </script>
</li>
<li>Complementary slackness holds (<span class="arithmatex">\(\mu_i^* g_i(x^*) = 0\)</span>).</li>
</ol>
<p>When these hold, <span class="arithmatex">\((x^*, \lambda^*, \mu^*)\)</span> is a primal–dual optimal pair and<br>
<script type="math/tex; mode=display">
f(x^*) = \theta(\lambda^*, \mu^*) = f^* = d^*.
</script>
</p>
<p>Geometrically, the primal and dual surfaces <em>touch</em> at the optimum — the tangent plane defined by <span class="arithmatex">\((\lambda^*, \mu^*)\)</span> supports <span class="arithmatex">\(f\)</span> exactly.</p>
<h2 id="convex-18_duality-98-interpreting-dual-variables">9.8 Interpreting Dual Variables<a class="headerlink" href="#convex-18_duality-98-interpreting-dual-variables" title="Permanent link">¶</a></h2>
<p>Dual variables have rich interpretations:</p>
<ul>
<li><span class="arithmatex">\(\mu_i^*\)</span>: the <em>shadow price</em> of relaxing inequality <span class="arithmatex">\(g_i(x) \le 0\)</span>.<br>
  A large <span class="arithmatex">\(\mu_i^*\)</span> means this constraint is expensive — small relaxation significantly reduces <span class="arithmatex">\(f\)</span>.</li>
<li><span class="arithmatex">\(\lambda_j^*\)</span>: the price or force associated with equality <span class="arithmatex">\(h_j(x)=0\)</span>.<br>
  Changing the equality’s right-hand side shifts the objective by roughly <span class="arithmatex">\(\lambda_j^*\)</span>.</li>
</ul>
<p>In economic or resource allocation problems:
- The dual problem represents <em>pricing</em> of limited resources.<br>
- The primal problem represents <em>allocation</em> given prices.</p>
<p>In machine learning:
- SVMs: dual variables correspond to support vectors.<br>
- Lasso and Elastic Net: <span class="arithmatex">\(\ell_1\)</span> penalties can be viewed as dual constraints on coefficient magnitudes.<br>
- Regularized losses: duality expresses the trade-off between data fit and model complexity.</p></body></html></section><section class="print-page" id="convex-18a_pareto" heading-number="2.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-10-pareto-optimality-and-multi-objective-convex-optimization">Chapter 10: Pareto Optimality and Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-chapter-10-pareto-optimality-and-multi-objective-convex-optimization" title="Permanent link">¶</a></h1>
<p>Optimization often focuses on a single objective function — minimizing one measure of performance. However, real-world problems rarely involve a single criterion. In practice, we must balance multiple conflicting goals: accuracy vs. complexity, fairness vs. utility, return vs. risk, etc.  </p>
<p>This chapter introduces Pareto optimality, which generalizes classical convex optimization to the multi-objective setting, and explores how scalarisation connects multi-objective problems to duality and regularisation.</p>
<h2 id="convex-18a_pareto-101-classical-optimality">10.1 Classical Optimality<a class="headerlink" href="#convex-18a_pareto-101-classical-optimality" title="Permanent link">¶</a></h2>
<p>In standard convex optimization, we minimize one convex function:
<script type="math/tex; mode=display">
x^* \in \arg\min_{x \in \mathcal{X}} f(x),
</script>
where <span class="arithmatex">\(\mathcal{X}\)</span> is a convex feasible set.</p>
<p>Optimality is <em>absolute</em>: there exists (or can exist) one best point minimizing a single criterion.<br>
But what happens when we must minimize <em>several</em> convex functions simultaneously?</p>
<h2 id="convex-18a_pareto-102-multi-objective-convex-optimization">10.2 Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-102-multi-objective-convex-optimization" title="Permanent link">¶</a></h2>
<p>In many learning and design problems, several objectives compete:</p>
<table>
<thead>
<tr>
<th>Application</th>
<th>Objective 1</th>
<th>Objective 2</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression</td>
<td>Fit error</td>
<td>Regularization</td>
<td>Accuracy vs complexity</td>
</tr>
<tr>
<td>Fair ML</td>
<td>Prediction loss</td>
<td>Fairness metric</td>
<td>Accuracy vs fairness</td>
</tr>
<tr>
<td>Portfolio design</td>
<td>Return</td>
<td>Risk</td>
<td>Profit vs stability</td>
</tr>
<tr>
<td>Information theory</td>
<td>Accuracy</td>
<td>Compression</td>
<td>Fit vs simplicity</td>
</tr>
</tbody>
</table>
<p>Formally:
<script type="math/tex; mode=display">
\min_{x \in \mathcal{X}} F(x) = (f_1(x), f_2(x), \dots, f_k(x)),
</script>
where each <span class="arithmatex">\(f_i(x)\)</span> is convex.<br>
Because no single <span class="arithmatex">\(x\)</span> minimizes all <span class="arithmatex">\(f_i\)</span> simultaneously, we define <em>optimality</em> differently — not as a single point, but as a set of <em>efficient trade-offs</em>.</p>
<h2 id="convex-18a_pareto-103-pareto-optimality">10.3 Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-103-pareto-optimality" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-a-strong-pareto-optimality">(a) Strong Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-a-strong-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^* \in \mathcal{X}\)</span> is Pareto optimal if no other <span class="arithmatex">\(x \in \mathcal{X}\)</span> satisfies:
<script type="math/tex; mode=display">
f_i(x) \le f_i(x^*) \quad \forall i,
</script>
with strict inequality for at least one <span class="arithmatex">\(j\)</span>.</p>
<p>Intuitively: no feasible solution can improve one objective without worsening another.</p>
<h3 id="convex-18a_pareto-b-weak-pareto-optimality">(b) Weak Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-b-weak-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^*\)</span> is weakly Pareto optimal if no <span class="arithmatex">\(x\)</span> satisfies:
<script type="math/tex; mode=display">
f_i(x) < f_i(x^*) \quad \forall i.
</script>
</p>
<p>That is, no feasible solution strictly improves all objectives simultaneously.</p>
<h3 id="convex-18a_pareto-c-geometric-intuition">(c) Geometric Intuition<a class="headerlink" href="#convex-18a_pareto-c-geometric-intuition" title="Permanent link">¶</a></h3>
<p>In two dimensions <span class="arithmatex">\((f_1, f_2)\)</span>, the Pareto frontier forms the <em>lower-left boundary</em> of the feasible region (for minimization):</p>
<ul>
<li>Points <em>on</em> the frontier are Pareto optimal — non-dominated.  </li>
<li>Points <em>above</em> or <em>inside</em> are dominated (inferior in all respects).</li>
</ul>
<blockquote>
<p>The frontier visualizes the fundamental trade-offs in the problem.</p>
</blockquote>
<h2 id="convex-18a_pareto-104-scalarisation-reducing-many-objectives-to-one">10.4 Scalarisation: Reducing Many Objectives to One<a class="headerlink" href="#convex-18a_pareto-104-scalarisation-reducing-many-objectives-to-one" title="Permanent link">¶</a></h2>
<p>Since multi-objective problems rarely have a unique minimizer, we often scalarise them — combine all objectives into a single composite scalar that we can minimize using standard methods.</p>
<h3 id="convex-18a_pareto-a-weighted-sum-scalarisation">(a) Weighted Sum Scalarisation<a class="headerlink" href="#convex-18a_pareto-a-weighted-sum-scalarisation" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} \; \sum_{i=1}^k w_i f_i(x),
\quad w_i \ge 0,\quad \sum_i w_i = 1.
\]</div>
<ul>
<li>The weights <span class="arithmatex">\(w_i\)</span> encode relative importance of objectives.</li>
<li>Each choice of <span class="arithmatex">\(w\)</span> yields a different point on the Pareto frontier.</li>
<li>Larger <span class="arithmatex">\(w_i\)</span> emphasizes objective <span class="arithmatex">\(f_i\)</span>.</li>
</ul>
<p>Convexity caveat:<br>
If the objectives and feasible set are convex, the weighted-sum method recovers the convex portion of the Pareto frontier. Nonconvex parts cannot be reached with simple weights.</p>
<h3 id="convex-18a_pareto-b-varepsilon-constraint-scalarisation">(b) <span class="arithmatex">\(\varepsilon\)</span>-Constraint Scalarisation<a class="headerlink" href="#convex-18a_pareto-b-varepsilon-constraint-scalarisation" title="Permanent link">¶</a></h3>
<p>Alternatively, minimize one objective while turning others into constraints:</p>
<div class="arithmatex">\[
\min_x f_1(x) \quad \text{s.t. } f_i(x) \le \varepsilon_i,\; i=2,\dots,k.
\]</div>
<ul>
<li>The tolerances <span class="arithmatex">\(\varepsilon_i\)</span> act as <em>performance budgets</em>.  </li>
<li>Varying them explores different Pareto-optimal trade-offs.  </li>
</ul>
<p>Example connection:<br>
Ridge regression minimizes fit error subject to a bound on model complexity:
<script type="math/tex; mode=display">
\min_x \|Ax - b\|_2^2 \quad \text{s.t. } \|x\|_2^2 \le \tau.
</script>
The Lagrangian form:
<script type="math/tex; mode=display">
\min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2
</script>
is a weighted-sum scalarisation — <span class="arithmatex">\(\lambda\)</span> acts as a trade-off parameter.</p>
<h3 id="convex-18a_pareto-c-duality-and-scalarisation">(c) Duality and Scalarisation<a class="headerlink" href="#convex-18a_pareto-c-duality-and-scalarisation" title="Permanent link">¶</a></h3>
<p>Scalarisation is closely related to duality (Chapter 9):</p>
<ul>
<li>The weights <span class="arithmatex">\(w_i\)</span> or Lagrange multipliers <span class="arithmatex">\(\lambda_i\)</span> act as dual variables.  </li>
<li>Changing them selects different Pareto-optimal points on the frontier.  </li>
<li>Regularisation parameters in ML (like <span class="arithmatex">\(\lambda\)</span>) are dual to constraint levels — they <em>move along</em> the Pareto frontier.</li>
</ul>
<h2 id="convex-18a_pareto-105-examples-and-applications">10.5 Examples and Applications<a class="headerlink" href="#convex-18a_pareto-105-examples-and-applications" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-example-1-regularized-least-squares">Example 1 – Regularized Least Squares<a class="headerlink" href="#convex-18a_pareto-example-1-regularized-least-squares" title="Permanent link">¶</a></h3>
<p>Objectives:
<script type="math/tex; mode=display">
f_1(x) = \|Ax - b\|_2^2, \qquad f_2(x) = \|x\|_2^2.
</script>
</p>
<p>Two equivalent forms:
1. Weighted sum:
   <script type="math/tex; mode=display">
   \min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
   </script>
2. <span class="arithmatex">\(\varepsilon\)</span>-constraint:
   <script type="math/tex; mode=display">
   \min_x \|Ax - b\|_2^2 \quad \text{s.t. } \|x\|_2^2 \le \tau.
   </script>
</p>
<p>Both yield Pareto-optimal solutions; <span class="arithmatex">\(\lambda\)</span> and <span class="arithmatex">\(\tau\)</span> trace the same curve — the bias–variance trade-off.</p>
<h3 id="convex-18a_pareto-example-2-portfolio-optimization-riskreturn">Example 2 – Portfolio Optimization (Risk–Return)<a class="headerlink" href="#convex-18a_pareto-example-2-portfolio-optimization-riskreturn" title="Permanent link">¶</a></h3>
<p>Decision variable: portfolio weights <span class="arithmatex">\(w \in \mathbb{R}^n\)</span>.<br>
Objectives:
<script type="math/tex; mode=display">
f_1(w) = -\mu^\top w \quad \text{(negative return)}, \qquad
f_2(w) = w^\top \Sigma w \quad \text{(risk)}.
</script>
</p>
<p>Weighted formulation:
<script type="math/tex; mode=display">
\min_w \; -\alpha \mu^\top w + (1 - \alpha) w^\top \Sigma w, \quad 0 \le \alpha \le 1.
</script>
</p>
<ul>
<li>Varying <span class="arithmatex">\(\alpha\)</span> traces the efficient frontier in risk–return space.</li>
<li>This forms the basis of Modern Portfolio Theory (Markowitz, 1952).</li>
</ul>
<h3 id="convex-18a_pareto-example-3-fairnessaccuracy-trade-off-in-ml">Example 3 – Fairness–Accuracy Trade-off in ML<a class="headerlink" href="#convex-18a_pareto-example-3-fairnessaccuracy-trade-off-in-ml" title="Permanent link">¶</a></h3>
<p>In fair machine learning, we minimize prediction loss while maintaining fairness:
<script type="math/tex; mode=display">
\min_\theta \; \mathbb{E}[\ell(y, f_\theta(x))] \quad \text{s.t. } \; D(f_\theta(x), y) \le \varepsilon,
</script>
where <span class="arithmatex">\(D\)</span> is a fairness measure (e.g., demographic disparity).</p>
<p>Equivalent scalarized form:
<script type="math/tex; mode=display">
\min_\theta \; \mathbb{E}[\ell(y, f_\theta(x))] + \lambda D(f_\theta(x), y).
</script>
Different <span class="arithmatex">\(\lambda\)</span> values trace the fairness–accuracy Pareto frontier.</p>
<h3 id="convex-18a_pareto-example-4-variational-autoencoders-and-beta-vae">Example 4 – Variational Autoencoders and <span class="arithmatex">\(\beta\)</span>-VAE<a class="headerlink" href="#convex-18a_pareto-example-4-variational-autoencoders-and-beta-vae" title="Permanent link">¶</a></h3>
<p>The ELBO in variational inference:
<script type="math/tex; mode=display">
\text{ELBO} = \mathbb{E}_{q(z)}[\log p(x|z)] - \mathrm{KL}(q(z)\|p(z)).
</script>
</p>
<p>Here we balance two objectives:
- Reconstruction accuracy (<span class="arithmatex">\(f_1\)</span>)
- Latent simplicity (<span class="arithmatex">\(f_2\)</span>)</p>
<p>The scalarized <span class="arithmatex">\(\beta\)</span>-VAE objective:
<script type="math/tex; mode=display">
\max_q \; \mathbb{E}_{q(z)}[\log p(x|z)] - \beta \, \mathrm{KL}(q(z)\|p(z)).
</script>
Parameter <span class="arithmatex">\(\beta\)</span> moves the solution along the Pareto frontier between data fidelity and disentanglement.</p></body></html></section><section class="print-page" id="convex-18b_regularization" heading-number="2.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-11-regularized-approximation-balancing-fit-and-complexity">Chapter 11: Regularized Approximation – Balancing Fit and Complexity<a class="headerlink" href="#convex-18b_regularization-chapter-11-regularized-approximation-balancing-fit-and-complexity" title="Permanent link">¶</a></h1>
<p>Many practical optimization problems involve a trade-off between fitting observed data and controlling model complexity.<br>
Regularization formalizes this trade-off as a convex optimization problem that balances these two competing goals.  </p>
<p>Building on Chapter 10 (Pareto Optimality), this chapter shows that regularized models correspond to specific points on a Pareto frontier between data fidelity and simplicity.  </p>
<h2 id="convex-18b_regularization-111-motivation-fit-vs-complexity">11.1 Motivation: Fit vs. Complexity<a class="headerlink" href="#convex-18b_regularization-111-motivation-fit-vs-complexity" title="Permanent link">¶</a></h2>
<p>When fitting a model, we want both:</p>
<ol>
<li>Data fidelity: minimize the loss or error <span class="arithmatex">\(f(x)\)</span>,  </li>
<li>Model simplicity: penalize unnecessary complexity <span class="arithmatex">\(R(x)\)</span>.</li>
</ol>
<p>This yields a bicriterion problem:
<script type="math/tex; mode=display">
\min_{x \in \mathbb{R}^n} (f(x), R(x)).
</script>
</p>
<p>Since improving both simultaneously is typically impossible, we form a scalarized problem:
<script type="math/tex; mode=display">
\min_x \; f(x) + \lambda R(x), \qquad \lambda > 0.
</script>
</p>
<ul>
<li><span class="arithmatex">\(f(x)\)</span> — data-fitting term (e.g. <span class="arithmatex">\(\|Ax-b\|_2^2\)</span>)  </li>
<li><span class="arithmatex">\(R(x)\)</span> — regularizer (e.g. <span class="arithmatex">\(\|x\|_1\)</span>, <span class="arithmatex">\(\|x\|_2^2\)</span>, total variation)  </li>
<li><span class="arithmatex">\(\lambda\)</span> — trade-off parameter controlling bias–variance or fit–complexity balance.</li>
</ul>
<p>Small <span class="arithmatex">\(\lambda\)</span> → better fit, possible overfitting.<br>
Large <span class="arithmatex">\(\lambda\)</span> → simpler, possibly underfit model.</p>
<h2 id="convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier">11.2 Bicriterion Optimization and the Pareto Frontier<a class="headerlink" href="#convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier" title="Permanent link">¶</a></h2>
<p>Regularization is a scalarised multi-objective problem (Chapter 10). A solution <span class="arithmatex">\(x^*\)</span> is Pareto optimal if no other feasible <span class="arithmatex">\(x\)</span> can reduce <span class="arithmatex">\(f(x)\)</span> without increasing <span class="arithmatex">\(R(x)\)</span>.</p>
<ul>
<li>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(R\)</span>, each <span class="arithmatex">\(\lambda \ge 0\)</span> yields a Pareto-optimal solution.  </li>
<li>The mapping between <span class="arithmatex">\(\lambda\)</span> (Lagrange multiplier) and constraint level <span class="arithmatex">\(R(x)\le t\)</span> is <strong>monotonic</strong>.  </li>
<li>Each <span class="arithmatex">\(\lambda\)</span> thus corresponds to one point on the <strong>fit–complexity frontier</strong>.</li>
</ul>
<p>If both <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(R\)</span> are convex, this frontier is also convex; otherwise, scalarisation may miss nonconvex trade-offs.</p>
<h2 id="convex-18b_regularization-113-why-keep-x-small">11.3 Why Keep <span class="arithmatex">\(x\)</span> Small?<a class="headerlink" href="#convex-18b_regularization-113-why-keep-x-small" title="Permanent link">¶</a></h2>
<p>Ill-posed or noisy inverse problems (<span class="arithmatex">\(Ax \approx b\)</span> with ill-conditioned <span class="arithmatex">\(A\)</span>) often admit many unstable solutions. If <span class="arithmatex">\(A\)</span> is ill-conditioned, small perturbations in <span class="arithmatex">\(b\)</span> cause large changes in <span class="arithmatex">\(x\)</span>. Regularization controls this instability by constraining the size or structure of <span class="arithmatex">\(x\)</span>.</p>
<p>Example: Ridge Regression
<script type="math/tex; mode=display">
\min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
</script>
The optimality condition (normal equations):
<script type="math/tex; mode=display">
(A^\top A + \lambda I)x = A^\top b.
</script>
</p>
<ul>
<li>The matrix <span class="arithmatex">\(A^\top A + \lambda I\)</span> is positive definite for <span class="arithmatex">\(\lambda&gt;0\)</span>.  </li>
<li>Even if <span class="arithmatex">\(A\)</span> is rank-deficient, the solution is unique and stable.  </li>
<li>Larger <span class="arithmatex">\(\lambda\)</span> improves conditioning but increases bias.</li>
</ul>
<blockquote>
<p>Interpretation: Regularization trades variance for stability — “smooths” the solution landscape and tames directions where data provides weak information.</p>
</blockquote>
<h2 id="convex-18b_regularization-114-constrained-and-lagrangian-forms">11.4 Constrained and Lagrangian Forms<a class="headerlink" href="#convex-18b_regularization-114-constrained-and-lagrangian-forms" title="Permanent link">¶</a></h2>
<p>Regularized problems can be equivalently written as constrained convex programs:
<script type="math/tex; mode=display">
\min_x f(x) \quad \text{s.t. } R(x) \le t.
</script>
</p>
<h3 id="convex-18b_regularization-lagrangian-formulation">Lagrangian Formulation<a class="headerlink" href="#convex-18b_regularization-lagrangian-formulation" title="Permanent link">¶</a></h3>
<p>The Lagrangian is
<script type="math/tex; mode=display">
\mathcal{L}(x,\lambda) = f(x) + \lambda (R(x)-t).
</script>
The associated penalized form
<script type="math/tex; mode=display">
\min_x f(x) + \lambda R(x)
</script>
corresponds to solving the constrained problem for some <span class="arithmatex">\(t&gt;0\)</span>.</p>
<p>Under convexity and Slater’s condition, the KKT conditions become:</p>
<div class="arithmatex">\[
0 \in \partial f(x^*) + \lambda^* \partial R(x^*), \quad
\lambda^* \ge 0, \quad
R(x^*) \le t, \quad
\lambda^*(R(x^*)-t)=0.
\]</div>
<ul>
<li>Penalized and constrained forms yield the same optimality structure.  </li>
<li>The mapping <span class="arithmatex">\(\lambda \leftrightarrow t\)</span> is monotonic but not bijective.  </li>
<li>Regularization parameters thus act as Lagrange multipliers, weighting one objective against another (Chapter 10).</li>
</ul>
<h2 id="convex-18b_regularization-115-common-regularizers">11.5 Common Regularizers<a class="headerlink" href="#convex-18b_regularization-115-common-regularizers" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-l2-regularization-ridge">(a) L2 Regularization (Ridge)<a class="headerlink" href="#convex-18b_regularization-a-l2-regularization-ridge" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
R(x)=\|x\|_2^2.
</script>
</p>
<ul>
<li>Smooth, strongly convex → unique minimizer.  </li>
<li>Shrinks coefficients uniformly; improves numerical conditioning.  </li>
<li>Bayesian view: corresponds to Gaussian prior <span class="arithmatex">\(x\sim \mathcal{N}(0,\tau^2I)\)</span>.</li>
</ul>
<h3 id="convex-18b_regularization-b-l1-regularization-lasso">(b) L1 Regularization (Lasso)<a class="headerlink" href="#convex-18b_regularization-b-l1-regularization-lasso" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
R(x)=\|x\|_1 = \sum_i |x_i|.
</script>
</p>
<ul>
<li>Convex but not smooth → promotes sparsity.  </li>
<li>The <span class="arithmatex">\(\ell_1\)</span> ball’s corners align with coordinate axes, leading to zeros in the solution.  </li>
<li>Proximal operator (soft-thresholding):
  <script type="math/tex">
  \operatorname{prox}_{\tau\|\cdot\|_1}(v)
  = \operatorname{sign}(v)\max(|v|-\tau,0).
  </script>
</li>
<li>Bayesian view: Laplace prior <span class="arithmatex">\(\sim e^{-|x_i|/\tau}\)</span>.</li>
</ul>
<h3 id="convex-18b_regularization-c-elastic-net">(c) Elastic Net<a class="headerlink" href="#convex-18b_regularization-c-elastic-net" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
R(x)=\alpha\|x\|_1+(1-\alpha)\|x\|_2^2.
</script>
</p>
<ul>
<li>Combines sparsity (L1) with stability (L2).  </li>
<li>Ensures uniqueness under correlated features.</li>
</ul>
<h3 id="convex-18b_regularization-d-beyond-l1l2">(d) Beyond L1/L2<a class="headerlink" href="#convex-18b_regularization-d-beyond-l1l2" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Regularizer</th>
<th>Definition</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>General Tikhonov</td>
<td><span class="arithmatex">\(R(x)=\|Lx\|_2^2\)</span></td>
<td>smoothness via linear operator <span class="arithmatex">\(L\)</span></td>
</tr>
<tr>
<td>Total Variation (TV)</td>
<td><span class="arithmatex">\(R(x)=\|\nabla x\|_1\)</span></td>
<td>piecewise-constant signals</td>
</tr>
<tr>
<td>Group Lasso</td>
<td><span class="arithmatex">\(R(x)=\sum_g \|x_g\|_2\)</span></td>
<td>structured sparsity</td>
</tr>
<tr>
<td>Nuclear Norm</td>
<td><span class="arithmatex">\(R(X)=\|X\|_* = \sum_i \sigma_i(X)\)</span></td>
<td>low-rank matrix recovery</td>
</tr>
</tbody>
</table>
<p>Each regularizer defines a distinct geometry — spheres, diamonds, polytopes — shaping the solution’s structure.</p>
<h2 id="convex-18b_regularization-116-choosing-the-regularization-parameter-lambda">11.6 Choosing the Regularization Parameter <span class="arithmatex">\(\lambda\)</span><a class="headerlink" href="#convex-18b_regularization-116-choosing-the-regularization-parameter-lambda" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-trade-off-behavior">(a) Trade-off Behavior<a class="headerlink" href="#convex-18b_regularization-a-trade-off-behavior" title="Permanent link">¶</a></h3>
<ul>
<li>Small <span class="arithmatex">\(\lambda\)</span> → high fit, high variance.  </li>
<li>Large <span class="arithmatex">\(\lambda\)</span> → smoother, more biased solution.<br>
<span class="arithmatex">\(\lambda\)</span> determines the location on the Pareto frontier.</li>
</ul>
<h3 id="convex-18b_regularization-b-cross-validation-cv">(b) Cross-Validation (CV)<a class="headerlink" href="#convex-18b_regularization-b-cross-validation-cv" title="Permanent link">¶</a></h3>
<p>Most common selection strategy:</p>
<ol>
<li>Split data into <span class="arithmatex">\(k\)</span> folds.  </li>
<li>Train on <span class="arithmatex">\(k-1\)</span> folds, validate on the remaining one.  </li>
<li>Average validation error, choose <span class="arithmatex">\(\lambda\)</span> minimizing it.</li>
</ol>
<p>Best practices</p>
<ul>
<li>Standardize features for L1/Elastic Net.  </li>
<li>For time series: use blocked or rolling CV.  </li>
<li>Use nested CV for fair model comparison.  </li>
<li>One-standard-error rule: choose simplest model within 1 SE of best error.</li>
</ul>
<h3 id="convex-18b_regularization-c-analytical-heuristic-alternatives">(c) Analytical / Heuristic Alternatives<a class="headerlink" href="#convex-18b_regularization-c-analytical-heuristic-alternatives" title="Permanent link">¶</a></h3>
<ul>
<li>Closed-form rules (ridge shrinkage factor).  </li>
<li>Information criteria (AIC/BIC for Lasso).  </li>
<li>Regularization paths: trace <span class="arithmatex">\(x^*(\lambda)\)</span> as <span class="arithmatex">\(\lambda\)</span> varies.  </li>
<li>Inverse problems: discrepancy principle or L-curve method.</li>
</ul>
<h2 id="convex-18b_regularization-117-algorithmic-perspective">11.7 Algorithmic Perspective<a class="headerlink" href="#convex-18b_regularization-117-algorithmic-perspective" title="Permanent link">¶</a></h2>
<p>Regularized convex problems typically take the form</p>
<p>
<script type="math/tex; mode=display">
\min_x f(x) + R(x),
</script>
where <span class="arithmatex">\(f\)</span> is smooth convex and <span class="arithmatex">\(R\)</span> convex, possibly nonsmooth.</p>
<p>Key algorithms:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Idea</th>
<th>Suitable For</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA/FISTA)</td>
<td>Gradient step on <span class="arithmatex">\(f\)</span>, prox step on <span class="arithmatex">\(R\)</span></td>
<td>L1, TV, nuclear norm</td>
</tr>
<tr>
<td>Coordinate Descent</td>
<td>Update one coordinate at a time</td>
<td>Lasso, Elastic Net</td>
</tr>
<tr>
<td>ADMM</td>
<td>Split <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(R\)</span> for parallel structure</td>
<td>Large-scale structured problems</td>
</tr>
</tbody>
</table>
<p>Proximal operators (Appendix G) handle the nonsmooth term efficiently:</p>
<ul>
<li>L2 → scaling (shrinkage)  </li>
<li>L1 → soft-thresholding  </li>
<li>TV/Nuclear → more advanced proximal maps</li>
</ul>
<h2 id="convex-18b_regularization-118-bayesian-interpretation">11.8 Bayesian Interpretation<a class="headerlink" href="#convex-18b_regularization-118-bayesian-interpretation" title="Permanent link">¶</a></h2>
<p>Regularization corresponds to MAP estimation in probabilistic models.</p>
<p>Given the linear model:
<script type="math/tex; mode=display">
b = A x + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,\sigma^2 I),
</script>
and prior <span class="arithmatex">\(x \sim \mathcal{N}(0,\tau^2 I)\)</span>,<br>
the MAP estimator is:
<script type="math/tex; mode=display">
\min_x \frac{1}{2\sigma^2}\|Ax - b\|_2^2 + \frac{1}{2\tau^2}\|x\|_2^2.
</script>
</p>
<p>The connection:
<script type="math/tex; mode=display">
\lambda = \frac{\sigma^2}{2\tau^2}.
</script>
</p>
<ul>
<li>Gaussian prior → L2 penalty (ridge).  </li>
<li>Laplace prior → L1 penalty (sparse MAP estimate).  </li>
</ul>
<p>Thus, regularization = prior knowledge: it encodes our beliefs about what solutions are likely before seeing data.</p></body></html></section><section class="print-page" id="convex-19_optimizationalgo" heading-number="2.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-12-algorithms-for-convex-optimization">Chapter 12: Algorithms for Convex Optimization<a class="headerlink" href="#convex-19_optimizationalgo-chapter-12-algorithms-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>In the previous chapters, we built the mathematical foundations of convex optimization — convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p>
<p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3–11) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept — gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13–15) extend these ideas to constrained, stochastic, and large-scale environments.</p>
<h2 id="convex-19_optimizationalgo-121-problem-classes-vs-method-classes">12.1 Problem classes vs method classes<a class="headerlink" href="#convex-19_optimizationalgo-121-problem-classes-vs-method-classes" title="Permanent link">¶</a></h2>
<p>Different convex problems call for different algorithmic structures.<br>
Here is the broad landscape:</p>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Formulation</th>
<th>Representative Methods</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth, unconstrained</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span>, convex and differentiable</td>
<td>Gradient descent, Accelerated gradient, Newton</td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Smooth with simple constraints</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span> s.t. <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex)</td>
<td>Projected gradient</td>
<td>Constrained regression, probability simplex</td>
</tr>
<tr>
<td>Composite convex (smooth + nonsmooth)</td>
<td><span class="arithmatex">\(\min_x f(x) + R(x)\)</span></td>
<td>Proximal gradient, coordinate descent</td>
<td>Lasso, Elastic Net, TV minimization</td>
</tr>
<tr>
<td>General constrained convex</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(g_i(x) \le 0, h_j(x)=0\)</span></td>
<td>Interior-point, primal–dual methods</td>
<td>LP, QP, SDP, SOCP</td>
</tr>
</tbody>
</table>
<h2 id="convex-19_optimizationalgo-122-first-order-methods-gradient-descent">12.2 First-order methods: Gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-122-first-order-methods-gradient-descent" title="Permanent link">¶</a></h2>
<h3 id="convex-19_optimizationalgo-1221-setting">12.2.1 Setting<a class="headerlink" href="#convex-19_optimizationalgo-1221-setting" title="Permanent link">¶</a></h3>
<p>We solve
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex, differentiable, and (ideally) <span class="arithmatex">\(L\)</span>-smooth: its gradient is Lipschitz with constant <span class="arithmatex">\(L\)</span>, meaning
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\|_2 \le L \|x-y\|_2 \quad \text{for all } x,y.
</script>
Smoothness lets us control step sizes.</p>
<h3 id="convex-19_optimizationalgo-1222-algorithm">12.2.2 Algorithm<a class="headerlink" href="#convex-19_optimizationalgo-1222-algorithm" title="Permanent link">¶</a></h3>
<p>Gradient descent iterates
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k),
</script>
where <span class="arithmatex">\(\alpha_k&gt;0\)</span> is the step size (also called learning rate in machine learning). A common choice is a constant <span class="arithmatex">\(\alpha_k = 1/L\)</span> when <span class="arithmatex">\(L\)</span> is known, or a backtracking line search when it is not.</p>
<blockquote>
<p>Derivation: Around <span class="arithmatex">\(x_t\)</span>, we approximate <span class="arithmatex">\(f\)</span> using its Taylor expansion:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle.
\]</div>
<ul>
<li>We assume <span class="arithmatex">\(f\)</span> behaves approximately like its tangent plane near <span class="arithmatex">\(x_t\)</span>.  </li>
<li>If we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent <span class="arithmatex">\(-\nabla f(x_t)\)</span>, which is not realistic or stable.</li>
</ul>
<p>This motivates adding a locality restriction — we trust the linear approximation near <span class="arithmatex">\(x_t\)</span>, not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from <span class="arithmatex">\(x_t\)</span>:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2,
\]</div>
<p>where <span class="arithmatex">\(\eta &gt; 0\)</span> is the learning rate or step size.</p>
<ul>
<li>The linear term pulls <span class="arithmatex">\(x\)</span> in the steepest descent direction.</li>
<li>The quadratic term acts like a trust region, discouraging large deviations from <span class="arithmatex">\(x_t\)</span>.</li>
<li><span class="arithmatex">\(\eta\)</span> trades off aggressive progress vs stability:<ul>
<li>Small <span class="arithmatex">\(\eta\)</span> → cautious updates.</li>
<li>Large <span class="arithmatex">\(\eta\)</span> → bold updates (risk of divergence).</li>
</ul>
</li>
</ul>
<p>We define the next iterate as the minimizer of the surrogate objective:</p>
<div class="arithmatex">\[
x_{t+1} = \arg\min_{x \in \mathcal{X}} \Big[ f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2 \Big].
\]</div>
<p>Ignoring the constant term <span class="arithmatex">\(f(x_t)\)</span> and differentiating w.r.t. <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
\nabla f(x_t) + \frac{1}{\eta}(x - x_t) = 0
\]</div>
<p>Solving:</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t)
\]</div>
</blockquote>
<h3 id="convex-19_optimizationalgo-1223-geometric-meaning">12.2.3 Geometric meaning<a class="headerlink" href="#convex-19_optimizationalgo-1223-geometric-meaning" title="Permanent link">¶</a></h3>
<p>From Chapter 3, the first-order Taylor model is
<script type="math/tex; mode=display">
f(x + d) \approx f(x) + \nabla f(x)^\top d.
</script>
This is minimised (under a step length constraint) by taking <span class="arithmatex">\(d\)</span> in the direction <span class="arithmatex">\(-\nabla f(x)\)</span>. So gradient descent is just “take a cautious step downhill”.</p>
<h3 id="convex-19_optimizationalgo-1224-convergence">12.2.4 Convergence<a class="headerlink" href="#convex-19_optimizationalgo-1224-convergence" title="Permanent link">¶</a></h3>
<p>For convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(f\)</span>, gradient descent with a suitable fixed step size satisfies
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k}\right),
</script>
where <span class="arithmatex">\(f^\star\)</span> is the global minimum. This <span class="arithmatex">\(O(1/k)\)</span> sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need <span class="arithmatex">\(\nabla f(x_k)\)</span>.</p>
<h3 id="convex-19_optimizationalgo-1225-when-to-use-gradient-descent">12.2.5 When to use gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-1225-when-to-use-gradient-descent" title="Permanent link">¶</a></h3>
<ul>
<li>Problems with millions of variables (large-scale ML).</li>
<li>You can afford many cheap iterations.</li>
<li>You only have access to gradients (or stochastic gradients).</li>
<li>You do not need very high precision.</li>
</ul>
<p>Gradient descent is the baseline first-order method. But we can do better.</p>
<h2 id="convex-19_optimizationalgo-123-accelerated-first-order-methods">12.3 Accelerated first-order methods<a class="headerlink" href="#convex-19_optimizationalgo-123-accelerated-first-order-methods" title="Permanent link">¶</a></h2>
<p>Plain gradient descent has an <span class="arithmatex">\(O(1/k)\)</span> rate for smooth convex problems. Remarkably, we can do better — and in fact, provably optimal — by adding <em>momentum</em>.</p>
<h3 id="convex-19_optimizationalgo-1231-nesterov-acceleration">12.3.1 Nesterov acceleration<a class="headerlink" href="#convex-19_optimizationalgo-1231-nesterov-acceleration" title="Permanent link">¶</a></h3>
<p>Nesterov’s accelerated gradient method modifies the update using a momentum-like extrapolation. One common presentation is:</p>
<ol>
<li>Maintain two sequences <span class="arithmatex">\(x_k\)</span> and <span class="arithmatex">\(y_k\)</span>.</li>
<li>Take a gradient step from <span class="arithmatex">\(y_k\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = y_k - \alpha \nabla f(y_k).
   </script>
</li>
<li>Extrapolate:
   <script type="math/tex; mode=display">
   y_{k+1} = x_{k+1} + \beta_k (x_{k+1} - x_k).
   </script>
</li>
</ol>
<p>The extra <span class="arithmatex">\(\beta_k\)</span> term “looks ahead,” helping the method exploit curvature better than plain gradient descent.</p>
<h3 id="convex-19_optimizationalgo-1232-optimal-first-order-rate">12.3.2 Optimal first-order rate<a class="headerlink" href="#convex-19_optimizationalgo-1232-optimal-first-order-rate" title="Permanent link">¶</a></h3>
<p>For smooth convex <span class="arithmatex">\(f\)</span>, accelerated gradient achieves
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k^2}\right),
</script>
which is <em>optimal</em> for any algorithm that uses only gradient information and not higher derivatives. In other words, you cannot beat <span class="arithmatex">\(O(1/k^2)\)</span> in the worst case using only first-order oracle calls.</p>
<h3 id="convex-19_optimizationalgo-1233-when-to-use-acceleration">12.3.3 When to use acceleration<a class="headerlink" href="#convex-19_optimizationalgo-1233-when-to-use-acceleration" title="Permanent link">¶</a></h3>
<ul>
<li>Same setting as gradient descent (large-scale smooth convex problems),</li>
<li>but you want to converge in fewer iterations.</li>
<li>You can tolerate a little more instability/parameter tuning (acceleration can overshoot if step sizes are not chosen carefully).</li>
</ul>
<p>Acceleration is the default upgrade from vanilla gradient descent in many smooth convex machine learning problems.</p>
<p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned—that is, highly anisotropic or elongated (not spherical)—the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence.</p>
<p>In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>
<h2 id="convex-19_optimizationalgo-124-steepest-descent-method">12.4 Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-124-steepest-descent-method" title="Permanent link">¶</a></h2>
<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of <em>maximum decrease</em> of the objective function under a unit norm constraint.</p>
<blockquote>
<p>The norm defines the “geometry” of optimization.
Gradient descent is steepest descent under the Euclidean norm.
Changing the norm changes what “steepest” means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.</p>
</blockquote>
<p>At a point <span class="arithmatex">\(x\)</span>, and for a chosen norm <span class="arithmatex">\(|\cdot|\)</span>:</p>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = \arg\min_{|v| = 1} \nabla f(x)^T v
\]</div>
<p>This defines the normalized steepest descent direction — the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of <span class="arithmatex">\(f\)</span>).</p>
<ul>
<li><span class="arithmatex">\(\Delta x_{\text{nsd}}\)</span>: normalized steepest descent direction</li>
<li><span class="arithmatex">\(\Delta x_{\text{sd}}\)</span>: unnormalized direction (scaled by the gradient norm)</li>
</ul>
<p>For small steps <span class="arithmatex">\(v\)</span>,
<script type="math/tex; mode=display">
f(x + v) \approx f(x) + \nabla f(x)^T v.
</script>
The term <span class="arithmatex">\(\nabla f(x)^T v\)</span> describes how fast <span class="arithmatex">\(f\)</span> increases in direction <span class="arithmatex">\(v\)</span>.
To decrease <span class="arithmatex">\(f\)</span> most rapidly, we pick <span class="arithmatex">\(v\)</span> that minimizes this inner product — subject to <span class="arithmatex">\(|v| = 1\)</span>.</p>
<ul>
<li>The result depends on which norm we use to measure the “size” of <span class="arithmatex">\(v\)</span>.</li>
<li>The corresponding dual norm <span class="arithmatex">\(|\cdot|_*\)</span> determines how we measure the gradient’s magnitude.</li>
</ul>
<p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>
<h2 id="convex-19_optimizationalgo-1241-mathematical-properties">12.4.1. Mathematical Properties<a class="headerlink" href="#convex-19_optimizationalgo-1241-mathematical-properties" title="Permanent link">¶</a></h2>
<h3 id="convex-19_optimizationalgo-a-normalized-direction">(a) Normalized direction<a class="headerlink" href="#convex-19_optimizationalgo-a-normalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{nsd}} = \arg\min_{|v|=1} \nabla f(x)^T v
</script>
→ unit vector with the most negative directional derivative.</p>
<h3 id="convex-19_optimizationalgo-b-unnormalized-direction">(b) Unnormalized direction<a class="headerlink" href="#convex-19_optimizationalgo-b-unnormalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{sd}} = |\nabla f(x)| , \Delta x*{\text{nsd}}
</script>
This gives the actual direction and magnitude used in updates.</p>
<h3 id="convex-19_optimizationalgo-c-key-identity">(c) Key identity<a class="headerlink" href="#convex-19_optimizationalgo-c-key-identity" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\nabla f(x)^T \Delta x_{\text{sd}} = -|\nabla f(x)|_*^2
</script>
The directional derivative equals the negative squared dual norm of the gradient.</p>
<h3 id="convex-19_optimizationalgo-1242-the-steepest-descent-method">12.4.2. The Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-1242-the-steepest-descent-method" title="Permanent link">¶</a></h3>
<p>The iterative update rule is:
<script type="math/tex; mode=display">
x_{k+1} = x_k + t_k , \Delta x_{\text{sd}},
</script>
where <span class="arithmatex">\(t_k &gt; 0\)</span> is a step size (from line search or a fixed rule).</p>
<ul>
<li>For the Euclidean norm, this reduces to ordinary gradient descent.</li>
<li>For other norms, it adapts the search direction to the geometry of the problem.</li>
</ul>
<p>Convergence: Similar to gradient descent — linear for general convex functions, potentially faster when level sets are well-conditioned.</p>
<h3 id="convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence">12.4.3. Role of the Norm and Its Influence<a class="headerlink" href="#convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence" title="Permanent link">¶</a></h3>
<p>The choice of norm determines:</p>
<ol>
<li>The shape of the unit ball <span class="arithmatex">\({v : |v| \le 1}\)</span>,</li>
<li>The direction of steepest descent, since the minimization is constrained by that shape,</li>
<li>The dual norm <span class="arithmatex">\(|\nabla f(x)|_*\)</span> that measures the gradient’s size.</li>
</ol>
<p>Different norms yield different “geometries” of descent:</p>
<table>
<thead>
<tr>
<th>Norm</th>
<th>Unit Ball Shape</th>
<th>Dual Norm</th>
<th>Effect on Direction</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Circle / sphere</td>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Direction is opposite to gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Diamond</td>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Moves along coordinate of largest gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Square</td>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Moves opposite to sum of all gradient signs</td>
</tr>
<tr>
<td>Quadratic <span class="arithmatex">\((x^T P x)^{1/2}\)</span></td>
<td>Ellipsoid</td>
<td>Weighted <span class="arithmatex">\(\ell_2\)</span></td>
<td>Scales direction by preconditioner <span class="arithmatex">\(P^{-1}\)</span></td>
</tr>
</tbody>
</table>
<p>Thus, the norm defines how “distance” and “steepness” are perceived, shaping how the algorithm moves through the landscape of <span class="arithmatex">\(f(x)\)</span>.</p>
<h3 id="convex-19_optimizationalgo-a-euclidean-norm-v_2">(a) Euclidean Norm <span class="arithmatex">\(|v|_2\)</span><a class="headerlink" href="#convex-19_optimizationalgo-a-euclidean-norm-v_2" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -\frac{\nabla f(x)}{|\nabla f(x)|*2},
\quad
\Delta x*{\text{sd}} = -\nabla f(x)
\]</div>
<p>This is standard gradient descent.
The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>
<h3 id="convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0">(b) Quadratic Norm <span class="arithmatex">\(|v|_P = (v^T P v)^{1/2}\)</span>, with <span class="arithmatex">\(P \succ 0\)</span><a class="headerlink" href="#convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0" title="Permanent link">¶</a></h3>
<p>Here, <span class="arithmatex">\(P\)</span> defines an ellipsoidal metric.
The dual norm is <span class="arithmatex">\(|y|_* = (y^T P^{-1} y)^{1/2}\)</span>.</p>
<div class="arithmatex">\[
\Delta x_{\text{sd}} = -P^{-1}\nabla f(x)
\]</div>
<p>This corresponds to preconditioned gradient descent, where <span class="arithmatex">\(P\)</span> rescales directions to counter anisotropy in level sets.</p>
<p>Interpretation:</p>
<ul>
<li>If <span class="arithmatex">\(P\)</span> approximates the Hessian, this becomes Newton’s method.</li>
<li>If <span class="arithmatex">\(P\)</span> is diagonal, it acts like an adaptive step size per coordinate.</li>
</ul>
<h3 id="convex-19_optimizationalgo-c-ell_1-norm">(c) <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#convex-19_optimizationalgo-c-ell_1-norm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -e_i, \quad i = \arg\max_j \left|\frac{\partial f}{\partial x_j}\right|
$$
and
$$
\Delta x_{\text{sd}} = -|\nabla f(x)|_\infty e_i
\]</div>
<p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p>
<p>Geometric intuition:
The <span class="arithmatex">\(\ell_1\)</span>-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p>
<ul>
<li>In <span class="arithmatex">\(\ell_2\)</span>-norm: the unit ball is a circle → the steepest direction is exactly opposite the gradient.</li>
<li>In <span class="arithmatex">\(\ell_1\)</span>-norm: the unit ball is a diamond → the steepest direction points to a corner (one coordinate).</li>
<li>In quadratic norms: the unit ball is an ellipsoid → the steepest direction follows the metric-adjusted gradient.</li>
</ul>
<p>Hence, the norm defines the geometry of what “steepest” means.</p>
<h2 id="convex-19_optimizationalgo-125-conjugate-gradient-method-efficient-optimization-for-quadratic-objectives">12.5 Conjugate Gradient Method — Efficient Optimization for Quadratic Objectives<a class="headerlink" href="#convex-19_optimizationalgo-125-conjugate-gradient-method-efficient-optimization-for-quadratic-objectives" title="Permanent link">¶</a></h2>
<p>Gradient descent can be slow when the objective’s level sets are highly elongated, a symptom of ill-conditioning in the Hessian. For quadratic functions of the form</p>
<div class="arithmatex">\[
f(x) = \tfrac{1}{2} x^\top A x - b^\top x, \quad A \succ 0,
\]</div>
<p>plain gradient descent takes many small steps along shallow directions of <span class="arithmatex">\(A\)</span>.</p>
<p>The Conjugate Gradient (CG) method accelerates convergence dramatically for such problems — it exploits the structure of the quadratic and uses curvature-aware search directions without explicitly forming or inverting the Hessian.</p>
<h3 id="convex-19_optimizationalgo-problem-setup">Problem Setup<a class="headerlink" href="#convex-19_optimizationalgo-problem-setup" title="Permanent link">¶</a></h3>
<p>Minimize a strictly convex quadratic:</p>
<div class="arithmatex">\[
\min_x f(x) = \tfrac{1}{2} x^\top A x - b^\top x, \quad A \in \mathbb{R}^{n \times n}, \; A \succ 0.
\]</div>
<p>This is equivalent to solving the linear system</p>
<div class="arithmatex">\[
A x = b.
\]</div>
<h3 id="convex-19_optimizationalgo-algorithm-linear-cg">Algorithm (Linear CG)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-linear-cg" title="Permanent link">¶</a></h3>
<p>Given an initial <span class="arithmatex">\(x_0\)</span>, define the residual <span class="arithmatex">\(r_0 = b - A x_0\)</span>  and the initial direction <span class="arithmatex">\(p_0 = r_0\)</span>.</p>
<p>For <span class="arithmatex">\(k = 0, 1, 2, \dots\)</span> until convergence:</p>
<ol>
<li>Compute step size<br>
<script type="math/tex; mode=display">
   \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k}.
   </script>
</li>
<li>Update the iterate<br>
<script type="math/tex; mode=display">
   x_{k+1} = x_k + \alpha_k p_k.
   </script>
</li>
<li>Update the residual<br>
<script type="math/tex; mode=display">
   r_{k+1} = r_k - \alpha_k A p_k.
   </script>
</li>
<li>Compute the new direction coefficient<br>
<script type="math/tex; mode=display">
   \beta_k = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
   </script>
</li>
<li>Update the direction<br>
<script type="math/tex; mode=display">
   p_{k+1} = r_{k+1} + \beta_k p_k.
   </script>
</li>
</ol>
<p>Terminate when <span class="arithmatex">\(\|r_k\|\)</span> is below tolerance <span class="arithmatex">\(\varepsilon\)</span>.</p>
<h3 id="convex-19_optimizationalgo-geometric-intuition">Geometric Intuition<a class="headerlink" href="#convex-19_optimizationalgo-geometric-intuition" title="Permanent link">¶</a></h3>
<p>Each search direction <span class="arithmatex">\(p_k\)</span> is <span class="arithmatex">\(A\)</span>-conjugate to the previous ones:</p>
<div class="arithmatex">\[
p_i^\top A p_j = 0 \quad \text{for } i \ne j.
\]</div>
<p>That means successive steps explore independent curvature directions of the quadratic. The residual <span class="arithmatex">\(r_k\)</span> (the negative gradient) becomes orthogonal to all previous directions, so the method never re-searches the same subspace.</p>
<p>As a result, in exact arithmetic, CG finds the exact minimizer in at most <span class="arithmatex">\(n\)</span> steps.</p>
<h3 id="convex-19_optimizationalgo-convergence-properties">Convergence Properties<a class="headerlink" href="#convex-19_optimizationalgo-convergence-properties" title="Permanent link">¶</a></h3>
<ul>
<li>For SPD <span class="arithmatex">\(A\)</span>, CG converges monotonically to the minimizer <span class="arithmatex">\(x^\star = A^{-1} b\)</span>.</li>
<li>The rate depends on the condition number <span class="arithmatex">\(\kappa(A) = \frac{\lambda_{\max}}{\lambda_{\min}}\)</span>:
  <script type="math/tex; mode=display">
  \|x_k - x^\star\|_A \le 2 \left( \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1} \right)^k \|x_0 - x^\star\|_A.
  </script>
</li>
<li>Preconditioning (using <span class="arithmatex">\(M^{-1}A\)</span> with well-chosen <span class="arithmatex">\(M\)</span>) further improves convergence.</li>
</ul>
<h3 id="convex-19_optimizationalgo-machine-learning-context">Machine Learning Context<a class="headerlink" href="#convex-19_optimizationalgo-machine-learning-context" title="Permanent link">¶</a></h3>
<p>In ML, CG is widely used for large-scale convex quadratic subproblems:</p>
<table>
<thead>
<tr>
<th>Application</th>
<th>Formulation</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ridge regression</td>
<td><span class="arithmatex">\(\min_x \|A x - b\|_2^2 + \lambda\|x\|_2^2\)</span></td>
<td>Normal equations are SPD; CG avoids explicit inversion.</td>
</tr>
<tr>
<td>Kernel ridge regression</td>
<td><span class="arithmatex">\((K + \lambda I)\alpha = y\)</span></td>
<td>CG solves this efficiently without forming full <span class="arithmatex">\(K^{-1}\)</span>.</td>
</tr>
<tr>
<td>Linear least squares</td>
<td><span class="arithmatex">\(\min_x \tfrac{1}{2}\|A x - b\|^2\)</span></td>
<td>Equivalent to solving <span class="arithmatex">\(A^\top A x = A^\top b\)</span>.</td>
</tr>
<tr>
<td>Large-scale Newton steps</td>
<td>Solve <span class="arithmatex">\(\nabla^2 f(x_k)p = -\nabla f(x_k)\)</span></td>
<td>CG acts as an inner solver for the Newton direction.</td>
</tr>
</tbody>
</table>
<h3 id="convex-19_optimizationalgo-practical-notes">Practical Notes<a class="headerlink" href="#convex-19_optimizationalgo-practical-notes" title="Permanent link">¶</a></h3>
<ul>
<li>CG requires only matrix–vector products with <span class="arithmatex">\(A\)</span>, not explicit storage.<br>
  It’s ideal when <span class="arithmatex">\(A\)</span> is large, sparse, or implicitly defined.</li>
<li>Sensitive to rounding errors; residual re-orthogonalization may be needed for long runs.</li>
<li>Preconditioners (Jacobi, incomplete Cholesky, etc.) can drastically reduce iterations.</li>
</ul>
<h3 id="convex-19_optimizationalgo-comparison-summary">Comparison Summary<a class="headerlink" href="#convex-19_optimizationalgo-comparison-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Memory</th>
<th>Curvature Use</th>
<th>Convergence</th>
<th>Typical Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(O(n)\)</span></td>
<td>None</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>General smooth convex</td>
</tr>
<tr>
<td>Newton’s Method</td>
<td><span class="arithmatex">\(O(n^2)\)</span></td>
<td>Full Hessian</td>
<td>Quadratic (local)</td>
<td>Small/medium convex</td>
</tr>
<tr>
<td>Conjugate Gradient</td>
<td><span class="arithmatex">\(O(n)\)</span></td>
<td>Implicit (via <span class="arithmatex">\(A\)</span>-conjugacy)</td>
<td>Fast linear / finite-step</td>
<td>Large quadratic systems</td>
</tr>
</tbody>
</table>
<p>-</p>
<h3 id="convex-19_optimizationalgo-key-insight">Key Insight<a class="headerlink" href="#convex-19_optimizationalgo-key-insight" title="Permanent link">¶</a></h3>
<blockquote>
<p>The Conjugate Gradient method is the exact gradient method for quadratic objectives<br>
that automatically builds curvature information through orthogonalized directions,<br>
achieving Newton-like efficiency without forming the Hessian.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-126-newtons-method-and-second-order-methods">12.6 Newton’s method and second-order methods<a class="headerlink" href="#convex-19_optimizationalgo-126-newtons-method-and-second-order-methods" title="Permanent link">¶</a></h2>
<p>First-order methods (like gradient descent) only use gradient information. Newton’s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p>
<h3 id="convex-19_optimizationalgo-1261-local-quadratic-model">12.6.1 Local quadratic model<a class="headerlink" href="#convex-19_optimizationalgo-1261-local-quadratic-model" title="Permanent link">¶</a></h3>
<p>From Chapter 3, the second-order Taylor approximation of <span class="arithmatex">\(f(x)\)</span> around a point <span class="arithmatex">\(x_k\)</span> is:</p>
<div class="arithmatex">\[
f(x_k + d)
\approx
f(x_k)
+ \nabla f(x_k)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x_k) d.
\]</div>
<p>If we temporarily trust this quadratic model, we can choose <span class="arithmatex">\(d\)</span> to minimize the right-hand side.<br>
Differentiating with respect to <span class="arithmatex">\(d\)</span> and setting to zero gives:</p>
<div class="arithmatex">\[
\nabla^2 f(x_k) \, d_{\text{newton}} = - \nabla f(x_k).
\]</div>
<p>Hence, the Newton step is:</p>
<div class="arithmatex">\[
d_{\text{newton}} = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k),
\quad
x_{k+1} = x_k + d_{\text{newton}}.
\]</div>
<p>This step points toward the minimizer of the local quadratic model, and near the true minimizer, Newton’s method exhibits quadratic convergence.</p>
<h3 id="convex-19_optimizationalgo-1262-convergence-behaviour">12.6.2 Convergence behaviour<a class="headerlink" href="#convex-19_optimizationalgo-1262-convergence-behaviour" title="Permanent link">¶</a></h3>
<ul>
<li>Near the minimiser of a strictly convex, twice-differentiable <span class="arithmatex">\(f\)</span>, Newton’s method converges quadratically: roughly, the number of correct digits doubles every iteration.  </li>
<li>This is dramatically faster than the <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span> rates typical of first-order methods — but only once the iterates enter the basin of attraction.  </li>
<li>Far from the minimiser, Newton’s method can behave erratically or even diverge.<br>
  To stabilise it, we typically pair it with a line search or trust region strategy to control step size.</li>
</ul>
<h3 id="convex-19_optimizationalgo-1263-implementation">12.6.3 Implementation<a class="headerlink" href="#convex-19_optimizationalgo-1263-implementation" title="Permanent link">¶</a></h3>
<p>The main computational effort in each iteration lies in evaluating derivatives and solving the Newton system:</p>
<div class="arithmatex">\[
H \, \Delta x = -g,
\]</div>
<p>where</p>
<div class="arithmatex">\[
H = \nabla^2 f(x), \quad g = \nabla f(x).
\]</div>
<h4 id="convex-19_optimizationalgo-solving-via-cholesky-factorization">Solving via Cholesky factorization<a class="headerlink" href="#convex-19_optimizationalgo-solving-via-cholesky-factorization" title="Permanent link">¶</a></h4>
<p>If <span class="arithmatex">\(H\)</span> is symmetric and positive definite, we can efficiently solve this system using a Cholesky factorization:</p>
<div class="arithmatex">\[
H = L L^{\top},
\]</div>
<p>where <span class="arithmatex">\(L\)</span> is lower triangular.<br>
The Newton step is then:</p>
<div class="arithmatex">\[
\Delta x_{\text{nt}} = -L^{-\top} L^{-1} g.
\]</div>
<p>This involves two triangular solves:</p>
<ol>
<li><span class="arithmatex">\(L y = -g\)</span></li>
<li><span class="arithmatex">\(L^{\top} \Delta x_{\text{nt}} = y\)</span></li>
</ol>
<p>This avoids explicitly computing <span class="arithmatex">\(H^{-1}\)</span> and ensures numerical stability.</p>
<h4 id="convex-19_optimizationalgo-newton-decrement">Newton decrement<a class="headerlink" href="#convex-19_optimizationalgo-newton-decrement" title="Permanent link">¶</a></h4>
<p>A useful measure of progress is the Newton decrement:</p>
<div class="arithmatex">\[
\lambda(x) = \| L^{-1} g \|_2,
\]</div>
<p>which approximates how far we are from the optimum.<br>
A common stopping criterion is <span class="arithmatex">\(\lambda(x)^2 / 2 &lt; \varepsilon\)</span>.</p>
<h3 id="convex-19_optimizationalgo-1264-computational-cost">12.6.4 Computational cost<a class="headerlink" href="#convex-19_optimizationalgo-1264-computational-cost" title="Permanent link">¶</a></h3>
<p>Each Newton step requires solving a linear system involving <span class="arithmatex">\(\nabla^2 f(x_k)\)</span>, which costs about as much as factoring the Hessian (or an approximation).</p>
<ul>
<li>For an unstructured, dense Hessian, Cholesky factorization requires approximately <span class="arithmatex">\((1/3) n^3\)</span> floating-point operations.  </li>
<li>If <span class="arithmatex">\(H\)</span> is sparse, banded, or has special structure, the cost can be much lower.  </li>
<li>Because of this cubic scaling, Newton’s method is most attractive for medium-scale problems where high accuracy is required.</li>
</ul>
<h3 id="convex-19_optimizationalgo-1265-why-convexity-helps">12.6.5 Why convexity helps<a class="headerlink" href="#convex-19_optimizationalgo-1265-why-convexity-helps" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex, then <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> is positive semidefinite (Chapter 5).<br>
This has two important implications:</p>
<ul>
<li>The local quadratic model is bowl-shaped, so the Newton direction points toward a minimiser.  </li>
<li>Regularised Newton steps (e.g. using <span class="arithmatex">\(H + \mu I\)</span> for small <span class="arithmatex">\(\mu &gt; 0\)</span>) are guaranteed to be descent directions and behave predictably.</li>
</ul>
<h3 id="convex-19_optimizationalgo-1266-quasi-newton-methods">12.6.6 Quasi-Newton methods<a class="headerlink" href="#convex-19_optimizationalgo-1266-quasi-newton-methods" title="Permanent link">¶</a></h3>
<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> or its inverse.<br>
These methods use gradient information from previous steps to estimate curvature.</p>
<p>The most famous examples are:</p>
<ul>
<li>BFGS (Broyden–Fletcher–Goldfarb–Shanno)  </li>
<li>DFP (Davidon–Fletcher–Powell)  </li>
<li>L-BFGS (Limited-memory BFGS) — for very large-scale problems.</li>
</ul>
<p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory</p>
<p>They maintain many of Newton’s fast local convergence properties, but with per-iteration costs similar to first-order methods.</p>
<p>For instance, BFGS maintains an approximation <span class="arithmatex">\(B_k \approx \nabla^2 f(x_k)^{-1}\)</span> updated via gradient and step differences:</p>
<div class="arithmatex">\[
B_{k+1} = B_k + \frac{(s_k^\top y_k + y_k^\top B_k y_k)}{(s_k^\top y_k)^2} s_k s_k^\top
- \frac{B_k y_k s_k^\top + s_k y_k^\top B_k}{s_k^\top y_k},
\]</div>
<p>where <span class="arithmatex">\(s_k = x_{k+1} - x_k\)</span> and <span class="arithmatex">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_k)\)</span>.</p>
<p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p>
<h3 id="convex-19_optimizationalgo-1267-when-to-use-newton-or-quasi-newton-methods">12.6.7 When to use Newton or quasi-Newton methods<a class="headerlink" href="#convex-19_optimizationalgo-1267-when-to-use-newton-or-quasi-newton-methods" title="Permanent link">¶</a></h3>
<p>Use Newton or quasi-Newton methods when:</p>
<ul>
<li>You need high-accuracy solutions.  </li>
<li>The problem is smooth and reasonably well-conditioned.  </li>
<li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li>
</ul>
<p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>
<blockquote>
<p>Newton-Raphson: The Newton step solves <span class="arithmatex">\(\nabla^2 f(x_k) p_k=-\nabla f(x_k)\)</span> and updates <span class="arithmatex">\(x_{k+1}=x_k+p_k\)</span> with line search or trust-region safeguards. Complexity hinges on solving linear systems; use sparse Cholesky, conjugate gradients with preconditioning, or low-rank structure to scale. For generalized linear models, iteratively reweighted least squares converges in few iterations, but regularization and damping are needed when data are nearly separable.</p>
<p>Gauss-Newton: For nonlinear least squares <span class="arithmatex">\(f(x)=\tfrac12\|r(x)\|^2\)</span>, the Gauss–Newton approximation uses <span class="arithmatex">\(H\approx J^\top J\)</span> where <span class="arithmatex">\(J\)</span> is the Jacobian of <span class="arithmatex">\(r\)</span>. Solve <span class="arithmatex">\((J^\top J)\Delta=-J^\top r\)</span> to get a step; Levenberg–Marquardt adds damping <span class="arithmatex">\((J^\top J+\lambda I)\Delta=-J^\top r\)</span> interpolating between gradient and Gauss–Newton. Effective for residual models where second-order residual terms are small; widely used in curve fitting and some deep learning layerwise updates.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-127-constraints-and-nonsmooth-terms-projection-and-proximal-methods">12.7 Constraints and nonsmooth terms: projection and proximal methods<a class="headerlink" href="#convex-19_optimizationalgo-127-constraints-and-nonsmooth-terms-projection-and-proximal-methods" title="Permanent link">¶</a></h2>
<p>In practice, most convex optimization problems are not purely smooth.<br>
They often include:</p>
<ul>
<li>Constraints: <span class="arithmatex">\(x \in \mathcal{X}\)</span>,</li>
<li>Nonsmooth regularisers: such as <span class="arithmatex">\(\|x\|_1\)</span>,</li>
<li>Penalties: promoting robustness or sparsity (see Chapter 6).</li>
</ul>
<p>Two core strategies handle such settings:</p>
<ol>
<li>Projected gradient methods — where we project each iterate back into the feasible set <span class="arithmatex">\(\mathcal{X}\)</span>.  </li>
<li>Proximal gradient methods — which generalize projection to handle nonsmooth but structured terms.</li>
</ol>
<p>These methods extend the ideas of gradient and Newton updates to the broader world of constrained and composite optimization.</p>
<h3 id="convex-19_optimizationalgo-1272-convergence-behaviour">12.7.2 Convergence behaviour<a class="headerlink" href="#convex-19_optimizationalgo-1272-convergence-behaviour" title="Permanent link">¶</a></h3>
<ul>
<li>Near the minimiser of a strictly convex, twice-differentiable <span class="arithmatex">\(f\)</span>, Newton’s method converges quadratically: roughly, the number of correct digits doubles every iteration.</li>
<li>This is dramatically faster than <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span>, but only once you’re in the “basin of attraction.”</li>
<li>Far from the minimiser, Newton can misbehave, so we pair it with a line search or trust region.</li>
</ul>
<h3 id="convex-19_optimizationalgo-1273-computational-cost">12.7.3 Computational cost<a class="headerlink" href="#convex-19_optimizationalgo-1273-computational-cost" title="Permanent link">¶</a></h3>
<p>Each Newton step requires solving a linear system involving <span class="arithmatex">\(\nabla^2 f(x_k)\)</span>, which costs about as much as factoring the Hessian (or an approximation). This is expensive in very high dimensions, which is why Newton is most attractive for medium-scale problems where high accuracy matters.</p>
<h3 id="convex-19_optimizationalgo-1274-why-convexity-helps">12.7.4 Why convexity helps<a class="headerlink" href="#convex-19_optimizationalgo-1274-why-convexity-helps" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex, then <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> is positive semidefinite (Chapter 5). This means:</p>
<ul>
<li>The quadratic model is bowl-shaped, so the Newton step makes sense.</li>
<li>Regularised Newton steps (adding a multiple of the identity to the Hessian) behave very predictably.</li>
</ul>
<h3 id="convex-19_optimizationalgo-1275-quasi-newton">12.7.5 Quasi-Newton<a class="headerlink" href="#convex-19_optimizationalgo-1275-quasi-newton" title="Permanent link">¶</a></h3>
<p>When Hessians are too expensive, we can build low-rank approximations of <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> or its inverse. Famous examples include BFGS and L-BFGS. These methods keep much of Newton’s fast local convergence but with per-iteration cost closer to first-order methods.</p>
<h3 id="convex-19_optimizationalgo-1276-when-to-use-newton-quasi-newton">12.7.6 When to use Newton / quasi-Newton<a class="headerlink" href="#convex-19_optimizationalgo-1276-when-to-use-newton-quasi-newton" title="Permanent link">¶</a></h3>
<ul>
<li>You need high-accuracy solutions.</li>
<li>The problem is smooth and reasonably well-conditioned.</li>
<li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g. via sparse linear algebra).</li>
</ul>
<h2 id="convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods">12.8 Constraints and nonsmooth terms: projection and proximal methods<a class="headerlink" href="#convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods" title="Permanent link">¶</a></h2>
<p>In practice, most convex objectives are not just “nice smooth <span class="arithmatex">\(f(x)\)</span>”. They often have:</p>
<ul>
<li>constraints <span class="arithmatex">\(x \in \mathcal{X}\)</span>,</li>
<li>nonsmooth regularisers like <span class="arithmatex">\(\|x\|_1\)</span>,</li>
<li>penalties that encode robustness or sparsity (Chapter 6).</li>
</ul>
<p>Two core ideas handle this: projected gradient and proximal gradient.</p>
<h3 id="convex-19_optimizationalgo-1281-projected-gradient-descent">12.8.1 Projected gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-1281-projected-gradient-descent" title="Permanent link">¶</a></h3>
<p>Setting:<br>
Minimise convex, differentiable <span class="arithmatex">\(f(x)\)</span> subject to <span class="arithmatex">\(x \in \mathcal{X}\)</span>, where <span class="arithmatex">\(\mathcal{X}\)</span> is a simple closed convex set (Chapter 4).</p>
<p>Algorithm:</p>
<ol>
<li>Gradient step:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Projection:
   <script type="math/tex; mode=display">
   x_{k+1}
   =
   \Pi_{\mathcal{X}}(y_k)
   :=
   \arg\min_{x \in \mathcal{X}} \|x - y_k\|_2^2~.
   </script>
</li>
</ol>
<p>Interpretation:</p>
<ul>
<li>You take an unconstrained step downhill,</li>
<li>then you “snap back” to feasibility by Euclidean projection.</li>
</ul>
<p>Examples of <span class="arithmatex">\(\mathcal{X}\)</span> where projection is cheap:</p>
<ul>
<li>A box: <span class="arithmatex">\(l \le x \le u\)</span> (clip each coordinate).</li>
<li>The probability simplex <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span> (there are fast projection routines).</li>
<li>An <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\(\{x : \|x\|_2 \le R\}\)</span> (scale down if needed).</li>
</ul>
<p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>
<h3 id="convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting">12.8.2 Proximal gradient (forward–backward splitting)<a class="headerlink" href="#convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting" title="Permanent link">¶</a></h3>
<p>Setting:<br>
Composite convex minimisation
<script type="math/tex; mode=display">
\min_x \; F(x) := f(x) + R(x),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> is convex, differentiable, with Lipschitz gradient,</li>
<li><span class="arithmatex">\(R\)</span> is convex, possibly nonsmooth.</li>
</ul>
<p>Typical choices of <span class="arithmatex">\(R(x)\)</span>:</p>
<ul>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_1\)</span> (sparsity),</li>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_2^2\)</span> (ridge),</li>
<li><span class="arithmatex">\(R(x)\)</span> is the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>, i.e. <span class="arithmatex">\(R(x)=0\)</span> if <span class="arithmatex">\(x \in \mathcal{X}\)</span> and <span class="arithmatex">\(+\infty\)</span> otherwise — this encodes a hard constraint.</li>
</ul>
<p>Define the proximal operator of <span class="arithmatex">\(R\)</span>:
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha R}(y)
=
\arg\min_x
\left(
R(x) + \frac{1}{2\alpha} \|x-y\|_2^2
\right).
</script>
</p>
<p>Proximal gradient method:</p>
<ol>
<li>Gradient step on <span class="arithmatex">\(f\)</span>:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Proximal step on <span class="arithmatex">\(R\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = \mathrm{prox}_{\alpha R}(y_k).
   </script>
</li>
</ol>
<p>This is also called forward–backward splitting: “forward” = gradient step, “backward” = prox step.</p>
<h4 id="convex-19_optimizationalgo-interpretation">Interpretation:<a class="headerlink" href="#convex-19_optimizationalgo-interpretation" title="Permanent link">¶</a></h4>
<ul>
<li>The prox step “handles” the nonsmooth or constrained part exactly.</li>
<li>For <span class="arithmatex">\(R(x)=\lambda \|x\|_1\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> is soft-thresholding, which promotes sparsity in <span class="arithmatex">\(x\)</span>.<br>
  This is the heart of <span class="arithmatex">\(\ell_1\)</span>-regularised least-squares (LASSO) and many sparse recovery problems.</li>
<li>For <span class="arithmatex">\(R\)</span> as an indicator of <span class="arithmatex">\(\mathcal{X}\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R} = \Pi_\mathcal{X}\)</span>, so projected gradient is a special case of proximal gradient.</li>
</ul>
<p>This unifies constraints and regularisation.</p>
<h4 id="convex-19_optimizationalgo-when-to-use-proximal-projected-gradient">When to use proximal / projected gradient<a class="headerlink" href="#convex-19_optimizationalgo-when-to-use-proximal-projected-gradient" title="Permanent link">¶</a></h4>
<ul>
<li>High-dimensional ML/statistics problems.</li>
<li>Objectives with <span class="arithmatex">\(\ell_1\)</span>, group sparsity, total variation, hinge loss, or indicator constraints.</li>
<li>You can evaluate <span class="arithmatex">\(\nabla f\)</span> and compute <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> cheaply.</li>
<li>You don’t need absurdly high accuracy, but you do need scalability.</li>
</ul>
<p>This is the standard tool for modern large-scale convex learning problems.</p>
<h2 id="convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods">12.9 Penalties, barriers, and interior-point methods<a class="headerlink" href="#convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods" title="Permanent link">¶</a></h2>
<p>So far we’ve assumed either:</p>
<ul>
<li>simple constraints we can project onto,</li>
<li>or nonsmooth terms we can prox.</li>
</ul>
<p>What if the constraints are general convex inequalities <span class="arithmatex">\(g_i(x)\le0\)</span>?<br>
Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>
<h3 id="convex-19_optimizationalgo-1291-penalty-methods">12.9.1 Penalty methods<a class="headerlink" href="#convex-19_optimizationalgo-1291-penalty-methods" title="Permanent link">¶</a></h3>
<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints.</p>
<p>Suppose we want
<script type="math/tex; mode=display">
\min_x f(x)
\quad \text{s.t.} \quad g_i(x) \le 0,\ i=1,\dots,m.
</script>
</p>
<p>A penalty method solves instead
<script type="math/tex; mode=display">
\min_x \; f(x) + \rho \sum_{i=1}^m \phi(g_i(x)),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(\phi(r)\)</span> is <span class="arithmatex">\(0\)</span> when <span class="arithmatex">\(r \le 0\)</span> (feasible),</li>
<li><span class="arithmatex">\(\phi(r)\)</span> grows when <span class="arithmatex">\(r&gt;0\)</span> (infeasible),</li>
<li><span class="arithmatex">\(\rho &gt; 0\)</span> is a penalty weight.</li>
</ul>
<p>As <span class="arithmatex">\(\rho \to \infty\)</span>, infeasible points become extremely expensive, so minimisers approach feasibility.  </p>
<p>This is conceptually simple and is sometimes effective, but:</p>
<ul>
<li>choosing <span class="arithmatex">\(\rho\)</span> is tricky,</li>
<li>very large <span class="arithmatex">\(\rho\)</span> can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li>
</ul>
<p>Penalty methods are closely linked to robust formulations and Huber-like losses: you replace a hard requirement by a soft cost. This is exactly what you do in robust regression and in <span class="arithmatex">\(\epsilon\)</span>-insensitive / Huber losses (see Section 9.7).</p>
<h3 id="convex-19_optimizationalgo-1292-barrier-methods">12.9.2 Barrier methods<a class="headerlink" href="#convex-19_optimizationalgo-1292-barrier-methods" title="Permanent link">¶</a></h3>
<p>Penalty methods penalise violation <em>after</em> you cross the boundary. Barrier methods make it impossible to even touch the boundary.</p>
<p>For inequality constraints <span class="arithmatex">\(g_i(x) \le 0\)</span>, define the logarithmic barrier
<script type="math/tex; mode=display">
b(x) = - \sum_{i=1}^m \log(-g_i(x)).
</script>
This is finite only if <span class="arithmatex">\(g_i(x) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>, i.e. <span class="arithmatex">\(x\)</span> is strictly feasible. As you approach the boundary <span class="arithmatex">\(g_i(x)=0\)</span>, <span class="arithmatex">\(b(x)\)</span> blows up to <span class="arithmatex">\(+\infty\)</span>.</p>
<p>We then solve, for a sequence of increasing parameters <span class="arithmatex">\(t\)</span>:
<script type="math/tex; mode=display">
\min_x \; F_t(x) := t f(x) + b(x),
</script>
subject to strict feasibility <span class="arithmatex">\(g_i(x)&lt;0\)</span>.</p>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, minimisers of <span class="arithmatex">\(F_t\)</span> approach the true constrained optimum. The path of minimisers <span class="arithmatex">\(x^*(t)\)</span> is called the central path.</p>
<p>Key points:</p>
<ul>
<li><span class="arithmatex">\(F_t\)</span> is smooth on the interior of the feasible region.</li>
<li>We can apply Newton’s method to <span class="arithmatex">\(F_t\)</span>.</li>
<li>Each Newton step solves a linear system involving the Hessian of <span class="arithmatex">\(F_t\)</span>, so the inner loop looks like a damped Newton method.</li>
<li>Increasing <span class="arithmatex">\(t\)</span> tightens the approximation; we “home in” on the boundary of feasibility.</li>
</ul>
<p>This is the core idea of interior-point methods.</p>
<h3 id="convex-19_optimizationalgo-1293-interior-point-methods-in-practice">12.9.3 Interior-point methods in practice<a class="headerlink" href="#convex-19_optimizationalgo-1293-interior-point-methods-in-practice" title="Permanent link">¶</a></h3>
<p>Interior-point methods:</p>
<ul>
<li>Are globally convergent for convex problems under mild assumptions (Slater’s condition; see Chapter 8).</li>
<li>Solve a series of smooth, strictly feasible subproblems.</li>
<li>Use Newton-like steps to update primal (and, implicitly, dual) variables.</li>
<li>Produce both primal and dual iterates — so they naturally produce a duality gap, which certifies how close you are to optimality (Chapter 8).</li>
</ul>
<p>Interior-point methods are the engine behind modern general-purpose convex solvers for:</p>
<ul>
<li>linear programs (LP),</li>
<li>quadratic programs (QP),</li>
<li>second-order cone programs (SOCP),</li>
<li>semidefinite programs (SDP).</li>
</ul>
<p>They give high-accuracy answers and KKT-based optimality certificates. They are more expensive per iteration than gradient methods, but need far fewer iterations, and they handle fully general convex constraints.</p>
<p>Summary: Penalty vs Barrier vs Interior-Point</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Feasibility During Iteration</th>
<th>Mechanism</th>
<th>Typical Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>Penalty</td>
<td>May violate constraints</td>
<td>Adds large penalty outside feasible region</td>
<td>Easy to implement but can be ill-conditioned</td>
</tr>
<tr>
<td>Barrier</td>
<td>Stays strictly feasible</td>
<td>Adds infinite cost near constraint boundary</td>
<td>Smooth approximation to constrained problem</td>
</tr>
<tr>
<td>Interior-Point</td>
<td>Always feasible (uses barrier)</td>
<td>Solves a sequence of barrier problems with increasing precision</td>
<td>Follows central path to true optimum</td>
</tr>
</tbody>
</table>
<h2 id="convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice">12.10 Choosing the right method in practice<a class="headerlink" href="#convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice" title="Permanent link">¶</a></h2>
<p>Let’s summarise the chapter in the form of a decision guide.</p>
<p>Case A. Smooth, unconstrained, very high dimensional.<br>
Example: logistic regression on millions of samples.<br>
Use: gradient descent or (better) accelerated gradient.<br>
Why: cheap iterations, easy to implement, scales.  </p>
<p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy.<br>
Example: convex nonlinear fitting with well-behaved Hessian.<br>
Use: Newton or quasi-Newton.<br>
Why: quadratic (or near-quadratic) convergence near optimum.  </p>
<p>Case C. Convex with simple feasible set <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex).<br>
Use: projected gradient.<br>
Why: projection is easy, maintains feasibility at each step.  </p>
<p>Case D. Composite objective <span class="arithmatex">\(f(x) + R(x)\)</span> where <span class="arithmatex">\(R\)</span> is nonsmooth (e.g. <span class="arithmatex">\(\ell_1\)</span>, indicator of a constraint set).<br>
Use: proximal gradient.<br>
Why: prox handles nonsmooth/constraint part exactly each step.  </p>
<p>Case E. General convex program with inequalities <span class="arithmatex">\(g_i(x)\le 0\)</span>.<br>
Use: interior-point methods.<br>
Why: they solve smooth barrier subproblems via Newton steps and give primal–dual certificates through KKT and duality (Chapters 7–8).  </p></body></html></section><section class="print-page" id="convex-19a_optimization_constraints" heading-number="2.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-13-optimization-algorithms-for-equality-constrained-problems">Chapter 13: Optimization Algorithms for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-chapter-13-optimization-algorithms-for-equality-constrained-problems" title="Permanent link">¶</a></h1>
<p>Equality-constrained optimization arises whenever the variables must satisfy one or more exact relations — such as conservation laws, normalization, or fairness criteria. We study algorithms for minimizing a function subject to linear or nonlinear equality constraints:</p>
<div class="arithmatex">\[
\min_x \; f(x) \quad \text{s.t.} \quad A x = b.
\]</div>
<p>Such problems are fundamental in convex optimization, quadratic programming, and many ML formulations involving exact invariants.</p>
<h2 id="convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold">13.1 Geometric View — Optimization on an Affine Manifold<a class="headerlink" href="#convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold" title="Permanent link">¶</a></h2>
<p>The constraint <span class="arithmatex">\(A x = b\)</span> defines an affine set, a lower-dimensional plane within <span class="arithmatex">\(\mathbb{R}^n\)</span>. The feasible region is:</p>
<div class="arithmatex">\[
\mathcal{X} = \{ x \in \mathbb{R}^n \mid A x = b \}.
\]</div>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{p \times n}\)</span> has full row rank (<span class="arithmatex">\(\operatorname{rank}(A)=p\)</span>), then <span class="arithmatex">\(\mathcal{X}\)</span> is an <span class="arithmatex">\((n-p)\)</span>-dimensional affine manifold.</p>
<p>Geometrically, optimization proceeds not over all <span class="arithmatex">\(\mathbb{R}^n\)</span>, but along this manifold. At the optimum, the gradient <span class="arithmatex">\(\nabla f(x^\star)\)</span> cannot point in a direction that stays feasible—hence it must be orthogonal to the feasible surface. This gives the first key optimality relation:</p>
<div class="arithmatex">\[
\nabla f(x^\star) = A^\top \nu^\star,
\]</div>
<p>where <span class="arithmatex">\(\nu^\star\)</span> is a vector of Lagrange multipliers capturing how sensitive the objective is to constraint perturbations.</p>
<blockquote>
<p>Intuition:<br>
The gradient of the objective at the optimum lies in the span of the constraint normals (rows of <span class="arithmatex">\(A\)</span>).<br>
Any feasible direction must lie in the null space of <span class="arithmatex">\(A\)</span>, orthogonal to <span class="arithmatex">\(\nabla f(x^\star)\)</span>.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system">13.2 Lagrange Function and KKT System<a class="headerlink" href="#convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system" title="Permanent link">¶</a></h2>
<p>Define the Lagrangian:</p>
<div class="arithmatex">\[
\mathcal{L}(x, \nu) = f(x) + \nu^\top (A x - b).
\]</div>
<p>The first-order (KKT) conditions for a feasible point <span class="arithmatex">\((x^\star, \nu^\star)\)</span> to be optimal are:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla f(x^\star) + A^\top \nu^\star &amp;= 0, \\
A x^\star &amp;= b.
\end{aligned}
\]</div>
<p>These equations express stationarity and feasibility simultaneously. They can be combined into the KKT linear system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\
\Delta \nu
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) + A^\top \nu \\
A x - b
\end{bmatrix}.
\]</div>
<p>At the optimum, the right-hand side is zero.</p>
<blockquote>
<p>ML Connection:<br>
Lagrange multipliers <span class="arithmatex">\(\nu\)</span> quantify trade-offs between objectives and hard constraints —<br>
for instance, enforcing weight normalization in a neural layer, balance constraints in fair classification, or conservation laws in physics-informed networks.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-133-the-quadratic-case">13.3 The Quadratic Case<a class="headerlink" href="#convex-19a_optimization_constraints-133-the-quadratic-case" title="Permanent link">¶</a></h2>
<p>For a quadratic objective
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2}x^\top P x + q^\top x + r,
</script>
with <span class="arithmatex">\(P \succeq 0\)</span>, the KKT conditions reduce to a linear system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
P &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
x^\star \\ \nu^\star
\end{bmatrix}
=
-
\begin{bmatrix}
q \\ -b
\end{bmatrix}.
\]</div>
<p>This is a saddle-point system, solvable by factorization or elimination. If <span class="arithmatex">\(P \succ 0\)</span> and <span class="arithmatex">\(A\)</span> has full row rank, the solution <span class="arithmatex">\((x^\star, \nu^\star)\)</span> is unique.</p>
<blockquote>
<p>In ML, such systems appear in constrained least squares, e.g. enforcing <span class="arithmatex">\(\sum_i w_i = 1\)</span> in portfolio optimization or convex combination weights in mixture models.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-134-the-null-space-reduced-variable-method">13.4 The Null-Space (Reduced Variable) Method<a class="headerlink" href="#convex-19a_optimization_constraints-134-the-null-space-reduced-variable-method" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A\)</span> has full row rank, we can find a particular feasible point <span class="arithmatex">\(x_0\)</span> such that <span class="arithmatex">\(A x_0 = b\)</span>, and a basis <span class="arithmatex">\(Z\)</span> for the null space of <span class="arithmatex">\(A\)</span> satisfying <span class="arithmatex">\(A Z = 0\)</span>.<br>
Then any feasible <span class="arithmatex">\(x\)</span> can be written as:</p>
<div class="arithmatex">\[
x = x_0 + Z y, \quad y \in \mathbb{R}^{n-p}.
\]</div>
<p>Substituting into the objective gives a reduced problem:</p>
<div class="arithmatex">\[
\min_y \; f(x_0 + Z y).
\]</div>
<p>This is an unconstrained problem in <span class="arithmatex">\(y\)</span>, solvable by gradient or Newton methods.<br>
The reduced gradient and Hessian are:</p>
<div class="arithmatex">\[
\nabla_y f = Z^\top \nabla_x f, \qquad \nabla_y^2 f = Z^\top \nabla_x^2 f \, Z.
\]</div>
<blockquote>
<p>Interpretation: Optimization proceeds only along feasible directions — those that do not violate the constraints (i.e., within <span class="arithmatex">\(\operatorname{Null}(A)\)</span>).<br>
This is equivalent to projecting all gradient steps onto the tangent space of the constraint manifold.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems">13.5 Newton’s Method for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems" title="Permanent link">¶</a></h2>
<p>For a twice differentiable <span class="arithmatex">\(f\)</span>, the equality-constrained Newton step solves the quadratic subproblem:</p>
<div class="arithmatex">\[
\begin{aligned}
\min_d &amp; \quad \tfrac{1}{2} d^\top \nabla^2 f(x) d + \nabla f(x)^\top d, \\
\text{s.t.} &amp; \quad A d = 0.
\end{aligned}
\]</div>
<p>This produces the step <span class="arithmatex">\((d, \lambda)\)</span> from the linearized KKT system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
d \\ \lambda
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) \\ 0
\end{bmatrix}.
\]</div>
<p>The update is <span class="arithmatex">\(x_{k+1} = x_k + \alpha d\)</span>, ensuring <span class="arithmatex">\(A x_{k+1} = b\)</span> if <span class="arithmatex">\(A x_k = b\)</span>.</p>
<p>Geometric insight:<br>
The Newton direction is the projection of the unconstrained Newton step onto the tangent space of the feasible set (directions satisfying <span class="arithmatex">\(A d = 0\)</span>).<br>
Thus, each step stays within the affine constraint manifold.</p>
<blockquote>
<p>In practice:<br>
The KKT system is typically solved by <em>Schur complement factorization</em>:
<script type="math/tex; mode=display">
(A (\nabla^2 f)^{-1} A^\top) \lambda = A (\nabla^2 f)^{-1} \nabla f,
</script>
which then yields <span class="arithmatex">\(d = -(\nabla^2 f)^{-1} (\nabla f + A^\top \lambda)\)</span>.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-136-infeasible-start-newton-method">13.6 Infeasible Start Newton Method<a class="headerlink" href="#convex-19a_optimization_constraints-136-infeasible-start-newton-method" title="Permanent link">¶</a></h2>
<p>When starting from an infeasible point (<span class="arithmatex">\(A x_0 \ne b\)</span>),<br>
we relax the constraint and drive feasibility progressively.<br>
At iteration <span class="arithmatex">\(k\)</span>, compute <span class="arithmatex">\((\Delta x, \Delta \nu)\)</span> by solving:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x_k) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\ \Delta \nu
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x_k) + A^\top \nu_k \\
A x_k - b
\end{bmatrix}.
\]</div>
<p>Then update:</p>
<div class="arithmatex">\[
x_{k+1} = x_k + \alpha \Delta x, \quad
\nu_{k+1} = \nu_k + \alpha \Delta \nu.
\]</div>
<p>This method enforces feasibility gradually, converging to <span class="arithmatex">\((x^\star, \nu^\star)\)</span> under mild conditions.</p>
<blockquote>
<p>In ML contexts, infeasible starts are typical — we rarely have feasible initialization (e.g., in constrained autoencoders or regularized fairness models).<br>
The infeasible Newton method ensures consistent progress in both primal feasibility (<span class="arithmatex">\(A x = b\)</span>) and dual stationarity (<span class="arithmatex">\(\nabla f + A^\top \nu = 0\)</span>).</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-137-computational-considerations">13.7 Computational Considerations<a class="headerlink" href="#convex-19a_optimization_constraints-137-computational-considerations" title="Permanent link">¶</a></h2>
<ul>
<li>Factorization: KKT systems can be large but structured. Exploiting sparsity in <span class="arithmatex">\(\nabla^2 f\)</span> and <span class="arithmatex">\(A\)</span> is essential in high-dimensional problems.</li>
<li>Stability: Adding small regularization to the (0,0) block of the KKT matrix improves conditioning:
  <script type="math/tex; mode=display">
  \begin{bmatrix}
  \nabla^2 f + \delta I & A^\top \\
  A & -\delta I
  \end{bmatrix}.
  </script>
</li>
<li>Schur Complement: Eliminating <span class="arithmatex">\(\Delta x\)</span> yields a smaller linear system in <span class="arithmatex">\(\Delta \nu\)</span>, which can be more efficient when <span class="arithmatex">\(p \ll n\)</span>.</li>
</ul>
<h2 id="convex-19a_optimization_constraints-138-connections-to-machine-learning">13.8 Connections to Machine Learning<a class="headerlink" href="#convex-19a_optimization_constraints-138-connections-to-machine-learning" title="Permanent link">¶</a></h2>
<p>Equality-constrained optimization appears in several ML and signal processing settings:</p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Equality Constraint</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Portfolio optimization</td>
<td><span class="arithmatex">\(\mathbf{1}^\top w = 1\)</span></td>
<td>Weights must sum to 1</td>
</tr>
<tr>
<td>Fair classification</td>
<td><span class="arithmatex">\(A w = 0\)</span></td>
<td>Enforces equal outcomes across groups</td>
</tr>
<tr>
<td>Orthogonal embeddings</td>
<td><span class="arithmatex">\(W^\top W = I\)</span></td>
<td>Preserves independence / energy</td>
</tr>
<tr>
<td>Normalization layers</td>
<td><span class="arithmatex">\(\|w\|_2^2 = 1\)</span></td>
<td>Scale invariance constraint</td>
</tr>
<tr>
<td>Physics-informed models</td>
<td><span class="arithmatex">\(\text{div}(F)=0\)</span></td>
<td>Conservation of mass / charge</td>
</tr>
</tbody>
</table>
<h3 id="convex-19a_optimization_constraints-summary-approaches-to-equality-constrained-optimization">Summary: Approaches to Equality-Constrained Optimization<a class="headerlink" href="#convex-19a_optimization_constraints-summary-approaches-to-equality-constrained-optimization" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Constraint Type</th>
<th>Feasibility (Local/Global)</th>
<th>Core Idea</th>
<th>Advantages</th>
<th>Limitations / Drawbacks</th>
<th>Typical ML / Optimization Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Null-Space (Variable Elimination)</td>
<td>Linear, full-rank <span class="arithmatex">\(A\)</span></td>
<td>Global</td>
<td>Parameterize feasible <span class="arithmatex">\(x = x_0 + Z y\)</span> with <span class="arithmatex">\(A Z = 0\)</span></td>
<td>Converts to unconstrained problem; dimension reduction; exact</td>
<td>Requires null-space basis <span class="arithmatex">\(Z\)</span>; destroys sparsity; expensive for large <span class="arithmatex">\(A\)</span></td>
<td>Constrained least squares, small-scale convex programs</td>
</tr>
<tr>
<td>Local Parameterization (Manifold Method)</td>
<td>Nonlinear <span class="arithmatex">\(g(x) = 0\)</span></td>
<td>Local (around feasible point)</td>
<td>Use implicit function theorem: locally express <span class="arithmatex">\(x = x(y)\)</span></td>
<td>Captures nonlinear manifold structure; geometric insight</td>
<td>Valid only locally; requires Jacobians; expensive</td>
<td>Manifold learning, orthogonal embeddings, equality-regularized networks</td>
</tr>
<tr>
<td>KKT / Lagrange System</td>
<td>Linear or nonlinear</td>
<td>Global (if convex)</td>
<td>Solve coupled system <span class="arithmatex">\(\nabla f + A^\top \nu = 0\)</span>, <span class="arithmatex">\(A x = b\)</span></td>
<td>Keeps structure; allows dual interpretation; works for large sparse systems</td>
<td>Larger system; more variables</td>
<td>Quadratic programming, convex solvers, equality-constrained ML models</td>
</tr>
<tr>
<td>Primal–Dual Newton Method</td>
<td>Linear or nonlinear</td>
<td>Global (convex)</td>
<td>Newton’s method on full KKT system</td>
<td>Quadratic convergence near optimum; stable numerically</td>
<td>Requires Hessians and factorizations</td>
<td>Interior-point solvers, primal–dual optimization, barrier methods</td>
</tr>
<tr>
<td>Penalty / Augmented Lagrangian</td>
<td>General (convex or nonconvex)</td>
<td>Approximate (drives feasibility)</td>
<td>Add penalty term <span class="arithmatex">\(\tfrac{\rho}{2}\|A x - b\|^2\)</span> or dual updates</td>
<td>Simple to implement; smooth transition from unconstrained</td>
<td>Needs tuning of <span class="arithmatex">\(\rho\)</span>; slow convergence to exact feasibility</td>
<td>Regularized fairness, soft constraints, physics-informed networks</td>
</tr>
<tr>
<td>Projection / Normalization Step</td>
<td>Linear or nonlinear (simple form)</td>
<td>Iterative (after each step)</td>
<td>Project back to feasible set: <span class="arithmatex">\(x_{k+1} = \Pi_{\{A x = b\}}(x_{k+1})\)</span></td>
<td>Keeps updates feasible; easy for simple constraints</td>
<td>Costly for complex <span class="arithmatex">\(A\)</span>; may distort gradient direction</td>
<td>Normalization layers, unit-norm or balance constraints</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-19b_optimization_constraints" heading-number="2.14"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-14-optimization-algorithms-for-inequality-constrained-problems">Chapter 14: Optimization Algorithms for Inequality-Constrained Problems<a class="headerlink" href="#convex-19b_optimization_constraints-chapter-14-optimization-algorithms-for-inequality-constrained-problems" title="Permanent link">¶</a></h1>
<p>In practice, optimization problems often include inequalities that restrict feasible solutions to a convex region. Examples include nonnegativity of variables, margin constraints in support vector machines, fairness or safety limits, and physical conservation laws. This chapter introduces algorithms for solving such problems efficiently, focusing on the logarithmic barrier and interior-point methods that underpin modern convex solvers.</p>
<h2 id="convex-19b_optimization_constraints-141-problem-setup">14.1 Problem Setup<a class="headerlink" href="#convex-19b_optimization_constraints-141-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider the general convex optimization problem with both equality and inequality constraints:</p>
<div class="arithmatex">\[
\begin{aligned}
\text{minimize}   &amp;\quad f_0(x) \\
\text{subject to} &amp;\quad f_i(x) \le 0, \quad i=1,\dots,m,\\
&amp;\quad A x = b.
\end{aligned}
\]</div>
<p>Assumptions:</p>
<ul>
<li>Each <span class="arithmatex">\(f_i\)</span> is convex and twice differentiable.  </li>
<li><span class="arithmatex">\(A \in \mathbb{R}^{p\times n}\)</span> has full row rank (<span class="arithmatex">\(\mathrm{rank}(A)=p\)</span>).  </li>
<li>There exists a strictly feasible point <span class="arithmatex">\(\bar{x}\)</span> such that <span class="arithmatex">\(f_i(\bar{x})&lt;0\)</span> and <span class="arithmatex">\(A\bar{x}=b\)</span> (Slater’s condition).  </li>
</ul>
<p>Under these assumptions, strong duality holds and the KKT conditions are necessary and sufficient for optimality.</p>
<h3 id="convex-19b_optimization_constraints-examples">Examples<a class="headerlink" href="#convex-19b_optimization_constraints-examples" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Problem</th>
<th><span class="arithmatex">\(f_0(x)\)</span></th>
<th><span class="arithmatex">\(f_i(x)\)</span></th>
<th>Notes / ML context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Program (LP)</td>
<td><span class="arithmatex">\(c^T x\)</span></td>
<td><span class="arithmatex">\(a_i^T x - b_i\)</span></td>
<td>Feature selection, resource allocation</td>
</tr>
<tr>
<td>Quadratic Program (QP)</td>
<td><span class="arithmatex">\(\tfrac{1}{2}x^T P x + q^T x\)</span></td>
<td>Linear <span class="arithmatex">\(a_i^T x - b_i\)</span></td>
<td>SVM training, ridge regression</td>
</tr>
<tr>
<td>QCQP</td>
<td>Quadratic</td>
<td>Quadratic</td>
<td>Portfolio optimization, control</td>
</tr>
<tr>
<td>Geometric Program (log domain)</td>
<td>Convex in <span class="arithmatex">\(\log x\)</span></td>
<td>Linear in <span class="arithmatex">\(\log x\)</span></td>
<td>Network flow, resource allocation</td>
</tr>
<tr>
<td>Entropy minimization</td>
<td><span class="arithmatex">\(\sum_i x_i \log x_i\)</span></td>
<td><span class="arithmatex">\(F x \le g\)</span></td>
<td>Probability calibration, information bottleneck</td>
</tr>
</tbody>
</table>
<h2 id="convex-19b_optimization_constraints-142-indicator-function-reformulation">14.2 Indicator-Function Reformulation<a class="headerlink" href="#convex-19b_optimization_constraints-142-indicator-function-reformulation" title="Permanent link">¶</a></h2>
<p>Define the indicator of the nonpositive orthant:</p>
<div class="arithmatex">\[
I_-(u)=
\begin{cases}
0, &amp; u \le 0,\\
+\infty, &amp; u &gt; 0.
\end{cases}
\]</div>
<p>Then the constrained problem is equivalent to</p>
<div class="arithmatex">\[
\min_x \; f_0(x) + \sum_{i=1}^m I_-(f_i(x))
\quad \text{s.t. } A x = b.
\]</div>
<p>This form is conceptually clear but nondifferentiable since <span class="arithmatex">\(I_-\)</span> is discontinuous. To apply Newton-type algorithms, we replace <span class="arithmatex">\(I_-\)</span> with a smooth approximation: the logarithmic barrier.</p>
<h2 id="convex-19b_optimization_constraints-143-logarithmic-barrier-approximation">14.3 Logarithmic-Barrier Approximation<a class="headerlink" href="#convex-19b_optimization_constraints-143-logarithmic-barrier-approximation" title="Permanent link">¶</a></h2>
<p>We approximate each <span class="arithmatex">\(I_-(f_i(x))\)</span> by a differentiable barrier function <span class="arithmatex">\(\Phi(u) = -\tfrac{1}{t} \log(-u)\)</span> for <span class="arithmatex">\(u &lt; 0\)</span>. The smoothed subproblem becomes</p>
<div class="arithmatex">\[
\min_x \; f_0(x) - \frac{1}{t} \sum_{i=1}^m \log(-f_i(x))
\quad \text{s.t. } A x = b.
\]</div>
<ul>
<li>For small <span class="arithmatex">\(t\)</span>: the barrier is strong and keeps points deep inside the feasible region.  </li>
<li>As <span class="arithmatex">\(t \to \infty\)</span>: the barrier weakens and the solution approaches the true optimum.</li>
</ul>
<p>Hence, the original inequality-constrained problem is replaced by a sequence of smooth equality-constrained subproblems.</p>
<h2 id="convex-19b_optimization_constraints-144-properties-of-the-barrier-function">14.4 Properties of the Barrier Function<a class="headerlink" href="#convex-19b_optimization_constraints-144-properties-of-the-barrier-function" title="Permanent link">¶</a></h2>
<p>Define</p>
<div class="arithmatex">\[
\phi(x) = -\sum_{i=1}^m \log(-f_i(x)), \qquad
\mathrm{dom}\,\phi = \{x : f_i(x) &lt; 0\}.
\]</div>
<p>Then <span class="arithmatex">\(\phi\)</span> is convex and twice differentiable:</p>
<div class="arithmatex">\[
\nabla \phi(x) = \sum_i \frac{1}{-f_i(x)} \nabla f_i(x),
\]</div>
<div class="arithmatex">\[
\nabla^2 \phi(x) =
\sum_i \frac{1}{f_i(x)^2} \nabla f_i(x)\nabla f_i(x)^T
+ \sum_i \frac{1}{-f_i(x)} \nabla^2 f_i(x).
\]</div>
<p>Near the boundary <span class="arithmatex">\(f_i(x)=0\)</span>, the gradient norm grows without bound — producing a repulsive force that prevents violation of constraints.</p>
<h2 id="convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions">14.5 Central Path and Approximate KKT Conditions<a class="headerlink" href="#convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions" title="Permanent link">¶</a></h2>
<p>For each <span class="arithmatex">\(t &gt; 0\)</span>, let <span class="arithmatex">\(x^*(t)\)</span> minimize the barrier problem</p>
<div class="arithmatex">\[
\min_x\; t f_0(x) + \phi(x)
\quad \text{s.t. } A x = b.
\]</div>
<p>The curve <span class="arithmatex">\(\{x^*(t) : t &gt; 0\}\)</span> is the central path. As <span class="arithmatex">\(t \to \infty\)</span>, <span class="arithmatex">\(x^*(t)\)</span> approaches the true optimal solution <span class="arithmatex">\(x^*\)</span>. Along this path there exist dual variables <span class="arithmatex">\((\lambda^*(t), v^*(t))\)</span> satisfying</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla f_0(x^*(t)) + \sum_i \lambda_i^*(t) \nabla f_i(x^*(t)) + A^T v^*(t) &amp;= 0,\\
A x^*(t) &amp;= b,\\
-\lambda_i^*(t) f_i(x^*(t)) &amp;= \tfrac{1}{t}, \quad \lambda_i^*(t) \ge 0.
\end{aligned}
\]</div>
<p>The complementarity condition is relaxed: <span class="arithmatex">\(\lambda_i f_i(x) = -1/t\)</span> instead of <span class="arithmatex">\(0\)</span>. As <span class="arithmatex">\(t \to \infty\)</span>, these approximate KKT conditions converge to the exact ones.</p>
<h2 id="convex-19b_optimization_constraints-146-geometric-and-physical-intuition">14.6 Geometric and Physical Intuition<a class="headerlink" href="#convex-19b_optimization_constraints-146-geometric-and-physical-intuition" title="Permanent link">¶</a></h2>
<p>The centering subproblem</p>
<div class="arithmatex">\[
\min_x\; t f_0(x) - \sum_i \log(-f_i(x))
\]</div>
<p>can be viewed as a particle system in a potential field:</p>
<ul>
<li>The objective <span class="arithmatex">\(f_0(x)\)</span> pulls toward lower cost (external force).  </li>
<li>Each constraint <span class="arithmatex">\(f_i(x)\le0\)</span> creates a repulsive potential that diverges near the boundary.  </li>
</ul>
<p>At equilibrium, these forces balance:</p>
<div class="arithmatex">\[
\nabla f_0(x^*(t)) + \sum_i \frac{1}{t(-f_i(x^*(t)))} \nabla f_i(x^*(t)) = 0.
\]</div>
<p>Thus, the solution remains strictly feasible — this is the essence of the interior-point philosophy.</p>
<h2 id="convex-19b_optimization_constraints-147-barrier-method-algorithm">14.7 Barrier-Method Algorithm<a class="headerlink" href="#convex-19b_optimization_constraints-147-barrier-method-algorithm" title="Permanent link">¶</a></h2>
<p>The barrier method converts the original inequality-constrained problem into a sequence of smooth equality-constrained subproblems.<br>
Each subproblem is solved exactly (to high precision) while a <em>barrier parameter</em> <span class="arithmatex">\(t\)</span> is gradually increased, allowing the iterates to approach the boundary and the true constrained optimum.</p>
<h3 id="convex-19b_optimization_constraints-algorithm-outline">Algorithm Outline<a class="headerlink" href="#convex-19b_optimization_constraints-algorithm-outline" title="Permanent link">¶</a></h3>
<p>Given:</p>
<ul>
<li>a strictly feasible starting point <span class="arithmatex">\(x\)</span> (so <span class="arithmatex">\(f_i(x) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>),</li>
<li>an initial barrier parameter <span class="arithmatex">\(t &gt; 0\)</span>,</li>
<li>a barrier scaling factor <span class="arithmatex">\(\mu &gt; 1\)</span> (usually between 10 and 20),</li>
<li>and a desired accuracy <span class="arithmatex">\(\varepsilon &gt; 0\)</span> (for stopping),</li>
</ul>
<p>the algorithm proceeds as follows:</p>
<ol>
<li>
<p>Centering step: Solve<br>
<script type="math/tex; mode=display">
   \min_x \; f_0(x) - \frac{1}{t} \sum_{i=1}^m \log(-f_i(x))
   \quad \text{s.t. } A x = b
   </script>
   using Newton’s method for equality-constrained optimization. The result <span class="arithmatex">\(x^*(t)\)</span> is the <em>centering point</em> for the current <span class="arithmatex">\(t\)</span>.</p>
</li>
<li>
<p>Update iterate:  Set <span class="arithmatex">\(x := x^*(t)\)</span>.</p>
</li>
<li>
<p>Stopping criterion:  Stop if 
   <script type="math/tex; mode=display">
   \frac{m}{t} < \varepsilon.
   </script>
   Here <span class="arithmatex">\(m\)</span> is the number of inequality constraints, and <span class="arithmatex">\(\varepsilon\)</span> is the desired tolerance on suboptimality. This rule is derived from the duality gap bound:
   <script type="math/tex; mode=display">
   f_0(x^*(t)) - p^* \le \frac{m}{t},
   </script>
   meaning that if <span class="arithmatex">\(m/t\)</span> is smaller than <span class="arithmatex">\(\varepsilon\)</span>, the current solution is guaranteed to be within <span class="arithmatex">\(\varepsilon\)</span> of the true optimum.</p>
</li>
<li>
<p>Increase barrier parameter: Set <span class="arithmatex">\(t := \mu t\)</span> and return to Step 1.
   Each centering subproblem maintains strict feasibility, and increasing <span class="arithmatex">\(t\)</span> gradually weakens the barrier, allowing the iterates to approach the true constraint boundary. A typical choice is <span class="arithmatex">\(\mu \in [10, 20]\)</span>.</p>
</li>
</ol>
<h3 id="convex-19b_optimization_constraints-understanding-varepsilon-the-accuracy-parameter">Understanding <span class="arithmatex">\(\varepsilon\)</span> — the Accuracy Parameter<a class="headerlink" href="#convex-19b_optimization_constraints-understanding-varepsilon-the-accuracy-parameter" title="Permanent link">¶</a></h3>
<p>The parameter <span class="arithmatex">\(\varepsilon\)</span> controls how close to the optimal solution we wish to stop.</p>
<ul>
<li>
<p>Mathematically, <span class="arithmatex">\(\varepsilon\)</span> specifies an upper bound on the duality gap:
  <script type="math/tex; mode=display">
  f_0(x) - p^* \le \varepsilon.
  </script>
</p>
</li>
<li>
<p>Conceptually, <span class="arithmatex">\(\varepsilon\)</span> represents the trade-off between accuracy and computational cost:</p>
</li>
<li>Smaller <span class="arithmatex">\(\varepsilon\)</span> → more iterations (larger <span class="arithmatex">\(t\)</span> required).</li>
<li>Larger <span class="arithmatex">\(\varepsilon\)</span> → faster termination, but lower accuracy.</li>
</ul>
<p>In practice:</p>
<ul>
<li>For numerical optimization or ML training, <span class="arithmatex">\(\varepsilon\)</span> is often set between <span class="arithmatex">\(10^{-3}\)</span> and <span class="arithmatex">\(10^{-8}\)</span> depending on problem size and desired precision.  </li>
<li>Convex solvers (like CVX, MOSEK, or ECOS) typically use <span class="arithmatex">\(\varepsilon \approx 10^{-6}\)</span> as a default high-accuracy target.</li>
</ul>
<h3 id="convex-19b_optimization_constraints-intuitive-interpretation">Intuitive Interpretation<a class="headerlink" href="#convex-19b_optimization_constraints-intuitive-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>Think of <span class="arithmatex">\(\varepsilon\)</span> as the “distance” between the current point and the true optimum in terms of objective value.  </li>
<li>The ratio <span class="arithmatex">\(m/t\)</span> acts like a thermometer for this distance — as <span class="arithmatex">\(t\)</span> grows, the temperature (error) cools down.</li>
<li>Once <span class="arithmatex">\(m/t &lt; \varepsilon\)</span>, we know the algorithm has cooled sufficiently: the point lies extremely close to the optimal constrained solution.</li>
</ul>
<h3 id="convex-19b_optimization_constraints-summary-of-key-parameters">Summary of Key Parameters<a class="headerlink" href="#convex-19b_optimization_constraints-summary-of-key-parameters" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Typical Value / Range</th>
<th>Intuitive Role</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(m\)</span></td>
<td>Number of inequality constraints</td>
<td>problem dependent</td>
<td>Total number of barrier terms</td>
</tr>
<tr>
<td><span class="arithmatex">\(t\)</span></td>
<td>Barrier parameter</td>
<td>starts small (1–10), grows by <span class="arithmatex">\(\mu\)</span></td>
<td>Controls strength of barrier</td>
</tr>
<tr>
<td><span class="arithmatex">\(\mu\)</span></td>
<td>Barrier growth factor</td>
<td>10–20</td>
<td>Controls how fast we approach constraint boundary</td>
</tr>
<tr>
<td><span class="arithmatex">\(\varepsilon\)</span></td>
<td>Desired accuracy (tolerance)</td>
<td><span class="arithmatex">\(10^{-3}\)</span> to <span class="arithmatex">\(10^{-8}\)</span></td>
<td>Stopping threshold based on duality gap</td>
</tr>
</tbody>
</table>
<h3 id="convex-19b_optimization_constraints-intuitive-summary">Intuitive Summary<a class="headerlink" href="#convex-19b_optimization_constraints-intuitive-summary" title="Permanent link">¶</a></h3>
<ul>
<li>Each centering step finds the best <em>interior</em> point for a given barrier strength <span class="arithmatex">\(1/t\)</span>.  </li>
<li>Increasing <span class="arithmatex">\(t\)</span> reduces the barrier effect, letting <span class="arithmatex">\(x\)</span> approach the boundary.  </li>
<li>The stopping rule <span class="arithmatex">\(m/t &lt; \varepsilon\)</span> ensures that the objective value of <span class="arithmatex">\(x\)</span> differs from the true optimum by less than <span class="arithmatex">\(\varepsilon\)</span>.  </li>
<li>Smaller <span class="arithmatex">\(\varepsilon\)</span> means tighter optimality, but more work (larger <span class="arithmatex">\(t\)</span> and more iterations).</li>
</ul>
<h2 id="convex-19b_optimization_constraints-148-computational-and-practical-notes">14.8 Computational and Practical Notes<a class="headerlink" href="#convex-19b_optimization_constraints-148-computational-and-practical-notes" title="Permanent link">¶</a></h2>
<ul>
<li>Each centering problem is solved by equality-constrained Newton steps (KKT system).  </li>
<li>Barrier methods inherit superlinear convergence near the optimum.  </li>
<li>Initialization must be strictly feasible; feasibility restoration can be costly.  </li>
<li>Large <span class="arithmatex">\(t\)</span> makes the barrier steep, so line search and step damping are essential.</li>
</ul>
<p>In machine learning:
- SVM and logistic regression margin constraints fit naturally in this form.<br>
- Interior-point solvers for QPs are used in sparse regression and convex relaxations.<br>
- Barrier penalties act as smooth approximations to hard constraints in physics-informed and fairness-aware models.</p>
<h2 id="convex-19b_optimization_constraints-149-comparison-equality-vs-inequality-constrained-methods">14.9 Comparison: Equality vs Inequality-Constrained Methods<a class="headerlink" href="#convex-19b_optimization_constraints-149-comparison-equality-vs-inequality-constrained-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Equality Constraints</th>
<th>Inequality Constraints</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasible set</td>
<td>Affine manifold</td>
<td>Convex region with boundary</td>
</tr>
<tr>
<td>Algorithms</td>
<td>Newton, projected Newton, KKT</td>
<td>Barrier, interior-point, primal–dual</td>
</tr>
<tr>
<td>Feasibility handling</td>
<td>Exact</td>
<td>Maintained via barrier term</td>
</tr>
<tr>
<td>Complementarity</td>
<td><span class="arithmatex">\(A x = b\)</span></td>
<td><span class="arithmatex">\(\lambda_i f_i(x) = 0\)</span> (or <span class="arithmatex">\(= -1/t\)</span>)</td>
</tr>
<tr>
<td>Feasible start</td>
<td>Optional</td>
<td>Required (strict)</td>
</tr>
<tr>
<td>ML relevance</td>
<td>Normalization, fairness, balance</td>
<td>Nonnegativity, margins, sparsity, safety constraints</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-20_advanced" heading-number="2.15"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-15-advanced-large-scale-and-structured-methods">Chapter 15: Advanced Large-Scale and Structured Methods<a class="headerlink" href="#convex-20_advanced-chapter-15-advanced-large-scale-and-structured-methods" title="Permanent link">¶</a></h1>
<p>Modern convex optimization often operates at massive scales — millions of variables, billions of data points, or constraints distributed across devices and networks.<br>
Classical Newton or interior-point algorithms, while theoretically elegant, become computationally impractical in these regimes.  </p>
<p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to solve large-scale convex problems efficiently.<br>
These ideas underpin the optimization engines behind most machine learning systems.</p>
<h2 id="convex-20_advanced-151-motivation-structure-and-scale">15.1 Motivation: Structure and Scale<a class="headerlink" href="#convex-20_advanced-151-motivation-structure-and-scale" title="Permanent link">¶</a></h2>
<p>In large-scale convex optimization, the difficulty lies not in theory but in computation.</p>
<ul>
<li>Memory limits: Storing the full Hessian or even the gradient can be infeasible.  </li>
<li>Data size: Evaluating the objective over the full dataset is expensive.  </li>
<li>Distributed data: Information may be spread across machines or devices.  </li>
<li>Sparsity and separability: Many objectives decompose nicely into smaller components.</li>
</ul>
<p>Thus, the goal is to design algorithms that make incremental or local progress while exploiting the structure of the problem.</p>
<p>Typical forms include:
<script type="math/tex; mode=display">
f(x) = \frac{1}{N}\sum_{i=1}^N f_i(x) + R(x),
</script>
where:
- each <span class="arithmatex">\(f_i(x)\)</span> represents a data-sample loss term, and<br>
- <span class="arithmatex">\(R(x)\)</span> is a regularizer (possibly nonsmooth, such as <span class="arithmatex">\(\lambda\|x\|_1\)</span>).</p>
<h2 id="convex-20_advanced-152-coordinate-descent">15.2 Coordinate Descent<a class="headerlink" href="#convex-20_advanced-152-coordinate-descent" title="Permanent link">¶</a></h2>
<p>Coordinate descent updates a single variable (or a small block) at a time while holding others fixed.  </p>
<h3 id="convex-20_advanced-algorithm">Algorithm<a class="headerlink" href="#convex-20_advanced-algorithm" title="Permanent link">¶</a></h3>
<p>Given <span class="arithmatex">\(x^{(k)}\)</span>, choose coordinate <span class="arithmatex">\(i\)</span> and update:
<script type="math/tex; mode=display">
x_i^{(k+1)} = \arg\min_{z} f(x_1^{(k+1)}, \ldots, x_{i-1}^{(k+1)}, z, x_{i+1}^{(k)}, \ldots, x_n^{(k)}).
</script>
</p>
<p>This can be seen as projecting the gradient onto the coordinate directions. For separable problems, it is computationally much cheaper than full gradient updates.</p>
<ul>
<li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li>
<li>For problems with separable structure — e.g. sums over features, or regularisers like <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span> — the coordinate update is extremely cheap.</li>
<li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li>
</ul>
<p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>
<h3 id="convex-20_advanced-convergence">Convergence<a class="headerlink" href="#convex-20_advanced-convergence" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex with Lipschitz-continuous partial derivatives, cyclic or randomized coordinate descent converges to the global optimum.</p>
<h3 id="convex-20_advanced-ml-context">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context" title="Permanent link">¶</a></h3>
<p>Coordinate descent is widely used in:
- LASSO and Elastic Net regression (where updates are closed-form soft-thresholding),
- logistic regression with <span class="arithmatex">\(\ell_1\)</span> penalty,
- matrix factorization and dictionary learning.</p>
<h2 id="convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods">15.3 Stochastic Gradient and Variance-Reduced Methods<a class="headerlink" href="#convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods" title="Permanent link">¶</a></h2>
<p>When the dataset is large, computing the full gradient
<script type="math/tex; mode=display">
\nabla f(x) = \frac{1}{N} \sum_{i=1}^N \nabla f_i(x)
</script>
can be prohibitively expensive, since it requires evaluating all <span class="arithmatex">\(N\)</span> samples at every iteration. Stochastic methods overcome this by using <em>unbiased gradient estimates</em> based on small random subsets (mini-batches) of the data.</p>
<h3 id="convex-20_advanced-1531-stochastic-gradient-descent-sgd">15.3.1 Stochastic Gradient Descent (SGD)<a class="headerlink" href="#convex-20_advanced-1531-stochastic-gradient-descent-sgd" title="Permanent link">¶</a></h3>
<p>At each iteration, choose a random sample (or mini-batch) <span class="arithmatex">\(\mathcal{B}_k\)</span> and perform the update:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \eta_k \, \widehat{\nabla f}(x_k),
$$
where
$$
\widehat{\nabla f}(x_k)
= \frac{1}{|\mathcal{B}_k|} \sum_{i \in \mathcal{B}_k} \nabla f_i(x_k)
\]</div>
<p>is a stochastic estimate of the true gradient,<br>
and <span class="arithmatex">\(\eta_k &gt; 0\)</span> is the step size (learning rate).</p>
<h4 id="convex-20_advanced-interpretation">Interpretation<a class="headerlink" href="#convex-20_advanced-interpretation" title="Permanent link">¶</a></h4>
<ul>
<li>SGD performs a <em>noisy gradient step</em>: it moves in approximately the right direction on average.</li>
<li>The noise introduced by sampling allows exploration of the parameter space and helps escape shallow local minima in nonconvex problems.</li>
<li>In convex settings, it trades accuracy for computational efficiency — each iteration is much cheaper, so we can afford many more of them.</li>
</ul>
<h3 id="convex-20_advanced-1532-step-size-and-averaging">15.3.2 Step Size and Averaging<a class="headerlink" href="#convex-20_advanced-1532-step-size-and-averaging" title="Permanent link">¶</a></h3>
<p>The step size <span class="arithmatex">\(\eta_k\)</span> controls the bias–variance tradeoff:
- If <span class="arithmatex">\(\eta_k\)</span> is too large → iterates oscillate due to stochastic noise.
- If <span class="arithmatex">\(\eta_k\)</span> is too small → progress slows down.</p>
<p>Common choices:
<script type="math/tex; mode=display">
\eta_k = \frac{c}{\sqrt{k}} \quad \text{(for convex <span class="arithmatex">\(f\)</span>)}, 
\qquad
\eta_k = \frac{c}{k} \quad \text{(for strongly convex <span class="arithmatex">\(f\)</span>)}.
</script>
</p>
<p>Two popular stabilization strategies:</p>
<ol>
<li>
<p>Decay learning rate.</p>
</li>
<li>
<p>Polyak–Ruppert averaging:
   Instead of returning the last iterate, return the running average
   <script type="math/tex; mode=display">
   \bar{x}_k = \frac{1}{k}\sum_{t=1}^k x_t.
   </script>
   Averaging cancels gradient noise and ensures convergence to the optimal solution in expectation.</p>
</li>
<li>
<p>Increasing mini-batch size:<br>
   As optimization proceeds, increasing <span class="arithmatex">\(|\mathcal{B}_k|\)</span> gradually reduces gradient variance while keeping updates efficient.</p>
</li>
</ol>
<h3 id="convex-20_advanced-1533-convergence-properties">15.3.3 Convergence Properties<a class="headerlink" href="#convex-20_advanced-1533-convergence-properties" title="Permanent link">¶</a></h3>
<p>For convex objectives:
- <span class="arithmatex">\(\mathbb{E}[f(x_k)] - f^\star = O(1/\sqrt{k})\)</span> with diminishing <span class="arithmatex">\(\eta_k\)</span>.</p>
<p>For <em>strongly convex</em> <span class="arithmatex">\(f\)</span>, with <span class="arithmatex">\(\eta_k = O(1/k)\)</span>:
- <span class="arithmatex">\(\mathbb{E}[\|x_k - x^\star\|^2] = O(1/k)\)</span>.</p>
<p>These are optimal rates for stochastic first-order methods:  no unbiased stochastic optimizer using the same amount of data can asymptotically converge faster than SGD with Polyak averaging.</p>
<h3 id="convex-20_advanced-1534-variance-reduction">15.3.4 Variance Reduction<a class="headerlink" href="#convex-20_advanced-1534-variance-reduction" title="Permanent link">¶</a></h3>
<p>Although SGD is simple, the stochastic noise prevents it from reaching very high accuracy.  Variance-reduced methods (SVRG, SAGA, SARAH) correct this by mixing stochastic and full-gradient information.</p>
<p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p>
<p>At outer iteration <span class="arithmatex">\(s\)</span>, compute a full gradient snapshot <span class="arithmatex">\(\nabla f(\tilde{x})\)</span>.<br>
Then, for inner iterations:
<script type="math/tex; mode=display">
v_k = \nabla f_i(x_k) - \nabla f_i(\tilde{x}) + \nabla f(\tilde{x}),
\quad
x_{k+1} = x_k - \eta v_k.
</script>
</p>
<ul>
<li><span class="arithmatex">\(v_k\)</span> is an unbiased estimate of <span class="arithmatex">\(\nabla f(x_k)\)</span> but with reduced variance.</li>
<li>For strongly convex <span class="arithmatex">\(f\)</span>, SVRG and SAGA achieve linear convergence, bridging the gap between SGD and full gradient descent.</li>
</ul>
<p>Intuitively, these methods “anchor” stochastic gradients around a periodically refreshed reference point, preventing the gradient noise from accumulating.</p>
<h3 id="convex-20_advanced-1535-stochastic-second-order-and-momentum-methods">15.3.5 Stochastic Second-Order and Momentum Methods<a class="headerlink" href="#convex-20_advanced-1535-stochastic-second-order-and-momentum-methods" title="Permanent link">¶</a></h3>
<p>SGD can be further improved by incorporating curvature or momentum information.</p>
<ol>
<li>
<p>Momentum / Nesterov acceleration:<br>
   Maintains an exponential moving average of past gradients:
   <script type="math/tex; mode=display">
   m_k = \beta m_{k-1} + (1 - \beta) \widehat{\nabla f}(x_k),
   \quad
   x_{k+1} = x_k - \eta m_k.
   </script>
   Momentum accelerates convergence in smooth regions and damps oscillations in narrow valleys.</p>
</li>
<li>
<p>Adaptive methods (Adam, RMSProp, Adagrad):<br>
   Use coordinate-wise scaling based on running averages of squared gradients to handle ill-conditioned curvature.</p>
</li>
<li>
<p>Stochastic second-order methods:<br>
   Approximate curvature matrices (e.g., Fisher or Hessian) via stochastic estimates and maintain them with exponential decay:
   <script type="math/tex; mode=display">
   H_k \approx (1 - \rho) H_{k-1} + \rho \, g_k g_k^\top.
   </script>
   Though theoretically limited by SGD’s asymptotic rate, they often yield better pre-asymptotic performance — crucial in practical deep learning where only a few passes over the data are feasible.</p>
</li>
</ol>
<h3 id="convex-20_advanced-1536-machine-learning-context-and-insights">15.3.6 Machine Learning Context and Insights<a class="headerlink" href="#convex-20_advanced-1536-machine-learning-context-and-insights" title="Permanent link">¶</a></h3>
<ul>
<li>Deep neural networks rely almost exclusively on SGD and its adaptive or momentum-based variants. The stochasticity helps generalization by acting as implicit regularization.</li>
<li>Large-scale convex ML problems — logistic regression, SVMs, ridge regression — use SGD or variance-reduced methods (SVRG/SAGA) for scalability.</li>
<li>The balance between <em>variance reduction</em> and <em>computational cost</em> defines practical performance.</li>
</ul>
<h3 id="convex-20_advanced-1537-summary">15.3.7 Summary<a class="headerlink" href="#convex-20_advanced-1537-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Convergence</th>
<th>Practical Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>Uses mini-batch gradients</td>
<td><span class="arithmatex">\(O(1/\sqrt{k})\)</span></td>
<td>Deep learning, online learning</td>
</tr>
<tr>
<td>SGD + Polyak averaging</td>
<td>Averaged iterates</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Theoretically optimal stochastic convergence</td>
</tr>
<tr>
<td>SVRG / SAGA</td>
<td>Variance-reduced updates</td>
<td>Linear for strongly convex</td>
<td>Convex ML, GLMs</td>
</tr>
<tr>
<td>Momentum / Adam</td>
<td>Smoothed gradient estimates</td>
<td>Empirical acceleration</td>
<td>Deep nets</td>
</tr>
<tr>
<td>Stochastic 2nd-order</td>
<td>Curvature tracking</td>
<td>Better pre-asymptotic</td>
<td>Large-batch training</td>
</tr>
</tbody>
</table>
<h2 id="convex-20_advanced-154-proximal-and-composite-optimization">15.4 Proximal and Composite Optimization<a class="headerlink" href="#convex-20_advanced-154-proximal-and-composite-optimization" title="Permanent link">¶</a></h2>
<p>Many modern objectives combine a smooth loss and a nonsmooth regularizer:
<script type="math/tex; mode=display">
\min_x \; g(x) + R(x),
</script>
where <span class="arithmatex">\(g\)</span> is differentiable with Lipschitz gradient and <span class="arithmatex">\(R\)</span> is convex but possibly nonsmooth.</p>
<p>The proximal gradient method updates as:
<script type="math/tex; mode=display">
x_{k+1} = \mathrm{prox}_{\alpha R}(x_k - \alpha \nabla g(x_k)),
</script>
where the proximal operator is:
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha R}(v) = \arg\min_x \left( R(x) + \frac{1}{2\alpha}\|x-v\|^2 \right).
</script>
</p>
<h3 id="convex-20_advanced-intuition">Intuition<a class="headerlink" href="#convex-20_advanced-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>The gradient step moves in a descent direction for <span class="arithmatex">\(g\)</span>.  </li>
<li>The proximal step performs a local “denoising” or shrinkage under <span class="arithmatex">\(R\)</span> (e.g., soft-thresholding for <span class="arithmatex">\(\ell_1\)</span> norms).</li>
</ul>
<h3 id="convex-20_advanced-ml-context_1">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_1" title="Permanent link">¶</a></h3>
<p>Proximal methods underpin:
- Sparse regression (LASSO, Elastic Net),
- matrix completion and compressed sensing,
- total-variation image denoising,
- low-rank and structured regularization.</p>
<h2 id="convex-20_advanced-155-alternating-direction-method-of-multipliers-admm">15.5 Alternating Direction Method of Multipliers (ADMM)<a class="headerlink" href="#convex-20_advanced-155-alternating-direction-method-of-multipliers-admm" title="Permanent link">¶</a></h2>
<p>When an objective separates into parts that depend on different variables, ADMM enables efficient distributed optimization.</p>
<p>Consider:
<script type="math/tex; mode=display">
\min_{x,z}\; f(x) + g(z) \quad \text{s.t. } A x + B z = c.
</script>
</p>
<h3 id="convex-20_advanced-augmented-lagrangian">Augmented Lagrangian<a class="headerlink" href="#convex-20_advanced-augmented-lagrangian" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
L_\rho(x,z,y) = f(x) + g(z) + y^T(Ax + Bz - c) + \frac{\rho}{2}\|A x + B z - c\|^2.
\]</div>
<h3 id="convex-20_advanced-iterations">Iterations<a class="headerlink" href="#convex-20_advanced-iterations" title="Permanent link">¶</a></h3>
<p>ADMM performs alternating updates:
<script type="math/tex; mode=display">
\begin{aligned}
x^{k+1} &= \arg\min_x L_\rho(x, z^k, y^k),\\
z^{k+1} &= \arg\min_z L_\rho(x^{k+1}, z, y^k),\\
y^{k+1} &= y^k + \rho (A x^{k+1} + B z^{k+1} - c).
\end{aligned}
</script>
</p>
<h3 id="convex-20_advanced-interpretation_1">Interpretation<a class="headerlink" href="#convex-20_advanced-interpretation_1" title="Permanent link">¶</a></h3>
<p>Each step solves an easier subproblem involving only part of the variables, followed by a dual update to enforce consistency.<br>
ADMM thus merges ideas from dual ascent and penalty methods.</p>
<h3 id="convex-20_advanced-convergence_1">Convergence<a class="headerlink" href="#convex-20_advanced-convergence_1" title="Permanent link">¶</a></h3>
<p>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span>, ADMM converges to the global optimum.<br>
It is particularly effective when the subproblems are simple (e.g., proximal operators).</p>
<h3 id="convex-20_advanced-ml-context_2">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_2" title="Permanent link">¶</a></h3>
<p>ADMM is a key tool for:
- distributed LASSO and logistic regression,
- matrix decomposition and factorization,
- consensus optimization in federated learning,
- distributed deep learning regularization.</p>
<h2 id="convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms">15.6 Majorization–Minimization (MM) and EM Algorithms<a class="headerlink" href="#convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms" title="Permanent link">¶</a></h2>
<p>The MM principle iteratively minimizes a surrogate function that upper-bounds the objective.</p>
<p>Given a current point <span class="arithmatex">\(x_k\)</span>, construct a surrogate <span class="arithmatex">\(g(x|x_k)\)</span> such that:
<script type="math/tex; mode=display">
g(x|x_k) \ge f(x), \quad g(x_k|x_k) = f(x_k).
</script>
</p>
<p>Then update:
<script type="math/tex; mode=display">
x_{k+1} = \arg\min_x g(x|x_k).
</script>
</p>
<p>Each iteration ensures <span class="arithmatex">\(f(x_{k+1}) \le f(x_k)\)</span>.</p>
<h3 id="convex-20_advanced-ml-context_3">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_3" title="Permanent link">¶</a></h3>
<ul>
<li>The Expectation–Maximization (EM) algorithm is an MM method for latent-variable models.  </li>
<li>IRLS (Iteratively Reweighted Least Squares) for logistic regression and <span class="arithmatex">\(\ell_p\)</span> regression follows the same idea.  </li>
<li>MM methods guarantee descent even for complex, nonconvex objectives.</li>
</ul>
<h2 id="convex-20_advanced-157-distributed-and-parallel-optimization">15.7 Distributed and Parallel Optimization<a class="headerlink" href="#convex-20_advanced-157-distributed-and-parallel-optimization" title="Permanent link">¶</a></h2>
<p>For large-scale convex problems distributed across multiple nodes, parallel methods are essential.</p>
<h3 id="convex-20_advanced-synchronous-and-asynchronous-updates">Synchronous and Asynchronous Updates<a class="headerlink" href="#convex-20_advanced-synchronous-and-asynchronous-updates" title="Permanent link">¶</a></h3>
<ul>
<li>Synchronous: all workers compute updates and synchronize (used in federated averaging).  </li>
<li>Asynchronous: updates proceed without waiting, improving throughput but increasing variance.</li>
</ul>
<h3 id="convex-20_advanced-consensus-optimization">Consensus Optimization<a class="headerlink" href="#convex-20_advanced-consensus-optimization" title="Permanent link">¶</a></h3>
<p>In distributed convex optimization, one solves
<script type="math/tex; mode=display">
\min_{x_1,\dots,x_p} \sum_{i=1}^p f_i(x_i)
\quad \text{s.t. } x_i = z,
</script>
which can be handled by ADMM or primal–dual methods.<br>
Each machine optimizes its local copy <span class="arithmatex">\(x_i\)</span>, and the shared variable <span class="arithmatex">\(z\)</span> enforces consensus.</p>
<h3 id="convex-20_advanced-ml-context_4">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_4" title="Permanent link">¶</a></h3>
<ul>
<li>Federated learning and parameter-server training frameworks (e.g., TensorFlow Distributed, PyTorch DDP) follow this model.  </li>
<li>Decentralized convex optimization appears in sensor networks and multi-agent control.</li>
</ul>
<h2 id="convex-20_advanced-158-handling-structure-sparsity-and-low-rank">15.8 Handling Structure: Sparsity and Low Rank<a class="headerlink" href="#convex-20_advanced-158-handling-structure-sparsity-and-low-rank" title="Permanent link">¶</a></h2>
<p>Many convex problems exhibit special structures that algorithms can exploit:</p>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Typical Regularizer</th>
<th>Algorithmic Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sparsity</td>
<td><span class="arithmatex">\(\ell_1\)</span> or group lasso</td>
<td>Coordinate updates, proximal shrinkage</td>
</tr>
<tr>
<td>Low rank</td>
<td>nuclear norm <span class="arithmatex">\(\|X\|_*\)</span></td>
<td>SVD-based proximal step</td>
</tr>
<tr>
<td>Block separability</td>
<td><span class="arithmatex">\(\sum_i f_i(x_i)\)</span></td>
<td>Parallel or distributed updates</td>
</tr>
<tr>
<td>Graph structure</td>
<td>total variation norm</td>
<td>Local neighborhood computations</td>
</tr>
<tr>
<td>Simplex or probability constraints</td>
<td>entropy or KL penalty</td>
<td>Mirror descent, projected methods</td>
</tr>
</tbody>
</table>
<p>Exploiting such structure yields orders-of-magnitude speedups in both memory and computation.</p>
<h2 id="convex-20_advanced-159-summary-and-practical-guidance">15.9 Summary and Practical Guidance<a class="headerlink" href="#convex-20_advanced-159-summary-and-practical-guidance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Gradient Access</th>
<th>Scalability</th>
<th>Parallelization</th>
<th>Convexity Required</th>
<th>Typical ML Uses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent</td>
<td>Partial / coordinate</td>
<td>High</td>
<td>Easy</td>
<td>Convex</td>
<td>LASSO, sparse models</td>
</tr>
<tr>
<td>SGD / SVRG / SAGA</td>
<td>Stochastic</td>
<td>Excellent</td>
<td>Natural</td>
<td>Convex / nonconvex</td>
<td>Deep learning, logistic regression</td>
</tr>
<tr>
<td>Proximal Gradient</td>
<td>Full gradient + prox</td>
<td>Moderate–High</td>
<td>Easy</td>
<td>Convex</td>
<td>Composite objectives</td>
</tr>
<tr>
<td>ADMM</td>
<td>Separable subproblems</td>
<td>High</td>
<td>Distributed</td>
<td>Convex</td>
<td>Consensus, large convex solvers</td>
</tr>
<tr>
<td>MM / EM</td>
<td>Surrogate-based</td>
<td>Moderate</td>
<td>Model-specific</td>
<td>Convex / nonconvex</td>
<td>Probabilistic models, IRLS</td>
</tr>
<tr>
<td>Distributed / Federated</td>
<td>Local gradients</td>
<td>Very high</td>
<td>Essential</td>
<td>Convex / smooth</td>
<td>Federated learning, large-scale convex optimization</td>
</tr>
</tbody>
</table>
<h2 id="convex-20_advanced-1510-key-takeaways">15.10 Key Takeaways<a class="headerlink" href="#convex-20_advanced-1510-key-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>Large-scale convex optimization relies on exploiting structure, stochasticity, and separability.  </li>
<li>Coordinate and proximal methods handle sparse and composite problems efficiently.  </li>
<li>Stochastic and variance-reduced methods scale to massive data.  </li>
<li>ADMM and distributed optimization enable multi-machine or federated settings.  </li>
<li>MM and EM extend convex ideas to broader nonconvex inference tasks.</li>
</ul></body></html></section><section class="print-page" id="convex-21_models" heading-number="2.16"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-16-modelling-patterns-and-algorithm-selection">Chapter 16: Modelling Patterns and Algorithm Selection<a class="headerlink" href="#convex-21_models-chapter-16-modelling-patterns-and-algorithm-selection" title="Permanent link">¶</a></h1>
<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often <em>tells</em> us which solver class to use.  In practice, solving machine learning problems looks like: modeling → recognize structure → pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>
<h2 id="convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff">16.1 Regularized estimation and the accuracy–simplicity tradeoff<a class="headerlink" href="#convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff" title="Permanent link">¶</a></h2>
<p>Many learning tasks use a regularized risk minimization form:
<script type="math/tex; mode=display">
\min_x \; \underbrace{\text{loss}(x)}_{\text{data-fit}} \;+\; \lambda\;\underbrace{\text{penalty}(x)}_{\text{complexity}}.
</script>
Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing <span class="arithmatex">\(\lambda\)</span> trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p>
<ul>
<li>
<p>Ridge regression (ℓ₂ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
  </script>
<br>
  This arises from Gaussian noise (squared-error loss) plus a quadratic prior on <span class="arithmatex">\(x\)</span>.  It is a smooth, strongly convex quadratic problem (Hessian <span class="arithmatex">\(A^TA + \lambda I \succ 0\)</span>).  One can solve it via Newton’s method or closed‐form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p>
</li>
<li>
<p>LASSO / Sparse regression (ℓ₁ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \tfrac12\|Ax - b\|_2^2 + \lambda \|x\|_1.
  </script>
<br>
  The <span class="arithmatex">\(\ell_1\)</span> penalty encourages many <span class="arithmatex">\(x_i=0\)</span> (sparsity) for interpretability.  The problem is convex but nonsmooth (since <span class="arithmatex">\(|\cdot|\)</span> is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for <span class="arithmatex">\(\ell_1\)</span>, which sets small entries to zero.  Coordinate descent is another popular solver – updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p>
</li>
<li>
<p>Elastic net (mixed ℓ₁+ℓ₂):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda_1\|x\|_1 + \lambda_2\|x\|_2^2.
  </script>
<br>
  This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for <span class="arithmatex">\(\lambda_2&gt;0\)</span>) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the ℓ₂ term, the objective is smooth and unique solution.</p>
</li>
<li>
<p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block <span class="arithmatex">\(\ell_{2,1}\)</span> norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p>
</li>
</ul>
<p>Algorithmic pointers for 11.1:  </p>
<ul>
<li><em>Smooth+ℓ₂ (strongly convex)</em> → Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li>
<li><em>Smooth + ℓ₁</em> → Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li>
<li><em>Mixed penalties (ℓ₁+ℓ₂)</em> → Still convex; often handle like ℓ₁ case since smooth part dominates curvature.  </li>
<li><em>Large-scale data</em> → Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li>
</ul>
<p><em>Remarks:</em>  Choose <span class="arithmatex">\(\lambda\)</span> via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (<span class="arithmatex">\(n\)</span> large), coordinate or stochastic methods often outperform direct second-order methods.</p>
<h2 id="convex-21_models-162-robust-regression-and-outlier-resistance">16.2 Robust regression and outlier resistance<a class="headerlink" href="#convex-21_models-162-robust-regression-and-outlier-resistance" title="Permanent link">¶</a></h2>
<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>
<h3 id="convex-21_models-1621-least-absolute-deviations-l1-loss">16.2.1 Least absolute deviations (ℓ₁ loss)<a class="headerlink" href="#convex-21_models-1621-least-absolute-deviations-l1-loss" title="Permanent link">¶</a></h3>
<p>Formulation:
<script type="math/tex; mode=display">
\min_x \sum_i \lvert a_i^\top x - b_i \rvert.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li>
<li>Unlike squared error, it penalizes big residuals <em>linearly</em>, not quadratically, so outliers hurt less.</li>
</ul>
<p>Geometry/structure:
- The objective is convex but nondifferentiable at zero residual (the kink in <span class="arithmatex">\(|r|\)</span> at <span class="arithmatex">\(r=0\)</span>).</p>
<p>How to solve it:</p>
<ol>
<li>
<p>As a linear program (LP).<br>
   Introduce slack variables <span class="arithmatex">\(t_i \ge 0\)</span> and rewrite:</p>
<ul>
<li>constraints:<br>
<span class="arithmatex">\(-t_i \le a_i^\top x - b_i \le t_i\)</span>,</li>
<li>objective:<br>
<span class="arithmatex">\(\min \sum_i t_i\)</span>.</li>
</ul>
<p>This is now a standard LP. You can solve it with:</p>
<ul>
<li>an interior-point LP solver,</li>
<li>or simplex.</li>
</ul>
<p>These methods give high-accuracy solutions and certificates.</p>
</li>
<li>
<p>First-order methods for large scale.  </p>
<p>For <em>very</em> large problems (millions of samples/features), you can apply:</p>
<ul>
<li>subgradient methods,</li>
<li>proximal methods (using the prox of <span class="arithmatex">\(|\cdot|\)</span>).</li>
</ul>
<p>These are slower in theory (subgradient is only <span class="arithmatex">\(O(1/\sqrt{t})\)</span> convergence), but they scale to huge data where generic LP solvers would struggle.</p>
</li>
</ol>
<h3 id="convex-21_models-1622-huber-loss">16.2.2 Huber loss<a class="headerlink" href="#convex-21_models-1622-huber-loss" title="Permanent link">¶</a></h3>
<p>Definition of the Huber penalty for residual <span class="arithmatex">\(r\)</span>:
<script type="math/tex; mode=display">
\rho_\delta(r) =
\begin{cases}
\frac{1}{2} r^2, & |r| \le \delta, \\
\delta |r| - \frac{1}{2}\delta^2, & |r| > \delta.
\end{cases}
</script>
</p>
<p>Huber regression solves:
<script type="math/tex; mode=display">
\min_x \sum_i \rho_\delta(a_i^\top x - b_i).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For small residuals (<span class="arithmatex">\(|r|\le\delta\)</span>): it acts like least-squares (<span class="arithmatex">\(\tfrac{1}{2}r^2\)</span>). So inliers are fit tightly.</li>
<li>For large residuals (<span class="arithmatex">\(|r|&gt;\delta\)</span>): it acts like <span class="arithmatex">\(\ell_1\)</span> (linear penalty), so outliers get down-weighted.</li>
<li>Intuition: “be aggressive on normal data, be forgiving on outliers.”</li>
</ul>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(\rho_\delta\)</span> is convex.</li>
<li>It is smooth except for a kink in its second derivative at <span class="arithmatex">\(|r|=\delta\)</span>.</li>
<li>Its gradient exists everywhere (the function is once-differentiable).</li>
</ul>
<p>How to solve it:</p>
<ol>
<li>
<p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.<br>
    Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p>
</li>
<li>
<p>Proximal / first-order methods.<br>
    You can apply proximal gradient methods, since each term is simple and has a known prox.</p>
</li>
<li>
<p>As a conic program (SOCP).<br>
    The Huber objective can be written with auxiliary variables and second-order cone constraints.<br>
    That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.<br>
    This is attractive when you want high accuracy and dual certificates.</p>
</li>
</ol>
<h3 id="convex-21_models-1623-worst-case-robust-regression">16.2.3 Worst-case robust regression<a class="headerlink" href="#convex-21_models-1623-worst-case-robust-regression" title="Permanent link">¶</a></h3>
<p>Sometimes we don’t just want “fit the data we saw,” but “fit any data within some uncertainty set.” This leads to min–max problems of the form:
<script type="math/tex; mode=display">
\min_x \;\max_{u \in \mathcal{U}} \; \| (A + u)x - b \|_2.
</script>
</p>
<p>Meaning:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{U}\)</span> is an uncertainty set describing how much you distrust the matrix <span class="arithmatex">\(A\)</span>, the inputs, or the measurements.</li>
<li>You choose <span class="arithmatex">\(x\)</span> that performs well even in the worst allowed perturbation.</li>
</ul>
<p>Why this is still tractable:</p>
<ul>
<li>
<p>If <span class="arithmatex">\(\mathcal{U}\)</span> is convex (for example, an <span class="arithmatex">\(\ell_2\)</span> ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p>
</li>
<li>
<p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p>
<ul>
<li>Example: if the rows of <span class="arithmatex">\(A\)</span> can move within an <span class="arithmatex">\(\ell_2\)</span> ball of radius <span class="arithmatex">\(\epsilon\)</span>, the robustified problem often picks up an additional <span class="arithmatex">\(\ell_2\)</span> term like <span class="arithmatex">\(\gamma \|x\|_2\)</span> in the objective.</li>
<li>The final problem is still convex (often a QP or SOCP).</li>
</ul>
</li>
</ul>
<p>How to solve it:</p>
<ul>
<li>
<p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p>
</li>
<li>
<p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p>
</li>
</ul>
<h2 id="convex-21_models-163-maximum-likelihood-and-loss-design">16.3 Maximum likelihood and loss design<a class="headerlink" href="#convex-21_models-163-maximum-likelihood-and-loss-design" title="Permanent link">¶</a></h2>
<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p>
<ul>
<li>
<p>Gaussian (normal) noise</p>
<p>Model:
<script type="math/tex; mode=display">
b = A x + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I).
</script>
</p>
<p>The negative log-likelihood (NLL) is proportional to:
<script type="math/tex; mode=display">
|A x - b|_2^2.
</script>
</p>
<p>This recovers the classic least-squares loss (as in linear regression).<br>
It is smooth and convex (strongly convex if <span class="arithmatex">\(A^T A\)</span> is full rank).</p>
<p>Algorithms:</p>
<ul>
<li>
<p>Closed-form via <span class="arithmatex">\((A^T A + \lambda I)^{-1} A^T b\)</span> (for ridge regression),</p>
</li>
<li>
<p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p>
</li>
<li>
<p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian <span class="arithmatex">\(A^T A\)</span>.</p>
</li>
</ul>
</li>
<li>
<p>Laplace (double-exponential) noise</p>
<p>If <span class="arithmatex">\(\varepsilon_i \sim \text{Laplace}(0, b)\)</span> i.i.d., the NLL is proportional to:
<script type="math/tex; mode=display">
\sum_i |a_i^T x - b_i|.
</script>
</p>
<p>This is exactly the ℓ₁ regression (least absolute deviations).<br>
It can be solved as an LP or with robust optimization solvers (interior-point),<br>
or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p>
</li>
<li>
<p>Logistic model (binary classification)</p>
<p>For <span class="arithmatex">\(y_i \in \{0,1\}\)</span>, model:
<script type="math/tex; mode=display">
\Pr(y_i = 1 \mid a_i, x) = \sigma(a_i^T x),
\quad \text{where } \sigma(z) = \frac{1}{1 + e^{-z}}.
</script>
</p>
<p>The negative log-likelihood (logistic loss) is:
<script type="math/tex; mode=display">
\sum_i \left[ -y_i (a_i^T x) + \log(1 + e^{a_i^T x}) \right].
</script>
</p>
<p>This loss is convex and smooth in <span class="arithmatex">\(x\)</span>.<br>
No closed-form solution exists.</p>
<p>Algorithms:</p>
<ul>
<li>With ℓ₂ regularization: smooth and (if <span class="arithmatex">\(\lambda&gt;0\)</span>) strongly convex → use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li>
<li>With ℓ₁ regularization (sparse logistic): composite convex → use proximal gradient (soft-thresholding) or coordinate descent.</li>
</ul>
</li>
<li>
<p>Softmax / Multinomial logistic (multiclass)</p>
<p>For <span class="arithmatex">\(K\)</span> classes with one-hot labels <span class="arithmatex">\(y_i \in \{e_1, \dots, e_K\}\)</span>, the softmax model gives NLL:
<script type="math/tex; mode=display">
-\sum_i \sum_{k=1}^K y_{ik}(a_i^T x_k)
+ \log\!\left(\sum_{j=1}^K e^{a_i^T x_j}\right).
</script>
</p>
<p>This loss is convex in the weight vectors <span class="arithmatex">\(\{x_k\}\)</span> and generalizes binary logistic to multiclass.</p>
<p>Algorithms:</p>
<ul>
<li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li>
<li>Stochastic gradient (SGD, Adam) for large datasets.</li>
</ul>
</li>
<li>
<p>Generalized linear models (GLMs)</p>
<p>In GLMs, <span class="arithmatex">\(y_i\)</span> given <span class="arithmatex">\(x\)</span> has an exponential-family distribution (Poisson, binomial, etc.) with mean related to <span class="arithmatex">\(a_i^T x\)</span>.<br>
The NLL is convex in <span class="arithmatex">\(x\)</span> for canonical links (e.g. log-link for Poisson, logit for binomial).</p>
<p>Examples:</p>
<ul>
<li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li>
<li>Probit models: convex but require iterative solvers.</li>
</ul>
</li>
</ul>
<h2 id="convex-21_models-164-structured-constraints-in-engineering-and-design">16.4 Structured constraints in engineering and design<a class="headerlink" href="#convex-21_models-164-structured-constraints-in-engineering-and-design" title="Permanent link">¶</a></h2>
<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<ul>
<li>
<p>Simple (projection-friendly) constraints</p>
<p>Examples:</p>
<ul>
<li>
<p>Box constraints: <span class="arithmatex">\(l \le x \le u\)</span><br>
    → Projection: clip each entry to <span class="arithmatex">\([\ell_i, u_i]\)</span>.</p>
</li>
<li>
<p>ℓ₂-ball: <span class="arithmatex">\(\|x\|_2 \le R\)</span><br>
    → Projection: rescale <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(\|x\|_2 &gt; R\)</span>.</p>
</li>
<li>
<p>Simplex: <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span><br>
    → Projection: sort and threshold coordinates (simple <span class="arithmatex">\(O(n \log n)\)</span> algorithm).</p>
</li>
</ul>
</li>
<li>
<p>General convex constraints (non-projection-friendly)
If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p>
<ol>
<li>
<p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p>
</li>
<li>
<p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p>
</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers for 11.4:</p>
<ul>
<li>Projection-friendly constraints → Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li>
<li>Complex constraints (cones, PSD, many linear) → Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li>
<li>LP/QP special cases → Use simplex or specialized LP/QP solvers (Section 11.5).</li>
</ul>
<p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection → projective methods; otherwise → interior-point or operator-splitting.</p>
<h2 id="convex-21_models-165-linear-and-conic-programming-the-canonical-models">16.5 Linear and conic programming: the canonical models<a class="headerlink" href="#convex-21_models-165-linear-and-conic-programming-the-canonical-models" title="Permanent link">¶</a></h2>
<p>Many practical problems reduce to linear programming (LP) or its convex extensions.<br>
LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p>
<ul>
<li>
<p>Linear programs: standard form</p>
<p>
<script type="math/tex; mode=display">
\min_x \; c^T x 
\quad \text{s.t.} \quad A x = b, \; x \ge 0.
</script>
Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed.
- Quadratic, SOCP, SDP:
Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p>
</li>
<li>
<p>Practical patterns:</p>
<ol>
<li>Resource allocation/flow (LP): linear costs and constraints.</li>
<li>Minimax/regret problems: e.g. <span class="arithmatex">\(\min_{x}\max_{i}|a_i^T x - b_i|\)</span> → LP (as in Chebyshev regression).</li>
<li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers for 11.5:
- Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable).
- Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale.
- Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. ℓ∞ regression → LP, ℓ2 regression with ℓ2 constraint → SOCP.)</p>
<h2 id="convex-21_models-166-risk-safety-margins-and-robust-design">16.6 Risk, safety margins, and robust design<a class="headerlink" href="#convex-21_models-166-risk-safety-margins-and-robust-design" title="Permanent link">¶</a></h2>
<p>Modern design often includes risk measures or robustness. Two common patterns:</p>
<ul>
<li>
<p>Chance constraints / risk-adjusted objectives
    E.g. require that <span class="arithmatex">\(Pr(\text{loss}(x,\xi) &gt; \tau) \le \delta\)</span>. A convex surrogate is to include mean and a multiple of the standard deviation:
    <script type="math/tex; mode=display">
    \min_x \; \mathbb{E}[\ell(x, \xi)] + \kappa \sqrt{\mathrm{Var}[\ell(x, \xi)]}.
    </script>
    Algebra often leads to second-order cone constraints (e.g. forcing <span class="arithmatex">\(\mathbb{E}\pm \kappa\sqrt{\mathrm{Var}}\)</span> below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p>
</li>
<li>
<p>Worst-case (robust) optimization:
    Specify an uncertainty set <span class="arithmatex">\(\mathcal{U}\)</span> for data (e.g. <span class="arithmatex">\(u\)</span> in a norm-ball) and minimize the worst-case cost <span class="arithmatex">\(\max_{u\in\mathcal{U}}\ell(x,u)\)</span>. Many losses <span class="arithmatex">\(\ell\)</span> and convex <span class="arithmatex">\(\mathcal{U}\)</span> yield a convex max-term (a support function or norm). The result is often a conic constraint (for ℓ₂ norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p>
</li>
</ul>
<p>Algorithmic pointers for 11.6:</p>
<ul>
<li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li>
<li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li>
<li>Distributed or iterative solutions: If <span class="arithmatex">\(\mathcal{U}\)</span> or loss separable, ADMM can distribute the computation (Chapter 10).</li>
</ul>
<h2 id="convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that">16.7 Cheat sheet: If your problem looks like this, use that<a class="headerlink" href="#convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that" title="Permanent link">¶</a></h2>
<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p>
<ul>
<li>
<p>(A) Smooth least-squares + ℓ₂:</p>
<ul>
<li>Model: <span class="arithmatex">\(|Ax-b|_2^2 + \lambda|x|_2^2\)</span>. </li>
<li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic ⇒ fast second-order methods.)</li>
</ul>
</li>
<li>
<p>(B) Sparse regression (ℓ₁):</p>
<ul>
<li>Model: <span class="arithmatex">\(\tfrac12|Ax-b|_2^2 + \lambda|x|_1\)</span>. </li>
<li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li>
</ul>
</li>
<li>
<p>(C) Robust regression (outliers):</p>
<ul>
<li>Models: <span class="arithmatex">\(\sum|a_i^T x - b_i|\)</span>, Huber loss, etc. </li>
<li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li>
</ul>
</li>
<li>
<p>(D) Logistic / log-loss (classification):</p>
<ul>
<li>Model: <span class="arithmatex">\(\sum[-y_i(w^Ta_i)+\log(1+e^{w^Ta_i})] + \lambda R(w)\)</span> with <span class="arithmatex">\(R(w)=|w|_2^2\)</span> or <span class="arithmatex">\(|w|_1\)</span>. </li>
<li>Solve:<ul>
<li>If <span class="arithmatex">\(R=\ell_2\)</span>: use Newton/gradient (smooth, strongly convex).</li>
<li>If <span class="arithmatex">\(R=\ell_1\)</span>: use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; ℓ₁ adds nonsmoothness.)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>(E) Constraints (hard limits):</p>
<ul>
<li>Model: <span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(x\in\mathcal{X}\)</span> with <span class="arithmatex">\(\mathcal{X}\)</span> simple. </li>
<li>Solve: Projected (stochastic) gradient or proximal methods if projection <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> is cheap (e.g. box, ball, simplex). If <span class="arithmatex">\(\mathcal{X}\)</span> is complex (second-order or SDP), use interior-point.</li>
</ul>
</li>
<li>
<p>(F) Separable structure:</p>
<ul>
<li>Model: <span class="arithmatex">\(\min_{x,z} f(x)+g(z)\)</span> s.t. <span class="arithmatex">\(Ax+Bz=c\)</span>. </li>
<li>Solve: ADMM (Chapter 10) – it decouples updates in <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(z\)</span>; suits distributed or block-structured data.</li>
</ul>
</li>
<li>
<p>(G) LP/QP/SOCP/SDP:</p>
<ul>
<li>Model: linear/quadratic objective with linear/conic constraints. </li>
<li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li>
</ul>
</li>
<li>
<p>(H) Nonconvex patterns:</p>
<ul>
<li>
<p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p>
</li>
<li>
<p>Solve: There is no single global solver – typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p>
</li>
</ul>
</li>
<li>
<p>(I) Logistic (multi-class softmax):</p>
<ul>
<li>
<p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p>
</li>
<li>
<p>Solve: Similar to binary case – Newton/gradient with L2, or proximal/coordinate with ℓ₁.</p>
</li>
</ul>
</li>
<li>
<p>(J) Poisson and count models:</p>
<ul>
<li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li>
<li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li>
</ul>
</li>
</ul>
<p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p>
<ul>
<li>Smooth &amp; strongly convex → (quasi-)Newton or accelerated gradient.</li>
<li>Smooth + ℓ₁ → Proximal gradient/coordinate.</li>
<li>Nonsmooth separable → Proximal or coordinate.</li>
<li>Easy projection constraint → Projected gradient.</li>
<li>Hard constraints or conic structure → Interior-point.</li>
<li>Large-scale separable → Stochastic gradient/ADMM.</li>
</ul>
<p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>
<h2 id="convex-21_models-167-matching-model-structure-to-algorithm-type">16.7 Matching Model Structure to Algorithm Type<a class="headerlink" href="#convex-21_models-167-matching-model-structure-to-algorithm-type" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Problem Form</th>
<th>Recommended Algorithms</th>
<th>Notes / ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span></td>
<td>Gradient descent, Newton, LBFGS</td>
<td>Small to medium problems; logistic regression, ridge regression</td>
</tr>
<tr>
<td>Nonsmooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x) + R(x)\)</span></td>
<td>Subgradient, proximal (ISTA/FISTA), coordinate descent</td>
<td>LASSO, hinge loss SVM</td>
</tr>
<tr>
<td>Equality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(A x = b\)</span></td>
<td>Projected gradient, augmented Lagrangian</td>
<td>Constrained least squares, balance conditions</td>
</tr>
<tr>
<td>Inequality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le 0\)</span></td>
<td>Barrier, primal–dual, interior-point</td>
<td>Quadratic programming, LPs, constrained regression</td>
</tr>
<tr>
<td>Separable / block structure</td>
<td><span class="arithmatex">\(\min \sum_i f_i(x_i)\)</span></td>
<td>ADMM, coordinate updates</td>
<td>Distributed optimization, federated learning</td>
</tr>
<tr>
<td>Stochastic / large data</td>
<td><span class="arithmatex">\(\min \frac{1}{N}\sum_i f_i(x_i)\)</span></td>
<td>SGD, SVRG, adaptive variants</td>
<td>Deep learning, online convex optimization</td>
</tr>
<tr>
<td>Low-rank / matrix structure</td>
<td><span class="arithmatex">\(\min f(X) + \lambda \|X\|_*\)</span></td>
<td>Proximal (SVD shrinkage), ADMM</td>
<td>Matrix completion, PCA variants</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-30_canonical_problems" heading-number="2.17"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-17-canonical-problems-in-convex-optimization">Chapter 17: Canonical Problems in Convex Optimization<a class="headerlink" href="#convex-30_canonical_problems-chapter-17-canonical-problems-in-convex-optimization" title="Permanent link">¶</a></h1>
<p>Convex optimization encompasses a wide range of problem classes.<br>
Despite their diversity, many real-world models reduce to a few canonical forms — each with characteristic geometry, structure, and algorithms.</p>
<h2 id="convex-30_canonical_problems-171-hierarchy-of-canonical-problems">17.1 Hierarchy of Canonical Problems<a class="headerlink" href="#convex-30_canonical_problems-171-hierarchy-of-canonical-problems" title="Permanent link">¶</a></h2>
<p>Convex programs form a nested hierarchy:</p>
<div class="arithmatex">\[
\text{LP} \subseteq \text{QP} \subseteq \text{SOCP} \subseteq \text{SDP}.
\]</div>
<p>Each inclusion represents an extension of representational power — from linear to quadratic, to conic, and finally to semidefinite constraints.<br>
Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Canonical Form</th>
<th>Key Condition</th>
<th>Typical Algorithms</th>
<th>ML / Applied Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td><span class="arithmatex">\(\min_x c^\top x\)</span> s.t. <span class="arithmatex">\(A x=b,\,x\ge0\)</span></td>
<td>Linear constraints</td>
<td>Simplex, Interior-point</td>
<td>Resource allocation, Chebyshev regression</td>
</tr>
<tr>
<td>QP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top Q x + c^\top x\)</span> s.t. <span class="arithmatex">\(A x\le b\)</span></td>
<td><span class="arithmatex">\(Q\succeq0\)</span></td>
<td>Interior-point, Active-set, CG</td>
<td>Ridge, SVM, Portfolio optimization</td>
</tr>
<tr>
<td>QCQP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top P_0 x + q_0^\top x\)</span> s.t. <span class="arithmatex">\(\tfrac12 x^\top P_i x + q_i^\top x \le0\)</span></td>
<td>All <span class="arithmatex">\(P_i\succeq0\)</span></td>
<td>Interior-point, SOCP reformulation</td>
<td>Robust regression, trust-region</td>
</tr>
<tr>
<td>SOCP</td>
<td><span class="arithmatex">\(\min_x f^\top x\)</span> s.t. <span class="arithmatex">\(\|A_i x + b_i\|_2 \le c_i^\top x + d_i\)</span></td>
<td>Cone constraints</td>
<td>Conic interior-point</td>
<td>Robust least-squares, risk limits</td>
</tr>
<tr>
<td>SDP</td>
<td><span class="arithmatex">\(\min_X \mathrm{Tr}(C^\top X)\)</span> s.t. <span class="arithmatex">\(\mathrm{Tr}(A_i^\top X)=b_i\)</span>, <span class="arithmatex">\(X\succeq0\)</span></td>
<td>Matrix PSD constraint</td>
<td>Interior-point, low-rank first-order</td>
<td>Covariance estimation, control</td>
</tr>
<tr>
<td>GP</td>
<td><span class="arithmatex">\(\min_{x&gt;0} f_0(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le1,\,g_j(x)=1\)</span></td>
<td>Log-convex after <span class="arithmatex">\(y=\log x\)</span></td>
<td>Log-transform + IPM</td>
<td>Circuit design, power control</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>$\min_x -\sum_i \log p(b_i</td>
<td>a_i^\top x)+\mathcal{R}(x)$</td>
<td>Log-concave likelihood</td>
<td>Newton, L-BFGS, Prox / SGD</td>
</tr>
</tbody>
</table>
<h2 id="convex-30_canonical_problems-172-linear-programming-lp">17.2 Linear Programming (LP)<a class="headerlink" href="#convex-30_canonical_problems-172-linear-programming-lp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x c^\top x \quad \text{s.t. } A x=b,\, x\ge0
\]</div>
<p>Geometry: Feasible region = polyhedron; optimum = vertex.<br>
Applications: Resource allocation, shortest path, flow, scheduling.<br>
Algorithms:</p>
<ol>
<li>Simplex: walks along edges (vertex-based).  </li>
<li>Interior-point: moves through the interior using log barriers.  </li>
<li>Decomposition: exploits block structure for large LPs.</li>
</ol>
<h2 id="convex-30_canonical_problems-173-quadratic-programming-qp">17.3 Quadratic Programming (QP)<a class="headerlink" href="#convex-30_canonical_problems-173-quadratic-programming-qp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top Q x + c^\top x 
\quad \text{s.t. } A x \le b,\, F x = g, \quad Q\succeq0
\]</div>
<p>Geometry: Objective = ellipsoids; feasible = polyhedron.<br>
Examples: Ridge regression, Markowitz portfolio, SVM.<br>
Algorithms:<br>
- Interior-point (smooth path).<br>
- Active-set (edge-following).<br>
- Conjugate Gradient for large unconstrained QPs.<br>
- First-order methods for massive <span class="arithmatex">\(n\)</span>.</p>
<h2 id="convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp">17.4 Quadratically Constrained QP (QCQP)<a class="headerlink" href="#convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top P_0x + q_0^\top x
\quad \text{s.t. } \tfrac12 x^\top P_i x + q_i^\top x + r_i \le 0
\]</div>
<p>Convex if all <span class="arithmatex">\(P_i\succeq0\)</span>.<br>
Geometry: Intersection of ellipsoids and half-spaces.<br>
Applications: Robust control, filter design, trust-region.<br>
Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>
<h2 id="convex-30_canonical_problems-175-second-order-cone-programming-socp">17.5 Second-Order Cone Programming (SOCP)<a class="headerlink" href="#convex-30_canonical_problems-175-second-order-cone-programming-socp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x f^\top x
\quad \text{s.t. } 
\|A_i x + b_i\|_2 \le c_i^\top x + d_i,\;
F x = g
\]</div>
<p>Interpretation: Linear objective, norm-bounded constraints.<br>
Applications: Robust regression, risk-aware portfolio, engineering design.<br>
Algorithms: Conic interior-point; scalable ADMM variants.<br>
Special case: Any QP or norm constraint can be written as an SOCP.</p>
<h2 id="convex-30_canonical_problems-176-semidefinite-programming-sdp">17.6 Semidefinite Programming (SDP)<a class="headerlink" href="#convex-30_canonical_problems-176-semidefinite-programming-sdp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_X \mathrm{Tr}(C^\top X)
\quad \text{s.t. } \mathrm{Tr}(A_i^\top X)=b_i,\; X\succeq0
\]</div>
<p>Meaning: Variable = PSD matrix <span class="arithmatex">\(X\)</span>; constraints = affine.<br>
Geometry: Feasible region = intersection of affine space with PSD cone.<br>
Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion.<br>
Algorithms: Interior-point for moderate <span class="arithmatex">\(n\)</span>; low-rank proximal / Frank–Wolfe for large-scale.</p>
<h2 id="convex-30_canonical_problems-177-geometric-programming-gp">17.7 Geometric Programming (GP)<a class="headerlink" href="#convex-30_canonical_problems-177-geometric-programming-gp" title="Permanent link">¶</a></h2>
<p>Original form</p>
<div class="arithmatex">\[
\min_{x&gt;0} f_0(x)
\quad \text{s.t. } f_i(x)\le1,\; g_j(x)=1
\]</div>
<p>where <span class="arithmatex">\(f_i\)</span> are posynomials and <span class="arithmatex">\(g_j\)</span> monomials.  </p>
<p>Log transformation: With <span class="arithmatex">\(y=\log x\)</span>, the problem becomes convex in <span class="arithmatex">\(y\)</span>.<br>
Applications: Circuit sizing, communication power control, resource allocation.<br>
Solvers: Convert to convex form → interior-point or primal-dual methods.</p>
<h2 id="convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms">17.8 Likelihood-Based Convex Models (MLE and GLMs)<a class="headerlink" href="#convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms" title="Permanent link">¶</a></h2>
<p>General form</p>
<div class="arithmatex">\[
\min_x -\sum_i \log p(b_i|a_i^\top x) + \mathcal{R}(x)
\]</div>
<p>Examples</p>
<table>
<thead>
<tr>
<th>Noise Model</th>
<th>Objective</th>
<th>Equivalent Problem</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Least squares</td>
</tr>
<tr>
<td>Laplacian</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Robust regression (LP)</td>
</tr>
<tr>
<td>Bernoulli</td>
<td><span class="arithmatex">\(\sum_i \log(1+e^{-y_i a_i^\top x})\)</span></td>
<td>Logistic regression</td>
</tr>
<tr>
<td>Poisson</td>
<td><span class="arithmatex">\(\sum_i [a_i^\top x - y_i\log(a_i^\top x)]\)</span></td>
<td>Poisson GLM</td>
</tr>
</tbody>
</table>
<p>Algorithms<br>
- Newton or IRLS (small–medium).<br>
- Quasi-Newton / L-BFGS (moderate).<br>
- Proximal or SGD (large-scale).</p>
<h2 id="convex-30_canonical_problems-179-solver-selection-summary">17.9 Solver Selection Summary<a class="headerlink" href="#convex-30_canonical_problems-179-solver-selection-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Convex Form</th>
<th>Key Solvers</th>
<th>ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td>Linear</td>
<td>Simplex, Interior-point</td>
<td>Minimax regression</td>
</tr>
<tr>
<td>QP</td>
<td>Quadratic</td>
<td>Interior-point, CG, Active-set</td>
<td>Ridge, SVM</td>
</tr>
<tr>
<td>QCQP</td>
<td>Quadratic + constraints</td>
<td>IPM, SOCP / SDP reformulation</td>
<td>Robust regression</td>
</tr>
<tr>
<td>SOCP</td>
<td>Cone</td>
<td>Conic IPM, ADMM</td>
<td>Robust least-squares</td>
</tr>
<tr>
<td>SDP</td>
<td>PSD cone</td>
<td>Interior-point, low-rank FW</td>
<td>Covariance, Max-cut relaxations</td>
</tr>
<tr>
<td>GP</td>
<td>Log-convex</td>
<td>Log-transform + IPM</td>
<td>Power allocation</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>Log-concave</td>
<td>Newton, L-BFGS, Prox-SGD</td>
<td>Logistic regression</td>
</tr>
</tbody>
</table></body></html></section></section>
                    <section class='print-page md-section' id='section-3' heading-number='3'>
                        <h1>Optimization Beyond Convexity<a class='headerlink' href='#section-3' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="nonconvex-41_intro" heading-number="3.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-non-convex-optimization-fundamentals">Chapter 1: Non-Convex Optimization Fundamentals<a class="headerlink" href="#nonconvex-41_intro-chapter-1-non-convex-optimization-fundamentals" title="Permanent link">¶</a></h1>
<h2 id="nonconvex-41_intro-11-why-non-convexity-matters">1.1 Why Non-Convexity Matters<a class="headerlink" href="#nonconvex-41_intro-11-why-non-convexity-matters" title="Permanent link">¶</a></h2>
<p>Convex optimization ensures a unique global minimum and strong theoretical guarantees.
However, many practical problems in machine learning, deep learning, control, and physics are non-convex:</p>
<ul>
<li>Neural network loss surfaces</li>
<li>Reinforcement learning value functions</li>
<li>Matrix factorization</li>
<li>Clustering and combinatorial tasks</li>
</ul>
<p>These landscapes cannot be handled efficiently with traditional convex methods.</p>
<hr>
<h2 id="nonconvex-41_intro-12-characteristics-of-non-convex-landscapes">1.2 Characteristics of Non-Convex Landscapes<a class="headerlink" href="#nonconvex-41_intro-12-characteristics-of-non-convex-landscapes" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Consequence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multiple local minima</td>
<td>Many suboptimal valleys</td>
<td>Gradient descent may get trapped</td>
</tr>
<tr>
<td>Saddle points</td>
<td>Flat or neutral zones</td>
<td>Slow or no convergence</td>
</tr>
<tr>
<td>Discontinuities</td>
<td>Non-differentiable regions</td>
<td>Gradients undefined</td>
</tr>
<tr>
<td>Non-linearity</td>
<td>Coupled variables</td>
<td>Non-trivial curvature and topology</td>
</tr>
</tbody>
</table>
<p>Visualization of 2D loss surfaces often reveals chaotic or fractal-like geometry.</p>
<hr>
<h2 id="nonconvex-41_intro-13-gradient-based-methods-and-their-limits">1.3 Gradient-Based Methods and Their Limits<a class="headerlink" href="#nonconvex-41_intro-13-gradient-based-methods-and-their-limits" title="Permanent link">¶</a></h2>
<p>Even though SGD, Adam, and similar methods dominate deep learning, they:</p>
<ul>
<li>Depend heavily on initialization</li>
<li>May converge to poor local minima or plateaus</li>
<li>Are sensitive to learning rate and batch size</li>
<li>Cannot handle discrete or combinatorial variables</li>
</ul>
<p>This motivates global search strategies that can explore the space more broadly.</p>
<hr>
<h2 id="nonconvex-41_intro-14-toward-global-optimization">1.4 Toward Global Optimization<a class="headerlink" href="#nonconvex-41_intro-14-toward-global-optimization" title="Permanent link">¶</a></h2>
<p>Global optimization aims to find near-optimal solutions without convexity assumptions.
Two main families exist:</p>
<ol>
<li>Deterministic global methods — exhaustive, branch-and-bound, interval analysis</li>
<li>Stochastic and metaheuristic methods — probabilistic, adaptive, and nature-inspired</li>
</ol>
<p>The remainder of this book focuses on the latter, due to their flexibility and robustness in black-box settings.</p></body></html></section><section class="print-page" id="nonconvex-42_meta" heading-number="3.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-metaheuristic-optimization-methods">Chapter 2: Metaheuristic Optimization Methods<a class="headerlink" href="#nonconvex-42_meta-chapter-2-metaheuristic-optimization-methods" title="Permanent link">¶</a></h1>
<p>Metaheuristics are <strong>general-purpose stochastic search algorithms</strong> inspired by natural or social processes.
They do not require gradient information and are particularly powerful for <strong>non-convex</strong>, <strong>discrete</strong>, and <strong>black-box</strong> problems.</p>
<hr>
<h2 id="nonconvex-42_meta-21-core-principles">2.1 Core Principles<a class="headerlink" href="#nonconvex-42_meta-21-core-principles" title="Permanent link">¶</a></h2>
<p>Metaheuristics operate by balancing two key dynamics:</p>
<ul>
<li><strong>Exploration:</strong> Searching new, unvisited regions of the solution space</li>
<li><strong>Exploitation:</strong> Refining promising areas to improve solution quality</li>
</ul>
<p>Algorithms differ in how they maintain this balance — through temperature schedules, populations, or probabilistic moves.</p>
<hr>
<h2 id="nonconvex-42_meta-22-major-families-of-metaheuristics">2.2 Major Families of Metaheuristics<a class="headerlink" href="#nonconvex-42_meta-22-major-families-of-metaheuristics" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Examples</th>
<th>Inspiration</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Trajectory-based</strong></td>
<td>Simulated Annealing (SA)</td>
<td>Thermodynamics</td>
</tr>
<tr>
<td><strong>Evolutionary algorithms</strong></td>
<td>Genetic Algorithms (GA), Differential Evolution (DE)</td>
<td>Natural selection</td>
</tr>
<tr>
<td><strong>Swarm intelligence</strong></td>
<td>Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO)</td>
<td>Collective animal behavior</td>
</tr>
<tr>
<td><strong>Physics/chemistry inspired</strong></td>
<td>Harmony Search, Firefly Algorithm, Gravitational Search</td>
<td>Physical processes</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="nonconvex-42_meta-23-simulated-annealing-sa">2.3 Simulated Annealing (SA)<a class="headerlink" href="#nonconvex-42_meta-23-simulated-annealing-sa" title="Permanent link">¶</a></h2>
<ul>
<li>Mimics the cooling of metals.</li>
<li>Accepts worse moves with a probability <code>exp(-ΔE/T)</code>, allowing escape from local minima.</li>
<li>Temperature <code>T</code> decreases over time.</li>
</ul>
<blockquote>
<p><strong>Key idea:</strong> Controlled randomness to avoid premature convergence.</p>
</blockquote>
<hr>
<h2 id="nonconvex-42_meta-24-genetic-algorithms-ga">2.4 Genetic Algorithms (GA)<a class="headerlink" href="#nonconvex-42_meta-24-genetic-algorithms-ga" title="Permanent link">¶</a></h2>
<ul>
<li>Maintain a <strong>population</strong> of solutions.</li>
<li>Apply <strong>selection</strong>, <strong>crossover</strong>, and <strong>mutation</strong> to evolve toward better candidates.</li>
<li>Works well for discrete, combinatorial, or mixed-variable problems.</li>
</ul>
<blockquote>
<p><strong>Mathematical insight:</strong> Balances exploitation (selection) and exploration (mutation).</p>
</blockquote>
<hr>
<h2 id="nonconvex-42_meta-25-swarm-based-methods">2.5 Swarm-Based Methods<a class="headerlink" href="#nonconvex-42_meta-25-swarm-based-methods" title="Permanent link">¶</a></h2>
<p>Inspired by collective behaviors in nature:</p>
<ul>
<li><strong>PSO:</strong> Particles move based on personal and social bests</li>
<li><strong>ACO:</strong> Agents deposit pheromones to guide search</li>
<li><strong>Firefly Algorithm:</strong> Movement toward brighter (better) peers</li>
</ul>
<blockquote>
<p>These methods excel in <strong>continuous</strong> search spaces and dynamic environments.</p>
</blockquote>
<hr>
<h2 id="nonconvex-42_meta-26-theoretical-considerations">2.6 Theoretical Considerations<a class="headerlink" href="#nonconvex-42_meta-26-theoretical-considerations" title="Permanent link">¶</a></h2>
<p>Although metaheuristics lack strong convex guarantees, their <strong>stochastic convergence</strong> can be studied via:</p>
<ul>
<li>Markov chain analysis</li>
<li>Expected improvement over iterations</li>
<li>Diversity measures within populations</li>
</ul>
<p>They often converge <strong>probabilistically</strong> to a near-optimal region rather than a single guaranteed optimum.</p></body></html></section><section class="print-page" id="nonconvex-43_hybrid" heading-number="3.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-hybrid-and-modern-optimization-methods">Chapter 3: Hybrid and Modern Optimization Methods<a class="headerlink" href="#nonconvex-43_hybrid-chapter-3-hybrid-and-modern-optimization-methods" title="Permanent link">¶</a></h1>
<p>Modern optimization integrates <strong>heuristic exploration</strong> with <strong>mathematical precision</strong>.
Hybrid and adaptive approaches leverage the strengths of both global and local methods.</p>
<hr>
<h2 id="nonconvex-43_hybrid-31-hybrid-metaheuristics">3.1 Hybrid Metaheuristics<a class="headerlink" href="#nonconvex-43_hybrid-31-hybrid-metaheuristics" title="Permanent link">¶</a></h2>
<p>Combine metaheuristics with classical optimization to accelerate convergence:</p>
<ul>
<li><strong>Memetic algorithms:</strong> GA + local search refinement</li>
<li><strong>Hybrid PSO:</strong> PSO with gradient descent fine-tuning</li>
<li><strong>Adaptive Simulated Annealing:</strong> Dynamic temperature and step size</li>
</ul>
<blockquote>
<p>The goal: exploit global search for exploration, and analytical methods for exploitation.</p>
</blockquote>
<hr>
<h2 id="nonconvex-43_hybrid-32-multi-objective-optimization">3.2 Multi-Objective Optimization<a class="headerlink" href="#nonconvex-43_hybrid-32-multi-objective-optimization" title="Permanent link">¶</a></h2>
<p>Many real-world problems have <strong>conflicting objectives</strong>, e.g., accuracy vs interpretability.</p>
<ul>
<li>Represent trade-offs via the <strong>Pareto front</strong></li>
<li>Search for <strong>non-dominated</strong> solutions using evolutionary multi-objective algorithms (e.g., <strong>NSGA-II</strong>, <strong>MOEA/D</strong>)</li>
<li>Use crowding distance and rank-based selection for diversity</li>
</ul>
<blockquote>
<p>These methods underpin design trade-offs in engineering and AutoML pipelines.</p>
</blockquote>
<hr>
<h2 id="nonconvex-43_hybrid-33-constraint-handling">3.3 Constraint Handling<a class="headerlink" href="#nonconvex-43_hybrid-33-constraint-handling" title="Permanent link">¶</a></h2>
<p>Constraints are incorporated via:</p>
<ul>
<li><strong>Penalty functions:</strong> Add cost for violations</li>
<li><strong>Repair mechanisms:</strong> Project invalid solutions back into feasible space</li>
<li><strong>Decoders:</strong> Convert unconstrained representations into feasible solutions</li>
</ul>
<p>These are essential for optimization in robotics, control, and combinatorial planning.</p>
<hr>
<h2 id="nonconvex-43_hybrid-34-modern-directions">3.4 Modern Directions<a class="headerlink" href="#nonconvex-43_hybrid-34-modern-directions" title="Permanent link">¶</a></h2>
<h3 id="nonconvex-43_hybrid-1-reinforcement-learning-and-evolution"><strong>1. Reinforcement Learning and Evolution</strong><a class="headerlink" href="#nonconvex-43_hybrid-1-reinforcement-learning-and-evolution" title="Permanent link">¶</a></h3>
<ul>
<li>Neuroevolution (e.g., NEAT)</li>
<li>Policy optimization via evolutionary strategies</li>
</ul>
<h3 id="nonconvex-43_hybrid-2-bayesian-and-surrogate-optimization"><strong>2. Bayesian and Surrogate Optimization</strong><a class="headerlink" href="#nonconvex-43_hybrid-2-bayesian-and-surrogate-optimization" title="Permanent link">¶</a></h3>
<ul>
<li>Gaussian processes + exploration policies</li>
<li>Efficient black-box optimization (used in hyperparameter tuning)</li>
</ul>
<h3 id="nonconvex-43_hybrid-3-quantum-inspired-and-neuro-symbolic-search"><strong>3. Quantum-Inspired and Neuro-symbolic Search</strong><a class="headerlink" href="#nonconvex-43_hybrid-3-quantum-inspired-and-neuro-symbolic-search" title="Permanent link">¶</a></h3>
<ul>
<li>Quantum annealing analogies</li>
<li>Neural controllers guiding metaheuristics</li>
<li>AutoML as meta-level optimization</li>
</ul>
<hr>
<h2 id="nonconvex-43_hybrid-35-summary">3.5 Summary<a class="headerlink" href="#nonconvex-43_hybrid-35-summary" title="Permanent link">¶</a></h2>
<p>Hybrid and modern metaheuristics represent a convergence of <strong>mathematics, biology, and computation</strong>.
They embrace stochasticity not as noise, but as a powerful tool for discovering high-quality solutions in complex, non-convex landscapes.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-4' heading-number='4'>
                        <h1>Deep Learning<a class='headerlink' href='#section-4' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="deeplearning-1_mlp" heading-number="4.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="an-introduction-to-neural-networks">An Introduction to Neural Networks<a class="headerlink" href="#deeplearning-1_mlp-an-introduction-to-neural-networks" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-1_mlp-1-neural-networks-as-computation-graphs">1. Neural Networks as Computation Graphs<a class="headerlink" href="#deeplearning-1_mlp-1-neural-networks-as-computation-graphs" title="Permanent link">¶</a></h2>
<p>Modern neural networks are best understood as differentiable computation graphs. 
They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p>
<p>Each node in this graph corresponds to a function:</p>
<div class="arithmatex">\[z_i = f_i(x_1, \dots, x_k)\]</div>
<p>and the entire network defines a composite function:</p>
<div class="arithmatex">\[f_\theta(x) = f_L \circ f_{L-1} \circ \dots \circ f_1(x)\]</div>
<p>where <span class="arithmatex">\(\theta = \{W_i, b_i\}\)</span> denotes all learnable parameters.</p>
<h3 id="deeplearning-1_mlp-formal-structure">Formal Structure<a class="headerlink" href="#deeplearning-1_mlp-formal-structure" title="Permanent link">¶</a></h3>
<p>For a Multilayer Perceptron (MLP):</p>
<div class="arithmatex">\[h_0 = x, \quad h_i = \sigma(W_i h_{i-1} + b_i), \quad i=1,\dots,L-1, \quad \hat{y} = W_L h_{L-1} + b_L\]</div>
<p>with: <span class="arithmatex">\(W_i \in \mathbb{R}^{d_i \times d_{i-1}}, \quad b_i \in \mathbb{R}^{d_i}\)</span></p>
<p>Each layer is a small differentiable function. When we connect them, we form a composite map — the fundamental abstraction underlying <em>autodiff</em>, <em>backprop</em>, and <em>learning</em>.</p>
<p>Key property: Because every node in the graph is differentiable, the entire function <span class="arithmatex">\(f_\theta(x)\)</span> is differentiable with respect to both input <span class="arithmatex">\(x\)</span> and parameters <span class="arithmatex">\(\theta\)</span>.</p>
<p>Graphically, the network is a directed acyclic graph (DAG):</p>
<ul>
<li>Edges: carry tensor values.</li>
<li>Nodes: represent differentiable functions.</li>
<li>Forward pass: evaluates node outputs.</li>
<li>Backward pass: propagates sensitivities (gradients) backward.</li>
</ul>
<blockquote>
<p>This graph abstraction unifies all architectures — CNNs, RNNs, Transformers, Diffusion Models — as differentiable computation graphs.</p>
</blockquote>
<h2 id="deeplearning-1_mlp-2-gradients-jacobians-and-differentiation">2. Gradients, Jacobians, and Differentiation<a class="headerlink" href="#deeplearning-1_mlp-2-gradients-jacobians-and-differentiation" title="Permanent link">¶</a></h2>
<p>For any function <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian matrix <span class="arithmatex">\(J_f(x)\)</span> encodes local derivatives:</p>
<div class="arithmatex">\[[J_f(x)]_{ij} = \frac{\partial f_i}{\partial x_j}\]</div>
<p>In neural networks, we often deal with a scalar loss function:</p>
<div class="arithmatex">\[L(\theta) = \ell(f_\theta(x), y)\]</div>
<p>and want: </p>
<div class="arithmatex">\[\nabla_\theta L = \frac{\partial L}{\partial \theta}\]</div>
<p>However, computing full Jacobians is computationally infeasible — for a network with millions of parameters, explicit Jacobians would have trillions of entries.<br>
Instead, automatic differentiation (autodiff) computes vector–Jacobian products efficiently.</p>
<p>For scalar loss <span class="arithmatex">\(L\)</span>: <span class="arithmatex">\(\nabla_\theta L = J_{f_\theta}(x)^T \nabla_{f_\theta} L\)</span></p>
<p>where <span class="arithmatex">\(J_{f_\theta}(x)\)</span> is the Jacobian of the output w.r.t. parameters.</p>
<p>This operation can be done efficiently in reverse-mode autodiff — the heart of backpropagation.</p>
<hr>
<h2 id="deeplearning-1_mlp-3-forward-and-backward-passes">3. Forward and Backward Passes<a class="headerlink" href="#deeplearning-1_mlp-3-forward-and-backward-passes" title="Permanent link">¶</a></h2>
<h4 id="deeplearning-1_mlp-forward-pass">Forward Pass<a class="headerlink" href="#deeplearning-1_mlp-forward-pass" title="Permanent link">¶</a></h4>
<p>Given input <span class="arithmatex">\(x\)</span> and parameters <span class="arithmatex">\(\theta\)</span>:</p>
<ol>
<li>Compute layer outputs sequentially: <span class="arithmatex">\(h_i = \sigma(W_i h_{i-1} + b_i)\)</span></li>
<li>Compute loss <span class="arithmatex">\(L = \ell(f_\theta(x), y)\)</span></li>
<li>Store intermediate activations <span class="arithmatex">\(h_i\)</span> for reuse during backpropagation.</li>
</ol>
<p>This pass evaluates the function <span class="arithmatex">\(L(\theta)\)</span>.</p>
<h4 id="deeplearning-1_mlp-backward-pass">Backward Pass<a class="headerlink" href="#deeplearning-1_mlp-backward-pass" title="Permanent link">¶</a></h4>
<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p>
<p><span class="arithmatex">\(\frac{\partial L}{\partial \theta_i} = 
\frac{\partial L}{\partial h_L}
\frac{\partial h_L}{\partial h_{L-1}}
\dots
\frac{\partial h_{i+1}}{\partial \theta_i}\)</span></p>
<p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p>
<p>Reverse-mode autodiff (backprop) algorithm:</p>
<ol>
<li>Initialize <span class="arithmatex">\(\bar{h}_L = \frac{\partial L}{\partial h_L} = 1\)</span>.</li>
<li>For each layer <span class="arithmatex">\(l = L, L-1, \dots, 1\)</span>:</li>
<li>Compute local derivative <span class="arithmatex">\(\frac{\partial h_l}{\partial h_{l-1}}\)</span></li>
<li>Accumulate gradient:<br>
<span class="arithmatex">\(\bar{h}_{l-1} = \bar{h}_l \frac{\partial h_l}{\partial h_{l-1}}\)</span></li>
<li>Compute parameter gradients:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial W_l} = \bar{h}_l (h_{l-1})^T\)</span></li>
<li>Return all <span class="arithmatex">\(\nabla_\theta L\)</span>.</li>
</ol>
<p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>
<h2 id="deeplearning-1_mlp-4-chain-rule-backpropagation-and-automatic-differentiation">4. Chain Rule, Backpropagation, and Automatic Differentiation<a class="headerlink" href="#deeplearning-1_mlp-4-chain-rule-backpropagation-and-automatic-differentiation" title="Permanent link">¶</a></h2>
<p>The chain rule underpins all gradient computation.<br>
For scalar functions:</p>
<p><span class="arithmatex">\(\frac{dL}{dx} = \frac{dL}{dz} \frac{dz}{dx}\)</span></p>
<p>and recursively for multivariate functions:</p>
<p><span class="arithmatex">\(\nabla_x L = J_{z}(x)^T \nabla_z L\)</span></p>
<p>Autodiff implements this automatically, performing either:</p>
<ul>
<li>Forward-mode AD: propagates derivatives forward, efficient when #inputs ≪ #outputs.</li>
<li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs ≪ #inputs (our case).</li>
</ul>
<p>Reverse-mode AD ≡ backpropagation.</p>
<p>Computational Complexity:
- Cost ≈ 2× forward pass (one forward, one backward).
- Memory ≈ size of stored activations.</p>
<p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p>
<p><span class="arithmatex">\(\min_\theta L(\theta)\)</span></p>
<p>where <span class="arithmatex">\(L\)</span> is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms.
s</p>
<h2 id="deeplearning-1_mlp-5-from-gradients-to-optimization">5. From Gradients to Optimization<a class="headerlink" href="#deeplearning-1_mlp-5-from-gradients-to-optimization" title="Permanent link">¶</a></h2>
<p>The Learning Problem - Training a neural network means solving:</p>
<p><span class="arithmatex">\(\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}} [\,\ell(f_\theta(x), y)\,]\)</span></p>
<p>Since the true data distribution <span class="arithmatex">\(\mathcal{D}\)</span> is unknown, we use empirical risk minimization (ERM):</p>
<p><span class="arithmatex">\(\min_\theta \frac{1}{N} \sum_{i=1}^N \ell(f_\theta(x_i), y_i)\)</span></p>
<p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods — powered by backpropagation — reliably find good solutions.</p>
<h3 id="deeplearning-1_mlp-first-order-optimization-algorithms">First-Order Optimization Algorithms<a class="headerlink" href="#deeplearning-1_mlp-first-order-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>All modern deep learning optimization relies on gradients:</p>
<p><span class="arithmatex">\(\nabla_\theta L = \frac{\partial L}{\partial \theta}\)</span></p>
<p>The basic rule: update parameters in the direction of <em>negative gradient</em>:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \nabla_\theta L_t\)</span></p>
<p>where <span class="arithmatex">\(\eta\)</span> is the learning rate.</p>
<h4 id="deeplearning-1_mlp-stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)<a class="headerlink" href="#deeplearning-1_mlp-stochastic-gradient-descent-sgd" title="Permanent link">¶</a></h4>
<p>We use mini-batches instead of full data:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \nabla_\theta \frac{1}{|B_t|}\sum_{i \in B_t} \ell(f_\theta(x_i), y_i)\)</span></p>
<ul>
<li>Cheap per-step computation.</li>
<li>Introduces <em>gradient noise</em>, which helps escape shallow minima and saddle points.</li>
</ul>
<h4 id="deeplearning-1_mlp-momentum">Momentum<a class="headerlink" href="#deeplearning-1_mlp-momentum" title="Permanent link">¶</a></h4>
<p>Accelerates learning by accumulating a velocity vector:</p>
<p><span class="arithmatex">\(v_{t+1} = \mu v_t - \eta \nabla_\theta L_t, \quad \theta_{t+1} = \theta_t + v_{t+1}\)</span></p>
<p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>
<h4 id="deeplearning-1_mlp-adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)<a class="headerlink" href="#deeplearning-1_mlp-adam-adaptive-moment-estimation" title="Permanent link">¶</a></h4>
<p>Maintains exponentially weighted averages of gradients and squared gradients:</p>
<p><span class="arithmatex">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1)\nabla_\theta L_t\)</span></p>
<p><span class="arithmatex">\(v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta L_t)^2\)</span></p>
<p>Bias-corrected updates:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\)</span></p>
<p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>
<h4 id="deeplearning-1_mlp-second-order-and-curvature-aware-methods">Second-Order and Curvature-Aware Methods<a class="headerlink" href="#deeplearning-1_mlp-second-order-and-curvature-aware-methods" title="Permanent link">¶</a></h4>
<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p>
<p><span class="arithmatex">\(H = \frac{\partial^2 L}{\partial \theta^2}\)</span></p>
<p>Newton’s update:</p>
<p><span class="arithmatex">\(\theta_{t+1} = \theta_t - H^{-1}\nabla_\theta L\)</span></p>
<p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets.<br>
Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>
<h3 id="deeplearning-1_mlp-optimization-landscape-and-gradient-flow">Optimization Landscape and Gradient Flow<a class="headerlink" href="#deeplearning-1_mlp-optimization-landscape-and-gradient-flow" title="Permanent link">¶</a></h3>
<p>Although neural network loss surfaces are highly nonconvex, they possess <em>favorable geometry</em>:</p>
<ul>
<li>Most critical points are saddle points, not local minima.</li>
<li>Wide, flat minima generalize better (implicit regularization of SGD).</li>
<li>Gradient noise helps explore valleys in high-dimensional space.</li>
</ul>
<p>Gradient flow (continuous limit of SGD):</p>
<p><span class="arithmatex">\(\frac{d\theta(t)}{dt} = - \nabla_\theta L(\theta(t))\)</span></p>
<p>describes a trajectory in parameter space governed by the vector field of gradients.</p>
<p>The optimization algorithm defines the <em>dynamics</em> of this flow (e.g., momentum adds inertia).</p>
<h2 id="deeplearning-1_mlp-6-what-mlps-cant-do">6. What MLPs Can’t Do?<a class="headerlink" href="#deeplearning-1_mlp-6-what-mlps-cant-do" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-1_mlp-a-multiplicative-interactions">(a) Multiplicative Interactions<a class="headerlink" href="#deeplearning-1_mlp-a-multiplicative-interactions" title="Permanent link">¶</a></h3>
<p>MLPs compute sums of weighted activations — inherently <em>additive</em> operations:</p>
<p><span class="arithmatex">\(h = \sigma(Wx + b)\)</span></p>
<p>They cannot naturally represent multiplicative relationships (like <span class="arithmatex">\(x_1 x_2\)</span>) unless approximated via nonlinear stacking, which is inefficient.</p>
<p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>
<h3 id="deeplearning-1_mlp-b-attention-and-dynamic-routing">(b) Attention and Dynamic Routing<a class="headerlink" href="#deeplearning-1_mlp-b-attention-and-dynamic-routing" title="Permanent link">¶</a></h3>
<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p>
<p><span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V\)</span></p>
<p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>
<h3 id="deeplearning-1_mlp-c-metric-learning-and-inductive-bias">(c) Metric Learning and Inductive Bias<a class="headerlink" href="#deeplearning-1_mlp-c-metric-learning-and-inductive-bias" title="Permanent link">¶</a></h3>
<p>MLPs lack structural priors about similarity or geometry.<br>
Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p>
<p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient — the landscape becomes smoother and gradients more informative.</p>
<h2 id="deeplearning-1_mlp-7-beyond-backprop-curvature-generalization-and-geometry">7. Beyond Backprop: Curvature, Generalization, and Geometry<a class="headerlink" href="#deeplearning-1_mlp-7-beyond-backprop-curvature-generalization-and-geometry" title="Permanent link">¶</a></h2>
<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>
<h3 id="deeplearning-1_mlp-natural-gradient">Natural Gradient<a class="headerlink" href="#deeplearning-1_mlp-natural-gradient" title="Permanent link">¶</a></h3>
<p>Instead of minimizing loss directly in parameter space, we minimize it in <em>function space</em>:</p>
<p><span class="arithmatex">\(\Delta \theta = - \eta F^{-1} \nabla_\theta L\)</span></p>
<p>where <span class="arithmatex">\(F\)</span> is the Fisher information matrix:</p>
<p><span class="arithmatex">\(F = \mathbb{E}\left[\nabla_\theta \log p_\theta(x) \nabla_\theta \log p_\theta(x)^T\right]\)</span></p>
<p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>
<h3 id="deeplearning-1_mlp-implicit-bias-of-gradient-descent">Implicit Bias of Gradient Descent<a class="headerlink" href="#deeplearning-1_mlp-implicit-bias-of-gradient-descent" title="Permanent link">¶</a></h3>
<p>Even in overparameterized models, gradient descent tends to find <em>low-norm</em> or <em>flat</em> minima that generalize better — a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>
<h3 id="deeplearning-1_mlp-optimization-as-inference">Optimization as Inference<a class="headerlink" href="#deeplearning-1_mlp-optimization-as-inference" title="Permanent link">¶</a></h3>
<p>Many modern perspectives view training as approximate inference:</p>
<p><span class="arithmatex">\(p(\theta | D) \propto e^{-L(\theta)/T}\)</span></p>
<p>Gradient descent samples from this energy landscape as <span class="arithmatex">\(T \to 0\)</span>; stochastic variants like SGD approximate Bayesian inference under certain limits.</p></body></html></section><section class="print-page" id="deeplearning-2_convnets" heading-number="4.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-convolutional-neural-networks-cnns">Chapter 2: Convolutional Neural Networks (CNNs)<a class="headerlink" href="#deeplearning-2_convnets-chapter-2-convolutional-neural-networks-cnns" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-2_convnets-1-core-principles-locality-and-translation-invariance">1. Core Principles: Locality and Translation Invariance<a class="headerlink" href="#deeplearning-2_convnets-1-core-principles-locality-and-translation-invariance" title="Permanent link">¶</a></h2>
<p>Before understanding convolutional networks, it’s crucial to grasp why they exist — the structural priors they impose on data.</p>
<h3 id="deeplearning-2_convnets-11-locality">1.1 Locality<a class="headerlink" href="#deeplearning-2_convnets-11-locality" title="Permanent link">¶</a></h3>
<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p>
<p>For example:</p>
<ul>
<li>Adjacent pixels in an image often belong to the same object or texture.</li>
<li>Neighboring audio samples belong to the same phoneme.</li>
<li>Nearby words in a sentence influence each other’s meaning.</li>
</ul>
<p>MLPs treat every input dimension as <em>independent</em>, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, <em>local region</em> of the input, called its receptive field.</p>
<p>Formally, for an input <span class="arithmatex">\(x \in \mathbb{R}^{H \times W}\)</span>, a neuron at position <span class="arithmatex">\((i,j)\)</span> in a CNN depends only on values in a small window <span class="arithmatex">\(\Omega(i,j)\)</span>:
<script type="math/tex; mode=display">
h_{i,j} = \sigma\!\left(\sum_{(m,n)\in \Omega(i,j)} W_{m,n} \, x_{i+m, j+n} + b\right)
</script>
This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>
<hr>
<h3 id="deeplearning-2_convnets-12-translation-invariance">1.2 Translation Invariance<a class="headerlink" href="#deeplearning-2_convnets-12-translation-invariance" title="Permanent link">¶</a></h3>
<p>Natural patterns are repeatable across locations — the same feature (e.g., an edge, a cat’s ear) can appear <em>anywhere</em> in the image.</p>
<p>An MLP would need to learn a separate detector for each position.<br>
CNNs overcome this through weight sharing: the same filter <span class="arithmatex">\(W\)</span> is applied across all spatial positions.</p>
<p>Mathematically:
<script type="math/tex; mode=display">
(I * W)(i,j) = \sum_{m,n} I(i+m,j+n)\, W(m,n)
</script>
</p>
<p>This operation — convolution — ensures translation equivariance:
<script type="math/tex; mode=display">
f(T_\Delta I) = T_\Delta f(I)
</script>
meaning if the input shifts by <span class="arithmatex">\(\Delta\)</span>, the output shifts by the same amount.<br>
After pooling, this becomes translation invariance, i.e. the output doesn’t change under small shifts.</p>
<p>These two properties — locality and translation invariance — are the foundation of convolutional architectures.</p>
<h2 id="deeplearning-2_convnets-2-motivation-why-convolutions">2. Motivation: Why Convolutions?<a class="headerlink" href="#deeplearning-2_convnets-2-motivation-why-convolutions" title="Permanent link">¶</a></h2>
<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos.<br>
An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p>
<p>Example:<br>
For a 256×256 RGB image (≈200K input features), even one hidden layer with 1,000 neurons requires:
<span class="arithmatex">\(<span class="arithmatex">\((256 \times 256 \times 3) \times 1000 = 196\,\text{million weights}.\)</span>\)</span></p>
<p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p>
<p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>
<h2 id="deeplearning-2_convnets-3-the-convolution-operation">3. The Convolution Operation<a class="headerlink" href="#deeplearning-2_convnets-3-the-convolution-operation" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-2_convnets-31-discrete-convolution">3.1 Discrete Convolution<a class="headerlink" href="#deeplearning-2_convnets-31-discrete-convolution" title="Permanent link">¶</a></h3>
<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p>
<p>For 2D inputs (e.g. images):</p>
<div class="arithmatex">\[
S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i+m, j+n) K(m,n)
\]</div>
<ul>
<li><span class="arithmatex">\(I\)</span> — input (image)</li>
<li><span class="arithmatex">\(K\)</span> — kernel (filter)</li>
<li><span class="arithmatex">\(S\)</span> — output feature map</li>
</ul>
<p>Each filter detects a specific local pattern (edges, corners, textures).</p>
<h3 id="deeplearning-2_convnets-32-convolution-in-neural-networks">3.2 Convolution in Neural Networks<a class="headerlink" href="#deeplearning-2_convnets-32-convolution-in-neural-networks" title="Permanent link">¶</a></h3>
<p>In CNNs, the convolution becomes a learnable operation:</p>
<div class="arithmatex">\[
h_{i,j,k} = \sigma\left( \sum_{c=1}^{C_\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \right)
\]</div>
<ul>
<li><span class="arithmatex">\(x_c\)</span>: input channel <span class="arithmatex">\(c\)</span> (e.g. R, G, B)</li>
<li><span class="arithmatex">\(W_{k,c}\)</span>: kernel for output channel <span class="arithmatex">\(k\)</span> and input channel <span class="arithmatex">\(c\)</span></li>
<li><span class="arithmatex">\(b_k\)</span>: bias for output channel <span class="arithmatex">\(k\)</span></li>
<li><span class="arithmatex">\(\sigma\)</span>: nonlinearity (ReLU, etc.)</li>
</ul>
<p>This produces <span class="arithmatex">\(C_\text{out}\)</span> feature maps, each representing a learned spatial pattern.</p>
<p>Weight sharing drastically reduces parameters:<br>
Each kernel might be <span class="arithmatex">\(3 \times 3\)</span> or <span class="arithmatex">\(5 \times 5\)</span> — independent of image size.</p>
<h2 id="deeplearning-2_convnets-4-building-blocks-of-cnns">4. Building Blocks of CNNs<a class="headerlink" href="#deeplearning-2_convnets-4-building-blocks-of-cnns" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-2_convnets-41-convolutional-layer">4.1 Convolutional Layer<a class="headerlink" href="#deeplearning-2_convnets-41-convolutional-layer" title="Permanent link">¶</a></h3>
<p>Performs learnable filtering and produces feature maps.</p>
<p>If input has shape <span class="arithmatex">\((H, W, C_\text{in})\)</span>:
- Kernel: <span class="arithmatex">\((k_H, k_W, C_\text{in}, C_\text{out})\)</span>
- Output: <span class="arithmatex">\((H', W', C_\text{out})\)</span></p>
<h3 id="deeplearning-2_convnets-42-nonlinear-activation">4.2 Nonlinear Activation<a class="headerlink" href="#deeplearning-2_convnets-42-nonlinear-activation" title="Permanent link">¶</a></h3>
<p>After convolution, apply nonlinearity (commonly ReLU):
<script type="math/tex; mode=display">
\text{ReLU}(x) = \max(0, x)
</script>
</p>
<h3 id="deeplearning-2_convnets-43-pooling-layer">4.3 Pooling Layer<a class="headerlink" href="#deeplearning-2_convnets-43-pooling-layer" title="Permanent link">¶</a></h3>
<p>Reduces spatial dimensions and increases invariance.</p>
<ul>
<li>Max pooling: selects the largest value in a patch.</li>
<li>Average pooling: takes mean value.</li>
</ul>
<p>Formally:
<script type="math/tex; mode=display">
y_{i,j} = \max_{(m,n)\in \Omega(i,j)} h_{m,n}
</script>
</p>
<p>Pooling introduces translation invariance — small shifts in input don’t drastically change outputs.</p>
<h3 id="deeplearning-2_convnets-44-flatten-fully-connected-layers">4.4 Flatten + Fully Connected Layers<a class="headerlink" href="#deeplearning-2_convnets-44-flatten-fully-connected-layers" title="Permanent link">¶</a></h3>
<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>
<h2 id="deeplearning-2_convnets-5-cnn-architecture-as-a-computation-graph">5. CNN Architecture as a Computation Graph<a class="headerlink" href="#deeplearning-2_convnets-5-cnn-architecture-as-a-computation-graph" title="Permanent link">¶</a></h2>
<p>A typical CNN defines a differentiable map:</p>
<div class="arithmatex">\[
f_\theta(x) = W_L (\text{Flatten}(h_{L-1})) + b_L
\]</div>
<p>where each layer <span class="arithmatex">\(h_l\)</span> is defined recursively as:</p>
<div class="arithmatex">\[
h_l = \sigma(\text{Conv}(h_{l-1}; W_l) + b_l), \quad l = 1, \dots, L-1
\]</div>
<p>Here, <code>Conv</code> represents the convolution operation.</p>
<p>Each layer is spatially local, translation-equivariant, and differentiable — meaning backpropagation works seamlessly, just as in MLPs.</p>
<h2 id="deeplearning-2_convnets-6-backpropagation-through-convolutions">6. Backpropagation Through Convolutions<a class="headerlink" href="#deeplearning-2_convnets-6-backpropagation-through-convolutions" title="Permanent link">¶</a></h2>
<p>The gradient computation is a direct extension of the chain rule.</p>
<h3 id="deeplearning-2_convnets-61-forward-pass">6.1 Forward Pass<a class="headerlink" href="#deeplearning-2_convnets-61-forward-pass" title="Permanent link">¶</a></h3>
<p>Compute:
<script type="math/tex; mode=display">
y = W * x + b
</script>
</p>
<h3 id="deeplearning-2_convnets-62-backward-pass">6.2 Backward Pass<a class="headerlink" href="#deeplearning-2_convnets-62-backward-pass" title="Permanent link">¶</a></h3>
<p>We need:
- Gradient w.r.t. weights:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial W} = x * \frac{\partial L}{\partial y}\)</span>
- Gradient w.r.t. input:<br>
<span class="arithmatex">\(\frac{\partial L}{\partial x} = \text{flip}(W) * \frac{\partial L}{\partial y}\)</span></p>
<p>The flipping arises from the mathematical property of convolution.<br>
Modern frameworks handle this efficiently via <em>convolution transpose</em> operations.</p>
<p>Optimization viewpoint:<br>
Convolution layers remain linear in their weights — the nonlinearity and local parameter sharing define their expressive power.</p>
<h2 id="deeplearning-2_convnets-7-inductive-biases-in-cnns">7. Inductive Biases in CNNs<a class="headerlink" href="#deeplearning-2_convnets-7-inductive-biases-in-cnns" title="Permanent link">¶</a></h2>
<p>Convolutional architectures embed <em>strong inductive biases</em>:</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Mathematical Mechanism</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local connectivity</td>
<td>Small kernels (3×3, 5×5)</td>
<td>Exploits spatial locality</td>
</tr>
<tr>
<td>Weight sharing</td>
<td>Same filter across space</td>
<td>Reduces parameters drastically</td>
</tr>
<tr>
<td>Translation equivariance</td>
<td>Convolution operation</td>
<td>Same pattern detection anywhere</td>
</tr>
<tr>
<td>Pooling invariance</td>
<td>Spatial downsampling</td>
<td>Robust to small shifts/noise</td>
</tr>
</tbody>
</table>
<p>These biases make CNNs data-efficient and easy to train — especially compared to fully connected networks on images.</p>
<h2 id="deeplearning-2_convnets-8-optimization-and-training-dynamics">8. Optimization and Training Dynamics<a class="headerlink" href="#deeplearning-2_convnets-8-optimization-and-training-dynamics" title="Permanent link">¶</a></h2>
<p>Training CNNs is similar to MLPs — we use gradient-based optimizers (SGD, Adam, etc.) — but with different landscape geometry:</p>
<ul>
<li>Parameter sharing makes the loss smoother (less overfitting).</li>
<li>Batch normalization stabilizes gradient flow:
  <script type="math/tex; mode=display">
  \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
  </script>
</li>
<li>Regularization via dropout or weight decay improves generalization.</li>
<li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li>
</ul>
<p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>
<h2 id="deeplearning-2_convnets-9-cnn-architectures-through-history">9. CNN Architectures Through History<a class="headerlink" href="#deeplearning-2_convnets-9-cnn-architectures-through-history" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Year</th>
<th>Key Innovation</th>
<th>Depth</th>
<th>Inductive Bias</th>
</tr>
</thead>
<tbody>
<tr>
<td>LeNet-5</td>
<td>1998</td>
<td>First practical CNN for handwritten digits</td>
<td>7 layers</td>
<td>Local receptive fields</td>
</tr>
<tr>
<td>AlexNet</td>
<td>2012</td>
<td>GPU training, ReLU, dropout</td>
<td>8 layers</td>
<td>Data augmentation</td>
</tr>
<tr>
<td>VGG</td>
<td>2014</td>
<td>Deep stacks of small 3×3 filters</td>
<td>19 layers</td>
<td>Uniform architecture</td>
</tr>
<tr>
<td>ResNet</td>
<td>2015</td>
<td>Skip connections for gradient flow</td>
<td>152 layers</td>
<td>Identity mapping</td>
</tr>
<tr>
<td>DenseNet</td>
<td>2016</td>
<td>Feature reuse via dense connectivity</td>
<td>201 layers</td>
<td>Multi-scale learning</td>
</tr>
<tr>
<td>EfficientNet</td>
<td>2019</td>
<td>Compound scaling</td>
<td>variable</td>
<td>Optimized parameter scaling</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-2_convnets-10-cnns-and-the-optimization-landscape">10. CNNs and the Optimization Landscape<a class="headerlink" href="#deeplearning-2_convnets-10-cnns-and-the-optimization-landscape" title="Permanent link">¶</a></h2>
<p>CNNs reshape the optimization problem compared to MLPs:</p>
<ul>
<li>Reduced parameter redundancy → fewer degenerate directions in gradient space.</li>
<li>Structured weight sharing → smoother loss surface, fewer sharp minima.</li>
<li>Skip connections (ResNets) introduce <em>identity mappings</em>, improving conditioning of the Jacobian and preventing vanishing gradients.</li>
</ul>
<p>In optimization terms, CNNs are better-conditioned models of the input–output mapping.</p>
<h2 id="deeplearning-2_convnets-11-beyond-classical-cnns">11. Beyond Classical CNNs<a class="headerlink" href="#deeplearning-2_convnets-11-beyond-classical-cnns" title="Permanent link">¶</a></h2>
<p>Modern vision architectures have evolved:
- Residual Networks (ResNets): skip connections allow training very deep models.
- Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count.
- Dilated Convolutions: expand receptive field without extra parameters.
- Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>
<h2 id="deeplearning-2_convnets-12-mathematical-summary">12. Mathematical Summary<a class="headerlink" href="#deeplearning-2_convnets-12-mathematical-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Convolution</td>
<td><span class="arithmatex">\((I * K)(i,j) = \sum_m \sum_n I(i+m,j+n) K(m,n)\)</span></td>
<td>Weighted local sum</td>
</tr>
<tr>
<td>CNN Layer</td>
<td><span class="arithmatex">\(h = \sigma(W * x + b)\)</span></td>
<td>Convolution + nonlinearity</td>
</tr>
<tr>
<td>Pooling</td>
<td><span class="arithmatex">\(y_{i,j} = \max_{(m,n)\in \Omega(i,j)} h_{m,n}\)</span></td>
<td>Downsampling</td>
</tr>
<tr>
<td>Gradient wrt weights</td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial W} = x * \frac{\partial L}{\partial y}\)</span></td>
<td>Backprop step</td>
</tr>
<tr>
<td>Gradient wrt input</td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial x} = \text{flip}(W) * \frac{\partial L}{\partial y}\)</span></td>
<td>Sensitivity propagation</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-2_convnets-13-intuitive-summary">13. Intuitive Summary<a class="headerlink" href="#deeplearning-2_convnets-13-intuitive-summary" title="Permanent link">¶</a></h2>
<p>Convolutional networks are:
- Local → they process neighborhoods of data.
- Hierarchical → deeper layers build on lower-level features.
- Translation-equivariant → same pattern anywhere is treated the same.
- Efficient → far fewer parameters than MLPs.</p>
<p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p></body></html></section><section class="print-page" id="deeplearning-3_sequence_data" heading-number="4.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-modeling-sequence-data-in-deep-learning">Chapter 3: Modeling Sequence Data in Deep Learning<a class="headerlink" href="#deeplearning-3_sequence_data-chapter-3-modeling-sequence-data-in-deep-learning" title="Permanent link">¶</a></h1>
<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length <span class="arithmatex">\(T\)</span> can be written as <span class="arithmatex">\((x_1,x_2,\dots,x_T)\)</span>, where each element <span class="arithmatex">\(x_t\)</span> is indexed by its position in the sequence. Elements can repeat (e.g. the word “the” may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p>
<p>Sequences are collection of elements where:</p>
<ul>
<li>Elements can be repeated.</li>
<li>Order matters.</li>
<li>Of variable length.</li>
</ul>
<h2 id="deeplearning-3_sequence_data-limitations-of-traditional-supervised-models">Limitations of Traditional Supervised Models:<a class="headerlink" href="#deeplearning-3_sequence_data-limitations-of-traditional-supervised-models" title="Permanent link">¶</a></h2>
<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data – by, say, collapsing the sequence into a fixed-size feature vector – ignores the important temporal or sequential structure. As one summary notes, “feedforward neural networks are severely limited when it comes to sequential data”. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p>
<p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>
<h2 id="deeplearning-3_sequence_data-the-simplest-assumption-independent-words-bag-of-words">The Simplest Assumption: Independent Words (Bag-of-Words)<a class="headerlink" href="#deeplearning-3_sequence_data-the-simplest-assumption-independent-words-bag-of-words" title="Permanent link">¶</a></h2>
<p>A naïve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word’s occurrence, treating all words as “independent features.” This ignores sequence structure: “the order of words in the original documents is irrelevant”. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: “bag-of-words assumes words are independent of one another”. In reality, words co-occur in context (“peanut butter” versus “peanut giraffe”) – bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>
<h2 id="deeplearning-3_sequence_data-n-gram-models-and-fixed-context-assumptions">N-gram Models and Fixed-Context Assumptions<a class="headerlink" href="#deeplearning-3_sequence_data-n-gram-models-and-fixed-context-assumptions" title="Permanent link">¶</a></h2>
<p>To go beyond complete independence, one can incorporate local context by using <span class="arithmatex">\(n\)</span>-gram models. An <span class="arithmatex">\(n\)</span>-gram model makes the (Markov) assumption that the probability of each element depends only on the previous <span class="arithmatex">\(n-1\)</span> elements. For language, a bigram model (2-gram) assumes <span class="arithmatex">\(P(w_t\mid w_{t-1})\)</span>, a trigram (3-gram) uses <span class="arithmatex">\(P(w_t\mid w_{t-2},w_{t-1})\)</span>, etc. In general, the chain rule with an <span class="arithmatex">\(N\)</span>-gram approximation is</p>
<div class="arithmatex">\[
P(x_1, \ldots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_{t-N+1}, \ldots, x_{t-1}) \, .
\]</div>
<p>This preserves some order information: the window of the last <span class="arithmatex">\(N-1\)</span> items is used to predict the next. However, <span class="arithmatex">\(n\)</span>-gram models have well-known downsides:</p>
<ul>
<li>
<p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language “cannot reason about context beyond the immediate <span class="arithmatex">\(n\)</span>-gram window”, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an <span class="arithmatex">\(n\)</span>-gram.</p>
</li>
<li>
<p>Data sparsity and scalability: The number of possible <span class="arithmatex">\(n\)</span>-grams grows exponentially with vocabulary size <span class="arithmatex">\(V\)</span>. For a vocabulary of size <span class="arithmatex">\(V\)</span>, there are <span class="arithmatex">\(V^N\)</span> possible <span class="arithmatex">\(N\)</span>-grams. Jurafsky &amp; Martin observe that even for Shakespeare’s corpus (<span class="arithmatex">\(V\approx 29{,}066\)</span>), there are <span class="arithmatex">\(V^2\approx8.4\times 10^8\)</span> possible bigrams and <span class="arithmatex">\(V^4\approx 7\times 10^{17}\)</span> possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough <span class="arithmatex">\(n\)</span>-gram counts, and storing these tables is impractical for large <span class="arithmatex">\(N\)</span> or <span class="arithmatex">\(V\)</span>. In practice, language models become “ridiculously sparse” and unwieldy.</p>
</li>
<li>
<p>No parametrization (non-differentiable): Traditional <span class="arithmatex">\(n\)</span>-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p>
</li>
</ul>
<p>In summary, while <span class="arithmatex">\(n\)</span>-grams preserve local order up to length <span class="arithmatex">\(N\)</span>, they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>
<h2 id="deeplearning-3_sequence_data-learnable-context-models-vectorization-and-neural-nets">Learnable Context Models: Vectorization and Neural Nets<a class="headerlink" href="#deeplearning-3_sequence_data-learnable-context-models-vectorization-and-neural-nets" title="Permanent link">¶</a></h2>
<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p>
<ul>
<li>
<p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p>
</li>
<li>
<p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p>
</li>
<li>
<p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can “operate in variable length of sequences”.</p>
</li>
<li>
<p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p>
</li>
<li>
<p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p>
</li>
</ul>
<p>For example, one could take the last few words, map each to an embedding <span class="arithmatex">\(\mathbf{x}{t-N+1},\dots,\mathbf{x}{t-1}\)</span>, concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word’s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (<span class="arithmatex">\(N-1\)</span>) and uses a separate weight for each position, so it’s not efficient or variable-length. </p>
<p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector <span class="arithmatex">\(\mathbf{h}_t\)</span> that evolves as the sequence is read. Such a context-vector “acts as memory” summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can “learn” how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>
<h2 id="deeplearning-3_sequence_data-recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)<a class="headerlink" href="#deeplearning-3_sequence_data-recurrent-neural-networks-rnns" title="Permanent link">¶</a></h2>
<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) – models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step <span class="arithmatex">\(t\)</span>, the RNN takes the current input <span class="arithmatex">\(\mathbf{x}t\)</span> and the previous hidden state <span class="arithmatex">\(\mathbf{h}{t-1}\)</span> and computes a new hidden state <span class="arithmatex">\(\mathbf{h}_t\)</span>. The simplest RNN update is:</p>
<div class="arithmatex">\[
h_t = \phi(W_h h_{t-1} + W_x x_t + b) \, .
\]</div>
<p>where <span class="arithmatex">\(\phi\)</span> is a nonlinear activation (often <span class="arithmatex">\(\tanh\)</span>) and <span class="arithmatex">\(W_h,W_x\)</span> are weight matrices. The same weight matrices <span class="arithmatex">\(W_h,W_x\)</span> are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p>
<p>Intuitively, the RNN’s hidden state <span class="arithmatex">\(\mathbf{h}_t\)</span> “remembers” the information from all prior inputs up to time <span class="arithmatex">\(t\)</span>. The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p>
<div class="arithmatex">\[
y_t = \mathrm{softmax}(W_y h_t + b_y) \, .
\]</div>
<p>so that <span class="arithmatex">\(P(x_{t+1}=w \mid \mathbf{h}_t)\)</span> is given by the corresponding component of <span class="arithmatex">\(\mathbf{y}_t\)</span>. In language modeling, for instance, <span class="arithmatex">\(y_t\)</span> gives a probability for each word in the vocabulary. As described in practice, “RNNs predict the output from the last hidden state along with output parameter <span class="arithmatex">\(W_y\)</span>; a softmax function to ensure the probability over all possible words”. </p>
<p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an “MLP with shared weights across time.” At time <span class="arithmatex">\(t\)</span>, the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>
<h2 id="deeplearning-3_sequence_data-unrolling-and-backpropagation-through-time-bptt">Unrolling and Backpropagation Through Time (BPTT)<a class="headerlink" href="#deeplearning-3_sequence_data-unrolling-and-backpropagation-through-time-bptt" title="Permanent link">¶</a></h2>
<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across <span class="arithmatex">\(T\)</span> time steps, creating a deep feedforward network of depth <span class="arithmatex">\(T\)</span> (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, “the network needs to be expanded, or unfolded, so that the parameters could be differentiated ... – hence backpropagation through time (BPTT)”. In practice, each weight matrix <span class="arithmatex">\(W\)</span> receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if <span class="arithmatex">\(L = -\sum_t \log P(x_t\mid \mathbf{h}_{t-1})\)</span> is the loss, then for each <span class="arithmatex">\(W\)</span> we compute</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial W} = \sum_{t} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W} \, .
\]</div>
<p>taking into account the influence of <span class="arithmatex">\(W\)</span> at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>
<h2 id="deeplearning-3_sequence_data-vanishing-and-exploding-gradients">Vanishing and Exploding Gradients<a class="headerlink" href="#deeplearning-3_sequence_data-vanishing-and-exploding-gradients" title="Permanent link">¶</a></h2>
<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative <span class="arithmatex">\(\partial \mathbf{h}t/\partial \mathbf{h}{t-1}\)</span> involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius <span class="arithmatex">\(&lt;1\)</span> causes the gradients to shrink exponentially (vanishing), while if the spectral radius is <span class="arithmatex">\(&gt;1\)</span> they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues <span class="arithmatex">\(&gt;1\)</span>), whereas the vanishing gradient problem occurs when long-term components of the gradient go “exponentially fast to norm 0”. Formally, for a linearized RNN one can show that if the largest eigenvalue <span class="arithmatex">\(\lambda_{\max}\)</span> of the recurrent weight matrix satisfies <span class="arithmatex">\(|\lambda_{\max}|&lt;1\)</span>, long-term gradients vanish as <span class="arithmatex">\(t\to\infty\)</span>, and if <span class="arithmatex">\(|\lambda_{\max}|&gt;1\)</span> they explode. </p>
<p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: “when long term components go to zero, the model cannot learn correlation between distant events.” In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>
<h2 id="deeplearning-3_sequence_data-gated-architectures-lstm-and-gru">Gated Architectures: LSTM and GRU<a class="headerlink" href="#deeplearning-3_sequence_data-gated-architectures-lstm-and-gru" title="Permanent link">¶</a></h2>
<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable “gates” that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state <span class="arithmatex">\(\mathbf{C}_t\)</span> and three gates: input (<span class="arithmatex">\(\mathbf{i}_t\)</span>), forget (<span class="arithmatex">\(\mathbf{f}_t\)</span>), and output (<span class="arithmatex">\(\mathbf{o}t\)</span>) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time <span class="arithmatex">\(t\)</span> with input <span class="arithmatex">\(\mathbf{x}t\)</span> and previous hidden <span class="arithmatex">\(\mathbf{h}{t-1}\)</span> and cell <span class="arithmatex">\(\mathbf{C}{t-1}\)</span>, the gates and cell update are given by (all operations are elementwise):</p>
<p>​<script type="math/tex; mode=display">
\begin{aligned}
i_t &= \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i), \\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f), \\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o), \\
\tilde{C}_t &= \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \, .
\end{aligned}
</script>
</p>
<p>The new cell state <span class="arithmatex">\(\mathbf{C}_t\)</span> is then updated by combining the old state and the candidate:</p>
<div class="arithmatex">\[
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \, .
\]</div>
<p>where <span class="arithmatex">\(\odot\)</span> denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p>
<div class="arithmatex">\[
h_t = o_t \odot \tanh(C_t) \,
\]</div>
<p>The intuition is that the forget gate <span class="arithmatex">\(\mathbf{f_t}\)</span> can reset or retain the old memory <span class="arithmatex">\(\mathbf{C}_{t-1}\)</span>, the input gate <span class="arithmatex">\(\mathbf{i}_t\)</span> controls how much new information <span class="arithmatex">\(\tilde{\mathbf{C}}_t\)</span> to write, and the output gate <span class="arithmatex">\(\mathbf{o}_t\)</span> controls how much of the cell state to expose as <span class="arithmatex">\(\mathbf{h}_t\)</span>. By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs “alleviate the vanishing gradient problem,” making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p>
<p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>
<h2 id="deeplearning-3_sequence_data-optimization-challenges-and-solutions">Optimization Challenges and Solutions<a class="headerlink" href="#deeplearning-3_sequence_data-optimization-challenges-and-solutions" title="Permanent link">¶</a></h2>
<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p>
<ul>
<li>
<p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping “solves the exploding gradients problem” by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p>
</li>
<li>
<p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing <span class="arithmatex">\(W_h\)</span> as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, “Orthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,” ensuring stable gradient propagation. In practice, some implementations initialize <span class="arithmatex">\(W_h\)</span> to random orthogonal (or unitary) matrices to encourage long memory.</p>
</li>
<li>
<p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p>
</li>
<li>
<p>Regularization: Some works add penalties to encourage <span class="arithmatex">\(W_h\)</span> to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p>
</li>
</ul>
<p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models – especially gated RNNs – provide a powerful framework: mathematically, they define hidden states <span class="arithmatex">\(\mathbf{h}_t\)</span> updated by <span class="arithmatex">\(\mathbf{h}t = f(\mathbf{h}{t-1},\mathbf{x}_t)\)</span> with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p></body></html></section><section class="print-page" id="deeplearning-4_nlp" heading-number="4.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="deep-learning-for-natural-language-processing">Deep Learning for Natural Language Processing<a class="headerlink" href="#deeplearning-4_nlp-deep-learning-for-natural-language-processing" title="Permanent link">¶</a></h1>
<ul>
<li>Natural language is context-dependent, compositional, and ambiguous.</li>
<li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation — ideal for modeling contextual relationships.</li>
<li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li>
</ul>
<h3 id="deeplearning-4_nlp-key-challenges-of-language">Key Challenges of Language<a class="headerlink" href="#deeplearning-4_nlp-key-challenges-of-language" title="Permanent link">¶</a></h3>
<p>Human language presents a unique set of challenges for computational models.<br>
Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p>
<ul>
<li>
<p>Words are not discrete symbols.<br>
  The same word can have several related senses depending on context — for example:<br>
<code>face₁</code> (human face), <code>face₂</code> (clock face), <code>face₃</code> (to confront), and <code>face₄</code> (a person or presence).<br>
  Treating these as independent dictionary entries loses the shared semantic structure between them.<br>
  A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p>
</li>
<li>
<p>Need for distributed representations.<br>
  Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).<br>
  This allows similarity, analogy, and composition to emerge geometrically — for instance, <code>king - man + woman ≈ queen</code>.</p>
</li>
<li>
<p>Disambiguation depends on context.<br>
  The meaning of a word or phrase is determined by its linguistic surroundings.<br>
  For example, in “The man who ate the pepper sneezed,” the subject of <em>sneezed</em> is determined by a non-adjacent clause (<em>the man</em>), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p>
</li>
<li>
<p>Non-local dependencies.<br>
  Natural language contains relationships between words that may be far apart in sequence.<br>
  Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.<br>
  Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p>
</li>
<li>
<p>Compositionality.<br>
  The meaning of larger expressions arises from the meanings of their parts and how they are combined.<br>
  However, this combination is not purely linear.<br>
  For example, <code>carnivorous plant</code> is not simply the sum of <em>carnivore</em> and <em>plant</em> — its interpretation depends on how the features interact (<em>a plant that eats insects</em>).<br>
  Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p>
</li>
</ul>
<p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically.<br>
Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>
<h2 id="deeplearning-4_nlp-the-transformer-architecture">The Transformer Architecture<a class="headerlink" href="#deeplearning-4_nlp-the-transformer-architecture" title="Permanent link">¶</a></h2>
<ul>
<li>Sequence models (RNNs, LSTMs) process tokens sequentially — limiting parallelism and long-range context.</li>
<li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li>
</ul>
<h3 id="deeplearning-4_nlp-core-mechanism-self-attention">Core Mechanism: Self-Attention<a class="headerlink" href="#deeplearning-4_nlp-core-mechanism-self-attention" title="Permanent link">¶</a></h3>
<p>Given token embeddings <script type="math/tex"> e_i \in \mathbb{R}^d </script>:</p>
<div class="arithmatex">\[
q_i = e_i W^Q, \quad k_i = e_i W^K, \quad v_i = e_i W^V
\]</div>
<p>Attention weights:</p>
<div class="arithmatex">\[
\alpha_{ij} = \mathrm{softmax}_j \left( \frac{q_i k_j^\top}{\sqrt{d}} \right)
\]</div>
<p>Output:</p>
<div class="arithmatex">\[
z_i = \sum_j \alpha_{ij} v_j
\]</div>
<p>Each token’s new representation <script type="math/tex"> z_i </script> is a contextual blend of all others.<br>
Captures semantic and syntactic relations without explicit recurrence.</p>
<h3 id="deeplearning-4_nlp-multi-head-attention">Multi-Head Attention<a class="headerlink" href="#deeplearning-4_nlp-multi-head-attention" title="Permanent link">¶</a></h3>
<p>Use multiple projections <span class="arithmatex">\((W^Q_h, W^K_h, W^V_h)\)</span> → multiple “heads.”<br>
Each head focuses on different relations (e.g. subject–verb, modifier–noun).<br>
Outputs are concatenated and projected back to dimension <span class="arithmatex">\(d\)</span>:</p>
<div class="arithmatex">\[
\text{MHA}(E) = [Z_1; Z_2; \dots; Z_H] W^O
\]</div>
<h3 id="deeplearning-4_nlp-position-encoding">Position Encoding<a class="headerlink" href="#deeplearning-4_nlp-position-encoding" title="Permanent link">¶</a></h3>
<p>Since attention is permutation-invariant, Transformers add position information:</p>
<div class="arithmatex">\[
\text{PE}_{(pos,2i)} = \sin(pos / 10000^{2i/d}), \quad
\text{PE}_{(pos,2i+1)} = \cos(pos / 10000^{2i/d})
\]</div>
<p>→ These sinusoidal signals are added to embeddings to encode word order.</p>
<h3 id="deeplearning-4_nlp-full-transformer-block">Full Transformer Block<a class="headerlink" href="#deeplearning-4_nlp-full-transformer-block" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>Input
  ↓
Multi-Head Self-Attention
  ↓
+ Skip Connection
  ↓
Layer Normalization
  ↓
Feedforward Network (ReLU)
  ↓
+ Skip Connection
  ↓
Layer Normalization
  ↓
Output
</code></pre></div>
<p>Skip connections enable gradient flow and top-down influence.<br>
Stacking <span class="arithmatex">\(N\)</span> blocks yields hierarchical contextualization of meaning.</p>
<h3 id="deeplearning-4_nlp-intuition">Intuition<a class="headerlink" href="#deeplearning-4_nlp-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>Self-attention handles non-local relations.</li>
<li>Multi-head captures multiple semantic dimensions simultaneously.</li>
<li>Stacked layers build abstraction — from word-level to phrase- and discourse-level features.</li>
</ul>
<h2 id="deeplearning-4_nlp-unsupervised-learning-and-bert">Unsupervised Learning and BERT<a class="headerlink" href="#deeplearning-4_nlp-unsupervised-learning-and-bert" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-4_nlp-the-need-for-contextualized-representations">The Need for Contextualized Representations<a class="headerlink" href="#deeplearning-4_nlp-the-need-for-contextualized-representations" title="Permanent link">¶</a></h3>
<ul>
<li>Word embeddings like Word2Vec are static: one vector per word.</li>
<li>Language understanding requires contextual embeddings: “bank” (river vs. finance).</li>
<li>Transformers enable bidirectional context — understanding a word from both sides.</li>
</ul>
<h3 id="deeplearning-4_nlp-bert-pretraining-objectives">BERT Pretraining Objectives<a class="headerlink" href="#deeplearning-4_nlp-bert-pretraining-objectives" title="Permanent link">¶</a></h3>
<ol>
<li>Masked Language Modeling (MLM)<br>
Randomly mask 15% of tokens, predict them:</li>
</ol>
<div class="arithmatex">\[
\text{Loss}_{MLM} = - \sum_{i \in M} \log P(w_i | \text{context})
\]</div>
<p>Encourages bidirectional encoding of meaning.</p>
<ol>
<li>Next Sentence Prediction (NSP)<br>
Model predicts if sentence B follows sentence A.<br>
Builds discourse-level coherence and world knowledge.</li>
</ol>
<h3 id="deeplearning-4_nlp-architecture">Architecture<a class="headerlink" href="#deeplearning-4_nlp-architecture" title="Permanent link">¶</a></h3>
<ul>
<li>Deep bidirectional Transformer encoder.</li>
<li>Uses special tokens:</li>
<li><code>[CLS]</code> – sentence-level classification embedding</li>
<li><code>[SEP]</code> – separates segments</li>
<li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li>
<li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li>
</ul>
<h3 id="deeplearning-4_nlp-significance">Significance<a class="headerlink" href="#deeplearning-4_nlp-significance" title="Permanent link">¶</a></h3>
<p>BERT shows self-supervised pretraining → transfer learning pipeline:</p>
<div class="highlight"><pre><span></span><code>Pretrain (unsupervised)
   ↓
Fine-tune (supervised)
   ↓
Task-specific adaptation
</code></pre></div>
<p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data.<br>
Learns semantic similarity, coreference, and discourse relations implicitly.</p>
<h2 id="deeplearning-4_nlp-grounded-and-embodied-language-learning">Grounded and Embodied Language Learning<a class="headerlink" href="#deeplearning-4_nlp-grounded-and-embodied-language-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-4_nlp-motivation">Motivation<a class="headerlink" href="#deeplearning-4_nlp-motivation" title="Permanent link">¶</a></h3>
<ul>
<li>Language understanding ultimately involves relating words to the world.</li>
<li>Humans learn language in context — perception, action, and social interaction.</li>
<li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li>
</ul>
<h3 id="deeplearning-4_nlp-grounded-agents">Grounded Agents<a class="headerlink" href="#deeplearning-4_nlp-grounded-agents" title="Permanent link">¶</a></h3>
<ul>
<li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li>
<li>Train via predictive modeling — anticipate sensory outcomes from language-conditioned actions.</li>
<li>Enables semantic grounding: linking word “red” to visual color, “pick up” to motor command.</li>
</ul>
<h3 id="deeplearning-4_nlp-predictive-and-self-supervised-paradigms">Predictive and Self-Supervised Paradigms<a class="headerlink" href="#deeplearning-4_nlp-predictive-and-self-supervised-paradigms" title="Permanent link">¶</a></h3>
<p>Agents learn representations by predicting future sensory or linguistic states:</p>
<div class="arithmatex">\[
\min_\theta \mathbb{E} [ \| f_\theta(s_t, a_t) - s_{t+1} \|^2 ]
\]</div>
<p>→ Connects to world models and predictive coding principles in neuroscience.<br>
The agent’s internal model encodes both linguistic meaning and causal structure of the environment.</p>
<h2 id="deeplearning-4_nlp-insights-from-deepmind-work">Insights from DeepMind Work<a class="headerlink" href="#deeplearning-4_nlp-insights-from-deepmind-work" title="Permanent link">¶</a></h2>
<ul>
<li>Embodied agents trained in simulated environments exhibit:</li>
<li>Systematic generalization (e.g., learning “pick up red object” → generalize to unseen colors).</li>
<li>Question answering and instruction following grounded in perception.</li>
<li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li>
</ul>
<h3 id="deeplearning-4_nlp-conceptual-shift">Conceptual Shift<a class="headerlink" href="#deeplearning-4_nlp-conceptual-shift" title="Permanent link">¶</a></h3>
<p>From pipeline → integrated model:</p>
<table>
<thead>
<tr>
<th>Classic Pipeline</th>
<th>Embodied / Interactive Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Letters → Words → Syntax → Meaning → Action</td>
<td>Multimodal loops: Perception ↔ Action ↔ Language ↔ Prediction</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-4_nlp-conceptual-map-from-representation-to-understanding">Conceptual Map: From Representation to Understanding<a class="headerlink" href="#deeplearning-4_nlp-conceptual-map-from-representation-to-understanding" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>Word Input
   ↓
Distributed Representations (embedding)
   ↓
Self-Attention Mechanism
   ↓
Multi-Head Parallel Processing
   ↓
Hierarchical Transformer Layers
   ↓
Contextualized Embeddings (BERT)
   ↓
Transfer Learning to Tasks
   ↓
Embodied Agents (Grounded Semantics)
   ↓
Language Understanding as Prediction + Interaction
</code></pre></div>
<h3 id="deeplearning-4_nlp-key-transitions">Key Transitions<a class="headerlink" href="#deeplearning-4_nlp-key-transitions" title="Permanent link">¶</a></h3>
<p>Symbol → Vector: Continuous representations enable learning of semantic gradients.</p>
<p>Sequence → Attention: Parallel context integration replaces recurrence.</p>
<p>Text → Context: Pretraining captures knowledge without explicit supervision.</p>
<p>Language → World: Grounding links linguistic representations to sensory and causal models.</p>
<h3 id="deeplearning-4_nlp-unifying-principle">Unifying Principle<a class="headerlink" href="#deeplearning-4_nlp-unifying-principle" title="Permanent link">¶</a></h3>
<p>Deep language understanding = predictive modeling of structured context
across both linguistic and environmental domains.</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Core Idea</th>
<th>Model / Mechanism</th>
</tr>
</thead>
<tbody>
<tr>
<td>Distributed representations</td>
<td>Meanings as patterns, not symbols</td>
<td>Embeddings</td>
</tr>
<tr>
<td>Context dependence</td>
<td>Sense resolution via interaction</td>
<td>Self-attention</td>
</tr>
<tr>
<td>Parallelism</td>
<td>All words attend to all others</td>
<td>Transformer</td>
</tr>
<tr>
<td>Bidirectionality</td>
<td>Context from both sides</td>
<td>BERT encoder</td>
</tr>
<tr>
<td>Transfer learning</td>
<td>Self-supervised → supervised</td>
<td>Fine-tuning</td>
</tr>
<tr>
<td>Grounding</td>
<td>Language tied to perception/action</td>
<td>Embodied agents</td>
</tr>
<tr>
<td>Predictive learning</td>
<td>Understanding as anticipation</td>
<td>World models</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="deeplearning-5_attention" heading-number="4.5"><h1 id="deeplearning-5_attention-deeplearning-5_attention">5. Transformers and Attention Mechanisms</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-attention-memory-and-cognition">1. Attention, Memory, and Cognition<a class="headerlink" href="#deeplearning-5_attention-1-attention-memory-and-cognition" title="Permanent link">¶</a></h2>
<ul>
<li>Attention = ability to focus on relevant signals and ignore distractions.  </li>
<li>Enables selective processing (e.g. cocktail party effect).  </li>
<li>
<p>Allows focusing on one thought or event at a time.</p>
</li>
<li>
<p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p>
</li>
<li>
<p>Together, they form the basis of cognition — controlling what to process, store, and recall.</p>
</li>
<li>
<p>Neural networks can model aspects of this by learning <em>what to attend to</em> and <em>what to remember</em>.</p>
</li>
<li>
<p>Goal of attention in DL:<br>
  Reduce complexity by focusing computation on the most informative parts of data or internal state.</p>
</li>
</ul>
<h2 id="deeplearning-5_attention-2-implicit-attention-in-neural-networks">2. Implicit Attention in Neural Networks<a class="headerlink" href="#deeplearning-5_attention-2-implicit-attention-in-neural-networks" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Neural networks are parametric nonlinear functions <span class="arithmatex">\(y = f_\theta(x)\)</span> mapping inputs to outputs.<br>
  They naturally exhibit <em>implicit attention</em>: certain input dimensions influence outputs more.</p>
</li>
<li>
<p>The Jacobian <span class="arithmatex">\(J = \frac{\partial y}{\partial x}\)</span> quantifies this sensitivity — shows which input parts the model “pays attention” to.</p>
</li>
<li>
<p>Example:<br>
  In deep RL, sensitivity maps reveal focus on <em>state-value</em> vs <em>action-advantage</em> components.</p>
</li>
<li>
<p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p>
</li>
<li>Hidden state <span class="arithmatex">\(h_t\)</span> stores past info.  </li>
<li>The <em>sequential Jacobian</em> <span class="arithmatex">\(\frac{\partial y_t}{\partial x_{t-k}}\)</span> shows which past inputs are remembered.  </li>
<li>
<p>Implicitly attends to relevant time steps (memory through recurrence).</p>
</li>
<li>
<p>In tasks like machine translation, implicit attention lets models reorder tokens:</p>
<blockquote>
<p>“to reach” → “zu erreichen”</p>
</blockquote>
</li>
</ul>
<h2 id="deeplearning-5_attention-3-explicit-hard-attention">3. Explicit (Hard) Attention<a class="headerlink" href="#deeplearning-5_attention-3-explicit-hard-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Explicit attention introduces a separate <em>attention mechanism</em> that decides where to look or what to read.<br>
  It restricts the data fed to the main network.</li>
</ul>
<h3 id="deeplearning-5_attention-why-explicit-attention">Why explicit attention?<a class="headerlink" href="#deeplearning-5_attention-why-explicit-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Efficiency: processes only selected parts of input.  </li>
<li>Scalability: works on large or variable-size data.  </li>
<li>Sequential processing: e.g. moving “gaze” across static images.  </li>
<li>Interpretability: easier to visualize focus regions.</li>
</ul>
<h3 id="deeplearning-5_attention-model-structure">Model structure<a class="headerlink" href="#deeplearning-5_attention-model-structure" title="Permanent link">¶</a></h3>
<ul>
<li>Network outputs attention parameters <span class="arithmatex">\(a\)</span> that define a <em>glimpse distribution</em> <span class="arithmatex">\(p(g|a)\)</span> over possible data regions.  </li>
<li>A glimpse <span class="arithmatex">\(g\)</span> (subset or window of data) is sampled and passed back as input.  </li>
<li>System becomes recurrent, even if the base network is not.</li>
</ul>
<h3 id="deeplearning-5_attention-training-non-differentiable">Training (non-differentiable)<a class="headerlink" href="#deeplearning-5_attention-training-non-differentiable" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>When glimpse selection is discrete or stochastic, use REINFORCE:
  <script type="math/tex; mode=display">
  \nabla_\theta \mathbb{E}[R] = \mathbb{E}[(R - b) \nabla_\theta \log \pi_\theta(g)]
  </script>
  where <span class="arithmatex">\(R\)</span> is the reward (e.g. task loss) and <span class="arithmatex">\(b\)</span> a baseline for variance reduction.</p>
</li>
<li>
<p>Thus, attention acts as a policy <span class="arithmatex">\(\pi_\theta(g)\)</span> over glimpses.</p>
</li>
</ul>
<h3 id="deeplearning-5_attention-examples">Examples<a class="headerlink" href="#deeplearning-5_attention-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li>
<li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li>
</ul>
<h2 id="deeplearning-5_attention-4-soft-attention">4. Soft Attention<a class="headerlink" href="#deeplearning-5_attention-4-soft-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Hard attention samples discrete glimpses → non-differentiable → needs RL.  </li>
<li>Soft attention computes a <em>weighted average</em> over all glimpses → differentiable → trainable by backprop.</li>
</ul>
<h3 id="deeplearning-5_attention-basic-idea">Basic idea<a class="headerlink" href="#deeplearning-5_attention-basic-idea" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Attention parameters <span class="arithmatex">\(a\)</span> define weights <span class="arithmatex">\(w_i\)</span> over input features <span class="arithmatex">\(v_i\)</span>:
  <script type="math/tex; mode=display">
  v = \sum_i w_i v_i, \quad \sum_i w_i = 1
  </script>
  The readout <span class="arithmatex">\(v\)</span> is a smooth combination of inputs.</p>
</li>
<li>
<p>Replaces sampling by <em>expectation</em> → continuous, differentiable.</p>
</li>
</ul>
<h3 id="deeplearning-5_attention-benefits">Benefits<a class="headerlink" href="#deeplearning-5_attention-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>Trained end-to-end with gradients.  </li>
<li>Easier and more stable than hard attention.  </li>
<li>Allows <em>focus distribution</em> rather than a single point.</li>
</ul>
<h3 id="deeplearning-5_attention-variants">Variants<a class="headerlink" href="#deeplearning-5_attention-variants" title="Permanent link">¶</a></h3>
<ul>
<li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li>
<li>Content-based attention: focuses by similarity of key <span class="arithmatex">\(k\)</span> to data vectors <span class="arithmatex">\(x_i\)</span> via score <span class="arithmatex">\(S(k, x_i)\)</span>, usually normalized by softmax:
  <script type="math/tex; mode=display">
  w_i = \frac{\exp(S(k, x_i))}{\sum_j \exp(S(k, x_j))}
  </script>
</li>
</ul>
<h3 id="deeplearning-5_attention-applications">Applications<a class="headerlink" href="#deeplearning-5_attention-applications" title="Permanent link">¶</a></h3>
<ul>
<li>Handwriting synthesis: RNN learns soft “window” over text sequence.  </li>
<li>Neural Machine Translation: associative attention aligns words between languages.  </li>
<li>
<p>DRAW model: uses Gaussian filters to read/write parts of an image.</p>
</li>
<li>
<p>Soft attention = <em>data-dependent dynamic weighting</em> (similar to convolution with adaptive filters).</p>
</li>
</ul>
<h2 id="deeplearning-5_attention-5-introspective-attention-and-memory">5. Introspective Attention and Memory<a class="headerlink" href="#deeplearning-5_attention-5-introspective-attention-and-memory" title="Permanent link">¶</a></h2>
<ul>
<li>So far: attention over external data.  </li>
<li>Now: attention over internal state or memory → “introspective attention.”  </li>
<li>Lets the network <em>read</em> or <em>write</em> selectively to memory locations.  </li>
<li>Enables reasoning, recall, and algorithmic behavior.</li>
</ul>
<h3 id="deeplearning-5_attention-neural-turing-machine-ntm">Neural Turing Machine (NTM)<a class="headerlink" href="#deeplearning-5_attention-neural-turing-machine-ntm" title="Permanent link">¶</a></h3>
<ul>
<li>Adds a differentiable memory matrix <span class="arithmatex">\(M \in \mathbb{R}^{N \times W}\)</span>.  </li>
<li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li>
</ul>
<p>Operations
- Write: modify selected rows in <span class="arithmatex">\(M\)</span> using attention weights <span class="arithmatex">\(w_t\)</span>.<br>
- Read: output weighted sum of memory slots:
  <script type="math/tex; mode=display">
  r_t = \sum_i w_{t,i} M_i
  </script>
- Addressing modes:
  - <em>Content-based</em>: match key vector <span class="arithmatex">\(k_t\)</span> to memory contents (via cosine similarity).<br>
  - <em>Location-based</em>: shift attention by relative position.</p>
<p>Training: fully differentiable — end-to-end via backprop.</p>
<p>Example task: copying sequences of variable length — learns algorithmic generalization.</p>
<h3 id="deeplearning-5_attention-differentiable-neural-computer-dnc">Differentiable Neural Computer (DNC)<a class="headerlink" href="#deeplearning-5_attention-differentiable-neural-computer-dnc" title="Permanent link">¶</a></h3>
<ul>
<li>Successor to NTM with richer memory access:</li>
<li>Tracks temporal links between writes.  </li>
<li>Supports dynamic memory allocation.  </li>
<li>Improves stability and scalability.</li>
</ul>
<p>Application: synthetic QA tasks (bAbI dataset) — answers questions requiring multiple supporting facts and temporal reasoning.</p>
<p>Key insight:<br>
Attention provides <em>selective access</em> to memory, acting like “addressing” in a differentiable data structure.</p>
<h2 id="deeplearning-5_attention-6-transformers-and-self-attention">6. Transformers and Self-Attention<a class="headerlink" href="#deeplearning-5_attention-6-transformers-and-self-attention" title="Permanent link">¶</a></h2>
<ul>
<li>Transformers: remove recurrence and convolution entirely — rely only on attention.</li>
</ul>
<h3 id="deeplearning-5_attention-self-attention">Self-Attention<a class="headerlink" href="#deeplearning-5_attention-self-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Each token attends to all others in the sequence:
  <script type="math/tex; mode=display">
  \text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
  </script>
  where:</li>
<li><span class="arithmatex">\(Q, K, V\)</span> are query, key, and value matrices (learned linear projections of input embeddings).</li>
<li>Produces context-aware representations for all tokens in parallel.</li>
</ul>
<h3 id="deeplearning-5_attention-multi-head-attention">Multi-Head Attention<a class="headerlink" href="#deeplearning-5_attention-multi-head-attention" title="Permanent link">¶</a></h3>
<ul>
<li>Multiple attention “heads” (<span class="arithmatex">\(H\)</span>) learn different relationships:
  <script type="math/tex; mode=display">
  \text{MultiHead}(Q,K,V) = [h_1; h_2; \dots; h_H] W^O
  </script>
  Each head captures a distinct pattern (syntax, semantics, position, etc.).</li>
</ul>
<h3 id="deeplearning-5_attention-transformer-block">Transformer Block<a class="headerlink" href="#deeplearning-5_attention-transformer-block" title="Permanent link">¶</a></h3>
<ul>
<li>Structure:</li>
<li>Multi-head self-attention  </li>
<li>Add &amp; LayerNorm  </li>
<li>Feedforward (ReLU + linear)  </li>
<li>Add &amp; LayerNorm  </li>
<li>Skip connections improve gradient flow and allow top-down signal mixing.</li>
</ul>
<h3 id="deeplearning-5_attention-positional-encoding">Positional Encoding<a class="headerlink" href="#deeplearning-5_attention-positional-encoding" title="Permanent link">¶</a></h3>
<ul>
<li>Since model is permutation-invariant, inject position information:
  <script type="math/tex; mode=display">
  \text{PE}_{(pos,2i)} = \sin(pos / 10000^{2i/d})
  </script>
<script type="math/tex; mode=display">
  \text{PE}_{(pos,2i+1)} = \cos(pos / 10000^{2i/d})
  </script>
  Added to input embeddings.</li>
</ul>
<h3 id="deeplearning-5_attention-intuition">Intuition<a class="headerlink" href="#deeplearning-5_attention-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>Self-attention generalizes RNN memory:</li>
<li>Recurrent → sequential access  </li>
<li>Transformer → <em>direct pairwise access</em> between all tokens.</li>
<li>Enables long-range dependencies and parallelization.</li>
</ul>
<h3 id="deeplearning-5_attention-key-result">Key result<a class="headerlink" href="#deeplearning-5_attention-key-result" title="Permanent link">¶</a></h3>
<ul>
<li>Attention-only models achieve SOTA in translation and NLP tasks.  </li>
<li>Forms basis for BERT, GPT, and modern large language models.</li>
</ul>
<h2 id="deeplearning-5_attention-7-adaptive-computation-time-act-and-summary">7. Adaptive Computation Time (ACT) and Summary<a class="headerlink" href="#deeplearning-5_attention-7-adaptive-computation-time-act-and-summary" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-5_attention-adaptive-computation-time-act">Adaptive Computation Time (ACT)<a class="headerlink" href="#deeplearning-5_attention-adaptive-computation-time-act" title="Permanent link">¶</a></h3>
<ul>
<li>Proposed by Graves (2016): allows networks to “ponder” variable amounts of time per input.  </li>
<li>Each step computes a halting probability <span class="arithmatex">\(p_t\)</span>; total halt when <span class="arithmatex">\(\sum_t p_t = 1\)</span>.</li>
<li>Output is a weighted sum of intermediate states:
  <script type="math/tex; mode=display">
  y = \sum_t p_t h_t
  </script>
</li>
<li>Encourages efficient use of computation — more steps for harder inputs, fewer for easy ones.</li>
<li>Regularized by a <em>time penalty</em> to avoid overthinking.</li>
</ul>
<h3 id="deeplearning-5_attention-universal-transformers">Universal Transformers<a class="headerlink" href="#deeplearning-5_attention-universal-transformers" title="Permanent link">¶</a></h3>
<ul>
<li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li>
<li>Shares parameters across layers — like an RNN unrolled over depth.</li>
<li>Combine parallel self-attention + iterative refinement + ACT.</li>
<li>Achieves better generalization and adaptive reasoning on sequence tasks.</li>
</ul>
<h2 id="deeplearning-5_attention-summary">Summary<a class="headerlink" href="#deeplearning-5_attention-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Attention = selective processing of relevant information.  </li>
<li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li>
<li>Explicit attention can be hard (sampled) or soft (differentiable).  </li>
<li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li>
<li>Transformers unify attention as the core mechanism — fully parallel, context-rich.  </li>
<li>Adaptive computation gives flexibility in processing time and complexity.</li>
</ul>
<p>Takeaway:<br>
Selective attention and memory — biological inspirations — are now core architectural principles driving modern deep learning.</p></body></html></section><section class="print-page" id="deeplearning-6_gans" heading-number="4.6"><h1 id="deeplearning-6_gans-deeplearning-6_gans">6. Generative Models and GANs</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-overview-generative-models">1. Overview: Generative Models<a class="headerlink" href="#deeplearning-6_gans-1-overview-generative-models" title="Permanent link">¶</a></h2>
<ul>
<li>Goal: learn a model of the true data distribution <span class="arithmatex">\(p^*(x)\)</span> from samples.</li>
</ul>
<h3 id="deeplearning-6_gans-types-of-generative-models">Types of Generative Models<a class="headerlink" href="#deeplearning-6_gans-types-of-generative-models" title="Permanent link">¶</a></h3>
<ol>
<li>Explicit likelihood models – define tractable <span class="arithmatex">\(p_\theta(x)\)</span></li>
<li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li>
<li>
<p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p>
</li>
<li>
<p>Implicit models – define <em>sampling procedure</em>, not explicit <span class="arithmatex">\(p_\theta(x)\)</span>  </p>
</li>
<li>Examples: GANs, Moment Matching Networks.</li>
</ol>
<h2 id="deeplearning-6_gans-11-the-gan-idea">1.1 The GAN Idea<a class="headerlink" href="#deeplearning-6_gans-11-the-gan-idea" title="Permanent link">¶</a></h2>
<ul>
<li>Two-player minimax game:</li>
<li>Generator (G): maps noise <span class="arithmatex">\(z \sim p(z)\)</span> to data space <span class="arithmatex">\(G(z)\)</span>.</li>
<li>
<p>Discriminator (D): classifies samples as <em>real</em> (from <span class="arithmatex">\(p^*(x)\)</span>) or <em>fake</em> (<span class="arithmatex">\(G(z)\)</span>).</p>
</li>
<li>
<p>Objectives:
  <script type="math/tex; mode=display">
  \min_G \max_D \; \mathbb{E}_{x\sim p^*(x)}[\log D(x)] +
  \mathbb{E}_{z\sim p(z)}[\log(1 - D(G(z)))]
  </script>
</p>
</li>
<li>
<p>Interpretation:</p>
</li>
<li><span class="arithmatex">\(D\)</span> learns to distinguish real from fake.</li>
<li><span class="arithmatex">\(G\)</span> learns to fool <span class="arithmatex">\(D\)</span>.</li>
<li>Training reaches equilibrium when <span class="arithmatex">\(p_G(x) = p^*(x)\)</span>.</li>
</ul>
<h2 id="deeplearning-6_gans-12-alternative-view-teacherstudent-analogy">1.2 Alternative View — Teacher–Student Analogy<a class="headerlink" href="#deeplearning-6_gans-12-alternative-view-teacherstudent-analogy" title="Permanent link">¶</a></h2>
<ul>
<li>Teacher (D): distinguishes real vs fake, providing feedback.</li>
<li>Student (G): improves by making fake data look real.</li>
<li>Cooperative interpretation of the adversarial process.</li>
</ul>
<h2 id="deeplearning-6_gans-13-gans-as-a-game">1.3 GANs as a Game<a class="headerlink" href="#deeplearning-6_gans-13-gans-as-a-game" title="Permanent link">¶</a></h2>
<ul>
<li>Zero-sum, bi-level optimization → strong connection to game theory.</li>
<li>GAN equilibrium = Nash equilibrium between <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.</li>
<li>Training alternates between optimizing <span class="arithmatex">\(D\)</span> and <span class="arithmatex">\(G\)</span>.</li>
</ul>
<p>Key Intuition:<br>
GANs learn by <em>competition</em> between a generator and discriminator rather than direct likelihood maximization.</p>
<h2 id="deeplearning-6_gans-2-gan-objective-as-divergence-minimization">2. GAN Objective as Divergence Minimization<a class="headerlink" href="#deeplearning-6_gans-2-gan-objective-as-divergence-minimization" title="Permanent link">¶</a></h2>
<ul>
<li>Generative modeling often aims to minimize a distance or divergence between
  the true data distribution <span class="arithmatex">\(p^*(x)\)</span> and model distribution <span class="arithmatex">\(p_G(x)\)</span>.</li>
</ul>
<h3 id="deeplearning-6_gans-21-kl-and-related-divergences">2.1 KL and Related Divergences<a class="headerlink" href="#deeplearning-6_gans-21-kl-and-related-divergences" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Maximum Likelihood Estimation (MLE):
  <script type="math/tex; mode=display">
  \min_\theta D_{\text{KL}}(p^*(x) \| p_\theta(x))
  </script>
  → drives <span class="arithmatex">\(p_\theta\)</span> to assign high probability to observed data.</p>
</li>
<li>
<p>But: implicit models (like GANs) don’t have explicit likelihoods, so MLE can’t be used directly.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-22-gan-as-jensenshannon-js-divergence-minimization">2.2 GAN as Jensen–Shannon (JS) Divergence Minimization<a class="headerlink" href="#deeplearning-6_gans-22-gan-as-jensenshannon-js-divergence-minimization" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>If discriminator <span class="arithmatex">\(D\)</span> is optimal:
  <script type="math/tex; mode=display">
  D^*(x) = \frac{p^*(x)}{p^*(x) + p_G(x)}
  </script>
  Plugging into the GAN loss shows that the generator minimizes:
  <script type="math/tex; mode=display">
  D_{\text{JS}}(p^*(x) \| p_G(x))
  </script>
  → GAN ≈ JS divergence minimization.</p>
</li>
<li>
<p>However, this relies on an <em>optimal discriminator</em> — not true in practice.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-23-limitations-of-kl-js-divergences">2.3 Limitations of KL / JS Divergences<a class="headerlink" href="#deeplearning-6_gans-23-limitations-of-kl-js-divergences" title="Permanent link">¶</a></h2>
<ul>
<li>If <span class="arithmatex">\(p_G\)</span> and <span class="arithmatex">\(p^*\)</span> have non-overlapping support,<br>
  → no useful gradient signal (zero gradient problem).</li>
<li>The density ratio <span class="arithmatex">\(\frac{p^*(x)}{p_G(x)}\)</span> becomes infinite where <span class="arithmatex">\(p_G=0\)</span>.</li>
<li>Thus, GANs can fail to learn when supports are disjoint.</li>
</ul>
<h2 id="deeplearning-6_gans-24-alternative-distances-divergences">2.4 Alternative Distances &amp; Divergences<a class="headerlink" href="#deeplearning-6_gans-24-alternative-distances-divergences" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-wasserstein-distance-earth-movers">(a) Wasserstein Distance (Earth Mover’s)<a class="headerlink" href="#deeplearning-6_gans-a-wasserstein-distance-earth-movers" title="Permanent link">¶</a></h3>
<ul>
<li>Measures minimal “cost” of moving probability mass:
  <script type="math/tex; mode=display">
  W(p^*, p_G) = \inf_{\gamma \in \Pi(p^*, p_G)} \mathbb{E}_{(x,y)\sim \gamma}[\|x - y\|]
  </script>
</li>
<li>Provides smooth, non-vanishing gradients even when supports don’t overlap.</li>
<li>WGAN: enforce 1-Lipschitz <span class="arithmatex">\(D\)</span> via:</li>
<li>weight clipping,</li>
<li>gradient penalty (WGAN-GP),</li>
<li>spectral normalization.</li>
</ul>
<h3 id="deeplearning-6_gans-b-mmd-maximum-mean-discrepancy">(b) MMD (Maximum Mean Discrepancy)<a class="headerlink" href="#deeplearning-6_gans-b-mmd-maximum-mean-discrepancy" title="Permanent link">¶</a></h3>
<ul>
<li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):
  <script type="math/tex; mode=display">
  \text{MMD}^2(p, q) = \|\mathbb{E}_p[\phi(x)] - \mathbb{E}_q[\phi(x)]\|^2
  </script>
</li>
<li>MMD-GAN: learns kernel features <span class="arithmatex">\(\phi\)</span> jointly with <span class="arithmatex">\(D\)</span>.</li>
</ul>
<h3 id="deeplearning-6_gans-c-f-divergences">(c) f-divergences<a class="headerlink" href="#deeplearning-6_gans-c-f-divergences" title="Permanent link">¶</a></h3>
<ul>
<li>General framework using convex functions <span class="arithmatex">\(f\)</span>:
  <script type="math/tex; mode=display">
  D_f(p \| q) = \mathbb{E}_q[f\!\left(\frac{p(x)}{q(x)}\right)]
  </script>
</li>
<li>GAN training derived via variational lower bound on <span class="arithmatex">\(D_f\)</span>.</li>
</ul>
<h2 id="deeplearning-6_gans-25-practical-view">2.5 Practical View<a class="headerlink" href="#deeplearning-6_gans-25-practical-view" title="Permanent link">¶</a></h2>
<ul>
<li>GANs are not pure divergence minimizers in practice:</li>
<li><span class="arithmatex">\(D\)</span> not optimal → approximate divergence.</li>
<li>Neural discriminator learns a smooth approximation to density ratio.</li>
<li>Provides <em>useful gradients</em> even when the true divergence would fail.</li>
</ul>
<h2 id="deeplearning-6_gans-26-summary-table">2.6 Summary Table<a class="headerlink" href="#deeplearning-6_gans-26-summary-table" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Perspective</th>
<th>Example</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>KL Divergence</td>
<td>MLE, VAEs</td>
<td>Explicit likelihoods</td>
</tr>
<tr>
<td>JS Divergence</td>
<td>Original GAN</td>
<td>Adversarial training</td>
</tr>
<tr>
<td>Wasserstein</td>
<td>WGAN</td>
<td>Smooth gradients</td>
</tr>
<tr>
<td>MMD</td>
<td>MMD-GAN</td>
<td>Kernel mean embedding</td>
</tr>
<tr>
<td>f-divergence</td>
<td>f-GAN</td>
<td>Variational bound family</td>
</tr>
</tbody>
</table>
<p>Insight:<br>
GANs can be viewed as learning a <em>neural divergence measure</em> that provides a stable, informative training signal.</p>
<h2 id="deeplearning-6_gans-3-evaluating-gans">3. Evaluating GANs<a class="headerlink" href="#deeplearning-6_gans-3-evaluating-gans" title="Permanent link">¶</a></h2>
<ul>
<li>Evaluating generative models is difficult — no single metric captures all aspects.</li>
<li>Must assess:</li>
<li>Sample quality (fidelity, realism)</li>
<li>Diversity / generalization</li>
<li>Representation learning (usefulness of learned features)</li>
</ul>
<h2 id="deeplearning-6_gans-31-why-not-log-likelihood">3.1 Why Not Log-Likelihood?<a class="headerlink" href="#deeplearning-6_gans-31-why-not-log-likelihood" title="Permanent link">¶</a></h2>
<ul>
<li>GANs are implicit models — no tractable <span class="arithmatex">\(p(x)\)</span>.</li>
<li>Estimating log-likelihood is expensive and unreliable.</li>
<li>Hence: use feature-based or classifier-based proxies.</li>
</ul>
<h2 id="deeplearning-6_gans-32-inception-score-is">3.2 Inception Score (IS)<a class="headerlink" href="#deeplearning-6_gans-32-inception-score-is" title="Permanent link">¶</a></h2>
<ul>
<li>Uses a pretrained Inception v3 classifier.</li>
<li>Compares predicted label distributions of generated samples.</li>
</ul>
<p>Formula:
<script type="math/tex; mode=display">
\text{IS} = \exp\!\left( \mathbb{E}_{x \sim G} [ D_{KL}(p(y|x) \| p(y)) ] \right)
</script>
</p>
<p>Intuition:
- High-quality images → confident predictions (<span class="arithmatex">\(p(y|x)\)</span> low entropy).<br>
- Diverse images → marginal label distribution <span class="arithmatex">\(p(y)\)</span> high entropy.</p>
<p>Properties:
- Measures <em>sample quality</em> and <em>diversity</em>.
- Correlates with human judgment.
- Fails to capture intra-class variation or features beyond ImageNet classes.</p>
<p>Higher is better.</p>
<h2 id="deeplearning-6_gans-33-frechet-inception-distance-fid">3.3 Fréchet Inception Distance (FID)<a class="headerlink" href="#deeplearning-6_gans-33-frechet-inception-distance-fid" title="Permanent link">¶</a></h2>
<ul>
<li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li>
</ul>
<p>Formula:
<script type="math/tex; mode=display">
\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
</script>
</p>
<p>where <span class="arithmatex">\((\mu_r, \Sigma_r)\)</span> and <span class="arithmatex">\((\mu_g, \Sigma_g)\)</span> are mean and covariance of real and generated data features.</p>
<p>Properties:
- Sensitive to mode dropping and artifacts.
- Correlates strongly with human evaluation.
- Lower is better.
- Biased for small sample sizes → use KID (Kernel Inception Distance) for correction.</p>
<h2 id="deeplearning-6_gans-34-overfitting-check-nearest-neighbours">3.4 Overfitting Check — Nearest Neighbours<a class="headerlink" href="#deeplearning-6_gans-34-overfitting-check-nearest-neighbours" title="Permanent link">¶</a></h2>
<ul>
<li>Compute nearest real images to generated samples in pretrained feature space.</li>
<li>Helps detect memorization (copying training images).</li>
</ul>
<h2 id="deeplearning-6_gans-35-evaluation-depends-on-goal">3.5 Evaluation Depends on Goal<a class="headerlink" href="#deeplearning-6_gans-35-evaluation-depends-on-goal" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Goal</th>
<th>Metric Example</th>
<th>Measures</th>
</tr>
</thead>
<tbody>
<tr>
<td>Image quality</td>
<td>FID, IS</td>
<td>Fidelity &amp; diversity</td>
</tr>
<tr>
<td>Representation learning</td>
<td>Linear probe accuracy</td>
<td>Feature usefulness</td>
</tr>
<tr>
<td>Data generation</td>
<td>Human evaluation</td>
<td>Perceptual quality</td>
</tr>
<tr>
<td>RL / control</td>
<td>Policy reward</td>
<td>Functional realism</td>
</tr>
</tbody>
</table>
<p>Key Takeaway:<br>
Use <em>multiple complementary metrics</em> — quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>
<h2 id="deeplearning-6_gans-4-the-gan-zoo">4. The GAN Zoo<a class="headerlink" href="#deeplearning-6_gans-4-the-gan-zoo" title="Permanent link">¶</a></h2>
<blockquote>
<p>GANs have evolved rapidly — from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>
</blockquote>
<h2 id="deeplearning-6_gans-41-the-original-gan">4.1 The Original GAN<a class="headerlink" href="#deeplearning-6_gans-41-the-original-gan" title="Permanent link">¶</a></h2>
<ul>
<li>First formulation of adversarial training.</li>
<li>Architecture: simple multilayer perceptrons (MLPs).</li>
<li>Trained on small images (e.g. 32×32).  </li>
<li>Ignored spatial structure (flattened pixels).  </li>
<li>Introduced the minimax objective still used today.</li>
</ul>
<h2 id="deeplearning-6_gans-42-conditional-gan">4.2 Conditional GAN<a class="headerlink" href="#deeplearning-6_gans-42-conditional-gan" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Adds <em>conditioning information</em> <span class="arithmatex">\(y\)</span> (e.g. class label or input image).<br>
<script type="math/tex; mode=display">
  \min_G \max_D \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{z,y}[\log(1 - D(G(z,y),y))]
  </script>
</p>
</li>
<li>
<p>Enables controlled generation — specify category or domain.<br>
  Examples:</p>
</li>
<li>Class-conditional image synthesis (e.g., "generate a dog").  </li>
<li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li>
</ul>
<h2 id="deeplearning-6_gans-43-laplacian-gan">4.3 Laplacian GAN<a class="headerlink" href="#deeplearning-6_gans-43-laplacian-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Generates images progressively, starting from low resolution.  </li>
<li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li>
<li>Fully convolutional — can produce arbitrarily large outputs.</li>
<li>Improves high-res synthesis through multi-scale structure.</li>
</ul>
<h2 id="deeplearning-6_gans-44-deep-convolutional-gan">4.4 Deep Convolutional GAN<a class="headerlink" href="#deeplearning-6_gans-44-deep-convolutional-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Replaces MLPs with deep convnets for both <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.</li>
<li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li>
<li>Enables smooth interpolation in latent space:</li>
<li><span class="arithmatex">\(G(z_1)\)</span> → <span class="arithmatex">\(G(\frac{1}{2}(z_1 + z_2))\)</span> → <span class="arithmatex">\(G(z_2)\)</span> produces semantically meaningful transitions.</li>
<li>Latent space exhibits semantic arithmetic (e.g. “man + glasses – woman”).</li>
</ul>
<h2 id="deeplearning-6_gans-45-spectrally-normalized-gan">4.5 Spectrally Normalized GAN<a class="headerlink" href="#deeplearning-6_gans-45-spectrally-normalized-gan" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Enforces 1-Lipschitz constraint on <span class="arithmatex">\(D\)</span> via spectral normalization:
  <script type="math/tex; mode=display">
  \bar{W} = \frac{W}{\sigma_{\max}(W)}
  </script>
  where <span class="arithmatex">\(\sigma_{\max}(W)\)</span> is the largest singular value.</p>
</li>
<li>
<p>Stabilizes training and improves generalization.</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-46-projection-discriminator">4.6 Projection Discriminator<a class="headerlink" href="#deeplearning-6_gans-46-projection-discriminator" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Adds class embedding projection inside <span class="arithmatex">\(D\)</span>:<br>
<script type="math/tex; mode=display">
  D(x, y) = h(x)^\top v_y + b_y
  </script>
  where <span class="arithmatex">\(v_y\)</span> is the embedding for class <span class="arithmatex">\(y\)</span>.</p>
</li>
<li>
<p>Theoretically consistent probabilistic discriminator formulation.  </p>
</li>
<li>Strong empirical results on class-conditional image synthesis.</li>
</ul>
<h2 id="deeplearning-6_gans-47-self-attention-gan">4.7 Self-Attention GAN<a class="headerlink" href="#deeplearning-6_gans-47-self-attention-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Introduces self-attention layers to capture long-range dependencies.  </li>
<li>Improves global structure and coherence in generated images.</li>
<li>Inspired by Transformer attention.</li>
</ul>
<h2 id="deeplearning-6_gans-48-biggan">4.8 BigGAN<a class="headerlink" href="#deeplearning-6_gans-48-biggan" title="Permanent link">¶</a></h2>
<ul>
<li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li>
<li>Key ingredients:</li>
<li>Hinge loss for <span class="arithmatex">\(D\)</span>  </li>
<li>Spectral normalization  </li>
<li>Self-attention  </li>
<li>Projection discriminator  </li>
<li>Orthogonal regularization  </li>
<li>Skip connections from noise  </li>
<li>Shared class embeddings  </li>
<li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li>
</ul>
<h2 id="deeplearning-6_gans-49-logan">4.9 LOGAN<a class="headerlink" href="#deeplearning-6_gans-49-logan" title="Permanent link">¶</a></h2>
<ul>
<li>Introduces latent optimization — optimize <span class="arithmatex">\(z\)</span> via gradient updates to improve adversarial dynamics.  </li>
<li>Uses natural gradient descent in latent space.  </li>
<li>Yields higher FID/IS improvements over BigGAN.</li>
</ul>
<h2 id="deeplearning-6_gans-410-progressive-gan">4.10 Progressive GAN<a class="headerlink" href="#deeplearning-6_gans-410-progressive-gan" title="Permanent link">¶</a></h2>
<ul>
<li>Trains from low to high resolution (4×4 → 8×8 → 16×16 …).  </li>
<li>Each stage adds new layers to <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span>.  </li>
<li>Dramatically improves stability and image quality (especially faces).</li>
</ul>
<h2 id="deeplearning-6_gans-411-stylegan">4.11 StyleGAN<a class="headerlink" href="#deeplearning-6_gans-411-stylegan" title="Permanent link">¶</a></h2>
<ul>
<li>Adds style-based generator architecture:</li>
<li>Latent vector <span class="arithmatex">\(z\)</span> transformed by MLP to intermediate <span class="arithmatex">\(w\)</span>.</li>
<li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li>
<li>
<p>Injects per-pixel noise for local details.</p>
</li>
<li>
<p>Learns disentangled representations — global attributes (style) vs local (texture).</p>
</li>
</ul>
<h2 id="deeplearning-6_gans-412-takeaways">4.12 Takeaways<a class="headerlink" href="#deeplearning-6_gans-412-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>GAN progress driven by:</li>
<li>Better architectures (Conv, Attention, Progressive, Style-based)</li>
<li>Normalization &amp; regularization</li>
<li>Stability techniques</li>
<li>Large-scale training</li>
</ul>
<p>Trend:<br>
From small MLPs → Conv architectures → Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>
<h2 id="deeplearning-6_gans-5-representation-learning-with-gans">5. Representation Learning with GANs<a class="headerlink" href="#deeplearning-6_gans-5-representation-learning-with-gans" title="Permanent link">¶</a></h2>
<blockquote>
<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>
</blockquote>
<h2 id="deeplearning-6_gans-51-motivation">5.1 Motivation<a class="headerlink" href="#deeplearning-6_gans-51-motivation" title="Permanent link">¶</a></h2>
<ul>
<li>GANs implicitly learn latent spaces that capture high-level semantics.</li>
<li>Exploring or constraining this latent space enables unsupervised representation learning.</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-52-evidence-from-dcgan">5.2 Evidence from DCGAN<a class="headerlink" href="#deeplearning-6_gans-52-evidence-from-dcgan" title="Permanent link">¶</a></h2>
<ul>
<li>DCGAN latent vectors encode meaningful directions:</li>
<li>Smooth interpolation between points → semantic transformations.</li>
<li>Linear arithmetic in latent space (e.g., <em>smiling woman – woman + man → smiling man</em>).</li>
<li>Suggests disentangled feature representations emerge naturally.</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-53-infogan">5.3 InfoGAN<a class="headerlink" href="#deeplearning-6_gans-53-infogan" title="Permanent link">¶</a></h2>
<ul>
<li>Extends GAN with information maximization objective:</li>
<li>Encourages some latent codes <span class="arithmatex">\(c\)</span> to be <em>interpretable</em> and <em>disentangled</em>.</li>
</ul>
<p>Objective:
<script type="math/tex; mode=display">
\min_G \max_D V(D,G) - \lambda I(c; G(z, c))
</script>
where <span class="arithmatex">\(I(c; G(z, c))\)</span> is mutual information between latent code and generated output.</p>
<ul>
<li>Adds an auxiliary network to infer <span class="arithmatex">\(c\)</span> from <span class="arithmatex">\(G(z, c)\)</span>.</li>
<li>Learns to associate:</li>
<li>Discrete codes → categories (digits, shapes)</li>
<li>Continuous codes → attributes (rotation, scale)</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-54-ali-bigan">5.4 ALI / BiGAN<a class="headerlink" href="#deeplearning-6_gans-54-ali-bigan" title="Permanent link">¶</a></h2>
<ul>
<li>Adds an encoder <span class="arithmatex">\(E(x)\)</span> mapping real data to latent space.</li>
<li>
<p>Joint discriminator distinguishes pairs:
  <script type="math/tex; mode=display">
  (x, E(x)) \quad \text{vs.} \quad (G(z), z)
  </script>
</p>
</li>
<li>
<p>At equilibrium:</p>
</li>
<li>
<p><span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(G\)</span> become approximate inverses:</p>
<ul>
<li><span class="arithmatex">\(x \approx G(E(x))\)</span></li>
<li><span class="arithmatex">\(z \approx E(G(z))\)</span></li>
</ul>
</li>
<li>
<p>Enables inference and representation learning simultaneously.</p>
</li>
</ul>
<hr>
<h2 id="deeplearning-6_gans-55-bigbigan">5.5 BigBiGAN<a class="headerlink" href="#deeplearning-6_gans-55-bigbigan" title="Permanent link">¶</a></h2>
<ul>
<li>Scales BiGAN to BigGAN architecture.</li>
<li>Uses large-scale encoders (<span class="arithmatex">\(E\)</span>) with ResNet blocks.</li>
<li>Learns strong unsupervised representations competitive with self-supervised models.</li>
</ul>
<p>Observations:
- Reconstructions <span class="arithmatex">\(G(E(x))\)</span> preserve semantic content, not exact pixels.
- Encoder features yield high ImageNet classification accuracy after linear probing.</p>
<hr>
<h2 id="deeplearning-6_gans-56-summary">5.6 Summary<a class="headerlink" href="#deeplearning-6_gans-56-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Key Idea</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr>
<td>DCGAN</td>
<td>Implicitly semantic latent space</td>
<td>Interpolations meaningful</td>
</tr>
<tr>
<td>InfoGAN</td>
<td>Maximize info between codes and outputs</td>
<td>Disentangled features</td>
</tr>
<tr>
<td>BiGAN / ALI</td>
<td>Add encoder, joint training</td>
<td>Bidirectional mapping</td>
</tr>
<tr>
<td>BigBiGAN</td>
<td>Large-scale BiGAN</td>
<td>Competitive unsupervised features</td>
</tr>
</tbody>
</table>
<p>Key Insight:<br>
GANs not only <em>generate</em>, but also <em>encode</em> — their latent structure can act as a rich, learned representation space.</p>
<h2 id="deeplearning-6_gans-6-gans-for-other-modalities-and-problems">6. GANs for Other Modalities and Problems<a class="headerlink" href="#deeplearning-6_gans-6-gans-for-other-modalities-and-problems" title="Permanent link">¶</a></h2>
<blockquote>
<p>GANs extend far beyond images — used for translation, audio, video, RL, and even art.</p>
</blockquote>
<hr>
<h2 id="deeplearning-6_gans-61-image-to-image-translation">6.1 Image-to-Image Translation<a class="headerlink" href="#deeplearning-6_gans-61-image-to-image-translation" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-pix2pix">(a) Pix2Pix<a class="headerlink" href="#deeplearning-6_gans-a-pix2pix" title="Permanent link">¶</a></h3>
<ul>
<li>Conditional GAN trained on <em>paired</em> datasets <span class="arithmatex">\((x, y)\)</span>.</li>
<li>Learns deterministic mapping between domains (e.g., edges → photos).</li>
<li>Loss combines adversarial term + L1 reconstruction:
  <script type="math/tex; mode=display">
  \mathcal{L}_{\text{Pix2Pix}} = \mathcal{L}_{\text{GAN}}(G,D) + \lambda \|y - G(x)\|_1
  </script>
</li>
</ul>
<h3 id="deeplearning-6_gans-b-cyclegan">(b) CycleGAN<a class="headerlink" href="#deeplearning-6_gans-b-cyclegan" title="Permanent link">¶</a></h3>
<ul>
<li>Unpaired domain translation — no 1:1 correspondence.</li>
<li>Uses cycle consistency:</li>
<li><span class="arithmatex">\(x \in A \to G_B(x) \to F_A(G_B(x)) \approx x\)</span></li>
<li>Enforces invertibility between domains.</li>
<li>Enables tasks like <em>horse ↔ zebra</em>, <em>summer ↔ winter</em>.</li>
</ul>
<h2 id="deeplearning-6_gans-62-audio-synthesis">6.2 Audio Synthesis<a class="headerlink" href="#deeplearning-6_gans-62-audio-synthesis" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-6_gans-a-wavegan">(a) WaveGAN<a class="headerlink" href="#deeplearning-6_gans-a-wavegan" title="Permanent link">¶</a></h3>
<ul>
<li>Adapts convolutional GANs to 1D waveforms.</li>
<li>Fully unsupervised raw-audio synthesis.</li>
</ul>
<h3 id="deeplearning-6_gans-b-melgan">(b) MelGAN<a class="headerlink" href="#deeplearning-6_gans-b-melgan" title="Permanent link">¶</a></h3>
<ul>
<li>Conditional GAN trained to generate mel-spectrogram waveforms.</li>
<li>Used in text-to-speech (GAN-TTS).</li>
</ul>
<h3 id="deeplearning-6_gans-c-gan-tts">(c) GAN-TTS<a class="headerlink" href="#deeplearning-6_gans-c-gan-tts" title="Permanent link">¶</a></h3>
<ul>
<li>High-fidelity speech synthesis model.</li>
<li>Achieves human-like audio quality via adversarial losses.</li>
</ul>
<h2 id="deeplearning-6_gans-63-video-synthesis-prediction">6.3 Video Synthesis &amp; Prediction<a class="headerlink" href="#deeplearning-6_gans-63-video-synthesis-prediction" title="Permanent link">¶</a></h2>
<ul>
<li>GANs extended to spatiotemporal data:</li>
<li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li>
<li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li>
<li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li>
</ul>
<h2 id="deeplearning-6_gans-64-gans-in-reinforcement-learning-imitation-control">6.4 GANs in Reinforcement Learning (Imitation &amp; Control)<a class="headerlink" href="#deeplearning-6_gans-64-gans-in-reinforcement-learning-imitation-control" title="Permanent link">¶</a></h2>
<ul>
<li>GAIL (Ho &amp; Ermon, 2016): <em>Generative Adversarial Imitation Learning</em>  </li>
<li>Discriminator distinguishes expert vs policy trajectories.</li>
<li>Generator = policy network optimizing to mimic experts.</li>
</ul>
<h2 id="deeplearning-6_gans-65-creative-applied-uses">6.5 Creative &amp; Applied Uses<a class="headerlink" href="#deeplearning-6_gans-65-creative-applied-uses" title="Permanent link">¶</a></h2>
<ul>
<li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li>
<li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li>
<li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li>
<li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li>
<li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li>
</ul>
<h2 id="deeplearning-6_gans-66-summary">6.6 Summary<a class="headerlink" href="#deeplearning-6_gans-66-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Example</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paired image translation</td>
<td>Pix2Pix</td>
<td>Conditional GAN + L1 loss</td>
</tr>
<tr>
<td>Unpaired translation</td>
<td>CycleGAN</td>
<td>Cycle consistency</td>
</tr>
<tr>
<td>Audio</td>
<td>MelGAN, WaveGAN</td>
<td>Conditional waveform generation</td>
</tr>
<tr>
<td>Video</td>
<td>DVD-GAN, TGAN-v2</td>
<td>Temporal adversarial modeling</td>
</tr>
<tr>
<td>RL / Imitation</td>
<td>GAIL</td>
<td>Adversarial trajectory matching</td>
</tr>
<tr>
<td>Art / Creativity</td>
<td>GauGAN, SPIRAL</td>
<td>Adversarial synthesis and style transfer</td>
</tr>
</tbody>
</table>
<p>Insight:<br>
Adversarial learning generalizes across domains — GANs serve as a <em>universal generator–critic framework</em> for structured data.</p></body></html></section><section class="print-page" id="deeplearning-7_unsuper" heading-number="4.7"><h1 id="deeplearning-7_unsuper-deeplearning-7_unsuper">7. Unsupervised and Self-Supervised Learning</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-what-is-unsupervised-learning">1. What is Unsupervised Learning?<a class="headerlink" href="#deeplearning-7_unsuper-1-what-is-unsupervised-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-definition">Definition<a class="headerlink" href="#deeplearning-7_unsuper-definition" title="Permanent link">¶</a></h3>
<ul>
<li>Goal: discover structure in data without explicit labels or rewards.  </li>
<li>Learns a compact, informative representation of input data.</li>
</ul>
<table>
<thead>
<tr>
<th>Learning Type</th>
<th>Goal</th>
<th>Supervision</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised</td>
<td>Map inputs → labels</td>
<td>Requires labeled data</td>
</tr>
<tr>
<td>Reinforcement</td>
<td>Learn actions maximizing future reward</td>
<td>Requires reward signal</td>
</tr>
<tr>
<td>Unsupervised</td>
<td>Find hidden structure</td>
<td>No labels or rewards</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-core-ideas">Core Ideas<a class="headerlink" href="#deeplearning-7_unsuper-core-ideas" title="Permanent link">¶</a></h3>
<ul>
<li>Model latent structure or relationships between observations.  </li>
<li>Examples:</li>
<li>Clustering: group similar data points.  </li>
<li>Dimensionality reduction: project data to low-dimensional latent space.  </li>
<li>Manifold learning / disentangling: uncover independent factors of variation.</li>
</ul>
<h3 id="deeplearning-7_unsuper-evaluation-challenges">Evaluation Challenges<a class="headerlink" href="#deeplearning-7_unsuper-evaluation-challenges" title="Permanent link">¶</a></h3>
<p>How do we know if unsupervised learning worked?</p>
<ul>
<li>Ambiguity of structure: multiple valid clusterings possible.<br>
  e.g., cluster by <em>leg count</em>, <em>arm number</em>, or <em>height</em> in robot dataset.</li>
<li>Metrics depend on downstream use:<br>
  useful representations should improve data efficiency, generalization, or transfer.</li>
</ul>
<h3 id="deeplearning-7_unsuper-classic-methods">Classic Methods<a class="headerlink" href="#deeplearning-7_unsuper-classic-methods" title="Permanent link">¶</a></h3>
<ul>
<li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li>
<li>ICA (Independent Component Analysis): separates statistically independent components.  </li>
<li>Modern goal: move beyond orthogonality → learn <em>disentangled</em> factors.</li>
</ul>
<h3 id="deeplearning-7_unsuper-summary">Summary<a class="headerlink" href="#deeplearning-7_unsuper-summary" title="Permanent link">¶</a></h3>
<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself — forming the foundation for <em>representation learning</em>.</p>
<h2 id="deeplearning-7_unsuper-2-why-is-unsupervised-learning-important">2. Why is Unsupervised Learning Important?<a class="headerlink" href="#deeplearning-7_unsuper-2-why-is-unsupervised-learning-important" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-21-historical-context-of-representation-learning">2.1 Historical Context of Representation Learning<a class="headerlink" href="#deeplearning-7_unsuper-21-historical-context-of-representation-learning" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Era</th>
<th>Key Milestone</th>
<th>Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td>1950s–2000s</td>
<td>Arthur Samuel (1959): <em>Machine Learning</em> coined</td>
<td>Feature engineering, clustering</td>
</tr>
<tr>
<td>2000s</td>
<td>Kernel methods (Hofmann et al., 2008)</td>
<td>Hand-crafted similarity functions</td>
</tr>
<tr>
<td>2006</td>
<td>Hinton &amp; Salakhutdinov: <em>RBMs &amp; Autoencoders</em></td>
<td>Layer-wise unsupervised pretraining</td>
</tr>
<tr>
<td>2012</td>
<td>Krizhevsky et al.: <em>AlexNet</em></td>
<td>End-to-end supervised learning dominates</td>
</tr>
</tbody>
</table>
<ul>
<li>Progress came from more data, deeper models, and better hardware — but not necessarily more efficient learning.</li>
</ul>
<h3 id="deeplearning-7_unsuper-22-limitations-of-purely-supervised-learning">2.2 Limitations of Purely Supervised Learning<a class="headerlink" href="#deeplearning-7_unsuper-22-limitations-of-purely-supervised-learning" title="Permanent link">¶</a></h3>
<p>Supervised models are:
- Data inefficient — need millions of labeled samples.
- Brittle — vulnerable to adversarial perturbations.
- Poor at transfer — struggle with new domains or tasks.
- Lack common sense — limited abstraction and reasoning.</p>
<h3 id="deeplearning-7_unsuper-23-evidence-of-current-gaps">2.3 Evidence of Current Gaps<a class="headerlink" href="#deeplearning-7_unsuper-23-evidence-of-current-gaps" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Example</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data efficiency</td>
<td>Learning from few examples</td>
<td>Lake et al. (2017)</td>
</tr>
<tr>
<td>Robustness</td>
<td>Adversarial examples, brittle decisions</td>
<td>Goodfellow et al. (2015)</td>
</tr>
<tr>
<td>Generalization</td>
<td>CoinRun, DMLab-30</td>
<td>Cobbe (2018), DeepMind</td>
</tr>
<tr>
<td>Transfer</td>
<td>Schema Networks</td>
<td>Kansky et al. (2017)</td>
</tr>
<tr>
<td>Common sense</td>
<td>Conceptual reasoning</td>
<td>Lake et al. (2015)</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-24-why-unsupervised-learning-matters">2.4 Why Unsupervised Learning Matters<a class="headerlink" href="#deeplearning-7_unsuper-24-why-unsupervised-learning-matters" title="Permanent link">¶</a></h3>
<ul>
<li>Enables data-efficient adaptation to new tasks.</li>
<li>Provides robust, generalizable features.</li>
<li>Promotes transfer learning by separating invariant factors.</li>
<li>Encourages abstract reasoning and causal understanding.</li>
</ul>
<h3 id="deeplearning-7_unsuper-25-towards-general-ai">2.5 Towards General AI<a class="headerlink" href="#deeplearning-7_unsuper-25-towards-general-ai" title="Permanent link">¶</a></h3>
<p>Unsupervised learning provides shared representations enabling:
- Rapid multi-task adaptation.<br>
- Reuse across vision, language, and control.<br>
- Reduced supervision in real-world learning.</p>
<p>Summary:<br>
Unsupervised representation learning addresses the core limits of current AI — aiming for <em>data efficiency, robustness, generalization, transfer,</em> and <em>common sense</em>.</p>
<h2 id="deeplearning-7_unsuper-3-what-makes-a-good-representation">3. What Makes a Good Representation?<a class="headerlink" href="#deeplearning-7_unsuper-3-what-makes-a-good-representation" title="Permanent link">¶</a></h2>
<blockquote>
<p>A representation is an internal model of the world — an abstraction that makes reasoning and prediction efficient.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-31-what-is-a-representation">3.1 What is a Representation?<a class="headerlink" href="#deeplearning-7_unsuper-31-what-is-a-representation" title="Permanent link">¶</a></h3>
<blockquote>
<p>“A formal system for making explicit certain entities or types of information, together with a specification of how the system does this.”</p>
</blockquote>
<ul>
<li>Represents <em>information about the world</em> in a way useful for computation.  </li>
<li>Not about a single feature, but the geometry or manifold shape in representational space.</li>
</ul>
<h3 id="deeplearning-7_unsuper-32-why-representation-form-matters">3.2 Why Representation Form Matters<a class="headerlink" href="#deeplearning-7_unsuper-32-why-representation-form-matters" title="Permanent link">¶</a></h3>
<ul>
<li>Determines which computations are easy.  </li>
<li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li>
</ul>
<h3 id="deeplearning-7_unsuper-33-desirable-properties">3.3 Desirable Properties<a class="headerlink" href="#deeplearning-7_unsuper-33-desirable-properties" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Intuition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Untangling</td>
<td>Simplifies complex input manifolds</td>
<td>Enables linear decoding</td>
</tr>
<tr>
<td>Attention</td>
<td>Allows selective focus on relevant factors</td>
<td>Supports task-specific filtering</td>
</tr>
<tr>
<td>Clustering</td>
<td>Groups similar experiences together</td>
<td>Facilitates generalization</td>
</tr>
<tr>
<td>Latent Information</td>
<td>Encodes hidden or inferred causes</td>
<td>Predicts unobserved aspects</td>
</tr>
<tr>
<td>Compositionality</td>
<td>Builds complex concepts from simple parts</td>
<td>Enables open-ended reasoning</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-34-information-bottleneck-principle">3.4 Information Bottleneck Principle<a class="headerlink" href="#deeplearning-7_unsuper-34-information-bottleneck-principle" title="Permanent link">¶</a></h3>
<ul>
<li>Good representations compress inputs while preserving information about outputs.
  <script type="math/tex; mode=display">
  \max I(Z; Y) - \beta I(Z; X)
  </script>
</li>
<li>Encourages minimal sufficient representations — compact yet predictive.</li>
</ul>
<h2 id="deeplearning-7_unsuper-4-evaluating-the-merit-of-a-representation">4. Evaluating the Merit of a Representation<a class="headerlink" href="#deeplearning-7_unsuper-4-evaluating-the-merit-of-a-representation" title="Permanent link">¶</a></h2>
<blockquote>
<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-41-the-evaluation-challenge">4.1 The Evaluation Challenge<a class="headerlink" href="#deeplearning-7_unsuper-41-the-evaluation-challenge" title="Permanent link">¶</a></h3>
<ul>
<li>No single metric defines a “good” representation.</li>
<li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li>
</ul>
<p>Representations should enable:
- Data efficiency — learn new tasks from few examples.<br>
- Robustness — resist noise or perturbations.<br>
- Generalization — perform well on new data.<br>
- Transfer — reuse knowledge in new settings.<br>
- Common sense — support reasoning and abstraction.</p>
<h3 id="deeplearning-7_unsuper-42-example-evaluating-representations-via-symmetries">4.2 Example: Evaluating Representations via Symmetries<a class="headerlink" href="#deeplearning-7_unsuper-42-example-evaluating-representations-via-symmetries" title="Permanent link">¶</a></h3>
<p>Let:
- <span class="arithmatex">\(W\)</span> = world space<br>
- <span class="arithmatex">\(Z\)</span> = representational space<br>
- <span class="arithmatex">\(G = G_x \times G_y \times G_c\)</span> = group of transformations (e.g., position, color)</p>
<p>A good representation <span class="arithmatex">\(f: W \rightarrow Z\)</span> should satisfy:
<script type="math/tex; mode=display">
f(g \cdot w) = g' \cdot f(w), \quad \forall g \in G, w \in W
</script>
</p>
<p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space → equivariance.</p>
<h3 id="deeplearning-7_unsuper-43-desirable-evaluation-criteria">4.3 Desirable Evaluation Criteria<a class="headerlink" href="#deeplearning-7_unsuper-43-desirable-evaluation-criteria" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Criterion</th>
<th>Desired Property</th>
<th>Example / Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>Equivariance</td>
<td>Transformations map consistently</td>
<td>Translation → shift in latent</td>
</tr>
<tr>
<td>Compositionality</td>
<td>Combine factors to form new concepts</td>
<td>Modular latent factors</td>
</tr>
<tr>
<td>Metric structure</td>
<td>Smooth distances reflect similarity</td>
<td><span class="arithmatex">\(L_2\)</span>, cosine</td>
</tr>
<tr>
<td>Attention</td>
<td>Selectively focus on task-relevant parts</td>
<td>Masking or gating mechanisms</td>
</tr>
<tr>
<td>Symmetries</td>
<td>Invariance to irrelevant transformations</td>
<td>Rotation, scale invariance</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-44-downstream-evaluation-tasks">4.4 Downstream Evaluation Tasks<a class="headerlink" href="#deeplearning-7_unsuper-44-downstream-evaluation-tasks" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Evaluation Setting</th>
<th>Example Task</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Perception / Control</td>
<td>Predict object color or position</td>
<td>Gens &amp; Domingos, <em>Deep Symmetry Networks</em> (2014)</td>
</tr>
<tr>
<td>Robustness</td>
<td>Classify images under adversarial noise</td>
<td>Gowal et al., 2019</td>
</tr>
<tr>
<td>Sequential Attention</td>
<td>Learn task-focused vision</td>
<td>Zoran et al., 2020</td>
</tr>
<tr>
<td>Transfer / RL</td>
<td>Zero-shot navigation (DARLA)</td>
<td>Higgins et al., ICML 2017</td>
</tr>
<tr>
<td>Lifelong Learning</td>
<td>Maintain latent structure over domains</td>
<td>Achille et al., NeurIPS 2018</td>
</tr>
<tr>
<td>Reasoning / Imagination</td>
<td>Compositional concept inference</td>
<td>Lake et al., <em>Science</em> 2015; Higgins et al., <em>ICLR</em> 2018</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-45-why-evaluation-matters">4.5 Why Evaluation Matters<a class="headerlink" href="#deeplearning-7_unsuper-45-why-evaluation-matters" title="Permanent link">¶</a></h3>
<p>A good representation supports simple mappings to downstream tasks:
- Linear classifiers for vision tasks (e.g., color or position recognition).<br>
- Efficient policy learning in RL with fewer samples.<br>
- Abstract reasoning and imagination — <em>“If rainbow elephants live in big cities, can we expect one in London?”</em></p>
<h2 id="deeplearning-7_unsuper-5-representation-learning-techniques">5. Representation Learning Techniques<a class="headerlink" href="#deeplearning-7_unsuper-5-representation-learning-techniques" title="Permanent link">¶</a></h2>
<blockquote>
<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches — all aiming to extract structure from data without labels.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-51-categories-of-methods">5.1 Categories of Methods<a class="headerlink" href="#deeplearning-7_unsuper-51-categories-of-methods" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>Core Idea</th>
<th>Typical Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generative Modeling</td>
<td>Learn <span class="arithmatex">\(p(x)\)</span> or a model that can <em>reconstruct</em> data</td>
<td>VAE, β-VAE, MONet, GQN, GANs</td>
</tr>
<tr>
<td>Contrastive Learning</td>
<td>Learn by <em>discriminating</em> similar vs dissimilar samples</td>
<td>CPC, SimCLR, word2vec</td>
</tr>
<tr>
<td>Self-Supervised Learning</td>
<td>Design <em>pretext tasks</em> that predict missing or reordered parts</td>
<td>BERT, Colorization, Context Prediction</td>
</tr>
</tbody>
</table>
<h2 id="deeplearning-7_unsuper-52-generative-modeling">5.2 Generative Modeling<a class="headerlink" href="#deeplearning-7_unsuper-52-generative-modeling" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-521-motivation">5.2.1 Motivation<a class="headerlink" href="#deeplearning-7_unsuper-521-motivation" title="Permanent link">¶</a></h3>
<ul>
<li>Goal: learn the underlying data distribution <span class="arithmatex">\(p(x)\)</span> to reveal hidden structure and causal factors.  </li>
<li>Unsupervised generative modeling captures common regularities in data — enabling representation learning, synthesis, and reasoning.  </li>
<li>Instead of directly memorizing examples, the model learns a probabilistic process that could have <em>generated</em> them.</li>
</ul>
<blockquote>
<p>Generative models explain the data by learning <em>how it might have arisen.</em></p>
</blockquote>
<h3 id="deeplearning-7_unsuper-522-from-maximum-likelihood-to-latent-variable-models">5.2.2 From Maximum Likelihood to Latent Variable Models<a class="headerlink" href="#deeplearning-7_unsuper-522-from-maximum-likelihood-to-latent-variable-models" title="Permanent link">¶</a></h3>
<h4 id="deeplearning-7_unsuper-maximum-likelihood-principle">Maximum Likelihood Principle<a class="headerlink" href="#deeplearning-7_unsuper-maximum-likelihood-principle" title="Permanent link">¶</a></h4>
<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:
<script type="math/tex; mode=display">
\mathbb{E}_{p^*(x)}[\log p_\theta(x)]
</script>
where <span class="arithmatex">\(p^*(x)\)</span> is the true data distribution and <span class="arithmatex">\(p_\theta(x)\)</span> is the model.</p>
<h4 id="deeplearning-7_unsuper-latent-variable-formulation">Latent Variable Formulation<a class="headerlink" href="#deeplearning-7_unsuper-latent-variable-formulation" title="Permanent link">¶</a></h4>
<ul>
<li>Assume data arises from hidden (latent) variables <span class="arithmatex">\(z\)</span>:
  <script type="math/tex; mode=display">
  \log p_\theta(x) = \log \int p_\theta(x|z)\,p(z)\,dz
  </script>
</li>
<li>Here:</li>
<li><span class="arithmatex">\(p(z)\)</span> — prior over latent variables (e.g., <span class="arithmatex">\(\mathcal{N}(0, I)\)</span>)  </li>
<li><span class="arithmatex">\(p_\theta(x|z)\)</span> — likelihood or <em>decoder</em> mapping latent codes to data</li>
</ul>
<p>This defines a latent variable model:<br>
the data-generating process maps from a <em>low-dimensional latent space</em> to the observed space.</p>
<h3 id="deeplearning-7_unsuper-523-inference-in-latent-variable-models">5.2.3 Inference in Latent Variable Models<a class="headerlink" href="#deeplearning-7_unsuper-523-inference-in-latent-variable-models" title="Permanent link">¶</a></h3>
<p>Goal: infer the posterior
<script type="math/tex; mode=display">
p(z|x) = \frac{p_\theta(x|z)p(z)}{p_\theta(x)}
</script>
to identify which latent factors <span class="arithmatex">\(z\)</span> most likely generated observation <span class="arithmatex">\(x\)</span>.</p>
<ul>
<li>Intuition:<br>
  Recover the underlying causes that explain the data — along with uncertainty estimates.</li>
<li>Problem:<br>
  Computing <span class="arithmatex">\(p(z|x)\)</span> is often intractable, since <span class="arithmatex">\(p_\theta(x)\)</span> involves integrating over all <span class="arithmatex">\(z\)</span>.<br>
  → We must approximate inference using neural networks.</li>
</ul>
<p>Thus, generative models combine:</p>
<ul>
<li>Generation: <span class="arithmatex">\(z \rightarrow x\)</span> (decode latent causes into data)</li>
<li>Inference: <span class="arithmatex">\(x \rightarrow z\)</span> (encode data into latent causes)</li>
</ul>
<h3 id="deeplearning-7_unsuper-524-variational-autoencoders-vaes">5.2.4 Variational Autoencoders (VAEs)<a class="headerlink" href="#deeplearning-7_unsuper-524-variational-autoencoders-vaes" title="Permanent link">¶</a></h3>
<p>To make inference tractable, VAEs introduce an approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span> and optimize a variational bound on the likelihood:</p>
<h4 id="deeplearning-7_unsuper-evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)<a class="headerlink" href="#deeplearning-7_unsuper-evidence-lower-bound-elbo" title="Permanent link">¶</a></h4>
<div class="arithmatex">\[
\log p_\theta(x)
\ge 
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
- D_{KL}[q_\phi(z|x)\,||\,p(z)]
\]</div>
<h4 id="deeplearning-7_unsuper-terms">Terms<a class="headerlink" href="#deeplearning-7_unsuper-terms" title="Permanent link">¶</a></h4>
<ol>
<li>
<p>Reconstruction term<br>
   Encourages the model to faithfully reproduce the input from its latent code.</p>
</li>
<li>
<p>KL divergence term<br>
   Regularizes the latent posterior to match the prior — ensuring smoothness and preventing overfitting.</p>
</li>
</ol>
<h4 id="deeplearning-7_unsuper-neural-implementation">Neural Implementation<a class="headerlink" href="#deeplearning-7_unsuper-neural-implementation" title="Permanent link">¶</a></h4>
<ul>
<li>Encoder <span class="arithmatex">\(q_\phi(z|x)\)</span>: approximates inference (maps data → latent code).  </li>
<li>Decoder <span class="arithmatex">\(p_\theta(x|z)\)</span>: generates data from the latent space (latent → data).  </li>
<li>Both are parameterized by deep neural networks.</li>
</ul>
<p>Reparameterization trick (Kingma &amp; Welling, 2014):<br>
<script type="math/tex; mode=display">
z = \mu_\phi(x) + \sigma_\phi(x)\odot\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
</script>
enables backpropagation through stochastic latent sampling.</p>
<h4 id="deeplearning-7_unsuper-why-vaes-matter">Why VAEs Matter<a class="headerlink" href="#deeplearning-7_unsuper-why-vaes-matter" title="Permanent link">¶</a></h4>
<ul>
<li>Provide continuous, structured latent spaces capturing generative factors.  </li>
<li>Support smooth interpolation and semantic manipulation.  </li>
<li>Foundation for disentangled and interpretable representation learning (e.g., β-VAE).  </li>
<li>Bridge probabilistic modeling with deep learning.</li>
</ul>
<blockquote>
<p>VAEs turn probabilistic inference into a scalable neural optimization problem — the cornerstone of modern generative representation learning.</p>
</blockquote>
<h3 id="deeplearning-7_unsuper-524-vae">5.2.4 β-VAE<a class="headerlink" href="#deeplearning-7_unsuper-524-vae" title="Permanent link">¶</a></h3>
<ul>
<li>Adds weight β to KL term:
  <script type="math/tex; mode=display">
  \mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta D_{KL}[q(z|x)||p(z)]
  </script>
</li>
<li>Encourages disentangled latent factors (position, shape, rotation, color).</li>
<li>
<p>Provides interpretable, semantically meaningful representations.</p>
</li>
<li>
<p>DARLA (Higgins et al., 2017): β-VAE for reinforcement learning → improved transfer and sim2real generalization.</p>
</li>
</ul>
<h3 id="deeplearning-7_unsuper-525-sequential-and-layered-models">5.2.5 Sequential and Layered Models<a class="headerlink" href="#deeplearning-7_unsuper-525-sequential-and-layered-models" title="Permanent link">¶</a></h3>
<p>ConvDRAW (Gregor et al., 2016)<br>
- Sequential VAE with recurrent refinement.<br>
- Models temporal and spatial dependencies.</p>
<p>MONet (Burgess et al., 2019)<br>
- Attention-based scene decomposition.<br>
- Each latent corresponds to one object → compositional representations.<br>
- Enables object-centric reasoning and RL transfer.</p>
<p>GQN (Eslami et al., 2018)<br>
- <em>Generative Query Networks</em>: learn neural scene representations.<br>
- Given partial observations, predict unseen viewpoints (3D reasoning).</p>
<p>VQ-VAE (van den Oord et al., 2017)<br>
- Learns discrete latent variables via vector quantization.<br>
- Enables hierarchical or symbolic structure.<br>
- Useful for speech, images, and video.
-</p>
<h3 id="deeplearning-7_unsuper-526-gans-goodfellow-et-al-2014">5.2.6 GANs (Goodfellow et al., 2014)<a class="headerlink" href="#deeplearning-7_unsuper-526-gans-goodfellow-et-al-2014" title="Permanent link">¶</a></h3>
<ul>
<li>Implicit generative models — learn by adversarial game:</li>
<li>Generator creates samples.</li>
<li>Discriminator provides learning signal (no reconstruction loss).</li>
<li>BigBiGAN (Donahue et al., 2019):</li>
<li>Adds encoder for inference.</li>
<li>Learns rich, high-level representations → SOTA semi-supervised performance on ImageNet.</li>
</ul>
<h3 id="deeplearning-7_unsuper-527-large-scale-generative-models">5.2.7 Large-Scale Generative Models<a class="headerlink" href="#deeplearning-7_unsuper-527-large-scale-generative-models" title="Permanent link">¶</a></h3>
<ul>
<li>GPT (Radford et al., 2019):  </li>
<li>Large transformer trained via language modeling.</li>
<li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li>
</ul>
<h2 id="deeplearning-7_unsuper-53-contrastive-learning">5.3 Contrastive Learning<a class="headerlink" href="#deeplearning-7_unsuper-53-contrastive-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-core-idea">Core Idea<a class="headerlink" href="#deeplearning-7_unsuper-core-idea" title="Permanent link">¶</a></h3>
<ul>
<li>No need to model <span class="arithmatex">\(p(x)\)</span> explicitly.</li>
<li>Learn representations that maximize mutual information between related samples.</li>
</ul>
<h3 id="deeplearning-7_unsuper-531-word2vec-mikolov-et-al-2013">5.3.1 word2vec (Mikolov et al., 2013)<a class="headerlink" href="#deeplearning-7_unsuper-531-word2vec-mikolov-et-al-2013" title="Permanent link">¶</a></h3>
<ul>
<li>Predict context words given a target word.  </li>
<li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li>
<li>Learns semantic embeddings; supports few-shot translation.</li>
</ul>
<h3 id="deeplearning-7_unsuper-532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018">5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)<a class="headerlink" href="#deeplearning-7_unsuper-532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018" title="Permanent link">¶</a></h3>
<ul>
<li>Maximize mutual information between current representation and future observations.  </li>
<li>Trains a classifier to distinguish real future samples from negatives.  </li>
<li>
<p>Learns features useful across modalities (vision, speech).</p>
</li>
<li>
<p>Data-efficient Image Recognition (Hénaff et al., 2019):<br>
  contrastive features outperform pixel-level training in low-data regimes.</p>
</li>
</ul>
<h3 id="deeplearning-7_unsuper-533-simclr-chen-et-al-2020">5.3.3 SimCLR (Chen et al., 2020)<a class="headerlink" href="#deeplearning-7_unsuper-533-simclr-chen-et-al-2020" title="Permanent link">¶</a></h3>
<ul>
<li>Simple, scalable contrastive framework:</li>
<li>Generate two augmented views of the same image.</li>
<li>Maximize agreement via contrastive loss (NT-Xent).</li>
<li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li>
<li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li>
</ul>
<h2 id="deeplearning-7_unsuper-54-self-supervised-learning">5.4 Self-Supervised Learning<a class="headerlink" href="#deeplearning-7_unsuper-54-self-supervised-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-7_unsuper-idea">Idea<a class="headerlink" href="#deeplearning-7_unsuper-idea" title="Permanent link">¶</a></h3>
<ul>
<li>Design <em>pretext tasks</em> that use natural structure in data as supervision.  </li>
<li>Representations are deterministic and transferable to new tasks.</li>
</ul>
<h3 id="deeplearning-7_unsuper-541-examples">5.4.1 Examples<a class="headerlink" href="#deeplearning-7_unsuper-541-examples" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Description</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Colorization</td>
<td>Predict color from grayscale image</td>
<td>Zhang et al., 2016</td>
</tr>
<tr>
<td>Context Prediction</td>
<td>Predict position of image patches</td>
<td>Doersch et al., 2015</td>
</tr>
<tr>
<td>Sequence Sorting</td>
<td>Predict correct frame order in videos</td>
<td>Lee et al., 2017</td>
</tr>
<tr>
<td>BERT (Devlin et al., 2019)</td>
<td>Masked language modeling + next sentence prediction</td>
<td>Revolutionized NLP</td>
</tr>
</tbody>
</table>
<h3 id="deeplearning-7_unsuper-542-key-benefits">5.4.2 Key Benefits<a class="headerlink" href="#deeplearning-7_unsuper-542-key-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>Requires no labels — just structure in data.  </li>
<li>Produces general features useful for:</li>
<li>Semi-supervised classification  </li>
<li>Transfer learning  </li>
<li>Downstream reasoning tasks</li>
</ul>
<h3 id="deeplearning-7_unsuper-55-design-principles">5.5 Design Principles<a class="headerlink" href="#deeplearning-7_unsuper-55-design-principles" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Consideration</th>
<th>Desired Property</th>
</tr>
</thead>
<tbody>
<tr>
<td>Modality</td>
<td>Align architecture with data type (image, text, audio)</td>
</tr>
<tr>
<td>Task Design</td>
<td>Choose pretext that aligns with useful features</td>
</tr>
<tr>
<td>Consistency</td>
<td>Maintain temporal/spatial coherence</td>
</tr>
<tr>
<td>Discrete + Continuous Latents</td>
<td>Enable symbolic and continuous reasoning</td>
</tr>
<tr>
<td>Adaptivity</td>
<td>Representations should evolve with experience</td>
</tr>
</tbody>
</table>
<p>Summary:<br>
Unsupervised representation learning uses <em>three complementary lenses</em>:
- Generative → model what the world looks like.<br>
- Contrastive → learn what is similar or different.<br>
- Self-supervised → create pseudo-tasks that reveal structure.<br>
Together, they aim for data-efficient, transferable, and interpretable representations.</p></body></html></section><section class="print-page" id="deeplearning-8_latentvariables" heading-number="4.8"><h1 id="deeplearning-8_latentvariables-deeplearning-8_latentvariables">8. Latent Variable Models</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="1-generative-modelling">1. Generative Modelling<a class="headerlink" href="#deeplearning-8_latentvariables-1-generative-modelling" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-11-what-are-generative-models">1.1 What Are Generative Models?<a class="headerlink" href="#deeplearning-8_latentvariables-11-what-are-generative-models" title="Permanent link">¶</a></h3>
<ul>
<li>Probabilistic models of high-dimensional data.</li>
<li>Describe how observations are generated from underlying processes.</li>
<li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-12-why-they-matter">1.2 Why They Matter<a class="headerlink" href="#deeplearning-8_latentvariables-12-why-they-matter" title="Permanent link">¶</a></h3>
<p>Generative models can:
- Estimate data density (detect outliers, anomalies).<br>
- Enable compression (encode → decode).<br>
- Map between domains (e.g., translation, text-to-speech).<br>
- Support model-based RL (predict future states).<br>
- Learn representations from raw data.<br>
- Improve understanding of data structure.</p>
<h2 id="deeplearning-8_latentvariables-13-types-of-generative-models-in-deep-learning">1.3 Types of Generative Models in Deep Learning<a class="headerlink" href="#deeplearning-8_latentvariables-13-types-of-generative-models-in-deep-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-a-autoregressive-models">(a) Autoregressive Models<a class="headerlink" href="#deeplearning-8_latentvariables-a-autoregressive-models" title="Permanent link">¶</a></h3>
<p>Model joint distribution via chain rule:
<script type="math/tex; mode=display">
p(x) = \prod_{i=1}^D p(x_i \mid x_{<i})
</script>
</p>
<p>Trained with maximum likelihood</p>
<p>Examples:</p>
<ul>
<li>RNN/Transformer LMs  </li>
<li>NADE  </li>
<li>PixelCNN / WaveNet</li>
</ul>
<p>Pros:</p>
<ul>
<li>Easy training (max. likelihood).</li>
<li>No sampling during training.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Slow generation (sequential).</li>
<li>Often capture local structure better than global structure.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-b-latent-variable-models">(b) Latent Variable Models<a class="headerlink" href="#deeplearning-8_latentvariables-b-latent-variable-models" title="Permanent link">¶</a></h3>
<p>Introduce an unobserved latent variable <span class="arithmatex">\(z\)</span>:</p>
<ul>
<li>Prior: <span class="arithmatex">\(p(z)\)</span>  </li>
<li>Likelihood: <span class="arithmatex">\(p_\theta(x\mid z)\)</span>  </li>
</ul>
<p>Joint:
<script type="math/tex; mode=display">
p_\theta(x, z) = p_\theta(x\mid z)\,p(z)
</script>
</p>
<p>Pros</p>
<ul>
<li>Flexible &amp; interpretable  </li>
<li>Natural for representation learning  </li>
<li>Fast generation  </li>
</ul>
<p>Cons
- Require approximate inference unless specially designed (e.g., invertible models).</p>
<h3 id="deeplearning-8_latentvariables-c-implicit-models-gans">(c) Implicit Models (GANs)<a class="headerlink" href="#deeplearning-8_latentvariables-c-implicit-models-gans" title="Permanent link">¶</a></h3>
<ul>
<li>Define a generator <span class="arithmatex">\(G(z)\)</span> with no explicit likelihood.</li>
<li>Trained adversarially using a discriminator.</li>
</ul>
<p>Pros</p>
<ul>
<li>Extremely realistic samples  </li>
<li>Fast sampling  </li>
</ul>
<p>Cons</p>
<ul>
<li>Cannot evaluate <span class="arithmatex">\(p(x)\)</span>  </li>
<li>Mode collapse  </li>
<li>Training instability  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-2-latent-variable-models-inference">2. Latent Variable Models &amp; Inference<a class="headerlink" href="#deeplearning-8_latentvariables-2-latent-variable-models-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-21-what-is-a-latent-variable-model-lvm">2.1 What is a Latent Variable Model (LVM)?<a class="headerlink" href="#deeplearning-8_latentvariables-21-what-is-a-latent-variable-model-lvm" title="Permanent link">¶</a></h3>
<p>A latent variable model introduces an unobserved variable <span class="arithmatex">\(z\)</span> that explains the observed data <span class="arithmatex">\(x\)</span>.</p>
<p>Model components:</p>
<ul>
<li>Prior over latent variables:  </li>
</ul>
<p>
<script type="math/tex; mode=display">
  p(z)
  </script>
</p>
<ul>
<li>Likelihood / decoder mapping latent → observation:  </li>
</ul>
<p>
<script type="math/tex; mode=display">
  p_\theta(x \mid z)
  </script>
</p>
<p>Joint distribution:</p>
<div class="arithmatex">\[
p_\theta(x, z) = p_\theta(x \mid z)\,p(z)
\]</div>
<p>Marginal likelihood (what we want to maximize when training):</p>
<div class="arithmatex">\[
p_\theta(x) = \int p_\theta(x \mid z)\,p(z)\,dz
\]</div>
<h3 id="deeplearning-8_latentvariables-22-intuition-latents-as-explanations">2.2 Intuition: Latents as “Explanations”<a class="headerlink" href="#deeplearning-8_latentvariables-22-intuition-latents-as-explanations" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>A particular value of <span class="arithmatex">\(z\)</span> is a hypothesis about hidden causes that produced <span class="arithmatex">\(x\)</span>.</p>
</li>
<li>
<p>Generation = sample latent → map it to data:
  <script type="math/tex; mode=display">
  z \sim p(z), \quad x \sim p_\theta(x\mid z)
  </script>
</p>
</li>
</ul>
<p>Most of the article focuses on the inverse of this:<br>
recovering <span class="arithmatex">\(z\)</span> from <span class="arithmatex">\(x\)</span>.</p>
<h2 id="deeplearning-8_latentvariables-23-what-is-inference">2.3 What Is Inference?<a class="headerlink" href="#deeplearning-8_latentvariables-23-what-is-inference" title="Permanent link">¶</a></h2>
<p>Inference means computing the posterior:
<script type="math/tex; mode=display">
p_\theta(z \mid x) = \frac{p_\theta(x\mid z)\,p(z)}{p_\theta(x)}
</script>
</p>
<p>Why it matters:</p>
<ul>
<li>Explains the observation (which latents likely produced it?)</li>
<li>Needed inside maximum-likelihood training<br>
  (the gradient depends on the posterior!)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-24-inference-requires-the-marginal-likelihood">2.4 Inference Requires the Marginal Likelihood<a class="headerlink" href="#deeplearning-8_latentvariables-24-inference-requires-the-marginal-likelihood" title="Permanent link">¶</a></h2>
<p>To compute the posterior, we need:
<script type="math/tex; mode=display">
p_\theta(x) = \int p_\theta(x \mid z)p(z)\,dz
</script>
This integral is often intractable.</p>
<p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>
<h2 id="deeplearning-8_latentvariables-25-example-mixture-of-gaussians">2.5 Example: Mixture of Gaussians<a class="headerlink" href="#deeplearning-8_latentvariables-25-example-mixture-of-gaussians" title="Permanent link">¶</a></h2>
<p>Model:</p>
<ul>
<li>Choose cluster <span class="arithmatex">\(k\)</span>  </li>
<li>Sample <span class="arithmatex">\(x\)</span> from Gaussian for that cluster</li>
</ul>
<p>Posterior:
<script type="math/tex; mode=display">
p(k \mid x)
= 
\frac{\pi_k\,\mathcal{N}(x \mid \mu_k, \Sigma_k)}
{\sum_j \pi_j \mathcal{N}(x \mid \mu_j, \Sigma_j)}
</script>
</p>
<p>This model is tractable because:</p>
<ul>
<li>Finite number of discrete states  </li>
<li>Closed-form posterior</li>
</ul>
<h2 id="deeplearning-8_latentvariables-26-the-need-for-inference-in-learning">2.6 The Need for Inference in Learning<a class="headerlink" href="#deeplearning-8_latentvariables-26-the-need-for-inference-in-learning" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-maximum-likelihood-as-the-core-training-principle">Maximum Likelihood as the Core Training Principle<a class="headerlink" href="#deeplearning-8_latentvariables-maximum-likelihood-as-the-core-training-principle" title="Permanent link">¶</a></h3>
<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters <span class="arithmatex">\(\theta\)</span> that make the observed training data as probable as possible:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta \sum_{i} \log p_\theta(x^{(i)})
\]</div>
<p>For latent variable models, the marginal likelihood is:
<script type="math/tex; mode=display">
\log p_\theta(x) = \log \int p_\theta(x, z)\,dz
</script>
This integral is rarely tractable, which makes direct maximization difficult.</p>
<h3 id="deeplearning-8_latentvariables-why-optimization-is-hard-in-latent-variable-models">Why Optimization Is Hard in Latent Variable Models<a class="headerlink" href="#deeplearning-8_latentvariables-why-optimization-is-hard-in-latent-variable-models" title="Permanent link">¶</a></h3>
<ul>
<li>The log-likelihood involves an integral (or sum) over the latent variables <span class="arithmatex">\(z\)</span>.  </li>
<li>Because this integral usually has no closed form, we must use iterative optimization methods.</li>
</ul>
<p>Common approaches:</p>
<ol>
<li>Gradient-based optimization (e.g., gradient descent)</li>
<li>Expectation-Maximization (EM)</li>
</ol>
<p>Below we explain why inference (computing the posterior <span class="arithmatex">\(p_\theta(z \mid x)\)</span>) is essential for both.</p>
<h2 id="deeplearning-8_latentvariables-261-gradient-based-learning-requires-the-posterior">2.6.1 Gradient-Based Learning Requires the Posterior<a class="headerlink" href="#deeplearning-8_latentvariables-261-gradient-based-learning-requires-the-posterior" title="Permanent link">¶</a></h2>
<p>Using the identity:</p>
<div class="arithmatex">\[
\nabla_\theta \log p_\theta(x)
=
\mathbb{E}_{p_\theta(z\mid x)}[\nabla_\theta \log p_\theta(x, z)]
\]</div>
<blockquote>
<p>Differentiate the log-marginal</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{\nabla_\theta p_\theta(x)}{p_\theta(x)}\]</div>
<p>Using: <span class="arithmatex">\(p_\theta(x)=\int p_\theta(x,z)\,dz,\)</span></p>
<p>differentiate under the integral:</p>
<div class="arithmatex">\[\nabla_\theta p_\theta(x)
= \nabla_\theta \int p_\theta(x,z)\,dz
= \int \nabla_\theta p_\theta(x,z)\,dz\]</div>
<p>Combine:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{1}{p_\theta(x)} \int \nabla_\theta p_\theta(x,z)\,dz\]</div>
<p>Apply the log-derivative identity</p>
<p>The identity:</p>
<div class="arithmatex">\[\nabla_\theta p_\theta(x,z)
= p_\theta(x,z)\,\nabla_\theta \log p_\theta(x,z)\]</div>
<p>Substitute:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\frac{1}{p_\theta(x)}
\int p_\theta(x,z)\,\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>Recognize the posterior</p>
<p>Bayes’ rule:</p>
<div class="arithmatex">\[p_\theta(z\mid x)
= \frac{p_\theta(x,z)}{p_\theta(x)}\]</div>
<p>Substitute into the integral:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\int
\frac{p_\theta(x,z)}{p_\theta(x)}
\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>This becomes:</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\int p_\theta(z\mid x)\,\nabla_\theta \log p_\theta(x,z)\,dz\]</div>
<p>Write as an expectation</p>
<div class="arithmatex">\[\nabla_\theta \log p_\theta(x)
=
\mathbb{E}_{p_\theta(z\mid x)}
\left[
\nabla_\theta \log p_\theta(x,z)
\right]\]</div>
<p>To compute the gradient of the marginal likelihood, we must take an expectation under the <em>posterior</em>  <span class="arithmatex">\(p_\theta(z\mid x)\)</span>.</p>
</blockquote>
<p>So:</p>
<ul>
<li>We cannot compute <span class="arithmatex">\(\nabla_\theta \log p_\theta(x)\)</span> without knowing the posterior.</li>
<li>Inference becomes part of every gradient step.</li>
<li>If inference is intractable → gradient is intractable.</li>
</ul>
<p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>
<h2 id="deeplearning-8_latentvariables-262-expectation-maximization-em-also-requires-inference">2.6.2 Expectation-Maximization (EM) Also Requires Inference<a class="headerlink" href="#deeplearning-8_latentvariables-262-expectation-maximization-em-also-requires-inference" title="Permanent link">¶</a></h2>
<p>EM is an alternative to gradient descent for maximizing likelihood.</p>
<h3 id="deeplearning-8_latentvariables-e-step">E-step:<a class="headerlink" href="#deeplearning-8_latentvariables-e-step" title="Permanent link">¶</a></h3>
<p>Compute (or approximate) the posterior:
<script type="math/tex; mode=display">
q(z) \approx p_\theta(z \mid x)
</script>
This assigns responsibilities to each latent configuration.</p>
<h3 id="deeplearning-8_latentvariables-m-step">M-step:<a class="headerlink" href="#deeplearning-8_latentvariables-m-step" title="Permanent link">¶</a></h3>
<p>Update parameters by maximizing the expected complete-data log-likelihood:
<script type="math/tex; mode=display">
\theta^{(t+1)} = \arg\max_\theta \mathbb{E}_{q(z)}[\log p_\theta(x, z)]
</script>
</p>
<p>Thus, the E-step directly requires inference.</p>
<h2 id="deeplearning-8_latentvariables-27-why-exact-inference-is-hard">2.7 Why Exact Inference Is Hard<a class="headerlink" href="#deeplearning-8_latentvariables-27-why-exact-inference-is-hard" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-continuous-latents">Continuous latents:<a class="headerlink" href="#deeplearning-8_latentvariables-continuous-latents" title="Permanent link">¶</a></h3>
<ul>
<li>Require multidimensional integration over nonlinear likelihoods.</li>
</ul>
<h3 id="deeplearning-8_latentvariables-discrete-latents">Discrete latents:<a class="headerlink" href="#deeplearning-8_latentvariables-discrete-latents" title="Permanent link">¶</a></h3>
<ul>
<li>Require summing over exponentially many configurations.</li>
</ul>
<p>Only a few cases allow closed-form inference:</p>
<ul>
<li>Mixture models  </li>
<li>Linear Gaussian systems  </li>
<li>Invertible / flow-based models (covered next)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-28-two-strategies-to-handle-intractability">2.8 Two Strategies to Handle Intractability<a class="headerlink" href="#deeplearning-8_latentvariables-28-two-strategies-to-handle-intractability" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-1-design-tractable-models">1. Design tractable models<a class="headerlink" href="#deeplearning-8_latentvariables-1-design-tractable-models" title="Permanent link">¶</a></h3>
<ul>
<li>Invertible models (normalizing flows)</li>
<li>Autoregressive latent structures<br>
Pros: exact inference<br>
Cons: restricted model class</li>
</ul>
<h3 id="deeplearning-8_latentvariables-2-approximate-inference">2. Approximate inference<a class="headerlink" href="#deeplearning-8_latentvariables-2-approximate-inference" title="Permanent link">¶</a></h3>
<ul>
<li>Use approximations to posterior <span class="arithmatex">\(p(z \mid x)\)</span>  </li>
<li>Variational Inference or MCMC<br>
Pros: flexible, expressive models<br>
Cons: introduces approximation error</li>
</ul>
<h2 id="deeplearning-8_latentvariables-3-invertible-models-exact-inference">3. Invertible Models &amp; Exact Inference<a class="headerlink" href="#deeplearning-8_latentvariables-3-invertible-models-exact-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-31-what-are-invertible-models">3.1 What Are Invertible Models?<a class="headerlink" href="#deeplearning-8_latentvariables-31-what-are-invertible-models" title="Permanent link">¶</a></h3>
<p>Invertible models (also called normalizing flows) are latent variable models where:</p>
<ul>
<li>The latent variable <span class="arithmatex">\(z\)</span> and data <span class="arithmatex">\(x\)</span> have the same dimensionality</li>
<li>There exists an invertible, differentiable mapping<br>
<script type="math/tex; mode=display">
  x = f_\theta(z)
  </script>
</li>
<li>Because <span class="arithmatex">\(f_\theta\)</span> is invertible:
  <script type="math/tex; mode=display">
  z = f_\theta^{-1}(x)
  </script>
</li>
</ul>
<p>Key property:<br>
Inference is exact and trivial — simply apply the inverse function.</p>
<h2 id="deeplearning-8_latentvariables-32-generative-process">3.2 Generative Process<a class="headerlink" href="#deeplearning-8_latentvariables-32-generative-process" title="Permanent link">¶</a></h2>
<p>To generate a sample:</p>
<ol>
<li>Sample <span class="arithmatex">\(z \sim p(z)\)</span> (usually a simple prior like <span class="arithmatex">\(\mathcal{N}(0, I)\)</span>)</li>
<li>Transform via<br>
<script type="math/tex; mode=display">
   x = f_\theta(z)
   </script>
</li>
</ol>
<p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>
<h2 id="deeplearning-8_latentvariables-33-why-are-invertible-models-attractive">3.3 Why Are Invertible Models Attractive?<a class="headerlink" href="#deeplearning-8_latentvariables-33-why-are-invertible-models-attractive" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Exact inference:<br>
<script type="math/tex; mode=display">
  p_\theta(z \mid x)
  </script>
<br>
  is computed by a single function evaluation (no approximation needed).</p>
</li>
<li>
<p>Exact likelihood:<br>
  Can compute <span class="arithmatex">\(\log p_\theta(x)\)</span> exactly using the change-of-variables formula.</p>
</li>
</ul>
<h2 id="deeplearning-8_latentvariables-34-change-of-variables-for-likelihood">3.4 Change of Variables for Likelihood<a class="headerlink" href="#deeplearning-8_latentvariables-34-change-of-variables-for-likelihood" title="Permanent link">¶</a></h2>
<p>Given an invertible mapping <span class="arithmatex">\(x = f_\theta(z)\)</span>:</p>
<div class="arithmatex">\[
p_\theta(x) = p(z) \left| \det \left( \frac{\partial f_\theta^{-1}(x)}{\partial x} \right) \right|
\]</div>
<p>Equivalently, using <span class="arithmatex">\(z = f_\theta^{-1}(x)\)</span>:</p>
<div class="arithmatex">\[
\log p_\theta(x)
=
\log p(z)
+
\log \left| \det J_{f_\theta^{-1}}(x) \right|
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(J_{f_\theta^{-1}}\)</span> is the Jacobian matrix of the inverse map  </li>
<li>The determinant accounts for volume change introduced by transformation</li>
</ul>
<h2 id="deeplearning-8_latentvariables-35-example-independent-component-analysis-ica">3.5 Example: Independent Component Analysis (ICA)<a class="headerlink" href="#deeplearning-8_latentvariables-35-example-independent-component-analysis-ica" title="Permanent link">¶</a></h2>
<p>ICA is the simplest invertible model:</p>
<ul>
<li>Latent prior:  factorial prior
  <script type="math/tex; mode=display">
  p(z) = \prod_i p(z_i)
  </script>
  with non-Gaussian heavy-tailed components</li>
<li>Linear invertible mixing:<br>
<script type="math/tex; mode=display">
  x = A z
  </script>
</li>
</ul>
<p>Inference:
<script type="math/tex; mode=display">
z = A^{-1} x
</script>
</p>
<p>ICA recovers independent sources that explain the observed signal.</p>
<h2 id="deeplearning-8_latentvariables-36-building-complex-invertible-models">3.6 Building Complex Invertible Models<a class="headerlink" href="#deeplearning-8_latentvariables-36-building-complex-invertible-models" title="Permanent link">¶</a></h2>
<p>Modern flows build <span class="arithmatex">\(f_\theta\)</span> by composing many simple invertible layers:</p>
<div class="arithmatex">\[
f_\theta = f_K \circ f_{K-1} \circ \dots \circ f_1
\]</div>
<p>Composition of invertible functions is invertible.</p>
<p>Building blocks:</p>
<ul>
<li>Linear transforms</li>
<li>Autoregressive flows (IAF, MAF)</li>
<li>Coupling layers (RealNVP, Glow)</li>
<li>Residual flows</li>
<li>Sylvester flows</li>
</ul>
<p>Design goal:</p>
<blockquote>
<p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>
</blockquote>
<h2 id="deeplearning-8_latentvariables-37-advantages-limitations">3.7 Advantages &amp; Limitations<a class="headerlink" href="#deeplearning-8_latentvariables-37-advantages-limitations" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-advantages">Advantages<a class="headerlink" href="#deeplearning-8_latentvariables-advantages" title="Permanent link">¶</a></h3>
<ul>
<li>Exact inference  </li>
<li>Exact log-likelihood  </li>
<li>Fast, parallel sampling  </li>
<li>Useful as components in larger probabilistic models</li>
</ul>
<h3 id="deeplearning-8_latentvariables-limitations">Limitations<a class="headerlink" href="#deeplearning-8_latentvariables-limitations" title="Permanent link">¶</a></h3>
<ul>
<li>Latent and data dimensions must match  </li>
<li>Latents must be continuous  </li>
<li>Observations must be continuous or quantized  </li>
<li>Very deep flows require large memory  </li>
<li>Hard to encode strong structure or sparsity  </li>
</ul>
<blockquote>
<p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>
</blockquote>
<h2 id="deeplearning-8_latentvariables-mar">Mar<a class="headerlink" href="#deeplearning-8_latentvariables-mar" title="Permanent link">¶</a></h2>
<h2 id="deeplearning-8_latentvariables-4-variational-inference-vi">4. Variational Inference (VI)<a class="headerlink" href="#deeplearning-8_latentvariables-4-variational-inference-vi" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-41-why-variational-inference">4.1 Why Variational Inference?<a class="headerlink" href="#deeplearning-8_latentvariables-41-why-variational-inference" title="Permanent link">¶</a></h3>
<p>In many latent variable models, the true posterior
<script type="math/tex; mode=display">
p_\theta(z \mid x)
</script>
is intractable because computing
<script type="math/tex; mode=display">
p_\theta(x) = \int p_\theta(x, z)\,dz
</script>
is impossible in closed form.</p>
<p>We still need the posterior for:</p>
<ul>
<li>Inference (explaining the observation)</li>
<li>Learning (MLE gradient depends on it)</li>
<li>EM algorithm E-step</li>
</ul>
<h4 id="deeplearning-8_latentvariables-approximate-inference">Approximate Inference<a class="headerlink" href="#deeplearning-8_latentvariables-approximate-inference" title="Permanent link">¶</a></h4>
<p>There are two major classes of approaches to approximate inference:</p>
<h4 id="deeplearning-8_latentvariables-markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)<a class="headerlink" href="#deeplearning-8_latentvariables-markov-chain-monte-carlo-mcmc" title="Permanent link">¶</a></h4>
<p>Generate samples from the exact posterior using a Markov chain.</p>
<ul>
<li>Very general; exact in the limit of infinite time / computation  </li>
<li>Computationally expensive  </li>
<li>Convergence is hard to diagnose  </li>
</ul>
<h4 id="deeplearning-8_latentvariables-2-variational-inference-vi">2. Variational Inference (VI)<a class="headerlink" href="#deeplearning-8_latentvariables-2-variational-inference-vi" title="Permanent link">¶</a></h4>
<p>Approximate the posterior with a tractable distribution<br>
(e.g., fully factorized, mixture, or autoregressive).</p>
<ul>
<li>Fairly efficient — inference reduces to optimization of distribution parameters  </li>
<li>Fast at test time (single forward pass of the inference network)  </li>
<li>Cannot easily trade computation for accuracy (unlike MCMC)  </li>
</ul>
<p>MCMC = flexible, asymptotically exact, but slow.<br>
VI = fast and scalable, but biased due to restricted approximating family.</p>
<h2 id="deeplearning-8_latentvariables-42-core-idea-of-variational-inference">4.2 Core Idea of Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-42-core-idea-of-variational-inference" title="Permanent link">¶</a></h2>
<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing.
Approximate the posterior with a simpler distribution:</p>
<div class="arithmatex">\[
q_\phi(z \mid x) \approx p_\theta(z \mid x)
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(q_\phi\)</span> is the variational posterior</li>
<li><span class="arithmatex">\(\phi\)</span> are variational parameters (learned)</li>
</ul>
<p>Requirements:</p>
<ol>
<li>We can sample from <span class="arithmatex">\(q_\phi(z \mid x)\)</span>  </li>
<li>We can compute <span class="arithmatex">\(\log q_\phi(z \mid x)\)</span> and its gradient wrt <span class="arithmatex">\(\phi\)</span>  </li>
</ol>
<p>Common choice: mean-field approximation</p>
<div class="arithmatex">\[
q_\phi(z \mid x) = \prod_i q_\phi(z_i \mid x)
\]</div>
<h2 id="deeplearning-8_latentvariables-43-training-with-variational-inference">4.3 Training with Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-43-training-with-variational-inference" title="Permanent link">¶</a></h2>
<p>Goal: maximize the marginal likelihood</p>
<div class="arithmatex">\[
\log p_\theta(x)
\]</div>
<p>Since it's intractable, VI uses a lower bound on this quantity.</p>
<h3 id="deeplearning-8_latentvariables-variational-lower-bound-elbo">Variational Lower Bound (ELBO)<a class="headerlink" href="#deeplearning-8_latentvariables-variational-lower-bound-elbo" title="Permanent link">¶</a></h3>
<p>Using Jensen’s inequality:</p>
<div class="arithmatex">\[
\log p_\theta(x)
\ge 
\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x, z)]
-
\mathbb{E}_{q_\phi(z \mid x)}[\log q_\phi(z \mid x)]
\]</div>
<p>This is the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\text{ELBO}(\theta, \phi)
=
\mathbb{E}_{q_\phi}\!\left[\log p_\theta(x, z)\right]
-
\mathbb{E}_{q_\phi}\!\left[\log q_\phi(z \mid x)\right]
\]</div>
<p>We maximize ELBO w.r.t both <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(\phi\)</span>.</p>
<h2 id="deeplearning-8_latentvariables-44-kl-interpretation-variational-gap">4.4 KL Interpretation (Variational Gap)<a class="headerlink" href="#deeplearning-8_latentvariables-44-kl-interpretation-variational-gap" title="Permanent link">¶</a></h2>
<p>Rewrite ELBO:</p>
<div class="arithmatex">\[
\log p_\theta(x)
=
\text{ELBO}(\theta, \phi)
+
D_{\text{KL}}(q_\phi(z \mid x) \,\|\, p_\theta(z \mid x))
\]</div>
<p>Thus:</p>
<ul>
<li>
<p>Maximizing ELBO wrt <span class="arithmatex">\(\phi\)</span><br>
  → minimizes the KL divergence between <span class="arithmatex">\(q_\phi\)</span> and the true posterior.</p>
</li>
<li>
<p>The variational gap is<br>
<script type="math/tex; mode=display">
  D_{\text{KL}}(q_\phi(z\mid x) || p_\theta(z\mid x))
  </script>
</p>
</li>
</ul>
<p>If <span class="arithmatex">\(q_\phi\)</span> is expressive enough:
<script type="math/tex; mode=display">
q_\phi(z\mid x) = p_\theta(z\mid x)
\quad \Rightarrow \quad
\text{gap} = 0
</script>
</p>
<h2 id="deeplearning-8_latentvariables-45-what-happens-when-updating-each-parameter-set">4.5 What Happens When Updating Each Parameter Set?<a class="headerlink" href="#deeplearning-8_latentvariables-45-what-happens-when-updating-each-parameter-set" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-updating-variational-parameters-phi">Updating variational parameters <span class="arithmatex">\(\phi\)</span>:<a class="headerlink" href="#deeplearning-8_latentvariables-updating-variational-parameters-phi" title="Permanent link">¶</a></h3>
<ul>
<li>Minimizes the variational gap  </li>
<li>Makes <span class="arithmatex">\(q_\phi(z \mid x)\)</span> closer to the true posterior  </li>
<li>Does not affect the model directly</li>
</ul>
<h3 id="deeplearning-8_latentvariables-updating-model-parameters-theta">Updating model parameters <span class="arithmatex">\(\theta\)</span>:<a class="headerlink" href="#deeplearning-8_latentvariables-updating-model-parameters-theta" title="Permanent link">¶</a></h3>
<ul>
<li>Increases <span class="arithmatex">\(\log p_\theta(x)\)</span> (good)</li>
<li>BUT often also reduces the gap by making the posterior simpler<br>
  → Risk: posterior collapse / variational pruning</li>
</ul>
<p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>
<h2 id="deeplearning-8_latentvariables-46-variational-pruning-posterior-collapse">4.6 Variational Pruning (Posterior Collapse)<a class="headerlink" href="#deeplearning-8_latentvariables-46-variational-pruning-posterior-collapse" title="Permanent link">¶</a></h2>
<p>Because VI pushes <span class="arithmatex">\(p_\theta(z \mid x)\)</span> towards <span class="arithmatex">\(q_\phi(z\mid x)\)</span>, the model may choose to ignore some latent dimensions:</p>
<div class="arithmatex">\[
p_\theta(z_i \mid x) = p(z_i)
\]</div>
<p>Meaning the latent variable carries no information about <span class="arithmatex">\(x\)</span>.</p>
<p>Pros:</p>
<ul>
<li>Automatically learns effective latent dimensionality</li>
</ul>
<p>Cons:</p>
<ul>
<li>Prevents fully utilizing the latent capacity  </li>
<li>Common issue in VAEs (particularly with strong decoders)</li>
</ul>
<h2 id="deeplearning-8_latentvariables-47-choosing-the-variational-posterior-family">4.7 Choosing the Variational Posterior Family<a class="headerlink" href="#deeplearning-8_latentvariables-47-choosing-the-variational-posterior-family" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-simple-mean-field-gaussian">Simple: Mean-field Gaussian<a class="headerlink" href="#deeplearning-8_latentvariables-simple-mean-field-gaussian" title="Permanent link">¶</a></h3>
<ul>
<li>Fast</li>
<li>Easy to optimize</li>
<li>But limited expressivity</li>
</ul>
<h3 id="deeplearning-8_latentvariables-more-expressive-options">More expressive options:<a class="headerlink" href="#deeplearning-8_latentvariables-more-expressive-options" title="Permanent link">¶</a></h3>
<ul>
<li>Mixture posteriors</li>
<li>Gaussians with full covariance</li>
<li>Autoregressive posteriors</li>
<li>Normalizing-flow posteriors</li>
</ul>
<p>Trade-off: accuracy vs speed.</p>
<h2 id="deeplearning-8_latentvariables-48-amortized-variational-inference">4.8 Amortized Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-48-amortized-variational-inference" title="Permanent link">¶</a></h2>
<p>Classic VI:</p>
<ul>
<li>Each datapoint <span class="arithmatex">\(x\)</span> has its own variational parameters  </li>
<li>Requires iterative optimization per datapoint  </li>
<li>Too slow for deep learning</li>
</ul>
<p>Amortized VI:</p>
<ul>
<li>Use an inference network (encoder)
  <script type="math/tex; mode=display">
  \phi(x) \mapsto \text{parameters of } q_\phi(z\mid x)
  </script>
</li>
<li>Fast inference  </li>
<li>Works with SGD  </li>
<li>Introduced in Helmholtz Machines  </li>
<li>Popularized by Variational Autoencoders</li>
</ul>
<h2 id="deeplearning-8_latentvariables-49-variational-vs-exact-inference">4.9 Variational vs Exact Inference<a class="headerlink" href="#deeplearning-8_latentvariables-49-variational-vs-exact-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-advantages-of-vi">Advantages of VI<a class="headerlink" href="#deeplearning-8_latentvariables-advantages-of-vi" title="Permanent link">¶</a></h3>
<ul>
<li>Scalable to modern deep models  </li>
<li>Fast inference  </li>
<li>Enables flexible model design  </li>
</ul>
<h3 id="deeplearning-8_latentvariables-disadvantages">Disadvantages<a class="headerlink" href="#deeplearning-8_latentvariables-disadvantages" title="Permanent link">¶</a></h3>
<ul>
<li>Approximation bias  </li>
<li>Posterior may be oversimplified  </li>
<li>Can limit expressiveness of the full model  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-410-summary-of-section-4">4.10 Summary of Section 4<a class="headerlink" href="#deeplearning-8_latentvariables-410-summary-of-section-4" title="Permanent link">¶</a></h2>
<ul>
<li>Variational inference approximates the true posterior with a tractable distribution.  </li>
<li>ELBO gives a trainable lower bound on the marginal likelihood.  </li>
<li>VI converts inference into optimization.  </li>
<li>Amortized VI enables neural inference (encoders).  </li>
<li>Variational pruning can arise naturally and must be managed.  </li>
</ul>
<h2 id="deeplearning-8_latentvariables-5-gradient-estimation-in-variational-inference">5. Gradient Estimation in Variational Inference<a class="headerlink" href="#deeplearning-8_latentvariables-5-gradient-estimation-in-variational-inference" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-51-why-do-we-need-gradient-estimators">5.1 Why Do We Need Gradient Estimators?<a class="headerlink" href="#deeplearning-8_latentvariables-51-why-do-we-need-gradient-estimators" title="Permanent link">¶</a></h3>
<p>To train a latent variable model with variational inference, we maximize the ELBO:</p>
<div class="arithmatex">\[
\text{ELBO}(\theta, \phi)
=
\mathbb{E}_{q_\phi(z\mid x)}\Big[ \log p_\theta(x, z) - \log q_\phi(z\mid x) \Big]
\]</div>
<p>We need gradients with respect to:</p>
<ol>
<li>Model parameters <span class="arithmatex">\(\theta\)</span></li>
<li>Variational parameters <span class="arithmatex">\(\phi\)</span></li>
</ol>
<p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-52-gradients-wrt-model-parameters-theta">5.2 Gradients w.r.t. Model Parameters (<span class="arithmatex">\(\theta\)</span>)<a class="headerlink" href="#deeplearning-8_latentvariables-52-gradients-wrt-model-parameters-theta" title="Permanent link">¶</a></h2>
<p>This part is easy.</p>
<p>Because <span class="arithmatex">\(q_\phi(z\mid x)\)</span> does not depend on <span class="arithmatex">\(\theta\)</span>:</p>
<div class="arithmatex">\[
\nabla_\theta \text{ELBO}
=
\mathbb{E}_{q_\phi(z\mid x)} \big[ \nabla_\theta \log p_\theta(x, z) \big]
\]</div>
<p>We estimate this using samples:</p>
<ol>
<li>Draw <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span>  </li>
<li>Compute <span class="arithmatex">\(\nabla_\theta \log p_\theta(x,z)\)</span>  </li>
<li>Average across samples</li>
</ol>
<p>No special techniques required.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-53-gradients-wrt-variational-parameters-phi">5.3 Gradients w.r.t. Variational Parameters (<span class="arithmatex">\(\phi\)</span>)<a class="headerlink" href="#deeplearning-8_latentvariables-53-gradients-wrt-variational-parameters-phi" title="Permanent link">¶</a></h2>
<p>This is more difficult.</p>
<p>We want:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z\mid x)}[f(z)]
\]</div>
<p>But <span class="arithmatex">\(q_\phi(z\mid x)\)</span> depends on <span class="arithmatex">\(\phi\)</span>.<br>
Two main strategies exist to handle this dependence:</p>
<hr>
<h1 id="deeplearning-8_latentvariables-54-two-families-of-gradient-estimators">5.4 Two Families of Gradient Estimators<a class="headerlink" href="#deeplearning-8_latentvariables-54-two-families-of-gradient-estimators" title="Permanent link">¶</a></h1>
<h2 id="deeplearning-8_latentvariables-1-likelihood-ratio-reinforce-estimator">🔷 1. Likelihood-Ratio / REINFORCE Estimator<a class="headerlink" href="#deeplearning-8_latentvariables-1-likelihood-ratio-reinforce-estimator" title="Permanent link">¶</a></h2>
<p>Uses the identity:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)]
=
\mathbb{E}_{q_\phi(z)}[f(z)\,\nabla_\phi \log q_\phi(z)]
\]</div>
<p>This allows gradients for:
- Discrete latent variables<br>
- Non-differentiable <span class="arithmatex">\(f(z)\)</span><br>
- Any distribution where we can compute <span class="arithmatex">\(\log q_\phi(z)\)</span></p>
<p>Pros
- Very general<br>
- Works for discrete and continuous latents  </p>
<p>Cons
- High variance<br>
- Requires variance reduction (baselines, control variates)</p>
<p>This is the same gradient estimator used in policy gradients in RL.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-2-reparameterization-pathwise-estimator">🔷 2. Reparameterization / Pathwise Estimator<a class="headerlink" href="#deeplearning-8_latentvariables-2-reparameterization-pathwise-estimator" title="Permanent link">¶</a></h2>
<p>Instead of sampling <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span> directly,
write it as a differentiable transformation of noise:</p>
<div class="arithmatex">\[
z = g_\phi(\epsilon, x), \quad \epsilon \sim p(\epsilon)
\]</div>
<p>Then:</p>
<div class="arithmatex">\[
\nabla_\phi \mathbb{E}_{q_\phi(z\mid x)}[f(z)]
=
\mathbb{E}_{\epsilon \sim p(\epsilon)}
\big[ \nabla_\phi f(g_\phi(\epsilon, x)) \big]
\]</div>
<p>This pushes the dependence on <span class="arithmatex">\(\phi\)</span> inside a differentiable function.</p>
<h3 id="deeplearning-8_latentvariables-example-gaussian-posterior">Example: Gaussian posterior<a class="headerlink" href="#deeplearning-8_latentvariables-example-gaussian-posterior" title="Permanent link">¶</a></h3>
<p>If
<script type="math/tex; mode=display">
q_\phi(z\mid x) = \mathcal{N}(z\mid \mu_\phi(x), \sigma_\phi(x)^2),
</script>
then:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon, \quad \epsilon\sim \mathcal{N}(0,1)
\]</div>
<p>Pros
- Low variance<br>
- Enables stable VAE training  </p>
<p>Cons
- Only works for continuous latent variables<br>
- Requires differentiable sampling procedure</p>
<hr>
<h2 id="deeplearning-8_latentvariables-55-comparison-table">5.5 Comparison Table<a class="headerlink" href="#deeplearning-8_latentvariables-55-comparison-table" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>REINFORCE</th>
<th>Reparameterization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Works for discrete latent variables</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Works for continuous latent variables</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Low-variance gradients</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Requires differentiable sampling</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Used in VAEs</td>
<td>sometimes</td>
<td>always</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="deeplearning-8_latentvariables-56-practical-notes">5.6 Practical Notes<a class="headerlink" href="#deeplearning-8_latentvariables-56-practical-notes" title="Permanent link">¶</a></h2>
<ul>
<li>Modern VAEs always use the reparameterization trick.  </li>
<li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li>
<li>Discrete VAEs use:</li>
<li>Gumbel-Softmax  </li>
<li>NVIL / REINFORCE with baselines  </li>
<li>VIMCO  </li>
</ul>
<hr>
<h2 id="deeplearning-8_latentvariables-57-summary-of-section-5">5.7 Summary of Section 5<a class="headerlink" href="#deeplearning-8_latentvariables-57-summary-of-section-5" title="Permanent link">¶</a></h2>
<ul>
<li>Gradient estimation is essential for training VI models.  </li>
<li><span class="arithmatex">\(\nabla_\theta\)</span> is easy: just sample from the variational posterior.  </li>
<li><span class="arithmatex">\(\nabla_\phi\)</span> is hard because sampling depends on parameters.  </li>
<li>Two estimators solve this:</li>
<li>Likelihood-ratio (REINFORCE)  </li>
<li>Reparameterization trick  </li>
<li>Reparameterization yields low-variance gradients and powers modern VAEs.</li>
</ul>
<h2 id="deeplearning-8_latentvariables-6-variational-autoencoders-vaes">6. Variational Autoencoders (VAEs)<a class="headerlink" href="#deeplearning-8_latentvariables-6-variational-autoencoders-vaes" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-61-what-is-a-vae">6.1 What Is a VAE?<a class="headerlink" href="#deeplearning-8_latentvariables-61-what-is-a-vae" title="Permanent link">¶</a></h3>
<p>A VAE is a latent variable generative model with:</p>
<ul>
<li>Continuous latent variables <span class="arithmatex">\(z\)</span></li>
<li>Neural networks for both:</li>
<li>Encoder (variational posterior) <span class="arithmatex">\(q_\phi(z \mid x)\)</span>  </li>
<li>Decoder (likelihood) <span class="arithmatex">\(p_\theta(x \mid z)\)</span></li>
<li>Training through amortized variational inference  </li>
<li>Gradients computed using the reparameterization trick</li>
</ul>
<p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-62-vae-model-components">6.2 VAE Model Components<a class="headerlink" href="#deeplearning-8_latentvariables-62-vae-model-components" title="Permanent link">¶</a></h2>
<h3 id="deeplearning-8_latentvariables-prior">Prior<a class="headerlink" href="#deeplearning-8_latentvariables-prior" title="Permanent link">¶</a></h3>
<p>Usually a factorized standard Gaussian:
<script type="math/tex; mode=display">
p(z) = \mathcal{N}(0, I)
</script>
</p>
<h3 id="deeplearning-8_latentvariables-likelihood-decoder">Likelihood / Decoder<a class="headerlink" href="#deeplearning-8_latentvariables-likelihood-decoder" title="Permanent link">¶</a></h3>
<p>Maps latents to a distribution over observations.</p>
<p>For binary data:
<script type="math/tex; mode=display">
p_\theta(x \mid z) = \text{Bernoulli}(x; f_\theta(z))
</script>
</p>
<p>For real-valued data:
<script type="math/tex; mode=display">
p_\theta(x \mid z) = \mathcal{N}(x; \mu_\theta(z), \sigma^2 I)
</script>
</p>
<h3 id="deeplearning-8_latentvariables-variational-posterior-encoder">Variational Posterior / Encoder<a class="headerlink" href="#deeplearning-8_latentvariables-variational-posterior-encoder" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x))
\]</div>
<p>All of these functions (encoder &amp; decoder) can be implemented with:
- MLPs<br>
- ConvNets<br>
- ResNets<br>
- Transformers<br>
depending on the domain.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-63-training-objective-the-elbo">6.3 Training Objective: The ELBO<a class="headerlink" href="#deeplearning-8_latentvariables-63-training-objective-the-elbo" title="Permanent link">¶</a></h2>
<p>VAEs maximize the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\mathcal{L}(x)
=
\mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
-
D_{\text{KL}}\!\Big(q_\phi(z\mid x)\,\|\, p(z)\Big)
\]</div>
<p>Interpretation:</p>
<ol>
<li>
<p>Reconstruction Term<br>
   Measures how well the model predicts <span class="arithmatex">\(x\)</span> from <span class="arithmatex">\(z\)</span>.<br>
   Encourages informative latents.</p>
</li>
<li>
<p>KL Regularization Term<br>
   Encourages <span class="arithmatex">\(q_\phi(z\mid x)\)</span> to stay close to the prior <span class="arithmatex">\(p(z)\)</span>.<br>
   Prevents overfitting and encourages smooth latent spaces.</p>
</li>
</ol>
<p>The KL term often has closed-form for Gaussian distributions.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-64-reparameterization-trick-key-to-vaes">6.4 Reparameterization Trick (Key to VAEs)<a class="headerlink" href="#deeplearning-8_latentvariables-64-reparameterization-trick-key-to-vaes" title="Permanent link">¶</a></h2>
<p>Direct backprop through a sample <span class="arithmatex">\(z \sim q_\phi(z\mid x)\)</span> is impossible.</p>
<p>Solution: rewrite sampling as a differentiable transformation of noise:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon,
\quad
\epsilon \sim \mathcal{N}(0, I)
\]</div>
<p>This allows gradient flow through <span class="arithmatex">\(z\)</span> and makes VAE training practical.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-65-vae-as-a-framework">6.5 VAE as a Framework<a class="headerlink" href="#deeplearning-8_latentvariables-65-vae-as-a-framework" title="Permanent link">¶</a></h2>
<p>The term “VAE” now refers to a broad family of models:
- Continuous latent variables
- Amortized inference
- Reparameterization-based gradients
- Trained by maximizing ELBO (or its variants)</p>
<p>Modern VAEs extend the basic version in many ways:
- Multiple latent layers<br>
- More expressive posteriors (flows, mixtures)<br>
- More expressive priors (hierarchical, autoregressive)<br>
- More expressive decoders (ResNets, autoregressive PixelCNN decoders)<br>
- Iterative inference networks<br>
- Variance reduction techniques</p>
<p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>
<hr>
<h2 id="deeplearning-8_latentvariables-66-summary-of-section-6">6.6 Summary of Section 6<a class="headerlink" href="#deeplearning-8_latentvariables-66-summary-of-section-6" title="Permanent link">¶</a></h2>
<ul>
<li>VAEs are tractable generative models with continuous latent variables.</li>
<li>They pair:</li>
<li>a decoder <span class="arithmatex">\(p_\theta(x\mid z)\)</span> and  </li>
<li>an encoder <span class="arithmatex">\(q_\phi(z\mid x)\)</span>
  using amortized VI.</li>
<li>Training uses ELBO + reparameterization trick.</li>
<li>VAEs balance reconstruction quality with regularized latent structure.</li>
<li>The VAE framework is highly extensible and central to modern deep generative modeling.</li>
</ul></body></html></section></section>
                    <section class='print-page md-section' id='section-5' heading-number='5'>
                        <h1>Distributed Systems<a class='headerlink' href='#section-5' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="distributedsystems-0_intro" heading-number="5.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="introduction">Introduction<a class="headerlink" href="#distributedsystems-0_intro-introduction" title="Permanent link">¶</a></h1>
<p>What is "distributed system":</p>
<p>A group of computers cooperating to provide a service</p>
<h2 id="distributedsystems-0_intro-why">Why?<a class="headerlink" href="#distributedsystems-0_intro-why" title="Permanent link">¶</a></h2>
<ol>
<li>to increase capacity via parallel processing</li>
<li>to tolerate faults via replication</li>
<li>to match distribution of physical devices e.g. sensors</li>
<li>to increase security via isolation</li>
</ol>
<h2 id="distributedsystems-0_intro-challanges">Challanges:<a class="headerlink" href="#distributedsystems-0_intro-challanges" title="Permanent link">¶</a></h2>
<ul>
<li>concurrency</li>
<li>complex interactions</li>
<li>performance bottlenecks</li>
<li>partial failure</li>
</ul>
<h2 id="distributedsystems-0_intro-key-topics">Key Topics<a class="headerlink" href="#distributedsystems-0_intro-key-topics" title="Permanent link">¶</a></h2>
<h3 id="distributedsystems-0_intro-fault-tolerance">Fault tolerance:<a class="headerlink" href="#distributedsystems-0_intro-fault-tolerance" title="Permanent link">¶</a></h3>
<ul>
<li>1000s of servers, big network -&gt; always something broken</li>
<li>We'd like to hide these failures from the application.</li>
<li>"High availability": service continues despite failures</li>
<li>Big idea: replicated servers. If one server crashes, can proceed using the other(s).</li>
</ul>
<h3 id="distributedsystems-0_intro-consistency">Consistency:<a class="headerlink" href="#distributedsystems-0_intro-consistency" title="Permanent link">¶</a></h3>
<ul>
<li>General-purpose infrastructure needs well-defined behavior. E.g. "read(x) yields the value from the most recent write(x)."</li>
<li>Achieving good behavior is hard! e.g. "replica" servers are hard to keep identical.</li>
</ul>
<h3 id="distributedsystems-0_intro-performance">Performance:<a class="headerlink" href="#distributedsystems-0_intro-performance" title="Permanent link">¶</a></h3>
<ul>
<li>The goal: scalable throughput. Nx servers -&gt; Nx total throughput via parallel CPU, RAM, disk, net.</li>
<li>Scaling gets harder as N grows:<ul>
<li>Load imbalance.</li>
<li>Slowest-of-N latency.</li>
</ul>
</li>
</ul>
<h3 id="distributedsystems-0_intro-tradeoffs">Tradeoffs:<a class="headerlink" href="#distributedsystems-0_intro-tradeoffs" title="Permanent link">¶</a></h3>
<ul>
<li>Fault-tolerance, consistency, and performance are enemies.</li>
<li>Fault tolerance and consistency require communication<ul>
<li>e.g., send data to backup server</li>
<li>e.g., check if cached data is up-to-date</li>
<li>communication is often slow and non-scalable</li>
</ul>
</li>
<li>Many designs sacrifice consistency to gain speed.<ul>
<li>e.g. read(x) might <em>not</em> yield the latest write(x)!</li>
<li>Painful for application programmers (or users).</li>
</ul>
</li>
</ul>
<h3 id="distributedsystems-0_intro-implementation">Implementation:<a class="headerlink" href="#distributedsystems-0_intro-implementation" title="Permanent link">¶</a></h3>
<ul>
<li>RPC, threads, concurrency control, configuration.</li>
</ul></body></html></section><section class="print-page" id="distributedsystems-1_mapreduce" heading-number="5.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="mapreduce-a-complete-guide">MapReduce: A Complete Guide<a class="headerlink" href="#distributedsystems-1_mapreduce-mapreduce-a-complete-guide" title="Permanent link">¶</a></h1>
<h2 id="distributedsystems-1_mapreduce-introduction">Introduction<a class="headerlink" href="#distributedsystems-1_mapreduce-introduction" title="Permanent link">¶</a></h2>
<p>Modern data analysis often involves multi-hour computations on multi-terabyte datasets —  for example, building search indexes, sorting massive logs, or analyzing web graphs.  Such tasks are only practical using thousands of computers working in parallel.</p>
<p>MapReduce (MR) is a programming model designed to make large-scale data processing  easy for non-specialist programmers. It lets you write simple sequential code, while the framework handles parallel execution, fault tolerance, and data distribution.</p>
<h2 id="distributedsystems-1_mapreduce-core-concept">Core Concept<a class="headerlink" href="#distributedsystems-1_mapreduce-core-concept" title="Permanent link">¶</a></h2>
<p>The programmer defines just two functions:</p>
<ul>
<li><code>Map()</code> – processes input data and emits key-value pairs.</li>
<li><code>Reduce()</code> – aggregates or summarizes all values associated with a given key.</li>
</ul>
<p>Everything else — input splitting, task scheduling, network communication, and fault recovery —  is handled automatically by the MapReduce framework.</p>
<h2 id="distributedsystems-1_mapreduce-how-mapreduce-works-word-count-example">How MapReduce Works (Word Count Example)<a class="headerlink" href="#distributedsystems-1_mapreduce-how-mapreduce-works-word-count-example" title="Permanent link">¶</a></h2>
<h3 id="distributedsystems-1_mapreduce-abstract-view">Abstract View<a class="headerlink" href="#distributedsystems-1_mapreduce-abstract-view" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>Input1 -&gt; Map -&gt; a,1 b,1
Input2 -&gt; Map -&gt;     b,1
Input3 -&gt; Map -&gt; a,1     c,1
                    |   |   |
                    |   |   -&gt; Reduce -&gt; c,1
                    |   -----&gt; Reduce -&gt; b,2
                    ---------&gt; Reduce -&gt; a,2
</code></pre></div>
<h3 id="distributedsystems-1_mapreduce-steps">Steps<a class="headerlink" href="#distributedsystems-1_mapreduce-steps" title="Permanent link">¶</a></h3>
<ol>
<li>Input Splitting — Data is divided into <code>M</code> splits (files or blocks).</li>
<li>Map Phase — Each split is processed by a Map task, generating <code>(key, value)</code> pairs.</li>
<li>Shuffle Phase — Intermediate pairs are grouped by key and distributed to Reduce tasks.</li>
<li>Reduce Phase — Each Reduce task processes one group and outputs final results.</li>
</ol>
<h2 id="distributedsystems-1_mapreduce-word-count-example">Word Count Example<a class="headerlink" href="#distributedsystems-1_mapreduce-word-count-example" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code><span class="c1"># Map function</span>
<span class="k">def</span> <span class="nf">Map</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">document</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">emit</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Reduce function</span>
<span class="k">def</span> <span class="nf">Reduce</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">emit</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>
</code></pre></div>
<p>Final Output:
</p><div class="highlight"><pre><span></span><code>a: 2
b: 2
c: 1
</code></pre></div><p></p>
<h2 id="distributedsystems-1_mapreduce-why-mapreduce-scales-so-well">Why MapReduce Scales So Well<a class="headerlink" href="#distributedsystems-1_mapreduce-why-mapreduce-scales-so-well" title="Permanent link">¶</a></h2>
<ul>
<li>Parallelism: Map and Reduce tasks run independently, enabling massive parallelism.</li>
<li>Automatic Management: The framework handles failures, scheduling, and communication.</li>
<li>Simplicity: Developers only implement <code>Map()</code> and <code>Reduce()</code>.</li>
</ul>
<h2 id="distributedsystems-1_mapreduce-input-output-storage-via-gfs">Input &amp; Output Storage (via GFS)<a class="headerlink" href="#distributedsystems-1_mapreduce-input-output-storage-via-gfs" title="Permanent link">¶</a></h2>
<p>MapReduce typically uses a distributed file system such as Google File System (GFS).</p>
<ul>
<li>Files split into 64 MB chunks, distributed across many servers.</li>
<li>Maps read input in parallel; Reduces write output in parallel.</li>
<li>Replication (2–3 copies) ensures fault tolerance.</li>
<li>Data locality: Tasks are often scheduled on the same machine where their data resides.</li>
</ul>
<h2 id="distributedsystems-1_mapreduce-inside-the-mapreduce-framework">Inside the MapReduce Framework<a class="headerlink" href="#distributedsystems-1_mapreduce-inside-the-mapreduce-framework" title="Permanent link">¶</a></h2>
<h3 id="distributedsystems-1_mapreduce-coordinators-role">Coordinator’s Role<a class="headerlink" href="#distributedsystems-1_mapreduce-coordinators-role" title="Permanent link">¶</a></h3>
<ol>
<li>Map Phase</li>
<li>Assigns Map tasks to workers.</li>
<li>Each Map writes intermediate output to its local disk.</li>
<li>
<p>Intermediate data is partitioned by <code>hash(key) mod R</code> (R = number of Reduces).</p>
</li>
<li>
<p>Reduce Phase</p>
</li>
<li>Coordinator assigns Reduce tasks.</li>
<li>Each Reduce fetches its partition (bucket) from all Maps.</li>
<li>
<p>Sorts data by key and processes each group.</p>
</li>
<li>
<p>Output</p>
</li>
<li>Each Reduce writes its final output to GFS.</li>
</ol>
<h2 id="distributedsystems-1_mapreduce-performance-and-bottlenecks">Performance and Bottlenecks<a class="headerlink" href="#distributedsystems-1_mapreduce-performance-and-bottlenecks" title="Permanent link">¶</a></h2>
<h3 id="distributedsystems-1_mapreduce-what-limits-performance">What Limits Performance?<a class="headerlink" href="#distributedsystems-1_mapreduce-what-limits-performance" title="Permanent link">¶</a></h3>
<p>Often, network speed is the main bottleneck — not CPU or disk speed.</p>
<p>Network Transfers Include:</p>
<ul>
<li>Maps reading input from GFS.</li>
<li>Reduces fetching intermediate (shuffled) data from Maps.</li>
<li>Reduces writing output to GFS.</li>
</ul>
<p>Because the shuffle phase may move data as large as the original input,<br>
network optimization is critical.</p>
<h3 id="distributedsystems-1_mapreduce-network-optimizations">Network Optimizations<a class="headerlink" href="#distributedsystems-1_mapreduce-network-optimizations" title="Permanent link">¶</a></h3>
<ul>
<li>Data Locality: Run Map tasks where their input data is stored.</li>
<li>Single Network Transfer: Intermediate data stored locally, not in GFS.</li>
<li>Hash Partitioning: Reduces transfer large data batches (buckets), minimizing small transfers.</li>
</ul>
<h2 id="distributedsystems-1_mapreduce-load-balancing">Load Balancing<a class="headerlink" href="#distributedsystems-1_mapreduce-load-balancing" title="Permanent link">¶</a></h2>
<p>Why it matters:  Uneven load causes idle workers waiting for “stragglers”.
Solution:  </p>
<ul>
<li>Create many more tasks than workers.</li>
<li>The Coordinator dynamically assigns tasks to free workers.</li>
<li>Faster machines handle more tasks; slower ones handle fewer.</li>
</ul>
<p>This keeps the cluster well-balanced and efficient.</p>
<h2 id="distributedsystems-1_mapreduce-fault-tolerance">Fault Tolerance<a class="headerlink" href="#distributedsystems-1_mapreduce-fault-tolerance" title="Permanent link">¶</a></h2>
<p>Failures are expected in large clusters. MapReduce handles them gracefully.</p>
<h3 id="distributedsystems-1_mapreduce-worker-failures">Worker Failures<a class="headerlink" href="#distributedsystems-1_mapreduce-worker-failures" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Map worker crash:</p>
</li>
<li>
<p>Intermediate data (stored locally) is lost.</p>
</li>
<li>Coordinator reassigns those Map tasks to new workers.</li>
<li>
<p>No need to rerun if Reduces already fetched the data.</p>
</li>
<li>
<p>Reduce worker crash:</p>
</li>
<li>
<p>Completed results are safe (stored in GFS).</p>
</li>
<li>Unfinished Reduce tasks are rerun elsewhere.</li>
</ul>
<h3 id="distributedsystems-1_mapreduce-deterministic-functions-required">Deterministic Functions Required<a class="headerlink" href="#distributedsystems-1_mapreduce-deterministic-functions-required" title="Permanent link">¶</a></h3>
<p>Because tasks may be re-executed:</p>
<ul>
<li><code>Map()</code> and <code>Reduce()</code> must be pure functions — deterministic and side-effect-free.</li>
<li>No external state, random numbers, or I/O beyond the framework.</li>
</ul>
<p>This guarantees identical results across re-runs.</p>
<h2 id="distributedsystems-1_mapreduce-handling-other-failures">Handling Other Failures<a class="headerlink" href="#distributedsystems-1_mapreduce-handling-other-failures" title="Permanent link">¶</a></h2>
<ul>
<li>Duplicate task execution:<br>
  Coordinator accepts output from only one instance.</li>
<li>Simultaneous Reduce outputs:<br>
  GFS’s atomic rename ensures one consistent final file.</li>
<li>Stragglers:<br>
  Coordinator launches backup copies of slow tasks.</li>
<li>Corrupted output or bad hardware:<br>
  Not handled — MR assumes fail-stop (crash, not corrupt) behavior.</li>
<li>Coordinator crash:<br>
  Not fully addressed in the original paper.</li>
</ul>
<h2 id="distributedsystems-1_mapreduce-where-mapreduce-works-well">Where MapReduce Works Well<a class="headerlink" href="#distributedsystems-1_mapreduce-where-mapreduce-works-well" title="Permanent link">¶</a></h2>
<p>Ideal Use Cases:</p>
<ul>
<li>Batch processing of huge datasets (TB–PB scale)</li>
<li>Log analysis (e.g., counting queries, clickstream analytics)</li>
<li>Index building for search engines</li>
<li>Data transformations (ETL pipelines)</li>
<li>Large-scale machine learning preprocessing</li>
<li>Sorting and aggregation across distributed data</li>
</ul>
<p>These workloads share common traits:</p>
<ul>
<li>Large, independent input records</li>
<li>Deterministic, parallel-friendly computation</li>
<li>No need for real-time feedback</li>
</ul>
<h2 id="distributedsystems-1_mapreduce-where-mapreduce-falls-short">Where MapReduce Falls Short<a class="headerlink" href="#distributedsystems-1_mapreduce-where-mapreduce-falls-short" title="Permanent link">¶</a></h2>
<p>Not Suitable For:</p>
<ul>
<li>
<p>Real-time or streaming data processing<br>
  MR is inherently batch-oriented; results appear only after job completion.</p>
</li>
<li>
<p>Interactive querying<br>
  Jobs take minutes to hours; unsuitable for low-latency analytics.</p>
</li>
<li>
<p>Iterative algorithms<br>
  Machine learning or graph algorithms (e.g., PageRank, K-means) need multiple 
  passes over data, causing heavy I/O.</p>
</li>
<li>
<p>Stateful or dependent tasks<br>
  MR disallows inter-task communication or shared state.</p>
</li>
<li>
<p>Small or medium datasets<br>
  Overhead of distributing tasks outweighs benefits.</p>
</li>
</ul>
<p>Modern systems like Apache Spark, Flink, or Beam were designed to overcome these limitations by enabling in-memory and streaming computation.</p></body></html></section><section class="print-page" id="distributedsystems-2_threads" heading-number="5.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="65840-lecture-2-2025-threads-and-rpc">6.5840 — Lecture 2 (2025): Threads and RPC<a class="headerlink" href="#distributedsystems-2_threads-65840-lecture-2-2025-threads-and-rpc" title="Permanent link">¶</a></h1>
<h2 id="distributedsystems-2_threads-introduction-implementing-distributed-systems">Introduction: Implementing Distributed Systems<a class="headerlink" href="#distributedsystems-2_threads-introduction-implementing-distributed-systems" title="Permanent link">¶</a></h2>
<p>This lecture introduces:
- Go threads (goroutines)
- Concurrency challenges
- The web crawler example
- Remote Procedure Calls (RPC)</p>
<p>Go is the language used for this writeup.</p>
<h2 id="distributedsystems-2_threads-why-go">Why Go?<a class="headerlink" href="#distributedsystems-2_threads-why-go" title="Permanent link">¶</a></h2>
<p>Go is well-suited for distributed systems:</p>
<ul>
<li>Excellent thread (goroutine) support  </li>
<li>Convenient RPC library</li>
<li>Type- and memory-safe</li>
<li>Garbage-collected (safe with concurrency)</li>
<li>Simpler than many other languages</li>
<li>Commonly used in production distributed systems</li>
</ul>
<p>👉 After the tutorial, read Effective Go:<br>
https://golang.org/doc/effective_go.html</p>
<h1 id="distributedsystems-2_threads-threads-goroutines">Threads (Goroutines)<a class="headerlink" href="#distributedsystems-2_threads-threads-goroutines" title="Permanent link">¶</a></h1>
<h2 id="distributedsystems-2_threads-what-is-a-thread">What is a Thread?<a class="headerlink" href="#distributedsystems-2_threads-what-is-a-thread" title="Permanent link">¶</a></h2>
<p>A thread is a “thread of execution”:</p>
<ul>
<li>Allows a program to do multiple things at once</li>
<li>Executes sequentially (like a program), but shares memory with other threads</li>
<li>Has its own program counter, registers, and stack</li>
</ul>
<p>In Go, threads are called goroutines.</p>
<h2 id="distributedsystems-2_threads-why-use-threads">Why Use Threads?<a class="headerlink" href="#distributedsystems-2_threads-why-use-threads" title="Permanent link">¶</a></h2>
<p>Three type of threads:</p>
<h3 id="distributedsystems-2_threads-1-io-concurrency">1. I/O Concurrency<a class="headerlink" href="#distributedsystems-2_threads-1-io-concurrency" title="Permanent link">¶</a></h3>
<ul>
<li>Client sends requests to many servers at once  </li>
<li>Server handles many clients concurrently  </li>
<li>When one thread blocks on I/O, another can run</li>
</ul>
<h3 id="distributedsystems-2_threads-2-multicore-performance">2. Multicore Performance<a class="headerlink" href="#distributedsystems-2_threads-2-multicore-performance" title="Permanent link">¶</a></h3>
<p>Use multiple CPU cores simultaneously.</p>
<h3 id="distributedsystems-2_threads-3-convenience">3. Convenience<a class="headerlink" href="#distributedsystems-2_threads-3-convenience" title="Permanent link">¶</a></h3>
<p>Run background tasks (e.g., periodic health checks).</p>
<h2 id="distributedsystems-2_threads-alternative-event-driven-systems">Alternative: Event-Driven Systems<a class="headerlink" href="#distributedsystems-2_threads-alternative-event-driven-systems" title="Permanent link">¶</a></h2>
<p>Instead of threads:</p>
<ul>
<li>Use a single-threaded system with an event loop</li>
<li>Explicitly interleave different activities</li>
<li>Maintain state tables for each ongoing operation</li>
</ul>
<p>Pros:  </p>
<ul>
<li>Good for I/O concurrency  </li>
<li>No thread overhead</li>
</ul>
<p>Cons:  </p>
<ul>
<li>No multicore usage  </li>
<li>Hard to program and maintain</li>
</ul>
<h1 id="distributedsystems-2_threads-threading-challenges">Threading Challenges<a class="headerlink" href="#distributedsystems-2_threads-threading-challenges" title="Permanent link">¶</a></h1>
<h2 id="distributedsystems-2_threads-1-safe-data-sharing">1. Safe Data Sharing<a class="headerlink" href="#distributedsystems-2_threads-1-safe-data-sharing" title="Permanent link">¶</a></h2>
<p>Race example:
</p><div class="highlight"><pre><span></span><code><span class="nx">n</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>
Two threads modifying <code>n</code> at the same time → race condition.<p></p>
<p>A race is when:
- Two threads access the same memory
- At least one is a write
- And there's no synchronization</p>
<p>Fixes:
- Use <code>sync.Mutex</code>
- Avoid sharing mutable data</p>
<h2 id="distributedsystems-2_threads-2-coordination-producerconsumer">2. Coordination (Producer–Consumer)<a class="headerlink" href="#distributedsystems-2_threads-2-coordination-producerconsumer" title="Permanent link">¶</a></h2>
<ul>
<li>One thread produces data  </li>
<li>Another consumes it  </li>
<li>Need a way for consumers to wait and wake up</li>
</ul>
<p>Tools:
- Go channels
- <code>sync.Cond</code>
- <code>sync.Wait</code>, <code>sync.WaitGroup</code></p>
<h2 id="distributedsystems-2_threads-3-deadlock">3. Deadlock<a class="headerlink" href="#distributedsystems-2_threads-3-deadlock" title="Permanent link">¶</a></h2>
<p>When threads wait on each other forever.<br>
Can happen via:</p>
<ul>
<li>Locks</li>
<li>Channels</li>
<li>RPC</li>
</ul>
<h1 id="distributedsystems-2_threads-web-crawler-example">Web Crawler Example<a class="headerlink" href="#distributedsystems-2_threads-web-crawler-example" title="Permanent link">¶</a></h1>
<p>A web crawler:</p>
<ul>
<li>Fetches web pages recursively starting from a URL</li>
<li>Follows links</li>
<li>Avoids revisiting pages</li>
<li>Avoids cycles</li>
<li>Exploits I/O concurrency for speed</li>
</ul>
<h2 id="distributedsystems-2_threads-1-serial-crawler">1. Serial Crawler<a class="headerlink" href="#distributedsystems-2_threads-1-serial-crawler" title="Permanent link">¶</a></h2>
<ul>
<li>Depth-first traversal  </li>
<li>A shared map tracks visited URLs  </li>
<li>Simple and correct  </li>
<li>Very slow — only fetches one page at a time  </li>
</ul>
<p>Adding <code>go</code> before recursive calls breaks correctness:</p>
<ul>
<li>Many threads may fetch same URL  </li>
<li>Finishing detection becomes difficult</li>
</ul>
<h1 id="distributedsystems-2_threads-2-concurrent-crawler-with-mutex">2. Concurrent Crawler with Mutex<a class="headerlink" href="#distributedsystems-2_threads-2-concurrent-crawler-with-mutex" title="Permanent link">¶</a></h1>
<h3 id="distributedsystems-2_threads-how-it-works">How it Works<a class="headerlink" href="#distributedsystems-2_threads-how-it-works" title="Permanent link">¶</a></h3>
<ul>
<li>Launch a goroutine per page</li>
<li>Shared <code>fetched</code> map  </li>
<li>Mutex ensures only one thread fetches each URL</li>
</ul>
<h3 id="distributedsystems-2_threads-why-the-mutex">Why the Mutex?<a class="headerlink" href="#distributedsystems-2_threads-why-the-mutex" title="Permanent link">¶</a></h3>
<h4 id="distributedsystems-2_threads-1-avoid-logical-races">1. Avoid Logical Races<a class="headerlink" href="#distributedsystems-2_threads-1-avoid-logical-races" title="Permanent link">¶</a></h4>
<p>Two threads may check the same URL at once:
- Both see <code>fetched[url] == false</code>
- Both fetch → wrong</p>
<p>Mutex ensures:
- One thread checks + sets at a time</p>
<h4 id="distributedsystems-2_threads-2-avoid-map-corruption">2. Avoid Map Corruption<a class="headerlink" href="#distributedsystems-2_threads-2-avoid-map-corruption" title="Permanent link">¶</a></h4>
<p>Go maps are not thread-safe.</p>
<h2 id="distributedsystems-2_threads-what-if-lock-is-removed">What If Lock Is Removed?<a class="headerlink" href="#distributedsystems-2_threads-what-if-lock-is-removed" title="Permanent link">¶</a></h2>
<ul>
<li>Program may appear to work sometimes  </li>
<li>But races still occur  </li>
<li>Use the race detector:</li>
</ul>
<div class="highlight"><pre><span></span><code>go run -race crawler.go
</code></pre></div>
<h2 id="distributedsystems-2_threads-completion-detection-using-syncwaitgroup">Completion Detection Using sync.WaitGroup<a class="headerlink" href="#distributedsystems-2_threads-completion-detection-using-syncwaitgroup" title="Permanent link">¶</a></h2>
<ul>
<li><code>Add(n)</code> increments  </li>
<li><code>Done()</code> decrements  </li>
<li><code>Wait()</code> blocks until count is zero  </li>
</ul>
<p>Ensures main thread waits for all children.</p>
<h1 id="distributedsystems-2_threads-3-concurrent-crawler-with-channels">3. Concurrent Crawler with Channels<a class="headerlink" href="#distributedsystems-2_threads-3-concurrent-crawler-with-channels" title="Permanent link">¶</a></h1>
<p>Channels provide:</p>
<ul>
<li>Communication  </li>
<li>Synchronization  </li>
</ul>
<h3 id="distributedsystems-2_threads-channel-basics">Channel Basics<a class="headerlink" href="#distributedsystems-2_threads-channel-basics" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span>

<span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">x</span><span class="w">   </span><span class="c1">// send (blocks)</span>
<span class="nx">y</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ch</span><span class="w"> </span><span class="c1">// receive (blocks)</span>
</code></pre></div>
<hr>
<h2 id="distributedsystems-2_threads-coordinator-workers-model">Coordinator + Workers Model<a class="headerlink" href="#distributedsystems-2_threads-coordinator-workers-model" title="Permanent link">¶</a></h2>
<ul>
<li>Coordinator creates workers via goroutines  </li>
<li>Workers fetch a page and send resulting URLs via channel  </li>
<li>Coordinator receives URLs, checks visited set</li>
</ul>
<h3 id="distributedsystems-2_threads-why-no-mutex">Why No Mutex?<a class="headerlink" href="#distributedsystems-2_threads-why-no-mutex" title="Permanent link">¶</a></h3>
<ul>
<li>Shared state is only in coordinator  </li>
<li>Workers never mutate shared maps  </li>
<li>Therefore no races</li>
</ul>
<h2 id="distributedsystems-2_threads-channel-safety">Channel Safety<a class="headerlink" href="#distributedsystems-2_threads-channel-safety" title="Permanent link">¶</a></h2>
<p>Example:</p>
<ul>
<li>Worker creates slice of URLs</li>
<li>Sends it to channel</li>
<li>Coordinator reads it</li>
</ul>
<p>Safe because:</p>
<ul>
<li>Worker writes slice before send completes</li>
<li>Coordinator reads slice after receive completes</li>
</ul>
<p>No overlap → no race.</p>
<h2 id="distributedsystems-2_threads-why-some-sends-need-a-goroutine">Why Some Sends Need a Goroutine?<a class="headerlink" href="#distributedsystems-2_threads-why-some-sends-need-a-goroutine" title="Permanent link">¶</a></h2>
<p>Without a goroutine:</p>
<ul>
<li>send blocks  </li>
<li>coordinator may not reach the receive  </li>
<li>→ deadlock</li>
</ul>
<h2 id="distributedsystems-2_threads-locks-vs-channels">Locks vs Channels<a class="headerlink" href="#distributedsystems-2_threads-locks-vs-channels" title="Permanent link">¶</a></h2>
<p>Both are powerful. Use whichever matches intuition:</p>
<ul>
<li>State-focused logic → locks</li>
<li>Communication-focused logic → channels</li>
</ul>
<p>In 6.5840 labs:
- Use sharing + locks for state
- Use channels, <code>sync.Cond</code>, or sleep-based polling for notifications</p>
<h1 id="distributedsystems-2_threads-remote-procedure-call-rpc">Remote Procedure Call (RPC)<a class="headerlink" href="#distributedsystems-2_threads-remote-procedure-call-rpc" title="Permanent link">¶</a></h1>
<p>RPC enables easy client-server communication.</p>
<h2 id="distributedsystems-2_threads-goals">Goals<a class="headerlink" href="#distributedsystems-2_threads-goals" title="Permanent link">¶</a></h2>
<ul>
<li>Hide network details</li>
<li>Provide a procedure-call interface</li>
<li>Automatically marshal/unmarshal data</li>
<li>Enable portability across systems</li>
</ul>
<h2 id="distributedsystems-2_threads-rpc-architecture">RPC Architecture<a class="headerlink" href="#distributedsystems-2_threads-rpc-architecture" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>Client               Server
  request ----&gt;
            &lt;---- response
</code></pre></div>
<p>Software structure:
</p><div class="highlight"><pre><span></span><code>Client App        Server Handlers
Client Stubs      Dispatcher
RPC Library  ---- RPC Library
 Network     ---- Network
</code></pre></div><p></p>
<h1 id="distributedsystems-2_threads-go-rpc-example-keyvalue-store">Go RPC Example: Key/Value Store<a class="headerlink" href="#distributedsystems-2_threads-go-rpc-example-keyvalue-store" title="Permanent link">¶</a></h1>
<p>Handlers:
- <code>Put(key, value)</code>
- <code>Get(key) -&gt; value</code></p>
<h2 id="distributedsystems-2_threads-client-side">Client Side<a class="headerlink" href="#distributedsystems-2_threads-client-side" title="Permanent link">¶</a></h2>
<ul>
<li>Use <code>Dial()</code> to connect  </li>
<li>Call RPC using:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nx">Call</span><span class="p">(</span><span class="s">"KVServer.Get"</span><span class="p">,</span><span class="w"> </span><span class="nx">args</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">reply</span><span class="p">)</span>
</code></pre></div>
<p>RPC library:
- Marshals args<br>
- Sends request<br>
- Waits for reply<br>
- Unmarshals reply<br>
- Returns error if something went wrong  </p>
<h2 id="distributedsystems-2_threads-server-side">Server Side<a class="headerlink" href="#distributedsystems-2_threads-server-side" title="Permanent link">¶</a></h2>
<p>Server must:
1. Declare a type with exported RPC methods<br>
2. Register the type<br>
3. Accept TCP connections and let RPC library handle them  </p>
<p>RPC library:</p>
<ul>
<li>Creates goroutine per request  </li>
<li>Unmarshals request  </li>
<li>Dispatches handler  </li>
<li>Marshals reply  </li>
<li>Sends reply  </li>
</ul>
<p>Handlers must use locks since multiple RPCs run concurrently.</p>
<h2 id="distributedsystems-2_threads-rpc-details">RPC Details<a class="headerlink" href="#distributedsystems-2_threads-rpc-details" title="Permanent link">¶</a></h2>
<h3 id="distributedsystems-2_threads-binding">Binding<a class="headerlink" href="#distributedsystems-2_threads-binding" title="Permanent link">¶</a></h3>
<p>Client must know <code>"server:port"</code> to dial.</p>
<h3 id="distributedsystems-2_threads-marshalling-rules">Marshalling Rules<a class="headerlink" href="#distributedsystems-2_threads-marshalling-rules" title="Permanent link">¶</a></h3>
<ul>
<li>Sends strings, arrays, structs, maps  </li>
<li>Cannot send channels or functions  </li>
<li>Only exported fields in structs are marshaled  </li>
<li>Pointers are sent by copying the pointee</li>
</ul>
<h1 id="distributedsystems-2_threads-rpc-failures">RPC Failures<a class="headerlink" href="#distributedsystems-2_threads-rpc-failures" title="Permanent link">¶</a></h1>
<p>Client may never get a reply:</p>
<p>Could mean:</p>
<ul>
<li>Server never received request  </li>
<li>Server crashed after executing  </li>
<li>Reply lost in network  </li>
<li>Network or server slow  </li>
</ul>
<p>RPC ≠ local function call.</p>
<h1 id="distributedsystems-2_threads-best-effort-rpc">Best-Effort RPC<a class="headerlink" href="#distributedsystems-2_threads-best-effort-rpc" title="Permanent link">¶</a></h1>
<p>Algorithm:
1. Send request<br>
2. Wait<br>
3. If no reply, resend<br>
4. After several tries → give up  </p>
<h3 id="distributedsystems-2_threads-problems">Problems<a class="headerlink" href="#distributedsystems-2_threads-problems" title="Permanent link">¶</a></h3>
<p>Example:
</p><div class="highlight"><pre><span></span><code>Put("k", 10)
Put("k", 20)
</code></pre></div><p></p>
<p>Retries can reorder or duplicate operations.</p>
<h2 id="distributedsystems-2_threads-when-is-best-effort-ok">When Is Best-Effort OK?<a class="headerlink" href="#distributedsystems-2_threads-when-is-best-effort-ok" title="Permanent link">¶</a></h2>
<ul>
<li>Read-only operations  </li>
<li>Idempotent operations (safe to repeat)</li>
</ul>
<h1 id="distributedsystems-2_threads-at-most-once-semantics">At-Most-Once Semantics<a class="headerlink" href="#distributedsystems-2_threads-at-most-once-semantics" title="Permanent link">¶</a></h1>
<p>Go RPC provides:</p>
<ul>
<li>One TCP connection  </li>
<li>Sends each request once  </li>
<li>No retries → no duplicates  </li>
</ul>
<p>But:</p>
<ul>
<li>Errors returned on timeouts  </li>
<li>Hard to build replicated fault-tolerant systems without retries  </li>
</ul>
<p>Later labs explore stronger semantics.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-6' heading-number='6'>
                        <h1>Information Theory<a class='headerlink' href='#section-6' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="informationtheory-1_intro_to_infotheory" heading-number="6.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-to-information-theory-for-machine-learning">Chapter 1 — Introduction to Information Theory (for Machine Learning)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-chapter-1-introduction-to-information-theory-for-machine-learning" title="Permanent link">¶</a></h1>
<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p>
<ul>
<li>loss functions (cross-entropy, NLL)</li>
<li>representation learning and contrastive learning</li>
<li>variational inference and VAEs</li>
<li>generative modeling (GANs, flows, diffusion)</li>
<li>reinforcement learning (entropy bonuses, policy KL constraints)</li>
<li>model capacity, generalization, and bottlenecks</li>
</ul>
<p>This chapter introduces the core motivations and conceptual tools.</p>
<h2 id="informationtheory-1_intro_to_infotheory-1-why-information-theory-matters-for-ml">1. Why Information Theory Matters for ML<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-1-why-information-theory-matters-for-ml" title="Permanent link">¶</a></h2>
<p>Information theory answers questions fundamental to ML:</p>
<ul>
<li>How much uncertainty does a model reduce?</li>
<li>How do we quantify the difference between two probability distributions?</li>
<li>How do we measure dependence between variables?</li>
<li>What is the maximum information a neural network layer can transmit?</li>
<li>How do we formalize compression and generalization?</li>
</ul>
<p>In ML, information theory is not abstract mathematics —<br>
it provides the <em>language</em> for describing learning itself:</p>
<blockquote>
<p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p>
</blockquote>
<p>This viewpoint unifies:</p>
<ul>
<li>Maximum likelihood  </li>
<li>Variational inference  </li>
<li>Contrastive learning  </li>
<li>GAN objectives  </li>
<li>Representation learning  </li>
<li>Reinforcement learning signal shaping  </li>
</ul>
<h2 id="informationtheory-1_intro_to_infotheory-2-the-communication-view-shannons-formulation">2. The Communication View (Shannon’s Formulation)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-2-the-communication-view-shannons-formulation" title="Permanent link">¶</a></h2>
<p>A classical communication system consists of:</p>
<ol>
<li>
<p>Source:    Generates data (symbols, images, text, states).</p>
</li>
<li>
<p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p>
</li>
<li>
<p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p>
</li>
<li>
<p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p>
</li>
<li>
<p>Receiver:   Obtains the final predictions or reconstructions.</p>
</li>
</ol>
<p>Information theory studies:</p>
<ul>
<li>Limits of efficient communication  </li>
<li>Optimal encoding and representation  </li>
<li>Tradeoffs between compression and fidelity  </li>
<li>Effect of noise on learnability</li>
</ul>
<h2 id="informationtheory-1_intro_to_infotheory-3-the-uncertainty-view-shannonbayesian-perspective">3. The Uncertainty View (Shannon–Bayesian Perspective)<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-3-the-uncertainty-view-shannonbayesian-perspective" title="Permanent link">¶</a></h2>
<p>Information theory also quantifies <em>uncertainty</em>:</p>
<ul>
<li>More uncertainty → more information needed  </li>
<li>Less uncertainty → easier prediction and compression  </li>
</ul>
<p>Key idea: Information is the reduction of uncertainty.</p>
<p>In ML:</p>
<ul>
<li>Entropy measures label uncertainty  </li>
<li>Cross-entropy measures model fit  </li>
<li>KL divergence measures mismatch  </li>
<li>Mutual information measures representation quality  </li>
<li>ELBO measures how well a generative model explains data  </li>
</ul>
<p>Thus, learning and compression are mathematically the same problem.</p>
<h2 id="informationtheory-1_intro_to_infotheory-4-machine-learning-as-communication">4. Machine Learning as Communication<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-4-machine-learning-as-communication" title="Permanent link">¶</a></h2>
<p>Modern ML pipelines resemble a communication system:</p>
<h3 id="informationtheory-1_intro_to_infotheory-data-encoder-latent-representation-decoder-output">Data → Encoder → Latent Representation → Decoder → Output<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-data-encoder-latent-representation-decoder-output" title="Permanent link">¶</a></h3>
<p>Examples:</p>
<ul>
<li>Autoencoders / VAEs: compress <span class="arithmatex">\(x\)</span> into <span class="arithmatex">\(z\)</span>, then reconstruct</li>
<li>Transformers: compress sequences into features, decode predictions</li>
<li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li>
<li>GANs: learn generator distributions close to data distribution</li>
<li>RL agents: compress sensory input into state representations</li>
</ul>
<p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>
<h3 id="informationtheory-1_intro_to_infotheory-5-roadmap-for-this-web-book">5. Roadmap for This Web-book<a class="headerlink" href="#informationtheory-1_intro_to_infotheory-5-roadmap-for-this-web-book" title="Permanent link">¶</a></h3>
<p>This web-book is structured to build information theory specifically for ML:</p>
<ol>
<li>
<p>Entropy &amp; Self-Information<br>
   Foundations of uncertainty, coding length, and compression.</p>
</li>
<li>
<p>Cross-Entropy &amp; Negative Log-Likelihood<br>
   Core ML loss; the bridge between probability and training objectives.</p>
</li>
<li>
<p>KL Divergence &amp; f-Divergences<br>
   Quantifying model mismatch, VI, GAN divergences.</p>
</li>
<li>
<p>Jensen–Shannon &amp; Wasserstein Distances<br>
   GAN stability, geometric learning, distribution metrics.</p>
</li>
<li>
<p>Mutual Information &amp; Estimation Bounds<br>
   Representation learning, contrastive learning, InfoNCE.</p>
</li>
<li>
<p>Variational Inference &amp; ELBO<br>
   VAEs, Bayesian deep learning, posterior approximations.</p>
</li>
<li>
<p>Information Bottleneck &amp; Representation Theory<br>
   Why deep networks compress, and how representations generalize.</p>
</li>
<li>
<p>Summary &amp; Concept Map<br>
   Unifying view of entropy → KL → MI → VI → representation learning.</p>
</li>
</ol></body></html></section><section class="print-page" id="informationtheory-2_entropy" heading-number="6.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-entropy-self-information-cross-entropy-information-measures">Chapter 2 — Entropy, Self-Information, Cross-Entropy &amp; Information Measures<a class="headerlink" href="#informationtheory-2_entropy-chapter-2-entropy-self-information-cross-entropy-information-measures" title="Permanent link">¶</a></h1>
<h2 id="informationtheory-2_entropy-1-self-information-surprisal">1. Self-Information (Surprisal)<a class="headerlink" href="#informationtheory-2_entropy-1-self-information-surprisal" title="Permanent link">¶</a></h2>
<p>Self-information quantifies the <em>surprise</em> of observing an event.</p>
<p>For an event with probability <span class="arithmatex">\(p(x)\)</span>:</p>
<div class="arithmatex">\[
I(x) = - \log_2 p(x)
\]</div>
<p>Why the log?</p>
<ul>
<li>Additivity of independent events  </li>
<li>Probability → information monotonicity  </li>
<li>Log base 2 → units in bits  </li>
<li>Log-likelihoods become additive → ML becomes convex (in many models)</li>
</ul>
<p>Interpretations:</p>
<ul>
<li>Unlikely events carry more information  </li>
<li>Certain events carry zero information  </li>
<li>Foundation of cross-entropy and negative log-likelihood  </li>
</ul>
<p>In ML:  </p>
<ul>
<li>The loss used in classification is simply the surprisal of the correct class.</li>
</ul>
<h2 id="informationtheory-2_entropy-2-entropy-expected-uncertainty">2. Entropy — Expected Uncertainty<a class="headerlink" href="#informationtheory-2_entropy-2-entropy-expected-uncertainty" title="Permanent link">¶</a></h2>
<p>Entropy is the expected self-information:</p>
<div class="arithmatex">\[
H(X) = -\sum_x p(x) \log p(x)
\]</div>
<p>Entropy measures:</p>
<ul>
<li>Uncertainty  </li>
<li>Randomness  </li>
<li>Compressibility  </li>
<li>Difficulty of prediction  </li>
</ul>
<h3 id="informationtheory-2_entropy-key-properties">Key properties:<a class="headerlink" href="#informationtheory-2_entropy-key-properties" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(H(X) = 0\)</span> if a variable is deterministic  </li>
<li>Maximum when distribution is uniform  </li>
<li>Upper bound on achievable compression (Shannon)</li>
</ul>
<h3 id="informationtheory-2_entropy-ml-interpretation">ML Interpretation:<a class="headerlink" href="#informationtheory-2_entropy-ml-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>High entropy labels → noisy dataset → harder learning</li>
<li>Activation entropy reflects network expressiveness</li>
<li>Entropy of output distribution measures model confidence</li>
<li>Entropy regularization improves exploration in RL</li>
</ul>
<h2 id="informationtheory-2_entropy-3-differential-entropy-continuous-entropy">3. Differential Entropy (Continuous Entropy)<a class="headerlink" href="#informationtheory-2_entropy-3-differential-entropy-continuous-entropy" title="Permanent link">¶</a></h2>
<p>For continuous variables:</p>
<div class="arithmatex">\[
h(X) = -\int p(x) \log p(x)\,dx
\]</div>
<p>Important differences:</p>
<ul>
<li>Can be negative</li>
<li>Not invariant under reparameterization</li>
<li>Not comparable between different coordinate systems</li>
</ul>
<h3 id="informationtheory-2_entropy-why-it-matters-in-ml">Why it matters in ML:<a class="headerlink" href="#informationtheory-2_entropy-why-it-matters-in-ml" title="Permanent link">¶</a></h3>
<ul>
<li>VAEs use continuous latent variables <span class="arithmatex">\(z\)</span></li>
<li>Flows and diffusion models use continuous densities</li>
<li>Score-based models estimate gradients of log-densities, not densities directly</li>
</ul>
<p>Differential entropy is not the same thing as Shannon entropy — a common source of confusion.</p>
<h2 id="informationtheory-2_entropy-4-joint-conditional-and-total-entropy">4. Joint, Conditional, and Total Entropy<a class="headerlink" href="#informationtheory-2_entropy-4-joint-conditional-and-total-entropy" title="Permanent link">¶</a></h2>
<h3 id="informationtheory-2_entropy-joint-entropy">Joint entropy:<a class="headerlink" href="#informationtheory-2_entropy-joint-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(X,Y) = -\sum_{x,y} p(x,y)\log p(x,y)
\]</div>
<h3 id="informationtheory-2_entropy-conditional-entropy">Conditional entropy:<a class="headerlink" href="#informationtheory-2_entropy-conditional-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(Y|X) = -\sum_{x,y} p(x,y)\log p(y|x)
\]</div>
<p>Interpretation:</p>
<ul>
<li>Average residual uncertainty in <span class="arithmatex">\(Y\)</span> after observing <span class="arithmatex">\(X\)</span></li>
</ul>
<h3 id="informationtheory-2_entropy-chain-rule-of-entropy">Chain rule of entropy:<a class="headerlink" href="#informationtheory-2_entropy-chain-rule-of-entropy" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(X,Y) = H(X) + H(Y|X)
\]</div>
<p>This rule is foundational for:</p>
<ul>
<li>Autoregressive modeling  </li>
<li>Sequence modeling  </li>
<li>Transformers (predictive factorization)  </li>
<li>Bayesian networks  </li>
</ul>
<h2 id="informationtheory-2_entropy-5-cross-entropy-coding-p-using-q">5. Cross-Entropy — Coding <span class="arithmatex">\(p\)</span> Using <span class="arithmatex">\(q\)</span><a class="headerlink" href="#informationtheory-2_entropy-5-cross-entropy-coding-p-using-q" title="Permanent link">¶</a></h2>
<p>Cross-entropy is the expected surprise under model <span class="arithmatex">\(q\)</span>:</p>
<div class="arithmatex">\[
H(p, q) = -\sum_x p(x)\log q(x)
\]</div>
<h3 id="informationtheory-2_entropy-crucial-identity">Crucial identity:<a class="headerlink" href="#informationtheory-2_entropy-crucial-identity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H(p, q) = H(p) + D_{\text{KL}}(p\|q)
\]</div>
<p>Meaning:</p>
<ul>
<li>True entropy + penalty for using the wrong distribution</li>
<li>Cross-entropy ≥ entropy</li>
</ul>
<h3 id="informationtheory-2_entropy-ml-interpretation_1">ML Interpretation:<a class="headerlink" href="#informationtheory-2_entropy-ml-interpretation_1" title="Permanent link">¶</a></h3>
<p>Cross-entropy = Negative Log Likelihood:</p>
<div class="arithmatex">\[
\mathcal{L} = - \log q(y_{\text{true}})
\]</div>
<p>This powers:</p>
<ul>
<li>Softmax classifiers  </li>
<li>Logistic regression  </li>
<li>Transformers (next-token prediction)  </li>
<li>Language models (autoregressive LM)  </li>
<li>Image segmentation (pixel-wise CE)  </li>
</ul>
<p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>
<h2 id="informationtheory-2_entropy-6-perplexity-entropy-in-language-modeling">6. Perplexity — Entropy in Language Modeling<a class="headerlink" href="#informationtheory-2_entropy-6-perplexity-entropy-in-language-modeling" title="Permanent link">¶</a></h2>
<p>Perplexity is:</p>
<div class="arithmatex">\[
\text{PPL} = 2^{H}
\]</div>
<p>Interpretation:</p>
<ul>
<li>The “effective vocabulary size” the model thinks it must guess from</li>
<li>Lower perplexity = better language model</li>
</ul>
<p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>
<h2 id="informationtheory-2_entropy-7-mutual-information-information-shared-between-variables">7. Mutual Information — Information Shared Between Variables<a class="headerlink" href="#informationtheory-2_entropy-7-mutual-information-information-shared-between-variables" title="Permanent link">¶</a></h2>
<div class="arithmatex">\[
I(X;Y) = D_{\text{KL}}(p(x,y)\|p(x)p(y))
\]</div>
<p>MI measures:</p>
<ul>
<li>How much knowing <span class="arithmatex">\(X\)</span> tells us about <span class="arithmatex">\(Y\)</span></li>
<li>Reduction in entropy of one variable after observing the other</li>
</ul>
<h3 id="informationtheory-2_entropy-equivalent-forms">Equivalent forms:<a class="headerlink" href="#informationtheory-2_entropy-equivalent-forms" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
I(X;Y) = H(X) - H(X|Y)
\]</div>
<div class="arithmatex">\[
I(X;Y) = H(X) + H(Y) - H(X,Y)
\]</div>
<p>MI links entropy and KL divergence into a unified measure of dependence.</p>
<h3 id="informationtheory-2_entropy-why-mi-is-critical-in-ml">Why MI is critical in ML:<a class="headerlink" href="#informationtheory-2_entropy-why-mi-is-critical-in-ml" title="Permanent link">¶</a></h3>
<ul>
<li>Representation learning (maximize MI with labels)</li>
<li>Contrastive learning (InfoNCE is a lower bound to MI)</li>
<li>InfoGAN (maximize MI between latent code and output)</li>
<li>Feature selection (choose features with highest MI to labels)</li>
<li>Stochastic encoders control MI with constraints</li>
</ul>
<h2 id="informationtheory-2_entropy-8-the-data-processing-inequality-dpi">8. The Data Processing Inequality (DPI)<a class="headerlink" href="#informationtheory-2_entropy-8-the-data-processing-inequality-dpi" title="Permanent link">¶</a></h2>
<p>If:</p>
<div class="arithmatex">\[
X \rightarrow Z \rightarrow Y
\]</div>
<p>is a Markov chain, then:</p>
<div class="arithmatex">\[
I(X;Y) \le I(X;Z)
\]</div>
<p>Meaning:</p>
<ul>
<li>Processing or compressing data cannot add information</li>
<li>Neural networks cannot create information about the input<br>
  — they can only discard or transform it</li>
</ul>
<p>ML relevance:</p>
<ul>
<li>Explains why deeper layers become more task-specialized  </li>
<li>Supports the Information Bottleneck theory in deep learning  </li>
<li>Ensures that any learned representation is bounded by input information  </li>
<li>Helps analyze generalization and compression in deep nets</li>
</ul>
<h2 id="informationtheory-2_entropy-9-entropy-in-neural-networks">9. Entropy in Neural Networks<a class="headerlink" href="#informationtheory-2_entropy-9-entropy-in-neural-networks" title="Permanent link">¶</a></h2>
<p>Entropy plays multiple roles in deep learning:</p>
<h3 id="informationtheory-2_entropy-output-entropy">Output entropy<a class="headerlink" href="#informationtheory-2_entropy-output-entropy" title="Permanent link">¶</a></h3>
<p>Low entropy → confident predictions<br>
High entropy → uncertainty</p>
<h3 id="informationtheory-2_entropy-entropy-of-hidden-representations">Entropy of hidden representations<a class="headerlink" href="#informationtheory-2_entropy-entropy-of-hidden-representations" title="Permanent link">¶</a></h3>
<ul>
<li>Early layers: reduce entropy (denoising)  </li>
<li>Deep layers: compress irrelevant information  </li>
<li>Good representations retain low entropy but high MI with labels</li>
</ul>
<h3 id="informationtheory-2_entropy-entropy-regularization-in-rl">Entropy regularization in RL<a class="headerlink" href="#informationtheory-2_entropy-entropy-regularization-in-rl" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
J(\pi) += \beta H(\pi(\cdot|s))
</script>
encourages exploration.</p>
<h3 id="informationtheory-2_entropy-dropout-increases-entropy">Dropout increases entropy<a class="headerlink" href="#informationtheory-2_entropy-dropout-increases-entropy" title="Permanent link">¶</a></h3>
<p>forcing models to encode more robust representations.</p></body></html></section><section class="print-page" id="informationtheory-3_kl" heading-number="6.3"><h1 id="informationtheory-3_kl-informationtheory-3_kl">3. Kullback-Leibler Divergence</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Chapter 3 — KL Divergence, f-Divergences, Jensen–Shannon Divergence, and Wasserstein Distance</p>
<p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>
<h2 id="informationtheory-3_kl-1-kl-divergence-measuring-distribution-mismatch">1. KL Divergence: Measuring Distribution Mismatch<a class="headerlink" href="#informationtheory-3_kl-1-kl-divergence-measuring-distribution-mismatch" title="Permanent link">¶</a></h2>
<p>The Kullback–Leibler (KL) divergence measures how different two probability distributions are. For distributions <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(p\|q)
= \sum_x p(x)\log\frac{p(x)}{q(x)}.
\]</div>
<p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from <span class="arithmatex">\(p\)</span> using a code optimized for <span class="arithmatex">\(q\)</span>. If <span class="arithmatex">\(q\)</span> assigns very low probability to events that occur frequently under <span class="arithmatex">\(p\)</span>, the KL divergence becomes large.</p>
<h3 id="informationtheory-3_kl-key-properties-of-kl-divergence">Key properties of KL divergence<a class="headerlink" href="#informationtheory-3_kl-key-properties-of-kl-divergence" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Non-negative<br>
<script type="math/tex; mode=display">
   D_{\text{KL}}(p\|q) \ge 0.
   </script>
</p>
</li>
<li>
<p>Zero only when the two distributions are identical.</p>
</li>
<li>
<p>Asymmetric<br>
<script type="math/tex; mode=display">
   D_{\text{KL}}(p\|q) \ne D_{\text{KL}}(q\|p).
   </script>
</p>
</li>
<li>
<p>Not a true metric, since it fails the triangle inequality.</p>
</li>
<li>
<p>Can be infinite when <span class="arithmatex">\(p(x) &gt; 0\)</span> but <span class="arithmatex">\(q(x) = 0\)</span>.<br>
   This is a crucial issue in generative modeling, where such mismatches occur frequently.</p>
</li>
</ol>
<h2 id="informationtheory-3_kl-2-kl-divergence-in-machine-learning">2. KL Divergence in Machine Learning<a class="headerlink" href="#informationtheory-3_kl-2-kl-divergence-in-machine-learning" title="Permanent link">¶</a></h2>
<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>
<h3 id="informationtheory-3_kl-21-maximum-likelihood-as-forward-kl-minimization">2.1 Maximum likelihood as forward KL minimization<a class="headerlink" href="#informationtheory-3_kl-21-maximum-likelihood-as-forward-kl-minimization" title="Permanent link">¶</a></h3>
<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p>
<div class="arithmatex">\[
\theta^*
= \arg\min_\theta D_{\text{KL}}(p_{\text{data}} \,\|\, q_\theta).
\]</div>
<p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p>
<p>This produces mode-covering behavior, which is characteristic of:</p>
<ul>
<li>normalizing flows  </li>
<li>autoregressive models  </li>
<li>density estimation models trained via log-likelihood  </li>
</ul>
<h3 id="informationtheory-3_kl-22-kl-divergence-in-variational-inference-vi">2.2 KL divergence in variational inference (VI)<a class="headerlink" href="#informationtheory-3_kl-22-kl-divergence-in-variational-inference-vi" title="Permanent link">¶</a></h3>
<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior <span class="arithmatex">\(q(z|x)\)</span> and the true posterior <span class="arithmatex">\(p(z|x)\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Since the true posterior is typically intractable, VAEs approximate this with:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z)).
\]</div>
<p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p>
<p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative “safe” solution.</p>
<h3 id="informationtheory-3_kl-23-kl-divergence-in-reinforcement-learning">2.3 KL divergence in reinforcement learning<a class="headerlink" href="#informationtheory-3_kl-23-kl-divergence-in-reinforcement-learning" title="Permanent link">¶</a></h3>
<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p>
<div class="arithmatex">\[
D_{\text{KL}}(\pi_{\text{old}} \,\|\, \pi_{\text{new}}).
\]</div>
<p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>
<h3 id="informationtheory-3_kl-24-kl-divergence-in-distillation-and-compression">2.4 KL divergence in distillation and compression<a class="headerlink" href="#informationtheory-3_kl-24-kl-divergence-in-distillation-and-compression" title="Permanent link">¶</a></h3>
<p>KL divergence compares two probability distributions directly and is used for:</p>
<ul>
<li>teacher–student distillation  </li>
<li>compressing large models into smaller ones  </li>
<li>aligning probability distributions across layers  </li>
<li>calibrating output probabilities  </li>
</ul>
<p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>
<h2 id="informationtheory-3_kl-3-understanding-kl-behavior-mode-covering-vs-mode-seeking">3. Understanding KL Behavior: Mode Covering vs. Mode Seeking<a class="headerlink" href="#informationtheory-3_kl-3-understanding-kl-behavior-mode-covering-vs-mode-seeking" title="Permanent link">¶</a></h2>
<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>
<h3 id="informationtheory-3_kl-forward-kl-d_textklpq">Forward KL: <span class="arithmatex">\(D_{\text{KL}}(p\|q)\)</span><a class="headerlink" href="#informationtheory-3_kl-forward-kl-d_textklpq" title="Permanent link">¶</a></h3>
<p><em>(Used in maximum likelihood, flows → mode covering)</em></p>
<p>Forward KL asks whether the model <span class="arithmatex">\(q\)</span> assigns sufficient probability wherever the data distribution <span class="arithmatex">\(p\)</span> has mass:</p>
<blockquote>
<p>“Does the model assign enough probability to every place where the data occurs?”</p>
</blockquote>
<p>If <span class="arithmatex">\(q\)</span> misses even a small region where <span class="arithmatex">\(p\)</span> has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p>
<p>Result: <strong>mode covering</strong><br>
The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p>
<p>Flows and MLE-based models display this behavior.</p>
<h3 id="informationtheory-3_kl-reverse-kl-d_textklqp">Reverse KL: <span class="arithmatex">\(D_{\text{KL}}(q\|p)\)</span><a class="headerlink" href="#informationtheory-3_kl-reverse-kl-d_textklqp" title="Permanent link">¶</a></h3>
<p><em>(Used in VI, VAEs, GAN-like behavior → mode seeking)</em></p>
<p>Reverse KL asks the opposite question:</p>
<blockquote>
<p>“Is the model placing probability in places where the data distribution is very small or zero?”</p>
</blockquote>
<p>Reverse KL heavily penalizes placing mass in low-density regions of <span class="arithmatex">\(p\)</span>, making the model conservative.</p>
<p>Result: <strong>mode seeking</strong><br>
The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p>
<p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>
<h2 id="informationtheory-3_kl-4-f-divergences-a-unified-family-of-divergences">4. f-Divergences: A Unified Family of Divergences<a class="headerlink" href="#informationtheory-3_kl-4-f-divergences-a-unified-family-of-divergences" title="Permanent link">¶</a></h2>
<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function <span class="arithmatex">\(f\)</span>:</p>
<div class="arithmatex">\[
D_f(p\|q) = \sum_x q(x)\, f\!\left(\frac{p(x)}{q(x)}\right).
\]</div>
<h2 id="informationtheory-3_kl-5-jensenshannon-divergence-the-original-gan-divergence">5. Jensen–Shannon Divergence: The Original GAN Divergence<a class="headerlink" href="#informationtheory-3_kl-5-jensenshannon-divergence-the-original-gan-divergence" title="Permanent link">¶</a></h2>
<p>The Jensen–Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p>
<div class="arithmatex">\[
\text{JS}(p\|q)
= \frac12 D_{\text{KL}}(p\|m)
+ \frac12 D_{\text{KL}}(q\|m)
\]</div>
<p>where the mixture is:</p>
<div class="arithmatex">\[
m = \frac12(p+q).
\]</div>
<p>JS divergence is symmetric and always lies between 0 and <span class="arithmatex">\(\log 2\)</span>.</p>
<h3 id="informationtheory-3_kl-why-js-appears-in-gans">Why JS appears in GANs<a class="headerlink" href="#informationtheory-3_kl-why-js-appears-in-gans" title="Permanent link">¶</a></h3>
<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p>
<div class="arithmatex">\[
\text{JS}(p\|q) - \log 2.
\]</div>
<p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>
<h2 id="informationtheory-3_kl-6-why-js-divergence-causes-gan-instability">6. Why JS Divergence Causes GAN Instability<a class="headerlink" href="#informationtheory-3_kl-6-why-js-divergence-causes-gan-instability" title="Permanent link">¶</a></h2>
<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> are disjoint:</p>
<div class="arithmatex">\[
\text{JS}(p\|q) = \log 2.
\]</div>
<p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p>
<p>Consequences:</p>
<ol>
<li>The discriminator immediately becomes perfect.  </li>
<li>The generator stops receiving meaningful gradients.  </li>
<li>Training often collapses, oscillates, or diverges.  </li>
</ol>
<p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>
<h2 id="informationtheory-3_kl-7-total-variation-and-hellinger-distances">7. Total Variation and Hellinger Distances<a class="headerlink" href="#informationtheory-3_kl-7-total-variation-and-hellinger-distances" title="Permanent link">¶</a></h2>
<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>
<h3 id="informationtheory-3_kl-71-total-variation-tv-distance">7.1 Total Variation (TV) Distance<a class="headerlink" href="#informationtheory-3_kl-71-total-variation-tv-distance" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\text{TV}(p,q) = \frac12\sum_x |p(x)-q(x)|.
\]</div>
<p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform <span class="arithmatex">\(p\)</span> into <span class="arithmatex">\(q\)</span>.</p>
<p>Applications in ML:</p>
<ul>
<li>Robustness under distribution shift  </li>
<li>Generalization bounds (PAC-Bayes)  </li>
<li>Fairness and safety  </li>
</ul>
<h3 id="informationtheory-3_kl-72-hellinger-distance">7.2 Hellinger Distance<a class="headerlink" href="#informationtheory-3_kl-72-hellinger-distance" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
H^2(p,q)
= \frac12 \sum_x\left(\sqrt{p(x)} - \sqrt{q(x)}\right)^2.
\]</div>
<p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p>
<p>Uses in ML include:</p>
<ul>
<li>Robust statistics  </li>
<li>Domain adaptation  </li>
<li>Generalization theory  </li>
<li>Some GAN formulations  </li>
</ul>
<h2 id="informationtheory-3_kl-8-wasserstein-distance-geometry-of-probability-distributions">8. Wasserstein Distance: Geometry of Probability Distributions<a class="headerlink" href="#informationtheory-3_kl-8-wasserstein-distance-geometry-of-probability-distributions" title="Permanent link">¶</a></h2>
<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>
<h3 id="informationtheory-3_kl-81-primal-form-earth-mover-interpretation">8.1 Primal form (Earth Mover interpretation)<a class="headerlink" href="#informationtheory-3_kl-81-primal-form-earth-mover-interpretation" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
W(p,q)
= \inf_{\gamma \in \Gamma(p,q)}
\mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|].
\]</div>
<p>It seeks the transport plan <span class="arithmatex">\(\gamma\)</span> requiring the least expected effort to turn <span class="arithmatex">\(p\)</span> into <span class="arithmatex">\(q\)</span>.</p>
<h3 id="informationtheory-3_kl-82-dual-form-used-in-wgan">8.2 Dual form (used in WGAN)<a class="headerlink" href="#informationtheory-3_kl-82-dual-form-used-in-wgan" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
W(p,q)
= \sup_{\|f\|_L\le 1}
\left(\mathbb{E}_p[f(x)]
     - \mathbb{E}_q[f(x)]\right).
\]</div>
<p>GANs implement <span class="arithmatex">\(f\)</span> as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>
<h3 id="informationtheory-3_kl-83-why-wasserstein-solves-gan-instability">8.3 Why Wasserstein solves GAN instability<a class="headerlink" href="#informationtheory-3_kl-83-why-wasserstein-solves-gan-instability" title="Permanent link">¶</a></h3>
<p>Wasserstein distance has several advantages:</p>
<ul>
<li>Provides informative gradients even with no overlap  </li>
<li>Reflects the actual geometry of the data space  </li>
<li>Avoids the saturation and vanishing gradients of JS divergence  </li>
<li>Works reliably in high-dimensional spaces  </li>
</ul>
<p>These properties make Wasserstein GANs far more stable than classical GANs.</p>
<h3 id="informationtheory-3_kl-84-wgan-gp-gradient-penalty">8.4 WGAN-GP: Gradient Penalty<a class="headerlink" href="#informationtheory-3_kl-84-wgan-gp-gradient-penalty" title="Permanent link">¶</a></h3>
<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p>
<div class="arithmatex">\[
\lambda(\|\nabla_x f(x)\|_2 - 1)^2.
\]</div>
<p>This produces smoother and more stable training compared to weight clipping.</p>
<h2 id="informationtheory-3_kl-9-divergence-versus-distance">9. Divergence versus Distance<a class="headerlink" href="#informationtheory-3_kl-9-divergence-versus-distance" title="Permanent link">¶</a></h2>
<p>Divergences such as KL and JS:</p>
<ul>
<li>may be infinite  </li>
<li>are asymmetric  </li>
<li>do not behave well when distributions have disjoint support  </li>
</ul>
<p>Distances such as Wasserstein, TV, and Hellinger:</p>
<ul>
<li>are symmetric  </li>
<li>obey triangle inequality  </li>
<li>remain meaningful under distribution shift  </li>
</ul>
<p>In machine learning:</p>
<ul>
<li>Divergences are useful for inference and likelihood  </li>
<li>Distances are useful for generative modeling and geometry  </li>
</ul>
<h2 id="informationtheory-3_kl-10-why-divergences-fail-in-high-dimensions">10. Why Divergences Fail in High Dimensions<a class="headerlink" href="#informationtheory-3_kl-10-why-divergences-fail-in-high-dimensions" title="Permanent link">¶</a></h2>
<p>In high-dimensional spaces:</p>
<ul>
<li>Real and generated samples rarely overlap  </li>
<li>KL divergence often becomes infinite  </li>
<li>JS divergence becomes flat  </li>
<li>Gradients vanish  </li>
</ul>
<p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p>
<hr>
<p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p>
<p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen–Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p>
<p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p></body></html></section><section class="print-page" id="informationtheory-4_bayes" heading-number="6.4"><h1 id="informationtheory-4_bayes-informationtheory-4_bayes">4. Bayesian Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p>
<p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>
<h2 id="informationtheory-4_bayes-1-bayes-rule">1. Bayes’ Rule<a class="headerlink" href="#informationtheory-4_bayes-1-bayes-rule" title="Permanent link">¶</a></h2>
<p>Bayes’ theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable <span class="arithmatex">\(z\)</span> and an observed variable <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x|z)\,p(z)}{p(x)}.
\]</div>
<p>Each term has a clear interpretation.</p>
<ul>
<li><span class="arithmatex">\(p(z)\)</span>: prior belief about the unknown variable  </li>
<li><span class="arithmatex">\(p(x|z)\)</span>: likelihood of observing <span class="arithmatex">\(x\)</span> given <span class="arithmatex">\(z\)</span>  </li>
<li><span class="arithmatex">\(p(x)\)</span>: marginal likelihood or evidence  </li>
<li><span class="arithmatex">\(p(z|x)\)</span>: posterior distribution after observing data  </li>
</ul>
<p>Bayesian inference is the task of computing <span class="arithmatex">\(p(z|x)\)</span>.</p>
<h2 id="informationtheory-4_bayes-2-priors-encoding-assumptions-about-hidden-variables">2. Priors: Encoding Assumptions About Hidden Variables<a class="headerlink" href="#informationtheory-4_bayes-2-priors-encoding-assumptions-about-hidden-variables" title="Permanent link">¶</a></h2>
<p>The prior <span class="arithmatex">\(p(z)\)</span> expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>
<h3 id="informationtheory-4_bayes-21-regularization">2.1 Regularization<a class="headerlink" href="#informationtheory-4_bayes-21-regularization" title="Permanent link">¶</a></h3>
<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields <span class="arithmatex">\(L_2\)</span> regularization.</p>
<h3 id="informationtheory-4_bayes-22-structural-assumptions">2.2 Structural assumptions<a class="headerlink" href="#informationtheory-4_bayes-22-structural-assumptions" title="Permanent link">¶</a></h3>
<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>
<h3 id="informationtheory-4_bayes-23-uncertainty">2.3 Uncertainty<a class="headerlink" href="#informationtheory-4_bayes-23-uncertainty" title="Permanent link">¶</a></h3>
<p>The prior makes explicit that before observing data, we do not know the true value of <span class="arithmatex">\(z\)</span>.</p>
<h3 id="informationtheory-4_bayes-24-generative-modeling">2.4 Generative modeling<a class="headerlink" href="#informationtheory-4_bayes-24-generative-modeling" title="Permanent link">¶</a></h3>
<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>
<h2 id="informationtheory-4_bayes-3-likelihood-connecting-latent-variables-to-observed-data">3. Likelihood: Connecting Latent Variables to Observed Data<a class="headerlink" href="#informationtheory-4_bayes-3-likelihood-connecting-latent-variables-to-observed-data" title="Permanent link">¶</a></h2>
<p>The likelihood <span class="arithmatex">\(p(x|z)\)</span> describes how the data are generated from latent causes. In many generative models:</p>
<ul>
<li><span class="arithmatex">\(z\)</span> represents latent structure  </li>
<li><span class="arithmatex">\(x\)</span> represents an image, time series, or text  </li>
<li><span class="arithmatex">\(p(x|z)\)</span> is parameterized by a neural network decoder  </li>
</ul>
<p>The likelihood term encourages the latent variable <span class="arithmatex">\(z\)</span> to explain the observed data.</p>
<h2 id="informationtheory-4_bayes-4-the-posterior-what-we-really-want-to-compute">4. The Posterior: What We Really Want to Compute<a class="headerlink" href="#informationtheory-4_bayes-4-the-posterior-what-we-really-want-to-compute" title="Permanent link">¶</a></h2>
<p>The goal of Bayesian inference is the posterior:</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x|z)p(z)}{p(x)}.
\]</div>
<p>The posterior expresses how our belief about <span class="arithmatex">\(z\)</span> changes after seeing <span class="arithmatex">\(x\)</span>. It incorporates both:</p>
<ul>
<li>prior knowledge  </li>
<li>evidence from data  </li>
</ul>
<p>Unfortunately, computing this posterior is usually intractable.</p>
<h2 id="informationtheory-4_bayes-5-why-exact-inference-is-hard">5. Why Exact Inference Is Hard<a class="headerlink" href="#informationtheory-4_bayes-5-why-exact-inference-is-hard" title="Permanent link">¶</a></h2>
<p>The denominator in Bayes’ rule is the marginal likelihood:</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz.
\]</div>
<p>This integral is often impossible to evaluate directly because:</p>
<ul>
<li>the latent space <span class="arithmatex">\(z\)</span> can be high-dimensional  </li>
<li>the joint distribution <span class="arithmatex">\(p(x,z)\)</span> may involve a complex neural network  </li>
<li>the integral has no analytic form  </li>
</ul>
<p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>
<h2 id="informationtheory-4_bayes-6-maximum-a-posteriori-map-vs-full-bayesian-inference">6. Maximum a Posteriori (MAP) vs Full Bayesian Inference<a class="headerlink" href="#informationtheory-4_bayes-6-maximum-a-posteriori-map-vs-full-bayesian-inference" title="Permanent link">¶</a></h2>
<p>There are two kinds of Bayesian computation.</p>
<h3 id="informationtheory-4_bayes-61-map-estimation">6.1 MAP estimation<a class="headerlink" href="#informationtheory-4_bayes-61-map-estimation" title="Permanent link">¶</a></h3>
<p>MAP finds the <em>single most likely</em> value of <span class="arithmatex">\(z\)</span>:</p>
<div class="arithmatex">\[
z_{\text{MAP}}
= \arg\max_z\, p(z|x).
\]</div>
<p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>
<h3 id="informationtheory-4_bayes-62-full-posterior-inference">6.2 Full posterior inference<a class="headerlink" href="#informationtheory-4_bayes-62-full-posterior-inference" title="Permanent link">¶</a></h3>
<p>The full posterior <span class="arithmatex">\(p(z|x)\)</span> describes a <em>distribution</em> over possible values of <span class="arithmatex">\(z\)</span>, reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>
<h2 id="informationtheory-4_bayes-7-bayesian-latent-variable-models">7. Bayesian Latent-Variable Models<a class="headerlink" href="#informationtheory-4_bayes-7-bayesian-latent-variable-models" title="Permanent link">¶</a></h2>
<p>Many generative models are Bayesian latent-variable models with:</p>
<ol>
<li>
<p>a prior over latent variables<br>
<script type="math/tex; mode=display">
   z \sim p(z)
   </script>
</p>
</li>
<li>
<p>a conditional likelihood<br>
<script type="math/tex; mode=display">
   x \sim p(x|z)
   </script>
</p>
</li>
<li>
<p>a posterior<br>
<script type="math/tex; mode=display">
   p(z|x)
   </script>
</p>
</li>
</ol>
<p>Examples include:</p>
<ul>
<li>VAEs  </li>
<li>mixture models  </li>
<li>topic models  </li>
<li>probabilistic PCA  </li>
<li>diffusion models (in a specific sense)  </li>
</ul>
<p>Bayesian inference is the foundation of these models.</p>
<h2 id="informationtheory-4_bayes-8-the-evidence-and-its-importance">8. The Evidence and Its Importance<a class="headerlink" href="#informationtheory-4_bayes-8-the-evidence-and-its-importance" title="Permanent link">¶</a></h2>
<p>The marginal likelihood, also called the evidence:</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz
\]</div>
<p>plays several roles:</p>
<ul>
<li>It normalizes the posterior.  </li>
<li>It evaluates how well a model explains data.  </li>
<li>It is used in Bayesian model comparison.  </li>
<li>Its logarithm appears in training objectives for VAEs and diffusion models.</li>
</ul>
<p>Maximizing evidence corresponds to learning a model that explains the data well.</p>
<h2 id="informationtheory-4_bayes-9-bayesian-interpretation-of-kl-divergence">9. Bayesian Interpretation of KL Divergence<a class="headerlink" href="#informationtheory-4_bayes-9-bayesian-interpretation-of-kl-divergence" title="Permanent link">¶</a></h2>
<p>KL divergence naturally appears when comparing an approximate posterior <span class="arithmatex">\(q(z|x)\)</span> with the true posterior <span class="arithmatex">\(p(z|x)\)</span>:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Minimizing this KL divergence means making the approximation <span class="arithmatex">\(q\)</span> as close as possible to the exact posterior.</p>
<p>This forms the basis of variational inference.</p>
<hr>
<h2 id="informationtheory-4_bayes-10-why-we-need-variational-inference">10. Why We Need Variational Inference<a class="headerlink" href="#informationtheory-4_bayes-10-why-we-need-variational-inference" title="Permanent link">¶</a></h2>
<p>Because the true posterior is intractable, we introduce a simpler distribution <span class="arithmatex">\(q(z|x)\)</span> and optimize it to approximate <span class="arithmatex">\(p(z|x)\)</span>.</p>
<p>We cannot compute:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x))
\]</div>
<p>directly, because <span class="arithmatex">\(p(z|x)\)</span> depends on <span class="arithmatex">\(p(x)\)</span>, which is the intractable integral.</p>
<p>Variational inference resolves this by rewriting <span class="arithmatex">\(\log p(x)\)</span> and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p>
<p>This is the topic of the next chapter.</p>
<hr>
<p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes’ rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p>
<p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p></body></html></section><section class="print-page" id="informationtheory-5_mc_intro" heading-number="6.5"><h1 id="informationtheory-5_mc_intro-informationtheory-5_mc_intro">5. Probability toolbox</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}, \qquad 
p(x) = \int p(x,z)\,dz.
\]</div>
<p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p>
<p>This section introduces four major families of methods:</p>
<ol>
<li>complete enumeration  </li>
<li>Laplace approximation  </li>
<li>Monte Carlo methods  </li>
<li>variational methods  </li>
</ol>
<p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>
<h2 id="informationtheory-5_mc_intro-1-complete-enumeration">1. Complete Enumeration<a class="headerlink" href="#informationtheory-5_mc_intro-1-complete-enumeration" title="Permanent link">¶</a></h2>
<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p>
<div class="arithmatex">\[
p(x) = \sum_z p(x,z)
\quad \text{or} \quad
p(x) = \int p(x,z)\,dz.
\]</div>
<p>This is feasible only when:</p>
<ul>
<li>the latent variable is low dimensional  </li>
<li>the domain is small or discrete  </li>
<li>the joint distribution has a simple closed form  </li>
</ul>
<p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>
<h2 id="informationtheory-5_mc_intro-2-laplace-approximation">2. Laplace Approximation<a class="headerlink" href="#informationtheory-5_mc_intro-2-laplace-approximation" title="Permanent link">¶</a></h2>
<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p>
<p>Given a posterior</p>
<div class="arithmatex">\[
p(z|x) \propto p(x,z),
\]</div>
<p>the Laplace approximation fits a Gaussian distribution</p>
<div class="arithmatex">\[
q(z|x) \approx \mathcal{N}(z_{\text{MAP}}, H^{-1}),
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(z_{\text{MAP}}\)</span> is the mode of <span class="arithmatex">\(p(z|x)\)</span>  </li>
<li><span class="arithmatex">\(H\)</span> is the Hessian of <span class="arithmatex">\(-\log p(z|x)\)</span> at the mode  </li>
</ul>
<p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>
<h2 id="informationtheory-5_mc_intro-3-monte-carlo-methods">3. Monte Carlo Methods<a class="headerlink" href="#informationtheory-5_mc_intro-3-monte-carlo-methods" title="Permanent link">¶</a></h2>
<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z|x)}[f(z)] 
\approx \frac{1}{N}\sum_{i=1}^N f(z_i),
\qquad z_i \sim p(z|x).
\]</div>
<p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p>
<p>Sampling strategies fall into two groups:</p>
<ul>
<li>independent sampling  </li>
<li>Markov chain–based sampling (MCMC)  </li>
</ul>
<p>The next chapter explains Monte Carlo and sampling methods in detail.</p>
<h2 id="informationtheory-5_mc_intro-4-variational-methods">4. Variational Methods<a class="headerlink" href="#informationtheory-5_mc_intro-4-variational-methods" title="Permanent link">¶</a></h2>
<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from <span class="arithmatex">\(p(z|x)\)</span>, we introduce a distribution <span class="arithmatex">\(q(z|x)\)</span> and optimize it to be close to the true posterior. The objective is to minimize</p>
<div class="arithmatex">\[
D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Because <span class="arithmatex">\(p(z|x)\)</span> is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\log p(x)
=
\mathcal{L}(x) + D_{\text{KL}}(q(z|x)\|p(z|x)).
\]</div>
<p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>
<h2 id="informationtheory-5_mc_intro-summary">Summary<a class="headerlink" href="#informationtheory-5_mc_intro-summary" title="Permanent link">¶</a></h2>
<p>Approximate inference methods can be understood as four major strategies:</p>
<ul>
<li>complete enumeration: exact but rarely feasible  </li>
<li>Laplace approximation: fast Gaussian approximation near the mode  </li>
<li>Monte Carlo methods: sampling-based numerical estimation  </li>
<li>variational methods: optimization-based posterior approximation  </li>
</ul>
<p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p></body></html></section><section class="print-page" id="informationtheory-6_mc" heading-number="6.6"><h1 id="informationtheory-6_mc-informationtheory-6_mc">6. Monte Carlo Methods</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p>
<p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>
<h2 id="informationtheory-6_mc-1-the-goal-of-sampling">1. The Goal of Sampling<a class="headerlink" href="#informationtheory-6_mc-1-the-goal-of-sampling" title="Permanent link">¶</a></h2>
<p>Many problems require computing expectations of the form</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)] = \int f(z)\,p(z)\,dz,
\]</div>
<p>or evaluating posterior quantities such as</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p>
<p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>
<h2 id="informationtheory-6_mc-2-independent-sampling">2. Independent Sampling<a class="headerlink" href="#informationtheory-6_mc-2-independent-sampling" title="Permanent link">¶</a></h2>
<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p>
<p>These methods work best when:</p>
<ul>
<li>sampling directly from <span class="arithmatex">\(p(z)\)</span> is tractable  </li>
<li>the distribution is low-dimensional  </li>
<li>the support is simple (e.g., Gaussian, uniform)  </li>
</ul>
<h3 id="informationtheory-6_mc-21-direct-sampling">2.1 Direct Sampling<a class="headerlink" href="#informationtheory-6_mc-21-direct-sampling" title="Permanent link">¶</a></h3>
<p>When the distribution has an invertible CDF <span class="arithmatex">\(F(z)\)</span>:</p>
<ol>
<li>sample <span class="arithmatex">\(u \sim \text{Uniform}(0,1)\)</span>  </li>
<li>compute <span class="arithmatex">\(z = F^{-1}(u)\)</span>  </li>
</ol>
<p>This yields exact samples. It is commonly used in:</p>
<ul>
<li>uniform sampling  </li>
<li>exponential distributions  </li>
<li>simple discrete distributions  </li>
</ul>
<h3 id="informationtheory-6_mc-22-importance-sampling">2.2 Importance Sampling<a class="headerlink" href="#informationtheory-6_mc-22-importance-sampling" title="Permanent link">¶</a></h3>
<p>When sampling from <span class="arithmatex">\(p(z)\)</span> is difficult but evaluating <span class="arithmatex">\(p(z)\)</span> is easy, importance sampling draws samples from a proposal distribution <span class="arithmatex">\(q(z)\)</span> and reweights them:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)]
=
\mathbb{E}_{q(z)}\left[f(z)\frac{p(z)}{q(z)}\right].
\]</div>
<p>Importance sampling is widely used for:</p>
<ul>
<li>likelihood estimation  </li>
<li>off-policy reinforcement learning  </li>
<li>correcting distribution mismatch  </li>
</ul>
<p>It suffers when <span class="arithmatex">\(p(z)/q(z)\)</span> has high variance.</p>
<h3 id="informationtheory-6_mc-23-rejection-sampling">2.3 Rejection Sampling<a class="headerlink" href="#informationtheory-6_mc-23-rejection-sampling" title="Permanent link">¶</a></h3>
<p>Rejection sampling uses a proposal distribution <span class="arithmatex">\(q(z)\)</span> and a constant <span class="arithmatex">\(M\)</span> such that</p>
<div class="arithmatex">\[
p(z) \le M q(z) \quad \text{for all } z.
\]</div>
<p>Procedure:</p>
<ol>
<li>sample <span class="arithmatex">\(z \sim q(z)\)</span>  </li>
<li>accept with probability <span class="arithmatex">\(\frac{p(z)}{M q(z)}\)</span>  </li>
</ol>
<p>It produces exact samples from <span class="arithmatex">\(p(z)\)</span>, but can be extremely inefficient in high dimensions.</p>
<h2 id="informationtheory-6_mc-3-monte-carlo-estimation">3. Monte Carlo Estimation<a class="headerlink" href="#informationtheory-6_mc-3-monte-carlo-estimation" title="Permanent link">¶</a></h2>
<p>Monte Carlo approximates expectations by:</p>
<div class="arithmatex">\[
\mathbb{E}_{p(z)}[f(z)] \approx \frac{1}{N}\sum_{i=1}^N f(z_i),
\quad z_i \sim p(z).
\]</div>
<p>Key properties:</p>
<ul>
<li>error scales as <span class="arithmatex">\(\mathcal{O}(1/\sqrt{N})\)</span>  </li>
<li>works in high dimensions  </li>
<li>accuracy depends on sampling quality  </li>
</ul>
<p>Monte Carlo is the backbone of almost all probabilistic computation.</p>
<h2 id="informationtheory-6_mc-4-markov-chain-monte-carlo-mcmc">4. Markov Chain Monte Carlo (MCMC)<a class="headerlink" href="#informationtheory-6_mc-4-markov-chain-monte-carlo-mcmc" title="Permanent link">¶</a></h2>
<p>When sampling directly from <span class="arithmatex">\(p(z)\)</span> is hard, Markov Chain Monte Carlo constructs a Markov chain</p>
<div class="arithmatex">\[
z_1 \to z_2 \to z_3 \to \cdots
\]</div>
<p>whose stationary distribution is <span class="arithmatex">\(p(z)\)</span>.</p>
<p>After a burn-in period, samples approximate <span class="arithmatex">\(p(z)\)</span> even if individual states are dependent.</p>
<p>MCMC is widely applicable because it does not require the normalization constant of <span class="arithmatex">\(p(z)\)</span>:</p>
<div class="arithmatex">\[
p(z|x) \propto p(x,z).
\]</div>
<h3 id="informationtheory-6_mc-41-metropolishastings-algorithm">4.1 Metropolis–Hastings Algorithm<a class="headerlink" href="#informationtheory-6_mc-41-metropolishastings-algorithm" title="Permanent link">¶</a></h3>
<p>Given the current state <span class="arithmatex">\(z\)</span>, propose <span class="arithmatex">\(z'\)</span> using a proposal distribution <span class="arithmatex">\(q(z'|z)\)</span>.<br>
Accept with probability:</p>
<div class="arithmatex">\[
\alpha = \min\left(1, 
\frac{p(z')\,q(z|z')}
     {p(z)\,q(z'|z)}
\right).
\]</div>
<p>If accepted, set <span class="arithmatex">\(z_{t+1} = z'\)</span>, otherwise keep <span class="arithmatex">\(z_{t+1} = z\)</span>.</p>
<p>Metropolis–Hastings forms the foundation for most MCMC methods.</p>
<h3 id="informationtheory-6_mc-42-gibbs-sampling">4.2 Gibbs Sampling<a class="headerlink" href="#informationtheory-6_mc-42-gibbs-sampling" title="Permanent link">¶</a></h3>
<p>Gibbs sampling updates one variable at a time by sampling from its conditional distribution:</p>
<div class="arithmatex">\[
z_i \sim p(z_i \mid z_{-i}).
\]</div>
<p>This requires all conditionals to be tractable.</p>
<p>Applications include:</p>
<ul>
<li>topic models (LDA)  </li>
<li>hidden Markov models  </li>
<li>Bayesian networks  </li>
</ul>
<h3 id="informationtheory-6_mc-43-slice-sampling">4.3 Slice Sampling<a class="headerlink" href="#informationtheory-6_mc-43-slice-sampling" title="Permanent link">¶</a></h3>
<p>Slice sampling chooses a height <span class="arithmatex">\(u\)</span> and samples uniformly along the slice:</p>
<div class="arithmatex">\[
\{z : p(z) &gt; u \}.
\]</div>
<p>It adapts automatically to the local shape of the distribution and requires minimal tuning.</p>
<h2 id="informationtheory-6_mc-5-reducing-random-walk-behaviour">5. Reducing Random-Walk Behaviour<a class="headerlink" href="#informationtheory-6_mc-5-reducing-random-walk-behaviour" title="Permanent link">¶</a></h2>
<p>Basic MCMC methods suffer from slow exploration due to random-walk behavior.<br>
Advanced methods reduce this inefficiency.</p>
<h3 id="informationtheory-6_mc-51-hamiltonian-monte-carlo-hmc">5.1 Hamiltonian Monte Carlo (HMC)<a class="headerlink" href="#informationtheory-6_mc-51-hamiltonian-monte-carlo-hmc" title="Permanent link">¶</a></h3>
<p>HMC introduces momentum variables and uses Hamiltonian dynamics to propose long-distance moves with high acceptance probability.</p>
<p>Advantages:</p>
<ul>
<li>avoids random walk behaviour  </li>
<li>efficient in high dimensions  </li>
<li>uses gradients of <span class="arithmatex">\(\log p(z)\)</span>  </li>
</ul>
<p>HMC is widely used in probabilistic programming systems (Stan, PyMC).</p>
<h3 id="informationtheory-6_mc-52-overrelaxation">5.2 Overrelaxation<a class="headerlink" href="#informationtheory-6_mc-52-overrelaxation" title="Permanent link">¶</a></h3>
<p>Overrelaxation proposes samples that are negatively correlated with the previous ones, improving mixing speed.</p>
<hr>
<p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p>
<p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p></body></html></section><section class="print-page" id="informationtheory-7a_vi_intro" heading-number="6.7"><h1 id="informationtheory-7a_vi_intro-informationtheory-7a_vi_intro">8. Optimization-Based Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="chapter-optimization-based-inference-map-em-and-the-path-to-variational-inference">Chapter — Optimization-Based Inference: MAP, EM, and the Path to Variational Inference<a class="headerlink" href="#informationtheory-7a_vi_intro-chapter-optimization-based-inference-map-em-and-the-path-to-variational-inference" title="Permanent link">¶</a></h2>
<p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p>
<p>This chapter introduces three optimization-based inference strategies:</p>
<ol>
<li>Maximum a posteriori (MAP) estimation  </li>
<li>Expectation–Maximization (EM)  </li>
<li>Variational inference (VI), in its simplest introductory form  </li>
</ol>
<p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-1-motivation-for-optimization-based-inference">1. Motivation for Optimization-Based Inference<a class="headerlink" href="#informationtheory-7a_vi_intro-1-motivation-for-optimization-based-inference" title="Permanent link">¶</a></h2>
<p>Bayesian inference requires the posterior</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>The challenge lies in computing the marginal likelihood</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz,
\]</div>
<p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p>
<ul>
<li>high-dimensional latent spaces  </li>
<li>multimodal posteriors  </li>
<li>large datasets  </li>
<li>models requiring gradient-based learning  </li>
</ul>
<p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-2-maximum-a-posteriori-map-estimation">2. Maximum A Posteriori (MAP) Estimation<a class="headerlink" href="#informationtheory-7a_vi_intro-2-maximum-a-posteriori-map-estimation" title="Permanent link">¶</a></h2>
<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes’ rule:</p>
<div class="arithmatex">\[
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)},
\]</div>
<p>MAP chooses the mode of the posterior:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= \arg\max_\theta p(\theta|x).
\]</div>
<p>Since <span class="arithmatex">\(p(x)\)</span> does not depend on <span class="arithmatex">\(\theta\)</span>, this is equivalent to:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= \arg\max_\theta \big[ \log p(x|\theta) + \log p(\theta) \big].
\]</div>
<p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through <span class="arithmatex">\(p(\theta)\)</span>. However, it returns only a point estimate and does not capture uncertainty.</p>
<p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-3-expectationmaximization-em">3. Expectation–Maximization (EM)<a class="headerlink" href="#informationtheory-7a_vi_intro-3-expectationmaximization-em" title="Permanent link">¶</a></h2>
<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p>
<div class="arithmatex">\[
\log p_\theta(x) 
= \log \sum_z p_\theta(x,z).
\]</div>
<p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>
<h3 id="informationtheory-7a_vi_intro-e-step">E-step<a class="headerlink" href="#informationtheory-7a_vi_intro-e-step" title="Permanent link">¶</a></h3>
<p>Compute the posterior over latent variables under the current parameters:</p>
<div class="arithmatex">\[
q(z) = p_\theta(z|x).
\]</div>
<h3 id="informationtheory-7a_vi_intro-m-step">M-step<a class="headerlink" href="#informationtheory-7a_vi_intro-m-step" title="Permanent link">¶</a></h3>
<p>Maximize the expected complete-data log-likelihood:</p>
<div class="arithmatex">\[
\theta \leftarrow 
\arg\max_\theta 
\mathbb{E}_{q(z)}[\log p_\theta(x,z)].
\]</div>
<p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p>
<ul>
<li>mixture of Gaussians  </li>
<li>hidden Markov models  </li>
<li>probabilistic PCA  </li>
<li>clustering and density estimation  </li>
</ul>
<p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior <span class="arithmatex">\(q(z) = p_\theta(z|x)\)</span>.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-4-em-and-map-map-em">4. EM and MAP: MAP-EM<a class="headerlink" href="#informationtheory-7a_vi_intro-4-em-and-map-map-em" title="Permanent link">¶</a></h2>
<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p>
<div class="arithmatex">\[
\theta_{\text{MAP}} 
= 
\arg\max_\theta 
\left[
\mathbb{E}_{p(z|x,\theta)}[\log p(x,z|\theta)]
+ \log p(\theta)
\right].
\]</div>
<p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-5-limitations-of-map-and-em">5. Limitations of MAP and EM<a class="headerlink" href="#informationtheory-7a_vi_intro-5-limitations-of-map-and-em" title="Permanent link">¶</a></h2>
<p>Both MAP and EM have limitations that motivate more general methods:</p>
<ol>
<li>MAP returns only a point estimate and discards posterior uncertainty.  </li>
<li>EM requires exact posterior computation in the E-step:
   <script type="math/tex; mode=display">
   q(z) = p_\theta(z|x),
   </script>
   which is often intractable.  </li>
<li>EM struggles with:</li>
<li>multimodal posteriors  </li>
<li>high-dimensional latent spaces  </li>
<li>arbitrary likelihood forms  </li>
</ol>
<p>These limitations lead naturally to variational inference.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-6-a-brief-introduction-to-variational-inference-vi">6. A Brief Introduction to Variational Inference (VI)<a class="headerlink" href="#informationtheory-7a_vi_intro-6-a-brief-introduction-to-variational-inference-vi" title="Permanent link">¶</a></h2>
<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p>
<div class="arithmatex">\[
q(z) = p_\theta(z|x),
\]</div>
<p>VI chooses a family of distributions</p>
<div class="arithmatex">\[
q_\phi(z|x) \in \mathcal{Q}
\]</div>
<p>and optimizes it to be close to the true posterior. The objective is:</p>
<div class="arithmatex">\[
\phi^* = 
\arg\min_\phi D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Because <span class="arithmatex">\(p(z|x)\)</span> contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\log p(x)
=
\mathcal{L}(x;\phi,\theta)
+
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p>
<p>VI:</p>
<ul>
<li>generalizes MAP (when <span class="arithmatex">\(q\)</span> is a delta function)  </li>
<li>generalizes EM (when <span class="arithmatex">\(q = p_\theta(z|x)\)</span>)  </li>
<li>supports flexible approximations  </li>
<li>scales to large datasets  </li>
<li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li>
</ul>
<p>The next chapter explores variational inference in detail.</p>
<hr>
<h2 id="informationtheory-7a_vi_intro-7-summary-of-the-chapter">7. Summary of the Chapter<a class="headerlink" href="#informationtheory-7a_vi_intro-7-summary-of-the-chapter" title="Permanent link">¶</a></h2>
<p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p>
<p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p>
<p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p></body></html></section><section class="print-page" id="informationtheory-7b_vi" heading-number="6.8"><h1 id="informationtheory-7b_vi-informationtheory-7b_vi">7. Variatonal Inference</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Variational inference (VI) is a fundamental technique in modern machine learning. It provides a way to approximate complicated probability distributions with simpler, tractable ones. VAEs, Bayesian neural networks, probabilistic latent-variable models, and diffusion models all rely on variational inference.</p>
<p>This chapter explains VI step by step, beginning with the inference problem itself, then deriving the Evidence Lower Bound (ELBO), and finally connecting these ideas to VAEs and deep learning.</p>
<h2 id="informationtheory-7b_vi-1-the-problem-of-inference">1. The Problem of Inference<a class="headerlink" href="#informationtheory-7b_vi-1-the-problem-of-inference" title="Permanent link">¶</a></h2>
<p>Many machine-learning models introduce hidden variables to explain observed data. For example:</p>
<ul>
<li>A VAE introduces latent variables <span class="arithmatex">\(z\)</span> describing an image <span class="arithmatex">\(x\)</span>.  </li>
<li>A Bayesian neural network introduces weight distributions.  </li>
<li>Mixture models introduce cluster assignments.  </li>
</ul>
<p>In each case, the goal is to understand the posterior distribution:</p>
<div class="arithmatex">\[
p(z|x) = \frac{p(x,z)}{p(x)}.
\]</div>
<p>The difficulty is the denominator:</p>
<div class="arithmatex">\[
p(x) = \int p(x,z)\,dz,
\]</div>
<p>which is often impossible to compute exactly because the integral spans a complicated, high-dimensional space. Because computing the true posterior is intractable, we approximate it.</p>
<h2 id="informationtheory-7b_vi-2-the-idea-of-variational-inference">2. The Idea of Variational Inference<a class="headerlink" href="#informationtheory-7b_vi-2-the-idea-of-variational-inference" title="Permanent link">¶</a></h2>
<p>Variational inference turns inference into an optimization problem. Instead of trying to compute <span class="arithmatex">\(p(z|x)\)</span> exactly, we choose a simpler, tractable family of distributions <span class="arithmatex">\(q_\phi(z|x)\)</span> and try to make it as close as possible to the true posterior.</p>
<p>This is done by minimizing the KL divergence:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>However, since <span class="arithmatex">\(p(z|x)\)</span> is unknown (that was the original problem), we cannot compute this KL divergence directly. Instead, VI reformulates the problem using quantities we <em>can</em> compute.</p>
<h2 id="informationtheory-7b_vi-3-deriving-the-elbo">3. Deriving the ELBO<a class="headerlink" href="#informationtheory-7b_vi-3-deriving-the-elbo" title="Permanent link">¶</a></h2>
<p>We begin with the log marginal likelihood:</p>
<div class="arithmatex">\[
\log p(x) = \log \int p(x,z)\,dz.
\]</div>
<p>We introduce the variational distribution <span class="arithmatex">\(q_\phi(z|x)\)</span> and use the identity:</p>
<div class="arithmatex">\[
\log p(x)
= \mathbb{E}_{q_\phi(z|x)}\left[
\log\frac{p(x,z)}{q_\phi(z|x)}
\right]
+ D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>Rearranging gives:</p>
<div class="arithmatex">\[
\log p(x)
= \mathcal{L}(x;\phi,\theta)
+ D_{\text{KL}}(q_\phi(z|x)\|p(z|x)),
\]</div>
<p>where</p>
<div class="arithmatex">\[
\mathcal{L}(x;\phi,\theta)
=
\mathbb{E}_{q_\phi(z|x)}
\left[\log p_\theta(x|z)\right]
- D_{\text{KL}}(q_\phi(z|x)\|p(z)).
\]</div>
<p>Since the KL divergence is non-negative, we have the Evidence Lower Bound (ELBO):</p>
<div class="arithmatex">\[
\mathcal{L}(x;\phi,\theta) \le \log p(x).
\]</div>
<p>Maximizing the ELBO minimizes the divergence between the approximate and true posteriors. Variational inference becomes an optimization problem.</p>
<h2 id="informationtheory-7b_vi-4-interpreting-the-elbo">4. Interpreting the ELBO<a class="headerlink" href="#informationtheory-7b_vi-4-interpreting-the-elbo" title="Permanent link">¶</a></h2>
<p>The ELBO consists of two terms:</p>
<h3 id="informationtheory-7b_vi-reconstruction-term">Reconstruction term<a class="headerlink" href="#informationtheory-7b_vi-reconstruction-term" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
\]</div>
<p>This ensures that the latent variable <span class="arithmatex">\(z\)</span> contains enough information to reconstruct <span class="arithmatex">\(x\)</span>. It corresponds to reconstruction accuracy in VAEs and log-likelihood in generative models.</p>
<h3 id="informationtheory-7b_vi-regularization-term">Regularization term<a class="headerlink" href="#informationtheory-7b_vi-regularization-term" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z))
\]</div>
<p>This forces the approximate posterior to stay close to the prior. In VAEs, <span class="arithmatex">\(p(z)\)</span> is usually a standard Gaussian, so this term pushes the latent codes to be smooth and structured.</p>
<h3 id="informationtheory-7b_vi-interpretation">Interpretation<a class="headerlink" href="#informationtheory-7b_vi-interpretation" title="Permanent link">¶</a></h3>
<p>The reconstruction term encourages expressiveness, while the KL term encourages simplicity. Together, they balance the trade-off between data fidelity and model complexity.</p>
<h2 id="informationtheory-7b_vi-5-why-vi-uses-reverse-kl">5. Why VI Uses Reverse KL<a class="headerlink" href="#informationtheory-7b_vi-5-why-vi-uses-reverse-kl" title="Permanent link">¶</a></h2>
<p>Variational inference minimizes:</p>
<div class="arithmatex">\[
D_{\text{KL}}(q_\phi(z|x)\|p(z|x)).
\]</div>
<p>This is reverse KL, which behaves conservatively:</p>
<ul>
<li>It avoids placing mass in regions where <span class="arithmatex">\(p(z|x)\)</span> is small.  </li>
<li>It prefers single high-density modes.  </li>
<li>It is willing to ignore alternative modes of the posterior.  </li>
</ul>
<p>As a result, VI tends to produce mode-seeking solutions, which explains why VAEs sometimes prefer “safe,” blurry reconstructions rather than sharp, multimodal samples. This behavior contrasts with forward KL, which tries to cover all modes.</p>
<h2 id="informationtheory-7b_vi-6-variational-autoencoders-vaes">6. Variational Autoencoders (VAEs)<a class="headerlink" href="#informationtheory-7b_vi-6-variational-autoencoders-vaes" title="Permanent link">¶</a></h2>
<p>VAEs are a practical deep-learning implementation of variational inference. They combine:</p>
<ul>
<li>a generative model <span class="arithmatex">\(p_\theta(x|z)\)</span>  </li>
<li>an approximate posterior <span class="arithmatex">\(q_\phi(z|x)\)</span>  </li>
<li>a prior distribution <span class="arithmatex">\(p(z)\)</span>  </li>
</ul>
<p>The VAE is trained by maximizing the ELBO over the dataset.</p>
<h3 id="informationtheory-7b_vi-61-the-generative-model">6.1 The generative model<a class="headerlink" href="#informationtheory-7b_vi-61-the-generative-model" title="Permanent link">¶</a></h3>
<p>A latent variable <span class="arithmatex">\(z\)</span> is sampled from a prior:</p>
<div class="arithmatex">\[
z \sim p(z),
\]</div>
<p>and an observation <span class="arithmatex">\(x\)</span> is generated from the decoder:</p>
<div class="arithmatex">\[
x \sim p_\theta(x|z).
\]</div>
<h3 id="informationtheory-7b_vi-62-the-inference-model">6.2 The inference model<a class="headerlink" href="#informationtheory-7b_vi-62-the-inference-model" title="Permanent link">¶</a></h3>
<p>A neural network encoder approximates the posterior:</p>
<div class="arithmatex">\[
q_\phi(z|x) \approx p(z|x),
\]</div>
<p>typically using a Gaussian distribution whose mean and variance depend on <span class="arithmatex">\(x\)</span>.</p>
<h3 id="informationtheory-7b_vi-63-training-objective">6.3 Training objective<a class="headerlink" href="#informationtheory-7b_vi-63-training-objective" title="Permanent link">¶</a></h3>
<p>The VAE maximizes the ELBO:</p>
<div class="arithmatex">\[
\mathcal{L}
=
\mathbb{E}_{q_\phi(z|x)}
[\log p_\theta(x|z)]
-
D_{\text{KL}}(q_\phi(z|x)\|p(z)).
\]</div>
<p>The first term encourages accurate reconstruction; the second encourages structured latent representations.</p>
<h2 id="informationtheory-7b_vi-7-the-reparameterization-trick">7. The Reparameterization Trick<a class="headerlink" href="#informationtheory-7b_vi-7-the-reparameterization-trick" title="Permanent link">¶</a></h2>
<p>To compute gradients through the expectation:</p>
<div class="arithmatex">\[
z \sim q_\phi(z|x),
\]</div>
<p>VAEs use the reparameterization:</p>
<div class="arithmatex">\[
z = \mu_\phi(x) + \sigma_\phi(x)\odot\epsilon,
\qquad \epsilon\sim\mathcal{N}(0,I).
\]</div>
<p>This converts sampling into a differentiable transformation, enabling backpropagation.</p>
<h2 id="informationtheory-7b_vi-8-consequences-of-reverse-kl-in-vaes">8. Consequences of Reverse KL in VAEs<a class="headerlink" href="#informationtheory-7b_vi-8-consequences-of-reverse-kl-in-vaes" title="Permanent link">¶</a></h2>
<p>The use of reverse KL causes VAEs to produce:</p>
<ul>
<li>smooth, conservative outputs  </li>
<li>blurry reconstructions in image models  </li>
<li>latent spaces with clear structure  </li>
<li>stable training dynamics  </li>
</ul>
<p>Mode-seeking behavior is not always ideal, which is why researchers develop:</p>
<ul>
<li><span class="arithmatex">\(\beta\)</span>-VAEs (more control over KL term)  </li>
<li>hierarchical VAEs  </li>
<li>richer posterior distributions  </li>
</ul>
<p>These extensions aim to improve sample quality or increase flexibility.</p>
<h2 id="informationtheory-7b_vi-9-vi-beyond-vaes">9. VI Beyond VAEs<a class="headerlink" href="#informationtheory-7b_vi-9-vi-beyond-vaes" title="Permanent link">¶</a></h2>
<p>Variational inference is far broader than VAEs.</p>
<h3 id="informationtheory-7b_vi-bayesian-neural-networks">Bayesian neural networks<a class="headerlink" href="#informationtheory-7b_vi-bayesian-neural-networks" title="Permanent link">¶</a></h3>
<p>Approximate weight posteriors:</p>
<div class="arithmatex">\[
q(w)\approx p(w|D).
\]</div>
<h3 id="informationtheory-7b_vi-diffusion-models">Diffusion models<a class="headerlink" href="#informationtheory-7b_vi-diffusion-models" title="Permanent link">¶</a></h3>
<p>Have a variational interpretation through score matching and likelihood bounds.</p>
<h3 id="informationtheory-7b_vi-normalizing-flows">Normalizing flows<a class="headerlink" href="#informationtheory-7b_vi-normalizing-flows" title="Permanent link">¶</a></h3>
<p>Can be used to create more expressive variational posteriors.</p>
<h3 id="informationtheory-7b_vi-reinforcement-learning">Reinforcement learning<a class="headerlink" href="#informationtheory-7b_vi-reinforcement-learning" title="Permanent link">¶</a></h3>
<p>Entropy-regularized RL can be derived using variational principles.</p>
<p>VI provides a general framework for approximating intractable distributions.</p>
<hr>
<p>Variational inference transforms posterior inference into an optimization problem by introducing a tractable distribution <span class="arithmatex">\(q_\phi(z|x)\)</span> and maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, revealing a trade-off between accuracy and simplicity. VAEs apply this framework in deep learning by parameterizing both the generative model and the approximate posterior with neural networks.</p>
<p>Reverse KL divergence drives the behavior of VI and explains why VAEs tend to produce smooth, conservative samples. The concepts developed here provide the foundation for understanding probabilistic deep learning, posterior approximations, representation learning, and modern generative models.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-7' heading-number='7'>
                        <h1>Cheat Sheets<a class='headerlink' href='#section-7' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="cheatsheets-20a_cheatsheet" heading-number="7.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="comprehensive-optimization-algorithm-cheat-sheet">Comprehensive Optimization Algorithm Cheat Sheet<a class="headerlink" href="#cheatsheets-20a_cheatsheet-comprehensive-optimization-algorithm-cheat-sheet" title="Permanent link">¶</a></h1>
<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search.<br>
It balances <strong>theoretical precision</strong> with <strong>practical intuition</strong>—from gradient-based solvers to black-box evolutionary methods.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-how-to-read-this-table">🧭 How to Read This Table<a class="headerlink" href="#cheatsheets-20a_cheatsheet-how-to-read-this-table" title="Permanent link">¶</a></h2>
<p>Each method lists:
- <strong>Problem Type</strong> — the class of objectives it applies to.
- <strong>Assumptions</strong> — smoothness, convexity, or structural conditions.
- <strong>Core Update Rule</strong> — canonical iteration.
- <strong>Scalability</strong> — computational feasibility.
- <strong>Per-Iteration Cost</strong> — approximate computational complexity.
- <strong>Applications</strong> — typical ML or engineering use cases.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-first-order-methods">🚀 First-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-first-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent (GD)</td>
<td>Unconstrained smooth (convex/nonconvex)</td>
<td>Differentiable; <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Nesterov’s Accelerated GD</td>
<td>Smooth convex (fast rate)</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(y_k = x_k + \frac{k-1}{k+2}(x_k - x_{k-1})\)</span>; <span class="arithmatex">\(x_{k+1} = y_k - \eta \nabla f(y_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Accelerated convex models</td>
</tr>
<tr>
<td>(Polyak) Heavy-Ball Momentum</td>
<td>Unconstrained smooth</td>
<td>Differentiable, <span class="arithmatex">\(\beta \in (0,1)\)</span></td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k) + \beta(x_k - x_{k-1})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Deep networks, convex smooth losses</td>
</tr>
<tr>
<td>Conjugate Gradient (CG)</td>
<td>Quadratic or linear systems <span class="arithmatex">\(Ax=b\)</span></td>
<td><span class="arithmatex">\(A\)</span> symmetric positive definite</td>
<td><span class="arithmatex">\(p_{k+1}=r_{k+1}+\beta_k p_k\)</span>, <span class="arithmatex">\(x_{k+1}=x_k+\alpha_k p_k\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Large-scale least squares, implicit Newton steps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td>Non-Euclidean geometry</td>
<td>Convex; mirror map <span class="arithmatex">\(\psi\)</span> strongly convex</td>
<td><span class="arithmatex">\(x_{k+1} = \nabla \psi^*(\nabla \psi(x_k) - \eta \nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Probability simplex, online learning</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Conjugate Gradient (CG)</em> bridges first- and second-order methods: it achieves exact convergence in at most <span class="arithmatex">\(d\)</span> steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-second-order-methods">⚙️ Second-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-second-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Newton’s Method</td>
<td>Smooth convex</td>
<td>Twice differentiable; <span class="arithmatex">\(\nabla^2 f(x)\)</span> PD</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1}\nabla f(x_k)\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>Logistic regression (IRLS), convex solvers</td>
</tr>
<tr>
<td>BFGS / L-BFGS</td>
<td>Smooth convex</td>
<td>Differentiable, approximate Hessian</td>
<td>Solve <span class="arithmatex">\(B_k p_k=-\nabla f(x_k)\)</span>; update <span class="arithmatex">\(B_k\)</span> via secant rule</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>GLMs, medium ML models</td>
</tr>
<tr>
<td>Trust-Region</td>
<td>Smooth convex/nonconvex</td>
<td>Twice differentiable</td>
<td><span class="arithmatex">\(\min_p \tfrac{1}{2}p^\top \nabla^2 f(x_k)p + \nabla f(x_k)^\top p\)</span> s.t. <span class="arithmatex">\(\|p\|\le\Delta_k\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>TRPO, physics-based ML</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods">🧮 Proximal, Projected &amp; Splitting Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA)</td>
<td>Composite <span class="arithmatex">\(f=g+h\)</span></td>
<td><span class="arithmatex">\(g\)</span> smooth, <span class="arithmatex">\(h\)</span> convex</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\alpha h}(x_k-\alpha\nabla g(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>LASSO, sparse recovery</td>
</tr>
<tr>
<td>FISTA</td>
<td>Same as ISTA</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(g\)</span></td>
<td>Like ISTA with momentum</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Compressed sensing</td>
</tr>
<tr>
<td>Projected Gradient (PG)</td>
<td>Convex constrained</td>
<td><span class="arithmatex">\(f\)</span> smooth; easy projection</td>
<td><span class="arithmatex">\(x_{k+1}=\Pi_C(x_k-\eta\nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> + projection</td>
<td>Box/simplex constraints</td>
</tr>
<tr>
<td>ADMM</td>
<td>Separable convex + linear constraints</td>
<td><span class="arithmatex">\(f,g\)</span> convex</td>
<td>Alternating minimization + dual update</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> per block</td>
<td>Distributed ML, consensus</td>
</tr>
<tr>
<td>Majorization–Minimization (MM)</td>
<td>Convex/nonconvex</td>
<td>$g(x</td>
<td>x_k)\ge f(x)$</td>
<td>$x_{k+1}=\arg\min g(x</td>
<td>x_k)$</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-coordinate-block-methods">🧩 Coordinate &amp; Block Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-coordinate-block-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent (CD)</td>
<td>Separable convex</td>
<td>Convex, differentiable</td>
<td>Update one coordinate: <span class="arithmatex">\(x_{i}^{k+1}=x_i^k-\eta\partial_i f(x^k)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(d)\)</span></td>
<td>LASSO, SVM duals</td>
</tr>
<tr>
<td>Block Coordinate Descent (BCD)</td>
<td>Block separable</td>
<td>Convex per block</td>
<td>Minimize over <span class="arithmatex">\(x^{(j)}\)</span> while fixing others</td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd_j)\)</span></td>
<td>Matrix factorization, alternating minimization</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods">🎲 Stochastic &amp; Mini-Batch Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stochastic Gradient Descent (SGD)</td>
<td>Large-scale / streaming</td>
<td>Unbiased stochastic gradients</td>
<td><span class="arithmatex">\(x_{k+1}=x_k-\eta_t\nabla f_{i_k}(x_k)\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Deep learning, online learning</td>
</tr>
<tr>
<td>Variance-Reduced (SVRG/SAGA/SARAH)</td>
<td>Finite-sum convex</td>
<td>Smooth, strongly convex</td>
<td><span class="arithmatex">\(v_k=\nabla f_{i_k}(x_k)-\nabla f_{i_k}(\tilde{x})+\nabla f(\tilde{x})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Logistic regression, GLMs</td>
</tr>
<tr>
<td>Adaptive SGD (Adam/RMSProp/Adagrad)</td>
<td>Nonconvex stochastic</td>
<td>Bounded variance</td>
<td><span class="arithmatex">\(m_k=\beta_1m_{k-1}+(1-\beta_1)g_k\)</span>, <span class="arithmatex">\(v_k=\beta_2v_{k-1}+(1-\beta_2)g_k^2\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Neural networks</td>
</tr>
<tr>
<td>Proximal Stochastic (Prox-SGD / Prox-SAGA)</td>
<td>Nonsmooth stochastic</td>
<td><span class="arithmatex">\(f=g+h\)</span> with prox of <span class="arithmatex">\(h\)</span> known</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\eta h}(x_k-\eta\widehat{\nabla g}(x_k))\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Sparse online learning</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-interior-point-augmented-methods">🧱 Interior-Point &amp; Augmented Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-interior-point-augmented-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interior-Point</td>
<td>Convex with inequalities</td>
<td>Slater’s condition, self-concordant barrier</td>
<td>Solve <span class="arithmatex">\(\min f_0(x)-\tfrac{1}{t}\sum_i\log(-g_i(x))\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>LP, QP, SDP</td>
</tr>
<tr>
<td>Augmented Lagrangian (ALM)</td>
<td>Constrained convex</td>
<td><span class="arithmatex">\(f,g\)</span> convex; equality constraints</td>
<td><span class="arithmatex">\(L_\rho(x,\lambda)=f(x)+\lambda^T g(x)+\tfrac{\rho}{2}\|g(x)\|^2\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Penalty methods, PDEs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization">🌐 Derivative-Free &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Idea</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nelder–Mead Simplex</td>
<td>Low-dimensional, smooth or noisy</td>
<td>No gradients; continuous <span class="arithmatex">\(f\)</span></td>
<td>Maintain simplex of <span class="arithmatex">\(d+1\)</span> points; reflect–expand–contract–shrink operations</td>
<td>Small</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>Parameter tuning, physics models</td>
</tr>
<tr>
<td>Simulated Annealing</td>
<td>Nonconvex, global</td>
<td>Stochastic exploration via temperature</td>
<td>Random perturbations accepted w.p. <span class="arithmatex">\(\exp(-\Delta f/T)\)</span>; <span class="arithmatex">\(T\downarrow\)</span></td>
<td>Medium</td>
<td>High (many samples)</td>
<td>Hyperparameter tuning, design optimization</td>
</tr>
<tr>
<td>Multi-start Local Search</td>
<td>Nonconvex</td>
<td>None; relies on restart diversity</td>
<td>Run local solver from multiple random inits, pick best result</td>
<td>Medium</td>
<td><span class="arithmatex">\(k\times\)</span> local solver</td>
<td>Avoids local minima; cheap global heuristic</td>
</tr>
<tr>
<td>Evolutionary Algorithms (EA)</td>
<td>Black-box, global</td>
<td>Population-based; fitness function only</td>
<td>Mutate, select, recombine candidates</td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span> per gen</td>
<td>Global optimization, control, AutoML</td>
</tr>
<tr>
<td>Genetic Algorithms (GA)</td>
<td>Combinatorial / continuous</td>
<td>Chromosomal encoding of solutions</td>
<td>Apply selection, crossover, mutation; evolve over generations</td>
<td>Medium–Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Feature selection, neural architecture search</td>
</tr>
<tr>
<td>Evolution Strategies (ES)</td>
<td>Continuous, black-box</td>
<td>Gaussian mutation around mean</td>
<td><span class="arithmatex">\(\theta_{k+1} = \theta_k + \eta \sum_i w_i \epsilon_i f(\theta_k+\sigma \epsilon_i)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Reinforcement learning, black-box control</td>
</tr>
<tr>
<td>Derivative-Free Optimization (DFO)</td>
<td>Black-box, noisy <span class="arithmatex">\(f\)</span></td>
<td>Only function values available</td>
<td>Gradient estimated via random perturbations: <span class="arithmatex">\(g\approx\frac{f(x+hu)-f(x)}{h}u\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d)\)</span>–<span class="arithmatex">\(O(d^2)\)</span></td>
<td>Robotics, policy search, design</td>
</tr>
<tr>
<td>Black-Box Optimization Framework</td>
<td>General</td>
<td>No analytical gradients; often stochastic</td>
<td>Unified term covering EA, GA, ES, and DFO</td>
<td>Medium–Large</td>
<td>varies</td>
<td>Hyperparameter search, AutoML, reinforcement learning</td>
</tr>
<tr>
<td>Numerical Encodings</td>
<td>Used in GA/EA</td>
<td>Represents variables in binary, integer, or floating-point form</td>
<td>Choice of encoding impacts mutation/crossover behavior</td>
<td>N/A</td>
<td>negligible</td>
<td>Optimization of mixed or discrete variables</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-convergence-complexity-snapshot">📈 Convergence &amp; Complexity Snapshot<a class="headerlink" href="#cheatsheets-20a_cheatsheet-convergence-complexity-snapshot" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method Type</th>
<th>Convergence (Convex)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Subgradient</td>
<td><span class="arithmatex">\(O(1/\sqrt{k})\)</span></td>
<td>Nonsmooth convex</td>
</tr>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Smooth convex</td>
</tr>
<tr>
<td>Accelerated Gradient</td>
<td><span class="arithmatex">\(O(1/k^2)\)</span></td>
<td>Optimal first-order</td>
</tr>
<tr>
<td>Newton / Quasi-Newton</td>
<td>Quadratic / Superlinear</td>
<td>Local only</td>
</tr>
<tr>
<td>Strongly Convex</td>
<td><span class="arithmatex">\((1-\mu/L)^k\)</span></td>
<td>Linear rate</td>
</tr>
<tr>
<td>Variance-Reduced</td>
<td>Linear (strongly convex)</td>
<td>Finite-sum optimization</td>
</tr>
<tr>
<td>ADMM / Proximal</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Composite convex</td>
</tr>
<tr>
<td>Interior-Point</td>
<td>Polynomial time</td>
<td>High-accuracy convex</td>
</tr>
<tr>
<td>Derivative-Free / Heuristics</td>
<td>No formal bound</td>
<td>Empirical convergence only</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-practitioner-summary">🧠 Practitioner Summary<a class="headerlink" href="#cheatsheets-20a_cheatsheet-practitioner-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradients available, smooth convex</td>
<td>Gradient Descent, Nesterov</td>
</tr>
<tr>
<td>Curvature matters, moderate scale</td>
<td>Newton, BFGS, Conjugate Gradient</td>
</tr>
<tr>
<td>Nonsmooth regularizer</td>
<td>Proximal Gradient, ADMM</td>
</tr>
<tr>
<td>Simple constraints</td>
<td>Projected Gradient</td>
</tr>
<tr>
<td>Large-scale / streaming</td>
<td>SGD, Adam, RMSProp</td>
</tr>
<tr>
<td>Finite-sum convex</td>
<td>SVRG, SAGA</td>
</tr>
<tr>
<td>Online / adaptive</td>
<td>Mirror Descent, FTRL</td>
</tr>
<tr>
<td>No gradients (black-box)</td>
<td>DFO, Nelder–Mead, ES, GA</td>
</tr>
<tr>
<td>Global nonconvex search</td>
<td>Simulated Annealing, Multi-starts, Evolutionary Algorithms</td>
</tr>
<tr>
<td>Distributed / separable</td>
<td>ADMM, ALM</td>
</tr>
<tr>
<td>High-precision convex programs</td>
<td>Interior-Point, Trust-Region</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization">🧩 Notes on Global &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Conjugate Gradient</strong>: memory-efficient quasi-second-order method for large convex quadratics.  </li>
<li><strong>Nelder–Mead</strong>: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li>
<li><strong>Simulated Annealing</strong>: probabilistic global search inspired by thermodynamics.  </li>
<li><strong>Multi-Starts</strong>: pragmatic global exploration by repeated local optimization.  </li>
<li><strong>Evolutionary / Genetic / ES</strong>: population-based global heuristics; robust to noise and discontinuity.  </li>
<li><strong>Derivative-Free Optimization (DFO)</strong>: umbrella for random, surrogate-based, or adaptive black-box methods.  </li>
<li><strong>Numerical Encoding</strong>: crucial in discrete search—how real or binary variables are represented determines performance.</li>
</ul>
<hr>
<blockquote>
<p><strong>Summary Insight:</strong><br>
- Convex + differentiable → use gradient-based or Newton-type methods.<br>
- Convex + nonsmooth → use proximal, ADMM, or coordinate descent.<br>
- Large-scale or stochastic → use SGD or adaptive variants.<br>
- No gradients or nonconvex → use derivative-free or evolutionary methods.<br>
- The structure of the objective, not its size alone, determines the optimal solver family.</p>
</blockquote></body></html></section></section>
                    <section class='print-page md-section' id='section-8' heading-number='8'>
                        <h1>Appendices<a class='headerlink' href='#section-8' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="appendices-120_ineqaulities" heading-number="8.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-a-common-inequalities-and-identities">Appendix A: Common Inequalities and Identities<a class="headerlink" href="#appendices-120_ineqaulities-appendix-a-common-inequalities-and-identities" title="Permanent link">¶</a></h1>
<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the “algebraic tools” you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemaréchal, 2001).</p>
<h2 id="appendices-120_ineqaulities-a1-cauchyschwarz-inequality">A.1 Cauchy–Schwarz inequality<a class="headerlink" href="#appendices-120_ineqaulities-a1-cauchyschwarz-inequality" title="Permanent link">¶</a></h2>
<p>For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>,
<script type="math/tex; mode=display">
|x^\top y| \le \|x\|_2 \, \|y\|_2.
</script>
</p>
<p>Equality holds if and only if <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent.</p>
<p>Consequences:</p>
<ul>
<li>Defines the notion of angle between vectors.</li>
<li>Justifies dual norms.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a2-jensens-inequality">A.2 Jensen’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a2-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex, and let <span class="arithmatex">\(X\)</span> be a random variable. Then
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>In finite form: for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum_i \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<p>Jensen’s inequality is equivalent to convexity: it says “the function at the average is no more than the average of the function values.” It is used constantly to prove convexity of expectations and log-sum-exp.</p>
<h2 id="appendices-120_ineqaulities-a3-amgm-inequality">A.3 AM–GM inequality<a class="headerlink" href="#appendices-120_ineqaulities-a3-amgm-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(x_1,\dots,x_n \ge 0\)</span>,
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^n x_i
\ge
\left(\prod_{i=1}^n x_i \right)^{1/n}.
</script>
</p>
<p>This can be proved using Jensen’s inequality with <span class="arithmatex">\(f(t) = \log t\)</span>, which is concave. AM–GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>
<h2 id="appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz">A.4 Hölder’s inequality (generalised Cauchy–Schwarz)<a class="headerlink" href="#appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(p,q \ge 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> (conjugate exponents),
<script type="math/tex; mode=display">
\sum_{i=1}^n |x_i y_i|
\le
\left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
\left( \sum_{i=1}^n |y_i|^q \right)^{1/q}.
</script>
</p>
<ul>
<li>When <span class="arithmatex">\(p=q=2\)</span>, Hölder becomes Cauchy–Schwarz.</li>
<li>Hölder underlies dual norms: the dual of <span class="arithmatex">\(\ell_p\)</span> is <span class="arithmatex">\(\ell_q\)</span>.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a5-youngs-inequality">A.5 Young’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a5-youngs-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(a,b \ge 0\)</span> and <span class="arithmatex">\(p,q &gt; 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>,
<script type="math/tex; mode=display">
ab \le \frac{a^p}{p} + \frac{b^q}{q}.
</script>
</p>
<p>This is useful in bounding cross terms in convergence proofs.</p>
<h2 id="appendices-120_ineqaulities-a6-fenchels-inequality">A.6 Fenchel’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a6-fenchels-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be a convex function and let <span class="arithmatex">\(f^*\)</span> be its convex conjugate:
<script type="math/tex; mode=display">
f^*(y) = \sup_x (y^\top x - f(x)).
</script>
</p>
<p>Then for all <span class="arithmatex">\(x,y\)</span>,
<script type="math/tex; mode=display">
f(x) + f^*(y) \ge y^\top x.
</script>
</p>
<p>Fenchel’s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel’s inequality.</p>
<h2 id="appendices-120_ineqaulities-a7-supporting-hyperplane-inequality">A.7 Supporting hyperplane inequality<a class="headerlink" href="#appendices-120_ineqaulities-a7-supporting-hyperplane-inequality" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is convex, then for any <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\(g \in \partial f(x)\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y-x)
\quad \text{for all } y.
</script>
</p>
<p>This can be viewed as “<span class="arithmatex">\(f\)</span> lies above all its tangent hyperplanes,” even when it’s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>
<h2 id="appendices-120_ineqaulities-a8-summary">A.8 Summary<a class="headerlink" href="#appendices-120_ineqaulities-a8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Cauchy–Schwarz and Hölder bound inner products.</li>
<li>Jensen shows convexity and expectation interact cleanly.</li>
<li>Fenchel’s inequality is the algebra of duality.</li>
<li>Supporting hyperplane inequality is the geometry of convexity.</li>
</ul>
<p>These inequalities are used implicitly all over convex optimisation.</p></body></html></section><section class="print-page" id="appendices-130_projections" heading-number="8.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about “dropping perpendiculars” to a subspace or convex set.</p>
<p>Projection onto a subspace: Let <span class="arithmatex">\(W \subseteq \mathbb{R}^n\)</span> be a subspace (e.g. defined by a set of linear equations <span class="arithmatex">\(Ax=0\)</span> or spanned by some basis vectors). The orthogonal projection of any <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> onto <span class="arithmatex">\(W\)</span> is the unique point <span class="arithmatex">\(P_W(x) \in W\)</span> such that <span class="arithmatex">\(x - P_W(x)\)</span> is orthogonal to <span class="arithmatex">\(W\)</span>. If <span class="arithmatex">\({q_1,\dots,q_k}\)</span> is an orthonormal basis of <span class="arithmatex">\(W\)</span>, then
​
<script type="math/tex; mode=display">
P_W(x) = \sum_{i=1}^k \langle x, q_i \rangle \, q_i
</script>
</p>
<p>as mentioned earlier. This <span class="arithmatex">\(P_W(x)\)</span> minimizes the distance <span class="arithmatex">\(|x - y|2\)</span> over all <span class="arithmatex">\(y\in W\)</span>. The residual <span class="arithmatex">\(r = x - P_W(x)\)</span> is orthogonal to every direction in <span class="arithmatex">\(W\)</span>. For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In <span class="arithmatex">\(\mathbb{R}^n\)</span>, <span class="arithmatex">\(P_W\)</span> is an <span class="arithmatex">\(n \times n\)</span> matrix (if <span class="arithmatex">\(W\)</span> is <span class="arithmatex">\(k\)</span>-dimensional, <span class="arithmatex">\(P_W\)</span> has rank <span class="arithmatex">\(k\)</span>) that satisfies <span class="arithmatex">\(P_W^2 = P_W\)</span> (idempotent) and <span class="arithmatex">\(P_W = P_W^T\)</span> (symmetric). In optimization, if we are constrained to <span class="arithmatex">\(W\)</span>, a projected gradient step does <span class="arithmatex">\(x_{k+1} = P_W(x_k - \alpha \nabla f(x_k))\)</span> to ensure <span class="arithmatex">\(x_{k+1} \in W\)</span>.</p>
<p>Projection onto a convex set: More generally, for a closed convex set <span class="arithmatex">\(C \subset \mathbb{R}^n\)</span>, the projection <span class="arithmatex">\(\operatorname{proj}_C(x)\)</span> is defined as the unique point in <span class="arithmatex">\(C\)</span> closest to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
P_C(x) = \arg\min_{y \in C} \|x - y\|_2
\]</div>
<p>For convex <span class="arithmatex">\(C\)</span>, this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box <span class="arithmatex">\([l,u]\)</span> just clips each coordinate between <span class="arithmatex">\(l\)</span> and <span class="arithmatex">\(u\)</span>). Some properties of convex projections: </p>
<ul>
<li>
<p><span class="arithmatex">\(P_C(x)\)</span> lies on the boundary of <span class="arithmatex">\(C\)</span> along the direction of <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(x \notin C\)</span>. </p>
</li>
<li>
<p>The first-order optimality condition for the minimization above says <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the tangent of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>, or equivalently <span class="arithmatex">\(\langle x - P_C(x), y - P_C(x)\rangle \le 0\)</span> for all <span class="arithmatex">\(y \in C\)</span>. This means the line from <span class="arithmatex">\(P_C(x)\)</span> to <span class="arithmatex">\(x\)</span> forms a supporting hyperplane to <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>. </p>
</li>
<li>Also, projections are firmly non-expansive: <span class="arithmatex">\(|P_C(x)-P_C(y)|^2 \le \langle P_C(x)-P_C(y), x-y \rangle \le |x-y|^2\)</span>. Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li>
</ul>
<p>Examples:</p>
<ul>
<li>
<p>Projection onto an affine set <span class="arithmatex">\(Ax=b\)</span> (assuming it’s consistent) can be derived via normal equations: one finds a correction <span class="arithmatex">\(\delta x\)</span> in the row space of <span class="arithmatex">\(A^T\)</span> such that <span class="arithmatex">\(A(x+\delta x)=b\)</span>. The solution is <span class="arithmatex">\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\)</span> (for full row rank <span class="arithmatex">\(A\)</span>).</p>
</li>
<li>
<p>Projection onto the nonnegative orthant <span class="arithmatex">\({x: x_i\ge0}\)</span> just sets <span class="arithmatex">\(x_i^- = \min(x_i,0)\)</span> to zero (i.e. <span class="arithmatex">\([x]^+ = \max{x,0}\)</span> componentwise). This is used in nonnegativity constraints.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\({|x|_2 \le \alpha}\)</span> scales <span class="arithmatex">\(x\)</span> down to have length <span class="arithmatex">\(\alpha\)</span> if <span class="arithmatex">\(|x|&gt;\alpha\)</span>, or does nothing if <span class="arithmatex">\(|x|\le\alpha\)</span>.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_1\)</span> ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal <span class="arithmatex">\(\alpha\)</span>.</p>
</li>
</ul>
<p>Why projections matter in optimization: Many convex optimization problems involve constraints <span class="arithmatex">\(x \in C\)</span> where <span class="arithmatex">\(C\)</span> is convex. If we can compute <span class="arithmatex">\(P_C(x)\)</span> easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to <span class="arithmatex">\(C\)</span>, we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the feasible region at <span class="arithmatex">\(P_C(x)\)</span> connects to KKT conditions: at optimum <span class="arithmatex">\(\hat{x}\)</span> with <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(\hat{x}))\)</span>, the vector <span class="arithmatex">\(-\nabla f(\hat{x})\)</span> must lie in the normal cone of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(\hat{x}\)</span>, meaning the gradient is “balanced” by the constraint boundary — this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(x^*))\)</span> for some step <span class="arithmatex">\(\alpha\)</span>, i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p>
<p>Orthogonal decomposition: Any vector <span class="arithmatex">\(x\)</span> can be uniquely decomposed relative to a subspace <span class="arithmatex">\(W\)</span> as <span class="arithmatex">\(x = P_W(x) + r\)</span> with <span class="arithmatex">\(r \perp W\)</span>. Moreover, <span class="arithmatex">\(|x|^2 = |P_W(x)|^2 + |r|^2\)</span> (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint <span class="arithmatex">\(x\in W\)</span>, any descent direction <span class="arithmatex">\(d\)</span> can be split into a part tangent to <span class="arithmatex">\(W\)</span> (which actually moves within <span class="arithmatex">\(W\)</span>) and a part normal to <span class="arithmatex">\(W\)</span> (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient <span class="arithmatex">\(\nabla f(x^)\)</span> being orthogonal to the feasible region means <span class="arithmatex">\(\nabla f(x^)\)</span> lies entirely in the normal subspace <span class="arithmatex">\(W^\perp\)</span> — no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: <span class="arithmatex">\(\nabla f(x^)\)</span> is in the row space of <span class="arithmatex">\(A\)</span> if <span class="arithmatex">\(Ax^=b\)</span> are active constraints, which leads to <span class="arithmatex">\(\nabla f(x^*) = A^T \lambda\)</span> for some <span class="arithmatex">\(\lambda\)</span> (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p>
<p>Projection algorithms: The simplicity or difficulty of computing <span class="arithmatex">\(P_C(x)\)</span> often determines if we can solve a problem efficiently. If <span class="arithmatex">\(C\)</span> is something like a polyhedron given by linear inequalities, <span class="arithmatex">\(P_C\)</span> might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing <span class="arithmatex">\(\operatorname{prox}{\gamma g}(x) = \arg\min_y {g(y) + \frac{1}{2\gamma}|y-x|^2}\)</span>, which for indicator functions of set <span class="arithmatex">\(C\)</span> yields <span class="arithmatex">\(\operatorname{prox}{\delta_C}(x) = P_C(x)\)</span>. Thus projection is a special proximal operator (one for constraints).</p>
<p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in <span class="arithmatex">\(C_1 \cap C_2\)</span> by <span class="arithmatex">\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\)</span>), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections — that the closest point condition yields orthogonality conditions and that projections do not expand distances — is crucial for understanding how constraint-handling algorithms converge.</p></body></html></section><section class="print-page" id="appendices-140_support" heading-number="8.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-b-support-functions-and-dual-geometry-advanced">Appendix B: Support Functions and Dual Geometry (Advanced)<a class="headerlink" href="#appendices-140_support-appendix-b-support-functions-and-dual-geometry-advanced" title="Permanent link">¶</a></h1>
<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>
<hr>
<h2 id="appendices-140_support-b1-support-functions">B.1 Support functions<a class="headerlink" href="#appendices-140_support-b1-support-functions" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be a nonempty set. The support function of <span class="arithmatex">\(C\)</span> is
<script type="math/tex; mode=display">
\sigma_C(y) = \sup_{x \in C} y^\top x.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For a given direction <span class="arithmatex">\(y\)</span>, <span class="arithmatex">\(\sigma_C(y)\)</span> tells you how far you can go in that direction while staying in <span class="arithmatex">\(C\)</span>.</li>
<li>It is the value of the linear maximisation problem
  <script type="math/tex; mode=display">
  \max_{x \in C} y^\top x.
  </script>
</li>
</ul>
<p>Key facts:</p>
<ol>
<li><span class="arithmatex">\(\sigma_C\)</span> is always convex, even if <span class="arithmatex">\(C\)</span> is not convex.</li>
<li>If <span class="arithmatex">\(C\)</span> is convex and closed, <span class="arithmatex">\(\sigma_C\)</span> essentially characterises <span class="arithmatex">\(C\)</span>.<br>
   In particular, <span class="arithmatex">\(C\)</span> can be recovered as the intersection of halfspaces
   <script type="math/tex; mode=display">
   x^\top y \le \sigma_C(y)\quad \text{for all } y.
   </script>
</li>
</ol>
<p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>
<hr>
<h2 id="appendices-140_support-b2-support-functions-and-dual-norms">B.2 Support functions and dual norms<a class="headerlink" href="#appendices-140_support-b2-support-functions-and-dual-norms" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(C\)</span> is the unit ball of a norm <span class="arithmatex">\(\|\cdot\|\)</span>, i.e.
<script type="math/tex; mode=display">
C = \{ x : \|x\| \le 1 \},
</script>
then
<script type="math/tex; mode=display">
\sigma_C(y)
=
\sup_{\|x\|\le 1} y^\top x
=
\|y\|_*,
</script>
the dual norm of <span class="arithmatex">\(\|\cdot\|\)</span>.</p>
<p>Example:</p>
<ul>
<li>For <span class="arithmatex">\(\ell_2\)</span>, <span class="arithmatex">\(\|\cdot\|_2\)</span> is self-dual, so <span class="arithmatex">\(\|y\|_2^* = \|y\|_2\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_1\)</span>, the dual norm is <span class="arithmatex">\(\ell_\infty\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_\infty\)</span>, the dual norm is <span class="arithmatex">\(\ell_1\)</span>.</li>
</ul>
<p>This shows that dual norms are just support functions of norm balls.</p>
<hr>
<h2 id="appendices-140_support-b3-indicator-functions-and-conjugates">B.3 Indicator functions and conjugates<a class="headerlink" href="#appendices-140_support-b3-indicator-functions-and-conjugates" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a set <span class="arithmatex">\(C\)</span>:
<script type="math/tex; mode=display">
\delta_C(x) =
\begin{cases}
0 & x \in C, \\
+\infty & x \notin C.
\end{cases}
</script>
</p>
<p>Its convex conjugate is
<script type="math/tex; mode=display">
\delta_C^*(y)
=
\sup_x (y^\top x - \delta_C(x))
=
\sup_{x \in C} y^\top x
=
\sigma_C(y).
</script>
</p>
<p>Thus,</p>
<blockquote>
<p>The support function <span class="arithmatex">\(\sigma_C\)</span> is the convex conjugate of the indicator of <span class="arithmatex">\(C\)</span>.</p>
</blockquote>
<p>This is extremely important conceptually:</p>
<ul>
<li>Conjugates turn sets into functions.</li>
<li>Duality in optimisation is often conjugacy in disguise.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b4-hyperplane-separation-revisited">B.4 Hyperplane separation revisited<a class="headerlink" href="#appendices-140_support-b4-hyperplane-separation-revisited" title="Permanent link">¶</a></h2>
<p>Recall: if <span class="arithmatex">\(C\)</span> is closed and convex, then at any boundary point <span class="arithmatex">\(x_0 \in C\)</span> there is a supporting hyperplane
<script type="math/tex; mode=display">
a^\top x \le a^\top x_0
\quad \text{for all } x \in C.
</script>
</p>
<p>This <span class="arithmatex">\(a\)</span> is exactly the kind of vector we would use in a support function evaluation. In fact, <span class="arithmatex">\(a^\top x_0 = \sigma_C(a)\)</span> if <span class="arithmatex">\(x_0\)</span> is an extreme point (or exposed point) in direction <span class="arithmatex">\(a\)</span>.</p>
<p>Geometric interpretation:</p>
<ul>
<li>Lagrange multipliers in the dual problem play the role of these <span class="arithmatex">\(a\)</span>’s.</li>
<li>They identify supporting hyperplanes that “witness” optimality.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b5-duality-as-support">B.5 Duality as support<a class="headerlink" href="#appendices-140_support-b5-duality-as-support" title="Permanent link">¶</a></h2>
<p>Consider the (convex) primal problem
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & x \in C,
\end{array}
</script>
where <span class="arithmatex">\(C\)</span> is a convex feasible set.</p>
<p>We can rewrite the problem as minimising
<script type="math/tex; mode=display">
f(x) + \delta_C(x).
</script>
</p>
<p>The convex conjugate of <span class="arithmatex">\(f + \delta_C\)</span> is
<script type="math/tex; mode=display">
(f + \delta_C)^*(y)
=
\inf_{u+v=y} \left( f^*(u) + \delta_C^*(v) \right)
=
\inf_{u+v=y} \left( f^*(u) + \sigma_C(v) \right).
</script>
</p>
<p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on <span class="arithmatex">\(f(x)\)</span> over <span class="arithmatex">\(x \in C\)</span> using conjugates and support functions (Rockafellar, 1970).</p>
<p>This view makes precise the slogan:</p>
<blockquote>
<p>“Dual variables are hyperplanes that support the feasible set and the objective from below.”</p>
</blockquote>
<hr>
<h2 id="appendices-140_support-b6-geometry-of-kkt-and-multipliers">B.6 Geometry of KKT and multipliers<a class="headerlink" href="#appendices-140_support-b6-geometry-of-kkt-and-multipliers" title="Permanent link">¶</a></h2>
<p>At the optimal point <span class="arithmatex">\(x^*\)</span> of a convex problem, there is typically a hyperplane that supports the feasible set at <span class="arithmatex">\(x^*\)</span> and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p>
<ul>
<li>The multipliers form a certificate that <span class="arithmatex">\(x^*\)</span> cannot be improved without violating feasibility.</li>
<li>The dual problem is the search for the “best” such certificate.</li>
</ul>
<p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater’s condition (Boyd and Vandenberghe, 2004).</p>
<hr>
<h2 id="appendices-140_support-b7-why-this-matters">B.7 Why this matters<a class="headerlink" href="#appendices-140_support-b7-why-this-matters" title="Permanent link">¶</a></h2>
<p>This geometric point of view is not just pretty:</p>
<ul>
<li>It explains why strong duality holds.</li>
<li>It explains what <span class="arithmatex">\(\mu_i^*\)</span> and <span class="arithmatex">\(\lambda_j^*\)</span> “mean.”</li>
<li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li>
</ul>
<!-- # F.2 Support Functions and Dual Geometry

Support functions are one of the most elegant bridges between convex sets and linear optimization. For any convex set, they describe its extent in a given direction — and thus appear naturally in:

- Duality theory and convex conjugates (see Section D.1)  
- Norm analysis and dual norms (Section A.4 and A.5)  
- Subgradient calculations and optimality conditions (Section A.7 and Section D.3)  
- Projection and cutting-plane algorithms in high-dimensional optimization  

Geometrically, a support function tells you:  
> *How far can I go in direction $y$ and still remain inside the set $C$?*

---

## Definition and Geometry

Let $C \subseteq \mathbb{R}^n$ be a nonempty convex set. The support function $\sigma_C : \mathbb{R}^n \to \mathbb{R}$ is defined as:

$$
\sigma_C(y) = \sup_{x \in C} \langle y, x \rangle
$$

- $y$ is the direction vector.  
- $\langle y, x \rangle$ is the inner product (see Section A.2).  
- $\sigma_C(y)$ gives the maximum projection of $C$ along direction $y$.

It corresponds to the furthest point of $C$ in direction $y$, and hence defines a supporting hyperplane to the set.

---

## Key Properties

- Positive Homogeneity:  
  $$
  \sigma_C(\alpha y) = \alpha \sigma_C(y) \quad \text{for } \alpha \ge 0
  $$
- Convexity:  
  $$
  \sigma_C(y_1 + y_2) \le \sigma_C(y_1) + \sigma_C(y_2)
  $$
- Attainment: If $C$ is closed and bounded (compact), the supremum is attained — the max is reached at some $x^\star \in C$.  
- Set Representation:  
  Every closed convex set can be recovered from its support function:
  $$
  C = \{ x \in \mathbb{R}^n \mid \langle y, x \rangle \le \sigma_C(y) \quad \forall y \in \mathbb{R}^n \}
  $$

---

## Computation and Intuition

To compute $\sigma_C(y)$:

1. Specify the convex set $C$ (e.g., a ball, polytope, or feasible region).
2. Fix the direction $y \in \mathbb{R}^n$.
3. Maximize the dot product $\langle y, x \rangle$ over $x \in C$.

This is a linear program over $C$.

### Links to Optimization:
- In duality theory (Section D.1), linear functionals $\langle y, x \rangle$ are used to lower-bound convex functions — support functions arise naturally.
- For constraint sets defined by indicator functions (Section A.8), the support function is their convex conjugate:
  $$
  \sigma_C = \delta_C^*
  $$

---

## Examples

### Example 1: $\ell_2$ Unit Ball

Let $C = \{ x \mid \|x\|_2 \le 1 \}$

Then the support function is:

$$
\sigma_C(y) = \sup_{\|x\|_2 \le 1} \langle y, x \rangle = \|y\|_2
$$

Interpretation: the farthest point in direction $y$ lies on the boundary and aligns with $y$.

👉 This reveals that the support function of a norm ball gives the dual norm — see Section A.4.

---

### Example 2: $\ell_1$ Unit Ball

Let $C = \{ x \mid \|x\|_1 \le 1 \}$

Then:

$$
\sigma_C(y) = \|y\|_\infty
$$

Intuition: in direction $y$, the maximal point in $C$ aligns with the coordinate having largest magnitude.

This dual norm relationship is fundamental in sparsity-inducing optimization (e.g., LASSO in Section F.1).

---

### Example 3: Polytope (Convex Hull)

Let $C = \text{conv}\{v_1, \dots, v_m\}$

Then:

$$
\sigma_C(y) = \max_{i=1,\dots,m} \langle y, v_i \rangle
$$

Interpretation: the maximum projection occurs at one of the vertices of the polytope.

In LP problems (see Section C.3), this is how extreme points determine optimal solutions.

---

## Applications in Optimization

### 🔄 Duality and Convex Conjugates

The support function is the Fenchel conjugate of an indicator function:

$$
\sigma_C(y) = \delta_C^*(y)
$$

This connection underpins many dual optimization frameworks, including saddle-point methods, dual norms, and variational formulations.

---

### 📐 Dual Norms

For any norm $\|\cdot\|$, the support function of its unit ball gives the dual norm:

$$
\sigma_{B}(y) = \sup_{\|x\| \le 1} \langle y, x \rangle = \|y\|_*
$$

See Section A.5 for the definition of dual norms, and Section C.1 for how they affect step sizes and convergence geometry.

---

### 📏 Geometric Use Cases

- Compute distances to sets via duality.  
- Generate separating hyperplanes for convex sets.  
- Implement projection algorithms (e.g., mirror descent in Section K.1).  
- Construct robust constraints and worst-case bounds in uncertainty modeling (see Section E.2).

---

## Summary and Takeaways

- The support function $\sigma_C(y)$ measures how far a convex set extends in direction $y$.
- It is always convex, positively homogeneous, and subadditive.
- Support functions appear in:
  - Duality theory via convex conjugates  
  - Norm analysis via dual norms  
  - Subgradients and projections  
  - Constraint representations and recovery of convex sets
- For norm balls, $\sigma_C$ gives the dual norm.  
- For polytopes, $\sigma_C$ is the max over vertices.  
- For machine learning, support functions help model constraints, regularization penalties, and geometric algorithms.

Mental model:  
Think of a support function as a “radar scan” — it tells you the furthest point of a convex set in any given direction.
 --></body></html></section><section class="print-page" id="appendices-160_conjugates" heading-number="8.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-d-convex-conjugates-and-fenchel-duality">Appendix D: Convex Conjugates and Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-appendix-d-convex-conjugates-and-fenchel-duality" title="Permanent link">¶</a></h1>
<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis.<br>
They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p>
<ul>
<li>Convex conjugates convert a function into its “slope-space” representation — capturing its tightest linear overestimates.  </li>
<li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li>
</ul>
<p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>
<h2 id="appendices-160_conjugates-d1-intuitive-picture">D.1 Intuitive Picture<a class="headerlink" href="#appendices-160_conjugates-d1-intuitive-picture" title="Permanent link">¶</a></h2>
<p>Imagine a convex function <span class="arithmatex">\(f(x)\)</span> drawn as a bowl in space.<br>
Each point <span class="arithmatex">\(y\)</span> defines a line (or hyperplane) of slope <span class="arithmatex">\(y\)</span>:
<script type="math/tex; mode=display">
x \mapsto \langle y, x \rangle - b.
</script>
The convex conjugate <span class="arithmatex">\(f^*(y)\)</span> is the smallest height <span class="arithmatex">\(b\)</span> such that this line always stays above <span class="arithmatex">\(f(x)\)</span>.<br>
In other words:</p>
<blockquote>
<p><span class="arithmatex">\(f^*(y)\)</span> measures the tightest linear overestimate of <span class="arithmatex">\(f\)</span> in direction <span class="arithmatex">\(y\)</span>.</p>
</blockquote>
<p>So <span class="arithmatex">\(f^*\)</span> encodes how “steep” <span class="arithmatex">\(f\)</span> can be in every direction — it transforms the geometry of <span class="arithmatex">\(f\)</span> into a new convex function on slope-space.</p>
<h2 id="appendices-160_conjugates-d2-definition-and-key-properties">D.2 Definition and Key Properties<a class="headerlink" href="#appendices-160_conjugates-d2-definition-and-key-properties" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\cup\{+\infty\}\)</span> be a proper convex function.<br>
Its convex (Fenchel) conjugate is
<script type="math/tex; mode=display">
f^*(y) = \sup_{x \in \mathbb{R}^n} \big( \langle y, x \rangle - f(x) \big).
</script>
</p>
<p>Interpretation
- <span class="arithmatex">\(y\)</span>: a slope or linear functional.
- The supremum seeks the largest gap between the linear function <span class="arithmatex">\(\langle y,x\rangle\)</span> and the graph of <span class="arithmatex">\(f\)</span>.
- <span class="arithmatex">\(f^*(y)\)</span> is always convex, even if <span class="arithmatex">\(f\)</span> isn’t strictly convex.</p>
<h3 id="appendices-160_conjugates-fundamental-identities">Fundamental Identities<a class="headerlink" href="#appendices-160_conjugates-fundamental-identities" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Fenchel–Young inequality
   <script type="math/tex; mode=display">
   \langle y,x\rangle \le f(x) + f^*(y),
   </script>
   with equality iff <span class="arithmatex">\(y \in \partial f(x)\)</span>.</p>
</li>
<li>
<p>Biconjugation
   <script type="math/tex; mode=display">
   f^{} = f \quad \text{if <span class="arithmatex">\(f\)</span> is proper, convex, and lower semicontinuous.}
   </script>
   This tells us the conjugate transform loses no information for convex functions.</p>
</li>
<li>
<p>Order reversal
   <span class="arithmatex">\(f \le g \;\Rightarrow\; f^* \ge g^*\)</span>.</p>
</li>
<li>
<p>Scaling and shift</p>
</li>
<li><span class="arithmatex">\((f + a)^*(y) = f^*(y) - a\)</span>,</li>
<li><span class="arithmatex">\((\alpha f)^*(y) = \alpha f^*(y/\alpha)\)</span> for <span class="arithmatex">\(\alpha&gt;0.\)</span></li>
</ol>
<hr>
<h2 id="appendices-160_conjugates-d3-canonical-examples">D.3 Canonical Examples<a class="headerlink" href="#appendices-160_conjugates-d3-canonical-examples" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Function <span class="arithmatex">\(f(x)\)</span></th>
<th>Conjugate <span class="arithmatex">\(f^*(y)\)</span></th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\( \tfrac{1}{2}\|x\|_2^2 \)</span></td>
<td><span class="arithmatex">\( \tfrac{1}{2}\|y\|_2^2 \)</span></td>
<td>Self-conjugate quadratic</td>
</tr>
<tr>
<td><span class="arithmatex">\( \|x\|_1 \)</span></td>
<td><span class="arithmatex">\( \delta_{\{\|y\|_\infty \le 1\}}(y) \)</span></td>
<td>Dual norm indicator</td>
</tr>
<tr>
<td><span class="arithmatex">\( \delta_C(x) \)</span></td>
<td><span class="arithmatex">\( \sigma_C(y)=\sup_{x\in C}\langle y,x\rangle \)</span></td>
<td>Support function of set <span class="arithmatex">\(C\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\( e^x \)</span></td>
<td><span class="arithmatex">\( y\log y - y,\, y&gt;0 \)</span></td>
<td>Appears in entropy and KL-divergence</td>
</tr>
</tbody>
</table>
<p>These examples illustrate how conjugation connects:
- Norms ↔ dual norms,<br>
- Sets ↔ support functions,<br>
- Exponentials ↔ entropy,<br>
- Quadratics ↔ themselves.</p>
<h2 id="appendices-160_conjugates-d4-geometric-interpretation">D.4 Geometric Interpretation<a class="headerlink" href="#appendices-160_conjugates-d4-geometric-interpretation" title="Permanent link">¶</a></h2>
<ul>
<li>Each point on <span class="arithmatex">\(f\)</span> has a tangent hyperplane whose slope is a subgradient.  </li>
<li>The collection of all such hyperplanes forms the epigraph of <span class="arithmatex">\(f^*\)</span>.  </li>
<li>The transformation <span class="arithmatex">\(f \mapsto f^*\)</span> swaps the roles of “position” and “slope”:<br>
  convex geometry ↔ supporting hyperplanes.</li>
</ul>
<p>Visually:<br>
- <span class="arithmatex">\(f\)</span> describes a bowl in <span class="arithmatex">\((x,t)\)</span>-space.<br>
- <span class="arithmatex">\(f^*\)</span> describes the envelope of tangent planes to that bowl.</p>
<h2 id="appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality">D.5 From Conjugates to Duality — Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality" title="Permanent link">¶</a></h2>
<p>Many convex optimization problems can be written as
<script type="math/tex; mode=display">
\min_x \; f(x) + g(Ax),
</script>
where <span class="arithmatex">\(f,g\)</span> are convex and <span class="arithmatex">\(A\)</span> is linear.<br>
Fenchel duality uses conjugates to build a dual problem in terms of <span class="arithmatex">\(f^*\)</span> and <span class="arithmatex">\(g^*\)</span>.</p>
<h3 id="appendices-160_conjugates-the-fenchel-dual-problem">The Fenchel Dual Problem<a class="headerlink" href="#appendices-160_conjugates-the-fenchel-dual-problem" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\max_y \; -f^*(A^\top y) - g^*(-y).
\]</div>
<p>Interpretation
- <span class="arithmatex">\(y\)</span> is the dual variable (similar to Lagrange multipliers).<br>
- The dual objective collects the best linear lower bounds on the primal cost.</p>
<h2 id="appendices-160_conjugates-d6-weak-and-strong-duality">D.6 Weak and Strong Duality<a class="headerlink" href="#appendices-160_conjugates-d6-weak-and-strong-duality" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Weak duality: For any <span class="arithmatex">\(x,y\)</span>,
  <script type="math/tex; mode=display">
  f(x)+g(Ax) \ge -f^*(A^\top y) - g^*(-y).
  </script>
  So the dual value always underestimates the primal value.</p>
</li>
<li>
<p>Strong duality:<br>
  If <span class="arithmatex">\(f,g\)</span> are closed convex and a mild constraint qualification holds (e.g. Slater’s condition — existence of strictly feasible <span class="arithmatex">\(x\)</span>), then
  <script type="math/tex; mode=display">
  \min_x [f(x)+g(Ax)] = \max_y [-f^*(A^\top y) - g^*(-y)].
  </script>
</p>
</li>
</ul>
<p>At the optimum:
<script type="math/tex; mode=display">
A^\top y^* \in \partial f(x^*), 
\qquad
-y^* \in \partial g(Ax^*).
</script>
These are the Fenchel–KKT conditions, directly linking primal and dual subgradients.</p>
<h2 id="appendices-160_conjugates-d7-illustrative-examples">D.7 Illustrative Examples<a class="headerlink" href="#appendices-160_conjugates-d7-illustrative-examples" title="Permanent link">¶</a></h2>
<h3 id="appendices-160_conjugates-a-linear-programming">(a) Linear Programming<a class="headerlink" href="#appendices-160_conjugates-a-linear-programming" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_{x \ge 0} c^\top x \quad \text{s.t. } Ax = b.
</script>
</p>
<p>Take<br>
<span class="arithmatex">\(f(x) = c^\top x + \delta_{\{x\ge0\}}(x)\)</span>,<br>
<span class="arithmatex">\(g(z)=\delta_{\{z=b\}}(z)\)</span>.</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y) = \delta_{\{y \le c\}}(y),
\qquad
g^*(y) = b^\top y.
</script>
</p>
<p>Dual:
<script type="math/tex; mode=display">
\max_y \; b^\top y \quad \text{s.t. } A^\top y \le c,
</script>
which is the standard LP dual.</p>
<h3 id="appendices-160_conjugates-b-quadratic-set-constraint">(b) Quadratic + Set Constraint<a class="headerlink" href="#appendices-160_conjugates-b-quadratic-set-constraint" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_x \tfrac{1}{2}\|x\|_2^2 + \delta_C(x).
</script>
</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y)=\tfrac{1}{2}\|y\|_2^2, \qquad g^*(y)=\sigma_C(y),
</script>
so the dual is
<script type="math/tex; mode=display">
\max_y -\tfrac{1}{2}\|y\|_2^2 - \sigma_C(y).
</script>
Optimality gives <span class="arithmatex">\(x^*=y^*\)</span>, the projection condition in Euclidean geometry.</p>
<h2 id="appendices-160_conjugates-d8-practical-significance">D.8 Practical Significance<a class="headerlink" href="#appendices-160_conjugates-d8-practical-significance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Area</th>
<th>How Fenchel Duality Appears</th>
</tr>
</thead>
<tbody>
<tr>
<td>Optimization theory</td>
<td>Derives general dual problems beyond inequality constraints.</td>
</tr>
<tr>
<td>Algorithm design</td>
<td>Basis for primal–dual and splitting methods (ADMM, Chambolle–Pock, Mirror Descent).</td>
</tr>
<tr>
<td>Geometry</td>
<td>Dual problem finds the “best supporting hyperplane” to the primal epigraph.</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Loss–regularizer pairs (hinge ↔ clipped loss, logistic ↔ log-sum-exp) often form conjugate pairs.</td>
</tr>
<tr>
<td>Proximal operators</td>
<td>Linked via Moreau identity:  <span class="arithmatex">\(\mathrm{prox}_{f^*}(y) = y - \mathrm{prox}_f(y)\)</span>.</td>
</tr>
</tbody>
</table>
<h2 id="appendices-160_conjugates-d9-conceptual-unification">D.9 Conceptual Unification<a class="headerlink" href="#appendices-160_conjugates-d9-conceptual-unification" title="Permanent link">¶</a></h2>
<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p>
<ul>
<li>From geometry: support functions, projections, subgradients (Appendices B–C).  </li>
<li>From analysis: inequalities like Fenchel’s and Jensen’s (Appendix A).  </li>
<li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7–8).  </li>
<li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9–10).</li>
</ul>
<p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>
<h2 id="appendices-160_conjugates-d10-summary-and-takeaways">D.10 Summary and Takeaways<a class="headerlink" href="#appendices-160_conjugates-d10-summary-and-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>The convex conjugate <span class="arithmatex">\(f^*\)</span> expresses <span class="arithmatex">\(f\)</span> through its linear support planes.  </li>
<li>The Fenchel–Young inequality connects primal variables and dual slopes.  </li>
<li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li>
<li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li>
<li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li>
</ul>
<hr>
<p>Further Reading</p>
<ul>
<li>Rockafellar, R. T. (1970). <em>Convex Analysis</em>. Princeton UP.  </li>
<li>Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex Optimization</em>, Chs. 3 &amp; 5.  </li>
<li>Bauschke, H. H., &amp; Combettes, P. L. (2017). <em>Convex Analysis and Monotone Operator Theory</em>.  </li>
<li>Hiriart-Urruty, J.-B., &amp; Lemaréchal, C. (2001). <em>Fundamentals of Convex Analysis</em>.  </li>
</ul></body></html></section><section class="print-page" id="appendices-170_probability" heading-number="8.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-e-convexity-in-probability-and-statistics">Appendix E : Convexity in Probability and Statistics<a class="headerlink" href="#appendices-170_probability-appendix-e-convexity-in-probability-and-statistics" title="Permanent link">¶</a></h1>
<p>Convex analysis is not just geometry and optimization — it is deeply woven into probability, statistics, and information theory.<br>
Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p>
<p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>
<h2 id="appendices-170_probability-e1-convexity-of-expectations">E.1 Convexity of Expectations<a class="headerlink" href="#appendices-170_probability-e1-convexity-of-expectations" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f:\mathbb{R}^n\!\to\!\mathbb{R}\)</span> be convex and <span class="arithmatex">\(X\)</span> a random vector.<br>
Then by Jensen’s inequality (Appendix A):</p>
<div class="arithmatex">\[
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
\]</div>
<h3 id="appendices-170_probability-consequences">Consequences<a class="headerlink" href="#appendices-170_probability-consequences" title="Permanent link">¶</a></h3>
<ul>
<li>Expectations preserve convexity:<br>
  if each <span class="arithmatex">\(f(\cdot,\xi)\)</span> is convex, then <span class="arithmatex">\(F(x)=\mathbb{E}_\xi[f(x,\xi)]\)</span> is convex.</li>
<li>Stochastic objectives in ML — e.g. expected loss <span class="arithmatex">\(\mathbb{E}_{(a,b)}[\ell(a^\top x,b)]\)</span> — are convex when the sample-wise loss is convex.</li>
</ul>
<p>Hence almost all <em>empirical risk minimization</em> problems are discrete approximations of convex expectations.</p>
<h2 id="appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions">E.2 Convexity of Log-Partition and Moment-Generating Functions<a class="headerlink" href="#appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions" title="Permanent link">¶</a></h2>
<p>For a random variable <span class="arithmatex">\(X\)</span>, the moment-generating function (MGF) and cumulant-generating function (CGF) are</p>
<div class="arithmatex">\[
M_X(t)=\mathbb{E}[e^{tX}], \qquad
K_X(t)=\log M_X(t).
\]</div>
<p>Fact: <span class="arithmatex">\(K_X(t)\)</span> is always convex in <span class="arithmatex">\(t\)</span>.</p>
<p>Reason: <span class="arithmatex">\(K_X''(t)=\mathrm{Var}_t(X)\ge0\)</span>;<br>
variance is nonnegative.  </p>
<h3 id="appendices-170_probability-implications">Implications<a class="headerlink" href="#appendices-170_probability-implications" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(K_X(t)\)</span> acts as a convex “potential” controlling exponential families.</li>
<li>The log-partition function in statistics,
  <script type="math/tex; mode=display">
  A(\theta)=\log \int e^{\langle \theta,T(x)\rangle}\,h(x)\,dx,
  </script>
  is convex in <span class="arithmatex">\(\theta\)</span> (strictly convex for full exponential families).</li>
<li>Its gradient gives the mean parameter: <span class="arithmatex">\(\nabla A(\theta)=\mathbb{E}_\theta[T(X)]\)</span>.</li>
</ul>
<p>Thus convexity of <span class="arithmatex">\(A\)</span> guarantees a one-to-one mapping between natural and mean parameters — a foundation of exponential-family inference.</p>
<h2 id="appendices-170_probability-e3-exponential-families-and-dual-convexity">E.3 Exponential Families and Dual Convexity<a class="headerlink" href="#appendices-170_probability-e3-exponential-families-and-dual-convexity" title="Permanent link">¶</a></h2>
<p>An exponential-family density has the form
<script type="math/tex; mode=display">
p_\theta(x)=\exp\big(\langle\theta,T(x)\rangle-A(\theta)\big)h(x).
</script>
</p>
<p>Properties:</p>
<ol>
<li><span class="arithmatex">\(A(\theta)\)</span> is convex, smooth, and serves as a potential function.</li>
<li>Its convex conjugate <span class="arithmatex">\(A^*(\mu)\)</span> defines the entropy of the family:
   <script type="math/tex; mode=display">
   A^*(\mu)=\sup_\theta(\langle\mu,\theta\rangle-A(\theta)) = -H(p_\mu),
   </script>
   where <span class="arithmatex">\(H\)</span> is the Shannon entropy of the distribution with mean <span class="arithmatex">\(\mu\)</span>.</li>
</ol>
<p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>
<h2 id="appendices-170_probability-e4-convex-divergences-and-information-measures">E.4 Convex Divergences and Information Measures<a class="headerlink" href="#appendices-170_probability-e4-convex-divergences-and-information-measures" title="Permanent link">¶</a></h2>
<h3 id="appendices-170_probability-a-kullbackleibler-kl-divergence">(a) Kullback–Leibler (KL) Divergence<a class="headerlink" href="#appendices-170_probability-a-kullbackleibler-kl-divergence" title="Permanent link">¶</a></h3>
<p>For densities <span class="arithmatex">\(p,q\)</span>,
<script type="math/tex; mode=display">
D_{\mathrm{KL}}(p\|q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx.
</script>
</p>
<ul>
<li><span class="arithmatex">\(D_{\mathrm{KL}}\)</span> is jointly convex in <span class="arithmatex">\((p,q)\)</span>.  </li>
<li>Proof: the function <span class="arithmatex">\((u,v)\mapsto u\log(u/v)\)</span> is convex on <span class="arithmatex">\(\mathbb{R}_+^2\)</span>.  </li>
<li>Consequently, mixtures of distributions cannot increase KL divergence — a key fact in variational inference and EM.</li>
</ul>
<h3 id="appendices-170_probability-b-bregman-divergences">(b) Bregman Divergences<a class="headerlink" href="#appendices-170_probability-b-bregman-divergences" title="Permanent link">¶</a></h3>
<p>Given a differentiable convex <span class="arithmatex">\(\phi\)</span>, define
<script type="math/tex; mode=display">
D_\phi(x\|y)=\phi(x)-\phi(y)-\langle\nabla\phi(y),x-y\rangle.
</script>
KL divergence is a Bregman divergence for <span class="arithmatex">\(\phi(p)=\sum_i p_i\log p_i\)</span>.<br>
Thus information-theoretic distances are <em>geometric shadows</em> of convex functions.</p>
<h3 id="appendices-170_probability-c-f-divergences">(c) f-Divergences<a class="headerlink" href="#appendices-170_probability-c-f-divergences" title="Permanent link">¶</a></h3>
<p>A general convex generator <span class="arithmatex">\(f\)</span> with <span class="arithmatex">\(f(1)=0\)</span> yields
<script type="math/tex; mode=display">
D_f(p\|q)=\int q(x)\,f\!\left(\frac{p(x)}{q(x)}\right)dx.
</script>
Convexity of <span class="arithmatex">\(f\)</span> ⇒ convexity of <span class="arithmatex">\(D_f\)</span>.<br>
Common choices recover KL, χ², Hellinger, and Jensen–Shannon divergences.</p>
<h2 id="appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning">E.5 Convex Loss Functions in Statistics and Machine Learning<a class="headerlink" href="#appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning" title="Permanent link">¶</a></h2>
<p>Convexity ensures estimators are globally optimal and algorithms converge.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Loss / Negative Log-Likelihood</th>
<th>Convexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian noise</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2\)</span></td>
<td>quadratic, strongly convex</td>
</tr>
<tr>
<td>Laplace noise</td>
<td><span class="arithmatex">\(\|Ax-b\|_1\)</span></td>
<td>convex, nonsmooth</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\log(1+e^{-y a^\top x})\)</span></td>
<td>convex, smooth</td>
</tr>
<tr>
<td>Poisson regression</td>
<td><span class="arithmatex">\(e^{a^\top x}-y a^\top x\)</span></td>
<td>convex, exponential</td>
</tr>
<tr>
<td>Huber loss</td>
<td>piecewise quadratic/linear</td>
<td>convex, robust</td>
</tr>
</tbody>
</table>
<p>Convexity of the negative log-likelihood follows from convexity of the log-partition function <span class="arithmatex">\(A(\theta)\)</span> in exponential families.</p>
<h2 id="appendices-170_probability-e6-convexity-and-bayesian-inference">E.6 Convexity and Bayesian Inference<a class="headerlink" href="#appendices-170_probability-e6-convexity-and-bayesian-inference" title="Permanent link">¶</a></h2>
<p>In Bayesian inference, convexity appears in:</p>
<ul>
<li>
<p>Log-concave posteriors:<br>
  If the likelihood and prior are log-concave, the posterior <span class="arithmatex">\(p(x|y)\propto \exp(-f(x))\)</span> is also log-concave ⇒<br>
<span class="arithmatex">\(\log p(x|y)\)</span> concave, <span class="arithmatex">\(f(x)\)</span> convex.</p>
</li>
<li>
<p>MAP estimation:<br>
  Maximizing <span class="arithmatex">\(\log p(x|y)\)</span> ≡ minimizing a convex function when <span class="arithmatex">\(p(x|y)\)</span> is log-concave ⇒ global optimum guaranteed.</p>
</li>
<li>
<p>Variational inference:<br>
  The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).<br>
  Optimizing it is equivalent to minimizing a convex divergence.</p>
</li>
</ul>
<p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>
<h2 id="appendices-170_probability-e7-statistical-risk-and-convex-surrogates">E.7 Statistical Risk and Convex Surrogates<a class="headerlink" href="#appendices-170_probability-e7-statistical-risk-and-convex-surrogates" title="Permanent link">¶</a></h2>
<p>Convex surrogate losses replace nonconvex 0–1 loss with convex approximations:</p>
<ul>
<li>Hinge loss (<span class="arithmatex">\(\max(0,1-y a^\top x)\)</span>) → support-vector machines.  </li>
<li>Logistic loss → probabilistic classification (cross-entropy).  </li>
<li>Exponential loss → AdaBoost.</li>
</ul>
<p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p></body></html></section><section class="print-page" id="appendices-180_subgradient_methods" heading-number="8.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-f-subgradient-method-derivation-geometry-and-convergence">Appendix F: Subgradient Method: Derivation, Geometry, and Convergence<a class="headerlink" href="#appendices-180_subgradient_methods-appendix-f-subgradient-method-derivation-geometry-and-convergence" title="Permanent link">¶</a></h1>
<p>This appendix presents the subgradient method—the fundamental algorithm for minimizing nonsmooth convex functions.<br>
It generalizes gradient descent to functions such as the <span class="arithmatex">\(\ell_1\)</span> norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>
<h2 id="appendices-180_subgradient_methods-f1-problem-setup">F.1 Problem Setup<a class="headerlink" href="#appendices-180_subgradient_methods-f1-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is convex but possibly nondifferentiable and <span class="arithmatex">\(\mathcal{X}\)</span> is a convex feasible set.</p>
<h2 id="appendices-180_subgradient_methods-f2-subgradients-and-geometry">F.2 Subgradients and Geometry<a class="headerlink" href="#appendices-180_subgradient_methods-f2-subgradients-and-geometry" title="Permanent link">¶</a></h2>
<p>A subgradient <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span> satisfies</p>
<div class="arithmatex">\[
f(y) \ge f(x_t) + \langle g_t,\, y - x_t \rangle, \quad \forall y \in \mathcal{X}.
\]</div>
<ul>
<li>If <span class="arithmatex">\(f\)</span> is differentiable, <span class="arithmatex">\(\partial f(x_t) = \{\nabla f(x_t)\}\)</span>.  </li>
<li>At a nonsmooth point (e.g. <span class="arithmatex">\(|x|\)</span> at <span class="arithmatex">\(x=0\)</span>), <span class="arithmatex">\(\partial f(x_t)\)</span> is a set of supporting slopes.  </li>
<li>Each subgradient defines a supporting hyperplane below the graph of <span class="arithmatex">\(f\)</span>.</li>
</ul>
<p>Hence a subgradient gives a descent direction even when <span class="arithmatex">\(f\)</span> lacks a unique gradient.</p>
<h2 id="appendices-180_subgradient_methods-f3-update-rule-and-projection-view">F.3 Update Rule and Projection View<a class="headerlink" href="#appendices-180_subgradient_methods-f3-update-rule-and-projection-view" title="Permanent link">¶</a></h2>
<p>The projected subgradient step is</p>
<div class="arithmatex">\[
x_{t+1} = \Pi_{\mathcal{X}}\!\big(x_t - \eta_t g_t\big),
\]</div>
<p>where
- <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span>,<br>
- <span class="arithmatex">\(\eta_t&gt;0\)</span> is the step size,<br>
- <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> projects onto <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<p>If <span class="arithmatex">\(\mathcal{X} = \mathbb{R}^n\)</span>, projection disappears:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta_t g_t.
</script>
</p>
<p>Geometric view: move in a subgradient direction, then project back to feasibility.<br>
The method “slides” along the edges of <span class="arithmatex">\(f\)</span>’s epigraph.</p>
<h2 id="appendices-180_subgradient_methods-f4-distance-analysis">F.4 Distance Analysis<a class="headerlink" href="#appendices-180_subgradient_methods-f4-distance-analysis" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x^\star\)</span> be an optimal solution. Expanding the squared distance:</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
= \|x_t - x^\star\|^2
- 2\eta_t\langle g_t, x_t - x^\star\rangle
+ \eta_t^2 \|g_t\|^2.
\]</div>
<p>By convexity,
<script type="math/tex; mode=display">
f(x_t) - f(x^\star) \le \langle g_t, x_t - x^\star\rangle.
</script>
</p>
<p>Substitute to get</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
\le
\|x_t - x^\star\|^2
- 2\eta_t\big(f(x_t)-f(x^\star)\big)
+ \eta_t^2 \|g_t\|^2.
\]</div>
<h2 id="appendices-180_subgradient_methods-f5-bounding-suboptimality">F.5 Bounding Suboptimality<a class="headerlink" href="#appendices-180_subgradient_methods-f5-bounding-suboptimality" title="Permanent link">¶</a></h2>
<p>Rearranging:</p>
<div class="arithmatex">\[
f(x_t)-f(x^\star)
\le
\frac{\|x_t-x^\star\|^2 - \|x_{t+1}-x^\star\|^2}{2\eta_t}
+ \frac{\eta_t}{2}\|g_t\|^2.
\]</div>
<p>This shows a trade-off:</p>
<ul>
<li>Large <span class="arithmatex">\(\eta_t\)</span> → faster steps but higher error term.  </li>
<li>Small <span class="arithmatex">\(\eta_t\)</span> → more precise but slower progress.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f6-convergence-rate">F.6 Convergence Rate<a class="headerlink" href="#appendices-180_subgradient_methods-f6-convergence-rate" title="Permanent link">¶</a></h2>
<p>Assume <span class="arithmatex">\(\|g_t\| \le G\)</span>. Summing over <span class="arithmatex">\(t=0,\dots,T-1\)</span>:</p>
<div class="arithmatex">\[
\sum_{t=0}^{T-1}\!\big(f(x_t)-f(x^\star)\big)
\le
\frac{\|x_0-x^\star\|^2}{2\eta}
+ \frac{\eta G^2 T}{2}.
\]</div>
<p>Define <span class="arithmatex">\(\bar{x}_T = \tfrac{1}{T}\sum_{t=0}^{T-1} x_t\)</span>.<br>
By convexity of <span class="arithmatex">\(f\)</span>,</p>
<div class="arithmatex">\[
f(\bar{x}_T)-f(x^\star)
\le
\frac{\|x_0-x^\star\|^2}{2\eta T}
+ \frac{\eta G^2}{2}.
\]</div>
<p>Choosing <span class="arithmatex">\(\eta_t = \tfrac{R}{G\sqrt{T}}\)</span> with <span class="arithmatex">\(R=\|x_0-x^\star\|\)</span> yields</p>
<p>
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^\star)
\le
\frac{RG}{\sqrt{T}},
</script>
i.e. a sublinear rate <span class="arithmatex">\(O(1/\sqrt{T})\)</span>.</p>
<h2 id="appendices-180_subgradient_methods-f7-interpretation-and-practice">F.7 Interpretation and Practice<a class="headerlink" href="#appendices-180_subgradient_methods-f7-interpretation-and-practice" title="Permanent link">¶</a></h2>
<ul>
<li>Works for any convex function, smooth or not.  </li>
<li>Converges slower than smooth-gradient methods (<span class="arithmatex">\(O(1/T)\)</span> or linear), but applies more generally.  </li>
<li>Step size schedule is crucial:<br>
<span class="arithmatex">\(\eta_t \!\downarrow 0\)</span> for convergence, or fixed <span class="arithmatex">\(\eta\)</span> for steady error.  </li>
<li>Averaging <span class="arithmatex">\(\bar{x}_T\)</span> improves stability.</li>
</ul>
<h3 id="appendices-180_subgradient_methods-typical-ml-uses">Typical ML Uses<a class="headerlink" href="#appendices-180_subgradient_methods-typical-ml-uses" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Objective</th>
<th>Nonsmooth Term</th>
</tr>
</thead>
<tbody>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td><span class="arithmatex">\(\ell_1\)</span> penalty</td>
</tr>
<tr>
<td>SVM</td>
<td><span class="arithmatex">\(\tfrac12\|w\|^2 + C\sum_i \max(0,1-y_i w^\top x_i)\)</span></td>
<td>hinge loss</td>
</tr>
<tr>
<td>Robust regression</td>
<td>$\sum_i</td>
<td>a_i^\top x - b_i</td>
</tr>
<tr>
<td>Neural nets</td>
<td><span class="arithmatex">\(\|w\|_1\)</span> or ReLU activations</td>
<td>piecewise linear</td>
</tr>
</tbody>
</table>
<h2 id="appendices-180_subgradient_methods-f8-beyond-basic-subgradients">F.8 Beyond Basic Subgradients<a class="headerlink" href="#appendices-180_subgradient_methods-f8-beyond-basic-subgradients" title="Permanent link">¶</a></h2>
<p>Many advanced methods refine or accelerate the basic idea:</p>
<ul>
<li>Stochastic subgradients: sample-based updates for large-scale ML.  </li>
<li>Mirror descent: adapt geometry via Bregman divergences.  </li>
<li>Proximal methods: replace step with proximal operator (see Appendix B).  </li>
<li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f9-summary">F.9 Summary<a class="headerlink" href="#appendices-180_subgradient_methods-f9-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Subgradients generalize gradients to nondifferentiable convex functions.  </li>
<li>The projected subgradient method provides a universal, robust minimization algorithm.  </li>
<li>Achieves <span class="arithmatex">\(O(1/\sqrt{T})\)</span> convergence under bounded subgradients.  </li>
<li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9–10.</li>
</ul></body></html></section><section class="print-page" id="appendices-190_proximal" heading-number="8.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization">Appendix G | Projections and Proximal Operators in Constrained Convex Optimization<a class="headerlink" href="#appendices-190_proximal-appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization" title="Permanent link">¶</a></h1>
<p>Many convex optimization problems involve constraints or nonsmooth penalties.<br>
This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>
<h2 id="appendices-190_proximal-g1-problem-setup">G.1 Problem Setup<a class="headerlink" href="#appendices-190_proximal-g1-problem-setup" title="Permanent link">¶</a></h2>
<p>We wish to minimize a convex, differentiable function <span class="arithmatex">\( f(x) \)</span> subject to a convex feasible set <span class="arithmatex">\( \mathcal{X} \subseteq \mathbb{R}^n \)</span>:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x).
\]</div>
<p>A plain gradient step,</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t),
\]</div>
<p>may leave <span class="arithmatex">\( x_{t+1} \notin \mathcal{X} \)</span>.<br>
We fix this by projecting the iterate back into the feasible region.</p>
<h2 id="appendices-190_proximal-g2-projection-operator">G.2 Projection Operator<a class="headerlink" href="#appendices-190_proximal-g2-projection-operator" title="Permanent link">¶</a></h2>
<p>The projection of a point <span class="arithmatex">\(y\)</span> onto a convex set <span class="arithmatex">\(\mathcal{X}\)</span> is</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2.
\]</div>
<p>Hence, the projected gradient descent update is</p>
<div class="arithmatex">\[
x_{t+1} = \text{Proj}_{\mathcal{X}}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<h3 id="appendices-190_proximal-geometric-insight">Geometric Insight<a class="headerlink" href="#appendices-190_proximal-geometric-insight" title="Permanent link">¶</a></h3>
<ul>
<li>Take a descent step possibly outside the feasible set.  </li>
<li>Project back to the closest feasible point.  </li>
<li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li>
</ul>
<p>Example — Euclidean ball:<br>
If <span class="arithmatex">\( \mathcal{X} = \{x : \|x\|_2 \le 1\} \)</span>, then</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y) = \frac{y}{\max(1, \|y\|_2)}.
\]</div>
<ul>
<li>Inside the ball → unchanged.  </li>
<li>Outside → scaled back to the boundary.</li>
</ul>
<hr>
<h2 id="appendices-190_proximal-g3-from-projections-to-proximal-operators">G.3 From Projections to Proximal Operators<a class="headerlink" href="#appendices-190_proximal-g3-from-projections-to-proximal-operators" title="Permanent link">¶</a></h2>
<p>Projections handle explicit constraints, but many problems use implicit penalties — e.g. sparsity (<span class="arithmatex">\(\|x\|_1\)</span>), total variation, or nonnegativity penalties.</p>
<p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>
<h3 id="appendices-190_proximal-definition">Definition<a class="headerlink" href="#appendices-190_proximal-definition" title="Permanent link">¶</a></h3>
<p>For a convex (possibly nondifferentiable) function <span class="arithmatex">\( g(x) \)</span>,</p>
<p>
<script type="math/tex; mode=display">
\text{prox}_{\lambda g}(y)
= \arg\min_x \Big( g(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big),
</script>
where <span class="arithmatex">\( \lambda &gt; 0 \)</span> balances regularization vs. proximity.</p>
<h3 id="appendices-190_proximal-interpretation">Interpretation<a class="headerlink" href="#appendices-190_proximal-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>The quadratic term <span class="arithmatex">\( \tfrac{1}{2\lambda}\|x - y\|^2 \)</span> keeps <span class="arithmatex">\(x\)</span> close to <span class="arithmatex">\(y\)</span>.  </li>
<li>The function <span class="arithmatex">\( g(x) \)</span> encourages structure (sparsity, smoothness, feasibility).  </li>
<li>Small <span class="arithmatex">\(\lambda\)</span>: conservative correction; large <span class="arithmatex">\(\lambda\)</span>: stronger regularization.</li>
</ul>
<p>The proximal step acts as a soft correction after a gradient step.</p>
<hr>
<h2 id="appendices-190_proximal-g4-projection-as-a-special-case">G.4 Projection as a Special Case<a class="headerlink" href="#appendices-190_proximal-g4-projection-as-a-special-case" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<div class="arithmatex">\[
I_{\mathcal{X}}(x) =
\begin{cases}
0, &amp; x \in \mathcal{X}, \\[4pt]
+\infty, &amp; x \notin \mathcal{X}.
\end{cases}
\]</div>
<p>Substitute <span class="arithmatex">\(g(x)=I_{\mathcal{X}}(x)\)</span> into the proximal definition:</p>
<div class="arithmatex">\[
\text{prox}_{\lambda I_{\mathcal{X}}}(y)
= \arg\min_x \Big( I_{\mathcal{X}}(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2
= \text{Proj}_{\mathcal{X}}(y).
\]</div>
<p>✅ Projection is just a proximal operator for an indicator function.</p>
<h2 id="appendices-190_proximal-g5-proximal-gradient-method">G.5 Proximal Gradient Method<a class="headerlink" href="#appendices-190_proximal-g5-proximal-gradient-method" title="Permanent link">¶</a></h2>
<p>When minimizing a composite convex objective
<script type="math/tex; mode=display">
\min_x \; f(x) + g(x),
</script>
where <span class="arithmatex">\(f\)</span> is smooth and <span class="arithmatex">\(g\)</span> convex (possibly nonsmooth), the proximal gradient method updates:</p>
<div class="arithmatex">\[
x_{t+1} = \text{prox}_{\eta g}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<ul>
<li>The gradient step reduces the smooth part <span class="arithmatex">\(f(x)\)</span>.  </li>
<li>The proximal step enforces structure via <span class="arithmatex">\(g(x)\)</span>.<br>
This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li>
</ul>
<h2 id="appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm">G.6 Example: Proximal Operator of the <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm" title="Permanent link">¶</a></h2>
<p>We seek</p>
<div class="arithmatex">\[
\text{prox}_{\lambda \|\cdot\|_1}(y)
= \arg\min_x \left( \lambda\|x\|_1 + \tfrac{1}{2}\|x - y\|^2 \right).
\]</div>
<h3 id="appendices-190_proximal-step-1-coordinate-separation">Step 1. Coordinate Separation<a class="headerlink" href="#appendices-190_proximal-step-1-coordinate-separation" title="Permanent link">¶</a></h3>
<p>The problem is separable across coordinates:
<script type="math/tex; mode=display">
\min_x \sum_i \Big(\lambda |x_i| + \tfrac{1}{2}(x_i - y_i)^2\Big),
</script>
so each coordinate solves
<script type="math/tex; mode=display">
\min_x \phi(x) = \lambda|x| + \tfrac{1}{2}(x - y)^2.
</script>
</p>
<h3 id="appendices-190_proximal-step-2-subgradient-optimality">Step 2. Subgradient Optimality<a class="headerlink" href="#appendices-190_proximal-step-2-subgradient-optimality" title="Permanent link">¶</a></h3>
<p>Optimality condition:
<script type="math/tex; mode=display">
0 \in \partial\phi(x^\star) = \lambda \partial|x^\star| + (x^\star - y).
</script>
Thus,
<script type="math/tex; mode=display">
x^\star = y - \lambda s, \quad s \in \partial |x^\star|.
</script>
</p>
<h3 id="appendices-190_proximal-step-3-case-analysis">Step 3. Case Analysis<a class="headerlink" href="#appendices-190_proximal-step-3-case-analysis" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Case</th>
<th>Condition</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x^\star&gt;0\)</span></td>
<td><span class="arithmatex">\(y&gt;\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y - \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star&lt;0\)</span></td>
<td><span class="arithmatex">\(y&lt;-\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y + \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star=0\)</span></td>
<td>(</td>
<td>y</td>
</tr>
</tbody>
</table>
<h3 id="appendices-190_proximal-step-4-compact-form">Step 4. Compact Form<a class="headerlink" href="#appendices-190_proximal-step-4-compact-form" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\boxed{
\text{prox}_{\lambda|\cdot|}(y)
= \text{sign}(y) \cdot \max(|y| - \lambda,\, 0)
}
\]</div>
<p>This is the soft-thresholding operator.</p>
<h3 id="appendices-190_proximal-step-5-vector-case">Step 5. Vector Case<a class="headerlink" href="#appendices-190_proximal-step-5-vector-case" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(y \in \mathbb{R}^n\)</span>,</p>
<div class="arithmatex">\[
\big(\text{prox}_{\lambda\|\cdot\|_1}(y)\big)_i
= \text{sign}(y_i)\cdot\max(|y_i| - \lambda, 0).
\]</div>
<p>Each coordinate is independently shrunk toward zero — producing sparse solutions.</p>
<h3 id="appendices-190_proximal-step-6-interpretation">Step 6. Interpretation<a class="headerlink" href="#appendices-190_proximal-step-6-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>Coordinates with <span class="arithmatex">\(|y_i| \le \lambda\)</span> → set to zero (promotes sparsity).  </li>
<li>Coordinates with <span class="arithmatex">\(|y_i| &gt; \lambda\)</span> → shrink by <span class="arithmatex">\(\lambda\)</span>.  </li>
<li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li>
</ul>
<h2 id="appendices-190_proximal-g7-geometry-and-connection-to-algorithms">G.7 Geometry and Connection to Algorithms<a class="headerlink" href="#appendices-190_proximal-g7-geometry-and-connection-to-algorithms" title="Permanent link">¶</a></h2>
<ul>
<li>Projection = nearest feasible point → handles <em>hard constraints</em>.  </li>
<li>Proximal operator = nearest structured point → handles <em>soft regularization</em>.  </li>
<li>Proximal gradient = combines both, yielding algorithms like:</li>
<li>ISTA / FISTA (sparse recovery, LASSO),</li>
<li>Projected gradient (feasibility),</li>
<li>ADMM (splitting into subproblems).</li>
</ul>
<p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>
<h2 id="appendices-190_proximal-g8-summary">G.8 Summary<a class="headerlink" href="#appendices-190_proximal-g8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li>
<li>Projection is a special case of the proximal operator for an indicator function.  </li>
<li>Proximal mappings handle nonsmooth regularizers (e.g., <span class="arithmatex">\(\ell_1\)</span>-norm).  </li>
<li>The proximal gradient method unifies constrained and regularized optimization.  </li>
<li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li>
</ul></body></html></section><section class="print-page" id="appendices-200_mirror" heading-number="8.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-h-mirror-descent-and-bregman-geometry">Appendix H: Mirror Descent and Bregman Geometry<a class="headerlink" href="#appendices-200_mirror-appendix-h-mirror-descent-and-bregman-geometry" title="Permanent link">¶</a></h1>
<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry.<br>
In many structured domains—such as probability simplices or sparse models—Euclidean updates can destroy problem structure or cause instability.  </p>
<p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence.<br>
It performs gradient-like updates in a dual space, respecting the <em>intrinsic geometry</em> of the domain.</p>
<h2 id="appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd">H.1 Motivation and Limitations of Euclidean GD<a class="headerlink" href="#appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd" title="Permanent link">¶</a></h2>
<p>Standard GD update:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta \nabla f(x_t)
</script>
assumes Euclidean distance
<script type="math/tex; mode=display">
\|x-y\|_2 = \sqrt{\sum_i (x_i - y_i)^2}.
</script>
</p>
<p>This works well in <span class="arithmatex">\(\mathbb{R}^n\)</span> without structure, but fails to respect constraints or sparsity.</p>
<p>In practice:</p>
<ul>
<li>Many parameters are nonnegative or normalized (probabilities, weights).  </li>
<li>Euclidean steps can violate constraints or zero out coordinates.  </li>
<li>The “flat” <span class="arithmatex">\(\ell_2\)</span> geometry treats all directions equally.</li>
</ul>
<blockquote>
<p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the <em>metric</em> via a mirror map.</p>
</blockquote>
<h2 id="appendices-200_mirror-h2-geometry-in-optimization">H.2 Geometry in Optimization<a class="headerlink" href="#appendices-200_mirror-h2-geometry-in-optimization" title="Permanent link">¶</a></h2>
<p>The “steepest descent” direction depends on the notion of distance.<br>
GD implicitly minimizes a <em>linearized loss</em> plus a Euclidean proximity term.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Natural Constraint</th>
<th>Appropriate Geometry</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability vectors</td>
<td><span class="arithmatex">\(x_i\ge0, \sum_i x_i=1\)</span></td>
<td>KL / entropy geometry</td>
</tr>
<tr>
<td>Sparse models</td>
<td><span class="arithmatex">\(\|x\|_1\)</span>-structured</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
</tr>
<tr>
<td>Online learning</td>
<td>multiplicative updates</td>
<td>log-space geometry</td>
</tr>
</tbody>
</table>
<p>Using Euclidean projections in these domains can cause:</p>
<ul>
<li>abrupt projection onto boundaries,</li>
<li>loss of positivity or sparsity,</li>
<li>geometric inconsistency.</li>
</ul>
<h2 id="appendices-200_mirror-h3-mirror-descent-framework">H.3 Mirror Descent Framework<a class="headerlink" href="#appendices-200_mirror-h3-mirror-descent-framework" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(\psi(x)\)</span> be a mirror map — a strictly convex, differentiable potential encoding the geometry.</p>
<p>Define the dual coordinate:
<script type="math/tex; mode=display">
u = \nabla \psi(x),
</script>
and its inverse mapping through the convex conjugate <span class="arithmatex">\(\psi^*\)</span>:
<script type="math/tex; mode=display">
x = \nabla \psi^*(u).
</script>
</p>
<h3 id="appendices-200_mirror-bregman-divergence">Bregman Divergence<a class="headerlink" href="#appendices-200_mirror-bregman-divergence" title="Permanent link">¶</a></h3>
<p>The geometry is quantified by the Bregman divergence:
<script type="math/tex; mode=display">
D_\psi(x \| y)
= \psi(x) - \psi(y) - \langle \nabla\psi(y), x - y \rangle.
</script>
</p>
<ul>
<li>Measures how nonlinear <span class="arithmatex">\(\psi\)</span> is between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\tfrac12\|x\|_2^2\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes <span class="arithmatex">\(\tfrac12\|x-y\|_2^2\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes KL divergence.</li>
</ul>
<h2 id="appendices-200_mirror-h4-mirror-descent-update-rule">H.4 Mirror Descent Update Rule<a class="headerlink" href="#appendices-200_mirror-h4-mirror-descent-update-rule" title="Permanent link">¶</a></h2>
<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:
<script type="math/tex; mode=display">
x_{t+1}
= \arg\min_{x\in\mathcal{X}}
\Big\{ \langle \nabla f(x_t), x - x_t\rangle
+ \tfrac{1}{\eta} D_\psi(x \| x_t) \Big\}.
</script>
</p>
<p>Equivalent dual-space form:
<script type="math/tex; mode=display">
\begin{aligned}
u_t &= \nabla \psi(x_t),\\
u_{t+1} &= u_t - \eta \nabla f(x_t),\\
x_{t+1} &= \nabla \psi^*(u_{t+1}).
\end{aligned}
</script>
</p>
<p>✅ MD is gradient descent in dual coordinates, where distances are measured by <span class="arithmatex">\(D_\psi\)</span> instead of <span class="arithmatex">\(\|x-y\|_2\)</span>.</p>
<h2 id="appendices-200_mirror-h5-comparing-gd-projected-gd-and-md">H.5 Comparing GD, Projected GD, and MD<a class="headerlink" href="#appendices-200_mirror-h5-comparing-gd-projected-gd-and-md" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Update Rule</th>
<th>Geometry</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(x - \eta\nabla f\)</span></td>
<td>Euclidean</td>
<td>may leave feasible set</td>
</tr>
<tr>
<td>Projected GD</td>
<td><span class="arithmatex">\(\text{Proj}(x - \eta\nabla f)\)</span></td>
<td>Euclidean + projection</td>
<td>can cause discontinuous jumps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td><span class="arithmatex">\(\arg\min_x \langle\nabla f, x - x_t\rangle + \frac{1}{\eta}D_\psi(x\|x_t)\)</span></td>
<td>Bregman</td>
<td>smooth, structure-preserving</td>
</tr>
</tbody>
</table>
<h2 id="appendices-200_mirror-h6-simplex-example-kl-geometry">H.6 Simplex Example (KL Geometry)<a class="headerlink" href="#appendices-200_mirror-h6-simplex-example-kl-geometry" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x\in\Delta^2=\{x\ge0, x_1+x_2=1\}\)</span>, objective <span class="arithmatex">\(f(x)=x_1^2+2x_2\)</span>, <span class="arithmatex">\(\eta=0.3\)</span>.</p>
<h3 id="appendices-200_mirror-euclidean-gd-projection">Euclidean GD + Projection<a class="headerlink" href="#appendices-200_mirror-euclidean-gd-projection" title="Permanent link">¶</a></h3>
<ol>
<li><span class="arithmatex">\(\nabla f=(2x_1,2)=(1,2)\)</span>,  </li>
<li><span class="arithmatex">\(y=x-\eta\nabla f=(0.2,-0.1)\)</span>,  </li>
<li>Project → <span class="arithmatex">\(x_{new}=(1,0)\)</span>.</li>
</ol>
<p>→ Projection kills one coordinate ⇒ lost smoothness.</p>
<h3 id="appendices-200_mirror-mirror-descent-with-negative-entropy">Mirror Descent with Negative Entropy<a class="headerlink" href="#appendices-200_mirror-mirror-descent-with-negative-entropy" title="Permanent link">¶</a></h3>
<p>Mirror map <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>.<br>
Update:
<script type="math/tex; mode=display">
x_i^{new}\propto x_i\exp(-\eta\nabla_i f(x)),
\quad \text{then normalize.}
</script>
Gives <span class="arithmatex">\(x\approx(0.57,0.43)\)</span> — smooth, positive, stays in simplex.</p>
<blockquote>
<p>MD follows the manifold of the simplex naturally—no harsh projection.</p>
</blockquote>
<h2 id="appendices-200_mirror-h7-choosing-the-mirror-map">H.7 Choosing the Mirror Map<a class="headerlink" href="#appendices-200_mirror-h7-choosing-the-mirror-map" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Mirror Map <span class="arithmatex">\(\psi(x)\)</span></th>
<th>Bregman Divergence <span class="arithmatex">\(D_\psi\)</span></th>
<th>Typical Domain / Application</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\tfrac12\|x\|_2^2\)</span></td>
<td>Euclidean distance</td>
<td>unconstrained <span class="arithmatex">\(\mathbb{R}^n\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(\sum_i x_i\log x_i\)</span></td>
<td>KL divergence</td>
<td>simplex, probabilities</td>
</tr>
<tr>
<td><span class="arithmatex">\(\|x\|_1\)</span> or variants</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
<td>sparse models</td>
</tr>
<tr>
<td>log-barrier <span class="arithmatex">\(\sum_i -\log x_i\)</span></td>
<td>barrier divergence</td>
<td>positive orthant</td>
</tr>
</tbody>
</table>
<p>Mirror maps act as design choices defining the optimization geometry.</p>
<h2 id="appendices-200_mirror-h8-practical-remarks">H.8 Practical Remarks<a class="headerlink" href="#appendices-200_mirror-h8-practical-remarks" title="Permanent link">¶</a></h2>
<p>When to prefer Mirror Descent:</p>
<ul>
<li>Structured domains (simplex, positive vectors, sparse spaces)</li>
<li>Smooth, structure-preserving updates desired</li>
<li>Avoiding discontinuous projections</li>
</ul>
<p>Computational notes:</p>
<ul>
<li>Some <span class="arithmatex">\(\psi\)</span> yield closed-form updates (e.g. multiplicative weights).  </li>
<li>Works with adaptive or momentum step-size schemes.  </li>
<li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li>
</ul>
<hr>
<h2 id="appendices-200_mirror-h9-convergence-at-a-glance">H.9 Convergence at a Glance<a class="headerlink" href="#appendices-200_mirror-h9-convergence-at-a-glance" title="Permanent link">¶</a></h2>
<p>For convex <span class="arithmatex">\(f\)</span> with bounded gradients <span class="arithmatex">\(\|\nabla f\|\le G\)</span> and strong convex mirror map <span class="arithmatex">\(\psi\)</span>,
Mirror Descent achieves the same sublinear rate as projected subgradient methods:
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^*)
\le O\!\left(\frac{1}{\sqrt{T}}\right),
</script>
but with improved <em>geometry-adapted</em> constants that exploit curvature of <span class="arithmatex">\(\psi\)</span>.</p></body></html></section><section class="print-page" id="appendices-300_matrixfactorization" heading-number="8.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="numerical-linear-algebra-for-convex-optimization">Numerical Linear Algebra for Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-numerical-linear-algebra-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>Numerical linear algebra is the computational foundation of convex optimization.
Every modern optimization algorithm — from Newton’s method to interior-point or proximal algorithms — ultimately requires solving a structured linear system:
<script type="math/tex; mode=display">
H x = b,
</script>
where <span class="arithmatex">\(H\)</span> may represent a Hessian, a normal equations matrix, or a KKT (Karush–Kuhn–Tucker) system.</p>
<p>In practice, we never compute <span class="arithmatex">\(H^{-1}\)</span> directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>
<h2 id="appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization">1. Why Linear Algebra Matters in Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization" title="Permanent link">¶</a></h2>
<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p>
<ul>
<li>
<p>Newton’s method:
  <script type="math/tex; mode=display">
  \nabla^2 f(x_k), \\ \Delta x = -\nabla f(x_k)
  </script>
</p>
</li>
<li>
<p>Interior-point methods (KKT systems):</p>
</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
H &amp; A^T \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\[3pt] \Delta \lambda
\end{bmatrix}
=
\begin{bmatrix}
-r_d \\[3pt] -r_p
\end{bmatrix}
\]</div>
<ul>
<li>Least-squares problems: <span class="arithmatex">\(A^T A x = A^T b\)</span></li>
</ul>
<p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>
<h2 id="appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox">2. The Matrix Factorization Toolbox<a class="headerlink" href="#appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox" title="Permanent link">¶</a></h2>
<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure.
They enable efficient triangular solves instead of direct inversion.</p>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Applies To</th>
<th>Form</th>
<th>Common Use</th>
<th>Key Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td>Any nonsingular matrix</td>
<td><span class="arithmatex">\(A = L U\)</span></td>
<td>General linear systems</td>
<td>Requires pivoting for stability</td>
</tr>
<tr>
<td>QR</td>
<td>Any (rectangular) matrix</td>
<td><span class="arithmatex">\(A = Q R\)</span></td>
<td>Least-squares</td>
<td>Orthogonal, stable</td>
</tr>
<tr>
<td>Cholesky</td>
<td>Symmetric positive definite</td>
<td><span class="arithmatex">\(A = L L^T\)</span></td>
<td>SPD systems, normal equations</td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td><span class="arithmatex">\(LDL^T\)</span></td>
<td>Symmetric indefinite</td>
<td><span class="arithmatex">\(A = L D L^T\)</span></td>
<td>KKT systems</td>
<td>Handles indefiniteness</td>
</tr>
<tr>
<td>Eigen</td>
<td>Symmetric/Hermitian</td>
<td><span class="arithmatex">\(A = Q \Lambda Q^T\)</span></td>
<td>Curvature, convexity checks</td>
<td>Diagonalizes <span class="arithmatex">\(A\)</span></td>
</tr>
<tr>
<td>SVD</td>
<td>Any matrix</td>
<td><span class="arithmatex">\(A = U \Sigma V^T\)</span></td>
<td>Rank, conditioning, pseudoinverse</td>
<td>Most stable, expensive</td>
</tr>
</tbody>
</table>
<p>Each factorization corresponds to a <em>numerically preferred strategy</em> for certain classes of problems.</p>
<h2 id="appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse">3. LU Factorization — <em>The General-Purpose Workhorse</em><a class="headerlink" href="#appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = P L U
</script>
where <span class="arithmatex">\(P\)</span> is a permutation matrix ensuring stability.</p>
<ul>
<li>Used for: General linear systems, nonsymmetric matrices.</li>
<li>Cost: <span class="arithmatex">\(\approx \tfrac{2}{3}n^3\)</span> (dense).</li>
<li>Stability: Requires partial pivoting (<span class="arithmatex">\(PA=LU\)</span>) to avoid numerical blow-up.</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving KKT systems in linear programming (LP simplex tableau).</li>
<li>Small dense systems with no symmetry or SPD property.</li>
</ul>
<p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or <span class="arithmatex">\(LDL^T\)</span>.</p>
<h2 id="appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable">4. QR Factorization — <em>Orthogonal and Stable</em><a class="headerlink" href="#appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = Q R, \quad Q^T Q = I, \ R \text{ upper triangular.}
</script>
</p>
<ul>
<li>Used for: Least-squares problems
  <script type="math/tex; mode=display">
  \min_x |A x - b|_2^2.
  </script>
  Instead of forming normal equations (<span class="arithmatex">\(A^T A x = A^T b\)</span>), we solve:
  <script type="math/tex; mode=display">
  R x = Q^T b.
  </script>
</li>
<li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Linear regression via least squares.</li>
<li>ADMM and proximal steps with overdetermined systems.</li>
<li>Orthogonal projections in signal processing.</li>
</ul>
<p>Variants:</p>
<ul>
<li>Householder QR: numerically robust, used in LAPACK.</li>
<li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems">5. Cholesky Factorization — <em>Fastest for SPD Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L L^T, \quad L \text{ lower triangular.}
</script>
Applicable when <span class="arithmatex">\(A\)</span> is symmetric positive definite (SPD) — common in convex problems.</p>
<p>Why it’s central:
Convexity ensures <span class="arithmatex">\(A \succeq 0\)</span>.
For strictly convex problems, <span class="arithmatex">\(A \succ 0\)</span> and Cholesky is the most efficient and stable method.</p>
<p>Cost: <span class="arithmatex">\(\tfrac{1}{3}n^3\)</span> operations — half of LU.</p>
<p>Example use cases:</p>
<ul>
<li>Newton’s method on unconstrained convex functions.</li>
<li>Solving normal equations <span class="arithmatex">\(A^T A x = A^T b\)</span>.</li>
<li>QP subproblems and ridge regression.</li>
</ul>
<p>Implementation detail:
No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>
<h2 id="appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems">6. LDLᵀ Factorization — <em>For Indefinite Symmetric Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L D L^T,
</script>
where <span class="arithmatex">\(D\)</span> is block diagonal (1×1 or 2×2 blocks), and <span class="arithmatex">\(L\)</span> is unit lower triangular.</p>
<p>Used when <span class="arithmatex">\(A\)</span> is symmetric but not SPD (e.g., KKT systems).</p>
<p>Example use cases:</p>
<ul>
<li>
<p>Interior-point methods for QPs and SDPs:
  <script type="math/tex; mode=display">
  \begin{bmatrix}
  Q & A^T \\ A & 0
  \end{bmatrix}
  \begin{bmatrix}
  \Delta x \\ \Delta \lambda
  \end{bmatrix} =
  \begin{bmatrix}
  r_1 \\ r_2
  \end{bmatrix}.
  </script>
</p>
</li>
<li>
<p>Equality-constrained least-squares.</p>
</li>
<li>Sparse symmetric indefinite systems in primal-dual algorithms.</li>
</ul>
<p>Algorithmic note:
Uses Bunch–Kaufman pivoting to maintain numerical stability.
In practice, LDLᵀ is used with sparse reordering and partial elimination.</p>
<h2 id="appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement">7. Block Systems and the Schur Complement<a class="headerlink" href="#appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement" title="Permanent link">¶</a></h2>
<p>Many KKT or structured systems naturally appear in block form:
<script type="math/tex; mode=display">
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix} =
\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}.
</script>
</p>
<p>Assuming <span class="arithmatex">\(A_{11}\)</span> is invertible:</p>
<ol>
<li>Eliminate <span class="arithmatex">\(x_1\)</span>:
   <script type="math/tex; mode=display">
   x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)
   </script>
</li>
<li>Substitute into the second block:
   <script type="math/tex; mode=display">
   (A_{22} - A_{21}A_{11}^{-1}A_{12})x_2 = b_2 - A_{21}A_{11}^{-1}b_1
   </script>
</li>
</ol>
<p>The matrix
<script type="math/tex; mode=display">
S = A_{22} - A_{21}A_{11}^{-1}A_{12}
</script>
is the Schur complement of <span class="arithmatex">\(A_{11}\)</span> in <span class="arithmatex">\(A\)</span>.</p>
<h3 id="appendices-300_matrixfactorization-schur-complement-in-optimization">Schur Complement in Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-schur-complement-in-optimization" title="Permanent link">¶</a></h3>
<ul>
<li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li>
<li>Preserves symmetry and often positive definiteness.</li>
<li>Foundation of block elimination and reduced Hessian methods.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Interior-point Newton systems (eliminate <span class="arithmatex">\(\Delta x\)</span> to get a system in <span class="arithmatex">\(\Delta \lambda\)</span>).</li>
<li>Partial elimination in sequential quadratic programming (SQP).</li>
<li>Covariance conditioning and Gaussian marginalization.</li>
</ul>
<p>Numerical caution: Never form <span class="arithmatex">\(A_{11}^{-1}\)</span> explicitly — use triangular solves via Cholesky or LU.</p>
<h2 id="appendices-300_matrixfactorization-8-block-elimination-algorithm">8. Block Elimination Algorithm<a class="headerlink" href="#appendices-300_matrixfactorization-8-block-elimination-algorithm" title="Permanent link">¶</a></h2>
<p>Given a nonsingular <span class="arithmatex">\(A_{11}\)</span>:</p>
<ol>
<li>Compute <span class="arithmatex">\(A_{11}^{-1}A_{12}\)</span> and <span class="arithmatex">\(A_{11}^{-1}b_1\)</span> by solving triangular systems.</li>
<li>Form <span class="arithmatex">\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\)</span>, <span class="arithmatex">\(\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\)</span>.</li>
<li>Solve <span class="arithmatex">\(Sx_2 = \tilde{b}\)</span>.</li>
<li>Recover <span class="arithmatex">\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\)</span>.</li>
</ol>
<p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p>
<p>Example use case:</p>
<ul>
<li>Partitioned least-squares with fixed and variable parameters.</li>
<li>Constrained optimization where some variables can be analytically eliminated.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices">9. Structured Plus Low-Rank Matrices<a class="headerlink" href="#appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices" title="Permanent link">¶</a></h2>
<p>Suppose we need to solve:
<script type="math/tex; mode=display">
(A + BC)x = b,
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(A \in \mathbb{R}^{n \times n}\)</span> is structured or easily invertible (e.g., diagonal or sparse),</li>
<li><span class="arithmatex">\(B \in \mathbb{R}^{n \times p}\)</span>, <span class="arithmatex">\(C \in \mathbb{R}^{p \times n}\)</span> are low rank.</li>
</ul>
<p>This situation arises when updating an existing system with a small modification.</p>
<h3 id="appendices-300_matrixfactorization-block-reformulation">Block Reformulation<a class="headerlink" href="#appendices-300_matrixfactorization-block-reformulation" title="Permanent link">¶</a></h3>
<p>Introduce <span class="arithmatex">\(y = Cx\)</span>, yielding:</p>
<p>$$
<script type="math/tex; mode=display">\begin{bmatrix}
A & B \\ C & -I
\end{bmatrix}</script>
<script type="math/tex; mode=display">\begin{bmatrix}
x \\ y
\end{bmatrix}</script>
=</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}
b \\ 0
\end{bmatrix}</script>.
$$</p>
<p>Block elimination gives:
<script type="math/tex; mode=display">
(I + C A^{-1} B)y = C A^{-1} b,
\quad
x = A^{-1}(b - By).
</script>
</p>
<h3 id="appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity">Matrix Inversion Lemma (Woodbury Identity)<a class="headerlink" href="#appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(A + BC\)</span> are nonsingular:
<script type="math/tex; mode=display">
(A + BC)^{-1} = A^{-1} - A^{-1}B(I + C A^{-1}B)^{-1}C A^{-1}.
</script>
</p>
<p>Example use cases:</p>
<ul>
<li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li>
<li>Ridge regression / kernel methods: low-rank updates to <span class="arithmatex">\((X^T X + \lambda I)^{-1}\)</span>.</li>
<li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li>
</ul>
<p>Numerical note: Avoid explicit inversion; use solves with <span class="arithmatex">\(A\)</span> and small dense matrices.</p>
<h2 id="appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity">10. Conditioning, Stability, and Sparsity<a class="headerlink" href="#appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-conditioning">Conditioning<a class="headerlink" href="#appendices-300_matrixfactorization-conditioning" title="Permanent link">¶</a></h3>
<ul>
<li>Condition number: <span class="arithmatex">\(\kappa(A) = |A||A^{-1}|\)</span> measures sensitivity to perturbations.</li>
<li>High <span class="arithmatex">\(\kappa(A)\)</span> ⇒ round-off errors amplified ⇒ ill-conditioning.</li>
<li>Regularization (adding <span class="arithmatex">\(\lambda I\)</span>) improves numerical stability.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-stability">Stability<a class="headerlink" href="#appendices-300_matrixfactorization-stability" title="Permanent link">¶</a></h3>
<ul>
<li>Orthogonal transformations (QR, SVD) are backward stable.</li>
<li>LU needs partial pivoting.</li>
<li>LDLᵀ needs symmetric pivoting (Bunch–Kaufman).</li>
<li>Cholesky is stable for SPD matrices.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-sparsity-and-fill-in">Sparsity and Fill-In<a class="headerlink" href="#appendices-300_matrixfactorization-sparsity-and-fill-in" title="Permanent link">¶</a></h3>
<ul>
<li>Large convex solvers exploit sparse Cholesky / LDLᵀ.</li>
<li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li>
<li>Symbolic factorization determines the pattern before numeric factorization.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning">11. Iterative Solvers and Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning" title="Permanent link">¶</a></h2>
<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>
<h3 id="appendices-300_matrixfactorization-common-iterative-methods">Common Iterative Methods<a class="headerlink" href="#appendices-300_matrixfactorization-common-iterative-methods" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>For</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>CG</td>
<td>SPD systems</td>
<td>Uses matrix–vector products; converges in ≤ n steps</td>
</tr>
<tr>
<td>MINRES / SYMMLQ</td>
<td>Symmetric indefinite</td>
<td>Handles KKT and saddle-point systems</td>
</tr>
<tr>
<td>GMRES / BiCGSTAB</td>
<td>Nonsymmetric</td>
<td>General-purpose Krylov solvers</td>
</tr>
</tbody>
</table>
<h3 id="appendices-300_matrixfactorization-preconditioning">Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-preconditioning" title="Permanent link">¶</a></h3>
<p>Preconditioners <span class="arithmatex">\(M \approx A^{-1}\)</span> improve convergence:</p>
<ul>
<li>Jacobi (diagonal): <span class="arithmatex">\(M = \text{diag}(A)^{-1}\)</span></li>
<li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li>
<li>Block preconditioners: use Schur complement approximations for KKT systems</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li>
<li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions">12. Eigenvalue and SVD Decompositions<a class="headerlink" href="#appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-eigenvalue-decomposition">Eigenvalue Decomposition<a class="headerlink" href="#appendices-300_matrixfactorization-eigenvalue-decomposition" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^T, \quad Q^T Q = I.
</script>
Reveals curvature, stability, and definiteness:</p>
<ul>
<li>Convexity ⇔ <span class="arithmatex">\(\Lambda \ge 0\)</span>.</li>
<li>Used in semidefinite programming (SDP) and spectral analysis.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#appendices-300_matrixfactorization-singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = U \Sigma V^T,
</script>
with <span class="arithmatex">\(\Sigma = \text{diag}(\sigma_i) \ge 0\)</span>.</p>
<p>Applications:</p>
<ul>
<li>Rank and condition number estimation (<span class="arithmatex">\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span>).</li>
<li>Low-rank approximation (<span class="arithmatex">\(A_k = U_k \Sigma_k V_k^T\)</span>).</li>
<li>Pseudoinverse: <span class="arithmatex">\(A^+ = V \Sigma^{-1} U^T\)</span>.</li>
<li>Convex relaxations: nuclear-norm minimization (matrix completion).</li>
</ul>
<h2 id="appendices-300_matrixfactorization-13-computational-complexity-summary">13. Computational Complexity Summary<a class="headerlink" href="#appendices-300_matrixfactorization-13-computational-complexity-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Dense Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td><span class="arithmatex">\(\frac{2}{3}n^3\)</span></td>
<td>Needs pivoting</td>
</tr>
<tr>
<td>Cholesky</td>
<td><span class="arithmatex">\(\frac{1}{3}n^3\)</span></td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td>QR</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>Stable, more memory</td>
</tr>
<tr>
<td>LDLᵀ</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>For indefinite</td>
</tr>
<tr>
<td>SVD</td>
<td><span class="arithmatex">\(\approx \frac{4}{3}n^3\)</span></td>
<td>Most accurate</td>
</tr>
<tr>
<td>CG / MINRES</td>
<td>Variable</td>
<td>Depends on condition number and preconditioning</td>
</tr>
</tbody>
</table>
<p>Sparse systems reduce cost to roughly <span class="arithmatex">\(O(n^{1.5})\)</span>–<span class="arithmatex">\(O(n^2)\)</span> depending on fill-in.</p>
<h2 id="appendices-300_matrixfactorization-14-example-applications-overview">14. Example Applications Overview<a class="headerlink" href="#appendices-300_matrixfactorization-14-example-applications-overview" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Matrix</th>
<th>Solver / Factorization</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unconstrained Newton step</td>
<td>SPD Hessian</td>
<td>Cholesky</td>
<td>Convex quadratic, ridge regression</td>
</tr>
<tr>
<td>Equality-constrained QP</td>
<td>Symmetric indefinite KKT</td>
<td>LDLᵀ</td>
<td>Interior-point QP solver</td>
</tr>
<tr>
<td>Overdetermined LS</td>
<td>Rectangular <span class="arithmatex">\(A\)</span></td>
<td>QR</td>
<td>Linear regression, ADMM subproblem</td>
</tr>
<tr>
<td>KKT block system</td>
<td>Block-symmetric</td>
<td>Schur complement</td>
<td>Primal-dual method</td>
</tr>
<tr>
<td>Low-rank correction</td>
<td><span class="arithmatex">\(A + U U^T\)</span></td>
<td>Woodbury</td>
<td>Kalman filter, online update</td>
</tr>
<tr>
<td>Rank-deficient system</td>
<td>Any</td>
<td>SVD</td>
<td>Matrix completion, regularization</td>
</tr>
<tr>
<td>Large-scale Hessian</td>
<td>SPD</td>
<td>CG + preconditioner</td>
<td>Logistic regression, large ML models</td>
</tr>
</tbody>
</table></body></html></section></section></div><style>.print-site-enumerate-headings #index > h1:before { content: '1 ' }

                .print-site-enumerate-headings #index h2:before { content: '1.' counter(counter-index-2) ' ' }
                .print-site-enumerate-headings #index h2 {  counter-reset: counter-index-3 ;  counter-increment: counter-index-2 }
            
                .print-site-enumerate-headings #index h3:before { content: '1.' counter(counter-index-2) '.' counter(counter-index-3) ' ' }
                .print-site-enumerate-headings #index h3 {  counter-increment: counter-index-3 }
            
.print-site-enumerate-headings #section-2 > h1:before { content: '2 ' }
.print-site-enumerate-headings #convex-11_intro > h1:before { content: '2.1 ' }

                .print-site-enumerate-headings #convex-11_intro h2:before { content: '2.1.' counter(counter-convex-11_intro-2) ' ' }
                .print-site-enumerate-headings #convex-11_intro h2 {  counter-increment: counter-convex-11_intro-2 }
            
.print-site-enumerate-headings #convex-12_vector > h1:before { content: '2.2 ' }

                .print-site-enumerate-headings #convex-12_vector h2:before { content: '2.2.' counter(counter-convex-12_vector-2) ' ' }
                .print-site-enumerate-headings #convex-12_vector h2 {  counter-increment: counter-convex-12_vector-2 }
            
.print-site-enumerate-headings #convex-13_calculus > h1:before { content: '2.3 ' }

                .print-site-enumerate-headings #convex-13_calculus h2:before { content: '2.3.' counter(counter-convex-13_calculus-2) ' ' }
                .print-site-enumerate-headings #convex-13_calculus h2 {  counter-increment: counter-convex-13_calculus-2 }
            
.print-site-enumerate-headings #convex-14_convexsets > h1:before { content: '2.4 ' }

                .print-site-enumerate-headings #convex-14_convexsets h2:before { content: '2.4.' counter(counter-convex-14_convexsets-2) ' ' }
                .print-site-enumerate-headings #convex-14_convexsets h2 {  counter-increment: counter-convex-14_convexsets-2 }
            
.print-site-enumerate-headings #convex-15_convexfunctions > h1:before { content: '2.5 ' }

                .print-site-enumerate-headings #convex-15_convexfunctions h2:before { content: '2.5.' counter(counter-convex-15_convexfunctions-2) ' ' }
                .print-site-enumerate-headings #convex-15_convexfunctions h2 {  counter-increment: counter-convex-15_convexfunctions-2 }
            
.print-site-enumerate-headings #convex-16_subgradients > h1:before { content: '2.6 ' }

                .print-site-enumerate-headings #convex-16_subgradients h2:before { content: '2.6.' counter(counter-convex-16_subgradients-2) ' ' }
                .print-site-enumerate-headings #convex-16_subgradients h2 {  counter-increment: counter-convex-16_subgradients-2 }
            
.print-site-enumerate-headings #convex-16a_optimality_conditions > h1:before { content: '2.7 ' }

                .print-site-enumerate-headings #convex-16a_optimality_conditions h2:before { content: '2.7.' counter(counter-convex-16a_optimality_conditions-2) ' ' }
                .print-site-enumerate-headings #convex-16a_optimality_conditions h2 {  counter-increment: counter-convex-16a_optimality_conditions-2 }
            
.print-site-enumerate-headings #convex-17_kkt > h1:before { content: '2.8 ' }

                .print-site-enumerate-headings #convex-17_kkt h2:before { content: '2.8.' counter(counter-convex-17_kkt-2) ' ' }
                .print-site-enumerate-headings #convex-17_kkt h2 {  counter-increment: counter-convex-17_kkt-2 }
            
.print-site-enumerate-headings #convex-18_duality > h1:before { content: '2.9 ' }

                .print-site-enumerate-headings #convex-18_duality h2:before { content: '2.9.' counter(counter-convex-18_duality-2) ' ' }
                .print-site-enumerate-headings #convex-18_duality h2 {  counter-increment: counter-convex-18_duality-2 }
            
.print-site-enumerate-headings #convex-18a_pareto > h1:before { content: '2.10 ' }

                .print-site-enumerate-headings #convex-18a_pareto h2:before { content: '2.10.' counter(counter-convex-18a_pareto-2) ' ' }
                .print-site-enumerate-headings #convex-18a_pareto h2 {  counter-increment: counter-convex-18a_pareto-2 }
            
.print-site-enumerate-headings #convex-18b_regularization > h1:before { content: '2.11 ' }

                .print-site-enumerate-headings #convex-18b_regularization h2:before { content: '2.11.' counter(counter-convex-18b_regularization-2) ' ' }
                .print-site-enumerate-headings #convex-18b_regularization h2 {  counter-increment: counter-convex-18b_regularization-2 }
            
.print-site-enumerate-headings #convex-19_optimizationalgo > h1:before { content: '2.12 ' }

                .print-site-enumerate-headings #convex-19_optimizationalgo h2:before { content: '2.12.' counter(counter-convex-19_optimizationalgo-2) ' ' }
                .print-site-enumerate-headings #convex-19_optimizationalgo h2 {  counter-increment: counter-convex-19_optimizationalgo-2 }
            
.print-site-enumerate-headings #convex-19a_optimization_constraints > h1:before { content: '2.13 ' }

                .print-site-enumerate-headings #convex-19a_optimization_constraints h2:before { content: '2.13.' counter(counter-convex-19a_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19a_optimization_constraints h2 {  counter-increment: counter-convex-19a_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-19b_optimization_constraints > h1:before { content: '2.14 ' }

                .print-site-enumerate-headings #convex-19b_optimization_constraints h2:before { content: '2.14.' counter(counter-convex-19b_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19b_optimization_constraints h2 {  counter-increment: counter-convex-19b_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-20_advanced > h1:before { content: '2.15 ' }

                .print-site-enumerate-headings #convex-20_advanced h2:before { content: '2.15.' counter(counter-convex-20_advanced-2) ' ' }
                .print-site-enumerate-headings #convex-20_advanced h2 {  counter-increment: counter-convex-20_advanced-2 }
            
.print-site-enumerate-headings #convex-21_models > h1:before { content: '2.16 ' }

                .print-site-enumerate-headings #convex-21_models h2:before { content: '2.16.' counter(counter-convex-21_models-2) ' ' }
                .print-site-enumerate-headings #convex-21_models h2 {  counter-increment: counter-convex-21_models-2 }
            
.print-site-enumerate-headings #convex-30_canonical_problems > h1:before { content: '2.17 ' }

                .print-site-enumerate-headings #convex-30_canonical_problems h2:before { content: '2.17.' counter(counter-convex-30_canonical_problems-2) ' ' }
                .print-site-enumerate-headings #convex-30_canonical_problems h2 {  counter-increment: counter-convex-30_canonical_problems-2 }
            
.print-site-enumerate-headings #section-3 > h1:before { content: '3 ' }
.print-site-enumerate-headings #nonconvex-41_intro > h1:before { content: '3.1 ' }

                .print-site-enumerate-headings #nonconvex-41_intro h2:before { content: '3.1.' counter(counter-nonconvex-41_intro-2) ' ' }
                .print-site-enumerate-headings #nonconvex-41_intro h2 {  counter-increment: counter-nonconvex-41_intro-2 }
            
.print-site-enumerate-headings #nonconvex-42_meta > h1:before { content: '3.2 ' }

                .print-site-enumerate-headings #nonconvex-42_meta h2:before { content: '3.2.' counter(counter-nonconvex-42_meta-2) ' ' }
                .print-site-enumerate-headings #nonconvex-42_meta h2 {  counter-increment: counter-nonconvex-42_meta-2 }
            
.print-site-enumerate-headings #nonconvex-43_hybrid > h1:before { content: '3.3 ' }

                .print-site-enumerate-headings #nonconvex-43_hybrid h2:before { content: '3.3.' counter(counter-nonconvex-43_hybrid-2) ' ' }
                .print-site-enumerate-headings #nonconvex-43_hybrid h2 {  counter-increment: counter-nonconvex-43_hybrid-2 }
            
.print-site-enumerate-headings #section-4 > h1:before { content: '4 ' }
.print-site-enumerate-headings #deeplearning-1_mlp > h1:before { content: '4.1 ' }

                .print-site-enumerate-headings #deeplearning-1_mlp h2:before { content: '4.1.' counter(counter-deeplearning-1_mlp-2) ' ' }
                .print-site-enumerate-headings #deeplearning-1_mlp h2 {  counter-increment: counter-deeplearning-1_mlp-2 }
            
.print-site-enumerate-headings #deeplearning-2_convnets > h1:before { content: '4.2 ' }

                .print-site-enumerate-headings #deeplearning-2_convnets h2:before { content: '4.2.' counter(counter-deeplearning-2_convnets-2) ' ' }
                .print-site-enumerate-headings #deeplearning-2_convnets h2 {  counter-increment: counter-deeplearning-2_convnets-2 }
            
.print-site-enumerate-headings #deeplearning-3_sequence_data > h1:before { content: '4.3 ' }

                .print-site-enumerate-headings #deeplearning-3_sequence_data h2:before { content: '4.3.' counter(counter-deeplearning-3_sequence_data-2) ' ' }
                .print-site-enumerate-headings #deeplearning-3_sequence_data h2 {  counter-increment: counter-deeplearning-3_sequence_data-2 }
            
.print-site-enumerate-headings #deeplearning-4_nlp > h1:before { content: '4.4 ' }

                .print-site-enumerate-headings #deeplearning-4_nlp h2:before { content: '4.4.' counter(counter-deeplearning-4_nlp-2) ' ' }
                .print-site-enumerate-headings #deeplearning-4_nlp h2 {  counter-increment: counter-deeplearning-4_nlp-2 }
            
.print-site-enumerate-headings #deeplearning-5_attention > h1:before { content: '4.5 ' }

                .print-site-enumerate-headings #deeplearning-5_attention h2:before { content: '4.5.' counter(counter-deeplearning-5_attention-2) ' ' }
                .print-site-enumerate-headings #deeplearning-5_attention h2 {  counter-increment: counter-deeplearning-5_attention-2 }
            
.print-site-enumerate-headings #deeplearning-6_gans > h1:before { content: '4.6 ' }

                .print-site-enumerate-headings #deeplearning-6_gans h2:before { content: '4.6.' counter(counter-deeplearning-6_gans-2) ' ' }
                .print-site-enumerate-headings #deeplearning-6_gans h2 {  counter-increment: counter-deeplearning-6_gans-2 }
            
.print-site-enumerate-headings #deeplearning-7_unsuper > h1:before { content: '4.7 ' }

                .print-site-enumerate-headings #deeplearning-7_unsuper h2:before { content: '4.7.' counter(counter-deeplearning-7_unsuper-2) ' ' }
                .print-site-enumerate-headings #deeplearning-7_unsuper h2 {  counter-increment: counter-deeplearning-7_unsuper-2 }
            
.print-site-enumerate-headings #deeplearning-8_latentvariables > h1:before { content: '4.8 ' }

                .print-site-enumerate-headings #deeplearning-8_latentvariables h2:before { content: '4.8.' counter(counter-deeplearning-8_latentvariables-2) ' ' }
                .print-site-enumerate-headings #deeplearning-8_latentvariables h2 {  counter-increment: counter-deeplearning-8_latentvariables-2 }
            
.print-site-enumerate-headings #section-5 > h1:before { content: '5 ' }
.print-site-enumerate-headings #distributedsystems-0_intro > h1:before { content: '5.1 ' }

                .print-site-enumerate-headings #distributedsystems-0_intro h2:before { content: '5.1.' counter(counter-distributedsystems-0_intro-2) ' ' }
                .print-site-enumerate-headings #distributedsystems-0_intro h2 {  counter-increment: counter-distributedsystems-0_intro-2 }
            
.print-site-enumerate-headings #distributedsystems-1_mapreduce > h1:before { content: '5.2 ' }

                .print-site-enumerate-headings #distributedsystems-1_mapreduce h2:before { content: '5.2.' counter(counter-distributedsystems-1_mapreduce-2) ' ' }
                .print-site-enumerate-headings #distributedsystems-1_mapreduce h2 {  counter-increment: counter-distributedsystems-1_mapreduce-2 }
            
.print-site-enumerate-headings #distributedsystems-2_threads > h1:before { content: '5.3 ' }

                .print-site-enumerate-headings #distributedsystems-2_threads h2:before { content: '5.3.' counter(counter-distributedsystems-2_threads-2) ' ' }
                .print-site-enumerate-headings #distributedsystems-2_threads h2 {  counter-increment: counter-distributedsystems-2_threads-2 }
            
.print-site-enumerate-headings #section-6 > h1:before { content: '6 ' }
.print-site-enumerate-headings #informationtheory-1_intro_to_infotheory > h1:before { content: '6.1 ' }

                .print-site-enumerate-headings #informationtheory-1_intro_to_infotheory h2:before { content: '6.1.' counter(counter-informationtheory-1_intro_to_infotheory-2) ' ' }
                .print-site-enumerate-headings #informationtheory-1_intro_to_infotheory h2 {  counter-increment: counter-informationtheory-1_intro_to_infotheory-2 }
            
.print-site-enumerate-headings #informationtheory-2_entropy > h1:before { content: '6.2 ' }

                .print-site-enumerate-headings #informationtheory-2_entropy h2:before { content: '6.2.' counter(counter-informationtheory-2_entropy-2) ' ' }
                .print-site-enumerate-headings #informationtheory-2_entropy h2 {  counter-increment: counter-informationtheory-2_entropy-2 }
            
.print-site-enumerate-headings #informationtheory-3_kl > h1:before { content: '6.3 ' }

                .print-site-enumerate-headings #informationtheory-3_kl h2:before { content: '6.3.' counter(counter-informationtheory-3_kl-2) ' ' }
                .print-site-enumerate-headings #informationtheory-3_kl h2 {  counter-increment: counter-informationtheory-3_kl-2 }
            
.print-site-enumerate-headings #informationtheory-4_bayes > h1:before { content: '6.4 ' }

                .print-site-enumerate-headings #informationtheory-4_bayes h2:before { content: '6.4.' counter(counter-informationtheory-4_bayes-2) ' ' }
                .print-site-enumerate-headings #informationtheory-4_bayes h2 {  counter-increment: counter-informationtheory-4_bayes-2 }
            
.print-site-enumerate-headings #informationtheory-5_mc_intro > h1:before { content: '6.5 ' }

                .print-site-enumerate-headings #informationtheory-5_mc_intro h2:before { content: '6.5.' counter(counter-informationtheory-5_mc_intro-2) ' ' }
                .print-site-enumerate-headings #informationtheory-5_mc_intro h2 {  counter-increment: counter-informationtheory-5_mc_intro-2 }
            
.print-site-enumerate-headings #informationtheory-6_mc > h1:before { content: '6.6 ' }

                .print-site-enumerate-headings #informationtheory-6_mc h2:before { content: '6.6.' counter(counter-informationtheory-6_mc-2) ' ' }
                .print-site-enumerate-headings #informationtheory-6_mc h2 {  counter-increment: counter-informationtheory-6_mc-2 }
            
.print-site-enumerate-headings #informationtheory-7a_vi_intro > h1:before { content: '6.7 ' }

                .print-site-enumerate-headings #informationtheory-7a_vi_intro h2:before { content: '6.7.' counter(counter-informationtheory-7a_vi_intro-2) ' ' }
                .print-site-enumerate-headings #informationtheory-7a_vi_intro h2 {  counter-increment: counter-informationtheory-7a_vi_intro-2 }
            
.print-site-enumerate-headings #informationtheory-7b_vi > h1:before { content: '6.8 ' }

                .print-site-enumerate-headings #informationtheory-7b_vi h2:before { content: '6.8.' counter(counter-informationtheory-7b_vi-2) ' ' }
                .print-site-enumerate-headings #informationtheory-7b_vi h2 {  counter-increment: counter-informationtheory-7b_vi-2 }
            
.print-site-enumerate-headings #section-7 > h1:before { content: '7 ' }
.print-site-enumerate-headings #cheatsheets-20a_cheatsheet > h1:before { content: '7.1 ' }

                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2:before { content: '7.1.' counter(counter-cheatsheets-20a_cheatsheet-2) ' ' }
                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2 {  counter-increment: counter-cheatsheets-20a_cheatsheet-2 }
            
.print-site-enumerate-headings #section-8 > h1:before { content: '8 ' }
.print-site-enumerate-headings #appendices-120_ineqaulities > h1:before { content: '8.1 ' }

                .print-site-enumerate-headings #appendices-120_ineqaulities h2:before { content: '8.1.' counter(counter-appendices-120_ineqaulities-2) ' ' }
                .print-site-enumerate-headings #appendices-120_ineqaulities h2 {  counter-increment: counter-appendices-120_ineqaulities-2 }
            
.print-site-enumerate-headings #appendices-130_projections > h1:before { content: '8.2 ' }

                .print-site-enumerate-headings #appendices-130_projections h2:before { content: '8.2.' counter(counter-appendices-130_projections-2) ' ' }
                .print-site-enumerate-headings #appendices-130_projections h2 {  counter-increment: counter-appendices-130_projections-2 }
            
.print-site-enumerate-headings #appendices-140_support > h1:before { content: '8.3 ' }

                .print-site-enumerate-headings #appendices-140_support h2:before { content: '8.3.' counter(counter-appendices-140_support-2) ' ' }
                .print-site-enumerate-headings #appendices-140_support h2 {  counter-increment: counter-appendices-140_support-2 }
            
.print-site-enumerate-headings #appendices-160_conjugates > h1:before { content: '8.4 ' }

                .print-site-enumerate-headings #appendices-160_conjugates h2:before { content: '8.4.' counter(counter-appendices-160_conjugates-2) ' ' }
                .print-site-enumerate-headings #appendices-160_conjugates h2 {  counter-increment: counter-appendices-160_conjugates-2 }
            
.print-site-enumerate-headings #appendices-170_probability > h1:before { content: '8.5 ' }

                .print-site-enumerate-headings #appendices-170_probability h2:before { content: '8.5.' counter(counter-appendices-170_probability-2) ' ' }
                .print-site-enumerate-headings #appendices-170_probability h2 {  counter-increment: counter-appendices-170_probability-2 }
            
.print-site-enumerate-headings #appendices-180_subgradient_methods > h1:before { content: '8.6 ' }

                .print-site-enumerate-headings #appendices-180_subgradient_methods h2:before { content: '8.6.' counter(counter-appendices-180_subgradient_methods-2) ' ' }
                .print-site-enumerate-headings #appendices-180_subgradient_methods h2 {  counter-increment: counter-appendices-180_subgradient_methods-2 }
            
.print-site-enumerate-headings #appendices-190_proximal > h1:before { content: '8.7 ' }

                .print-site-enumerate-headings #appendices-190_proximal h2:before { content: '8.7.' counter(counter-appendices-190_proximal-2) ' ' }
                .print-site-enumerate-headings #appendices-190_proximal h2 {  counter-increment: counter-appendices-190_proximal-2 }
            
.print-site-enumerate-headings #appendices-200_mirror > h1:before { content: '8.8 ' }

                .print-site-enumerate-headings #appendices-200_mirror h2:before { content: '8.8.' counter(counter-appendices-200_mirror-2) ' ' }
                .print-site-enumerate-headings #appendices-200_mirror h2 {  counter-increment: counter-appendices-200_mirror-2 }
            
.print-site-enumerate-headings #appendices-300_matrixfactorization > h1:before { content: '8.9 ' }

                .print-site-enumerate-headings #appendices-300_matrixfactorization h2:before { content: '8.9.' counter(counter-appendices-300_matrixfactorization-2) ' ' }
                .print-site-enumerate-headings #appendices-300_matrixfactorization h2 {  counter-increment: counter-appendices-300_matrixfactorization-2 }
            </style>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "/convex_optimization/", "features": ["navigation.top", "toc.integrate", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tabs.link", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>