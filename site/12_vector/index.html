<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/12_vector/">
      
      
        <link rel="prev" href="../11_intro/">
      
      
        <link rel="next" href="../13_calculus/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>2. Linear Algebra Foundations - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-2-linear-algebra-foundations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2. Linear Algebra Foundations
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/12_vector.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/12_vector.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-2-linear-algebra-foundations">Chapter 2: Linear Algebra Foundations<a class="headerlink" href="#chapter-2-linear-algebra-foundations" title="Permanent link">¶</a></h1>
<p>Convex optimisation is geometric. To talk about convex sets, supporting hyperplanes, projections, and quadratic forms, we need linear algebra. This chapter reviews the specific linear algebra tools we will use throughout: vector spaces, inner products, norms, projections, eigenvalues, and positive semidefinite matrices.</p>
<h2 id="21-vector-spaces-subspaces-and-affine-sets">2.1 Vector spaces, subspaces, and affine sets<a class="headerlink" href="#21-vector-spaces-subspaces-and-affine-sets" title="Permanent link">¶</a></h2>
<p>A vector space over <span class="arithmatex">\(\mathbb{R}\)</span> is a set <span class="arithmatex">\(V\)</span> equipped with addition and scalar multiplication satisfying the usual axioms: closure, associativity, distributivity, etc. In this book we mostly work with <span class="arithmatex">\(V = \mathbb{R}^n\)</span>.</p>
<p>A subspace <span class="arithmatex">\(S \subseteq \mathbb{R}^n\)</span> is a subset that:</p>
<ol>
<li>contains <span class="arithmatex">\(0\)</span>,</li>
<li>is closed under addition,</li>
<li>is closed under scalar multiplication.</li>
</ol>
<p>For example, the set of all solutions to <span class="arithmatex">\(Ax = 0\)</span> is a subspace, called the nullspace or kernel of <span class="arithmatex">\(A\)</span>.</p>
<p>An affine set is a translated subspace. A set <span class="arithmatex">\(A\)</span> is affine if for any <span class="arithmatex">\(x,y \in A\)</span> and any <span class="arithmatex">\(\theta \in \mathbb{R}\)</span>,<br>
<script type="math/tex; mode=display">
\theta x + (1-\theta) y \in A~.
</script>
Every affine set can be written as
<script type="math/tex; mode=display">
x_0 + S = \{ x_0 + s : s \in S \},
</script>
where <span class="arithmatex">\(S\)</span> is a subspace. Affine sets appear as the solution sets to linear equality constraints <span class="arithmatex">\(Ax = b\)</span>.</p>
<p>Affine sets are important in optimisation because:</p>
<ul>
<li>Feasible sets defined by equality constraints are affine.</li>
<li>Affine functions preserve convexity.</li>
</ul>
<h2 id="22-linear-combinations-span-basis-dimension">2.2 Linear combinations, span, basis, dimension<a class="headerlink" href="#22-linear-combinations-span-basis-dimension" title="Permanent link">¶</a></h2>
<p>Given vectors <span class="arithmatex">\(v_1,\dots,v_k\)</span>, any vector of the form
<script type="math/tex; mode=display">
\alpha_1 v_1 + \cdots + \alpha_k v_k
</script>
is a linear combination. The set of all linear combinations is called the span:
<script type="math/tex; mode=display">
\mathrm{span}\{v_1,\dots,v_k\} = \left\{ \sum_{i=1}^k \alpha_i v_i : \alpha_i \in \mathbb{R} \right\}.
</script>
</p>
<p>A list of vectors is linearly independent if no nontrivial linear combination gives <span class="arithmatex">\(0\)</span>. A basis of a subspace <span class="arithmatex">\(S\)</span> is a set of linearly independent vectors whose span is <span class="arithmatex">\(S\)</span>. The number of vectors in a basis is the dimension of <span class="arithmatex">\(S\)</span>.</p>
<p>Rank and nullity facts:</p>
<ul>
<li>The column space of <span class="arithmatex">\(A\)</span> is the span of its columns. Its dimension is <span class="arithmatex">\(\mathrm{rank}(A)\)</span>.</li>
<li>The nullspace of <span class="arithmatex">\(A\)</span> is <span class="arithmatex">\(\{ x : Ax = 0 \}\)</span>.</li>
<li>The rank-nullity theorem** states:
<script type="math/tex; mode=display">
\mathrm{rank}(A) + \mathrm{nullity}(A) = n,
</script>
where <span class="arithmatex">\(n\)</span> is the number of columns of <span class="arithmatex">\(A\)</span>.</li>
</ul>
<p>In constrained optimisation, <span class="arithmatex">\(\mathrm{rank}(A)\)</span> encodes the “number of independent constraints”, and the nullspace encodes feasible directions that do not violate certain constraints.</p>
<blockquote>
<p>Column Space is a set of all possible "outputs" you can create by computing <span class="arithmatex">\(Ax\)</span>. Its answers the question does a solution <span class="arithmatex">\(x\)</span> even exist for <span class="arithmatex">\(Ax = b\)</span>. Solution exists if only if vector <span class="arithmatex">\(b\)</span> lives inside the column space <span class="arithmatex">\(C(A)\)</span>. </p>
<p>Null Space is all sets of "inputs" <span class="arithmatex">\(x\)</span> that get "squashed to zero" by A i.e. All <span class="arithmatex">\(x\)</span> such that <span class="arithmatex">\(Ax = 0\)</span>. It answers the question if a solution exisits, is it the only one.If the nullspace contains non-zero vectors (<span class="arithmatex">\(\mathrm{nullity}(A) &gt; 0\)</span>), there are infinitely many solutions.</p>
<p>Multicollinearity (ML): If one feature in your data matrix <span class="arithmatex">\(X\)</span> is a combination of others (e.g., <span class="arithmatex">\(feature_3 = 2 \times feature_1 + feature_2\)</span>), this creates a non-zero vector in the nullspace. This means there are infinitely many different weight vectors <span class="arithmatex">\(w\)</span> that produce the exact same predictions. The model is "unidentifiable." This is why <span class="arithmatex">\(X^T X\)</span> becomes non-invertible, and it’s the primary motivation for using regularization (like Ridge or Lasso) to pick one "good" solution from this infinite set.</p>
<p>Feasible Directions (Optimization): As you noted, in a constrained problem like <span class="arithmatex">\(Ax = b\)</span>, the nullspace is the set of all directions <span class="arithmatex">\(d\)</span> you can move from a feasible point <span class="arithmatex">\(x\)</span> without violating the constraints. If you are at <span class="arithmatex">\(x\)</span> and move to <span class="arithmatex">\(x+d\)</span> (where <span class="arithmatex">\(d \in N(A)\)</span>), your constraints are still met: <span class="arithmatex">\(A(x+d) = Ax + Ad = b + 0 = b\)</span>. This tells you your "space of free movement"</p>
</blockquote>
<h2 id="23-inner-products-and-orthogonality">2.3 Inner products and orthogonality<a class="headerlink" href="#23-inner-products-and-orthogonality" title="Permanent link">¶</a></h2>
<p>An inner product on <span class="arithmatex">\(\mathbb{R}^n\)</span> is a map <span class="arithmatex">\(\langle \cdot,\cdot\rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\)</span> such that for all <span class="arithmatex">\(x,y,z\)</span> and all scalars <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\langle x,y \rangle = \langle y,x\rangle\)</span> (symmetry),</li>
<li><span class="arithmatex">\(\langle x+y,z \rangle = \langle x,z \rangle + \langle y,z\rangle\)</span> (linearity in first argument),</li>
<li><span class="arithmatex">\(\langle \alpha x, y\rangle = \alpha \langle x, y\rangle\)</span>,</li>
<li><span class="arithmatex">\(\langle x, x\rangle \ge 0\)</span> with equality iff <span class="arithmatex">\(x=0\)</span> (positive definiteness).</li>
</ol>
<p>In <span class="arithmatex">\(\mathbb{R}^n\)</span>, the standard inner product is the dot product:
<script type="math/tex; mode=display">
\langle x,y \rangle = x^\top y = \sum_{i=1}^n x_i y_i~.
</script>
</p>
<p>The inner product induces:</p>
<ul>
<li>length (norm): <span class="arithmatex">\(\|x\|_2 = \sqrt{\langle x,x\rangle}\)</span>,</li>
<li>angle: 
<script type="math/tex; mode=display">
\cos \theta = \frac{\langle x,y\rangle}{\|x\|\|y\|}~.
</script>
</li>
</ul>
<p>Two vectors are orthogonal if <span class="arithmatex">\(\langle x,y\rangle = 0\)</span>. A set of vectors <span class="arithmatex">\(\{v_i\}\)</span> is orthonormal if each <span class="arithmatex">\(\|v_i\| = 1\)</span> and <span class="arithmatex">\(\langle v_i, v_j\rangle = 0\)</span> for <span class="arithmatex">\(i\ne j\)</span>.</p>
<p>More generally, an inner product endows <span class="arithmatex">\(V\)</span> with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p>
<p><strong>Geometry from the inner product:</strong> An inner product induces a norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x \rangle}\)</span> and a notion of distance <span class="arithmatex">\(d(x,y) = \|x-y\|\)</span>. It also defines angles: <span class="arithmatex">\(\langle x,y \rangle = 0\)</span> means <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: <span class="arithmatex">\(\|x+y\|^2 + \|x-y\|^2 = 2\|x\|^2 + 2\|y\|^2\)</span>.  </p>
<p><strong>The Cauchy–Schwarz inequality:</strong> For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>:
<script type="math/tex; mode=display">
|\langle x,y\rangle| \le \|x\|\|y\|~,
</script>
with equality iff <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent Geometrically, it means the absolute inner product is maximized when <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> point in the same or opposite direction. </p>
<p><strong>Examples of inner products:</strong></p>
<ul>
<li>
<p><strong>Standard (Euclidean) inner product:</strong> <span class="arithmatex">\(\langle x,y\rangle = x^\top y = \sum_i x_i y_i\)</span>. This underlies most optimization algorithms on <span class="arithmatex">\(\mathbb{R}^n\)</span>, where <span class="arithmatex">\(\nabla f(x)\)</span> is defined via this inner product (so that <span class="arithmatex">\(\langle \nabla f(x), h\rangle\)</span> gives the directional derivative in direction <span class="arithmatex">\(h\)</span>).  </p>
</li>
<li>
<p><strong>Weighted inner product:</strong> <span class="arithmatex">\(\langle x,y\rangle_W = x^\top W y\)</span> for some symmetric positive-definite matrix <span class="arithmatex">\(W\)</span>. Here <span class="arithmatex">\(\|x\|_W = \sqrt{x^\top W x}\)</span> is a weighted length. Such inner products appear in preconditioning: by choosing <span class="arithmatex">\(W\)</span> cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses <span class="arithmatex">\(W = \Sigma^{-1}\)</span> for covariance <span class="arithmatex">\(\Sigma\)</span>).  </p>
</li>
<li>
<p><strong>Function space inner product:</strong> <span class="arithmatex">\(\langle f, g \rangle = \int_a^b f(t)\,g(t)\,dt\)</span>. This turns the space of square-integrable functions on <span class="arithmatex">\([a,b]\)</span> into an inner product space (a Hilbert space, <span class="arithmatex">\(L^2[a,b]\)</span>). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p>
</li>
</ul>
<p>Any vector space with an inner product has an orthonormal basis (via the Gram–Schmidt process). Gram–Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as <span class="arithmatex">\(A = QR\)</span> where <span class="arithmatex">\(Q\)</span> has orthonormal columns and <span class="arithmatex">\(R\)</span> is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve <span class="arithmatex">\(Ax=b\)</span> and to analyze subspaces. For example, for an overdetermined system (<span class="arithmatex">\(m&gt;n\)</span> i.e. more equations than unknowns), <span class="arithmatex">\(Ax=b\)</span> has a least-squares solution <span class="arithmatex">\(x = R^{-1}(Q^\top b)\)</span>, and for underdetermined (<span class="arithmatex">\(m&lt;n\)</span>), <span class="arithmatex">\(Ax=b\)</span> has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p>
<p><strong>Applications in optimization:</strong> Inner product geometry is indispensable in convex optimization.  </p>
<ul>
<li>
<p><strong>Gradients:</strong> The gradient <span class="arithmatex">\(\nabla f(x)\)</span> is defined as the vector satisfying <span class="arithmatex">\(f(x+h)\approx f(x) + \langle \nabla f(x), h\rangle\)</span>. Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction <span class="arithmatex">\(-\nabla f(x)\)</span> because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix <span class="arithmatex">\(W\)</span>), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p>
</li>
<li>
<p><strong>Orthogonal projections:</strong> Many algorithms require projecting onto a constraint set. For linear constraints <span class="arithmatex">\(Ax=b\)</span> (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(\mathrm{range}(A)\)</span>) and quadratic programs (where each iteration might involve a projection).  </p>
</li>
<li>
<p><strong>Orthonormal representations:</strong> Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p>
</li>
<li>
<p><strong>Conditioning and Gram matrix:</strong> The inner product concept leads to the Gram matrix <span class="arithmatex">\(G_{ij} = \langle x_i, x_j\rangle\)</span> for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: <span class="arithmatex">\(X^\top X\)</span> is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix’s condition number and thus algorithm performance.</p>
</li>
</ul>
<h2 id="24-norms-and-distances">2.4 Norms and distances<a class="headerlink" href="#24-norms-and-distances" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(\|\cdot\|: \mathbb{R}^n \to \mathbb{R}\)</span> is a norm if for all <span class="arithmatex">\(x,y\)</span> and scalar <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\|x\| \ge 0\)</span> and <span class="arithmatex">\(\|x\| = 0 \iff x=0\)</span>,</li>
<li><span class="arithmatex">\(\|\alpha x\| = |\alpha|\|x\|\)</span> (absolute homogeneity),</li>
<li><span class="arithmatex">\(\|x+y\| \le \|x\| + \|y\|\)</span> (triangle inequality).</li>
</ol>
<p>If the vector space has an inner product, the norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x\rangle}\)</span> is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry.<br>
Common examples on <span class="arithmatex">\(\mathbb{R}^n\)</span>:  </p>
<ul>
<li>
<p><span class="arithmatex">\(\ell_2\)</span> norm (Euclidean): <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span>, the usual length in space.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>, measuring taxicab distance. In <span class="arithmatex">\(\mathbb{R}^2\)</span>, its unit ball is a diamond.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span>, measuring the largest coordinate magnitude. Its unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a square.  </p>
</li>
<li>
<p>General <span class="arithmatex">\(\ell_p\)</span> norm: <span class="arithmatex">\(\|x\|_p = \left(\sum_i |x_i|^p\right)^{1/p}\)</span> for <span class="arithmatex">\(p\ge1\)</span>. This interpolates between <span class="arithmatex">\(\ell_1\)</span> and <span class="arithmatex">\(\ell_2\)</span>, and approaches <span class="arithmatex">\(\ell_\infty\)</span> as <span class="arithmatex">\(p\to\infty\)</span>. All <span class="arithmatex">\(\ell_p\)</span> norms are convex and satisfy the norm axioms.  </p>
</li>
</ul>
<p>Every norm induces a metric (distance) <span class="arithmatex">\(d(x,y) = |x-y|\)</span> on the space. Norms thus define the shape of “balls” (sets <span class="arithmatex">\({x: |x|\le \text{constant}}\)</span>) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm’s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../images/norms.png" data-desc-position="bottom"><img alt="Alt text" src="../images/norms.png" style="float:right; margin-right:15px; width:400px;"></a></p>
<p><strong>Unit-ball geometry:</strong> The shape of the unit ball <span class="arithmatex">\({x: |x| \le 1}\)</span> reveals how a norm treats different directions. For example, the <span class="arithmatex">\(\ell_2\)</span> unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a perfect circle, treating all directions uniformly, whereas the <span class="arithmatex">\(\ell_1\)</span> unit ball is a diamond with corners along the axes, indicating that <span class="arithmatex">\(\ell_1\)</span> treats the coordinate axes as special (those are “cheaper” directions since the ball extends further along axes, touching them at <span class="arithmatex">\((\pm1,0)\)</span> and <span class="arithmatex">\((0,\pm1)\)</span>). The <span class="arithmatex">\(\ell_\infty\)</span> unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (<span class="arithmatex">\(\ell_1\)</span>), green circle (<span class="arithmatex">\(\ell_2\)</span>), and blue square (<span class="arithmatex">\(\ell_\infty\)</span>) in <span class="arithmatex">\(\mathbb{R}^2\)</span> . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an <span class="arithmatex">\(\ell_1\)</span> norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an <span class="arithmatex">\(\ell_2\)</span> ball encourages more evenly-distributed changes. An <span class="arithmatex">\(\ell_\infty\)</span> constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p>
<p><strong>Dual norms:</strong> Each norm <span class="arithmatex">\(\|\cdot\|\)</span> has a dual norm <span class="arithmatex">\(\|\cdot\|_*\)</span> defined by
<script type="math/tex; mode=display">
\|y\|_* = \sup_{\|x\|\le 1} x^\top y~.
</script>
For example, the dual of <span class="arithmatex">\(\ell_1\)</span> is <span class="arithmatex">\(\ell_\infty\)</span>, and the dual of <span class="arithmatex">\(\ell_2\)</span> is itself.</p>
<blockquote>
<p>Imagine the vector <span class="arithmatex">\(x\)</span> lives inside the original norm ball (<span class="arithmatex">\(\|x\| \le 1\)</span>). The term <span class="arithmatex">\(x^\top y\)</span> is the dot product, which measures the alignment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>. The dual norm <span class="arithmatex">\(\|y\|_*\)</span> is the maximum possible value you can get by taking the dot product of <span class="arithmatex">\(y\)</span> with any vector <span class="arithmatex">\(x\)</span> that fits inside the original norm ball.If the dual norm <span class="arithmatex">\(\|y\|_*\)</span> is large, it means <span class="arithmatex">\(y\)</span> is strongly aligned with a direction <span class="arithmatex">\(x\)</span> that is "small" (size <span class="arithmatex">\(\le 1\)</span>) according to the original norm.If the dual norm is small, <span class="arithmatex">\(y\)</span> must be poorly aligned with all vectors <span class="arithmatex">\(x\)</span> in the ball.</p>
</blockquote>
<p><strong>Norms in optimization algorithms:</strong> Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use <span class="arithmatex">\(\ell_\infty\)</span> (since one coordinate move at a time is like a step in <span class="arithmatex">\(\ell_\infty\)</span> unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using <span class="arithmatex">\(\ell_1\)</span> norm for sparse problems). The norm also figures in complexity bounds: an algorithm’s convergence rate may depend on the diameter of the feasible set in the chosen norm, <span class="arithmatex">\(D = \max_{\text{feasible}}|x - x^*|\)</span>. For instance, in subgradient methods, having a smaller <span class="arithmatex">\(\ell_2\)</span> diameter or <span class="arithmatex">\(\ell_1\)</span> diameter can improve bounds. Moreover, when constraints are given by norms (like <span class="arithmatex">\(|x|_1 \le t\)</span>), projections and proximal operators with respect to that norm become subroutines in algorithms.</p>
<p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (<span class="arithmatex">\(|x_k - x^*|\)</span>), how to constrain solutions (<span class="arithmatex">\(|x| \le R\)</span>), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>
<h2 id="25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices">2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices<a class="headerlink" href="#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{n\times n}\)</span> is linear, a nonzero <span class="arithmatex">\(v\)</span> is an eigenvector with eigenvalue <span class="arithmatex">\(\lambda\)</span> if</p>
<div class="arithmatex">\[
Av = \lambda v~.
\]</div>
<p>When <span class="arithmatex">\(A\)</span> is symmetric (<span class="arithmatex">\(A = A^\top\)</span>), it has:</p>
<ul>
<li>real eigenvalues,</li>
<li>an orthonormal eigenbasis,</li>
<li>a spectral decomposition</li>
</ul>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^\top,
</script>
where <span class="arithmatex">\(Q\)</span> is orthonormal and <span class="arithmatex">\(\Lambda\)</span> is diagonal.</p>
<p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along <span class="arithmatex">\(n\)</span> orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by <span class="arithmatex">\(\lambda_i\)</span>.</p>
<blockquote>
<p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p>
<p>In optimization, the Hessian matrix of a multivariate function <span class="arithmatex">\(f(x)\)</span> is symmetric. Its eigenvalues <span class="arithmatex">\(\lambda_i(\nabla^2 f(x))\)</span> tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there’s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p>
</blockquote>
<p><strong>Positive semidefinite matrices:</strong> A symmetric matrix <span class="arithmatex">\(Q\)</span> is positive semidefinite (PSD) if</p>
<div class="arithmatex">\[
x^\top Q x \ge 0 \quad \text{for all } x~.
\]</div>
<p>If <span class="arithmatex">\(x^\top Q x &gt; 0\)</span> for all <span class="arithmatex">\(x\ne 0\)</span>, then <span class="arithmatex">\(Q\)</span> is positive definite (PD).</p>
<p>Why this matters: if <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top Q x + c^\top x + d\)</span>, then</p>
<div class="arithmatex">\[
\nabla^2 f(x) = Q~.
\]</div>
<p>So <span class="arithmatex">\(f\)</span> is convex iff <span class="arithmatex">\(Q\)</span> is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not (Boyd and Vandenberghe, 2004). This is the algebraic test for convexity of quadratic forms.</p>
<p><strong>Implications of definiteness:</strong> If <span class="arithmatex">\(A \succ 0\)</span>, the quadratic function <span class="arithmatex">\(x^T A x\)</span> is strictly convex and has a unique minimizer at <span class="arithmatex">\(x=0\)</span>. If <span class="arithmatex">\(A \succeq 0\)</span>, <span class="arithmatex">\(x^T A x\)</span> is convex but could be flat in some directions (if some <span class="arithmatex">\(\lambda_i = 0\)</span>, those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> means <span class="arithmatex">\(f\)</span> has a unique local (and global, if domain convex) minimum at that <span class="arithmatex">\(x\)</span> (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater’s condition for strong duality.</p>
<p><strong>Condition number and convergence:</strong> For iterative methods on convex quadratics <span class="arithmatex">\(f(x) = \frac{1}{2}x^T Q x - b^T x\)</span>, the eigenvalues of <span class="arithmatex">\(Q\)</span> dictate convergence speed. Gradient descent’s error after <span class="arithmatex">\(k\)</span> steps satisfies roughly <span class="arithmatex">\(|x_k - x^*| \le (\frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}})^k |x_0 - x^*|\)</span> (for normalized step). So the ratio <span class="arithmatex">\(\frac{\lambda_{\max}}{\lambda_{\min}} = \kappa(Q)\)</span> appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton’s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to <span class="arithmatex">\(\kappa\)</span> (locally). This explains why second-order methods shine on ill-conditioned problems: they “whiten” the curvature by dividing by eigenvalues.</p>
<p><strong>Optimization interpretation of eigenvectors:</strong> The eigenvectors of <span class="arithmatex">\(\nabla^2 f(x^*)\)</span> at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton’s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p>
<h2 id="26-orthogonal-projections-and-least-squares">2.6 Orthogonal projections and least squares<a class="headerlink" href="#26-orthogonal-projections-and-least-squares" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(S\)</span> be a subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span>. The orthogonal projection of a vector <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is the unique vector <span class="arithmatex">\(p \in S\)</span> minimising <span class="arithmatex">\(\|b - p\|_2\)</span>. Geometrically, <span class="arithmatex">\(p\)</span> is the closest point in <span class="arithmatex">\(S\)</span> to <span class="arithmatex">\(b\)</span>.</p>
<p>If <span class="arithmatex">\(S = \mathrm{span}\{a_1,\dots,a_k\}\)</span> and <span class="arithmatex">\(A = [a_1~\cdots~a_k]\)</span>, then projecting <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is equivalent to solving the least-squares problem</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2~.
$$
The solution $x^*$ satisfies the normal equations
$$
A^\top A x^* = A^\top b~.
\]</div>
<p>This is our first real convex optimisation problem:</p>
<ul>
<li>the objective <span class="arithmatex">\(\|Ax-b\|_2^2\)</span> is convex,</li>
<li>there are no constraints,</li>
<li>we can solve it in closed form.</li>
</ul>
<h2 id="27-advanced-concepts">2.7 Advanced Concepts<a class="headerlink" href="#27-advanced-concepts" title="Permanent link">¶</a></h2>
<p><strong>Operator norm:</strong> Given a matrix (linear map) <span class="arithmatex">\(A: \mathbb{R}^n \to \mathbb{R}^m\)</span> and given a choice of vector norms on input and output, one can define the induced operator norm. If we use <span class="arithmatex">\(|\cdot|_p\)</span> on <span class="arithmatex">\(\mathbb{R}^n\)</span> and <span class="arithmatex">\(|\cdot|_q\)</span> on <span class="arithmatex">\(\mathbb{R}^m\)</span>, the operator norm is</p>
<div class="arithmatex">\[
\|A\|_{p \to q}
= \sup_{x \ne 0} \frac{\|Ax\|_q}{\|x\|_p}
= \sup_{\|x\|_p \le 1} \|Ax\|_q
\]</div>
<p>This gives the maximum factor by which <span class="arithmatex">\(A\)</span> can stretch a vector (measuring <span class="arithmatex">\(x\)</span> in norm <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(Ax\)</span> in norm <span class="arithmatex">\(q\)</span>).pecial cases are common: with <span class="arithmatex">\(p = q = 2\)</span>, <span class="arithmatex">\(|A|_{2 \to 2}\)</span> (often just written <span class="arithmatex">\(|A|_2\)</span>) is the spectral norm, which equals the largest singular value of <span class="arithmatex">\(A\)</span> (more on singular values below).
If <span class="arithmatex">\(p = q = 1\)</span>, <span class="arithmatex">\(|A|_{1 \to 1}\)</span> is the maximum absolute column sum of <span class="arithmatex">\(A\)</span>.
If <span class="arithmatex">\(p = q = \infty\)</span>, <span class="arithmatex">\(|A|{\infty \to \infty}\)</span> is the maximum absolute row sum.</p>
<p>Operator norms tell us the worst-case amplification of signals by <span class="arithmatex">\(A\)</span>. In gradient descent on <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top A x - b^\top x\)</span> (a quadratic form), the step size must be <span class="arithmatex">\(\le \tfrac{2}{|A|_2}\)</span> for convergence; here <span class="arithmatex">\(|A|_2 = \lambda_{\max}(A)\)</span> if <span class="arithmatex">\(A\)</span> is symmetric (it’s related to Hessian eigenvalues, Chapter 5). In general, controlling <span class="arithmatex">\(|A|\)</span> controls stability: if <span class="arithmatex">\(|A| &lt; 1\)</span>, the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p>
<p><strong>Singular Value Decomposition (SVD):</strong> Any matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as</p>
<div class="arithmatex">\[
A = U \Sigma V^\top
\]</div>
<p>where <span class="arithmatex">\(U \in \mathbb{R}^{m\times m}\)</span> and <span class="arithmatex">\(V \in \mathbb{R}^{n\times n}\)</span> are orthogonal matrices (their columns are orthonormal bases of <span class="arithmatex">\(\mathbb{R}^m\)</span> and <span class="arithmatex">\(\mathbb{R}^n\)</span>, respectively), and <span class="arithmatex">\(\Sigma\)</span> is an <span class="arithmatex">\(m\times n\)</span> diagonal matrix with nonnegative entries <span class="arithmatex">\(\sigma_1 \ge \sigma_2 \ge \cdots \ge 0\)</span> on the diagonal. The <span class="arithmatex">\(\sigma_i\)</span> are the singular values of <span class="arithmatex">\(A\)</span>. Geometrically, <span class="arithmatex">\(A\)</span> sends the unit ball in <span class="arithmatex">\(\mathbb{R}^n\)</span> to an ellipsoid in <span class="arithmatex">\(\mathbb{R}^m\)</span> whose principal semi-axes lengths are the singular values and directions are the columns of <span class="arithmatex">\(V\)</span> (mapped to columns of <span class="arithmatex">\(U\)</span>). The largest singular value <span class="arithmatex">\(\sigma_{\max} = |A|_2\)</span> is the spectral norm. The smallest (if <span class="arithmatex">\(n \le m\)</span>, <span class="arithmatex">\(\sigma{\min}\)</span> of those <span class="arithmatex">\(n\)</span>) indicates how <span class="arithmatex">\(A\)</span> contracts the least – if <span class="arithmatex">\(\sigma_{\min} = 0\)</span>, <span class="arithmatex">\(A\)</span> is rank-deficient.</p>
<p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number <span class="arithmatex">\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span> (when <span class="arithmatex">\(\sigma_{\min}&gt;0\)</span>), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in <span class="arithmatex">\(x\)</span>-space hardly change <span class="arithmatex">\(Ax\)</span> (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means <span class="arithmatex">\(A\)</span> is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of <span class="arithmatex">\(A\)</span> (Eckart–Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p>
<p><strong>Low-rank structure:</strong> The rank of <span class="arithmatex">\(A\)</span> equals the number of nonzero singular values. If <span class="arithmatex">\(A\)</span> has rank <span class="arithmatex">\(r \ll \min(n,m)\)</span>, it means <span class="arithmatex">\(A\)</span> effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem’s data matrix <span class="arithmatex">\(A\)</span> is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p>
<p><strong>Operator norm in optimization:</strong> Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem <span class="arithmatex">\(f(x) = \frac{1}{2}x^TQx - b^Tx\)</span>, the Hessian is <span class="arithmatex">\(Q\)</span> and gradient descent converges if <span class="arithmatex">\(\alpha &lt; 2/\lambda_{\max}(Q)\)</span>. Preconditioning aims to transform <span class="arithmatex">\(Q\)</span> into one with a smaller condition number by multiplying by some <span class="arithmatex">\(P\)</span> (like using <span class="arithmatex">\(P^{-1}Q\)</span>) — effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex <span class="arithmatex">\(f\)</span>, the Lipschitz constant of <span class="arithmatex">\(\nabla f\)</span> (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p>
<p><strong>Summary of spectral properties:</strong></p>
<ul>
<li>
<p>The <strong>spectral norm</strong> <span class="arithmatex">\(|A|_2 = \sigma_{\max}(A)\)</span> quantifies the largest stretching. It determines stability and step sizes.</p>
</li>
<li>
<p>The smallest singular value <span class="arithmatex">\(\sigma_{\min}\)</span> (if <span class="arithmatex">\(A\)</span> is tall full-rank) tells if <span class="arithmatex">\(A\)</span> is invertible and how sensitive the inverse is. If <span class="arithmatex">\(\sigma_{\min}\)</span> is tiny, small changes in output cause huge changes in solving <span class="arithmatex">\(Ax=b\)</span>.</p>
</li>
<li>
<p>The <strong>condition number</strong> <span class="arithmatex">\(\kappa = \sigma_{\max}/\sigma_{\min}\)</span> is a figure of merit for algorithms: gradient descent iterations needed often scale with <span class="arithmatex">\(\kappa\)</span> (worse conditioning = slower). Regularization like adding <span class="arithmatex">\(\mu I\)</span> increases <span class="arithmatex">\(\sigma_{\min}\)</span>, thereby reducing <span class="arithmatex">\(\kappa\)</span> and accelerating convergence (at the expense of bias).</p>
</li>
<li>
<p><strong>Nuclear norm</strong> (sum of singular values) and <strong>spectral norm</strong> often appear in optimization as convex surrogates for rank and as constraints to limit the operator’s impact.</p>
</li>
</ul>
<p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>