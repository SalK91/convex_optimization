<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/1_0_intro/">
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../1_1_vector/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Basics - Convex Optimization</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mathematics-for-convex-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Convex Optimization" class="md-header__button md-logo" aria-label="Convex Optimization" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Convex Optimization
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Basics
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Convex Optimization" class="md-nav__button md-logo" aria-label="Convex Optimization" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Convex Optimization
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Basics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Basics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Basics
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_1_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Vector Spaces and Linear Mappings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_2_innerproducts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Inner Product Spaces and Orthogonality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_3_norms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Norms and Metric Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_4_linearoperator/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Linear Operators, Spectral Norms, and SVD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_5_eigenvalues/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Eigenvalues, Positive Definiteness, and Quadratic Forms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_6_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Projections onto Subspaces and Convex Sets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Calculus Essentials -  Gradients, Jacobians, and Hessians
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%200intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%201la_foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra Review
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%202innerproducts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inner Products
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%203norms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Norms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%204operators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%205eigenvalues/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Eigenvalues and Eigenvectors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%206projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%207calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculus Review
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%208convexity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Smoothness and Convexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%209Convex%20Sets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Sets and Related Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0a%2010Convex%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Analysis
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Convex Analysis
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%201Separation%20Theorems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Separation Theorems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%202Support%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Support Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%203Convex%20Conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Conjugates
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%204Subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%205Fenchel%20Duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fenchel Duality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%206Moreau%20and%20Proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Moreau Envelopes &amp; Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0b%207Indicator%20and%20Distance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Indicator &amp; Distance Functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4">
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Identifying Convex Problems and Tractability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Identifying Convex Problems and Tractability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%201Convex%20Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convex Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%202Optimality%20Conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimality Conditions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0c%203Tractable%20Problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tractable Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5">
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Optimization Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e%20Optimization%20Algos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e1%20Gradient%20Descent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e2%20subgradient%20method/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Subgradient Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0e3%20accelerated%20gs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Accelerated GD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0f%20Convergence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convergence Properties
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0g%20Proximal%20and%20Projected%20Gradient%20Descent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projections &amp; Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0g1%20proximal%20ga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Proximal Gradient Algorithm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0h%20lasso/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LASSO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0k%20mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mirror Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0m%20sgd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stochastic Gradient Descent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0n%20newtons/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Newton's Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0o%20quasi_n/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quasi-Newton Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0q%20interior/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interior Point Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0h AdvancedAlgos.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6">
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Common Convex Optimization Problems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Common Convex Optimization Problems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Common Convex Optimization Problems.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1a%20LP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1c%20Least%20Square/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Least Squares
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1b%20QP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1d%20QCQP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QCQP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1e%20SOCP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SOCP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1f%20GeometricInterpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Geometric Interpretation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1g%20GP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GP
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7">
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Duality &amp; Regularization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Duality &amp; Regularization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2a%20Duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Duality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3a%20Huber/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Huber
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3b%20Penalty%20Functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Penalty Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3c%20Regularized/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularized Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3d%20Robust%20Approximation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robust Approximation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3e%20MLE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MLE
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8">
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Discrimination &amp; Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Discrimination &amp; Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4a%20Linear%20Discrimination/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Discrimination
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6a%20First%20Order%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    First Order Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9">
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Miscellaneous
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Miscellaneous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7a%20pareto%20optimal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pareto Optimality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Example
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/1_0_intro.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/1_0_intro.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="mathematics-for-convex-optimization">Mathematics for Convex Optimization<a class="headerlink" href="#mathematics-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>Modern optimization is geometric at its core. When we minimize a loss function, we are navigating through a high-dimensional landscape defined by vectors, matrices, subspaces, projections, and curvature. Without a clear understanding of these structures, optimization algorithms can feel like black boxes. With the right intuition, gradient descent and its variants can be seen not merely as formulas, but as geometric motions toward feasibility and optimality. In convex optimization, problems are often expressed as minimizing a function over a vector space or a convex subset of it, subject to linear or convex constraints. Each mathematical concept we will cover — from vector spaces and inner products to convex sets and duality — connects directly to this geometric view of optimization.</p>
<p>This section develops the mathematical foundations of convex optimization with an emphasis on geometric structure. Vectors and matrices are treated as operators that shape feasible regions and descent directions. Norms and inner products define the geometry in which distances and angles are measured, influencing step sizes and convergence criteria. Projections and orthogonality arise in constrained problems, dictating how we enforce feasibility. Smoothness and strong convexity introduce curvature that determines convergence rates of algorithms. Spectral properties such as eigenvalues and singular values capture conditioning and guide algorithmic design. By connecting each concept to its role in optimization, algorithms become intelligible not as abstract routines, but as geometric processes acting on structured spaces.</p>
<p>We proceed through the prerequisite mathematics in a logical sequence. We begin with the basics of linear algebra — vector spaces, linear mappings, and subspaces — then introduce inner product spaces and the geometry of lengths, angles, and orthogonality. Next, we explore norms, unit balls, and dual norms, which measure distances and sizes of vectors. We then examine linear operators, eigenvalues, and positive definiteness, which provide insight into conditioning and curvature. With this linear algebra apparatus in hand, we move to multivariate calculus (gradients, Jacobians, Hessians) as tools for describing change and optimality. Building on that, we cover affine and convex sets (the geometry of feasible regions), convex functions and their analysis (including inequalities like Jensen’s), and finally optimality conditions and duality. Throughout, the style prioritizes intuition and practical understanding over formal proofs, using visual metaphors and concrete examples to illustrate each concept. Graduate-level learners and ML practitioners should find a cohesive narrative that prepares them to understand and innovate in convex optimization.</p>
<hr>
<p>s.</p>
<h3 id="inner-product-spaces">Inner Product Spaces<a class="headerlink" href="#inner-product-spaces" title="Permanent link">¶</a></h3>
<p>A <strong>vector space</strong> <span class="arithmatex">\(V\)</span> with an inner product <span class="arithmatex">\(\langle \cdot, \cdot \rangle\)</span> is called an <strong>inner product space</strong>. Inner product spaces provide a natural geometric structure for optimization by defining <strong>lengths, angles, and orthogonality</strong>. This structure generalizes Euclidean geometry to higher dimensions, function spaces, and weighted or structured metrics.  </p>
<h3 id="rank-nullspace-and-range">Rank, Nullspace, and Range<a class="headerlink" href="#rank-nullspace-and-range" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(A \in \mathbb{R}^{m \times n}\)</span>.  </p>
<ul>
<li>
<p><strong>Range (column space):</strong><br>
<span class="arithmatex">\(\text{range}(A) = \{y \in \mathbb{R}^m : y = A x \text{ for some } x \in \mathbb{R}^n\}\)</span>.<br>
  Represents all achievable outputs; its dimension is the rank of <span class="arithmatex">\(A\)</span>.  </p>
</li>
<li>
<p><strong>Nullspace (kernel):</strong><br>
<span class="arithmatex">\(\text{null}(A) = \{x \in \mathbb{R}^n : A x = 0\}\)</span>.<br>
  Directions along which <span class="arithmatex">\(A\)</span> maps to zero; important in constrained optimization and redundancy analysis.  </p>
</li>
<li>
<p><strong>Rank-Nullity theorem:</strong><br>
<script type="math/tex; mode=display">
  \text{rank}(A) + \dim(\text{null}(A)) = n.
  </script>
</p>
</li>
</ul>
<p><strong>Relevance:</strong>  </p>
<ul>
<li>Nullspace directions indicate invariance in the objective.  </li>
<li>Rank reveals independent features in data, affecting dimensionality reduction and regularization.  </li>
<li>Full-rank matrices guarantee unique solutions; rank-deficient matrices imply multiple or no solutions.</li>
</ul>
<h3 id="orthonormal-bases-and-qr-decomposition">Orthonormal Bases and QR Decomposition<a class="headerlink" href="#orthonormal-bases-and-qr-decomposition" title="Permanent link">¶</a></h3>
<p><strong>Orthonormal bases:</strong> <span class="arithmatex">\(\{q_1, \dots, q_n\}\)</span> satisfy <span class="arithmatex">\(\langle q_i, q_j \rangle = \delta_{ij}\)</span>. They simplify computations, stabilize numerics, and enable straightforward projections: <span class="arithmatex">\(P_W(x) = \sum_i \langle x, q_i \rangle q_i\)</span>.  </p>
<p><strong>Gram–Schmidt process:</strong> Converts any linearly independent set <span class="arithmatex">\(\{v_1, \dots, v_n\}\)</span> into an orthonormal set by iterative projection subtraction.  </p>
<p><strong>QR decomposition:</strong> For full-rank <span class="arithmatex">\(A \in \mathbb{R}^{m \times n}\)</span>: <span class="arithmatex">\(A = Q R\)</span>, with orthonormal <span class="arithmatex">\(Q\)</span> and upper-triangular <span class="arithmatex">\(R\)</span>.  </p>
<ul>
<li>Overdetermined systems (<span class="arithmatex">\(m &gt; n\)</span>): solve <span class="arithmatex">\(R x = Q^\top b\)</span> via back-substitution for least-squares solutions.  </li>
<li>Underdetermined systems (<span class="arithmatex">\(m &lt; n\)</span>): infinitely many solutions; regularization selects a meaningful solution.  </li>
</ul>
<p><strong>Applications:</strong>  </p>
<ul>
<li>Efficient solution of linear systems in regression.  </li>
<li>Feature orthogonalization and conditioning improvement.  </li>
<li>Low-dimensional representations in PCA.  </li>
</ul>
<p><strong>Numerical considerations:</strong>  </p>
<ul>
<li>Ill-conditioning occurs when columns are nearly dependent; quantified by condition number.  </li>
<li>Mitigation: scale columns, use Modified Gram–Schmidt or Householder reflections for stability.  </li>
<li>Critical for iterative convex optimization algorithms, where small errors propagate quickly.</li>
</ul>
<h2 id="inner-products-and-geometry">Inner Products and Geometry<a class="headerlink" href="#inner-products-and-geometry" title="Permanent link">¶</a></h2>
<p>Understanding the geometry of vector spaces is essential for convex optimization. Inner products, norms, orthogonality, and projections provide the tools to analyze gradients, measure distances, and enforce constraints. This chapter develops these concepts rigorously, links them to optimization, and provides examples relevant to machine learning.</p>
<h3 id="inner-product-and-induced-norm">Inner Product and Induced Norm<a class="headerlink" href="#inner-product-and-induced-norm" title="Permanent link">¶</a></h3>
<p>An inner product on a vector space <span class="arithmatex">\(V\)</span> is a function <span class="arithmatex">\(\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}\)</span> satisfying:</p>
<ul>
<li>Symmetry: <span class="arithmatex">\(\langle x, y \rangle = \langle y, x \rangle\)</span>  </li>
<li>Linearity: <span class="arithmatex">\(\langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle\)</span>  </li>
<li>Positive definiteness: <span class="arithmatex">\(\langle x, x \rangle \ge 0\)</span> with equality only if <span class="arithmatex">\(x = 0\)</span></li>
</ul>
<p>The Euclidean inner product is <span class="arithmatex">\(\langle x, y \rangle = x^\top y\)</span>. From any inner product, we can define a norm:</p>
<div class="arithmatex">\[
\|x\| = \sqrt{\langle x, x \rangle}.
\]</div>
<p>Cauchy–Schwarz inequality states that for any <span class="arithmatex">\(x, y \in V\)</span>:</p>
<div class="arithmatex">\[
|\langle x, y \rangle| \le \|x\| \, \|y\|.
\]</div>
<p>This inequality is fundamental for deriving bounds on gradient steps, dual norms, and subgradient inequalities in optimization.</p>
<h3 id="parallelogram-law-and-polarization-identity">Parallelogram Law and Polarization Identity<a class="headerlink" href="#parallelogram-law-and-polarization-identity" title="Permanent link">¶</a></h3>
<p>A norm <span class="arithmatex">\(\|\cdot\|\)</span> induced by an inner product satisfies the parallelogram law:</p>
<div class="arithmatex">\[
\|x + y\|^2 + \|x - y\|^2 = 2\|x\|^2 + 2\|y\|^2.
\]</div>
<p>Intuitively, this law describes a geometric property of Euclidean-like spaces: the sum of the squares of the diagonals of a parallelogram equals the sum of the squares of all sides. It captures the essence of inner-product geometry in terms of vector lengths.</p>
<p>Conversely, any norm satisfying this law arises from an inner product via the polarization identity:</p>
<div class="arithmatex">\[
\langle x, y \rangle = \frac{1}{4} \left( \|x + y\|^2 - \|x - y\|^2 \right).
\]</div>
<p>This provides a direct connection between norms and inner products, which is particularly useful when extending linear algebra and optimization concepts beyond standard Euclidean spaces.</p>
<h4 id="hilbert-spaces">Hilbert Spaces<a class="headerlink" href="#hilbert-spaces" title="Permanent link">¶</a></h4>
<p>A Hilbert space is a complete vector space equipped with an inner product. Completeness here means that every Cauchy sequence (a sequence where vectors get arbitrarily close together) converges to a limit within the space. Hilbert spaces generalize Euclidean spaces to potentially infinite dimensions, allowing us to work with functions, sequences, or other objects as “vectors.”</p>
<p>Intuition and relevance in machine learning:</p>
<ul>
<li>Many optimization algorithms are first formulated in finite-dimensional Euclidean spaces (<span class="arithmatex">\(\mathbb{R}^n\)</span>) but can be generalized to Hilbert spaces, enabling algorithms to handle <strong>infinite-dimensional feature spaces</strong>.</li>
<li>Reproducing Kernel Hilbert Spaces (RKHS) are Hilbert spaces of functions with a kernel-based inner product. This allows <strong>kernel methods</strong> (e.g., SVMs, kernel ridge regression) to operate efficiently in very high- or infinite-dimensional spaces without explicitly computing high-dimensional coordinates (the "kernel trick").</li>
<li>The properties like Cauchy–Schwarz, parallelogram law, and induced norms remain valid in Hilbert spaces, which ensures that gradient-based and convex optimization methods can be extended to these functional spaces in a mathematically sound way.</li>
</ul>
<h3 id="gram-matrices-and-least-squares-geometry">Gram Matrices and Least-Squares Geometry<a class="headerlink" href="#gram-matrices-and-least-squares-geometry" title="Permanent link">¶</a></h3>
<p>Given vectors <span class="arithmatex">\(x_1, \dots, x_n \in \mathbb{R}^m\)</span>, the Gram matrix <span class="arithmatex">\(G \in \mathbb{R}^{n \times n}\)</span> is</p>
<div class="arithmatex">\[
G_{ij} = \langle x_i, x_j \rangle.
\]</div>
<p>i.e., it contains all pairwise inner products between the vectors.  </p>
<p>Properties of the Gram Matrix:</p>
<ul>
<li>Symmetric and positive semidefinite:<br>
<script type="math/tex; mode=display">
  z^\top G z \ge 0 \quad \text{for all } z \in \mathbb{R}^m
  </script>
</li>
<li>Rank: The rank of <span class="arithmatex">\(G\)</span> equals the dimension of the span of <span class="arithmatex">\(\{x_1, \dots, x_n\}\)</span>.  </li>
<li>Geometric interpretation: <span class="arithmatex">\(G\)</span> encodes the angles and lengths of the vectors, capturing their correlations and linear dependencies.</li>
</ul>
<h4 id="connection-to-least-squares">Connection to Least-Squares<a class="headerlink" href="#connection-to-least-squares" title="Permanent link">¶</a></h4>
<p>In least-squares problems</p>
<div class="arithmatex">\[ 
X \beta \approx y, 
\]</div>
<p>the normal equations are</p>
<div class="arithmatex">\[ 
X^\top X \beta = X^\top y.
\]</div>
<p>Here, <span class="arithmatex">\(X^\top X\)</span> is the Gram matrix of the columns of <span class="arithmatex">\(X\)</span>. Geometrically:</p>
<ul>
<li><span class="arithmatex">\(X^\top X\)</span> measures how the features relate to each other via inner products.  </li>
<li>Its eigenvalues determine the shape of the error surface in <span class="arithmatex">\(\beta\)</span>-space. If columns of <span class="arithmatex">\(X\)</span> are nearly linearly dependent, the surface becomes elongated, like a stretched ellipse.  </li>
<li>The condition number of <span class="arithmatex">\(X^\top X\)</span> (ratio of largest to smallest eigenvalue) quantifies this elongation:  </li>
<li>High condition number (ill-conditioned): some directions in <span class="arithmatex">\(\beta\)</span>-space change very slowly under gradient-based updates, leading to slow convergence.  </li>
<li>Low condition number (well-conditioned): all directions are updated more evenly, and convergence is faster.</li>
</ul>
<p>Implications: Preprocessing techniques such as feature scaling, orthogonalization (QR decomposition), or regularization improve the condition number and accelerate convergence of optimization algorithms.</p>
<h3 id="orthogonality-and-projections">Orthogonality and Projections<a class="headerlink" href="#orthogonality-and-projections" title="Permanent link">¶</a></h3>
<p>Given a subspace <span class="arithmatex">\(W \subseteq V\)</span> with orthonormal basis <span class="arithmatex">\(\{q_1, \dots, q_k\}\)</span>, the projection of <span class="arithmatex">\(x \in V\)</span> onto <span class="arithmatex">\(W\)</span> is</p>
<div class="arithmatex">\[
P_W(x) = \sum_{i=1}^k \langle x, q_i \rangle q_i.
\]</div>
<p>Properties of projections:</p>
<ul>
<li><span class="arithmatex">\(x - P_W(x)\)</span> is orthogonal to <span class="arithmatex">\(W\)</span>: <span class="arithmatex">\(\langle x - P_W(x), y \rangle = 0\)</span> for all <span class="arithmatex">\(y \in W\)</span>  </li>
<li>Projections are linear and idempotent: <span class="arithmatex">\(P_W(P_W(x)) = P_W(x)\)</span></li>
</ul>
<p>In optimization, projected gradient methods rely on computing <span class="arithmatex">\(P_W(x - \alpha \nabla f(x))\)</span>, ensuring iterates remain feasible within a subspace or convex set.</p>
<p>Metric projections onto convex sets <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> satisfy uniqueness and firm nonexpansiveness:</p>
<div class="arithmatex">\[
\|P_C(x) - P_C(y)\|^2 \le \langle P_C(x) - P_C(y), x - y \rangle.
\]</div>
<p>This property guarantees algorithmic stability and is fundamental in projected gradient and proximal algorithms. Many convex optimization problems can be reformulated using proximal operators, which generalize metric projections. The firm nonexpansiveness of projections ensures that proximal iterations behave predictably and do not amplify errors. The projection can be thought of as “snapping” a point onto the feasible set in the most efficient way. Because of convexity, there’s exactly one closest point, and the firm nonexpansiveness ensures that nearby points stay nearby after projection, which is essential for stable numerical algorithms.</p>
<h3 id="norms-and-unit-ball-geometry">Norms and Unit-Ball Geometry<a class="headerlink" href="#norms-and-unit-ball-geometry" title="Permanent link">¶</a></h3>
<p>Norms induce metrics via <span class="arithmatex">\(d(x, y) = \|x - y\|\)</span>. Common examples:</p>
<ul>
<li><span class="arithmatex">\(\ell_2\)</span> (Euclidean) norm: <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span>  </li>
<li><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>  </li>
<li><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span></li>
</ul>
<p>Unit-ball geometry affects optimization behavior. <span class="arithmatex">\(\ell_1\)</span> balls have corners promoting sparsity, while <span class="arithmatex">\(\ell_2\)</span> balls are smooth, influencing gradient descent and mirror descent choices. Dual norms are defined as</p>
<div class="arithmatex">\[
\|y\|_* = \sup_{\|x\| \le 1} \langle y, x \rangle.
\]</div>
<p>For <span class="arithmatex">\(\ell_p\)</span> norms, the dual norm is <span class="arithmatex">\(\ell_q\)</span> with <span class="arithmatex">\(1/p + 1/q = 1\)</span>. Dual norms underpin subgradient inequalities:</p>
<div class="arithmatex">\[
\langle g, x - x^* \rangle \le \|g\|_* \|x - x^*\|.
\]</div>
<p>These inequalities are essential in primal–dual and subgradient methods.</p>
<h2 id="summary-and-connections-to-optimization">Summary and Connections to Optimization<a class="headerlink" href="#summary-and-connections-to-optimization" title="Permanent link">¶</a></h2>
<ul>
<li>Inner products define angles, lengths, and steepest descent directions.  </li>
<li>The parallelogram law ensures norms arise from inner products in Hilbert spaces.  </li>
<li>Gram matrices encode feature correlations and conditioning for least-squares problems.  </li>
<li>Projections onto subspaces and convex sets enforce feasibility and stability in iterative algorithms.  </li>
<li>Unit-ball geometry and dual norms influence step directions, sparsity, and convergence bounds.</li>
</ul>
<h2 id="norms-and-metric-geometry">Norms and Metric Geometry<a class="headerlink" href="#norms-and-metric-geometry" title="Permanent link">¶</a></h2>
<p>Norms and metrics provide the mathematical framework to measure distances and sizes of vectors in optimization. They are central to analyzing convergence, defining constraints, and designing algorithms. This chapter develops the theory of norms, induced metrics, unit-ball geometry, and dual norms, emphasizing their role in convex optimization and machine learning.</p>
<h3 id="norms-and-induced-metrics">Norms and Induced Metrics<a class="headerlink" href="#norms-and-induced-metrics" title="Permanent link">¶</a></h3>
<p>A norm on a vector space <span class="arithmatex">\(V\)</span> is a function <span class="arithmatex">\(\|\cdot\|: V \to \mathbb{R}\)</span> satisfying:</p>
<ul>
<li>Positive definiteness: <span class="arithmatex">\(\|x\| \ge 0\)</span> and <span class="arithmatex">\(\|x\| = 0 \iff x = 0\)</span>  </li>
<li>Homogeneity: <span class="arithmatex">\(\|\alpha x\| = |\alpha| \|x\|\)</span> for all <span class="arithmatex">\(\alpha \in \mathbb{R}\)</span>  </li>
<li>Triangle inequality: <span class="arithmatex">\(\|x + y\| \le \|x\| + \|y\|\)</span></li>
</ul>
<p>Examples of common norms:</p>
<ul>
<li><span class="arithmatex">\(\ell_2\)</span> norm: <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span> (Euclidean)  </li>
<li><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>  </li>
<li><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span>  </li>
<li>General <span class="arithmatex">\(\ell_p\)</span> norm: <span class="arithmatex">\(\|x\|_p = \left( \sum_i |x_i|^p \right)^{1/p}\)</span></li>
</ul>
<p>A norm induces a metric (distance function) <span class="arithmatex">\(d(x, y) = \|x - y\|\)</span>. Metrics satisfy non-negativity, symmetry, and the triangle inequality. In optimization, metrics define step sizes, stopping criteria, and convergence guarantees.</p>
<p>Example: In gradient descent with step size <span class="arithmatex">\(\alpha\)</span>, the next iterate is</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha \nabla f(x_k),
\]</div>
<p>and convergence is analyzed using <span class="arithmatex">\(\|x_{k+1} - x^*\|\)</span> in the chosen norm.</p>
<p>In an inner product space, the <strong>norm is induced by the inner product</strong>:  </p>
<div class="arithmatex">\[ 
\|x\| = \sqrt{\langle x, x \rangle}. 
\]</div>
<p>This connection is fundamental in optimization: it defines the length of gradients, step sizes, and distances to feasible sets.  </p>
<p><strong>Cauchy–Schwarz Inequality</strong></p>
<p>The Cauchy–Schwarz inequality provides a key bound in inner product spaces: for all <span class="arithmatex">\(x, y \in V\)</span>,  </p>
<div class="arithmatex">\[ 
|\langle x, y \rangle| \le \|x\| \, \|y\|. 
\]</div>
<p><strong>Implications:</strong>  </p>
<ul>
<li>Measures the maximum correlation between two directions; equality occurs when <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent.  </li>
<li>Ensures that the <strong>projection</strong> of one vector onto another does not exceed the product of their lengths:<br>
<script type="math/tex; mode=display"> 
  \text{proj}_y(x) = \frac{\langle x, y \rangle}{\|y\|^2} y 
  </script>
</li>
<li>Forms the foundation for many optimization inequalities, bounds, and convergence analyses.  </li>
</ul>
<h4 id="parallelogram-law-and-polarization-identity_1">Parallelogram Law and Polarization Identity<a class="headerlink" href="#parallelogram-law-and-polarization-identity_1" title="Permanent link">¶</a></h4>
<p>A norm <span class="arithmatex">\(\|\cdot\|\)</span> induced by an inner product satisfies:  </p>
<div class="arithmatex">\[ 
\|x + y\|^2 + \|x - y\|^2 = 2\|x\|^2 + 2\|y\|^2. 
\]</div>
<p>Conversely, any norm satisfying this law comes from an inner product via the <strong>polarization identity</strong>:  </p>
<div class="arithmatex">\[ 
\langle x, y \rangle = \frac{1}{4} \left( \|x + y\|^2 - \|x - y\|^2 \right). 
\]</div>
<h3 id="unit-ball-geometry-and-intuition">Unit-Ball Geometry and Intuition<a class="headerlink" href="#unit-ball-geometry-and-intuition" title="Permanent link">¶</a></h3>
<p>The unit ball of a norm <span class="arithmatex">\(\|\cdot\|\)</span> is</p>
<div class="arithmatex">\[
B = \{ x \in V \mid \|x\| \le 1 \}.
\]</div>
<p>The unit-ball geometry of a norm becomes significant whenever the norm is used in an optimization problem, either as a regularizer or as a constraint. The unit ball represents the set of all points whose norm is less than or equal to one, and its shape provides deep geometric insight into how the optimizer can move and what kinds of solutions it prefers.  </p>
<p>To understand its significance, consider the interaction between the level sets of the objective function and the shape of the unit ball. Level sets are contours along which the objective function has the same value. In unconstrained optimization without norms, the optimizer moves along the steepest descent of these level sets, and the solution is determined entirely by the objective’s curvature and gradients. However, when a norm is added either as a constraint (e.g., <span class="arithmatex">\(\|x\| \le 1\)</span>) or as a regularization term (e.g., <span class="arithmatex">\(\lambda \|x\|\)</span>) the unit ball effectively modifies the landscape: it defines directions that are more expensive or restricted, and it interacts with the level sets to shape the solution path.  </p>
<ul>
<li><span class="arithmatex">\(\ell_2\)</span> norm:<br>
  The unit ball is smooth and round. All directions are treated equally. Level sets intersect the ball symmetrically, allowing gradient steps to proceed smoothly in any direction. <span class="arithmatex">\(\ell_2\)</span> regularization encourages small but evenly distributed values across all coordinates, producing smooth solutions.  </li>
<li><span class="arithmatex">\(\ell_1\)</span> norm:<br>
  The unit ball has sharp corners along the coordinate axes. Level sets often intersect the corners, meaning some coordinates are driven exactly to zero. This produces sparse solutions, as the edges of the <span class="arithmatex">\(\ell_1\)</span> ball act like “funnels” guiding the optimizer toward axes.  </li>
<li><span class="arithmatex">\(\ell_\infty\)</span> norm:<br>
  The unit ball is a cube. It constrains the maximum magnitude of any coordinate, allowing free movement along directions that keep all components within bounds but blocking steps that exceed the faces. This is useful for preventing extreme values in any dimension.  </li>
</ul>
<h3 id="dual-norms">Dual Norms<a class="headerlink" href="#dual-norms" title="Permanent link">¶</a></h3>
<p>In constrained optimization and duality theory, expressions like <span class="arithmatex">\(\langle y, x \rangle\)</span> naturally appear—often representing how a force (such as a gradient or dual variable) interacts with a feasible step direction. To measure how much influence such a vector <span class="arithmatex">\(y\)</span> can exert when movement is restricted by a norm constraint, we define the dual norm:</p>
<div class="arithmatex">\[ 
\|y\|_* = \sup_{\|x\| \le 1} \langle y, x \rangle. 
\]</div>
<p>This definition asks:  </p>
<blockquote>
<p>If movement is only allowed within the unit ball of the original norm, what is the maximum directional effect that <span class="arithmatex">\(y\)</span> can generate?</p>
</blockquote>
<h4 id="intuition-movement-vs-influence">Intuition — movement vs. influence<a class="headerlink" href="#intuition-movement-vs-influence" title="Permanent link">¶</a></h4>
<p>In constrained optimization, the primal norm defines where you are allowed to move, and the dual norm measures how effectively the gradient can move you within that region.</p>
<ul>
<li>The primal norm determines the shape of the feasible directions, forming a mobility region (for example, an <span class="arithmatex">\(\ell_2\)</span> ball is round and smooth, an <span class="arithmatex">\(\ell_1\)</span> ball is sharp and cornered).  </li>
<li>The dual norm tells how much progress a gradient can make when pushing against that region.  </li>
<li>If the gradient aligns with a flat face or a corner of the feasible region, movement becomes limited (as in <span class="arithmatex">\(\ell_1\)</span> geometry, leading to sparse solutions).  </li>
<li>If the feasible region is smooth (as in <span class="arithmatex">\(\ell_2\)</span> geometry), the gradient can always push effectively, producing smooth updates.</li>
</ul>
<h4 id="example-maximum-decrease-under-a-norm-constraint">Example — maximum decrease under a norm constraint<a class="headerlink" href="#example-maximum-decrease-under-a-norm-constraint" title="Permanent link">¶</a></h4>
<p>Consider the constrained problem:</p>
<div class="arithmatex">\[ 
\min_x f(x) \quad \text{subject to} \quad \|x\| \le 1. 
\]</div>
<p>For a small step <span class="arithmatex">\(s\)</span>, the change in <span class="arithmatex">\(f\)</span> is approximately</p>
<div class="arithmatex">\[ 
f(x+s) \approx f(x) + \langle \nabla f(x), s \rangle. 
\]</div>
<p>To decrease <span class="arithmatex">\(f\)</span>, we want to <strong>minimize</strong> <span class="arithmatex">\(\langle \nabla f(x), s \rangle\)</span> over feasible steps <span class="arithmatex">\(\|s\| \le 1\)</span>, or equivalently:</p>
<div class="arithmatex">\[ 
\max_{\|s\| \le 1} \langle -\nabla f(x), s \rangle. 
\]</div>
<p>By definition of the dual norm, this maximum is exactly</p>
<div class="arithmatex">\[ 
\max_{\|s\| \le 1} \langle -\nabla f(x), s \rangle = \|\nabla f(x)\|_*.
\]</div>
<p>Intuition:</p>
<ul>
<li>The primal norm defines the feasible region of allowed steps.  </li>
<li>The dual norm measures the largest possible influence the gradient can exert within that region.  </li>
<li>If the unit ball is round (smooth), the gradient can push efficiently in any direction; if it has corners (as in <span class="arithmatex">\(\ell_1\)</span>), the gradient’s effective action is concentrated along certain axes.  </li>
</ul>
<p>Thus, the dual norm is the true measure of how powerful a gradient or dual variable is under a constraint, not just its raw magnitude. It appears naturally in optimality conditions, Lagrangian duality, and subgradient methods, providing a precise bound on the effect of forces inside the feasible set.</p>
<h3 id="metric-properties-in-optimization">Metric Properties in Optimization<a class="headerlink" href="#metric-properties-in-optimization" title="Permanent link">¶</a></h3>
<p>Metrics derived from norms allow analysis of convergence rates. For an <span class="arithmatex">\(L\)</span>-smooth function <span class="arithmatex">\(f\)</span>, the update <span class="arithmatex">\(x_{k+1} = x_k - \alpha \nabla f(x_k)\)</span> satisfies</p>
<div class="arithmatex">\[
\|x_{k+1} - x^*\| \le \|x_k - x^*\| - \alpha \Big(1 - \frac{L \alpha}{2}\Big) \|\nabla f(x_k)\|^2.
\]</div>
<p>This shows the choice of norm directly affects step size rules, stopping criteria, and algorithmic stability.</p>
<p>Unit-ball shapes also influence <strong>proximal operators</strong>. For a regularizer <span class="arithmatex">\(R(x) = \lambda \|x\|_1\)</span>, the proximal step shrinks components along the axes, exploiting the corners of the <span class="arithmatex">\(\ell_1\)</span> unit ball to enforce sparsity.</p>
<h3 id="norms-on-function-spaces-l_p-norms">Norms on Function Spaces: <span class="arithmatex">\(L_p\)</span> Norms<a class="headerlink" href="#norms-on-function-spaces-l_p-norms" title="Permanent link">¶</a></h3>
<p>Function spaces generalize vector norms. For functions defined on an interval <span class="arithmatex">\([a,b]\)</span>, the <span class="arithmatex">\(L_p\)</span> norm is</p>
<div class="arithmatex">\[
\|f - g\|_{L^p} = \left( \int_a^b |f(x) - g(x)|^p \, dx \right)^{1/p},
\]</div>
<p>for <span class="arithmatex">\(1 \le p &lt; \infty\)</span>, and</p>
<div class="arithmatex">\[
\|f - g\|_{L^\infty} = \operatorname*{ess\,sup}_{x \in [a,b]} |f(x) - g(x)|.
\]</div>
<p>Interpretations:</p>
<ul>
<li><span class="arithmatex">\(L_1\)</span> measures total absolute discrepancy, the shaded area between graphs:<br>
<script type="math/tex; mode=display">
  \|f - g\|_{L^1} = \int_a^b |f(x) - g(x)| \, dx.
  </script>
<br>
  Small, widespread differences accumulate; a narrow spike of small width contributes little to <span class="arithmatex">\(L_1\)</span>.  </li>
<li><span class="arithmatex">\(L_2\)</span> is the RMS or energy of the error:<br>
<script type="math/tex; mode=display">
  \|f - g\|_{L^2} = \left( \int_a^b |f(x) - g(x)|^2 \, dx \right)^{1/2}.
  </script>
<br>
  Squaring amplifies large errors; a spike of height <span class="arithmatex">\(A\)</span> and width <span class="arithmatex">\(\varepsilon\)</span> contributes about <span class="arithmatex">\(A^2 \varepsilon\)</span> to the squared norm.  </li>
<li><span class="arithmatex">\(L_\infty\)</span> is the worst-case deviation:<br>
<script type="math/tex; mode=display">
  \|f - g\|_{L^\infty} = \sup_{x \in [a,b]} |f(x) - g(x)|.
  </script>
<br>
  A single pointwise deviation, no matter how narrow, can dominate the norm. Use <span class="arithmatex">\(L_\infty\)</span> in robust optimization and adversarial settings.</li>
</ul>
<h2 id="linear-operators-and-operator-norms">Linear Operators and Operator Norms<a class="headerlink" href="#linear-operators-and-operator-norms" title="Permanent link">¶</a></h2>
<p>Linear operators and their norms quantify how matrices or linear maps amplify vectors. Understanding these concepts is crucial for step size selection, conditioning, low-rank approximations, and stability of optimization algorithms. This chapter introduces operator norms, special cases, and the singular value decomposition, highlighting their role in convex optimization and machine learning.</p>
<h3 id="operator-norms">Operator Norms<a class="headerlink" href="#operator-norms" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(A: \mathbb{R}^n \to \mathbb{R}^m\)</span> be a linear operator (matrix). The operator norm induced by vector norms <span class="arithmatex">\(\|\cdot\|_p\)</span> and <span class="arithmatex">\(\|\cdot\|_q\)</span> is defined as</p>
<div class="arithmatex">\[
\|A\|_{p \to q} = \sup_{x \neq 0} \frac{\|Ax\|_q}{\|x\|_p} = \sup_{\|x\|_p \le 1} \|Ax\|_q.
\]</div>
<p>Intuition: <span class="arithmatex">\(\|A\|_{p \to q}\)</span> measures the maximum <strong>amplification factor</strong> of a vector under <span class="arithmatex">\(A\)</span> from <span class="arithmatex">\(\ell_p\)</span> space to <span class="arithmatex">\(\ell_q\)</span> space.</p>
<p>Special cases:</p>
<ul>
<li><span class="arithmatex">\(\|A\|_{1 \to 1} = \max_{j} \sum_{i} |A_{ij}|\)</span> (maximum absolute column sum)  </li>
<li><span class="arithmatex">\(\|A\|_{\infty \to \infty} = \max_{i} \sum_{j} |A_{ij}|\)</span> (maximum absolute row sum)  </li>
<li><span class="arithmatex">\(\|A\|_{2 \to 2} = \sigma_{\max}(A)\)</span>, the largest singular value of <span class="arithmatex">\(A\)</span>  </li>
</ul>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>Any matrix <span class="arithmatex">\(A \in \mathbb{R}^{m \times n}\)</span> admits a singular value decomposition:</p>
<div class="arithmatex">\[
A = U \Sigma V^\top,
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(U \in \mathbb{R}^{m \times m}\)</span> and <span class="arithmatex">\(V \in \mathbb{R}^{n \times n}\)</span> are orthogonal matrices  </li>
<li><span class="arithmatex">\(\Sigma \in \mathbb{R}^{m \times n}\)</span> is diagonal with non-negative entries <span class="arithmatex">\(\sigma_1 \ge \sigma_2 \ge \dots \ge 0\)</span>, called <strong>singular values</strong></li>
</ul>
<p>Interpretation:</p>
<ul>
<li><span class="arithmatex">\(V^\top\)</span> rotates coordinates in the input space  </li>
<li><span class="arithmatex">\(\Sigma\)</span> scales each coordinate along its principal direction  </li>
<li><span class="arithmatex">\(U\)</span> rotates coordinates in the output space</li>
</ul>
<p>Geometric intuition: The action of <span class="arithmatex">\(A\)</span> on the unit sphere in <span class="arithmatex">\(\mathbb{R}^n\)</span> produces an <strong>ellipsoid</strong> in <span class="arithmatex">\(\mathbb{R}^m\)</span>, with axes given by singular vectors and lengths given by singular values.</p>
<h3 id="applications-in-optimization-and-machine-learning">Applications in Optimization and Machine Learning<a class="headerlink" href="#applications-in-optimization-and-machine-learning" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Conditioning:</strong> The ratio <span class="arithmatex">\(\kappa(A) = \sigma_{\max}(A) / \sigma_{\min}(A)\)</span> determines sensitivity of linear systems <span class="arithmatex">\(Ax = b\)</span> and least-squares problems to perturbations. Poor conditioning slows convergence.  </li>
<li><strong>Low-rank approximations:</strong> The best rank-<span class="arithmatex">\(k\)</span> approximation of <span class="arithmatex">\(A\)</span> (in Frobenius or spectral norm) is obtained by truncating its SVD. This is widely used in PCA and collaborative filtering.  </li>
<li><strong>Preconditioning:</strong> Linear transformations can be preconditioned using SVD to accelerate gradient-based methods.  </li>
<li><strong>Step amplification:</strong> For gradient descent on quadratic objectives <span class="arithmatex">\(f(x) = \frac{1}{2}x^\top A x - b^\top x\)</span>, the largest singular value <span class="arithmatex">\(\sigma_{\max}(A)\)</span> determines the safe step size <span class="arithmatex">\(\alpha \le 2 / \sigma_{\max}(A)^2\)</span>.</li>
</ul>
<h3 id="numerical-considerations">Numerical Considerations<a class="headerlink" href="#numerical-considerations" title="Permanent link">¶</a></h3>
<ul>
<li>Computing full SVD is expensive for large matrices; truncated SVD or randomized SVD is preferred in high dimensions.  </li>
<li>Operator norms help identify <strong>directions of largest amplification</strong> and potential instability.  </li>
<li>Column scaling or whitening of matrices improves conditioning and convergence of iterative optimization algorithms.</li>
</ul>
<h3 id="summary-and-optimization-connections">Summary and Optimization Connections<a class="headerlink" href="#summary-and-optimization-connections" title="Permanent link">¶</a></h3>
<ul>
<li>Operator norms quantify maximum amplification of vectors under linear transformations.  </li>
<li>Singular values provide geometric insight into the shape of linear maps and determine condition numbers.  </li>
<li>These tools are essential for step size selection, preconditioning, and low-rank modeling in convex optimization and ML.</li>
</ul>
<h2 id="eigenvalues-symmetric-matrices-and-psdpd-matrices">Eigenvalues, Symmetric Matrices, and PSD/PD Matrices<a class="headerlink" href="#eigenvalues-symmetric-matrices-and-psdpd-matrices" title="Permanent link">¶</a></h2>
<p>Eigenvalues and positive semidefinite/definite matrices play a central role in convex optimization. They provide insight into curvature, conditioning, step scaling, and uniqueness of solutions. This chapter develops the theory of eigenpairs, diagonalization of symmetric matrices, and properties of PSD/PD matrices, with direct applications to optimization algorithms and machine learning.</p>
<h3 id="eigenpairs-and-diagonalization-of-symmetric-matrices">Eigenpairs and Diagonalization of Symmetric Matrices<a class="headerlink" href="#eigenpairs-and-diagonalization-of-symmetric-matrices" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(A \in \mathbb{R}^{n \times n}\)</span>. A scalar <span class="arithmatex">\(\lambda \in \mathbb{R}\)</span> is an eigenvalue of <span class="arithmatex">\(A\)</span> if there exists a nonzero vector <span class="arithmatex">\(v \in \mathbb{R}^n\)</span> such that</p>
<div class="arithmatex">\[
A v = \lambda v.
\]</div>
<p>The vector <span class="arithmatex">\(v\)</span> is called an eigenvector associated with <span class="arithmatex">\(\lambda\)</span>.</p>
<p>For symmetric matrices <span class="arithmatex">\(A = A^\top\)</span>, several important properties hold:</p>
<ul>
<li>All eigenvalues are real: <span class="arithmatex">\(\lambda_i \in \mathbb{R}\)</span>  </li>
<li>Eigenvectors corresponding to distinct eigenvalues are orthogonal  </li>
<li><span class="arithmatex">\(A\)</span> can be diagonalized by an orthogonal matrix <span class="arithmatex">\(Q\)</span>:<br>
<script type="math/tex; mode=display">
  A = Q \Lambda Q^\top,
  </script>
<br>
  where <span class="arithmatex">\(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\)</span> and <span class="arithmatex">\(Q^\top Q = I\)</span>.</li>
</ul>
<p>Geometric intuition: Symmetric matrices act as stretching or compressing along orthogonal directions, where the eigenvectors indicate the principal directions and eigenvalues the scaling factors.</p>
<p>In optimization, the eigenvalues of the Hessian <span class="arithmatex">\(\nabla^2 f(x)\)</span> determine curvature along different directions. Positive eigenvalues indicate convexity along that direction, while negative eigenvalues indicate concavity.</p>
<h3 id="positive-semidefinite-and-positive-definite-matrices">Positive Semidefinite and Positive Definite Matrices<a class="headerlink" href="#positive-semidefinite-and-positive-definite-matrices" title="Permanent link">¶</a></h3>
<p>A symmetric matrix <span class="arithmatex">\(A\)</span> is positive semidefinite (PSD) if</p>
<div class="arithmatex">\[
x^\top A x \ge 0 \quad \forall x \in \mathbb{R}^n,
\]</div>
<p>and <strong>positive definite (PD)</strong> if</p>
<div class="arithmatex">\[
x^\top A x &gt; 0 \quad \forall x \neq 0.
\]</div>
<p>Equivalently:</p>
<ul>
<li>PSD: all eigenvalues <span class="arithmatex">\(\lambda_i \ge 0\)</span>  </li>
<li>PD: all eigenvalues <span class="arithmatex">\(\lambda_i &gt; 0\)</span></li>
</ul>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(A \succeq 0 \implies\)</span> quadratic form <span class="arithmatex">\(x^\top A x\)</span> is convex  </li>
<li><span class="arithmatex">\(A \succ 0 \implies\)</span> quadratic form is strictly convex with a unique minimizer</li>
</ul>
<p>In convex optimization, PD Hessians guarantee unique minimizers and enable Newton-type methods with reliable step scaling. PSD matrices appear in quadratic programming and covariance matrices in statistics and machine learning.</p>
<h3 id="spectral-connections-to-optimization">Spectral Connections to Optimization<a class="headerlink" href="#spectral-connections-to-optimization" title="Permanent link">¶</a></h3>
<ol>
<li><strong>Step size selection:</strong> For gradient descent on <span class="arithmatex">\(f(x) = \frac{1}{2} x^\top A x - b^\top x\)</span>, the largest eigenvalue <span class="arithmatex">\(\lambda_{\max}(A)\)</span> determines the maximum safe step size <span class="arithmatex">\(\alpha \le 2/\lambda_{\max}(A)\)</span>.  </li>
<li><strong>Conditioning:</strong> The condition number <span class="arithmatex">\(\kappa(A) = \lambda_{\max}/\lambda_{\min}\)</span> controls the convergence rate of gradient descent and other iterative methods.  </li>
<li><strong>Curvature analysis:</strong> Eigenvectors of the Hessian indicate directions of fastest or slowest curvature, guiding preconditioning and variable rescaling.  </li>
<li><strong>Low-rank approximations:</strong> PSD matrices can be truncated along small eigenvalues to reduce dimensionality in ML applications such as PCA.</li>
</ol>
<h2 id="projections-and-orthogonal-decompositions">Projections and Orthogonal Decompositions<a class="headerlink" href="#projections-and-orthogonal-decompositions" title="Permanent link">¶</a></h2>
<p>Projections and orthogonal decompositions are fundamental tools in convex optimization. They allow us to enforce constraints, compute optimality conditions, and analyze the geometry of feasible regions. This chapter develops projections onto subspaces and convex sets, orthogonal decomposition, and their connections to algorithms.</p>
<h3 id="projection-onto-subspaces">Projection onto Subspaces<a class="headerlink" href="#projection-onto-subspaces" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(W \subseteq \mathbb{R}^n\)</span> be a subspace with an orthonormal basis <span class="arithmatex">\(\{q_1, \dots, q_k\}\)</span>. The projection of <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> onto <span class="arithmatex">\(W\)</span> is</p>
<div class="arithmatex">\[
P_W(x) = \sum_{i=1}^k \langle x, q_i \rangle q_i.
\]</div>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(x - P_W(x)\)</span> is orthogonal to <span class="arithmatex">\(W\)</span>: <span class="arithmatex">\(\langle x - P_W(x), y \rangle = 0\)</span> for all <span class="arithmatex">\(y \in W\)</span>  </li>
<li><span class="arithmatex">\(P_W\)</span> is linear and idempotent: <span class="arithmatex">\(P_W(P_W(x)) = P_W(x)\)</span>  </li>
<li><span class="arithmatex">\(P_W(x)\)</span> minimizes the distance to <span class="arithmatex">\(W\)</span>: <span class="arithmatex">\(\|x - P_W(x)\| = \min_{y \in W} \|x - y\|\)</span></li>
</ul>
<p>Geometric intuition: <span class="arithmatex">\(P_W(x)\)</span> is the “shadow” of <span class="arithmatex">\(x\)</span> onto the subspace <span class="arithmatex">\(W\)</span>. In optimization, projections are used in projected gradient descent, where iterates are kept within a feasible linear subspace.</p>
<h3 id="projection-onto-convex-sets">Projection onto Convex Sets<a class="headerlink" href="#projection-onto-convex-sets" title="Permanent link">¶</a></h3>
<p>For a closed convex set <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span>, the metric projection of <span class="arithmatex">\(x\)</span> onto <span class="arithmatex">\(C\)</span> is</p>
<div class="arithmatex">\[
P_C(x) = \arg\min_{y \in C} \|x - y\|.
\]</div>
<p>Properties:</p>
<ul>
<li><strong>Uniqueness:</strong> There exists a unique minimizer <span class="arithmatex">\(P_C(x)\)</span>.  </li>
<li><strong>Firm nonexpansiveness:</strong>
  <script type="math/tex; mode=display">
  \|P_C(x) - P_C(y)\|^2 \le \langle P_C(x) - P_C(y), x - y \rangle \quad \forall x, y \in \mathbb{R}^n
  </script>
</li>
<li><strong>First-order optimality:</strong> <span class="arithmatex">\(\langle x - P_C(x), y - P_C(x) \rangle \le 0\)</span> for all <span class="arithmatex">\(y \in C\)</span></li>
</ul>
<p>Applications in optimization:</p>
<ul>
<li>Ensures iterates remain feasible in projected gradient methods  </li>
<li>Guarantees stability and convergence due to nonexpansiveness  </li>
<li>Appears in proximal operators when <span class="arithmatex">\(C\)</span> is a level set of a regularizer</li>
</ul>
<p>A useful corollary is nonexpansiveness:<br>
<script type="math/tex; mode=display">
\|\operatorname{proj}_C(x) - \operatorname{proj}_C(y)\| \le \|x - y\|.
</script>
</p>
<h3 id="orthogonal-decomposition">Orthogonal Decomposition<a class="headerlink" href="#orthogonal-decomposition" title="Permanent link">¶</a></h3>
<p>Any vector <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> can be uniquely decomposed into components along a subspace <span class="arithmatex">\(W\)</span> and its orthogonal complement <span class="arithmatex">\(W^\perp\)</span>:</p>
<div class="arithmatex">\[
x = P_W(x) + (x - P_W(x)), \quad P_W(x) \in W, \quad x - P_W(x) \in W^\perp.
\]</div>
<p>Properties:</p>
<ul>
<li>The decomposition is unique.  </li>
<li><span class="arithmatex">\(\|x\|^2 = \|P_W(x)\|^2 + \|x - P_W(x)\|^2\)</span> (Pythagoras’ theorem).  </li>
</ul>
<p>Applications:</p>
<ul>
<li>In constrained optimization, the gradient can be decomposed into components tangent to constraints and normal to them, forming the basis for KKT conditions.  </li>
<li>In machine learning, projections onto feature subspaces allow dimensionality reduction and orthogonalization of data.</li>
</ul>
<h2 id="calculus-essentials-gradients-hessians-and-taylor-expansions">Calculus Essentials: Gradients, Hessians, and Taylor Expansions<a class="headerlink" href="#calculus-essentials-gradients-hessians-and-taylor-expansions" title="Permanent link">¶</a></h2>
<p>Calculus provides the foundation for optimization algorithms. Gradients indicate directions of steepest ascent or descent, Hessians encode curvature, and Taylor expansions give local approximations of functions. This chapter develops these concepts with an eye toward convex optimization and machine learning applications.</p>
<h3 id="gradient-and-directional-derivative">Gradient and Directional Derivative<a class="headerlink" href="#gradient-and-directional-derivative" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> be differentiable. The <strong>gradient</strong> of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> is the vector</p>
<div class="arithmatex">\[
\nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}.
\]</div>
<p>Properties:</p>
<ul>
<li>Linear approximation: <span class="arithmatex">\(f(x + h) \approx f(x) + \langle \nabla f(x), h \rangle\)</span> for small <span class="arithmatex">\(h\)</span>  </li>
<li>Steepest ascent direction: <span class="arithmatex">\(\nabla f(x)\)</span>  </li>
<li>Steepest descent direction: <span class="arithmatex">\(-\nabla f(x)\)</span></li>
</ul>
<p>The directional derivative of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> in direction <span class="arithmatex">\(u\)</span> is</p>
<div class="arithmatex">\[
D_u f(x) = \lim_{t \to 0} \frac{f(x + t u) - f(x)}{t} = \langle \nabla f(x), u \rangle.
\]</div>
<p>Optimization application: Gradient descent updates are</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha \nabla f(x_k),
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is a step size, moving in the direction of steepest decrease.</p>
<h3 id="hessian-and-second-order-directional-derivatives">Hessian and Second-Order Directional Derivatives<a class="headerlink" href="#hessian-and-second-order-directional-derivatives" title="Permanent link">¶</a></h3>
<p>The Hessian of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> is the symmetric matrix of second partial derivatives:</p>
<div class="arithmatex">\[
\nabla^2 f(x) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}.
\]</div>
<p>Properties:</p>
<ul>
<li>Symmetric: <span class="arithmatex">\(\nabla^2 f(x) = (\nabla^2 f(x))^\top\)</span>  </li>
<li>Quadratic form: <span class="arithmatex">\(u^\top \nabla^2 f(x) u\)</span> measures curvature along <span class="arithmatex">\(u\)</span>  </li>
<li>Positive semidefinite Hessian <span class="arithmatex">\(\implies\)</span> local convexity</li>
</ul>
<p>Optimization application: Newton’s method updates</p>
<div class="arithmatex">\[
x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
\]</div>
<p>use curvature information to accelerate convergence, especially near minimizers.</p>
<h3 id="taylor-expansions">Taylor Expansions<a class="headerlink" href="#taylor-expansions" title="Permanent link">¶</a></h3>
<p>The second-order Taylor expansion of <span class="arithmatex">\(f\)</span> around <span class="arithmatex">\(x\)</span> is</p>
<div class="arithmatex">\[
f(x + h) \approx f(x) + \langle \nabla f(x), h \rangle + \frac{1}{2} h^\top \nabla^2 f(x) h.
\]</div>
<p>Properties:</p>
<ul>
<li>Linear term captures slope (first-order approximation).  </li>
<li>Quadratic term captures curvature.  </li>
<li>Provides local quadratic models used in Newton and quasi-Newton methods.  </li>
</ul>
<h3 id="optimization-connections">Optimization Connections<a class="headerlink" href="#optimization-connections" title="Permanent link">¶</a></h3>
<ul>
<li>Gradients determine update directions in first-order methods.  </li>
<li>Hessians determine curvature, step scaling, and Newton updates.  </li>
<li>Taylor expansions provide local approximations, guiding line search and trust-region methods.  </li>
<li>Eigenvalues of the Hessian determine convexity, step sizes, and conditioning.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>