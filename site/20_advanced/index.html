<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/20_advanced/">
      
      
        <link rel="prev" href="../19_optimizationalgo/">
      
      
        <link rel="next" href="../21_models/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>10. Advanced Large-Scale and Structured Methods - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-10-advanced-large-scale-and-structured-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              10. Advanced Large-Scale and Structured Methods
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/20_advanced.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/20_advanced.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-10-advanced-large-scale-and-structured-methods">Chapter 10: Advanced Large-Scale and Structured Methods<a class="headerlink" href="#chapter-10-advanced-large-scale-and-structured-methods" title="Permanent link">¶</a></h1>
<p>In Chapter 9 we focused on “classical convex solvers”: gradient methods, accelerated methods, Newton and quasi-Newton methods, projected/proximal methods, and interior-point methods. Those are the canonical tools of convex optimisation.</p>
<p>This chapter moves one step further.</p>
<p>Here we study methods that:
- exploit <strong>problem structure</strong> (sparsity, separability, block structure),
- scale to extremely high dimensions,
- or are widely used in practice for machine learning and signal processing — including in problems that are not convex.</p>
<p>Some of these methods were first analysed in the convex setting (often with strong guarantees), and then adopted — sometimes recklessly — in the nonconvex world (training neural nets, matrix factorisation, etc.). You’ll absolutely see them in modern optimisation and ML code.</p>
<p>We’ll cover:
1. Coordinate (block) descent,
2. Stochastic gradient and mini-batch methods,
3. ADMM (Alternating Direction Method of Multipliers),
4. Proximal coordinate / coordinate proximal variants,
5. Majorization–minimization and iterative reweighted schemes.</p>
<p>Throughout we’ll emphasise:
- When they are provably correct for convex problems,
- Why people also use them in nonconvex problems.</p>
<hr>
<h2 id="101-coordinate-descent-and-block-coordinate-descent">10.1 Coordinate descent and block coordinate descent<a class="headerlink" href="#101-coordinate-descent-and-block-coordinate-descent" title="Permanent link">¶</a></h2>
<h3 id="1011-idea">10.1.1 Idea<a class="headerlink" href="#1011-idea" title="Permanent link">¶</a></h3>
<p>Instead of updating <strong>all</strong> coordinates of <span class="arithmatex">\(x\)</span> at once using a full gradient or Newton direction, we update <strong>one coordinate (or one block of coordinates)</strong> at a time, holding the others fixed.</p>
<p>Suppose we want to minimise a convex function
<script type="math/tex; mode=display">
\min_x F(x),
</script>
and write <span class="arithmatex">\(x = (x_1, x_2, \dots, x_p)\)</span> in coordinates or blocks.<br>
Coordinate descent cycles through <span class="arithmatex">\(i = 1,2,\dots,p\)</span> and solves (or approximately solves)
<script type="math/tex; mode=display">
x_i^{(k+1)}
=
\arg\min_{z} \; F\big(x_1^{(k+1)}, \dots, x_{i-1}^{(k+1)}, z, x_{i+1}^{(k)}, \dots, x_p^{(k)}\big).
</script>
</p>
<p>In other words: update coordinate <span class="arithmatex">\(i\)</span> by optimising over just that coordinate (or block), treating the rest as constants.</p>
<h3 id="1012-why-this-can-be-fast">10.1.2 Why this can be fast<a class="headerlink" href="#1012-why-this-can-be-fast" title="Permanent link">¶</a></h3>
<ul>
<li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li>
<li>For problems with separable structure — e.g. sums over features, or regularisers like <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span> — the coordinate update is extremely cheap.</li>
<li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li>
</ul>
<p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>
<h3 id="1013-convergence-in-convex-problems">10.1.3 Convergence in convex problems<a class="headerlink" href="#1013-convergence-in-convex-problems" title="Permanent link">¶</a></h3>
<p>For many convex, continuously differentiable problems with certain regularity (e.g. strictly convex objective, or convex plus separable nonsmooth terms), cyclic coordinate descent is guaranteed to converge to the global minimiser. There are also randomized versions that pick a coordinate uniformly at random, which often give cleaner expected-rate guarantees.</p>
<p>For <span class="arithmatex">\(\ell_1\)</span>-regularised least squares, i.e.
<script type="math/tex; mode=display">
\min_x \; \tfrac12 \|Ax - b\|_2^2 + \lambda \|x\|_1,
</script>
each coordinate update becomes a scalar soft-thresholding step — so coordinate descent becomes an extremely efficient sparse regression solver.</p>
<h3 id="1014-block-coordinate-descent">10.1.4 Block coordinate descent<a class="headerlink" href="#1014-block-coordinate-descent" title="Permanent link">¶</a></h3>
<p>When coordinates are naturally grouped (for example, <span class="arithmatex">\(x\)</span> is really <span class="arithmatex">\((x^{(1)}, x^{(2)}, \dots)\)</span> where each <span class="arithmatex">\(x^{(j)}\)</span> is a vector of parameters for a submodule or layer), we generalise to <strong>block coordinate descent</strong>. Each step solves
<script type="math/tex; mode=display">
x^{(j)} \leftarrow \arg\min_{z} F(\dots, z, \dots)\,.
</script>
</p>
<p>Block coordinate descent is the backbone of many alternating minimisation schemes in signal processing, matrix factorisation, dictionary learning, etc.</p>
<h3 id="1015-use-in-nonconvex-problems">10.1.5 Use in nonconvex problems<a class="headerlink" href="#1015-use-in-nonconvex-problems" title="Permanent link">¶</a></h3>
<p>Even when <span class="arithmatex">\(F\)</span> is not convex, people still run block coordinate descent (under names like “alternating minimisation” or “alternating least squares”), because:</p>
<ul>
<li>each block subproblem might be convex even if the joint problem isn’t,</li>
<li>it is easy to implement,</li>
<li>it often works “well enough” in practice.</li>
</ul>
<p>You see this in low-rank matrix factorisation (recommender systems), where you fix all user factors and update item factors, then swap. There are no global guarantees in general (no convexity), but empirically it converges to useful solutions.</p>
<p>So:<br>
- In convex settings → provable global convergence.<br>
- In nonconvex settings → heuristic that often finds acceptable stationary points.</p>
<hr>
<h2 id="102-stochastic-gradient-and-mini-batch-methods">10.2 Stochastic gradient and mini-batch methods<a class="headerlink" href="#102-stochastic-gradient-and-mini-batch-methods" title="Permanent link">¶</a></h2>
<h3 id="1021-full-gradient-vs-stochastic-gradient">10.2.1 Full gradient vs stochastic gradient<a class="headerlink" href="#1021-full-gradient-vs-stochastic-gradient" title="Permanent link">¶</a></h3>
<p>In Chapter 9, gradient descent uses the full gradient <span class="arithmatex">\(\nabla f(x)\)</span> at each step. In large-scale learning problems, <span class="arithmatex">\(f\)</span> is almost always an average over data:
<script type="math/tex; mode=display">
f(x) = \frac{1}{N} \sum_{i=1}^N \ell_i(x),
</script>
where <span class="arithmatex">\(\ell_i\)</span> is the loss on sample <span class="arithmatex">\(i\)</span>.</p>
<p>Computing <span class="arithmatex">\(\nabla f(x)\)</span> exactly costs <span class="arithmatex">\(O(N)\)</span> per step, which is huge.</p>
<p><strong>Stochastic Gradient Descent (SGD)</strong> replaces <span class="arithmatex">\(\nabla f(x)\)</span> with an unbiased estimate. At each iteration we:</p>
<ol>
<li>Sample <span class="arithmatex">\(i\)</span> uniformly from <span class="arithmatex">\(\{1,\dots,N\}\)</span>,</li>
<li>Use <span class="arithmatex">\(g_k = \nabla \ell_i(x_k)\)</span>,</li>
<li>Update
   <script type="math/tex; mode=display">
   x_{k+1} = x_k - \alpha_k g_k.
   </script>
</li>
</ol>
<p>This is extremely cheap: one data point (or a small mini-batch) per step.</p>
<h3 id="1022-convergence-in-convex-problems">10.2.2 Convergence in convex problems<a class="headerlink" href="#1022-convergence-in-convex-problems" title="Permanent link">¶</a></h3>
<p>For convex problems, with diminishing step sizes <span class="arithmatex">\(\alpha_k\)</span>, SGD converges to the global optimum in expectation, and more refined analyses show <span class="arithmatex">\(O(1/\sqrt{k})\)</span> suboptimality rates for general convex Lipschitz losses, improving to <span class="arithmatex">\(O(1/k)\)</span> in strongly convex smooth cases with appropriate averaging.</p>
<p>That is slower (per iteration) than deterministic gradient descent in theory, but each iteration is <em>much</em> cheaper. So SGD wins in wall-clock time for huge <span class="arithmatex">\(N\)</span>.</p>
<h3 id="1023-momentum-adam-rmsprop-nonconvex-practice-convex-roots">10.2.3 Momentum, Adam, RMSProp (nonconvex practice, convex roots)<a class="headerlink" href="#1023-momentum-adam-rmsprop-nonconvex-practice-convex-roots" title="Permanent link">¶</a></h3>
<p>In modern machine learning, methods like momentum SGD, Adam, RMSProp, Adagrad, etc., are used routinely to train enormous nonconvex models (deep networks). These are variations of first-order methods with:</p>
<ul>
<li>adaptive step sizes,</li>
<li>running averages of squared gradients,</li>
<li>momentum terms.</li>
</ul>
<p>While the most common use is for nonconvex problems, many of these methods (e.g. Adagrad-type adaptive steps, momentum acceleration) have their theoretical roots in convex optimisation and mirror-descent style analyses.</p>
<p>So stochastic first-order methods are:</p>
<ul>
<li>rigorous for convex problems,</li>
<li>widely used heuristically for nonconvex problems.</li>
</ul>
<hr>
<h2 id="103-admm-alternating-direction-method-of-multipliers">10.3 ADMM: Alternating Direction Method of Multipliers<a class="headerlink" href="#103-admm-alternating-direction-method-of-multipliers" title="Permanent link">¶</a></h2>
<p>ADMM is one of the most important algorithms in modern convex optimisation for structured problems. It is used constantly in signal processing, sparse learning, distributed optimisation, and large-scale statistical estimation.</p>
<h3 id="1031-problem-form">10.3.1 Problem form<a class="headerlink" href="#1031-problem-form" title="Permanent link">¶</a></h3>
<p>ADMM solves problems of the form
<script type="math/tex; mode=display">
\min_{x,z} \; f(x) + g(z)
\quad
\text{subject to} \quad
Ax + Bz = c,
</script>
where <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are convex.</p>
<p>This form appears everywhere:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> is a data-fit term,</li>
<li><span class="arithmatex">\(g\)</span> is a regulariser or constraint indicator,</li>
<li><span class="arithmatex">\(Ax + Bz = c\)</span> ties them together.</li>
</ul>
<p>For example, LASSO can be written by introducing a copy variable and enforcing <span class="arithmatex">\(x=z\)</span>.</p>
<h3 id="1032-augmented-lagrangian">10.3.2 Augmented Lagrangian<a class="headerlink" href="#1032-augmented-lagrangian" title="Permanent link">¶</a></h3>
<p>ADMM applies the augmented Lagrangian method, which is like dual ascent but with a quadratic penalty on constraint violation. The augmented Lagrangian is
<script type="math/tex; mode=display">
\mathcal{L}_\rho(x,z,y)
=
f(x) + g(z)
+ y^\top (Ax + Bz - c)
+ \frac{\rho}{2} \|Ax + Bz - c\|_2^2,
</script>
with dual variable (Lagrange multiplier) <span class="arithmatex">\(y\)</span> and penalty parameter <span class="arithmatex">\(\rho&gt;0\)</span>.</p>
<h3 id="1033-the-admm-updates-two-block-case">10.3.3 The ADMM updates (two-block case)<a class="headerlink" href="#1033-the-admm-updates-two-block-case" title="Permanent link">¶</a></h3>
<p>Iterate the following:
1. <strong><span class="arithmatex">\(x\)</span>-update:</strong>
   <script type="math/tex; mode=display">
   x^{k+1}
   :=
   \arg\min_x \mathcal{L}_\rho(x, z^k, y^k)
   </script>
   (holding <span class="arithmatex">\(z,y\)</span> fixed).
2. <strong><span class="arithmatex">\(z\)</span>-update:</strong>
   <script type="math/tex; mode=display">
   z^{k+1}
   :=
   \arg\min_z \mathcal{L}_\rho(x^{k+1}, z, y^k).
   </script>
3. <strong>Dual update:</strong>
   <script type="math/tex; mode=display">
   y^{k+1}
   :=
   y^k
   + \rho (A x^{k+1} + B z^{k+1} - c).
   </script>
</p>
<p>That is: optimise <span class="arithmatex">\(x\)</span> given <span class="arithmatex">\(z\)</span>, optimise <span class="arithmatex">\(z\)</span> given <span class="arithmatex">\(x\)</span>, then update the multiplier.</p>
<h3 id="1034-why-admm-is-powerful">10.3.4 Why ADMM is powerful<a class="headerlink" href="#1034-why-admm-is-powerful" title="Permanent link">¶</a></h3>
<ul>
<li>Each subproblem often becomes simple and separable:</li>
<li>The <span class="arithmatex">\(x\)</span>-update might be a least-squares or a smooth convex minimisation,</li>
<li>The <span class="arithmatex">\(z\)</span>-update might be a proximal operator (soft-thresholding, projection, etc.).</li>
<li>You never have to solve the full coupled problem in one shot.</li>
<li>ADMM is embarrassingly parallel / distributable: different blocks can be solved on different machines then averaged via the multiplier step.</li>
</ul>
<h3 id="1035-convergence">10.3.5 Convergence<a class="headerlink" href="#1035-convergence" title="Permanent link">¶</a></h3>
<p>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span>, under mild assumptions (closed proper convex functions, some regularity), ADMM converges to a solution of the primal problem, and the dual variable <span class="arithmatex">\(y^k\)</span> converges to an optimal dual multiplier (Boyd and Vandenberghe, 2004, Ch. 5; also classical ADMM literature).</p>
<p>This is deeply tied to duality (Chapter 8): ADMM is best understood as a method of solving the dual with decomposability, but returning primal iterates along the way.</p>
<h3 id="1036-use-in-nonconvex-problems">10.3.6 Use in nonconvex problems<a class="headerlink" href="#1036-use-in-nonconvex-problems" title="Permanent link">¶</a></h3>
<p>In practice, ADMM is often extended to nonconvex problems by simply “pretending it’s fine.” Each subproblem is solved anyway, and the dual variable is updated the same way. The method is no longer guaranteed to find a global minimiser — but it often finds a stationary point that is good enough (e.g. in nonconvex regularised matrix completion, dictionary learning, etc.).</p>
<p>You will see ADMM used in imaging, sparse coding, variational inference, etc., even when parts of the model are not convex.</p>
<hr>
<h2 id="104-proximal-coordinate-and-coordinate-prox-methods">10.4 Proximal coordinate and coordinate-prox methods<a class="headerlink" href="#104-proximal-coordinate-and-coordinate-prox-methods" title="Permanent link">¶</a></h2>
<p>There’s a natural fusion of the ideas in Sections 10.1 (coordinate descent) and 9.5 (proximal methods): <strong>proximal coordinate descent</strong>.</p>
<h3 id="1041-problem-form">10.4.1 Problem form<a class="headerlink" href="#1041-problem-form" title="Permanent link">¶</a></h3>
<p>Consider composite convex objectives
<script type="math/tex; mode=display">
F(x) = f(x) + R(x),
</script>
with <span class="arithmatex">\(f\)</span> smooth convex and <span class="arithmatex">\(R\)</span> convex, possibly nonsmooth and separable across coordinates or blocks:
<script type="math/tex; mode=display">
R(x) = \sum_{j=1}^p R_j(x_j).
</script>
</p>
<h3 id="1042-algorithm-sketch">10.4.2 Algorithm sketch<a class="headerlink" href="#1042-algorithm-sketch" title="Permanent link">¶</a></h3>
<p>At each iteration, pick coordinate (or block) <span class="arithmatex">\(j\)</span>, and update only <span class="arithmatex">\(x_j\)</span> by solving the 1D (or low-dim) proximal subproblem:
<script type="math/tex; mode=display">
x_j^{(k+1)}
=
\arg\min_{z}
\left[
\underbrace{
f\big(x_1^{(k+1)}, \dots, x_{j-1}^{(k+1)}, z, x_{j+1}^{(k)}, \dots \big)
}_{\text{local linear/quadratic approximation}}
+ R_j(z)
\right].
</script>
</p>
<p>Often we linearise <span class="arithmatex">\(f\)</span> around the current point in that block and add a quadratic term, just like a proximal gradient step but on one coordinate at a time.</p>
<h3 id="1043-why-its-useful">10.4.3 Why it’s useful<a class="headerlink" href="#1043-why-its-useful" title="Permanent link">¶</a></h3>
<ul>
<li>When <span class="arithmatex">\(R\)</span> is separable (e.g. <span class="arithmatex">\(\ell_1\)</span> sparsity penalties), each coordinate subproblem becomes a scalar shrinkage / thresholding step.</li>
<li>Memory footprint is tiny.</li>
<li>You get sparsity “for free” as many coordinates get driven to zero and stay there.</li>
<li>Randomised versions (pick a coordinate at random) are simple and have good expected convergence guarantees in convex problems.</li>
</ul>
<h3 id="1044-use-in-nonconvex-settings">10.4.4 Use in nonconvex settings<a class="headerlink" href="#1044-use-in-nonconvex-settings" title="Permanent link">¶</a></h3>
<p>People run proximal coordinate descent in nonconvex sparse learning (e.g. <span class="arithmatex">\(\ell_0\)</span>-like surrogates, nonconvex penalties for variable selection). The convex convergence guarantees are gone, but empirically the method still often converges to a structured, interpretable solution.</p>
<hr>
<h2 id="105-majorizationminimization-mm-and-reweighted-schemes">10.5 Majorization–minimization (MM) and reweighted schemes<a class="headerlink" href="#105-majorizationminimization-mm-and-reweighted-schemes" title="Permanent link">¶</a></h2>
<p>Majorization–minimization (MM) is a general pattern:</p>
<ol>
<li>Build a simple convex surrogate that upper-bounds (majorises) your objective at the current iterate,</li>
<li>Minimise the surrogate,</li>
<li>Repeat.</li>
</ol>
<p>It is sometimes called “iterative reweighted” or “successive convex approximation.”</p>
<h3 id="1051-mm-template">10.5.1 MM template<a class="headerlink" href="#1051-mm-template" title="Permanent link">¶</a></h3>
<p>Suppose we want to minimise <span class="arithmatex">\(F(x)\)</span> (convex or not). We construct <span class="arithmatex">\(G(x \mid x^{(k)})\)</span> such that:</p>
<ul>
<li><span class="arithmatex">\(G(x^{(k)} \mid x^{(k)}) = F(x^{(k)})\)</span> (touches at current iterate),</li>
<li><span class="arithmatex">\(G(x \mid x^{(k)}) \ge F(x)\)</span> for all <span class="arithmatex">\(x\)</span> (majorises <span class="arithmatex">\(F\)</span>),</li>
<li><span class="arithmatex">\(G(\cdot \mid x^{(k)})\)</span> is easy to minimise (often convex, often separable).</li>
</ul>
<p>Then we set
<script type="math/tex; mode=display">
x^{(k+1)} \in \arg\min_x G(x \mid x^{(k)}).
</script>
</p>
<p>This guarantees <span class="arithmatex">\(F(x^{(k+1)}) \le F(x^{(k)})\)</span>. So the objective is monotonically nonincreasing.</p>
<h3 id="1052-iterative-reweighted-ell_1-ell_2">10.5.2 Iterative reweighted <span class="arithmatex">\(\ell_1\)</span> / <span class="arithmatex">\(\ell_2\)</span><a class="headerlink" href="#1052-iterative-reweighted-ell_1-ell_2" title="Permanent link">¶</a></h3>
<p>A classical example: to promote sparsity or robustness, you might want to minimise something like
<script type="math/tex; mode=display">
\sum_i w_i(x) \, |x_i|
</script>
or a concave penalty on residuals. You replace that concave / nonconvex penalty with a weighted convex penalty that depends on the previous iterate. Then you update the weights and solve again.</p>
<p>In the convex world, MM is just another way to design descent methods.<br>
In the nonconvex world, MM is a way to attack nonconvex penalties using a sequence of convex subproblems.</p>
<p>This is extremely common in robust regression, compressed sensing with nonconvex sparsity surrogates, and low-rank matrix recovery.</p>
<h3 id="1053-relation-to-proximal-methods">10.5.3 Relation to proximal methods<a class="headerlink" href="#1053-relation-to-proximal-methods" title="Permanent link">¶</a></h3>
<p>MM can often be interpreted as doing a proximal step on a locally quadratic or linearised upper bound. In that sense, it is philosophically close to proximal gradient (Chapter 9) and to Newton-like local quadratic approximation (Chapter 9), but with the additional twist that we are allowed to handle nonconvex <span class="arithmatex">\(F\)</span> as long as we <em>majorise</em> it with something convex.</p>
<hr>
<h2 id="106-summary-and-perspective">10.6 Summary and perspective<a class="headerlink" href="#106-summary-and-perspective" title="Permanent link">¶</a></h2>
<p>We’ve now seen several algorithmic families that are particularly important at large scale and/or under structural constraints:</p>
<ol>
<li>
<p><strong>Coordinate descent / block coordinate descent</strong> </p>
<ul>
<li>Updates one coordinate block at a time.  </li>
<li>Converges globally for many convex problems.  </li>
<li>Scales extremely well in high dimensions.  </li>
<li>Used heuristically in nonconvex alternating minimisation.</li>
</ul>
</li>
<li>
<p><strong>Stochastic and mini-batch gradient methods</strong>  </p>
<ul>
<li>Use noisy gradient estimates to get cheap iterations.  </li>
<li>Converge (in expectation) for convex problems.  </li>
<li>Power all of modern large-scale ML, including nonconvex deep learning.</li>
</ul>
</li>
<li>
<p><strong>ADMM (Alternating Direction Method of Multipliers)</strong>  </p>
<ul>
<li>Splits a problem into simpler subproblems linked by linear constraints.  </li>
<li>Closely tied to duality and KKT (Chapters 7–8).  </li>
<li>Converges for convex problems.  </li>
<li>Used everywhere, including nonconvex settings, due to its modularity and parallelisability.</li>
</ul>
</li>
<li>
<p><strong>Proximal coordinate / coordinate-prox methods</strong>  </p>
<ul>
<li>Merge sparsity-inducing penalties (Chapter 6) with blockwise updates.  </li>
<li>Ideal for <span class="arithmatex">\(\ell_1\)</span>-type structure, group lasso, etc.  </li>
<li>Often extended to nonconvex penalties for even “more sparse” solutions.</li>
</ul>
</li>
<li>
<p><strong>Majorization–minimization (MM)</strong>  </p>
<ul>
<li>Iteratively builds and minimises convex surrogates.  </li>
<li>Guarantees monotone descent of the true objective.  </li>
<li>Provides a clean bridge from convex optimisation theory into heuristic nonconvex optimisation.</li>
</ul>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>