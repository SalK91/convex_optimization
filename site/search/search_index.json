{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Optimization lies at the heart of mathematics, data science, and engineering. It formalizes the intuitive idea of choosing the \u201cbest\u201d decision among many possible ones. In its most general form, an optimization problem seeks to minimize (or maximize) a real-valued function subject to constraints that define the feasible set of solutions.</p> <p>Formally, a constrained optimization problem can be written as:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_0(x) \\\\ \\text{subject to} \\quad &amp; f_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\ &amp; g_i(x) = 0, \\quad i = 1, \\ldots, p, \\end{aligned} \\] <p>where:  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the vector of decision variables,  </li> <li>\\(f_0(x)\\) is the objective function to be minimized,  </li> <li>\\(f_i(x)\\) represent inequality constraints, and  </li> <li>\\(g_i(x)\\) represent equality constraints.</li> </ul> <p>This general framework encompasses a vast range of applications, from resource allocation and control to statistical estimation and machine learning. Variants include maximization problems, problems with multiple objectives, and stochastic formulations in which some elements are random.</p>"},{"location":"#convex-optimization","title":"Convex Optimization","text":"<p>A convex optimization problem is a special, yet remarkably powerful, subclass of optimization problems where both the objective and the feasible set have convex structure:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_0(x) \\\\ \\text{subject to} \\quad &amp; f_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\ &amp; A x = b. \\end{aligned} \\] <p>Here:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the optimization variable,  </li> <li>the equality constraints \\(A x = b\\) are linear, and  </li> <li>each function \\(f_i\\) is convex, satisfying </li> </ul> <p>Convexity implies that the feasible region is a convex set and that the objective function curves upward. The consequence is profound: every local minimum is a global minimum. This property makes convex optimization both theoretically elegant and computationally tractable. Algorithms can be designed with guarantees of convergence and optimality, features that are rare in general nonlinear optimization.</p>"},{"location":"#convex-optimization-in-modern-applications","title":"Convex Optimization in Modern Applications","text":"<p>Convex optimization underpins a wide array of models and algorithms in modern machine learning and statistics, including:</p> <ul> <li>Lasso (\\(\\ell_1\\)-regularized regression)  </li> <li>Ridge regression (\\(\\ell_2\\)-regularized regression)  </li> <li>Logistic regression and maximum likelihood estimation  </li> <li>Support Vector Machines (SVMs)  </li> <li>Principal Component Analysis (PCA) and low-rank relaxations  </li> <li>Convex relaxations of discrete or probabilistic models  </li> </ul> <p>Mastering convex optimization enables one to:</p> <ul> <li>Recognize when a problem admits a convex formulation and can thus be solved efficiently,  </li> <li>Design and analyze algorithms with provable performance,  </li> <li>Predict convergence behavior from first principles, and  </li> <li>Engineer models with explicit guarantees of robustness and generalization.</li> </ul>"},{"location":"#structure-of-this-webbook","title":"Structure of This Webbook","text":"<p>This webbook is organized around three tightly connected themes:</p> <ol> <li> <p>Foundations of Convex Optimization    Geometry of convex sets, properties of convex functions, and the structure of optimization landscapes that enable global guarantees.</p> </li> <li> <p>Algorithms for Convex Problems    Gradient-based methods, subgradients, proximal operators, duality theory, and the relationships between first-order and second-order methods.</p> </li> <li> <p>Convex Models in Machine Learning    Sparse regression, support vector machines, dimensionality reduction, and convex relaxations of more complex models.</p> </li> </ol> <p>The goal is not merely to present definitions and theorems, but to develop the intuition to recognize convex structure in real problems and the technical skill to exploit it effectively. Convex optimization thus serves as both a unifying theory and a practical toolkit, a lens through which modern machine learning can be designed, understood, and improved with mathematical rigor.</p>"},{"location":"0a%200intro/","title":"Introduction","text":"<p>Modern optimization is geometric at its core. When we minimize a loss function, we are navigating through a high-dimensional landscape defined by vectors, matrices, subspaces, projections, and curvature. Without a clear understanding of these structures, optimization algorithms can feel like black boxes. With the right intuition, gradient descent and its variants can be seen not merely as formulas, but as geometric motions toward feasibility and optimality.</p> <p>This chapter develops the mathematical foundations of convex optimization with an emphasis on geometric structure. Vectors and matrices are treated as operators that shape feasible regions and descent directions. Norms and inner products define the geometry in which distances and angles are measured, influencing step sizes and stopping criteria. Projections and orthogonality arise naturally in constrained optimization problems. Smoothness and strong convexity introduce curvature bounds that determine convergence rates. Spectral quantities such as eigenvalues and singular values capture conditioning and guide algorithmic design.</p> <p>By connecting each mathematical concept directly to its role in optimization, this chapter aims to make algorithms intelligible not as abstract routines, but as geometric processes acting on structured spaces.</p>"},{"location":"0a%2010Convex%20Functions/","title":"Convex Functions","text":""},{"location":"0a%2010Convex%20Functions/#convex-functions","title":"Convex Functions","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if its domain \\(\\mathrm{dom}(f) \\subseteq \\mathbb{R}^n\\) is convex and, for all \\(x_1, x_2 \\in \\mathrm{dom}(f)\\) and all \\(\\theta \\in [0,1]\\):</p> \\[ f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta) f(x_2) \\] <ul> <li>Convex domain: Any line segment between two points in \\(\\mathrm{dom}(f)\\) stays entirely inside the domain.  </li> <li>Graph intuition: The graph of \\(f\\) lies below the straight line connecting any two points on it \u2014 it\u2019s \u201cbowl-shaped\u201d or at least flat in all directions.  </li> </ul> <p>ML intuition: Convex functions ensure that local minima are global minima, which is crucial for reliable model training.</p>"},{"location":"0a%2010Convex%20Functions/#first-order-condition","title":"First-Order Condition","text":"<p>For a convex function \\(f\\) on a convex domain:</p>"},{"location":"0a%2010Convex%20Functions/#differentiable-case","title":"Differentiable Case","text":"<p>If \\(f\\) is differentiable at \\(x\\), the gradient \\(\\nabla f(x)\\) satisfies, for all \\(y \\in \\mathrm{dom}(f)\\):</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^T (y - x) \\] <ul> <li>Geometric meaning: The tangent hyperplane at \\(x\\) lies below the graph everywhere.  </li> <li>ML intuition: Gradient points in the direction of steepest ascent, and its negative is a descent direction.</li> </ul>"},{"location":"0a%2010Convex%20Functions/#non-differentiable-case-subgradients","title":"Non-Differentiable Case (Subgradients)","text":"<p>If \\(f\\) is convex but not differentiable at \\(x\\), a subgradient \\(g \\in \\mathbb{R}^n\\) satisfies:</p> \\[ f(y) \\ge f(x) + g^T (y - x), \\quad \\forall y \\in \\mathrm{dom}(f) \\] <ul> <li>The set of all such \\(g\\) is called the subdifferential:</li> </ul> \\[ \\partial f(x) = \\{ g \\in \\mathbb{R}^n \\mid f(y) \\ge f(x) + g^T (y - x), \\ \\forall y \\in \\mathrm{dom}(f) \\} \\] <ul> <li>Geometric meaning: Even at a \u201ckink,\u201d there exists a hyperplane (with slope \\(g\\)) that supports the graph from below.  </li> <li>If \\(f\\) is differentiable, \\(\\partial f(x) = \\{\\nabla f(x)\\}\\).</li> </ul> <p>ML intuition: Subgradients let us optimize nonsmooth functions like \\(L_1\\) regularization or hinge loss.</p>"},{"location":"0a%2010Convex%20Functions/#second-order-hessian-condition","title":"Second-Order (Hessian) Condition","text":"<p>If \\(f\\) is twice differentiable, \\(f\\) is convex if and only if:</p> \\[ \\nabla^2 f(x) \\succeq 0, \\quad \\forall x \\in \\mathrm{dom}(f) \\] <ul> <li>Positive semidefinite Hessian means the function curves upward or is flat in all directions \u2014 never downward.  </li> <li>Intuition: \u201cCurvature is nonnegative in all directions.\u201d</li> </ul>"},{"location":"0a%2010Convex%20Functions/#examples-of-convex-functions","title":"Examples of Convex Functions","text":"<ol> <li>Quadratic functions: \\(f(x) = \\frac{1}{2} x^T Q x + b^T x + c\\), \\(Q \\succeq 0\\).  </li> <li>Norms: \\(\\|x\\|_p\\), \\(p \\ge 1\\).  </li> <li>Exponential: \\(f(x) = e^x\\).  </li> <li>Negative logarithm: \\(f(x) = -\\log(x)\\) for \\(x&gt;0\\).  </li> <li>Linear functions: \\(f(x) = a^T x + b\\).</li> </ol>"},{"location":"0a%2010Convex%20Functions/#subgradients-nonsmooth-functions","title":"Subgradients &amp; Nonsmooth Functions","text":"<p>Many ML problems involve nonsmooth convex functions:</p> <ul> <li>Absolute value: \\(f(x) = |x|\\) </li> <li>Hinge loss: \\(f(x) = \\max(0, 1-x)\\) </li> <li>\\(L_1\\) norm: \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\) </li> </ul> <p>Definition: \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if:</p> \\[ f(y) \\ge f(x) + g^T (y-x), \\quad \\forall y \\in \\mathbb{R}^n \\] <ul> <li>The set of all subgradients is \\(\\partial f(x)\\).  </li> <li>Geometric meaning: Subgradients define supporting hyperplanes at kinks.</li> </ul>"},{"location":"0a%2010Convex%20Functions/#example-1-absolute-value","title":"Example 1: Absolute Value","text":"<p>\\(f(x) = |x|\\) </p> <ul> <li>\\(x &gt; 0\\): \\(\\nabla f(x) = 1\\) </li> <li>\\(x &lt; 0\\): \\(\\nabla f(x) = -1\\) </li> <li>\\(x = 0\\): derivative doesn\u2019t exist, but</li> </ul> \\[ \\partial f(0) = \\{ g \\in [-1,1] \\} \\] <p>Intuition: At \\(x=0\\), there\u2019s a fan of supporting lines.</p>"},{"location":"0a%2010Convex%20Functions/#example-2-hinge-loss","title":"Example 2: Hinge Loss","text":"<p>\\(f(x) = \\max(0,1-x)\\) </p> <ul> <li>\\(x &lt; 1\\): \\(\\nabla f(x) = -1\\) </li> <li>\\(x &gt; 1\\): \\(\\nabla f(x) = 0\\) </li> <li>\\(x = 1\\):  </li> </ul> \\[ \\partial f(1) = [-1,0] \\] <p>ML relevance: SVM optimization uses these subgradients.</p>"},{"location":"0a%2010Convex%20Functions/#convex-optimization-problems","title":"Convex Optimization Problems","text":"<p>A general convex optimization problem:</p> \\[ \\begin{aligned} &amp; \\min_x \\quad &amp; f_0(x) \\\\ &amp; \\text{s.t.} \\quad &amp; f_i(x) \\le 0, \\quad i=1,\\dots,m \\\\ &amp; &amp; h_j(x) = 0, \\quad j=1,\\dots,p \\end{aligned} \\] <ul> <li>\\(f_0, f_i\\): convex  </li> <li>\\(h_j\\): affine  </li> <li>Feasible set is convex, so any local minimum is global</li> </ul> <p>ML relevance: This framework covers:</p> <ul> <li>Linear and quadratic programs  </li> <li>SVM, Lasso, Ridge regression  </li> <li>Constrained deep learning layers</li> </ul>"},{"location":"0a%201la_foundations/","title":"Linear Algebra Review","text":""},{"location":"0a%201la_foundations/#linear-algebra-foundations-for-convex-optimization","title":"Linear Algebra Foundations for Convex Optimization","text":"<p>Linear algebra is the natural language of optimization. It provides the notation, geometric intuition, and computational tools needed to describe and solve problems in high-dimensional spaces. In convex optimization, problems are often expressed as minimizing (or maximizing) a function over a vector space or a convex subset thereof, subject to linear or convex constraints. Understanding the linear structure of these spaces is essential for interpreting algorithmic behavior, defining feasible directions, and designing efficient methods.  </p> <p>At its core, linear algebra deals with vectors and matrices. A vector represents a direction and magnitude in space, while a matrix represents a linear transformation mapping vectors from one space to another. In optimization, model parameters, gradients, constraints, and data are all naturally represented as vectors or matrices. Linear algebra provides the rules for combining and transforming these objects, allowing us to describe feasible directions, transformations, and projections precisely.  </p> <p>Why emphasize linear structures? Linear transformations preserve combinations: if we know how a transformation acts on a set of basis vectors, we know how it acts on every vector formed from those bases. Even nonlinear problems behave approximately linearly when examined locally: around any smooth point, the function or constraint surface can be approximated by its tangent space, which captures the local linear behavior. A tangent space at a point on a manifold or constraint set is the vector space of directions in which infinitesimal movement remains feasible. For example, at a point on a spherical constraint, the tangent space is the plane that touches the sphere at that point. Many optimization algorithms, including gradient descent and projected methods, operate within or relative to such tangent spaces.  </p> <p>From this perspective, linear algebra is not just a computational tool: it is the geometric framework of optimization. Vectors and subspaces define directions, inner products define lengths and angles, and matrices encode linear transformations. Understanding these concepts allows us to interpret gradients, constraints, and projections in a unified geometric and algebraic framework.  </p> <p>This chapter builds that foundation step by step. We begin with vector spaces and subspaces, then study inner products and orthogonality, examine rank, nullspace, and range, and conclude with orthonormal bases and matrix decompositions. Each concept is connected to its role in optimization and machine learning, providing both intuition and rigorous tools for computation.</p>"},{"location":"0a%201la_foundations/#vector-spaces-and-subspaces","title":"Vector Spaces and Subspaces","text":"<p>A vector space \\(V\\) over \\(\\mathbb{R}\\) is a set of elements called vectors, equipped with addition and scalar multiplication satisfying closure, associativity, commutativity, distributivity, the existence of a zero vector, and additive inverses. A subspace \\(W \\subseteq V\\) is a subset that itself forms a vector space under the same operations.  </p> <ul> <li> <p>Span: The span of vectors \\(\\{v_1, \\dots, v_k\\} \\subseteq V\\) is the set of all linear combinations:    This defines all directions reachable by combining the given vectors.</p> </li> <li> <p>Linear independence: Vectors are linearly independent if no vector can be written as a combination of the others: </p> </li> <li> <p>Basis and dimension: A basis is a linearly independent set that spans the space. The dimension is the number of vectors in any basis. Once a basis \\(\\{b_1, \\dots, b_n\\}\\) is chosen, any vector \\(x \\in V\\) can be uniquely expressed as    where \\(x_i\\) are coordinates in that basis.</p> </li> <li> <p>Affine sets: An affine set is a translate of a subspace:    where \\(A\\) is a matrix and \\(b\\) is a vector. Affine sets generalize lines and planes that do not pass through the origin.</p> </li> </ul> <p>Relevance to optimization: </p> <ul> <li>Subspaces define feasible directions along which movement does not violate constraints.  </li> <li>Affine sets describe linear constraints, forming the geometry of feasible regions.  </li> <li>Low-dimensional subspaces support dimensionality reduction, e.g., in PCA.  </li> <li>Choosing a basis enables coordinate representation of gradients, constraints, and other quantities.</li> </ul>"},{"location":"0a%201la_foundations/#inner-product","title":"Inner Product","text":"<p>An inner product on a vector space \\(V\\) is a map \\(\\langle \\cdot, \\cdot \\rangle: V \\times V \\to \\mathbb{R}\\) satisfying:  </p> <ul> <li>Symmetry: \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\) </li> <li>Linearity: \\(\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\\) </li> <li>Positive-definiteness: \\(\\langle x, x \\rangle \\ge 0\\) with equality iff \\(x = 0\\) </li> </ul> <p>Inner products define the geometry of a space: lengths, angles, and orthogonality. They underpin projections, orthonormal bases, and the definition of gradients in optimization.</p> <p>Induced metric: The inner product induces a norm and metric:  which defines lengths, angles, and distances between vectors.</p> <p>Examples: </p> <ul> <li> <p>Euclidean inner product: \\(\\langle x, y \\rangle = x^\\top y\\). Standard vector length: \\(\\|x\\| = \\sqrt{x^\\top x}\\). Orthogonality corresponds to perpendicular vectors. Defines conventional gradients in \\(\\mathbb{R}^n\\).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x, y \\rangle_W = x^\\top W y\\) for positive definite \\(W\\). Used in preconditioning and Mahalanobis metrics, scaling directions according to problem structure.  </p> </li> <li> <p>Frobenius inner product (matrices): \\(\\langle A, B \\rangle = \\text{trace}(A^\\top B)\\). Induces Frobenius norm \\(\\|A\\|_F = \\sqrt{\\text{trace}(A^\\top A)}\\). Used in low-rank approximation, PCA, and matrix completion.  </p> </li> <li> <p>Function-space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t) g(t) dt\\). Induces \\(L^2\\) norm and orthogonality for functions, used in functional PCA and Hilbert-space optimization.  </p> </li> </ul> <p>Applications in optimization: </p> <ul> <li> <p>Projections: For a subspace \\(W\\) with orthonormal basis \\(\\{q_i\\}\\), the projection of \\(x\\) is \\(P_W(x) = \\sum_i \\langle x, q_i \\rangle q_i\\), minimizing distance to the subspace. Essential in projected gradient methods.  </p> </li> <li> <p>Gradient definition: For differentiable \\(f: V \\to \\mathbb{R}\\), the gradient \\(\\nabla f(x)\\) satisfies    for small \\(h \\in V\\). The inner product determines the direction of steepest ascent, which changes with the geometry of the space.</p> </li> </ul>"},{"location":"0a%201la_foundations/#inner-product-spaces","title":"Inner Product Spaces","text":""},{"location":"0a%201la_foundations/#a-vector-space-v-with-an-inner-product-langle-cdot-cdot-rangle-is-called-an-inner-product-space-inner-product-spaces-provide-a-natural-geometric-structure-for-optimization-by-defining-lengths-angles-and-orthogonality-this-structure-generalizes-euclidean-geometry-to-higher-dimensions-function-spaces-and-weighted-or-structured-metrics","title":"A vector space \\(V\\) with an inner product \\(\\langle \\cdot, \\cdot \\rangle\\) is called an inner product space. Inner product spaces provide a natural geometric structure for optimization by defining lengths, angles, and orthogonality. This structure generalizes Euclidean geometry to higher dimensions, function spaces, and weighted or structured metrics.","text":""},{"location":"0a%201la_foundations/#rank-nullspace-and-range","title":"Rank, Nullspace, and Range","text":"<p>Let \\(A \\in \\mathbb{R}^{m \\times n}\\).  </p> <ul> <li> <p>Range (column space): \\(\\text{range}(A) = \\{y \\in \\mathbb{R}^m : y = A x \\text{ for some } x \\in \\mathbb{R}^n\\}\\).   Represents all achievable outputs; its dimension is the rank of \\(A\\).  </p> </li> <li> <p>Nullspace (kernel): \\(\\text{null}(A) = \\{x \\in \\mathbb{R}^n : A x = 0\\}\\).   Directions along which \\(A\\) maps to zero; important in constrained optimization and redundancy analysis.  </p> </li> <li> <p>Rank-Nullity theorem: </p> </li> </ul> <p>Relevance: </p> <ul> <li>Nullspace directions indicate invariance in the objective.  </li> <li>Rank reveals independent features in data, affecting dimensionality reduction and regularization.  </li> <li>Full-rank matrices guarantee unique solutions; rank-deficient matrices imply multiple or no solutions.</li> </ul>"},{"location":"0a%201la_foundations/#orthonormal-bases-and-qr-decomposition","title":"Orthonormal Bases and QR Decomposition","text":"<p>Orthonormal bases: \\(\\{q_1, \\dots, q_n\\}\\) satisfy \\(\\langle q_i, q_j \\rangle = \\delta_{ij}\\). They simplify computations, stabilize numerics, and enable straightforward projections: \\(P_W(x) = \\sum_i \\langle x, q_i \\rangle q_i\\).  </p> <p>Gram\u2013Schmidt process: Converts any linearly independent set \\(\\{v_1, \\dots, v_n\\}\\) into an orthonormal set by iterative projection subtraction.  </p> <p>QR decomposition: For full-rank \\(A \\in \\mathbb{R}^{m \\times n}\\): \\(A = Q R\\), with orthonormal \\(Q\\) and upper-triangular \\(R\\).  </p> <ul> <li>Overdetermined systems (\\(m &gt; n\\)): solve \\(R x = Q^\\top b\\) via back-substitution for least-squares solutions.  </li> <li>Underdetermined systems (\\(m &lt; n\\)): infinitely many solutions; regularization selects a meaningful solution.  </li> </ul> <p>Applications: </p> <ul> <li>Efficient solution of linear systems in regression.  </li> <li>Feature orthogonalization and conditioning improvement.  </li> <li>Low-dimensional representations in PCA.  </li> </ul> <p>Numerical considerations: </p> <ul> <li>Ill-conditioning occurs when columns are nearly dependent; quantified by condition number.  </li> <li>Mitigation: scale columns, use Modified Gram\u2013Schmidt or Householder reflections for stability.  </li> <li>Critical for iterative convex optimization algorithms, where small errors propagate quickly.</li> </ul>"},{"location":"0a%202innerproducts/","title":"Inner Products","text":""},{"location":"0a%202innerproducts/#inner-products-and-geometry","title":"Inner Products and Geometry","text":"<p>Understanding the geometry of vector spaces is essential for convex optimization. Inner products, norms, orthogonality, and projections provide the tools to analyze gradients, measure distances, and enforce constraints. This chapter develops these concepts rigorously, links them to optimization, and provides examples relevant to machine learning.</p>"},{"location":"0a%202innerproducts/#inner-product-and-induced-norm","title":"Inner Product and Induced Norm","text":"<p>An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) satisfying:</p> <ul> <li>Symmetry: \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\) </li> <li>Linearity: \\(\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\\) </li> <li>Positive definiteness: \\(\\langle x, x \\rangle \\ge 0\\) with equality only if \\(x = 0\\)</li> </ul> <p>The Euclidean inner product is \\(\\langle x, y \\rangle = x^\\top y\\). From any inner product, we can define a norm:</p> \\[ \\|x\\| = \\sqrt{\\langle x, x \\rangle}. \\] <p>Cauchy\u2013Schwarz inequality states that for any \\(x, y \\in V\\):</p> \\[ |\\langle x, y \\rangle| \\le \\|x\\| \\, \\|y\\|. \\] <p>This inequality is fundamental for deriving bounds on gradient steps, dual norms, and subgradient inequalities in optimization.</p>"},{"location":"0a%202innerproducts/#parallelogram-law-and-polarization-identity","title":"Parallelogram Law and Polarization Identity","text":"<p>A norm \\(\\|\\cdot\\|\\) induced by an inner product satisfies the parallelogram law:</p> \\[ \\|x + y\\|^2 + \\|x - y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2. \\] <p>Intuitively, this law describes a geometric property of Euclidean-like spaces: the sum of the squares of the diagonals of a parallelogram equals the sum of the squares of all sides. It captures the essence of inner-product geometry in terms of vector lengths.</p> <p>Conversely, any norm satisfying this law arises from an inner product via the polarization identity:</p> \\[ \\langle x, y \\rangle = \\frac{1}{4} \\left( \\|x + y\\|^2 - \\|x - y\\|^2 \\right). \\] <p>This provides a direct connection between norms and inner products, which is particularly useful when extending linear algebra and optimization concepts beyond standard Euclidean spaces.</p>"},{"location":"0a%202innerproducts/#hilbert-spaces","title":"Hilbert Spaces","text":"<p>A Hilbert space is a complete vector space equipped with an inner product. Completeness here means that every Cauchy sequence (a sequence where vectors get arbitrarily close together) converges to a limit within the space. Hilbert spaces generalize Euclidean spaces to potentially infinite dimensions, allowing us to work with functions, sequences, or other objects as \u201cvectors.\u201d</p> <p>Intuition and relevance in machine learning:</p> <ul> <li>Many optimization algorithms are first formulated in finite-dimensional Euclidean spaces (\\(\\mathbb{R}^n\\)) but can be generalized to Hilbert spaces, enabling algorithms to handle infinite-dimensional feature spaces.</li> <li>Reproducing Kernel Hilbert Spaces (RKHS) are Hilbert spaces of functions with a kernel-based inner product. This allows kernel methods (e.g., SVMs, kernel ridge regression) to operate efficiently in very high- or infinite-dimensional spaces without explicitly computing high-dimensional coordinates (the \"kernel trick\").</li> <li>The properties like Cauchy\u2013Schwarz, parallelogram law, and induced norms remain valid in Hilbert spaces, which ensures that gradient-based and convex optimization methods can be extended to these functional spaces in a mathematically sound way.</li> </ul>"},{"location":"0a%202innerproducts/#gram-matrices-and-least-squares-geometry","title":"Gram Matrices and Least-Squares Geometry","text":"<p>Given vectors \\(x_1, \\dots, x_n \\in \\mathbb{R}^m\\), the Gram matrix \\(G \\in \\mathbb{R}^{n \\times n}\\) is</p> \\[ G_{ij} = \\langle x_i, x_j \\rangle. \\] <p>i.e., it contains all pairwise inner products between the vectors.  </p> <p>Properties of the Gram Matrix:</p> <ul> <li>Symmetric and positive semidefinite: </li> <li>Rank: The rank of \\(G\\) equals the dimension of the span of \\(\\{x_1, \\dots, x_n\\}\\).  </li> <li>Geometric interpretation: \\(G\\) encodes the angles and lengths of the vectors, capturing their correlations and linear dependencies.</li> </ul>"},{"location":"0a%202innerproducts/#connection-to-least-squares","title":"Connection to Least-Squares","text":"<p>In least-squares problems</p> \\[ X \\beta \\approx y, \\] <p>the normal equations are</p> \\[ X^\\top X \\beta = X^\\top y. \\] <p>Here, \\(X^\\top X\\) is the Gram matrix of the columns of \\(X\\). Geometrically:</p> <ul> <li>\\(X^\\top X\\) measures how the features relate to each other via inner products.  </li> <li>Its eigenvalues determine the shape of the error surface in \\(\\beta\\)-space. If columns of \\(X\\) are nearly linearly dependent, the surface becomes elongated, like a stretched ellipse.  </li> <li>The condition number of \\(X^\\top X\\) (ratio of largest to smallest eigenvalue) quantifies this elongation:</li> <li>High condition number (ill-conditioned): some directions in \\(\\beta\\)-space change very slowly under gradient-based updates, leading to slow convergence.  </li> <li>Low condition number (well-conditioned): all directions are updated more evenly, and convergence is faster.</li> </ul> <p>Implications: Preprocessing techniques such as feature scaling, orthogonalization (QR decomposition), or regularization improve the condition number and accelerate convergence of optimization algorithms.</p>"},{"location":"0a%202innerproducts/#orthogonality-and-projections","title":"Orthogonality and Projections","text":"<p>Given a subspace \\(W \\subseteq V\\) with orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\), the projection of \\(x \\in V\\) onto \\(W\\) is</p> \\[ P_W(x) = \\sum_{i=1}^k \\langle x, q_i \\rangle q_i. \\] <p>Properties of projections:</p> <ul> <li>\\(x - P_W(x)\\) is orthogonal to \\(W\\): \\(\\langle x - P_W(x), y \\rangle = 0\\) for all \\(y \\in W\\) </li> <li>Projections are linear and idempotent: \\(P_W(P_W(x)) = P_W(x)\\)</li> </ul> <p>In optimization, projected gradient methods rely on computing \\(P_W(x - \\alpha \\nabla f(x))\\), ensuring iterates remain feasible within a subspace or convex set.</p> <p>Metric projections onto convex sets \\(C \\subseteq \\mathbb{R}^n\\) satisfy uniqueness and firm nonexpansiveness:</p> \\[ \\|P_C(x) - P_C(y)\\|^2 \\le \\langle P_C(x) - P_C(y), x - y \\rangle. \\] <p>This property guarantees algorithmic stability and is fundamental in projected gradient and proximal algorithms. Many convex optimization problems can be reformulated using proximal operators, which generalize metric projections. The firm nonexpansiveness of projections ensures that proximal iterations behave predictably and do not amplify errors. The projection can be thought of as \u201csnapping\u201d a point onto the feasible set in the most efficient way.  Because of convexity, there\u2019s exactly one closest point, and the firm nonexpansiveness ensures that nearby points stay nearby after projection, which is essential for stable numerical algorithms.</p>"},{"location":"0a%202innerproducts/#norms-and-unit-ball-geometry","title":"Norms and Unit-Ball Geometry","text":"<p>Norms induce metrics via \\(d(x, y) = \\|x - y\\|\\). Common examples:</p> <ul> <li>\\(\\ell_2\\) (Euclidean) norm: \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\) </li> <li>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\) </li> <li>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\)</li> </ul> <p>Unit-ball geometry affects optimization behavior. \\(\\ell_1\\) balls have corners promoting sparsity, while \\(\\ell_2\\) balls are smooth, influencing gradient descent and mirror descent choices. Dual norms are defined as</p> \\[ \\|y\\|_* = \\sup_{\\|x\\| \\le 1} \\langle y, x \\rangle. \\] <p>For \\(\\ell_p\\) norms, the dual norm is \\(\\ell_q\\) with \\(1/p + 1/q = 1\\). Dual norms underpin subgradient inequalities:</p> \\[ \\langle g, x - x^* \\rangle \\le \\|g\\|_* \\|x - x^*\\|. \\] <p>These inequalities are essential in primal\u2013dual and subgradient methods.</p>"},{"location":"0a%202innerproducts/#summary-and-connections-to-optimization","title":"Summary and Connections to Optimization","text":"<ul> <li>Inner products define angles, lengths, and steepest descent directions.  </li> <li>The parallelogram law ensures norms arise from inner products in Hilbert spaces.  </li> <li>Gram matrices encode feature correlations and conditioning for least-squares problems.  </li> <li>Projections onto subspaces and convex sets enforce feasibility and stability in iterative algorithms.  </li> <li>Unit-ball geometry and dual norms influence step directions, sparsity, and convergence bounds.</li> </ul>"},{"location":"0a%203norms/","title":"Norms","text":""},{"location":"0a%203norms/#norms-and-metric-geometry","title":"Norms and Metric Geometry","text":"<p>Norms and metrics provide the mathematical framework to measure distances and sizes of vectors in optimization. They are central to analyzing convergence, defining constraints, and designing algorithms. This chapter develops the theory of norms, induced metrics, unit-ball geometry, and dual norms, emphasizing their role in convex optimization and machine learning.</p>"},{"location":"0a%203norms/#norms-and-induced-metrics","title":"Norms and Induced Metrics","text":"<p>A norm on a vector space \\(V\\) is a function \\(\\|\\cdot\\|: V \\to \\mathbb{R}\\) satisfying:</p> <ul> <li>Positive definiteness: \\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x = 0\\) </li> <li>Homogeneity: \\(\\|\\alpha x\\| = |\\alpha| \\|x\\|\\) for all \\(\\alpha \\in \\mathbb{R}\\) </li> <li>Triangle inequality: \\(\\|x + y\\| \\le \\|x\\| + \\|y\\|\\)</li> </ul> <p>Examples of common norms:</p> <ul> <li>\\(\\ell_2\\) norm: \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\) (Euclidean)  </li> <li>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\) </li> <li>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\) </li> <li>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left( \\sum_i |x_i|^p \\right)^{1/p}\\)</li> </ul> <p>A norm induces a metric (distance function) \\(d(x, y) = \\|x - y\\|\\). Metrics satisfy non-negativity, symmetry, and the triangle inequality. In optimization, metrics define step sizes, stopping criteria, and convergence guarantees.</p> <p>Example: In gradient descent with step size \\(\\alpha\\), the next iterate is</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k), \\] <p>and convergence is analyzed using \\(\\|x_{k+1} - x^*\\|\\) in the chosen norm.</p> <p>In an inner product space, the norm is induced by the inner product:  </p> \\[ \\|x\\| = \\sqrt{\\langle x, x \\rangle}. \\] <p>This connection is fundamental in optimization: it defines the length of gradients, step sizes, and distances to feasible sets.  </p> <p>Cauchy\u2013Schwarz Inequality</p> <p>The Cauchy\u2013Schwarz inequality provides a key bound in inner product spaces: for all \\(x, y \\in V\\),  </p> \\[ |\\langle x, y \\rangle| \\le \\|x\\| \\, \\|y\\|. \\] <p>Implications: </p> <ul> <li>Measures the maximum correlation between two directions; equality occurs when \\(x\\) and \\(y\\) are linearly dependent.  </li> <li>Ensures that the projection of one vector onto another does not exceed the product of their lengths: </li> <li>Forms the foundation for many optimization inequalities, bounds, and convergence analyses.  </li> </ul>"},{"location":"0a%203norms/#parallelogram-law-and-polarization-identity","title":"Parallelogram Law and Polarization Identity","text":"<p>A norm \\(\\|\\cdot\\|\\) induced by an inner product satisfies: </p> <p>Conversely, any norm satisfying this law comes from an inner product via the polarization identity: </p>"},{"location":"0a%203norms/#unit-ball-geometry-and-intuition","title":"Unit-Ball Geometry and Intuition","text":"<p>The unit ball of a norm \\(\\|\\cdot\\|\\) is</p> \\[ B = \\{ x \\in V \\mid \\|x\\| \\le 1 \\}. \\] <p>The unit-ball geometry of a norm becomes significant whenever the norm is used in an optimization problem, either as a regularizer or as a constraint. The unit ball represents the set of all points whose norm is less than or equal to one, and its shape provides deep geometric insight into how the optimizer can move and what kinds of solutions it prefers.  </p> <p>To understand its significance, consider the interaction between the level sets of the objective function and the shape of the unit ball. Level sets are contours along which the objective function has the same value. In unconstrained optimization without norms, the optimizer moves along the steepest descent of these level sets, and the solution is determined entirely by the objective\u2019s curvature and gradients. However, when a norm is added either as a constraint (e.g., \\(\\|x\\| \\le 1\\)) or as a regularization term (e.g., \\(\\lambda \\|x\\|\\)) the unit ball effectively modifies the landscape: it defines directions that are more expensive or restricted, and it interacts with the level sets to shape the solution path.  </p> <ul> <li> <p>\\(\\ell_2\\) norm:   The unit ball is smooth and round. All directions are treated equally. Level sets intersect the ball symmetrically, allowing gradient steps to proceed smoothly in any direction. \\(\\ell_2\\) regularization encourages small but evenly distributed values across all coordinates, producing smooth solutions.  </p> </li> <li> <p>\\(\\ell_1\\) norm: The unit ball has sharp corners along the coordinate axes. Level sets often intersect the corners, meaning some coordinates are driven exactly to zero. This produces sparse solutions, as the edges of the \\(\\ell_1\\) ball act like \u201cfunnels\u201d guiding the optimizer toward axes.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm:  The unit ball is a cube. It constrains the maximum magnitude of any coordinate, allowing free movement along directions that keep all components within bounds but blocking steps that exceed the faces. This is useful for preventing extreme values in any dimension.  </p> </li> </ul>"},{"location":"0a%203norms/#dual-norms","title":"Dual Norms","text":"<p>In constrained optimisation and duality theory, expressions like \\(\\langle y, x \\rangle\\) naturally appear\u2014often representing how a force (such as a gradient or dual variable) interacts with a feasible step direction. To measure how much influence such a vector \\(y\\) can exert when movement is restricted by a norm constraint, we define the dual norm:</p> \\[ \\|y\\|_* = \\sup_{\\|x\\| \\le 1} \\langle y, x \\rangle. \\] <p>This definition asks:  </p> <p>If movement is only allowed within the unit ball of the original norm, what is the maximum directional effect that \\(y\\) can generate?</p>"},{"location":"0a%203norms/#intuition-movement-vs-influence","title":"Intuition \u2014 movement vs. influence","text":"<p>In constrained optimization, the primal norm defines where you are allowed to move, and the dual norm measures how effectively the gradient can move you within that region.</p> <ul> <li>The primal norm determines the shape of the feasible directions, forming a mobility region (for example, an \\(\\ell_2\\) ball is round and smooth, an \\(\\ell_1\\) ball is sharp and cornered).  </li> <li>The dual norm tells how much progress a gradient can make when pushing against that region.  </li> <li>If the gradient aligns with a flat face or a corner of the feasible region, movement becomes limited (as in \\(\\ell_1\\) geometry, leading to sparse solutions).  </li> <li>If the feasible region is smooth (as in \\(\\ell_2\\) geometry), the gradient can always push effectively, producing smooth updates.</li> </ul>"},{"location":"0a%203norms/#example-maximum-decrease-under-a-norm-constraint","title":"Example \u2014 maximum decrease under a norm constraint","text":"<p>Consider the constrained problem:</p> \\[ \\min_x f(x) \\quad \\text{subject to} \\quad \\|x\\| \\le 1. \\] <p>For a small step \\(s\\), the change in \\(f\\) is approximately</p> \\[ f(x+s) \\approx f(x) + \\langle \\nabla f(x), s \\rangle. \\] <p>To decrease \\(f\\), we want to minimize \\(\\langle \\nabla f(x), s \\rangle\\) over feasible steps \\(\\|s\\| \\le 1\\), or equivalently:</p> \\[ \\max_{\\|s\\| \\le 1} \\langle -\\nabla f(x), s \\rangle. \\] <p>By definition of the dual norm, this maximum is exactly</p> \\[ \\max_{\\|s\\| \\le 1} \\langle -\\nabla f(x), s \\rangle = \\|\\nabla f(x)\\|_*. \\] <p>Intuition:</p> <ul> <li>The primal norm defines the feasible region of allowed steps.  </li> <li>The dual norm** measures the largest possible influence the gradient can exert within that region.  </li> <li>If the unit ball is round (smooth), the gradient can push efficiently in any direction; if it has corners (as in \\(\\ell_1\\)), the gradient\u2019s effective action is concentrated along certain axes.  </li> </ul> <p>Thus, the dual norm is the true measure of how powerful a gradient or dual variable is under a constraint, not just its raw magnitude. It appears naturally in optimality conditions, Lagrangian duality, and subgradient methods, providing a precise bound on the effect of forces inside the feasible set.</p>"},{"location":"0a%203norms/#metric-properties-in-optimization","title":"Metric Properties in Optimization","text":"<p>Metrics derived from norms allow analysis of convergence rates. For an \\(L\\)-smooth function \\(f\\), the update \\(x_{k+1} = x_k - \\alpha \\nabla f(x_k)\\) satisfies</p> \\[ \\|x_{k+1} - x^*\\| \\le \\|x_k - x^*\\| - \\alpha (1 - \\frac{L \\alpha}{2}) \\|\\nabla f(x_k)\\|^2. \\] <p>This shows the choice of norm directly affects step size rules, stopping criteria, and algorithmic stability.</p> <p>Unit-ball shapes also influence proximal operators. For a regularizer \\(R(x) = \\lambda \\|x\\|_1\\), the proximal step shrinks components along the axes, exploiting the corners of the \\(\\ell_1\\) unit ball to enforce sparsity.</p>"},{"location":"0a%203norms/#norms-on-function-spaces-l_p-norms","title":"Norms on Function Spaces: \\(L_p\\) Norms","text":"<p>Function spaces generalise vector norms. For functions defined on an interval \\([a,b]\\), the \\(L_p\\) norm is  for \\(1 \\le p &lt; \\infty\\), and  </p> <p>Interpretations:</p> <ul> <li>\\(L_1\\) measures total absolute discrepancy, the shaded area between graphs:      Small, widespread differences accumulate; a narrow spike of small width contributes little to \\(L_1\\).</li> <li>\\(L_2\\) is the RMS or energy of the error:      Squaring amplifies large errors; a spike of height \\(A\\) and width \\(\\varepsilon\\) contributes about \\(A^2 \\varepsilon\\) to the squared norm.</li> <li>\\(L_\\infty\\) is the worst-case deviation:      A single pointwise deviation, no matter how narrow, can dominate the norm. Use \\(L_\\infty\\) in robust optimisation and adversarial settings.</li> </ul>"},{"location":"0a%204operators/","title":"Linear Operators","text":""},{"location":"0a%204operators/#linear-operators-and-operator-norms","title":"Linear Operators and Operator Norms","text":"<p>Linear operators and their norms quantify how matrices or linear maps amplify vectors. Understanding these concepts is crucial for step size selection, conditioning, low-rank approximations, and stability of optimization algorithms. This chapter introduces operator norms, special cases, and the singular value decomposition, highlighting their role in convex optimization and machine learning.</p>"},{"location":"0a%204operators/#operator-norms","title":"Operator Norms","text":"<p>Let \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear operator (matrix). The operator norm induced by vector norms \\(\\|\\cdot\\|_p\\) and \\(\\|\\cdot\\|_q\\) is defined as</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q. \\] <p>Intuition: \\(\\|A\\|_{p \\to q}\\) measures the maximum amplification factor of a vector under \\(A\\) from \\(\\ell_p\\) space to \\(\\ell_q\\) space.</p> <p>Special cases:</p> <ul> <li>\\(\\|A\\|_{1 \\to 1} = \\max_{j} \\sum_{i} |A_{ij}|\\) (maximum absolute column sum)  </li> <li>\\(\\|A\\|_{\\infty \\to \\infty} = \\max_{i} \\sum_{j} |A_{ij}|\\) (maximum absolute row sum)  </li> <li>\\(\\|A\\|_{2 \\to 2} = \\sigma_{\\max}(A)\\), the largest singular value of \\(A\\) </li> </ul>"},{"location":"0a%204operators/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a singular value decomposition:</p> \\[ A = U \\Sigma V^\\top, \\] <p>where:</p> <ul> <li>\\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n}\\) are orthogonal matrices  </li> <li>\\(\\Sigma \\in \\mathbb{R}^{m \\times n}\\) is diagonal with non-negative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0\\), called singular values</li> </ul> <p>Interpretation:</p> <ul> <li>\\(V^\\top\\) rotates coordinates in the input space  </li> <li>\\(\\Sigma\\) scales each coordinate along its principal direction  </li> <li>\\(U\\) rotates coordinates in the output space</li> </ul> <p>Geometric intuition: The action of \\(A\\) on the unit sphere in \\(\\mathbb{R}^n\\) produces an ellipsoid in \\(\\mathbb{R}^m\\), with axes given by singular vectors and lengths given by singular values.</p>"},{"location":"0a%204operators/#applications-in-optimization-and-machine-learning","title":"Applications in Optimization and Machine Learning","text":"<ul> <li>Conditioning: The ratio \\(\\kappa(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)\\) determines sensitivity of linear systems \\(Ax = b\\) and least-squares problems to perturbations. Poor conditioning slows convergence.  </li> <li>Low-rank approximations: The best rank-\\(k\\) approximation of \\(A\\) (in Frobenius or spectral norm) is obtained by truncating its SVD. This is widely used in PCA and collaborative filtering.  </li> <li>Preconditioning: Linear transformations can be preconditioned using SVD to accelerate gradient-based methods.  </li> <li>Step amplification: For gradient descent on quadratic objectives \\(f(x) = \\frac{1}{2}x^\\top A x - b^\\top x\\), the largest singular value \\(\\sigma_{\\max}(A)\\) determines the safe step size \\(\\alpha \\le 2 / \\sigma_{\\max}(A)^2\\).</li> </ul>"},{"location":"0a%204operators/#numerical-considerations","title":"Numerical Considerations","text":"<ul> <li>Computing full SVD is expensive for large matrices; truncated SVD or randomized SVD is preferred in high dimensions.  </li> <li>Operator norms help identify directions of largest amplification and potential instability.  </li> <li>Column scaling or whitening of matrices improves conditioning and convergence of iterative optimization algorithms.</li> </ul>"},{"location":"0a%204operators/#summary-and-optimization-connections","title":"Summary and Optimization Connections","text":"<ul> <li>Operator norms quantify maximum amplification of vectors under linear transformations.  </li> <li>Singular values provide geometric insight into the shape of linear maps and determine condition numbers.  </li> <li>These tools are essential for step size selection, preconditioning, and low-rank modeling in convex optimization and ML.</li> </ul>"},{"location":"0a%205eigenvalues/","title":"Eigenvalues and Eigenvectors","text":""},{"location":"0a%205eigenvalues/#eigenvalues-symmetric-matrices-and-psdpd-matrices","title":"Eigenvalues, Symmetric Matrices, and PSD/PD Matrices","text":"<p>Eigenvalues and positive semidefinite/definite matrices play a central role in convex optimization. They provide insight into curvature, conditioning, step scaling, and uniqueness of solutions. This chapter develops the theory of eigenpairs, diagonalization of symmetric matrices, and properties of PSD/PD matrices, with direct applications to optimization algorithms and machine learning.</p>"},{"location":"0a%205eigenvalues/#eigenpairs-and-diagonalization-of-symmetric-matrices","title":"Eigenpairs and Diagonalization of Symmetric Matrices","text":"<p>Let \\(A \\in \\mathbb{R}^{n \\times n}\\). A scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue of \\(A\\) if there exists a nonzero vector \\(v \\in \\mathbb{R}^n\\) such that</p> \\[ A v = \\lambda v. \\] <p>The vector \\(v\\) is called an eigenvector associated with \\(\\lambda\\).</p> <p>For symmetric matrices \\(A = A^\\top\\), several important properties hold:</p> <ul> <li>All eigenvalues are real: \\(\\lambda_i \\in \\mathbb{R}\\) </li> <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal  </li> <li>\\(A\\) can be diagonalized by an orthogonal matrix \\(Q\\):  </li> </ul> \\[ A = Q \\Lambda Q^\\top, \\] <p>where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) and \\(Q^\\top Q = I\\).</p> <p>Geometric intuition: Symmetric matrices act as stretching or compressing along orthogonal directions, where the eigenvectors indicate the principal directions and eigenvalues the scaling factors.</p> <p>In optimization, the eigenvalues of the Hessian \\(\\nabla^2 f(x)\\) determine curvature along different directions. Positive eigenvalues indicate convexity along that direction, while negative eigenvalues indicate concavity.</p>"},{"location":"0a%205eigenvalues/#positive-semidefinite-and-positive-definite-matrices","title":"Positive Semidefinite and Positive Definite Matrices","text":"<p>A symmetric matrix \\(A\\) is positive semidefinite (PSD) if</p> \\[ x^\\top A x \\ge 0 \\quad \\forall x \\in \\mathbb{R}^n, \\] <p>and positive definite (PD) if</p> \\[ x^\\top A x &gt; 0 \\quad \\forall x \\neq 0. \\] <p>Equivalently:</p> <ul> <li>PSD: all eigenvalues \\(\\lambda_i \\ge 0\\) </li> <li>PD: all eigenvalues \\(\\lambda_i &gt; 0\\)</li> </ul> <p>Properties:</p> <ul> <li>\\(A \\succeq 0 \\implies\\) quadratic form \\(x^\\top A x\\) is convex  </li> <li>\\(A \\succ 0 \\implies\\) quadratic form is strictly convex with a unique minimizer</li> </ul> <p>In convex optimization, PD Hessians guarantee unique minimizers and enable Newton-type methods with reliable step scaling. PSD matrices appear in quadratic programming and covariance matrices in statistics and machine learning.</p>"},{"location":"0a%205eigenvalues/#spectral-connections-to-optimization","title":"Spectral Connections to Optimization","text":"<ol> <li>Step size selection: For gradient descent on \\(f(x) = \\frac{1}{2} x^\\top A x - b^\\top x\\), the largest eigenvalue \\(\\lambda_{\\max}(A)\\) determines the maximum safe step size \\(\\alpha \\le 2/\\lambda_{\\max}(A)\\).  </li> <li>Conditioning: The condition number \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\) controls the convergence rate of gradient descent and other iterative methods.  </li> <li>Curvature analysis: Eigenvectors of the Hessian indicate directions of fastest or slowest curvature, guiding preconditioning and variable rescaling.  </li> <li>Low-rank approximations: PSD matrices can be truncated along small eigenvalues to reduce dimensionality in ML applications such as PCA.</li> </ol>"},{"location":"0a%206projections/","title":"Projections","text":""},{"location":"0a%206projections/#projections-and-orthogonal-decompositions","title":"Projections and Orthogonal Decompositions","text":"<p>Projections and orthogonal decompositions are fundamental tools in convex optimization. They allow us to enforce constraints, compute optimality conditions, and analyze the geometry of feasible regions. This chapter develops projections onto subspaces and convex sets, orthogonal decomposition, and their connections to algorithms.</p>"},{"location":"0a%206projections/#projection-onto-subspaces","title":"Projection onto Subspaces","text":"<p>Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace with an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\). The projection of \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is</p> \\[ P_W(x) = \\sum_{i=1}^k \\langle x, q_i \\rangle q_i. \\] <p>Properties:</p> <ul> <li>\\(x - P_W(x)\\) is orthogonal to \\(W\\): \\(\\langle x - P_W(x), y \\rangle = 0\\) for all \\(y \\in W\\) </li> <li>\\(P_W\\) is linear and idempotent: \\(P_W(P_W(x)) = P_W(x)\\) </li> <li>\\(P_W(x)\\) minimizes the distance to \\(W\\): \\(\\|x - P_W(x)\\| = \\min_{y \\in W} \\|x - y\\|\\)</li> </ul> <p>Geometric intuition: \\(P_W(x)\\) is the \u201cshadow\u201d of \\(x\\) onto the subspace \\(W\\). In optimization, projections are used in projected gradient descent, where iterates are kept within a feasible linear subspace.</p>"},{"location":"0a%206projections/#projection-onto-convex-sets","title":"Projection onto Convex Sets","text":"<p>For a closed convex set \\(C \\subseteq \\mathbb{R}^n\\), the metric projection of \\(x\\) onto \\(C\\) is</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|. \\] <p>Properties:</p> <ul> <li>Uniqueness: There exists a unique minimizer \\(P_C(x)\\) </li> <li>Firm nonexpansiveness:</li> </ul> \\[ \\|P_C(x) - P_C(y)\\|^2 \\le \\langle P_C(x) - P_C(y), x - y \\rangle \\quad \\forall x, y \\in \\mathbb{R}^n \\] <ul> <li>First-order optimality: \\(\\langle x - P_C(x), y - P_C(x) \\rangle \\le 0\\) for all \\(y \\in C\\)</li> </ul> <p>Applications in optimization:</p> <ul> <li>Ensures iterates remain feasible in projected gradient methods  </li> <li>Guarantees stability and convergence due to nonexpansiveness  </li> <li>Appears in proximal operators when \\(C\\) is a level set of a regularizer</li> </ul> <p>A useful corollary is nonexpansiveness:  </p>"},{"location":"0a%206projections/#orthogonal-decomposition","title":"Orthogonal Decomposition","text":"<p>Any vector \\(x \\in \\mathbb{R}^n\\) can be uniquely decomposed into components along a subspace \\(W\\) and its orthogonal complement \\(W^\\perp\\):</p> \\[ x = P_W(x) + (x - P_W(x)), \\quad P_W(x) \\in W, \\quad x - P_W(x) \\in W^\\perp \\] <p>Properties:</p> <ul> <li>The decomposition is unique  </li> <li>\\(\\|x\\|^2 = \\|P_W(x)\\|^2 + \\|x - P_W(x)\\|^2\\) (Pythagoras\u2019 theorem)  </li> </ul> <p>Applications:</p> <ul> <li>In constrained optimization, the gradient can be decomposed into components tangent to constraints and normal to them, forming the basis for KKT conditions.  </li> <li>In machine learning, projections onto feature subspaces allow dimensionality reduction and orthogonalization of data.</li> </ul>"},{"location":"0a%207calculus/","title":"Calculus Review","text":""},{"location":"0a%207calculus/#calculus-essentials-gradients-hessians-and-taylor-expansions","title":"Calculus Essentials: Gradients, Hessians, and Taylor Expansions","text":"<p>Calculus provides the foundation for optimization algorithms. Gradients indicate directions of steepest ascent or descent, Hessians encode curvature, and Taylor expansions give local approximations of functions. This chapter develops these concepts with an eye toward convex optimization and machine learning applications.</p>"},{"location":"0a%207calculus/#gradient-and-directional-derivative","title":"Gradient and Directional Derivative","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be differentiable. The gradient of \\(f\\) at \\(x\\) is the vector</p> \\[ \\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}. \\] <p>Properties:</p> <ul> <li>Linear approximation: \\(f(x + h) \\approx f(x) + \\langle \\nabla f(x), h \\rangle\\) for small \\(h\\) </li> <li>Steepest ascent direction: \\(\\nabla f(x)\\) </li> <li>Steepest descent direction: \\(-\\nabla f(x)\\)</li> </ul> <p>The directional derivative of \\(f\\) at \\(x\\) in direction \\(u\\) is</p> \\[ D_u f(x) = \\lim_{t \\to 0} \\frac{f(x + t u) - f(x)}{t} = \\langle \\nabla f(x), u \\rangle. \\] <p>Optimization application: Gradient descent updates are</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k), \\] <p>where \\(\\alpha\\) is a step size, moving in the direction of steepest decrease.</p>"},{"location":"0a%207calculus/#hessian-and-second-order-directional-derivatives","title":"Hessian and Second-Order Directional Derivatives","text":"<p>The Hessian of \\(f\\) at \\(x\\) is the symmetric matrix of second partial derivatives:</p> \\[ \\nabla^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}. \\] <p>Properties:</p> <ul> <li>Symmetric: \\(\\nabla^2 f(x) = (\\nabla^2 f(x))^\\top\\) </li> <li>Quadratic form: \\(u^\\top \\nabla^2 f(x) u\\) measures curvature along \\(u\\) </li> <li>Positive semidefinite Hessian \\(\\implies\\) local convexity</li> </ul> <p>Optimization application: Newton\u2019s method updates</p> \\[ x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k) \\] <p>use curvature information to accelerate convergence, especially near minimizers.</p>"},{"location":"0a%207calculus/#taylor-expansions","title":"Taylor Expansions","text":"<p>The second-order Taylor expansion of \\(f\\) around \\(x\\) is</p> \\[ f(x + h) \\approx f(x) + \\langle \\nabla f(x), h \\rangle + \\frac{1}{2} h^\\top \\nabla^2 f(x) h. \\] <p>Properties:</p> <ul> <li>Linear term captures slope (first-order approximation)  </li> <li>Quadratic term captures curvature  </li> <li>Provides local quadratic models used in Newton and quasi-Newton methods.  </li> </ul>"},{"location":"0a%207calculus/#optimization-connections","title":"Optimization Connections","text":"<ul> <li>Gradients determine update directions in first-order methods  </li> <li>Hessians determine curvature, step scaling, and Newton updates.  </li> <li>Taylor expansions provide local approximations, guiding line search and trust-region methods.  </li> <li>Eigenvalues of the Hessian determine convexity, step sizes, and conditioning.</li> </ul>"},{"location":"0a%208convexity/","title":"Smoothness and Convexity","text":""},{"location":"0a%208convexity/#smoothness-and-strong-convexity","title":"Smoothness and Strong Convexity","text":"<p>Smoothness and strong convexity are fundamental concepts in optimization that describe the behavior of gradients and curvature of functions. They determine step sizes, convergence rates, and preconditioning strategies in both first-order and second-order methods. This chapter develops these concepts with explicit links to convex optimization algorithms.</p>"},{"location":"0a%208convexity/#lipschitz-continuity-and-smoothness","title":"Lipschitz Continuity and Smoothness","text":"<p>A differentiable function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is \\(L\\)-smooth (or has Lipschitz continuous gradient) if there exists \\(L &gt; 0\\) such that</p> \\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\| \\quad \\forall x, y \\in \\mathbb{R}^n. \\] <p>Equivalent quadratic upper bound:</p> \\[ f(y) \\le f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2} \\|y - x\\|^2. \\] <p>Interpretation:</p> <ul> <li>\\(L\\) controls how rapidly the gradient can change  </li> <li>Larger \\(L\\) implies steep curvature and smaller safe step sizes</li> </ul> <p>Optimization application: For gradient descent, the step size \\(\\alpha\\) must satisfy \\(\\alpha \\le 1/L\\) to guarantee decrease of \\(f\\).</p> <p>Example: \\(f(x) = \\frac{1}{2} x^\\top A x\\) with \\(A \\succeq 0\\) has \\(L = \\lambda_{\\max}(A)\\).</p>"},{"location":"0a%208convexity/#strong-convexity","title":"Strong Convexity","text":"<p>A differentiable function \\(f\\) is \\(\\mu\\)-strongly convex if there exists \\(\\mu &gt; 0\\) such that</p> \\[ f(y) \\ge f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{\\mu}{2} \\|y - x\\|^2 \\quad \\forall x, y \\in \\mathbb{R}^n. \\] <p>Equivalent condition: The Hessian satisfies \\(\\nabla^2 f(x) \\succeq \\mu I\\) for all \\(x\\).</p> <p>Interpretation:</p> <ul> <li>\\(\\mu\\) provides a lower bound on curvature  </li> <li>Strong convexity ensures a unique minimizer  </li> <li>Condition number: \\(\\kappa = L/\\mu\\) measures problem conditioning</li> </ul> <p>Optimization application: Gradient descent on a \\(\\mu\\)-strongly convex, \\(L\\)-smooth function satisfies linear convergence:</p> \\[ \\|x_k - x^*\\| \\le \\left(1 - \\frac{\\mu}{L}\\right)^k \\|x_0 - x^*\\|. \\] <p>Example: \\(f(x) = x_1^2 + 2x_2^2\\) is \\(1\\)-strongly convex along \\(x_1\\) and \\(2\\)-strongly convex along \\(x_2\\), with \\(L = 2\\) and \\(\\kappa = 2\\).</p>"},{"location":"0a%208convexity/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>Smoothness bounds how steeply gradients can change; it controls the \u201cflatness\u201d of the function landscape  </li> <li>Strong convexity** ensures the function curves upward everywhere, providing a \u201cbowl shape\u201d that guarantees a unique minimizer  </li> <li>The ratio \\(L/\\mu\\) (condition number) indicates how stretched the bowl is; larger \\(\\kappa\\) implies elongated level sets and slower gradient descent convergence</li> </ul>"},{"location":"0a%208convexity/#connections-to-optimization-algorithms","title":"Connections to Optimization Algorithms","text":"<ul> <li>Step-size selection: \\(\\alpha \\le 1/L\\) ensures monotone decrease of \\(f\\) </li> <li>Convergence rates: linear convergence in strongly convex functions, sublinear in general convex functions  </li> <li>Preconditioning: transforming variables to reduce \\(\\kappa = L/\\mu\\) accelerates convergence  </li> <li>Applicable to gradient descent, accelerated gradient methods (Nesterov), and Newton-type methods</li> </ul>"},{"location":"0a%209Convex%20Sets/","title":"Convex Sets and Related Concepts","text":""},{"location":"0a%209Convex%20Sets/#convex-sets-and-related-concepts","title":"Convex Sets and Related Concepts","text":"<p>Understanding convex sets and their associated geometric structures is central to convex optimization. These objects define feasible regions, constraints, and the geometry that algorithms navigate. In machine learning, they appear everywhere: in feasible weight sets, regularizers, polyhedral constraints, and in dual formulations.</p>"},{"location":"0a%209Convex%20Sets/#convex-set","title":"Convex Set","text":"<p>A set \\(C \\subseteq \\mathbb{R}^n\\) is convex if, for any two points \\(x_1, x_2 \\in C\\), the line segment joining them lies entirely within the set:</p> \\[ \\theta x_1 + (1-\\theta) x_2 \\in C, \\quad \\forall \\theta \\in [0,1]. \\] <ul> <li>Closed sets: A set is closed if it contains all its limit points. Closed sets ensure that sequences of feasible points do not \u201cescape\u201d the set, which is important for guarantees on convergence in optimization.  </li> <li>Extreme points: An extreme point of a convex set cannot be written as a convex combination of other points. For polyhedra, these correspond to vertices, and in optimization, optimal solutions of linear programs lie at extreme points.</li> </ul>"},{"location":"0a%209Convex%20Sets/#convex-combination","title":"Convex Combination","text":"<p>A convex combination of points \\(x_1, \\dots, x_k\\) is:</p> \\[ x = \\sum_{i=1}^k \\theta_i x_i, \\quad \\theta_i \\ge 0, \\quad \\sum_{i=1}^k \\theta_i = 1. \\] <ul> <li>Think of it as a weighted average where weights sum to 1.  </li> <li>Convex combinations never leave the convex hull, which is the \u201csafe zone\u201d defined by your points.</li> </ul> <p>ML Intuition: Gradient-based updates can be seen as small convex combinations between the current point and a target direction.</p>"},{"location":"0a%209Convex%20Sets/#convex-hull","title":"Convex Hull","text":"<p>The convex hull of a set \\(S\\) is the set of all convex combinations of points in \\(S\\). It is the smallest convex set containing \\(S\\).  </p> <ul> <li>Geometric intuition: Imagine stretching a rubber band around the points \u2014 the shape it forms is the convex hull.  </li> <li>ML Relevance: Convex hulls appear in support vector machines (margin between classes) and in approximating non-convex feasible sets using convex relaxations.</li> </ul>"},{"location":"0a%209Convex%20Sets/#cones","title":"Cones","text":"<p>A cone is a set \\(K \\subseteq \\mathbb{R}^n\\) such that if \\(x \\in K\\) and \\(\\alpha \\ge 0\\), then \\(\\alpha x \\in K\\).  </p> <ul> <li>Cones are closed under nonnegative scaling, but not necessarily addition.  </li> <li>Conic hull (convex cone): Collection of all conic combinations of points in \\(S\\):  </li> </ul> \\[ \\text{cone}(S) = \\Big\\{ \\sum_{i=1}^k \\theta_i x_i \\;\\Big|\\; x_i \\in S, \\; \\theta_i \\ge 0 \\Big\\}. \\] <ul> <li>A cone is not necessarily a subspace (negative multiples may not be included).  </li> <li>A convex cone is closed under addition and nonnegative scaling.  </li> </ul>"},{"location":"0a%209Convex%20Sets/#polar-cones","title":"Polar Cones","text":"<p>Given a cone \\(K \\subseteq \\mathbb{R}^n\\), the polar cone is:</p> \\[ K^\\circ = \\{ y \\in \\mathbb{R}^n \\mid \\langle y, x \\rangle \\le 0, \\; \\forall x \\in K \\}. \\] <ul> <li>Intuition: polar cone vectors form non-acute angles with every vector in \\(K\\).  </li> <li>Properties:  <ul> <li>Always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, \\(K^\\circ\\) is the orthogonal complement.  </li> <li>Duality: \\((K^\\circ)^\\circ = K\\) for closed convex cones.  </li> </ul> </li> </ul> <p>ML relevance: Polar cones naturally appear in dual problems and in the derivation of optimality conditions.</p>"},{"location":"0a%209Convex%20Sets/#tangent-cone","title":"Tangent Cone","text":"<p>For a set \\(C\\) and point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) contains all directions in which one can \u201cmove infinitesimally\u201d while remaining in \\(C\\):</p> \\[ T_C(x) = \\Big\\{ d \\in \\mathbb{R}^n \\;\\Big|\\; \\exists t_k \\downarrow 0, \\; x_k \\in C, \\; x_k \\to x, \\; \\frac{x_k - x}{t_k} \\to d \\Big\\}. \\] <ul> <li>Interior point: \\(T_C(x) = \\mathbb{R}^n\\).  </li> <li>Boundary point: \\(T_C(x)\\) restricts movement to directions staying inside \\(C\\).  </li> </ul> <p>ML intuition: Tangent cones define feasible directions for projected gradient steps or constrained optimization.</p>"},{"location":"0a%209Convex%20Sets/#normal-cone","title":"Normal Cone","text":"<p>For a convex set \\(C\\) at point \\(x \\in C\\):</p> \\[ N_C(x) = \\{ v \\in \\mathbb{R}^n \\mid \\langle v, y - x \\rangle \\le 0, \\; \\forall y \\in C \\}. \\] <ul> <li>Each \\(v \\in N_C(x)\\) defines a supporting hyperplane at \\(x\\).  </li> <li>Relation: \\(N_C(x) = \\big(T_C(x)\\big)^\\circ\\) \u2014 polar of tangent cone.  </li> <li>Interior point: \\(N_C(x) = \\{0\\}\\).  </li> <li>Boundary/corner: \\(N_C(x)\\) is a cone of outward normals.  </li> </ul> <p>ML relevance: Appears in first-order optimality conditions:</p> \\[ 0 \\in \\partial f(x^*) + N_C(x^*), \\] <p>where the subgradient of \\(f\\) is balanced by the \u201cpush-back\u201d of constraints.</p>"},{"location":"0a%209Convex%20Sets/#comparison-of-tangent-normal-and-polar-cones","title":"Comparison of Tangent, Normal, and Polar Cones","text":"Cone Applies To Meaning Interior Boundary Key Facts Tangent \\(T_C(x)\\) Any set Feasible directions \\(\\mathbb{R}^n\\) Restricted Local geometry Normal \\(N_C(x)\\) Convex sets Outward blocking directions \\(\\{0\\}\\) Outward cone Closed, convex; \\(N_C = T^\\circ\\) Polar \\(K^\\circ\\) Any cone Non-acute directions N/A N/A Closed, convex; \\((K^\\circ)^\\circ = K\\)"},{"location":"0a%209Convex%20Sets/#hyperplanes-and-half-spaces","title":"Hyperplanes and Half-Spaces","text":"<ul> <li>Hyperplane: \\(a^T x = b\\).  </li> <li>Half-space: One side of a hyperplane, \\(a^T x \\le b\\) or \\(a^T x \\ge b\\).  </li> <li>Convex sets can be built from intersections of half-spaces, which is why linear constraints are convex.  </li> </ul> <p>Separation &amp; Supporting Hyperplanes:</p> <ul> <li>Separating hyperplane theorem: Two disjoint convex sets can be separated by a hyperplane.  </li> <li>Supporting hyperplane: Touches a convex set at a point (or face) without cutting through.  </li> </ul>"},{"location":"0b%201Separation%20Theorems/","title":"Separation Theorems","text":""},{"location":"0b%201Separation%20Theorems/#separation-theorems","title":"Separation Theorems","text":"<p>Separation theorems are a cornerstone of convex analysis. They formalize the intuitive idea that two convex sets that do not overlap can be \u201cseparated\u201d by a hyperplane.</p> <p>Why this matters in optimization:</p> <ul> <li>They provide the geometric foundation for duality, showing how constraints can be represented as linear functionals.  </li> <li>Separation arguments are used in proving optimality conditions, subgradient existence, and support function properties.  </li> <li>In practical algorithms, they justify cutting-plane methods and projection-based methods.</li> </ul> <p>Intuitively: imagine two convex blobs in space. If they don\u2019t intersect, you can always draw a flat sheet (hyperplane) between them without touching either. This hyperplane captures the direction along which one set is \u201clarger\u201d or \u201csmaller\u201d than the other.</p>"},{"location":"0b%201Separation%20Theorems/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":""},{"location":"0b%201Separation%20Theorems/#convex-sets","title":"Convex Sets","text":"<p>A set \\(C \\subseteq \\mathbb{R}^n\\) is convex if for all \\(x, y \\in C\\) and \\(\\theta \\in [0,1]\\):</p> \\[ \\theta x + (1-\\theta) y \\in C. \\]"},{"location":"0b%201Separation%20Theorems/#hyperplane-separation","title":"Hyperplane Separation","text":"<p>A hyperplane is defined by a nonzero vector \\(a \\in \\mathbb{R}^n\\) and scalar \\(b \\in \\mathbb{R}\\):</p> \\[ H = \\{x \\in \\mathbb{R}^n : a^\\top x = b \\}. \\] <p>It divides space into two half-spaces:  </p> \\[ H^+ = \\{x : a^\\top x \\ge b\\}, \\quad H^- = \\{x : a^\\top x \\le b\\}. \\]"},{"location":"0b%201Separation%20Theorems/#separation-theorems_1","title":"Separation Theorems","text":"<ul> <li>Basic Separation Theorem:   If \\(C\\) is a convex set and \\(x_0 \\notin C\\), there exists \\(a \\neq 0\\) and \\(b \\in \\mathbb{R}\\) such that  </li> </ul> \\[ a^\\top x_0 &gt; b \\quad \\text{and} \\quad a^\\top x \\le b \\quad \\forall x \\in C. \\] <ul> <li>Strong Separation (for disjoint convex sets):   If \\(C, D \\subset \\mathbb{R}^n\\) are convex and disjoint, then there exists \\(a \\neq 0\\) and \\(b\\) such that  </li> </ul> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\quad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <ul> <li>Strict Separation:   If \\(C\\) and \\(D\\) are convex, disjoint, and at least one is open, there exists a hyperplane such that  </li> </ul> \\[ a^\\top x &lt; b \\quad \\forall x \\in C, \\quad a^\\top y &gt; b \\quad \\forall y \\in D. \\]"},{"location":"0b%201Separation%20Theorems/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<p>To identify a separating hyperplane:</p> <ol> <li>Verify that the sets \\(C\\) and \\(D\\) are convex.  </li> <li>Check that \\(C \\cap D = \\emptyset\\) (or \\(x_0 \\notin C\\) for point separation).  </li> <li>Solve for \\(a, b\\) (analytically or via convex optimization):  <ul> <li>Often, the hyperplane can be found using the closest-point method: minimize \\(\\|x - y\\|^2\\) subject to \\(x \\in C, y \\in D\\).  </li> <li>The vector \\(a = y - x\\) between closest points defines the separating hyperplane.  </li> </ul> </li> </ol>"},{"location":"0b%201Separation%20Theorems/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Duality Theory: Separation theorems underpin Farkas\u2019 lemma, a foundation for linear programming duality.  </li> <li>Subgradient Existence: Every convex function can be locally approximated by a supporting hyperplane, directly derived from separation principles.  </li> <li>Optimization Algorithms:  </li> <li>Cutting-plane methods rely on separating hyperplanes to iteratively shrink the feasible region.  </li> <li>Projection methods use hyperplanes to maintain feasibility in constrained updates.</li> </ul>"},{"location":"0b%201Separation%20Theorems/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Any point outside a convex set can be separated by a hyperplane.  </li> <li>Disjoint convex sets can always be separated; strict separation occurs if one is open.  </li> <li>The vector defining the hyperplane often comes from the closest-point argument.  </li> <li>Separation theorems are geometric foundations for duality, subgradients, and cutting-plane algorithms.</li> </ul>"},{"location":"0b%202Support%20Functions/","title":"Support Functions","text":""},{"location":"0b%202Support%20Functions/#support-functions","title":"Support Functions","text":"<p>Support functions are a fundamental tool in convex analysis that capture the \u201cextent\u201d of a convex set in a given direction. They are widely used to:</p> <ul> <li>Represent convex sets in terms of linear functionals.  </li> <li>Compute distances, dual norms, and subgradients.  </li> <li>Facilitate duality theory, optimization algorithms, and geometric reasoning in high-dimensional spaces.</li> </ul>"},{"location":"0b%202Support%20Functions/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty convex set. The support function\\(\\sigma_C: \\mathbb{R}^n \\to \\mathbb{R}\\) is defined as:</p> \\[ \\sigma_C(y) = \\sup_{x \\in C} \\langle y, x \\rangle \\] <ul> <li>\\(y\\) is the direction vector.  </li> <li>\\(\\langle y, x \\rangle\\) is the standard inner product in \\(\\mathbb{R}^n\\).  </li> <li>\\(\\sigma_C(y)\\) measures the maximum projection of the set \\(C\\) along direction \\(y\\).</li> </ul>"},{"location":"0b%202Support%20Functions/#key-properties","title":"Key Properties","text":"<ul> <li>\\(\\sigma_C\\) is positively homogeneous: \\(\\sigma_C(\\alpha y) = \\alpha \\sigma_C(y)\\) for \\(\\alpha \\ge 0\\).  </li> <li>\\(\\sigma_C\\) is subadditive (convex): \\(\\sigma_C(y_1 + y_2) \\le \\sigma_C(y_1) + \\sigma_C(y_2)\\).  </li> <li>If \\(C\\) is compact, the supremum is attained: there exists \\(x^* \\in C\\) such that \\(\\sigma_C(y) = \\langle y, x^* \\rangle\\).  </li> <li>The support function uniquely characterizes a closed convex set: \\(C = \\{ x : \\langle y, x \\rangle \\le \\sigma_C(y) \\; \\forall y \\in \\mathbb{R}^n \\}\\).</li> </ul>"},{"location":"0b%202Support%20Functions/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<p>To compute or apply a support function:</p> <ol> <li>Identify the convex set \\(C\\).  </li> <li>Choose a direction vector \\(y\\).  </li> <li>Solve the linear optimization problem: </li> <li>Use properties:  </li> <li>For norm balls, \\(\\sigma_{B}(y)\\) equals the dual norm \\(\\|y\\|_*\\).  </li> <li>For polytopes, the supremum occurs at one of the vertices.  </li> <li>Apply in algorithms: support functions often appear in dual formulations, subgradient computations, and projection-based methods.</li> </ol>"},{"location":"0b%202Support%20Functions/#examples","title":"Examples","text":""},{"location":"0b%202Support%20Functions/#example-1-unit-ell_2-ball","title":"Example 1: Unit \\(\\ell_2\\) Ball","text":"<p>Let \\(C = \\{x \\in \\mathbb{R}^n : \\|x\\|_2 \\le 1\\}\\). Then</p> \\[ \\sigma_C(y) = \\sup_{\\|x\\|_2 \\le 1} \\langle y, x \\rangle = \\|y\\|_2 \\] <ul> <li>Intuition: the farthest point along \\(y\\) on the unit ball is in the direction of \\(y\\).  </li> </ul>"},{"location":"0b%202Support%20Functions/#example-2-unit-ell_1-ball","title":"Example 2: Unit \\(\\ell_1\\) Ball","text":"<p>Let \\(C = \\{x : \\|x\\|_1 \\le 1\\}\\). Then</p> \\[ \\sigma_C(y) = \\sup_{\\|x\\|_1 \\le 1} \\langle y, x \\rangle = \\|y\\|_\\infty \\] <ul> <li>Intuition: for an \\(\\ell_1\\) ball, the extreme point along a direction is at a vertex, giving the maximum coordinate magnitude.</li> </ul>"},{"location":"0b%202Support%20Functions/#example-3-polytope","title":"Example 3: Polytope","text":"<p>Let \\(C = \\text{conv}\\{v_1, v_2, \\dots, v_m\\}\\), the convex hull of vertices. Then</p> \\[ \\sigma_C(y) = \\max_{i=1,\\dots,m} \\langle y, v_i \\rangle \\] <ul> <li>Intuition: the maximum along a direction occurs at a vertex of the polytope.</li> </ul>"},{"location":"0b%202Support%20Functions/#applications-implications","title":"Applications / Implications","text":"<ul> <li> <p>Duality and Convex Conjugates: The support function is the convex conjugate of the indicator function of the set:  This provides a bridge between sets and functions in dual optimization formulations.</p> </li> <li> <p>Norms and Dual Norms: For any norm ball, the support function gives the dual norm. This is fundamental in constrained optimization and step size analysis.</p> </li> <li> <p>Geometric Algorithms: </p> </li> <li>Computing distances between sets  </li> <li>Generating separating hyperplanes  </li> <li>Cutting-plane and projection algorithms</li> </ul>"},{"location":"0b%202Support%20Functions/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Support functions measure the maximum extent of a convex set along a direction.  </li> <li>They are convex, positively homogeneous, and fully characterize closed convex sets.  </li> <li>They provide a link between primal sets and dual norms, appearing naturally in duality theory and optimization algorithms.  </li> <li>For polytopes or norm balls, support functions are easy to compute, often giving geometric intuition about extreme points and directions.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/","title":"Convex Conjugates","text":""},{"location":"0b%203Convex%20Conjugates/#convex-conjugates","title":"Convex Conjugates","text":"<p>Convex conjugates, also known as Fenchel conjugates, are a fundamental tool in convex analysis and optimization. They provide a dual representation of functions and are central to:</p> <ul> <li>Deriving dual problems in convex optimization  </li> <li>Understanding subgradients and supporting hyperplanes  </li> <li>Designing efficient algorithms such as proximal and primal-dual methods  </li> </ul> <p>Intuition: the convex conjugate of a function \\(f\\) captures the maximum linear overestimate of \\(f\\). It tells us, for any direction \\(y\\), what is the steepest slope that does not underestimate \\(f\\).</p>"},{"location":"0b%203Convex%20Conjugates/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) be a proper convex function. The convex conjugate \\(f^*: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) is defined as:</p> \\[ f^*(y) = \\sup_{x \\in \\mathbb{R}^n} \\left( \\langle y, x \\rangle - f(x) \\right) \\] <ul> <li>\\(y\\) is the dual variable, representing a slope or linear functional.  </li> <li>Geometric intuition: for each \\(y\\), \\(f^*(y)\\) is the height of the tightest linear overestimate of \\(f\\) along \\(y\\).</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#key-properties","title":"Key Properties","text":"<ul> <li>\\(f^*\\) is always convex, even if \\(f\\) is not strictly convex.  </li> <li>The biconjugate \\(f^{**}\\) equals \\(f\\) if \\(f\\) is proper, convex, and lower semicontinuous: </li> <li>Fenchel-Young inequality: For all \\(x, y \\in \\mathbb{R}^n\\):  Equality holds if and only if \\(y \\in \\partial f(x)\\) (subgradient condition).</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex function \\(f(x)\\).  </li> <li>For a given dual vector \\(y\\), compute </li> <li>Use the properties:  </li> <li>For norm functions, conjugates give dual norms.  </li> <li>For indicator functions, conjugates give support functions.  </li> <li>Fenchel conjugates are used to construct dual problems in convex optimization.  </li> <li>Apply in optimization algorithms:  </li> <li>Proximal operators of \\(f^*\\) relate to those of \\(f\\) (Moreau identity)  </li> <li>Subgradients of \\(f^*\\) are linked to primal solutions</li> </ol>"},{"location":"0b%203Convex%20Conjugates/#examples","title":"Examples","text":""},{"location":"0b%203Convex%20Conjugates/#example-1-quadratic-function","title":"Example 1: Quadratic Function","text":"<p>Let \\(f(x) = \\frac{1}{2} \\|x\\|_2^2\\). Then</p> \\[ f^*(y) = \\sup_x \\left( \\langle y, x \\rangle - \\frac{1}{2} \\|x\\|_2^2 \\right) \\] <ul> <li>Differentiate: \\(\\nabla_x (\\langle y, x \\rangle - \\frac{1}{2} \\|x\\|_2^2) = y - x = 0 \\implies x = y\\) </li> <li>Substitute back: </li> </ul> <p>Observation: quadratic is self-conjugate.</p>"},{"location":"0b%203Convex%20Conjugates/#example-2-ell_1-norm","title":"Example 2: \\(\\ell_1\\) Norm","text":"<p>Let \\(f(x) = \\|x\\|_1\\). Then</p> \\[ f^*(y) = \\sup_{\\|x\\|_1 \\le \\infty} \\left( \\langle y, x \\rangle - \\|x\\|_1 \\right) \\] <ul> <li>The supremum is finite only if \\(\\|y\\|_\\infty \\le 1\\), otherwise \\(+\\infty\\).  </li> <li>Therefore:  where \\(\\delta\\) is the indicator function.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#example-3-indicator-function","title":"Example 3: Indicator Function","text":"<p>Let \\(f(x) = \\delta_C(x)\\), the indicator of a convex set \\(C\\). Then</p> \\[ f^*(y) = \\sup_{x \\in C} \\langle y, x \\rangle = \\sigma_C(y) \\] <ul> <li>Observation: support functions are conjugates of indicator functions.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Dual Optimization Problems: Fenchel duals are derived by taking conjugates of primal objectives and constraints.  </li> <li>Subgradient Methods: Fenchel-Young inequality provides a direct link between primal and dual subgradients.  </li> <li>Proximal Algorithms: Proximal operators of conjugates allow efficient updates in primal-dual splitting methods.  </li> <li>Norm Duality: Conjugates of norm functions yield dual norms, connecting directly to constrained optimization theory.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Convex conjugates provide a dual perspective of convex functions, capturing maximal linear overestimates.  </li> <li>Biconjugation recovers the original function if it is convex and lower semicontinuous.  </li> <li>Fenchel-Young inequality links primal and dual variables via subgradients.  </li> <li>Indicator functions and norm functions have conjugates that reveal support functions and dual norms.  </li> <li>Conjugates are foundational in dual optimization, proximal algorithms, and geometric analysis.</li> </ul>"},{"location":"0b%204Subgradients/","title":"Subgradients","text":""},{"location":"0b%204Subgradients/#subgradients","title":"Subgradients","text":"<p>Subgradients generalize the concept of gradients to nonsmooth convex functions, allowing us to perform optimization even when the function is not differentiable. They are crucial for:</p> <ul> <li>Nonsmooth convex optimization (e.g., \\(\\ell_1\\) regularization, hinge loss)  </li> <li>Defining optimality conditions for convex functions  </li> <li>Linking primal and dual formulations via Fenchel conjugates  </li> </ul> <p>Intuition: For a convex function \\(f\\), the subgradient at a point \\(x\\) is the slope of a supporting hyperplane that lies below the graph of \\(f\\). It tells you a direction along which the function does not decrease, even if the gradient does not exist.</p>"},{"location":"0b%204Subgradients/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) be convex. A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if:</p> \\[ f(y) \\ge f(x) + \\langle g, y - x \\rangle \\quad \\forall y \\in \\mathbb{R}^n \\] <p>The subdifferential \\(\\partial f(x)\\) is the set of all subgradients at \\(x\\):</p> \\[ \\partial f(x) = \\{ g \\in \\mathbb{R}^n : f(y) \\ge f(x) + \\langle g, y - x \\rangle, \\forall y \\in \\mathbb{R}^n \\} \\] <p>Key properties:</p> <ul> <li>If \\(f\\) is differentiable at \\(x\\), then \\(\\partial f(x) = \\{\\nabla f(x)\\}\\).  </li> <li>The subdifferential is convex and closed.  </li> <li>\\(0 \\in \\partial f(x)\\) if and only if \\(x\\) is a global minimizer of \\(f\\).  </li> </ul>"},{"location":"0b%204Subgradients/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<p>To compute or apply subgradients:</p> <ol> <li>Identify if the function \\(f\\) is convex.  </li> <li>Check if \\(f\\) is differentiable at the point \\(x\\):  </li> <li>If yes, the gradient is the only subgradient.  </li> <li>If no, identify supporting hyperplanes that satisfy the subgradient inequality.  </li> <li>Use subgradients in algorithms:  </li> <li>Subgradient descent: update \\(x_{k+1} = x_k - \\alpha_k g_k\\) with \\(g_k \\in \\partial f(x_k)\\) </li> <li>Optimality checks: \\(0 \\in \\partial f(x)\\) implies \\(x\\) is optimal  </li> <li>Leverage relationships with conjugates:  </li> <li>\\(g \\in \\partial f(x) \\iff x \\in \\partial f^*(g)\\) (duality link)</li> </ol>"},{"location":"0b%204Subgradients/#examples","title":"Examples","text":""},{"location":"0b%204Subgradients/#example-1-absolute-value","title":"Example 1: Absolute Value","text":"<p>Let \\(f(x) = |x|\\):</p> <ul> <li>For \\(x &gt; 0\\), \\(\\partial f(x) = \\{1\\}\\) </li> <li>For \\(x &lt; 0\\), \\(\\partial f(x) = \\{-1\\}\\) </li> <li>For \\(x = 0\\), \\(\\partial f(0) = [-1, 1]\\) </li> </ul> <p>Intuition: At 0, any slope between -1 and 1 forms a supporting hyperplane.</p>"},{"location":"0b%204Subgradients/#example-2-ell_1-norm","title":"Example 2: \\(\\ell_1\\) Norm","text":"<p>Let \\(f(x) = \\|x\\|_1\\) for \\(x \\in \\mathbb{R}^n\\):</p> \\[ \\partial \\|x\\|_1 = \\{ g \\in \\mathbb{R}^n : g_i = \\text{sign}(x_i) \\text{ if } x_i \\neq 0, \\, g_i \\in [-1,1] \\text{ if } x_i = 0 \\} \\] <ul> <li>Each component can vary in \\([-1,1]\\) if the corresponding \\(x_i = 0\\).  </li> <li>This property promotes sparsity in optimization problems like LASSO.</li> </ul>"},{"location":"0b%204Subgradients/#example-3-indicator-function","title":"Example 3: Indicator Function","text":"<p>Let \\(f(x) = \\delta_C(x)\\), the indicator of a convex set \\(C\\):</p> \\[ \\partial \\delta_C(x) = \\begin{cases} \\{ g : \\langle g, y - x \\rangle \\le 0, \\forall y \\in C \\} &amp; x \\in C \\\\ \\emptyset &amp; x \\notin C \\end{cases} \\] <ul> <li>Interpretation: the subdifferential is the normal cone to the set at \\(x\\).</li> </ul>"},{"location":"0b%204Subgradients/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Nonsmooth Optimization: Subgradient descent generalizes gradient descent to nonsmooth convex functions.  </li> <li>Optimality Conditions: \\(0 \\in \\partial f(x)\\) characterizes global minimizers.  </li> <li>Duality: Subgradients connect primal and dual variables via Fenchel conjugates.  </li> <li>Sparsity and Regularization: Subdifferentials of norms define constraints and thresholds in proximal algorithms.</li> </ul>"},{"location":"0b%204Subgradients/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Subgradients extend the concept of gradients to nonsmooth convex functions.  </li> <li>The subdifferential is convex, closed, and provides supporting hyperplanes.  </li> <li>Zero subgradient characterizes global optimality.  </li> <li>Subgradients are foundational in nonsmooth optimization, duality, and proximal algorithms.  </li> <li>Examples include absolute value, \\(\\ell_1\\) norms, and indicator functions of convex sets.</li> </ul>"},{"location":"0b%205Fenchel%20Duality/","title":"Fenchel Duality","text":""},{"location":"0b%205Fenchel%20Duality/#fenchel-duality","title":"Fenchel Duality","text":"<p>Fenchel duality generalizes the idea of Lagrange duality to general convex optimization problems. It provides a systematic way to:</p> <ul> <li>Derive dual problems for convex programs  </li> <li>Analyze optimality conditions via subgradients  </li> <li>Link primal and dual solutions geometrically and algorithmically  </li> </ul> <p>Intuition: Fenchel duality captures the interplay between a convex function and its conjugate. By representing constraints and objectives via conjugates, we can often solve a dual problem that is simpler or more structured than the primal.</p>"},{"location":"0b%205Fenchel%20Duality/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) and \\(g: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) be proper convex functions. Consider the primal problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\, f(x) + g(Ax) \\] <p>where \\(A \\in \\mathbb{R}^{m \\times n}\\).</p>"},{"location":"0b%205Fenchel%20Duality/#fenchel-dual-problem","title":"Fenchel Dual Problem","text":"<p>The Fenchel dual is defined as:</p> \\[ \\max_{y \\in \\mathbb{R}^m} \\, -f^*(A^\\top y) - g^*(-y) \\] <ul> <li>\\(f^*\\) and \\(g^*\\) are the convex conjugates of \\(f\\) and \\(g\\).  </li> <li>\\(y\\) is the dual variable, often representing Lagrange multipliers for the linear mapping \\(Ax\\).</li> </ul>"},{"location":"0b%205Fenchel%20Duality/#weak-and-strong-duality","title":"Weak and Strong Duality","text":"<ul> <li>Weak duality: For any primal feasible \\(x\\) and dual feasible \\(y\\),</li> </ul> \\[ f(x) + g(Ax) \\ge -f^*(A^\\top y) - g^*(-y) \\] <ul> <li>Strong duality: If \\(f\\) and \\(g\\) are convex and satisfy constraint qualifications (e.g., Slater's condition), then</li> </ul> \\[ \\min_x f(x) + g(Ax) = \\max_y -f^*(A^\\top y) - g^*(-y) \\] <p>and the dual optimal solution \\(y^*\\) gives information about the primal optimal \\(x^*\\) via subgradients:</p> \\[ A^\\top y^* \\in \\partial f(x^*), \\quad -y^* \\in \\partial g(Ax^*) \\]"},{"location":"0b%205Fenchel%20Duality/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex functions \\(f\\) and \\(g\\) and linear map \\(A\\).  </li> <li>Compute the convex conjugates \\(f^*\\) and \\(g^*\\).  </li> <li>Form the dual problem:</li> </ol> \\[ \\max_y -f^*(A^\\top y) - g^*(-y) \\] <ol> <li>Solve the dual problem (often easier than the primal).  </li> <li>Recover the primal solution from dual optimality using subgradients:</li> </ol> \\[ x^* \\in \\partial f^*(A^\\top y^*) \\]"},{"location":"0b%205Fenchel%20Duality/#examples","title":"Examples","text":""},{"location":"0b%205Fenchel%20Duality/#example-1-linear-programming","title":"Example 1: Linear Programming","text":"<p>Primal LP:</p> \\[ \\min_{x \\ge 0} c^\\top x \\quad \\text{s.t.} \\quad Ax = b \\] <ul> <li>Let \\(f(x) = c^\\top x + \\delta_{\\{x \\ge 0\\}}(x)\\), \\(g(z) = \\delta_{\\{z = b\\}}(z)\\) </li> <li>Conjugates: \\(f^*(y) = \\delta_{\\{y \\le c\\}}(y)\\), \\(g^*(y) = b^\\top y\\) </li> <li>Fenchel dual:</li> </ul> \\[ \\max_y b^\\top y \\quad \\text{s.t.} \\quad A^\\top y \\le c \\] <p>This is the standard LP dual.</p>"},{"location":"0b%205Fenchel%20Duality/#example-2-quadratic-problem","title":"Example 2: Quadratic Problem","text":"<p>Primal: \\(\\min_x \\frac{1}{2}\\|x\\|_2^2 + \\delta_C(x)\\), where \\(C\\) is convex.  </p> <ul> <li>\\(f(x) = \\frac{1}{2}\\|x\\|_2^2\\), \\(g(x) = \\delta_C(x)\\) </li> <li>Conjugates: \\(f^*(y) = \\frac{1}{2}\\|y\\|_2^2\\), \\(g^*(y) = \\sigma_C(y)\\) </li> <li> <p>Fenchel dual: \\(\\max_y -\\frac{1}{2}\\|y\\|_2^2 - \\sigma_C(y)\\) </p> </li> <li> <p>Optimal \\(x^*\\) recovered via: \\(x^* = y^*\\) (gradient of \\(f^*\\))</p> </li> </ul>"},{"location":"0b%205Fenchel%20Duality/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Algorithm Design: Fenchel duality is the foundation for primal-dual algorithms and proximal splitting methods.  </li> <li>Optimality Checks: Dual solutions provide bounds on primal objectives via weak duality.  </li> <li>Geometric Interpretation: The dual problem represents the tightest linear lower bounds on the primal objective.  </li> <li>Norms and Conjugates: Links to support functions and dual norms are direct consequences of Fenchel duality.</li> </ul>"},{"location":"0b%205Fenchel%20Duality/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Fenchel duality generalizes Lagrange duality using convex conjugates.  </li> <li>Weak duality always holds; strong duality requires convexity and constraint qualifications.  </li> <li>Dual solutions provide bounds, optimality conditions, and subgradient relationships for the primal.  </li> <li>Fenchel duality is fundamental in convex optimization, primal-dual algorithms, and nonsmooth analysis.</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/","title":"Moreau Envelopes & Proximal Operators","text":""},{"location":"0b%206Moreau%20and%20Proximal/#moreau-envelopes-proximal-operators","title":"Moreau Envelopes &amp; Proximal Operators","text":"<p>Moreau envelopes and proximal operators are central concepts in nonsmooth convex optimization. They allow us to:</p> <ul> <li>Smooth nonsmooth functions for easier optimization  </li> <li>Define proximal updates that generalize gradient steps  </li> <li>Connect primal and dual problems via Fenchel conjugates </li> </ul> <p>Intuition: A Moreau envelope is a smoothed version of a convex function that approximates it while retaining its convexity. The proximal operator finds the point closest to a given input while balancing the original function, effectively performing a \u201csoft minimization.\u201d</p>"},{"location":"0b%206Moreau%20and%20Proximal/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":""},{"location":"0b%206Moreau%20and%20Proximal/#moreau-envelope","title":"Moreau Envelope","text":"<p>For a proper convex function \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) and parameter \\(\\lambda &gt; 0\\), the Moreau envelope \\(f_\\lambda\\) is defined as:</p> \\[ f_\\lambda(x) = \\min_{y \\in \\mathbb{R}^n} \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ul> <li>\\(f_\\lambda(x)\\) is smooth, even if \\(f\\) is nonsmooth.  </li> <li>It provides a lower bound on \\(f\\), approaching \\(f\\) as \\(\\lambda \\to 0\\).  </li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#proximal-operator","title":"Proximal Operator","text":"<p>The proximal operator of \\(f\\) is defined as:</p> \\[ \\text{prox}_{\\lambda f}(x) = \\arg\\min_{y \\in \\mathbb{R}^n} \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ul> <li>The proximal operator returns the point \\(y\\) that balances minimizing \\(f\\) with staying close to \\(x\\).  </li> <li>Relation: \\(f_\\lambda(x) = f(\\text{prox}_{\\lambda f}(x)) + \\frac{1}{2\\lambda} \\| \\text{prox}_{\\lambda f}(x) - x \\|_2^2\\)</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex function \\(f\\) and choose a parameter \\(\\lambda &gt; 0\\).  </li> <li>Compute the proximal operator:  </li> </ol> \\[ y^* = \\text{prox}_{\\lambda f}(x) = \\arg\\min_y \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ol> <li>Compute the Moreau envelope if a smooth approximation is desired:  </li> </ol> \\[ f_\\lambda(x) = f(y^*) + \\frac{1}{2\\lambda} \\|y^* - x\\|_2^2 \\] <ol> <li>Use in optimization algorithms:  </li> <li>Proximal gradient descent: \\(x_{k+1} = \\text{prox}_{\\lambda g}(x_k - \\lambda \\nabla f(x_k))\\) </li> <li>Splitting methods: Handle \\(f\\) and \\(g\\) separately using their proximal operators  </li> </ol>"},{"location":"0b%206Moreau%20and%20Proximal/#examples","title":"Examples","text":""},{"location":"0b%206Moreau%20and%20Proximal/#example-1-ell_1-norm-soft-thresholding","title":"Example 1: \\(\\ell_1\\) Norm (Soft Thresholding)","text":"<p>Let \\(f(x) = \\|x\\|_1\\). Then</p> \\[ \\text{prox}_{\\lambda \\| \\cdot \\|_1}(x)_i = \\text{sign}(x_i) \\cdot \\max(|x_i| - \\lambda, 0) \\] <ul> <li>Each component is shrunk toward zero, promoting sparsity.  </li> <li>Widely used in LASSO regression and compressed sensing.</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#example-2-indicator-function","title":"Example 2: Indicator Function","text":"<p>Let \\(f(x) = \\delta_C(x)\\), the indicator of a convex set \\(C\\). Then</p> \\[ \\text{prox}_{\\lambda \\delta_C}(x) = \\arg\\min_{y \\in C} \\frac{1}{2\\lambda} \\|y - x\\|_2^2 = P_C(x) \\] <ul> <li>The proximal operator reduces to the metric projection onto \\(C\\).  </li> <li>Intuition: move \\(x\\) to the closest feasible point in \\(C\\).</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#example-3-quadratic-function","title":"Example 3: Quadratic Function","text":"<p>Let \\(f(x) = \\frac{1}{2} \\|x\\|_2^2\\). Then</p> \\[ \\text{prox}_{\\lambda f}(x) = \\frac{x}{1 + \\lambda}, \\quad f_\\lambda(x) = \\frac{1}{2(1+\\lambda)} \\|x\\|_2^2 \\] <ul> <li>Smooths the function and scales down the input.  </li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Proximal Gradient Methods: Handle composite objectives \\(f(x) + g(x)\\) where \\(f\\) is smooth and \\(g\\) is nonsmooth.  </li> <li>Splitting Algorithms: Alternating updates with proximal operators allow decomposition in high-dimensional problems.  </li> <li>Regularization: \\(\\ell_1\\), nuclear norm, and indicator functions are easily handled using proximal operators.  </li> <li>Duality: Proximal operators relate to Fenchel conjugates via the Moreau decomposition:</li> </ul> \\[ x = \\text{prox}_{\\lambda f}(x) + \\lambda \\, \\text{prox}_{f^*/\\lambda}(x/\\lambda) \\]"},{"location":"0b%206Moreau%20and%20Proximal/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Moreau envelopes provide smooth approximations of nonsmooth convex functions.  </li> <li>Proximal operators generalize gradient steps to nonsmooth settings.  </li> <li>Proximal updates often have closed-form solutions for many common functions.  </li> <li>They are central to modern optimization algorithms, including proximal gradient, ADMM, and primal-dual splitting.  </li> <li>The connection with duality and conjugates makes them a versatile and powerful tool in convex optimization.</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/","title":"Indicator & Distance Functions","text":""},{"location":"0b%207Indicator%20and%20Distance/#indicator-distance-functions","title":"Indicator &amp; Distance Functions","text":"<p>Indicator and distance functions are fundamental in convex analysis and optimization. They provide a convenient way to represent constraints and measure distances to sets:</p> <ul> <li>Indicator functions encode whether a point is feasible.  </li> <li>Distance functions quantify how far a point is from feasibility.  </li> <li>Both are essential in projection algorithms, proximal methods, and dual formulations.</li> </ul> <p>Intuition: Think of a feasible region in space. The indicator function is zero inside the region and infinite outside\u2014it tells you \u201callowed or not allowed.\u201d The distance function tells you how far you are from being allowed and naturally guides projection-based updates.</p>"},{"location":"0b%207Indicator%20and%20Distance/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":""},{"location":"0b%207Indicator%20and%20Distance/#indicator-function","title":"Indicator Function","text":"<p>For a set \\(C \\subseteq \\mathbb{R}^n\\), the indicator function \\(\\delta_C: \\mathbb{R}^n \\to \\{0, +\\infty\\}\\) is defined as:</p> \\[ \\delta_C(x) = \\begin{cases} 0 &amp; \\text{if } x \\in C \\\\ +\\infty &amp; \\text{if } x \\notin C \\end{cases} \\] <ul> <li>Encodes hard constraints in optimization.  </li> <li>Appears naturally in proximal operators: \\(\\text{prox}_{\\delta_C}(x) = P_C(x)\\), the metric projection onto \\(C\\).</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/#distance-function","title":"Distance Function","text":"<p>The distance function to a set \\(C\\) is:</p> \\[ d_C(x) = \\inf_{y \\in C} \\|x - y\\| \\] <ul> <li>Measures how far \\(x\\) is from the set \\(C\\).  </li> <li>Related to the indicator function: \\(\\delta_C(x) = 0\\) if \\(d_C(x) = 0\\), otherwise \\(+\\infty\\).  </li> <li>Differentiable almost everywhere if \\(C\\) is convex, and its gradient points toward the closest point in \\(C\\).</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the feasible set \\(C\\) for constraints.  </li> <li>Use the indicator function to include constraints in unconstrained optimization formulations: </li> <li>Use the distance function to measure infeasibility or guide projections.  </li> <li>Apply in algorithms:  </li> <li>Proximal operators: \\(\\text{prox}_{\\delta_C}(x) = P_C(x)\\) </li> <li>Projected gradient methods: \\(x_{k+1} = P_C(x_k - \\alpha \\nabla f(x_k))\\) </li> <li>Penalty methods: \\(f(x) + \\frac{\\rho}{2} d_C(x)^2\\) approximates constraints softly.</li> </ol>"},{"location":"0b%207Indicator%20and%20Distance/#examples","title":"Examples","text":""},{"location":"0b%207Indicator%20and%20Distance/#example-1-nonnegative-orthant","title":"Example 1: Nonnegative Orthant","text":"<p>Let \\(C = \\{ x \\in \\mathbb{R}^n : x \\ge 0 \\}\\):</p> <ul> <li>Indicator function: \\(\\delta_C(x) = 0\\) if \\(x_i \\ge 0\\) for all \\(i\\), \\(+\\infty\\) otherwise.  </li> <li>Distance function: \\(d_C(x) = \\|x - P_C(x)\\|_2\\), where \\(P_C(x) = \\max(x, 0)\\) componentwise.  </li> <li>Proximal operator: \\(\\text{prox}_{\\delta_C}(x) = P_C(x)\\) projects negative entries to zero.</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/#example-2-euclidean-ball","title":"Example 2: Euclidean Ball","text":"<p>Let \\(C = \\{x : \\|x\\|_2 \\le r\\}\\):</p> <ul> <li>Distance: \\(d_C(x) = \\max(0, \\|x\\|_2 - r)\\) </li> <li>Projection: \\(P_C(x) = x\\) if \\(\\|x\\|_2 \\le r\\), otherwise \\(P_C(x) = r \\frac{x}{\\|x\\|_2}\\)</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Constraint Handling: Indicator functions allow transforming constrained problems into unconstrained forms suitable for proximal algorithms.  </li> <li>Proximal Algorithms: Projection onto \\(C\\) is equivalent to applying the proximal operator of \\(\\delta_C\\).  </li> <li>Distance Minimization: Distance functions quantify infeasibility and guide penalty or barrier methods.  </li> <li>Duality &amp; Conjugates: Indicator functions are conjugate to support functions, connecting geometry and duality.</li> </ul>"},{"location":"0b%207Indicator%20and%20Distance/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Indicator functions encode hard constraints; distance functions measure infeasibility.  </li> <li>Proximal operators of indicator functions are metric projections.  </li> <li>Distance functions and projections are central in projection-based optimization algorithms.  </li> <li>They link geometric intuition with algebraic optimization tools, forming the basis for constraints, duality, and proximal methods.</li> </ul>"},{"location":"0c%201Convex%20Problems/","title":"Convex Problems","text":""},{"location":"0c%201Convex%20Problems/#convex-problems","title":"Convex Problems","text":"<p>Convex problems form the backbone of optimization theory because they have well-defined geometric and algebraic properties. A problem is convex if the objective function is convex and the feasible set is convex, i.e., any weighted average of feasible points remains feasible. Convexity is essential to guarantee global optimality of solutions.</p>"},{"location":"0c%201Convex%20Problems/#convexity-of-objective-functions","title":"Convexity of Objective Functions","text":"<ul> <li>Linear / Affine: \\(f(x) = c^\\top x + d\\) \u2192 convex.  </li> <li>Quadratic: \\(f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x\\), convex if \\(Q \\succeq 0\\).  </li> <li>Sum of Convex Functions: convex.  </li> <li>Maximum / Supremum of Convex Functions: convex.  </li> <li>Composition Rules: \\(f(g(x))\\) convex if \\(f\\) convex and non-decreasing, \\(g\\) convex.  </li> </ul> <p>Tools for verification:</p> <ul> <li>Hessian: \\(\\nabla^2 f(x) \\succeq 0\\) for twice-differentiable \\(f\\).  </li> <li>Epigraph: \\(\\text{epi}(f) = \\{(x,t): f(x) \\le t\\}\\) convex \u2192 \\(f\\) convex.  </li> <li>Subgradient checks for nonsmooth functions.</li> </ul>"},{"location":"0c%201Convex%20Problems/#convexity-of-constraints","title":"Convexity of Constraints","text":"<ul> <li>Inequalities: \\(f_i(x) \\le 0\\), convex \\(f_i\\) \u2192 convex feasible region.  </li> <li>Equalities: \\(h_j(x) = 0\\), must be affine.  </li> <li>Conic/Set Membership: \\(x \\in C\\) with convex \\(C\\).  </li> </ul>"},{"location":"0c%201Convex%20Problems/#common-non-convex-structures","title":"Common Non-Convex Structures","text":"<ul> <li>Bilinear or indefinite quadratic terms.  </li> <li>Nonlinear equality constraints.  </li> <li>Products, ratios, fractional terms.  </li> <li>Discrete/integer variables.  </li> </ul>"},{"location":"0c%201Convex%20Problems/#examples","title":"Examples","text":"<ul> <li>Least Squares: convex.  </li> <li>LASSO: convex.  </li> <li>Quadratic Program with PSD Hessian: convex.  </li> <li>Bilinear \\(x_1 x_2\\) problem: non-convex.</li> </ul>"},{"location":"0c%201Convex%20Problems/#checklist-for-convexity","title":"Checklist for Convexity","text":"<ol> <li>Is the objective convex?  </li> <li>Are inequalities convex?  </li> <li>Are equalities affine?  </li> <li>Avoid non-convex structures.  </li> <li>Use Hessian, epigraph, or subgradient verification.  </li> <li>Optional: dual/Fenchel check.</li> </ol>"},{"location":"0c%202Optimality%20Conditions/","title":"Optimality Conditions","text":""},{"location":"0c%202Optimality%20Conditions/#optimality-conditions-for-convex-optimization-problems","title":"Optimality Conditions for Convex Optimization Problems","text":"<p>Convex optimization problems have the important property that any local minimum is also a global minimum. Depending on whether the problem is unconstrained or constrained, and whether the function is differentiable or not, the optimality conditions differ.</p>"},{"location":"0c%202Optimality%20Conditions/#unconstrained-problems","title":"Unconstrained Problems","text":""},{"location":"0c%202Optimality%20Conditions/#differentiable-convex-functions","title":"Differentiable Convex Functions","text":"<p>For a differentiable convex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\):</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\] <p>Optimality condition:</p> \\[ \\nabla f(\\hat{x}) = 0 \\] <p>Intuition: The gradient points in the direction of steepest increase, so a zero gradient indicates a flat spot (minimum).</p> <p>Examples:</p> <p>a) Quadratic function: \\(f(x) = x^2 - 4x + 7\\) \\(\\nabla f(x) = 2x - 4\\) \u2192 set to 0 \u2192 \\(\\hat{x} = 2\\)</p> <p>b) Sum of Squared Errors:</p> <p>\\(f(x) = \\sum_{i=1}^{n} (x - y_i)^2\\) \\(\\nabla f(x) = 2\\sum_{i=1}^{n} (x - y_i)\\) \u2192 set to 0 </p> <p>\u2192 \\(\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\) (the mean)</p>"},{"location":"0c%202Optimality%20Conditions/#non-differentiable-convex-functions","title":"Non-Differentiable Convex Functions","text":"<p>For convex but non-differentiable functions:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\] <p>Optimality condition:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>Intuition: The subgradient generalizes the derivative for functions with \"kinks.\"</p> <p>Examples:</p> <p>a) Sum of Absolute Errors (SAE): \\(f(x) = \\sum_{i=1}^{n} |x - y_i|\\) \\(\\hat{x}\\) = median of the data points.</p> <p>b) Maximum function: \\(f(x) = \\max(x-1, 2-x)\\) Subgradient: \\(\\partial f(x) = \\begin{cases} 1, &amp; x &gt; 1.5 \\\\ [-1,1], &amp; x = 1.5 \\\\ -1, &amp; x &lt; 1.5 \\end{cases}\\) \u2192 minimum at \\(x = 1.5\\)</p>"},{"location":"0c%202Optimality%20Conditions/#constrained-problems","title":"Constrained Problems","text":"<p>Consider a convex optimization problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{s.t.} \\quad x \\in \\mathcal{X} \\] <p>where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a convex feasible set.</p>"},{"location":"0c%202Optimality%20Conditions/#interior-point","title":"Interior Point:","text":"<p>If \\(\\hat{x}\\) lies strictly inside the feasible set, then the unconstrained condition applies:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>Intuition: There are no boundary restrictions, so the gradient (or subgradient) must vanish.</p>"},{"location":"0c%202Optimality%20Conditions/#boundary-point","title":"Boundary Point:","text":"<p>If \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\), then for \\(\\hat{x}\\) to be optimal:</p> <ul> <li>The negative gradient must lie in the normal cone of \\(\\mathcal{X}\\) at \\(\\hat{x}\\):</li> </ul> \\[ - \\nabla f(\\hat{x}) \\in N_{\\mathcal{X}}(\\hat{x}) \\] <ul> <li>Equivalently, the gradient must form an angle of at least \\(90^\\circ\\) with any feasible direction \\(d\\) inside \\(\\mathcal{X}\\):</li> </ul> \\[ \\nabla f(\\hat{x})^\\top d \\ge 0 \\quad \\forall d \\in T_{\\mathcal{X}}(\\hat{x}) \\] <p>where \\(T_{\\mathcal{X}}(\\hat{x})\\) is the tangent (feasible) cone at \\(\\hat{x}\\), and \\(N_{\\mathcal{X}}(\\hat{x})\\) is the normal cone at \\(\\hat{x}\\).</p> <p>Intuition: At the boundary, the optimal direction cannot point into the feasible set because any movement along a feasible direction increases the objective.</p>"},{"location":"0c%202Optimality%20Conditions/#compact-form","title":"Compact Form","text":"<p>Combining interior and boundary cases:</p> \\[ 0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x}) \\] <p>where:</p> <ul> <li>\\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\) for interior points  </li> <li>\\(N_{\\mathcal{X}}(\\hat{x})\\) is the normal cone for boundary points  </li> </ul> <p>This is a general convex optimality condition for constrained problems, valid for both differentiable and non-differentiable \\(f\\).</p>"},{"location":"0c%202Optimality%20Conditions/#intution","title":"Intution","text":"<p>Imagine a region of allowed points, called the feasible set  \\(\\mathcal{X}\\). Points strictly inside the region form the interior, where movement in any direction is possible without leaving the set. The edges and corners of the region form the boundary, where movement is restricted because you can only move along directions that remain feasible. Consider standing at a point \ud835\udc65 on this boundary. From here, you cannot move freely in all directions; you can only move along directions that stay inside the feasible set. These allowable directions form what is called the tangent cone at x, encompassing movements along the boundary or slightly into the interior.</p> <p>Opposing these feasible directions is the normal cone, which consists of vectors that point outward from the feasible region, effectively \u201cblocking\u201d any movement that would stay inside. At an optimal boundary point, the gradient of the objective function points outward, lying within the normal cone. This means that moving along any feasible direction \u2014 whether along the boundary or slightly into the interior \u2014 cannot decrease the objective function. The gradient \u201cpushes against\u201d all allowable moves, so any small displacement that respects the constraints either increases the objective or leaves it unchanged.</p> <p>This behavior contrasts with an interior optimum, where the gradient is zero and movement in any direction does not change the objective. At a boundary optimum, the gradient is non-zero but oriented such that all feasible directions are blocked from reducing the objective. Even though the gradient is not zero, the point is still optimal because the boundary restricts movement: every allowed step either raises the objective or keeps it the same. In this way, a boundary point can be a true optimum, and the outward-pointing gradient is the formal expression of the intuitive idea that you cannot \u201cgo downhill\u201d without leaving the feasible region.</p>"},{"location":"0c%202Optimality%20Conditions/#example-quadratic-with-constraint","title":"Example: Quadratic with Constraint","text":"\\[ \\min f(x) = x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Feasible set: \\(\\mathcal{X} = [1, \\infty)\\) </li> <li>Gradient: \\(\\nabla f(x) = 2x\\) </li> </ul> <p>Check optimality:</p> <ul> <li>Interior check (\\(x&gt;1\\)): \\(2x = 0 \\implies x = 0\\) \u2192 infeasible  </li> <li>Boundary check (\\(x=1\\)): \\(-\\nabla f(1) = -2 \\in N_{\\mathcal{X}}(1) = \\mathbb{R}_+\\) \u2192 satisfied  </li> </ul> <p>Solution: \\(\\hat{x} = 1\\)</p>"},{"location":"0c%203Tractable%20Problems/","title":"Tractable Problems","text":""},{"location":"0c%203Tractable%20Problems/#tractable-problems","title":"Tractable Problems","text":"<p>A problem is tractable if it can be solved efficiently with guarantees (typically polynomial time). Convexity is the key to tractability: convex problems are tractable under mild assumptions. Non-convex problems are generally intractable or require heuristics.</p>"},{"location":"0c%203Tractable%20Problems/#relationship-between-convexity-and-tractability","title":"Relationship Between Convexity and Tractability","text":"<ul> <li>Convex objective + convex feasible set \u2192 global optima reachable efficiently.  </li> <li>Non-convex objectives or feasible sets \u2192 may have multiple local minima \u2192 not tractable in general.  </li> <li>Tractability also depends on:</li> <li>Dimension of the problem </li> <li>Smoothness / Lipschitz properties </li> <li>Availability of projections / proximal operators </li> </ul>"},{"location":"0c%203Tractable%20Problems/#tools-to-assess-tractability","title":"Tools to Assess Tractability","text":"<ul> <li>Gradient-based methods: require convexity for guaranteed convergence.  </li> <li>Proximal algorithms: require convex objectives or convex regularizers.  </li> <li>Duality: strong duality holds for convex problems \u2192 efficient solution via dual.  </li> <li>Numerical stability: condition number of Hessian/Gram matrix affects convergence.  </li> </ul>"},{"location":"0c%203Tractable%20Problems/#examples-of-tractable-problems","title":"Examples of Tractable Problems","text":"<ol> <li>Least Squares Regression: convex \u2192 tractable via gradient descent or closed-form solution.  </li> <li>LASSO / Sparse Regression: convex \u2192 tractable via proximal methods.  </li> <li>Quadratic Programs (PSD Hessian): convex \u2192 tractable.  </li> <li>Non-convex bilinear problem: not tractable \u2192 requires heuristics or relaxation.</li> </ol>"},{"location":"0c%203Tractable%20Problems/#practical-checklist-for-tractability","title":"Practical Checklist for Tractability","text":"<ol> <li>Confirm the problem is convex.  </li> <li>Check smoothness, Lipschitz continuity, and gradient availability.  </li> <li>Verify constraints are simple enough for projections or proximal steps.  </li> <li>Consider dual formulations for efficient computation.  </li> <li>Ensure numerical conditioning is acceptable for iterative solvers.</li> </ol>"},{"location":"0e%20Optimization%20Algos/","title":"Optimization Algorithms","text":""},{"location":"0e%20Optimization%20Algos/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>Optimization is at the heart of machine learning: training a model is equivalent to finding the parameters \\(\\theta\\) that minimize a loss function \\(L(\\theta)\\) or maximize a likelihood. In other words, we solve problems of the form:</p> \\[ \\min_{\\theta \\in \\mathcal{X}} L(\\theta), \\] <p>where \\(\\mathcal{X}\\) is the feasible set of parameters.</p> <p>The landscape of optimization problems varies widely:</p> <ul> <li>Some functions are smooth and differentiable, allowing gradient-based methods.  </li> <li>Some functions are nonsmooth or piecewise, requiring subgradient or proximal methods.  </li> <li>Constraints or bounds on parameters require projection or constrained optimization.  </li> <li>The scale of the problem (large datasets or high-dimensional parameters) affects algorithm choice.  </li> <li>Stochasticity (e.g., noisy gradients from mini-batches) motivates stochastic optimization methods.</li> </ul> <p>Choosing the right algorithm involves understanding:</p> <ol> <li>Function properties: convexity, smoothness, Lipschitz constants.  </li> <li>Step size and momentum requirements: trade-off between speed and stability.  </li> <li>Convergence guarantees: linear vs. sublinear, deterministic vs. stochastic.  </li> </ol>"},{"location":"0e1%20Gradient%20Descent/","title":"Gradient Descent","text":""},{"location":"0e1%20Gradient%20Descent/#gradient-descent-derivation-and-convergence","title":"Gradient Descent: Derivation and Convergence","text":"<p>Gradient descent is one of the most fundamental algorithms in optimization and machine learning. It forms the backbone of training neural networks, logistic regression, matrix factorization, and many other models.</p>"},{"location":"0e1%20Gradient%20Descent/#problem-setup","title":"Problem Setup","text":"<p>We aim to minimize a differentiable function over a feasible convex set \\(\\mathcal{X}\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>At each iteration \\(t\\), we hold a current iterate \\(x_t\\) and wish to take a step that reduces the objective. But instead of minimizing \\(f\\) directly (which may be complex), we construct a local surrogate model.</p>"},{"location":"0e1%20Gradient%20Descent/#local-linear-approximation-first-order-model","title":"Local Linear Approximation (First-order Model)","text":"<p>Around \\(x_t\\), we approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>Intuition: - We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\). - If we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable.</p> <p>This motivates adding a locality restriction \u2014 we trust the linear approximation near \\(x_t\\), not globally.</p>"},{"location":"0e1%20Gradient%20Descent/#adding-a-quadratic-regularization-term","title":"Adding a Quadratic Regularization Term","text":"<p>To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <p>Geometric Interpretation: - The linear term pulls \\(x\\) in the steepest descent direction. - The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\). - \\(\\eta\\) trades off aggressive progress vs stability:   - Small \\(\\eta\\) \u2192 cautious updates.   - Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</p>"},{"location":"0e1%20Gradient%20Descent/#deriving-the-gradient-descent-update","title":"Deriving the Gradient Descent Update","text":"<p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Gradient Descent Update: </p>"},{"location":"0e1%20Gradient%20Descent/#convergence-analysis","title":"Convergence Analysis","text":"<p>To analyze convergence, we assume:</p>"},{"location":"0e1%20Gradient%20Descent/#smoothness-lipschitz-gradient","title":"Smoothness (Lipschitz Gradient)","text":"\\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|, \\quad \\forall x, y. \\] <p>This says the gradient does not change too abruptly. Most ML objectives satisfy this.</p>"},{"location":"0e1%20Gradient%20Descent/#strong-convexity","title":"Strong Convexity","text":"<p>If, in addition, \\(f\\) is \\(\\mu\\)-strongly convex, then:</p> \\[ f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2} \\|y-x\\|^2. \\] <p>This implies \\(f\\) has a unique minimizer \\(x^\\*\\) and its level sets are bowl-shaped, not flat.</p>"},{"location":"0e1%20Gradient%20Descent/#descent-lemma-why-gradient-descent-decreases-f","title":"Descent Lemma: Why Gradient Descent Decreases \\(f\\)","text":"<p>For an \\(L\\)-smooth function,</p> \\[ f(x_{t+1}) \\le f(x_t) + \\langle \\nabla f(x_t), x_{t+1}-x_t \\rangle + \\frac{L}{2} \\|x_{t+1}-x_t\\|^2. \\] <p>Using \\(x_{t+1} = x_t - \\eta \\nabla f(x_t)\\):</p> \\[ \\begin{aligned} f(x_{t+1})  &amp;\\le f(x_t) - \\eta \\|\\nabla f(x_t)\\|^2 + \\frac{L \\eta^2}{2} \\|\\nabla f(x_t)\\|^2 \\\\ &amp;= f(x_t) - \\left( \\eta - \\frac{L\\eta^2}{2} \\right) \\|\\nabla f(x_t)\\|^2. \\end{aligned} \\] <p>If \\(\\eta \\le \\frac{1}{L}\\), then the decrease term is positive \u21d2 every step reduces the objective.</p>"},{"location":"0e1%20Gradient%20Descent/#convergence-rates","title":"Convergence Rates","text":""},{"location":"0e1%20Gradient%20Descent/#convex-but-not-strongly-convex","title":"Convex but not strongly convex","text":"\\[ f(x_T) - f(x^*) \\le \\frac{L \\|x_0 - x^*\\|^2}{2T} \\quad \\Rightarrow \\quad O(1/T) \\text{ convergence}. \\] <p>This is called sublinear convergence.</p>"},{"location":"0e1%20Gradient%20Descent/#strongly-convex-case-linear-convergence","title":"Strongly Convex Case: Linear Convergence","text":"<p>If \\(f\\) is \\(\\mu\\)-strongly convex and \\(\\eta = \\frac{2}{\\mu + L}\\):</p> \\[ \\|x_{t+1} - x^*\\| \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right) \\|x_t - x^*\\| \\] <p>This gives:</p> \\[ \\|x_T - x^*\\| \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right)^T \\|x_0 - x^*\\| \\quad \\Rightarrow \\quad \\text{Linear (geometric) convergence}. \\] <p>Meaning: Error shrinks by a constant factor every iteration.</p>"},{"location":"0e1%20Gradient%20Descent/#summary-and-ml-interpretation","title":"Summary and ML Interpretation","text":"Assumption on \\(f\\) Convergence Rate Typical ML Scenario Convex + Smooth \\(O(1/T)\\) Unregularized logistic regression, basic convex losses Strongly Convex + Smooth \\(O(\\rho^T)\\) (linear) L2-regularized models, ridge regression"},{"location":"0e2%20subgradient%20method/","title":"Subgradient Method","text":""},{"location":"0e2%20subgradient%20method/#subgradient-method-derivation-geometry-and-convergence","title":"Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>Let's consider the problem of minimizing a convex, possibly nonsmooth, function:</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) may not be differentiable everywhere (e.g., hinge loss, \\(L_1\\) norm, ReLU penalties\u2014common in ML). Classical gradient descent cannot be applied directly, so we use subgradients.</p>"},{"location":"0e2%20subgradient%20method/#subgradients-and-geometric-meaning","title":"Subgradients and Geometric Meaning","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) is any vector that supports the function from below:</p> \\[ f(y) \\ge f(x_t) + \\langle g_t, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>When \\(f\\) is smooth, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\) and \\(g_t\\) coincides with the gradient.</li> <li>When \\(f\\) is nonsmooth (like \\(|x|\\) at \\(x=0\\)), the subdifferential \\(\\partial f(x_t)\\) is a set of valid slopes.</li> <li>Intuitively, any subgradient defines a supporting hyperplane that lies below the graph of \\(f\\) and touches it at \\(x_t\\).</li> </ul> <p>This generalization allows us to move in a descent direction even when a unique gradient does not exist.</p>"},{"location":"0e2%20subgradient%20method/#subgradient-update-and-projection-view","title":"Subgradient Update and Projection View","text":"<p>The update rule of the projected subgradient method is:</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}} \\big( x_t - \\eta_t g_t \\big), \\] <p>where: - \\(g_t \\in \\partial f(x_t)\\) is a valid subgradient, - \\(\\eta_t &gt; 0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) denotes projection onto \\(\\mathcal{X}\\) to ensure feasibility.</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\) (unconstrained case), projection disappears:</p> \\[ x_{t+1} = x_t - \\eta_t g_t. \\] <p>Geometric insight: we move in the direction of a subgradient and then \"snap back\" to the feasible region if needed. This is analogous to gradient descent but more flexible, tolerating kinks in the objective.</p>"},{"location":"0e2%20subgradient%20method/#distance-analysis-and-role-of-convexity","title":"Distance Analysis and Role of Convexity","text":"<p>Let \\(x^\\star\\) be an optimal solution. Consider the squared distance after an update:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - \\eta_t g_t - x^\\star\\|^2. \\] <p>Expanding the norm:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t \\langle g_t, x_t - x^\\star \\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity of \\(f\\):</p> \\[ f(x_t) - f(x^\\star) \\le \\langle g_t, x_t - x^\\star \\rangle. \\] <p>Substitute this into the distance inequality to relate movement to function decrease:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t \\big(f(x_t) - f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"0e2%20subgradient%20method/#bounding-suboptimality","title":"Bounding Suboptimality","text":"<p>Rearranging gives a direct bound on how far we are from optimum in function value:</p> \\[ f(x_t) - f(x^\\star) \\le \\frac{\\|x_t - x^\\star\\|^2 - \\|x_{t+1} - x^\\star\\|^2}{2 \\eta_t} + \\frac{\\eta_t}{2} \\|g_t\\|^2. \\] <p>This shows a trade-off: large step sizes make faster jumps but increase the \\(\\eta_t \\|g_t\\|^2\\) error; small step sizes ensure precision but slow progress.</p>"},{"location":"0e2%20subgradient%20method/#convergence-rate-and-step-size-insight","title":"Convergence Rate and Step Size Insight","text":"<p>Summing over \\(t = 0, \\dots, T-1\\) and assuming \\(\\|g_t\\| \\le G\\):</p> \\[ \\sum_{t=0}^{T-1} \\big(f(x_t) - f(x^\\star)\\big) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Dividing by \\(T\\) and using \\(\\bar{x}_T = \\frac{1}{T} \\sum_{t=0}^{T-1} x_t\\):</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2 \\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing:</p> \\[ \\eta_t = \\frac{R}{G \\sqrt{T}}, \\quad R = \\|x_0 - x^\\star\\|, \\] <p>gives the sublinear convergence rate:</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{R G}{\\sqrt{T}} \\quad \\Rightarrow \\quad O\\left(\\frac{1}{\\sqrt{T}}\\right). \\] <p>This is slower than gradient descent on smooth functions (\\(O(1/T)\\) or linear), reflecting the cost of nonsmoothness.</p>"},{"location":"0e2%20subgradient%20method/#practical-and-ml-perspective","title":"Practical and ML Perspective","text":"<ul> <li>Subgradients power many ML methods with nonsmooth penalties: L1 regularization, hinge loss (SVMs), ReLU activations, TV regularization in imaging.</li> <li>Step size choice is everything. Too large \u2192 oscillation. Too small \u2192 stagnation.</li> <li>Averaging iterates improves convergence behavior and stability.</li> <li>Unlike gradient descent, the method does not converge to a single point but to a region near the optimum unless step size goes to zero.</li> </ul> <p>In high-dimensional ML models, the subgradient method's simplicity and robustness often outweigh its slower convergence rate\u2014especially when structure (sparsity, hinge-like losses) matters more than raw speed.</p>"},{"location":"0e3%20accelerated%20gs/","title":"Accelerated GD","text":""},{"location":"0e3%20accelerated%20gs/#accelerated-gradient-descent-momentum","title":"Accelerated Gradient Descent: Momentum","text":"<p>The standard gradient descent (GD) update is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>where: - \\(x_t\\) is the current iterate, - \\(\\eta &gt; 0\\) is the step size (learning rate), - \\(\\nabla f(x_t)\\) is the gradient at \\(x_t\\).</p> <p>Intuition: Imagine rolling a ball on a hill. The ball moves in the steepest downhill direction at each step. In long, narrow valleys, standard GD can zig-zag, taking many small steps to reach the bottom.</p>"},{"location":"0e3%20accelerated%20gs/#1-momentum-adding-inertia","title":"1. Momentum: Adding Inertia","text":"<p>Momentum adds the idea of velocity, allowing the optimization to \"remember\" previous directions:</p> \\[ v_t = x_t - x_{t-1}, \\] <p>where \\(v_t\\) represents the velocity of the iterate.  </p> <ul> <li>The update now combines the current gradient and the previous motion.  </li> <li>Momentum helps move faster along flat or consistent slopes and reduces zig-zagging in steep valleys.  </li> </ul> <p>Analogy: - No momentum \u2192 ball stops after each step, carefully following the slope. - With momentum \u2192 ball keeps rolling, building speed along the valley, only slowing when gradients push against it.</p>"},{"location":"0e3%20accelerated%20gs/#2-gradient-descent-with-momentum","title":"2. Gradient Descent with Momentum","text":"<p>The update rule with momentum is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) + \\beta (x_t - x_{t-1}), \\] <p>where: - \\(\\beta \\in [0,1)\\) is the momentum parameter, controlling how much past velocity is retained.</p>"},{"location":"0e3%20accelerated%20gs/#breakdown","title":"Breakdown","text":"<ol> <li>Gradient step: \\(-\\eta \\nabla f(x_t)\\) \u2192 moves downhill.  </li> <li>Momentum step: \\(\\beta (x_t - x_{t-1})\\) \u2192 continues moving along previous direction.</li> </ol> <p>Intuition: - If gradients consistently point in the same direction, momentum accelerates the steps. - If gradients oscillate, momentum smooths the path, reducing overshooting.</p>"},{"location":"0e3%20accelerated%20gs/#3-alternative-form-velocity-update","title":"3. Alternative Form: Velocity Update","text":"<p>Another common formulation introduces an explicit velocity variable \\(v_t\\):</p> \\[ \\begin{aligned} v_{t+1} &amp;= \\beta v_t - \\eta \\nabla f(x_t) \\\\ x_{t+1} &amp;= x_t + v_{t+1} \\end{aligned} \\] <ul> <li>Here, \\(v_t\\) accumulates the past updates weighted by \\(\\beta\\).  </li> <li>This makes the analogy to a rolling ball more explicit.</li> </ul>"},{"location":"0e3%20accelerated%20gs/#4-convergence-intuition","title":"4. Convergence Intuition","text":"<ul> <li>For convex and smooth functions, momentum accelerates convergence:  </li> <li>Standard GD: \\(O(1/t)\\) </li> <li> <p>GD + Momentum / Nesterov: \\(O(1/t^2)\\)</p> </li> <li> <p>Momentum combines current slope and accumulated speed from past steps.  </p> </li> <li>Acts like a frictionless ball in a valley: keeps moving in the right direction, accelerating convergence.</li> </ul> <p>Key idea: Momentum builds up speed along consistent gradient directions and smooths oscillations along steep valleys.</p>"},{"location":"0e3%20accelerated%20gs/#5-practical-remarks","title":"5. Practical Remarks","text":"<ol> <li>Momentum is memory: it remembers the direction of previous steps.  </li> <li>Reduces oscillations in narrow valleys.  </li> <li>Accelerates convergence along consistent gradient directions.  </li> <li>Hyperparameter \\(\\beta\\) controls inertia:  </li> <li>Higher \\(\\beta\\) \u2192 longer memory, faster but potentially riskier steps.  </li> <li>Typical values: \\(\\beta = 0.9\\) or \\(0.99\\).  </li> <li>Can be combined with Nesterov acceleration for theoretically optimal rates.</li> </ol>"},{"location":"0e3%20accelerated%20gs/#6-summary","title":"6. Summary","text":"<p>Momentum modifies gradient descent by combining:</p> <ul> <li>Immediate gradient information (\\(-\\eta \\nabla f(x_t)\\))  </li> <li>Past velocity (\\(\\beta (x_t - x_{t-1})\\))  </li> </ul> <p>Effectively, it allows the optimizer to roll through valleys faster, reduce zig-zagging, and achieve accelerated convergence, especially for convex and smooth functions.</p>"},{"location":"0f%20Convergence/","title":"Convergence Properties","text":""},{"location":"0f%20Convergence/#function-properties-for-optimization-strong-convexity-smoothness-and-conditioning","title":"Function Properties for Optimization: Strong Convexity, Smoothness, and Conditioning","text":""},{"location":"0f%20Convergence/#strong-convexity","title":"Strong Convexity","text":"<p>A function \\(f\\) is \\(\\mu\\)-strongly convex if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) + \\frac{\\mu}{2}\\|y-x\\|^2. \\] <p>If \\(f\\) is twice differentiable, this is equivalent to</p> \\[ \\nabla^2 f(x) \\succeq \\mu I \\quad \\text{for all } x. \\] <ul> <li>Guarantees a unique minimizer.  </li> <li>Gradient-based methods achieve linear convergence.  </li> <li>Prevents flat regions where optimization would stall.  </li> </ul>"},{"location":"0f%20Convergence/#why-convergence-may-be-slow-without-strong-convexity","title":"Why Convergence May Be Slow Without Strong Convexity","text":"<ul> <li>If \\(f\\) is convex but not strongly convex, it can have flat regions (zero curvature).  </li> <li>Gradients may be very small in these directions \u2192 gradient steps shrink, and convergence becomes sublinear:  </li> </ul> \\[ f(x_t) - f(x^\\star) = O\\left(\\frac{1}{t}\\right). \\] <ul> <li> <p>Example: \\(f(x) = x^4\\) is convex but not strongly convex near \\(x=0\\). Gradient descent steps become tiny near the minimum \u2192 slow convergence.  </p> </li> <li> <p>Contrast: \\(f(x) = x^2\\) is strongly convex (\\(\\mu=2\\)) \u2192 linear convergence.  </p> </li> </ul>"},{"location":"0f%20Convergence/#examples","title":"Examples","text":"<ol> <li> <p>Quadratic function: \\(f(x) = x^2\\) \u2192 \\(\\mu=2\\), strongly convex \u2192 fast convergence.  </p> </li> <li> <p>Quartic function: \\(f(x) = x^4\\) \u2192 convex but not strongly convex near \\(0\\) \u2192 slow convergence.  </p> </li> <li> <p>Ridge Regression (L2 Regularization): </p> <ul> <li>The first term \\(\\|Xw - y\\|^2\\) is convex.  </li> <li>The L2 term \\(\\lambda \\|w\\|^2\\) is strongly convex (\\(\\nabla^2 (\\lambda\\|w\\|^2) = 2\\lambda I \\succeq \\lambda I\\)).  </li> <li>Adding the L2 penalty makes the entire objective strongly convex with \\(\\mu = \\lambda\\).  </li> <li>Implications: <ul> <li>Unique solution:  even if \\(X^\\top X\\) is singular or ill-conditioned.  </li> <li>Stable optimization: gradient-based methods converge linearly.  </li> <li>Prevents overfitting by controlling the size of weights.  </li> </ul> </li> </ul> </li> </ol>"},{"location":"0f%20Convergence/#smoothness-l-smoothness","title":"Smoothness (L-smoothness)","text":"<p>A function \\(f\\) is \\(L\\)-smooth if</p> \\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|. \\] <p>If twice differentiable:</p> \\[ \\nabla^2 f(x) \\preceq L I \\quad \\text{for all } x. \\] <ul> <li>Limits how steep \\(f\\) can be.  </li> <li>Ensures gradients change gradually \u2192 stable gradient steps.  </li> <li>Guarantees safe step sizes: \\(\\alpha &lt; 1/L\\) for gradient descent.  </li> </ul>"},{"location":"0f%20Convergence/#why-smoothness-matters-for-convergence","title":"Why Smoothness Matters for Convergence","text":"<ul> <li>Without smoothness, the gradient can change abruptly.  </li> <li>A large gradient could lead to overshooting, oscillation, or divergence.  </li> <li>Smoothness ensures predictable, stable progress along the gradient.  </li> </ul> <p>Examples: - Quadratic \\(f(x) = \\frac{1}{2}x^\\top Qx\\): \\(L = \\lambda_{\\max}(Q)\\). - Logistic regression loss: smooth with \\(L\\) depending on \\(\\|X\\|^2\\). - Non-smooth case: \\(f(x) = |x|\\) \u2192 gradient jumps at \\(x=0\\), cannot guarantee smooth progress \u2192 need subgradient methods.  </p>"},{"location":"0f%20Convergence/#condition-number","title":"Condition Number","text":"<p>The condition number is defined as</p> \\[ \\kappa = \\frac{L}{\\mu}. \\] <ul> <li>Measures how \u201cstretched\u201d the optimization landscape is.  </li> <li>High \\(\\kappa\\) \u2192 narrow, elongated valleys \u2192 gradient descent zig-zags, converges slowly.  </li> <li>Low \\(\\kappa\\) \u2192 round bowl \u2192 fast convergence.  </li> </ul> <p>Examples: - \\(Q=I\\): \\(\\mu=L=1\\), \\(\\kappa=1\\) \u2192 fastest convergence. - \\(Q=\\text{diag}(1,1000)\\): \\(\\mu=1\\), \\(L=1000\\), \\(\\kappa=1000\\) \u2192 ill-conditioned, very slow. - In ML, normalization (batch norm, feature scaling, whitening) reduces \\(\\kappa\\), improving training speed.  </p>"},{"location":"0f%20Convergence/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"0f%20Convergence/#strong-convexity_1","title":"Strong Convexity","text":"<ul> <li>Unique solution (ridge regression).  </li> <li>Linear convergence of gradient-based methods.  </li> <li>Stabilizes optimization by avoiding flatness.  </li> </ul>"},{"location":"0f%20Convergence/#smoothness","title":"Smoothness","text":"<ul> <li>Ensures safe and predictable step sizes.  </li> <li>Avoids overshooting or divergence.  </li> <li>Justifies constant learning rates for many ML losses.  </li> </ul>"},{"location":"0f%20Convergence/#condition-number_1","title":"Condition Number","text":"<ul> <li>Predicts convergence speed.  </li> <li>Guides preprocessing: scaling, normalization, whitening.  </li> <li>Central in designing adaptive optimizers and preconditioning methods.  </li> </ul>"},{"location":"0f%20Convergence/#convergence-rates-of-first-order-methods","title":"Convergence Rates of First-Order Methods","text":"Function Property Gradient Descent Rate Accelerated Gradient (Nesterov) Subgradient Method Rate Convex (not strongly convex) \\(O(1/t)\\) \\(O(1/t^2)\\) \\(O(1/\\sqrt{t})\\) \\(\\mu\\)-Strongly Convex Linear: \\(O\\big((1-\\eta\\mu)^t\\big)\\) Linear: faster than GD \\(O(\\log t / t)\\) Condition Number \\(\\kappa\\) Iterations \\(\\sim O(\\kappa \\log(1/\\epsilon))\\) Iterations \\(\\sim O(\\sqrt{\\kappa}\\log(1/\\epsilon))\\) \u2013"},{"location":"0f%20Convergence/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Strong convexity: bowl is always curved enough \u2192 unique and fast convergence.  </li> <li>Smoothness: bowl is not too steep \u2192 safe steps, avoids overshooting.  </li> <li>Condition number: how round vs stretched the bowl is \u2192 dictates optimization difficulty.  </li> <li>Without strong convexity \u2192 flat regions \u2192 slow sublinear convergence.  </li> <li>Without smoothness \u2192 steep gradient changes \u2192 possible divergence or oscillations.</li> </ul>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/","title":"Projections & Proximal Operators","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projections-and-proximal-operators-in-constrained-convex-optimization","title":"Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>In many convex optimization problems, we want to minimize a convex, differentiable function \\(f(x)\\) subject to some constraint that limits \\(x\\) to a feasible region \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x) \\] <p>A standard gradient descent step is</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>but this update might move \\(x_{t+1}\\) outside the feasible region \\(\\mathcal{X}\\). To fix that, we add a projection step that brings the point back into the allowed set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projection-operator","title":"Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is the closest point in the set to \\(y\\):</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 \\] <p>So the projected gradient descent update becomes</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#geometric-intuition","title":"Geometric intuition","text":"<p>Think of taking a gradient step in the direction of steepest descent, possibly leaving the feasible region. The projection then \u201csnaps\u201d that point back to the nearest feasible location. This ensures all iterates \\(x_t\\) stay within \\(\\mathcal{X}\\) while still moving downhill with respect to \\(f\\).</p> <p>Example: If \\(\\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\}\\) (the unit ball), the projection is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)} \\] <p>That means: - If \\(y\\) is inside the ball, it stays there (\\(\\|y\\|_2 \\le 1\\)). - If \\(y\\) is outside, scale it down to lie exactly on the boundary.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#from-projections-to-proximal-operators","title":"From Projections to Proximal Operators","text":"<p>Projection helps when constraints are explicitly defined by a set (e.g., nonnegativity or norm bounds). But many optimization problems include non-smooth regularization terms instead \u2014 for example, \\(g(x) = \\lambda \\|x\\|_1\\) to promote sparsity.</p> <p>The proximal operator generalizes projection to handle such non-smooth functions directly.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#definition","title":"Definition","text":"<p>For a convex (possibly non-differentiable) function \\(g(x)\\), its proximal operator is defined as:</p> \\[ \\text{prox}_{\\lambda g}(y) = \\arg\\min_x \\left( g(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#interpretation","title":"Interpretation","text":"<p>The proximal operator finds a point \\(x\\) that balances two objectives:</p> <ol> <li>Stay close to \\(y\\) \u2014 enforced by the squared term \\(\\frac{1}{2\\lambda}\\|x - y\\|^2\\).</li> <li>Reduce \\(g(x)\\) \u2014 the regularization or penalty term.</li> </ol> <p>The parameter \\(\\lambda &gt; 0\\) controls this trade-off:</p> <ul> <li>A small \\(\\lambda\\) \u2192 stronger pull toward \\(y\\) (less movement).  </li> <li>A large \\(\\lambda\\) \u2192 more freedom to reduce \\(g(x)\\).</li> </ul> <p>The squared distance term acts as a soft tether, keeping \\(x\\) near \\(y\\) while allowing it to move toward regions where \\(g(x)\\) is smaller or structured.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#indicator-function-and-connection-to-projection","title":"Indicator Function and Connection to Projection","text":"<p>Let\u2019s see how projection appears as a special case of the proximal operator.</p> <p>Define the indicator function of a convex set \\(\\mathcal{X}\\) as:</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X} \\\\ +\\infty, &amp; x \\notin \\mathcal{X} \\end{cases} \\] <p>Now, substitute \\(g(x) = I_{\\mathcal{X}}(x)\\) into the definition of the proximal operator:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\Big) \\] <p>Because \\(I_{\\mathcal{X}}(x)\\) is infinite outside \\(\\mathcal{X}\\), the minimization is effectively restricted to \\(x \\in \\mathcal{X}\\). Thus we get:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y) \\] <p>\u2705 Therefore, projection is just a proximal operator for the indicator of a set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#understanding-the-proximal-step","title":"Understanding the Proximal Step","text":"<p>The proximal operator can be viewed as a correction step:</p> <ul> <li>The gradient step moves toward minimizing the smooth part \\(f(x)\\).</li> <li>The proximal step adjusts that move to respect the structure imposed by \\(g(x)\\) \u2014 e.g., sparsity, nonnegativity, or feasibility.</li> </ul> <p>When combining both, we get the proximal gradient method:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>This algorithm generalizes projected gradient descent \u2014 it works for both constraint sets (through indicator functions) and regularizers (like \\(\\ell_1\\)-norms).</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#example-proximal-of-the-ell_1-norm","title":"Example: Proximal of the \\(\\ell_1\\)-Norm","text":"<p>We want to compute the proximal operator of the \\(\\ell_1\\)-norm:</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda \\|x\\|_1 + \\frac{1}{2}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-1-coordinate-wise-separation","title":"Step 1. Coordinate-wise separation","text":"<p>Because both \\(\\|x\\|_1\\) and \\(\\|x - y\\|^2\\) are separable across coordinates, we can solve for each component independently:</p> \\[ \\min_x \\left( \\lambda |x| + \\frac{1}{2}(x - y)^2 \\right) \\] <p>Thus, we only need to handle the scalar problem for one coordinate \\(y \\in \\mathbb{R}\\):</p> \\[ \\phi(x) = \\lambda |x| + \\frac{1}{2}(x - y)^2 \\] <p>and find</p> \\[ x^\\star = \\arg\\min_x \\phi(x) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-2-subgradient-optimality-condition","title":"Step 2. Subgradient optimality condition","text":"<p>Since \\(\\phi\\) is convex (but not differentiable at \\(x = 0\\)), the optimality condition is</p> \\[ 0 \\in \\partial \\phi(x^\\star) \\] <p>Compute the subgradient:</p> \\[ \\partial \\phi(x) = \\lambda \\, \\partial |x| + (x - y) \\] <p>where</p> \\[ \\partial |x| = \\begin{cases} \\{1\\}, &amp; x &gt; 0 \\\\[4pt] [-1, 1], &amp; x = 0 \\\\[4pt] \\{-1\\}, &amp; x &lt; 0 \\end{cases} \\] <p>Hence, the optimality condition becomes</p> \\[ 0 \\in \\lambda s + (x^\\star - y), \\quad s \\in \\partial |x^\\star| \\] <p>Rewriting:</p> \\[ x^\\star = y - \\lambda s, \\quad s \\in \\partial |x^\\star| \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-3-case-analysis","title":"Step 3. Case Analysis","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-1-xstar-0","title":"Case 1: \\(x^\\star &gt; 0\\)","text":"<p>Then \\(s = 1\\), so</p> \\[ x^\\star = y - \\lambda \\] <p>This is valid only if \\(x^\\star &gt; 0 \\implies y &gt; \\lambda\\).</p> <p>Hence, when \\(y &gt; \\lambda\\), the minimizer is:</p> \\[ x^\\star = y - \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-2-xstar-0","title":"Case 2: \\(x^\\star &lt; 0\\)","text":"<p>Then \\(s = -1\\), so</p> \\[ x^\\star = y + \\lambda \\] <p>This is valid only if \\(x^\\star &lt; 0 \\implies y &lt; -\\lambda\\).</p> <p>Hence, when \\(y &lt; -\\lambda\\), the minimizer is:</p> \\[ x^\\star = y + \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-3-xstar-0","title":"Case 3: \\(x^\\star = 0\\)","text":"<p>Then \\(s \\in [-1, 1]\\), and the condition</p> \\[ 0 \\in \\lambda s + (0 - y) \\] <p>means there exists \\(s \\in [-1, 1]\\) such that \\(y = \\lambda s\\). This happens exactly when \\(y \\in [-\\lambda, \\lambda]\\).</p> <p>Hence, when \\(|y| \\le \\lambda\\), the minimizer is:</p> \\[ x^\\star = 0 \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-4-combine-the-cases","title":"Step 4. Combine the cases","text":"<p>Putting the three cases together:</p> \\[ \\text{prox}_{\\lambda |\\cdot|}(y) = \\begin{cases} y - \\lambda, &amp; y &gt; \\lambda \\\\[6pt] 0, &amp; |y| \\le \\lambda \\\\[6pt] y + \\lambda, &amp; y &lt; -\\lambda \\end{cases} \\] <p>Or equivalently, in compact form:</p> \\[ \\boxed{ \\text{prox}_{\\lambda |\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda, 0) } \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-5-extend-to-vector-case","title":"Step 5. Extend to vector case","text":"<p>For a vector \\(y \\in \\mathbb{R}^n\\), the proximal operator applies coordinate-wise:</p> \\[ \\big(\\text{prox}_{\\lambda \\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i) \\cdot \\max(|y_i| - \\lambda, 0) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-6-intuition","title":"Step 6. Intuition","text":"<ul> <li>When \\(|y_i| \\le \\lambda\\), the quadratic term cannot compensate for the \\(\\ell_1\\) penalty, so the coordinate shrinks to zero (sparsity).</li> <li>When \\(|y_i| &gt; \\lambda\\), the coordinate is shrunk by \\(\\lambda\\) toward zero but remains nonzero.</li> <li>This behavior is called soft-thresholding, and it is the key to algorithms like LASSO and ISTA for sparse recovery.</li> </ul>"},{"location":"0g1%20proximal%20ga/","title":"Proximal Gradient Algorithm","text":""},{"location":"0g1%20proximal%20ga/#proximal-gradient-algorithm","title":"Proximal Gradient Algorithm","text":"<p>Many optimization problems involve composite objectives of the form:</p> \\[ \\min_{x \\in \\mathbb{R}^n} F(x) := f(x) + g(x) \\] <p>where:  </p> <ul> <li>\\(f(x)\\) is convex and differentiable with a Lipschitz continuous gradient (\\(\\nabla f\\) exists and is \\(L\\)-Lipschitz).  </li> <li>\\(g(x)\\) is convex but possibly non-differentiable (e.g., \\(\\ell_1\\)-norm, indicator of a constraint set).  </li> </ul> <p>This structure appears in many applications: LASSO (\\(f = \\text{least squares}, g = \\lambda \\|x\\|_1\\)), elastic net, constrained optimization, etc.</p>"},{"location":"0g1%20proximal%20ga/#1-motivation","title":"1. Motivation","text":"<ul> <li>Standard gradient descent cannot handle \\(g(x)\\) if it is non-differentiable.  </li> <li>Projected gradient descent works only if \\(g\\) is an indicator function of a set.  </li> <li>The proximal gradient method generalizes both approaches and allows efficient updates even when \\(g\\) is non-smooth.</li> </ul>"},{"location":"0g1%20proximal%20ga/#2-proximal-gradient-update","title":"2. Proximal Gradient Update","text":"<p>For step size \\(\\eta &gt; 0\\), the proximal gradient update is:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>Interpretation:</p> <ol> <li>Take a gradient step on the smooth part \\(f\\):</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply the proximal operator of \\(g\\) to handle the non-smooth part:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <p>This ensures:</p> <ul> <li>\\(f(x)\\) decreases via the gradient step.  </li> <li>\\(g(x)\\) is accounted for via the proximal step.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#3-step-size-selection","title":"3. Step Size Selection","text":"<p>For convergence, the step size \\(\\eta\\) is typically chosen as:</p> \\[ 0 &lt; \\eta \\le \\frac{1}{L} \\] <p>where \\(L\\) is the Lipschitz constant of \\(\\nabla f\\).  </p> <ul> <li>Smaller \\(\\eta\\) \u2192 conservative steps.  </li> <li>Larger \\(\\eta\\) may overshoot and break convergence guarantees.  </li> <li>Adaptive strategies (like backtracking line search) can also be used.</li> </ul>"},{"location":"0g1%20proximal%20ga/#4-algorithm-proximal-gradient-method-ista","title":"4. Algorithm (Proximal Gradient Method / ISTA)","text":"<p>Input: \\(x_0\\), step size \\(\\eta &gt; 0\\) </p> <p>Repeat for \\(t = 0, 1, 2, \\dots\\):  </p> <ol> <li>Compute gradient step:</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply proximal operator:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <ol> <li>Check convergence (e.g., \\(\\|x_{t+1} - x_t\\| &lt; \\epsilon\\)).</li> </ol>"},{"location":"0g1%20proximal%20ga/#5-special-cases","title":"5. Special Cases","text":"Non-smooth term \\(g(x)\\) Proximal operator \\(\\text{prox}_{\\eta g}(y)\\) Interpretation \\(\\lambda \\|x\\|_1\\) Soft-thresholding: $\\text{sign}(y_i)\\max( y_i Indicator \\(I_{\\mathcal{X}}(x)\\) Projection: \\(\\text{Proj}_{\\mathcal{X}}(y)\\) Constrained optimization \\(\\lambda \\|x\\|_2^2\\) Shrinkage: \\(y / (1 + 2\\eta\\lambda)\\) Smooth regularization"},{"location":"0g1%20proximal%20ga/#6-properties-of-proximal-operators","title":"6. Properties of Proximal Operators","text":"<p>Proximal operators have several useful mathematical properties:</p>"},{"location":"0g1%20proximal%20ga/#non-expansiveness-lipschitz-continuity","title":"Non-expansiveness (Lipschitz continuity):","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2 \\le \\|x - y\\|_2, \\quad \\forall x, y \\]"},{"location":"0g1%20proximal%20ga/#firmly-non-expansive","title":"Firmly non-expansive:","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2^2 \\le \\langle \\text{prox}_{g}(x) - \\text{prox}_{g}(y), x - y \\rangle \\]"},{"location":"0g1%20proximal%20ga/#fixed-point-characterization","title":"Fixed point characterization:","text":"\\[ x^\\star = \\text{prox}_{g}(x^\\star - \\eta \\nabla f(x^\\star)) \\quad \\Longleftrightarrow \\quad 0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\]"},{"location":"0g1%20proximal%20ga/#translation-property","title":"Translation property:","text":"\\[ \\text{prox}_{g}(x + c) = \\text{prox}_{g(\\cdot - c)}(x) + c \\]"},{"location":"0g1%20proximal%20ga/#separable-for-sums-over-coordinates","title":"Separable for sums over coordinates:","text":"<p>If \\(g(x) = \\sum_i g_i(x_i)\\), then</p> \\[ \\text{prox}_{g}(x) = \\big( \\text{prox}_{g_1}(x_1), \\dots, \\text{prox}_{g_n}(x_n) \\big) \\] <p>This is why soft-thresholding works coordinate-wise.</p>"},{"location":"0g1%20proximal%20ga/#7-why-proximal-gradient-works","title":"7. Why Proximal Gradient Works","text":"<ul> <li>The proximal gradient method splits the objective into smooth and non-smooth parts.  </li> <li>The gradient step moves toward minimizing \\(f(x)\\) (smooth).  </li> <li>The proximal step moves toward minimizing \\(g(x)\\) (structure or constraints).  </li> <li> <p>Geometrically, the proximal operator finds a point close to the gradient update but also reduces the non-smooth term, ensuring convergence under convexity and Lipschitz continuity.  </p> </li> <li> <p>If \\(g = 0\\), it reduces to gradient descent.  </p> </li> <li>If \\(g\\) is an indicator function, it reduces to projected gradient descent.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#8-convergence","title":"8. Convergence","text":"<p>For convex \\(f\\) and \\(g\\):</p> \\[ F(x_t) - F(x^\\star) = \\mathcal{O}\\Big(\\frac{1}{t}\\Big) \\] <ul> <li>Accelerated variants (like FISTA) improve the rate to \\(\\mathcal{O}(1/t^2)\\).  </li> <li>Requires convexity and Lipschitz continuity of \\(\\nabla f\\).</li> </ul>"},{"location":"0g1%20proximal%20ga/#9-fast-iterative-shrinkage-thresholding-algorithm-fista","title":"9. Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)","text":"<p>While ISTA converges at \\(\\mathcal{O}(1/t)\\), FISTA introduces a clever momentum / extrapolation term that accelerates convergence to \\(\\mathcal{O}(1/t^2)\\).</p> <p>Algorithm (FISTA):</p> <p>Initialize \\(x_0\\), set \\(y_0 = x_0\\), \\(t_0 = 1\\).</p> <p>For \\(k = 0, 1, 2, \\dots\\):</p> <ol> <li>Gradient and proximal update:</li> </ol> \\[ x_{k+1} = \\text{prox}_{\\eta g}(y_k - \\eta \\nabla f(y_k)) \\] <ol> <li>Update momentum parameter:</li> </ol> \\[ t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2} \\] <ol> <li>Extrapolation (Nesterov acceleration):</li> </ol> \\[ y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}}(x_{k+1} - x_k) \\]"},{"location":"0g1%20proximal%20ga/#key-insight","title":"Key Insight","text":"<ul> <li>ISTA updates only based on \\(x_k\\).  </li> <li>FISTA introduces a look-ahead point \\(y_k\\) that combines past iterates, similar to momentum methods in deep learning.</li> <li>This extrapolation step dramatically speeds up convergence without changing the proximal operator.</li> </ul>"},{"location":"0g1%20proximal%20ga/#comparison-summary","title":"Comparison Summary","text":"Method Update Uses Convergence Rate Gradient Descent (\\(g = 0\\)) \\(\\nabla f(x_t)\\) \\(\\mathcal{O}(1/t)\\) ISTA \\(\\nabla f(x_t)\\) + prox \\(\\mathcal{O}(1/t)\\) FISTA \\(\\nabla f(y_t)\\) + prox + momentum \\(\\mathcal{O}(1/t^2)\\) \u2705"},{"location":"0h%20lasso/","title":"LASSO","text":""},{"location":"0h%20lasso/#lasso-and-optimization-methods","title":"LASSO and Optimization Methods","text":"<p>The LASSO problem is formulated as:</p> \\[ \\min_{x \\in \\mathbb{R}^p} \\ \\|Ax - y\\|_2^2 + \\lambda \\|x\\|_1 \\] <ul> <li>\\(A \\in \\mathbb{R}^{n \\times p}\\): measurement/design matrix.</li> <li>\\(y \\in \\mathbb{R}^n\\): observations.</li> <li>\\(\\|x\\|_1 = \\sum_{i=1}^p |x_i|\\): promotes sparsity.</li> <li>Assumption: \\(x\\) is \\(s\\)-sparse, meaning only \\(s \\ll p\\) entries are nonzero \u2192 compressed sensing.</li> </ul>"},{"location":"0h%20lasso/#1-lasso-via-subgradient-descent","title":"1.  LASSO via Subgradient Descent","text":"<p>Since \\(\\|x\\|_1\\) is non-smooth, we use subgradient descent:</p> <p>Update rule: </p> <p>where the subgradient \\(z \\in \\partial \\|x\\|_1\\) is:</p> \\[ z_i = \\begin{cases} +1, &amp; x_i &gt; 0 \\\\ -1, &amp; x_i &lt; 0 \\\\ [-1, 1], &amp; x_i = 0 \\end{cases} \\] <ul> <li>Convergence rate: \\(\\mathcal{O}(1/\\sqrt{t})\\) \u2014 slow for practical use.</li> <li>Still important conceptually but inefficient compared to proximal methods.</li> </ul>"},{"location":"0h%20lasso/#2-proximal-gradient-for-lasso-ista","title":"2. Proximal Gradient for LASSO (ISTA)","text":"<p>To efficiently handle the non-smooth \\(\\ell_1\\) term, we use the proximal gradient (ISTA) method.</p>"},{"location":"0h%20lasso/#step-1-gradient-update-on-smooth-term-ax-y_22","title":"Step 1 \u2014 Gradient update on smooth term \\(\\|Ax - y\\|_2^2\\):","text":"\\[ y_t = x_t - \\eta \\cdot 2A^\\top(Ax_t - y) \\]"},{"location":"0h%20lasso/#step-2-apply-proximal-operator-of-ell_1-soft-thresholding","title":"Step 2 \u2014 Apply proximal operator of \\(\\ell_1\\) (soft thresholding):","text":"\\[ x_{t+1} = \\text{prox}_{\\eta \\lambda \\|\\cdot\\|_1}(y_t) \\] <p>The soft-thresholding operator:</p> \\[ \\text{prox}_{\\alpha \\|\\cdot\\|_1}(z)_i = \\text{sign}(z_i) \\cdot \\max(|z_i| - \\alpha, 0) \\] <p>Thus the ISTA update becomes:</p> \\[ x_{t+1} = \\text{sign}(y_{t,i}) \\cdot \\max(|y_{t,i}| - \\eta \\lambda, 0) \\] <p>Interpretation: Gradient descent + shrinkage toward zero \u2192 automatically induces sparsity.</p>"},{"location":"0h%20lasso/#3-fista-accelerated-proximal-gradient-for-lasso","title":"3. FISTA \u2014 Accelerated Proximal Gradient for LASSO","text":"<p>ISTA has convergence rate \\(\\mathcal{O}(1/t)\\). FISTA (Fast ISTA) improves it to:</p> \\[ \\mathcal{O}(1/t^2) \\quad \\text{(optimal for first-order methods)} \\] <p>Algorithm (FISTA):</p> <ul> <li>Initialize: \\(x_0\\), set \\(y_0 = x_0\\), \\(t_0 = 1\\).</li> <li> <p>For \\(k = 0, 1, 2, \\dots\\):</p> </li> <li> <p>Proximal gradient step: </p> </li> <li> <p>Update momentum parameter: </p> </li> <li> <p>Nesterov extrapolation step: </p> </li> </ul> <p>Key idea: Instead of updating only from \\(x_k\\) (like ISTA), FISTA uses a look-ahead point \\(y_k\\) to inject momentum and accelerate convergence.</p>"},{"location":"0h%20lasso/#method-comparison","title":"\ud83d\udcca Method Comparison","text":"Method Handles \\(\\ell_1\\)? Uses Prox? Convergence Rate Gradient Descent \u274c \u274c Fast (smooth only) Subgradient Descent \u2705 \u274c \\(\\mathcal{O}(1/\\sqrt{t})\\) (slow) ISTA (Proximal GD) \u2705 \u2705 \\(\\mathcal{O}(1/t)\\) FISTA (Accelerated) \u2705 \u2705 \u2705 \\(\\mathcal{O}(1/t^2)\\) \u2705"},{"location":"0k%20AdvancedAlgos/","title":"0k AdvancedAlgos","text":""},{"location":"0k%20AdvancedAlgos/#first-order-gradient-based-methods","title":"First-Order Gradient-Based Methods","text":"<p>Used when: Only gradient information is available; scalable to high-dimensional problems.</p>"},{"location":"0k%20AdvancedAlgos/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<ul> <li>Problem: Minimize smooth or convex functions.  </li> <li>Update: </li> <li>Convergence: Convex \u2192 \\(O(1/k)\\); Strongly convex \u2192 linear.  </li> <li>Use case: Small convex problems, theoretical baseline.  </li> <li>Pitfalls: Step size too small \u2192 slow; too large \u2192 divergence.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<ul> <li>Problem: Minimize empirical risk over large datasets.  </li> <li>Update:   (mini-batch gradient)  </li> <li>Pros: Scales to huge datasets; cheap per iteration.  </li> <li>Cons: Noisy updates \u2192 requires learning rate schedules.  </li> <li>ML use: Deep learning, large-scale logistic regression.  </li> </ul> <p>Best Practices: Learning rate warmup, linear scaling with batch size, momentum to stabilize updates, cyclic learning rates for exploration.</p>"},{"location":"0k%20AdvancedAlgos/#momentum-nesterov-accelerated-gradient","title":"Momentum &amp; Nesterov Accelerated Gradient","text":"<ul> <li>Problem: Reduce oscillations and accelerate convergence in ill-conditioned problems.  </li> <li>Momentum: </li> <li>Nesterov: Gradient computed at lookahead point \u2192 theoretically optimal for convex problems.  </li> <li>ML use: CNNs, ResNets, EfficientNet.  </li> <li>Pitfalls: High momentum \u2192 oscillations; careful learning rate tuning required.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#adaptive-methods-adagrad-rmsprop-adam-adamw","title":"Adaptive Methods (AdaGrad, RMSProp, Adam, AdamW)","text":"<ul> <li>Problem: Adjust learning rate per parameter for fast/stable convergence.  </li> <li>Behavior: </li> <li>AdaGrad \u2192 aggressive decay, good for sparse features.  </li> <li>RMSProp \u2192 fixes AdaGrad\u2019s rapid decay.  </li> <li>Adam \u2192 RMSProp + momentum.  </li> <li>AdamW \u2192 decouples weight decay for better generalization.  </li> <li>ML use: Transformers, NLP, sparse models.  </li> <li>Pitfalls: Adam may converge to sharp minima \u2192 worse generalization than SGD in CNNs.  </li> <li>Best Practices: Warmup, cosine LR decay, weight decay with AdamW.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#second-order-curvature-aware-methods","title":"Second-Order &amp; Curvature-Aware Methods","text":"<p>Used when: Hessian or curvature information improves convergence; mostly for small/medium models.</p>"},{"location":"0k%20AdvancedAlgos/#newtons-method","title":"Newton\u2019s Method","text":"<ul> <li>Problem: Solve  with smooth Hessian.  </li> <li>Update: </li> <li>Pros: Quadratic convergence.  </li> <li>Cons: Hessian expensive in high dimensions.  </li> <li>ML use: GLMs, small convex models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#quasi-newton-bfgs-l-bfgs","title":"Quasi-Newton (BFGS, L-BFGS)","text":"<ul> <li>Problem: Approximate Hessian using low-rank updates.  </li> <li>Pros: Efficient for medium-scale problems.  </li> <li>Cons: BFGS memory-heavy; L-BFGS preferred.  </li> <li>ML use: Logistic regression, Cox models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#conjugate-gradient","title":"Conjugate Gradient","text":"<ul> <li>Problem: Solve large linear/quadratic problems efficiently.  </li> <li>ML use: Hessian-free optimization; combined with Pearlmutter trick for Hessian-vector products.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#natural-gradient-k-fac","title":"Natural Gradient &amp; K-FAC","text":"<ul> <li>Problem: Precondition gradients using Fisher Information \u2192 invariant to parameterization.  </li> <li>ML use: Large CNNs, transformers; improves convergence in distributed training.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#constrained-specialized-optimization","title":"Constrained &amp; Specialized Optimization","text":""},{"location":"0k%20AdvancedAlgos/#interior-point","title":"Interior-Point","text":"<ul> <li>Problem: Constrained optimization via barrier functions.  </li> <li>ML use: Structured convex problems, LP/QP.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#admm-augmented-lagrangian","title":"ADMM / Augmented Lagrangian","text":"<ul> <li>Problem: Split constraints into easier subproblems with dual updates.  </li> <li>ML use: Distributed optimization, structured sparsity.  </li> </ul>"},{"location":"0k%20AdvancedAlgos/#frankwolfe","title":"Frank\u2013Wolfe","text":"<ul> <li>Problem: Projection-free constrained optimization; linear subproblem instead of projection.  </li> <li>ML use: Simplex, nuclear norm problems.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#coordinate-descent","title":"Coordinate Descent","text":"<ul> <li>Problem: Update one variable at a time.  </li> <li>ML use: Lasso, GLMs, sparse regression.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#proximal-methods","title":"Proximal Methods","text":"<ul> <li>Problem: Efficiently handle nonsmooth penalties.  </li> <li>Algorithms: ISTA (\\(O(1/k)\\)), FISTA (\\(O(1/k^2)\\))  </li> <li>ML use: Sparse coding, Lasso, elastic net.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#derivative-free-black-box","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Optimize when gradients unavailable or unreliable.  </li> <li>Algorithms: Nelder\u2013Mead, CMA-ES, Bayesian Optimization  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#optimization-problem-styles","title":"Optimization Problem Styles","text":""},{"location":"0k%20AdvancedAlgos/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<ul> <li>Problem: Maximize likelihood or minimize negative log-likelihood: </li> <li>Algorithms: Newton/Fisher scoring, L-BFGS, SGD, EM, Proximal/Coordinate.  </li> <li>ML use: Logistic regression, GLMs, Gaussian mixture models, HMMs.  </li> <li>Notes: EM guarantees monotonic likelihood increase; Fisher scoring uses expected curvature \u2192 stable.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#empirical-risk-minimization-erm","title":"Empirical Risk Minimization (ERM)","text":"<ul> <li>Problem: Minimize average loss with optional regularization: </li> <li>Algorithms: GD, SGD, Momentum, Adam, L-BFGS, Proximal.  </li> <li>ML use: Regression, classification, deep learning.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#regularized-penalized-optimization","title":"Regularized / Penalized Optimization","text":"<ul> <li>Problem: Add penalties to encourage sparsity or smoothness: </li> <li>Algorithms: Proximal gradient, ADMM, Coordinate Descent, ISTA/FISTA.  </li> <li>ML use: Lasso, Elastic Net, sparse dictionary learning.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#constrained-optimization","title":"Constrained Optimization","text":"<ul> <li>Problem: Minimize with equality/inequality constraints.  </li> <li>Algorithms: Interior-point, ADMM, Frank\u2013Wolfe, penalty/barrier methods.  </li> <li>ML use: Fairness constraints, structured prediction.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#bayesian-map-optimization","title":"Bayesian / MAP Optimization","text":"<ul> <li>Problem: Maximize posterior: </li> <li>Algorithms: Gradient-based, Laplace approximation, Variational Inference, MCMC.  </li> <li>ML use: Bayesian neural networks, probabilistic models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#minimax-adversarial-optimization","title":"Minimax / Adversarial Optimization","text":"<ul> <li>Problem: </li> <li>Algorithms: Gradient descent/ascent, extragradient, mirror descent.  </li> <li>ML use: GANs, adversarial training, robust optimization.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#reinforcement-learning-policy-optimization","title":"Reinforcement Learning / Policy Optimization","text":"<ul> <li>Problem: Maximize expected cumulative reward: </li> <li>Algorithms: Policy gradient, Actor-Critic, Natural Gradient.  </li> <li>ML use: RL agents, sequential decision-making.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<ul> <li>Problem: Optimize multiple competing objectives \u2192 Pareto front.  </li> <li>Algorithms: Scalarization, weighted sum, evolutionary algorithms.  </li> <li>ML use: Multi-task learning, accuracy vs fairness trade-offs.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#metric-embedding-learning","title":"Metric / Embedding Learning","text":"<ul> <li>Problem: Learn embeddings preserving similarity/distance: </li> <li>Algorithms: SGD/Adam with careful sampling.  </li> <li>ML use: Contrastive learning, triplet loss, Siamese networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#combinatorial-discrete-optimization","title":"Combinatorial / Discrete Optimization","text":"<ul> <li>Problem: Optimize discrete/integer variables.  </li> <li>Algorithms: Branch-and-bound, integer programming, RL-based relaxation, Gumbel-softmax.  </li> <li>ML use: Feature selection, neural architecture search, graph matching.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#derivative-free-black-box_1","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Gradients unavailable or noisy.  </li> <li>Algorithms: Bayesian Optimization, CMA-ES, Nelder\u2013Mead.  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#learning-rate-practical-tips","title":"Learning Rate &amp; Practical Tips","text":"<ul> <li>Step decay, cosine annealing, OneCycle, warmup.  </li> <li>Gradient clipping (global norm 1\u20135), batch/layer normalization, FP16 mixed precision.  </li> <li>Decouple weight decay from Adam (AdamW).</li> </ul>"},{"location":"0k%20AdvancedAlgos/#summary","title":"Summary","text":"Algorithm Problem Type ML / AI Use Case GD Smooth / convex Small convex models, baseline SGD Large-scale ERM Deep learning, logistic regression SGD + Momentum Ill-conditioned / deep nets CNNs (ResNet, EfficientNet) Nesterov Accelerated GD Convex / ill-conditioned CNNs, small convex models AdaGrad Sparse features NLP, sparse embeddings RMSProp Stabilized adaptive LR RNNs, sequence models Adam Adaptive large-scale Transformers, small nets AdamW Adaptive + weight decay Transformers, NLP Newton / Fisher Scoring Smooth convex GLMs, small MLE BFGS / L-BFGS Medium convex Logistic regression, Cox models Conjugate Gradient Linear / quadratic Hessian-free optimization, linear regression Natural Gradient / K-FAC Deep nets CNNs, transformers Proximal / ISTA / FISTA Nonsmooth / sparse Lasso, sparse coding, elastic net Coordinate Descent Separable / sparse Lasso, GLMs Interior-Point Constrained convex LP/QP problems ADMM Distributed convex Sparse or structured optimization Frank\u2013Wolfe Projection-free constraints Simplex, nuclear norm problems EM Algorithm Latent variable MLE GMM, HMM, LDA Policy Gradient / Actor-Critic Sequential / RL RL agents Bayesian Optimization Black-box / derivative-free Hyperparameter tuning, NAS CMA-ES / Nelder-Mead Black-box Small networks, continuous black-box Minimax / Gradient Ascent-Descent Adversarial GANs, robust optimization Multi-Objective / Evolutionary Multiple objectives Multi-task learning, fairness Metric Learning / Triplet Loss Similarity embedding Contrastive learning, Siamese nets"},{"location":"0k%20mirror/","title":"Mirror Descent","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, it implicitly assumes Euclidean geometry, which may not respect the natural structure of many problems. Mirror Descent generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence, making it particularly suitable for constrained, probabilistic, and sparse domains. </p>"},{"location":"0k%20mirror/#1-introduction-and-motivation","title":"1. Introduction and Motivation","text":"<p>Gradient Descent is often introduced as the default optimization method:</p> \\[x_{t+1} = x_t - \\eta \\nabla f(x_t)\\] <p>This seemingly simple update assumes that the underlying optimization space is Euclidean, where distance is measured using the \\(\\ell_2\\) norm:</p> \\[\\|x - y\\|_2 = \\sqrt{\\sum_i (x_i - y_i)^2}\\] <p>Works well for unconstrained problems in \\(\\mathbb{R}^n\\) with no additional structure.  </p> <p>However, in real-world machine learning and optimization problems:</p> <ul> <li>Parameters often live in structured spaces like probability simplices or sparse domains.</li> <li>Euclidean distance is often not the most natural notion of distance.</li> <li>Applying Euclidean updates can destroy problem structure or create instabilities.</li> </ul> <p>Key insight: Gradient Descent is not inherently \u201cwrong\u201d\u2014it\u2019s just geometry-specific. Mirror Descent generalizes GD to respect the intrinsic geometry of the problem.</p>"},{"location":"0k%20mirror/#2-geometry-in-optimization-why-it-matters","title":"2. Geometry in Optimization \u2014 Why It Matters","text":"<p>Standard GD treats all directions equally. The steepest descent direction is simply the gradient \\(\\nabla f(x)\\), derived from Euclidean distance. This is equivalent to asking: \"In which direction does \\(f(x)\\) decrease fastest if distance is measured by the \\(\\ell_2\\) norm?\"</p> <p>While sufficient for many unconstrained problems, this implicitly assumes:</p> <ul> <li>The feasible set is unbounded or flat.</li> <li>Movement along all axes is equally \u201ccostly\u201d.</li> <li>There are no constraints like positivity or normalization.</li> </ul>"},{"location":"0k%20mirror/#when-euclidean-geometry-fails","title":"When Euclidean Geometry Fails","text":"<p>Many modern optimization problems involve structured domains:</p> Scenario Constraint / Structure Natural Geometry Probability vectors \\(x_i \\ge 0, \\sum_i x_i = 1\\) KL divergence / simplex geometry Attention weights Positive and normalized Entropy geometry Sparse models Preference for zeros \\(\\ell_1\\) geometry Online learning Avoid drastic updates Multiplicative weights / log-space <p>Using Euclidean GD in these settings can lead to:</p> <ul> <li>Harsh projections that instantly zero out components.</li> <li>Violation of sparsity or positivity constraints.</li> <li>Loss of smoothness or natural probabilistic interpretation.</li> </ul> <p>Observation: Gradient Descent works \u201clocally,\u201d but may be incompatible with global geometry of the feasible domain.</p>"},{"location":"0k%20mirror/#3-mirror-descent-a-geometric-generalization","title":"3. Mirror Descent: A Geometric Generalization","text":"<p>Mirror Descent adapts Gradient Descent to non-Euclidean geometries, encoding the structure of the optimization space.</p>"},{"location":"0k%20mirror/#mirror-maps-and-dual-coordinates","title":"Mirror Maps and Dual Coordinates","text":"<p>A mirror map \\(\\psi(x)\\) is a strictly convex, differentiable function representing the geometry:</p> \\[u = \\nabla \\psi(x)\\] <ul> <li>\\(x\\) = primal variable  </li> <li>\\(u\\) = dual variable (coordinates in transformed space)</li> </ul> <p>Updates occur in the dual space, then are mapped back using the convex conjugate:</p> \\[x = \\nabla \\psi^*(u)\\] <p>This allows GD-like updates while respecting geometry constraints.</p>"},{"location":"0k%20mirror/#bregman-divergence-and-interpretation","title":"Bregman Divergence and Interpretation","text":"<p>The Bregman divergence associated with \\(\\psi\\) generalizes squared Euclidean distance:</p> \\[D_\\psi(x \\| y) = \\psi(x) - \\psi(y) - \\langle \\nabla \\psi(y), x - y \\rangle\\]"},{"location":"0k%20mirror/#intuition","title":"Intuition:","text":"<ul> <li>Think of \\(D_\\psi(x \\| y)\\) as a geometry-aware distance measure.  </li> <li>It captures how far \\(x\\) is from \\(y\\) in the space defined by \\(\\psi\\), not just in straight-line Euclidean distance.</li> <li> <p>Conceptually, it measures the error between the linear approximation of \\(\\psi\\) at \\(y\\) and the true value at \\(x\\):</p> </li> <li> <p>\\(\\psi(y) + \\langle \\nabla \\psi(y), x - y \\rangle\\) = linear approximation  </p> </li> <li> <p>\\(\\psi(x) - (\\text{linear approximation})\\) = how \"nonlinear\" the space feels from \\(y\\) to \\(x\\) </p> </li> <li> <p>When \\(\\psi(x) = \\frac12 \\|x\\|_2^2\\), the Bregman divergence reduces to Euclidean distance squared.  </p> </li> <li> <p>For negative entropy (common in probability spaces), it reduces to KL divergence.</p> </li> <li> <p>Purpose in MD: Ensures updates respect the intrinsic geometry, balancing movement in the objective with staying \u201cclose\u201d in the right geometry.</p> </li> </ul> <p>Mirror Descent update (primal form):</p> \\[x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\left\\{ \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{\\eta} D_\\psi(x \\| x_t) \\right\\}\\] <p>Move in a descent direction while staying \u201cclose\u201d according to Bregman divergence, not Euclidean distance.</p>"},{"location":"0k%20mirror/#4-gradient-descent-vs-mirror-descent","title":"4. Gradient Descent vs Mirror Descent","text":""},{"location":"0k%20mirror/#primal-view-projection-vs-bregman-step","title":"Primal View (Projection vs Bregman Step)","text":"<ul> <li>GD: Step in Euclidean space; may leave the feasible domain \u2192 project back.  </li> <li>MD: Step along geometry-aware Bregman divergence; no harsh projection needed.</li> </ul> Method Update Rule Notes Gradient Descent \\(x - \\eta \\nabla f\\) Euclidean, may leave domain Projected GD \\(\\text{Proj}(x - \\eta \\nabla f)\\) Projection may destroy smoothness Mirror Descent \\(\\arg\\min_x \\langle \\nabla f, x - x_t \\rangle + \\frac{1}{\\eta} D_\\psi(x\\|x_t)\\) Structure-preserving"},{"location":"0k%20mirror/#dual-view-gd-in-dual-space","title":"Dual View (GD in Dual Space)","text":"<p>Mirror Descent can also be understood as Gradient Descent in dual coordinates:</p> \\[ \\begin{aligned} u_t &amp;= \\nabla \\psi(x_t) \\\\ u_{t+1} &amp;= u_t - \\eta \\nabla f(x_t) \\\\ x_{t+1} &amp;= \\nabla \\psi^*(u_{t+1}) \\end{aligned} \\] <p>\u2705 MD is GD in a warped coordinate system, where distance and directions are geometry-aware.</p>"},{"location":"0k%20mirror/#5-intuitive-example-on-the-simplex","title":"5. Intuitive Example on the Simplex","text":"<p>Consider \\(x \\in \\Delta^2 = \\{ x \\ge 0, x_1 + x_2 = 1 \\}\\) and objective:</p> \\[f(x) = x_1^2 + 2 x_2\\] <p>Initial point: \\(x = (0.5, 0.5)\\), step size \\(\\eta = 0.3\\).</p>"},{"location":"0k%20mirror/#behavior-of-gd-projection","title":"Behavior of GD + Projection","text":"<ol> <li>Gradient: \\(\\nabla f = (2x_1, 2) = (1,2)\\)</li> <li>Step: \\(y = x - \\eta \\nabla f = (0.2, -0.1)\\)</li> <li>Project onto simplex: \\(x_{\\text{new}} = (1, 0)\\)</li> </ol> <p>\u274c Projection abruptly kills one component. Smoothness and probabilistic structure are lost.</p>"},{"location":"0k%20mirror/#behavior-of-mirror-descent-kl-negative-entropy","title":"Behavior of Mirror Descent (KL / Negative Entropy)","text":"<p>Mirror map: \\(\\psi(x) = \\sum_i x_i \\log x_i\\) </p> <p>Update rule:</p> <p>\\(x_i^{\\text{new}} \\propto x_i \\exp(-\\eta \\nabla_i f(x))\\)</p> <p>Normalized:</p> <p>\\(x \\approx (0.57, 0.43)\\)</p> <p>\u2705 Smooth, positive, stays in the simplex, no harsh projection.</p>"},{"location":"0k%20mirror/#interpretation","title":"Interpretation","text":"Method Intuition GD + Projection Walks straight \u2192 hits boundary \u2192 forced back Mirror Descent Walks along curved, geometry-aware space \u2192 never violates constraints <p>Mirror Descent = optimization with geometry turned ON.</p>"},{"location":"0k%20mirror/#6-choosing-the-mirror-map-geometry-as-a-design-choice","title":"6. Choosing the Mirror Map \u2014 Geometry as a Design Choice","text":""},{"location":"0k%20mirror/#entropy-geometry-simplex","title":"Entropy Geometry (Simplex)","text":"<ul> <li>Mirror map: \\(\\psi(x) = \\sum_i x_i \\log x_i\\)</li> <li>Divergence: KL divergence</li> <li>Update: multiplicative weights  </li> <li>Applications: probability vectors, attention mechanisms</li> </ul>"},{"location":"0k%20mirror/#ell_1-geometry-sparsity","title":"\\(\\ell_1\\) Geometry (Sparsity)","text":"<ul> <li>Mirror map encourages sparse updates</li> <li>Useful in compressed sensing, feature selection</li> </ul>"},{"location":"0k%20mirror/#euclidean-as-a-special-case","title":"Euclidean as a Special Case","text":"<ul> <li>Mirror map: \\(\\psi(x) = \\frac12 \\|x\\|_2^2\\)</li> <li>Divergence: squared Euclidean distance</li> <li>Recovers standard GD</li> </ul>"},{"location":"0k%20mirror/#7-practical-guidance-for-practitioners","title":"7. Practical Guidance for Practitioners","text":""},{"location":"0k%20mirror/#when-to-prefer-mirror-descent","title":"When to Prefer Mirror Descent","text":"<ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates required</li> <li>Avoiding costly or disruptive Euclidean projections</li> </ul>"},{"location":"0k%20mirror/#computational-remarks","title":"Computational Remarks","text":"<ul> <li>Choice of mirror map affects efficiency (some dual mappings are cheap, others expensive)</li> <li>Often simple closed-form updates exist (multiplicative weights, exponentiated gradient)</li> <li>Integration with adaptive step sizes or momentum is possible</li> </ul> <p>Mirror Descent is a powerful generalization of Gradient Descent, making the geometry of the domain explicit in the update rule. By carefully choosing a mirror map, one can design updates that:</p> <ul> <li>Preserve constraints naturally</li> <li>Avoid projection shocks</li> <li>Respect sparsity or probability structure</li> <li>Connect elegantly to modern ML methods like attention, boosting, and natural gradient</li> </ul>"},{"location":"0m%20sgd/","title":"Stochastic Gradient Descent","text":"<p>So far, all optimization methods we have discussed, including Gradient Descent (GD) and Mirror Descent (MD), assume exact evaluation of gradients or subgradients. In practice, however, computing exact gradients is often impossible or computationally expensive. Stochastic Gradient Descent (SGD) and related methods address this by using noisy or approximate gradients, enabling scalable optimization in large-scale or complex settings</p>"},{"location":"0m%20sgd/#1-motivation","title":"1. Motivation","text":"<p>Classical optimization relies on computing the exact gradient \\(\\nabla f(x)\\) at each iteration. However, in many real-world scenarios, this is impractical:</p> <ol> <li>Exact gradient unavailable </li> <li>Some functions are non-differentiable or analytically intractable.  </li> <li> <p>Example: complex composite objectives or non-smooth loss functions.</p> </li> <li> <p>Computational cost is prohibitive </p> </li> <li>Large datasets make full gradient computation expensive.  </li> <li> <p>Neural networks require backpropagation over all samples per step, which can be infeasible.</p> </li> <li> <p>Stochastic optimization naturally arises </p> </li> <li>Data scale: Objective involves a sum or expectation over massive datasets.  </li> <li>Intrinsic randomness: Objective defined as an expectation over a stochastic process, e.g., in reinforcement learning, online optimization, or probabilistic modeling.</li> </ol>"},{"location":"0m%20sgd/#2-stochastic-gradient-descent-sgd","title":"2. Stochastic Gradient Descent (SGD)","text":"<p>Instead of computing the full gradient, SGD uses a stochastic estimate \\(g_t\\) such that:</p> \\[ \\mathbb{E}[g_t \\mid x_t] = \\nabla f(x_t) \\] <p>The update rule becomes:</p> \\[ x_{t+1} = x_t - \\eta g_t \\] <p>where:</p> <ul> <li>\\(g_t\\) is a stochastic (noisy) gradient,  </li> <li>\\(\\eta\\) is the step size or learning rate.</li> </ul> <p>Intuition: We follow a \u201cnoisy compass\u201d that points roughly in the right direction instead of computing the exact gradient at every step.</p>"},{"location":"0m%20sgd/#21-sources-of-gradient-noise","title":"2.1 Sources of Gradient Noise","text":"<ul> <li>Finite datasets: Only a subset (mini-batch) of data is used.  </li> <li>Random sampling: Randomly select samples to approximate the full gradient.  </li> <li>Inherent stochasticity: In reinforcement learning or simulation-based optimization, gradients are intrinsically noisy.</li> </ul> <p>Example: Empirical Risk Minimization (ERM)</p> <p>For a dataset \\(\\{z_1, \\dots, z_n\\}\\) and loss function \\(\\ell(x; z_i)\\):</p> \\[ f(x) = \\frac{1}{n} \\sum_{i=1}^n \\ell(x; z_i) \\] <ul> <li>Exact gradient: \\(\\nabla f(x) = \\frac{1}{n} \\sum_i \\nabla \\ell(x; z_i)\\) </li> <li>SGD gradient: \\(g_t = \\nabla \\ell(x; z_{i_t})\\), where \\(i_t\\) is randomly sampled</li> </ul> <p>\u2705 Reduces per-step computation from \\(O(n)\\) to \\(O(1)\\) or \\(O(\\text{batch size})\\).</p>"},{"location":"0m%20sgd/#22-sgd-update-rule","title":"2.2 SGD Update Rule","text":"<ol> <li>Sample a stochastic gradient \\(g_t\\) (single sample or mini-batch).  </li> <li>Update:</li> </ol> \\[ x_{t+1} = x_t - \\eta g_t \\] <ul> <li>Converges in expectation under standard assumptions (convexity, bounded variance).  </li> <li>Simple, yet highly effective for large-scale optimization.</li> </ul>"},{"location":"0m%20sgd/#23-comparison-gd-vs-sgd","title":"2.3 Comparison: GD vs SGD","text":"Feature Gradient Descent Stochastic Gradient Descent Gradient Full / exact Noisy / approximate Step cost High (entire dataset) Low (single sample / mini-batch) Update direction Accurate Approximate, stochastic Trajectory Smooth Noisy, zig-zag Convergence Deterministic In expectation, slower per iteration, but scalable <p>Visual intuition: SGD zig-zags along the gradient surface but gradually converges toward a minimum.</p>"},{"location":"0m%20sgd/#3-practical-considerations","title":"3. Practical Considerations","text":""},{"location":"0m%20sgd/#31-step-size-learning-rate","title":"3.1 Step Size / Learning Rate","text":"<ul> <li>Constant or decaying \\(\\eta_t\\) </li> <li>Too large \u2192 divergence or oscillation  </li> <li>Too small \u2192 slow convergence</li> </ul>"},{"location":"0m%20sgd/#32-mini-batching","title":"3.2 Mini-batching","text":"<ul> <li>Trades off variance vs computation:  </li> <li>Smaller batches \u2192 noisier but cheaper updates  </li> <li>Larger batches \u2192 smoother updates but more computation</li> </ul>"},{"location":"0m%20sgd/#33-momentum-and-variants","title":"3.3 Momentum and Variants","text":"<ul> <li>Momentum, RMSProp, Adam: reduce stochastic noise, improve convergence.</li> </ul>"},{"location":"0m%20sgd/#34-noise-as-a-feature","title":"3.4 Noise as a Feature","text":"<ul> <li>Noise can help escape shallow local minima.  </li> <li>SGD is more robust in non-convex landscapes, e.g., deep neural networks.</li> </ul>"},{"location":"0m%20sgd/#4-convergence-guarantees","title":"4. Convergence Guarantees","text":"<p>For convex \\(f\\) with bounded gradient variance:</p> \\[ \\mathbb{E}[f(\\bar{x}_T)] - f(x^*) \\le O\\left(\\frac{1}{\\sqrt{T}}\\right) \\] <ul> <li>\\(\\bar{x}_T = \\frac{1}{T} \\sum_{t=1}^T x_t\\) </li> <li>\\(T\\) = number of iterations  </li> </ul> <p>Slower than exact GD (\\(O(1/T)\\) for smooth convex problems), but much cheaper per iteration.</p>"},{"location":"0m%20sgd/#5-example-stochastic-optimization-in-regression","title":"5. Example: Stochastic Optimization in Regression","text":"<p>Consider minimizing expected squared error:</p> \\[ \\min_x \\mathbb{E}_\\xi \\big[ (y - x^\\top \\xi)^2 \\big] \\] <p>Given a dataset \\(\\{(y_1, \\xi_1), \\dots, (y_n, \\xi_n)\\}\\), the empirical risk is:</p> \\[ \\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n (y_i - x^\\top \\xi_i)^2 \\]"},{"location":"0m%20sgd/#51-gradient-descent-gd","title":"5.1 Gradient Descent (GD)","text":"\\[ x_{t+1} = x_t - \\eta \\nabla \\hat{F}(x_t)  = x_t - \\eta \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(x_t) \\] <ul> <li>Full gradient requires a pass over all \\(n\\) samples \u2192 expensive for large datasets.</li> </ul>"},{"location":"0m%20sgd/#52-stochastic-gradient-descent-sgd","title":"5.2 Stochastic Gradient Descent (SGD)","text":"<p>Randomly sample an index \\(i_t \\in \\{1, \\dots, n\\}\\) and compute:</p> \\[ g_t = \\nabla f_{i_t}(x_t) \\] <p>Update rule:</p> \\[ x_{t+1} = x_t - \\eta_t g_t \\] <ul> <li>Key property:</li> </ul> \\[ \\mathbb{E}[g_t] = \\nabla \\hat{F}(x_t) \\] <p>Thus, \\(g_t\\) is an unbiased estimate of the true gradient.</p>"},{"location":"0m%20sgd/#6-randomized-coordinate-descent-rcd","title":"6. Randomized Coordinate Descent (RCD)","text":"<p>Another approach to reduce computational cost is Randomized Coordinate Descent (RCD). Instead of computing the full gradient, RCD updates only a randomly selected coordinate (or block of coordinates) at each iteration:</p> \\[ x_{t+1}^{(i)} = x_t^{(i)} - \\eta \\frac{\\partial f(x_t)}{\\partial x^{(i)}}, \\quad i \\sim \\text{Uniform}(\\{1, \\dots, d\\}) \\] <ul> <li>Key idea: Update only a subset of coordinates to reduce per-iteration cost from \\(O(d)\\) to \\(O(1)\\) (or \\(O(\\text{block size})\\)).  </li> <li>Connection to SGD: Both are stochastic approximations of full gradient descent:</li> <li>SGD introduces randomness via data sampling.  </li> <li>RCD introduces randomness via coordinate selection.  </li> </ul>"},{"location":"0m%20sgd/#61-benefits-of-rcd","title":"6.1 Benefits of RCD","text":"<ul> <li>Efficient for high-dimensional problems.  </li> <li>Exploits sparsity in variables.  </li> <li>Supports parallel and distributed updates through block-coordinate schemes.</li> </ul>"},{"location":"0m%20sgd/#62-variants","title":"6.2 Variants","text":"<ul> <li>Cyclic coordinate descent: systematically updates each coordinate in order.  </li> <li>Importance sampling: selects coordinates with probability proportional to the magnitude of partial derivatives to accelerate convergence.</li> </ul>"},{"location":"0m%20sgd/#63-comparison-to-gd-and-sgd","title":"6.3 Comparison to GD and SGD","text":"Method Gradient Computation Iteration Cost Convergence Behavior GD Full gradient High (\\(O(d)\\)) Deterministic, fast for small-scale problems SGD Stochastic gradient Low (\\(O(\\text{mini-batch})\\)) Noisy updates, scalable for large datasets RCD Partial gradient (coordinate) Very low (\\(O(1)\\) or block) Stochastic, efficient for high-dimensional sparse problems <p>RCD and SGD illustrate a unified principle: use a cheaper, stochastic approximation of the true gradient to achieve scalable optimization.</p>"},{"location":"0m%20sgd/#7-mini-batching-and-variance-reduction","title":"7. Mini-batching and Variance Reduction","text":""},{"location":"0m%20sgd/#71-mini-batch-sgd","title":"7.1 Mini-batch SGD","text":"<p>Compute the gradient over a small subset (mini-batch) of size \\(b\\):</p> \\[ g_t = \\frac{1}{b} \\sum_{i \\in \\mathcal{B}_t} \\nabla f_i(x_t) \\] <ul> <li>Benefits: </li> <li>Reduces variance of the gradient estimate.  </li> <li>Parallelizable on GPUs/TPUs.  </li> <li> <p>Smoother update trajectory than single-sample SGD.</p> </li> <li> <p>Trade-offs: </p> </li> <li>Smaller batch \u2192 cheaper but noisier updates.  </li> <li>Larger batch \u2192 more computation, lower variance.</li> </ul>"},{"location":"0m%20sgd/#72-variance-reduced-gradient-methods-svrg","title":"7.2 Variance-Reduced Gradient Methods (SVRG)","text":"<p>SVRG (Stochastic Variance Reduced Gradient) improves convergence by correcting stochastic gradients:</p> \\[ g_t = \\nabla f_{i_t}(x_t) - \\nabla f_{i_t}(\\tilde{x}) + \\nabla f(\\tilde{x}) \\] <p>where:</p> <ul> <li>\\(i_t\\) is a random index,  </li> <li> <p>\\(\\tilde{x}\\) is a reference point with precomputed full gradient \\(\\nabla f(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\tilde{x})\\).</p> </li> <li> <p>Benefits: </p> </li> <li>Faster convergence than standard SGD for strongly convex problems.  </li> <li>Maintains low per-iteration cost similar to SGD.  </li> <li> <p>Reduces stochastic oscillations by anchoring gradients around a reference.</p> </li> <li> <p>Intuition: Standard SGD \u201cwanders\u201d due to noise; SVRG periodically corrects it using a reference full gradient.</p> </li> </ul>"},{"location":"0n%20newtons/","title":"Newton's Method","text":""},{"location":"0n%20newtons/#newtons-method","title":"Newton\u2019s Method","text":"<p>Optimization algorithms can be broadly classified based on the type of information they use. Gradient Descent (GD) uses only first-order information (the gradient), while Newton\u2019s Method incorporates second-order information via the Hessian matrix, allowing it to adaptively rescale updates based on local curvature.</p>"},{"location":"0n%20newtons/#1-taylor-expansions-and-local-models","title":"1. Taylor Expansions and Local Models","text":""},{"location":"0n%20newtons/#11-gradient-descent-linear-approximation","title":"1.1 Gradient Descent \u2014 Linear Approximation","text":"<p>Gradient Descent is based on a first-order Taylor approximation of \\(f\\) around \\(x\\):</p> \\[ f(x + d) \\approx f(x) + \\nabla f(x)^\\top d \\] <p>To prevent uncontrolled steps, we regularize with a quadratic trust term:</p> \\[ \\min_d \\; \\nabla f(x)^\\top d + \\frac{1}{2\\eta}\\|d\\|^2 \\] <p>This yields the solution:</p> \\[ d = -\\eta \\nabla f(x), \\quad x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Thus, GD follows the steepest descent direction with respect to the Euclidean metric.</p>"},{"location":"0n%20newtons/#12-newtons-method-quadratic-approximation","title":"1.2 Newton\u2019s Method \u2014 Quadratic Approximation","text":"<p>Newton's Method instead constructs a second-order Taylor approximation:</p> \\[ f(x + d) \\approx f(x) + \\nabla f(x)^\\top d + \\frac{1}{2} d^\\top \\nabla^2 f(x) d \\] <p>Minimizing this quadratic model:</p> \\[ \\min_d \\; \\nabla f(x)^\\top d + \\frac{1}{2} d^\\top H(x) d \\quad \\text{with} \\quad H(x) = \\nabla^2 f(x) \\] <p>Setting derivative to zero gives:</p> \\[ H(x)d = -\\nabla f(x) \\quad \\Rightarrow \\quad d = -H(x)^{-1} \\nabla f(x) \\] <p>Update rule:</p> \\[ x_{t+1} = x_t - H(x_t)^{-1} \\nabla f(x_t) \\] <p>Newton\u2019s step directly targets the minimizer of the local quadratic model, adjusting the direction based on curvature.</p>"},{"location":"0n%20newtons/#2-smoothness-and-strong-convexity-assumptions","title":"2. Smoothness and Strong Convexity Assumptions","text":"<p>Assume:</p> <ul> <li>\\(f\\) is \\(\\beta\\)-smooth: \\(\\|\\nabla f(x) - \\nabla f(y)\\| \\le \\beta \\|x - y\\|\\) </li> <li>\\(f\\) is \\(\\alpha\\)-strongly convex: \\(f(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) + \\frac{\\alpha}{2}\\|y-x\\|^2\\)</li> </ul> <p>These assumptions enable convergence analysis for both GD and Newton\u2019s method.</p>"},{"location":"0n%20newtons/#3-convergence-rates","title":"3. Convergence Rates","text":""},{"location":"0n%20newtons/#31-gradient-descent-linear-convergence","title":"3.1 Gradient Descent \u2014 Linear Convergence","text":"<p>For smooth and strongly convex \\(f\\):</p> \\[ f(x_t) - f(x^*) \\le \\left(1 - \\frac{\\alpha}{\\beta}\\right)^t (f(x_0) - f(x^*)) \\] <ul> <li>Convergence is linear.</li> <li>Each iteration reduces the error by a constant factor.</li> </ul>"},{"location":"0n%20newtons/#32-newtons-method-quadratic-convergence","title":"3.2 Newton\u2019s Method \u2014 Quadratic Convergence","text":"<p>Assuming \\(f\\) has Lipschitz-continuous Hessian and \\(x_t\\) is sufficiently close to \\(x^*\\):</p> \\[ \\|x_{t+1} - x^*\\| \\le C \\|x_t - x^*\\|^2 \\] <ul> <li>Convergence is quadratic.</li> <li>The number of correct digits doubles after each iteration.</li> </ul>"},{"location":"0n%20newtons/#4-damped-vs-non-damped-newton-method","title":"4. Damped vs. Non-Damped Newton Method","text":""},{"location":"0n%20newtons/#41-classical-non-damped-newton","title":"4.1 Classical (Non-Damped) Newton","text":"<p>Uses the raw step:</p> \\[ x_{t+1} = x_t - H(x_t)^{-1} \\nabla f(x_t) \\] <ul> <li>Extremely fast near optimum.</li> <li>However, if \\(x_t\\) is far from \\(x^*\\), the quadratic model may be inaccurate, causing divergence.</li> </ul>"},{"location":"0n%20newtons/#42-damped-newton-method","title":"4.2 Damped Newton Method","text":"<p>To improve global stability, introduce a damping factor \\(\\lambda_t \\in (0,1]\\):</p> \\[ x_{t+1} = x_t - \\lambda_t H(x_t)^{-1} \\nabla f(x_t) \\] <ul> <li>\\(\\lambda_t\\) is often chosen via line search to ensure sufficient decrease.</li> <li>Damping makes Newton\u2019s method globally convergent, transitioning to full Newton steps when close to optimum.</li> </ul> <p>Insight: - Non-damped Newton is extremely fast but potentially unstable. - Damped Newton trades some speed for robustness during early iterations.</p>"},{"location":"0n%20newtons/#5-affine-transformation-perspective-geometry-matters","title":"5. Affine Transformation Perspective \u2014 Geometry Matters","text":"<p>A powerful perspective is to analyze both methods under an affine change of coordinates:</p> \\[ x = Ay + b, \\quad A \\in \\mathbb{R}^{d \\times d} \\text{ invertible} \\]"},{"location":"0n%20newtons/#effect-on-gradient-descent","title":"Effect on Gradient Descent","text":"<ul> <li>Gradient transforms as \\(\\nabla_y f = A^\\top \\nabla_x f\\).</li> <li>Update becomes dependent on the coordinate system.</li> <li>GD is not affine invariant \u2014 performance heavily depends on scaling and conditioning.</li> </ul>"},{"location":"0n%20newtons/#effect-on-newtons-method","title":"Effect on Newton\u2019s Method","text":"<ul> <li>Hessian transforms as \\(H_y = A^\\top H_x A\\).</li> <li>Update:</li> </ul> \\[ y_{t+1} = y_t - (A^\\top H_x A)^{-1} (A^\\top \\nabla_x f) \\] <p>which simplifies to the same geometric step as in original coordinates.</p> <p>Newton\u2019s Method is affine invariant \u2014 it adapts to the local curvature, effectively removing ill-conditioning.</p> <p>Interpretation:</p> <ul> <li>Gradient Descent uses isotropic steps \u2014 same metric in every direction.</li> <li>Newton Method uses the Hessian-induced metric, effectively rescaling space so level sets become spherical.</li> </ul>"},{"location":"0n%20newtons/#6-computational-trade-off-summary","title":"6. Computational Trade-Off Summary","text":"Method Local Model Metric Used Per-Step Cost Convergence Affine Invariance GD Linear Euclidean (\\(I\\)) \\(O(d)\\) Linear No Newton Quadratic Hessian-inverse (\\(H^{-1}\\)) \\(O(d^3)\\) Quadratic Yes Damped Newton Quadratic + Line Search Hessian-inverse \\(O(d^3)\\) + line search Quadratic near optimum, stable globally Yes"},{"location":"0n%20newtons/#7-strategic-use","title":"7. Strategic Use","text":"<ul> <li>Use GD/SGD for large-scale problems or as a warm start.</li> <li>Switch to (damped) Newton or quasi-Newton methods when close to optimal region.</li> <li>The Hessian captures intrinsic geometry, removing conditioning issues that slow down GD.</li> </ul>"},{"location":"0o%20quasi_n/","title":"Quasi-Newton Methods","text":""},{"location":"0o%20quasi_n/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Optimization often struggles not because of bad algorithms but because the geometry of the loss landscape is distorted \u2014 stretched, skewed, ill-conditioned.</p> <p>Newton\u2019s method fixes this by using curvature, but computing full Hessians is expensive.</p> <p>Quasi-Newton methods are a clever hack: They learn curvature from past gradients without ever computing second derivatives explicitly.</p>"},{"location":"0o%20quasi_n/#newtons-update","title":"Newton's Update","text":"<p>The classical Newton update is:</p> \\[ x_{t+1} = x_t - H^{-1}(x_t) \\, \\nabla f(x_t) \\] <ul> <li>\\(H(x_t)\\) is the Hessian (curvature matrix).</li> <li>Fast convergence but requires computing and inverting \\(H\\).</li> <li>For dimension \\(d\\), storing \\(H\\) costs \\(O(d^2)\\) and inverting costs \\(O(d^3)\\) \u2192 impractical at scale.</li> </ul>"},{"location":"0o%20quasi_n/#key-idea-of-quasi-newton-methods","title":"Key Idea of Quasi-Newton Methods","text":"<p>Instead of computing \\(H^{-1}\\), build an approximation, call it \\(B_t \\approx H^{-1}(x_t)\\).</p> <p>We extract curvature information from how gradients change after each step.</p> <p>Define:</p> <ul> <li>Step vector: </li> <li>Gradient change: </li> </ul> <p>Then we impose the secant condition:</p> \\[ B_{t+1} \\, y_t = s_t \\] <p>\ud83d\udcac Interpretation: \"If moving by \\(s_t\\) caused gradient to change by \\(y_t\\), then our internal curvature model should map \\(y_t \\mapsto s_t\\).\"</p> <p>This imitates Newton's relation without computing actual Hessians.</p>"},{"location":"0o%20quasi_n/#bfgs-the-core-quasi-newton-algorithm","title":"BFGS \u2014 The Core Quasi-Newton Algorithm","text":"<p>BFGS maintains and updates an approximation of the inverse Hessian.</p> <p>The update rule is:</p> \\[ B_{t+1} = \\left(I - \\frac{s_t y_t^\\top}{y_t^\\top s_t}\\right) B_t  \\left(I - \\frac{y_t s_t^\\top}{y_t^\\top s_t}\\right)  + \\frac{s_t s_t^\\top}{y_t^\\top s_t} \\] <p>Properties: - Rank-2 update \u2192 efficient - Preserves symmetry and positive definiteness - No Hessian needed \u2014 only gradients</p> <p>Update the parameters using:</p> \\[ x_{t+1} = x_t - B_{t+1} \\nabla f(x_t) \\]"},{"location":"0o%20quasi_n/#intuitive-geometry","title":"Intuitive Geometry","text":"<ul> <li>Gradient Descent assumes the world is a sphere \u2192 same learning rate in all directions.</li> <li>Newton knows the true ellipse shape of level sets and reshapes space so it becomes spherical.</li> <li>BFGS starts blind like GD but learns to reshape space gradually based on past gradient motion.</li> </ul> <p>Think of BFGS as an optimizer that reconstructs a mental map of terrain curvature from memory.</p>"},{"location":"0o%20quasi_n/#why-bfgs-works-memory-of-curvature","title":"Why BFGS Works \u2014 Memory of Curvature","text":"<p>Each \\((s_t, y_t)\\) pair captures 1D curvature information in the direction of motion.</p> <p>Storing many of these directions lets \\(B_t\\) approximate true curvature more and more accurately. Eventually, the loss surface feels spherical, and optimization becomes fast and direct.</p>"},{"location":"0o%20quasi_n/#l-bfgs-making-bfgs-scalable","title":"L-BFGS \u2014 Making BFGS Scalable","text":"<p>Storing full \\(B_t\\) takes \\(O(d^2)\\) memory \u2192 too large when \\(d\\) is in millions.</p> <p>Limited-memory BFGS (L-BFGS): - Keep only the last \\(m\\) pairs \\((s_t, y_t)\\) (with \\(m \\ll d\\)) - Reconstruct the vector product \\(B_t \\cdot \\nabla f(x_t)\\) using a two-loop recursion - Memory: \\(O(md)\\) instead of \\(O(d^2)\\)</p> <p>This makes L-BFGS practical for high-dimensional problems \u2014 used in: - Logistic regression - NLP models - Deep learning fine-tuning - SciPy &amp; PyTorch optimizers</p>"},{"location":"0o%20quasi_n/#comparison-table","title":"Comparison Table","text":"Method Memory Uses Hessian? Update Direction Convergence Speed Affine-Aware? Gradient Descent \\(O(d)\\) \u274c \\(-\\nabla f\\) Linear \u274c Newton \\(O(d^2)\\) \u2705 Full \\(-H^{-1} \\nabla f\\) Quadratic \u2705 BFGS \\(O(d^2)\\) \u2705 Approx. \\(-B_t \\nabla f\\) Superlinear \u2705 (approx) L-BFGS \\(O(md)\\) \u2705 Approx. (limited) Fast via recursion Superlinear \u2705 (approx)"},{"location":"0o%20quasi_n/#final-mental-model","title":"Final Mental Model","text":"<p>Gradient Descent: Walks downhill with fixed stride \u2014 doesn't care about terrain shape. Newton: Has a full curvature map \u2014 picks the fastest path, but expensive. BFGS/L-BFGS: Starts blind like GD but learns terrain structure from past steps, gradually adapting stride and direction like Newton \u2014 without ever seeing the actual Hessian.</p>"},{"location":"0q%20interior/","title":"Interior Point Methods","text":""},{"location":"0q%20interior/#motivation","title":"Motivation","text":"<p>We begin with the linear programming (LP) problem:</p> \\[ \\min_x \\; c^\\top x \\quad \\text{subject to} \\quad A x \\le b \\] <p>Equivalent standard form:</p> \\[ \\min_x \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\quad x \\ge 0 \\]"},{"location":"0q%20interior/#why-is-lp-so-powerful","title":"Why is LP so powerful?","text":"<ul> <li>LP appears in resource allocation, scheduling, routing, and many combinatorial optimization problems.</li> <li>If an optimal solution exists, it is always located at a vertex (extreme point) of the polytope defined by \\(A x \\le b\\).</li> <li>The feasible region \\(\\mathcal{P} = \\{ x \\in \\mathbb{R}^n \\mid A x \\le b \\}\\) is a polyhedron, which typically has many corners.</li> </ul>"},{"location":"0q%20interior/#extreme-points-and-combinatorial-explosion","title":"Extreme Points and Combinatorial Explosion","text":"<p>Let:</p> \\[ \\mathcal{P} = \\{x \\mid A x \\le b\\} \\] <ul> <li>With \\(m\\) inequality constraints in \\(\\mathbb{R}^n\\), the number of extreme points can be exponential in \\(m\\).</li> <li>Simple example: hypercube \\([0,1]^n\\) \u2014 it has \\(2^n\\) vertices.</li> <li>Thus, corner-based search (like Simplex) may require visiting many corners in the worst case.</li> </ul>"},{"location":"0q%20interior/#classical-methods-simplex-and-projected-gradient","title":"Classical Methods: Simplex and Projected Gradient","text":""},{"location":"0q%20interior/#1-projected-gradient-descent-for-lp","title":"1. Projected Gradient Descent for LP","text":"<p>Minimize a quadratic surrogate:</p> \\[ \\min_x \\|x - x_k\\|^2 \\quad \\text{subject to} \\quad A x \\le b \\] <p>Main challenge: projection onto polytope is expensive.</p>"},{"location":"0q%20interior/#2-simplex-method","title":"2. Simplex Method","text":"<ul> <li>Moves from corner to corner.</li> <li>Efficient in practice, but not polynomial-time guaranteed.</li> <li>Geometrically: slides along edges of polytope \u2014 stays on the boundary.</li> </ul>"},{"location":"0q%20interior/#motivation-for-interior-point-methods","title":"Motivation for Interior Point Methods","text":"<ul> <li>Simplex walks on the boundary, potentially traversing exponentially many vertices.</li> <li>Gradient projection struggles with feasibility projection.</li> <li>Interior Point Methods take a different path:</li> <li>They stay strictly inside the polytope.</li> <li>They follow a central trajectory rather than bouncing between corners.</li> <li>They use Newton\u2019s method on a barrier-regularized objective, giving both feasibility and fast convergence.</li> </ul> <p>n</p>"},{"location":"0q%20interior/#interior-point-methods-barrier-and-newton-fusion","title":"Interior Point Methods \u2014 Barrier and Newton Fusion","text":"<p>We now revisit the constrained problem:</p> \\[ \\min_x \\; f(x) \\quad \\text{subject to } A x \\le b \\] <p>Introduce the log-barrier for each constraint:</p> \\[ B(x) = -\\sum_{i=1}^m \\log(b_i - a_i^\\top x) \\] <p>Construct the barrier objective:</p> \\[ \\Phi_t(x) = t f(x) + B(x), \\quad t &gt; 0 \\] <p>Interpretation: As we increase \\(t\\), we push harder toward the true optimum, while the barrier keeps us in the interior.</p>"},{"location":"0q%20interior/#newton-method-applied-to-barrier-objective","title":"Newton Method Applied to Barrier Objective","text":"<p>Compute:</p> \\[ \\nabla \\Phi_t(x) = t \\nabla f(x) + \\sum_{i=1}^m \\frac{1}{b_i - a_i^\\top x} a_i \\] \\[ \\nabla^2 \\Phi_t(x) = t \\nabla^2 f(x) + \\sum_{i=1}^m \\frac{1}{(b_i - a_i^\\top x)^2} a_i a_i^\\top \\] <p>Newton Direction:</p> \\[ d = - \\left[\\nabla^2 \\Phi_t(x)\\right]^{-1} \\nabla \\Phi_t(x) \\] <p>Update with damping to keep strict feasibility:</p> \\[ x_{k+1} = x_k + \\lambda_k d \\quad \\text{such that } A x_{k+1} &lt; b \\]"},{"location":"0q%20interior/#interior-point-algorithm-flow","title":"Interior Point Algorithm Flow","text":"<p>``` Given strictly feasible x\u2080 and initial t: repeat:     Solve the barrier problem: minimize \u03a6\u209c(x) using damped Newton     Increase parameter t \u2190 \u03bc t  (\u03bc &gt; 1) until m / t &lt; \u03b5   # duality gap condition</p>"},{"location":"0ssc%20Epigraphs/","title":"Epigraphs","text":""},{"location":"0ssc%20Epigraphs/#epigraphs-and-convex-optimization","title":"Epigraphs and Convex Optimization","text":""},{"location":"0ssc%20Epigraphs/#1-definition-of-an-epigraph","title":"1. Definition of an Epigraph","text":"<p>For a function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the epigraph is the set of points lying on or above its graph:</p> \\[ \\operatorname{epi}(f) = \\{ (x, t) \\in \\mathbb{R}^n \\times \\mathbb{R} \\;\\mid\\; f(x) \\le t \\}. \\] <ul> <li>\\((x, t)\\) is a point in \\((n+1)\\)-dimensional space.  </li> <li>For each \\(x\\), the condition \\(f(x) \\le t\\) means \\(t\\) is at or above the function value.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#2-intuition","title":"2. Intuition","text":"<ul> <li>If you draw a 2D function \\(f(x)\\), the epigraph is the region above the curve.  </li> <li>For example, if \\(f(x) = x^2\\), then the epigraph is everything above the parabola.  </li> </ul> <p>So:  </p> <ul> <li>Graph = the curve itself.  </li> <li>Epigraph = the curve + everything above it.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#3-convexity-via-epigraphs","title":"3. Convexity via Epigraphs","text":"<p>A function \\(f\\) is convex if and only if its epigraph is a convex set.  </p> <ul> <li>A set is convex if the line segment between any two points in the set lies entirely within the set.  </li> <li>Geometrically: the \"roof\" (epigraph) above the function must form a bowl-shaped region, not a cave.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#4-examples","title":"4. Examples","text":"<ol> <li>Convex function: \\(f(x) = x^2\\) </li> <li>Epigraph = everything above the parabola.  </li> <li>This region is convex: if you connect any two points above the parabola, the line stays above the parabola.  </li> <li> <p>\u21d2 \\(f(x)\\) is convex.</p> </li> <li> <p>Non-convex function: \\(f(x) = -x^2\\) </p> </li> <li>Epigraph = everything above an upside-down parabola.  </li> <li>This region is not convex: connecting two points above the parabola can dip below it.  </li> <li>\u21d2 \\(f(x)\\) is not convex.</li> </ol>"},{"location":"0ssc%20Epigraphs/#5-why-epigraphs-matter-in-optimization","title":"5. Why Epigraphs Matter in Optimization","text":"<p>Many optimization problems can be written in epigraph form:</p> \\[ \\min_x f(x) \\quad \\equiv \\quad  \\min_{x,t} \\; t \\quad \\text{s.t. } f(x) \\le t. \\] <ul> <li>We \"lift\" the problem into one extra dimension.  </li> <li>The feasible region is the epigraph of \\(f\\).  </li> <li>Optimization over convex sets (epigraphs) is much more tractable.</li> </ul>"},{"location":"0ssc%20Epigraphs/#summary","title":"\u2705 Summary","text":"<ul> <li>The epigraph of a function is the region above its graph.  </li> <li>A function is convex iff its epigraph is convex.  </li> <li>Epigraphs let us reformulate optimization problems in a way that makes convexity clear and usable.  </li> </ul>"},{"location":"1_0_intro/","title":"Basics","text":""},{"location":"1_0_intro/#mathematics-for-convex-optimization","title":"Mathematics for Convex Optimization","text":"<p>Modern optimization is geometric at its core. When we minimize a loss function, we are navigating through a high-dimensional landscape defined by vectors, matrices, subspaces, projections, and curvature. Without a clear understanding of these structures, optimization algorithms can feel like black boxes. With the right intuition, gradient descent and its variants can be seen not merely as formulas, but as geometric motions toward feasibility and optimality. In convex optimization, problems are often expressed as minimizing a function over a vector space or a convex subset of it, subject to linear or convex constraints. Each mathematical concept we will cover \u2014 from vector spaces and inner products to convex sets and duality \u2014 connects directly to this geometric view of optimization.</p> <p>This section develops the mathematical foundations of convex optimization with an emphasis on geometric structure. Vectors and matrices are treated as operators that shape feasible regions and descent directions. Norms and inner products define the geometry in which distances and angles are measured, influencing step sizes and convergence criteria. Projections and orthogonality arise in constrained problems, dictating how we enforce feasibility. Smoothness and strong convexity introduce curvature that determines convergence rates of algorithms. Spectral properties such as eigenvalues and singular values capture conditioning and guide algorithmic design. By connecting each concept to its role in optimization, algorithms become intelligible not as abstract routines, but as geometric processes acting on structured spaces.</p> <p>We proceed through the prerequisite mathematics in a logical sequence. We begin with the basics of linear algebra \u2014 vector spaces, linear mappings, and subspaces \u2014 then introduce inner product spaces and the geometry of lengths, angles, and orthogonality. Next, we explore norms, unit balls, and dual norms, which measure distances and sizes of vectors. We then examine linear operators, eigenvalues, and positive definiteness, which provide insight into conditioning and curvature. With this linear algebra apparatus in hand, we move to multivariate calculus (gradients, Jacobians, Hessians) as tools for describing change and optimality. Building on that, we cover affine and convex sets (the geometry of feasible regions), convex functions and their analysis (including inequalities like Jensen\u2019s), and finally optimality conditions and duality. Throughout, the style prioritizes intuition and practical understanding over formal proofs, using visual metaphors and concrete examples to illustrate each concept. Graduate-level learners and ML practitioners should find a cohesive narrative that prepares them to understand and innovate in convex optimization.</p> <p>s.</p>"},{"location":"1_0_intro/#inner-product-spaces","title":"Inner Product Spaces","text":"<p>A vector space \\(V\\) with an inner product \\(\\langle \\cdot, \\cdot \\rangle\\) is called an inner product space. Inner product spaces provide a natural geometric structure for optimization by defining lengths, angles, and orthogonality. This structure generalizes Euclidean geometry to higher dimensions, function spaces, and weighted or structured metrics.  </p>"},{"location":"1_0_intro/#rank-nullspace-and-range","title":"Rank, Nullspace, and Range","text":"<p>Let \\(A \\in \\mathbb{R}^{m \\times n}\\).  </p> <ul> <li> <p>Range (column space): \\(\\text{range}(A) = \\{y \\in \\mathbb{R}^m : y = A x \\text{ for some } x \\in \\mathbb{R}^n\\}\\).   Represents all achievable outputs; its dimension is the rank of \\(A\\).  </p> </li> <li> <p>Nullspace (kernel): \\(\\text{null}(A) = \\{x \\in \\mathbb{R}^n : A x = 0\\}\\).   Directions along which \\(A\\) maps to zero; important in constrained optimization and redundancy analysis.  </p> </li> <li> <p>Rank-Nullity theorem: </p> </li> </ul> <p>Relevance: </p> <ul> <li>Nullspace directions indicate invariance in the objective.  </li> <li>Rank reveals independent features in data, affecting dimensionality reduction and regularization.  </li> <li>Full-rank matrices guarantee unique solutions; rank-deficient matrices imply multiple or no solutions.</li> </ul>"},{"location":"1_0_intro/#orthonormal-bases-and-qr-decomposition","title":"Orthonormal Bases and QR Decomposition","text":"<p>Orthonormal bases: \\(\\{q_1, \\dots, q_n\\}\\) satisfy \\(\\langle q_i, q_j \\rangle = \\delta_{ij}\\). They simplify computations, stabilize numerics, and enable straightforward projections: \\(P_W(x) = \\sum_i \\langle x, q_i \\rangle q_i\\).  </p> <p>Gram\u2013Schmidt process: Converts any linearly independent set \\(\\{v_1, \\dots, v_n\\}\\) into an orthonormal set by iterative projection subtraction.  </p> <p>QR decomposition: For full-rank \\(A \\in \\mathbb{R}^{m \\times n}\\): \\(A = Q R\\), with orthonormal \\(Q\\) and upper-triangular \\(R\\).  </p> <ul> <li>Overdetermined systems (\\(m &gt; n\\)): solve \\(R x = Q^\\top b\\) via back-substitution for least-squares solutions.  </li> <li>Underdetermined systems (\\(m &lt; n\\)): infinitely many solutions; regularization selects a meaningful solution.  </li> </ul> <p>Applications: </p> <ul> <li>Efficient solution of linear systems in regression.  </li> <li>Feature orthogonalization and conditioning improvement.  </li> <li>Low-dimensional representations in PCA.  </li> </ul> <p>Numerical considerations: </p> <ul> <li>Ill-conditioning occurs when columns are nearly dependent; quantified by condition number.  </li> <li>Mitigation: scale columns, use Modified Gram\u2013Schmidt or Householder reflections for stability.  </li> <li>Critical for iterative convex optimization algorithms, where small errors propagate quickly.</li> </ul>"},{"location":"1_0_intro/#inner-products-and-geometry","title":"Inner Products and Geometry","text":"<p>Understanding the geometry of vector spaces is essential for convex optimization. Inner products, norms, orthogonality, and projections provide the tools to analyze gradients, measure distances, and enforce constraints. This chapter develops these concepts rigorously, links them to optimization, and provides examples relevant to machine learning.</p>"},{"location":"1_0_intro/#inner-product-and-induced-norm","title":"Inner Product and Induced Norm","text":"<p>An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) satisfying:</p> <ul> <li>Symmetry: \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\) </li> <li>Linearity: \\(\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\\) </li> <li>Positive definiteness: \\(\\langle x, x \\rangle \\ge 0\\) with equality only if \\(x = 0\\)</li> </ul> <p>The Euclidean inner product is \\(\\langle x, y \\rangle = x^\\top y\\). From any inner product, we can define a norm:</p> \\[ \\|x\\| = \\sqrt{\\langle x, x \\rangle}. \\] <p>Cauchy\u2013Schwarz inequality states that for any \\(x, y \\in V\\):</p> \\[ |\\langle x, y \\rangle| \\le \\|x\\| \\, \\|y\\|. \\] <p>This inequality is fundamental for deriving bounds on gradient steps, dual norms, and subgradient inequalities in optimization.</p>"},{"location":"1_0_intro/#parallelogram-law-and-polarization-identity","title":"Parallelogram Law and Polarization Identity","text":"<p>A norm \\(\\|\\cdot\\|\\) induced by an inner product satisfies the parallelogram law:</p> \\[ \\|x + y\\|^2 + \\|x - y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2. \\] <p>Intuitively, this law describes a geometric property of Euclidean-like spaces: the sum of the squares of the diagonals of a parallelogram equals the sum of the squares of all sides. It captures the essence of inner-product geometry in terms of vector lengths.</p> <p>Conversely, any norm satisfying this law arises from an inner product via the polarization identity:</p> \\[ \\langle x, y \\rangle = \\frac{1}{4} \\left( \\|x + y\\|^2 - \\|x - y\\|^2 \\right). \\] <p>This provides a direct connection between norms and inner products, which is particularly useful when extending linear algebra and optimization concepts beyond standard Euclidean spaces.</p>"},{"location":"1_0_intro/#hilbert-spaces","title":"Hilbert Spaces","text":"<p>A Hilbert space is a complete vector space equipped with an inner product. Completeness here means that every Cauchy sequence (a sequence where vectors get arbitrarily close together) converges to a limit within the space. Hilbert spaces generalize Euclidean spaces to potentially infinite dimensions, allowing us to work with functions, sequences, or other objects as \u201cvectors.\u201d</p> <p>Intuition and relevance in machine learning:</p> <ul> <li>Many optimization algorithms are first formulated in finite-dimensional Euclidean spaces (\\(\\mathbb{R}^n\\)) but can be generalized to Hilbert spaces, enabling algorithms to handle infinite-dimensional feature spaces.</li> <li>Reproducing Kernel Hilbert Spaces (RKHS) are Hilbert spaces of functions with a kernel-based inner product. This allows kernel methods (e.g., SVMs, kernel ridge regression) to operate efficiently in very high- or infinite-dimensional spaces without explicitly computing high-dimensional coordinates (the \"kernel trick\").</li> <li>The properties like Cauchy\u2013Schwarz, parallelogram law, and induced norms remain valid in Hilbert spaces, which ensures that gradient-based and convex optimization methods can be extended to these functional spaces in a mathematically sound way.</li> </ul>"},{"location":"1_0_intro/#gram-matrices-and-least-squares-geometry","title":"Gram Matrices and Least-Squares Geometry","text":"<p>Given vectors \\(x_1, \\dots, x_n \\in \\mathbb{R}^m\\), the Gram matrix \\(G \\in \\mathbb{R}^{n \\times n}\\) is</p> \\[ G_{ij} = \\langle x_i, x_j \\rangle. \\] <p>i.e., it contains all pairwise inner products between the vectors.  </p> <p>Properties of the Gram Matrix:</p> <ul> <li>Symmetric and positive semidefinite: </li> <li>Rank: The rank of \\(G\\) equals the dimension of the span of \\(\\{x_1, \\dots, x_n\\}\\).  </li> <li>Geometric interpretation: \\(G\\) encodes the angles and lengths of the vectors, capturing their correlations and linear dependencies.</li> </ul>"},{"location":"1_0_intro/#connection-to-least-squares","title":"Connection to Least-Squares","text":"<p>In least-squares problems</p> \\[  X \\beta \\approx y,  \\] <p>the normal equations are</p> \\[  X^\\top X \\beta = X^\\top y. \\] <p>Here, \\(X^\\top X\\) is the Gram matrix of the columns of \\(X\\). Geometrically:</p> <ul> <li>\\(X^\\top X\\) measures how the features relate to each other via inner products.  </li> <li>Its eigenvalues determine the shape of the error surface in \\(\\beta\\)-space. If columns of \\(X\\) are nearly linearly dependent, the surface becomes elongated, like a stretched ellipse.  </li> <li>The condition number of \\(X^\\top X\\) (ratio of largest to smallest eigenvalue) quantifies this elongation:  </li> <li>High condition number (ill-conditioned): some directions in \\(\\beta\\)-space change very slowly under gradient-based updates, leading to slow convergence.  </li> <li>Low condition number (well-conditioned): all directions are updated more evenly, and convergence is faster.</li> </ul> <p>Implications: Preprocessing techniques such as feature scaling, orthogonalization (QR decomposition), or regularization improve the condition number and accelerate convergence of optimization algorithms.</p>"},{"location":"1_0_intro/#orthogonality-and-projections","title":"Orthogonality and Projections","text":"<p>Given a subspace \\(W \\subseteq V\\) with orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\), the projection of \\(x \\in V\\) onto \\(W\\) is</p> \\[ P_W(x) = \\sum_{i=1}^k \\langle x, q_i \\rangle q_i. \\] <p>Properties of projections:</p> <ul> <li>\\(x - P_W(x)\\) is orthogonal to \\(W\\): \\(\\langle x - P_W(x), y \\rangle = 0\\) for all \\(y \\in W\\) </li> <li>Projections are linear and idempotent: \\(P_W(P_W(x)) = P_W(x)\\)</li> </ul> <p>In optimization, projected gradient methods rely on computing \\(P_W(x - \\alpha \\nabla f(x))\\), ensuring iterates remain feasible within a subspace or convex set.</p> <p>Metric projections onto convex sets \\(C \\subseteq \\mathbb{R}^n\\) satisfy uniqueness and firm nonexpansiveness:</p> \\[ \\|P_C(x) - P_C(y)\\|^2 \\le \\langle P_C(x) - P_C(y), x - y \\rangle. \\] <p>This property guarantees algorithmic stability and is fundamental in projected gradient and proximal algorithms. Many convex optimization problems can be reformulated using proximal operators, which generalize metric projections. The firm nonexpansiveness of projections ensures that proximal iterations behave predictably and do not amplify errors. The projection can be thought of as \u201csnapping\u201d a point onto the feasible set in the most efficient way. Because of convexity, there\u2019s exactly one closest point, and the firm nonexpansiveness ensures that nearby points stay nearby after projection, which is essential for stable numerical algorithms.</p>"},{"location":"1_0_intro/#norms-and-unit-ball-geometry","title":"Norms and Unit-Ball Geometry","text":"<p>Norms induce metrics via \\(d(x, y) = \\|x - y\\|\\). Common examples:</p> <ul> <li>\\(\\ell_2\\) (Euclidean) norm: \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\) </li> <li>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\) </li> <li>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\)</li> </ul> <p>Unit-ball geometry affects optimization behavior. \\(\\ell_1\\) balls have corners promoting sparsity, while \\(\\ell_2\\) balls are smooth, influencing gradient descent and mirror descent choices. Dual norms are defined as</p> \\[ \\|y\\|_* = \\sup_{\\|x\\| \\le 1} \\langle y, x \\rangle. \\] <p>For \\(\\ell_p\\) norms, the dual norm is \\(\\ell_q\\) with \\(1/p + 1/q = 1\\). Dual norms underpin subgradient inequalities:</p> \\[ \\langle g, x - x^* \\rangle \\le \\|g\\|_* \\|x - x^*\\|. \\] <p>These inequalities are essential in primal\u2013dual and subgradient methods.</p>"},{"location":"1_0_intro/#summary-and-connections-to-optimization","title":"Summary and Connections to Optimization","text":"<ul> <li>Inner products define angles, lengths, and steepest descent directions.  </li> <li>The parallelogram law ensures norms arise from inner products in Hilbert spaces.  </li> <li>Gram matrices encode feature correlations and conditioning for least-squares problems.  </li> <li>Projections onto subspaces and convex sets enforce feasibility and stability in iterative algorithms.  </li> <li>Unit-ball geometry and dual norms influence step directions, sparsity, and convergence bounds.</li> </ul>"},{"location":"1_0_intro/#norms-and-metric-geometry","title":"Norms and Metric Geometry","text":"<p>Norms and metrics provide the mathematical framework to measure distances and sizes of vectors in optimization. They are central to analyzing convergence, defining constraints, and designing algorithms. This chapter develops the theory of norms, induced metrics, unit-ball geometry, and dual norms, emphasizing their role in convex optimization and machine learning.</p>"},{"location":"1_0_intro/#norms-and-induced-metrics","title":"Norms and Induced Metrics","text":"<p>A norm on a vector space \\(V\\) is a function \\(\\|\\cdot\\|: V \\to \\mathbb{R}\\) satisfying:</p> <ul> <li>Positive definiteness: \\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x = 0\\) </li> <li>Homogeneity: \\(\\|\\alpha x\\| = |\\alpha| \\|x\\|\\) for all \\(\\alpha \\in \\mathbb{R}\\) </li> <li>Triangle inequality: \\(\\|x + y\\| \\le \\|x\\| + \\|y\\|\\)</li> </ul> <p>Examples of common norms:</p> <ul> <li>\\(\\ell_2\\) norm: \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\) (Euclidean)  </li> <li>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\) </li> <li>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\) </li> <li>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left( \\sum_i |x_i|^p \\right)^{1/p}\\)</li> </ul> <p>A norm induces a metric (distance function) \\(d(x, y) = \\|x - y\\|\\). Metrics satisfy non-negativity, symmetry, and the triangle inequality. In optimization, metrics define step sizes, stopping criteria, and convergence guarantees.</p> <p>Example: In gradient descent with step size \\(\\alpha\\), the next iterate is</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k), \\] <p>and convergence is analyzed using \\(\\|x_{k+1} - x^*\\|\\) in the chosen norm.</p> <p>In an inner product space, the norm is induced by the inner product:  </p> \\[  \\|x\\| = \\sqrt{\\langle x, x \\rangle}.  \\] <p>This connection is fundamental in optimization: it defines the length of gradients, step sizes, and distances to feasible sets.  </p> <p>Cauchy\u2013Schwarz Inequality</p> <p>The Cauchy\u2013Schwarz inequality provides a key bound in inner product spaces: for all \\(x, y \\in V\\),  </p> \\[  |\\langle x, y \\rangle| \\le \\|x\\| \\, \\|y\\|.  \\] <p>Implications: </p> <ul> <li>Measures the maximum correlation between two directions; equality occurs when \\(x\\) and \\(y\\) are linearly dependent.  </li> <li>Ensures that the projection of one vector onto another does not exceed the product of their lengths: </li> <li>Forms the foundation for many optimization inequalities, bounds, and convergence analyses.  </li> </ul>"},{"location":"1_0_intro/#parallelogram-law-and-polarization-identity_1","title":"Parallelogram Law and Polarization Identity","text":"<p>A norm \\(\\|\\cdot\\|\\) induced by an inner product satisfies:  </p> \\[  \\|x + y\\|^2 + \\|x - y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2.  \\] <p>Conversely, any norm satisfying this law comes from an inner product via the polarization identity:  </p> \\[  \\langle x, y \\rangle = \\frac{1}{4} \\left( \\|x + y\\|^2 - \\|x - y\\|^2 \\right).  \\]"},{"location":"1_0_intro/#unit-ball-geometry-and-intuition","title":"Unit-Ball Geometry and Intuition","text":"<p>The unit ball of a norm \\(\\|\\cdot\\|\\) is</p> \\[ B = \\{ x \\in V \\mid \\|x\\| \\le 1 \\}. \\] <p>The unit-ball geometry of a norm becomes significant whenever the norm is used in an optimization problem, either as a regularizer or as a constraint. The unit ball represents the set of all points whose norm is less than or equal to one, and its shape provides deep geometric insight into how the optimizer can move and what kinds of solutions it prefers.  </p> <p>To understand its significance, consider the interaction between the level sets of the objective function and the shape of the unit ball. Level sets are contours along which the objective function has the same value. In unconstrained optimization without norms, the optimizer moves along the steepest descent of these level sets, and the solution is determined entirely by the objective\u2019s curvature and gradients. However, when a norm is added either as a constraint (e.g., \\(\\|x\\| \\le 1\\)) or as a regularization term (e.g., \\(\\lambda \\|x\\|\\)) the unit ball effectively modifies the landscape: it defines directions that are more expensive or restricted, and it interacts with the level sets to shape the solution path.  </p> <ul> <li>\\(\\ell_2\\) norm:   The unit ball is smooth and round. All directions are treated equally. Level sets intersect the ball symmetrically, allowing gradient steps to proceed smoothly in any direction. \\(\\ell_2\\) regularization encourages small but evenly distributed values across all coordinates, producing smooth solutions.  </li> <li>\\(\\ell_1\\) norm:   The unit ball has sharp corners along the coordinate axes. Level sets often intersect the corners, meaning some coordinates are driven exactly to zero. This produces sparse solutions, as the edges of the \\(\\ell_1\\) ball act like \u201cfunnels\u201d guiding the optimizer toward axes.  </li> <li>\\(\\ell_\\infty\\) norm:   The unit ball is a cube. It constrains the maximum magnitude of any coordinate, allowing free movement along directions that keep all components within bounds but blocking steps that exceed the faces. This is useful for preventing extreme values in any dimension.  </li> </ul>"},{"location":"1_0_intro/#dual-norms","title":"Dual Norms","text":"<p>In constrained optimization and duality theory, expressions like \\(\\langle y, x \\rangle\\) naturally appear\u2014often representing how a force (such as a gradient or dual variable) interacts with a feasible step direction. To measure how much influence such a vector \\(y\\) can exert when movement is restricted by a norm constraint, we define the dual norm:</p> \\[  \\|y\\|_* = \\sup_{\\|x\\| \\le 1} \\langle y, x \\rangle.  \\] <p>This definition asks:  </p> <p>If movement is only allowed within the unit ball of the original norm, what is the maximum directional effect that \\(y\\) can generate?</p>"},{"location":"1_0_intro/#intuition-movement-vs-influence","title":"Intuition \u2014 movement vs. influence","text":"<p>In constrained optimization, the primal norm defines where you are allowed to move, and the dual norm measures how effectively the gradient can move you within that region.</p> <ul> <li>The primal norm determines the shape of the feasible directions, forming a mobility region (for example, an \\(\\ell_2\\) ball is round and smooth, an \\(\\ell_1\\) ball is sharp and cornered).  </li> <li>The dual norm tells how much progress a gradient can make when pushing against that region.  </li> <li>If the gradient aligns with a flat face or a corner of the feasible region, movement becomes limited (as in \\(\\ell_1\\) geometry, leading to sparse solutions).  </li> <li>If the feasible region is smooth (as in \\(\\ell_2\\) geometry), the gradient can always push effectively, producing smooth updates.</li> </ul>"},{"location":"1_0_intro/#example-maximum-decrease-under-a-norm-constraint","title":"Example \u2014 maximum decrease under a norm constraint","text":"<p>Consider the constrained problem:</p> \\[  \\min_x f(x) \\quad \\text{subject to} \\quad \\|x\\| \\le 1.  \\] <p>For a small step \\(s\\), the change in \\(f\\) is approximately</p> \\[  f(x+s) \\approx f(x) + \\langle \\nabla f(x), s \\rangle.  \\] <p>To decrease \\(f\\), we want to minimize \\(\\langle \\nabla f(x), s \\rangle\\) over feasible steps \\(\\|s\\| \\le 1\\), or equivalently:</p> \\[  \\max_{\\|s\\| \\le 1} \\langle -\\nabla f(x), s \\rangle.  \\] <p>By definition of the dual norm, this maximum is exactly</p> \\[  \\max_{\\|s\\| \\le 1} \\langle -\\nabla f(x), s \\rangle = \\|\\nabla f(x)\\|_*. \\] <p>Intuition:</p> <ul> <li>The primal norm defines the feasible region of allowed steps.  </li> <li>The dual norm measures the largest possible influence the gradient can exert within that region.  </li> <li>If the unit ball is round (smooth), the gradient can push efficiently in any direction; if it has corners (as in \\(\\ell_1\\)), the gradient\u2019s effective action is concentrated along certain axes.  </li> </ul> <p>Thus, the dual norm is the true measure of how powerful a gradient or dual variable is under a constraint, not just its raw magnitude. It appears naturally in optimality conditions, Lagrangian duality, and subgradient methods, providing a precise bound on the effect of forces inside the feasible set.</p>"},{"location":"1_0_intro/#metric-properties-in-optimization","title":"Metric Properties in Optimization","text":"<p>Metrics derived from norms allow analysis of convergence rates. For an \\(L\\)-smooth function \\(f\\), the update \\(x_{k+1} = x_k - \\alpha \\nabla f(x_k)\\) satisfies</p> \\[ \\|x_{k+1} - x^*\\| \\le \\|x_k - x^*\\| - \\alpha \\Big(1 - \\frac{L \\alpha}{2}\\Big) \\|\\nabla f(x_k)\\|^2. \\] <p>This shows the choice of norm directly affects step size rules, stopping criteria, and algorithmic stability.</p> <p>Unit-ball shapes also influence proximal operators. For a regularizer \\(R(x) = \\lambda \\|x\\|_1\\), the proximal step shrinks components along the axes, exploiting the corners of the \\(\\ell_1\\) unit ball to enforce sparsity.</p>"},{"location":"1_0_intro/#norms-on-function-spaces-l_p-norms","title":"Norms on Function Spaces: \\(L_p\\) Norms","text":"<p>Function spaces generalize vector norms. For functions defined on an interval \\([a,b]\\), the \\(L_p\\) norm is</p> \\[ \\|f - g\\|_{L^p} = \\left( \\int_a^b |f(x) - g(x)|^p \\, dx \\right)^{1/p}, \\] <p>for \\(1 \\le p &lt; \\infty\\), and</p> \\[ \\|f - g\\|_{L^\\infty} = \\operatorname*{ess\\,sup}_{x \\in [a,b]} |f(x) - g(x)|. \\] <p>Interpretations:</p> <ul> <li>\\(L_1\\) measures total absolute discrepancy, the shaded area between graphs:    Small, widespread differences accumulate; a narrow spike of small width contributes little to \\(L_1\\).  </li> <li>\\(L_2\\) is the RMS or energy of the error:    Squaring amplifies large errors; a spike of height \\(A\\) and width \\(\\varepsilon\\) contributes about \\(A^2 \\varepsilon\\) to the squared norm.  </li> <li>\\(L_\\infty\\) is the worst-case deviation:    A single pointwise deviation, no matter how narrow, can dominate the norm. Use \\(L_\\infty\\) in robust optimization and adversarial settings.</li> </ul>"},{"location":"1_0_intro/#linear-operators-and-operator-norms","title":"Linear Operators and Operator Norms","text":"<p>Linear operators and their norms quantify how matrices or linear maps amplify vectors. Understanding these concepts is crucial for step size selection, conditioning, low-rank approximations, and stability of optimization algorithms. This chapter introduces operator norms, special cases, and the singular value decomposition, highlighting their role in convex optimization and machine learning.</p>"},{"location":"1_0_intro/#operator-norms","title":"Operator Norms","text":"<p>Let \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear operator (matrix). The operator norm induced by vector norms \\(\\|\\cdot\\|_p\\) and \\(\\|\\cdot\\|_q\\) is defined as</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q. \\] <p>Intuition: \\(\\|A\\|_{p \\to q}\\) measures the maximum amplification factor of a vector under \\(A\\) from \\(\\ell_p\\) space to \\(\\ell_q\\) space.</p> <p>Special cases:</p> <ul> <li>\\(\\|A\\|_{1 \\to 1} = \\max_{j} \\sum_{i} |A_{ij}|\\) (maximum absolute column sum)  </li> <li>\\(\\|A\\|_{\\infty \\to \\infty} = \\max_{i} \\sum_{j} |A_{ij}|\\) (maximum absolute row sum)  </li> <li>\\(\\|A\\|_{2 \\to 2} = \\sigma_{\\max}(A)\\), the largest singular value of \\(A\\) </li> </ul>"},{"location":"1_0_intro/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a singular value decomposition:</p> \\[ A = U \\Sigma V^\\top, \\] <p>where:</p> <ul> <li>\\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n}\\) are orthogonal matrices  </li> <li>\\(\\Sigma \\in \\mathbb{R}^{m \\times n}\\) is diagonal with non-negative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0\\), called singular values</li> </ul> <p>Interpretation:</p> <ul> <li>\\(V^\\top\\) rotates coordinates in the input space  </li> <li>\\(\\Sigma\\) scales each coordinate along its principal direction  </li> <li>\\(U\\) rotates coordinates in the output space</li> </ul> <p>Geometric intuition: The action of \\(A\\) on the unit sphere in \\(\\mathbb{R}^n\\) produces an ellipsoid in \\(\\mathbb{R}^m\\), with axes given by singular vectors and lengths given by singular values.</p>"},{"location":"1_0_intro/#applications-in-optimization-and-machine-learning","title":"Applications in Optimization and Machine Learning","text":"<ul> <li>Conditioning: The ratio \\(\\kappa(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)\\) determines sensitivity of linear systems \\(Ax = b\\) and least-squares problems to perturbations. Poor conditioning slows convergence.  </li> <li>Low-rank approximations: The best rank-\\(k\\) approximation of \\(A\\) (in Frobenius or spectral norm) is obtained by truncating its SVD. This is widely used in PCA and collaborative filtering.  </li> <li>Preconditioning: Linear transformations can be preconditioned using SVD to accelerate gradient-based methods.  </li> <li>Step amplification: For gradient descent on quadratic objectives \\(f(x) = \\frac{1}{2}x^\\top A x - b^\\top x\\), the largest singular value \\(\\sigma_{\\max}(A)\\) determines the safe step size \\(\\alpha \\le 2 / \\sigma_{\\max}(A)^2\\).</li> </ul>"},{"location":"1_0_intro/#numerical-considerations","title":"Numerical Considerations","text":"<ul> <li>Computing full SVD is expensive for large matrices; truncated SVD or randomized SVD is preferred in high dimensions.  </li> <li>Operator norms help identify directions of largest amplification and potential instability.  </li> <li>Column scaling or whitening of matrices improves conditioning and convergence of iterative optimization algorithms.</li> </ul>"},{"location":"1_0_intro/#summary-and-optimization-connections","title":"Summary and Optimization Connections","text":"<ul> <li>Operator norms quantify maximum amplification of vectors under linear transformations.  </li> <li>Singular values provide geometric insight into the shape of linear maps and determine condition numbers.  </li> <li>These tools are essential for step size selection, preconditioning, and low-rank modeling in convex optimization and ML.</li> </ul>"},{"location":"1_0_intro/#eigenvalues-symmetric-matrices-and-psdpd-matrices","title":"Eigenvalues, Symmetric Matrices, and PSD/PD Matrices","text":"<p>Eigenvalues and positive semidefinite/definite matrices play a central role in convex optimization. They provide insight into curvature, conditioning, step scaling, and uniqueness of solutions. This chapter develops the theory of eigenpairs, diagonalization of symmetric matrices, and properties of PSD/PD matrices, with direct applications to optimization algorithms and machine learning.</p>"},{"location":"1_0_intro/#eigenpairs-and-diagonalization-of-symmetric-matrices","title":"Eigenpairs and Diagonalization of Symmetric Matrices","text":"<p>Let \\(A \\in \\mathbb{R}^{n \\times n}\\). A scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue of \\(A\\) if there exists a nonzero vector \\(v \\in \\mathbb{R}^n\\) such that</p> \\[ A v = \\lambda v. \\] <p>The vector \\(v\\) is called an eigenvector associated with \\(\\lambda\\).</p> <p>For symmetric matrices \\(A = A^\\top\\), several important properties hold:</p> <ul> <li>All eigenvalues are real: \\(\\lambda_i \\in \\mathbb{R}\\) </li> <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal  </li> <li>\\(A\\) can be diagonalized by an orthogonal matrix \\(Q\\):    where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) and \\(Q^\\top Q = I\\).</li> </ul> <p>Geometric intuition: Symmetric matrices act as stretching or compressing along orthogonal directions, where the eigenvectors indicate the principal directions and eigenvalues the scaling factors.</p> <p>In optimization, the eigenvalues of the Hessian \\(\\nabla^2 f(x)\\) determine curvature along different directions. Positive eigenvalues indicate convexity along that direction, while negative eigenvalues indicate concavity.</p>"},{"location":"1_0_intro/#positive-semidefinite-and-positive-definite-matrices","title":"Positive Semidefinite and Positive Definite Matrices","text":"<p>A symmetric matrix \\(A\\) is positive semidefinite (PSD) if</p> \\[ x^\\top A x \\ge 0 \\quad \\forall x \\in \\mathbb{R}^n, \\] <p>and positive definite (PD) if</p> \\[ x^\\top A x &gt; 0 \\quad \\forall x \\neq 0. \\] <p>Equivalently:</p> <ul> <li>PSD: all eigenvalues \\(\\lambda_i \\ge 0\\) </li> <li>PD: all eigenvalues \\(\\lambda_i &gt; 0\\)</li> </ul> <p>Properties:</p> <ul> <li>\\(A \\succeq 0 \\implies\\) quadratic form \\(x^\\top A x\\) is convex  </li> <li>\\(A \\succ 0 \\implies\\) quadratic form is strictly convex with a unique minimizer</li> </ul> <p>In convex optimization, PD Hessians guarantee unique minimizers and enable Newton-type methods with reliable step scaling. PSD matrices appear in quadratic programming and covariance matrices in statistics and machine learning.</p>"},{"location":"1_0_intro/#spectral-connections-to-optimization","title":"Spectral Connections to Optimization","text":"<ol> <li>Step size selection: For gradient descent on \\(f(x) = \\frac{1}{2} x^\\top A x - b^\\top x\\), the largest eigenvalue \\(\\lambda_{\\max}(A)\\) determines the maximum safe step size \\(\\alpha \\le 2/\\lambda_{\\max}(A)\\).  </li> <li>Conditioning: The condition number \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\) controls the convergence rate of gradient descent and other iterative methods.  </li> <li>Curvature analysis: Eigenvectors of the Hessian indicate directions of fastest or slowest curvature, guiding preconditioning and variable rescaling.  </li> <li>Low-rank approximations: PSD matrices can be truncated along small eigenvalues to reduce dimensionality in ML applications such as PCA.</li> </ol>"},{"location":"1_0_intro/#projections-and-orthogonal-decompositions","title":"Projections and Orthogonal Decompositions","text":"<p>Projections and orthogonal decompositions are fundamental tools in convex optimization. They allow us to enforce constraints, compute optimality conditions, and analyze the geometry of feasible regions. This chapter develops projections onto subspaces and convex sets, orthogonal decomposition, and their connections to algorithms.</p>"},{"location":"1_0_intro/#projection-onto-subspaces","title":"Projection onto Subspaces","text":"<p>Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace with an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\). The projection of \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is</p> \\[ P_W(x) = \\sum_{i=1}^k \\langle x, q_i \\rangle q_i. \\] <p>Properties:</p> <ul> <li>\\(x - P_W(x)\\) is orthogonal to \\(W\\): \\(\\langle x - P_W(x), y \\rangle = 0\\) for all \\(y \\in W\\) </li> <li>\\(P_W\\) is linear and idempotent: \\(P_W(P_W(x)) = P_W(x)\\) </li> <li>\\(P_W(x)\\) minimizes the distance to \\(W\\): \\(\\|x - P_W(x)\\| = \\min_{y \\in W} \\|x - y\\|\\)</li> </ul> <p>Geometric intuition: \\(P_W(x)\\) is the \u201cshadow\u201d of \\(x\\) onto the subspace \\(W\\). In optimization, projections are used in projected gradient descent, where iterates are kept within a feasible linear subspace.</p>"},{"location":"1_0_intro/#projection-onto-convex-sets","title":"Projection onto Convex Sets","text":"<p>For a closed convex set \\(C \\subseteq \\mathbb{R}^n\\), the metric projection of \\(x\\) onto \\(C\\) is</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|. \\] <p>Properties:</p> <ul> <li>Uniqueness: There exists a unique minimizer \\(P_C(x)\\).  </li> <li>Firm nonexpansiveness: </li> <li>First-order optimality: \\(\\langle x - P_C(x), y - P_C(x) \\rangle \\le 0\\) for all \\(y \\in C\\)</li> </ul> <p>Applications in optimization:</p> <ul> <li>Ensures iterates remain feasible in projected gradient methods  </li> <li>Guarantees stability and convergence due to nonexpansiveness  </li> <li>Appears in proximal operators when \\(C\\) is a level set of a regularizer</li> </ul> <p>A useful corollary is nonexpansiveness: </p>"},{"location":"1_0_intro/#orthogonal-decomposition","title":"Orthogonal Decomposition","text":"<p>Any vector \\(x \\in \\mathbb{R}^n\\) can be uniquely decomposed into components along a subspace \\(W\\) and its orthogonal complement \\(W^\\perp\\):</p> \\[ x = P_W(x) + (x - P_W(x)), \\quad P_W(x) \\in W, \\quad x - P_W(x) \\in W^\\perp. \\] <p>Properties:</p> <ul> <li>The decomposition is unique.  </li> <li>\\(\\|x\\|^2 = \\|P_W(x)\\|^2 + \\|x - P_W(x)\\|^2\\) (Pythagoras\u2019 theorem).  </li> </ul> <p>Applications:</p> <ul> <li>In constrained optimization, the gradient can be decomposed into components tangent to constraints and normal to them, forming the basis for KKT conditions.  </li> <li>In machine learning, projections onto feature subspaces allow dimensionality reduction and orthogonalization of data.</li> </ul>"},{"location":"1_0_intro/#calculus-essentials-gradients-hessians-and-taylor-expansions","title":"Calculus Essentials: Gradients, Hessians, and Taylor Expansions","text":"<p>Calculus provides the foundation for optimization algorithms. Gradients indicate directions of steepest ascent or descent, Hessians encode curvature, and Taylor expansions give local approximations of functions. This chapter develops these concepts with an eye toward convex optimization and machine learning applications.</p>"},{"location":"1_0_intro/#gradient-and-directional-derivative","title":"Gradient and Directional Derivative","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be differentiable. The gradient of \\(f\\) at \\(x\\) is the vector</p> \\[ \\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}. \\] <p>Properties:</p> <ul> <li>Linear approximation: \\(f(x + h) \\approx f(x) + \\langle \\nabla f(x), h \\rangle\\) for small \\(h\\) </li> <li>Steepest ascent direction: \\(\\nabla f(x)\\) </li> <li>Steepest descent direction: \\(-\\nabla f(x)\\)</li> </ul> <p>The directional derivative of \\(f\\) at \\(x\\) in direction \\(u\\) is</p> \\[ D_u f(x) = \\lim_{t \\to 0} \\frac{f(x + t u) - f(x)}{t} = \\langle \\nabla f(x), u \\rangle. \\] <p>Optimization application: Gradient descent updates are</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k), \\] <p>where \\(\\alpha\\) is a step size, moving in the direction of steepest decrease.</p>"},{"location":"1_0_intro/#hessian-and-second-order-directional-derivatives","title":"Hessian and Second-Order Directional Derivatives","text":"<p>The Hessian of \\(f\\) at \\(x\\) is the symmetric matrix of second partial derivatives:</p> \\[ \\nabla^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}. \\] <p>Properties:</p> <ul> <li>Symmetric: \\(\\nabla^2 f(x) = (\\nabla^2 f(x))^\\top\\) </li> <li>Quadratic form: \\(u^\\top \\nabla^2 f(x) u\\) measures curvature along \\(u\\) </li> <li>Positive semidefinite Hessian \\(\\implies\\) local convexity</li> </ul> <p>Optimization application: Newton\u2019s method updates</p> \\[ x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k) \\] <p>use curvature information to accelerate convergence, especially near minimizers.</p>"},{"location":"1_0_intro/#taylor-expansions","title":"Taylor Expansions","text":"<p>The second-order Taylor expansion of \\(f\\) around \\(x\\) is</p> \\[ f(x + h) \\approx f(x) + \\langle \\nabla f(x), h \\rangle + \\frac{1}{2} h^\\top \\nabla^2 f(x) h. \\] <p>Properties:</p> <ul> <li>Linear term captures slope (first-order approximation).  </li> <li>Quadratic term captures curvature.  </li> <li>Provides local quadratic models used in Newton and quasi-Newton methods.  </li> </ul>"},{"location":"1_0_intro/#optimization-connections","title":"Optimization Connections","text":"<ul> <li>Gradients determine update directions in first-order methods.  </li> <li>Hessians determine curvature, step scaling, and Newton updates.  </li> <li>Taylor expansions provide local approximations, guiding line search and trust-region methods.  </li> <li>Eigenvalues of the Hessian determine convexity, step sizes, and conditioning.</li> </ul>"},{"location":"1_1_vector/","title":"1. Vector Spaces and Linear Mappings","text":"<p>At its core, linear algebra deals with vectors and matrices. A vector represents a direction and magnitude in space, while a matrix represents a linear transformation mapping vectors from one space to another. In optimization, model parameters, gradients, constraints, and data are all naturally represented as vectors or matrices. Linear algebra provides the rules for combining and transforming these objects, allowing us to describe feasible directions, transformations, and projections precisely.  </p> <p>Why emphasize linear structures? Linear transformations preserve combinations: if we know how a transformation acts on a set of basis vectors, we know how it acts on every vector formed from those bases. Even nonlinear problems behave approximately linearly when examined locally: around any smooth point, the function or constraint surface can be approximated by its tangent space, which captures the local linear behavior. A tangent space at a point on a manifold or constraint set is the vector space of directions in which infinitesimal movement remains feasible. For example, at a point on a spherical constraint, the tangent space is the plane that touches the sphere at that point. Many optimization algorithms, including gradient descent and projected methods, operate within or relative to such tangent spaces.  </p> <p>Vector spaces: A vector space \\(V\\) over \\(\\mathbb{R}\\) is a set of elements called vectors, equipped with two operations: vector addition and scalar multiplication. These operations satisfy the usual axioms (closure, associativity, commutativity, distributivity, existence of a zero vector and additive inverses, etc.). Intuitively, a vector space is a collection of objects that can be scaled and added like arrows in space. Key examples include:</p> <ul> <li>\\(\\mathbb{R}^n\\) (all length-\\(n\\) real coordinate vectors)</li> <li>the space of \\(m \\times n\\) real matrices</li> <li>the space of real-valued functions on a domain.</li> </ul> <p>Subspaces: A subspace \\(W \\subseteq V\\) is a subset of a vector space that is itself a vector space (closed under addition and scalar multiplication). For example, all vectors lying in a plane through the origin form a 2D subspace of \\(\\mathbb{R}^3\\). The concept of subspace formalizes \u201cfeasible directions\u201d or \u201cinvariant sets\u201d \u2014 if \\(W\\) is a subspace, any linear combination of vectors in \\(W\\) stays in \\(W\\). In optimization, subspaces often describe directions that satisfy certain linear constraints to first order.</p> <p>Linear combinations and span: Given vectors \\(v_1, \\dots, v_k \\in V\\), any vector of the form  (with scalars \\(\\alpha_i \\in \\mathbb{R}\\)) is a linear combination of those vectors. The set of all linear combinations is called their span:  </p> <p>Geometrically, the span is the flat (line, plane, or hyperplane through the origin) containing all combinations of \\(v_1,\\dots,v_k\\). For instance, in \\(\\mathbb{R}^3\\), the span of two non-collinear vectors is the plane through the origin that contains them.</p> <p>Linear independence and bases: Vectors \\(v_1,\\dots,v_k\\) are linearly independent if none of them can be expressed as a linear combination of the others. Equivalently, . A basis of a vector space \\(V\\) is a set of linearly independent vectors that spans \\(V\\). Every vector in \\(V\\) can be expressed uniquely as a combination of basis vectors. The number of vectors in any basis of \\(V\\) is the dimension of \\(V\\). For example, \\(\\mathbb{R}^n\\) has the standard basis of \\(n\\) unit vectors (each with a 1 in one coordinate and 0 in others); its dimension is \\(n\\). Choosing a basis lets us represent vectors by coordinates and perform computations. In optimization, the choice of basis can simplify problems \u2014 e.g., diagonalizing a quadratic form or aligning with constraint directions.</p> <p>Affine sets: An affine set is a geometric structure related to subspaces. Formally, \\(S \\subseteq \\mathbb{R}^n\\) is affine if for any \\(x,y\\in S\\), the line through \\(x\\) and \\(y\\) lies in \\(S\\). Equivalently, \\(S\\) is a translation of a subspace: \\(S = \\{x_0 + w : w \\in W\\}\\) for some subspace \\(W\\). A common description is \\(S = \\{x \\in \\mathbb{R}^n : Ax = b\\}\\) for some matrix \\(A\\) and vector \\(b\\). For example, a line not through the origin is an affine set (a translate of a 1-D subspace). Affine sets generalize feasible sets of systems of linear equations. In optimization, hyperplanes (defined by a single linear equation \\(a^T x = b\\)) and affine subspaces (solutions of \\(Ax=b\\)) describe constraints that form flat surfaces. These sets are important because many algorithms move along or project onto affine sets defined by active constraints.</p> <p>Linear mappings and matrices: A function \\(T: V \\to U\\) between vector spaces is linear if \\(T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y)\\) for all vectors \\(x,y \\in V\\) and scalars \\(\\alpha,\\beta\\). Linear mappings are precisely those we represent with matrices. If \\(x \\in \\mathbb{R}^n\\) and \\(A\\) is an \\(m\\times n\\) matrix, the product \\(Ax \\in \\mathbb{R}^m\\) is the image of \\(x\\) under the linear map defined by \\(A\\). Understanding the structure of a linear map (through its matrix) is crucial in optimization: constraints often take the form \\(Ax = b\\), and iterative algorithms repeatedly multiply by matrices (e.g. Hessians, Jacobians). Key subspaces associated with a matrix \\(A\\):</p> <ul> <li> <p>Range (Column space): \\(\\text{range}(A) = \\{y \\in \\mathbb{R}^m : y = Ax \\text{ for some } x\\},\\) the set of all outputs achievable by \\(A\\).This subspace is spanned by the columns of \\(A\\), and its dimension is called the rank of \\(A\\). Intuitively, rank is the number of independent directions in the output \u2014 it measures how much \\(A\\) can \u201cspread out\u201d or transform the input space.In data terms, if \\(A\\) is a feature matrix, \\(\\text{rank}(A)\\) is the number of linearly independent features.</p> </li> <li> <p>Nullspace (Kernel): \\(\\text{null}(A) = \\{x \\in \\mathbb{R}^n : A x = 0\\},\\) the set of inputs that map to the zero vector. This subspace contains the \u201cdirections of freedom\u201d that \\(A\\) kills off. Its dimension is \\(n - \\text{rank}(A)\\) by the rank-nullity theorem. In optimization, the nullspace of constraint matrices is critical: directions in the nullspace of \\(A\\) do not change \\(Ax\\), so they represent feasible directions for movement along linear constraints.Redundancies or dependencies in constraints show up as a nonzero nullspace.</p> </li> </ul> <p>Rank-nullity theorem: The rank-nullity theorem states: \\(\\dim(\\text{range}(A)) + \\dim(\\text{null}(A)) = n.\\) Full-rank matrices (rank \\(= n\\) for \\(A\\) mapping \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\)) have trivial nullspace (only \\(0\\)), which means \\(Ax = b\\) has at most one solution.If \\(A\\) is not full rank, either many solutions or none exist for \\(Ax = b\\), affecting existence and uniqueness of solutions in linear systems and optimization problems. </p> <p>Relevance to optimization: Understanding subspaces associated with matrices allows us to analyze constraints and optimality. Nullspace vectors correspond to feasible directions that keep \\(Ax=b\\) unchanged (important for Lagrange multipliers and KKT conditions), the range of \\(A^T\\) (the row space) is where multipliers live, rank reveals if constraints or features are redundant (rank deficiency) or well-posed. In linear programming and convex optimization, the concept of extreme points (see Chapter 9) is tied to linear independence of constraint normals (full row rank of \\(A\\)). In summary, vector spaces and linear mappings form the language in which feasible sets and update steps are described in convex optimization.</p>"},{"location":"1_2_innerproducts/","title":"2. Inner Product Spaces and Orthogonality","text":""},{"location":"1_2_innerproducts/#2-inner-product-spaces-and-orthogonality","title":"2. Inner Product Spaces and Orthogonality","text":"<p>Once we have vector spaces, the next ingredient is a way to measure angles and lengths. An inner product on a real vector space \\(V\\) is a function \\(\\langle \\cdot,\\cdot \\rangle: V \\times V \\to \\mathbb{R}\\) that is symmetric, bilinear, and positive-definite. For any \\(x,y,z \\in V\\) and scalar \\(\\alpha\\):  </p> <ul> <li>Symmetry: \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\)</li> <li>Linearity: \\(\\langle \\alpha x + y, z \\rangle = \\alpha \\langle x, z \\rangle + \\langle y, z \\rangle\\).  </li> <li>Positive-definiteness: \\(\\langle x, x \\rangle \\ge 0\\), with equality only if \\(x = 0\\).  </li> </ul> <p>The canonical example is the Euclidean inner product on \\(\\mathbb{R}^n\\): \\(\\langle x, y \\rangle = x^\\top y = \\sum_{i=1}^n x_i y_i\\). This yields the familiar length \\(\\|x\\|_2 = \\sqrt{x^\\top x}\\) and angle via \\(\\cos\\theta = \\frac{\\langle x,y\\rangle}{\\|x\\|\\|y\\|}\\). More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>Cauchy\u2013Schwarz inequality: Perhaps the most important inequality in an inner product space is Cauchy\u2013Schwarz:</p> \\[\\|\\langle x, y \\rangle\\| \\le \\|x\\| \\|y\\|\\] <p>with equality if and only if \\(x\\) and \\(y\\) are linearly dependent. Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. Cauchy\u2013Schwarz is ubiquitous in optimization, providing bounds on cosine similarity, error estimates, and is the basis for many other inequalities (like H\u00f6lder\u2019s inequality in Chapter 11). For example, it ensures that projecting one vector onto another (or onto a subspace) cannot increase its length.  </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Orthogonality and orthonormal bases: Two vectors \\(x,y\\) are orthogonal if \\(\\langle x,y\\rangle=0\\). A set of vectors \\(\\{q_1,\\dots,q_k\\}\\) is orthonormal if each \\(q_i\\) has unit length and they are mutually orthogonal: \\(\\langle q_i, q_j\\rangle = \\delta_{ij}\\) (1 if \\(i=j\\), else 0). Orthonormal vectors are by definition linearly independent. If an orthonormal set spans a subspace \\(W\\), it is the orthonormal basis of that subspace. Orthonormal bases greatly simplify computations because coordinates decouple. Any vector \\(x\\) can be projected onto \\(W\\) easily: the projection onto \\(W\\) is</p> \\[P_W(x)=\\sum_{i=1}^k \\langle x,q_i\\rangle q_i\\] <p>which is the closest vector in \\(W\\) to \\(x\\). Importantly, \\(x - P_W(x)\\) is orthogonal to \\(W\\), and \\(P_W\\) is the linear map that is idempotent (\\(P_W(P_W(x))=P_W(x)\\)). In optimization, projected gradient descent uses this operation to remain feasible: if we need \\(x_{k+1}\\) to lie in a subspace \\(W\\), we take a step \\(x_k - \\alpha \\nabla f(x_k)\\) and then project back to \\(W\\). </p> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). The Gram\u2013Schmidt process takes a linearly independent set \\(\\{v_1,\\dots,v_n\\}\\) and constructs orthonormal \\(q_1,\\dots,q_n\\) spanning the same subspace: one sets \\(q_1 = v_1/\\|v_1\\|\\), then subtracts out the \\(q_1\\) component from \\(v_2\\) and normalizes to get \\(q_2\\), and so on. Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\)), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"1_3_norms/","title":"3. Norms and Metric Geometry","text":"<p>Optimization problems require measuring sizes of objects: the length of a step, the size of a residual, the magnitude of a gradient, or the distance between two points or functions. A norm is a function \\(\\|\\cdot\\|: V \\to [0,\\infty)\\) that assigns a length to each vector, satisfying three properties:  </p> <ol> <li>Positive definiteness: \\(\\|x\\| \\ge 0\\) for all \\(x\\), and \\(\\|x\\| = 0\\) if and only if \\(x=0\\).  </li> <li>Homogeneity: \\(\\|\\alpha x\\| = |\\alpha|\\,\\|x\\|\\) for any scalar \\(\\alpha\\).  </li> <li>Triangle inequality: \\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) for all \\(x,y\\). (This generalizes the fact that any side of a triangle is no longer than the sum of the other two.)</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Triangle inequality and Minkowski sum: The triangle inequality implies that the set of points reachable by two steps (one in direction of \\(x\\), one in direction of \\(y\\)) is contained in a \u201csum\u201d of the balls around each. For \\(\\ell_2\\), this is the usual geometric triangle property; in general, norm balls are convex sets, and the triangle inequality says \\(B + B \\subset 2B\\) (where \\(B\\) is the unit ball). In analysis, the triangle inequality is also known as Minkowski\u2019s inequality for \\(L^p\\) spaces. A simple consequence in optimization is for any step \\(s\\), \\(\\|x+s-x^*\\|\\le \\|x-x^*\\|+\\|s\\|\\), which is used to bound progress of algorithms.</p> <p>Dual norms: Every norm has an associated dual norm defined by</p> <p>\\(\\|y\\|_* = \\sup_{\\|x\\|\\le1}\\langle y,x\\rangle\\) </p> <p>the maximum pairing of \\(y\\) with any unit vector in the original norm. Intuitively, if the primal norm defines what steps are allowed (the unit ball of \\(x\\)), the dual norm measures the \u201csteepness\u201d or influence of a linear direction \\(y\\) against that allowed region. For \\(\\ell_2\\), the dual is itself (\\(\\ell_2^* = \\ell_2\\)) since the sphere is centrally symmetric. For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\) (the maximum of components) \u2014 a gradient bounded in \\(\\ell_\\infty\\) norm means it has a limited sum of effects on each coordinate, aligning with \\(\\ell_1\\) feasible steps. Conversely \\(\\ell_\\infty\\)\u2019s dual is \\(\\ell_1\\). In general, the dual of \\(\\ell_p\\) is \\(\\ell_q\\) where \\(\\frac1p + \\frac1q = 1\\). The Cauchy\u2013Schwarz inequality is actually a special case: it tells us the dual of \\(\\ell_2\\) is \\(\\ell_2\\) by showing \\(\\langle x,y\\rangle \\le |x|2|y|2\\). H\u00f6lder\u2019s inequality (Chapter 11) generalizes this to \\(p,q\\) pairs. Dual norms appear naturally in optimality conditions: e.g. a subgradient inequality \\(\\langle g, x-x^*\\rangle \\le 0\\) can be bounded by \\(|g|* |x-x^*|\\), so if \\(|g|\\) is small, \\(x\\) is nearly optimal. Also, the concept of duality in optimization often pairs a primal problem (in \\(x\\)) with a dual variable \\(y\\) that lives in the dual norm space. For instance, gradient descent steps are often bounded by \\(|\\nabla f|\\) since we restrict \\(|x_{k+1}-x_k| \\le \\alpha\\) in the primal norm.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"1_4_linearoperator/","title":"4. Linear Operators, Spectral Norms, and SVD","text":"<p>Linear algebra not only studies vectors but also linear operators (matrices) that act on these vectors. In optimization, matrices represent constraint gradients, Hessians, update rules, etc., so understanding their action is crucial. We focus here on measuring how \u201cbig\u201d a linear operator is and how it distorts space, which leads to the ideas of operator norms and singular values.</p> <p>Operator norm: Given a matrix (linear map) \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and given a choice of vector norms on input and output, one can define the induced operator norm. If we use \\(|\\cdot|_p\\) on \\(\\mathbb{R}^n\\) and \\(|\\cdot|_q\\) on \\(\\mathbb{R}^m\\), the operator norm is</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q \\] <p>This gives the maximum factor by which \\(A\\) can stretch a vector (measuring \\(x\\) in norm \\(p\\) and \\(Ax\\) in norm \\(q\\)).pecial cases are common: with \\(p = q = 2\\), \\(|A|_{2 \\to 2}\\) (often just written \\(|A|_2\\)) is the spectral norm, which equals the largest singular value of \\(A\\) (more on singular values below). If \\(p = q = 1\\), \\(|A|_{1 \\to 1}\\) is the maximum absolute column sum of \\(A\\). If \\(p = q = \\infty\\), \\(|A|{\\infty \\to \\infty}\\) is the maximum absolute row sum.</p> <p>Operator norms tell us the worst-case amplification of signals by \\(A\\). In gradient descent on \\(f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x\\) (a quadratic form), the step size must be \\(\\le \\tfrac{2}{|A|_2}\\) for convergence; here \\(|A|_2 = \\lambda_{\\max}(A)\\) if \\(A\\) is symmetric (it\u2019s related to Hessian eigenvalues, Chapter 5). In general, controlling \\(|A|\\) controls stability: if \\(|A| &lt; 1\\), the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p> <p>Singular Value Decomposition (SVD): Any matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as</p> \\[ A = U \\Sigma V^\\top \\] <p>where \\(U \\in \\mathbb{R}^{m\\times m}\\) and \\(V \\in \\mathbb{R}^{n\\times n}\\) are orthogonal matrices (their columns are orthonormal bases of \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\), respectively), and \\(\\Sigma\\) is an \\(m\\times n\\) diagonal matrix with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0\\) on the diagonal. The \\(\\sigma_i\\) are the singular values of \\(A\\). Geometrically, \\(A\\) sends the unit ball in \\(\\mathbb{R}^n\\) to an ellipsoid in \\(\\mathbb{R}^m\\) whose principal semi-axes lengths are the singular values and directions are the columns of \\(V\\) (mapped to columns of \\(U\\)). The largest singular value \\(\\sigma_{\\max} = |A|_2\\) is the spectral norm. The smallest (if \\(n \\le m\\), \\(\\sigma{\\min}\\) of those \\(n\\)) indicates how \\(A\\) contracts the least \u2013 if \\(\\sigma_{\\min} = 0\\), \\(A\\) is rank-deficient.</p> <p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number \\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\) (when \\(\\sigma_{\\min}&gt;0\\)), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in \\(x\\)-space hardly change \\(Ax\\) (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means \\(A\\) is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of \\(A\\) (Eckart\u2013Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p> <p>Low-rank structure: The rank of \\(A\\) equals the number of nonzero singular values. If \\(A\\) has rank \\(r \\ll \\min(n,m)\\), it means \\(A\\) effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem\u2019s data matrix \\(A\\) is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p> <p>Operator norm in optimization: Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\), the Hessian is \\(Q\\) and gradient descent converges if \\(\\alpha &lt; 2/\\lambda_{\\max}(Q)\\). Preconditioning aims to transform \\(Q\\) into one with a smaller condition number by multiplying by some \\(P\\) (like using \\(P^{-1}Q\\)) \u2014 effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex \\(f\\), the Lipschitz constant of \\(\\nabla f\\) (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p> <p>Summary of spectral properties:</p> <ul> <li> <p>The spectral norm \\(|A|_2 = \\sigma_{\\max}(A)\\) quantifies the largest stretching. It determines stability and step sizes.</p> </li> <li> <p>The smallest singular value \\(\\sigma_{\\min}\\) (if \\(A\\) is tall full-rank) tells if \\(A\\) is invertible and how sensitive the inverse is. If \\(\\sigma_{\\min}\\) is tiny, small changes in output cause huge changes in solving \\(Ax=b\\).</p> </li> <li> <p>The condition number \\(\\kappa = \\sigma_{\\max}/\\sigma_{\\min}\\) is a figure of merit for algorithms: gradient descent iterations needed often scale with \\(\\kappa\\) (worse conditioning = slower). Regularization like adding \\(\\mu I\\) increases \\(\\sigma_{\\min}\\), thereby reducing \\(\\kappa\\) and accelerating convergence (at the expense of bias).</p> </li> <li> <p>Nuclear norm (sum of singular values) and spectral norm often appear in optimization as convex surrogates for rank and as constraints to limit the operator\u2019s impact.</p> </li> </ul> <p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p>"},{"location":"1_5_eigenvalues/","title":"5. Eigenvalues, Positive Definiteness, and Quadratic Forms","text":"<p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>Eigenvalues and eigenvectors: For a square matrix \\(A \\in \\mathbb{R}^{n\\times n}\\), an eigenvector \\(v \\neq 0\\) and scalar eigenvalue \\(\\lambda\\) satisfy \\(A v = \\lambda v\\). In general \\(A\\) might be defective or complex eigenvalues might appear, but for symmetric matrices \\(A = A^T\\), a beautiful theory simplifies things: all eigenvalues are real, and there is an orthonormal basis of \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A\\). Thus we can write</p> \\[ A = Q \\Lambda Q^\\top \\] <p>with \\(Q\\) orthogonal and \\(\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)\\). This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(A\\) is positive semidefinite (PSD), written \\(A \\succeq 0\\), if \\(x^T A x \\ge 0\\) for all \\(x\\). It\u2019s positive definite (PD), \\(A \\succ 0\\), if \\(x^T A x &gt; 0\\) for all \\(x\\neq0\\). In terms of eigenvalues, \\(A \\succeq 0\\) iff all \\(\\lambda_i \\ge 0\\), and \\(A \\succ 0\\) iff all \\(\\lambda_i &gt; 0\\). PSD matrices generalize the notion of a nonnegative scalar. They define convex quadratic forms: \\(x^T A x\\) is a convex function of \\(x\\) if \\(A\\) is PSD. In fact, a twice-differentiable function \\(f\\) is convex on a region if and only if its Hessian is PSD at every point in that region. So Hessian eigenvalues \\(\\lambda_{\\min}(\\nabla^2 f) \\ge 0\\) for all points is a certificate of convexity.</p> <p>Some important properties and examples:</p> <ul> <li> <p>Identity matrix \\(I\\) has all eigenvalues 1, is PD. Scaling \\(I\\) by \\(\\alpha&gt;0\\) yields eigenvalues \\(\\alpha\\), still PD.</p> </li> <li> <p>Diagonal matrices have eigenvalues equal to their diagonal entries (so easy to check PSD by seeing if all diagonal are nonnegative, provided matrix is diagonal in some basis).</p> </li> <li> <p>Covariance matrices (like \\(X^T X\\) or the Gram matrix \\(G\\)) are PSD: for any \\(z\\), \\(z^T X^T X z = |Xz|^2 \\ge 0\\). In ML, covariance being PSD corresponds to variance being nonnegative.</p> </li> <li> <p>Laplacian matrices in graphs are PSD, which connects to convex quadratic energies.</p> </li> </ul> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p> <p>In summary, eigenvalues and definiteness connect algebra to geometry: they tell us if a quadratic bowl opens upward (convex) or downward, how elongated or skewed that bowl is, and whether an optimization problem has a unique solution. Much of convex optimization theory, especially duality and KKT conditions, assumes convexity which can often be verified via positive semidefiniteness of certain matrices (Hessians or constraint Hessians in second-order cone programs, etc.). We often constrain matrices to be PSD in semidefinite programming, which is essentially convex optimization in the space of eigenvalues.</p>"},{"location":"1_6_projections/","title":"6. Projections onto Subspaces and Convex Sets","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"1_7_calculus/","title":"7. Calculus Essentials -  Gradients, Jacobians, and Hessians","text":"<p>Calculus provides the analytical tools to optimize functions: it describes how functions change when we tweak the input. In convex optimization we often assume differentiability (at least for the objective, if not constraints), so we rely on gradients and Hessians to characterize optimal points and design algorithms.</p> <p>Gradient and directional derivative: Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be differentiable. The gradient \\(\\nabla f(x)\\) is the vector of partial derivatives</p> \\[ \\nabla f(x) = \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] <p>so that for a small step \\(h\\), \\(f(x+h) \\approx f(x) + \\langle \\nabla f(x),h\\rangle\\). This linear approximation is the first-order Taylor expansion. The gradient \\(\\nabla f(x)\\) points in the direction of steepest increase of \\(f\\); \\(-\\nabla f(x)\\) is the direction of steepest decrease. Specifically, \\(\\nabla f(x)\\) is orthogonal to level sets of \\(f\\) at \\(x\\). In optimization, setting \\(\\nabla f(x) = 0\\) finds stationary points (candidates for optima). Gradient descent uses the update \\(x_{k+1} = x_k - \\alpha \\nabla f(x_k)\\), taking a small step opposite the gradient to reduce \\(f\\). The magnitude \\(|\\nabla f(x)|\\) indicates how steep \\(f\\) is; when \\(\\nabla f(\\hat{x})=0\\), the function is flat to first order at \\(\\hat{x}\\). For convex \\(f\\), any stationary point is a global minimum.</p> <p>The directional derivative in direction \\(u\\) is \\(D_u f(x) = \\lim_{t\\to0} \\frac{f(x+tu)-f(x)}{t} = \\langle \\nabla f(x), u\\rangle\\). This shows how the gradient inner product with \\(u\\) gives the instantaneous rate of change of \\(f\\) along \\(u\\). In particular, \\(D_u f(x)\\) is maximized when \\(u\\) points along \\(\\nabla f(x)\\) (steepest ascent) and minimized when \\(u\\) is opposite.</p> <p>Jacobian for vector-valued mappings: If \\(g: \\mathbb{R}^n \\to \\mathbb{R}^m\\) (with \\(m&gt;1\\) outputs), the Jacobian matrix \\(J_g(x)\\) is the \\(m \\times n\\) matrix of partial derivatives: its \\((i,j)\\) entry is \\(\\partial g_i/\\partial x_j\\). The \\(i\\) th row is \\((\\nabla g_i(x))^T\\). For example, if \\(g(x) = Ax\\) (linear map), then \\(J_g(x)=A\\) constant. If \\(g(x) = (f(x), h(x))\\) combines two scalars, the Jacobian has two rows: \\(\\nabla f(x)^T\\) and \\(\\nabla h(x)^T\\). The Jacobian represents the best linear approximation of \\(g\\) near \\(x\\): \\(g(x+h) \\approx g(x) + J_g(x),h\\). When \\(m=n\\) and \\(J_g(x)\\) is invertible, \\(g\\) is locally invertible (by the Inverse Function Theorem) and the Jacobian\u2019s determinant indicates how volumes scale under \\(g\\). In optimization, Jacobians appear in constraints: if we have vector constraints \\(g(x)=0\\), \\(J_g(x)\\) is the constraint Jacobian matrix used in KKT conditions. They also appear when optimizing compositions of functions (via chain rule, below). In machine learning, the Jacobian of a network\u2019s layers is used to propagate gradients backward (backpropagation is an application of chain rule on a composed function).</p> <p>Chain rule: If \\(h(x) = f(g(x))\\) is a composition \\(\\mathbb{R}^n \\xrightarrow{g} \\mathbb{R}^m \\xrightarrow{f} \\mathbb{R}\\), then by the chain rule the gradient is</p> \\[ \\nabla h(x) = J_g(x)^\\top \\, \\nabla f(g(x)) \\] <p>In coordinates, \\(\\frac{\\partial h}{\\partial x_j} = \\sum_{i=1}^m \\frac{\\partial f}{\\partial y_i}(g(x)) \\frac{\\partial g_i}{\\partial x_j}(x)\\). This general rule shows that to compute the gradient of a nested function, we multiply the Jacobians going backward. For example, if \\(f(y)\\) is scalar and \\(g(x)\\) yields features, \\(\\nabla_x f(g(x)) = J_g(x)^T \\nabla f(y)|_{y=g(x)}\\). This is exactly how backpropagation in neural networks works: the gradient w.r.t. inputs is obtained by propagating the output error gradient through each layer\u2019s Jacobian (which are often simple elementwise operations or linear weight matrices). Thus, the chain rule is fundamental for efficient gradient calculations. In convex optimization, if \\(g(x)\\) is an affine function and \\(f\\) is convex and differentiable, then \\(h(x)=f(g(x))\\) is convex and \\(\\nabla h(x) = J_g(x)^T \\nabla f(g(x))\\) provides the needed gradient for algorithms.</p> <p>Hessian and second-order derivatives: The Hessian of \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is the \\(n \\times n\\) symmetric matrix of second partials, \\(\\nabla^2 f(x)\\), where \\((\\nabla^2 f(x))_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\). The Hessian matrix captures the quadratic curvature of \\(f\\) around \\(x\\). Specifically, the second-order Taylor expansion is</p> <p>\ud835\udc53 ( \ud835\udc65 + \u210e ) \u2248 \ud835\udc53 ( \ud835\udc65 ) + \u27e8 \u2207 \ud835\udc53 ( \ud835\udc65 ) , \u210e \u27e9 + 1 2 \u210e \ud835\udc47 ( \u2207 2 \ud835\udc53 ( \ud835\udc65 ) ) \u2009 \u210e . f(x+h)\u2248f(x)+\u27e8\u2207f(x),h\u27e9+ 2 1     \u200b</p> <p>h T (\u2207 2 f(x))h.</p> <p>The quadratic term \\(h^T \\nabla^2 f(x) h / 2\\) approximates how the gradient itself changes with \\(h\\). Properties of Hessian: if \\(\\nabla^2 f(x) \\succeq 0\\) (PSD) for all \\(x\\) in a region, \\(f\\) is convex on that region. If \\(\\nabla^2 f(x) \\succ 0\\) (PD) for all \\(x\\), \\(f\\) is strictly convex (one minimizer). On the other hand, if \\(\\nabla^2 f(x)\\) has a negative eigenvalue, \\(f\\) is locally concave in that direction (not convex). Thus Hessian definiteness is a local convexity test. Many convex functions have constant Hessians (e.g. \\(f(x)=\\frac{1}{2}x^TQx\\) has \\(\\nabla^2 f = Q\\)).</p> <p>In optimization algorithms, Hessians are used in Newton\u2019s method, which iteratively updates</p> <p>\ud835\udc65 \ud835\udc58 + 1 = \ud835\udc65 \ud835\udc58 \u2212 [ \u2207 2 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 ) ] \u2212 1 \u2009 \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 ) . x k+1     \u200b</p> <p>=x k     \u200b</p> <p>\u2212[\u2207 2 f(x k     \u200b</p> <p>)] \u22121 \u2207f(x k     \u200b</p> <p>).</p> <p>This uses the Hessian inverse as a linear approximation to the curvature, jumping to where the gradient would be zero if the quadratic model were exact. Newton\u2019s method converges in a few iterations for quadratic objectives and generally superlinearly for well-behaved convex functions, but it requires solving linear systems involving \\(\\nabla^2 f(x)\\), which can be expensive for large \\(n\\). Quasi-Newton methods (like BFGS) build approximations to the Hessian on the fly. Regardless, understanding Hessian is crucial for high-dimensional convex optimization: it tells us how sensitive the gradient is to changes in \\(x\\), which directly affects step sizes and convergence.</p> <p>Example \u2013 quadratic function: \\(f(x) = \\frac{1}{2}x^TQx - b^T x\\). Here \\(\\nabla f(x) = Qx - b\\) (linear), and \\(\\nabla^2 f(x) = Q\\). Solving \\(\\nabla f=0\\) yields \\(Qx=b\\), so if \\(Q \\succ 0\\) the unique minimizer is \\(x^* = Q^{-1}b\\). The Hessian being \\(Q \\succ 0\\) confirms convexity. If \\(Q\\) has large eigenvalues, gradient \\(Qx - b\\) changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of \\(Q\\) controls difficulty of minimizing \\(f\\).</p> <p>Optimality conditions (unconstrained): For an unconstrained differentiable problem \\(\\min_x f(x)\\), the first-order necessary condition is \\(\\nabla f(x^) = 0\\). If \\(f\\) is convex, this is also sufficient: any \\(x\\) with \\(\\nabla f(x)=0\\) is a global minimizer. If \\(f\\) is twice differentiable, second-order conditions say: \\(\\nabla f(x^)=0\\) and \\(\\nabla^2 f(x^*) \\succeq 0\\) for a local minimum. In convex problems the Hessian condition is automatically satisfied everywhere (since convex \\(f\\) has PSD Hessian throughout), so checking \\(\\nabla f(x)=0\\) is enough.</p> <p>Gradient Lipschitz continuity: A concept often used in convergence analysis is Lipschitz continuity of the gradient. If there exists \\(L\\) such that \\(|\\nabla f(x) - \\nabla f(y)| \\le L |x-y|\\) for all \\(x,y\\), we say the gradient is \\(L\\)-Lipschitz (or \\(f\\) is \\(L\\)-smooth). \\(L\\) is essentially an upper bound on the Hessian eigenvalues (for \\(\\ell_2\\) norm): \\(L \\ge \\lambda_{\\max}(\\nabla^2 f(x))\\) for all \\(x\\). Smoothness is important because it ensures gradient descent with step \\(\\alpha = 1/L\\) converges, and it gives a bound \\(f(x_{k+1}) \\le f(x_k) - \\frac{1}{2L}|\\nabla f(x_k)|^2\\) (so the function value decreases at least proportionally to the squared gradient norm). Many convex functions in optimization are \\(L\\)-smooth (e.g. quadratic forms with \\(\\lambda_{\\max}(Q)=L\\)). Smoothness together with strong convexity (defined shortly) yields linear convergence rates for gradient descent.</p> <p>Strong convexity: A differentiable function \\(f\\) is \\(\\mu\\)-strongly convex if \\(f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2}|y-x|^2\\) for all \\(x,y\\). Equivalently, \\(f(x) - \\frac{\\mu}{2}|x|^2\\) is convex, which implies \\(\\nabla^2 f(x) \\succeq \\mu I\\) (Hessian bounded below by \\(\\mu\\)) when \\(f\\) is twice differentiable. Strong convexity means \\(f\\) has a quadratic curvature of at least \\(\\mu\\) \u2013 it grows at least as fast as a parabola. Strongly convex functions have unique minimizers (the bowl can\u2019t flatten out). They also yield much faster convergence: for \\(\\mu\\)-strongly convex and \\(L\\)-smooth \\(f\\), gradient descent with \\(\\alpha=1/L\\) converges like \\((1-\\mu/L)^k\\) (linear rate). Intuitively, the condition number \\(\\kappa = L/\\mu\\) comes into play. Examples: the quadratic form above is strongly convex with \\(\\mu = \\lambda_{\\min}(Q)\\). Adding a small ridge term \\(\\frac{\\mu}{2}|x|^2\\) to any convex \\(f\\) makes it \\(\\mu\\)-strongly convex and improves conditioning at the cost of bias.</p> <p>In summary, the tools of calculus \u2014 gradients for direction of improvement, Hessians for curvature, Jacobians for constraint and composite mappings, and inequalities like Lipschitz bounds \u2014 all feed into understanding and solving convex optimization problems. The optimality conditions formalize the simple idea: at optimum, the gradient must vanish or be balanced by constraints. The next chapter will build on this by considering those constraints explicitly and introducing Lagrange multipliers and duality, giving deeper insight into optimality in constrained problems.</p>"},{"location":"1a%20LP/","title":"Linear Programming","text":""},{"location":"1a%20LP/#linear-programming-lp-problem","title":"Linear Programming (LP) Problem","text":"<p>Linear Programming (LP) is a cornerstone of convex optimization. It is used to find a decision vector \\(x\\) that minimizes a linear objective function subject to linear constraints:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where: - \\(x \\in \\mathbb{R}^n\\) \u2014 decision variables, - \\(c \\in \\mathbb{R}^n\\) \u2014 cost vector, - \\(d \\in \\mathbb{R}\\) \u2014 constant offset (shifts objective but does not affect optimizer), - \\(G \\in \\mathbb{R}^{m \\times n}, \\; h \\in \\mathbb{R}^m\\) \u2014 inequality constraints, - \\(A \\in \\mathbb{R}^{p \\times n}, \\; b \\in \\mathbb{R}^p\\) \u2014 equality constraints.  </p>"},{"location":"1a%20LP/#why-lps-are-convex-optimization-problems","title":"Why LPs Are Convex Optimization Problems","text":"<p>A problem is convex if: 1. The objective is convex, 2. The feasible region is convex.  </p>"},{"location":"1a%20LP/#convexity-of-the-objective","title":"Convexity of the Objective","text":"<p>The LP objective is affine:</p> \\[ f(x) = c^T x + d \\] <ul> <li>Affine functions are both convex and concave (zero curvature).  </li> <li>Thus, no spurious local minima: every local optimum is global.  </li> </ul>"},{"location":"1a%20LP/#convexity-of-the-feasible-set","title":"Convexity of the Feasible Set","text":"<ul> <li>Each inequality \\(a^T x \\leq b\\) defines a half-space \u2014 convex.  </li> <li>Each equality \\(a^T x = b\\) defines a hyperplane \u2014 convex.  </li> <li>The intersection of convex sets is convex.  </li> </ul> <p>Hence, the feasible region is a convex polyhedron.</p>"},{"location":"1a%20LP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>Inequalities act like flat walls, keeping feasible points on one side.  </li> <li>Equalities act like flat sheets, slicing through space.  </li> <li>The feasible region is a polyhedron (possibly unbounded).  </li> <li>LP solutions always occur at a vertex (extreme point) of this polyhedron \u2014 this fact powers the simplex algorithm.  </li> </ul>"},{"location":"1a%20LP/#canonical-and-standard-forms","title":"Canonical and Standard Forms","text":"<p>LPs are often reformulated for theory and solvers:</p> <ul> <li>Canonical form (minimization):</li> </ul> \\[ \\min \\; c^T x \\quad \\text{s.t. } A x = b, \\; x \\geq 0 \\] <ul> <li>Standard form (maximization):</li> </ul> \\[ \\max \\; c^T x \\quad \\text{s.t. } A x \\leq b, \\; x \\geq 0 \\] <p>Any LP can be transformed into one of these forms via slack variables and variable splitting.</p>"},{"location":"1a%20LP/#duality-in-linear-programming","title":"Duality in Linear Programming","text":"<p>Every LP has a dual problem:</p> <ul> <li>Primal (minimization):</li> </ul> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } Gx \\leq h, \\; A x = b \\] <ul> <li>Dual:</li> </ul> \\[ \\max_{\\lambda, \\nu} \\; -h^T \\lambda + b^T \\nu \\quad \\text{s.t. } G^T \\lambda + A^T \\nu = c, \\; \\lambda \\geq 0 \\]"},{"location":"1a%20LP/#properties","title":"Properties:","text":"<ul> <li>Weak duality: Dual objective \\(\\leq\\) Primal objective.  </li> <li>Strong duality: Holds under mild conditions (Slater\u2019s condition).  </li> <li>Complementary slackness provides optimality certificates.  </li> </ul> <p>Duality underpins modern algorithms like interior-point methods.</p>"},{"location":"1a%20LP/#robust-linear-programming-rlp","title":"Robust Linear Programming (RLP)","text":"<p>In many applications, the LP data (\\(c, A, G, b, h\\)) are uncertain due to noise, estimation errors, or worst-case planning requirements. Robust Optimization handles this by requiring constraints to hold for all possible realizations of the uncertain parameters within a given uncertainty set.</p>"},{"location":"1a%20LP/#general-robust-lp-formulation","title":"General Robust LP Formulation","text":"<p>Consider an uncertain LP:</p> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } G(u) x \\leq h(u), \\quad \\forall u \\in \\mathcal{U} \\] <ul> <li>\\(\\mathcal{U}\\): uncertainty set (polyhedron, ellipsoid, box, etc.)  </li> <li>\\(u\\): uncertain parameters affecting \\(G, h\\).  </li> </ul> <p>The robust counterpart requires feasibility for all \\(u \\in \\mathcal{U}\\).</p>"},{"location":"1a%20LP/#box-uncertainty-interval-uncertainty","title":"Box Uncertainty (Interval Uncertainty)","text":"<p>Suppose \\(G = G_0 + \\Delta G\\), with each row uncertain in a box set:</p> \\[ \\{ g_i : g_i = g_i^0 + \\delta, \\; \\|\\delta\\|_\\infty \\leq \\rho \\} \\] <p>Robust constraint:</p> \\[ g_i^T x \\leq h_i, \\quad \\forall g_i \\in \\mathcal{U} \\] <p>This is equivalent to:</p> \\[ g_i^{0T} x + \\rho \\|x\\|_1 \\leq h_i \\] <p>Thus, a robust LP under box uncertainty is still a deterministic convex program (LP with additional \\(\\ell_1\\) terms).</p>"},{"location":"1a%20LP/#ellipsoidal-uncertainty","title":"Ellipsoidal Uncertainty","text":"<p>If uncertainty lies in an ellipsoid:</p> \\[ \\mathcal{U} = \\{ g : g = g^0 + Q^{1/2} u, \\; \\|u\\|_2 \\leq 1 \\} \\] <p>then the robust counterpart becomes:</p> \\[ g^{0T} x + \\| Q^{1/2} x \\|_2 \\leq h \\] <p>This is a Second-Order Cone Program (SOCP), still convex but more general than LP.</p>"},{"location":"1a%20LP/#robust-objective","title":"Robust Objective","text":"<p>When cost vector \\(c\\) is uncertain in \\(\\mathcal{U}_c\\):</p> \\[ \\min_{x} \\max_{c \\in \\mathcal{U}_c} c^T x \\] <ul> <li>If \\(\\mathcal{U}_c\\) is a box: inner max = \\(c^T x + \\rho \\|x\\|_1\\) </li> <li>If \\(\\mathcal{U}_c\\) is ellipsoidal: inner max = \\(c^T x + \\|Q^{1/2} x\\|_2\\) </li> </ul> <p>Thus, robust objectives often introduce regularization-like terms.  </p>"},{"location":"1a%20LP/#applications-of-robust-lp","title":"Applications of Robust LP","text":"<ul> <li>Supply chain optimization: demand uncertainty \u2192 robust inventory policies.  </li> <li>Finance: portfolio selection under uncertain returns.  </li> <li>Energy systems: robust scheduling under uncertain loads.  </li> <li>AI/ML: adversarial optimization, distributionally robust ML training.  </li> </ul>"},{"location":"1a%20LP/#how-lp-scales-in-practice","title":"How LP Scales in Practice","text":""},{"location":"1a%20LP/#polynomial-time-solvability","title":"Polynomial-Time Solvability","text":"<ul> <li>LPs can be solved in polynomial time using Interior-Point Methods (IPMs).  </li> <li>For an LP with \\(n\\) variables and \\(m\\) constraints, classical IPM complexity is roughly:</li> </ul> \\[ O((n+m)^3) \\] <ul> <li>But real-world performance depends on sparsity and problem structure. Sparse LPs are often solved in nearly linear time with specialized solvers.</li> </ul>"},{"location":"1a%20LP/#solver-ecosystem","title":"Solver Ecosystem","text":"<ul> <li>Commercial solvers: Gurobi, CPLEX, Mosek \u2014 highly optimized, exploit sparsity and parallelism, support warm-starts. These dominate large-scale industrial and financial problems.  </li> <li>Open-source solvers: HiGHS, GLPK, SCIP \u2014 robust for moderate problems, widely integrated into Python/Julia (via PuLP, Pyomo, CVXPY).  </li> <li>ML integration: CVXPY and PyTorch integrations make LP-based optimization easily callable inside ML pipelines.  </li> </ul>"},{"location":"1a%20LP/#algorithmic-tradeoffs","title":"Algorithmic Tradeoffs","text":"<ul> <li>Simplex method: moves along vertices of the feasible polyhedron.  </li> <li>Often very fast in practice, though exponential in theory.  </li> <li>Warm-starts make it excellent for iterative ML problems.  </li> <li>Interior-Point Methods (IPMs): follow a central path through the feasible region.  </li> <li>Polynomial worst-case guarantees.  </li> <li>Very robust to degeneracy, well-suited to dense problems.  </li> <li>First-order and decomposition methods:  </li> <li>ADMM, primal-dual splitting, stochastic coordinate descent.  </li> <li>Scale to massive LPs with billions of variables.  </li> <li>Sacrifice exactness for approximate but usable solutions.  </li> </ul>"},{"location":"1a%20LP/#comparison-of-lp-solvers","title":"Comparison of LP Solvers","text":"Method Complexity (theory) Scaling in practice Strengths Weaknesses ML/Engineering Use Cases Simplex Worst-case exponential Very fast in practice (near-linear for sparse LPs) Supports warm-starts, excellent for re-solving May stall on degenerate problems Iterative ML models, resource allocation, network flow Interior-Point (IPM) \\(O((n+m)^3)\\) Handles millions of variables if sparse Polynomial guarantees, robust, finds central solutions Memory-heavy (factorization of large matrices) Large dense LPs, convex relaxations in ML, finance First-order methods Sublinear (per iteration) Scales to billions of variables Memory-efficient, parallelizable Only approximate solutions MAP inference in CRFs, structured SVMs, massive embeddings Decomposition methods Problem-dependent Linear or near-linear scaling when structure exploited Breaks huge problems into smaller ones Requires separable structure Supply chain optimization, distributed training, scheduling"},{"location":"1a%20LP/#solving-large-scale-lps-in-ml-and-engineering","title":"Solving Large-Scale LPs in ML and Engineering","text":"<p>When problem sizes explode (e.g., \\(10^8\\) variables in embeddings or large-scale resource scheduling), standard solvers may fail due to memory or time.</p>"},{"location":"1a%20LP/#strategies","title":"Strategies","text":"<ul> <li>Decomposition methods:  </li> <li>Dantzig\u2013Wolfe, Benders, Lagrangian relaxation break problems into subproblems solved iteratively.  </li> <li>Column generation:  </li> <li>Introduces only a subset of variables initially, generating new ones as needed.  </li> <li>Stochastic and online optimization:  </li> <li>Replaces full LP solves with SGD-like updates, used in ML training pipelines.  </li> <li>Approximate relaxations:  </li> <li>In structured ML, approximate LP solutions often suffice (e.g., in structured prediction tasks).  </li> </ul>"},{"location":"1a%20LP/#ml-perspective","title":"ML Perspective","text":"<ul> <li>Structured prediction: LP relaxations approximate inference in CRFs, structured SVMs.  </li> <li>Adversarial robustness: Worst-case perturbation problems often reduce to LP relaxations, especially under \\(\\ell_\\infty\\) constraints.  </li> <li>Fairness: Linear constraints encode fairness requirements inside risk minimization objectives.  </li> <li>Large-scale systems: Recommender systems, resource allocation, energy scheduling \u2192 decomposition + approximate LP solvers.  </li> </ul>"},{"location":"1a%20LP/#where-lp-struggles-failure-modes","title":"Where LP Struggles (Failure Modes)","text":"<p>Despite its power, LPs face limitations:</p> <ol> <li>Nonlinearities </li> <li>Many ML objectives (e.g., log-likelihood, quadratic loss) are nonlinear.  </li> <li> <p>LP relaxations may be loose, requiring QP, SOCP, or nonlinear solvers.  </p> </li> <li> <p>Integrality </p> </li> <li>LP cannot enforce discrete decisions.  </li> <li> <p>Mixed-Integer Linear Programs (MILPs) are NP-hard, limiting scalability.  </p> </li> <li> <p>Uncertainty </p> </li> <li>Classical LP assumes perfect knowledge of data.  </li> <li> <p>Real problems require Robust LP or Stochastic LP.  </p> </li> <li> <p>Numerical conditioning </p> </li> <li>Poorly scaled coefficients lead to solver instability.  </li> <li> <p>Always normalize inputs for ML-scale LPs.  </p> </li> <li> <p>Memory bottlenecks </p> </li> <li>IPMs require factorizing large dense matrices \u2014 infeasible for extremely large-scale ML problems.  </li> </ol>"},{"location":"1b%20QP/","title":"QP","text":""},{"location":"1b%20QP/#quadratic-programming-qp-problem","title":"Quadratic Programming (QP) Problem","text":"<p>Quadratic Programming (QP) is an optimization framework that generalizes Linear Programming by allowing a quadratic objective function, while keeping the constraints linear. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q x + c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector to be optimized.  </li> <li>\\(Q \\in \\mathbb{R}^{n \\times n}\\) \u2014 the Hessian matrix defining the curvature of the objective.  </li> <li>\\(c \\in \\mathbb{R}^n\\) \u2014 the linear cost term.  </li> <li>\\(d \\in \\mathbb{R}\\) \u2014 a constant offset (does not affect the minimizer\u2019s location).  </li> <li>\\(G \\in \\mathbb{R}^{m \\times n}\\), \\(h \\in \\mathbb{R}^m\\) \u2014 inequality constraints \\(Gx \\leq h\\).  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints \\(Ax = b\\).  </li> </ul>"},{"location":"1b%20QP/#why-qps-can-be-convex-optimization-problems","title":"Why QPs Can Be Convex Optimization Problems","text":"<p>Whether a QP is convex depends on one key condition:</p> <ul> <li>Convex QP: The Hessian \\(Q\\) is positive semidefinite (\\(Q \\succeq 0\\)).  </li> <li>Nonconvex QP: The Hessian has negative eigenvalues (some directions curve downward).</li> </ul>"},{"location":"1b%20QP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QP objective is:</p> \\[ f(x) = \\frac{1}{2} x^T Q x + c^T x + d \\] <p>This is a quadratic function, which is:</p> <ul> <li>Convex if \\(Q \\succeq 0\\) (all curvature is flat or bowl-shaped).</li> <li>Strictly convex if \\(Q \\succ 0\\) (curvature is strictly bowl-shaped, ensuring a unique minimizer).</li> <li>Nonconvex if \\(Q\\) has negative eigenvalues (some directions slope downward).</li> </ul> <p>The gradient and Hessian are:</p> \\[ \\nabla f(x) = Qx + c, \\quad \\nabla^2 f(x) = Q \\] <p>Since the Hessian is constant in QPs, checking convexity is straightforward:  </p> <p>Positive semidefinite Hessian \u2192 Convex objective \u2192 No spurious local minima.</p>"},{"location":"1b%20QP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>Exactly as in LPs:</p> <ul> <li>Each inequality \\(a^T x \\leq b\\) is a half-space (convex).  </li> <li>Each equality \\(a^T x = b\\) is a hyperplane (convex).  </li> </ul> <p>The feasible set:</p> \\[ \\mathcal{F} = \\{ x \\mid Gx \\leq h, \\quad Ax = b \\} \\] <p>is the intersection of convex sets, hence convex.</p>"},{"location":"1b%20QP/#the-feasible-set-is-a-convex-polyhedron","title":"The Feasible Set is a Convex Polyhedron","text":"<p>For convex QPs:</p> <ul> <li>The feasible region \\(\\mathcal{F}\\) is still a convex polyhedron (because constraints are the same as in LPs).  </li> <li>The objective is a convex quadratic \"bowl\" sitting over that polyhedron.  </li> <li>The optimal solution is where the bowl\u2019s lowest point touches the feasible polyhedron.</li> </ul>"},{"location":"1b%20QP/#geometric-intuition-visualizing-qp","title":"Geometric Intuition: Visualizing QP","text":"<ul> <li>In LP, the objective is a flat plane sliding over a polyhedron.  </li> <li>In convex QP, the objective is a curved bowl sliding over the same polyhedron.  </li> <li>If the bowl\u2019s center lies inside the feasible region, the optimum is at that center.  </li> <li>If not, the bowl \u201cleans\u201d against the polyhedron\u2019s faces, edges, or vertices \u2014 which is where the optimal solution lies.</li> </ul> <p>\u2705 Summary: A QP is a convex optimization problem if and only if \\(Q \\succeq 0\\). In that case:</p> <ul> <li>Objective: Convex quadratic.  </li> <li>Constraints: Linear, hence convex.  </li> <li>Feasible set: Convex polyhedron.  </li> <li>Solution: Found at the point in the feasible set where the quadratic surface reaches its lowest value.</li> </ul>"},{"location":"1c%20Least%20Square/","title":"Least Squares","text":""},{"location":"1c%20Least%20Square/#least-squares-problem","title":"Least Squares Problem","text":"<p>Least Squares is one of the canonical convex optimization problems in statistics, machine learning, and signal processing. It seeks the vector \\(x \\in \\mathbb{R}^n\\) that minimizes the sum of squared errors:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\; \\|A x - b\\|_2^2 \\] <p>Where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) \u2014 data or measurement matrix, - \\(b \\in \\mathbb{R}^m\\) \u2014 observation or target vector, - \\(x \\in \\mathbb{R}^n\\) \u2014 decision vector (unknowns to estimate).  </p>"},{"location":"1c%20Least%20Square/#objective-expansion","title":"Objective Expansion","text":"<p>Expanding the squared norm:</p> \\[ \\|A x - b\\|_2^2 = (A x - b)^T (A x - b) = x^T A^T A x - 2 b^T A x + b^T b \\] <p>This is a quadratic convex function. In standard quadratic form:</p> \\[ f(x) = \\tfrac{1}{2} x^T Q x + c^T x + d \\] <p>with</p> \\[ Q = 2 A^T A, \\quad c = -2 A^T b, \\quad d = b^T b \\]"},{"location":"1c%20Least%20Square/#convexity","title":"Convexity","text":"<ul> <li>\\(Q = 2 A^T A \\succeq 0\\) since for any \\(z\\):  </li> </ul> \\[ z^T (A^T A) z = \\|A z\\|_2^2 \\geq 0 \\] <ul> <li>Hence LS is convex.  </li> <li>If \\(A\\) has full column rank, then \\(A^T A \\succ 0\\), making the problem strictly convex with a unique minimizer.  </li> </ul>"},{"location":"1c%20Least%20Square/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>When \\(m &gt; n\\) (overdetermined system), the equations \\(A x = b\\) may not be solvable.  </li> <li>LS finds \\(x^\\star\\) such that \\(A x^\\star\\) is the orthogonal projection of \\(b\\) onto the column space of \\(A\\).  </li> <li>The residual \\(r = b - A x^\\star\\) is orthogonal to \\(\\text{col}(A)\\):</li> </ul> \\[ A^T (b - A x^\\star) = 0 \\]"},{"location":"1c%20Least%20Square/#solutions","title":"Solutions","text":"<ul> <li>Normal Equations (full-rank case):</li> </ul> \\[ x^\\star = (A^T A)^{-1} A^T b \\] <ul> <li>General Case (possibly rank-deficient): The solution set is affine. The minimum-norm solution is given by the Moore\u2013Penrose pseudoinverse:</li> </ul> \\[ x^\\star = A^+ b \\] <ul> <li>Numerical Considerations: Normal equations can be ill-conditioned. In practice:</li> <li>Use QR decomposition or  </li> <li>SVD (stable, gives pseudoinverse).  </li> </ul>"},{"location":"1c%20Least%20Square/#constrained-least-squares-cls","title":"Constrained Least Squares (CLS)","text":"<p>Many practical problems require constraints on the solution. A general CLS formulation is:</p> \\[ \\begin{aligned} \\min_{x} \\quad &amp; \\|A x - b\\|_2^2 \\\\ \\text{s.t.} \\quad &amp; G x \\leq h \\\\ &amp; A_{\\text{eq}} x = b_{\\text{eq}} \\end{aligned} \\] <ul> <li>Objective: convex quadratic.  </li> <li>Constraints: linear.  </li> <li>Therefore: CLS is always a Quadratic Program (QP).</li> </ul>"},{"location":"1c%20Least%20Square/#example-1-wear-and-tear-allocation-cls-with-inequalities","title":"Example 1: Wear-and-Tear Allocation (CLS with Inequalities)","text":"<p>Suppose a landlord models annual apartment wear-and-tear costs as:</p> \\[ c_t \\approx a t + b, \\quad t = 1,\\dots,n \\] <p>with parameters \\(x = [a, b]^T\\).  </p> <p>CLS formulation:</p> \\[ \\min_{a,b} \\sum_{t=1}^n (a t + b - c_t)^2 \\] <p>Constraints (practical feasibility):</p> <ul> <li>Costs cannot be negative: </li> </ul> <p>This yields a CLS problem with linear inequality constraints, hence a QP.  </p> <p>-</p>"},{"location":"1c%20Least%20Square/#example-2-energy-consumption-fitting-cls-with-box-constraints","title":"\ud83d\udca1 Example 2: Energy Consumption Fitting (CLS with Box Constraints)","text":"<p>Suppose we fit energy usage from appliance data:  </p> <ul> <li>\\(A \\in \\mathbb{R}^{m \\times n}\\) usage matrix,  </li> <li>\\(b \\in \\mathbb{R}^m\\) observed energy bills.  </li> </ul> <p>CLS formulation:</p> \\[ \\min_{x} \\|A x - b\\|^2 \\] <p>Constraints: each appliance has a usage cap:  </p> \\[ 0 \\leq x_i \\leq u_i, \\quad i = 1,\\dots,n \\] <p>This is a QP with box constraints, often solved efficiently by projected gradient or interior-point methods.  </p>"},{"location":"1c%20Least%20Square/#regularized-least-squares-ridge-regression","title":"Regularized Least Squares (Ridge Regression)","text":"<p>A common extension in ML is regularized LS, e.g., ridge regression:</p> \\[ \\min_x \\|A x - b\\|^2 + \\lambda \\|x\\|_2^2 \\] <ul> <li>Equivalent to CLS with a quadratic penalty on \\(x\\).  </li> <li>Ensures uniqueness even if \\(A\\) is rank-deficient.  </li> <li>Solution:</li> </ul> \\[ x^\\star = (A^T A + \\lambda I)^{-1} A^T b \\]"},{"location":"1d%20QCQP/","title":"QCQP","text":""},{"location":"1d%20QCQP/#quadratically-constrained-quadratic-programming-qcqp-problem","title":"Quadratically Constrained Quadratic Programming (QCQP) Problem","text":"<p>A Quadratically Constrained Quadratic Program (QCQP) is an optimization problem in which both the objective and the constraints can be quadratic functions. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\\\ \\text{subject to} \\quad &amp; \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0, \\quad i = 1, \\dots, m \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(Q_0, Q_i \\in \\mathbb{R}^{n \\times n}\\) \u2014 symmetric matrices defining curvature of the objective and constraints.  </li> <li>\\(c_0, c_i \\in \\mathbb{R}^n\\) \u2014 linear terms in the objective and constraints.  </li> <li>\\(d_0, d_i \\in \\mathbb{R}\\) \u2014 constant offsets.  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints (linear).  </li> </ul>"},{"location":"1d%20QCQP/#why-qcqps-can-be-convex-optimization-problems","title":"Why QCQPs Can Be Convex Optimization Problems","text":"<p>QCQPs are not automatically convex \u2014 convexity requires specific conditions:</p> <ol> <li> <p>Objective convexity: \\(Q_0 \\succeq 0\\) (positive semidefinite Hessian for the objective).</p> </li> <li> <p>Constraint convexity:    For each inequality constraint \\(i\\), \\(Q_i \\succeq 0\\) so that  defines a convex set.</p> </li> <li> <p>Equality constraints:    Must be affine (linear), e.g., \\(A x = b\\).</p> </li> </ol>"},{"location":"1d%20QCQP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QCQP objective:</p> \\[ f_0(x) = \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\] <p>is convex iff \\(Q_0 \\succeq 0\\).</p>"},{"location":"1d%20QCQP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>A single quadratic constraint:</p> \\[ f_i(x) = \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0 \\] <p>defines a convex feasible set iff \\(Q_i \\succeq 0\\).  </p> <p>If any \\(Q_i\\) is not positive semidefinite, the constraint set becomes nonconvex, and the overall problem is nonconvex.</p>"},{"location":"1d%20QCQP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>If all \\(Q_i \\succeq 0\\), inequality constraints define convex quadratic regions (ellipsoids, elliptic cylinders, or half-spaces).</li> <li>Equality constraints \\(A x = b\\) cut flat slices through these regions.</li> <li>The feasible set is the intersection of convex quadratic sets and affine sets \u2014 hence convex.</li> </ul>"},{"location":"1d%20QCQP/#geometric-intuition-visualizing-qcqp","title":"Geometric Intuition: Visualizing QCQP","text":"<ul> <li>In QP, only the objective is curved; constraints are flat.  </li> <li>In QCQP, constraints can also be curved \u2014 forming shapes like ellipsoids or paraboloids.  </li> <li>Convex QCQPs look like a \u201cbowl\u201d objective contained within (or pressed against) curved convex walls.  </li> <li>Nonconvex QCQPs can have holes or disconnected regions, making them much harder to solve.</li> </ul> <p>\u2705 Summary: A QCQP is a convex optimization problem if and only if:</p> <ul> <li>\\(Q_0 \\succeq 0\\) (objective convexity), and  </li> <li>\\(Q_i \\succeq 0\\) for all \\(i\\) (each quadratic inequality constraint convex), and  </li> <li>All equality constraints are affine.  </li> </ul> <p>When these hold: - Objective: Convex quadratic. - Constraints: Convex quadratic or affine. - Feasible set: Intersection of convex sets (can be curved). - Solution: Found where the objective\u2019s minimum touches the convex feasible region.</p>"},{"location":"1e%20SOCP/","title":"SOCP","text":""},{"location":"1e%20SOCP/#second-order-cone-programming-socp-problem","title":"Second-Order Cone Programming (SOCP) Problem","text":"<p>Second-Order Cone Programming (SOCP) is a class of convex optimization problems that generalizes Linear and (certain) Quadratic Programs by allowing constraints involving second-order (quadratic) cones.  </p> <p>Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f^T x \\\\ \\text{subject to} \\quad &amp; \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i, \\quad i = 1, \\dots, m \\\\ &amp; F x = g \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(f \\in \\mathbb{R}^n\\) \u2014 the linear objective coefficients.  </li> <li>\\(A_i \\in \\mathbb{R}^{k_i \\times n}\\), \\(b_i \\in \\mathbb{R}^{k_i}\\) \u2014 define the affine transformation inside the norm for cone \\(i\\).  </li> <li>\\(c_i \\in \\mathbb{R}^n\\), \\(d_i \\in \\mathbb{R}\\) \u2014 define the affine term on the right-hand side.  </li> <li>\\(F \\in \\mathbb{R}^{p \\times n}\\), \\(g \\in \\mathbb{R}^p\\) \u2014 define linear equality constraints.</li> </ul>"},{"location":"1e%20SOCP/#the-second-order-quadratic-cone","title":"The Second-Order (Quadratic) Cone","text":"<p>A second-order cone in \\(\\mathbb{R}^k\\) is:</p> \\[ \\mathcal{Q}^k = \\left\\{ (u,t) \\in \\mathbb{R}^{k-1} \\times \\mathbb{R} \\ \\middle|\\ \\|u\\|_2 \\leq t \\right\\} \\] <p>Key properties: - Convex set. - Rotationally symmetric around the \\(t\\)-axis. - Contains all rays pointing \u201cupward\u201d inside the cone.</p>"},{"location":"1e%20SOCP/#why-socp-is-a-convex-optimization-problem","title":"Why SOCP is a Convex Optimization Problem","text":""},{"location":"1e%20SOCP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<ul> <li>The SOCP objective \\(f^T x\\) is affine.</li> <li>Affine functions are both convex and concave \u2014 no curvature.</li> </ul>"},{"location":"1e%20SOCP/#2-convexity-of-the-constraints","title":"2. Convexity of the Constraints","text":"<p>Each second-order cone constraint:</p> \\[ \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i \\] <p>is convex because: - The left-hand side \\(\\|A_i x + b_i\\|_2\\) is a convex function of \\(x\\). - The right-hand side \\(c_i^T x + d_i\\) is affine. - The set \\(\\{x \\mid \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i\\}\\) is a convex set.</p> <p>Equality constraints \\(F x = g\\) define a hyperplane, which is convex.</p> <p>Since the feasible region is the intersection of convex sets, it is convex.</p>"},{"location":"1e%20SOCP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>Each SOCP constraint defines a rotated or shifted cone in \\(x\\)-space.</li> <li>Equality constraints slice the space with flat hyperplanes.</li> <li>The feasible set is the intersection of these cones and hyperplanes.</li> </ul>"},{"location":"1e%20SOCP/#special-cases-of-socp","title":"Special Cases of SOCP","text":"<ul> <li>Linear Programs (LP): If all \\(A_i = 0\\), cone constraints reduce to linear inequalities.</li> <li>Certain Quadratic Programs (QP): Quadratic inequalities of the form \\(\\|Q^{1/2}x\\|_2 \\leq a^T x + b\\) can be rewritten as SOCP constraints.</li> <li>Norm Constraints: Bounds on \\(\\ell_2\\)-norms (e.g., \\(\\|x\\|_2 \\leq t\\)) are directly SOCP constraints.</li> </ul>"},{"location":"1e%20SOCP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>In LP, constraints are flat walls.</li> <li>In QP, objective is curved but constraints are flat.</li> <li>In SOCP, constraints themselves are curved (cone-shaped), allowing more modeling flexibility.</li> <li>The optimal solution is where the objective plane just \u201ctouches\u201d the feasible cone-shaped region.</li> </ul> <p>\u2705 Summary: - Objective: Affine (linear) \u2192 convex. - Constraints: Intersection of affine equalities and convex second-order cone inequalities. - Feasible set: Convex \u2014 shaped by cones and hyperplanes. - Power: Captures LPs, norm minimization, robust optimization, and some QCQPs. - Solution: Found efficiently by interior-point methods specialized for conic programming.</p>"},{"location":"1f%20GeometricInterpretation/","title":"Geometric Interpretation","text":"Feature LP QP QCQP SOCP Objective Linear: \\(c^T x\\) Quadratic: \\(\\frac{1}{2} x^T Q x + c^T x\\), convex if \\(Q \\succeq 0\\), indefinite \\(Q\\) \u2192 nonconvex Quadratic: \\(\\frac{1}{2} x^T Q_0 x + c_0^T x\\), convex if \\(Q_0 \\succeq 0\\), indefinite \u2192 nonconvex Linear: \\(c^T x\\), always convex Objective level sets Hyperplanes (flat, parallel) Quadrics: ellipsoids/paraboloids if \\(Q \\succeq 0\\), hyperboloids if indefinite Quadrics: ellipsoids/paraboloids; shape depends on \\(Q_0\\) definiteness Hyperplanes (flat, parallel) Constraints Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Quadratic: \\(\\frac{1}{2} x^T Q_i x + c_i^T x \\le b_i\\) \u2192 curved surfaces; convex if \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Second-order cone: \\(\\|A_i x + b_i\\|_2 \\le c_i^T x + d_i\\) \u2192 convex, curved conic surfaces Feasible region Polyhedron (flat faces, convex) Polyhedron (flat faces, convex) Curved region; convex if all \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Intersection of convex cones; curved, convex region Optimum location Vertex (extreme point of polyhedron) Face, edge, vertex, or interior if unconstrained minimizer feasible Boundary or interior; multiple local minima possible if nonconvex Boundary or interior; linear objective touches cone tangentially 2D Example Max \\(x_1 + 2x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 4\\) \u2192 polygon Min \\(x_1^2 + x_2^2 + x_1 + x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 3\\) \u2192 polygon feasible, elliptical contours Min \\(x_1^2 + x_2^2\\), s.t. \\(x_1^2 + x_2^2 \\le 4\\), \\(x_1 + x_2 \\le 3\\) \u2192 circular + linear \u2192 curved feasible Min \\(x_1 + x_2\\), s.t. \\(\\sqrt{x_1^2 + x_2^2} \\le 2 - 0.5 x_1\\) \u2192 tilted cone 3D Example Max \\(x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 polyhedron Min \\(x_1^2 + x_2^2 + x_3^2 + x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 4\\) \u2192 polyhedron + ellipsoid Min \\(x_1^2 + x_2^2 + x_3^2\\), s.t. \\(x_1^2 + x_2^2 + x_3^2 \\le 9\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 spherical + plane \u2192 curved feasible Min \\(x_1 + x_2 + x_3\\), s.t. \\(\\sqrt{x_1^2 + x_2^2 + x_3^2} \\le 4 - 0.5 x_3\\) \u2192 3D cone 4D Intuition Polytope + 3D hyperplane Polytope + 4D ellipsoid Curved 4D region + 4D ellipsoid; convex if \\(Q_0,Q_i \\succeq 0\\) 4D cone + 3D hyperplane Curvature Hint Flat objective / flat constraints Curved objective / flat constraints Curved objective / curved constraints Flat objective / curved constraints"},{"location":"1g%20GP/","title":"GP","text":""},{"location":"1g%20GP/#geometric-programming-gp","title":"\ud83d\udcd8 Geometric Programming (GP)","text":"<p>Geometric Programming (GP) is a flexible, widely used optimization class (communications, circuit design, resource allocation, control, ML model fitting). In its natural variable form it looks nonconvex, but \u2014 crucially \u2014 there is a canonical change of variables and a monotone transformation that converts a GP into a convex optimization problem.</p>"},{"location":"1g%20GP/#definitions-monomials-posynomials-and-the-standard-gp","title":"Definitions: monomials, posynomials, and the standard GP","text":"<p>Let \\(x=(x_1,\\dots,x_n)\\) with \\(x_i&gt;0\\).</p> <ul> <li> <p>A monomial (in GP terminology) is a function of the form  where \\(c&gt;0\\) and the exponents \\(a_i\\in\\mathbb{R}\\) (real exponents allowed).  </p> <p>Note: in GP literature \"monomial\" means positive coefficient times a power product (not to be confused with polynomial monomial which has nonnegative integer powers).</p> </li> <li> <p>A posynomial is a sum of monomials:  </p> </li> <li> <p>Standard (inequality) form of a geometric program:  where each \\(p_i\\) is a posynomial and each \\(m_j\\) is a monomial. (Any GP with other RHS values can be normalized to this form by dividing.)</p> </li> </ul>"},{"location":"1g%20GP/#why-gp-in-the-original-x-variables-is-not-convex","title":"\u2757 Why GP (in the original \\(x\\) variables) is not convex","text":"<ul> <li> <p>A monomial \\(m(x)=c x^a\\) (single variable) is convex on \\(x&gt;0\\) only for certain exponent ranges (e.g. \\(a\\le 0\\) or \\(a\\ge 1\\)). For \\(0&lt;a&lt;1\\) it is concave; for general real \\(a\\) it can be neither globally convex nor concave over \\(x&gt;0\\).   Example: \\(f(x)=x^{1/2}\\) (\\(0&lt;a&lt;1\\)) is concave on \\((0,\\infty)\\) (second derivative \\(=\\tfrac{1}{4}x^{-3/2}&gt;0\\)? \u2014 check sign; in fact \\(f''(x)= -\\tfrac{1}{4} x^{-3/2}&lt;0\\) showing concavity).</p> </li> <li> <p>A posynomial is a sum of monomials. Sums of nonconvex (or concave) terms are generally nonconvex. There is no general convexity guarantee for posynomials in the original variables \\(x\\).</p> </li> <li> <p>Therefore the objective \\(p_0(x)\\) and constraints \\(p_i(x)\\le 1\\) are not convex functions/constraints in \\(x\\), so the GP is not a convex program in the \\(x\\)-space.</p> </li> </ul> <p>Concrete counterexample (1D): take \\(p(x)=x^{1/2}+x^{-1}\\). The term \\(x^{1/2}\\) is concave on \\((0,\\infty)\\), \\(x^{-1}\\) is convex, and the sum is neither convex nor concave. One can find points \\(x_1,x_2\\) and \\(\\theta\\in(0,1)\\) that violate the convexity inequality.</p>"},{"location":"1g%20GP/#how-to-make-gp-convex-the-log-change-of-variables-and-log-transformation","title":"\u2705 How to make GP convex: the log-change of variables and log transformation","text":"<p>Key facts that enable convexification:</p> <ul> <li> <p>Monomials become exponentials of affine functions in log-variables.   Define \\(y_i = \\log x_i\\) (so \\(x_i = e^{y_i}\\)) and write \\(y=(y_1,\\dots,y_n)\\). For a monomial      we have      which is affine in \\(y\\).</p> </li> <li> <p>Posynomials become sums of exponentials of affine functions. For a posynomial      where \\(a_k\\) is the exponent-vector for the \\(k\\)th monomial and \\(y=\\log x\\).</p> </li> <li> <p>Taking the log of a posynomial yields a log-sum-exp function, i.e.      where \\(\\operatorname{LSE}(z_1,\\dots,z_K)=\\log\\!\\sum_{k} e^{z_k}\\).</p> </li> <li> <p>The log-sum-exp function is convex. Hence constraints of the form \\(p_i(x)\\le 1\\) become      i.e. a convex constraint in \\(y\\) because \\(\\log p_i(e^y)\\) is convex.</p> </li> <li> <p>Since \\(\\log(\\cdot)\\) is monotone, minimizing \\(p_0(x)\\) is equivalent to minimizing \\(\\log p_0(x)\\). Therefore one may transform the GP to the equivalent convex program in \\(y\\):</p> </li> </ul> <p>\\(\\(\\boxed{%   \\begin{aligned}   \\min_{y\\in\\mathbb{R}^n}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_0} c_{0k} e^{a_{0k}^T y}\\Big) \\\\   \\text{s.t.}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}\\Big) \\le 0,\\quad i=1,\\dots,m,\\\\   &amp; a_{j}^T y + \\log c_j = 0,\\quad \\text{(for each monomial equality } m_j(x)=1).   \\end{aligned}}\\)\\)</p> <p>This \\(y\\)-problem is convex: log-sum-exp objective/constraints are convex; monomial equalities are affine in \\(y\\).</p>"},{"location":"1g%20GP/#why-log-sum-exp-is-convex-brief-proof-via-hessian","title":"\ud83d\udd0d Why log-sum-exp is convex (brief proof via Hessian)","text":"<p>Let \\(g(y)=\\log\\!\\sum_{k=1}^K e^{u_k(y)}\\) with \\(u_k(y)=a_k^T y + b_k\\) (affine). Define  - Gradient:  - Hessian:  where \\(\\bar a=\\sum_k p_k a_k\\). The Hessian is a weighted covariance matrix of the vectors \\(a_k\\) (weights \\(p_k\\ge0\\)), hence PSD. Thus \\(g\\) is convex.</p>"},{"location":"1g%20GP/#monomials-as-affine-constraints-in-y","title":"\u2733\ufe0f Monomials as affine constraints in \\(y\\)","text":"<p>A monomial equality \\(c x^{a} = 1\\) becomes  an affine equality in \\(y\\). So monomial equality constraints become linear equalities after the log change.</p>"},{"location":"1g%20GP/#equivalence-and-solving-workflow","title":"\ud83d\udd01 Equivalence and solving workflow","text":"<ol> <li>Start with GP in \\(x&gt;0\\): minimize \\(p_0(x)\\) subject to posynomial constraints and monomial equalities.  </li> <li>Change variables: \\(y=\\log x\\) (domain becomes all \\(\\mathbb{R}^n\\)).  </li> <li>Apply log to posynomials (objective + inequality LHS). Because \\(\\log\\) is monotone increasing, inequalities maintain direction.  </li> <li>Solve the convex problem in \\(y\\) (log-sum-exp objective, convex constraints). Use interior-point or other convex solvers.  </li> <li>Recover \\(x^\\star = e^{y^\\star}\\).</li> </ol> <p>Because \\(x\\mapsto \\log x\\) is a bijection for \\(x&gt;0\\), solutions correspond exactly.</p>"},{"location":"1g%20GP/#worked-out-simple-example-2-variables","title":"\ud83d\udd27 Worked-out simple example (2 variables)","text":"<p>Original GP (standard form): </p> <p>Change variables: \\(y_1=\\log x_1,\\; y_2=\\log x_2\\).</p> <ul> <li>Transform terms:</li> <li>\\(3 x_1^{-1} = 3 e^{-y_1}\\) with \\(\\log\\) term \\(\\log 3 - y_1\\).</li> <li>\\(2 x_1 x_2 = 2 e^{y_1+y_2}\\) with \\(\\log\\) term \\(\\log 2 + y_1 + y_2\\).</li> <li>Constraint posynomial: \\(0.5 e^{-y_1} + e^{y_2}\\).</li> </ul> <p>Convex form (in \\(y\\)): </p> <p>Both objective and constraint are log-sum-exp functions (convex). Solve for \\(y^\\star\\) with a convex solver; then \\(x^\\star = e^{y^\\star}\\).</p>"},{"location":"1g%20GP/#numerical-implementation-remarks","title":"\u2699\ufe0f Numerical &amp; implementation remarks","text":"<ul> <li> <p>Domain requirement: GP requires \\(x_i&gt;0\\). The log transform only works on the positive orthant. If some variables can be zero, model reformulation (introducing small positive lower bounds) may be necessary.</p> </li> <li> <p>Normalization: Standard GPs use constraints \\(p_i(x)\\le 1\\). If you have \\(p_i(x) \\le t\\), divide by \\(t\\) to normalize.</p> </li> <li> <p>Numerical stability: Use the stable log-sum-exp implementation:      to avoid overflow/underflow.</p> </li> <li> <p>Solvers: After convexification the problem can be passed to generic convex solvers (CVX, CVXOPT, MOSEK, SCS, ECOS). Many solvers accept the log-sum-exp cone directly. Interior-point methods are effective on moderate-size GPs.</p> </li> <li> <p>Interpretation: The convexified problem is not an LP; it is a convex program with log-sum-exp terms (equivalently representable using exponential/relative entropy cones or by second-order cone approximations in some cases).</p> </li> </ul>"},{"location":"1g%20GP/#limitations-and-extensions","title":"\ud83d\udeab Limitations and extensions","text":"<ul> <li> <p>Signomials: If the problem contains negative coefficients (e.g. sums of monomials with arbitrary signs), it is a signomial program and the log transform does not yield a convex problem. Signomial programs are generally nonconvex and require local optimization or sequential convex approximations (e.g., successive convex approximation / condensation, branch-and-bound heuristics).</p> </li> <li> <p>Robust GP: Uncertainty in coefficients \\(c_k\\) or exponents \\(a_k\\) can sometimes be handled by robustification. If uncertainty is modeled multiplicatively (log-convex uncertainty sets), the robust counterpart often remains convex in \\(y\\); specifics depend on the uncertainty set shape (box \u2192 additional terms, ellipsoidal \u2192 conic forms). Robust GP is a rich topic \u2014 it frequently yields convex reformulations after log-transform if the uncertainty preserves log-convexity.</p> </li> <li> <p>Sparsity/exponents: Exponents \\(a_{ik}\\) need not be integers \u2014 positive real exponents are allowed. Sparsity in exponent matrix (\\(a_{ik}\\)) often leads to efficient implementations.</p> </li> </ul>"},{"location":"1g%20GP/#takeaway-summary-concise","title":"\u2705 Takeaway / Summary (concise)","text":"<ul> <li>GP in \\(x\\) is typically nonconvex because posynomials are not convex in \\(x\\).  </li> <li>Key transformation: \\(y=\\log x\\), and take \\(\\log\\) of posynomials \u2192 posynomial \\(\\mapsto\\) log-sum-exp of affine functions in \\(y\\).  </li> <li>Result: The GP becomes a convex optimization problem in \\(y\\): log-sum-exp objective/constraints (convex), and monomial equalities become linear equalities.  </li> <li>Solve in \\(y\\), then exponentiate to get \\(x\\).  </li> <li>Caveats: requires \\(x&gt;0\\); signomials (negative coefficients) remain nonconvex and need other techniques.</li> </ul> <p>If you want, I can: - provide a numerical, step-by-step example solving the worked GP above (with numbers) and show the solver output and \\(x^\\star\\); - show how robust multiplicative uncertainty in coefficients is handled in log domain; or - give a short code snippet (CVX/MOSEK or CVXPY) that constructs and solves the convexified GP.</p> <p>Which of those would you like next?</p>"},{"location":"2a%20Duality/","title":"Duality","text":""},{"location":"2a%20Duality/#duality-theory-kkt-conditions-and-duality-gap","title":"Duality Theory, KKT Conditions, and Duality Gap","text":"<p>Duality theory is a central tool in optimization and machine learning. It provides alternative perspectives on problems, certificates of optimality, and insights into algorithm design. Applications include Support Vector Machines (SVMs), Lasso, and ridge regression.</p>"},{"location":"2a%20Duality/#1-convex-optimization-problem","title":"1. Convex Optimization Problem","text":"<p>Consider the standard convex optimization problem:</p> \\[ \\begin{aligned} &amp; \\min_{x \\in \\mathbb{R}^n} &amp; f_0(x) \\\\ &amp; \\text{s.t.} &amp; f_i(x) \\le 0, \\quad i = 1, \\dots, m \\\\ &amp; &amp; h_j(x) = 0, \\quad j = 1, \\dots, p \\end{aligned} \\] <ul> <li>\\(f_0\\): objective function.  </li> <li>\\(f_i\\): convex inequality constraints.  </li> <li>\\(h_j\\): affine equality constraints.  </li> </ul> <p>Feasible set:</p> \\[ \\mathcal{D} = \\{ x \\in \\mathbb{R}^n \\mid f_i(x) \\le 0, \\ h_j(x) = 0 \\}. \\]"},{"location":"2a%20Duality/#2-lagrangian-function","title":"2. Lagrangian Function","text":"<p>The Lagrangian incorporates constraints into the objective:</p> \\[ \\mathcal{L}(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{j=1}^p \\nu_j h_j(x) \\] <ul> <li>\\(\\lambda_i \\ge 0\\): dual variables for inequalities.  </li> <li>\\(\\nu_j\\): dual variables for equalities.  </li> </ul> <p>Intuition: \\(\\lambda_i\\) represents the \u201cprice\u201d of violating constraint \\(f_i(x) \\le 0\\). Larger \\(\\lambda_i\\) penalizes violations more.</p>"},{"location":"2a%20Duality/#3-dual-function-and-infimum","title":"3. Dual Function and Infimum","text":"<p>The dual function is:</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu), \\quad \\lambda \\ge 0 \\]"},{"location":"2a%20Duality/#31-infimum","title":"3.1 Infimum","text":"<ul> <li>The infimum (inf) of a function is its greatest lower bound: the largest number that is less than or equal to all function values.  </li> <li>Formally, for \\(f(x)\\):</li> </ul> \\[ \\inf_x f(x) = \\sup \\{ y \\in \\mathbb{R} \\mid f(x) \\ge y, \\ \\forall x \\}. \\] <ul> <li>Intuition: </li> <li>If \\(f(x)\\) has a minimum, the infimum equals the minimum.  </li> <li>If no minimum exists, the infimum is the value approached but never reached.</li> </ul> <p>Examples:</p> <ol> <li>\\(f(x) = x^2\\), \\(x \\in \\mathbb{R}\\) \u2192 \\(\\inf f(x) = 0\\) at \\(x = 0\\).  </li> <li>\\(f(x) = 1/x\\), \\(x &gt; 0\\) \u2192 \\(\\inf f(x) = 0\\), never attained.  </li> </ol>"},{"location":"2a%20Duality/#32-supremum","title":"3.2 Supremum","text":"<ul> <li>The supremum (sup) of a set \\(S \\subset \\mathbb{R}\\) is the least upper bound: the smallest number greater than or equal to all elements of \\(S\\).</li> </ul> \\[ \\sup S = \\inf \\{ y \\in \\mathbb{R} \\mid y \\ge s, \\ \\forall s \\in S \\} \\] <p>Example: \\(S = \\{ x \\in \\mathbb{R} \\mid x &lt; 1 \\}\\) \u2192 \\(\\sup S = 1\\), although no maximum exists.</p>"},{"location":"2a%20Duality/#33-why-the-dual-function-provides-a-lower-bound","title":"3.3 Why the Dual Function Provides a Lower Bound","text":"<p>For any feasible \\(x \\in \\mathcal{D}\\) and \\(\\lambda \\ge 0\\), \\(\\nu\\):</p> \\[ \\lambda_i f_i(x) \\le 0, \\quad \\nu_j h_j(x) = 0 \\implies \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x) \\] <p>Taking the infimum over all \\(x\\):</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x), \\quad \\forall x \\in \\mathcal{D} \\] <p>Thus, for any dual variables:</p> \\[ g(\\lambda, \\nu) \\le p^\\star \\] <ul> <li>Interpretation: The dual function is always a lower bound on the primal optimum.  </li> <li>Geometric intuition: Think of the dual as the highest \u201cfloor\u201d under the primal objective that supports the feasible region.  </li> </ul>"},{"location":"2a%20Duality/#4-the-dual-problem","title":"4. The Dual Problem","text":"<p>The dual problem seeks the tightest lower bound:</p> \\[ \\begin{aligned} \\max_{\\lambda, \\nu} \\quad &amp; g(\\lambda, \\nu) \\\\ \\text{s.t.} \\quad &amp; \\lambda \\ge 0 \\end{aligned} \\] <ul> <li>Dual optimal value: \\(d^\\star = \\max_{\\lambda \\ge 0, \\nu} g(\\lambda, \\nu)\\).  </li> <li>Always satisfies weak duality: \\(d^\\star \\le p^\\star\\).  </li> <li>If convexity + Slater's condition hold, strong duality: \\(d^\\star = p^\\star\\).</li> </ul>"},{"location":"2a%20Duality/#5-duality-gap","title":"5. Duality Gap","text":"<p>The duality gap measures the difference between primal and dual optima:</p> \\[ \\text{Gap} = p^\\star - d^\\star \\ge 0 \\] <ul> <li>Zero gap: strong duality (common in convex ML problems).  </li> <li>Positive gap: weak duality only; dual provides only a lower bound.  </li> </ul>"},{"location":"2a%20Duality/#51-causes-of-positive-gap","title":"5.1 Causes of Positive Gap","text":"<ol> <li>Nonconvex objective.  </li> <li>Constraint qualification fails (e.g., Slater\u2019s condition not satisfied).  </li> <li>Dual problem is infeasible or unbounded.</li> </ol>"},{"location":"2a%20Duality/#52-example","title":"5.2 Example","text":"<p>Primal problem:</p> \\[ \\min_x -x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Primal optimum: \\(p^\\star = -1\\) at \\(x^\\star = 1\\) </li> <li>Dual problem: \\(d^\\star = -\\infty\\) (unbounded below)  </li> <li>Gap: \\(p^\\star - d^\\star = \\infty\\) \u2192 positive duality gap.</li> </ul> <p>Interpretation: dual gives a guaranteed lower bound but may not achieve the primal optimum.</p>"},{"location":"2a%20Duality/#6-karushkuhntucker-kkt-conditions","title":"6. Karush\u2013Kuhn\u2013Tucker (KKT) Conditions","text":"<p>For convex problems with strong duality, KKT conditions fully characterize optimality.</p> <p>Let \\(x^\\star\\) be primal optimal and \\((\\lambda^\\star, \\nu^\\star)\\) dual optimal:</p> <ol> <li> <p>Primal feasibility: </p> </li> <li> <p>Dual feasibility: </p> </li> <li> <p>Stationarity: </p> </li> <li> <p>Use subgradients for nonsmooth problems (e.g., Lasso).  </p> </li> <li> <p>Complementary slackness: </p> </li> </ol> <p>Intuition: Only active constraints contribute; the \u201cforces\u201d of objective and constraints balance.</p>"},{"location":"2a%20Duality/#7-applications-in-machine-learning","title":"7. Applications in Machine Learning","text":""},{"location":"2a%20Duality/#71-ridge-regression","title":"7.1 Ridge Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 \\] <ul> <li>Smooth shrinkage, unique solution.  </li> <li>Dual view useful in kernelized ridge regression.</li> </ul>"},{"location":"2a%20Duality/#72-lasso-regression","title":"7.2 Lasso Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1 \\] <ul> <li>KKT conditions explain sparsity:</li> </ul> \\[ X_j^\\top (y - Xw^\\star) = \\lambda s_j, \\quad s_j \\in  \\begin{cases} \\{\\text{sign}(w_j^\\star)\\}, &amp; w_j^\\star \\neq 0 \\\\ [-1,1], &amp; w_j^\\star = 0 \\end{cases} \\] <ul> <li>Basis for coordinate descent and soft-thresholding algorithms.</li> </ul>"},{"location":"2a%20Duality/#73-support-vector-machines-svms","title":"7.3 Support Vector Machines (SVMs)","text":"<ul> <li>Dual depends only on inner products \\(x_i^\\top x_j\\), enabling kernel methods.  </li> <li>Often more efficient if number of features \\(d\\) exceeds number of data points \\(n\\).</li> </ul>"},{"location":"2a%20Duality/#8-constrained-vs-penalized-optimization","title":"8. Constrained vs. Penalized Optimization","text":"<ul> <li>Constrained form:</li> </ul> \\[ \\min_w \\text{Loss}(w) \\quad \\text{s.t. } R(w) \\le t \\] <ul> <li>Penalized form:</li> </ul> \\[ \\min_w \\text{Loss}(w) + \\lambda R(w) \\] <ul> <li>Lagrange multiplier \\(\\lambda\\) acts as a \u201cprice\u201d on the constraint.  </li> <li>Equivalence holds for convex problems, but mapping \\(t \\leftrightarrow \\lambda\\) may be non-unique.</li> </ul>"},{"location":"2a%20Duality/#9-summary","title":"9. Summary","text":"<ol> <li>Infimum and supremum: greatest lower bound and least upper bound.  </li> <li>Dual function: \\(g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu)\\) always provides a lower bound.  </li> <li>Duality gap: \\(p^\\star - d^\\star\\), zero under strong duality, positive when dual does not attain primal optimum.  </li> <li>KKT conditions: necessary and sufficient for convex problems with strong duality.  </li> <li>ML connections: Ridge, Lasso, and SVM exploit duality for computation, sparsity, and kernelization.</li> </ol> <p>Key intuition: The dual function can be visualized as the highest supporting \u201cfloor\u201d under the primal objective. Maximizing it gives the tightest lower bound, and when strong duality holds, it meets the primal optimum exactly.</p>"},{"location":"3a%20Huber/","title":"Huber","text":""},{"location":"3a%20Huber/#huber-penalty-loss","title":"Huber Penalty Loss","text":"<p>The Huber loss is a robust loss function that combines the advantages of squared loss and absolute loss, making it less sensitive to outliers while remaining convex. It is defined as:</p> \\[ L_\\delta(r) =  \\begin{cases}  \\frac{1}{2} r^2 &amp; \\text{if } |r| \\le \\delta, \\\\ \\delta (|r| - \\frac{1}{2}\\delta) &amp; \\text{if } |r| &gt; \\delta, \\end{cases} \\] <p>where \\(r = y - \\hat{y}\\) is the residual, and \\(\\delta &gt; 0\\) is a threshold parameter.  </p>"},{"location":"3a%20Huber/#key-properties","title":"Key Properties","text":"<ul> <li>Quadratic for small residuals (\\(|r| \\le \\delta\\)) \u2192 behaves like least squares.  </li> <li>Linear for large residuals (\\(|r| &gt; \\delta\\)) \u2192 reduces the influence of outliers.  </li> <li>Convex, so standard convex optimization techniques apply.  </li> </ul>"},{"location":"3a%20Huber/#use","title":"Use","text":"<ul> <li>Commonly used in robust regression to estimate parameters in the presence of outliers.</li> <li>Balances efficiency (like least squares) and robustness (like absolute loss).</li> </ul>"},{"location":"3b%20Penalty%20Functions/","title":"Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#convex-optimization-notes-penalty-function-approximation","title":"Convex Optimization Notes: Penalty Function Approximation","text":""},{"location":"3b%20Penalty%20Functions/#penalty-function-approximation","title":"Penalty Function Approximation","text":"<p>We solve:</p> <p>\\(\\min \\; \\phi(r_1) + \\cdots + \\phi(r_m) \\quad \\text{subject to} \\quad r = Ax - b\\)</p> <p>where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) - \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) is a convex penalty function</p> <p>The choice of \\(\\phi\\) determines how residuals are penalized.</p>"},{"location":"3b%20Penalty%20Functions/#common-penalty-functions","title":"Common Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#1-quadratic-least-squares","title":"1. Quadratic (Least Squares)","text":"<p>\\(\\phi(u) = u^2\\)</p> <ul> <li>Strongly convex, smooth.  </li> <li>Penalizes large residuals heavily.  </li> <li>Equivalent to Gaussian noise model in statistics.  </li> <li>Leads to unique minimizer.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#2-absolute-value-least-absolute-deviations","title":"2. Absolute Value (Least Absolute Deviations)","text":"<p>\\(\\phi(u) = |u|\\)</p> <ul> <li>Convex but nonsmooth at \\(u=0\\) (subgradient methods needed).  </li> <li>Robust to outliers compared to quadratic.  </li> <li>Equivalent to Laplace noise model in statistics.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#why-does-it-lead-to-sparsity","title":"Why does it lead to sparsity?","text":"<ul> <li>The sharp corner at \\(u=0\\) makes it favorable for optimization to set many residuals (or coefficients) exactly to zero.  </li> <li>In contrast, quadratic penalties (\\(u^2\\)) only shrink values toward zero but rarely make them exactly zero.  </li> <li>Geometric intuition: the \\(\\ell_1\\) ball has corners aligned with coordinate axes \u2192 solutions land on axes \u2192 sparse.  </li> <li>Statistical interpretation: corresponds to a Laplace prior, which induces sparsity, whereas \\(\\ell_2\\) corresponds to a Gaussian prior (no sparsity).  </li> </ul> <p>\ud83d\udc49 This property is the foundation of Lasso regression and many compressed sensing methods.  </p>"},{"location":"3b%20Penalty%20Functions/#3-deadzone-linear","title":"3. Deadzone-Linear","text":"<p>\\(\\phi(u) = \\max \\{ 0, |u| - \\alpha \\}\\), where \\(\\alpha &gt; 0\\)</p> <ul> <li>Ignores small deviations (\\(|u| &lt; \\alpha\\)).  </li> <li>Linear growth outside the \u201cdeadzone.\u201d  </li> <li>Used in support vector regression (SVR) with \\(\\epsilon\\)-insensitive loss.  </li> <li>Convex, but not strictly convex \u2192 possibly multiple minimizers.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#4-log-barrier","title":"4. Log-Barrier","text":"<p>\\(\\phi(u) = \\begin{cases} -\\alpha^2 \\log \\left(1 - (u/\\alpha)^2 \\right), &amp; |u| &lt; \\alpha \\\\ \\infty, &amp; \\text{otherwise} \\end{cases}\\)</p> <ul> <li>Smooth, convex inside domain \\(|u| &lt; \\alpha\\).  </li> <li>Grows steeply as \\(|u| \\to \\alpha\\).  </li> <li>Effectively enforces constraint \\(|u| &lt; \\alpha\\).  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#histograms-of-residuals-effect-of-penalty-choice","title":"Histograms of Residuals (Effect of Penalty Choice)","text":"<p>For \\(A \\in \\mathbb{R}^{100 \\times 30}\\), the residual distribution \\(r\\) depends on \\(\\phi\\):</p> <ul> <li>Quadratic (\\(u^2\\)): residuals spread out (Gaussian-like).  </li> <li>Absolute value (\\(|u|\\)): sharper peak at 0, heavier tails (Laplace-like).  </li> <li>Deadzone: many residuals exactly at 0 (ignored region).  </li> <li>Log-barrier: residuals concentrate away from the boundary \\(|u| = 1\\).  </li> </ul> <p>\ud83d\udc49 Takeaway: Choice of \\(\\phi\\) directly shapes residual distribution.</p>"},{"location":"3b%20Penalty%20Functions/#huber-penalty-function","title":"Huber Penalty Function","text":"<p>The Huber penalty combines quadratic and linear growth:</p> <p>\\(\\phi_{\\text{huber}}(u) = \\begin{cases} u^2, &amp; |u| \\leq M \\\\ 2M|u| - M^2, &amp; |u| &gt; M \\end{cases}\\)</p>"},{"location":"3b%20Penalty%20Functions/#properties","title":"Properties","text":"<ul> <li>Quadratic near 0 (\\(|u| \\leq M\\)) \u2192 efficient for small noise.  </li> <li>Linear for large \\(|u|\\) \u2192 robust to outliers.  </li> <li>Smooth, convex.  </li> <li>Interpolates between least squares and least absolute deviations.  </li> </ul> <p>\ud83d\udc49 Called a robust penalty, widely used in robust regression.</p>"},{"location":"3b%20Penalty%20Functions/#summary-choosing-a-penalty-function","title":"Summary: Choosing a Penalty Function","text":"<ul> <li>Quadratic: efficient, but sensitive to outliers.  </li> <li>Absolute value: robust, but nonsmooth.  </li> <li>Deadzone: ignores small errors, good for sparse modeling (e.g., SVR).  </li> <li>Log-barrier: enforces domain constraints smoothly.  </li> <li>Huber: best of both worlds \u2192 quadratic for small residuals, linear for large ones.  </li> </ul>"},{"location":"3c%20Regularized/","title":"Regularized Problems","text":""},{"location":"3c%20Regularized/#regularized-approximation","title":"Regularized Approximation","text":""},{"location":"3c%20Regularized/#1-motivation-fit-vs-complexity","title":"1. Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we often want to balance two competing goals:</p> <ol> <li>Data fidelity: minimize how poorly the model fits the observed data (\\(f(x)\\)).  </li> <li>Model simplicity: discourage overly complex solutions (\\(R(x)\\)).</li> </ol> <p>This is naturally a bicriterion optimization problem:</p> <ul> <li>Criterion 1: \\(f(x)\\) = data-fitting term (e.g., least squares loss \\(\\|Ax-b\\|_2^2\\)).  </li> <li>Criterion 2: \\(R(x)\\) = regularization term (e.g., \\(\\|x\\|_1\\), \\(\\|x\\|_2^2\\), TV).  </li> </ul> <p>Since minimizing both simultaneously is usually impossible, we form the scalarized problem:</p> \\[ \\min_x \\; f(x) + \\lambda R(x), \\quad \\lambda &gt; 0 \\] <p>Here, \\(\\lambda\\) controls the trade-off: small \\(\\lambda\\) emphasizes fit, large \\(\\lambda\\) emphasizes simplicity.</p>"},{"location":"3c%20Regularized/#2-bicriterion-and-pareto-frontier","title":"2. Bicriterion and Pareto Frontier","text":"<ul> <li>Pareto optimality: a solution \\(x^\\star\\) is Pareto optimal if no other \\(x\\) improves one criterion without worsening the other.  </li> <li>Weighted sum method:  </li> <li>For convex \\(f\\) and \\(R\\), every Pareto optimal solution can be obtained from some \\(\\lambda \\ge 0\\).  </li> <li>For nonconvex problems, weighted sums may miss parts of the frontier.  </li> </ul> <p>Thus, regularization is a way of choosing a point on the Pareto frontier between fit and complexity.</p>"},{"location":"3c%20Regularized/#3-why-keep-x-small","title":"3. Why Keep \\(x\\) Small?","text":"<p>Ill-posed or noisy problems (e.g., \\(Ax \\approx b\\) with ill-conditioned \\(A\\)) often admit solutions with very large \\(\\|x\\|\\). - These large values overfit noise and are unstable. - Regularization (especially \\(\\ell_2\\)) controls the size of \\(x\\), yielding stable and robust solutions.  </p> <p>Example (Ridge regression):</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\] <p>Leads to the normal equations:</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b \\] <ul> <li>\\(A^\\top A + \\lambda I\\) is always positive definite.  </li> <li>Even if \\(A\\) is rank-deficient, the solution is unique and stable.</li> </ul>"},{"location":"3c%20Regularized/#4-lagrangian-interpretation","title":"4. Lagrangian Interpretation","text":"<p>Regularized approximation is equivalent to a constrained optimization formulation:</p> \\[ \\min_x f(x) \\quad \\text{s.t.} \\quad R(x) \\le t \\] <p>for some bound \\(t &gt; 0\\).  </p>"},{"location":"3c%20Regularized/#kkt-and-duality","title":"KKT and Duality","text":"<ul> <li>The Lagrangian is:</li> </ul> \\[ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda(R(x)-t) \\] <ul> <li>Under convexity and Slater\u2019s condition, strong duality holds.  </li> <li>KKT conditions:</li> </ul> \\[ 0 \\in \\partial f(x^\\star) + \\lambda^\\star \\partial R(x^\\star), \\quad  \\lambda^\\star \\ge 0, \\quad R(x^\\star) \\le t, \\quad  \\lambda^\\star (R(x^\\star)-t) = 0 \\] <ul> <li>The penalized form:</li> </ul> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>has the same optimality condition:</p> \\[ 0 \\in \\partial f(x^\\star) + \\lambda \\partial R(x^\\star) \\] <p>Hence, solving the penalized problem corresponds to solving the constrained one for some \\(t\\), though the \\(\\lambda \\leftrightarrow t\\) mapping is monotone but not one-to-one.</p>"},{"location":"3c%20Regularized/#5-common-regularizers","title":"5. Common Regularizers","text":""},{"location":"3c%20Regularized/#l2-ridge","title":"L2 (Ridge)","text":"<p>\\(R(x) = \\|x\\|_2^2\\) - Strongly convex \u2192 unique solution. - Encourages small coefficients, smooth solutions. - Bayesian view: Gaussian prior on \\(x\\). - Improves conditioning of \\(A^\\top A\\).  </p>"},{"location":"3c%20Regularized/#l1-lasso","title":"L1 (Lasso)","text":"<p>\\(R(x) = \\|x\\|_1\\) - Convex, but not strongly convex \u2192 solutions may be non-unique. - Promotes sparsity: many coefficients exactly zero. - Geometric view: \\(\\ell_1\\) ball has corners aligned with coordinate axes; intersections often occur at corners \u2192 sparse solutions. - Bayesian view: Laplace prior on \\(x\\). - Proximal operator: soft-thresholding </p>"},{"location":"3c%20Regularized/#elastic-net","title":"Elastic Net","text":"<p>\\(R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2\\) - Combines L1 sparsity with L2 stability. - Ensures uniqueness even when features are correlated.  </p>"},{"location":"3c%20Regularized/#beyond-l1l2","title":"Beyond L1/L2","text":"<ul> <li>General Tikhonov: \\(R(x) = \\|Lx\\|_2^2\\), where \\(L\\) encodes smoothness (e.g., derivative operator).  </li> <li>Total Variation (TV): \\(R(x) = \\|\\nabla x\\|_1\\), promotes piecewise-constant signals.  </li> <li>Group Lasso: \\(R(x) = \\sum_g \\|x_g\\|_2\\), induces structured sparsity.  </li> <li>Nuclear Norm: \\(R(X) = \\|X\\|_\\ast\\) (sum of singular values), promotes low-rank matrices.  </li> </ul>"},{"location":"3c%20Regularized/#6-choosing-the-regularization-parameter-lambda","title":"6. Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"3c%20Regularized/#trade-off","title":"Trade-off","text":"<ul> <li>Too small \\(\\lambda\\): weak regularization \u2192 overfitting, unstable solutions.  </li> <li>Too large \\(\\lambda\\): strong regularization \u2192 underfitting, biased solutions.  </li> </ul> <p>\\(\\lambda\\) determines where on the Pareto frontier the solution lies.</p>"},{"location":"3c%20Regularized/#practical-selection","title":"Practical Selection","text":"<ul> <li>Cross-validation (CV): </li> <li>Split data into \\(k\\) folds.  </li> <li>Train on \\(k-1\\) folds, validate on the held-out fold.  </li> <li>Average validation error across folds.  </li> <li> <p>Choose \\(\\lambda\\) minimizing average error.  </p> </li> <li> <p>Best practices: </p> </li> <li>Standardize features before using L1/Elastic Net.  </li> <li>For time series, use blocked or rolling CV (avoid leakage).  </li> <li>Use nested CV for model comparison.  </li> <li>One-standard-error rule: prefer larger \\(\\lambda\\) within one SE of min error \u2192 simpler model.  </li> </ul>"},{"location":"3c%20Regularized/#alternatives","title":"Alternatives","text":"<ul> <li>Analytical rules (ridge regression has closed-form shrinkage).  </li> <li>Information criteria (AIC/BIC; heuristic for Lasso).  </li> <li>Regularization path (trace solutions as \\(\\lambda\\) varies, pick best by validation error).  </li> <li>Inverse problems: discrepancy principle, L-curve, generalized CV.  </li> </ul>"},{"location":"3c%20Regularized/#7-algorithmic-perspective","title":"7. Algorithmic Perspective","text":"<p>Regularized problems often have the form:</p> \\[ \\min_x f(x) + R(x) \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex but possibly nonsmooth.</p> <ul> <li>Proximal Gradient (ISTA, FISTA):   Iterative updates using gradient of \\(f\\) and prox of \\(R\\).  </li> <li>Coordinate Descent: very effective for Lasso/Elastic Net.  </li> <li>ADMM: handles separable structures and constraints well.  </li> </ul> <p>Proximal operators are key: - L2: shrinkage (scaling). - L1: soft-thresholding. - TV/nuclear norm: more advanced proximal maps.  </p>"},{"location":"3c%20Regularized/#8-bayesian-interpretation","title":"8. Bayesian Interpretation","text":"<ul> <li>Regularization corresponds to MAP estimation.  </li> <li>Example: Gaussian noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)\\) and Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\) yields:</li> </ul> \\[ \\min_x \\frac{1}{2\\sigma^2}\\|Ax-b\\|_2^2 + \\frac{1}{2\\tau^2}\\|x\\|_2^2 \\] <p>So \\(\\lambda = \\frac{\\sigma^2}{2\\tau^2}\\) (up to scaling). - L1 corresponds to a Laplace prior, inducing sparsity.</p>"},{"location":"3c%20Regularized/#9-key-takeaways","title":"9. Key Takeaways","text":"<ul> <li>Regularized approximation = bicriterion optimization (fit vs. complexity).  </li> <li>Penalized and constrained forms are connected via duality and KKT.  </li> <li>Regularization stabilizes ill-posed problems and improves generalization.  </li> <li>Choice of regularizer shapes the solution (small \\(\\ell_2\\), sparse \\(\\ell_1\\), structured TV/group/nuclear).  </li> <li>\\(\\lambda\\) is critical \u2014 usually chosen by cross-validation or problem-specific heuristics.  </li> <li>Proximal algorithms make regularized optimization scalable.  </li> <li>Bayesian view ties \\(\\lambda\\) to prior assumptions and noise models.</li> </ul>"},{"location":"3d%20Robust%20Approximation/","title":"Robust Approximation","text":""},{"location":"3d%20Robust%20Approximation/#robust-regression-stochastic-vs-worst-case-formulations","title":"Robust Regression: Stochastic vs. Worst-Case Formulations","text":""},{"location":"3d%20Robust%20Approximation/#setup","title":"Setup","text":"<p>We study linear regression with uncertain design matrix:</p> \\[ y = A x + \\varepsilon, \\quad A = \\bar{A} + U, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the decision variable,  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the observed response,  </li> <li>\\(\\bar{A}\\) is the nominal design matrix,  </li> <li>\\(U\\) is an uncertainty term.  </li> </ul> <p>The treatment of \\(U\\) gives rise to two main formulations: stochastic (probabilistic uncertainty) and worst-case (deterministic uncertainty).</p>"},{"location":"3d%20Robust%20Approximation/#1-stochastic-formulation","title":"1. Stochastic Formulation","text":"<p>Assume \\(U\\) is random with  </p> <ul> <li>\\(\\mathbb{E}[U] = 0\\),  </li> <li>\\(\\mathbb{E}[U^\\top U] = P \\succeq 0\\),  </li> <li>finite second moment,  </li> <li>independent of \\(y\\).  </li> </ul> <p>We minimize the expected squared residual:</p> \\[ \\min_x \\; \\mathbb{E}\\!\\left[\\|(\\bar{A} + U)x - y\\|_2^2\\right]. \\]"},{"location":"3d%20Robust%20Approximation/#expansion","title":"Expansion","text":"\\[ \\|(\\bar{A}+U)x - y\\|_2^2 = \\|\\bar{A}x - y\\|_2^2 + 2(\\bar{A}x - y)^\\top Ux + \\|Ux\\|_2^2. \\] <ul> <li>Cross-term vanishes since \\(\\mathbb{E}[U]=0\\) and \\(U\\) is independent of \\(y\\):  </li> </ul> \\[ \\mathbb{E}[(\\bar{A}x - y)^\\top Ux] = 0. \\] <ul> <li>Variance term simplifies:  </li> </ul> \\[ \\mathbb{E}[\\|Ux\\|_2^2] = x^\\top P x. \\]"},{"location":"3d%20Robust%20Approximation/#resulting-problem","title":"Resulting Problem","text":"\\[ \\min_x \\; \\|\\bar{A}x - y\\|_2^2 + x^\\top P x. \\] <ul> <li>If \\(P = \\rho I\\): ridge regression (L2 regularization).  </li> <li>If \\(P \\succeq 0\\) general: generalized Tikhonov regularization, with anisotropic penalty \\(\\|P^{1/2}x\\|_2^2\\).  </li> </ul>"},{"location":"3d%20Robust%20Approximation/#convexity","title":"Convexity","text":"<p>The Hessian is  </p> \\[ \\nabla^2 f(x) = 2(\\bar{A}^\\top \\bar{A} + P) \\succeq 0. \\] <p>Thus the problem is convex. If \\(P \\succ 0\\), it is strongly convex and the minimizer is unique.  </p>"},{"location":"3d%20Robust%20Approximation/#2-worst-case-formulation","title":"2. Worst-Case Formulation","text":"<p>Suppose \\(U\\) is unknown but bounded:</p> \\[ \\|U\\|_2 \\leq \\rho, \\] <p>where \\(\\|\\cdot\\|_2\\) is the spectral norm (largest singular value). We minimize the worst-case squared residual:</p> \\[ \\min_x \\; \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A} + U)x - y\\|_2^2. \\]"},{"location":"3d%20Robust%20Approximation/#expansion-via-spectral-norm-bound","title":"Expansion via Spectral Norm Bound","text":"<p>For spectral norm uncertainty:</p> \\[ \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A}+U)x - y\\|_2 = \\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2. \\] <p>This identity uses the fact that \\(Ux\\) can align with the residual direction when \\(\\|U\\|_2 \\leq \\rho\\). Note: If a different norm bound is used (Frobenius, \\(\\ell_\\infty\\), etc.), the expression changes.</p>"},{"location":"3d%20Robust%20Approximation/#resulting-problem_1","title":"Resulting Problem","text":"\\[ \\min_x \\; \\left(\\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2\\right)^2. \\] <p>This is convex but not quadratic. Unlike ridge regression, the regularization is coupled inside the residual norm, making the solution more conservative.</p>"},{"location":"3d%20Robust%20Approximation/#3-comparison","title":"3. Comparison","text":"Aspect Stochastic Formulation Worst-Case Formulation Model of \\(U\\) Random, mean zero, finite variance Deterministic, bounded \\(\\|U\\|_2 \\leq \\rho\\) Objective \\(\\|\\bar{A}x - y\\|_2^2 + x^\\top P x\\) \\((\\|\\bar{A}x - y\\|_2 + \\rho\\|x\\|_2)^2\\) Regularization Quadratic penalty (ellipsoidal shrinkage) Norm inflation coupled with residual Geometry Ellipsoidal shrinkage of \\(x\\) (Mahalanobis norm) Inflated residual tube, more conservative Convexity Convex quadratic; strongly convex if \\(P \\succ 0\\) Convex but non-quadratic"},{"location":"3d%20Robust%20Approximation/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Stochastic robust regression \u2192 ridge/Tikhonov regression (quadratic L2 penalty).  </li> <li>Worst-case robust regression \u2192 inflated residual norm with L2 penalty inside the loss, more conservative than ridge.  </li> <li>Both are convex, but their geometry differs:  </li> <li>Stochastic: smooth ellipsoidal shrinkage of coefficients.  </li> <li>Worst-case: enlarged residual \u201ctube\u201d that hedges against adversarial perturbations.  </li> </ul>"},{"location":"3e%20MLE/","title":"MLE","text":""},{"location":"3e%20MLE/#statistical-estimation-and-maximum-likelihood","title":"Statistical Estimation and Maximum Likelihood","text":""},{"location":"3e%20MLE/#1-maximum-likelihood-estimation-mle","title":"1. Maximum Likelihood Estimation (MLE)","text":"<p>Suppose we have a family of probability densities</p> \\[ p_x(y), \\quad x \\in \\mathcal{X}, \\] <p>where \\(x\\) (often written as \\(\\theta\\) in statistics) is the parameter to be estimated.  </p> <ul> <li>\\(p_x(y) = 0\\) for invalid parameter values \\(x\\).  </li> <li>The function \\(p_x(y)\\), viewed as a function of \\(x\\) with \\(y\\) fixed, is called the likelihood function.  </li> <li>The log-likelihood is defined as  </li> </ul> \\[ \\ell(x) = \\log p_x(y). \\] <ul> <li>The maximum likelihood estimate (MLE) is  </li> </ul> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathcal{X}} \\; p_x(y)  = \\arg\\max_{x \\in \\mathcal{X}} \\; \\ell(x). \\]"},{"location":"3e%20MLE/#convexity-perspective","title":"Convexity Perspective","text":"<ul> <li>If \\(\\ell(x)\\) is concave in \\(x\\) for each fixed \\(y\\), then the MLE problem is a convex optimization problem.  </li> <li>Important distinction: this requires concavity in \\(x\\), not in \\(y\\).  </li> <li>Example: \\(p_x(y)\\) may be a log-concave density in \\(y\\) (common in statistics),  </li> <li>but this does not imply that \\(\\ell(x)\\) is concave in \\(x\\).  </li> </ul> <p>Thus, convexity of the MLE depends on the parameterization of the distribution family.  </p>"},{"location":"3e%20MLE/#2-linear-measurements-with-iid-noise","title":"2. Linear Measurements with IID Noise","text":"<p>Consider the linear measurement model:</p> \\[ y_i = a_i^\\top x + v_i, \\quad i = 1, \\ldots, m, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the unknown parameter vector,  </li> <li>\\(a_i \\in \\mathbb{R}^n\\) are known measurement vectors,  </li> <li>\\(v_i\\) are i.i.d. noise variables with density \\(p(z)\\),  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the vector of observed measurements.  </li> </ul>"},{"location":"3e%20MLE/#likelihood-function","title":"Likelihood Function","text":"<p>Since the noise terms are independent:</p> \\[ p_x(y) = \\prod_{i=1}^m p\\!\\left(y_i - a_i^\\top x\\right). \\] <p>Taking logs:</p> \\[ \\ell(x) = \\log p_x(y)  = \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#mle-problem","title":"MLE Problem","text":"<p>The MLE is any solution to:</p> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathbb{R}^n} \\; \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#convexity-note","title":"Convexity Note","text":"<ul> <li>If \\(p(z)\\) is log-concave in \\(z\\), then \\(\\log p(y_i - a_i^\\top x)\\) is concave in \\(x\\).  </li> <li>Therefore, under log-concave noise distributions (e.g. Gaussian, Laplace, logistic), the MLE problem is a concave maximization problem, hence equivalent to a convex optimization problem after sign change:</li> </ul> \\[ \\min_x \\; -\\ell(x). \\]"},{"location":"4a%20Linear%20Discrimination/","title":"Linear Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#1-linear-discrimination-lp-feasibility","title":"1. Linear Discrimination (LP Feasibility)","text":""},{"location":"4a%20Linear%20Discrimination/#problem-setup","title":"Problem Setup","text":"<ul> <li>Variables: \\((a,b) \\in \\mathbb{R}^{n+1}\\)</li> <li>Constraints:</li> <li>\\(a^T x_i - b \\geq 1\\)</li> <li>\\(a^T y_j - b \\leq -1\\)</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity","title":"Convexity","text":"<ul> <li>Each constraint is affine in \\((a,b)\\).</li> <li>Affine inequalities define convex half-spaces.</li> <li>Intersection of half-spaces = convex polyhedron.</li> <li>No objective \u2192 pure LP feasibility.</li> </ul> <p>Type: Convex LP feasibility problem.</p>"},{"location":"4a%20Linear%20Discrimination/#2-robust-linear-discrimination-hard-margin-svm","title":"2. Robust Linear Discrimination (Hard-Margin SVM)","text":""},{"location":"4a%20Linear%20Discrimination/#problem","title":"Problem","text":"\\[ \\min \\tfrac{1}{2}\\|a\\|_2^2 \\quad  \\text{s.t. } a^T x_i - b \\geq 1, \\; a^T y_j - b \\leq -1. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_1","title":"Convexity","text":"<ul> <li>Objective: \\(\\tfrac{1}{2}\\|a\\|_2^2\\) is convex quadratic (strictly convex in \\(a\\)).</li> <li>Constraints: Affine \\(\\Rightarrow\\) convex feasible set.</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#3-soft-margin-svm","title":"3. Soft-Margin SVM","text":""},{"location":"4a%20Linear%20Discrimination/#problem_1","title":"Problem","text":"\\[ \\min_{a,b,\\xi} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\xi_i $$ subject to $$ y_i(a^T z_i - b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_2","title":"Convexity","text":"<ul> <li>Objective: Sum of convex quadratic (\\(\\|a\\|^2\\)) and linear (\\(\\sum_i \\xi_i\\)).</li> <li>Constraints: Affine in \\((a,b,\\xi)\\).</li> <li>Feasible set = intersection of half-spaces (convex).</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#4-hinge-loss-formulation","title":"4. Hinge Loss Formulation","text":""},{"location":"4a%20Linear%20Discrimination/#problem_2","title":"Problem","text":"\\[ \\min_{a,b} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\max(0, 1 - y_i(a^T z_i - b)). \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_3","title":"Convexity","text":"<ul> <li>\\(\\tfrac{1}{2}\\|a\\|_2^2\\): convex quadratic.</li> <li>Inside hinge: \\(1 - y_i(a^T z_i - b)\\) is affine.</li> <li>\\(\\max(0, \\text{affine})\\) = convex function.</li> <li>Sum of convex functions = convex.</li> </ul> <p>Type: Unconstrained convex optimization problem.</p>"},{"location":"4a%20Linear%20Discrimination/#5-dual-svm-problem","title":"5. Dual SVM Problem","text":""},{"location":"4a%20Linear%20Discrimination/#problem_3","title":"Problem","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j z_i^T z_j $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_4","title":"Convexity","text":"<ul> <li>Quadratic form has negative semidefinite Hessian (concave).</li> <li>Maximization of concave function over convex set \u2192 convex optimization.</li> </ul> <p>Type: Convex quadratic program in dual variables.</p>"},{"location":"4a%20Linear%20Discrimination/#6-nonlinear-discrimination-with-kernels","title":"6. Nonlinear Discrimination with Kernels","text":""},{"location":"4a%20Linear%20Discrimination/#problem-dual-with-kernel","title":"Problem (Dual with Kernel)","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(z_i,z_j) $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_5","title":"Convexity","text":"<ul> <li>If \\(K\\) is positive semidefinite (Mercer kernel), quadratic form is convex.</li> <li>Maximization remains convex program.</li> </ul> <p>Type: Convex QP with kernel matrix.</p>"},{"location":"4a%20Linear%20Discrimination/#7-quadratic-discrimination","title":"7. Quadratic Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#problem_4","title":"Problem","text":"<p>Classifier: \\(f(z) = z^T P z + q^T z + r\\) with variables \\((P,q,r)\\).</p> <p>Constraints: - \\(x_i^T P x_i + q^T x_i + r \\geq 1\\) - \\(y_j^T P y_j + q^T y_j + r \\leq -1\\)</p>"},{"location":"4a%20Linear%20Discrimination/#convexity_6","title":"Convexity","text":"<ul> <li>\\(x_i^T P x_i = \\mathrm{Tr}(P x_i x_i^T)\\), affine in \\(P\\).</li> <li>Constraints affine in \\((P,q,r)\\).</li> <li>If additional constraint \\(P \\succeq 0\\), this is convex (semidefinite cone).</li> </ul> <p>Type: LP feasibility or SDP (semidefinite program).</p>"},{"location":"4a%20Linear%20Discrimination/#8-polynomial-feature-maps","title":"8. Polynomial Feature Maps","text":""},{"location":"4a%20Linear%20Discrimination/#setup","title":"Setup","text":"<ul> <li>Map \\(z \\mapsto F(z)\\) with monomials up to degree \\(d\\).</li> <li>Classifier: \\(f(z) = \\theta^T F(z)\\).</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity_7","title":"Convexity","text":"<ul> <li>Constraints: \\(\\theta^T F(x_i) \\geq 1\\), affine in \\(\\theta\\).</li> <li>Margin maximization objective: \\(\\|\\theta\\|^2\\), convex quadratic.</li> </ul> <p>Type: LP feasibility or convex QP.</p>"},{"location":"4a%20Linear%20Discrimination/#9-summary-of-convex-structures","title":"9. Summary of Convex Structures","text":"<ul> <li>LP feasibility: Linear separation.  </li> <li>QP: Hard-margin and soft-margin SVM.  </li> <li>Unconstrained convex problem: Hinge loss.  </li> <li>Dual SVM: Convex QP in dual variables.  </li> <li>Kernel SVM: Convex QP with PSD kernel.  </li> <li>Quadratic/Polynomial discrimination: LP or SDP, depending on constraints.  </li> </ul>"},{"location":"4d%20Algos/","title":"4d Algos","text":""},{"location":"4d%20Algos/#algorithms-for-convex-optimisation","title":"Algorithms for Convex Optimisation","text":"<p>Convex optimization algorithms exploit the geometry of convex sets and functions. Because every local minimum is global, algorithms can converge reliably without worrying about bad local minima.</p> <p>We divide algorithms into three main families:</p> <ol> <li>First-order methods \u2013 use gradients (scalable, but slower convergence).  </li> <li>Second-order methods \u2013 use Hessians (faster convergence, more expensive).  </li> <li>Interior-point methods \u2013 general-purpose, highly accurate solvers.</li> </ol>"},{"location":"4d%20Algos/#gradient-descent-first-order-method","title":"Gradient Descent (First-Order Method)","text":""},{"location":"4d%20Algos/#algorithm","title":"Algorithm","text":"<p>For step size \\(\\alpha &gt; 0\\):  </p>"},{"location":"4d%20Algos/#convergence","title":"Convergence","text":"<ul> <li>If \\(f\\) is convex and \\(\\nabla f\\) is Lipschitz continuous with constant \\(L\\):</li> <li>With fixed step \\(\\alpha \\le \\tfrac{1}{L}\\), we have:      </li> <li>If \\(f\\) is \\(\\mu\\)-strongly convex:          (linear convergence).</li> </ul>"},{"location":"4d%20Algos/#pros-cons","title":"Pros &amp; Cons","text":"<ul> <li>\u2705 Simple, scalable to very high dimensions.  </li> <li>\u274c Slow convergence compared to higher-order methods.</li> </ul>"},{"location":"4d%20Algos/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<ul> <li>Nesterov\u2019s Accelerated Gradient (NAG): Improves convergence rate from \\(\\mathcal{O}(1/k)\\) to \\(\\mathcal{O}(1/k^2)\\).  </li> <li>Widely used in machine learning (e.g., training deep neural networks).  </li> </ul>"},{"location":"4d%20Algos/#newtons-method-second-order-method","title":"Newton\u2019s Method (Second-Order Method)","text":""},{"location":"4d%20Algos/#algorithm_1","title":"Algorithm","text":"<p>Update rule:  </p>"},{"location":"4d%20Algos/#convergence_1","title":"Convergence","text":"<ul> <li>Quadratic near optimum: </li> <li>Very fast, but requires Hessian and solving linear systems.</li> </ul>"},{"location":"4d%20Algos/#damped-newton","title":"Damped Newton","text":"<p>To maintain global convergence:  </p>"},{"location":"4d%20Algos/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Approximate the Hessian to reduce cost.</p> <ul> <li>BFGS and L-BFGS (limited memory version).  </li> <li>Used in large-scale optimization (e.g., machine learning, statistics).  </li> <li>Convergence: superlinear.  </li> </ul>"},{"location":"4d%20Algos/#subgradient-methods","title":"Subgradient Methods","text":"<p>For nondifferentiable convex functions (e.g., \\(f(x) = \\|x\\|_1\\)).</p> <p>Update rule:  </p> <ul> <li>\\(\\partial f(x)\\): subdifferential (set of all subgradients).  </li> <li>Convergence: \\(\\mathcal{O}(1/\\sqrt{k})\\) with diminishing step sizes.  </li> <li>Useful in large-scale, nonsmooth optimization.</li> </ul>"},{"location":"4d%20Algos/#proximal-methods","title":"Proximal Methods","text":"<p>For composite problems:  where \\(f\\) is smooth convex, \\(g\\) convex but possibly nonsmooth.</p> <p>Proximal operator: </p> <ul> <li>Proximal gradient descent: </li> <li>Widely used in sparse optimization (e.g., Lasso).</li> </ul>"},{"location":"4d%20Algos/#interior-point-methods","title":"Interior-Point Methods","text":"<p>Transform constrained problem into a sequence of unconstrained problems using barrier functions.</p> <p>For constraint \\(g_i(x) \\le 0\\), replace with barrier:  </p> <p>Solve:  for increasing \\(t\\).</p>"},{"location":"4d%20Algos/#properties","title":"Properties","text":"<ul> <li>Polynomial-time complexity for convex problems.  </li> <li>Extremely accurate solutions.  </li> <li>Basis of general-purpose solvers (e.g., CVX, MOSEK, Gurobi).  </li> </ul>"},{"location":"4d%20Algos/#coordinate-descent","title":"Coordinate Descent","text":"<p>At each iteration, optimize w.r.t. one coordinate (or block of coordinates):</p> \\[ x_i^{k+1} = \\arg\\min_{z} f(x_1^k, \\dots, x_{i-1}^k, z, x_{i+1}^k, \\dots, x_n^k) \\] <ul> <li>Works well for high-dimensional problems.  </li> <li>Used in Lasso, logistic regression, and large-scale ML problems.</li> </ul>"},{"location":"4d%20Algos/#primal-dual-and-splitting-methods","title":"Primal-Dual and Splitting Methods","text":"<ul> <li> <p>ADMM (Alternating Direction Method of Multipliers):   Splits problem into subproblems, solves in parallel.   Popular in distributed optimization and ML.  </p> </li> <li> <p>Primal-dual interior-point methods:   Solve both primal and dual simultaneously.  </p> </li> </ul>"},{"location":"4d%20Algos/#summary-of-convergence-rates","title":"Summary of Convergence Rates","text":"Method Smooth Convex Strongly Convex Gradient Descent \\(\\mathcal{O}(1/k)\\) Linear Accelerated Gradient \\(\\mathcal{O}(1/k^2)\\) Linear Subgradient \\(\\mathcal{O}(1/\\sqrt{k})\\) \u2013 Newton\u2019s Method Quadratic (local) Quadratic Interior-Point Polynomial-time Polynomial-time"},{"location":"4d%20Algos/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<ul> <li>Small problems, high accuracy: Newton, Interior-point.  </li> <li>Large-scale smooth problems: Gradient descent, Nesterov acceleration, L-BFGS.  </li> <li>Large-scale nonsmooth problems: Subgradient, Proximal, ADMM.  </li> <li>Sparse / structured constraints: Coordinate descent, Proximal methods.  </li> </ul>"},{"location":"4d%20Algos/#log-concavity-and-log-convexity","title":"Log-concavity and Log-convexity","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}_{++}\\) is:</p> <ul> <li>Log-concave if \\(\\log f(x)\\) is concave.</li> <li>Log-convex if \\(\\log f(x)\\) is convex.</li> </ul>"},{"location":"4d%20Algos/#relevance","title":"Relevance","text":"<ul> <li>Log-concave functions appear in probability (many common distributions have log-concave densities, such as Gaussian, exponential, and uniform). This ensures tractability of maximum likelihood estimation.</li> <li>Log-convexity is useful in geometric programming, where monomials and posynomials are log-convex.</li> </ul>"},{"location":"4d%20Algos/#examples","title":"Examples","text":"<ul> <li>Gaussian density is log-concave.</li> <li>Exponential function is log-convex.</li> </ul>"},{"location":"4d%20Algos/#geometric-programming","title":"Geometric Programming","text":"<p>Geometric programming (GP) is a class of problems of the form:  where each \\(f_i\\) is a posynomial.</p> <ul> <li>Monomial: \\(f(x) = c x_1^{a_1} x_2^{a_2} \\dots x_n^{a_n}\\), with \\(c &gt; 0\\), exponents real.</li> <li>Posynomial: Sum of monomials.</li> </ul> <p>By applying the log transformation \\(y_i = \\log x_i\\), the problem becomes convex.</p>"},{"location":"6a%20First%20Order%20Optimization/","title":"First Order Optimization","text":""},{"location":"6a%20First%20Order%20Optimization/#first-order-optimization-methods","title":"First-Order Optimization Methods","text":"<p>In machine learning, especially at large scale, we often cannot afford to solve convex problems using heavy, exact solvers (like simplex or interior-point methods). Instead, we rely on first-order methods \u2014 algorithms that use only function values and gradients to iteratively approach the solution.</p>"},{"location":"6a%20First%20Order%20Optimization/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<p>Gradient descent is the most fundamental algorithm for minimizing differentiable convex functions.</p> <p>Basic Idea: At each iteration, move in the direction opposite to the gradient (the steepest descent direction), because it points toward lower values of the function.</p> <p>Update Rule:  where: - \\(\\nabla f(x^{(k)})\\) \u2014 gradient at the current point. - \\(\\alpha_k\\) \u2014 step size (learning rate).</p> <p>Convergence (Convex Case): - If \\(f\\) is convex and has Lipschitz-continuous gradients, gradient descent converges at rate \\(O(1/k)\\) for constant step size. - If \\(f\\) is also strongly convex, the rate improves to linear convergence.</p> <p>Step Size Selection: - Constant: simple but requires tuning. - Diminishing: \\(\\alpha_k \\to 0\\) ensures convergence but may be slow. - Backtracking Line Search: adaptively chooses \\(\\alpha_k\\) for efficiency.</p>"},{"location":"6a%20First%20Order%20Optimization/#subgradient-methods","title":"Subgradient Methods","text":"<p>Many important convex functions in ML are non-differentiable (e.g., \\(\\|x\\|_1\\)). The subgradient generalizes the gradient for such functions.</p> <p>Subgradient Definition: A vector \\(g\\) is a subgradient of \\(f\\) at \\(x\\) if:  </p> <p>Update Rule: </p> <p>Key Trade-Off: Subgradient methods are robust but converge more slowly (\\(O(1/\\sqrt{k})\\) in general).</p>"},{"location":"6a%20First%20Order%20Optimization/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>SGD is the workhorse of large-scale machine learning.</p> <p>When to Use: When the objective is a sum over many data points:  </p> <p>Update Rule: - Pick a random index \\(i_k\\) - Use \\(\\nabla f_{i_k}(x^{(k)})\\) as an unbiased estimate of the full gradient:  </p> <p>Advantages: - Much faster per iteration for large \\(N\\). - Enables online learning.</p> <p>Disadvantages: - Introduces variance; iterates \u201cbounce\u201d around the optimum. - Requires careful learning rate schedules.</p>"},{"location":"6a%20First%20Order%20Optimization/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<p>Nesterov\u2019s Accelerated Gradient (NAG) achieves the optimal convergence rate for smooth convex functions: \\(O(1/k^2)\\).</p> <p>Key Idea: Introduce a momentum term that anticipates the next position, correcting the gradient direction.</p> <p>Update: </p> <p>When Useful: - Smooth convex problems where plain gradient descent is too slow. - Large-scale ML tasks with batch updates.</p>"},{"location":"6a%20First%20Order%20Optimization/#why-first-order-methods-matter-for-ml","title":"Why First-Order Methods Matter for ML","text":"<ul> <li>Handle huge datasets efficiently.</li> <li>Require only gradient information, which is cheap for many models.</li> <li>Naturally fit into streaming and online learning setups.</li> <li>Form the backbone of deep learning optimizers (SGD, Adam, RMSProp \u2014 though deep nets are non-convex).</li> </ul>"},{"location":"7a%20pareto%20optimal/","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#classical-optimality","title":"Classical Optimality","text":"<p>In standard convex optimisation, we consider a single objective function \\(f(x)\\) and aim to find a globally optimal solution:  where \\(\\mathcal{X}\\) is the feasible set.  </p> <p>Here, optimality is absolute: there exists a single best point (or set of equivalent best points) with respect to one measure of performance.</p>"},{"location":"7a%20pareto%20optimal/#multi-objective-optimisation","title":"Multi-objective Optimisation","text":"<p>Many practical problems in machine learning and optimisation involve multiple competing objectives. For instance:</p> <ul> <li>In supervised learning, one wishes to minimise prediction error while also controlling model complexity.  </li> <li>In fairness-aware learning, we want high accuracy while limiting demographic disparity.  </li> <li>In finance, an investor balances expected return against risk.  </li> </ul> <p>Formally, a multi-objective optimisation problem is written as:  where \\(f_1, f_2, \\dots, f_k\\) are the competing objectives.</p>"},{"location":"7a%20pareto%20optimal/#pareto-optimality_1","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A solution \\(x^* \\in \\mathcal{X}\\) is Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  with strict inequality for at least one objective \\(j\\).  </p> <p>Intuitively, no feasible point strictly improves one objective without worsening another.</p>"},{"location":"7a%20pareto%20optimal/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A solution \\(x^*\\) is weakly Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  </p> <p>In other words, no solution improves every objective simultaneously.</p>"},{"location":"7a%20pareto%20optimal/#geometric-intuition","title":"Geometric Intuition","text":"<p>If we plot feasible solutions in the objective space \\((f_1(x), f_2(x))\\), the Pareto frontier is the lower-left boundary for minimisation problems. - Points on the frontier are non-dominated (Pareto optimal). - Points inside the feasible region but above the boundary are dominated.  </p>"},{"location":"7a%20pareto%20optimal/#scalarisation","title":"Scalarisation","text":"<p>Since multi-objective optimisation problems usually admit a set of Pareto optimal solutions rather than a single best point, practitioners use scalarisation. This reduces multiple objectives to a single scalar objective that can be optimised with standard methods.</p>"},{"location":"7a%20pareto%20optimal/#weighted-sum-scalarisation","title":"Weighted Sum Scalarisation","text":"<p>The most common approach is the weighted sum:  </p> <ul> <li>Each choice of weights \\(w\\) corresponds to a different point on the Pareto frontier.  </li> <li>Larger \\(w_i\\) prioritises objective \\(f_i\\) relative to others.  </li> </ul> <p>Convexity caveat: If the feasible set and objectives are convex, weighted sum scalarisation can recover the convex part of the Pareto frontier. Non-convex regions of the frontier may not be attainable using weighted sums alone.</p>"},{"location":"7a%20pareto%20optimal/#varepsilon-constraint-method","title":"\\(\\varepsilon\\)-Constraint Method","text":"<p>Another approach is to optimise one objective while converting others into constraints:  Here \\(\\varepsilon_i\\) are tolerance levels. By adjusting them, we can explore different trade-offs.  </p> <p>This connects directly to regularisation in machine learning: - In ridge regression, we minimise data fit subject to a complexity budget \\(\\|x\\|_2^2 \\leq \\tau\\). - The equivalent penalised form \\(\\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2\\) is obtained via Lagrangian duality, where \\(\\lambda\\) is the multiplier associated with \\(\\tau\\).  </p>"},{"location":"7a%20pareto%20optimal/#duality-and-scalarisation","title":"Duality and Scalarisation","text":"<p>Scalarisation is deeply connected to duality in convex optimisation: - The weights \\(w_i\\) or multipliers \\(\\lambda\\) can be interpreted as Lagrange multipliers balancing objectives. - Adjusting these parameters changes the point on the Pareto frontier that is selected. - This explains why hyperparameters like \\(\\lambda\\) in regularisation are so influential: they represent trade-offs in a hidden multi-objective problem.</p>"},{"location":"7a%20pareto%20optimal/#example-1-regularised-least-squares","title":"Example 1: Regularised Least Squares","text":"<p>Consider the regression problem with data matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) and target \\(b \\in \\mathbb{R}^m\\).  </p> <p>We want to minimise: 1. Prediction error: \\(f_1(x) = \\|Ax - b\\|_2^2\\) 2. Model complexity: \\(f_2(x) = \\|x\\|_2^2\\) </p> <p>This is a two-objective optimisation problem.  </p> <ul> <li> <p>Using the weighted sum:  where \\(\\lambda \\geq 0\\) determines the trade-off.  </p> </li> <li> <p>Alternatively, using the \\(\\varepsilon\\)-constraint:  </p> </li> </ul> <p>Both formulations yield Pareto optimal solutions, with \\(\\lambda\\) and \\(\\tau\\) providing different parametrisations of the frontier.</p>"},{"location":"7a%20pareto%20optimal/#example-2-portfolio-optimisation-riskreturn","title":"Example 2: Portfolio Optimisation (Risk\u2013Return)","text":"<p>In finance, suppose an investor chooses portfolio weights \\(w \\in \\mathbb{R}^n\\).</p> <ul> <li>Expected return: \\(f_1(w) = -\\mu^\\top w\\) (we minimise negative return).  </li> <li>Risk: \\(f_2(w) = w^\\top \\Sigma w\\) (variance of returns, a convex function).  </li> </ul> <p>The problem is:  </p> <ul> <li>Using weighted sum scalarisation:  </li> <li>Different \\(\\alpha\\) values give different points on the efficient frontier.  </li> </ul> <p>This convex formulation underpins modern portfolio theory.</p>"},{"location":"7a%20pareto%20optimal/#example-3-probabilistic-modelling-elbo","title":"Example 3: Probabilistic Modelling (ELBO)","text":"<p>In variational inference, the Evidence Lower Bound (ELBO) is:  </p> <p>This can be seen as a scalarisation of two competing objectives: 1. Data fit (reconstruction term). 2. Simplicity or prior adherence (KL divergence).  </p> <p>By weighting the KL divergence with a parameter \\(\\beta\\), we obtain the \\(\\beta\\)-VAE:  </p> <p>Here, \\(\\beta\\) plays the role of a scalarisation weight, selecting different Pareto optimal trade-offs between reconstruction accuracy and disentanglement.</p>"},{"location":"7a%20pareto%20optimal/#broader-connections-in-ai-and-ml","title":"Broader Connections in AI and ML","text":"<ul> <li>Fairness vs accuracy: Balancing accuracy with fairness metrics is a multi-objective problem often approached via scalarisation.  </li> <li>Generalisation vs training error: Regularisation is a scalarisation of fit versus complexity.  </li> <li>Compression vs performance: The information bottleneck principle is a Pareto trade-off between accuracy and representation complexity.  </li> <li>Inference vs divergence: Variational inference (ELBO) is naturally a scalarised multi-objective problem.  </li> </ul>"},{"location":"7a%20pareto%20optimal/#summary","title":"Summary","text":"<ul> <li>Classical optimisation yields a single best solution.  </li> <li>Multi-objective optimisation gives a set of non-dominated (Pareto optimal) solutions.  </li> <li>Scalarisation provides practical methods to compute Pareto optimal solutions.  </li> <li>Weighted sums recover convex parts of the frontier, while \\(\\varepsilon\\)-constraints provide flexibility.  </li> <li>Scalarisation connects directly to duality, where weights act as Lagrange multipliers.  </li> <li>Examples in ML (ridge regression, ELBO) and finance (portfolio optimisation) demonstrate its wide relevance.  </li> </ul> <p>Scalarisation is not only a mathematical device but the foundation for understanding regularisation, fairness, generalisation, and many practical trade-offs in machine learning.</p>"},{"location":"Example/","title":"Example","text":""},{"location":"Example/#galactic-cargo-delivery-optimization-lp-formulation","title":"Galactic Cargo Delivery Optimization \u2014 LP Formulation","text":"<p>You are the logistics commander of an interstellar fleet tasked with delivering vital supplies across the galaxy. Your fleet consists of \\(N\\) starship pilots, and you have \\(K\\) distinct types of cargo crates to deliver. Each cargo type \\(j\\) has a known volume \\(v_j\\), representing the number of crates that must reach their destinations.</p> <p>To maintain fleet balance and operational efficiency, each pilot must carry the same total number of crates. Your mission is to assign crates to pilots to minimize the total expected delivery time, accounting for each pilot\u2019s unique speed and proficiency with different cargo types.</p>"},{"location":"Example/#notation","title":"Notation","text":"<ul> <li>\\(i = 1, \\ldots, N\\): indices for starship pilots  </li> <li>\\(j = 1, \\ldots, K\\): indices for cargo types  </li> <li>\\(v_j\\): volume (number of crates) of cargo type \\(j\\) </li> <li>\\(d_{ij}\\): estimated delivery time for pilot \\(i\\) to deliver one crate of type \\(j\\)</li> </ul>"},{"location":"Example/#decision-variables","title":"Decision Variables","text":"<p>\\(x_{ij} \\geq 0\\)</p> <p>Number of crates of cargo type \\(j\\) assigned to pilot \\(i\\).</p>"},{"location":"Example/#objective-function","title":"Objective Function","text":"<p>Minimize the total delivery time across all pilots:</p> \\[\\min \\sum_{i=1}^N \\sum_{j=1}^K d_{ij} x_{ij}\\] <p>Or equivalently, in vector form:</p> \\[\\min c^T x\\] <p>where</p> \\[ c = \\begin{bmatrix} d_{11}, d_{12}, \\ldots, d_{1K}, d_{21}, \\ldots, d_{NK} \\end{bmatrix}^T, \\quad x = \\begin{bmatrix} x_{11}, x_{12}, \\ldots, x_{1K}, x_{21}, \\ldots, x_{NK} \\end{bmatrix}^T \\]"},{"location":"Example/#constraints","title":"Constraints","text":"<ol> <li>All crates must be delivered:</li> </ol> \\[\\sum_{i=1}^N x_{ij} = v_j, \\quad \\forall j = 1, \\ldots, K\\] <ol> <li>Each pilot carries the same total number of crates:</li> </ol> \\[\\sum_{j=1}^K x_{ij} = \\frac{V}{N}, \\quad \\forall i = 1, \\ldots, N \\quad \\text{where} \\quad V = \\sum_{j=1}^K v_j\\] <ol> <li>Non-negativity: \\(\\(x_{ij} \\geq 0, \\quad \\forall i,j\\)\\)</li> </ol>"},{"location":"Example/#lp-formulation","title":"LP Formulation","text":"\\[ \\begin{aligned} \\min_{x \\in \\mathbb{R}^{N \\times K}} \\quad &amp; c^T x \\\\ \\text{subject to} \\quad &amp; \\begin{cases} A_{eq} x = b_{eq} \\\\ x \\geq 0 \\end{cases} \\end{aligned}\\] <p>Where:</p> <ul> <li>\\(A_{eq} \\in \\mathbb{R}^{(N + K) \\times (N \\cdot K)}\\) encodes the equality constraints for cargo delivery and load balancing.</li> <li>\\(b_{eq} \\in \\mathbb{R}^{N + K}\\) combines the crate volumes \\(v_j\\) and equal load targets \\(\\frac{V}{N}\\).</li> </ul>"}]}