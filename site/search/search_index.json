{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p> <ul> <li>Concise derivations of essential results</li> <li>Geometric intuition and figures where helpful</li> <li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li> <li>Appendices that extend into more advanced or specialized topics</li> </ul> <p>Whether you\u2019re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p>"},{"location":"appendices/120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"appendices/120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"appendices/120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"appendices/120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"appendices/120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"appendices/120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"appendices/120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"appendices/120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"appendices/120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"appendices/120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"appendices/130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"appendices/140_support/","title":"Appendix C - Support Functions and Dual Geometry","text":""},{"location":"appendices/140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"appendices/140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"appendices/140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"appendices/140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"appendices/140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"appendices/140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"appendices/140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"appendices/140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"appendices/160_conjugates/","title":"Appendix D - Convex Conjugates and Fenchel Duality","text":""},{"location":"appendices/160_conjugates/#appendix-d-convex-conjugates-and-fenchel-duality","title":"Appendix D: Convex Conjugates and Fenchel Duality","text":"<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis. They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p> <ul> <li>Convex conjugates convert a function into its \u201cslope-space\u201d representation \u2014 capturing its tightest linear overestimates.  </li> <li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li> </ul> <p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>"},{"location":"appendices/160_conjugates/#d1-intuitive-picture","title":"D.1 Intuitive Picture","text":"<p>Imagine a convex function \\(f(x)\\) drawn as a bowl in space. Each point \\(y\\) defines a line (or hyperplane) of slope \\(y\\):  The convex conjugate \\(f^*(y)\\) is the smallest height \\(b\\) such that this line always stays above \\(f(x)\\). In other words:</p> <p>\\(f^*(y)\\) measures the tightest linear overestimate of \\(f\\) in direction \\(y\\).</p> <p>So \\(f^*\\) encodes how \u201csteep\u201d \\(f\\) can be in every direction \u2014 it transforms the geometry of \\(f\\) into a new convex function on slope-space.</p>"},{"location":"appendices/160_conjugates/#d2-definition-and-key-properties","title":"D.2 Definition and Key Properties","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}\\) be a proper convex function. Its convex (Fenchel) conjugate is  </p> <p>Interpretation - \\(y\\): a slope or linear functional. - The supremum seeks the largest gap between the linear function \\(\\langle y,x\\rangle\\) and the graph of \\(f\\). - \\(f^*(y)\\) is always convex, even if \\(f\\) isn\u2019t strictly convex.</p>"},{"location":"appendices/160_conjugates/#fundamental-identities","title":"Fundamental Identities","text":"<ol> <li> <p>Fenchel\u2013Young inequality        with equality iff \\(y \\in \\partial f(x)\\).</p> </li> <li> <p>Biconjugation        This tells us the conjugate transform loses no information for convex functions.</p> </li> <li> <p>Order reversal    \\(f \\le g \\;\\Rightarrow\\; f^* \\ge g^*\\).</p> </li> <li> <p>Scaling and shift</p> </li> <li>\\((f + a)^*(y) = f^*(y) - a\\),</li> <li>\\((\\alpha f)^*(y) = \\alpha f^*(y/\\alpha)\\) for \\(\\alpha&gt;0.\\)</li> </ol>"},{"location":"appendices/160_conjugates/#d3-canonical-examples","title":"D.3 Canonical Examples","text":"Function \\(f(x)\\) Conjugate \\(f^*(y)\\) Notes \\( \\tfrac{1}{2}\\|x\\|_2^2 \\) \\( \\tfrac{1}{2}\\|y\\|_2^2 \\) Self-conjugate quadratic \\( \\|x\\|_1 \\) \\( \\delta_{\\{\\|y\\|_\\infty \\le 1\\}}(y) \\) Dual norm indicator \\( \\delta_C(x) \\) \\( \\sigma_C(y)=\\sup_{x\\in C}\\langle y,x\\rangle \\) Support function of set \\(C\\) \\( e^x \\) \\( y\\log y - y,\\, y&gt;0 \\) Appears in entropy and KL-divergence <p>These examples illustrate how conjugation connects: - Norms \u2194 dual norms, - Sets \u2194 support functions, - Exponentials \u2194 entropy, - Quadratics \u2194 themselves.</p>"},{"location":"appendices/160_conjugates/#d4-geometric-interpretation","title":"D.4 Geometric Interpretation","text":"<ul> <li>Each point on \\(f\\) has a tangent hyperplane whose slope is a subgradient.  </li> <li>The collection of all such hyperplanes forms the epigraph of \\(f^*\\).  </li> <li>The transformation \\(f \\mapsto f^*\\) swaps the roles of \u201cposition\u201d and \u201cslope\u201d:   convex geometry \u2194 supporting hyperplanes.</li> </ul> <p>Visually: - \\(f\\) describes a bowl in \\((x,t)\\)-space. - \\(f^*\\) describes the envelope of tangent planes to that bowl.</p>"},{"location":"appendices/160_conjugates/#d5-from-conjugates-to-duality-fenchel-duality","title":"D.5 From Conjugates to Duality \u2014 Fenchel Duality","text":"<p>Many convex optimization problems can be written as  where \\(f,g\\) are convex and \\(A\\) is linear. Fenchel duality uses conjugates to build a dual problem in terms of \\(f^*\\) and \\(g^*\\).</p>"},{"location":"appendices/160_conjugates/#the-fenchel-dual-problem","title":"The Fenchel Dual Problem","text":"\\[ \\max_y \\; -f^*(A^\\top y) - g^*(-y). \\] <p>Interpretation - \\(y\\) is the dual variable (similar to Lagrange multipliers). - The dual objective collects the best linear lower bounds on the primal cost.</p>"},{"location":"appendices/160_conjugates/#d6-weak-and-strong-duality","title":"D.6 Weak and Strong Duality","text":"<ul> <li> <p>Weak duality: For any \\(x,y\\),      So the dual value always underestimates the primal value.</p> </li> <li> <p>Strong duality:   If \\(f,g\\) are closed convex and a mild constraint qualification holds (e.g. Slater\u2019s condition \u2014 existence of strictly feasible \\(x\\)), then    </p> </li> </ul> <p>At the optimum:  These are the Fenchel\u2013KKT conditions, directly linking primal and dual subgradients.</p>"},{"location":"appendices/160_conjugates/#d7-illustrative-examples","title":"D.7 Illustrative Examples","text":""},{"location":"appendices/160_conjugates/#a-linear-programming","title":"(a) Linear Programming","text":"<p>Primal:  </p> <p>Take \\(f(x) = c^\\top x + \\delta_{\\{x\\ge0\\}}(x)\\), \\(g(z)=\\delta_{\\{z=b\\}}(z)\\).</p> <p>Then  </p> <p>Dual:  which is the standard LP dual.</p>"},{"location":"appendices/160_conjugates/#b-quadratic-set-constraint","title":"(b) Quadratic + Set Constraint","text":"<p>Primal:  </p> <p>Then  so the dual is  Optimality gives \\(x^*=y^*\\), the projection condition in Euclidean geometry.</p>"},{"location":"appendices/160_conjugates/#d8-practical-significance","title":"D.8 Practical Significance","text":"Area How Fenchel Duality Appears Optimization theory Derives general dual problems beyond inequality constraints. Algorithm design Basis for primal\u2013dual and splitting methods (ADMM, Chambolle\u2013Pock, Mirror Descent). Geometry Dual problem finds the \u201cbest supporting hyperplane\u201d to the primal epigraph. Machine Learning Loss\u2013regularizer pairs (hinge \u2194 clipped loss, logistic \u2194 log-sum-exp) often form conjugate pairs. Proximal operators Linked via Moreau identity:  \\(\\mathrm{prox}_{f^*}(y) = y - \\mathrm{prox}_f(y)\\)."},{"location":"appendices/160_conjugates/#d9-conceptual-unification","title":"D.9 Conceptual Unification","text":"<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p> <ul> <li>From geometry: support functions, projections, subgradients (Appendices B\u2013C).  </li> <li>From analysis: inequalities like Fenchel\u2019s and Jensen\u2019s (Appendix A).  </li> <li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7\u20138).  </li> <li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9\u201310).</li> </ul> <p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>"},{"location":"appendices/160_conjugates/#d10-summary-and-takeaways","title":"D.10 Summary and Takeaways","text":"<ul> <li>The convex conjugate \\(f^*\\) expresses \\(f\\) through its linear support planes.  </li> <li>The Fenchel\u2013Young inequality connects primal variables and dual slopes.  </li> <li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li> <li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li> <li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li> </ul> <p>Further Reading</p> <ul> <li>Rockafellar, R. T. (1970). Convex Analysis. Princeton UP.  </li> <li>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization, Chs. 3 &amp; 5.  </li> <li>Bauschke, H. H., &amp; Combettes, P. L. (2017). Convex Analysis and Monotone Operator Theory.  </li> <li>Hiriart-Urruty, J.-B., &amp; Lemar\u00e9chal, C. (2001). Fundamentals of Convex Analysis.  </li> </ul>"},{"location":"appendices/170_probability/","title":"Appendix E - Convexity in Probability and Statistics","text":""},{"location":"appendices/170_probability/#appendix-e-convexity-in-probability-and-statistics","title":"Appendix E : Convexity in Probability and Statistics","text":"<p>Convex analysis is not just geometry and optimization \u2014 it is deeply woven into probability, statistics, and information theory. Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p> <p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>"},{"location":"appendices/170_probability/#e1-convexity-of-expectations","title":"E.1 Convexity of Expectations","text":"<p>Let \\(f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}\\) be convex and \\(X\\) a random vector. Then by Jensen\u2019s inequality (Appendix A):</p> \\[ f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]. \\]"},{"location":"appendices/170_probability/#consequences","title":"Consequences","text":"<ul> <li>Expectations preserve convexity:   if each \\(f(\\cdot,\\xi)\\) is convex, then \\(F(x)=\\mathbb{E}_\\xi[f(x,\\xi)]\\) is convex.</li> <li>Stochastic objectives in ML \u2014 e.g. expected loss \\(\\mathbb{E}_{(a,b)}[\\ell(a^\\top x,b)]\\) \u2014 are convex when the sample-wise loss is convex.</li> </ul> <p>Hence almost all empirical risk minimization problems are discrete approximations of convex expectations.</p>"},{"location":"appendices/170_probability/#e2-convexity-of-log-partition-and-moment-generating-functions","title":"E.2 Convexity of Log-Partition and Moment-Generating Functions","text":"<p>For a random variable \\(X\\), the moment-generating function (MGF) and cumulant-generating function (CGF) are</p> \\[ M_X(t)=\\mathbb{E}[e^{tX}], \\qquad K_X(t)=\\log M_X(t). \\] <p>Fact: \\(K_X(t)\\) is always convex in \\(t\\).</p> <p>Reason: \\(K_X''(t)=\\mathrm{Var}_t(X)\\ge0\\); variance is nonnegative.  </p>"},{"location":"appendices/170_probability/#implications","title":"Implications","text":"<ul> <li>\\(K_X(t)\\) acts as a convex \u201cpotential\u201d controlling exponential families.</li> <li>The log-partition function in statistics,      is convex in \\(\\theta\\) (strictly convex for full exponential families).</li> <li>Its gradient gives the mean parameter: \\(\\nabla A(\\theta)=\\mathbb{E}_\\theta[T(X)]\\).</li> </ul> <p>Thus convexity of \\(A\\) guarantees a one-to-one mapping between natural and mean parameters \u2014 a foundation of exponential-family inference.</p>"},{"location":"appendices/170_probability/#e3-exponential-families-and-dual-convexity","title":"E.3 Exponential Families and Dual Convexity","text":"<p>An exponential-family density has the form  </p> <p>Properties:</p> <ol> <li>\\(A(\\theta)\\) is convex, smooth, and serves as a potential function.</li> <li>Its convex conjugate \\(A^*(\\mu)\\) defines the entropy of the family:        where \\(H\\) is the Shannon entropy of the distribution with mean \\(\\mu\\).</li> </ol> <p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>"},{"location":"appendices/170_probability/#e4-convex-divergences-and-information-measures","title":"E.4 Convex Divergences and Information Measures","text":""},{"location":"appendices/170_probability/#a-kullbackleibler-kl-divergence","title":"(a) Kullback\u2013Leibler (KL) Divergence","text":"<p>For densities \\(p,q\\),  </p> <ul> <li>\\(D_{\\mathrm{KL}}\\) is jointly convex in \\((p,q)\\).  </li> <li>Proof: the function \\((u,v)\\mapsto u\\log(u/v)\\) is convex on \\(\\mathbb{R}_+^2\\).  </li> <li>Consequently, mixtures of distributions cannot increase KL divergence \u2014 a key fact in variational inference and EM.</li> </ul>"},{"location":"appendices/170_probability/#b-bregman-divergences","title":"(b) Bregman Divergences","text":"<p>Given a differentiable convex \\(\\phi\\), define  KL divergence is a Bregman divergence for \\(\\phi(p)=\\sum_i p_i\\log p_i\\). Thus information-theoretic distances are geometric shadows of convex functions.</p>"},{"location":"appendices/170_probability/#c-f-divergences","title":"(c) f-Divergences","text":"<p>A general convex generator \\(f\\) with \\(f(1)=0\\) yields  Convexity of \\(f\\) \u21d2 convexity of \\(D_f\\). Common choices recover KL, \u03c7\u00b2, Hellinger, and Jensen\u2013Shannon divergences.</p>"},{"location":"appendices/170_probability/#e5-convex-loss-functions-in-statistics-and-machine-learning","title":"E.5 Convex Loss Functions in Statistics and Machine Learning","text":"<p>Convexity ensures estimators are globally optimal and algorithms converge.</p> Setting Loss / Negative Log-Likelihood Convexity Gaussian noise \\(\\tfrac12\\|Ax-b\\|_2^2\\) quadratic, strongly convex Laplace noise \\(\\|Ax-b\\|_1\\) convex, nonsmooth Logistic regression \\(\\log(1+e^{-y a^\\top x})\\) convex, smooth Poisson regression \\(e^{a^\\top x}-y a^\\top x\\) convex, exponential Huber loss piecewise quadratic/linear convex, robust <p>Convexity of the negative log-likelihood follows from convexity of the log-partition function \\(A(\\theta)\\) in exponential families.</p>"},{"location":"appendices/170_probability/#e6-convexity-and-bayesian-inference","title":"E.6 Convexity and Bayesian Inference","text":"<p>In Bayesian inference, convexity appears in:</p> <ul> <li> <p>Log-concave posteriors:   If the likelihood and prior are log-concave, the posterior \\(p(x|y)\\propto \\exp(-f(x))\\) is also log-concave \u21d2 \\(\\log p(x|y)\\) concave, \\(f(x)\\) convex.</p> </li> <li> <p>MAP estimation:   Maximizing \\(\\log p(x|y)\\) \u2261 minimizing a convex function when \\(p(x|y)\\) is log-concave \u21d2 global optimum guaranteed.</p> </li> <li> <p>Variational inference:   The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).   Optimizing it is equivalent to minimizing a convex divergence.</p> </li> </ul> <p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>"},{"location":"appendices/170_probability/#e7-statistical-risk-and-convex-surrogates","title":"E.7 Statistical Risk and Convex Surrogates","text":"<p>Convex surrogate losses replace nonconvex 0\u20131 loss with convex approximations:</p> <ul> <li>Hinge loss (\\(\\max(0,1-y a^\\top x)\\)) \u2192 support-vector machines.  </li> <li>Logistic loss \u2192 probabilistic classification (cross-entropy).  </li> <li>Exponential loss \u2192 AdaBoost.</li> </ul> <p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p>"},{"location":"appendices/180_subgradient_methods/","title":"Appendix F - Subgradient Method and Variants","text":""},{"location":"appendices/180_subgradient_methods/#appendix-f-subgradient-method-derivation-geometry-and-convergence","title":"Appendix F: Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>This appendix presents the subgradient method\u2014the fundamental algorithm for minimizing nonsmooth convex functions. It generalizes gradient descent to functions such as the \\(\\ell_1\\) norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>"},{"location":"appendices/180_subgradient_methods/#f1-problem-setup","title":"F.1 Problem Setup","text":"<p>We consider</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but possibly nondifferentiable and \\(\\mathcal{X}\\) is a convex feasible set.</p>"},{"location":"appendices/180_subgradient_methods/#f2-subgradients-and-geometry","title":"F.2 Subgradients and Geometry","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) satisfies</p> \\[ f(y) \\ge f(x_t) + \\langle g_t,\\, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>If \\(f\\) is differentiable, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).  </li> <li>At a nonsmooth point (e.g. \\(|x|\\) at \\(x=0\\)), \\(\\partial f(x_t)\\) is a set of supporting slopes.  </li> <li>Each subgradient defines a supporting hyperplane below the graph of \\(f\\).</li> </ul> <p>Hence a subgradient gives a descent direction even when \\(f\\) lacks a unique gradient.</p>"},{"location":"appendices/180_subgradient_methods/#f3-update-rule-and-projection-view","title":"F.3 Update Rule and Projection View","text":"<p>The projected subgradient step is</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}}\\!\\big(x_t - \\eta_t g_t\\big), \\] <p>where - \\(g_t \\in \\partial f(x_t)\\), - \\(\\eta_t&gt;0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) projects onto \\(\\mathcal{X}\\).</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\), projection disappears:  </p> <p>Geometric view: move in a subgradient direction, then project back to feasibility. The method \u201cslides\u201d along the edges of \\(f\\)\u2019s epigraph.</p>"},{"location":"appendices/180_subgradient_methods/#f4-distance-analysis","title":"F.4 Distance Analysis","text":"<p>Let \\(x^\\star\\) be an optimal solution. Expanding the squared distance:</p> \\[ \\|x_{t+1}-x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t\\langle g_t, x_t - x^\\star\\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity,  </p> <p>Substitute to get</p> \\[ \\|x_{t+1}-x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t\\big(f(x_t)-f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"appendices/180_subgradient_methods/#f5-bounding-suboptimality","title":"F.5 Bounding Suboptimality","text":"<p>Rearranging:</p> \\[ f(x_t)-f(x^\\star) \\le \\frac{\\|x_t-x^\\star\\|^2 - \\|x_{t+1}-x^\\star\\|^2}{2\\eta_t} + \\frac{\\eta_t}{2}\\|g_t\\|^2. \\] <p>This shows a trade-off:</p> <ul> <li>Large \\(\\eta_t\\) \u2192 faster steps but higher error term.  </li> <li>Small \\(\\eta_t\\) \u2192 more precise but slower progress.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f6-convergence-rate","title":"F.6 Convergence Rate","text":"<p>Assume \\(\\|g_t\\| \\le G\\). Summing over \\(t=0,\\dots,T-1\\):</p> \\[ \\sum_{t=0}^{T-1}\\!\\big(f(x_t)-f(x^\\star)\\big) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Define \\(\\bar{x}_T = \\tfrac{1}{T}\\sum_{t=0}^{T-1} x_t\\). By convexity of \\(f\\),</p> \\[ f(\\bar{x}_T)-f(x^\\star) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing \\(\\eta_t = \\tfrac{R}{G\\sqrt{T}}\\) with \\(R=\\|x_0-x^\\star\\|\\) yields</p> <p>  i.e. a sublinear rate \\(O(1/\\sqrt{T})\\).</p>"},{"location":"appendices/180_subgradient_methods/#f7-interpretation-and-practice","title":"F.7 Interpretation and Practice","text":"<ul> <li>Works for any convex function, smooth or not.  </li> <li>Converges slower than smooth-gradient methods (\\(O(1/T)\\) or linear), but applies more generally.  </li> <li>Step size schedule is crucial: \\(\\eta_t \\!\\downarrow 0\\) for convergence, or fixed \\(\\eta\\) for steady error.  </li> <li>Averaging \\(\\bar{x}_T\\) improves stability.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#typical-ml-uses","title":"Typical ML Uses","text":"Model Objective Nonsmooth Term LASSO \\(\\tfrac12\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1\\) \\(\\ell_1\\) penalty SVM \\(\\tfrac12\\|w\\|^2 + C\\sum_i \\max(0,1-y_i w^\\top x_i)\\) hinge loss Robust regression $\\sum_i a_i^\\top x - b_i Neural nets \\(\\|w\\|_1\\) or ReLU activations piecewise linear"},{"location":"appendices/180_subgradient_methods/#f8-beyond-basic-subgradients","title":"F.8 Beyond Basic Subgradients","text":"<p>Many advanced methods refine or accelerate the basic idea:</p> <ul> <li>Stochastic subgradients: sample-based updates for large-scale ML.  </li> <li>Mirror descent: adapt geometry via Bregman divergences.  </li> <li>Proximal methods: replace step with proximal operator (see Appendix B).  </li> <li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f9-summary","title":"F.9 Summary","text":"<ul> <li>Subgradients generalize gradients to nondifferentiable convex functions.  </li> <li>The projected subgradient method provides a universal, robust minimization algorithm.  </li> <li>Achieves \\(O(1/\\sqrt{T})\\) convergence under bounded subgradients.  </li> <li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9\u201310.</li> </ul>"},{"location":"appendices/190_proximal/","title":"Appendix G - Proximal Operators","text":""},{"location":"appendices/190_proximal/#appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization","title":"Appendix G | Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>Many convex optimization problems involve constraints or nonsmooth penalties. This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>"},{"location":"appendices/190_proximal/#g1-problem-setup","title":"G.1 Problem Setup","text":"<p>We wish to minimize a convex, differentiable function \\( f(x) \\) subject to a convex feasible set \\( \\mathcal{X} \\subseteq \\mathbb{R}^n \\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>A plain gradient step,</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>may leave \\( x_{t+1} \\notin \\mathcal{X} \\). We fix this by projecting the iterate back into the feasible region.</p>"},{"location":"appendices/190_proximal/#g2-projection-operator","title":"G.2 Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2. \\] <p>Hence, the projected gradient descent update is</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\]"},{"location":"appendices/190_proximal/#geometric-insight","title":"Geometric Insight","text":"<ul> <li>Take a descent step possibly outside the feasible set.  </li> <li>Project back to the closest feasible point.  </li> <li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li> </ul> <p>Example \u2014 Euclidean ball: If \\( \\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\} \\), then</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)}. \\] <ul> <li>Inside the ball \u2192 unchanged.  </li> <li>Outside \u2192 scaled back to the boundary.</li> </ul>"},{"location":"appendices/190_proximal/#g3-from-projections-to-proximal-operators","title":"G.3 From Projections to Proximal Operators","text":"<p>Projections handle explicit constraints, but many problems use implicit penalties \u2014 e.g. sparsity (\\(\\|x\\|_1\\)), total variation, or nonnegativity penalties.</p> <p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>"},{"location":"appendices/190_proximal/#definition","title":"Definition","text":"<p>For a convex (possibly nondifferentiable) function \\( g(x) \\),</p> <p>  where \\( \\lambda &gt; 0 \\) balances regularization vs. proximity.</p>"},{"location":"appendices/190_proximal/#interpretation","title":"Interpretation","text":"<ul> <li>The quadratic term \\( \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\) keeps \\(x\\) close to \\(y\\).  </li> <li>The function \\( g(x) \\) encourages structure (sparsity, smoothness, feasibility).  </li> <li>Small \\(\\lambda\\): conservative correction; large \\(\\lambda\\): stronger regularization.</li> </ul> <p>The proximal step acts as a soft correction after a gradient step.</p>"},{"location":"appendices/190_proximal/#g4-projection-as-a-special-case","title":"G.4 Projection as a Special Case","text":"<p>Define the indicator function of a convex set \\(\\mathcal{X}\\):</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X}, \\\\[4pt] +\\infty, &amp; x \\notin \\mathcal{X}. \\end{cases} \\] <p>Substitute \\(g(x)=I_{\\mathcal{X}}(x)\\) into the proximal definition:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\Big) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y). \\] <p>\u2705 Projection is just a proximal operator for an indicator function.</p>"},{"location":"appendices/190_proximal/#g5-proximal-gradient-method","title":"G.5 Proximal Gradient Method","text":"<p>When minimizing a composite convex objective  where \\(f\\) is smooth and \\(g\\) convex (possibly nonsmooth), the proximal gradient method updates:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\] <ul> <li>The gradient step reduces the smooth part \\(f(x)\\).  </li> <li>The proximal step enforces structure via \\(g(x)\\). This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li> </ul>"},{"location":"appendices/190_proximal/#g6-example-proximal-operator-of-the-ell_1-norm","title":"G.6 Example: Proximal Operator of the \\(\\ell_1\\)-Norm","text":"<p>We seek</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda\\|x\\|_1 + \\tfrac{1}{2}\\|x - y\\|^2 \\right). \\]"},{"location":"appendices/190_proximal/#step-1-coordinate-separation","title":"Step 1. Coordinate Separation","text":"<p>The problem is separable across coordinates:  so each coordinate solves  </p>"},{"location":"appendices/190_proximal/#step-2-subgradient-optimality","title":"Step 2. Subgradient Optimality","text":"<p>Optimality condition:  Thus,  </p>"},{"location":"appendices/190_proximal/#step-3-case-analysis","title":"Step 3. Case Analysis","text":"Case Condition Solution \\(x^\\star&gt;0\\) \\(y&gt;\\lambda\\) \\(x^\\star = y - \\lambda\\) \\(x^\\star&lt;0\\) \\(y&lt;-\\lambda\\) \\(x^\\star = y + \\lambda\\) \\(x^\\star=0\\) ( y"},{"location":"appendices/190_proximal/#step-4-compact-form","title":"Step 4. Compact Form","text":"\\[ \\boxed{ \\text{prox}_{\\lambda|\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda,\\, 0) } \\] <p>This is the soft-thresholding operator.</p>"},{"location":"appendices/190_proximal/#step-5-vector-case","title":"Step 5. Vector Case","text":"<p>For \\(y \\in \\mathbb{R}^n\\),</p> \\[ \\big(\\text{prox}_{\\lambda\\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i)\\cdot\\max(|y_i| - \\lambda, 0). \\] <p>Each coordinate is independently shrunk toward zero \u2014 producing sparse solutions.</p>"},{"location":"appendices/190_proximal/#step-6-interpretation","title":"Step 6. Interpretation","text":"<ul> <li>Coordinates with \\(|y_i| \\le \\lambda\\) \u2192 set to zero (promotes sparsity).  </li> <li>Coordinates with \\(|y_i| &gt; \\lambda\\) \u2192 shrink by \\(\\lambda\\).  </li> <li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li> </ul>"},{"location":"appendices/190_proximal/#g7-geometry-and-connection-to-algorithms","title":"G.7 Geometry and Connection to Algorithms","text":"<ul> <li>Projection = nearest feasible point \u2192 handles hard constraints.  </li> <li>Proximal operator = nearest structured point \u2192 handles soft regularization.  </li> <li>Proximal gradient = combines both, yielding algorithms like:</li> <li>ISTA / FISTA (sparse recovery, LASSO),</li> <li>Projected gradient (feasibility),</li> <li>ADMM (splitting into subproblems).</li> </ul> <p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>"},{"location":"appendices/190_proximal/#g8-summary","title":"G.8 Summary","text":"<ul> <li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li> <li>Projection is a special case of the proximal operator for an indicator function.  </li> <li>Proximal mappings handle nonsmooth regularizers (e.g., \\(\\ell_1\\)-norm).  </li> <li>The proximal gradient method unifies constrained and regularized optimization.  </li> <li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li> </ul>"},{"location":"appendices/200_mirror/","title":"Appendix H - Mirror Descent and Bregman Geometry","text":""},{"location":"appendices/200_mirror/#appendix-h-mirror-descent-and-bregman-geometry","title":"Appendix H: Mirror Descent and Bregman Geometry","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry. In many structured domains\u2014such as probability simplices or sparse models\u2014Euclidean updates can destroy problem structure or cause instability.  </p> <p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence. It performs gradient-like updates in a dual space, respecting the intrinsic geometry of the domain.</p>"},{"location":"appendices/200_mirror/#h1-motivation-and-limitations-of-euclidean-gd","title":"H.1 Motivation and Limitations of Euclidean GD","text":"<p>Standard GD update:  assumes Euclidean distance  </p> <p>This works well in \\(\\mathbb{R}^n\\) without structure, but fails to respect constraints or sparsity.</p> <p>In practice:</p> <ul> <li>Many parameters are nonnegative or normalized (probabilities, weights).  </li> <li>Euclidean steps can violate constraints or zero out coordinates.  </li> <li>The \u201cflat\u201d \\(\\ell_2\\) geometry treats all directions equally.</li> </ul> <p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the metric via a mirror map.</p>"},{"location":"appendices/200_mirror/#h2-geometry-in-optimization","title":"H.2 Geometry in Optimization","text":"<p>The \u201csteepest descent\u201d direction depends on the notion of distance. GD implicitly minimizes a linearized loss plus a Euclidean proximity term.</p> Scenario Natural Constraint Appropriate Geometry Probability vectors \\(x_i\\ge0, \\sum_i x_i=1\\) KL / entropy geometry Sparse models \\(\\|x\\|_1\\)-structured \\(\\ell_1\\) geometry Online learning multiplicative updates log-space geometry <p>Using Euclidean projections in these domains can cause:</p> <ul> <li>abrupt projection onto boundaries,</li> <li>loss of positivity or sparsity,</li> <li>geometric inconsistency.</li> </ul>"},{"location":"appendices/200_mirror/#h3-mirror-descent-framework","title":"H.3 Mirror Descent Framework","text":"<p>Let \\(\\psi(x)\\) be a mirror map \u2014 a strictly convex, differentiable potential encoding the geometry.</p> <p>Define the dual coordinate:  and its inverse mapping through the convex conjugate \\(\\psi^*\\):  </p>"},{"location":"appendices/200_mirror/#bregman-divergence","title":"Bregman Divergence","text":"<p>The geometry is quantified by the Bregman divergence:  </p> <ul> <li>Measures how nonlinear \\(\\psi\\) is between \\(x\\) and \\(y\\).  </li> <li>When \\(\\psi(x)=\\tfrac12\\|x\\|_2^2\\), \\(D_\\psi\\) becomes \\(\\tfrac12\\|x-y\\|_2^2\\).  </li> <li>When \\(\\psi(x)=\\sum_i x_i\\log x_i\\), \\(D_\\psi\\) becomes KL divergence.</li> </ul>"},{"location":"appendices/200_mirror/#h4-mirror-descent-update-rule","title":"H.4 Mirror Descent Update Rule","text":"<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:  </p> <p>Equivalent dual-space form:  </p> <p>\u2705 MD is gradient descent in dual coordinates, where distances are measured by \\(D_\\psi\\) instead of \\(\\|x-y\\|_2\\).</p>"},{"location":"appendices/200_mirror/#h5-comparing-gd-projected-gd-and-md","title":"H.5 Comparing GD, Projected GD, and MD","text":"Method Update Rule Geometry Comments Gradient Descent \\(x - \\eta\\nabla f\\) Euclidean may leave feasible set Projected GD \\(\\text{Proj}(x - \\eta\\nabla f)\\) Euclidean + projection can cause discontinuous jumps Mirror Descent \\(\\arg\\min_x \\langle\\nabla f, x - x_t\\rangle + \\frac{1}{\\eta}D_\\psi(x\\|x_t)\\) Bregman smooth, structure-preserving"},{"location":"appendices/200_mirror/#h6-simplex-example-kl-geometry","title":"H.6 Simplex Example (KL Geometry)","text":"<p>Let \\(x\\in\\Delta^2=\\{x\\ge0, x_1+x_2=1\\}\\), objective \\(f(x)=x_1^2+2x_2\\), \\(\\eta=0.3\\).</p>"},{"location":"appendices/200_mirror/#euclidean-gd-projection","title":"Euclidean GD + Projection","text":"<ol> <li>\\(\\nabla f=(2x_1,2)=(1,2)\\),  </li> <li>\\(y=x-\\eta\\nabla f=(0.2,-0.1)\\),  </li> <li>Project \u2192 \\(x_{new}=(1,0)\\).</li> </ol> <p>\u2192 Projection kills one coordinate \u21d2 lost smoothness.</p>"},{"location":"appendices/200_mirror/#mirror-descent-with-negative-entropy","title":"Mirror Descent with Negative Entropy","text":"<p>Mirror map \\(\\psi(x)=\\sum_i x_i\\log x_i\\). Update:  Gives \\(x\\approx(0.57,0.43)\\) \u2014 smooth, positive, stays in simplex.</p> <p>MD follows the manifold of the simplex naturally\u2014no harsh projection.</p>"},{"location":"appendices/200_mirror/#h7-choosing-the-mirror-map","title":"H.7 Choosing the Mirror Map","text":"Mirror Map \\(\\psi(x)\\) Bregman Divergence \\(D_\\psi\\) Typical Domain / Application \\(\\tfrac12\\|x\\|_2^2\\) Euclidean distance unconstrained \\(\\mathbb{R}^n\\) \\(\\sum_i x_i\\log x_i\\) KL divergence simplex, probabilities \\(\\|x\\|_1\\) or variants \\(\\ell_1\\) geometry sparse models log-barrier \\(\\sum_i -\\log x_i\\) barrier divergence positive orthant <p>Mirror maps act as design choices defining the optimization geometry.</p>"},{"location":"appendices/200_mirror/#h8-practical-remarks","title":"H.8 Practical Remarks","text":"<p>When to prefer Mirror Descent:</p> <ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates desired</li> <li>Avoiding discontinuous projections</li> </ul> <p>Computational notes:</p> <ul> <li>Some \\(\\psi\\) yield closed-form updates (e.g. multiplicative weights).  </li> <li>Works with adaptive or momentum step-size schemes.  </li> <li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li> </ul>"},{"location":"appendices/200_mirror/#h9-convergence-at-a-glance","title":"H.9 Convergence at a Glance","text":"<p>For convex \\(f\\) with bounded gradients \\(\\|\\nabla f\\|\\le G\\) and strong convex mirror map \\(\\psi\\), Mirror Descent achieves the same sublinear rate as projected subgradient methods:  but with improved geometry-adapted constants that exploit curvature of \\(\\psi\\).</p>"},{"location":"appendices/300_matrixfactorization/","title":"Appendix I - Matrix Factorization","text":""},{"location":"appendices/300_matrixfactorization/#numerical-linear-algebra-for-convex-optimization","title":"Numerical Linear Algebra for Convex Optimization","text":"<p>Numerical linear algebra is the computational foundation of convex optimization. Every modern optimization algorithm \u2014 from Newton\u2019s method to interior-point or proximal algorithms \u2014 ultimately requires solving a structured linear system:  where \\(H\\) may represent a Hessian, a normal equations matrix, or a KKT (Karush\u2013Kuhn\u2013Tucker) system.</p> <p>In practice, we never compute \\(H^{-1}\\) directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>"},{"location":"appendices/300_matrixfactorization/#1-why-linear-algebra-matters-in-convex-optimization","title":"1. Why Linear Algebra Matters in Convex Optimization","text":"<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p> <ul> <li> <p>Newton\u2019s method:    </p> </li> <li> <p>Interior-point methods (KKT systems):</p> </li> </ul> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\[3pt] \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -r_d \\\\[3pt] -r_p \\end{bmatrix} \\] <ul> <li>Least-squares problems: \\(A^T A x = A^T b\\)</li> </ul> <p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>"},{"location":"appendices/300_matrixfactorization/#2-the-matrix-factorization-toolbox","title":"2. The Matrix Factorization Toolbox","text":"<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure. They enable efficient triangular solves instead of direct inversion.</p> Factorization Applies To Form Common Use Key Notes LU Any nonsingular matrix \\(A = L U\\) General linear systems Requires pivoting for stability QR Any (rectangular) matrix \\(A = Q R\\) Least-squares Orthogonal, stable Cholesky Symmetric positive definite \\(A = L L^T\\) SPD systems, normal equations Fastest for SPD \\(LDL^T\\) Symmetric indefinite \\(A = L D L^T\\) KKT systems Handles indefiniteness Eigen Symmetric/Hermitian \\(A = Q \\Lambda Q^T\\) Curvature, convexity checks Diagonalizes \\(A\\) SVD Any matrix \\(A = U \\Sigma V^T\\) Rank, conditioning, pseudoinverse Most stable, expensive <p>Each factorization corresponds to a numerically preferred strategy for certain classes of problems.</p>"},{"location":"appendices/300_matrixfactorization/#3-lu-factorization-the-general-purpose-workhorse","title":"3. LU Factorization \u2014 The General-Purpose Workhorse","text":"<p>Form:  where \\(P\\) is a permutation matrix ensuring stability.</p> <ul> <li>Used for: General linear systems, nonsymmetric matrices.</li> <li>Cost: \\(\\approx \\tfrac{2}{3}n^3\\) (dense).</li> <li>Stability: Requires partial pivoting (\\(PA=LU\\)) to avoid numerical blow-up.</li> </ul> <p>Example use case:</p> <ul> <li>Solving KKT systems in linear programming (LP simplex tableau).</li> <li>Small dense systems with no symmetry or SPD property.</li> </ul> <p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or \\(LDL^T\\).</p>"},{"location":"appendices/300_matrixfactorization/#4-qr-factorization-orthogonal-and-stable","title":"4. QR Factorization \u2014 Orthogonal and Stable","text":"<p>Form:  </p> <ul> <li>Used for: Least-squares problems      Instead of forming normal equations (\\(A^T A x = A^T b\\)), we solve:    </li> <li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li> </ul> <p>Example use cases:</p> <ul> <li>Linear regression via least squares.</li> <li>ADMM and proximal steps with overdetermined systems.</li> <li>Orthogonal projections in signal processing.</li> </ul> <p>Variants:</p> <ul> <li>Householder QR: numerically robust, used in LAPACK.</li> <li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#5-cholesky-factorization-fastest-for-spd-systems","title":"5. Cholesky Factorization \u2014 Fastest for SPD Systems","text":"<p>Form:  Applicable when \\(A\\) is symmetric positive definite (SPD) \u2014 common in convex problems.</p> <p>Why it\u2019s central: Convexity ensures \\(A \\succeq 0\\). For strictly convex problems, \\(A \\succ 0\\) and Cholesky is the most efficient and stable method.</p> <p>Cost: \\(\\tfrac{1}{3}n^3\\) operations \u2014 half of LU.</p> <p>Example use cases:</p> <ul> <li>Newton\u2019s method on unconstrained convex functions.</li> <li>Solving normal equations \\(A^T A x = A^T b\\).</li> <li>QP subproblems and ridge regression.</li> </ul> <p>Implementation detail: No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>"},{"location":"appendices/300_matrixfactorization/#6-ldlt-factorization-for-indefinite-symmetric-systems","title":"6. LDL\u1d40 Factorization \u2014 For Indefinite Symmetric Systems","text":"<p>Form:  where \\(D\\) is block diagonal (1\u00d71 or 2\u00d72 blocks), and \\(L\\) is unit lower triangular.</p> <p>Used when \\(A\\) is symmetric but not SPD (e.g., KKT systems).</p> <p>Example use cases:</p> <ul> <li> <p>Interior-point methods for QPs and SDPs:    </p> </li> <li> <p>Equality-constrained least-squares.</p> </li> <li>Sparse symmetric indefinite systems in primal-dual algorithms.</li> </ul> <p>Algorithmic note: Uses Bunch\u2013Kaufman pivoting to maintain numerical stability. In practice, LDL\u1d40 is used with sparse reordering and partial elimination.</p>"},{"location":"appendices/300_matrixfactorization/#7-block-systems-and-the-schur-complement","title":"7. Block Systems and the Schur Complement","text":"<p>Many KKT or structured systems naturally appear in block form:  </p> <p>Assuming \\(A_{11}\\) is invertible:</p> <ol> <li>Eliminate \\(x_1\\):     </li> <li>Substitute into the second block:     </li> </ol> <p>The matrix  is the Schur complement of \\(A_{11}\\) in \\(A\\).</p>"},{"location":"appendices/300_matrixfactorization/#schur-complement-in-optimization","title":"Schur Complement in Optimization","text":"<ul> <li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li> <li>Preserves symmetry and often positive definiteness.</li> <li>Foundation of block elimination and reduced Hessian methods.</li> </ul> <p>Example use cases:</p> <ul> <li>Interior-point Newton systems (eliminate \\(\\Delta x\\) to get a system in \\(\\Delta \\lambda\\)).</li> <li>Partial elimination in sequential quadratic programming (SQP).</li> <li>Covariance conditioning and Gaussian marginalization.</li> </ul> <p>Numerical caution: Never form \\(A_{11}^{-1}\\) explicitly \u2014 use triangular solves via Cholesky or LU.</p>"},{"location":"appendices/300_matrixfactorization/#8-block-elimination-algorithm","title":"8. Block Elimination Algorithm","text":"<p>Given a nonsingular \\(A_{11}\\):</p> <ol> <li>Compute \\(A_{11}^{-1}A_{12}\\) and \\(A_{11}^{-1}b_1\\) by solving triangular systems.</li> <li>Form \\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\\), \\(\\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\\).</li> <li>Solve \\(Sx_2 = \\tilde{b}\\).</li> <li>Recover \\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\\).</li> </ol> <p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p> <p>Example use case:</p> <ul> <li>Partitioned least-squares with fixed and variable parameters.</li> <li>Constrained optimization where some variables can be analytically eliminated.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#9-structured-plus-low-rank-matrices","title":"9. Structured Plus Low-Rank Matrices","text":"<p>Suppose we need to solve:  where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{n \\times n}\\) is structured or easily invertible (e.g., diagonal or sparse),</li> <li>\\(B \\in \\mathbb{R}^{n \\times p}\\), \\(C \\in \\mathbb{R}^{p \\times n}\\) are low rank.</li> </ul> <p>This situation arises when updating an existing system with a small modification.</p>"},{"location":"appendices/300_matrixfactorization/#block-reformulation","title":"Block Reformulation","text":"<p>Introduce \\(y = Cx\\), yielding:</p> <p>$$   =</p> <p> . $$</p> <p>Block elimination gives:  </p>"},{"location":"appendices/300_matrixfactorization/#matrix-inversion-lemma-woodbury-identity","title":"Matrix Inversion Lemma (Woodbury Identity)","text":"<p>If \\(A\\) and \\(A + BC\\) are nonsingular:  </p> <p>Example use cases:</p> <ul> <li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li> <li>Ridge regression / kernel methods: low-rank updates to \\((X^T X + \\lambda I)^{-1}\\).</li> <li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li> </ul> <p>Numerical note: Avoid explicit inversion; use solves with \\(A\\) and small dense matrices.</p>"},{"location":"appendices/300_matrixfactorization/#10-conditioning-stability-and-sparsity","title":"10. Conditioning, Stability, and Sparsity","text":""},{"location":"appendices/300_matrixfactorization/#conditioning","title":"Conditioning","text":"<ul> <li>Condition number: \\(\\kappa(A) = |A||A^{-1}|\\) measures sensitivity to perturbations.</li> <li>High \\(\\kappa(A)\\) \u21d2 round-off errors amplified \u21d2 ill-conditioning.</li> <li>Regularization (adding \\(\\lambda I\\)) improves numerical stability.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#stability","title":"Stability","text":"<ul> <li>Orthogonal transformations (QR, SVD) are backward stable.</li> <li>LU needs partial pivoting.</li> <li>LDL\u1d40 needs symmetric pivoting (Bunch\u2013Kaufman).</li> <li>Cholesky is stable for SPD matrices.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#sparsity-and-fill-in","title":"Sparsity and Fill-In","text":"<ul> <li>Large convex solvers exploit sparse Cholesky / LDL\u1d40.</li> <li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li> <li>Symbolic factorization determines the pattern before numeric factorization.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#11-iterative-solvers-and-preconditioning","title":"11. Iterative Solvers and Preconditioning","text":"<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>"},{"location":"appendices/300_matrixfactorization/#common-iterative-methods","title":"Common Iterative Methods","text":"Method For Description CG SPD systems Uses matrix\u2013vector products; converges in \u2264 n steps MINRES / SYMMLQ Symmetric indefinite Handles KKT and saddle-point systems GMRES / BiCGSTAB Nonsymmetric General-purpose Krylov solvers"},{"location":"appendices/300_matrixfactorization/#preconditioning","title":"Preconditioning","text":"<p>Preconditioners \\(M \\approx A^{-1}\\) improve convergence:</p> <ul> <li>Jacobi (diagonal): \\(M = \\text{diag}(A)^{-1}\\)</li> <li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li> <li>Block preconditioners: use Schur complement approximations for KKT systems</li> </ul> <p>Example use case:</p> <ul> <li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li> <li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#12-eigenvalue-and-svd-decompositions","title":"12. Eigenvalue and SVD Decompositions","text":""},{"location":"appendices/300_matrixfactorization/#eigenvalue-decomposition","title":"Eigenvalue Decomposition","text":"<p>  Reveals curvature, stability, and definiteness:</p> <ul> <li>Convexity \u21d4 \\(\\Lambda \\ge 0\\).</li> <li>Used in semidefinite programming (SDP) and spectral analysis.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>  with \\(\\Sigma = \\text{diag}(\\sigma_i) \\ge 0\\).</p> <p>Applications:</p> <ul> <li>Rank and condition number estimation (\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\)).</li> <li>Low-rank approximation (\\(A_k = U_k \\Sigma_k V_k^T\\)).</li> <li>Pseudoinverse: \\(A^+ = V \\Sigma^{-1} U^T\\).</li> <li>Convex relaxations: nuclear-norm minimization (matrix completion).</li> </ul>"},{"location":"appendices/300_matrixfactorization/#13-computational-complexity-summary","title":"13. Computational Complexity Summary","text":"Factorization Dense Cost Notes LU \\(\\frac{2}{3}n^3\\) Needs pivoting Cholesky \\(\\frac{1}{3}n^3\\) Fastest for SPD QR \\(\\approx \\frac{2}{3}n^3\\) Stable, more memory LDL\u1d40 \\(\\approx \\frac{2}{3}n^3\\) For indefinite SVD \\(\\approx \\frac{4}{3}n^3\\) Most accurate CG / MINRES Variable Depends on condition number and preconditioning <p>Sparse systems reduce cost to roughly \\(O(n^{1.5})\\)\u2013\\(O(n^2)\\) depending on fill-in.</p>"},{"location":"appendices/300_matrixfactorization/#14-example-applications-overview","title":"14. Example Applications Overview","text":"Problem Type Typical Matrix Solver / Factorization Example Unconstrained Newton step SPD Hessian Cholesky Convex quadratic, ridge regression Equality-constrained QP Symmetric indefinite KKT LDL\u1d40 Interior-point QP solver Overdetermined LS Rectangular \\(A\\) QR Linear regression, ADMM subproblem KKT block system Block-symmetric Schur complement Primal-dual method Low-rank correction \\(A + U U^T\\) Woodbury Kalman filter, online update Rank-deficient system Any SVD Matrix completion, regularization Large-scale Hessian SPD CG + preconditioner Logistic regression, large ML models"},{"location":"cheatsheets/20a_cheatsheet/","title":"Optimization Algos - Cheat Sheet","text":""},{"location":"cheatsheets/20a_cheatsheet/#comprehensive-optimization-algorithm-cheat-sheet","title":"Comprehensive Optimization Algorithm Cheat Sheet","text":"<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search. It balances theoretical precision with practical intuition\u2014from gradient-based solvers to black-box evolutionary methods.</p>"},{"location":"cheatsheets/20a_cheatsheet/#how-to-read-this-table","title":"\ud83e\udded How to Read This Table","text":"<p>Each method lists: - Problem Type \u2014 the class of objectives it applies to. - Assumptions \u2014 smoothness, convexity, or structural conditions. - Core Update Rule \u2014 canonical iteration. - Scalability \u2014 computational feasibility. - Per-Iteration Cost \u2014 approximate computational complexity. - Applications \u2014 typical ML or engineering use cases.</p>"},{"location":"cheatsheets/20a_cheatsheet/#first-order-methods","title":"\ud83d\ude80 First-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Gradient Descent (GD) Unconstrained smooth (convex/nonconvex) Differentiable; \\(L\\)-smooth \\(x_{k+1} = x_k - \\eta \\nabla f(x_k)\\) Medium \\(O(nd)\\) Logistic regression, least squares Nesterov\u2019s Accelerated GD Smooth convex (fast rate) Convex, \\(L\\)-smooth \\(y_k = x_k + \\frac{k-1}{k+2}(x_k - x_{k-1})\\); \\(x_{k+1} = y_k - \\eta \\nabla f(y_k)\\) Medium \\(O(nd)\\) Accelerated convex models (Polyak) Heavy-Ball Momentum Unconstrained smooth Differentiable, \\(\\beta \\in (0,1)\\) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k) + \\beta(x_k - x_{k-1})\\) Large \\(O(nd)\\) Deep networks, convex smooth losses Conjugate Gradient (CG) Quadratic or linear systems \\(Ax=b\\) \\(A\\) symmetric positive definite \\(p_{k+1}=r_{k+1}+\\beta_k p_k\\), \\(x_{k+1}=x_k+\\alpha_k p_k\\) Large \\(O(nd)\\) Large-scale least squares, implicit Newton steps Mirror Descent Non-Euclidean geometry Convex; mirror map \\(\\psi\\) strongly convex \\(x_{k+1} = \\nabla \\psi^*(\\nabla \\psi(x_k) - \\eta \\nabla f(x_k))\\) Medium \\(O(nd)\\) Probability simplex, online learning <p>Conjugate Gradient (CG) bridges first- and second-order methods: it achieves exact convergence in at most \\(d\\) steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>"},{"location":"cheatsheets/20a_cheatsheet/#second-order-methods","title":"\u2699\ufe0f Second-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Newton\u2019s Method Smooth convex Twice differentiable; \\(\\nabla^2 f(x)\\) PD \\(x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\\) Small\u2013Medium \\(O(d^3)\\) Logistic regression (IRLS), convex solvers BFGS / L-BFGS Smooth convex Differentiable, approximate Hessian Solve \\(B_k p_k=-\\nabla f(x_k)\\); update \\(B_k\\) via secant rule Medium \\(O(d^2)\\) GLMs, medium ML models Trust-Region Smooth convex/nonconvex Twice differentiable \\(\\min_p \\tfrac{1}{2}p^\\top \\nabla^2 f(x_k)p + \\nabla f(x_k)^\\top p\\) s.t. \\(\\|p\\|\\le\\Delta_k\\) Medium \\(O(d^2)\\) TRPO, physics-based ML"},{"location":"cheatsheets/20a_cheatsheet/#proximal-projected-splitting-methods","title":"\ud83e\uddee Proximal, Projected &amp; Splitting Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Proximal Gradient (ISTA) Composite \\(f=g+h\\) \\(g\\) smooth, \\(h\\) convex \\(x_{k+1}=\\operatorname{prox}_{\\alpha h}(x_k-\\alpha\\nabla g(x_k))\\) Medium \\(O(nd)\\) LASSO, sparse recovery FISTA Same as ISTA Convex, \\(L\\)-smooth \\(g\\) Like ISTA with momentum Medium \\(O(nd)\\) Compressed sensing Projected Gradient (PG) Convex constrained \\(f\\) smooth; easy projection \\(x_{k+1}=\\Pi_C(x_k-\\eta\\nabla f(x_k))\\) Medium \\(O(nd)\\) + projection Box/simplex constraints ADMM Separable convex + linear constraints \\(f,g\\) convex Alternating minimization + dual update Medium \\(O(nd)\\) per block Distributed ML, consensus Majorization\u2013Minimization (MM) Convex/nonconvex $g(x x_k)\\ge f(x)$ $x_{k+1}=\\arg\\min g(x x_k)$ Medium"},{"location":"cheatsheets/20a_cheatsheet/#coordinate-block-methods","title":"\ud83e\udde9 Coordinate &amp; Block Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Coordinate Descent (CD) Separable convex Convex, differentiable Update one coordinate: \\(x_{i}^{k+1}=x_i^k-\\eta\\partial_i f(x^k)\\) Large \\(O(d)\\) LASSO, SVM duals Block Coordinate Descent (BCD) Block separable Convex per block Minimize over \\(x^{(j)}\\) while fixing others Large \\(O(nd_j)\\) Matrix factorization, alternating minimization <p>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</p>"},{"location":"cheatsheets/20a_cheatsheet/#stochastic-mini-batch-methods","title":"\ud83c\udfb2 Stochastic &amp; Mini-Batch Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Stochastic Gradient Descent (SGD) Large-scale / streaming Unbiased stochastic gradients \\(x_{k+1}=x_k-\\eta_t\\nabla f_{i_k}(x_k)\\) Very Large \\(O(bd)\\) Deep learning, online learning Variance-Reduced (SVRG/SAGA/SARAH) Finite-sum convex Smooth, strongly convex \\(v_k=\\nabla f_{i_k}(x_k)-\\nabla f_{i_k}(\\tilde{x})+\\nabla f(\\tilde{x})\\) Large \\(O(bd)\\) Logistic regression, GLMs Adaptive SGD (Adam/RMSProp/Adagrad) Nonconvex stochastic Bounded variance \\(m_k=\\beta_1m_{k-1}+(1-\\beta_1)g_k\\), \\(v_k=\\beta_2v_{k-1}+(1-\\beta_2)g_k^2\\) Very Large \\(O(bd)\\) Neural networks Proximal Stochastic (Prox-SGD / Prox-SAGA) Nonsmooth stochastic \\(f=g+h\\) with prox of \\(h\\) known \\(x_{k+1}=\\operatorname{prox}_{\\eta h}(x_k-\\eta\\widehat{\\nabla g}(x_k))\\) Large \\(O(bd)\\) Sparse online learning"},{"location":"cheatsheets/20a_cheatsheet/#interior-point-augmented-methods","title":"\ud83e\uddf1 Interior-Point &amp; Augmented Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Interior-Point Convex with inequalities Slater\u2019s condition, self-concordant barrier Solve \\(\\min f_0(x)-\\tfrac{1}{t}\\sum_i\\log(-g_i(x))\\) Small\u2013Medium \\(O(d^3)\\) LP, QP, SDP Augmented Lagrangian (ALM) Constrained convex \\(f,g\\) convex; equality constraints \\(L_\\rho(x,\\lambda)=f(x)+\\lambda^T g(x)+\\tfrac{\\rho}{2}\\|g(x)\\|^2\\) Medium \\(O(nd)\\) Penalty methods, PDEs"},{"location":"cheatsheets/20a_cheatsheet/#derivative-free-black-box-optimization","title":"\ud83c\udf10 Derivative-Free &amp; Black-Box Optimization","text":"Method Problem Type Assumptions Core Idea Scalability Cost Applications Nelder\u2013Mead Simplex Low-dimensional, smooth or noisy No gradients; continuous \\(f\\) Maintain simplex of \\(d+1\\) points; reflect\u2013expand\u2013contract\u2013shrink operations Small \\(O(d^2)\\) Parameter tuning, physics models Simulated Annealing Nonconvex, global Stochastic exploration via temperature Random perturbations accepted w.p. \\(\\exp(-\\Delta f/T)\\); \\(T\\downarrow\\) Medium High (many samples) Hyperparameter tuning, design optimization Multi-start Local Search Nonconvex None; relies on restart diversity Run local solver from multiple random inits, pick best result Medium \\(k\\times\\) local solver Avoids local minima; cheap global heuristic Evolutionary Algorithms (EA) Black-box, global Population-based; fitness function only Mutate, select, recombine candidates Large \\(O(Pd)\\) per gen Global optimization, control, AutoML Genetic Algorithms (GA) Combinatorial / continuous Chromosomal encoding of solutions Apply selection, crossover, mutation; evolve over generations Medium\u2013Large \\(O(Pd)\\) Feature selection, neural architecture search Evolution Strategies (ES) Continuous, black-box Gaussian mutation around mean \\(\\theta_{k+1} = \\theta_k + \\eta \\sum_i w_i \\epsilon_i f(\\theta_k+\\sigma \\epsilon_i)\\) Large \\(O(Pd)\\) Reinforcement learning, black-box control Derivative-Free Optimization (DFO) Black-box, noisy \\(f\\) Only function values available Gradient estimated via random perturbations: \\(g\\approx\\frac{f(x+hu)-f(x)}{h}u\\) Medium \\(O(d)\\)\u2013\\(O(d^2)\\) Robotics, policy search, design Black-Box Optimization Framework General No analytical gradients; often stochastic Unified term covering EA, GA, ES, and DFO Medium\u2013Large varies Hyperparameter search, AutoML, reinforcement learning Numerical Encodings Used in GA/EA Represents variables in binary, integer, or floating-point form Choice of encoding impacts mutation/crossover behavior N/A negligible Optimization of mixed or discrete variables <p>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</p>"},{"location":"cheatsheets/20a_cheatsheet/#convergence-complexity-snapshot","title":"\ud83d\udcc8 Convergence &amp; Complexity Snapshot","text":"Method Type Convergence (Convex) Notes Subgradient \\(O(1/\\sqrt{k})\\) Nonsmooth convex Gradient Descent \\(O(1/k)\\) Smooth convex Accelerated Gradient \\(O(1/k^2)\\) Optimal first-order Newton / Quasi-Newton Quadratic / Superlinear Local only Strongly Convex \\((1-\\mu/L)^k\\) Linear rate Variance-Reduced Linear (strongly convex) Finite-sum optimization ADMM / Proximal \\(O(1/k)\\) Composite convex Interior-Point Polynomial time High-accuracy convex Derivative-Free / Heuristics No formal bound Empirical convergence only"},{"location":"cheatsheets/20a_cheatsheet/#practitioner-summary","title":"\ud83e\udde0 Practitioner Summary","text":"Situation Recommended Methods Gradients available, smooth convex Gradient Descent, Nesterov Curvature matters, moderate scale Newton, BFGS, Conjugate Gradient Nonsmooth regularizer Proximal Gradient, ADMM Simple constraints Projected Gradient Large-scale / streaming SGD, Adam, RMSProp Finite-sum convex SVRG, SAGA Online / adaptive Mirror Descent, FTRL No gradients (black-box) DFO, Nelder\u2013Mead, ES, GA Global nonconvex search Simulated Annealing, Multi-starts, Evolutionary Algorithms Distributed / separable ADMM, ALM High-precision convex programs Interior-Point, Trust-Region"},{"location":"cheatsheets/20a_cheatsheet/#notes-on-global-black-box-optimization","title":"\ud83e\udde9 Notes on Global &amp; Black-Box Optimization","text":"<ul> <li>Conjugate Gradient: memory-efficient quasi-second-order method for large convex quadratics.  </li> <li>Nelder\u2013Mead: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li> <li>Simulated Annealing: probabilistic global search inspired by thermodynamics.  </li> <li>Multi-Starts: pragmatic global exploration by repeated local optimization.  </li> <li>Evolutionary / Genetic / ES: population-based global heuristics; robust to noise and discontinuity.  </li> <li>Derivative-Free Optimization (DFO): umbrella for random, surrogate-based, or adaptive black-box methods.  </li> <li>Numerical Encoding: crucial in discrete search\u2014how real or binary variables are represented determines performance.</li> </ul> <p>Summary Insight: - Convex + differentiable \u2192 use gradient-based or Newton-type methods. - Convex + nonsmooth \u2192 use proximal, ADMM, or coordinate descent. - Large-scale or stochastic \u2192 use SGD or adaptive variants. - No gradients or nonconvex \u2192 use derivative-free or evolutionary methods. - The structure of the objective, not its size alone, determines the optimal solver family.</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1:  Introduction and Overview","text":"<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p> <ol> <li>No bad local minima: any local minimum is also the global minimum.  </li> <li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li> <li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li> </ol> <p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p> <p>This web-book is written for practitioners who have basic familiarity with optimization, especially gradient-based methods, and want to understand how convex optimization principles help guide reliable machine-learning practice.</p>"},{"location":"convex/11_intro/#11-motivation-optimization-in-machine-learning","title":"1.1 Motivation: Optimization in Machine Learning","text":"<p>Many supervised learning problems can be written in a common form:</p> \\[ \\min_{x \\in \\mathcal{X}}  \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i)  + \\lambda R(x), \\] <p>where</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function that measures how well the model predicts \\(b_i\\) from \\(a_i\\),  </li> <li>\\(R(x)\\) is a regularizer that encourages certain structure (such as sparsity or small weights),  </li> <li>\\(\\mathcal{X}\\) is a set of allowed parameter values, often simple and convex.</li> </ul> <p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the \\(\\ell_1\\) norm, and the \\(\\ell_2\\) norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>"},{"location":"convex/11_intro/#12-convex-sets-and-convex-functions-first-intuition","title":"1.2 Convex Sets and Convex Functions \u2014 First Intuition","text":"<p>A set \\(\\mathcal{C}\\) is convex if, whenever you pick two points in the set, the line segment between them stays entirely inside the set:</p> \\[ \\theta x + (1-\\theta)y \\in \\mathcal{C}  \\quad \\text{for all } x,y \\in \\mathcal{C},\\; \\theta \\in [0,1]. \\] <p>Convex functions follow a similar idea. A function \\(f\\) is convex if its graph never dips below the straight line connecting two points on the function:</p> \\[ f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta) f(y). \\] <p>Intuitively, convex functions look like bowls: they curve upward and have at most one global minimum. Affine functions are both convex and concave, and quadratics with positive semidefinite Hessians are convex. Many ML loss functions share this shape, which makes them easy to optimize.</p>"},{"location":"convex/11_intro/#13-why-convex-optimization-remains-central-in-ml","title":"1.3 Why Convex Optimization Remains Central in ML","text":"<p>Although many modern models are nonconvex, convex optimization continues to play a major role in three ways:</p> <ol> <li> <p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the \\(0\\text{\u2013}1\\) loss. They make optimization practical while still leading to models that generalize well.</p> </li> <li> <p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p> </li> <li> <p>Implicit bias in linear models:  In overparameterized linear least-squares problems, gradient descent starting from zero converges to the minimum-norm solution. This phenomenon helps explain generalization and implicit regularization in linear and kernel models.</p> </li> </ol> <p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>"},{"location":"convex/11_intro/#14-from-global-optima-to-algorithms","title":"1.4 From Global Optima to Algorithms","text":"<p>A major advantage of convex optimization is that it eliminates the possibility of non-global local minima. For a differentiable convex function on an open domain:</p> \\[ \\nabla f(x^*) = 0  \\quad \\Rightarrow \\quad x^* \\text{ is a global minimizer}. \\] <p>This means that simply finding a point where the gradient is zero is enough. For constrained or nondifferentiable problems, optimality is checked using subgradients or KKT conditions:</p> \\[ 0 \\in \\partial f(x^*) + N_{\\mathcal{X}}(x^*), \\] <p>where \\(N_{\\mathcal{X}}(x^*)\\) represents the outward directions that are blocked by the constraint set. These conditions are useful because many iterative algorithms aim to drive the gradient or subgradient toward zero.</p>"},{"location":"convex/11_intro/#15-canonical-convex-ml-problems-at-a-glance","title":"1.5 Canonical Convex ML Problems at a Glance","text":"Problem Objective Typical Solver Least squares \\(\\|A x - b\\|_2^2\\) Gradient descent, conjugate gradient Ridge regression \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_2^2\\) Closed form, gradient methods LASSO \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_1\\) Proximal gradient (ISTA/FISTA) Logistic regression \\(\\sum_i \\log(1+\\exp(-y_i a_i^\\top x)) + \\lambda\\|x\\|_2^2\\) Newton, quasi-Newton, SGD SVM (hinge loss) \\(\\tfrac{1}{2}\\|x\\|^2 + C\\sum_i \\max(0,1-y_i a_i^\\top x)\\) Subgradient, coordinate methods, SMO Robust regression \\(\\|A x - b\\|_1\\) Linear programming Elastic Net \\(\\|A x-b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|x\\|_2^2\\) Coordinate descent <p>These problems illustrate how convex models appear throughout ML.</p>"},{"location":"convex/11_intro/#16-web-book-roadmap-and-how-to-use-it","title":"1.6 Web-Book Roadmap and How to Use It","text":"Question Where to Look Key Idea What makes a function or set convex? Chapters 2\u20135 Geometry and basic properties of convexity How do gradients, subgradients, and KKT conditions define optimality? Chapters 6\u20139 Optimality conditions and duality How are convex problems solved in practice? Chapters 10\u201314 First-order, second-order, and interior-point methods How to choose an algorithm for a given optimization problem? Chapters 15\u201317 Large-scale and structured optimization techniques"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model \\(x\\) to data \\((A, b)\\) takes the form:</p> \\[ \\min_x \\ \\|A x - b\\|_2^2. \\] <p>Later in this chapter, we will see that this objective finds the point in the column space of \\(A\\) that is closest to \\(b\\). Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p> <p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas \u2014 projections, subspaces, orthogonality, eigenvalues, singular values, and norms \u2014 because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>"},{"location":"convex/12_vector/#21-vector-spaces-subspaces-and-affine-sets","title":"2.1 Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set of vectors that can be added and scaled without leaving the set. The familiar example is \\(\\mathbb{R}^n\\), where operations like \\(\\alpha x + \\beta y\\) keep us within the same space.</p> <p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are \u201cflat\u201d objects that always pass through the origin, such as lines or planes in \\(\\mathbb{R}^3\\). </p> <p>Affine sets extend this idea by allowing a shift away from the origin. A set \\(A\\) is affine if it contains the entire line passing through any two of its points. Equivalently, for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  \\(\\theta x + (1 - \\theta) y \\in A.\\) That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\). By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set. </p> <p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: \\(A = x_0 + S = \\{\\, x_0 + s : s \\in S \\,\\},\\) where \\(S\\) is a subspace and \\(x_0\\) is any point in the set. This representation is extremely useful in optimization. If \\(Ax = b\\) is a linear constraint, then its solution set is an affine set. A single particular solution \\(x_0\\) gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of \\(A\\). Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p> <p>Finally, affine transformations play a central role in both machine learning and optimization. A mapping of the form</p> <p>Affine Transformations: An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as \\(f(x) = A x + b,\\) where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex. is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p> <p>In summary, vector spaces describe the ambient space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the book.</p>"},{"location":"convex/12_vector/#22-linear-combinations-span-basis-dimension","title":"2.2 Linear combinations, span, basis, dimension","text":"<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p> <p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form is a linear combination. The set of all linear combinations is called the span:  The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p> <p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p> <p>A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). The number of basis vectors is the dimension \\(\\dim(V)\\).</p> <p>Rank and nullity facts:</p> <ul> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states: \\(\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n,\\) where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>Column Space: The column space of a matrix , denoted , is the set of all possible output vectors  that can be written as  for some . In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system  have a solution?\u201d is equivalent to asking whether . If  lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of , denoted , is the set of all input vectors  that are mapped to zero:  . It answers a different question: If a solution to  exists, is it unique? If the null space contains only the zero vector (), the solution is unique. But if  contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix  is a linear combination of others for example, \u2014the columns of  become linearly dependent. This creates a nonzero vector in the null space of , meaning multiple weight vectors  can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and  becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p> <p>Feasible Directions: In a constrained optimization problem of the form , the null space of  characterizes the directions along which one can move without violating the constraints. If , then moving from a feasible point  to  preserves feasibility, since  . Thus, the null space defines the space of free movement directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of , denoted , is the span of the rows of  (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to . The row space is orthogonal to the null space of :  .  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in  affect the constraints.</p> <p>Left Null Space: The left null space, denoted , is the set of all vectors  such that . These vectors are orthogonal to the columns of , and therefore orthogonal to the column space itself. In least squares problems,  represents residual directions\u2014components of  that cannot be explained by the model .</p> <p>Projection Interpretation (Least Squares):  When  has no exact solution (as in overdetermined systems), the least squares solution finds  such that  is the projection of  onto the column space of :  and the residual  lies in the left null space . This provides a geometric view: the solution projects  onto the closest point in the subspace that  can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of  is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where  is the number of columns of . This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in .</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by .  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map .</p>"},{"location":"convex/12_vector/#23-inner-products-and-orthogonality","title":"2.3 Inner products and orthogonality","text":"<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality\u2014concepts that appear repeatedly in convex optimization and machine learning.</p> <p>An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\):  with equality iff \\(x\\) and \\(y\\) are linearly dependent Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#24-norms-and-distances","title":"2.4 Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms: Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), and the dual of \\(\\ell_2\\) is itself.</p> <p>Imagine the vector \\(x\\) lives inside the original norm ball (\\(\\|x\\| \\le 1\\)). The term \\(x^\\top y\\) is the dot product, which measures the alignment between \\(x\\) and \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible value you can get by taking the dot product of \\(y\\) with any vector \\(x\\) that fits inside the original norm ball.If the dual norm \\(\\|y\\|_*\\) is large, it means \\(y\\) is strongly aligned with a direction \\(x\\) that is \"small\" (size \\(\\le 1\\)) according to the original norm.If the dual norm is small, \\(y\\) must be poorly aligned with all vectors \\(x\\) in the ball.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>"},{"location":"convex/12_vector/#26-orthogonal-projections-and-least-squares","title":"2.6 Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\). If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#27-operator-norms-singular-values-and-spectral-structure","title":"2.7 Operator norms, singular values, and spectral structure","text":"<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>"},{"location":"convex/12_vector/#operator-norms","title":"Operator norms","text":"<p>Given a matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and norms \\(\\|\\cdot\\|_p\\) on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_q\\) on \\(\\mathbb{R}^m\\), the induced operator norm is defined as  This quantity measures the largest amount by which \\(A\\) can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p> <ul> <li>\\(\\|A\\|_{2 \\to 2}\\), the spectral norm, equals the largest singular value of \\(A\\).</li> <li>\\(\\|A\\|_{1 \\to 1}\\) is the maximum absolute column sum.</li> <li>\\(\\|A\\|_{\\infty \\to \\infty}\\) is the maximum absolute row sum.</li> </ul> <p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function  converges for step sizes \\(\\alpha &lt; 2 / \\|Q\\|_2\\). This shows that controlling the operator norm of the Hessian\u2014or a Lipschitz constant of the gradient\u2014directly governs how aggressively an algorithm can move.</p>"},{"location":"convex/12_vector/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a factorization  where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is diagonal with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots\\). The \\(\\sigma_i\\) are the singular values of \\(A\\).</p> <p>Geometrically, the SVD shows how \\(A\\) transforms the unit ball into an ellipsoid. The columns of \\(V\\) give the principal input directions, the singular values are the lengths of the ellipsoid\u2019s axes, and the columns of \\(U\\) give the output directions. The largest singular value \\(\\sigma_{\\max}\\) equals the spectral norm \\(\\|A\\|_2\\), while the smallest \\(\\sigma_{\\min}\\) describes the least expansion (or exact flattening if \\(\\sigma_{\\min} = 0\\)).</p> <p>SVD is a powerful diagnostic tool in optimization. The ratio  is the condition number of \\(A\\). A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means \\(A\\) behaves more like a uniform scaling, which is ideal for optimization.</p>"},{"location":"convex/12_vector/#low-rank-structure","title":"Low-rank structure","text":"<p>The rank of \\(A\\) is the number of nonzero singular values. When \\(A\\) has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p> <p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>"},{"location":"convex/12_vector/#operator-norms-and-optimization-algorithms","title":"Operator norms and optimization algorithms","text":"<p>Operator norms help determine step sizes, convergence rates, and preconditioning strategies. For a general smooth convex function, the Lipschitz constant of its gradient often corresponds to a spectral norm of a Hessian or Jacobian, and this constant controls the safe step size for gradient descent. Preconditioning modifies the geometry of the problem\u2014changing the inner product or scaling the variables\u2014in order to reduce the effective operator norm and improve conditioning.</p> <p>These spectral considerations appear in both first-order and second-order methods. Newton\u2019s method, for example, implicitly rescales the space using the inverse Hessian, which equalizes curvature by transforming eigenvalues toward 1. This explains its rapid local convergence when the Hessian is well behaved.</p>"},{"location":"convex/12_vector/#summary","title":"Summary","text":"<ul> <li>The operator norm measures the maximum stretching effect of a matrix.</li> <li>Singular values give a complete geometric description of this stretching.</li> <li>The condition number captures how unevenly the matrix acts in different directions.</li> <li>Low-rank structure reveals underlying dimension and enables efficient computation.</li> <li>All of these properties strongly influence the behavior and design of optimization algorithms.</li> </ul> <p>Understanding operator norms and singular values provides valuable insight into when optimization problems are well conditioned, how algorithms will behave, and how to modify a problem to improve performance.</p>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p> <p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton\u2019s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p> <p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>"},{"location":"convex/13_calculus/#31-gradients-and-directional-derivatives","title":"3.1 Gradients and Directional Derivatives","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The function is differentiable at a point \\(x\\) if there exists a vector \\(\\nabla f(x)\\) such that  meaning that the linear function \\(h \\mapsto \\nabla f(x)^\\top h\\) provides the best local approximation to \\(f\\) near \\(x\\). The gradient is the unique vector with this property.</p> <p>A closely related concept is the directional derivative. For any direction \\(v \\in \\mathbb{R}^n\\), the directional derivative of \\(f\\) at \\(x\\) in the direction \\(v\\) is  If \\(f\\) is differentiable, then  Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction \\(v\\) tells us how rapidly \\(f\\) increases when we move infinitesimally along \\(v\\).</p> <p>This immediately yields an important geometric fact. Among all unit directions \\(u\\),  is maximized when \\(u\\) points in the direction of \\(\\nabla f(x)\\), the direction of steepest ascent. The steepest descent direction is therefore \\(-\\nabla f(x)\\), which motivates gradient-descent algorithms for minimizing functions.</p> <p>For any real number \\(c\\), the level set of \\(f\\) is   </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#32-jacobians","title":"3.2  Jacobians","text":"<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>"},{"location":"convex/13_calculus/#from-derivative-to-gradient","title":"From derivative to gradient","text":"<p>For a scalar function , differentiability means that near any point ,  The gradient vector  collects all partial derivatives. Each component measures how sensitive \\(f\\) is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>"},{"location":"convex/13_calculus/#from-gradient-to-jacobian","title":"From gradient to Jacobian","text":"<p>Now consider a vector-valued function \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\),  Each output \\(F_i\\) has its own gradient. Stacking these row vectors yields the Jacobian matrix:  </p> <p>The Jacobian provides the best linear approximation of \\(F\\) near \\(x\\):  Thus, locally, the nonlinear map \\(F\\) behaves like the linear map \\(h \\mapsto J_F(x)h\\). A small displacement \\(h\\) in input space is transformed into an output change governed by the Jacobian.</p>"},{"location":"convex/13_calculus/#interpreting-the-jacobian","title":"Interpreting the Jacobian","text":"Component of \\(J_F(x)\\) Meaning Row \\(i\\) Gradient of output \\(F_i(x)\\): how the \\(i\\)-th output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to \\(x_j\\): how varying input \\(x_j\\) affects the entire output vector. Determinant (when \\(m=n\\)) Local volume scaling: how \\(F\\) expands or compresses space near \\(x\\). Rank Local dimension of the image: whether any input directions are lost or collapsed. <p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>"},{"location":"convex/13_calculus/#33-the-hessian-and-curvature","title":"3.3 The Hessian and Curvature","text":"<p>For a twice\u2013differentiable function , the Hessian matrix collects all second-order partial derivatives:  </p> <p>The Hessian describes the local curvature of the function. While the gradient indicates the direction of steepest change, the Hessian tells us how that directional change itself varies\u2014whether the surface curves upward, curves downward, or remains nearly flat.</p>"},{"location":"convex/13_calculus/#curvature-and-positive-definiteness","title":"Curvature and positive definiteness","text":"<p>The eigenvalues of the Hessian determine its geometric behavior:</p> <ul> <li>If  (all eigenvalues nonnegative), the function is locally convex near \\(x\\).  </li> <li>If , the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li> <li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li> </ul> <p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>"},{"location":"convex/13_calculus/#example-quadratic-functions","title":"Example: Quadratic functions","text":"<p>Consider the quadratic function  where \\(Q\\) is symmetric. The gradient and Hessian are  Setting the gradient to zero gives the stationary point  If \\(Q \\succ 0\\), the solution  is the unique minimizer. The Hessian \\(Q\\) being positive definite confirms strict convexity.</p> <p>The eigenvalues of \\(Q\\) also explain the difficulty of minimizing \\(f\\):</p> <ul> <li>Large eigenvalues produce very steep, narrow directions\u2014optimization methods must take small steps.  </li> <li>Small eigenvalues produce flat directions\u2014progress is slow, especially for gradient descent.  </li> </ul> <p>The ratio of largest to smallest eigenvalue, the condition number, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>"},{"location":"convex/13_calculus/#why-the-hessian-matters-in-optimization","title":"Why the Hessian matters in optimization","text":"<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p> <ul> <li>Newton\u2019s method uses the Hessian to rescale directions, effectively \u201cwhitening\u2019\u2019 curvature and often converging rapidly.  </li> <li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li> <li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li> </ul> <p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>"},{"location":"convex/13_calculus/#34-taylor-approximation","title":"3.4 Taylor approximation","text":"<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>"},{"location":"convex/13_calculus/#first-order-approximation","title":"First-order approximation","text":"<p>If \\(f\\) is differentiable at \\(x\\), then for small steps \\(d\\),  The gradient gives the best linear model of the function near \\(x\\). This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>"},{"location":"convex/13_calculus/#second-order-approximation","title":"Second-order approximation","text":"<p>If \\(f\\) is twice differentiable, we can include curvature information:  The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p> <ul> <li>If , the quadratic term is nonnegative and the function curves upward\u2014locally bowl-shaped.</li> <li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others\u2014characteristic of saddle points.</li> </ul>"},{"location":"convex/13_calculus/#role-in-optimization-algorithms","title":"Role in optimization algorithms","text":"<p>Second-order Taylor models are the basis of Newton-type methods. Newton\u2019s method chooses \\(d\\) by approximately minimizing the quadratic model,  which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p> <p>Thus, Taylor expansions connect a function\u2019s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>"},{"location":"convex/13_calculus/#35-smoothness-and-strong-convexity","title":"3.5 Smoothness and Strong Convexity","text":"<p>In optimization, the behavior of a function\u2019s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>"},{"location":"convex/13_calculus/#lipschitz-continuous-gradients-l-smoothness","title":"Lipschitz continuous gradients (L-smoothness)","text":"<p>A differentiable function  has an \\(L\\)-Lipschitz continuous gradient if  This condition limits how quickly the gradient can change. Intuitively, an \\(L\\)-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:  This inequality states that every \\(L\\)-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p> <p>In gradient descent, smoothness directly determines a safe step size: choosing  ensures that each update decreases the function value for convex objectives. In machine learning, the constant \\(L\\) effectively controls how large the learning rate can be before training becomes unstable.</p>"},{"location":"convex/13_calculus/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function  is -strongly convex if, for some ,  This condition guarantees that \\(f\\) has at least \\(\\mu\\) amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p> <p>Strong convexity has major optimization implications:</p> <ul> <li>The minimizer is unique.  </li> <li>Gradient descent converges linearly with step size \\(\\eta \\le 1/L\\).  </li> <li>The ratio \\(L / \\mu\\) (the condition number) dictates convergence speed.</li> </ul>"},{"location":"convex/13_calculus/#curvature-in-both-directions","title":"Curvature in both directions","text":"<p>Together, smoothness and strong convexity bound the curvature of \\(f\\):  Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p> <p>These concepts\u2014, imiting curvature from above via \\(L\\) and from below via \\(\\mu\\), form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p> <p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>"},{"location":"convex/14_convexsets/#41-convex-sets","title":"4.1 Convex sets","text":"<p>A set  is convex if for any two points  and any ,  That is, the entire line segment between \\(x\\) and \\(y\\) lies inside the set. Convex sets have no \u201choles\u201d or \u201cindentations,\u201d and this geometric regularity is what makes optimization over them tractable.</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>Affine subspaces: .  </li> <li>Halfspaces: .  </li> <li>Euclidean balls: .  </li> <li>  balls (axis-aligned boxes): .  </li> <li>Probability simplex: .  </li> </ul> <p>A set fails to be convex whenever some segment between two feasible points leaves the set\u2014for example, a crescent or an annulus.</p>"},{"location":"convex/14_convexsets/#42-affine-sets-hyperplanes-and-halfspaces","title":"4.2 Affine sets, hyperplanes, and halfspaces","text":"<p>Affine sets generalize linear subspaces by allowing a shift. A set \\(A\\) is affine if for some point \\(x_0\\) and subspace \\(S\\),  Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p> <p>A hyperplane is an affine set defined by a single linear equation:  Hyperplanes act as the \u201cflat boundaries\u201d of higher-dimensional space and are the fundamental building blocks of polyhedra.</p> <p>A halfspace is one side of a hyperplane:  Halfspaces are convex and serve as basic local approximations to general convex sets.</p>"},{"location":"convex/14_convexsets/#43-convex-combinations-and-convex-hulls","title":"4.3 Convex combinations and convex hulls","text":"<p>A convex combination of points  is a weighted average  Convex sets are precisely those that contain all convex combinations of their points.</p> <p>The convex hull of a set \\(S\\), denoted \\(\\operatorname{conv}(S)\\), is the set of all convex combinations of finitely many points in \\(S\\). It is the smallest convex set containing \\(S\\). Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p> <p>Convex hulls are important because:</p> <ul> <li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li> <li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li> </ul>"},{"location":"convex/14_convexsets/#44-polyhedra-and-polytopes","title":"4.4 Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are always convex; they may be bounded or unbounded.</p> <p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>"},{"location":"convex/14_convexsets/#45-extreme-points","title":"4.5 Extreme points","text":"<p>Let  be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if  implies .</p> <p>Geometrically, extreme points are the \u201ccorners\u201d of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems\u2014such as linear programs\u2014achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>"},{"location":"convex/14_convexsets/#46-cones","title":"4.6 Cones","text":"<p>Cones generalize the idea of \u201cdirections\u201d in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>"},{"location":"convex/14_convexsets/#basic-definition","title":"Basic definition","text":"<p>A set \\(K \\subseteq \\mathbb{R}^n\\) is a cone if  A cone is convex if it is also closed under addition:  </p> <p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant , each coordinate axis direction is an extreme ray.</p>"},{"location":"convex/14_convexsets/#conic-hull","title":"Conic hull","text":"<p>Given any set \\(S\\), its conic hull is the set of all conic combinations:  This is the smallest convex cone containing \\(S\\). Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>"},{"location":"convex/14_convexsets/#polar-cones","title":"Polar cones","text":"<p>For a cone \\(K\\), the polar cone is defined as  </p> <p>Intuition:</p> <ul> <li>Polar vectors make a nonacute angle with every vector in \\(K\\).  </li> </ul> <p>Key properties:</p> <ul> <li>\\(K^\\circ\\) is always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, then \\(K^\\circ\\) is the orthogonal complement.  </li> <li>For any closed convex cone, </li> </ul> <p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>"},{"location":"convex/14_convexsets/#tangent-cones","title":"Tangent cones","text":"<p>For a set \\(C\\) and a point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) consists of all feasible \u201cinfinitesimal directions\u201d from \\(x\\):  </p> <p>Intuition:</p> <ul> <li>At an interior point, \\(T_C(x) = \\mathbb{R}^n\\): all small moves are allowed.  </li> <li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li> </ul> <p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>"},{"location":"convex/14_convexsets/#normal-cones","title":"Normal cones","text":"<p>For a convex set \\(C\\), the normal cone at a point \\(x \\in C\\) is  </p> <p>Interpretation:</p> <ul> <li>Every \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\).  </li> <li>At interior points, the normal cone is \\(\\{0\\}\\).  </li> <li>At boundary or corner points, it becomes a pointed cone of outward normals.</li> </ul> <p>A fundamental relationship ties tangent and normal cones together:  </p> <p>Normal cones appear directly in first-order optimality conditions. For a constrained problem  a point \\(x^*\\) is optimal only if  This expresses a balance between the objective\u2019s slope and the \u201cpushback\u2019\u2019 from the constraint set.</p> <p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>"},{"location":"convex/14_convexsets/#47-supporting-hyperplanes-and-separation","title":"4.7 Supporting Hyperplanes and Separation","text":"<p>One of the most important geometric facts about convex sets is that they can be supported or separated by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting Hyperplane Theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be nonempty, closed, and convex, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero vector \\(a\\) such that</p> \\[ a^\\top x \\le a^\\top x_0 \\qquad \\forall x \\in C. \\] <p>This means that the hyperplane</p> \\[ a^\\top x = a^\\top x_0 \\] <p>touches \\(C\\) at \\(x_0\\) but does not cut through it. The vector \\(a\\) is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating Hyperplane Theorem","text":"<p>If \\(C\\) and \\(D\\) are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector \\(a\\) and scalar \\(b\\) such that</p> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\qquad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <p>The hyperplane \\(a^\\top x = b\\) places all points of \\(C\\) on one side and all points of \\(D\\) on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p>"},{"location":"convex/14_convexsets/#why-this-matters-for-optimisation","title":"Why This Matters for Optimisation","text":"<p>These geometric results are central to convex optimisation:</p> <ul> <li>Subgradients correspond to supporting hyperplanes of the epigraph of a convex function.</li> <li>Dual variables arise from separating infeasible points from the feasible region.</li> <li>KKT conditions express the balance between the gradient of the objective and the normals of active constraints.</li> <li>Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.</li> </ul> <p>Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>Convex functions play a central role in optimisation and machine learning. When the objective function is convex, the optimisation landscape has a single global minimum, gradient-based algorithms behave predictably, and optimality conditions have clean geometric interpretations. Many common ML losses\u2014least squares, logistic loss, hinge loss, Huber loss\u2014are convex precisely for these reasons.</p> <p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>"},{"location":"convex/15_convexfunctions/#51-definitions-of-convexity","title":"5.1 Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),  </p> <p>The graph of \\(f\\) never dips below the straight line between \\((x,f(x))\\) and \\((y,f(y))\\). If the inequality is strict whenever \\(x \\neq y\\), the function is strictly convex.</p> <p>A powerful geometric viewpoint comes from the epigraph:  The function \\(f\\) is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>"},{"location":"convex/15_convexfunctions/#52-first-order-characterisation","title":"5.2 First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if  </p> <p>Interpretation:</p> <ul> <li>The tangent plane at any point \\(x\\) lies below the function everywhere.</li> <li>\\(\\nabla f(x)\\) defines a supporting hyperplane to the epigraph.</li> <li>The gradient provides a global linear underestimator of \\(f\\).</li> </ul> <p>This geometric picture is crucial in optimisation: at a minimiser \\(x^\\star\\), convexity implies </p> <p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>"},{"location":"convex/15_convexfunctions/#53-second-order-characterisation","title":"5.3 Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then convexity can be checked via curvature:</p> \\[ f \\text{ is convex } \\iff \\nabla^2 f(x) \\succeq 0 \\text{ for all } x. \\] <ul> <li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li> <li>If \\(\\nabla^2 f(x) \\succ 0\\) everywhere, the function is strictly convex.  </li> <li>Negative eigenvalues indicate directions of negative curvature \u2014 impossible for convex functions.</li> </ul> <p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>"},{"location":"convex/15_convexfunctions/#54-examples-of-convex-functions","title":"5.4 Examples of convex functions","text":"<ol> <li> <p>Affine functions:     Always convex (and concave). They define supporting hyperplanes.</p> </li> <li> <p>Quadratic functions with PSD Hessian:     Convex because the curvature matrix \\(Q\\) is PSD.</p> </li> <li> <p>Norms:     All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p> </li> <li> <p>Maximum of affine functions:     Convex because the maximum of convex functions is convex.    (Important in SVM hinge loss.)</p> </li> <li> <p>Log-sum-exp:     A smooth approximation to the max; convex by Jensen\u2019s inequality. Appears in softmax, logistic regression, partition functions.</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#55-jensens-inequality","title":"5.5 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex and \\(X\\) a random variable in its domain. Then:  </p> <p>This generalises the definition of convexity from finite averages to expectations. Practically:</p> <ul> <li>convex functions \u201cpull upward\u201d under averaging,</li> <li>log-sum-exp is convex because exponential is convex,</li> <li>EM and variational methods rely on Jensen to construct lower bounds.</li> </ul> <p>As a finite form, for \\(\\theta_i \\ge 0\\) with \\(\\sum \\theta_i = 1\\),  </p>"},{"location":"convex/15_convexfunctions/#56-operations-that-preserve-convexity","title":"5.6 Operations that preserve convexity","text":"<p>Convexity is preserved under many natural constructions:</p> <ul> <li> <p>Nonnegative scaling:   If \\(f\\) is convex and \\(\\alpha \\ge 0\\), then \\(\\alpha f\\) is convex.</p> </li> <li> <p>Addition:   If \\(f\\) and \\(g\\) are convex, then \\(f+g\\) is convex.</p> </li> <li> <p>Maximum: \\(\\max\\{f,g\\}\\) is convex.</p> </li> <li> <p>Affine pre-composition:   If \\(A\\) is a matrix,      is convex.</p> </li> <li> <p>Monotone composition rule:   If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex,   then \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex.</p> </li> </ul> <p>These rules allow construction of complex convex models from simple building blocks.</p>"},{"location":"convex/15_convexfunctions/#57-level-sets-of-convex-functions","title":"5.7 Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), the sublevel set is  </p> <p>If \\(f\\) is convex, every sublevel set is convex. This property is crucial because inequalities \\(f(x) \\le \\alpha\\) are ubiquitous in constraints.</p> <p>Examples:</p> <ul> <li>Norm balls: \\(\\{ x : \\|x\\|_2 \\le r \\}\\) </li> <li>Linear regression confidence ellipsoids: \\(\\{ x : \\|Ax - b\\|_2 \\le \\epsilon \\}\\)</li> </ul> <p>These sets enable convex constrained optimisation formulations.</p>"},{"location":"convex/15_convexfunctions/#58-strict-and-strong-convexity","title":"5.8 Strict and strong convexity","text":""},{"location":"convex/15_convexfunctions/#strict-convexity","title":"Strict convexity","text":"<p>A function is strictly convex if  for all \\(x \\neq y\\) and \\(\\theta \\in (0,1)\\).</p> <p>Strict convexity implies unique minimisers.</p>"},{"location":"convex/15_convexfunctions/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function is \\(\\mu\\)-strongly convex if  </p> <p>Strong convexity adds quantitative curvature: the function grows at least quadratically away from its minimiser.</p> <p>Consequences:</p> <ul> <li>unique minimiser,</li> <li>gradient descent achieves linear convergence rate,   error shrinks as </li> <li>conditioning (\\(\\kappa = L/\\mu\\)) governs algorithmic difficulty.</li> </ul> <p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds \\(\\tfrac{\\lambda}{2}\\|x\\|_2^2\\)).</p>"},{"location":"convex/15_convexfunctions/#summary","title":"Summary","text":"<p>Convex functions form the analytical backbone of convex optimisation. They provide:</p> <ul> <li>predictable geometry,</li> <li>clean gradient conditions,</li> <li>reliable convergence behaviour,</li> <li>tractable constraints via convex sublevel sets,</li> <li>stability under composition and modelling operations.</li> </ul> <p>These properties make convex objectives indispensable across machine learning, signal processing, and optimisation theory.</p>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p> <ul> <li>the  norm  (nondifferentiable at zero),</li> <li>pointwise-max functions such as ,</li> <li>the hinge loss  used in SVMs,</li> <li>regularisers like total variation or indicator functions of convex sets.</li> </ul> <p>Although these functions have \u201ckinks\u201d, they remain convex\u2014and convexity guarantees the existence of supporting hyperplanes at every point. Subgradients formalise this idea and allow optimisation algorithms to operate even when no derivative exists.</p> <p>This chapter introduces subgradients, subdifferentials, subgradient calculus, and the basic subgradient method.</p>"},{"location":"convex/16_subgradients/#61-subgradients-and-the-subdifferential","title":"6.1 Subgradients and the Subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex.  A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if</p> \\[ f(y) \\ge f(x) + g^\\top (y - x) \\quad \\text{for all } y. \\] <p>Geometric interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top(y-x)\\) is a global underestimator of \\(f\\).</li> <li>Each subgradient defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).</li> <li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li> <li>At kinks, there may be infinitely many supporting hyperplanes.</li> </ul> <p>The subdifferential of \\(f\\) at \\(x\\) is the set  </p> <p>Properties:</p> <ul> <li>  is always a nonempty convex set (if \\(x\\) is in the interior of the domain).</li> <li>If \\(f\\) is differentiable at \\(x\\), then </li> <li>If \\(f\\) is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li> </ul> <p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>"},{"location":"convex/16_subgradients/#62-examples","title":"6.2 Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\). Then:</p> <ul> <li>If \\(t &gt; 0\\),  \\(\\partial f(t) = \\{1\\}\\).</li> <li>If \\(t &lt; 0\\),  \\(\\partial f(t) = \\{-1\\}\\).</li> <li>If \\(t = 0\\), </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) supports the graph from below.</p>"},{"location":"convex/16_subgradients/#the-ell_1-norm","title":"The  norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\):</p> \\[ g \\in \\partial \\|x\\|_1 \\quad\\Longleftrightarrow\\quad g_i \\in \\partial |x_i|. \\] <p>Thus:</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>"},{"location":"convex/16_subgradients/#pointwise-maximum-of-affine-functions","title":"Pointwise maximum of affine functions","text":"<p>Let </p> <ul> <li> <p>If only one index \\(i^\\star\\) achieves the maximum at \\(x\\), then </p> </li> <li> <p>If multiple indices are tied, then    the convex hull of the active slopes.</p> </li> </ul> <p>This structure underlies SVM hinge loss and ReLU-type functions.</p>"},{"location":"convex/16_subgradients/#63-subgradient-optimality-condition","title":"6.3 Subgradient Optimality Condition","text":"<p>For the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>a point \\(x^\\star\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^\\star). \\] <p>Interpretation:</p> <ul> <li>At optimality, no subgradient points to a direction that would decrease \\(f\\).</li> <li>Geometrically, the supporting hyperplane at \\(x^\\star\\) is horizontal, forming the flat bottom of the convex bowl.</li> <li>This generalises the smooth condition .</li> </ul>"},{"location":"convex/16_subgradients/#64-subgradient-calculus-useful-rules","title":"6.4 Subgradient Calculus (Useful Rules)","text":"<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let \\(f\\) and \\(g\\) be convex.</p>"},{"location":"convex/16_subgradients/#sum-rule","title":"Sum rule","text":"\\[ \\partial(f+g)(x) \\subseteq \\partial f(x) + \\partial g(x) = \\{ u+v : u \\in \\partial f(x),\\ v \\in \\partial g(x) \\}. \\] <p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>"},{"location":"convex/16_subgradients/#affine-composition","title":"Affine composition","text":"<p>If \\(h(x) = f(Ax + b)\\), then  </p> <p>This rule is heavily used in machine learning models, where losses depend on linear predictions \\(Ax\\).</p>"},{"location":"convex/16_subgradients/#maximum-of-convex-functions","title":"Maximum of convex functions","text":"<p>If \\(f(x) = \\max_i f_i(x)\\), then  </p> <p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>"},{"location":"convex/16_subgradients/#65-subgradient-methods","title":"6.5 Subgradient Methods","text":"<p>Even when \\(f\\) is not differentiable, we can minimise it using subgradient descent:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k, \\qquad g_k \\in \\partial f(x_k). \\] <p>Key features:</p> <ul> <li>Requires only a subgradient (no differentiability needed).</li> <li>Works for any convex function.</li> <li>Stepsizes must typically decrease (e.g. , ).</li> <li>Guaranteed convergence for convex \\(f\\), but generally slow.</li> </ul>"},{"location":"convex/16_subgradients/#convergence-rates-worst-case","title":"Convergence rates (worst case)","text":"<ul> <li>Smooth convex gradient descent: \\(O(1/k)\\) or \\(O(1/k^2)\\).  </li> <li>Nonsmooth subgradient descent: </li> </ul> <p>This slower rate reflects the lack of curvature information at kinks.</p>"},{"location":"convex/16_subgradients/#why-it-still-matters-in-ml","title":"Why it still matters in ML","text":"<p>Many training objectives behave nonsmoothly:</p> <ul> <li>SVM hinge loss  </li> <li> -regularised models (sparse optimisation)  </li> <li>ReLUs and piecewise-linear networks  </li> <li>Projections onto convex sets  </li> </ul> <p>Even modern deep-learning optimisers operate as subgradient methods whenever the network contains nonsmooth operations.</p>"},{"location":"convex/16_subgradients/#66-proximal-and-smoothed-alternatives","title":"6.6 Proximal and Smoothed Alternatives","text":"<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>"},{"location":"convex/16_subgradients/#1-proximal-methods","title":"(1) Proximal methods","text":"<p>For a convex function \\(f\\), the proximal operator is  </p> <p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p> <ul> <li>  regularisation,</li> <li>indicator functions of convex sets,</li> <li>total variation penalties.</li> </ul> <p>They achieve faster and more stable convergence than basic subgradient descent.</p>"},{"location":"convex/16_subgradients/#2-smoothing-techniques","title":"(2) Smoothing techniques","text":"<p>Many nonsmooth convex functions have smooth approximations:</p> <ul> <li>Replace  with the Huber loss.</li> <li>Replace  with softplus.</li> <li>Replace  with log-sum-exp, a smooth convex approximation.</li> </ul> <p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>"},{"location":"convex/16_subgradients/#summary","title":"Summary","text":"<ul> <li>Nonsmooth convex functions arise naturally in ML.  </li> <li>Subgradients generalise gradients: they give supporting hyperplanes.  </li> <li>Optimality: \\(0 \\in \\partial f(x^\\star)\\).  </li> <li>Subgradient calculus enables reasoning about complex nonsmooth models.  </li> <li>Subgradient descent converges globally but slowly.  </li> <li>Proximal and smoothing methods yield faster practical algorithms.</li> </ul> <p>Subgradients complete the picture of convex analysis by extending optimisation tools beyond differentiable functions, setting the stage for modern first-order methods.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar \u201czero gradient\u201d condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) framework developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization","title":"7.1 Orders of Optimality: Why First Order is Enough in Convex Optimization","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer \\(x^\\star\\):</p> Order Object inspected Role First-order \\(\\nabla f(x^\\star)\\) or subgradients Detects existence of a local descent direction Second-order Hessian \\(\\nabla^2 f(x^\\star)\\) Examines curvature (minimum vs saddle vs maximum) Higher-order Third derivative and beyond Rarely used; only for degenerate cases with vanishing curvature <p>In general nonconvex optimization, these conditions are used together: a point may have \\(\\nabla f(x^\\star) = 0\\) but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p> <p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{whenever the Hessian exists}. \\] <p>Therefore:</p> <ul> <li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li> <li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li> </ul> <p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>"},{"location":"convex/16a_optimality_conditions/#72-motivation","title":"7.2 Motivation","text":"<p>Consider the basic convex problem  where \\(f\\) is convex and \\(\\mathcal{X}\\) is a convex set.</p> <p>Intuitively, a point \\(\\hat{x}\\) is optimal if there is no feasible direction in which we can move and strictly decrease \\(f\\). In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside \\(\\mathcal{X}\\) are allowed.</p> <p>Thus, optimality can be seen as an equilibrium:</p> <ul> <li>the objective\u2019s tendency to decrease (captured by its gradient or subgradient)  </li> <li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li> </ul> <p>In machine learning, this appears as:</p> <ul> <li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li> <li>training until the force from regularization/constraints balances the data fit term (e.g., in \\(\\ell_1\\)-regularized models).</li> </ul> <p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>"},{"location":"convex/16a_optimality_conditions/#73-unconstrained-convex-problems","title":"7.3 Unconstrained Convex Problems","text":"<p>For the unconstrained problem  with \\(f\\) convex, the optimality conditions are especially simple.</p>"},{"location":"convex/16a_optimality_conditions/#smooth-case","title":"Smooth case","text":"<p>If \\(f\\) is differentiable, then a point \\(\\hat{x}\\) is optimal if and only if  </p> <p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>"},{"location":"convex/16a_optimality_conditions/#nonsmooth-case","title":"Nonsmooth case","text":"<p>If \\(f\\) is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes  </p> <p>Interpretation:</p> <ul> <li>The origin lies in the set of all subgradients at \\(\\hat{x}\\).  </li> <li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\).  </li> <li>No direction in \\(\\mathbb{R}^n\\) gives a first-order improvement in the objective.</li> </ul> <p>For smooth \\(f\\), this reduces to the usual condition \\(\\nabla f(\\hat{x}) = 0\\).</p>"},{"location":"convex/16a_optimality_conditions/#74-constrained-convex-problems","title":"7.4 Constrained Convex Problems","text":"<p>Now consider the constrained problem  where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a nonempty closed convex set.</p> <p>If \\(\\hat{x}\\) lies strictly inside \\(\\mathcal{X}\\), then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,  remains the necessary and sufficient condition for optimality.</p> <p>The interesting case is when \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\).</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-order condition with constraints","text":"<p>The general first-order optimality condition for the constrained convex problem is:  </p> <p>That is, there exist</p> <ul> <li>a subgradient \\(g \\in \\partial f(\\hat{x})\\), and  </li> <li>a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\)</li> </ul> <p>such that  </p> <p>Interpretation:</p> <ul> <li>The objective\u2019s slope \\(g\\) is exactly balanced by a normal vector \\(v\\) coming from the constraint set.  </li> <li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which \\(f\\) can decrease.  </li> <li>Geometrically, the epigraph of \\(f\\) and the feasible set meet with aligned supporting hyperplanes at \\(\\hat{x}\\).</li> </ul> <p>Special cases:</p> <ul> <li>If \\(\\hat{x}\\) is an interior point, then \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so we recover the unconstrained condition \\(0 \\in \\partial f(\\hat{x})\\).  </li> <li>If \\(\\mathcal{X}\\) is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li> </ul>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-the-kkt-framework","title":"Chapter 8: Lagrange Multipliers and the KKT Framework","text":"<p>We now have the ingredients for understanding optimality in convex optimization:</p> <ul> <li>convex functions define well-behaved objectives,</li> <li>convex sets describe feasible regions,</li> <li>gradients and subgradients encode descent directions.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions\u2014the Karush\u2013Kuhn\u2013Tucker (KKT) conditions\u2014is the central optimality framework for constrained convex optimization.</p> <p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by \u201cforces\u2019\u2019 coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>"},{"location":"convex/17_kkt/#81-unconstrained-convex-minimization","title":"8.1 Unconstrained Convex Minimization","text":"<p>Consider the problem  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent iteratively updates  with step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Moving opposite the gradient decreases \\(f\\).</li> <li>If the gradient is Lipschitz continuous and the step size is small enough (\\(\\alpha_k \\le 1/L\\)), then gradient descent converges to a global minimizer.</li> <li>If \\(f\\) is strongly convex, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li> </ul> <p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>"},{"location":"convex/17_kkt/#82-equality-constrained-problems-and-lagrange-multipliers","title":"8.2 Equality-Constrained Problems and Lagrange Multipliers","text":"<p>Now consider minimizing \\(f\\) subject to equality constraints:  </p> <p>Define the Lagrangian  where \\(\\lambda = (\\lambda_1,\\dots,\\lambda_p)\\) are the Lagrange multipliers.</p> <p>Under differentiability and regularity assumptions, a point \\(x^*\\) is optimal only if:</p> <ol> <li> <p>Primal feasibility     </p> </li> <li> <p>Stationarity     </p> </li> </ol> <p>Geometric meaning:</p> <ul> <li>The feasible set  is typically a smooth manifold.</li> <li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li> <li>The multipliers \\(\\lambda_j^*\\) weight the constraint normals to exactly cancel the objective\u2019s gradient.</li> </ul> <p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>"},{"location":"convex/17_kkt/#83-inequality-constraints-and-the-kkt-conditions","title":"8.3 Inequality Constraints and the KKT Conditions","text":"<p>Now consider the general convex problem:  </p> <p>Form the Lagrangian  with:</p> <ul> <li>  (equality multipliers),</li> <li>  (inequality multipliers).</li> </ul> <p>A point \\(x^*\\) with multipliers \\((\\lambda^*,\\mu^*)\\) satisfies the KKT conditions:</p>"},{"location":"convex/17_kkt/#1-primal-feasibility","title":"1. Primal feasibility","text":"\\[ g_i(x^*) \\le 0,\\quad \\forall i, \\qquad h_j(x^*) = 0,\\quad \\forall j. \\]"},{"location":"convex/17_kkt/#2-dual-feasibility","title":"2. Dual feasibility","text":"\\[ \\mu_i^* \\ge 0,\\quad \\forall i. \\]"},{"location":"convex/17_kkt/#3-stationarity","title":"3. Stationarity","text":"\\[ \\nabla f(x^*)  + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*) = 0. \\]"},{"location":"convex/17_kkt/#4-complementary-slackness","title":"4. Complementary slackness","text":"\\[ \\mu_i^*\\, g_i(x^*) = 0, \\quad i=1,\\dots,m. \\] <p>Complementary slackness expresses a clear dichotomy:</p> <ul> <li>If constraint \\(g_i(x) \\le 0\\) is inactive (strictly \\(&lt;0\\)), then it applies no force: \\(\\mu_i^* = 0\\).</li> <li>If a constraint is active at the boundary, it may exert a force: \\(\\mu_i^* &gt; 0\\), and then \\(g_i(x^*) = 0\\).</li> </ul> <p>Only active constraints can push back against the objective.</p>"},{"location":"convex/17_kkt/#84-slaters-condition-guaranteeing-strong-duality","title":"8.4 Slater\u2019s Condition \u2014 Guaranteeing Strong Duality","text":"<p>The KKT conditions always provide necessary conditions for optimality. For them to also be sufficient (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p> <p>For convex problems with convex \\(g_i\\) and affine \\(h_j\\), Slater\u2019s condition holds if there exists a strictly feasible point:  </p> <p>Interpretation:</p> <ul> <li>The feasible region contains an interior point.</li> <li>The constraints are not \u201ctight\u201d everywhere.</li> <li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li> </ul> <p>When Slater\u2019s condition holds:</p> <ol> <li> <p>Strong duality holds: </p> </li> <li> <p>The dual optimum is attained.</p> </li> <li> <p>The KKT conditions are both necessary and sufficient for optimality.</p> </li> </ol>"},{"location":"convex/17_kkt/#duality-gap","title":"Duality gap","text":"<p>For a primal problem with optimum \\(p^*\\) and its dual with optimum \\(d^*\\), the duality gap is  </p> <ul> <li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li> <li>Slater\u2019s condition ensures the gap is zero.</li> </ul> <p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>"},{"location":"convex/17_kkt/#85-geometric-and-physical-interpretation","title":"8.5 Geometric and Physical Interpretation","text":"<p>The KKT conditions describe an equilibrium of forces:</p> <ul> <li>The objective gradient pushes the point in the direction of steepest decrease.</li> <li>Active constraints push back through normal vectors scaled by multipliers.</li> <li>At optimality, these forces exactly cancel.</li> </ul> <p>Physically:</p> <ul> <li>Lagrange multipliers are \u201creaction forces\u2019\u2019 keeping a system on the constraint surface.</li> <li>In economics, they are \u201cshadow prices\u2019\u2019 indicating how much the objective would improve if a constraint were relaxed.</li> <li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li> </ul> <p>KKT theory unifies all earlier ideas\u2014convexity, gradients/subgradients, feasible regions, tangent and normal cones\u2014into one clean, general optimality framework.</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p> <ul> <li>lower bounds on the primal optimal value,</li> <li>certificates of optimality,</li> <li>interpretations of constraint \u201cprices,\u201d</li> <li>and alternative algorithmic routes to solutions.</li> </ul> <p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality \u2014 strong duality \u2014 lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>"},{"location":"convex/18_duality/#91-the-primal-problem","title":"9.1 The Primal Problem","text":"<p>Consider the general convex problem</p> \\[ \\begin{array}{ll} \\text{minimize} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i=1,\\dots,m, \\\\  &amp; h_j(x) = 0,\\quad j=1,\\dots,p, \\end{array} \\] <p>where:</p> <ul> <li>\\(f\\) and each \\(g_i\\) are convex,</li> <li>each equality constraint \\(h_j\\) is affine.</li> </ul> <p>The optimal value is</p> \\[ f^\\star = \\inf\\{ f(x) : g_i(x) \\le 0,\\ h_j(x)=0 \\}. \\] <p>The infimum allows for the possibility that the best value is approached but not attained.</p>"},{"location":"convex/18_duality/#92-why-duality","title":"9.2 Why Duality?","text":"<p>A constrained problem can be viewed as:</p> <p>minimize \\(f(x)\\) but pay a penalty whenever constraints are violated.</p> <p>If the penalties are chosen \u201ccorrectly,\u201d one can recover the original constrained problem from an unconstrained penalized problem. Dual variables \u2014 \\(\\mu_i\\) for inequalities and \\(\\lambda_j\\) for equalities \u2014 precisely encode these penalties:</p> <ul> <li>\\(\\mu_i\\) measures how costly it is to violate \\(g_i(x)\\le 0\\),</li> <li>\\(\\lambda_j\\) measures the sensitivity of the objective to relaxing \\(h_j(x)=0\\).</li> </ul> <p>Duality converts constraints into prices, and transforms geometry into algebra.</p>"},{"location":"convex/18_duality/#93-the-lagrangian","title":"9.3 The Lagrangian","text":"<p>The Lagrangian function is</p> \\[ L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^p \\lambda_j h_j(x), \\] <p>with:</p> <ul> <li>\\(\\mu_i \\ge 0\\) for inequality constraints,</li> <li>\\(\\lambda_j \\in \\mathbb{R}\\) unrestricted for equalities.</li> </ul> <p>Interpretation:</p> <ul> <li>If \\(\\mu_i &gt; 0\\), violating \\(g_i(x)\\le 0\\) incurs a penalty proportional to \\(\\mu_i\\).</li> <li>If \\(\\mu_i = 0\\), that constraint does not influence the Lagrangian at that point.</li> </ul>"},{"location":"convex/18_duality/#94-the-dual-function-lower-bounds-from-penalties","title":"9.4 The Dual Function: Lower Bounds from Penalties","text":"<p>Fix \\((\\lambda,\\mu)\\) and minimize the Lagrangian with respect to \\(x\\):</p> \\[ \\theta(\\lambda, \\mu) = \\inf_x L(x,\\lambda,\\mu). \\] <p>Because \\(g_i(x) \\le 0\\) for feasible \\(x\\) and \\(\\mu_i \\ge 0\\),</p> \\[ L(x,\\lambda,\\mu) \\le f(x), \\] <p>so taking the infimum over all \\(x\\) yields</p> \\[ \\theta(\\lambda,\\mu) \\le f^\\star. \\] <p>Thus \\(\\theta\\) always produces lower bounds on the true optimal value (weak duality).</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\) (infimum of affine functions).</li> <li>It may be \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul>"},{"location":"convex/18_duality/#95-the-dual-problem","title":"9.5 The Dual Problem","text":"<p>The dual problem maximizes these lower bounds:</p> \\[ \\begin{array}{ll} \\text{maximize}_{\\lambda,\\mu} &amp; \\theta(\\lambda,\\mu) \\\\ \\text{subject to} &amp; \\mu \\ge 0. \\end{array} \\] <p>Let \\(d^\\star\\) be the optimal dual value. Weak duality guarantees:</p> \\[ d^\\star \\le f^\\star. \\] <p>The dual problem is always a concave maximization, i.e., a convex optimization problem in \\((\\lambda,\\mu)\\).</p>"},{"location":"convex/18_duality/#96-strong-duality-and-the-duality-gap","title":"9.6 Strong Duality and the Duality Gap","text":"<p>If</p> \\[ d^\\star = f^\\star, \\] <p>we say strong duality holds. The duality gap is zero.</p>"},{"location":"convex/18_duality/#slaters-condition","title":"Slater\u2019s Condition","text":"<p>If:</p> <ul> <li>\\(g_i\\) are convex,</li> <li>\\(h_j\\) are affine,</li> <li>and there exists a \\(\\tilde{x}\\) such that </li> </ul> <p>then:</p> <ul> <li>strong duality holds (\\(f^\\star = d^\\star\\)),</li> <li>dual maximizers exist,</li> <li>KKT conditions fully characterize primal\u2013dual optimality.</li> </ul> <p>Slater\u2019s condition ensures the feasible region has interior \u2014 the constraints are not tight everywhere.</p>"},{"location":"convex/18_duality/#97-duality-and-the-kkt-conditions","title":"9.7 Duality and the KKT Conditions","text":"<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>"},{"location":"convex/18_duality/#1-primal-feasibility","title":"1. Primal feasibility","text":"\\[ g_i(x^\\star) \\le 0,\\qquad h_j(x^\\star)=0. \\]"},{"location":"convex/18_duality/#2-dual-feasibility","title":"2. Dual feasibility","text":"\\[ \\mu_i^\\star \\ge 0. \\]"},{"location":"convex/18_duality/#3-stationarity","title":"3. Stationarity","text":"\\[ \\nabla f(x^\\star) + \\sum_{i=1}^m \\mu_i^\\star \\nabla g_i(x^\\star) + \\sum_{j=1}^p \\lambda_j^\\star \\nabla h_j(x^\\star) = 0. \\]"},{"location":"convex/18_duality/#4-complementary-slackness","title":"4. Complementary slackness","text":"\\[ \\mu_i^\\star g_i(x^\\star) = 0,\\qquad \\forall i. \\] <p>Together these conditions ensure:</p> \\[ f(x^\\star) = \\theta(\\lambda^\\star,\\mu^\\star) = f^\\star = d^\\star. \\] <p>Geometrically, the gradients of the active constraints form a supporting hyperplane that \u201ctouches\u2019\u2019 the objective exactly at the optimum.</p>"},{"location":"convex/18_duality/#98-interpretation-of-dual-variables","title":"9.8 Interpretation of Dual Variables","text":"<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>"},{"location":"convex/18_duality/#shadow-prices-constraint-forces","title":"Shadow Prices / Constraint Forces","text":"<ul> <li> <p>\\(\\mu_i^\\star\\): the shadow price for relaxing \\(g_i(x)\\le 0\\).   Large \\(\\mu_i^\\star\\) means the constraint is tight and costly to relax.</p> </li> <li> <p>\\(\\lambda_j^\\star\\): sensitivity of the optimal value to perturbations of \\(h_j(x)=0\\).</p> </li> </ul>"},{"location":"convex/18_duality/#ml-interpretations","title":"ML Interpretations","text":"<ul> <li>Support Vector Machines: dual variables select support vectors (only points with \\(\\mu_i^\\star &gt; 0\\) matter).</li> <li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li> <li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li> </ul> <p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-multi-objective-convex-optimization","title":"Chapter 10: Multi-Objective Convex Optimization","text":"<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p> <ul> <li>accuracy vs. regularity,</li> <li>loss vs. fairness,</li> <li>return vs. risk,</li> <li>reconstruction vs. compression,</li> <li>energy use vs. performance.</li> </ul> <p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p> <p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>"},{"location":"convex/18a_pareto/#101-classical-optimality-one-objective","title":"10.1 Classical Optimality (One Objective)","text":"<p>In standard convex optimization, we solve:</p> \\[ x^* \\in \\arg\\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex and \\(\\mathcal{X}\\) is convex. In this setting, it is natural to speak of the minimizer \u2014 or set of minimizers \u2014 because the task is governed by a single quantitative measure.</p> <p>However, when multiple objectives \\((f_1,\\dots,f_k)\\) must be minimized simultaneously, a single \u201cbest\u201d point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#102-multi-objective-convex-optimization","title":"10.2 Multi-Objective Convex Optimization","text":"<p>A multi-objective optimization problem takes the form</p> \\[ \\min_{x \\in \\mathcal{X}} F(x) = (f_1(x), \\dots, f_k(x)), \\] <p>where each \\(f_i\\) is convex. This framework appears in many ML and statistical tasks:</p> Domain Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs. complexity Fair ML Loss Fairness metric Utility vs. fairness Portfolio Return Risk Profit vs. stability Autoencoders Reconstruction KL divergence Fidelity vs. disentanglement <p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes Pareto efficiency.</p>"},{"location":"convex/18a_pareto/#103-pareto-optimality","title":"10.3 Pareto Optimality","text":""},{"location":"convex/18a_pareto/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A point \\(x^*\\) is Pareto optimal if there is no other \\(x\\) such that</p> \\[ f_i(x) \\le f_i(x^*)\\quad \\forall i, \\] <p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>"},{"location":"convex/18a_pareto/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no feasible point satisfies</p> \\[ f_i(x) &lt; f_i(x^*)\\quad \\forall i. \\] <p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>"},{"location":"convex/18a_pareto/#geometric-view","title":"Geometric View","text":"<p>For two objectives \\((f_1, f_2)\\), the feasible set in objective space is a region in \\(\\mathbb{R}^2\\). Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p> <ul> <li>Points on the frontier are the best achievable trade-offs.</li> <li>Points above or inside the region are dominated and thus suboptimal.</li> </ul> <p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>"},{"location":"convex/18a_pareto/#104-scalarization-turning-many-objectives-into-one","title":"10.4 Scalarization: Turning Many Objectives into One","text":"<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>"},{"location":"convex/18a_pareto/#weighted-sum-scalarization","title":"Weighted-Sum Scalarization","text":"\\[ \\min_{x \\in \\mathcal{X}} \\sum_{i=1}^k w_i f_i(x), \\qquad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights encode relative importance.  </li> <li>Varying \\(w\\) traces (part of) the Pareto frontier.  </li> <li>When \\(f_i\\) and \\(\\mathcal{X}\\) are convex, this method recovers the convex portion of the frontier.</li> </ul>"},{"location":"convex/18a_pareto/#-constraint-method","title":"\u03b5-Constraint Method","text":"\\[ \\min_{x} \\ f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\ \\ i = 2,\\dots,k. \\] <ul> <li>Here the tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Each choice of \\(\\varepsilon\\) yields a different Pareto-efficient point.</li> </ul> <p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>"},{"location":"convex/18a_pareto/#duality-connection","title":"Duality Connection","text":"<p>Scalarization has a tight relationship with duality (Chapter 9):</p> <ul> <li>Weights \\(w_i\\) in a weighted sum act like dual variables.</li> <li>Regularization parameters (e.g., the \\(\\lambda\\) in L2 or L1 regularization) correspond to dual multipliers.</li> <li>Moving along \\(\\lambda\\) traces the Pareto frontier between data fit and model complexity.</li> </ul> <p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>"},{"location":"convex/18a_pareto/#105-examples-and-applications","title":"10.5 Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1: Regularized Least Squares","text":"<p>Consider</p> \\[ f_1(x) = \\|Ax - b\\|_2^2,\\qquad  f_2(x) = \\|x\\|_2^2. \\] <p>Two scalarizations:</p> <ol> <li> <p>Weighted:     </p> </li> <li> <p>\u03b5-constraint:     </p> </li> </ol> <p>\\(\\lambda\\) and \\(\\tau\\) trace the same Pareto curve \u2014 the classical bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2: Portfolio Optimization (Risk\u2013Return)","text":"<p>Let \\(w\\) be portfolio weights, \\(\\mu\\) expected returns, and \\(\\Sigma\\) the covariance matrix. Objectives:</p> \\[ f_1(w) = -\\mu^\\top w, \\qquad f_2(w) = w^\\top \\Sigma w. \\] <p>Weighted scalarization:</p> \\[ \\min_w \\ -\\alpha \\mu^\\top w + (1-\\alpha) w^\\top \\Sigma w, \\quad 0 \\le \\alpha \\le 1. \\] <p>Varying \\(\\alpha\\) recovers the efficient frontier of Modern Portfolio Theory.</p>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-in-ml","title":"Example 3: Fairness\u2013Accuracy in ML","text":"\\[ \\min_\\theta \\ \\mathbb{E}[\\ell(y, f_\\theta(x))] \\quad \\text{s.t. } D(f_\\theta(x),y) \\le \\varepsilon, \\] <p>where \\(D\\) is a fairness metric. Scalarized form:</p> \\[ \\min_\\theta\\  \\mathbb{E}[\\ell(y, f_\\theta(x))] + \\lambda D(f_\\theta(x), y). \\] <p>Tuning \\(\\lambda\\) walks across the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-vae","title":"Example 4: Variational Autoencoders and \u03b2-VAE","text":"<p>The ELBO is:</p> \\[ \\mathbb{E}_{q(z)}[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z)). \\] <p>Objectives:</p> <ul> <li>Reconstruction fidelity,</li> <li>Latent simplicity.</li> </ul> <p>\u03b2-VAE scalarization:</p> \\[ \\max_q \\ \\mathbb{E}[\\log p(x|z)] - \\beta \\,\\mathrm{KL}(q(z)\\|p(z)). \\] <p>\\(\\beta\\) controls the trade-off between reconstruction and disentanglement \u2014 a Pareto frontier in latent space.</p> <p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-balancing-fit-and-complexity","title":"Chapter 11:  Balancing Fit and Complexity","text":"<p>Most real-world learning and estimation problems must balance two competing goals:</p> <ol> <li>Fit the observed data well, and  </li> <li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li> </ol> <p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>"},{"location":"convex/18b_regularization/#111-motivation-fit-vs-complexity","title":"11.1 Motivation: Fit vs. Complexity","text":"<p>Suppose we wish to estimate parameters \\(x\\) from data via a loss function \\(f(x)\\). If the data are noisy or the model is high-dimensional, solutions minimizing \\(f\\) alone may be unstable or overly complex. We introduce a regularizer \\(R(x)\\), typically convex, to encourage desirable structure:</p> \\[ \\min_{x} \\; f(x) + \\lambda R(x), \\qquad \\lambda &gt; 0. \\] <ul> <li>\\(f(x)\\): measures data misfit (e.g., squared loss, logistic loss).  </li> <li>\\(R(x)\\): penalizes complexity (e.g., \\(\\ell_1\\) norm for sparsity, \\(\\ell_2\\) norm for smoothness).  </li> <li>\\(\\lambda\\): controls the trade-off.<ul> <li>Small \\(\\lambda\\): excellent data fit, potentially overfitting.  </li> <li>Large \\(\\lambda\\): simpler model, potentially underfitting.</li> </ul> </li> </ul> <p>This is a scalarized multi-objective optimization problem of \\((f, R)\\).</p>"},{"location":"convex/18b_regularization/#112-bicriterion-optimization-and-the-pareto-frontier","title":"11.2 Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization corresponds to the bicriterion objective:</p> \\[ \\min_{x} \\; (f(x), R(x)). \\] <p>A point \\(x^*\\) is Pareto optimal if there is no feasible \\(x\\) such that:  with strict inequality in at least one component.</p> <p>For convex \\(f\\) and \\(R\\):</p> <ul> <li>Every \\(\\lambda \\ge 0\\) yields a Pareto-optimal point,</li> <li>The mapping from \\(\\lambda\\) to constraint level \\(R(x^*)\\) is monotone,</li> <li>The Pareto frontier is convex and can be traced continuously by varying \\(\\lambda\\).</li> </ul> <p>Thus, tuning \\(\\lambda\\) moves the solution along the fit\u2013complexity frontier.</p>"},{"location":"convex/18b_regularization/#113-why-control-the-size-of-the-solution","title":"11.3 Why Control the Size of the Solution?","text":"<p>Inverse problems such as \\(Ax \\approx b\\) are often ill-posed or ill-conditioned:</p> <ul> <li>Small noise in \\(b\\) may cause large variability in the solution \\(x\\).  </li> <li>If \\(A\\) is rank-deficient or nearly singular, infinitely many solutions exist.</li> </ul> <p>Example: ridge regression</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2. \\] <p>The optimality condition is</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b. \\] <p>Benefits of L2 regularization:</p> <ul> <li>\\(A^\\top A + \\lambda I\\) becomes positive definite for any \\(\\lambda &gt; 0\\),  </li> <li>the solution becomes unique and stable,  </li> <li>small singular directions of \\(A\\) are suppressed.</li> </ul> <p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>"},{"location":"convex/18b_regularization/#114-constrained-vs-penalized-formulations","title":"11.4 Constrained vs. Penalized Formulations","text":"<p>Regularized problems can be expressed equivalently as constrained problems:</p> \\[ \\min_x f(x)  \\quad \\text{s.t. } R(x) \\le t. \\] <p>The Lagrangian is</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda (R(x) - t), \\qquad \\lambda \\ge 0. \\] <p>The penalized form</p> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>is the dual of the constrained form. Under convexity and Slater\u2019s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*),  \\] \\[ R(x^*) \\le t,\\qquad \\lambda^* \\ge 0,\\qquad \\lambda^*(R(x^*) - t) = 0. \\] <p>Here:</p> <ul> <li>If \\(R(x^*) &lt; t\\), then \\(\\lambda^* = 0\\).  </li> <li>If \\(\\lambda^* &gt; 0\\), then \\(R(x^*) = t\\) (constraint active).</li> </ul> <p>Thus \\(\\lambda\\) is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>"},{"location":"convex/18b_regularization/#115-common-regularizers-and-their-effects","title":"11.5 Common Regularizers and Their Effects","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"\\[ R(x) = \\|x\\|_2^2. \\] <ul> <li>Smooth and strongly convex.  </li> <li>Shrinks coefficients uniformly.  </li> <li>Improves conditioning.  </li> <li>MAP interpretation: Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"\\[ R(x) = \\|x\\|_1 = \\sum_i |x_i|. \\] <ul> <li>Convex but not differentiable \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball has corners aligned with coordinate axes, encouraging zeros in \\(x\\).  </li> <li>Proximal operator (soft-thresholding):</li> </ul> \\[ \\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(v) = \\operatorname{sign}(v)\\,\\max(|v|-\\tau, 0). \\] <ul> <li>MAP interpretation: Laplace prior.</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"\\[ R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2. \\] <ul> <li>Combines sparsity with numerical stability.  </li> <li>Useful with correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2-structured-regularizers","title":"(d) Beyond L1/L2: Structured Regularizers","text":"Regularizer Formula Effect Tikhonov \\(\\|Lx\\|_2^2\\) smoothness via operator \\(L\\) Total Variation \\(\\|\\nabla x\\|_1\\) piecewise-constant signals/images Group Lasso \\(\\sum_g \\|x_g\\|_2\\) structured sparsity across groups Nuclear Norm \\(\\|X\\|_* = \\sum_i \\sigma_i\\) low-rank matrices <p>Each regularizer defines a geometry for the solution \u2014 ellipsoids, diamonds, polytopes, or spectral shapes.</p>"},{"location":"convex/18b_regularization/#116-choosing-the-regularization-parameter-lambda","title":"11.6 Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-Off Behavior","text":"<ul> <li>\\(\\lambda \\downarrow\\): favors small training error, high variance.  </li> <li>\\(\\lambda \\uparrow\\): favors simplicity, higher bias.  </li> </ul> <p>\\(\\lambda\\) selects a point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#b-cross-validation","title":"(b) Cross-Validation","text":"<p>The most common practice:</p> <ol> <li>Split data into folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining fold.  </li> <li>Choose \\(\\lambda\\) minimizing average validation error.</li> </ol> <p>Guidelines:</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>Use time-aware CV for dependent data.  </li> <li>Use the \u201cone-standard-error rule\u201d for simpler models.</li> </ul>"},{"location":"convex/18b_regularization/#c-other-selection-methods","title":"(c) Other Selection Methods","text":"<ul> <li>Information criteria (AIC, BIC) for sparsity.  </li> <li>L-curve or discrepancy principle in inverse problems.  </li> <li>Regularization paths: computing \\(x^*(\\lambda)\\) for many \\(\\lambda\\).</li> </ul>"},{"location":"convex/18b_regularization/#117-algorithmic-view","title":"11.7 Algorithmic View","text":"<p>Most regularized problems have the form:</p> \\[ \\min_x \\ f(x) + R(x), \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex (possibly nonsmooth).</p> <p>Common algorithms:</p> Method Idea When Useful Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), proximal step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update coordinates cyclically Lasso, Elastic Net ADMM Split problem to exploit structure Large-scale or distributed settings <p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal \\(O(1/k^2)\\) rate for smooth+convex problems.</p>"},{"location":"convex/18b_regularization/#118-bayesian-interpretation","title":"11.8 Bayesian Interpretation","text":"<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p> <p>Linear model:</p> \\[ b = Ax + \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I). \\] <p>With prior \\(x \\sim p(x)\\), MAP estimation solves:</p> \\[ \\min_x \\ \\frac{1}{2\\sigma^2}\\|Ax - b\\|_2^2 - \\log p(x). \\] <p>Examples:</p> <ul> <li>Gaussian prior \\(p(x) \\propto e^{-\\|x\\|_2^2 / (2\\tau^2)}\\)   \u2192 L2 penalty with \\(\\lambda = \\sigma^2/(2\\tau^2)\\).  </li> <li>Laplace prior   \u2192 L1 penalty and sparse MAP estimate.</li> </ul> <p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p> <p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#121-problem-classes-vs-method-classes","title":"12.1 Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#122-first-order-methods-gradient-descent","title":"12.2 First-order methods: Gradient descent","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  </p> <p>Smoothness lets us control step sizes.</p> <p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). Typical choices:</p> <ul> <li>constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known,</li> <li>backtracking line search when \\(L\\) is unknown,</li> <li>diminishing step sizes in some settings.</li> </ul> <p>Derivation: </p> <p>Around \\(x_t\\), we can approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Convergence: For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p> <p>When to use gradient descent:</p> <ul> <li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li> <li>You can compute gradients cheaply.</li> <li>You only need moderate accuracy.</li> <li>Memory constraints rule out storing or factoring Hessians.</li> </ul>"},{"location":"convex/19_optimizationalgo/#123-accelerated-first-order-methods","title":"12.3 Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#1231-nesterov-acceleration","title":"12.3.1 Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences \\(x_k\\) and \\(y_k\\):</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra momentum term \\(\\beta_k (x_{k+1}-x_k)\\) uses past iterates to \u201clook ahead\u201d and can significantly accelerate convergence.</p> <p>Convergece: For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives.</p> <ul> <li>Acceleration is effective for well-behaved smooth convex problems.</li> <li>It can be more sensitive to step size and noise than plain gradient descent.</li> <li>Variants such as FISTA apply acceleration in the composite setting \\(f + R\\).</li> </ul> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#124-steepest-descent-method","title":"12.4 Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization.cGradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>"},{"location":"convex/19_optimizationalgo/#1241-mathematical-properties","title":"12.4.1. Mathematical Properties","text":""},{"location":"convex/19_optimizationalgo/#a-normalized-direction","title":"(a) Normalized direction","text":"<p>  \u2192 unit vector with the most negative directional derivative.</p>"},{"location":"convex/19_optimizationalgo/#b-unnormalized-direction","title":"(b) Unnormalized direction","text":"<p>  This gives the actual direction and magnitude used in updates.</p>"},{"location":"convex/19_optimizationalgo/#c-key-identity","title":"(c) Key identity","text":"<p>  The directional derivative equals the negative squared dual norm of the gradient.</p>"},{"location":"convex/19_optimizationalgo/#1242-the-steepest-descent-method","title":"12.4.2. The Steepest Descent Method","text":"<p>The iterative update rule is:  where \\(t_k &gt; 0\\) is a step size (from line search or a fixed rule).</p> <ul> <li>For the Euclidean norm, this reduces to ordinary gradient descent.</li> <li>For other norms, it adapts the search direction to the geometry of the problem.</li> </ul> <p>Convergence: Similar to gradient descent \u2014 linear for general convex functions, potentially faster when level sets are well-conditioned.</p>"},{"location":"convex/19_optimizationalgo/#1243-role-of-the-norm-and-its-influence","title":"12.4.3. Role of the Norm and Its Influence","text":"<p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#a-euclidean-norm-v_2","title":"(a) Euclidean Norm \\(|v|_2\\)","text":"\\[ \\Delta x_{\\text{nsd}} = -\\frac{\\nabla f(x)}{|\\nabla f(x)|*2}, \\quad \\Delta x*{\\text{sd}} = -\\nabla f(x) \\] <p>This is standard gradient descent. The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>"},{"location":"convex/19_optimizationalgo/#b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0","title":"(b) Quadratic Norm \\(|v|_P = (v^T P v)^{1/2}\\), with \\(P \\succ 0\\)","text":"<p>Here, \\(P\\) defines an ellipsoidal metric. The dual norm is \\(|y|_* = (y^T P^{-1} y)^{1/2}\\).</p> \\[ \\Delta x_{\\text{sd}} = -P^{-1}\\nabla f(x) \\] <p>This corresponds to preconditioned gradient descent, where \\(P\\) rescales directions to counter anisotropy in level sets.</p> <p>Interpretation:</p> <ul> <li>If \\(P\\) approximates the Hessian, this becomes Newton\u2019s method.</li> <li>If \\(P\\) is diagonal, it acts like an adaptive step size per coordinate.</li> </ul>"},{"location":"convex/19_optimizationalgo/#c-ell_1-norm","title":"(c) \\(\\ell_1\\)-Norm","text":"\\[ \\Delta x_{\\text{nsd}} = -e_i, \\quad i = \\arg\\max_j \\left|\\frac{\\partial f}{\\partial x_j}\\right| \\] <p>and</p> \\[ \\Delta x_{\\text{sd}} = -|\\nabla f(x)|_\\infty e_i \\] <p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p> <p>Geometric intuition: The \\(\\ell_1\\)-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p> <ul> <li>In \\(\\ell_2\\)-norm: the unit ball is a circle \u2192 the steepest direction is exactly opposite the gradient.</li> <li>In \\(\\ell_1\\)-norm: the unit ball is a diamond \u2192 the steepest direction points to a corner (one coordinate).</li> <li>In quadratic norms: the unit ball is an ellipsoid \u2192 the steepest direction follows the metric-adjusted gradient.</li> </ul> <p>Hence, the norm defines the geometry of what \u201csteepest\u201d means.</p>"},{"location":"convex/19_optimizationalgo/#125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives","title":"12.5 Conjugate Gradient Method \u2014 Fast Optimization for Quadratic Objectives","text":"<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p> <p>CG is a first-order method that behaves like a second-order method for quadratics.</p> <p>For a quadratic objective function:</p> \\[ f(x) = \\tfrac12 x^\\top A x - b^\\top x  \\] <p>with \\(A \\succ 0\\), the level sets are ellipses shaped by the eigenvalues of \\(A\\). If \\(A\\) is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p> <p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors \\(p_i, p_j\\) are said to be A-conjugate if</p> \\[ p_i^\\top A p_j = 0. \\] <p>This is orthogonality measured in the geometry induced by the Hessian \\(A\\). Why is this useful?</p> <ul> <li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of \\(A\\).</li> <li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li> <li>After \\(n\\) mutually A-conjugate directions, all curvature directions are resolved \u2192 exact solution.</li> </ul> <p>In contrast, gradient descent repeatedly re-corrects previous progress.</p> <p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system \\(Ax = b\\). Let</p> \\[ r_0 = b - A x_0, \\qquad p_0 = r_0. \\] <p>For \\(k = 0,1,2,\\dots\\):</p> <ol> <li> <p>Step size     </p> </li> <li> <p>Update iterate     </p> </li> <li> <p>Update residual (negative gradient)     </p> </li> <li> <p>Direction scaling     </p> </li> <li> <p>New conjugate direction     </p> </li> </ol> <p>Stop when \\(\\|r_k\\|\\) is below tolerance.</p> <p>Every new direction \\(p_{k+1}\\) is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p> <p>Why CG Is Fast: For an \\(n\\)-dimensional quadratic, CG solves the problem in at most \\(n\\) iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in \\(O(\\sqrt{\\kappa})\\) iterations, where \\(\\kappa = \\lambda_{\\max}/\\lambda_{\\min}\\) is the condition number. The convergence bound in the A-norm is:</p> \\[ \\|x_k - x^\\star\\|_A \\le  2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k  \\|x_0 - x^\\star\\|_A. \\] <p>This is dramatically better than the \\(O(1/k)\\) rate of gradient descent.</p> <p>CG is ideal when:</p> <ul> <li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix \\(A\\).</li> <li>\\(A\\) is large and sparse or available as a matrix\u2013vector product.</li> <li>You cannot form or store \\(A^{-1}\\) or even the full matrix \\(A\\).</li> <li>You want a Hessian-aware method but cannot afford Newton\u2019s method.</li> </ul> <p>Typical scenarios:</p> Application Why CG fits Large linear systems \\(A x = b\\) Only requires \\(A p\\), not factorization. Ridge regression Normal equations form an SPD matrix. Kernel ridge regression Solves \\((K+\\lambda I)\\alpha = y\\) efficiently. Newton steps in ML Inner solver for Hessian systems without forming Hessian. PDEs and scientific computing Sparse SPD matrices, ideal for CG. <p>Assumptions Required for CG: To guarantee correctness of linear CG, we require:</p> <ul> <li>\\(A\\) is symmetric</li> <li>\\(A\\) is positive definite</li> <li>Objective is strictly convex quadratic</li> <li>Arithmetic is exact (for the finite-step guarantee)</li> </ul> <p>If the function is not quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p> <p>Practical Notes:</p> <ul> <li>You only need matrix\u2013vector products \\(Ap\\).  </li> <li>Storage cost is \\(O(n)\\).  </li> <li>Preconditioning (replacing the system with \\(M^{-1} A\\)) improves conditioning and accelerates convergence dramatically.  </li> <li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li> </ul> <p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>"},{"location":"convex/19_optimizationalgo/#126-newtons-method-and-second-order-methods","title":"12.6 Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p> <p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex \\(f\\), Newton\u2019s method achieves quadratic convergence\u2014dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order algorithms.</p> <p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton\u2019s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>"},{"location":"convex/19_optimizationalgo/#solving-the-newton-system","title":"Solving the Newton System","text":"<p>Each iteration requires solving</p> \\[ H \\,\\Delta x = -g, \\qquad H = \\nabla^2 f(x), \\;\\; g = \\nabla f(x). \\] <p>If \\(H\\) is symmetric positive definite, a Cholesky factorization</p> \\[ H = L L^\\top \\] <p>allows efficient and numerically stable solution via two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^\\top \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids forming \\(H^{-1}\\) explicitly.</p> <p>The Newton decrement:</p> \\[ \\lambda(x) = \\|L^{-1} g\\|_2 \\] <p>gauges proximity to the optimum and provides a natural stopping criterion: \\(\\lambda(x)^2/2 &lt; \\varepsilon\\).</p> <p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs \\(\\approx (1/3)n^3\\) operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton\u2019s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix\u2013free iterative methods.</p>"},{"location":"convex/19_optimizationalgo/#gaussnewton-method","title":"Gauss\u2013Newton Method","text":"<p>The Gauss\u2013Newton method is a specialization of Newton\u2019s method for nonlinear least squares problems</p> \\[ f(x) = \\tfrac12 \\| r(x) \\|^2, \\] <p>where \\(r(x)\\) is a vector of residual functions and a nonlinear function of \\(x\\) and \\(J\\) is its Jacobian. Newton\u2019s Hessian decomposes as</p> \\[ \\nabla^2 f(x) = J^\\top J \\;+\\; \\sum_i r_i(x)\\, \\nabla^2 r_i(x). \\] <p>The second term involves the curvature of the residuals. When \\(r(x)\\) is approximately linear near the optimum, this term is small. Gauss\u2013Newton drops it, giving the approximation</p> \\[ \\nabla^2 f(x) \\approx J^\\top J, \\] <p>leading to the Gauss\u2013Newton step:</p> \\[ (J^\\top J)\\, \\Delta = -J^\\top r. \\] <p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg\u2013Marquardt method adds a damping term,</p> \\[ (J^\\top J + \\lambda I)\\, \\Delta = -J^\\top r, \\] <p>which interpolates smoothly between  </p> <ul> <li>gradient descent (large \\(\\lambda\\)), and  </li> <li>Gauss\u2013Newton (small \\(\\lambda\\)).</li> </ul> <p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss\u2013Newton and Levenberg\u2013Marquardt are highly effective when the residuals are nearly linear\u2014common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning\u2014yielding fast convergence without the expense of full second derivatives.</p>"},{"location":"convex/19_optimizationalgo/#quasi-newton-methods","title":"Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p> <p>When to use Newton or quasi-Newton methods:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#128-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"12.8 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#1281-projected-gradient-descent","title":"12.8.1 Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#1282-proximal-gradient-forwardbackward-splitting","title":"12.8.2 Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#129-penalties-barriers-and-interior-point-methods","title":"12.9 Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\): Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#1291-penalty-methods","title":"12.9.1 Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-basic-penalty-method-quadratic-or-general-penalization","title":"Algorithm: Basic Penalty Method (Quadratic or General Penalization)","text":"<p>Goal:  Solve </p> <p>Penalty formulation:  where  </p> <ul> <li>\\(\\phi(r) = 0\\) if \\(r \\le 0\\),  </li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (e.g., \\(\\phi(r)=\\max\\{0,r\\}^2\\)),  </li> <li>\\(\\rho &gt; 0\\) is the penalty weight.</li> </ul> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>constraints \\(g_i(x)\\) </li> <li>penalty function \\(\\phi\\) </li> <li>initial point \\(x_0\\) </li> <li>initial penalty parameter \\(\\rho_0 &gt; 0\\) </li> <li>penalty update factor \\(\\gamma &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose \\(x_0\\), \\(\\rho_0 &gt; 0\\).  </li> <li>For \\(k = 0, 1, 2, \\dots\\):  <ol> <li>Solve the penalized subproblem  \\(x_{k+1} = \\arg\\min_x F_{\\rho_k}(x)\\) using Newton\u2019s method, gradient descent, quasi-Newton, etc.  </li> <li>Check feasibility / stopping:  If \\(\\max_i g_i(x_{k+1}) \\le \\varepsilon, \\quad   \\|x_{k+1} - x_k\\| \\le \\varepsilon\\)  stop and return \\(x_{k+1}\\).  </li> <li>Increase penalty parameter  \\(\\rho_{k+1} = \\gamma\\, \\rho_k\\)   with typical \\(\\gamma \\in [5,10]\\).  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#1292-barrier-methods","title":"12.9.2 Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-barrier-method-logarithmic-barrier-interior-approximation","title":"Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)","text":"<p>Goal: Solve the constrained problem </p> <p>Logarithmic barrier:  defined only for strictly feasible points \\(g_i(x)&lt;0\\).</p> <p>Barrier subproblem:  where \\(t&gt;0\\) is the barrier parameter.</p> <p>As \\(t \\to \\infty\\), minimizers of \\(F_t\\) approach the constrained optimum.</p> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>barrier function \\(b(x)\\) </li> <li>strictly feasible starting point \\(x_0\\) (\\(g_i(x_0) &lt; 0\\))  </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>barrier growth factor \\(\\mu &gt; 1\\) (often \\(\\mu = 10\\))  </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose strictly feasible \\(x_0\\), and pick \\(t_0 &gt; 0\\).  </li> <li>For \\(k = 0,1,2,\\dots\\):  <ol> <li>Centering step (inner loop):  Solve the barrier subproblem    Typically use Newton\u2019s method (damped) on \\(F_{t_k}\\).  Stop when the Newton decrement satisfies  \\(\\lambda(x_{k+1})^2/2 \\le \\varepsilon\\)</li> <li>Optimality / stopping test:    If  \\(\\frac{m}{t_k} \\le \\varepsilon,\\)   then \\(x_{k+1}\\) is an \\(\\varepsilon\\)-approximate solution of the original constrained problem; stop and return \\(x_{k+1}\\).  </li> <li>Increase barrier parameter:  \\(t_{k+1} = \\mu\\, t_k,\\)   which tightens the approximation and moves closer to the boundary.  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#1293-interior-point-methods","title":"12.9.3 Interior-point methods","text":"<p>Interior-point methods combine barrier functions with Newton\u2019s method to solve general convex programs:</p> <ul> <li>They maintain strict feasibility throughout.</li> <li>Each iteration solves a Newton system for the barrier-augmented objective.</li> <li>They naturally generate primal\u2013dual pairs and duality gap estimates.</li> <li>Under standard assumptions (e.g., Slater\u2019s condition), they converge in a predictable number of iterations.</li> </ul> <p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>"},{"location":"convex/19_optimizationalgo/#algorithm-primaldual-interior-point-method-for-convex-inequality-constraints","title":"Algorithm: Primal\u2013Dual Interior-Point Method (for convex inequality constraints)","text":"<p>We consider the problem  </p> <p>Introduce Lagrange multipliers \\(\\lambda \\ge 0\\). The KKT conditions are  </p> <p>Interior-point methods enforce the relaxed condition  which keeps iterates strictly feasible.</p>"},{"location":"convex/19_optimizationalgo/#inputs","title":"Inputs","text":"<ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>initial primal point \\(x_0\\) with \\(g_i(x_0)&lt;0\\) </li> <li>initial dual variable \\(\\lambda_0 &gt; 0\\) </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>growth factor \\(\\mu &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul>"},{"location":"convex/19_optimizationalgo/#procedure","title":"Procedure","text":"<ol> <li> <p>Choose strictly feasible \\(x_0\\), positive \\(\\lambda_0\\), and \\(t_0\\).</p> </li> <li> <p>For \\(k = 0,1,2,\\dots\\):</p> <p>(a) Form the perturbed KKT system.  Solve for the Newton direction \\((\\Delta x, \\Delta \\lambda)\\):</p> <p> </p> <p>(b) Line search to keep strict feasibility. Choose the maximum \\(\\alpha\\in(0,1]\\) such that:</p> <ul> <li>\\(g_i(x + \\alpha \\Delta x) &lt; 0\\),</li> <li>\\(\\lambda + \\alpha \\Delta \\lambda &gt; 0\\).</li> </ul> <p>(c) Update: \\(x \\leftarrow x + \\alpha \\Delta x,    \\qquad  \\lambda \\leftarrow \\lambda + \\alpha \\Delta \\lambda.\\)</p> <p>(d) Check duality gap: \\(\\text{gap} = - g(x)^\\top \\lambda\\) If \\(\\text{gap} \\le \\varepsilon\\), stop.</p> <p>(e) Increase barrier parameter \\(t \\leftarrow \\mu t.\\)</p> </li> <li> <p>Return \\(x\\).</p> </li> </ol>"},{"location":"convex/19_optimizationalgo/#1210-choosing-the-right-method-in-practice","title":"12.10 Choosing the right method in practice","text":"<p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever variables must satisfy exact relationships, such as conservation laws, normalization, or linear invariants. In this chapter we focus on problems of the form</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>where \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is (typically convex and differentiable) and \\(A \\in \\mathbb{R}^{p \\times n}\\) has rank \\(p\\). This linear equality structure appears in constrained least squares, portfolio optimization, and many ML formulations that impose exact balance or normalization constraints.</p>"},{"location":"convex/19a_optimization_constraints/#131-geometric-view-optimization-on-an-affine-manifold","title":"13.1 Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(\\operatorname{rank}(A) = p\\), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine subspace of \\(\\mathbb{R}^n\\): a \u201cflat\u201d lower-dimensional plane embedded in the ambient space. Optimization now happens along this plane, not in all of \\(\\mathbb{R}^n\\). Any feasible direction \\(d\\) must keep us in \\(\\mathcal{X}\\), so it must satisfy</p> \\[ A (x + d) = b \\quad \\Rightarrow \\quad A d = 0. \\] <p>Thus, feasible directions lie in the null space of \\(A\\):</p> \\[ \\mathcal{D}_{\\text{feas}} = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\} = \\operatorname{Null}(A). \\] <p>At an optimal point \\(x^\\star \\in \\mathcal{X}\\), moving in any feasible direction \\(d\\) cannot decrease \\(f\\). For differentiable \\(f\\), this means</p> \\[ \\nabla f(x^\\star)^\\top d \\ge 0 \\quad \\text{for all } d \\text{ with } A d = 0. \\] <p>Equivalently, \\(\\nabla f(x^\\star)\\) must be orthogonal to all feasible directions, i.e. it lies in the row space of \\(A\\). Therefore there exists a vector of Lagrange multipliers \\(\\nu^\\star\\) such that</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star. \\] <p>This is the basic geometric optimality condition: at the optimum, the gradient of \\(f\\) is a linear combination of the constraint normals (rows of \\(A\\)), and every feasible direction is orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#132-lagrange-function-and-kkt-system","title":"13.2 Lagrange Function and KKT System","text":"<p>The Lagrangian for the equality-constrained problem is</p> \\[ \\mathcal{L}(x,\\nu) = f(x) + \\nu^\\top (A x - b), \\] <p>where \\(\\nu \\in \\mathbb{R}^p\\) are Lagrange multipliers. The first-order (KKT) conditions for a point \\((x^\\star,\\nu^\\star)\\) to be optimal are</p> \\[ \\begin{aligned} \\nabla_x \\mathcal{L}(x^\\star,\\nu^\\star) &amp;= \\nabla f(x^\\star) + A^\\top \\nu^\\star = 0  \\quad &amp;\\text{(stationarity)},\\\\ A x^\\star &amp;= b  \\quad &amp;\\text{(primal feasibility)}. \\end{aligned} \\] <p>When \\(f\\) is convex and \\(A\\) has full row rank, these conditions are necessary and sufficient for global optimality. For Newton-type methods we linearize these conditions around a current iterate \\((x,\\nu)\\) and solve for corrections \\((\\Delta x,\\Delta \\nu)\\) from</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>This linear system is called the (equality-constrained) KKT system. At the optimum the right-hand side is zero.</p>"},{"location":"convex/19a_optimization_constraints/#133-quadratic-objectives","title":"13.3 Quadratic Objectives","text":"<p>A particularly important case is a convex quadratic objective</p> \\[ f(x) = \\tfrac{1}{2} x^\\top P x + q^\\top x + r, \\] <p>with \\(P \\succeq 0\\). The equality-constrained problem</p> \\[ \\min_x \\tfrac{1}{2} x^\\top P x + q^\\top x + r  \\quad \\text{s.t.} \\quad A x = b \\] <p>has KKT conditions</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>If \\(P \\succ 0\\) and \\(A\\) has full row rank, this system has a unique solution \\((x^\\star,\\nu^\\star)\\). This is the standard linear system solved in equality-constrained least squares and quadratic programming.</p> <p>Examples in ML and statistics:</p> <ul> <li>constrained least squares with sum-to-one constraints on coefficients;  </li> <li>portfolio optimization with ;  </li> <li>quadratic surrogate subproblems inside second-order methods.</li> </ul> <p>The structure of the KKT matrix (symmetric, indefinite, with blocks \\(P\\), \\(A\\)) can be exploited by specialized linear solvers and factorizations.</p>"},{"location":"convex/19a_optimization_constraints/#134-null-space-reduced-variable-method","title":"13.4 Null-Space (Reduced Variable) Method","text":"<p>When the constraints are linear and of full row rank, a natural approach is to eliminate them explicitly.</p> <p>Choose:</p> <ul> <li>a particular feasible point \\(x_0\\) satisfying \\(A x_0 = b\\),  </li> <li>a matrix \\(Z \\in \\mathbb{R}^{n \\times (n-p)}\\) whose columns form a basis of the null space of \\(A\\):    </li> </ul> <p>Then every feasible \\(x\\) can be written as</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective yields an unconstrained reduced problem in the smaller variable \\(y\\):</p> \\[ \\min_{y} \\; \\phi(y) := f(x_0 + Z y). \\] <p>Gradients and Hessians transform as</p> \\[ \\nabla_y \\phi(y) = Z^\\top \\nabla_x f(x_0 + Z y), \\qquad \\nabla_y^2 \\phi(y) = Z^\\top \\nabla_x^2 f(x_0 + Z y) \\, Z. \\] <p>We can now apply any unconstrained method (gradient descent, CG, Newton) to \\(\\phi(y)\\). The corresponding updates in the original space are mapped back via \\(x = x_0 + Z y\\).</p> <p>Key points:</p> <ul> <li>Optimization is restricted to feasible directions \\(\\operatorname{Null}(A)\\) by construction.  </li> <li>The dimension drops from \\(n\\) to \\(n-p\\), which can be advantageous if \\(p\\) is large.  </li> <li>The cost is computing and storing a suitable null-space basis \\(Z\\), which may destroy sparsity and be expensive for large-scale problems.</li> </ul> <p>Null-space methods are attractive when:</p> <ul> <li>the number of constraints is moderate,  </li> <li>a good factorization of \\(A\\) is available,  </li> <li>and we want an unconstrained algorithm in reduced coordinates.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#135-newtons-method-for-equality-constrained-problems","title":"13.5 Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice-differentiable convex \\(f\\), we can derive an equality-constrained Newton step by solving a local quadratic approximation subject to linearized constraints.</p> <p>At a point \\(x\\), approximate \\(f(x+d)\\) by its second-order Taylor expansion:</p> \\[ f(x+d) \\approx f(x) + \\nabla f(x)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d. \\] <p>We seek a step \\(d\\) that approximately minimizes this quadratic model while remaining feasible to first order, i.e.</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d\\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>The KKT conditions for this quadratic subproblem are</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>Solving this system gives the Newton step \\(d_{\\text{nt}}\\) and a multiplier update \\(\\lambda\\). The primal update is</p> \\[ x_{k+1} = x_k + \\alpha_k d_{\\text{nt}}, \\] <p>with a step size \\(\\alpha_k \\in (0,1]\\) chosen by line search to ensure sufficient decrease and preservation of feasibility (for equality constraints, \\(A d_{\\text{nt}} = 0\\) guarantees \\(A x_{k+1} = b\\) whenever \\(A x_k = b\\)).</p> <p>Geometrically:</p> <ul> <li>unconstrained Newton would move by \\(-\\nabla^2 f(x)^{-1} \\nabla f(x)\\);  </li> <li>equality-constrained Newton projects this step onto the tangent space \\(\\{ d : A d = 0 \\}\\) of the affine constraint set.</li> </ul> <p>For strictly convex \\(f\\) with positive definite Hessian on the feasible directions, this method enjoys quadratic convergence near the solution, much like the unconstrained Newton method.</p>"},{"location":"convex/19a_optimization_constraints/#136-connections-to-machine-learning-and-signal-processing","title":"13.6 Connections to Machine Learning and Signal Processing","text":"<p>Linear equality constraints appear naturally in ML and related areas:</p> Setting Equality constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights sum to one (full investment) Constrained regression \\(C x = d\\) Enforce domain-specific linear relations between coefficients Mixture models / convex combinations \\(\\mathbf{1}^\\top \\alpha = 1, \\; \\alpha \\ge 0\\) Mixture weights form a probability simplex Fairness constraints (linearized) \\(A w = 0\\) Enforce equal averages across groups or balance conditions Physics-informed models (discretized) \\(A x = b\\) Discrete conservation laws (mass, charge, energy) <p>More generally, nonlinear equality constraints (e.g. \\(W^\\top W = I\\) for orthonormal embeddings, or \\(\\|w\\|_2^2 = 1\\) for normalized weights) lead to optimization on curved manifolds. Techniques from this chapter extend to those settings when combined with Riemannian optimization or local parameterizations, but here we focus on the linear case as the fundamental building block.</p>"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In many applications, we must optimize an objective while respecting inequality constraints: nonnegativity of variables, margin constraints in SVMs, capacity or safety limits, physical bounds, fairness budgets, and more. Mathematically, the feasible region is now a convex set with a boundary, and the optimizer often lies on that boundary.</p> <p>This chapter introduces algorithms for solving such problems, focusing on logarithmic barrier and interior-point methods. These are the workhorses behind modern general-purpose convex solvers (for LP, QP, SOCP, SDP) and provide a smooth way to enforce inequalities while still using Newton-type methods.</p>"},{"location":"convex/19b_optimization_constraints/#141-problem-setup","title":"14.1 Problem Setup","text":"<p>We consider the general convex problem with inequality and equality constraints  where</p> <ul> <li>\\(f_0, f_1,\\dots,f_m\\) are convex, typically twice differentiable,</li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank,</li> <li>there exists a strictly feasible point \\(\\bar{x}\\) such that   \\(f_i(\\bar{x}) &lt; 0\\) for all \\(i\\) and \\(A\\bar{x} = b\\) (Slater\u2019s condition).</li> </ul> <p>Under these assumptions:</p> <ul> <li>the problem is convex,</li> <li>strong duality holds (zero duality gap),</li> <li>and the KKT conditions characterize optimality.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem type \\(f_0(x)\\) Constraints \\(f_i(x)\\le0\\) ML / applications Linear program (LP) \\(c^\\top x\\) \\(a_i^\\top x - b_i \\le 0\\) resource allocation, feature selection Quadratic program \\(\\tfrac12 x^\\top P x + q^\\top x\\) linear SVMs, ridge with box constraints QCQP quadratic quadratic portfolio optimization, control Entropy models \\(\\sum_i x_i \\log x_i\\) \\(F x - g \\le 0\\) probability calibration, max-entropy Nonnegativity arbitrary convex \\(-x_i \\le 0\\) sparse coding, nonnegative factorization <p>Many machine-learning training problems can be written in this template by expressing regularization, margins, fairness, or safety conditions as convex inequalities.</p>"},{"location":"convex/19b_optimization_constraints/#142-indicator-function-view-of-constraints","title":"14.2 Indicator-Function View of Constraints","text":"<p>Conceptually, we can write inequality constraints using an indicator function. Define  </p> <p>Then the inequality-constrained problem is equivalent to  </p> <ul> <li>If \\(x\\) is feasible (\\(f_i(x) \\le 0\\) for all \\(i\\)), the indicators contribute \\(0\\).</li> <li>If any constraint is violated (\\(f_i(x) &gt; 0\\)), the objective becomes \\(+\\infty\\).</li> </ul> <p>This formulation is clean but not numerically friendly:</p> <ul> <li>\\(I_{-}\\) is discontinuous and nonsmooth.</li> <li>We cannot directly apply Newton-type methods.</li> </ul> <p>The key idea of barrier methods is to replace the hard indicator with a smooth approximation that grows to \\(+\\infty\\) as we approach the boundary.</p>"},{"location":"convex/19b_optimization_constraints/#143-logarithmic-barrier-approximation","title":"14.3 Logarithmic Barrier Approximation","text":"<p>We approximate the indicator \\(I_{-}\\) with a smooth barrier function  </p> <p>For each inequality \\(f_i(x) \\le 0\\), we introduce a barrier term \\(-\\log(-f_i(x))\\). For a given parameter \\(t &gt; 0\\), we solve the barrier subproblem  where  </p> <p>Equivalently,  </p> <p>Interpretation:</p> <ul> <li>The barrier term \\(\\phi(x)\\) is finite only for strictly feasible points (\\(f_i(x) &lt; 0\\)).</li> <li>As \\(x\\) approaches the boundary \\(f_i(x) \\to 0^-\\), the term \\(-\\log(-f_i(x)) \\to +\\infty\\).</li> <li>The parameter \\(t\\) controls the trade-off:</li> <li>small \\(t\\) (large \\(1/t\\)) \u2192 strong barrier, solution stays deep inside the feasible set;</li> <li>large \\(t\\) \u2192 barrier is weaker, solutions can move closer to the boundary.</li> </ul> <p>As \\(t \\to \\infty\\), solutions of the barrier subproblem approach the solution of the original constrained problem.</p>"},{"location":"convex/19b_optimization_constraints/#144-derivatives-of-the-barrier","title":"14.4 Derivatives of the Barrier","text":"<p>Let  Then \\(\\phi\\) is convex and twice differentiable on its domain. Its gradient and Hessian are  </p> <p>Key features:</p> <ul> <li>As \\(f_i(x) \\uparrow 0\\) (approaching the boundary from inside), the factor \\(1/(-f_i(x))\\) blows up, so \\(\\|\\nabla \\phi(x)\\|\\) becomes very large.</li> <li>This creates a strong repulsive force that prevents iterates from crossing the boundary.</li> <li>The barrier \u201cpushes\u201d the solution away from constraint violation, while the original objective \\(f_0(x)\\) pulls toward lower cost.</li> </ul> <p>The barrier subproblem  is a smooth equality-constrained problem. We can therefore apply equality-constrained Newton methods (Chapter 13) at each fixed \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#145-central-path-and-approximate-kkt-conditions","title":"14.5 Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^\\star(t)\\) be a minimizer of the barrier problem  </p> <p>The set \\(\\{x^\\star(t) : t &gt; 0\\}\\) is called the central path. As \\(t \\to \\infty\\), \\(x^\\star(t)\\) converges to a solution \\(x^\\star\\) of the original inequality-constrained problem.</p> <p>We can associate approximate dual variables to \\(x^\\star(t)\\):  </p> <p>Then the KKT-like relations hold:  </p> <p>Compare with the exact KKT conditions (for optimal \\((x^\\star,\\lambda^\\star,v^\\star)\\)):  </p> <p>Along the central path we have the relaxed complementarity condition  which tends to \\(0\\) as \\(t \\to \\infty\\). Hence the barrier formulation naturally yields approximate primal\u2013dual solutions whose KKT residuals shrink as we increase \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#146-geometric-and-physical-intuition","title":"14.6 Geometric and Physical Intuition","text":"<p>Consider the barrier-augmented objective  </p> <p>We can interpret this as:</p> <ul> <li>\\(t f_0(x)\\): an \u201cexternal potential\u201d pulling us toward low objective values.</li> <li>\\(-\\log(-f_i(x))\\): repulsive potentials that become infinite near the boundary \\(f_i(x)=0\\).</li> </ul> <p>At a minimizer \\(x^\\star(t)\\), we have  </p> <p>The gradient of the objective is exactly balanced by a weighted sum of constraint gradients. This is a force-balance condition:</p> <ul> <li>constraints \u201cpush back\u201d more strongly when \\(x\\) is close to their boundary,</li> <li>the interior-point iterates follow a smooth path that stays strictly feasible   and moves gradually toward the optimal boundary point.</li> </ul> <p>This picture explains both:</p> <ul> <li>why iterates never leave the feasible region, and  </li> <li>why the method naturally generates dual variables (the weights on constraint gradients).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#147-the-barrier-method","title":"14.7 The Barrier Method","text":"<p>The barrier method solves the original inequality-constrained problem by solving a sequence of barrier subproblems with increasing \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-barrier-method-conceptual-form","title":"Algorithm: Barrier Method (Conceptual Form)","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (\\(f_i(x) &lt; 0\\), \\(A x = b\\)),</li> <li>initial barrier parameter \\(t &gt; 0\\),</li> <li>barrier growth factor \\(\\mu &gt; 1\\) (e.g. \\(\\mu \\in [10,20]\\)),</li> <li>accuracy tolerance \\(\\varepsilon &gt; 0\\),</li> </ul> <p>repeat:</p> <ol> <li> <p>Centering step    Solve the equality-constrained problem        using an equality-constrained Newton method.    (In practice, we start from the previous solution and take a small number of Newton steps rather than \u201csolve exactly\u201d.)</p> </li> <li> <p>Update iterate    Let \\(x\\) be the resulting point (the approximate minimizer for current \\(t\\)).</p> </li> <li> <p>Check stopping criterion    For the barrier problem, one can show        where \\(p^\\star\\) is the optimal value of the original problem.    If        then stop: \\(x\\) is guaranteed to be within \\(\\varepsilon\\) (in objective value) of optimal.</p> </li> <li> <p>Increase \\(t\\)    Set \\(t := \\mu t\\) to weaken the barrier and move closer to the true boundary, then go back to Step 1.</p> </li> </ol> <p>Key parameters:</p> Symbol Role \\(t\\) barrier strength (larger \\(t\\) = weaker barrier, closer to solution) \\(\\mu\\) growth factor for \\(t\\) \\(\\varepsilon\\) desired accuracy (duality-gap based) \\(m\\) number of inequality constraints <p>In practice:</p> <ul> <li>\\(\\varepsilon\\) is often in the range \\(10^{-3}\\)\u2013\\(10^{-8}\\),</li> <li>\\(\\mu\\) is chosen to balance outer iterations vs inner Newton steps,</li> <li>the centering step is usually solved to modest accuracy, not exactness.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#148-from-barrier-methods-to-interior-point-methods","title":"14.8 From Barrier Methods to Interior-Point Methods","text":"<p>Pure barrier methods conceptually \u201csolve a sequence of problems for increasing \\(t\\)\u201d. Modern interior-point methods refine this idea:</p> <ul> <li>they update both primal variables \\(x\\) and dual variables \\((\\lambda, v)\\),</li> <li>they use Newton\u2019s method on the (perturbed) KKT system,</li> <li>they follow the central path by simultaneously enforcing:</li> <li>primal feasibility (\\(f_i(x) \\le 0\\), \\(A x = b\\)),</li> <li>dual feasibility (\\(\\lambda_i \\ge 0\\)),</li> <li>relaxed complementarity (\\(-\\lambda_i f_i(x) \\approx 1/t\\)).</li> </ul> <p>A typical primal\u2013dual step solves a linearized KKT system of the form  </p> <p>Newton\u2019s method applied to these equations yields search directions for \\((x,\\lambda,v)\\) that move toward the central path and reduce primal and dual residuals simultaneously. This is what modern LP/QP/SOCP/SDP solvers implement.</p> <p>You do not need to implement these methods from scratch to use them: in practice, you describe your problem in a modeling language (e.g. CVX, CVXPY, JuMP) and rely on an interior-point solver under the hood.</p>"},{"location":"convex/19b_optimization_constraints/#149-computational-and-practical-notes","title":"14.9 Computational and Practical Notes","text":"<p>Some important practical aspects:</p> <ol> <li> <p>Equality-constrained Newton inside    Each barrier subproblem is solved by equality-constrained Newton (Chapter 13). The main cost is solving the KKT linear system at each Newton step.</p> </li> <li> <p>Strict feasibility    Barrier and interior-point methods require a strictly feasible starting point \\(x\\) with \\(f_i(x) &lt; 0\\).  </p> </li> <li>Sometimes this is easy (e.g. nonnegativity constraints with a positive initial vector).  </li> <li> <p>Otherwise, a separate phase I problem is solved to find such a point or to certify infeasibility.</p> </li> <li> <p>Step size control    Because the barrier blows up near the boundary, too aggressive Newton steps may try to leave the feasible region. A backtracking line search is used to ensure:</p> </li> <li>sufficient decrease in the barrier objective,</li> <li> <p>and preservation of strict feasibility (\\(f_i(x) &lt; 0\\) remains true).</p> </li> <li> <p>Accuracy vs cost    The duality-gap bound \\(m/t\\) provides a clear trade-off:</p> </li> <li>small \\(m/t\\) (large \\(t\\)) \u2192 high accuracy, more iterations,</li> <li> <p>larger \\(m/t\\) \u2192 faster but less precise.</p> </li> <li> <p>Sparsity and structure    For large problems, exploiting sparsity in \\(A\\) and in the Hessians \\(\\nabla^2 f_i(x)\\) is crucial. Interior-point methods scale well when linear algebra is carefully optimized.</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#1410-equality-vs-inequality-constrained-algorithms","title":"14.10 Equality vs Inequality-Constrained Algorithms","text":"<p>Finally, it is helpful to contrast the equality-only case (Chapter 13) with the inequality case.</p> Aspect Equality constraints \\(A x = b\\) Inequality constraints \\(f_i(x) \\le 0\\) Feasible set Affine subspace General convex region with boundary Typical algorithms Lagrange/KKT, equality-constrained Newton, null-space Barrier methods, primal\u2013dual interior-point methods Feasibility during iteration Can start infeasible and converge to \\(A x = b\\) Iterates kept strictly feasible (\\(f_i(x) &lt; 0\\)) Complementarity Not present (only equalities) \\(\\lambda_i f_i(x) = 0\\) at optimum, or \\(\\approx -1/t\\) along central path Geometric picture Optimization on a flat manifold Optimization in a convex region, repelled from boundary ML relevance Normalization, linear invariants, balance constraints Nonnegativity, margin constraints, safety/fairness limits <p>In summary:</p> <ul> <li>Equality-constrained methods operate directly on an affine manifold using KKT and Newton.  </li> <li>Inequality-constrained methods use smooth barriers (or primal\u2013dual perturbed KKT systems) to stay in the interior and gradually approach the boundary and the optimal point.</li> </ul> <p>Interior-point methods unify these perspectives and are the backbone of modern convex optimization software.</p>"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often runs at massive scale: millions (or billions) of variables, datasets too large to fit in memory, and constraints spread across machines or devices. Classical Newton or interior-point methods are beautiful mathematically, but their per-iteration cost and memory usage often make them impractical for these regimes.</p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to make convex optimization scalable. These ideas underpin the optimization engines behind most modern machine learning systems.</p>"},{"location":"convex/20_advanced/#151-motivation-structure-and-scale","title":"15.1 Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the challenge is not \u201cdoes a solution exist?\u201d but rather \u201ccan we compute it in time and memory?\u201d.</p> <p>Bottlenecks include:</p> <ul> <li>Memory: storing Hessians (or even full gradients) may be impossible.</li> <li>Data size: one full pass over all samples can already be expensive.</li> <li>Distributed data: samples are spread across devices / workers.</li> <li>Sparsity and separability: the problem often decomposes into many small pieces.</li> </ul> <p>A common template is the empirical risk + regularizer form  where</p> <ul> <li>each \\(f_i(x)\\) is a loss term for sample \\(i\\),</li> <li>\\(R(x)\\) is a regularizer (possibly nonsmooth, e.g. \\(\\lambda\\|x\\|_1\\)).</li> </ul> <p>The methods in this chapter are designed to exploit this structure:</p> <ul> <li>update only parts of \\(x\\) at a time (coordinate/block methods),</li> <li>use only some data per step (stochastic methods),</li> <li>split the problem into simpler subproblems (proximal / ADMM),</li> <li>or distribute computation across multiple machines (consensus methods).</li> </ul>"},{"location":"convex/20_advanced/#152-coordinate-descent","title":"15.2 Coordinate Descent","text":"<p>Coordinate descent updates one coordinate (or a small block of coordinates) at a time, holding all others fixed. It is especially effective when updates along a single coordinate are cheap to compute. Given \\(x^{(k)}\\), choose coordinate \\(i_k\\) and define</p> \\[ x^{(k+1)}_i = \\begin{cases} \\displaystyle \\arg\\min_{z \\in \\mathbb{R}} \\; f\\big(x_1^{(k+1)},\\dots,x_{i_k-1}^{(k+1)},z,x_{i_k+1}^{(k)},\\dots,x_n^{(k)}\\big), &amp; i = i_k,\\\\[4pt] x_i^{(k)}, &amp; i \\ne i_k. \\end{cases} \\] <p>In practice:</p> <ul> <li>\\(i_k\\) is chosen either cyclically (\\(1,2,\\dots,n,1,2,\\dots\\)) or randomly.</li> <li>Each coordinate update often has a closed form (e.g. soft-thresholding for LASSO).</li> <li>You never form or store the full gradient; you only need partial derivatives.</li> </ul> <p>Why it scales:</p> <ul> <li>Each step is very cheap \u2014 often \\(O(1)\\) or proportional to the number of nonzeros in the column corresponding to coordinate \\(i_k\\).</li> <li>In high dimensions (e.g., millions of features), this can be far more efficient than updating all coordinates at once.</li> </ul> <p>Convergence: If \\(f\\) is convex and has Lipschitz-continuous partial derivatives, coordinate descent (cyclic or randomized) converges to the global minimizer. Randomized coordinate descent often has clean expected convergence rates.</p> <p>ML context:</p> <ul> <li>LASSO / Elastic Net regression (coordinate updates are soft-thresholding),</li> <li>\\(\\ell_1\\)-penalized logistic regression,</li> <li>matrix factorization and dictionary learning (updating one factor vector at a time),</li> <li>problems where \\(R(x)\\) is separable: \\(R(x) = \\sum_i R_i(x_i)\\).</li> </ul>"},{"location":"convex/20_advanced/#153-stochastic-gradient-and-variance-reduced-methods","title":"15.3 Stochastic Gradient and Variance-Reduced Methods","text":"<p>When \\(N\\) (number of samples) is huge, computing the full gradient  every iteration is too expensive. Stochastic methods replace this full gradient with cheap, unbiased estimates based on small random subsets (mini-batches) of data.</p>"},{"location":"convex/20_advanced/#1531-stochastic-gradient-descent-sgd","title":"15.3.1 Stochastic Gradient Descent (SGD)","text":"<p>At iteration \\(k\\):</p> <ol> <li>Sample a mini-batch \\(\\mathcal{B}_k \\subset \\{1,\\dots,N\\}\\).</li> <li>Form the stochastic gradient     </li> <li>Update        where \\(\\eta_k &gt; 0\\) is the learning rate.</li> </ol> <p>Properties:</p> <ul> <li>\\(\\mathbb{E}[\\widehat{\\nabla f}(x_k) \\mid x_k] = \\nabla f(x_k)\\) (unbiased),</li> <li>Each iteration is cheap (depends only on \\(|\\mathcal{B}_k|\\), not \\(N\\)),</li> <li>The noise can help escape shallow nonconvex traps (in deep learning).</li> </ul> <p>In convex settings, SGD trades off per-iteration cost against convergence speed: many cheap noisy steps instead of fewer expensive precise ones.</p>"},{"location":"convex/20_advanced/#1532-step-sizes-and-averaging","title":"15.3.2 Step Sizes and Averaging","text":"<p>The step size \\(\\eta_k\\) is crucial:</p> <ul> <li>Too large \u2192 iterates diverge or oscillate.</li> <li>Too small \u2192 extremely slow progress.</li> </ul> <p>Typical schedules for convex problems:</p> <ul> <li>General convex: \\(\\eta_k = \\frac{c}{\\sqrt{k}}\\),</li> <li>Strongly convex: \\(\\eta_k = \\frac{c}{k}\\).</li> </ul> <p>Two important stabilization techniques:</p> <ol> <li>Decay the learning rate over time.</li> <li>Polyak\u2013Ruppert averaging: return the average        instead of the last iterate. Averaging cancels noise and leads to optimal \\(O(1/k)\\) rates in strongly convex settings.</li> </ol> <p>Mini-batch size can also grow with \\(k\\), gradually reducing variance while keeping early iterations cheap.</p>"},{"location":"convex/20_advanced/#1533-convergence-rates","title":"15.3.3 Convergence Rates","text":"<p>For convex \\(f\\):</p> <ul> <li>With appropriate diminishing \\(\\eta_k\\), \\(\\mathbb{E}[f(x_k)] - f^\\star = O(k^{-1/2})\\).</li> </ul> <p>For strongly convex \\(f\\):</p> <ul> <li>With \\(\\eta_k = O(1/k)\\) and averaging, \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</li> </ul> <p>These rates are optimal for unbiased first-order stochastic methods.</p>"},{"location":"convex/20_advanced/#1534-variance-reduced-methods-svrg-saga-sarah","title":"15.3.4 Variance-Reduced Methods (SVRG, SAGA, SARAH)","text":"<p>Plain SGD cannot easily reach very high accuracy because the gradient noise never fully disappears. Variance-reduced methods reduce this noise, especially near the solution, by periodically using the full gradient.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <ul> <li>Pick a reference point \\(\\tilde{x}\\) and compute \\(\\nabla f(\\tilde{x})\\).</li> <li>For inner iterations:      where \\(i_k\\) is a random sample index.</li> </ul> <p>Here \\(v_k\\) is still an unbiased estimator of \\(\\nabla f(x_k)\\), but its variance decays as \\(x_k\\) approaches \\(\\tilde{x}\\). For strongly convex \\(f\\), methods like SVRG and SAGA achieve linear convergence, comparable to full gradient descent but at near-SGD cost.</p>"},{"location":"convex/20_advanced/#1535-momentum-and-adaptive-methods","title":"15.3.5 Momentum and Adaptive Methods","text":"<p>In practice, large-scale learning often uses SGD with various modifications:</p> <ul> <li> <p>Momentum / Nesterov: keep a moving average of gradients      which accelerates progress along consistent directions and damps oscillations.</p> </li> <li> <p>Adaptive methods (Adagrad, RMSProp, Adam): maintain coordinate-wise scales based on past squared gradients, effectively using a diagonal preconditioner that adapts to curvature and feature scales.</p> </li> </ul> <p>These methods are especially popular in deep learning. For convex problems, their theoretical behavior is subtle, but empirically they often converge faster in wall-clock time.</p>"},{"location":"convex/20_advanced/#154-proximal-and-composite-optimization","title":"15.4 Proximal and Composite Optimization","text":"<p>Many large-scale convex problems are composite:  where</p> <ul> <li>\\(g\\) is smooth convex with Lipschitz gradient (e.g. data-fitting term),</li> <li>\\(R\\) is convex but possibly nonsmooth (e.g. \\(\\lambda\\|x\\|_1\\), indicator of a constraint set, nuclear norm).</li> </ul> <p>The proximal gradient method (a.k.a. ISTA) updates as  where the proximal operator is  </p> <p>Intuition:</p> <ul> <li>The gradient step moves \\(x\\) in a direction that lowers the smooth term \\(g\\).</li> <li>The prox step solves a small \u201cregularized\u201d problem, pulling \\(x\\) toward a structure favored by \\(R\\) (sparsity, low rank, feasibility, etc.).</li> </ul> <p>Examples of prox operators:</p> <ul> <li>\\(R(x) = \\lambda\\|x\\|_1\\) \u2192 soft-thresholding (coordinate-wise shrinkage).</li> <li>\\(R\\) = indicator of a convex set \\(\\mathcal{X}\\) \u2192 projection onto \\(\\mathcal{X}\\) (so projected gradient is a special case).</li> <li>\\(R(X) = \\|X\\|_*\\) (nuclear norm) \u2192 singular value soft-thresholding.</li> </ul> <p>For large-scale problems:</p> <ul> <li>Proximal gradient scales like gradient descent: each iteration uses only \\(\\nabla g\\) and a prox (often cheap and parallelizable).</li> <li>Accelerated variants (FISTA) achieve \\(O(1/k^2)\\) rates for smooth \\(g\\).</li> </ul>"},{"location":"convex/20_advanced/#155-alternating-direction-method-of-multipliers-admm","title":"15.5 Alternating Direction Method of Multipliers (ADMM)","text":"<p>When objectives naturally split into simpler pieces depending on different variables, ADMM is a powerful tool. It is especially useful when:</p> <ul> <li>\\(f\\) and \\(g\\) have simple prox operators,</li> <li>the problem is distributed or separable across machines.</li> </ul> <p>Consider  </p> <p>The augmented Lagrangian is  with dual variable \\(y\\) and penalty parameter \\(\\rho &gt; 0\\).</p> <p>ADMM performs the iterations:  </p> <p>Interpretation:</p> <ul> <li>The \\(x\\)-update solves a subproblem involving \\(f\\) only.</li> <li>The \\(z\\)-update solves a subproblem involving \\(g\\) only.</li> <li>The \\(y\\)-update nudges the constraint \\(A x + B z = c\\) toward satisfaction.</li> </ul> <p>For convex \\(f,g\\), ADMM converges to a primal\u2013dual optimal point. It is particularly effective when the \\(x\\)- and \\(z\\)-subproblems have closed-form prox solutions or can be solved cheaply in parallel.</p> <p>ML use cases:</p> <ul> <li>Distributed LASSO / logistic regression,</li> <li>matrix completion and robust PCA,</li> <li>consensus optimization (each worker has local data but shares a global model),</li> <li>some federated learning formulations.</li> </ul>"},{"location":"convex/20_advanced/#156-majorizationminimization-mm-and-em-algorithms","title":"15.6 Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The Majorization\u2013Minimization (MM) principle constructs at each iterate \\(x_k\\) a surrogate function \\(g(\\cdot \\mid x_k)\\) that upper-bounds \\(f\\) and is easier to minimize.</p> <p>Requirements:  </p> <p>Then define  </p> <p>This guarantees monotone decrease:  </p> <p>The famous Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models, where the surrogate arises from Jensen\u2019s inequality and missing-data structure.</p> <p>Other examples:</p> <ul> <li>Iteratively Reweighted Least Squares (IRLS) for logistic regression and robust regression,</li> <li>MM surrogates for nonconvex penalties (e.g. smoothly approximating \\(\\ell_0\\)),</li> <li>mixture models and variational inference.</li> </ul>"},{"location":"convex/20_advanced/#157-distributed-and-parallel-optimization","title":"15.7 Distributed and Parallel Optimization","text":"<p>When data or variables are split across machines, we need distributed or parallel optimization schemes.</p>"},{"location":"convex/20_advanced/#1571-synchronous-vs-asynchronous","title":"15.7.1 Synchronous vs Asynchronous","text":"<ul> <li>Synchronous methods: all workers compute local gradients or updates, then synchronize (e.g. parameter server, federated averaging).</li> <li>Asynchronous methods: workers update parameters without global synchronization, improving hardware utilization but introducing staleness and variance.</li> </ul>"},{"location":"convex/20_advanced/#1572-consensus-optimization","title":"15.7.2 Consensus Optimization","text":"<p>A standard pattern is consensus form:  where \\(f_i\\) is the local objective on worker \\(i\\) and \\(z\\) is the global consensus variable.</p> <p>ADMM applied to this problem:</p> <ul> <li>Each worker updates its local \\(x_i\\) using only local data,</li> <li>The global variable \\(z\\) is updated by averaging or aggregation,</li> <li>Dual variables enforce agreement \\(x_i \\approx z\\).</li> </ul> <p>This template underlies many federated learning and parameter-server architectures.</p>"},{"location":"convex/20_advanced/#1573-ml-context","title":"15.7.3 ML Context","text":"<ul> <li>Federated learning (phone/edge devices update local models and send summaries to a server),</li> <li>Large-scale convex optimization over sharded datasets,</li> <li>Distributed sparse regression, matrix factorization, and graphical model learning.</li> </ul>"},{"location":"convex/20_advanced/#158-handling-structure-sparsity-and-low-rank","title":"15.8 Handling Structure: Sparsity and Low Rank","text":"<p>Large-scale convex problems often have additional structure that we can exploit algorithmically:</p> Structure Typical Regularizer / Constraint Algorithmic Benefit Sparsity \\(\\ell_1\\), group lasso Cheap coordinate updates, soft-thresholding Low rank Nuclear norm \\(\\|X\\|_*\\) SVD-based prox; rank truncation Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed block updates Graph structure Total variation on graphs Local neighborhood computations Probability simplex simplex constraint or entropy term Mirror descent, simplex projections <p>Examples:</p> <ul> <li>In compressed sensing, \\(\\ell_1\\) regularization + sparse sensing matrices \u2192 very cheap mat\u2013vecs + prox operations.</li> <li>In matrix completion, nuclear norm structure + low-rank iterates \u2192 approximate SVD instead of full SVD.</li> <li>In TV denoising, local difference structure \u2192 each prox step involves only neighboring pixels/vertices.</li> </ul> <p>Exploiting structure can yield orders-of-magnitude speedups compared to generic solvers.</p>"},{"location":"convex/20_advanced/#159-summary-and-practical-guidance","title":"15.9 Summary and Practical Guidance","text":"<p>Different large-scale methods are appropriate in different regimes:</p> Method Gradient Access Scalability Parallelization Convexity Needed Typical Uses Coordinate Descent Partial gradients High Easy (blockwise) Convex LASSO, sparse GLMs, matrix factorization SGD / Mini-batch SGD Stochastic gradients Excellent Natural (data parallel) Convex / nonconvex Deep learning, logistic regression SVRG / SAGA (VR methods) Stochastic + periodic full gradient High Data parallel Convex, often strongly convex Large-scale convex ML, GLMs Proximal Gradient (ISTA/FISTA) Full gradient + prox Moderate\u2013High Easy Convex Composite objectives with structure ADMM Local subproblems High Designed for distributed Convex Consensus, distributed convex solvers MM / EM Surrogates Moderate Model-specific Convex / nonconvex Latent-variable models, IRLS Distributed / Federated Local gradients Very high Essential Often convex / smooth Federated learning, multi-agent systems"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#161-regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"16.1 Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithms Summary:  </p> <ul> <li>Smooth + \u2113\u2082 (strongly convex):   Newton / quasi-Newton, conjugate gradient, or accelerated gradient.</li> <li>Smooth + \u2113\u2081 (and variants):   proximal gradient or coordinate descent; for huge data, stochastic/proximal variants.</li> <li>Mixed penalties (\u2113\u2081 + \u2113\u2082):   treat as composite smooth + nonsmooth; prox and coordinate methods still apply.</li> <li>Large \\(N\\) or \\(n\\):   mini-batch / stochastic gradients (SGD, SVRG, etc.) on the smooth part + prox for the regulariser.</li> </ul> <p>Regularisation strength \\(\\lambda\\) is usually chosen via cross-validation or a validation set, exploring the accuracy\u2013simplicity trade-off.</p>"},{"location":"convex/21_models/#162-robust-regression-and-outlier-resistance","title":"16.2 Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#1621-least-absolute-deviations-l1-loss","title":"16.2.1 Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#1622-huber-loss","title":"16.2.2 Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints. That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly. This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#1623-worst-case-robust-regression","title":"16.2.3 Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#163-maximum-likelihood-and-loss-design","title":"16.3 Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#164-structured-constraints-in-engineering-and-design","title":"16.4 Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#165-linear-and-conic-programming-the-canonical-models","title":"16.5 Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#166-risk-safety-margins-and-robust-design","title":"16.6 Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#167-cheat-sheet-if-your-problem-looks-like-this-use-that","title":"16.7 Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#167-matching-model-structure-to-algorithm-type","title":"16.7 Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#171-hierarchy-of-canonical-problems","title":"17.1 Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#172-linear-programming-lp","title":"17.2 Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#173-quadratic-programming-qp","title":"17.3 Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#174-quadratically-constrained-qp-qcqp","title":"17.4 Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#175-second-order-cone-programming-socp","title":"17.5 Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#176-semidefinite-programming-sdp","title":"17.6 Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#177-geometric-programming-gp","title":"17.7 Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#178-likelihood-based-convex-models-mle-and-glms","title":"17.8 Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#179-solver-selection-summary","title":"17.9 Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"convex/35_modern/","title":"18. Modern Optimizers in Machine Learning Frameworks","text":""},{"location":"convex/35_modern/#chapter-18-modern-optimizers-in-machine-learning","title":"Chapter 18: Modern Optimizers in Machine Learning","text":"<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization\u2014balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p> <p>This chapter connects the principles of convex optimization to the modern optimizers that power today\u2019s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>"},{"location":"convex/35_modern/#181-stochastic-optimization-overview","title":"18.1 Stochastic Optimization Overview","text":"<p>In machine learning, we often minimize an empirical risk:  where \\(\\ell(x; z_i)\\) is the loss on data sample \\(z_i\\).</p> <p>Computing the full gradient \\(\\nabla f(x)\\) is infeasible when \\(N\\) is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p> \\[ g_k = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla \\ell(x_k; z_i). $$ This yields the Stochastic Gradient Descent (SGD) update: $$ x_{k+1} = x_k - \\alpha_k g_k. \\] <p>SGD is the foundation for nearly all deep learning optimizers.</p>"},{"location":"convex/35_modern/#182-momentum-and-acceleration","title":"18.2 Momentum and Acceleration","text":"<p>SGD\u2019s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p> <p>  where \\(\\beta \\in [0,1)\\) controls inertia.</p> <p>Nesterov momentum adds a correction term anticipating the future position:</p> \\[ v_{k+1} = \\beta v_k + g(x_k - \\alpha \\beta v_k), \\quad x_{k+1} = x_k - \\alpha v_{k+1}. \\] <p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>"},{"location":"convex/35_modern/#183-adaptive-learning-rate-methods","title":"18.3 Adaptive Learning Rate Methods","text":"<p>Different parameters often require different step sizes. Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>"},{"location":"convex/35_modern/#1831-adagrad","title":"18.3.1 AdaGrad","text":"<p>Keeps a cumulative sum of squared gradients:</p> <p>  and updates parameters as:</p> <p>  Good for sparse data, but the learning rate can shrink too quickly.</p>"},{"location":"convex/35_modern/#1832-rmsprop","title":"18.3.2 RMSProp","text":"<p>A refinement of AdaGrad using exponential averaging:</p> \\[ E[g^2]_k = \\beta E[g^2]_{k-1} + (1-\\beta) g_k^2, \\] \\[ x_{k+1} = x_k - \\frac{\\alpha}{\\sqrt{E[g^2]_k + \\epsilon}} g_k. \\] <p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>"},{"location":"convex/35_modern/#1833-adam-adaptive-moment-estimation","title":"18.3.3 Adam: Adaptive Moment Estimation","text":"<p>Adam combines momentum and adaptive scaling:</p> \\[ m_k = \\beta_1 m_{k-1} + (1-\\beta_1) g_k, \\quad v_k = \\beta_2 v_{k-1} + (1-\\beta_2) g_k^2, \\] \\[ \\hat{m}_k = \\frac{m_k}{1-\\beta_1^k}, \\quad \\hat{v}_k = \\frac{v_k}{1-\\beta_2^k}, \\] \\[ x_{k+1} = x_k - \\alpha \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\epsilon}. \\] <p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>"},{"location":"convex/35_modern/#184-variants-and-modern-extensions","title":"18.4 Variants and Modern Extensions","text":"Optimizer Key Idea Notes AdamW Decoupled weight decay from gradient update Better regularization RAdam Rectified Adam\u2014adaptive variance correction Improves stability early in training Lookahead Combines fast and slow weights Enhances robustness and convergence AdaBelief Uses prediction error instead of raw gradient variance More adaptive learning rates Lion Uses sign-based updates and momentum Efficient for large-scale training <p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>"},{"location":"convex/35_modern/#185-implicit-regularization-and-generalization","title":"18.5 Implicit Regularization and Generalization","text":"<p>Modern optimizers not only minimize loss\u2014they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p> <p>Empirical findings suggest:</p> <ul> <li>Large-batch training finds sharper minima (risk of overfitting).  </li> <li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li> <li>Adaptive optimizers may converge faster but generalize slightly worse.</li> </ul> <p>This trade-off drives ongoing research into optimizer design.</p>"},{"location":"convex/35_modern/#186-practical-considerations","title":"18.6 Practical Considerations","text":"Aspect Guideline Learning Rate Most critical hyperparameter; use warm-up and decay schedules Batch Size Balances gradient noise and hardware efficiency Initialization Affects early dynamics, especially for Adam variants Gradient Clipping Prevents instability in exploding gradients Mixed Precision Use with adaptive optimizers for speed and memory savings"},{"location":"convex/35_modern/#187-comparative-behavior","title":"18.7 Comparative Behavior","text":"Method Adaptivity Speed Memory Typical Use SGD + Momentum Moderate Slow-medium Low General-purpose, good generalization RMSProp Adaptive per-parameter Medium-fast Medium Recurrent networks, nonstationary data Adam / AdamW Fully adaptive Fast High Deep networks, large-scale training RAdam / AdaBelief / Lion Advanced adaptivity Fast Medium Cutting-edge training tasks"},{"location":"convex/35_modern/#188-optimization-in-modern-deep-networks","title":"18.8 Optimization in Modern Deep Networks","text":"<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p> <ul> <li>Batch normalization modifies effective learning rates.  </li> <li>Skip connections ease gradient flow.  </li> <li>Large-scale distributed training relies on adaptive optimizers for stability.  </li> </ul> <p>Optimization is no longer an isolated procedure but part of the model\u2019s design philosophy.</p> <p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning\u2014adapting dynamically, balancing efficiency and generalization.</p>"},{"location":"convex/40_nonconvex/","title":"19. Beyond Convexity \u2013 Nonconvex and Global Optimization","text":""},{"location":"convex/40_nonconvex/#chapter-19-beyond-convexity-nonconvex-and-global-optimization","title":"Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization","text":"<p>Optimization extends far beyond the comfortable world of convexity. In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p> <p>Convex optimization gives us strong guarantees \u2014 every local minimum is global, and algorithms converge predictably. But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>"},{"location":"convex/40_nonconvex/#191-the-landscape-of-nonconvex-optimization","title":"19.1 The Landscape of Nonconvex Optimization","text":"<p>A nonconvex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) violates convexity; i.e., for some \\(x, y\\) and \\(\\theta \\in (0,1)\\),  Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p> <p>A typical nonconvex landscape looks like a mountainous terrain \u2014 smooth in some regions, rugged in others. An optimization algorithm\u2019s path depends strongly on initialization and stochastic effects.</p>"},{"location":"convex/40_nonconvex/#example-a-simple-nonconvex-function","title":"Example: A Simple Nonconvex Function","text":"<p>  This function has multiple stationary points: - \\((0,0)\\) (a saddle), - \\((1,1)\\) and \\((-1,-1)\\) (local minima), - \\((1,-1)\\) and \\((-1,1)\\) (local maxima).</p> <p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>"},{"location":"convex/40_nonconvex/#192-local-vs-global-minima","title":"19.2 Local vs. Global Minima","text":"<p>A point \\(x^*\\) is a local minimum if:  </p> <p>A global minimum satisfies the stronger condition:  </p> <p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad \u2014 and there may be exponentially many of them.</p>"},{"location":"convex/40_nonconvex/#193-classes-of-nonconvex-problems","title":"19.3 Classes of Nonconvex Problems","text":"<p>Nonconvex problems appear in several distinct forms:</p> Type Example Challenge Smooth nonconvex Neural network training Multiple minima, saddle points Nonsmooth nonconvex Sparse regularization, ReLU activations Undefined gradients Discrete / combinatorial Scheduling, routing, integer programs Exponential search space Black-box Simulation-based optimization No derivatives or analytical form <p>Each category requires different algorithmic strategies \u2014 from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>"},{"location":"convex/40_nonconvex/#194-local-optimization-strategies","title":"19.4 Local Optimization Strategies","text":"<p>Even in nonconvex settings, local optimization remains useful when: - The problem is nearly convex (e.g., locally convex around good minima), - The initialization is close to a desired basin of attraction, - Or the goal is approximate, not exact, optimality.</p>"},{"location":"convex/40_nonconvex/#gradient-descent-and-its-variants","title":"Gradient Descent and Its Variants","text":"<p>Gradient descent behaves well if \\(f\\) is smooth and Lipschitz-continuous:  However, convergence is only to a stationary point \u2014 not necessarily a minimum.</p> <p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>"},{"location":"convex/40_nonconvex/#195-global-optimization-strategies","title":"19.5 Global Optimization Strategies","text":"<p>To seek the global minimum, algorithms must explore the search space more broadly. Common strategies include:</p> <ol> <li> <p>Multiple Starts:    Run local optimization from diverse random initial points and keep the best solution.</p> </li> <li> <p>Continuation and Homotopy Methods:    Start from a smooth, convex approximation \\(f_\\lambda\\) of \\(f\\) and gradually transform it into the true objective as \\(\\lambda \\to 0\\).</p> </li> <li> <p>Stochastic Search and Simulated Annealing:    Introduce randomness in updates to jump between basins.</p> </li> <li> <p>Population-Based Methods:    Maintain a swarm or population of candidate solutions evolving by selection and variation \u2014 leading to metaheuristic algorithms like GA and PSO.</p> </li> </ol>"},{"location":"convex/40_nonconvex/#196-theoretical-challenges","title":"19.6 Theoretical Challenges","text":"<p>Without convexity, most strong results vanish:</p> <ul> <li>Global optimality cannot be guaranteed.</li> <li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li> <li>Complexity often grows exponentially with problem size.</li> </ul> <p>However, theory is not hopeless:</p> <ul> <li>Many nonconvex problems are \u201cbenign\u201d \u2014 e.g., matrix factorization, phase retrieval, or deep linear networks \u2014 having no bad local minima.  </li> <li>Random initialization and overparameterization often aid convergence to global minima in practice.</li> </ul>"},{"location":"convex/40_nonconvex/#197-geometry-of-saddle-points","title":"19.7 Geometry of Saddle Points","text":"<p>A saddle point satisfies \\(\\nabla f(x)=0\\) but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p> <p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>"},{"location":"convex/40_nonconvex/#198-deterministic-vs-stochastic-global-methods","title":"19.8 Deterministic vs. Stochastic Global Methods","text":"Deterministic Methods Stochastic Methods Systematic exploration of space (branch &amp; bound, interval analysis) Randomized search (simulated annealing, evolutionary algorithms) Can provide certificates of global optimality Typically approximate but scalable High computational cost Naturally parallelizable <p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>"},{"location":"convex/40_nonconvex/#199-a-taxonomy-of-optimization-beyond-convexity","title":"19.9 A Taxonomy of Optimization Beyond Convexity","text":"Family Typical Algorithms When to Use Derivative-Free (Black-Box) Nelder\u2013Mead, CMA-ES, Bayesian Opt. When gradients unavailable Metaheuristic (Evolutionary) GA, PSO, DE, ACO Complex landscapes, combinatorial problems Modern Stochastic Gradient Adam, RMSProp, Lion Deep learning, large-scale models Combinatorial / Discrete Branch &amp; Bound, Tabu, SA Integer or graph-based problems Learning-Based Optimizers Meta-learning, Reinforcement methods Adaptive, data-driven optimization"},{"location":"convex/42_derivativefree/","title":"20. Derivative-Free and Black-Box Optimization","text":""},{"location":"convex/42_derivativefree/#chapter-20-derivative-free-and-black-box-optimization","title":"Chapter 20: Derivative-Free and Black-Box Optimization","text":"<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p> <p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>"},{"location":"convex/42_derivativefree/#201-motivation-and-challenges","title":"20.1 Motivation and Challenges","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be an objective function.  </p> <p>A derivative-free algorithm seeks to minimize \\(f(x)\\) using only evaluations of \\(f(x)\\), without access to \\(\\nabla f(x)\\) or \\(\\nabla^2 f(x)\\).</p> <p>Key challenges:</p> <ul> <li>No gradient information \u2192 difficult to infer descent directions.  </li> <li>Expensive evaluations \u2192 every call to \\(f(x)\\) might require a simulation or experiment.  </li> <li>Noise and stochasticity \u2192 evaluations may be corrupted by measurement or sampling error.  </li> <li>High-dimensionality \u2192 sampling-based methods scale poorly with \\(n\\).</li> </ul> <p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>"},{"location":"convex/42_derivativefree/#202-classification-of-derivative-free-methods","title":"20.2 Classification of Derivative-Free Methods","text":"Category Representative Algorithms Main Idea Direct Search Nelder\u2013Mead, Pattern Search, MADS Explore the space via geometric moves or meshes Model-Based BOBYQA, Trust-Region DFO Build local quadratic or surrogate models of \\(f\\) Evolutionary / Population-Based CMA-ES, Differential Evolution Evolve a population using stochastic operators Probabilistic / Bayesian Bayesian Optimization Use probabilistic surrogate models to guide exploration"},{"location":"convex/42_derivativefree/#203-direct-search-methods","title":"20.3 Direct Search Methods","text":"<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>"},{"location":"convex/42_derivativefree/#2031-neldermead-simplex-method","title":"20.3.1 Nelder\u2013Mead Simplex Method","text":"<p>Perhaps the most famous derivative-free algorithm, Nelder\u2013Mead maintains a simplex \u2014 a polytope of \\(n+1\\) vertices in \\(\\mathbb{R}^n\\).</p> <p>At each iteration:</p> <ol> <li>Evaluate \\(f\\) at all simplex vertices.</li> <li>Reflect, expand, contract, or shrink the simplex depending on performance.</li> <li>Continue until simplex collapses near a minimum.</li> </ol> <p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>"},{"location":"convex/42_derivativefree/#2032-pattern-search-methods","title":"20.3.2 Pattern Search Methods","text":"<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p> <p>Typical update rule:  </p> <p>where \\(d_i\\) is a direction from a finite set (e.g., coordinate axes). If a direction yields improvement, move there; otherwise, shrink \\(\\Delta_k\\).</p>"},{"location":"convex/42_derivativefree/#2033-mesh-adaptive-direct-search-mads","title":"20.3.3 Mesh Adaptive Direct Search (MADS)","text":"<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>"},{"location":"convex/42_derivativefree/#204-model-based-methods","title":"20.4 Model-Based Methods","text":"<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>"},{"location":"convex/42_derivativefree/#2041-trust-region-dfo","title":"20.4.1 Trust-Region DFO","text":"<p>A local model \\(m_k(x)\\) (often quadratic) is built to approximate \\(f\\) near the current iterate \\(x_k\\):  The next iterate solves a trust-region subproblem:  The trust region size \\(\\Delta_k\\) adapts based on how well \\(m_k\\) predicts true function values.</p>"},{"location":"convex/42_derivativefree/#2042-bobyqa-bound-optimization-by-quadratic-approximation","title":"20.4.2 BOBYQA (Bound Optimization BY Quadratic Approximation)","text":"<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>"},{"location":"convex/42_derivativefree/#205-evolution-strategies-and-population-methods","title":"20.5 Evolution Strategies and Population Methods","text":"<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>"},{"location":"convex/42_derivativefree/#2051-covariance-matrix-adaptation-evolution-strategy-cma-es","title":"20.5.1 Covariance Matrix Adaptation Evolution Strategy (CMA-ES)","text":"<p>CMA-ES is a powerful stochastic search algorithm. It iteratively samples new points from a multivariate Gaussian distribution:  where \\(m_k\\) is the current mean, \\(\\sigma_k\\) the global step-size, and \\(C_k\\) the covariance matrix.</p> <p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p> <p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>"},{"location":"convex/42_derivativefree/#2052-differential-evolution-de","title":"20.5.2 Differential Evolution (DE)","text":"<p>DE evolves a population \\(\\{x_i\\}\\) via vector differences:   where \\(r1, r2, r3\\) are random distinct indices and \\(F\\) controls mutation strength.</p> <p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>"},{"location":"convex/42_derivativefree/#206-bayesian-optimization","title":"20.6 Bayesian Optimization","text":"<p>When function evaluations are expensive (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>"},{"location":"convex/42_derivativefree/#2061-core-idea","title":"20.6.1 Core Idea","text":"<p>Model the objective as a random function \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))\\) (Gaussian Process prior). After each evaluation, update the posterior mean and variance to quantify uncertainty.</p> <p>Use an acquisition function \\(a(x)\\) to select the next evaluation point:  balancing exploration (high uncertainty) and exploitation (low expected value).</p> <p>Common acquisition functions:</p> <ul> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Upper Confidence Bound (UCB)</li> </ul>"},{"location":"convex/42_derivativefree/#2062-surrogate-models-beyond-gaussian-processes","title":"20.6.2 Surrogate Models Beyond Gaussian Processes","text":"<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs: - Tree-structured Parzen Estimators (TPE) - Random forests (SMAC) - Neural network surrogates (Bayesian neural networks)</p> <p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>"},{"location":"convex/42_derivativefree/#207-hybrid-and-adaptive-approaches","title":"20.7 Hybrid and Adaptive Approaches","text":"<p>Modern applications often combine derivative-free and gradient-based techniques:</p> <ul> <li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li> <li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li> <li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li> </ul> <p>Such hybridization reflects a pragmatic view: no single optimizer is best \u2014 adaptability matters most.</p>"},{"location":"convex/42_derivativefree/#208-practical-considerations","title":"20.8 Practical Considerations","text":"Aspect Guideline Function evaluations expensive Use Bayesian or model-based methods Noisy evaluations Use averaging, smoothing, or robust estimators High dimension (\\(n &gt; 50\\)) Prefer CMA-ES or evolutionary strategies Box constraints Methods like BOBYQA, DE, or PSO Parallel computation available Population-based methods excel <p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize anything we can evaluate. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p> <p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors \u2014 turning randomness into a powerful search strategy.</p>"},{"location":"convex/44_metaheuristic/","title":"21. Metaheuristic and Evolutionary Optimization","text":""},{"location":"convex/44_metaheuristic/#chapter-21-metaheuristic-and-evolutionary-algorithms","title":"Chapter 21: Metaheuristic and Evolutionary Algorithms","text":"<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions. In these settings, metaheuristic algorithms\u2014inspired by nature, biology, and collective behavior\u2014provide robust and flexible alternatives.</p> <p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas. They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>"},{"location":"convex/44_metaheuristic/#211-principles-of-metaheuristic-optimization","title":"21.1 Principles of Metaheuristic Optimization","text":"<p>All metaheuristics share three key principles:</p> <ol> <li> <p>Population-Based Search:    Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p> </li> <li> <p>Variation Operators:    Create new solutions via mutation, recombination, or stochastic perturbations.</p> </li> <li> <p>Selection and Adaptation:    Favor candidates with better objective values, guiding the search toward promising regions.</p> </li> </ol> <p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>"},{"location":"convex/44_metaheuristic/#212-genetic-algorithms-ga","title":"21.2 Genetic Algorithms (GA)","text":""},{"location":"convex/44_metaheuristic/#2121-biological-inspiration","title":"21.2.1 Biological Inspiration","text":"<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>"},{"location":"convex/44_metaheuristic/#2122-representation","title":"21.2.2 Representation","text":"<p>A solution (individual) is represented as a chromosome\u2014often a binary string, vector of reals, or permutation. Each position (gene) encodes part of the decision variable.</p>"},{"location":"convex/44_metaheuristic/#2123-algorithm-outline","title":"21.2.3 Algorithm Outline","text":"<ol> <li>Initialize a population \\(\\{x_i\\}_{i=1}^N\\) randomly.  </li> <li>Evaluate fitness \\(f(x_i)\\) for all individuals.  </li> <li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li> <li> <p>Apply:</p> <ul> <li>Crossover: combine genetic material of two parents.  </li> <li>Mutation: randomly alter some genes to maintain diversity.  </li> </ul> </li> <li> <p>Form a new population and repeat until convergence.</p> </li> </ol>"},{"location":"convex/44_metaheuristic/#2124-crossover-and-mutation-examples","title":"21.2.4 Crossover and Mutation Examples","text":"<ul> <li>Single-point crossover: exchange genes after a random index.  </li> <li>Gaussian mutation: add small noise to continuous parameters.  </li> </ul>"},{"location":"convex/44_metaheuristic/#2125-strengths-and-weaknesses","title":"21.2.5 Strengths and Weaknesses","text":"Strengths Weaknesses Highly parallel, robust, domain-independent Requires many function evaluations Effective for combinatorial and discrete optimization Parameter tuning (mutation, crossover rates) is nontrivial"},{"location":"convex/44_metaheuristic/#213-differential-evolution-de","title":"21.3 Differential Evolution (DE)","text":"<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>"},{"location":"convex/44_metaheuristic/#2131-core-idea","title":"21.3.1 Core Idea","text":"<p>Mutation is performed using differences of population members:  where \\(r1, r2, r3\\) are random distinct indices and \\(F \\in [0,2]\\) controls mutation amplitude.</p> <p>Then crossover forms trial vectors:  and selection chooses between \\(x_i\\) and \\(u_i\\) based on objective value.</p>"},{"location":"convex/44_metaheuristic/#2132-features","title":"21.3.2 Features","text":"<ul> <li>Self-adaptive exploration of the search space.</li> <li>Suitable for continuous, multimodal functions.</li> <li>Simple to implement, with few control parameters.</li> </ul>"},{"location":"convex/44_metaheuristic/#214-particle-swarm-optimization-pso","title":"21.4 Particle Swarm Optimization (PSO)","text":"<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p> <p>Each particle \\(i\\) has position \\(x_i\\) and velocity \\(v_i\\), updated as:   where:</p> <ul> <li>\\(p_i\\) = personal best position of particle \\(i\\),</li> <li>\\(g\\) = best global position found by the swarm,</li> <li>\\(w\\), \\(c_1\\), \\(c_2\\) are weight and learning coefficients,</li> <li>\\(r_1\\), \\(r_2\\) are random numbers in \\([0,1]\\).</li> </ul> <p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>"},{"location":"convex/44_metaheuristic/#2141-convergence-behavior","title":"21.4.1 Convergence Behavior","text":"<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>"},{"location":"convex/44_metaheuristic/#2142-strengths","title":"21.4.2 Strengths","text":"<ul> <li>Few parameters, easy to implement.</li> <li>Works well for noisy or discontinuous problems.</li> <li>Naturally parallelizable.</li> </ul>"},{"location":"convex/44_metaheuristic/#215-simulated-annealing-sa","title":"21.5 Simulated Annealing (SA)","text":"<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy \u2014 a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual \u201ccooling\u201d in the search for a global minimum.</p>"},{"location":"convex/44_metaheuristic/#2151-physical-analogy","title":"21.5.1 Physical Analogy","text":"<p>In thermodynamics, a system at temperature \\(T\\) has probability of occupying a state with energy \\(E\\) given by the Boltzmann distribution:</p> \\[ P(E) \\propto e^{-E / (kT)}. \\] <p>At high temperature, the system freely explores many states. As \\(T\\) decreases, it becomes increasingly likely to remain near states of minimal energy.</p> <p>Simulated Annealing maps this principle to optimization by treating:</p> <ul> <li>The objective function \\(f(x)\\) as the system\u2019s energy.</li> <li>The solution vector \\(x\\) as a configuration.</li> <li>The temperature \\(T\\) as a control parameter determining randomness.</li> </ul>"},{"location":"convex/44_metaheuristic/#2152-algorithm-outline","title":"21.5.2 Algorithm Outline","text":"<ol> <li> <p>Initialization</p> <ul> <li>Choose an initial solution \\(x_0\\) and initial temperature \\(T_0\\).</li> <li>Set a cooling schedule \\(T_{k+1} = \\alpha T_k\\), with \\(\\alpha \\in (0,1)\\).</li> </ul> </li> <li> <p>Iteration</p> <ul> <li>Generate a candidate \\(x'\\) from \\(x_k\\) via a small random perturbation.</li> <li>Compute \\(\\Delta f = f(x') - f(x_k)\\).</li> <li>Accept or reject based on the Metropolis criterion:</li> </ul> <p> </p> </li> <li> <p>Cooling</p> <ul> <li> <p>Reduce the temperature gradually according to the schedule.</p> </li> <li> <p>Repeat until \\(T\\) becomes sufficiently small or the system stabilizes.</p> </li> </ul> </li> </ol>"},{"location":"convex/44_metaheuristic/#2153-interpretation","title":"21.5.3 Interpretation","text":"<ul> <li> <p>At high temperatures, SA accepts both better and worse moves \u2192 exploration.  </p> </li> <li> <p>At low temperatures, it becomes increasingly selective \u2192 exploitation.</p> </li> </ul> <p>This balance allows SA to escape local minima and approach the global optimum over time.</p>"},{"location":"convex/44_metaheuristic/#2154-cooling-schedules","title":"21.5.4 Cooling Schedules","text":"<p>The temperature schedule determines convergence quality:</p> Type Formula Behavior Exponential \\(T_{k+1} = \\alpha T_k\\) Simple, widely used Linear \\(T_{k+1} = T_0 - \\beta k\\) Faster cooling, less exploration Logarithmic \\(T_k = \\frac{T_0}{\\log(k + c)}\\) Theoretically convergent (slow) Adaptive Adjust based on recent acceptance rates Practical and self-tuning <p>A slower cooling schedule improves accuracy but increases computational cost.</p>"},{"location":"convex/44_metaheuristic/#216-ant-colony-optimization-aco","title":"21.6 Ant Colony Optimization (ACO)","text":""},{"location":"convex/44_metaheuristic/#2161-biological-basis","title":"21.6.1 Biological Basis","text":"<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p> <p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity \\(\\tau_{ij}\\) and heuristic visibility \\(\\eta_{ij}\\):  </p>"},{"location":"convex/44_metaheuristic/#2162-pheromone-update","title":"21.6.2 Pheromone Update","text":"<p>After all ants construct their tours:  where \\(\\rho\\) controls evaporation and \\(\\Delta\\tau_{ij}\\) reinforces paths used by good solutions.</p> <p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>"},{"location":"convex/44_metaheuristic/#217-exploration-vs-exploitation","title":"21.7 Exploration vs. Exploitation","text":"<p>Every metaheuristic must balance: - Exploration: sampling diverse regions to escape local minima. - Exploitation: refining known good solutions to reach local optima.</p> High Exploration High Exploitation GA with strong mutation PSO with low inertia DE with high \\(F\\) ACO with low evaporation rate Random restarts Local refinement <p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>"},{"location":"convex/44_metaheuristic/#218-hybrid-and-memetic-algorithms","title":"21.8 Hybrid and Memetic Algorithms","text":"<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p> <p>Example:</p> <ol> <li>Use PSO or GA to explore broadly.  </li> <li>Apply gradient descent or Nelder\u2013Mead locally near promising candidates.</li> </ol> <p>This hybridization often yields faster convergence and improved accuracy.</p>"},{"location":"convex/44_metaheuristic/#219-performance-and-practical-tips","title":"21.9 Performance and Practical Tips","text":"Aspect Guideline Initialization Use wide, random distributions to promote diversity Parameter Tuning Use adaptive schedules (e.g., cooling, inertia decay) Population Size Larger for global search, smaller for fine-tuning Parallelism Evaluate populations concurrently for efficiency Stopping Criteria Use both iteration limits and stagnation detection <p>Metaheuristics are heuristic by design \u2014 they do not guarantee global optimality, but offer practical success across many fields.</p> <p>Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p> <p>In the next chapter, we turn to modern stochastic optimizers that bridge theoretical foundations and practical success in machine learning\u2014methods like Adam, RMSProp, and Lion that dominate large-scale nonconvex optimization.</p>"},{"location":"convex/48_advanced_combinatorial/","title":"22. Advanced Topics in Combinatorial Optimization","text":""},{"location":"convex/48_advanced_combinatorial/#chapter-22-advanced-topics-in-combinatorial-optimization","title":"Chapter 22: Advanced Topics in Combinatorial Optimization","text":"<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p> <p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>"},{"location":"convex/48_advanced_combinatorial/#221-nature-of-combinatorial-problems","title":"22.1 Nature of Combinatorial Problems","text":"<p>A combinatorial optimization problem can be expressed as:</p> \\[ \\min_{x \\in \\mathcal{F}} f(x), \\] <p>where \\(\\mathcal{F}\\) is a finite or countable set of feasible solutions, often exponentially large in size.</p> <p>Example forms include:</p> <ul> <li>Binary decisions: \\(x_i \\in \\{0,1\\}\\)</li> <li>Integer constraints: \\(x_i \\in \\mathbb{Z}\\)</li> <li>Permutations: ordering or ranking elements</li> </ul> <p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>"},{"location":"convex/48_advanced_combinatorial/#222-graph-theoretic-foundations","title":"22.2 Graph-Theoretic Foundations","text":"<p>Many combinatorial problems are naturally represented as graphs \\(G = (V, E)\\).</p>"},{"location":"convex/48_advanced_combinatorial/#2221-shortest-path-problem","title":"22.2.1 Shortest Path Problem","text":"<p>Given edge weights \\(w_{ij}\\), find a path from \\(s\\) to \\(t\\) minimizing total weight:  Efficiently solvable by Dijkstra\u2019s or Bellman\u2013Ford algorithms.</p>"},{"location":"convex/48_advanced_combinatorial/#2222-minimum-spanning-tree-mst","title":"22.2.2 Minimum Spanning Tree (MST)","text":"<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal\u2019s or Prim\u2019s algorithm in \\(O(E\\log V)\\) time.</p>"},{"location":"convex/48_advanced_combinatorial/#2223-maximum-flow-minimum-cut","title":"22.2.3 Maximum Flow / Minimum Cut","text":"<p>Determine how much \u201cflow\u201d can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>"},{"location":"convex/48_advanced_combinatorial/#223-integer-linear-programming-ilp","title":"22.3 Integer Linear Programming (ILP)","text":"<p>An integer program seeks:  </p> <p>It generalizes many classical problems:</p> <ul> <li>Knapsack  </li> <li>Assignment  </li> <li>Scheduling  </li> <li>Facility location</li> </ul> <p>Relaxing \\(x \\in \\mathbb{Z}^n\\) to \\(x \\in \\mathbb{R}^n\\) yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>"},{"location":"convex/48_advanced_combinatorial/#224-relaxation-and-rounding","title":"22.4 Relaxation and Rounding","text":"<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>"},{"location":"convex/48_advanced_combinatorial/#2241-lp-relaxation","title":"22.4.1 LP Relaxation","text":"<p>For binary variables \\(x_i \\in \\{0,1\\}\\), relax to \\(0 \\le x_i \\le 1\\) and solve via simplex or interior-point methods.</p>"},{"location":"convex/48_advanced_combinatorial/#2242-semidefinite-relaxation","title":"22.4.2 Semidefinite Relaxation","text":"<p>For quadratic binary problems, lift to a positive semidefinite matrix \\(X = xx^\\top\\):  Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>"},{"location":"convex/48_advanced_combinatorial/#2243-randomized-rounding","title":"22.4.3 Randomized Rounding","text":"<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>"},{"location":"convex/48_advanced_combinatorial/#225-branch-and-bound-and-search-trees","title":"22.5 Branch-and-Bound and Search Trees","text":"<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>"},{"location":"convex/48_advanced_combinatorial/#2251-basic-principle","title":"22.5.1 Basic Principle","text":"<ol> <li>Partition the feasible set into subsets (branching).  </li> <li>Compute upper/lower bounds for each subset.  </li> <li>Prune branches that cannot contain the optimum.  </li> </ol> <p>The algorithm systematically explores a search tree, guided by bounds.</p>"},{"location":"convex/48_advanced_combinatorial/#2252-bounding-via-relaxations","title":"22.5.2 Bounding via Relaxations","text":"<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>"},{"location":"convex/48_advanced_combinatorial/#226-dynamic-programming","title":"22.6 Dynamic Programming","text":"<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p> \\[ \\text{OPT}(S) = \\min_{x \\in S} \\{ c(x) + \\text{OPT}(S') \\}. \\] <p>It is exact but can suffer from exponential growth (\u201ccurse of dimensionality\u201d).</p> <p>Applications:</p> <ul> <li>Shortest paths</li> <li>Sequence alignment</li> <li>Knapsack</li> <li>Resource allocation</li> </ul> <p>DP offers exact solutions when structure allows sequential decomposition.</p>"},{"location":"convex/48_advanced_combinatorial/#227-heuristics-and-metaheuristics-for-combinatorial-problems","title":"22.7 Heuristics and Metaheuristics for Combinatorial Problems","text":"<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>"},{"location":"convex/48_advanced_combinatorial/#2271-greedy-heuristics","title":"22.7.1 Greedy Heuristics","text":"<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal\u2019s MST). Fast but not always globally optimal.</p>"},{"location":"convex/48_advanced_combinatorial/#2272-local-search-and-hill-climbing","title":"22.7.2 Local Search and Hill Climbing","text":"<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>"},{"location":"convex/48_advanced_combinatorial/#2273-metaheuristic-extensions","title":"22.7.3 Metaheuristic Extensions","text":"<ul> <li>Simulated Annealing: controlled random acceptance of worse moves.  </li> <li>Tabu Search: memory-based diversification.  </li> <li>Ant Colony Optimization: probabilistic path construction.  </li> <li>Genetic Algorithms and PSO: population-based evolution.  </li> </ul> <p>These approaches generalize to discrete structures with minimal problem-specific design.</p>"},{"location":"convex/48_advanced_combinatorial/#228-approximation-algorithms","title":"22.8 Approximation Algorithms","text":"<p>Some combinatorial problems are provably intractable but allow approximation guarantees:  where \\(\\alpha \\ge 1\\) is the approximation ratio.</p> <p>Examples:</p> <ul> <li>Greedy Set Cover: \\(\\alpha = \\ln n + 1\\) </li> <li>Christofides\u2019 Algorithm for TSP: \\(\\alpha = 1.5\\) </li> <li>MAX-CUT SDP Relaxation: \\(\\alpha \\approx 0.878\\)</li> </ul> <p>Approximation theory blends combinatorics with convex relaxation insights.</p>"},{"location":"convex/48_advanced_combinatorial/#229-advanced-topics-constraint-programming-and-decomposition","title":"22.9 Advanced Topics: Constraint Programming and Decomposition","text":""},{"location":"convex/48_advanced_combinatorial/#2291-constraint-programming-cp","title":"22.9.1 Constraint Programming (CP)","text":"<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>"},{"location":"convex/48_advanced_combinatorial/#2292-benders-and-dantzigwolfe-decomposition","title":"22.9.2 Benders and Dantzig\u2013Wolfe Decomposition","text":"<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>"},{"location":"convex/48_advanced_combinatorial/#2293-cutting-plane-methods","title":"22.9.3 Cutting Plane Methods","text":"<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>"},{"location":"convex/48_advanced_combinatorial/#2210-applications-across-domains","title":"22.10 Applications Across Domains","text":"Field Combinatorial Problem Examples Logistics Vehicle routing, warehouse layout Telecommunications Network design, channel allocation Machine Learning Feature selection, clustering, model compression Finance Portfolio optimization with integer positions Bioinformatics Genome assembly, protein structure inference <p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p> <p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p>"},{"location":"convex/50_future/","title":"23. The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":""},{"location":"convex/50_future/#chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence","title":"Chapter 23: The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":"<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead \u2014 toward optimization methods that learn, adapt, and reason \u2014 merging human insight, data-driven modeling, and algorithmic intelligence.</p>"},{"location":"convex/50_future/#231-from-fixed-algorithms-to-adaptive-systems","title":"23.1 From Fixed Algorithms to Adaptive Systems","text":"<p>Traditional optimization algorithms are designed by experts and fixed in form:</p> \\[ x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\] <p>or</p> \\[ x_{k+1} = \\text{Update}(x_k, \\nabla f(x_k); \\theta_{\\text{fixed}}). \\] <p>But real-world problems change over time \u2014 data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>"},{"location":"convex/50_future/#232-optimization-as-learning","title":"23.2 Optimization as Learning","text":"<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p> <p>A meta-optimizer \\(\\text{Opt}_\\theta\\) is parameterized by \\(\\theta\\), and trained to minimize:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{f \\sim \\mathcal{D}}[f(\\text{Opt}_\\theta(f))], \\] <p>where \\(\\mathcal{D}\\) is a distribution over problem instances.</p> <p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>"},{"location":"convex/50_future/#233-reinforcement-learned-optimization","title":"23.3 Reinforcement-Learned Optimization","text":"<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p> <p>At each iteration:</p> <ul> <li>State: current iterate \\(x_t\\), gradient \\(\\nabla f(x_t)\\), and loss \\(f(x_t)\\) </li> <li>Action: choose an update \\(\\Delta x_t\\) </li> <li>Reward: improvement in objective, \\(r_t = -[f(x_{t+1}) - f(x_t)]\\)</li> </ul> <p>A policy \\(\\pi_\\theta\\) learns to output update steps that maximize expected reward. This creates an optimizer that discovers efficient update strategies through experience.</p> <p>RL-based optimizers have been successfully applied in:</p> <ul> <li>Hyperparameter tuning  </li> <li>Neural architecture search  </li> <li>Online control systems  </li> <li>Adaptive sampling and scheduling</li> </ul>"},{"location":"convex/50_future/#234-neuroevolution-and-population-learning","title":"23.4 Neuroevolution and Population Learning","text":"<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly. Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p> <p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p> \\[ \\nabla_\\theta \\mathbb{E}[f(\\theta)] \\approx \\frac{1}{\\sigma} \\mathbb{E}[f(\\theta + \\sigma \\epsilon)\\epsilon]. \\] <p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration\u2013exploitation.</p>"},{"location":"convex/50_future/#235-optimization-and-generative-models","title":"23.5 Optimization and Generative Models","text":"<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective: Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p> <p>For example:</p> <ul> <li>Optimize a latent vector \\(z\\) to generate a design with desired properties.  </li> <li>Use differentiable surrogates to backpropagate through generative pipelines.  </li> <li>Apply gradient-based search within learned manifolds.</li> </ul> <p>This blending of optimization and generation enables creativity \u2014 from molecule design to engineering shape synthesis.</p>"},{"location":"convex/50_future/#236-federated-and-decentralized-optimization","title":"23.6 Federated and Decentralized Optimization","text":"<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization. Each client \\(i\\) holds local data \\(D_i\\) and solves:</p> \\[ \\min_x \\; F(x) = \\frac{1}{N}\\sum_i f_i(x), \\] <p>without sharing raw data.</p> <p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p> <p>Challenges include:</p> <ul> <li>Communication efficiency  </li> <li>Heterogeneity of data and computation  </li> <li>Privacy and fairness constraints</li> </ul>"},{"location":"convex/50_future/#237-optimization-under-uncertainty","title":"23.7 Optimization Under Uncertainty","text":"<p>Modern systems often face uncertain environments: - Random perturbations in data - Dynamic constraints - Unpredictable feedback</p> <p>Approaches to manage uncertainty include:</p> <ol> <li> <p>Robust Optimization:    Minimize worst-case loss under bounded perturbations:     </p> </li> <li> <p>Stochastic Programming:    Optimize expected value or risk measure:     </p> </li> <li> <p>Distributionally Robust Optimization (DRO):    Hedge against model misspecification by optimizing over nearby probability distributions.</p> </li> </ol> <p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>"},{"location":"convex/50_future/#238-quantum-and-analog-optimization","title":"23.8 Quantum and Analog Optimization","text":"<p>As hardware advances, new paradigms emerge: - Quantum Annealing: uses quantum tunneling to escape local minima. - Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem. - Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p> <p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>"},{"location":"convex/50_future/#239-optimization-and-intelligence","title":"23.9 Optimization and Intelligence","text":"<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes \u2014 in essence, optimization loops.</p> <p>Emerging research seeks to unify:</p> <ul> <li>Learning to optimize \u2014 algorithms that adapt through data.  </li> <li>Optimizing to learn \u2014 systems that adjust representations via optimization.  </li> <li>Self-improving optimizers \u2014 algorithms that recursively tune their own parameters.</li> </ul> <p>This convergence blurs the line between optimizer and learner.</p> <p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve \u2014 autonomously, efficiently, and creatively.</p> <p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems \u2014 and thinkers \u2014 to improve themselves.</p>"},{"location":"deeplearning/1_mlp/","title":"1. Introduction to Deep Learning Optimization","text":""},{"location":"deeplearning/1_mlp/#an-introduction-to-neural-networks","title":"An Introduction to Neural Networks","text":""},{"location":"deeplearning/1_mlp/#1-neural-networks-as-computation-graphs","title":"1. Neural Networks as Computation Graphs","text":"<p>Modern neural networks are best understood as differentiable computation graphs.  They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p> <p>Each node in this graph corresponds to a function:</p> \\[z_i = f_i(x_1, \\dots, x_k)\\] <p>and the entire network defines a composite function:</p> \\[f_\\theta(x) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(x)\\] <p>where \\(\\theta = \\{W_i, b_i\\}\\) denotes all learnable parameters.</p>"},{"location":"deeplearning/1_mlp/#formal-structure","title":"Formal Structure","text":"<p>For a Multilayer Perceptron (MLP):</p> \\[h_0 = x, \\quad h_i = \\sigma(W_i h_{i-1} + b_i), \\quad i=1,\\dots,L-1, \\quad \\hat{y} = W_L h_{L-1} + b_L\\] <p>with: \\(W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}, \\quad b_i \\in \\mathbb{R}^{d_i}\\)</p> <p>Each layer is a small differentiable function. When we connect them, we form a composite map \u2014 the fundamental abstraction underlying autodiff, backprop, and learning.</p> <p>Key property: Because every node in the graph is differentiable, the entire function \\(f_\\theta(x)\\) is differentiable with respect to both input \\(x\\) and parameters \\(\\theta\\).</p> <p>Graphically, the network is a directed acyclic graph (DAG):</p> <ul> <li>Edges: carry tensor values.</li> <li>Nodes: represent differentiable functions.</li> <li>Forward pass: evaluates node outputs.</li> <li>Backward pass: propagates sensitivities (gradients) backward.</li> </ul> <p>This graph abstraction unifies all architectures \u2014 CNNs, RNNs, Transformers, Diffusion Models \u2014 as differentiable computation graphs.</p>"},{"location":"deeplearning/1_mlp/#2-gradients-jacobians-and-differentiation","title":"2. Gradients, Jacobians, and Differentiation","text":"<p>For any function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian matrix \\(J_f(x)\\) encodes local derivatives:</p> \\[[J_f(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\] <p>In neural networks, we often deal with a scalar loss function:</p> \\[L(\\theta) = \\ell(f_\\theta(x), y)\\] <p>and want: </p> \\[\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\] <p>However, computing full Jacobians is computationally infeasible \u2014 for a network with millions of parameters, explicit Jacobians would have trillions of entries. Instead, automatic differentiation (autodiff) computes vector\u2013Jacobian products efficiently.</p> <p>For scalar loss \\(L\\): \\(\\nabla_\\theta L = J_{f_\\theta}(x)^T \\nabla_{f_\\theta} L\\)</p> <p>where \\(J_{f_\\theta}(x)\\) is the Jacobian of the output w.r.t. parameters.</p> <p>This operation can be done efficiently in reverse-mode autodiff \u2014 the heart of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#3-forward-and-backward-passes","title":"3. Forward and Backward Passes","text":""},{"location":"deeplearning/1_mlp/#forward-pass","title":"Forward Pass","text":"<p>Given input \\(x\\) and parameters \\(\\theta\\):</p> <ol> <li>Compute layer outputs sequentially: \\(h_i = \\sigma(W_i h_{i-1} + b_i)\\)</li> <li>Compute loss \\(L = \\ell(f_\\theta(x), y)\\)</li> <li>Store intermediate activations \\(h_i\\) for reuse during backpropagation.</li> </ol> <p>This pass evaluates the function \\(L(\\theta)\\).</p>"},{"location":"deeplearning/1_mlp/#backward-pass","title":"Backward Pass","text":"<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p> <p>\\(\\frac{\\partial L}{\\partial \\theta_i} =  \\frac{\\partial L}{\\partial h_L} \\frac{\\partial h_L}{\\partial h_{L-1}} \\dots \\frac{\\partial h_{i+1}}{\\partial \\theta_i}\\)</p> <p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p> <p>Reverse-mode autodiff (backprop) algorithm:</p> <ol> <li>Initialize \\(\\bar{h}_L = \\frac{\\partial L}{\\partial h_L} = 1\\).</li> <li>For each layer \\(l = L, L-1, \\dots, 1\\):</li> <li>Compute local derivative \\(\\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Accumulate gradient: \\(\\bar{h}_{l-1} = \\bar{h}_l \\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Compute parameter gradients: \\(\\frac{\\partial L}{\\partial W_l} = \\bar{h}_l (h_{l-1})^T\\)</li> <li>Return all \\(\\nabla_\\theta L\\).</li> </ol> <p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#4-chain-rule-backpropagation-and-automatic-differentiation","title":"4. Chain Rule, Backpropagation, and Automatic Differentiation","text":"<p>The chain rule underpins all gradient computation. For scalar functions:</p> <p>\\(\\frac{dL}{dx} = \\frac{dL}{dz} \\frac{dz}{dx}\\)</p> <p>and recursively for multivariate functions:</p> <p>\\(\\nabla_x L = J_{z}(x)^T \\nabla_z L\\)</p> <p>Autodiff implements this automatically, performing either:</p> <ul> <li>Forward-mode AD: propagates derivatives forward, efficient when #inputs \u226a #outputs.</li> <li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs \u226a #inputs (our case).</li> </ul> <p>Reverse-mode AD \u2261 backpropagation.</p> <p>Computational Complexity: - Cost \u2248 2\u00d7 forward pass (one forward, one backward). - Memory \u2248 size of stored activations.</p> <p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p> <p>\\(\\min_\\theta L(\\theta)\\)</p> <p>where \\(L\\) is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms. s</p>"},{"location":"deeplearning/1_mlp/#5-from-gradients-to-optimization","title":"5. From Gradients to Optimization","text":"<p>The Learning Problem - Training a neural network means solving:</p> <p>\\(\\min_\\theta \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} [\\,\\ell(f_\\theta(x), y)\\,]\\)</p> <p>Since the true data distribution \\(\\mathcal{D}\\) is unknown, we use empirical risk minimization (ERM):</p> <p>\\(\\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\ell(f_\\theta(x_i), y_i)\\)</p> <p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods \u2014 powered by backpropagation \u2014 reliably find good solutions.</p>"},{"location":"deeplearning/1_mlp/#first-order-optimization-algorithms","title":"First-Order Optimization Algorithms","text":"<p>All modern deep learning optimization relies on gradients:</p> <p>\\(\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\)</p> <p>The basic rule: update parameters in the direction of negative gradient:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_t\\)</p> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"deeplearning/1_mlp/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>We use mini-batches instead of full data:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\frac{1}{|B_t|}\\sum_{i \\in B_t} \\ell(f_\\theta(x_i), y_i)\\)</p> <ul> <li>Cheap per-step computation.</li> <li>Introduces gradient noise, which helps escape shallow minima and saddle points.</li> </ul>"},{"location":"deeplearning/1_mlp/#momentum","title":"Momentum","text":"<p>Accelerates learning by accumulating a velocity vector:</p> <p>\\(v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L_t, \\quad \\theta_{t+1} = \\theta_t + v_{t+1}\\)</p> <p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>"},{"location":"deeplearning/1_mlp/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Maintains exponentially weighted averages of gradients and squared gradients:</p> <p>\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla_\\theta L_t\\)</p> <p>\\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla_\\theta L_t)^2\\)</p> <p>Bias-corrected updates:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\\)</p> <p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>"},{"location":"deeplearning/1_mlp/#second-order-and-curvature-aware-methods","title":"Second-Order and Curvature-Aware Methods","text":"<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p> <p>\\(H = \\frac{\\partial^2 L}{\\partial \\theta^2}\\)</p> <p>Newton\u2019s update:</p> <p>\\(\\theta_{t+1} = \\theta_t - H^{-1}\\nabla_\\theta L\\)</p> <p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets. Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>"},{"location":"deeplearning/1_mlp/#optimization-landscape-and-gradient-flow","title":"Optimization Landscape and Gradient Flow","text":"<p>Although neural network loss surfaces are highly nonconvex, they possess favorable geometry:</p> <ul> <li>Most critical points are saddle points, not local minima.</li> <li>Wide, flat minima generalize better (implicit regularization of SGD).</li> <li>Gradient noise helps explore valleys in high-dimensional space.</li> </ul> <p>Gradient flow (continuous limit of SGD):</p> <p>\\(\\frac{d\\theta(t)}{dt} = - \\nabla_\\theta L(\\theta(t))\\)</p> <p>describes a trajectory in parameter space governed by the vector field of gradients.</p> <p>The optimization algorithm defines the dynamics of this flow (e.g., momentum adds inertia).</p>"},{"location":"deeplearning/1_mlp/#6-what-mlps-cant-do","title":"6. What MLPs Can\u2019t Do?","text":""},{"location":"deeplearning/1_mlp/#a-multiplicative-interactions","title":"(a) Multiplicative Interactions","text":"<p>MLPs compute sums of weighted activations \u2014 inherently additive operations:</p> <p>\\(h = \\sigma(Wx + b)\\)</p> <p>They cannot naturally represent multiplicative relationships (like \\(x_1 x_2\\)) unless approximated via nonlinear stacking, which is inefficient.</p> <p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>"},{"location":"deeplearning/1_mlp/#b-attention-and-dynamic-routing","title":"(b) Attention and Dynamic Routing","text":"<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V\\)</p> <p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>"},{"location":"deeplearning/1_mlp/#c-metric-learning-and-inductive-bias","title":"(c) Metric Learning and Inductive Bias","text":"<p>MLPs lack structural priors about similarity or geometry. Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p> <p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient \u2014 the landscape becomes smoother and gradients more informative.</p>"},{"location":"deeplearning/1_mlp/#7-beyond-backprop-curvature-generalization-and-geometry","title":"7. Beyond Backprop: Curvature, Generalization, and Geometry","text":"<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>"},{"location":"deeplearning/1_mlp/#natural-gradient","title":"Natural Gradient","text":"<p>Instead of minimizing loss directly in parameter space, we minimize it in function space:</p> <p>\\(\\Delta \\theta = - \\eta F^{-1} \\nabla_\\theta L\\)</p> <p>where \\(F\\) is the Fisher information matrix:</p> <p>\\(F = \\mathbb{E}\\left[\\nabla_\\theta \\log p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x)^T\\right]\\)</p> <p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>"},{"location":"deeplearning/1_mlp/#implicit-bias-of-gradient-descent","title":"Implicit Bias of Gradient Descent","text":"<p>Even in overparameterized models, gradient descent tends to find low-norm or flat minima that generalize better \u2014 a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>"},{"location":"deeplearning/1_mlp/#optimization-as-inference","title":"Optimization as Inference","text":"<p>Many modern perspectives view training as approximate inference:</p> <p>\\(p(\\theta | D) \\propto e^{-L(\\theta)/T}\\)</p> <p>Gradient descent samples from this energy landscape as \\(T \\to 0\\); stochastic variants like SGD approximate Bayesian inference under certain limits.</p>"},{"location":"deeplearning/2_convnets/","title":"2. Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#chapter-2-convolutional-neural-networks-cnns","title":"Chapter 2: Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#1-core-principles-locality-and-translation-invariance","title":"1. Core Principles: Locality and Translation Invariance","text":"<p>Before understanding convolutional networks, it\u2019s crucial to grasp why they exist \u2014 the structural priors they impose on data.</p>"},{"location":"deeplearning/2_convnets/#11-locality","title":"1.1 Locality","text":"<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p> <p>For example:</p> <ul> <li>Adjacent pixels in an image often belong to the same object or texture.</li> <li>Neighboring audio samples belong to the same phoneme.</li> <li>Nearby words in a sentence influence each other\u2019s meaning.</li> </ul> <p>MLPs treat every input dimension as independent, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, local region of the input, called its receptive field.</p> <p>Formally, for an input \\(x \\in \\mathbb{R}^{H \\times W}\\), a neuron at position \\((i,j)\\) in a CNN depends only on values in a small window \\(\\Omega(i,j)\\):  This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>"},{"location":"deeplearning/2_convnets/#12-translation-invariance","title":"1.2 Translation Invariance","text":"<p>Natural patterns are repeatable across locations \u2014 the same feature (e.g., an edge, a cat\u2019s ear) can appear anywhere in the image.</p> <p>An MLP would need to learn a separate detector for each position. CNNs overcome this through weight sharing: the same filter \\(W\\) is applied across all spatial positions.</p> <p>Mathematically:  </p> <p>This operation \u2014 convolution \u2014 ensures translation equivariance:  meaning if the input shifts by \\(\\Delta\\), the output shifts by the same amount. After pooling, this becomes translation invariance, i.e. the output doesn\u2019t change under small shifts.</p> <p>These two properties \u2014 locality and translation invariance \u2014 are the foundation of convolutional architectures.</p>"},{"location":"deeplearning/2_convnets/#2-motivation-why-convolutions","title":"2. Motivation: Why Convolutions?","text":"<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos. An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p> <p>Example: For a 256\u00d7256 RGB image (\u2248200K input features), even one hidden layer with 1,000 neurons requires: \\(\\((256 \\times 256 \\times 3) \\times 1000 = 196\\,\\text{million weights}.\\)\\)</p> <p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p> <p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>"},{"location":"deeplearning/2_convnets/#3-the-convolution-operation","title":"3. The Convolution Operation","text":""},{"location":"deeplearning/2_convnets/#31-discrete-convolution","title":"3.1 Discrete Convolution","text":"<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p> <p>For 2D inputs (e.g. images):</p> \\[ S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n) \\] <ul> <li>\\(I\\) \u2014 input (image)</li> <li>\\(K\\) \u2014 kernel (filter)</li> <li>\\(S\\) \u2014 output feature map</li> </ul> <p>Each filter detects a specific local pattern (edges, corners, textures).</p>"},{"location":"deeplearning/2_convnets/#32-convolution-in-neural-networks","title":"3.2 Convolution in Neural Networks","text":"<p>In CNNs, the convolution becomes a learnable operation:</p> \\[ h_{i,j,k} = \\sigma\\left( \\sum_{c=1}^{C_\\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \\right) \\] <ul> <li>\\(x_c\\): input channel \\(c\\) (e.g. R, G, B)</li> <li>\\(W_{k,c}\\): kernel for output channel \\(k\\) and input channel \\(c\\)</li> <li>\\(b_k\\): bias for output channel \\(k\\)</li> <li>\\(\\sigma\\): nonlinearity (ReLU, etc.)</li> </ul> <p>This produces \\(C_\\text{out}\\) feature maps, each representing a learned spatial pattern.</p> <p>Weight sharing drastically reduces parameters: Each kernel might be \\(3 \\times 3\\) or \\(5 \\times 5\\) \u2014 independent of image size.</p>"},{"location":"deeplearning/2_convnets/#4-building-blocks-of-cnns","title":"4. Building Blocks of CNNs","text":""},{"location":"deeplearning/2_convnets/#41-convolutional-layer","title":"4.1 Convolutional Layer","text":"<p>Performs learnable filtering and produces feature maps.</p> <p>If input has shape \\((H, W, C_\\text{in})\\): - Kernel: \\((k_H, k_W, C_\\text{in}, C_\\text{out})\\) - Output: \\((H', W', C_\\text{out})\\)</p>"},{"location":"deeplearning/2_convnets/#42-nonlinear-activation","title":"4.2 Nonlinear Activation","text":"<p>After convolution, apply nonlinearity (commonly ReLU):  </p>"},{"location":"deeplearning/2_convnets/#43-pooling-layer","title":"4.3 Pooling Layer","text":"<p>Reduces spatial dimensions and increases invariance.</p> <ul> <li>Max pooling: selects the largest value in a patch.</li> <li>Average pooling: takes mean value.</li> </ul> <p>Formally:  </p> <p>Pooling introduces translation invariance \u2014 small shifts in input don\u2019t drastically change outputs.</p>"},{"location":"deeplearning/2_convnets/#44-flatten-fully-connected-layers","title":"4.4 Flatten + Fully Connected Layers","text":"<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>"},{"location":"deeplearning/2_convnets/#5-cnn-architecture-as-a-computation-graph","title":"5. CNN Architecture as a Computation Graph","text":"<p>A typical CNN defines a differentiable map:</p> \\[ f_\\theta(x) = W_L (\\text{Flatten}(h_{L-1})) + b_L \\] <p>where each layer \\(h_l\\) is defined recursively as:</p> \\[ h_l = \\sigma(\\text{Conv}(h_{l-1}; W_l) + b_l), \\quad l = 1, \\dots, L-1 \\] <p>Here, <code>Conv</code> represents the convolution operation.</p> <p>Each layer is spatially local, translation-equivariant, and differentiable \u2014 meaning backpropagation works seamlessly, just as in MLPs.</p>"},{"location":"deeplearning/2_convnets/#6-backpropagation-through-convolutions","title":"6. Backpropagation Through Convolutions","text":"<p>The gradient computation is a direct extension of the chain rule.</p>"},{"location":"deeplearning/2_convnets/#61-forward-pass","title":"6.1 Forward Pass","text":"<p>Compute:  </p>"},{"location":"deeplearning/2_convnets/#62-backward-pass","title":"6.2 Backward Pass","text":"<p>We need: - Gradient w.r.t. weights: \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) - Gradient w.r.t. input: \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\)</p> <p>The flipping arises from the mathematical property of convolution. Modern frameworks handle this efficiently via convolution transpose operations.</p> <p>Optimization viewpoint: Convolution layers remain linear in their weights \u2014 the nonlinearity and local parameter sharing define their expressive power.</p>"},{"location":"deeplearning/2_convnets/#7-inductive-biases-in-cnns","title":"7. Inductive Biases in CNNs","text":"<p>Convolutional architectures embed strong inductive biases:</p> Property Mathematical Mechanism Effect Local connectivity Small kernels (3\u00d73, 5\u00d75) Exploits spatial locality Weight sharing Same filter across space Reduces parameters drastically Translation equivariance Convolution operation Same pattern detection anywhere Pooling invariance Spatial downsampling Robust to small shifts/noise <p>These biases make CNNs data-efficient and easy to train \u2014 especially compared to fully connected networks on images.</p>"},{"location":"deeplearning/2_convnets/#8-optimization-and-training-dynamics","title":"8. Optimization and Training Dynamics","text":"<p>Training CNNs is similar to MLPs \u2014 we use gradient-based optimizers (SGD, Adam, etc.) \u2014 but with different landscape geometry:</p> <ul> <li>Parameter sharing makes the loss smoother (less overfitting).</li> <li>Batch normalization stabilizes gradient flow:    </li> <li>Regularization via dropout or weight decay improves generalization.</li> <li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li> </ul> <p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>"},{"location":"deeplearning/2_convnets/#9-cnn-architectures-through-history","title":"9. CNN Architectures Through History","text":"Model Year Key Innovation Depth Inductive Bias LeNet-5 1998 First practical CNN for handwritten digits 7 layers Local receptive fields AlexNet 2012 GPU training, ReLU, dropout 8 layers Data augmentation VGG 2014 Deep stacks of small 3\u00d73 filters 19 layers Uniform architecture ResNet 2015 Skip connections for gradient flow 152 layers Identity mapping DenseNet 2016 Feature reuse via dense connectivity 201 layers Multi-scale learning EfficientNet 2019 Compound scaling variable Optimized parameter scaling"},{"location":"deeplearning/2_convnets/#10-cnns-and-the-optimization-landscape","title":"10. CNNs and the Optimization Landscape","text":"<p>CNNs reshape the optimization problem compared to MLPs:</p> <ul> <li>Reduced parameter redundancy \u2192 fewer degenerate directions in gradient space.</li> <li>Structured weight sharing \u2192 smoother loss surface, fewer sharp minima.</li> <li>Skip connections (ResNets) introduce identity mappings, improving conditioning of the Jacobian and preventing vanishing gradients.</li> </ul> <p>In optimization terms, CNNs are better-conditioned models of the input\u2013output mapping.</p>"},{"location":"deeplearning/2_convnets/#11-beyond-classical-cnns","title":"11. Beyond Classical CNNs","text":"<p>Modern vision architectures have evolved: - Residual Networks (ResNets): skip connections allow training very deep models. - Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count. - Dilated Convolutions: expand receptive field without extra parameters. - Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>"},{"location":"deeplearning/2_convnets/#12-mathematical-summary","title":"12. Mathematical Summary","text":"Concept Formula Description Convolution \\((I * K)(i,j) = \\sum_m \\sum_n I(i+m,j+n) K(m,n)\\) Weighted local sum CNN Layer \\(h = \\sigma(W * x + b)\\) Convolution + nonlinearity Pooling \\(y_{i,j} = \\max_{(m,n)\\in \\Omega(i,j)} h_{m,n}\\) Downsampling Gradient wrt weights \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) Backprop step Gradient wrt input \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\) Sensitivity propagation"},{"location":"deeplearning/2_convnets/#13-intuitive-summary","title":"13. Intuitive Summary","text":"<p>Convolutional networks are: - Local \u2192 they process neighborhoods of data. - Hierarchical \u2192 deeper layers build on lower-level features. - Translation-equivariant \u2192 same pattern anywhere is treated the same. - Efficient \u2192 far fewer parameters than MLPs.</p> <p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p>"},{"location":"deeplearning/3_sequence_data/","title":"3. Sequence Data and Recurrent Neural Networks (RNNs)","text":""},{"location":"deeplearning/3_sequence_data/#chapter-3-modeling-sequence-data-in-deep-learning","title":"Chapter 3: Modeling Sequence Data in Deep Learning","text":"<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length \\(T\\) can be written as \\((x_1,x_2,\\dots,x_T)\\), where each element \\(x_t\\) is indexed by its position in the sequence. Elements can repeat (e.g. the word \u201cthe\u201d may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p> <p>Sequences are collection of elements where:</p> <ul> <li>Elements can be repeated.</li> <li>Order matters.</li> <li>Of variable length.</li> </ul>"},{"location":"deeplearning/3_sequence_data/#limitations-of-traditional-supervised-models","title":"Limitations of Traditional Supervised Models:","text":"<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data \u2013 by, say, collapsing the sequence into a fixed-size feature vector \u2013 ignores the important temporal or sequential structure. As one summary notes, \u201cfeedforward neural networks are severely limited when it comes to sequential data\u201d. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p> <p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>"},{"location":"deeplearning/3_sequence_data/#the-simplest-assumption-independent-words-bag-of-words","title":"The Simplest Assumption: Independent Words (Bag-of-Words)","text":"<p>A na\u00efve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word\u2019s occurrence, treating all words as \u201cindependent features.\u201d This ignores sequence structure: \u201cthe order of words in the original documents is irrelevant\u201d. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: \u201cbag-of-words assumes words are independent of one another\u201d. In reality, words co-occur in context (\u201cpeanut butter\u201d versus \u201cpeanut giraffe\u201d) \u2013 bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>"},{"location":"deeplearning/3_sequence_data/#n-gram-models-and-fixed-context-assumptions","title":"N-gram Models and Fixed-Context Assumptions","text":"<p>To go beyond complete independence, one can incorporate local context by using \\(n\\)-gram models. An \\(n\\)-gram model makes the (Markov) assumption that the probability of each element depends only on the previous \\(n-1\\) elements. For language, a bigram model (2-gram) assumes \\(P(w_t\\mid w_{t-1})\\), a trigram (3-gram) uses \\(P(w_t\\mid w_{t-2},w_{t-1})\\), etc. In general, the chain rule with an \\(N\\)-gram approximation is</p> \\[ P(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{t-N+1}, \\ldots, x_{t-1}) \\, . \\] <p>This preserves some order information: the window of the last \\(N-1\\) items is used to predict the next. However, \\(n\\)-gram models have well-known downsides:</p> <ul> <li> <p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language \u201ccannot reason about context beyond the immediate \\(n\\)-gram window\u201d, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an \\(n\\)-gram.</p> </li> <li> <p>Data sparsity and scalability: The number of possible \\(n\\)-grams grows exponentially with vocabulary size \\(V\\). For a vocabulary of size \\(V\\), there are \\(V^N\\) possible \\(N\\)-grams. Jurafsky &amp; Martin observe that even for Shakespeare\u2019s corpus (\\(V\\approx 29{,}066\\)), there are \\(V^2\\approx8.4\\times 10^8\\) possible bigrams and \\(V^4\\approx 7\\times 10^{17}\\) possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough \\(n\\)-gram counts, and storing these tables is impractical for large \\(N\\) or \\(V\\). In practice, language models become \u201cridiculously sparse\u201d and unwieldy.</p> </li> <li> <p>No parametrization (non-differentiable): Traditional \\(n\\)-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p> </li> </ul> <p>In summary, while \\(n\\)-grams preserve local order up to length \\(N\\), they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>"},{"location":"deeplearning/3_sequence_data/#learnable-context-models-vectorization-and-neural-nets","title":"Learnable Context Models: Vectorization and Neural Nets","text":"<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p> <ul> <li> <p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p> </li> <li> <p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p> </li> <li> <p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can \u201coperate in variable length of sequences\u201d.</p> </li> <li> <p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p> </li> <li> <p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p> </li> </ul> <p>For example, one could take the last few words, map each to an embedding \\(\\mathbf{x}{t-N+1},\\dots,\\mathbf{x}{t-1}\\), concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word\u2019s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (\\(N-1\\)) and uses a separate weight for each position, so it\u2019s not efficient or variable-length. </p> <p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector \\(\\mathbf{h}_t\\) that evolves as the sequence is read. Such a context-vector \u201cacts as memory\u201d summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can \u201clearn\u201d how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>"},{"location":"deeplearning/3_sequence_data/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) \u2013 models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step \\(t\\), the RNN takes the current input \\(\\mathbf{x}t\\) and the previous hidden state \\(\\mathbf{h}{t-1}\\) and computes a new hidden state \\(\\mathbf{h}_t\\). The simplest RNN update is:</p> \\[ h_t = \\phi(W_h h_{t-1} + W_x x_t + b) \\, . \\] <p>where \\(\\phi\\) is a nonlinear activation (often \\(\\tanh\\)) and \\(W_h,W_x\\) are weight matrices. The same weight matrices \\(W_h,W_x\\) are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p> <p>Intuitively, the RNN\u2019s hidden state \\(\\mathbf{h}_t\\) \u201cremembers\u201d the information from all prior inputs up to time \\(t\\). The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p> \\[ y_t = \\mathrm{softmax}(W_y h_t + b_y) \\, . \\] <p>so that \\(P(x_{t+1}=w \\mid \\mathbf{h}_t)\\) is given by the corresponding component of \\(\\mathbf{y}_t\\). In language modeling, for instance, \\(y_t\\) gives a probability for each word in the vocabulary. As described in practice, \u201cRNNs predict the output from the last hidden state along with output parameter \\(W_y\\); a softmax function to ensure the probability over all possible words\u201d. </p> <p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an \u201cMLP with shared weights across time.\u201d At time \\(t\\), the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>"},{"location":"deeplearning/3_sequence_data/#unrolling-and-backpropagation-through-time-bptt","title":"Unrolling and Backpropagation Through Time (BPTT)","text":"<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across \\(T\\) time steps, creating a deep feedforward network of depth \\(T\\) (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, \u201cthe network needs to be expanded, or unfolded, so that the parameters could be differentiated ... \u2013 hence backpropagation through time (BPTT)\u201d. In practice, each weight matrix \\(W\\) receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if \\(L = -\\sum_t \\log P(x_t\\mid \\mathbf{h}_{t-1})\\) is the loss, then for each \\(W\\) we compute</p> \\[ \\frac{\\partial L}{\\partial W} = \\sum_{t} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} \\, . \\] <p>taking into account the influence of \\(W\\) at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>"},{"location":"deeplearning/3_sequence_data/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative \\(\\partial \\mathbf{h}t/\\partial \\mathbf{h}{t-1}\\) involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius \\(&lt;1\\) causes the gradients to shrink exponentially (vanishing), while if the spectral radius is \\(&gt;1\\) they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues \\(&gt;1\\)), whereas the vanishing gradient problem occurs when long-term components of the gradient go \u201cexponentially fast to norm 0\u201d. Formally, for a linearized RNN one can show that if the largest eigenvalue \\(\\lambda_{\\max}\\) of the recurrent weight matrix satisfies \\(|\\lambda_{\\max}|&lt;1\\), long-term gradients vanish as \\(t\\to\\infty\\), and if \\(|\\lambda_{\\max}|&gt;1\\) they explode. </p> <p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: \u201cwhen long term components go to zero, the model cannot learn correlation between distant events.\u201d In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>"},{"location":"deeplearning/3_sequence_data/#gated-architectures-lstm-and-gru","title":"Gated Architectures: LSTM and GRU","text":"<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable \u201cgates\u201d that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state \\(\\mathbf{C}_t\\) and three gates: input (\\(\\mathbf{i}_t\\)), forget (\\(\\mathbf{f}_t\\)), and output (\\(\\mathbf{o}t\\)) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time \\(t\\) with input \\(\\mathbf{x}t\\) and previous hidden \\(\\mathbf{h}{t-1}\\) and cell \\(\\mathbf{C}{t-1}\\), the gates and cell update are given by (all operations are elementwise):</p> <p>\u200b </p> <p>The new cell state \\(\\mathbf{C}_t\\) is then updated by combining the old state and the candidate:</p> \\[ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\, . \\] <p>where \\(\\odot\\) denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p> \\[ h_t = o_t \\odot \\tanh(C_t) \\, \\] <p>The intuition is that the forget gate \\(\\mathbf{f_t}\\) can reset or retain the old memory \\(\\mathbf{C}_{t-1}\\), the input gate \\(\\mathbf{i}_t\\) controls how much new information \\(\\tilde{\\mathbf{C}}_t\\) to write, and the output gate \\(\\mathbf{o}_t\\) controls how much of the cell state to expose as \\(\\mathbf{h}_t\\). By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs \u201calleviate the vanishing gradient problem,\u201d making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p> <p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>"},{"location":"deeplearning/3_sequence_data/#optimization-challenges-and-solutions","title":"Optimization Challenges and Solutions","text":"<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p> <ul> <li> <p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping \u201csolves the exploding gradients problem\u201d by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p> </li> <li> <p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing \\(W_h\\) as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, \u201cOrthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,\u201d ensuring stable gradient propagation. In practice, some implementations initialize \\(W_h\\) to random orthogonal (or unitary) matrices to encourage long memory.</p> </li> <li> <p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p> </li> <li> <p>Regularization: Some works add penalties to encourage \\(W_h\\) to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p> </li> </ul> <p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models \u2013 especially gated RNNs \u2013 provide a powerful framework: mathematically, they define hidden states \\(\\mathbf{h}_t\\) updated by \\(\\mathbf{h}t = f(\\mathbf{h}{t-1},\\mathbf{x}_t)\\) with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p>"},{"location":"deeplearning/4_nlp/","title":"4. Natural Language Processing (NLP) with Deep Learning","text":""},{"location":"deeplearning/4_nlp/#deep-learning-for-natural-language-processing","title":"Deep Learning for Natural Language Processing","text":"<ul> <li>Natural language is context-dependent, compositional, and ambiguous.</li> <li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation \u2014 ideal for modeling contextual relationships.</li> <li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li> </ul>"},{"location":"deeplearning/4_nlp/#key-challenges-of-language","title":"Key Challenges of Language","text":"<p>Human language presents a unique set of challenges for computational models. Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p> <ul> <li> <p>Words are not discrete symbols.   The same word can have several related senses depending on context \u2014 for example: <code>face\u2081</code> (human face), <code>face\u2082</code> (clock face), <code>face\u2083</code> (to confront), and <code>face\u2084</code> (a person or presence).   Treating these as independent dictionary entries loses the shared semantic structure between them.   A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p> </li> <li> <p>Need for distributed representations.   Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).   This allows similarity, analogy, and composition to emerge geometrically \u2014 for instance, <code>king - man + woman \u2248 queen</code>.</p> </li> <li> <p>Disambiguation depends on context.   The meaning of a word or phrase is determined by its linguistic surroundings.   For example, in \u201cThe man who ate the pepper sneezed,\u201d the subject of sneezed is determined by a non-adjacent clause (the man), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p> </li> <li> <p>Non-local dependencies.   Natural language contains relationships between words that may be far apart in sequence.   Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.   Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p> </li> <li> <p>Compositionality.   The meaning of larger expressions arises from the meanings of their parts and how they are combined.   However, this combination is not purely linear.   For example, <code>carnivorous plant</code> is not simply the sum of carnivore and plant \u2014 its interpretation depends on how the features interact (a plant that eats insects).   Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p> </li> </ul> <p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically. Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>"},{"location":"deeplearning/4_nlp/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Sequence models (RNNs, LSTMs) process tokens sequentially \u2014 limiting parallelism and long-range context.</li> <li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li> </ul>"},{"location":"deeplearning/4_nlp/#core-mechanism-self-attention","title":"Core Mechanism: Self-Attention","text":"<p>Given token embeddings :</p> \\[ q_i = e_i W^Q, \\quad k_i = e_i W^K, \\quad v_i = e_i W^V \\] <p>Attention weights:</p> \\[ \\alpha_{ij} = \\mathrm{softmax}_j \\left( \\frac{q_i k_j^\\top}{\\sqrt{d}} \\right) \\] <p>Output:</p> \\[ z_i = \\sum_j \\alpha_{ij} v_j \\] <p>Each token\u2019s new representation  is a contextual blend of all others. Captures semantic and syntactic relations without explicit recurrence.</p>"},{"location":"deeplearning/4_nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Use multiple projections \\((W^Q_h, W^K_h, W^V_h)\\) \u2192 multiple \u201cheads.\u201d Each head focuses on different relations (e.g. subject\u2013verb, modifier\u2013noun). Outputs are concatenated and projected back to dimension \\(d\\):</p> \\[ \\text{MHA}(E) = [Z_1; Z_2; \\dots; Z_H] W^O \\]"},{"location":"deeplearning/4_nlp/#position-encoding","title":"Position Encoding","text":"<p>Since attention is permutation-invariant, Transformers add position information:</p> \\[ \\text{PE}_{(pos,2i)} = \\sin(pos / 10000^{2i/d}), \\quad \\text{PE}_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d}) \\] <p>\u2192 These sinusoidal signals are added to embeddings to encode word order.</p>"},{"location":"deeplearning/4_nlp/#full-transformer-block","title":"Full Transformer Block","text":"<pre><code>Input\n  \u2193\nMulti-Head Self-Attention\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nFeedforward Network (ReLU)\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nOutput\n</code></pre> <p>Skip connections enable gradient flow and top-down influence. Stacking \\(N\\) blocks yields hierarchical contextualization of meaning.</p>"},{"location":"deeplearning/4_nlp/#intuition","title":"Intuition","text":"<ul> <li>Self-attention handles non-local relations.</li> <li>Multi-head captures multiple semantic dimensions simultaneously.</li> <li>Stacked layers build abstraction \u2014 from word-level to phrase- and discourse-level features.</li> </ul>"},{"location":"deeplearning/4_nlp/#unsupervised-learning-and-bert","title":"Unsupervised Learning and BERT","text":""},{"location":"deeplearning/4_nlp/#the-need-for-contextualized-representations","title":"The Need for Contextualized Representations","text":"<ul> <li>Word embeddings like Word2Vec are static: one vector per word.</li> <li>Language understanding requires contextual embeddings: \u201cbank\u201d (river vs. finance).</li> <li>Transformers enable bidirectional context \u2014 understanding a word from both sides.</li> </ul>"},{"location":"deeplearning/4_nlp/#bert-pretraining-objectives","title":"BERT Pretraining Objectives","text":"<ol> <li>Masked Language Modeling (MLM) Randomly mask 15% of tokens, predict them:</li> </ol> \\[ \\text{Loss}_{MLM} = - \\sum_{i \\in M} \\log P(w_i | \\text{context}) \\] <p>Encourages bidirectional encoding of meaning.</p> <ol> <li>Next Sentence Prediction (NSP) Model predicts if sentence B follows sentence A. Builds discourse-level coherence and world knowledge.</li> </ol>"},{"location":"deeplearning/4_nlp/#architecture","title":"Architecture","text":"<ul> <li>Deep bidirectional Transformer encoder.</li> <li>Uses special tokens:</li> <li><code>[CLS]</code> \u2013 sentence-level classification embedding</li> <li><code>[SEP]</code> \u2013 separates segments</li> <li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li> <li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li> </ul>"},{"location":"deeplearning/4_nlp/#significance","title":"Significance","text":"<p>BERT shows self-supervised pretraining \u2192 transfer learning pipeline:</p> <pre><code>Pretrain (unsupervised)\n   \u2193\nFine-tune (supervised)\n   \u2193\nTask-specific adaptation\n</code></pre> <p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data. Learns semantic similarity, coreference, and discourse relations implicitly.</p>"},{"location":"deeplearning/4_nlp/#grounded-and-embodied-language-learning","title":"Grounded and Embodied Language Learning","text":""},{"location":"deeplearning/4_nlp/#motivation","title":"Motivation","text":"<ul> <li>Language understanding ultimately involves relating words to the world.</li> <li>Humans learn language in context \u2014 perception, action, and social interaction.</li> <li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li> </ul>"},{"location":"deeplearning/4_nlp/#grounded-agents","title":"Grounded Agents","text":"<ul> <li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li> <li>Train via predictive modeling \u2014 anticipate sensory outcomes from language-conditioned actions.</li> <li>Enables semantic grounding: linking word \u201cred\u201d to visual color, \u201cpick up\u201d to motor command.</li> </ul>"},{"location":"deeplearning/4_nlp/#predictive-and-self-supervised-paradigms","title":"Predictive and Self-Supervised Paradigms","text":"<p>Agents learn representations by predicting future sensory or linguistic states:</p> \\[ \\min_\\theta \\mathbb{E} [ \\| f_\\theta(s_t, a_t) - s_{t+1} \\|^2 ] \\] <p>\u2192 Connects to world models and predictive coding principles in neuroscience. The agent\u2019s internal model encodes both linguistic meaning and causal structure of the environment.</p>"},{"location":"deeplearning/4_nlp/#insights-from-deepmind-work","title":"Insights from DeepMind Work","text":"<ul> <li>Embodied agents trained in simulated environments exhibit:</li> <li>Systematic generalization (e.g., learning \u201cpick up red object\u201d \u2192 generalize to unseen colors).</li> <li>Question answering and instruction following grounded in perception.</li> <li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li> </ul>"},{"location":"deeplearning/4_nlp/#conceptual-shift","title":"Conceptual Shift","text":"<p>From pipeline \u2192 integrated model:</p> Classic Pipeline Embodied / Interactive Model Letters \u2192 Words \u2192 Syntax \u2192 Meaning \u2192 Action Multimodal loops: Perception \u2194 Action \u2194 Language \u2194 Prediction"},{"location":"deeplearning/4_nlp/#conceptual-map-from-representation-to-understanding","title":"Conceptual Map: From Representation to Understanding","text":"<pre><code>Word Input\n   \u2193\nDistributed Representations (embedding)\n   \u2193\nSelf-Attention Mechanism\n   \u2193\nMulti-Head Parallel Processing\n   \u2193\nHierarchical Transformer Layers\n   \u2193\nContextualized Embeddings (BERT)\n   \u2193\nTransfer Learning to Tasks\n   \u2193\nEmbodied Agents (Grounded Semantics)\n   \u2193\nLanguage Understanding as Prediction + Interaction\n</code></pre>"},{"location":"deeplearning/4_nlp/#key-transitions","title":"Key Transitions","text":"<p>Symbol \u2192 Vector: Continuous representations enable learning of semantic gradients.</p> <p>Sequence \u2192 Attention: Parallel context integration replaces recurrence.</p> <p>Text \u2192 Context: Pretraining captures knowledge without explicit supervision.</p> <p>Language \u2192 World: Grounding links linguistic representations to sensory and causal models.</p>"},{"location":"deeplearning/4_nlp/#unifying-principle","title":"Unifying Principle","text":"<p>Deep language understanding = predictive modeling of structured context across both linguistic and environmental domains.</p> Concept Core Idea Model / Mechanism Distributed representations Meanings as patterns, not symbols Embeddings Context dependence Sense resolution via interaction Self-attention Parallelism All words attend to all others Transformer Bidirectionality Context from both sides BERT encoder Transfer learning Self-supervised \u2192 supervised Fine-tuning Grounding Language tied to perception/action Embodied agents Predictive learning Understanding as anticipation World models"},{"location":"deeplearning/5_attention/","title":"5. Transformers and Attention Mechanisms","text":""},{"location":"deeplearning/5_attention/#1-attention-memory-and-cognition","title":"1. Attention, Memory, and Cognition","text":"<ul> <li>Attention = ability to focus on relevant signals and ignore distractions.  </li> <li>Enables selective processing (e.g. cocktail party effect).  </li> <li> <p>Allows focusing on one thought or event at a time.</p> </li> <li> <p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p> </li> <li> <p>Together, they form the basis of cognition \u2014 controlling what to process, store, and recall.</p> </li> <li> <p>Neural networks can model aspects of this by learning what to attend to and what to remember.</p> </li> <li> <p>Goal of attention in DL:   Reduce complexity by focusing computation on the most informative parts of data or internal state.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#2-implicit-attention-in-neural-networks","title":"2. Implicit Attention in Neural Networks","text":"<ul> <li> <p>Neural networks are parametric nonlinear functions \\(y = f_\\theta(x)\\) mapping inputs to outputs.   They naturally exhibit implicit attention: certain input dimensions influence outputs more.</p> </li> <li> <p>The Jacobian \\(J = \\frac{\\partial y}{\\partial x}\\) quantifies this sensitivity \u2014 shows which input parts the model \u201cpays attention\u201d to.</p> </li> <li> <p>Example:   In deep RL, sensitivity maps reveal focus on state-value vs action-advantage components.</p> </li> <li> <p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p> </li> <li>Hidden state \\(h_t\\) stores past info.  </li> <li>The sequential Jacobian \\(\\frac{\\partial y_t}{\\partial x_{t-k}}\\) shows which past inputs are remembered.  </li> <li> <p>Implicitly attends to relevant time steps (memory through recurrence).</p> </li> <li> <p>In tasks like machine translation, implicit attention lets models reorder tokens:</p> <p>\u201cto reach\u201d \u2192 \u201czu erreichen\u201d</p> </li> </ul>"},{"location":"deeplearning/5_attention/#3-explicit-hard-attention","title":"3. Explicit (Hard) Attention","text":"<ul> <li>Explicit attention introduces a separate attention mechanism that decides where to look or what to read.   It restricts the data fed to the main network.</li> </ul>"},{"location":"deeplearning/5_attention/#why-explicit-attention","title":"Why explicit attention?","text":"<ul> <li>Efficiency: processes only selected parts of input.  </li> <li>Scalability: works on large or variable-size data.  </li> <li>Sequential processing: e.g. moving \u201cgaze\u201d across static images.  </li> <li>Interpretability: easier to visualize focus regions.</li> </ul>"},{"location":"deeplearning/5_attention/#model-structure","title":"Model structure","text":"<ul> <li>Network outputs attention parameters \\(a\\) that define a glimpse distribution \\(p(g|a)\\) over possible data regions.  </li> <li>A glimpse \\(g\\) (subset or window of data) is sampled and passed back as input.  </li> <li>System becomes recurrent, even if the base network is not.</li> </ul>"},{"location":"deeplearning/5_attention/#training-non-differentiable","title":"Training (non-differentiable)","text":"<ul> <li> <p>When glimpse selection is discrete or stochastic, use REINFORCE:      where \\(R\\) is the reward (e.g. task loss) and \\(b\\) a baseline for variance reduction.</p> </li> <li> <p>Thus, attention acts as a policy \\(\\pi_\\theta(g)\\) over glimpses.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#examples","title":"Examples","text":"<ul> <li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li> <li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li> </ul>"},{"location":"deeplearning/5_attention/#4-soft-attention","title":"4. Soft Attention","text":"<ul> <li>Hard attention samples discrete glimpses \u2192 non-differentiable \u2192 needs RL.  </li> <li>Soft attention computes a weighted average over all glimpses \u2192 differentiable \u2192 trainable by backprop.</li> </ul>"},{"location":"deeplearning/5_attention/#basic-idea","title":"Basic idea","text":"<ul> <li> <p>Attention parameters \\(a\\) define weights \\(w_i\\) over input features \\(v_i\\):      The readout \\(v\\) is a smooth combination of inputs.</p> </li> <li> <p>Replaces sampling by expectation \u2192 continuous, differentiable.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#benefits","title":"Benefits","text":"<ul> <li>Trained end-to-end with gradients.  </li> <li>Easier and more stable than hard attention.  </li> <li>Allows focus distribution rather than a single point.</li> </ul>"},{"location":"deeplearning/5_attention/#variants","title":"Variants","text":"<ul> <li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li> <li>Content-based attention: focuses by similarity of key \\(k\\) to data vectors \\(x_i\\) via score \\(S(k, x_i)\\), usually normalized by softmax:    </li> </ul>"},{"location":"deeplearning/5_attention/#applications","title":"Applications","text":"<ul> <li>Handwriting synthesis: RNN learns soft \u201cwindow\u201d over text sequence.  </li> <li>Neural Machine Translation: associative attention aligns words between languages.  </li> <li> <p>DRAW model: uses Gaussian filters to read/write parts of an image.</p> </li> <li> <p>Soft attention = data-dependent dynamic weighting (similar to convolution with adaptive filters).</p> </li> </ul>"},{"location":"deeplearning/5_attention/#5-introspective-attention-and-memory","title":"5. Introspective Attention and Memory","text":"<ul> <li>So far: attention over external data.  </li> <li>Now: attention over internal state or memory \u2192 \u201cintrospective attention.\u201d  </li> <li>Lets the network read or write selectively to memory locations.  </li> <li>Enables reasoning, recall, and algorithmic behavior.</li> </ul>"},{"location":"deeplearning/5_attention/#neural-turing-machine-ntm","title":"Neural Turing Machine (NTM)","text":"<ul> <li>Adds a differentiable memory matrix \\(M \\in \\mathbb{R}^{N \\times W}\\).  </li> <li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li> </ul> <p>Operations - Write: modify selected rows in \\(M\\) using attention weights \\(w_t\\). - Read: output weighted sum of memory slots:    - Addressing modes:   - Content-based: match key vector \\(k_t\\) to memory contents (via cosine similarity).   - Location-based: shift attention by relative position.</p> <p>Training: fully differentiable \u2014 end-to-end via backprop.</p> <p>Example task: copying sequences of variable length \u2014 learns algorithmic generalization.</p>"},{"location":"deeplearning/5_attention/#differentiable-neural-computer-dnc","title":"Differentiable Neural Computer (DNC)","text":"<ul> <li>Successor to NTM with richer memory access:</li> <li>Tracks temporal links between writes.  </li> <li>Supports dynamic memory allocation.  </li> <li>Improves stability and scalability.</li> </ul> <p>Application: synthetic QA tasks (bAbI dataset) \u2014 answers questions requiring multiple supporting facts and temporal reasoning.</p> <p>Key insight: Attention provides selective access to memory, acting like \u201caddressing\u201d in a differentiable data structure.</p>"},{"location":"deeplearning/5_attention/#6-transformers-and-self-attention","title":"6. Transformers and Self-Attention","text":"<ul> <li>Transformers: remove recurrence and convolution entirely \u2014 rely only on attention.</li> </ul>"},{"location":"deeplearning/5_attention/#self-attention","title":"Self-Attention","text":"<ul> <li>Each token attends to all others in the sequence:      where:</li> <li>\\(Q, K, V\\) are query, key, and value matrices (learned linear projections of input embeddings).</li> <li>Produces context-aware representations for all tokens in parallel.</li> </ul>"},{"location":"deeplearning/5_attention/#multi-head-attention","title":"Multi-Head Attention","text":"<ul> <li>Multiple attention \u201cheads\u201d (\\(H\\)) learn different relationships:      Each head captures a distinct pattern (syntax, semantics, position, etc.).</li> </ul>"},{"location":"deeplearning/5_attention/#transformer-block","title":"Transformer Block","text":"<ul> <li>Structure:</li> <li>Multi-head self-attention  </li> <li>Add &amp; LayerNorm  </li> <li>Feedforward (ReLU + linear)  </li> <li>Add &amp; LayerNorm  </li> <li>Skip connections improve gradient flow and allow top-down signal mixing.</li> </ul>"},{"location":"deeplearning/5_attention/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Since model is permutation-invariant, inject position information:       Added to input embeddings.</li> </ul>"},{"location":"deeplearning/5_attention/#intuition","title":"Intuition","text":"<ul> <li>Self-attention generalizes RNN memory:</li> <li>Recurrent \u2192 sequential access  </li> <li>Transformer \u2192 direct pairwise access between all tokens.</li> <li>Enables long-range dependencies and parallelization.</li> </ul>"},{"location":"deeplearning/5_attention/#key-result","title":"Key result","text":"<ul> <li>Attention-only models achieve SOTA in translation and NLP tasks.  </li> <li>Forms basis for BERT, GPT, and modern large language models.</li> </ul>"},{"location":"deeplearning/5_attention/#7-adaptive-computation-time-act-and-summary","title":"7. Adaptive Computation Time (ACT) and Summary","text":""},{"location":"deeplearning/5_attention/#adaptive-computation-time-act","title":"Adaptive Computation Time (ACT)","text":"<ul> <li>Proposed by Graves (2016): allows networks to \u201cponder\u201d variable amounts of time per input.  </li> <li>Each step computes a halting probability \\(p_t\\); total halt when \\(\\sum_t p_t = 1\\).</li> <li>Output is a weighted sum of intermediate states:    </li> <li>Encourages efficient use of computation \u2014 more steps for harder inputs, fewer for easy ones.</li> <li>Regularized by a time penalty to avoid overthinking.</li> </ul>"},{"location":"deeplearning/5_attention/#universal-transformers","title":"Universal Transformers","text":"<ul> <li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li> <li>Shares parameters across layers \u2014 like an RNN unrolled over depth.</li> <li>Combine parallel self-attention + iterative refinement + ACT.</li> <li>Achieves better generalization and adaptive reasoning on sequence tasks.</li> </ul>"},{"location":"deeplearning/5_attention/#summary","title":"Summary","text":"<ul> <li>Attention = selective processing of relevant information.  </li> <li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li> <li>Explicit attention can be hard (sampled) or soft (differentiable).  </li> <li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li> <li>Transformers unify attention as the core mechanism \u2014 fully parallel, context-rich.  </li> <li>Adaptive computation gives flexibility in processing time and complexity.</li> </ul> <p>Takeaway: Selective attention and memory \u2014 biological inspirations \u2014 are now core architectural principles driving modern deep learning.</p>"},{"location":"deeplearning/6_gans/","title":"6. Generative Models and GANs","text":""},{"location":"deeplearning/6_gans/#1-overview-generative-models","title":"1. Overview: Generative Models","text":"<ul> <li>Goal: learn a model of the true data distribution \\(p^*(x)\\) from samples.</li> </ul>"},{"location":"deeplearning/6_gans/#types-of-generative-models","title":"Types of Generative Models","text":"<ol> <li>Explicit likelihood models \u2013 define tractable \\(p_\\theta(x)\\)</li> <li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li> <li> <p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p> </li> <li> <p>Implicit models \u2013 define sampling procedure, not explicit \\(p_\\theta(x)\\) </p> </li> <li>Examples: GANs, Moment Matching Networks.</li> </ol>"},{"location":"deeplearning/6_gans/#11-the-gan-idea","title":"1.1 The GAN Idea","text":"<ul> <li>Two-player minimax game:</li> <li>Generator (G): maps noise \\(z \\sim p(z)\\) to data space \\(G(z)\\).</li> <li> <p>Discriminator (D): classifies samples as real (from \\(p^*(x)\\)) or fake (\\(G(z)\\)).</p> </li> <li> <p>Objectives:    </p> </li> <li> <p>Interpretation:</p> </li> <li>\\(D\\) learns to distinguish real from fake.</li> <li>\\(G\\) learns to fool \\(D\\).</li> <li>Training reaches equilibrium when \\(p_G(x) = p^*(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#12-alternative-view-teacherstudent-analogy","title":"1.2 Alternative View \u2014 Teacher\u2013Student Analogy","text":"<ul> <li>Teacher (D): distinguishes real vs fake, providing feedback.</li> <li>Student (G): improves by making fake data look real.</li> <li>Cooperative interpretation of the adversarial process.</li> </ul>"},{"location":"deeplearning/6_gans/#13-gans-as-a-game","title":"1.3 GANs as a Game","text":"<ul> <li>Zero-sum, bi-level optimization \u2192 strong connection to game theory.</li> <li>GAN equilibrium = Nash equilibrium between \\(G\\) and \\(D\\).</li> <li>Training alternates between optimizing \\(D\\) and \\(G\\).</li> </ul> <p>Key Intuition: GANs learn by competition between a generator and discriminator rather than direct likelihood maximization.</p>"},{"location":"deeplearning/6_gans/#2-gan-objective-as-divergence-minimization","title":"2. GAN Objective as Divergence Minimization","text":"<ul> <li>Generative modeling often aims to minimize a distance or divergence between   the true data distribution \\(p^*(x)\\) and model distribution \\(p_G(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#21-kl-and-related-divergences","title":"2.1 KL and Related Divergences","text":"<ul> <li> <p>Maximum Likelihood Estimation (MLE):      \u2192 drives \\(p_\\theta\\) to assign high probability to observed data.</p> </li> <li> <p>But: implicit models (like GANs) don\u2019t have explicit likelihoods, so MLE can\u2019t be used directly.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#22-gan-as-jensenshannon-js-divergence-minimization","title":"2.2 GAN as Jensen\u2013Shannon (JS) Divergence Minimization","text":"<ul> <li> <p>If discriminator \\(D\\) is optimal:      Plugging into the GAN loss shows that the generator minimizes:      \u2192 GAN \u2248 JS divergence minimization.</p> </li> <li> <p>However, this relies on an optimal discriminator \u2014 not true in practice.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#23-limitations-of-kl-js-divergences","title":"2.3 Limitations of KL / JS Divergences","text":"<ul> <li>If \\(p_G\\) and \\(p^*\\) have non-overlapping support,   \u2192 no useful gradient signal (zero gradient problem).</li> <li>The density ratio \\(\\frac{p^*(x)}{p_G(x)}\\) becomes infinite where \\(p_G=0\\).</li> <li>Thus, GANs can fail to learn when supports are disjoint.</li> </ul>"},{"location":"deeplearning/6_gans/#24-alternative-distances-divergences","title":"2.4 Alternative Distances &amp; Divergences","text":""},{"location":"deeplearning/6_gans/#a-wasserstein-distance-earth-movers","title":"(a) Wasserstein Distance (Earth Mover\u2019s)","text":"<ul> <li>Measures minimal \u201ccost\u201d of moving probability mass:    </li> <li>Provides smooth, non-vanishing gradients even when supports don\u2019t overlap.</li> <li>WGAN: enforce 1-Lipschitz \\(D\\) via:</li> <li>weight clipping,</li> <li>gradient penalty (WGAN-GP),</li> <li>spectral normalization.</li> </ul>"},{"location":"deeplearning/6_gans/#b-mmd-maximum-mean-discrepancy","title":"(b) MMD (Maximum Mean Discrepancy)","text":"<ul> <li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):    </li> <li>MMD-GAN: learns kernel features \\(\\phi\\) jointly with \\(D\\).</li> </ul>"},{"location":"deeplearning/6_gans/#c-f-divergences","title":"(c) f-divergences","text":"<ul> <li>General framework using convex functions \\(f\\):    </li> <li>GAN training derived via variational lower bound on \\(D_f\\).</li> </ul>"},{"location":"deeplearning/6_gans/#25-practical-view","title":"2.5 Practical View","text":"<ul> <li>GANs are not pure divergence minimizers in practice:</li> <li>\\(D\\) not optimal \u2192 approximate divergence.</li> <li>Neural discriminator learns a smooth approximation to density ratio.</li> <li>Provides useful gradients even when the true divergence would fail.</li> </ul>"},{"location":"deeplearning/6_gans/#26-summary-table","title":"2.6 Summary Table","text":"Perspective Example Key Idea KL Divergence MLE, VAEs Explicit likelihoods JS Divergence Original GAN Adversarial training Wasserstein WGAN Smooth gradients MMD MMD-GAN Kernel mean embedding f-divergence f-GAN Variational bound family <p>Insight: GANs can be viewed as learning a neural divergence measure that provides a stable, informative training signal.</p>"},{"location":"deeplearning/6_gans/#3-evaluating-gans","title":"3. Evaluating GANs","text":"<ul> <li>Evaluating generative models is difficult \u2014 no single metric captures all aspects.</li> <li>Must assess:</li> <li>Sample quality (fidelity, realism)</li> <li>Diversity / generalization</li> <li>Representation learning (usefulness of learned features)</li> </ul>"},{"location":"deeplearning/6_gans/#31-why-not-log-likelihood","title":"3.1 Why Not Log-Likelihood?","text":"<ul> <li>GANs are implicit models \u2014 no tractable \\(p(x)\\).</li> <li>Estimating log-likelihood is expensive and unreliable.</li> <li>Hence: use feature-based or classifier-based proxies.</li> </ul>"},{"location":"deeplearning/6_gans/#32-inception-score-is","title":"3.2 Inception Score (IS)","text":"<ul> <li>Uses a pretrained Inception v3 classifier.</li> <li>Compares predicted label distributions of generated samples.</li> </ul> <p>Formula:  </p> <p>Intuition: - High-quality images \u2192 confident predictions (\\(p(y|x)\\) low entropy). - Diverse images \u2192 marginal label distribution \\(p(y)\\) high entropy.</p> <p>Properties: - Measures sample quality and diversity. - Correlates with human judgment. - Fails to capture intra-class variation or features beyond ImageNet classes.</p> <p>Higher is better.</p>"},{"location":"deeplearning/6_gans/#33-frechet-inception-distance-fid","title":"3.3 Fr\u00e9chet Inception Distance (FID)","text":"<ul> <li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li> </ul> <p>Formula:  </p> <p>where \\((\\mu_r, \\Sigma_r)\\) and \\((\\mu_g, \\Sigma_g)\\) are mean and covariance of real and generated data features.</p> <p>Properties: - Sensitive to mode dropping and artifacts. - Correlates strongly with human evaluation. - Lower is better. - Biased for small sample sizes \u2192 use KID (Kernel Inception Distance) for correction.</p>"},{"location":"deeplearning/6_gans/#34-overfitting-check-nearest-neighbours","title":"3.4 Overfitting Check \u2014 Nearest Neighbours","text":"<ul> <li>Compute nearest real images to generated samples in pretrained feature space.</li> <li>Helps detect memorization (copying training images).</li> </ul>"},{"location":"deeplearning/6_gans/#35-evaluation-depends-on-goal","title":"3.5 Evaluation Depends on Goal","text":"Goal Metric Example Measures Image quality FID, IS Fidelity &amp; diversity Representation learning Linear probe accuracy Feature usefulness Data generation Human evaluation Perceptual quality RL / control Policy reward Functional realism <p>Key Takeaway: Use multiple complementary metrics \u2014 quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>"},{"location":"deeplearning/6_gans/#4-the-gan-zoo","title":"4. The GAN Zoo","text":"<p>GANs have evolved rapidly \u2014 from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>"},{"location":"deeplearning/6_gans/#41-the-original-gan","title":"4.1 The Original GAN","text":"<ul> <li>First formulation of adversarial training.</li> <li>Architecture: simple multilayer perceptrons (MLPs).</li> <li>Trained on small images (e.g. 32\u00d732).  </li> <li>Ignored spatial structure (flattened pixels).  </li> <li>Introduced the minimax objective still used today.</li> </ul>"},{"location":"deeplearning/6_gans/#42-conditional-gan","title":"4.2 Conditional GAN","text":"<ul> <li> <p>Adds conditioning information \\(y\\) (e.g. class label or input image). </p> </li> <li> <p>Enables controlled generation \u2014 specify category or domain.   Examples:</p> </li> <li>Class-conditional image synthesis (e.g., \"generate a dog\").  </li> <li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li> </ul>"},{"location":"deeplearning/6_gans/#43-laplacian-gan","title":"4.3 Laplacian GAN","text":"<ul> <li>Generates images progressively, starting from low resolution.  </li> <li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li> <li>Fully convolutional \u2014 can produce arbitrarily large outputs.</li> <li>Improves high-res synthesis through multi-scale structure.</li> </ul>"},{"location":"deeplearning/6_gans/#44-deep-convolutional-gan","title":"4.4 Deep Convolutional GAN","text":"<ul> <li>Replaces MLPs with deep convnets for both \\(G\\) and \\(D\\).</li> <li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li> <li>Enables smooth interpolation in latent space:</li> <li>\\(G(z_1)\\) \u2192 \\(G(\\frac{1}{2}(z_1 + z_2))\\) \u2192 \\(G(z_2)\\) produces semantically meaningful transitions.</li> <li>Latent space exhibits semantic arithmetic (e.g. \u201cman + glasses \u2013 woman\u201d).</li> </ul>"},{"location":"deeplearning/6_gans/#45-spectrally-normalized-gan","title":"4.5 Spectrally Normalized GAN","text":"<ul> <li> <p>Enforces 1-Lipschitz constraint on \\(D\\) via spectral normalization:      where \\(\\sigma_{\\max}(W)\\) is the largest singular value.</p> </li> <li> <p>Stabilizes training and improves generalization.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#46-projection-discriminator","title":"4.6 Projection Discriminator","text":"<ul> <li> <p>Adds class embedding projection inside \\(D\\):    where \\(v_y\\) is the embedding for class \\(y\\).</p> </li> <li> <p>Theoretically consistent probabilistic discriminator formulation.  </p> </li> <li>Strong empirical results on class-conditional image synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#47-self-attention-gan","title":"4.7 Self-Attention GAN","text":"<ul> <li>Introduces self-attention layers to capture long-range dependencies.  </li> <li>Improves global structure and coherence in generated images.</li> <li>Inspired by Transformer attention.</li> </ul>"},{"location":"deeplearning/6_gans/#48-biggan","title":"4.8 BigGAN","text":"<ul> <li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li> <li>Key ingredients:</li> <li>Hinge loss for \\(D\\) </li> <li>Spectral normalization  </li> <li>Self-attention  </li> <li>Projection discriminator  </li> <li>Orthogonal regularization  </li> <li>Skip connections from noise  </li> <li>Shared class embeddings  </li> <li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li> </ul>"},{"location":"deeplearning/6_gans/#49-logan","title":"4.9 LOGAN","text":"<ul> <li>Introduces latent optimization \u2014 optimize \\(z\\) via gradient updates to improve adversarial dynamics.  </li> <li>Uses natural gradient descent in latent space.  </li> <li>Yields higher FID/IS improvements over BigGAN.</li> </ul>"},{"location":"deeplearning/6_gans/#410-progressive-gan","title":"4.10 Progressive GAN","text":"<ul> <li>Trains from low to high resolution (4\u00d74 \u2192 8\u00d78 \u2192 16\u00d716 \u2026).  </li> <li>Each stage adds new layers to \\(G\\) and \\(D\\).  </li> <li>Dramatically improves stability and image quality (especially faces).</li> </ul>"},{"location":"deeplearning/6_gans/#411-stylegan","title":"4.11 StyleGAN","text":"<ul> <li>Adds style-based generator architecture:</li> <li>Latent vector \\(z\\) transformed by MLP to intermediate \\(w\\).</li> <li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li> <li> <p>Injects per-pixel noise for local details.</p> </li> <li> <p>Learns disentangled representations \u2014 global attributes (style) vs local (texture).</p> </li> </ul>"},{"location":"deeplearning/6_gans/#412-takeaways","title":"4.12 Takeaways","text":"<ul> <li>GAN progress driven by:</li> <li>Better architectures (Conv, Attention, Progressive, Style-based)</li> <li>Normalization &amp; regularization</li> <li>Stability techniques</li> <li>Large-scale training</li> </ul> <p>Trend: From small MLPs \u2192 Conv architectures \u2192 Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>"},{"location":"deeplearning/6_gans/#5-representation-learning-with-gans","title":"5. Representation Learning with GANs","text":"<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>"},{"location":"deeplearning/6_gans/#51-motivation","title":"5.1 Motivation","text":"<ul> <li>GANs implicitly learn latent spaces that capture high-level semantics.</li> <li>Exploring or constraining this latent space enables unsupervised representation learning.</li> </ul>"},{"location":"deeplearning/6_gans/#52-evidence-from-dcgan","title":"5.2 Evidence from DCGAN","text":"<ul> <li>DCGAN latent vectors encode meaningful directions:</li> <li>Smooth interpolation between points \u2192 semantic transformations.</li> <li>Linear arithmetic in latent space (e.g., smiling woman \u2013 woman + man \u2192 smiling man).</li> <li>Suggests disentangled feature representations emerge naturally.</li> </ul>"},{"location":"deeplearning/6_gans/#53-infogan","title":"5.3 InfoGAN","text":"<ul> <li>Extends GAN with information maximization objective:</li> <li>Encourages some latent codes \\(c\\) to be interpretable and disentangled.</li> </ul> <p>Objective:  where \\(I(c; G(z, c))\\) is mutual information between latent code and generated output.</p> <ul> <li>Adds an auxiliary network to infer \\(c\\) from \\(G(z, c)\\).</li> <li>Learns to associate:</li> <li>Discrete codes \u2192 categories (digits, shapes)</li> <li>Continuous codes \u2192 attributes (rotation, scale)</li> </ul>"},{"location":"deeplearning/6_gans/#54-ali-bigan","title":"5.4 ALI / BiGAN","text":"<ul> <li>Adds an encoder \\(E(x)\\) mapping real data to latent space.</li> <li> <p>Joint discriminator distinguishes pairs:    </p> </li> <li> <p>At equilibrium:</p> </li> <li> <p>\\(E\\) and \\(G\\) become approximate inverses:</p> <ul> <li>\\(x \\approx G(E(x))\\)</li> <li>\\(z \\approx E(G(z))\\)</li> </ul> </li> <li> <p>Enables inference and representation learning simultaneously.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#55-bigbigan","title":"5.5 BigBiGAN","text":"<ul> <li>Scales BiGAN to BigGAN architecture.</li> <li>Uses large-scale encoders (\\(E\\)) with ResNet blocks.</li> <li>Learns strong unsupervised representations competitive with self-supervised models.</li> </ul> <p>Observations: - Reconstructions \\(G(E(x))\\) preserve semantic content, not exact pixels. - Encoder features yield high ImageNet classification accuracy after linear probing.</p>"},{"location":"deeplearning/6_gans/#56-summary","title":"5.6 Summary","text":"Model Key Idea Outcome DCGAN Implicitly semantic latent space Interpolations meaningful InfoGAN Maximize info between codes and outputs Disentangled features BiGAN / ALI Add encoder, joint training Bidirectional mapping BigBiGAN Large-scale BiGAN Competitive unsupervised features <p>Key Insight: GANs not only generate, but also encode \u2014 their latent structure can act as a rich, learned representation space.</p>"},{"location":"deeplearning/6_gans/#6-gans-for-other-modalities-and-problems","title":"6. GANs for Other Modalities and Problems","text":"<p>GANs extend far beyond images \u2014 used for translation, audio, video, RL, and even art.</p>"},{"location":"deeplearning/6_gans/#61-image-to-image-translation","title":"6.1 Image-to-Image Translation","text":""},{"location":"deeplearning/6_gans/#a-pix2pix","title":"(a) Pix2Pix","text":"<ul> <li>Conditional GAN trained on paired datasets \\((x, y)\\).</li> <li>Learns deterministic mapping between domains (e.g., edges \u2192 photos).</li> <li>Loss combines adversarial term + L1 reconstruction:    </li> </ul>"},{"location":"deeplearning/6_gans/#b-cyclegan","title":"(b) CycleGAN","text":"<ul> <li>Unpaired domain translation \u2014 no 1:1 correspondence.</li> <li>Uses cycle consistency:</li> <li>\\(x \\in A \\to G_B(x) \\to F_A(G_B(x)) \\approx x\\)</li> <li>Enforces invertibility between domains.</li> <li>Enables tasks like horse \u2194 zebra, summer \u2194 winter.</li> </ul>"},{"location":"deeplearning/6_gans/#62-audio-synthesis","title":"6.2 Audio Synthesis","text":""},{"location":"deeplearning/6_gans/#a-wavegan","title":"(a) WaveGAN","text":"<ul> <li>Adapts convolutional GANs to 1D waveforms.</li> <li>Fully unsupervised raw-audio synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#b-melgan","title":"(b) MelGAN","text":"<ul> <li>Conditional GAN trained to generate mel-spectrogram waveforms.</li> <li>Used in text-to-speech (GAN-TTS).</li> </ul>"},{"location":"deeplearning/6_gans/#c-gan-tts","title":"(c) GAN-TTS","text":"<ul> <li>High-fidelity speech synthesis model.</li> <li>Achieves human-like audio quality via adversarial losses.</li> </ul>"},{"location":"deeplearning/6_gans/#63-video-synthesis-prediction","title":"6.3 Video Synthesis &amp; Prediction","text":"<ul> <li>GANs extended to spatiotemporal data:</li> <li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li> <li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li> <li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li> </ul>"},{"location":"deeplearning/6_gans/#64-gans-in-reinforcement-learning-imitation-control","title":"6.4 GANs in Reinforcement Learning (Imitation &amp; Control)","text":"<ul> <li>GAIL (Ho &amp; Ermon, 2016): Generative Adversarial Imitation Learning </li> <li>Discriminator distinguishes expert vs policy trajectories.</li> <li>Generator = policy network optimizing to mimic experts.</li> </ul>"},{"location":"deeplearning/6_gans/#65-creative-applied-uses","title":"6.5 Creative &amp; Applied Uses","text":"<ul> <li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li> <li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li> <li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li> <li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li> <li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li> </ul>"},{"location":"deeplearning/6_gans/#66-summary","title":"6.6 Summary","text":"Domain Example Key Idea Paired image translation Pix2Pix Conditional GAN + L1 loss Unpaired translation CycleGAN Cycle consistency Audio MelGAN, WaveGAN Conditional waveform generation Video DVD-GAN, TGAN-v2 Temporal adversarial modeling RL / Imitation GAIL Adversarial trajectory matching Art / Creativity GauGAN, SPIRAL Adversarial synthesis and style transfer <p>Insight: Adversarial learning generalizes across domains \u2014 GANs serve as a universal generator\u2013critic framework for structured data.</p>"},{"location":"deeplearning/7_unsuper/","title":"7. Unsupervised and Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#1-what-is-unsupervised-learning","title":"1. What is Unsupervised Learning?","text":""},{"location":"deeplearning/7_unsuper/#definition","title":"Definition","text":"<ul> <li>Goal: discover structure in data without explicit labels or rewards.  </li> <li>Learns a compact, informative representation of input data.</li> </ul> Learning Type Goal Supervision Supervised Map inputs \u2192 labels Requires labeled data Reinforcement Learn actions maximizing future reward Requires reward signal Unsupervised Find hidden structure No labels or rewards"},{"location":"deeplearning/7_unsuper/#core-ideas","title":"Core Ideas","text":"<ul> <li>Model latent structure or relationships between observations.  </li> <li>Examples:</li> <li>Clustering: group similar data points.  </li> <li>Dimensionality reduction: project data to low-dimensional latent space.  </li> <li>Manifold learning / disentangling: uncover independent factors of variation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#evaluation-challenges","title":"Evaluation Challenges","text":"<p>How do we know if unsupervised learning worked?</p> <ul> <li>Ambiguity of structure: multiple valid clusterings possible.   e.g., cluster by leg count, arm number, or height in robot dataset.</li> <li>Metrics depend on downstream use:   useful representations should improve data efficiency, generalization, or transfer.</li> </ul>"},{"location":"deeplearning/7_unsuper/#classic-methods","title":"Classic Methods","text":"<ul> <li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li> <li>ICA (Independent Component Analysis): separates statistically independent components.  </li> <li>Modern goal: move beyond orthogonality \u2192 learn disentangled factors.</li> </ul>"},{"location":"deeplearning/7_unsuper/#summary","title":"Summary","text":"<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself \u2014 forming the foundation for representation learning.</p>"},{"location":"deeplearning/7_unsuper/#2-why-is-unsupervised-learning-important","title":"2. Why is Unsupervised Learning Important?","text":""},{"location":"deeplearning/7_unsuper/#21-historical-context-of-representation-learning","title":"2.1 Historical Context of Representation Learning","text":"Era Key Milestone Approach 1950s\u20132000s Arthur Samuel (1959): Machine Learning coined Feature engineering, clustering 2000s Kernel methods (Hofmann et al., 2008) Hand-crafted similarity functions 2006 Hinton &amp; Salakhutdinov: RBMs &amp; Autoencoders Layer-wise unsupervised pretraining 2012 Krizhevsky et al.: AlexNet End-to-end supervised learning dominates <ul> <li>Progress came from more data, deeper models, and better hardware \u2014 but not necessarily more efficient learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#22-limitations-of-purely-supervised-learning","title":"2.2 Limitations of Purely Supervised Learning","text":"<p>Supervised models are: - Data inefficient \u2014 need millions of labeled samples. - Brittle \u2014 vulnerable to adversarial perturbations. - Poor at transfer \u2014 struggle with new domains or tasks. - Lack common sense \u2014 limited abstraction and reasoning.</p>"},{"location":"deeplearning/7_unsuper/#23-evidence-of-current-gaps","title":"2.3 Evidence of Current Gaps","text":"Challenge Example Reference Data efficiency Learning from few examples Lake et al. (2017) Robustness Adversarial examples, brittle decisions Goodfellow et al. (2015) Generalization CoinRun, DMLab-30 Cobbe (2018), DeepMind Transfer Schema Networks Kansky et al. (2017) Common sense Conceptual reasoning Lake et al. (2015)"},{"location":"deeplearning/7_unsuper/#24-why-unsupervised-learning-matters","title":"2.4 Why Unsupervised Learning Matters","text":"<ul> <li>Enables data-efficient adaptation to new tasks.</li> <li>Provides robust, generalizable features.</li> <li>Promotes transfer learning by separating invariant factors.</li> <li>Encourages abstract reasoning and causal understanding.</li> </ul>"},{"location":"deeplearning/7_unsuper/#25-towards-general-ai","title":"2.5 Towards General AI","text":"<p>Unsupervised learning provides shared representations enabling: - Rapid multi-task adaptation. - Reuse across vision, language, and control. - Reduced supervision in real-world learning.</p> <p>Summary: Unsupervised representation learning addresses the core limits of current AI \u2014 aiming for data efficiency, robustness, generalization, transfer, and common sense.</p>"},{"location":"deeplearning/7_unsuper/#3-what-makes-a-good-representation","title":"3. What Makes a Good Representation?","text":"<p>A representation is an internal model of the world \u2014 an abstraction that makes reasoning and prediction efficient.</p>"},{"location":"deeplearning/7_unsuper/#31-what-is-a-representation","title":"3.1 What is a Representation?","text":"<p>\u201cA formal system for making explicit certain entities or types of information, together with a specification of how the system does this.\u201d</p> <ul> <li>Represents information about the world in a way useful for computation.  </li> <li>Not about a single feature, but the geometry or manifold shape in representational space.</li> </ul>"},{"location":"deeplearning/7_unsuper/#32-why-representation-form-matters","title":"3.2 Why Representation Form Matters","text":"<ul> <li>Determines which computations are easy.  </li> <li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li> </ul>"},{"location":"deeplearning/7_unsuper/#33-desirable-properties","title":"3.3 Desirable Properties","text":"Property Description Intuition Untangling Simplifies complex input manifolds Enables linear decoding Attention Allows selective focus on relevant factors Supports task-specific filtering Clustering Groups similar experiences together Facilitates generalization Latent Information Encodes hidden or inferred causes Predicts unobserved aspects Compositionality Builds complex concepts from simple parts Enables open-ended reasoning"},{"location":"deeplearning/7_unsuper/#34-information-bottleneck-principle","title":"3.4 Information Bottleneck Principle","text":"<ul> <li>Good representations compress inputs while preserving information about outputs.    </li> <li>Encourages minimal sufficient representations \u2014 compact yet predictive.</li> </ul>"},{"location":"deeplearning/7_unsuper/#4-evaluating-the-merit-of-a-representation","title":"4. Evaluating the Merit of a Representation","text":"<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>"},{"location":"deeplearning/7_unsuper/#41-the-evaluation-challenge","title":"4.1 The Evaluation Challenge","text":"<ul> <li>No single metric defines a \u201cgood\u201d representation.</li> <li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li> </ul> <p>Representations should enable: - Data efficiency \u2014 learn new tasks from few examples. - Robustness \u2014 resist noise or perturbations. - Generalization \u2014 perform well on new data. - Transfer \u2014 reuse knowledge in new settings. - Common sense \u2014 support reasoning and abstraction.</p>"},{"location":"deeplearning/7_unsuper/#42-example-evaluating-representations-via-symmetries","title":"4.2 Example: Evaluating Representations via Symmetries","text":"<p>Let: - \\(W\\) = world space - \\(Z\\) = representational space - \\(G = G_x \\times G_y \\times G_c\\) = group of transformations (e.g., position, color)</p> <p>A good representation \\(f: W \\rightarrow Z\\) should satisfy:  </p> <p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space \u2192 equivariance.</p>"},{"location":"deeplearning/7_unsuper/#43-desirable-evaluation-criteria","title":"4.3 Desirable Evaluation Criteria","text":"Criterion Desired Property Example / Metric Equivariance Transformations map consistently Translation \u2192 shift in latent Compositionality Combine factors to form new concepts Modular latent factors Metric structure Smooth distances reflect similarity \\(L_2\\), cosine Attention Selectively focus on task-relevant parts Masking or gating mechanisms Symmetries Invariance to irrelevant transformations Rotation, scale invariance"},{"location":"deeplearning/7_unsuper/#44-downstream-evaluation-tasks","title":"4.4 Downstream Evaluation Tasks","text":"Evaluation Setting Example Task Reference Perception / Control Predict object color or position Gens &amp; Domingos, Deep Symmetry Networks (2014) Robustness Classify images under adversarial noise Gowal et al., 2019 Sequential Attention Learn task-focused vision Zoran et al., 2020 Transfer / RL Zero-shot navigation (DARLA) Higgins et al., ICML 2017 Lifelong Learning Maintain latent structure over domains Achille et al., NeurIPS 2018 Reasoning / Imagination Compositional concept inference Lake et al., Science 2015; Higgins et al., ICLR 2018"},{"location":"deeplearning/7_unsuper/#45-why-evaluation-matters","title":"4.5 Why Evaluation Matters","text":"<p>A good representation supports simple mappings to downstream tasks: - Linear classifiers for vision tasks (e.g., color or position recognition). - Efficient policy learning in RL with fewer samples. - Abstract reasoning and imagination \u2014 \u201cIf rainbow elephants live in big cities, can we expect one in London?\u201d</p>"},{"location":"deeplearning/7_unsuper/#5-representation-learning-techniques","title":"5. Representation Learning Techniques","text":"<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches \u2014 all aiming to extract structure from data without labels.</p>"},{"location":"deeplearning/7_unsuper/#51-categories-of-methods","title":"5.1 Categories of Methods","text":"Category Core Idea Typical Example Generative Modeling Learn \\(p(x)\\) or a model that can reconstruct data VAE, \u03b2-VAE, MONet, GQN, GANs Contrastive Learning Learn by discriminating similar vs dissimilar samples CPC, SimCLR, word2vec Self-Supervised Learning Design pretext tasks that predict missing or reordered parts BERT, Colorization, Context Prediction"},{"location":"deeplearning/7_unsuper/#52-generative-modeling","title":"5.2 Generative Modeling","text":""},{"location":"deeplearning/7_unsuper/#521-motivation","title":"5.2.1 Motivation","text":"<ul> <li>Goal: learn the underlying data distribution \\(p(x)\\) to reveal hidden structure and causal factors.  </li> <li>Unsupervised generative modeling captures common regularities in data \u2014 enabling representation learning, synthesis, and reasoning.  </li> <li>Instead of directly memorizing examples, the model learns a probabilistic process that could have generated them.</li> </ul> <p>Generative models explain the data by learning how it might have arisen.</p>"},{"location":"deeplearning/7_unsuper/#522-from-maximum-likelihood-to-latent-variable-models","title":"5.2.2 From Maximum Likelihood to Latent Variable Models","text":""},{"location":"deeplearning/7_unsuper/#maximum-likelihood-principle","title":"Maximum Likelihood Principle","text":"<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:  where \\(p^*(x)\\) is the true data distribution and \\(p_\\theta(x)\\) is the model.</p>"},{"location":"deeplearning/7_unsuper/#latent-variable-formulation","title":"Latent Variable Formulation","text":"<ul> <li>Assume data arises from hidden (latent) variables \\(z\\):    </li> <li>Here:</li> <li>\\(p(z)\\) \u2014 prior over latent variables (e.g., \\(\\mathcal{N}(0, I)\\))  </li> <li>\\(p_\\theta(x|z)\\) \u2014 likelihood or decoder mapping latent codes to data</li> </ul> <p>This defines a latent variable model: the data-generating process maps from a low-dimensional latent space to the observed space.</p>"},{"location":"deeplearning/7_unsuper/#523-inference-in-latent-variable-models","title":"5.2.3 Inference in Latent Variable Models","text":"<p>Goal: infer the posterior  to identify which latent factors \\(z\\) most likely generated observation \\(x\\).</p> <ul> <li>Intuition:   Recover the underlying causes that explain the data \u2014 along with uncertainty estimates.</li> <li>Problem:   Computing \\(p(z|x)\\) is often intractable, since \\(p_\\theta(x)\\) involves integrating over all \\(z\\).   \u2192 We must approximate inference using neural networks.</li> </ul> <p>Thus, generative models combine:</p> <ul> <li>Generation: \\(z \\rightarrow x\\) (decode latent causes into data)</li> <li>Inference: \\(x \\rightarrow z\\) (encode data into latent causes)</li> </ul>"},{"location":"deeplearning/7_unsuper/#524-variational-autoencoders-vaes","title":"5.2.4 Variational Autoencoders (VAEs)","text":"<p>To make inference tractable, VAEs introduce an approximate posterior \\(q_\\phi(z|x)\\) and optimize a variational bound on the likelihood:</p>"},{"location":"deeplearning/7_unsuper/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"\\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}[q_\\phi(z|x)\\,||\\,p(z)] \\]"},{"location":"deeplearning/7_unsuper/#terms","title":"Terms","text":"<ol> <li> <p>Reconstruction term    Encourages the model to faithfully reproduce the input from its latent code.</p> </li> <li> <p>KL divergence term    Regularizes the latent posterior to match the prior \u2014 ensuring smoothness and preventing overfitting.</p> </li> </ol>"},{"location":"deeplearning/7_unsuper/#neural-implementation","title":"Neural Implementation","text":"<ul> <li>Encoder \\(q_\\phi(z|x)\\): approximates inference (maps data \u2192 latent code).  </li> <li>Decoder \\(p_\\theta(x|z)\\): generates data from the latent space (latent \u2192 data).  </li> <li>Both are parameterized by deep neural networks.</li> </ul> <p>Reparameterization trick (Kingma &amp; Welling, 2014):  enables backpropagation through stochastic latent sampling.</p>"},{"location":"deeplearning/7_unsuper/#why-vaes-matter","title":"Why VAEs Matter","text":"<ul> <li>Provide continuous, structured latent spaces capturing generative factors.  </li> <li>Support smooth interpolation and semantic manipulation.  </li> <li>Foundation for disentangled and interpretable representation learning (e.g., \u03b2-VAE).  </li> <li>Bridge probabilistic modeling with deep learning.</li> </ul> <p>VAEs turn probabilistic inference into a scalable neural optimization problem \u2014 the cornerstone of modern generative representation learning.</p>"},{"location":"deeplearning/7_unsuper/#524-vae","title":"5.2.4 \u03b2-VAE","text":"<ul> <li>Adds weight \u03b2 to KL term:    </li> <li>Encourages disentangled latent factors (position, shape, rotation, color).</li> <li> <p>Provides interpretable, semantically meaningful representations.</p> </li> <li> <p>DARLA (Higgins et al., 2017): \u03b2-VAE for reinforcement learning \u2192 improved transfer and sim2real generalization.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#525-sequential-and-layered-models","title":"5.2.5 Sequential and Layered Models","text":"<p>ConvDRAW (Gregor et al., 2016) - Sequential VAE with recurrent refinement. - Models temporal and spatial dependencies.</p> <p>MONet (Burgess et al., 2019) - Attention-based scene decomposition. - Each latent corresponds to one object \u2192 compositional representations. - Enables object-centric reasoning and RL transfer.</p> <p>GQN (Eslami et al., 2018) - Generative Query Networks: learn neural scene representations. - Given partial observations, predict unseen viewpoints (3D reasoning).</p> <p>VQ-VAE (van den Oord et al., 2017) - Learns discrete latent variables via vector quantization. - Enables hierarchical or symbolic structure. - Useful for speech, images, and video. -</p>"},{"location":"deeplearning/7_unsuper/#526-gans-goodfellow-et-al-2014","title":"5.2.6 GANs (Goodfellow et al., 2014)","text":"<ul> <li>Implicit generative models \u2014 learn by adversarial game:</li> <li>Generator creates samples.</li> <li>Discriminator provides learning signal (no reconstruction loss).</li> <li>BigBiGAN (Donahue et al., 2019):</li> <li>Adds encoder for inference.</li> <li>Learns rich, high-level representations \u2192 SOTA semi-supervised performance on ImageNet.</li> </ul>"},{"location":"deeplearning/7_unsuper/#527-large-scale-generative-models","title":"5.2.7 Large-Scale Generative Models","text":"<ul> <li>GPT (Radford et al., 2019):  </li> <li>Large transformer trained via language modeling.</li> <li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li> </ul>"},{"location":"deeplearning/7_unsuper/#53-contrastive-learning","title":"5.3 Contrastive Learning","text":""},{"location":"deeplearning/7_unsuper/#core-idea","title":"Core Idea","text":"<ul> <li>No need to model \\(p(x)\\) explicitly.</li> <li>Learn representations that maximize mutual information between related samples.</li> </ul>"},{"location":"deeplearning/7_unsuper/#531-word2vec-mikolov-et-al-2013","title":"5.3.1 word2vec (Mikolov et al., 2013)","text":"<ul> <li>Predict context words given a target word.  </li> <li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li> <li>Learns semantic embeddings; supports few-shot translation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018","title":"5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)","text":"<ul> <li>Maximize mutual information between current representation and future observations.  </li> <li>Trains a classifier to distinguish real future samples from negatives.  </li> <li> <p>Learns features useful across modalities (vision, speech).</p> </li> <li> <p>Data-efficient Image Recognition (H\u00e9naff et al., 2019):   contrastive features outperform pixel-level training in low-data regimes.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#533-simclr-chen-et-al-2020","title":"5.3.3 SimCLR (Chen et al., 2020)","text":"<ul> <li>Simple, scalable contrastive framework:</li> <li>Generate two augmented views of the same image.</li> <li>Maximize agreement via contrastive loss (NT-Xent).</li> <li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li> <li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#54-self-supervised-learning","title":"5.4 Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#idea","title":"Idea","text":"<ul> <li>Design pretext tasks that use natural structure in data as supervision.  </li> <li>Representations are deterministic and transferable to new tasks.</li> </ul>"},{"location":"deeplearning/7_unsuper/#541-examples","title":"5.4.1 Examples","text":"Task Description Reference Colorization Predict color from grayscale image Zhang et al., 2016 Context Prediction Predict position of image patches Doersch et al., 2015 Sequence Sorting Predict correct frame order in videos Lee et al., 2017 BERT (Devlin et al., 2019) Masked language modeling + next sentence prediction Revolutionized NLP"},{"location":"deeplearning/7_unsuper/#542-key-benefits","title":"5.4.2 Key Benefits","text":"<ul> <li>Requires no labels \u2014 just structure in data.  </li> <li>Produces general features useful for:</li> <li>Semi-supervised classification  </li> <li>Transfer learning  </li> <li>Downstream reasoning tasks</li> </ul>"},{"location":"deeplearning/7_unsuper/#55-design-principles","title":"5.5 Design Principles","text":"Consideration Desired Property Modality Align architecture with data type (image, text, audio) Task Design Choose pretext that aligns with useful features Consistency Maintain temporal/spatial coherence Discrete + Continuous Latents Enable symbolic and continuous reasoning Adaptivity Representations should evolve with experience <p>Summary: Unsupervised representation learning uses three complementary lenses: - Generative \u2192 model what the world looks like. - Contrastive \u2192 learn what is similar or different. - Self-supervised \u2192 create pseudo-tasks that reveal structure. Together, they aim for data-efficient, transferable, and interpretable representations.</p>"},{"location":"deeplearning/8_latentvariables/","title":"8. Latent Variable Models","text":""},{"location":"deeplearning/8_latentvariables/#1-generative-modelling","title":"1. Generative Modelling","text":""},{"location":"deeplearning/8_latentvariables/#11-what-are-generative-models","title":"1.1 What Are Generative Models?","text":"<ul> <li>Probabilistic models of high-dimensional data.</li> <li>Describe how observations are generated from underlying processes.</li> <li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#12-why-they-matter","title":"1.2 Why They Matter","text":"<p>Generative models can: - Estimate data density (detect outliers, anomalies). - Enable compression (encode \u2192 decode). - Map between domains (e.g., translation, text-to-speech). - Support model-based RL (predict future states). - Learn representations from raw data. - Improve understanding of data structure.</p>"},{"location":"deeplearning/8_latentvariables/#13-types-of-generative-models-in-deep-learning","title":"1.3 Types of Generative Models in Deep Learning","text":""},{"location":"deeplearning/8_latentvariables/#a-autoregressive-models","title":"(a) Autoregressive Models","text":"<p>Model joint distribution via chain rule:  </p> <p>Trained with maximum likelihood</p> <p>Examples:</p> <ul> <li>RNN/Transformer LMs  </li> <li>NADE  </li> <li>PixelCNN / WaveNet</li> </ul> <p>Pros:</p> <ul> <li>Easy training (max. likelihood).</li> <li>No sampling during training.</li> </ul> <p>Cons:</p> <ul> <li>Slow generation (sequential).</li> <li>Often capture local structure better than global structure.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#b-latent-variable-models","title":"(b) Latent Variable Models","text":"<p>Introduce an unobserved latent variable \\(z\\):</p> <ul> <li>Prior: \\(p(z)\\) </li> <li>Likelihood: \\(p_\\theta(x\\mid z)\\) </li> </ul> <p>Joint:  </p> <p>Pros</p> <ul> <li>Flexible &amp; interpretable  </li> <li>Natural for representation learning  </li> <li>Fast generation  </li> </ul> <p>Cons - Require approximate inference unless specially designed (e.g., invertible models).</p>"},{"location":"deeplearning/8_latentvariables/#c-implicit-models-gans","title":"(c) Implicit Models (GANs)","text":"<ul> <li>Define a generator \\(G(z)\\) with no explicit likelihood.</li> <li>Trained adversarially using a discriminator.</li> </ul> <p>Pros</p> <ul> <li>Extremely realistic samples  </li> <li>Fast sampling  </li> </ul> <p>Cons</p> <ul> <li>Cannot evaluate \\(p(x)\\) </li> <li>Mode collapse  </li> <li>Training instability  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-latent-variable-models-inference","title":"2. Latent Variable Models &amp; Inference","text":""},{"location":"deeplearning/8_latentvariables/#21-what-is-a-latent-variable-model-lvm","title":"2.1 What is a Latent Variable Model (LVM)?","text":"<p>A latent variable model introduces an unobserved variable \\(z\\) that explains the observed data \\(x\\).</p> <p>Model components:</p> <ul> <li>Prior over latent variables:  </li> </ul> <p> </p> <ul> <li>Likelihood / decoder mapping latent \u2192 observation:  </li> </ul> <p> </p> <p>Joint distribution:</p> \\[ p_\\theta(x, z) = p_\\theta(x \\mid z)\\,p(z) \\] <p>Marginal likelihood (what we want to maximize when training):</p> \\[ p_\\theta(x) = \\int p_\\theta(x \\mid z)\\,p(z)\\,dz \\]"},{"location":"deeplearning/8_latentvariables/#22-intuition-latents-as-explanations","title":"2.2 Intuition: Latents as \u201cExplanations\u201d","text":"<ul> <li> <p>A particular value of \\(z\\) is a hypothesis about hidden causes that produced \\(x\\).</p> </li> <li> <p>Generation = sample latent \u2192 map it to data:    </p> </li> </ul> <p>Most of the article focuses on the inverse of this: recovering \\(z\\) from \\(x\\).</p>"},{"location":"deeplearning/8_latentvariables/#23-what-is-inference","title":"2.3 What Is Inference?","text":"<p>Inference means computing the posterior:  </p> <p>Why it matters:</p> <ul> <li>Explains the observation (which latents likely produced it?)</li> <li>Needed inside maximum-likelihood training   (the gradient depends on the posterior!)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#24-inference-requires-the-marginal-likelihood","title":"2.4 Inference Requires the Marginal Likelihood","text":"<p>To compute the posterior, we need:  This integral is often intractable.</p> <p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>"},{"location":"deeplearning/8_latentvariables/#25-example-mixture-of-gaussians","title":"2.5 Example: Mixture of Gaussians","text":"<p>Model:</p> <ul> <li>Choose cluster \\(k\\) </li> <li>Sample \\(x\\) from Gaussian for that cluster</li> </ul> <p>Posterior:  </p> <p>This model is tractable because:</p> <ul> <li>Finite number of discrete states  </li> <li>Closed-form posterior</li> </ul>"},{"location":"deeplearning/8_latentvariables/#26-the-need-for-inference-in-learning","title":"2.6 The Need for Inference in Learning","text":""},{"location":"deeplearning/8_latentvariables/#maximum-likelihood-as-the-core-training-principle","title":"Maximum Likelihood as the Core Training Principle","text":"<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters \\(\\theta\\) that make the observed training data as probable as possible:</p> \\[ \\theta^* = \\arg\\max_\\theta \\sum_{i} \\log p_\\theta(x^{(i)}) \\] <p>For latent variable models, the marginal likelihood is:  This integral is rarely tractable, which makes direct maximization difficult.</p>"},{"location":"deeplearning/8_latentvariables/#why-optimization-is-hard-in-latent-variable-models","title":"Why Optimization Is Hard in Latent Variable Models","text":"<ul> <li>The log-likelihood involves an integral (or sum) over the latent variables \\(z\\).  </li> <li>Because this integral usually has no closed form, we must use iterative optimization methods.</li> </ul> <p>Common approaches:</p> <ol> <li>Gradient-based optimization (e.g., gradient descent)</li> <li>Expectation-Maximization (EM)</li> </ol> <p>Below we explain why inference (computing the posterior \\(p_\\theta(z \\mid x)\\)) is essential for both.</p>"},{"location":"deeplearning/8_latentvariables/#261-gradient-based-learning-requires-the-posterior","title":"2.6.1 Gradient-Based Learning Requires the Posterior","text":"<p>Using the identity:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)}[\\nabla_\\theta \\log p_\\theta(x, z)] \\] <p>Differentiate the log-marginal</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{\\nabla_\\theta p_\\theta(x)}{p_\\theta(x)}\\] <p>Using: \\(p_\\theta(x)=\\int p_\\theta(x,z)\\,dz,\\)</p> <p>differentiate under the integral:</p> \\[\\nabla_\\theta p_\\theta(x) = \\nabla_\\theta \\int p_\\theta(x,z)\\,dz = \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Combine:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Apply the log-derivative identity</p> <p>The identity:</p> \\[\\nabla_\\theta p_\\theta(x,z) = p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\] <p>Substitute:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Recognize the posterior</p> <p>Bayes\u2019 rule:</p> \\[p_\\theta(z\\mid x) = \\frac{p_\\theta(x,z)}{p_\\theta(x)}\\] <p>Substitute into the integral:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int \\frac{p_\\theta(x,z)}{p_\\theta(x)} \\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>This becomes:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int p_\\theta(z\\mid x)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Write as an expectation</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)} \\left[ \\nabla_\\theta \\log p_\\theta(x,z) \\right]\\] <p>To compute the gradient of the marginal likelihood, we must take an expectation under the posterior \\(p_\\theta(z\\mid x)\\).</p> <p>So:</p> <ul> <li>We cannot compute \\(\\nabla_\\theta \\log p_\\theta(x)\\) without knowing the posterior.</li> <li>Inference becomes part of every gradient step.</li> <li>If inference is intractable \u2192 gradient is intractable.</li> </ul> <p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>"},{"location":"deeplearning/8_latentvariables/#262-expectation-maximization-em-also-requires-inference","title":"2.6.2 Expectation-Maximization (EM) Also Requires Inference","text":"<p>EM is an alternative to gradient descent for maximizing likelihood.</p>"},{"location":"deeplearning/8_latentvariables/#e-step","title":"E-step:","text":"<p>Compute (or approximate) the posterior:  This assigns responsibilities to each latent configuration.</p>"},{"location":"deeplearning/8_latentvariables/#m-step","title":"M-step:","text":"<p>Update parameters by maximizing the expected complete-data log-likelihood:  </p> <p>Thus, the E-step directly requires inference.</p>"},{"location":"deeplearning/8_latentvariables/#27-why-exact-inference-is-hard","title":"2.7 Why Exact Inference Is Hard","text":""},{"location":"deeplearning/8_latentvariables/#continuous-latents","title":"Continuous latents:","text":"<ul> <li>Require multidimensional integration over nonlinear likelihoods.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#discrete-latents","title":"Discrete latents:","text":"<ul> <li>Require summing over exponentially many configurations.</li> </ul> <p>Only a few cases allow closed-form inference:</p> <ul> <li>Mixture models  </li> <li>Linear Gaussian systems  </li> <li>Invertible / flow-based models (covered next)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#28-two-strategies-to-handle-intractability","title":"2.8 Two Strategies to Handle Intractability","text":""},{"location":"deeplearning/8_latentvariables/#1-design-tractable-models","title":"1. Design tractable models","text":"<ul> <li>Invertible models (normalizing flows)</li> <li>Autoregressive latent structures Pros: exact inference Cons: restricted model class</li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-approximate-inference","title":"2. Approximate inference","text":"<ul> <li>Use approximations to posterior \\(p(z \\mid x)\\) </li> <li>Variational Inference or MCMC Pros: flexible, expressive models Cons: introduces approximation error</li> </ul>"},{"location":"deeplearning/8_latentvariables/#3-invertible-models-exact-inference","title":"3. Invertible Models &amp; Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#31-what-are-invertible-models","title":"3.1 What Are Invertible Models?","text":"<p>Invertible models (also called normalizing flows) are latent variable models where:</p> <ul> <li>The latent variable \\(z\\) and data \\(x\\) have the same dimensionality</li> <li>There exists an invertible, differentiable mapping </li> <li>Because \\(f_\\theta\\) is invertible:    </li> </ul> <p>Key property: Inference is exact and trivial \u2014 simply apply the inverse function.</p>"},{"location":"deeplearning/8_latentvariables/#32-generative-process","title":"3.2 Generative Process","text":"<p>To generate a sample:</p> <ol> <li>Sample \\(z \\sim p(z)\\) (usually a simple prior like \\(\\mathcal{N}(0, I)\\))</li> <li>Transform via </li> </ol> <p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>"},{"location":"deeplearning/8_latentvariables/#33-why-are-invertible-models-attractive","title":"3.3 Why Are Invertible Models Attractive?","text":"<ul> <li> <p>Exact inference:    is computed by a single function evaluation (no approximation needed).</p> </li> <li> <p>Exact likelihood:   Can compute \\(\\log p_\\theta(x)\\) exactly using the change-of-variables formula.</p> </li> </ul>"},{"location":"deeplearning/8_latentvariables/#34-change-of-variables-for-likelihood","title":"3.4 Change of Variables for Likelihood","text":"<p>Given an invertible mapping \\(x = f_\\theta(z)\\):</p> \\[ p_\\theta(x) = p(z) \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(x)}{\\partial x} \\right) \\right| \\] <p>Equivalently, using \\(z = f_\\theta^{-1}(x)\\):</p> \\[ \\log p_\\theta(x) = \\log p(z) + \\log \\left| \\det J_{f_\\theta^{-1}}(x) \\right| \\] <p>Where:</p> <ul> <li>\\(J_{f_\\theta^{-1}}\\) is the Jacobian matrix of the inverse map  </li> <li>The determinant accounts for volume change introduced by transformation</li> </ul>"},{"location":"deeplearning/8_latentvariables/#35-example-independent-component-analysis-ica","title":"3.5 Example: Independent Component Analysis (ICA)","text":"<p>ICA is the simplest invertible model:</p> <ul> <li>Latent prior:  factorial prior      with non-Gaussian heavy-tailed components</li> <li>Linear invertible mixing: </li> </ul> <p>Inference:  </p> <p>ICA recovers independent sources that explain the observed signal.</p>"},{"location":"deeplearning/8_latentvariables/#36-building-complex-invertible-models","title":"3.6 Building Complex Invertible Models","text":"<p>Modern flows build \\(f_\\theta\\) by composing many simple invertible layers:</p> \\[ f_\\theta = f_K \\circ f_{K-1} \\circ \\dots \\circ f_1 \\] <p>Composition of invertible functions is invertible.</p> <p>Building blocks:</p> <ul> <li>Linear transforms</li> <li>Autoregressive flows (IAF, MAF)</li> <li>Coupling layers (RealNVP, Glow)</li> <li>Residual flows</li> <li>Sylvester flows</li> </ul> <p>Design goal:</p> <p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>"},{"location":"deeplearning/8_latentvariables/#37-advantages-limitations","title":"3.7 Advantages &amp; Limitations","text":""},{"location":"deeplearning/8_latentvariables/#advantages","title":"Advantages","text":"<ul> <li>Exact inference  </li> <li>Exact log-likelihood  </li> <li>Fast, parallel sampling  </li> <li>Useful as components in larger probabilistic models</li> </ul>"},{"location":"deeplearning/8_latentvariables/#limitations","title":"Limitations","text":"<ul> <li>Latent and data dimensions must match  </li> <li>Latents must be continuous  </li> <li>Observations must be continuous or quantized  </li> <li>Very deep flows require large memory  </li> <li>Hard to encode strong structure or sparsity  </li> </ul> <p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>"},{"location":"deeplearning/8_latentvariables/#mar","title":"Mar","text":""},{"location":"deeplearning/8_latentvariables/#4-variational-inference-vi","title":"4. Variational Inference (VI)","text":""},{"location":"deeplearning/8_latentvariables/#41-why-variational-inference","title":"4.1 Why Variational Inference?","text":"<p>In many latent variable models, the true posterior  is intractable because computing  is impossible in closed form.</p> <p>We still need the posterior for:</p> <ul> <li>Inference (explaining the observation)</li> <li>Learning (MLE gradient depends on it)</li> <li>EM algorithm E-step</li> </ul>"},{"location":"deeplearning/8_latentvariables/#approximate-inference","title":"Approximate Inference","text":"<p>There are two major classes of approaches to approximate inference:</p>"},{"location":"deeplearning/8_latentvariables/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>Generate samples from the exact posterior using a Markov chain.</p> <ul> <li>Very general; exact in the limit of infinite time / computation  </li> <li>Computationally expensive  </li> <li>Convergence is hard to diagnose  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-variational-inference-vi","title":"2. Variational Inference (VI)","text":"<p>Approximate the posterior with a tractable distribution (e.g., fully factorized, mixture, or autoregressive).</p> <ul> <li>Fairly efficient \u2014 inference reduces to optimization of distribution parameters  </li> <li>Fast at test time (single forward pass of the inference network)  </li> <li>Cannot easily trade computation for accuracy (unlike MCMC)  </li> </ul> <p>MCMC = flexible, asymptotically exact, but slow. VI = fast and scalable, but biased due to restricted approximating family.</p>"},{"location":"deeplearning/8_latentvariables/#42-core-idea-of-variational-inference","title":"4.2 Core Idea of Variational Inference","text":"<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing. Approximate the posterior with a simpler distribution:</p> \\[ q_\\phi(z \\mid x) \\approx p_\\theta(z \\mid x) \\] <p>Where:</p> <ul> <li>\\(q_\\phi\\) is the variational posterior</li> <li>\\(\\phi\\) are variational parameters (learned)</li> </ul> <p>Requirements:</p> <ol> <li>We can sample from \\(q_\\phi(z \\mid x)\\) </li> <li>We can compute \\(\\log q_\\phi(z \\mid x)\\) and its gradient wrt \\(\\phi\\) </li> </ol> <p>Common choice: mean-field approximation</p> \\[ q_\\phi(z \\mid x) = \\prod_i q_\\phi(z_i \\mid x) \\]"},{"location":"deeplearning/8_latentvariables/#43-training-with-variational-inference","title":"4.3 Training with Variational Inference","text":"<p>Goal: maximize the marginal likelihood</p> \\[ \\log p_\\theta(x) \\] <p>Since it's intractable, VI uses a lower bound on this quantity.</p>"},{"location":"deeplearning/8_latentvariables/#variational-lower-bound-elbo","title":"Variational Lower Bound (ELBO)","text":"<p>Using Jensen\u2019s inequality:</p> \\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x, z)] - \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log q_\\phi(z \\mid x)] \\] <p>This is the Evidence Lower Bound (ELBO):</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi}\\!\\left[\\log p_\\theta(x, z)\\right] - \\mathbb{E}_{q_\\phi}\\!\\left[\\log q_\\phi(z \\mid x)\\right] \\] <p>We maximize ELBO w.r.t both \\(\\theta\\) and \\(\\phi\\).</p>"},{"location":"deeplearning/8_latentvariables/#44-kl-interpretation-variational-gap","title":"4.4 KL Interpretation (Variational Gap)","text":"<p>Rewrite ELBO:</p> \\[ \\log p_\\theta(x) = \\text{ELBO}(\\theta, \\phi) + D_{\\text{KL}}(q_\\phi(z \\mid x) \\,\\|\\, p_\\theta(z \\mid x)) \\] <p>Thus:</p> <ul> <li> <p>Maximizing ELBO wrt \\(\\phi\\)   \u2192 minimizes the KL divergence between \\(q_\\phi\\) and the true posterior.</p> </li> <li> <p>The variational gap is </p> </li> </ul> <p>If \\(q_\\phi\\) is expressive enough:  </p>"},{"location":"deeplearning/8_latentvariables/#45-what-happens-when-updating-each-parameter-set","title":"4.5 What Happens When Updating Each Parameter Set?","text":""},{"location":"deeplearning/8_latentvariables/#updating-variational-parameters-phi","title":"Updating variational parameters \\(\\phi\\):","text":"<ul> <li>Minimizes the variational gap  </li> <li>Makes \\(q_\\phi(z \\mid x)\\) closer to the true posterior  </li> <li>Does not affect the model directly</li> </ul>"},{"location":"deeplearning/8_latentvariables/#updating-model-parameters-theta","title":"Updating model parameters \\(\\theta\\):","text":"<ul> <li>Increases \\(\\log p_\\theta(x)\\) (good)</li> <li>BUT often also reduces the gap by making the posterior simpler   \u2192 Risk: posterior collapse / variational pruning</li> </ul> <p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>"},{"location":"deeplearning/8_latentvariables/#46-variational-pruning-posterior-collapse","title":"4.6 Variational Pruning (Posterior Collapse)","text":"<p>Because VI pushes \\(p_\\theta(z \\mid x)\\) towards \\(q_\\phi(z\\mid x)\\), the model may choose to ignore some latent dimensions:</p> \\[ p_\\theta(z_i \\mid x) = p(z_i) \\] <p>Meaning the latent variable carries no information about \\(x\\).</p> <p>Pros:</p> <ul> <li>Automatically learns effective latent dimensionality</li> </ul> <p>Cons:</p> <ul> <li>Prevents fully utilizing the latent capacity  </li> <li>Common issue in VAEs (particularly with strong decoders)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#47-choosing-the-variational-posterior-family","title":"4.7 Choosing the Variational Posterior Family","text":""},{"location":"deeplearning/8_latentvariables/#simple-mean-field-gaussian","title":"Simple: Mean-field Gaussian","text":"<ul> <li>Fast</li> <li>Easy to optimize</li> <li>But limited expressivity</li> </ul>"},{"location":"deeplearning/8_latentvariables/#more-expressive-options","title":"More expressive options:","text":"<ul> <li>Mixture posteriors</li> <li>Gaussians with full covariance</li> <li>Autoregressive posteriors</li> <li>Normalizing-flow posteriors</li> </ul> <p>Trade-off: accuracy vs speed.</p>"},{"location":"deeplearning/8_latentvariables/#48-amortized-variational-inference","title":"4.8 Amortized Variational Inference","text":"<p>Classic VI:</p> <ul> <li>Each datapoint \\(x\\) has its own variational parameters  </li> <li>Requires iterative optimization per datapoint  </li> <li>Too slow for deep learning</li> </ul> <p>Amortized VI:</p> <ul> <li>Use an inference network (encoder)    </li> <li>Fast inference  </li> <li>Works with SGD  </li> <li>Introduced in Helmholtz Machines  </li> <li>Popularized by Variational Autoencoders</li> </ul>"},{"location":"deeplearning/8_latentvariables/#49-variational-vs-exact-inference","title":"4.9 Variational vs Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#advantages-of-vi","title":"Advantages of VI","text":"<ul> <li>Scalable to modern deep models  </li> <li>Fast inference  </li> <li>Enables flexible model design  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#disadvantages","title":"Disadvantages","text":"<ul> <li>Approximation bias  </li> <li>Posterior may be oversimplified  </li> <li>Can limit expressiveness of the full model  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#410-summary-of-section-4","title":"4.10 Summary of Section 4","text":"<ul> <li>Variational inference approximates the true posterior with a tractable distribution.  </li> <li>ELBO gives a trainable lower bound on the marginal likelihood.  </li> <li>VI converts inference into optimization.  </li> <li>Amortized VI enables neural inference (encoders).  </li> <li>Variational pruning can arise naturally and must be managed.  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#5-gradient-estimation-in-variational-inference","title":"5. Gradient Estimation in Variational Inference","text":""},{"location":"deeplearning/8_latentvariables/#51-why-do-we-need-gradient-estimators","title":"5.1 Why Do We Need Gradient Estimators?","text":"<p>To train a latent variable model with variational inference, we maximize the ELBO:</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z\\mid x)}\\Big[ \\log p_\\theta(x, z) - \\log q_\\phi(z\\mid x) \\Big] \\] <p>We need gradients with respect to:</p> <ol> <li>Model parameters \\(\\theta\\)</li> <li>Variational parameters \\(\\phi\\)</li> </ol> <p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>"},{"location":"deeplearning/8_latentvariables/#52-gradients-wrt-model-parameters-theta","title":"5.2 Gradients w.r.t. Model Parameters (\\(\\theta\\))","text":"<p>This part is easy.</p> <p>Because \\(q_\\phi(z\\mid x)\\) does not depend on \\(\\theta\\):</p> \\[ \\nabla_\\theta \\text{ELBO} = \\mathbb{E}_{q_\\phi(z\\mid x)} \\big[ \\nabla_\\theta \\log p_\\theta(x, z) \\big] \\] <p>We estimate this using samples:</p> <ol> <li>Draw \\(z \\sim q_\\phi(z\\mid x)\\) </li> <li>Compute \\(\\nabla_\\theta \\log p_\\theta(x,z)\\) </li> <li>Average across samples</li> </ol> <p>No special techniques required.</p>"},{"location":"deeplearning/8_latentvariables/#53-gradients-wrt-variational-parameters-phi","title":"5.3 Gradients w.r.t. Variational Parameters (\\(\\phi\\))","text":"<p>This is more difficult.</p> <p>We want:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] \\] <p>But \\(q_\\phi(z\\mid x)\\) depends on \\(\\phi\\). Two main strategies exist to handle this dependence:</p>"},{"location":"deeplearning/8_latentvariables/#54-two-families-of-gradient-estimators","title":"5.4 Two Families of Gradient Estimators","text":""},{"location":"deeplearning/8_latentvariables/#1-likelihood-ratio-reinforce-estimator","title":"\ud83d\udd37 1. Likelihood-Ratio / REINFORCE Estimator","text":"<p>Uses the identity:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}[f(z)] = \\mathbb{E}_{q_\\phi(z)}[f(z)\\,\\nabla_\\phi \\log q_\\phi(z)] \\] <p>This allows gradients for: - Discrete latent variables - Non-differentiable \\(f(z)\\) - Any distribution where we can compute \\(\\log q_\\phi(z)\\)</p> <p>Pros - Very general - Works for discrete and continuous latents  </p> <p>Cons - High variance - Requires variance reduction (baselines, control variates)</p> <p>This is the same gradient estimator used in policy gradients in RL.</p>"},{"location":"deeplearning/8_latentvariables/#2-reparameterization-pathwise-estimator","title":"\ud83d\udd37 2. Reparameterization / Pathwise Estimator","text":"<p>Instead of sampling \\(z \\sim q_\\phi(z\\mid x)\\) directly, write it as a differentiable transformation of noise:</p> \\[ z = g_\\phi(\\epsilon, x), \\quad \\epsilon \\sim p(\\epsilon) \\] <p>Then:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)} \\big[ \\nabla_\\phi f(g_\\phi(\\epsilon, x)) \\big] \\] <p>This pushes the dependence on \\(\\phi\\) inside a differentiable function.</p>"},{"location":"deeplearning/8_latentvariables/#example-gaussian-posterior","title":"Example: Gaussian posterior","text":"<p>If  then:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1) \\] <p>Pros - Low variance - Enables stable VAE training  </p> <p>Cons - Only works for continuous latent variables - Requires differentiable sampling procedure</p>"},{"location":"deeplearning/8_latentvariables/#55-comparison-table","title":"5.5 Comparison Table","text":"Property REINFORCE Reparameterization Works for discrete latent variables \u2705 \u274c Works for continuous latent variables \u2705 \u2705 Low-variance gradients \u274c \u2705 Requires differentiable sampling \u274c \u2705 Used in VAEs sometimes always"},{"location":"deeplearning/8_latentvariables/#56-practical-notes","title":"5.6 Practical Notes","text":"<ul> <li>Modern VAEs always use the reparameterization trick.  </li> <li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li> <li>Discrete VAEs use:</li> <li>Gumbel-Softmax  </li> <li>NVIL / REINFORCE with baselines  </li> <li>VIMCO  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#57-summary-of-section-5","title":"5.7 Summary of Section 5","text":"<ul> <li>Gradient estimation is essential for training VI models.  </li> <li>\\(\\nabla_\\theta\\) is easy: just sample from the variational posterior.  </li> <li>\\(\\nabla_\\phi\\) is hard because sampling depends on parameters.  </li> <li>Two estimators solve this:</li> <li>Likelihood-ratio (REINFORCE)  </li> <li>Reparameterization trick  </li> <li>Reparameterization yields low-variance gradients and powers modern VAEs.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":""},{"location":"deeplearning/8_latentvariables/#61-what-is-a-vae","title":"6.1 What Is a VAE?","text":"<p>A VAE is a latent variable generative model with:</p> <ul> <li>Continuous latent variables \\(z\\)</li> <li>Neural networks for both:</li> <li>Encoder (variational posterior) \\(q_\\phi(z \\mid x)\\) </li> <li>Decoder (likelihood) \\(p_\\theta(x \\mid z)\\)</li> <li>Training through amortized variational inference  </li> <li>Gradients computed using the reparameterization trick</li> </ul> <p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>"},{"location":"deeplearning/8_latentvariables/#62-vae-model-components","title":"6.2 VAE Model Components","text":""},{"location":"deeplearning/8_latentvariables/#prior","title":"Prior","text":"<p>Usually a factorized standard Gaussian:  </p>"},{"location":"deeplearning/8_latentvariables/#likelihood-decoder","title":"Likelihood / Decoder","text":"<p>Maps latents to a distribution over observations.</p> <p>For binary data:  </p> <p>For real-valued data:  </p>"},{"location":"deeplearning/8_latentvariables/#variational-posterior-encoder","title":"Variational Posterior / Encoder","text":"\\[ q_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu_\\phi(x), \\sigma_\\phi^2(x)) \\] <p>All of these functions (encoder &amp; decoder) can be implemented with: - MLPs - ConvNets - ResNets - Transformers depending on the domain.</p>"},{"location":"deeplearning/8_latentvariables/#63-training-objective-the-elbo","title":"6.3 Training Objective: The ELBO","text":"<p>VAEs maximize the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - D_{\\text{KL}}\\!\\Big(q_\\phi(z\\mid x)\\,\\|\\, p(z)\\Big) \\] <p>Interpretation:</p> <ol> <li> <p>Reconstruction Term    Measures how well the model predicts \\(x\\) from \\(z\\).    Encourages informative latents.</p> </li> <li> <p>KL Regularization Term    Encourages \\(q_\\phi(z\\mid x)\\) to stay close to the prior \\(p(z)\\).    Prevents overfitting and encourages smooth latent spaces.</p> </li> </ol> <p>The KL term often has closed-form for Gaussian distributions.</p>"},{"location":"deeplearning/8_latentvariables/#64-reparameterization-trick-key-to-vaes","title":"6.4 Reparameterization Trick (Key to VAEs)","text":"<p>Direct backprop through a sample \\(z \\sim q_\\phi(z\\mid x)\\) is impossible.</p> <p>Solution: rewrite sampling as a differentiable transformation of noise:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>This allows gradient flow through \\(z\\) and makes VAE training practical.</p>"},{"location":"deeplearning/8_latentvariables/#65-vae-as-a-framework","title":"6.5 VAE as a Framework","text":"<p>The term \u201cVAE\u201d now refers to a broad family of models: - Continuous latent variables - Amortized inference - Reparameterization-based gradients - Trained by maximizing ELBO (or its variants)</p> <p>Modern VAEs extend the basic version in many ways: - Multiple latent layers - More expressive posteriors (flows, mixtures) - More expressive priors (hierarchical, autoregressive) - More expressive decoders (ResNets, autoregressive PixelCNN decoders) - Iterative inference networks - Variance reduction techniques</p> <p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>"},{"location":"deeplearning/8_latentvariables/#66-summary-of-section-6","title":"6.6 Summary of Section 6","text":"<ul> <li>VAEs are tractable generative models with continuous latent variables.</li> <li>They pair:</li> <li>a decoder \\(p_\\theta(x\\mid z)\\) and  </li> <li>an encoder \\(q_\\phi(z\\mid x)\\)   using amortized VI.</li> <li>Training uses ELBO + reparameterization trick.</li> <li>VAEs balance reconstruction quality with regularized latent structure.</li> <li>The VAE framework is highly extensible and central to modern deep generative modeling.</li> </ul>"},{"location":"deeplearning/alogirthmic_detials/","title":"Alogirthmic detials","text":"<ol> <li>Batch normalization</li> </ol>"},{"location":"deeplearning/alogirthmic_detials/#211-batchnorm2d-layer","title":"2.1.1 - <code>BatchNorm2d Layer</code>","text":"<p>As part of this new, improved block, you will also introduce a powerful new layer: <code>BatchNorm2d</code>. This layer is a pivotal technique for building modern, high performing deep neural networks.</p> <p>Think of Batch Normalization as a traffic controller for the data flowing between your network's layers. After a convolutional layer processes a batch of images, the outputs (or activations) can have widely varying distributions from one batch to the next. <code>BatchNorm2d</code> steps in and normalizes these activations within each mini batch, adjusting them to have a consistent mean and standard deviation. It then uses two learnable parameters to scale and shift this normalized output, allowing the network itself to learn the optimal distribution for the data at that point.</p> <p>This seemingly simple step provides three profound benefits:</p> <ul> <li> <p>It Stabilizes and Accelerates Training: By keeping the distribution of data consistent between layers, it prevents later layers from having to constantly adapt to a shifting input from the layers before them. This stability allows you to use higher learning rates, which can dramatically speed up how quickly your model learns.</p> </li> <li> <p>It Acts as a Regularizer: Because the normalization statistics are calculated for each unique mini batch, it introduces a slight amount of noise into the training process. This noise makes it harder for the model to perfectly memorize the training data, encouraging it to learn more general features and thus reducing overfitting.</p> </li> <li> <p>It Reduces Sensitivity to Initialization: The layer makes your model less dependent on the specific random weights it starts with, leading to more reliable and repeatable training results.</p> </li> </ul> <p>By adding <code>BatchNorm2d</code> to your <code>CNNBlock</code>, you are not just adding another layer; you are fundamentally making your model's training process more stable, efficient, and robust.</p> <ol> <li>contrastive loss</li> <li>Sematic segmentation</li> <li>class, bounding box</li> <li>poly nn</li> <li>Representation learning</li> </ol>"},{"location":"distributedsystems/0_intro/","title":"Introduction","text":""},{"location":"distributedsystems/0_intro/#introduction","title":"Introduction","text":"<p>What is \"distributed system\":</p> <p>A group of computers cooperating to provide a service</p>"},{"location":"distributedsystems/0_intro/#why","title":"Why?","text":"<ol> <li>to increase capacity via parallel processing</li> <li>to tolerate faults via replication</li> <li>to match distribution of physical devices e.g. sensors</li> <li>to increase security via isolation</li> </ol>"},{"location":"distributedsystems/0_intro/#challanges","title":"Challanges:","text":"<ul> <li>concurrency</li> <li>complex interactions</li> <li>performance bottlenecks</li> <li>partial failure</li> </ul>"},{"location":"distributedsystems/0_intro/#key-topics","title":"Key Topics","text":""},{"location":"distributedsystems/0_intro/#fault-tolerance","title":"Fault tolerance:","text":"<ul> <li>1000s of servers, big network -&gt; always something broken</li> <li>We'd like to hide these failures from the application.</li> <li>\"High availability\": service continues despite failures</li> <li>Big idea: replicated servers. If one server crashes, can proceed using the other(s).</li> </ul>"},{"location":"distributedsystems/0_intro/#consistency","title":"Consistency:","text":"<ul> <li>General-purpose infrastructure needs well-defined behavior. E.g. \"read(x) yields the value from the most recent write(x).\"</li> <li>Achieving good behavior is hard! e.g. \"replica\" servers are hard to keep identical.</li> </ul>"},{"location":"distributedsystems/0_intro/#performance","title":"Performance:","text":"<ul> <li>The goal: scalable throughput. Nx servers -&gt; Nx total throughput via parallel CPU, RAM, disk, net.</li> <li>Scaling gets harder as N grows:<ul> <li>Load imbalance.</li> <li>Slowest-of-N latency.</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#tradeoffs","title":"Tradeoffs:","text":"<ul> <li>Fault-tolerance, consistency, and performance are enemies.</li> <li>Fault tolerance and consistency require communication<ul> <li>e.g., send data to backup server</li> <li>e.g., check if cached data is up-to-date</li> <li>communication is often slow and non-scalable</li> </ul> </li> <li>Many designs sacrifice consistency to gain speed.<ul> <li>e.g. read(x) might not yield the latest write(x)!</li> <li>Painful for application programmers (or users).</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#implementation","title":"Implementation:","text":"<ul> <li>RPC, threads, concurrency control, configuration.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#mapreduce-a-complete-guide","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#introduction","title":"Introduction","text":"<p>Modern data analysis often involves multi-hour computations on multi-terabyte datasets \u2014  for example, building search indexes, sorting massive logs, or analyzing web graphs.  Such tasks are only practical using thousands of computers working in parallel.</p> <p>MapReduce (MR) is a programming model designed to make large-scale data processing  easy for non-specialist programmers. It lets you write simple sequential code, while the framework handles parallel execution, fault tolerance, and data distribution.</p>"},{"location":"distributedsystems/1_mapreduce/#core-concept","title":"Core Concept","text":"<p>The programmer defines just two functions:</p> <ul> <li><code>Map()</code> \u2013 processes input data and emits key-value pairs.</li> <li><code>Reduce()</code> \u2013 aggregates or summarizes all values associated with a given key.</li> </ul> <p>Everything else \u2014 input splitting, task scheduling, network communication, and fault recovery \u2014  is handled automatically by the MapReduce framework.</p>"},{"location":"distributedsystems/1_mapreduce/#how-mapreduce-works-word-count-example","title":"How MapReduce Works (Word Count Example)","text":""},{"location":"distributedsystems/1_mapreduce/#abstract-view","title":"Abstract View","text":"<pre><code>Input1 -&gt; Map -&gt; a,1 b,1\nInput2 -&gt; Map -&gt;     b,1\nInput3 -&gt; Map -&gt; a,1     c,1\n                    |   |   |\n                    |   |   -&gt; Reduce -&gt; c,1\n                    |   -----&gt; Reduce -&gt; b,2\n                    ---------&gt; Reduce -&gt; a,2\n</code></pre>"},{"location":"distributedsystems/1_mapreduce/#steps","title":"Steps","text":"<ol> <li>Input Splitting \u2014 Data is divided into <code>M</code> splits (files or blocks).</li> <li>Map Phase \u2014 Each split is processed by a Map task, generating <code>(key, value)</code> pairs.</li> <li>Shuffle Phase \u2014 Intermediate pairs are grouped by key and distributed to Reduce tasks.</li> <li>Reduce Phase \u2014 Each Reduce task processes one group and outputs final results.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#word-count-example","title":"Word Count Example","text":"<pre><code># Map function\ndef Map(document):\n    words = document.split()\n    for word in words:\n        emit(word, 1)\n\n# Reduce function\ndef Reduce(word, values):\n    emit(word, sum(values))\n</code></pre> <p>Final Output: </p><pre><code>a: 2\nb: 2\nc: 1\n</code></pre><p></p>"},{"location":"distributedsystems/1_mapreduce/#why-mapreduce-scales-so-well","title":"Why MapReduce Scales So Well","text":"<ul> <li>Parallelism: Map and Reduce tasks run independently, enabling massive parallelism.</li> <li>Automatic Management: The framework handles failures, scheduling, and communication.</li> <li>Simplicity: Developers only implement <code>Map()</code> and <code>Reduce()</code>.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#input-output-storage-via-gfs","title":"Input &amp; Output Storage (via GFS)","text":"<p>MapReduce typically uses a distributed file system such as Google File System (GFS).</p> <ul> <li>Files split into 64 MB chunks, distributed across many servers.</li> <li>Maps read input in parallel; Reduces write output in parallel.</li> <li>Replication (2\u20133 copies) ensures fault tolerance.</li> <li>Data locality: Tasks are often scheduled on the same machine where their data resides.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#inside-the-mapreduce-framework","title":"Inside the MapReduce Framework","text":""},{"location":"distributedsystems/1_mapreduce/#coordinators-role","title":"Coordinator\u2019s Role","text":"<ol> <li>Map Phase</li> <li>Assigns Map tasks to workers.</li> <li>Each Map writes intermediate output to its local disk.</li> <li> <p>Intermediate data is partitioned by <code>hash(key) mod R</code> (R = number of Reduces).</p> </li> <li> <p>Reduce Phase</p> </li> <li>Coordinator assigns Reduce tasks.</li> <li>Each Reduce fetches its partition (bucket) from all Maps.</li> <li> <p>Sorts data by key and processes each group.</p> </li> <li> <p>Output</p> </li> <li>Each Reduce writes its final output to GFS.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#performance-and-bottlenecks","title":"Performance and Bottlenecks","text":""},{"location":"distributedsystems/1_mapreduce/#what-limits-performance","title":"What Limits Performance?","text":"<p>Often, network speed is the main bottleneck \u2014 not CPU or disk speed.</p> <p>Network Transfers Include:</p> <ul> <li>Maps reading input from GFS.</li> <li>Reduces fetching intermediate (shuffled) data from Maps.</li> <li>Reduces writing output to GFS.</li> </ul> <p>Because the shuffle phase may move data as large as the original input, network optimization is critical.</p>"},{"location":"distributedsystems/1_mapreduce/#network-optimizations","title":"Network Optimizations","text":"<ul> <li>Data Locality: Run Map tasks where their input data is stored.</li> <li>Single Network Transfer: Intermediate data stored locally, not in GFS.</li> <li>Hash Partitioning: Reduces transfer large data batches (buckets), minimizing small transfers.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#load-balancing","title":"Load Balancing","text":"<p>Why it matters:  Uneven load causes idle workers waiting for \u201cstragglers\u201d. Solution:  </p> <ul> <li>Create many more tasks than workers.</li> <li>The Coordinator dynamically assigns tasks to free workers.</li> <li>Faster machines handle more tasks; slower ones handle fewer.</li> </ul> <p>This keeps the cluster well-balanced and efficient.</p>"},{"location":"distributedsystems/1_mapreduce/#fault-tolerance","title":"Fault Tolerance","text":"<p>Failures are expected in large clusters. MapReduce handles them gracefully.</p>"},{"location":"distributedsystems/1_mapreduce/#worker-failures","title":"Worker Failures","text":"<ul> <li> <p>Map worker crash:</p> </li> <li> <p>Intermediate data (stored locally) is lost.</p> </li> <li>Coordinator reassigns those Map tasks to new workers.</li> <li> <p>No need to rerun if Reduces already fetched the data.</p> </li> <li> <p>Reduce worker crash:</p> </li> <li> <p>Completed results are safe (stored in GFS).</p> </li> <li>Unfinished Reduce tasks are rerun elsewhere.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#deterministic-functions-required","title":"Deterministic Functions Required","text":"<p>Because tasks may be re-executed:</p> <ul> <li><code>Map()</code> and <code>Reduce()</code> must be pure functions \u2014 deterministic and side-effect-free.</li> <li>No external state, random numbers, or I/O beyond the framework.</li> </ul> <p>This guarantees identical results across re-runs.</p>"},{"location":"distributedsystems/1_mapreduce/#handling-other-failures","title":"Handling Other Failures","text":"<ul> <li>Duplicate task execution:   Coordinator accepts output from only one instance.</li> <li>Simultaneous Reduce outputs:   GFS\u2019s atomic rename ensures one consistent final file.</li> <li>Stragglers:   Coordinator launches backup copies of slow tasks.</li> <li>Corrupted output or bad hardware:   Not handled \u2014 MR assumes fail-stop (crash, not corrupt) behavior.</li> <li>Coordinator crash:   Not fully addressed in the original paper.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-works-well","title":"Where MapReduce Works Well","text":"<p>Ideal Use Cases:</p> <ul> <li>Batch processing of huge datasets (TB\u2013PB scale)</li> <li>Log analysis (e.g., counting queries, clickstream analytics)</li> <li>Index building for search engines</li> <li>Data transformations (ETL pipelines)</li> <li>Large-scale machine learning preprocessing</li> <li>Sorting and aggregation across distributed data</li> </ul> <p>These workloads share common traits:</p> <ul> <li>Large, independent input records</li> <li>Deterministic, parallel-friendly computation</li> <li>No need for real-time feedback</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-falls-short","title":"Where MapReduce Falls Short","text":"<p>Not Suitable For:</p> <ul> <li> <p>Real-time or streaming data processing   MR is inherently batch-oriented; results appear only after job completion.</p> </li> <li> <p>Interactive querying   Jobs take minutes to hours; unsuitable for low-latency analytics.</p> </li> <li> <p>Iterative algorithms   Machine learning or graph algorithms (e.g., PageRank, K-means) need multiple    passes over data, causing heavy I/O.</p> </li> <li> <p>Stateful or dependent tasks   MR disallows inter-task communication or shared state.</p> </li> <li> <p>Small or medium datasets   Overhead of distributing tasks outweighs benefits.</p> </li> </ul> <p>Modern systems like Apache Spark, Flink, or Beam were designed to overcome these limitations by enabling in-memory and streaming computation.</p>"},{"location":"distributedsystems/2_threads/","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#65840-lecture-2-2025-threads-and-rpc","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#introduction-implementing-distributed-systems","title":"Introduction: Implementing Distributed Systems","text":"<p>This lecture introduces: - Go threads (goroutines) - Concurrency challenges - The web crawler example - Remote Procedure Calls (RPC)</p> <p>Go is the language used for this writeup.</p>"},{"location":"distributedsystems/2_threads/#why-go","title":"Why Go?","text":"<p>Go is well-suited for distributed systems:</p> <ul> <li>Excellent thread (goroutine) support  </li> <li>Convenient RPC library</li> <li>Type- and memory-safe</li> <li>Garbage-collected (safe with concurrency)</li> <li>Simpler than many other languages</li> <li>Commonly used in production distributed systems</li> </ul> <p>\ud83d\udc49 After the tutorial, read Effective Go: https://golang.org/doc/effective_go.html</p>"},{"location":"distributedsystems/2_threads/#threads-goroutines","title":"Threads (Goroutines)","text":""},{"location":"distributedsystems/2_threads/#what-is-a-thread","title":"What is a Thread?","text":"<p>A thread is a \u201cthread of execution\u201d:</p> <ul> <li>Allows a program to do multiple things at once</li> <li>Executes sequentially (like a program), but shares memory with other threads</li> <li>Has its own program counter, registers, and stack</li> </ul> <p>In Go, threads are called goroutines.</p>"},{"location":"distributedsystems/2_threads/#why-use-threads","title":"Why Use Threads?","text":"<p>Three type of threads:</p>"},{"location":"distributedsystems/2_threads/#1-io-concurrency","title":"1. I/O Concurrency","text":"<ul> <li>Client sends requests to many servers at once  </li> <li>Server handles many clients concurrently  </li> <li>When one thread blocks on I/O, another can run</li> </ul>"},{"location":"distributedsystems/2_threads/#2-multicore-performance","title":"2. Multicore Performance","text":"<p>Use multiple CPU cores simultaneously.</p>"},{"location":"distributedsystems/2_threads/#3-convenience","title":"3. Convenience","text":"<p>Run background tasks (e.g., periodic health checks).</p>"},{"location":"distributedsystems/2_threads/#alternative-event-driven-systems","title":"Alternative: Event-Driven Systems","text":"<p>Instead of threads:</p> <ul> <li>Use a single-threaded system with an event loop</li> <li>Explicitly interleave different activities</li> <li>Maintain state tables for each ongoing operation</li> </ul> <p>Pros:  </p> <ul> <li>Good for I/O concurrency  </li> <li>No thread overhead</li> </ul> <p>Cons:  </p> <ul> <li>No multicore usage  </li> <li>Hard to program and maintain</li> </ul>"},{"location":"distributedsystems/2_threads/#threading-challenges","title":"Threading Challenges","text":""},{"location":"distributedsystems/2_threads/#1-safe-data-sharing","title":"1. Safe Data Sharing","text":"<p>Race example: </p><pre><code>n = n + 1\n</code></pre> Two threads modifying <code>n</code> at the same time \u2192 race condition.<p></p> <p>A race is when: - Two threads access the same memory - At least one is a write - And there's no synchronization</p> <p>Fixes: - Use <code>sync.Mutex</code> - Avoid sharing mutable data</p>"},{"location":"distributedsystems/2_threads/#2-coordination-producerconsumer","title":"2. Coordination (Producer\u2013Consumer)","text":"<ul> <li>One thread produces data  </li> <li>Another consumes it  </li> <li>Need a way for consumers to wait and wake up</li> </ul> <p>Tools: - Go channels - <code>sync.Cond</code> - <code>sync.Wait</code>, <code>sync.WaitGroup</code></p>"},{"location":"distributedsystems/2_threads/#3-deadlock","title":"3. Deadlock","text":"<p>When threads wait on each other forever. Can happen via:</p> <ul> <li>Locks</li> <li>Channels</li> <li>RPC</li> </ul>"},{"location":"distributedsystems/2_threads/#web-crawler-example","title":"Web Crawler Example","text":"<p>A web crawler:</p> <ul> <li>Fetches web pages recursively starting from a URL</li> <li>Follows links</li> <li>Avoids revisiting pages</li> <li>Avoids cycles</li> <li>Exploits I/O concurrency for speed</li> </ul>"},{"location":"distributedsystems/2_threads/#1-serial-crawler","title":"1. Serial Crawler","text":"<ul> <li>Depth-first traversal  </li> <li>A shared map tracks visited URLs  </li> <li>Simple and correct  </li> <li>Very slow \u2014 only fetches one page at a time  </li> </ul> <p>Adding <code>go</code> before recursive calls breaks correctness:</p> <ul> <li>Many threads may fetch same URL  </li> <li>Finishing detection becomes difficult</li> </ul>"},{"location":"distributedsystems/2_threads/#2-concurrent-crawler-with-mutex","title":"2. Concurrent Crawler with Mutex","text":""},{"location":"distributedsystems/2_threads/#how-it-works","title":"How it Works","text":"<ul> <li>Launch a goroutine per page</li> <li>Shared <code>fetched</code> map  </li> <li>Mutex ensures only one thread fetches each URL</li> </ul>"},{"location":"distributedsystems/2_threads/#why-the-mutex","title":"Why the Mutex?","text":""},{"location":"distributedsystems/2_threads/#1-avoid-logical-races","title":"1. Avoid Logical Races","text":"<p>Two threads may check the same URL at once: - Both see <code>fetched[url] == false</code> - Both fetch \u2192 wrong</p> <p>Mutex ensures: - One thread checks + sets at a time</p>"},{"location":"distributedsystems/2_threads/#2-avoid-map-corruption","title":"2. Avoid Map Corruption","text":"<p>Go maps are not thread-safe.</p>"},{"location":"distributedsystems/2_threads/#what-if-lock-is-removed","title":"What If Lock Is Removed?","text":"<ul> <li>Program may appear to work sometimes  </li> <li>But races still occur  </li> <li>Use the race detector:</li> </ul> <pre><code>go run -race crawler.go\n</code></pre>"},{"location":"distributedsystems/2_threads/#completion-detection-using-syncwaitgroup","title":"Completion Detection Using sync.WaitGroup","text":"<ul> <li><code>Add(n)</code> increments  </li> <li><code>Done()</code> decrements  </li> <li><code>Wait()</code> blocks until count is zero  </li> </ul> <p>Ensures main thread waits for all children.</p>"},{"location":"distributedsystems/2_threads/#3-concurrent-crawler-with-channels","title":"3. Concurrent Crawler with Channels","text":"<p>Channels provide:</p> <ul> <li>Communication  </li> <li>Synchronization  </li> </ul>"},{"location":"distributedsystems/2_threads/#channel-basics","title":"Channel Basics","text":"<pre><code>ch := make(chan int)\n\nch &lt;- x   // send (blocks)\ny := &lt;-ch // receive (blocks)\n</code></pre>"},{"location":"distributedsystems/2_threads/#coordinator-workers-model","title":"Coordinator + Workers Model","text":"<ul> <li>Coordinator creates workers via goroutines  </li> <li>Workers fetch a page and send resulting URLs via channel  </li> <li>Coordinator receives URLs, checks visited set</li> </ul>"},{"location":"distributedsystems/2_threads/#why-no-mutex","title":"Why No Mutex?","text":"<ul> <li>Shared state is only in coordinator  </li> <li>Workers never mutate shared maps  </li> <li>Therefore no races</li> </ul>"},{"location":"distributedsystems/2_threads/#channel-safety","title":"Channel Safety","text":"<p>Example:</p> <ul> <li>Worker creates slice of URLs</li> <li>Sends it to channel</li> <li>Coordinator reads it</li> </ul> <p>Safe because:</p> <ul> <li>Worker writes slice before send completes</li> <li>Coordinator reads slice after receive completes</li> </ul> <p>No overlap \u2192 no race.</p>"},{"location":"distributedsystems/2_threads/#why-some-sends-need-a-goroutine","title":"Why Some Sends Need a Goroutine?","text":"<p>Without a goroutine:</p> <ul> <li>send blocks  </li> <li>coordinator may not reach the receive  </li> <li>\u2192 deadlock</li> </ul>"},{"location":"distributedsystems/2_threads/#locks-vs-channels","title":"Locks vs Channels","text":"<p>Both are powerful. Use whichever matches intuition:</p> <ul> <li>State-focused logic \u2192 locks</li> <li>Communication-focused logic \u2192 channels</li> </ul> <p>In 6.5840 labs: - Use sharing + locks for state - Use channels, <code>sync.Cond</code>, or sleep-based polling for notifications</p>"},{"location":"distributedsystems/2_threads/#remote-procedure-call-rpc","title":"Remote Procedure Call (RPC)","text":"<p>RPC enables easy client-server communication.</p>"},{"location":"distributedsystems/2_threads/#goals","title":"Goals","text":"<ul> <li>Hide network details</li> <li>Provide a procedure-call interface</li> <li>Automatically marshal/unmarshal data</li> <li>Enable portability across systems</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-architecture","title":"RPC Architecture","text":"<pre><code>Client               Server\n  request ----&gt;\n            &lt;---- response\n</code></pre> <p>Software structure: </p><pre><code>Client App        Server Handlers\nClient Stubs      Dispatcher\nRPC Library  ---- RPC Library\n Network     ---- Network\n</code></pre><p></p>"},{"location":"distributedsystems/2_threads/#go-rpc-example-keyvalue-store","title":"Go RPC Example: Key/Value Store","text":"<p>Handlers: - <code>Put(key, value)</code> - <code>Get(key) -&gt; value</code></p>"},{"location":"distributedsystems/2_threads/#client-side","title":"Client Side","text":"<ul> <li>Use <code>Dial()</code> to connect  </li> <li>Call RPC using:</li> </ul> <pre><code>Call(\"KVServer.Get\", args, &amp;reply)\n</code></pre> <p>RPC library: - Marshals args - Sends request - Waits for reply - Unmarshals reply - Returns error if something went wrong  </p>"},{"location":"distributedsystems/2_threads/#server-side","title":"Server Side","text":"<p>Server must: 1. Declare a type with exported RPC methods 2. Register the type 3. Accept TCP connections and let RPC library handle them  </p> <p>RPC library:</p> <ul> <li>Creates goroutine per request  </li> <li>Unmarshals request  </li> <li>Dispatches handler  </li> <li>Marshals reply  </li> <li>Sends reply  </li> </ul> <p>Handlers must use locks since multiple RPCs run concurrently.</p>"},{"location":"distributedsystems/2_threads/#rpc-details","title":"RPC Details","text":""},{"location":"distributedsystems/2_threads/#binding","title":"Binding","text":"<p>Client must know <code>\"server:port\"</code> to dial.</p>"},{"location":"distributedsystems/2_threads/#marshalling-rules","title":"Marshalling Rules","text":"<ul> <li>Sends strings, arrays, structs, maps  </li> <li>Cannot send channels or functions  </li> <li>Only exported fields in structs are marshaled  </li> <li>Pointers are sent by copying the pointee</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-failures","title":"RPC Failures","text":"<p>Client may never get a reply:</p> <p>Could mean:</p> <ul> <li>Server never received request  </li> <li>Server crashed after executing  </li> <li>Reply lost in network  </li> <li>Network or server slow  </li> </ul> <p>RPC \u2260 local function call.</p>"},{"location":"distributedsystems/2_threads/#best-effort-rpc","title":"Best-Effort RPC","text":"<p>Algorithm: 1. Send request 2. Wait 3. If no reply, resend 4. After several tries \u2192 give up  </p>"},{"location":"distributedsystems/2_threads/#problems","title":"Problems","text":"<p>Example: </p><pre><code>Put(\"k\", 10)\nPut(\"k\", 20)\n</code></pre><p></p> <p>Retries can reorder or duplicate operations.</p>"},{"location":"distributedsystems/2_threads/#when-is-best-effort-ok","title":"When Is Best-Effort OK?","text":"<ul> <li>Read-only operations  </li> <li>Idempotent operations (safe to repeat)</li> </ul>"},{"location":"distributedsystems/2_threads/#at-most-once-semantics","title":"At-Most-Once Semantics","text":"<p>Go RPC provides:</p> <ul> <li>One TCP connection  </li> <li>Sends each request once  </li> <li>No retries \u2192 no duplicates  </li> </ul> <p>But:</p> <ul> <li>Errors returned on timeouts  </li> <li>Hard to build replicated fault-tolerant systems without retries  </li> </ul> <p>Later labs explore stronger semantics.</p>"},{"location":"informationtheory/1_intro_to_infotheory/","title":"1. Introduction","text":""},{"location":"informationtheory/1_intro_to_infotheory/#chapter-1-introduction-to-information-theory-for-machine-learning","title":"Chapter 1 \u2014 Introduction to Information Theory (for Machine Learning)","text":"<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p> <ul> <li>loss functions (cross-entropy, NLL)</li> <li>representation learning and contrastive learning</li> <li>variational inference and VAEs</li> <li>generative modeling (GANs, flows, diffusion)</li> <li>reinforcement learning (entropy bonuses, policy KL constraints)</li> <li>model capacity, generalization, and bottlenecks</li> </ul> <p>This chapter introduces the core motivations and conceptual tools.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#1-why-information-theory-matters-for-ml","title":"1. Why Information Theory Matters for ML","text":"<p>Information theory answers questions fundamental to ML:</p> <ul> <li>How much uncertainty does a model reduce?</li> <li>How do we quantify the difference between two probability distributions?</li> <li>How do we measure dependence between variables?</li> <li>What is the maximum information a neural network layer can transmit?</li> <li>How do we formalize compression and generalization?</li> </ul> <p>In ML, information theory is not abstract mathematics \u2014 it provides the language for describing learning itself:</p> <p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p> <p>This viewpoint unifies:</p> <ul> <li>Maximum likelihood  </li> <li>Variational inference  </li> <li>Contrastive learning  </li> <li>GAN objectives  </li> <li>Representation learning  </li> <li>Reinforcement learning signal shaping  </li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#2-the-communication-view-shannons-formulation","title":"2. The Communication View (Shannon\u2019s Formulation)","text":"<p>A classical communication system consists of:</p> <ol> <li> <p>Source:    Generates data (symbols, images, text, states).</p> </li> <li> <p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p> </li> <li> <p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p> </li> <li> <p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p> </li> <li> <p>Receiver:   Obtains the final predictions or reconstructions.</p> </li> </ol> <p>Information theory studies:</p> <ul> <li>Limits of efficient communication  </li> <li>Optimal encoding and representation  </li> <li>Tradeoffs between compression and fidelity  </li> <li>Effect of noise on learnability</li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#3-the-uncertainty-view-shannonbayesian-perspective","title":"3. The Uncertainty View (Shannon\u2013Bayesian Perspective)","text":"<p>Information theory also quantifies uncertainty:</p> <ul> <li>More uncertainty \u2192 more information needed  </li> <li>Less uncertainty \u2192 easier prediction and compression  </li> </ul> <p>Key idea: Information is the reduction of uncertainty.</p> <p>In ML:</p> <ul> <li>Entropy measures label uncertainty  </li> <li>Cross-entropy measures model fit  </li> <li>KL divergence measures mismatch  </li> <li>Mutual information measures representation quality  </li> <li>ELBO measures how well a generative model explains data  </li> </ul> <p>Thus, learning and compression are mathematically the same problem.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#4-machine-learning-as-communication","title":"4. Machine Learning as Communication","text":"<p>Modern ML pipelines resemble a communication system:</p>"},{"location":"informationtheory/1_intro_to_infotheory/#data-encoder-latent-representation-decoder-output","title":"Data \u2192 Encoder \u2192 Latent Representation \u2192 Decoder \u2192 Output","text":"<p>Examples:</p> <ul> <li>Autoencoders / VAEs: compress \\(x\\) into \\(z\\), then reconstruct</li> <li>Transformers: compress sequences into features, decode predictions</li> <li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li> <li>GANs: learn generator distributions close to data distribution</li> <li>RL agents: compress sensory input into state representations</li> </ul> <p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#5-roadmap-for-this-web-book","title":"5. Roadmap for This Web-book","text":"<p>This web-book is structured to build information theory specifically for ML:</p> <ol> <li> <p>Entropy &amp; Self-Information    Foundations of uncertainty, coding length, and compression.</p> </li> <li> <p>Cross-Entropy &amp; Negative Log-Likelihood    Core ML loss; the bridge between probability and training objectives.</p> </li> <li> <p>KL Divergence &amp; f-Divergences    Quantifying model mismatch, VI, GAN divergences.</p> </li> <li> <p>Jensen\u2013Shannon &amp; Wasserstein Distances    GAN stability, geometric learning, distribution metrics.</p> </li> <li> <p>Mutual Information &amp; Estimation Bounds    Representation learning, contrastive learning, InfoNCE.</p> </li> <li> <p>Variational Inference &amp; ELBO    VAEs, Bayesian deep learning, posterior approximations.</p> </li> <li> <p>Information Bottleneck &amp; Representation Theory    Why deep networks compress, and how representations generalize.</p> </li> <li> <p>Summary &amp; Concept Map    Unifying view of entropy \u2192 KL \u2192 MI \u2192 VI \u2192 representation learning.</p> </li> </ol>"},{"location":"informationtheory/2_entropy/","title":"2. Entropy, Self-Information & Cross-Entropy","text":""},{"location":"informationtheory/2_entropy/#chapter-2-entropy-self-information-cross-entropy-information-measures","title":"Chapter 2 \u2014 Entropy, Self-Information, Cross-Entropy &amp; Information Measures","text":""},{"location":"informationtheory/2_entropy/#1-self-information-surprisal","title":"1. Self-Information (Surprisal)","text":"<p>Self-information quantifies the surprise of observing an event.</p> <p>For an event with probability \\(p(x)\\):</p> \\[ I(x) = - \\log_2 p(x) \\] <p>Why the log?</p> <ul> <li>Additivity of independent events  </li> <li>Probability \u2192 information monotonicity  </li> <li>Log base 2 \u2192 units in bits  </li> <li>Log-likelihoods become additive \u2192 ML becomes convex (in many models)</li> </ul> <p>Interpretations:</p> <ul> <li>Unlikely events carry more information  </li> <li>Certain events carry zero information  </li> <li>Foundation of cross-entropy and negative log-likelihood  </li> </ul> <p>In ML:  </p> <ul> <li>The loss used in classification is simply the surprisal of the correct class.</li> </ul>"},{"location":"informationtheory/2_entropy/#2-entropy-expected-uncertainty","title":"2. Entropy \u2014 Expected Uncertainty","text":"<p>Entropy is the expected self-information:</p> \\[ H(X) = -\\sum_x p(x) \\log p(x) \\] <p>Entropy measures:</p> <ul> <li>Uncertainty  </li> <li>Randomness  </li> <li>Compressibility  </li> <li>Difficulty of prediction  </li> </ul>"},{"location":"informationtheory/2_entropy/#key-properties","title":"Key properties:","text":"<ul> <li>\\(H(X) = 0\\) if a variable is deterministic  </li> <li>Maximum when distribution is uniform  </li> <li>Upper bound on achievable compression (Shannon)</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation","title":"ML Interpretation:","text":"<ul> <li>High entropy labels \u2192 noisy dataset \u2192 harder learning</li> <li>Activation entropy reflects network expressiveness</li> <li>Entropy of output distribution measures model confidence</li> <li>Entropy regularization improves exploration in RL</li> </ul>"},{"location":"informationtheory/2_entropy/#3-differential-entropy-continuous-entropy","title":"3. Differential Entropy (Continuous Entropy)","text":"<p>For continuous variables:</p> \\[ h(X) = -\\int p(x) \\log p(x)\\,dx \\] <p>Important differences:</p> <ul> <li>Can be negative</li> <li>Not invariant under reparameterization</li> <li>Not comparable between different coordinate systems</li> </ul>"},{"location":"informationtheory/2_entropy/#why-it-matters-in-ml","title":"Why it matters in ML:","text":"<ul> <li>VAEs use continuous latent variables \\(z\\)</li> <li>Flows and diffusion models use continuous densities</li> <li>Score-based models estimate gradients of log-densities, not densities directly</li> </ul> <p>Differential entropy is not the same thing as Shannon entropy \u2014 a common source of confusion.</p>"},{"location":"informationtheory/2_entropy/#4-joint-conditional-and-total-entropy","title":"4. Joint, Conditional, and Total Entropy","text":""},{"location":"informationtheory/2_entropy/#joint-entropy","title":"Joint entropy:","text":"\\[ H(X,Y) = -\\sum_{x,y} p(x,y)\\log p(x,y) \\]"},{"location":"informationtheory/2_entropy/#conditional-entropy","title":"Conditional entropy:","text":"\\[ H(Y|X) = -\\sum_{x,y} p(x,y)\\log p(y|x) \\] <p>Interpretation:</p> <ul> <li>Average residual uncertainty in \\(Y\\) after observing \\(X\\)</li> </ul>"},{"location":"informationtheory/2_entropy/#chain-rule-of-entropy","title":"Chain rule of entropy:","text":"\\[ H(X,Y) = H(X) + H(Y|X) \\] <p>This rule is foundational for:</p> <ul> <li>Autoregressive modeling  </li> <li>Sequence modeling  </li> <li>Transformers (predictive factorization)  </li> <li>Bayesian networks  </li> </ul>"},{"location":"informationtheory/2_entropy/#5-cross-entropy-coding-p-using-q","title":"5. Cross-Entropy \u2014 Coding \\(p\\) Using \\(q\\)","text":"<p>Cross-entropy is the expected surprise under model \\(q\\):</p> \\[ H(p, q) = -\\sum_x p(x)\\log q(x) \\]"},{"location":"informationtheory/2_entropy/#crucial-identity","title":"Crucial identity:","text":"\\[ H(p, q) = H(p) + D_{\\text{KL}}(p\\|q) \\] <p>Meaning:</p> <ul> <li>True entropy + penalty for using the wrong distribution</li> <li>Cross-entropy \u2265 entropy</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation_1","title":"ML Interpretation:","text":"<p>Cross-entropy = Negative Log Likelihood:</p> \\[ \\mathcal{L} = - \\log q(y_{\\text{true}}) \\] <p>This powers:</p> <ul> <li>Softmax classifiers  </li> <li>Logistic regression  </li> <li>Transformers (next-token prediction)  </li> <li>Language models (autoregressive LM)  </li> <li>Image segmentation (pixel-wise CE)  </li> </ul> <p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>"},{"location":"informationtheory/2_entropy/#6-perplexity-entropy-in-language-modeling","title":"6. Perplexity \u2014 Entropy in Language Modeling","text":"<p>Perplexity is:</p> \\[ \\text{PPL} = 2^{H} \\] <p>Interpretation:</p> <ul> <li>The \u201ceffective vocabulary size\u201d the model thinks it must guess from</li> <li>Lower perplexity = better language model</li> </ul> <p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>"},{"location":"informationtheory/2_entropy/#7-mutual-information-information-shared-between-variables","title":"7. Mutual Information \u2014 Information Shared Between Variables","text":"\\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)) \\] <p>MI measures:</p> <ul> <li>How much knowing \\(X\\) tells us about \\(Y\\)</li> <li>Reduction in entropy of one variable after observing the other</li> </ul>"},{"location":"informationtheory/2_entropy/#equivalent-forms","title":"Equivalent forms:","text":"\\[ I(X;Y) = H(X) - H(X|Y) \\] \\[ I(X;Y) = H(X) + H(Y) - H(X,Y) \\] <p>MI links entropy and KL divergence into a unified measure of dependence.</p>"},{"location":"informationtheory/2_entropy/#why-mi-is-critical-in-ml","title":"Why MI is critical in ML:","text":"<ul> <li>Representation learning (maximize MI with labels)</li> <li>Contrastive learning (InfoNCE is a lower bound to MI)</li> <li>InfoGAN (maximize MI between latent code and output)</li> <li>Feature selection (choose features with highest MI to labels)</li> <li>Stochastic encoders control MI with constraints</li> </ul>"},{"location":"informationtheory/2_entropy/#8-the-data-processing-inequality-dpi","title":"8. The Data Processing Inequality (DPI)","text":"<p>If:</p> \\[ X \\rightarrow Z \\rightarrow Y \\] <p>is a Markov chain, then:</p> \\[ I(X;Y) \\le I(X;Z) \\] <p>Meaning:</p> <ul> <li>Processing or compressing data cannot add information</li> <li>Neural networks cannot create information about the input   \u2014 they can only discard or transform it</li> </ul> <p>ML relevance:</p> <ul> <li>Explains why deeper layers become more task-specialized  </li> <li>Supports the Information Bottleneck theory in deep learning  </li> <li>Ensures that any learned representation is bounded by input information  </li> <li>Helps analyze generalization and compression in deep nets</li> </ul>"},{"location":"informationtheory/2_entropy/#9-entropy-in-neural-networks","title":"9. Entropy in Neural Networks","text":"<p>Entropy plays multiple roles in deep learning:</p>"},{"location":"informationtheory/2_entropy/#output-entropy","title":"Output entropy","text":"<p>Low entropy \u2192 confident predictions High entropy \u2192 uncertainty</p>"},{"location":"informationtheory/2_entropy/#entropy-of-hidden-representations","title":"Entropy of hidden representations","text":"<ul> <li>Early layers: reduce entropy (denoising)  </li> <li>Deep layers: compress irrelevant information  </li> <li>Good representations retain low entropy but high MI with labels</li> </ul>"},{"location":"informationtheory/2_entropy/#entropy-regularization-in-rl","title":"Entropy regularization in RL","text":"<p>  encourages exploration.</p>"},{"location":"informationtheory/2_entropy/#dropout-increases-entropy","title":"Dropout increases entropy","text":"<p>forcing models to encode more robust representations.</p>"},{"location":"informationtheory/3_KL/","title":"3. Kullback-Leibler Divergence","text":"<p>Chapter 3 \u2014 KL Divergence, f-Divergences, Jensen\u2013Shannon Divergence, and Wasserstein Distance</p> <p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>"},{"location":"informationtheory/3_KL/#1-kl-divergence-measuring-distribution-mismatch","title":"1. KL Divergence: Measuring Distribution Mismatch","text":"<p>The Kullback\u2013Leibler (KL) divergence measures how different two probability distributions are. For distributions \\(p\\) and \\(q\\):</p> \\[ D_{\\text{KL}}(p\\|q) = \\sum_x p(x)\\log\\frac{p(x)}{q(x)}. \\] <p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from \\(p\\) using a code optimized for \\(q\\). If \\(q\\) assigns very low probability to events that occur frequently under \\(p\\), the KL divergence becomes large.</p>"},{"location":"informationtheory/3_KL/#key-properties-of-kl-divergence","title":"Key properties of KL divergence","text":"<ol> <li> <p>Non-negative </p> </li> <li> <p>Zero only when the two distributions are identical.</p> </li> <li> <p>Asymmetric </p> </li> <li> <p>Not a true metric, since it fails the triangle inequality.</p> </li> <li> <p>Can be infinite when \\(p(x) &gt; 0\\) but \\(q(x) = 0\\).    This is a crucial issue in generative modeling, where such mismatches occur frequently.</p> </li> </ol>"},{"location":"informationtheory/3_KL/#2-kl-divergence-in-machine-learning","title":"2. KL Divergence in Machine Learning","text":"<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>"},{"location":"informationtheory/3_KL/#21-maximum-likelihood-as-forward-kl-minimization","title":"2.1 Maximum likelihood as forward KL minimization","text":"<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p> \\[ \\theta^* = \\arg\\min_\\theta D_{\\text{KL}}(p_{\\text{data}} \\,\\|\\, q_\\theta). \\] <p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p> <p>This produces mode-covering behavior, which is characteristic of:</p> <ul> <li>normalizing flows  </li> <li>autoregressive models  </li> <li>density estimation models trained via log-likelihood  </li> </ul>"},{"location":"informationtheory/3_KL/#22-kl-divergence-in-variational-inference-vi","title":"2.2 KL divergence in variational inference (VI)","text":"<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior \\(q(z|x)\\) and the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Since the true posterior is typically intractable, VAEs approximate this with:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p> <p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative \u201csafe\u201d solution.</p>"},{"location":"informationtheory/3_KL/#23-kl-divergence-in-reinforcement-learning","title":"2.3 KL divergence in reinforcement learning","text":"<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p> \\[ D_{\\text{KL}}(\\pi_{\\text{old}} \\,\\|\\, \\pi_{\\text{new}}). \\] <p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>"},{"location":"informationtheory/3_KL/#24-kl-divergence-in-distillation-and-compression","title":"2.4 KL divergence in distillation and compression","text":"<p>KL divergence compares two probability distributions directly and is used for:</p> <ul> <li>teacher\u2013student distillation  </li> <li>compressing large models into smaller ones  </li> <li>aligning probability distributions across layers  </li> <li>calibrating output probabilities  </li> </ul> <p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>"},{"location":"informationtheory/3_KL/#3-understanding-kl-behavior-mode-covering-vs-mode-seeking","title":"3. Understanding KL Behavior: Mode Covering vs. Mode Seeking","text":"<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>"},{"location":"informationtheory/3_KL/#forward-kl-d_textklpq","title":"Forward KL: \\(D_{\\text{KL}}(p\\|q)\\)","text":"<p>(Used in maximum likelihood, flows \u2192 mode covering)</p> <p>Forward KL asks whether the model \\(q\\) assigns sufficient probability wherever the data distribution \\(p\\) has mass:</p> <p>\u201cDoes the model assign enough probability to every place where the data occurs?\u201d</p> <p>If \\(q\\) misses even a small region where \\(p\\) has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p> <p>Result: mode covering The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p> <p>Flows and MLE-based models display this behavior.</p>"},{"location":"informationtheory/3_KL/#reverse-kl-d_textklqp","title":"Reverse KL: \\(D_{\\text{KL}}(q\\|p)\\)","text":"<p>(Used in VI, VAEs, GAN-like behavior \u2192 mode seeking)</p> <p>Reverse KL asks the opposite question:</p> <p>\u201cIs the model placing probability in places where the data distribution is very small or zero?\u201d</p> <p>Reverse KL heavily penalizes placing mass in low-density regions of \\(p\\), making the model conservative.</p> <p>Result: mode seeking The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p> <p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>"},{"location":"informationtheory/3_KL/#4-f-divergences-a-unified-family-of-divergences","title":"4. f-Divergences: A Unified Family of Divergences","text":"<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function \\(f\\):</p> \\[ D_f(p\\|q) = \\sum_x q(x)\\, f\\!\\left(\\frac{p(x)}{q(x)}\\right). \\]"},{"location":"informationtheory/3_KL/#5-jensenshannon-divergence-the-original-gan-divergence","title":"5. Jensen\u2013Shannon Divergence: The Original GAN Divergence","text":"<p>The Jensen\u2013Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p> \\[ \\text{JS}(p\\|q) = \\frac12 D_{\\text{KL}}(p\\|m) + \\frac12 D_{\\text{KL}}(q\\|m) \\] <p>where the mixture is:</p> \\[ m = \\frac12(p+q). \\] <p>JS divergence is symmetric and always lies between 0 and \\(\\log 2\\).</p>"},{"location":"informationtheory/3_KL/#why-js-appears-in-gans","title":"Why JS appears in GANs","text":"<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p> \\[ \\text{JS}(p\\|q) - \\log 2. \\] <p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>"},{"location":"informationtheory/3_KL/#6-why-js-divergence-causes-gan-instability","title":"6. Why JS Divergence Causes GAN Instability","text":"<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of \\(p\\) and \\(q\\) are disjoint:</p> \\[ \\text{JS}(p\\|q) = \\log 2. \\] <p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p> <p>Consequences:</p> <ol> <li>The discriminator immediately becomes perfect.  </li> <li>The generator stops receiving meaningful gradients.  </li> <li>Training often collapses, oscillates, or diverges.  </li> </ol> <p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>"},{"location":"informationtheory/3_KL/#7-total-variation-and-hellinger-distances","title":"7. Total Variation and Hellinger Distances","text":"<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>"},{"location":"informationtheory/3_KL/#71-total-variation-tv-distance","title":"7.1 Total Variation (TV) Distance","text":"\\[ \\text{TV}(p,q) = \\frac12\\sum_x |p(x)-q(x)|. \\] <p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform \\(p\\) into \\(q\\).</p> <p>Applications in ML:</p> <ul> <li>Robustness under distribution shift  </li> <li>Generalization bounds (PAC-Bayes)  </li> <li>Fairness and safety  </li> </ul>"},{"location":"informationtheory/3_KL/#72-hellinger-distance","title":"7.2 Hellinger Distance","text":"\\[ H^2(p,q) = \\frac12 \\sum_x\\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^2. \\] <p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p> <p>Uses in ML include:</p> <ul> <li>Robust statistics  </li> <li>Domain adaptation  </li> <li>Generalization theory  </li> <li>Some GAN formulations  </li> </ul>"},{"location":"informationtheory/3_KL/#8-wasserstein-distance-geometry-of-probability-distributions","title":"8. Wasserstein Distance: Geometry of Probability Distributions","text":"<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>"},{"location":"informationtheory/3_KL/#81-primal-form-earth-mover-interpretation","title":"8.1 Primal form (Earth Mover interpretation)","text":"\\[ W(p,q) = \\inf_{\\gamma \\in \\Gamma(p,q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]. \\] <p>It seeks the transport plan \\(\\gamma\\) requiring the least expected effort to turn \\(p\\) into \\(q\\).</p>"},{"location":"informationtheory/3_KL/#82-dual-form-used-in-wgan","title":"8.2 Dual form (used in WGAN)","text":"\\[ W(p,q) = \\sup_{\\|f\\|_L\\le 1} \\left(\\mathbb{E}_p[f(x)]      - \\mathbb{E}_q[f(x)]\\right). \\] <p>GANs implement \\(f\\) as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>"},{"location":"informationtheory/3_KL/#83-why-wasserstein-solves-gan-instability","title":"8.3 Why Wasserstein solves GAN instability","text":"<p>Wasserstein distance has several advantages:</p> <ul> <li>Provides informative gradients even with no overlap  </li> <li>Reflects the actual geometry of the data space  </li> <li>Avoids the saturation and vanishing gradients of JS divergence  </li> <li>Works reliably in high-dimensional spaces  </li> </ul> <p>These properties make Wasserstein GANs far more stable than classical GANs.</p>"},{"location":"informationtheory/3_KL/#84-wgan-gp-gradient-penalty","title":"8.4 WGAN-GP: Gradient Penalty","text":"<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p> \\[ \\lambda(\\|\\nabla_x f(x)\\|_2 - 1)^2. \\] <p>This produces smoother and more stable training compared to weight clipping.</p>"},{"location":"informationtheory/3_KL/#9-divergence-versus-distance","title":"9. Divergence versus Distance","text":"<p>Divergences such as KL and JS:</p> <ul> <li>may be infinite  </li> <li>are asymmetric  </li> <li>do not behave well when distributions have disjoint support  </li> </ul> <p>Distances such as Wasserstein, TV, and Hellinger:</p> <ul> <li>are symmetric  </li> <li>obey triangle inequality  </li> <li>remain meaningful under distribution shift  </li> </ul> <p>In machine learning:</p> <ul> <li>Divergences are useful for inference and likelihood  </li> <li>Distances are useful for generative modeling and geometry  </li> </ul>"},{"location":"informationtheory/3_KL/#10-why-divergences-fail-in-high-dimensions","title":"10. Why Divergences Fail in High Dimensions","text":"<p>In high-dimensional spaces:</p> <ul> <li>Real and generated samples rarely overlap  </li> <li>KL divergence often becomes infinite  </li> <li>JS divergence becomes flat  </li> <li>Gradients vanish  </li> </ul> <p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p> <p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p> <p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen\u2013Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p> <p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p>"},{"location":"informationtheory/4_bayes/","title":"4. Bayesian Inference","text":"<p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p> <p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>"},{"location":"informationtheory/4_bayes/#1-bayes-rule","title":"1. Bayes\u2019 Rule","text":"<p>Bayes\u2019 theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable \\(z\\) and an observed variable \\(x\\):</p> \\[ p(z|x) = \\frac{p(x|z)\\,p(z)}{p(x)}. \\] <p>Each term has a clear interpretation.</p> <ul> <li>\\(p(z)\\): prior belief about the unknown variable  </li> <li>\\(p(x|z)\\): likelihood of observing \\(x\\) given \\(z\\) </li> <li>\\(p(x)\\): marginal likelihood or evidence  </li> <li>\\(p(z|x)\\): posterior distribution after observing data  </li> </ul> <p>Bayesian inference is the task of computing \\(p(z|x)\\).</p>"},{"location":"informationtheory/4_bayes/#2-priors-encoding-assumptions-about-hidden-variables","title":"2. Priors: Encoding Assumptions About Hidden Variables","text":"<p>The prior \\(p(z)\\) expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>"},{"location":"informationtheory/4_bayes/#21-regularization","title":"2.1 Regularization","text":"<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields \\(L_2\\) regularization.</p>"},{"location":"informationtheory/4_bayes/#22-structural-assumptions","title":"2.2 Structural assumptions","text":"<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>"},{"location":"informationtheory/4_bayes/#23-uncertainty","title":"2.3 Uncertainty","text":"<p>The prior makes explicit that before observing data, we do not know the true value of \\(z\\).</p>"},{"location":"informationtheory/4_bayes/#24-generative-modeling","title":"2.4 Generative modeling","text":"<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>"},{"location":"informationtheory/4_bayes/#3-likelihood-connecting-latent-variables-to-observed-data","title":"3. Likelihood: Connecting Latent Variables to Observed Data","text":"<p>The likelihood \\(p(x|z)\\) describes how the data are generated from latent causes. In many generative models:</p> <ul> <li>\\(z\\) represents latent structure  </li> <li>\\(x\\) represents an image, time series, or text  </li> <li>\\(p(x|z)\\) is parameterized by a neural network decoder  </li> </ul> <p>The likelihood term encourages the latent variable \\(z\\) to explain the observed data.</p>"},{"location":"informationtheory/4_bayes/#4-the-posterior-what-we-really-want-to-compute","title":"4. The Posterior: What We Really Want to Compute","text":"<p>The goal of Bayesian inference is the posterior:</p> \\[ p(z|x) = \\frac{p(x|z)p(z)}{p(x)}. \\] <p>The posterior expresses how our belief about \\(z\\) changes after seeing \\(x\\). It incorporates both:</p> <ul> <li>prior knowledge  </li> <li>evidence from data  </li> </ul> <p>Unfortunately, computing this posterior is usually intractable.</p>"},{"location":"informationtheory/4_bayes/#5-why-exact-inference-is-hard","title":"5. Why Exact Inference Is Hard","text":"<p>The denominator in Bayes\u2019 rule is the marginal likelihood:</p> \\[ p(x) = \\int p(x,z)\\,dz. \\] <p>This integral is often impossible to evaluate directly because:</p> <ul> <li>the latent space \\(z\\) can be high-dimensional  </li> <li>the joint distribution \\(p(x,z)\\) may involve a complex neural network  </li> <li>the integral has no analytic form  </li> </ul> <p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>"},{"location":"informationtheory/4_bayes/#6-maximum-a-posteriori-map-vs-full-bayesian-inference","title":"6. Maximum a Posteriori (MAP) vs Full Bayesian Inference","text":"<p>There are two kinds of Bayesian computation.</p>"},{"location":"informationtheory/4_bayes/#61-map-estimation","title":"6.1 MAP estimation","text":"<p>MAP finds the single most likely value of \\(z\\):</p> \\[ z_{\\text{MAP}} = \\arg\\max_z\\, p(z|x). \\] <p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>"},{"location":"informationtheory/4_bayes/#62-full-posterior-inference","title":"6.2 Full posterior inference","text":"<p>The full posterior \\(p(z|x)\\) describes a distribution over possible values of \\(z\\), reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>"},{"location":"informationtheory/4_bayes/#7-bayesian-latent-variable-models","title":"7. Bayesian Latent-Variable Models","text":"<p>Many generative models are Bayesian latent-variable models with:</p> <ol> <li> <p>a prior over latent variables </p> </li> <li> <p>a conditional likelihood </p> </li> <li> <p>a posterior </p> </li> </ol> <p>Examples include:</p> <ul> <li>VAEs  </li> <li>mixture models  </li> <li>topic models  </li> <li>probabilistic PCA  </li> <li>diffusion models (in a specific sense)  </li> </ul> <p>Bayesian inference is the foundation of these models.</p>"},{"location":"informationtheory/4_bayes/#8-the-evidence-and-its-importance","title":"8. The Evidence and Its Importance","text":"<p>The marginal likelihood, also called the evidence:</p> \\[ p(x) = \\int p(x,z)\\,dz \\] <p>plays several roles:</p> <ul> <li>It normalizes the posterior.  </li> <li>It evaluates how well a model explains data.  </li> <li>It is used in Bayesian model comparison.  </li> <li>Its logarithm appears in training objectives for VAEs and diffusion models.</li> </ul> <p>Maximizing evidence corresponds to learning a model that explains the data well.</p>"},{"location":"informationtheory/4_bayes/#9-bayesian-interpretation-of-kl-divergence","title":"9. Bayesian Interpretation of KL Divergence","text":"<p>KL divergence naturally appears when comparing an approximate posterior \\(q(z|x)\\) with the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Minimizing this KL divergence means making the approximation \\(q\\) as close as possible to the exact posterior.</p> <p>This forms the basis of variational inference.</p>"},{"location":"informationtheory/4_bayes/#10-why-we-need-variational-inference","title":"10. Why We Need Variational Inference","text":"<p>Because the true posterior is intractable, we introduce a simpler distribution \\(q(z|x)\\) and optimize it to approximate \\(p(z|x)\\).</p> <p>We cannot compute:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)) \\] <p>directly, because \\(p(z|x)\\) depends on \\(p(x)\\), which is the intractable integral.</p> <p>Variational inference resolves this by rewriting \\(\\log p(x)\\) and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p> <p>This is the topic of the next chapter.</p> <p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes\u2019 rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p> <p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p>"},{"location":"informationtheory/5_mc_intro/","title":"5. Probability toolbox","text":"<p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}, \\qquad  p(x) = \\int p(x,z)\\,dz. \\] <p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p> <p>This section introduces four major families of methods:</p> <ol> <li>complete enumeration  </li> <li>Laplace approximation  </li> <li>Monte Carlo methods  </li> <li>variational methods  </li> </ol> <p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>"},{"location":"informationtheory/5_mc_intro/#1-complete-enumeration","title":"1. Complete Enumeration","text":"<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p> \\[ p(x) = \\sum_z p(x,z) \\quad \\text{or} \\quad p(x) = \\int p(x,z)\\,dz. \\] <p>This is feasible only when:</p> <ul> <li>the latent variable is low dimensional  </li> <li>the domain is small or discrete  </li> <li>the joint distribution has a simple closed form  </li> </ul> <p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>"},{"location":"informationtheory/5_mc_intro/#2-laplace-approximation","title":"2. Laplace Approximation","text":"<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p> <p>Given a posterior</p> \\[ p(z|x) \\propto p(x,z), \\] <p>the Laplace approximation fits a Gaussian distribution</p> \\[ q(z|x) \\approx \\mathcal{N}(z_{\\text{MAP}}, H^{-1}), \\] <p>where:</p> <ul> <li>\\(z_{\\text{MAP}}\\) is the mode of \\(p(z|x)\\) </li> <li>\\(H\\) is the Hessian of \\(-\\log p(z|x)\\) at the mode  </li> </ul> <p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>"},{"location":"informationtheory/5_mc_intro/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":"<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p> \\[ \\mathbb{E}_{p(z|x)}[f(z)]  \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\qquad z_i \\sim p(z|x). \\] <p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p> <p>Sampling strategies fall into two groups:</p> <ul> <li>independent sampling  </li> <li>Markov chain\u2013based sampling (MCMC)  </li> </ul> <p>The next chapter explains Monte Carlo and sampling methods in detail.</p>"},{"location":"informationtheory/5_mc_intro/#4-variational-methods","title":"4. Variational Methods","text":"<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from \\(p(z|x)\\), we introduce a distribution \\(q(z|x)\\) and optimize it to be close to the true posterior. The objective is to minimize</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x) + D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>"},{"location":"informationtheory/5_mc_intro/#summary","title":"Summary","text":"<p>Approximate inference methods can be understood as four major strategies:</p> <ul> <li>complete enumeration: exact but rarely feasible  </li> <li>Laplace approximation: fast Gaussian approximation near the mode  </li> <li>Monte Carlo methods: sampling-based numerical estimation  </li> <li>variational methods: optimization-based posterior approximation  </li> </ul> <p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p>"},{"location":"informationtheory/6_mc/","title":"6. Monte Carlo Methods","text":"<p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p> <p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>"},{"location":"informationtheory/6_mc/#1-the-goal-of-sampling","title":"1. The Goal of Sampling","text":"<p>Many problems require computing expectations of the form</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\int f(z)\\,p(z)\\,dz, \\] <p>or evaluating posterior quantities such as</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p> <p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>"},{"location":"informationtheory/6_mc/#2-independent-sampling","title":"2. Independent Sampling","text":"<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p> <p>These methods work best when:</p> <ul> <li>sampling directly from \\(p(z)\\) is tractable  </li> <li>the distribution is low-dimensional  </li> <li>the support is simple (e.g., Gaussian, uniform)  </li> </ul>"},{"location":"informationtheory/6_mc/#21-direct-sampling","title":"2.1 Direct Sampling","text":"<p>When the distribution has an invertible CDF \\(F(z)\\):</p> <ol> <li>sample \\(u \\sim \\text{Uniform}(0,1)\\) </li> <li>compute \\(z = F^{-1}(u)\\) </li> </ol> <p>This yields exact samples. It is commonly used in:</p> <ul> <li>uniform sampling  </li> <li>exponential distributions  </li> <li>simple discrete distributions  </li> </ul>"},{"location":"informationtheory/6_mc/#22-importance-sampling","title":"2.2 Importance Sampling","text":"<p>When sampling from \\(p(z)\\) is difficult but evaluating \\(p(z)\\) is easy, importance sampling draws samples from a proposal distribution \\(q(z)\\) and reweights them:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\mathbb{E}_{q(z)}\\left[f(z)\\frac{p(z)}{q(z)}\\right]. \\] <p>The weights</p> <p>   \u200b</p> <p>correct for the fact that samples were drawn from \\(q\\) instead of \\(p\\).</p> <p>The main challenge is weight variability. If \\(q(z)\\) does not closely match \\(p(z)\\), the ratio \\(p(z)/q(z)\\) becomes extremely uneven: most samples have tiny weights, while a few samples have very large weights. This produces high-variance estimates because the estimator becomes dominated by a handful of rare but extremely influential samples. In high-dimensional spaces, designing a proposal \\(q(z)\\) that covers the important regions of \\(p(z)\\) is especially difficult, making importance sampling unreliable unless the proposal distribution is carefully chosen.</p>"},{"location":"informationtheory/6_mc/#23-rejection-sampling","title":"2.3 Rejection Sampling","text":"<p>Rejection sampling draws exact samples from a target distribution \\(p(z)\\) by using a simpler proposal distribution \\(q(z)\\) and accepting or rejecting candidate samples based on how well \\(q\\) covers \\(p\\). The method requires a constant \\(M\\) such that  </p> <p>This condition ensures that \\(Mq(z)\\) forms an envelope that completely contains \\(p(z)\\).</p> <p>Procedure:</p> <ol> <li>sample \\(z \\sim q(z)\\) </li> <li>accept with probability \\(\\frac{p(z)}{M q(z)}\\) </li> </ol> <p>If accepted, \\(z\\) is guaranteed to be an exact draw from \\(p\\).</p> <p>Rejection sampling is conceptually simple and does not distort the target distribution, but it becomes inefficient in many settings. When \\(q(z)\\) is a poor match for \\(p(z)\\), the constant \\(M\\) must be large, which means that most samples are rejected. In high-dimensional spaces, the mismatch between \\(p\\) and \\(q\\) typically worsens exponentially, making the acceptance probability extremely small. As a result, rejection sampling is rarely practical for modern high-dimensional machine-learning models, although it remains useful in low-dimensional problems or when \\(q\\) can be chosen to closely match \\(p\\).</p>"},{"location":"informationtheory/6_mc/#3-monte-carlo-estimation","title":"3. Monte Carlo Estimation","text":"<p>Monte Carlo approximates expectations by:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\quad z_i \\sim p(z). \\] <p>Key properties:</p> <ul> <li>error scales as \\(\\mathcal{O}(1/\\sqrt{N})\\) </li> <li>works in high dimensions  </li> <li>accuracy depends on sampling quality  </li> </ul> <p>Monte Carlo is the backbone of almost all probabilistic computation.</p>"},{"location":"informationtheory/6_mc/#4-markov-chain-monte-carlo-mcmc","title":"4. Markov Chain Monte Carlo (MCMC)","text":"<p>When sampling directly from \\(p(z)\\) is hard, Markov Chain Monte Carlo constructs a Markov chain</p> \\[ z_1 \\to z_2 \\to z_3 \\to \\cdots \\] <p>whose stationary distribution is \\(p(z)\\).</p> <p>After a burn-in period, samples approximate \\(p(z)\\) even if individual states are dependent.</p> <p>MCMC is widely applicable because it does not require the normalization constant of \\(p(z)\\):</p> \\[ p(z|x) \\propto p(x,z). \\]"},{"location":"informationtheory/6_mc/#41-metropolishastings-algorithm","title":"4.1 Metropolis\u2013Hastings Algorithm","text":"<p>The Metropolis\u2013Hastings (MH) algorithm constructs a Markov chain whose stationary distribution is the target distribution \\(p(z)\\), even when \\(p(z)\\) is known only up to a proportionality constant. This makes MH suitable for Bayesian inference, where the posterior is often available only in unnormalized form:</p> \\[p(z \\mid x) \\propto p(x, z)\\] <p>MH works by proposing a new point based on the current state and then accepting or rejecting it according to how well it aligns with the target distribution.</p> <p>Given a current sample \\(z\\), the algorithm proceeds as follows:</p> <ol> <li> <p>propose a new sample \\(z'\\) using a proposal distribution \\(z' \\sim q(z' \\mid z)\\).</p> </li> <li> <p>compute the acceptance probability       </p> </li> <li> <p>accept the proposal with probability \\(\\alpha(z,z')\\) otherwise remain at the current state If accepted, set \\(z_{t+1} = z'\\), otherwise keep \\(z_{t+1} = z\\).</p> </li> </ol> <p>This simple rule ensures that the Markov chain satisfies detailed balance and converges to the desired distribution \\(p(z)\\).</p> <p>Metropolis\u2013Hastings is flexible and works with virtually any distribution from which we can evaluate \\(p(z)\\) up to a constant. However, its efficiency depends strongly on the proposal distribution. If the proposal steps are too small, the chain performs a random walk and mixes slowly. If the steps are too large, most proposals are rejected. Choosing or adapting the proposal distribution is therefore crucial for performance, especially in high-dimensional settings.</p>"},{"location":"informationtheory/6_mc/#42-gibbs-sampling","title":"4.2 Gibbs Sampling","text":"<p>Gibbs sampling is a special case of MCMC designed for multivariate distributions where sampling from the full conditional distributions is easy. Instead of proposing a new state and accepting or rejecting it, Gibbs sampling updates one variable at a time by drawing directly from its exact conditional distribution.</p> <p>For a latent vector:</p> \\[z = (z_1, z_2, \\dots, z_d)\\] <p>a Gibbs update for coordinate \\(i\\) samples:</p> \\[ z_i \\sim p(z_i \\mid z_{-i}). \\] <p>where \\(z_{-i}\\) denotes all components except \\(z_i\\).</p> <p>By cycling through all coordinates repeatedly, the Markov chain eventually converges to the target joint distribution \\(p(z)\\).</p> <p>The key requirement is that each full conditional distribution</p> \\[p(z_i \\mid z_{-i})\\] <p>must be analytically tractable and easy to sample from. When this holds, Gibbs sampling is simple to implement and avoids the accept\u2013reject step of Metropolis\u2013Hastings.</p> <p>However, Gibbs sampling can mix slowly when variables are strongly correlated, since updating one coordinate at a time may explore the space inefficiently. Gibbs sampling is widely used in models where conditional distributions are naturally available, including:</p> <ul> <li>topic models such as Latent Dirichlet Allocation (LDA)</li> <li>hidden Markov models</li> <li>Gaussian graphical models</li> <li>Bayesian networks with conjugate priors</li> </ul>"},{"location":"informationtheory/6_mc/#43-slice-sampling","title":"4.3 Slice Sampling","text":"<p>Slice sampling is an MCMC method that avoids choosing a proposal distribution by sampling uniformly from the region (the slice) where the probability density is above a randomly chosen threshold. Given the current point \\(z\\), slice sampling introduces an auxiliary variable \\(u\\):</p> <ol> <li> <p>Draw a height</p> <p>\\(\\(u \\sim \\text{Uniform}(0,\\, p(z))\\)\\)</p> </li> <li> <p>Define the horizontal slice</p> <p>\\(\\(S = \\{ z' : p(z') &gt; u \\}\\)\\)</p> </li> <li> <p>Sample the next state</p> <p>\\(\\(z' \\sim \\text{Uniform}(S)\\)\\)</p> </li> </ol> <p>This procedure constructs a Markov chain whose stationary distribution is \\(p(z)\\). Intuitively, the algorithm first chooses a horizontal level \\(u\\) below the current density value and then samples uniformly from the region of the density that lies above this level.</p> <p>Slice sampling adapts naturally to the local geometry of the target distribution: narrow peaks produce narrow slices, and broad regions produce wide slices, without requiring manual tuning of proposal scales. In practice, slice sampling often mixes better than basic random-walk proposals, especially when the target density varies in amplitude across different regions.</p> <p>However, slice sampling requires an efficient way to identify or approximate the slice region, which may be challenging in high-dimensional or multimodal settings.</p>"},{"location":"informationtheory/6_mc/#5-reducing-random-walk-behaviour","title":"5. Reducing Random-Walk Behaviour","text":"<p>Basic MCMC algorithms such as Metropolis\u2013Hastings often move in small, local steps and therefore explore the state space slowly. This random-walk behaviour leads to poor mixing and long autocorrelation times, especially in high-dimensional or strongly correlated distributions.</p> <p>Several advanced MCMC techniques attempt to reduce random-walk dynamics by proposing more informed or distant moves.</p>"},{"location":"informationtheory/6_mc/#51-hamiltonian-monte-carlo-hmc","title":"5.1 Hamiltonian Monte Carlo (HMC)","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving smoothly through the probability landscape.</p> <p>Let \\(z\\) denote the position (the latent variable) and let \\(r\\) denote an auxiliary momentum variable. The joint density of \\((z, r)\\) is defined through a Hamiltonian:</p> \\[ H(z, r) = -\\log p(z) + \\frac{1}{2} r^\\top r. \\] <p>The first term acts like a potential energy, and the second acts like kinetic energy. The total Hamiltonian is approximately conserved under Hamiltonian dynamics governed by:</p> \\[ \\frac{dz}{dt} = r, \\qquad \\frac{dr}{dt} = \\nabla_z \\log p(z). \\] <p>Following these dynamics moves the system along continuous trajectories that remain mostly in high-probability regions, allowing the sampler to travel long distances without being rejected.</p>"},{"location":"informationtheory/6_mc/#leapfrog-integration-and-step-size","title":"Leapfrog Integration and Step Size","text":"<p>Exact Hamiltonian dynamics cannot be simulated analytically, so HMC uses a numerical integrator, typically the leapfrog method. Leapfrog integration updates position and momentum in small steps of size \\(\\epsilon\\):</p> <ol> <li> <p>half-step momentum update </p> </li> <li> <p>full-step position update </p> </li> <li> <p>half-step momentum update </p> </li> </ol> <p>These updates are repeated \\(L\\) times, producing a proposal \\((z', r')\\) after a simulated trajectory of length \\(L \\epsilon\\).</p> <p>The step size \\(\\epsilon\\) strongly influences performance:</p> <ul> <li>if \\(\\epsilon\\) is too large, numerical integration becomes inaccurate and proposals are rejected  </li> <li>if \\(\\epsilon\\) is too small, trajectories progress slowly and exploration becomes inefficient  </li> </ul> <p>Adaptive schemes such as dual averaging automatically tune \\(\\epsilon\\).</p>"},{"location":"informationtheory/6_mc/#acceptance-step","title":"Acceptance Step","text":"<p>Although leapfrog integration nearly preserves energy, numerical error accumulates. Therefore HMC applies a Metropolis acceptance step:</p> \\[ \\alpha = \\min\\left(1,\\, \\exp\\big(-H(z',r') + H(z,r)\\big) \\right). \\] <p>Because leapfrog integration is reversible and volume-preserving, acceptance rates remain high even for long trajectories.</p>"},{"location":"informationtheory/6_mc/#choosing-the-trajectory-length","title":"Choosing the Trajectory Length","text":"<p>The number of leapfrog steps \\(L\\) (or total integration time \\(L\\epsilon\\)) affects how far the sampler travels:</p> <ul> <li>small \\(L\\) results in short trajectories, similar to random-walk proposals  </li> <li>large \\(L\\) explores further but may waste computation or return near the starting point  </li> </ul> <p>The No-U-Turn Sampler (NUTS) automatically selects an appropriate integration length and forms the basis of modern HMC implementations such as Stan.</p>"},{"location":"informationtheory/6_mc/#summary-of-advantages","title":"Summary of Advantages","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving through the probability landscape.</p> <p>Hamiltonian Monte Carlo offers several advantages:</p> <ul> <li>efficient exploration in high-dimensional or correlated distributions  </li> <li>large, directed moves that avoid random-walk behaviour  </li> <li>low autocorrelation between samples  </li> <li>high acceptance rates due to approximate energy conservation  </li> <li>uses gradients of \\(\\log p(z)\\) to guide proposals  </li> </ul> <p>HMC is widely used in probabilistic programming frameworks such as Stan, PyMC, and NumPyro, largely because of its scalability and efficiency in challenging Bayesian inference problems.</p>"},{"location":"informationtheory/6_mc/#52-overrelaxation","title":"5.2 Overrelaxation","text":"<p>Overrelaxation modifies proposals so that successive samples are negatively correlated. Instead of randomly perturbing the current state, overrelaxation proposes a point on the opposite side of the conditional mean.</p> <p>Intuitively, if the current sample lies above the mean, the overrelaxation proposal nudges it below the mean, and vice versa. This helps the chain avoid local sticking and oscillation.</p> <p>Overrelaxation is most effective when the conditional distribution is approximately Gaussian or when the model exhibits strong linear structure.</p>"},{"location":"informationtheory/6_mc/#6-sensitivity-to-step-size","title":"6. Sensitivity to Step Size","text":"<p>The efficiency of MCMC algorithms depends critically on the choice of step size (or proposal scale):</p> <ul> <li>If the step size is too small, the chain takes tiny moves and mixes slowly.  </li> <li>If the step size is too large, most proposals are rejected.</li> </ul> <p>Finding an appropriate step size is essential for balancing exploration and acceptance.  </p> <p>For random-walk Metropolis\u2013Hastings:</p> <ul> <li>acceptance rates near 0.2\u20130.4 often work well in high dimensions  </li> <li>smaller dimensions tolerate larger acceptance rates</li> </ul> <p>For HMC, step size affects both the numerical integration quality and the acceptance probability. Too large a step size causes integration error and rejections; too small a step size results in slow exploration.</p> <p>Adaptive MCMC methods automatically tune the step size to achieve target acceptance rates.</p>"},{"location":"informationtheory/6_mc/#7-when-to-stop-convergence-and-diagnostics","title":"7. When to Stop: Convergence and Diagnostics","text":"<p>Running an MCMC chain forever is impossible, so practical inference requires diagnosing convergence.</p> <p>Several indicators are commonly used:</p>"},{"location":"informationtheory/6_mc/#71-burn-in","title":"7.1 Burn-in","text":"<p>The initial part of the chain (the burn-in period) may not represent the target distribution. These early samples are discarded until the chain reaches a stable region.</p>"},{"location":"informationtheory/6_mc/#72-autocorrelation","title":"7.2 Autocorrelation","text":"<p>High autocorrelation indicates slow mixing. Effective sample size (ESS) measures the number of independent samples equivalent to the correlated MCMC draws.</p>"},{"location":"informationtheory/6_mc/#73-multiple-chains","title":"7.3 Multiple chains","text":"<p>Running several independent chains allows comparison. If chains converge to the same region, the sampler is more likely to have reached equilibrium.</p>"},{"location":"informationtheory/6_mc/#74-gelmanrubin-statistic-r-hat","title":"7.4 Gelman\u2013Rubin statistic (R-hat)","text":"<p>R-hat compares within-chain and between-chain variance. Values close to 1 indicate convergence.</p>"},{"location":"informationtheory/6_mc/#75-visual-inspection","title":"7.5 Visual inspection","text":"<p>Trace plots, autocorrelation plots, and histograms provide qualitative insight into mixing and stability.</p> <p>There is no single perfect test, but combining multiple diagnostics provides reasonable confidence that the Markov chain has approximated the target distribution.</p> <p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p> <p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p> <p>Random-walk behaviour limits the efficiency of basic MCMC methods. Hamiltonian Monte Carlo reduces this by exploiting gradient information and simulating Hamiltonian dynamics, while overrelaxation introduces negative correlation to speed up mixing. Step size must be chosen carefully to balance exploration and acceptance. Convergence diagnostics such as burn-in, effective sample size, and R-hat help determine when to stop sampling and assess the quality of the generated samples.</p>"},{"location":"informationtheory/7a_vi_intro/","title":"8. Optimization-Based Inference","text":"<p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p> <p>This chapter introduces three optimization-based inference strategies:</p> <ol> <li>Maximum a posteriori (MAP) estimation  </li> <li>Expectation\u2013Maximization (EM)  </li> <li>Variational inference (VI), in its simplest introductory form  </li> </ol> <p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>"},{"location":"informationtheory/7a_vi_intro/#1-motivation-for-optimization-based-inference","title":"1. Motivation for Optimization-Based Inference","text":"<p>Bayesian inference requires the posterior</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The challenge lies in computing the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p> <ul> <li>high-dimensional latent spaces  </li> <li>multimodal posteriors  </li> <li>large datasets  </li> <li>models requiring gradient-based learning  </li> </ul> <p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>"},{"location":"informationtheory/7a_vi_intro/#2-maximum-a-posteriori-map-estimation","title":"2. Maximum A Posteriori (MAP) Estimation","text":"<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes\u2019 rule:</p> \\[ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}, \\] <p>MAP chooses the mode of the posterior:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta p(\\theta|x). \\] <p>Since \\(p(x)\\) does not depend on \\(\\theta\\), this is equivalent to:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta \\big[ \\log p(x|\\theta) + \\log p(\\theta) \\big]. \\] <p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through \\(p(\\theta)\\). However, it returns only a point estimate and does not capture uncertainty.</p> <p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>"},{"location":"informationtheory/7a_vi_intro/#3-expectationmaximization-em","title":"3. Expectation\u2013Maximization (EM)","text":"<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p> \\[ \\log p_\\theta(x)  = \\log \\sum_z p_\\theta(x,z). \\] <p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>"},{"location":"informationtheory/7a_vi_intro/#e-step","title":"E-step","text":"<p>Compute the posterior over latent variables under the current parameters:</p> \\[ q(z) = p_\\theta(z|x). \\]"},{"location":"informationtheory/7a_vi_intro/#m-step","title":"M-step","text":"<p>Maximize the expected complete-data log-likelihood:</p> \\[ \\theta \\leftarrow  \\arg\\max_\\theta  \\mathbb{E}_{q(z)}[\\log p_\\theta(x,z)]. \\] <p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p> <ul> <li>mixture of Gaussians  </li> <li>hidden Markov models  </li> <li>probabilistic PCA  </li> <li>clustering and density estimation  </li> </ul> <p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior \\(q(z) = p_\\theta(z|x)\\).</p>"},{"location":"informationtheory/7a_vi_intro/#4-em-and-map-map-em","title":"4. EM and MAP: MAP-EM","text":"<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p> \\[ \\theta_{\\text{MAP}}  =  \\arg\\max_\\theta  \\left[ \\mathbb{E}_{p(z|x,\\theta)}[\\log p(x,z|\\theta)] + \\log p(\\theta) \\right]. \\] <p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>"},{"location":"informationtheory/7a_vi_intro/#5-limitations-of-map-and-em","title":"5. Limitations of MAP and EM","text":"<p>Both MAP and EM have limitations that motivate more general methods:</p> <ol> <li>MAP returns only a point estimate and discards posterior uncertainty.  </li> <li>EM requires exact posterior computation in the E-step:        which is often intractable.  </li> <li>EM struggles with:</li> <li>multimodal posteriors  </li> <li>high-dimensional latent spaces  </li> <li>arbitrary likelihood forms  </li> </ol> <p>These limitations lead naturally to variational inference.</p>"},{"location":"informationtheory/7a_vi_intro/#6-a-brief-introduction-to-variational-inference-vi","title":"6. A Brief Introduction to Variational Inference (VI)","text":"<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p> \\[ q(z) = p_\\theta(z|x), \\] <p>VI chooses a family of distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q} \\] <p>and optimizes it to be close to the true posterior. The objective is:</p> \\[ \\phi^* =  \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p> <p>VI:</p> <ul> <li>generalizes MAP (when \\(q\\) is a delta function)  </li> <li>generalizes EM (when \\(q = p_\\theta(z|x)\\))  </li> <li>supports flexible approximations  </li> <li>scales to large datasets  </li> <li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li> </ul> <p>The next chapter explores variational inference in detail.</p> <p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p> <p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p> <p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p>"},{"location":"informationtheory/7b_vi/","title":"7. Variatonal Inference","text":"<p>Variational inference (VI) provides a general framework for approximating difficult probability distributions with simpler, tractable ones. Many modern machine-learning models rely on VI, including variational autoencoders (VAEs), Bayesian neural networks, latent-variable models, and diffusion models. VI offers a scalable alternative to sampling-based inference and converts the inference problem into an optimization problem.</p>"},{"location":"informationtheory/7b_vi/#1-the-problem-of-inference","title":"1. The Problem of Inference","text":"<p>Many probabilistic models introduce hidden variables to explain observations. Examples include:</p> <ul> <li>latent variables \\(z\\) in VAEs  </li> <li>weight distributions in Bayesian neural networks  </li> <li>cluster indicators in mixture models  </li> <li>hidden states in topic models and HMMs  </li> </ul> <p>The goal is to compute the posterior distribution</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The difficulty lies in the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is often intractable in high-dimensional or complex models. Exact Bayesian inference becomes impossible, which motivates approximate methods. Variational inference addresses this challenge.</p>"},{"location":"informationtheory/7b_vi/#2-the-idea-of-variational-inference","title":"2. The Idea of Variational Inference","text":"<p>Variational inference replaces the intractable posterior with a tractable approximation. Instead of trying to compute \\(p(z|x)\\) exactly, VI introduces a family of simpler distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q}, \\] <p>and chooses the member that is closest to the true posterior. Closeness is measured using the KL divergence:</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>The goal is:</p> \\[ \\phi^* = \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>However, the KL depends on \\(p(z|x)\\), which is unknown, making direct minimization impossible. The key insight is that the KL can be rewritten in terms of computable quantities, leading to the Evidence Lower Bound (ELBO).</p>"},{"location":"informationtheory/7b_vi/#3-deriving-the-elbo","title":"3. Deriving the ELBO","text":"<p>We start from the marginal likelihood:</p> \\[ \\log p(x) = \\log \\int p(x,z)\\,dz. \\] <p>We multiply and divide by \\(q_\\phi(z|x)\\):</p> \\[ \\log p(x) = \\log \\int q_\\phi(z|x)\\, \\frac{p(x,z)}{q_\\phi(z|x)}\\,dz. \\] <p>Applying Jensen\u2019s inequality yields:</p> \\[ \\log p(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p(x,z)}{q_\\phi(z|x)} \\right]. \\] <p>This expression is the ELBO:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x,z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]. \\] <p>A useful identity reveals:</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Since KL divergence is non-negative:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) \\le \\log p(x). \\] <p>Maximizing the ELBO is equivalent to minimizing the KL divergence between \\(q_\\phi(z|x)\\) and the true posterior.</p>"},{"location":"informationtheory/7b_vi/#4-interpreting-the-elbo","title":"4. Interpreting the ELBO","text":"<p>The ELBO can be decomposed into two terms that have clear interpretations. Writing</p> \\[ p(x,z) = p_\\theta(x|z)p(z), \\] <p>and substituting into the ELBO gives:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\]"},{"location":"informationtheory/7b_vi/#reconstruction-term","title":"Reconstruction term","text":"\\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]. \\] <p>This term ensures that \\(z\\) captures enough information to generate or reconstruct the observed data. It corresponds to likelihood or reconstruction accuracy.</p>"},{"location":"informationtheory/7b_vi/#regularization-term","title":"Regularization term","text":"\\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>This term ensures that the posterior approximation does not drift too far from the prior. In VAEs, the prior is usually a Gaussian, making the latent space smooth and structured.</p> <p>The ELBO therefore expresses a balance:</p> <ul> <li>the first term rewards informative latent variables  </li> <li>the second term penalizes overly complex or irregular latent distributions  </li> </ul>"},{"location":"informationtheory/7b_vi/#5-why-vi-uses-reverse-kl","title":"5. Why VI Uses Reverse KL","text":"<p>Variational inference minimizes</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)), \\] <p>which is reverse KL. Reverse KL has important behavioral properties:</p> <ul> <li>it heavily penalizes assigning probability mass where the true posterior is low  </li> <li>it allows \\(q\\) to ignore some modes of \\(p(z|x)\\) </li> <li>it prefers tight, conservative approximations  </li> </ul> <p>As a result:</p> <ul> <li>VI tends to be mode seeking  </li> <li>it focuses on a single high-density region  </li> <li>it can miss multimodal structure of the true posterior  </li> </ul> <p>This behavior explains why VAEs sometimes produce smooth or blurry samples: the latent space favors safe, central modes.</p>"},{"location":"informationtheory/7b_vi/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":"<p>A VAE applies variational inference to a deep latent-variable model. It introduces:</p> <ol> <li>a latent prior </li> <li>a decoder (generative model) </li> <li>an encoder (variational posterior) </li> </ol> <p>The encoder and decoder are neural networks, trained jointly by maximizing the ELBO over all data points.</p>"},{"location":"informationtheory/7b_vi/#61-generative-model","title":"6.1 Generative model","text":"<p>Given \\(z\\) sampled from the prior, the decoder produces a distribution over possible \\(x\\):</p> \\[ p_\\theta(x|z). \\]"},{"location":"informationtheory/7b_vi/#62-inference-model","title":"6.2 Inference model","text":"<p>The encoder produces the parameters of the approximate posterior:</p> \\[ q_\\phi(z|x) = \\mathcal{N}(z\\mid \\mu_\\phi(x), \\sigma^2_\\phi(x)). \\] <p>This is the distribution used inside the ELBO.</p>"},{"location":"informationtheory/7b_vi/#63-vae-training-objective","title":"6.3 VAE training objective","text":"<p>The objective for each data point is:</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>The first term encourages correct reconstruction; the second keeps latent codes regularized.</p>"},{"location":"informationtheory/7b_vi/#7-the-reparameterization-trick","title":"7. The Reparameterization Trick","text":"<p>The expectation in the ELBO involves sampling from \\(q_\\phi(z|x)\\). To differentiate through this sampling step, VAEs use the reparameterization:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\odot\\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0,I). \\] <p>This expresses sampling as a deterministic transformation of noise, allowing gradients to flow through the encoder.</p> <p>This trick is central to making VI scalable and efficient in deep learning.</p>"},{"location":"informationtheory/7b_vi/#8-consequences-of-reverse-kl-in-vaes","title":"8. Consequences of Reverse KL in VAEs","text":"<p>The reverse KL term shapes the behavior of the VAE:</p> <ul> <li>it encourages smooth, overlapping latent regions  </li> <li>it prefers safe latent representations  </li> <li>it explains why VAEs sometimes produce blurry or conservative samples  </li> <li>it stabilizes training  </li> <li>it produces well-structured latent spaces  </li> </ul> <p>Extensions such as the \\(\\beta\\)-VAE, hierarchical VAEs, and flows inside the encoder allow for more expressive or disentangled representations.</p>"},{"location":"informationtheory/7b_vi/#9-variational-inference-beyond-vaes","title":"9. Variational Inference Beyond VAEs","text":"<p>VI provides a general-purpose framework for approximate inference in many settings.</p>"},{"location":"informationtheory/7b_vi/#bayesian-neural-networks","title":"Bayesian neural networks","text":"<p>Posterior distributions over weights are approximated by variational distributions:</p> \\[ q(w)\\approx p(w|D). \\]"},{"location":"informationtheory/7b_vi/#diffusion-models","title":"Diffusion models","text":"<p>The training objective resembles a variational bound on the data likelihood, using KL divergences between transition kernels.</p>"},{"location":"informationtheory/7b_vi/#normalizing-flows-for-vi","title":"Normalizing flows for VI","text":"<p>Flows can produce more expressive variational posteriors than simple Gaussians.</p>"},{"location":"informationtheory/7b_vi/#reinforcement-learning","title":"Reinforcement learning","text":"<p>Entropy-regularized RL and soft Q-learning can be interpreted through variational principles.</p> <p>VI therefore offers a unifying viewpoint across deep generative models, Bayesian inference, and probabilistic deep learning.</p> <p>Variational inference replaces an intractable posterior distribution with a tractable approximation and optimizes this approximation by maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, capturing the trade-off between accuracy and complexity. VAEs are an important application of VI, using neural networks to parameterize both the generative model and the approximate posterior. Reverse KL explains the conservative behavior of VI-based models. Variational inference provides a flexible approach for approximate Bayesian inference and underlies many modern generative and representation-learning techniques.</p>"},{"location":"informationtheory/8_reperesentation/","title":"8 reperesentation","text":""},{"location":"informationtheory/8_reperesentation/#chapter-5-representation-learning-mutual-information-and-the-information-bottleneck","title":"Chapter 5 \u2014 Representation Learning, Mutual Information, and the Information Bottleneck","text":"<p>Representation learning seeks transformations of data that make tasks such as prediction, compression, and reasoning easier. A representation \\(Z\\) is typically obtained by applying an encoder to an input \\(X\\). Information theory provides a natural way to formalize what makes a representation useful by analyzing the mutual information between \\(Z\\), the input \\(X\\), and the target \\(Y\\).</p> <p>This chapter introduces mutual information as a measure of shared structure, explains the Information Bottleneck framework, and connects these ideas to deep learning methods such as contrastive learning, VAEs, and self-supervised learning.</p>"},{"location":"informationtheory/8_reperesentation/#1-what-is-a-representation","title":"1. What Is a Representation?","text":"<p>A representation is a transformed form of input data:</p> \\[ Z = f_\\theta(X), \\] <p>where \\(f_\\theta\\) is usually a neural network. A good representation should satisfy two goals:</p> <ol> <li>It should retain information that is relevant for predicting \\(Y\\).  </li> <li>It should discard noise or irrelevant aspects of \\(X\\).</li> </ol> <p>Information theory allows us to express these goals using mutual information.</p>"},{"location":"informationtheory/8_reperesentation/#2-mutual-information-connecting-two-variables","title":"2. Mutual Information: Connecting Two Variables","text":"<p>Mutual information (MI) measures how much knowledge of one variable reduces uncertainty about another:</p> \\[ I(X;Y) = H(X) - H(X|Y). \\] <p>It can also be written as a KL divergence:</p> \\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)). \\] <p>MI is zero when \\(X\\) and \\(Y\\) are independent and increases as \\(Y\\) becomes more predictable from \\(X\\).</p> <p>In representation learning, we are often interested in the two quantities:</p> \\[ I(Z;Y), \\qquad I(Z;X). \\] <p>These measure how informative the representation \\(Z\\) is regarding the target \\(Y\\) and how much irrelevant detail from \\(X\\) is still present.</p>"},{"location":"informationtheory/8_reperesentation/#3-the-role-of-mi-in-representation-learning","title":"3. The Role of MI in Representation Learning","text":"<p>A representation \\(Z\\) is desirable when:</p> <ul> <li> <p>\\(I(Z;Y)\\) is large   (the representation captures features relevant to prediction)</p> </li> <li> <p>\\(I(Z;X)\\) is small   (the representation removes noise and redundancy)</p> </li> </ul> <p>This idea appears in supervised learning, contrastive methods, and generative modeling.</p> <p>Some examples:</p> <ul> <li>In supervised learning, we want features that preserve label information.  </li> <li>In contrastive learning, we want features that preserve the information common across augmented views.  </li> <li>In generative models, latent variables should retain structure that explains the data while avoiding unnecessary detail.</li> </ul>"},{"location":"informationtheory/8_reperesentation/#4-the-information-bottleneck-principle","title":"4. The Information Bottleneck Principle","text":"<p>The Information Bottleneck (IB) formalizes the trade-off between informativeness and compression. The goal is:</p> \\[ \\max I(Z;Y) \\quad \\text{s.t.} \\quad  I(Z;X) \\text{ is small}. \\] <p>This can be written as the Lagrangian:</p> \\[ \\mathcal{L}_{\\text{IB}} = I(Z;Y) - \\beta I(Z;X). \\] <p>The parameter \\(\\beta\\) controls how aggressively the representation is compressed.</p> <ul> <li>Large \\(\\beta\\) leads to simpler, more compressed representations.  </li> <li>Small \\(\\beta\\) allows more expressive, detailed representations.</li> </ul> <p>IB provides a theoretical explanation for the behavior of learned features in deep neural networks.</p>"},{"location":"informationtheory/8_reperesentation/#5-deep-learning-and-the-information-bottleneck","title":"5. Deep Learning and the Information Bottleneck","text":"<p>IB theory suggests several statements about deep networks:</p> <ol> <li>Early layers preserve much of the information in \\(X\\).  </li> <li>Later layers tend to compress \\(X\\) while emphasizing information predictive of \\(Y\\).  </li> <li>Networks may first memorize and later compress during training.  </li> <li>Generalization is linked to discarding unnecessary information.</li> </ol> <p>Although the exact dynamics remain debated, the overall perspective helps interpret the evolution of features during training.</p>"},{"location":"informationtheory/8_reperesentation/#6-variational-information-bottleneck-vib","title":"6. Variational Information Bottleneck (VIB)","text":"<p>Mutual information terms are often difficult to compute directly. The Variational Information Bottleneck approximates them using variational distributions.</p> <p>We treat the representation as a random variable drawn from \\(q(z|x)\\) and estimate MI with tractable terms. The VIB objective is:</p> \\[ \\mathcal{L}_{\\text{VIB}} = \\mathbb{E}_{p(x,y)}\\! \\left[ \\mathbb{E}_{q(z|x)}\\![\\log p(y|z)] \\right] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>This resembles the VAE objective, but \\(p(y|z)\\) replaces the reconstruction term. The first term encourages predictive features, while the KL term compresses the representation.</p> <p>VIB therefore provides a practical implementation of the Information Bottleneck.</p>"},{"location":"informationtheory/8_reperesentation/#7-mutual-information-and-contrastive-learning","title":"7. Mutual Information and Contrastive Learning","text":"<p>Contrastive learning uses mutual information to learn representations without labels. The idea is:</p> <ul> <li>Generate two augmented views of the same input: \\((x_1, x_2)\\).  </li> <li>Encode them as \\((z_1, z_2)\\).  </li> <li>Encourage \\(z_1\\) and \\(z_2\\) to be similar.  </li> <li>Encourage representations of different inputs to be dissimilar.</li> </ul> <p>This encourages \\(Z\\) to retain the information that is preserved under augmentation, while ignoring irrelevant aspects of the input.</p> <p>Many methods follow this structure:</p> <ul> <li>SimCLR  </li> <li>MoCo  </li> <li>BYOL  </li> <li>InfoNCE  </li> <li>CPC  </li> <li>Deep InfoMax  </li> </ul> <p>The InfoNCE objective is:</p> \\[ \\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}\\left[ \\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)} {\\sum_k \\exp(\\text{sim}(z_i,z_k)/\\tau)} \\right], \\] <p>where \\((i,j)\\) is a positive pair. InfoNCE is a variational lower bound on \\(I(Z_1;Z_2)\\).</p>"},{"location":"informationtheory/8_reperesentation/#8-mi-in-generative-modeling","title":"8. MI in Generative Modeling","text":"<p>Mutual information also appears in generative models:</p>"},{"location":"informationtheory/8_reperesentation/#81-vaes","title":"8.1 VAEs","text":"<p>The KL term controls the structure and redundancy of \\(Z\\), and the decoder ensures \\(I(Z;X)\\) stays large enough for accurate reconstruction.</p>"},{"location":"informationtheory/8_reperesentation/#82-infogan","title":"8.2 InfoGAN","text":"<p>This model maximizes:</p> \\[ I(c; G(z,c)), \\] <p>encouraging the generator to learn interpretable latent factors.</p>"},{"location":"informationtheory/8_reperesentation/#83-normalizing-flows","title":"8.3 Normalizing flows","text":"<p>Flows maintain \\(I(X;Z) = H(X)\\) because they are invertible; they do not compress the input.</p>"},{"location":"informationtheory/8_reperesentation/#84-diffusion-models","title":"8.4 Diffusion models","text":"<p>Diffusion models gradually reduce noise and can be interpreted using information-theoretic ideas related to KL divergence and score matching.</p>"},{"location":"informationtheory/8_reperesentation/#9-mi-and-disentanglement","title":"9. MI and Disentanglement","text":"<p>Disentangled representations aim to separate independent generative factors such as orientation or color. The \\(\\beta\\)-VAE objective:</p> \\[ \\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)) \\] <p>encourages disentanglement by increasing compression in the latent space. A larger \\(\\beta\\) pushes different dimensions of \\(Z\\) to encode more independent aspects of the data.</p>"},{"location":"informationtheory/8_reperesentation/#10-estimating-mi-in-high-dimensions","title":"10. Estimating MI in High Dimensions","text":"<p>Mutual information is difficult to compute exactly in high dimensions. Neural estimation relies on variational bounds such as:</p> <ul> <li>InfoNCE  </li> <li>NWJ bound  </li> <li>Donsker\u2013Varadhan bound  </li> <li>MINE estimator  </li> <li>f-divergence lower bounds  </li> </ul> <p>These allow MI to be used in representation learning even when the true quantities are intractable.</p>"},{"location":"informationtheory/8_reperesentation/#11-summary-of-chapter-5","title":"11. Summary of Chapter 5","text":"<p>Mutual information provides a principled measure of what makes a useful representation: it should retain information relevant for prediction and discard irrelevant detail. The Information Bottleneck formalizes this trade-off and motivates practical methods such as the Variational Information Bottleneck.</p> <p>Contrastive learning methods maximize MI between augmented views, enabling self-supervised representation learning. Generative models such as VAEs, GANs, flows, and diffusion models each manipulate mutual information in different ways, leading to distinct behaviors and capabilities.</p> <p>Information theory therefore provides a unified lens through which to understand representation learning in modern deep networks.</p>"},{"location":"informationtheory/intro/","title":"Intro","text":"<p>https://www.inference.org.uk/itprnn_lectures/</p> <p>https://www.youtube.com/watch?v=sN_0iGWcyLI&amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&amp;index=12</p> <p>60mins Lect 12</p> <p>Application in  1. GANS</p>"},{"location":"informationtheory/intro/#game-theory","title":"Game theory","text":"<ul> <li>zero sum?</li> <li>min max V(D,G)</li> <li>nash equiblria</li> </ul> <p>Application in GAN</p>"},{"location":"llms/chahllanges/","title":"Chahllanges","text":"<p>Data Challenges: This pertains to the data used for training and how the model addresses gaps or missing data. Ethical Challenges: This involves addressing issues such as mitigating biases, ensuring privacy, and preventing the generation of harmful content in the deployment of LLMs. Technical Challenges: These challenges focus on the practical implementation of LLMs. Deployment Challenges: Concerned with the specific processes involved in transitioning fully-functional LLMs into real-world use-cases (productionization) Data Challenges:</p> <p>Data Bias: The presence of prejudices and imbalances in the training data leading to biased model outputs. Limited World Knowledge and Hallucination: LLMs may lack comprehensive understanding of real-world events and information and tend to hallucinate information. Note that training them on new data is a long and expensive process. Dependency on Training Data Quality: LLM performance is heavily influenced by the quality and representativeness of the training data. Ethical and Social Challenges:</p> <p>Ethical Concerns: Concerns regarding the responsible and ethical use of language models, especially in sensitive contexts. Bias Amplification: Biases present in the training data may be exacerbated, resulting in unfair or discriminatory outputs. Legal and Copyright Issues: Potential legal complications arising from generated content that infringes copyrights or violates laws. User Privacy Concerns: Risks associated with generating text based on user inputs, especially when dealing with private or sensitive information. Technical Challenges:</p> <p>Computational Resources: Significant computing power required for training and deploying large language models. Interpretability: Challenges in understanding and explaining the decision-making process of complex models. Evaluation: Evaluation presents a notable challenge as assessing models across diverse tasks and domains is inadequately designed, particularly due to the challenges posed by freely generated content. Fine-tuning Challenges: Difficulties in adapting pre-trained models to specific tasks or domains. Contextual Understanding: LLMs may face challenges in maintaining coherent context over longer passages or conversations. Robustness to Adversarial Attacks: Vulnerability to intentional manipulations of input data leading to incorrect outputs. Long-Term Context: Struggles in maintaining context and coherence over extended pieces of text or discussions. Deployment Challenges:</p> <p>Scalability: Ensuring that the model can scale efficiently to handle increased workloads and demand in production environments. Latency: Minimizing the response time or latency of the model to provide quick and efficient interactions, especially in real-time applications. Monitoring and Maintenance: Implementing robust monitoring systems to track model performance, detect issues, and perform regular maintenance to avoid downtime. Integration with Existing Systems: Ensuring smooth integration of LLMs with existing software, databases, and infrastructure within an organization. Cost Management: Optimizing the cost of deploying and maintaining large language models, as they can be resource-intensive in terms of both computation and storage. Security Concerns: Addressing potential security vulnerabilities and risks associated with deploying language models in production, including safeguarding against malicious attacks. Interoperability: Ensuring compatibility with other tools, frameworks, or systems that may be part of the overall production pipeline. User Feedback Incorporation: Developing mechanisms to incorporate user feedback to continuously improve and update the model in a production environment. Regulatory Compliance: Adhering to regulatory requirements and compliance standards, especially in industries with strict data protection and privacy regulations. Dynamic Content Handling: Managing the generation of text in dynamic environments where content and user interactions change frequently.</p> <p>Types of Domain Adaptation Methods There are several methods to incorporate domain-specific knowledge into LLMs, each with its own advantages and limitations. Here are three classes of approaches:</p> <p>Domain-Specific Pre-Training:</p> <p>Training Duration: Days to weeks to months Summary: Requires a large amount of domain training data; can customize model architecture, size, tokenizer, etc. In this method, LLMs are pre-trained on extensive datasets representing various natural language use cases. For instance, models like PaLM 540B, GPT-3, and LLaMA 2 have been pre-trained on datasets with sizes ranging from 499 billion to 2 trillion tokens. Examples of domain-specific pre-training include models like ESMFold, ProGen2 for protein sequences, Galactica for science, BloombergGPT for finance, and StarCoder for code. These models outperform generalist models within their domains but still face limitations in terms of accuracy and potential hallucinations.</p> <p>Domain-Specific Fine-Tuning:</p> <p>Training Duration: Minutes to hours Summary: Adds domain-specific data; tunes for specific tasks; updates LLM model Fine-tuning involves training a pre-trained LLM on a specific task or domain, adapting its knowledge to a narrower context. Examples include Alpaca (fine-tuned LLaMA-7B model for general tasks), xFinance (fine-tuned LLaMA-13B model for financial-specific tasks), and ChatDoctor (fine-tuned LLaMA-7B model for medical chat). The costs for fine-tuning are significantly smaller compared to pre-training.</p> <p>Retrieval Augmented Generation (RAG):</p> <p>Training Duration: Not required Summary: No model weights; external information retrieval system can be tuned RAG involves grounding the LLM's parametric knowledge with external or non-parametric knowledge from an information retrieval system. This external knowledge is provided as additional context in the prompt to the LLM. The advantages of RAG include no training costs, low expertise requirement, and the ability to cite sources for human verification. This approach addresses limitations such as hallucinations and allows for precise manipulation of knowledge. The knowledge base is easily updatable without changing the LLM. Strategies to combine non-parametric knowledge with an LLM's parametric knowledge are actively researched.</p> <p>Use Domain-Specific Pre-Training When: Exclusive Domain Focus: Pre-training is suitable when you require a model exclusively trained on data from a specific domain, creating a specialized language model for that domain. Customizing Model Architecture: It allows you to customize various aspects of the model architecture, size, tokenizer, etc., based on the specific requirements of the domain. Extensive Training Data Available: Effective pre-training often requires a large amount of domain-specific training data to ensure the model captures the intricacies of the chosen domain. Use Domain-Specific Fine-Tuning When: Specialization Needed: Fine-tuning is suitable when you already have a pre-trained LLM, and you want to adapt it for specific tasks or within a particular domain. Task Optimization: It allows you to adjust the model's parameters related to the task, such as architecture, size, or tokenizer, for optimal performance in the chosen domain. Time and Resource Efficiency: Fine-tuning saves time and computational resources compared to training a model from scratch since it leverages the knowledge gained during the pre-training phase. Use RAG When: Information Freshness Matters: RAG provides up-to-date, context-specific data from external sources. Reducing Hallucination is Crucial: Ground LLMs with verifiable facts and citations from an external knowledge base. Cost-Efficiency is a Priority: Avoid extensive model training or fine-tuning; implement without the need for training.</p>"},{"location":"llms/foundations/","title":"Foundations","text":""},{"location":"monetcarlo_simulations/plan/","title":"Plan","text":"<ol> <li>Markov Chain</li> </ol>"},{"location":"nn_training/1_intro/","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#understanding-autograd-the-engine-behind-deep-learning-with-a-micrograd-style-walkthrough","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#what-is-autograd","title":"What is Autograd?","text":"<p>Autograd \u2014 short for automatic differentiation \u2014 is a computational technique that automatically computes derivatives of functions expressed as computer programs. It is the mathematical and computational backbone of deep learning frameworks like PyTorch, TensorFlow, and JAX.</p> <p>At its core, autograd implements reverse-mode automatic differentiation, an algorithm that efficiently computes gradients of a scalar output (such as a loss) with respect to many input parameters (model weights).</p>"},{"location":"nn_training/1_intro/#how-it-works","title":"How It Works","text":"<p>When a function is executed, autograd records all elementary operations (addition, multiplication, non-linearities, etc.) in a computational graph. Each node represents a tensor or scalar value, and each edge represents an operation with a known local derivative.</p> <p>During the forward pass, the graph is constructed dynamically. During the backward pass, the engine traverses the graph in reverse order, applying the chain rule to compute gradients:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy}\\cdot\\frac{dy}{dx}. \\] <p>This process is often referred to as back-propagation. In practice, the framework automatically handles these derivative computations.</p> <p>For example, in Andrej Karpathy\u2019s micrograd, a minimal autograd engine, each scalar <code>Value</code> object keeps track of both its data and gradient, as well as the operation that produced it. The <code>.backward()</code> method propagates gradients backward through the graph, applying local chain rules for each operation.</p>"},{"location":"nn_training/1_intro/#differentiation-methods-overview","title":"Differentiation Methods Overview","text":"Method Description Pros Cons Numerical Finite difference approximation Simple Inaccurate, slow Symbolic Algebraic manipulation (e.g., SymPy) Exact Symbol explosion, not scalable Automatic (AD) Local derivatives + chain rule Exact, efficient Requires graph bookkeeping <p>Unlike numerical differentiation (which is approximate) or symbolic differentiation (which manipulates expressions), autograd computes exact derivatives efficiently by chaining local gradients.</p>"},{"location":"nn_training/1_intro/#why-use-autograd","title":"Why Use Autograd?","text":"<ol> <li> <p>Eliminates Manual Derivative Computation    Without autograd, practitioners would need to derive and code gradients manually for each model parameter. This is not only tedious but error-prone, especially for complex architectures.</p> </li> <li> <p>Ensures Correctness and Reliability    By systematically applying the chain rule, autograd frameworks guarantee correct gradient flow through even the most intricate models, reducing human error.</p> </li> <li> <p>Supports Dynamic and Flexible Graphs    Modern frameworks like PyTorch and micrograd construct computation graphs dynamically \u2014 rebuilding them on each forward pass. This allows for loops, conditionals, and recursion within model definitions.</p> </li> <li> <p>Caches Intermediate Results    Autograd stores intermediate activations during the forward pass so they can be reused efficiently during the backward pass. This improves computational speed but increases memory usage.</p> </li> <li> <p>Higher-Order Derivatives    Since the backward pass itself is differentiable, autograd can compute higher-order derivatives \u2014 useful in meta-learning, optimization research, and differentiable physics.</p> </li> <li> <p>Performance and Hardware Optimization    Frameworks optimize backward passes using techniques like operation fusion and kernel caching, ensuring gradient computations remain efficient on GPUs and TPUs.</p> </li> </ol> <p>A minimal implementation like micrograd reveals these mechanics transparently, allowing students and researchers to understand what happens under the hood of massive frameworks.</p>"},{"location":"nn_training/1_intro/#importance-in-deep-learning","title":"Importance in Deep Learning","text":""},{"location":"nn_training/1_intro/#1-the-foundation-of-backpropagation","title":"1. The Foundation of Backpropagation","text":"<p>Training neural networks relies on minimizing a loss function \\(L(\\theta)\\) with respect to parameters \\(\\theta\\). The update rule for parameters (via gradient descent) is:</p> \\[ \\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}. \\] <p>Here, autograd automates the computation of \\(\\frac{\\partial L}{\\partial \\theta}\\) \u2014 the essential ingredient of learning.</p>"},{"location":"nn_training/1_intro/#2-enabling-complex-architectures","title":"2. Enabling Complex Architectures","text":"<p>Modern networks (e.g., Transformers, ResNets, GNNs) have deep stacks, skip connections, and nonlinear branches. Autograd ensures that gradients flow correctly through these complex graphs \u2014 enabling architectural innovation without requiring users to manually derive derivatives.</p>"},{"location":"nn_training/1_intro/#3-scalability-and-efficiency","title":"3. Scalability and Efficiency","text":"<p>Reverse-mode AD (autograd) is ideal for functions mapping many inputs to a single scalar output \u2014 exactly the case for deep learning. Its computational cost is roughly proportional to the cost of the forward pass, but with a higher memory footprint.</p> <p>Compute\u2013Memory Trade-off:</p> <ul> <li>Compute: The backward pass roughly doubles compute time.  </li> <li>Memory: Storing intermediate activations increases RAM/GPU usage.</li> </ul> <p>Frameworks mitigate this using gradient checkpointing, where certain intermediate activations are recomputed on-demand to save memory.</p>"},{"location":"nn_training/1_intro/#a-micrograd-style-value-class-with-line-by-line-commentary","title":"A micrograd-style <code>Value</code> class \u2014 with line-by-line commentary","text":"<p>Below is a faithful, lightly extended micrograd-style engine. Every key line is annotated to explain what it references and why it matters for autograd.</p> <pre><code>import math\n\nclass Value:\n    # ---------------------- Initialization ----------------------\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data                # (float) the scalar value of this node\n        self.grad = 0.0                 # (float) d(output)/d(this node), filled during backprop\n        self._backward = lambda: None   # a closure set by each op to push grad to parents\n        self._prev = set(_children)     # (set[Value]) parents (inputs) that produced this node\n        self._op = _op                  # (str) op name for graph/debug ('+','*','tanh','exp','k',...)\n        self.label = label              # (str) optional name for visualization\n\n    def __repr__(self):\n        # nice debug print to see the forward value\n        return f\"Value(data={self.data})\"\n\n    # ---------------------- Binary Ops: + ----------------------\n    def __add__(self, other):\n        # allow mixing with Python scalars: 2 + Value(3)\n        other = other if isinstance(other, Value) else Value(other)\n\n        # forward pass: create the child node 'out' from parents (self, other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # local partials for z = x + y are \u2202z/\u2202x = 1, \u2202z/\u2202y = 1\n            # chain rule: x.grad += 1 * out.grad; y.grad += 1 * out.grad\n            self.grad  += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward       # attach the gradient propagation rule to 'out'\n        return out\n\n    def __radd__(self, other):\n        # support Python's other + self\n        return self + other\n\n    # ---------------------- Binary Ops: * ----------------------\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # for z = x * y: \u2202z/\u2202x = y, \u2202z/\u2202y = x\n            self.grad  += other.data * out.grad\n            other.grad += self.data  * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        # support Python's other * self\n        return self * other\n\n    # ---------------------- Power, Neg, Sub, Div ----------------------\n    def __pow__(self, other):\n        # only scalar exponents for simplicity\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.dataother, (self,), f'{other}')\n\n        def _backward():\n            # for z = x^k: \u2202z/\u2202x = k * x^(k-1)\n            self.grad += other * (self.data  (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def __truediv__(self, other):  # self / other\n        # use x / y = x * y^{-1}\n        return self * (other  -1)\n\n    def __neg__(self):             # -self\n        # use -x = (-1) * x\n        return self * -1\n\n    def __sub__(self, other):      # self - other\n        # x - y = x + (-y)\n        return self + (-other)\n\n    # ---------------------- Nonlinearities ----------------------\n    def tanh(self):\n        # forward: compute t = tanh(x) (closed form used here; math.tanh is fine too)\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n\n        def _backward():\n            # derivative: d/dx tanh(x) = 1 - tanh(x)^2 = 1 - t^2\n            self.grad += (1 - t2) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        # forward: e^x\n        x = self.data\n        out = Value(math.exp(x), (self,), 'exp')\n\n        def _backward():\n            # derivative: d/dx e^x = e^x; note e^x is out.data\n            self.grad += out.data * out.grad\n        out._backward = _backward\n        return out\n\n    # ---------------------- Backprop Driver ----------------------\n    def backward(self):\n        # Build a topological ordering of the graph so every node's\n        # _backward() runs after all of its children have pushed grads.\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for parent in v._prev:   # traverse to parents (inputs)\n                    build_topo(parent)\n                topo.append(v)            # append after traversing parents\n\n        build_topo(self)\n\n        # seed the gradient at the output node: d(self)/d(self) = 1\n        self.grad = 1.0\n\n        # go in reverse topological order and apply each node's local chain rule\n        for node in reversed(topo):\n            node._backward()\n</code></pre>"},{"location":"nn_training/1_intro/#what-each-attributemethod-references","title":"What each attribute/method references","text":"<ul> <li><code>self.data</code>: the scalar numeric value stored at this node (forward pass result).  </li> <li><code>self.grad</code>: the accumulated derivative \\(\\frac{\\partial \\text{(final output)}}{\\partial \\text{this node}}\\) after <code>.backward()</code>.  </li> <li><code>self._prev</code>: a set of parent nodes (inputs) that produced <code>self</code>; used to traverse the graph.  </li> <li><code>self._op</code>: operation label for debugging/visualization.  </li> <li><code>self._backward</code>: a closure that knows how to push gradient from this node back to its parents using local partial derivatives.  </li> <li>Binary ops (<code>__add__</code>, <code>__mul__</code>, etc.): create a new child node <code>out</code> from parent nodes <code>(self, other)</code> and attach a <code>_backward</code> rule encoding the local Jacobian.  </li> <li><code>backward()</code>: performs a reverse topological traversal starting from the target scalar node, seeding its gradient with <code>1.0</code>, then calling every node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#worked-example-build-a-small-graph-and-differentiate","title":"Worked example: build a small graph and differentiate","text":"<p>We\u2019ll compute  and obtain gradients \\(\\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\frac{\\partial f}{\\partial c}\\).</p> <pre><code># create leaf nodes (parameters / inputs)\na = Value(2.0, label='a')\nb = Value(3.0, label='b')\nc = Value(0.5, label='c')\n\n# forward build\nd = a * b            # d = a*b\ne = c.tanh()         # e = tanh(c)\nf = d + e - 0.5*(a2)\n\n# backpropagate from scalar output 'f'\nf.backward()\n\nprint(\"f:\", f.data)\nprint(\"df/da:\", a.grad)\nprint(\"df/db:\", b.grad)\nprint(\"df/dc:\", c.grad)\n</code></pre>"},{"location":"nn_training/1_intro/#hand-derivative-sanity-check","title":"Hand-derivative sanity check","text":"<ul> <li>\\(d = a b \\Rightarrow \\frac{\\partial d}{\\partial a} = b,\\; \\frac{\\partial d}{\\partial b} = a\\) </li> <li>\\(e = \\tanh(c) \\Rightarrow \\frac{\\partial e}{\\partial c} = 1 - \\tanh^2(c)\\) </li> <li>\\(f = d + e - \\tfrac{1}{2} a^2\\)</li> </ul> <p>Therefore:  </p> <p>Your printed grads should match these values numerically (up to floating point).</p>"},{"location":"nn_training/1_intro/#how-methods-reference-values-and-variables-naming-clarity","title":"How methods reference values and variables (naming clarity)","text":"<ul> <li>In methods like <code>__add__</code> and <code>__mul__</code>, <code>self</code> is the left operand, <code>other</code> is the right operand (which we coerce to <code>Value</code> when it\u2019s a Python scalar).  </li> <li>The new node created by an operation is named <code>out</code>. It references:</li> <li><code>out.data</code>: the forward result of the op.  </li> <li><code>out._prev</code>: the set <code>{self, other}</code> \u2014 i.e., the parents that produced <code>out</code>.  </li> <li><code>out._backward</code>: a closure capturing <code>self</code>, <code>other</code>, and <code>out</code>, used to push gradient contributions back to <code>self.grad</code> and <code>other.grad</code> via local partials.</li> <li>During <code>.backward()</code>, we compute a topological order over the graph using <code>_prev</code> links (parents). We seed the target node\u2019s gradient with <code>1.0</code>, then walk in reverse order, calling each node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#practical-notes-tips","title":"Practical notes &amp; tips","text":"<ul> <li>Zeroing grads: Before a new backward pass, set <code>.grad = 0.0</code> for all leaves to avoid mixing gradients across iterations, just like <code>optimizer.zero_grad()</code> in PyTorch.  </li> <li>Numerical stability: Prefer <code>math.tanh(x)</code> to the closed form for large \\(|x|\\).  </li> <li>Extensibility: New ops just need (1) a forward value, (2) parent tracking in <code>_prev</code>, and (3) a <code>_backward</code> closure with correct local derivatives.  </li> <li>Scalars vs. tensors: This toy engine is scalar-valued. Full frameworks generalize this to tensors, broadcasting rules, and highly optimized kernels.</li> </ul>"},{"location":"nn_training/1_intro/#references","title":"References","text":"<ul> <li>Karpathy, A. micrograd (minimal autograd engine).  </li> <li>PyTorch documentation: Autograd mechanics.  </li> <li>D2L.ai: Automatic differentiation.</li> </ul>"},{"location":"nn_training/2_initial/","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#practical-guide-to-weight-initialization-activation-distributions-dead-neurons-and-gradient-flow-in-deep-neural-networks","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#1-why-weight-initialization-matters","title":"1. Why Weight Initialization Matters","text":"<p>Initialization determines:</p> <ul> <li>how activations propagate forward  </li> <li>how gradients propagate backward  </li> <li>whether neurons remain active  </li> <li>whether the optimizer can begin learning  </li> <li>whether training is stable or diverges  </li> </ul> <p>Poor initialization leads to:</p> <ul> <li>vanishing gradients  </li> <li>exploding gradients  </li> <li>saturated activations (tanh = \u00b11)  </li> <li>dead neurons (ReLU stuck at 0)  </li> <li>slow \u201chockey-stick\u201d learning curves  </li> <li>unstable early training  </li> </ul> <p>Modern deep learning succeeds because initializations are designed to maintain statistical stability across depth.</p>"},{"location":"nn_training/2_initial/#2-the-role-of-n-why-we-divide-by-sqrtn","title":"2. The Role of \\(N\\): Why We Divide by \\(\\sqrt{N}\\)","text":"<p>Consider a neuron:</p> \\[ z = \\sum_{i=1}^{N} w_i x_i \\] <p>If weights and inputs are independent and zero-mean:</p> \\[ \\mathrm{Var}(z) = N \\cdot \\mathrm{Var}(w) \\cdot \\mathrm{Var}(x) \\] <p>Thus:</p> <ul> <li>large \\(N\\) \u2192 exploded activations  </li> <li>small \\(N\\) \u2192 collapsed activations  </li> </ul> <p>To keep the variance stable:</p> \\[ \\mathrm{Var}(w) = \\frac{1}{N} \\quad \\Rightarrow \\quad \\text{std}(w) = \\frac{1}{\\sqrt{N}} \\] <p>This is the core idea behind all modern weight initializers.</p>"},{"location":"nn_training/2_initial/#what-exactly-is-n","title":"What exactly is \\(N\\)?","text":"<p>It depends on the layer:</p> <ul> <li>Dense layer: \\(N = \\text{fan\\_in}\\) </li> <li>Conv layer: \\(N = \\text{in\\_channels} \\times k_h \\times k_w\\) </li> <li>Transformer linear layer: \\(N = d_{\\text{model}}\\) or \\(d_{\\text{ff}}\\) </li> <li>RNN input/recurrent matrix: input or hidden size  </li> </ul> <p>The goal is always to stabilize forward and backward signal flow.</p>"},{"location":"nn_training/2_initial/#3-activation-functions-and-their-stability-regions","title":"3. Activation Functions and Their Stability Regions","text":""},{"location":"nn_training/2_initial/#tanh","title":"Tanh","text":"<ul> <li>useful only near 0  </li> <li>saturates at \u00b11 outside a small input range  </li> <li>derivative approaches 0 \u2192 vanishing gradients  </li> </ul>"},{"location":"nn_training/2_initial/#relu","title":"ReLU","text":"<p>  - linear for \\(z &gt; 0\\) - zero output + zero gradient for \\(z \\le 0\\) </p>"},{"location":"nn_training/2_initial/#gelu-silu-swish-softplus","title":"GELU / SiLU (Swish) / SoftPlus","text":"<ul> <li>smoother transitions  </li> <li>reduce probability of dead neurons  </li> <li>default in Transformers (GELU)</li> </ul> <p>Initialization must place activations in the high-gradient region of whichever activation is used.</p>"},{"location":"nn_training/2_initial/#4-how-tanh-saturation-happens","title":"4. How Tanh Saturation Happens","text":"<p>If pre-activations \\(z\\) have high variance, tanh outputs cluster near \u00b11:</p> <p>This is saturation.</p>"},{"location":"nn_training/2_initial/#why-its-bad","title":"Why it\u2019s bad:","text":"\\[ \\tanh'(z) = 1 - \\tanh^2(z) \\approx 0 \\] <p>Thus almost no gradient flows backward.</p> <p>Tanh neurons become functionally dead when always saturated.</p>"},{"location":"nn_training/2_initial/#5-dead-neurons-in-practice","title":"5. Dead Neurons in Practice","text":""},{"location":"nn_training/2_initial/#51-dead-tanh-neurons","title":"5.1 Dead Tanh Neurons","text":"<p>A tanh neuron is effectively dead if:</p> <ul> <li>\\(z\\) is always large positive or negative  </li> <li>output always \u00b11  </li> <li>derivative \u2248 0  </li> </ul> <p>Causes include:</p> <ul> <li>too-large initial weight variance  </li> <li>unnormalized inputs  </li> <li>deep networks without residuals or normalization  </li> </ul>"},{"location":"nn_training/2_initial/#52-dead-relu-neurons","title":"5.2 Dead ReLU Neurons","text":"<p>A ReLU neuron is dead if:</p> <ul> <li>\\(z \\le 0\\) for all inputs  </li> <li>output always 0  </li> <li>gradient always 0  </li> </ul> <p>Common causes:</p> <ul> <li>weights initialized too negative  </li> <li>bias drift  </li> <li>large learning rates  </li> <li>skewed input distributions  </li> <li>poor initial variance  </li> </ul> <p>Dead ReLUs can sometimes recover (with normalization), but often remain inactive.</p>"},{"location":"nn_training/2_initial/#6-classical-initialization-methods","title":"6. Classical Initialization Methods","text":""},{"location":"nn_training/2_initial/#61-xavier-glorot-tanh-sigmoid","title":"6.1 Xavier / Glorot (Tanh, Sigmoid)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}} \\] <p>Balances forward and backward variance.</p>"},{"location":"nn_training/2_initial/#62-he-kaiming-relu-gelu","title":"6.2 He / Kaiming (ReLU, GELU)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}}} \\] <p>Based on the fact that ReLU zeroes about half of normally distributed inputs.</p>"},{"location":"nn_training/2_initial/#63-orthogonal-initialization-rnns","title":"6.3 Orthogonal Initialization (RNNs)","text":"<p>Stabilizes recurrent dynamics by preserving vector norms.</p>"},{"location":"nn_training/2_initial/#64-lsuv-initialization","title":"6.4 LSUV Initialization","text":"<p>Adjusts initial weights empirically to achieve unit variance layer-by-layer.</p>"},{"location":"nn_training/2_initial/#7-modern-initialization-for-deep-architectures","title":"7. Modern Initialization for Deep Architectures","text":"<p>Deep networks (especially Transformers and ResNets) require additional considerations.</p>"},{"location":"nn_training/2_initial/#71-layernorm-rmsnorm","title":"7.1 LayerNorm / RMSNorm","text":"<p>LayerNorm normalizes activations:</p> <ul> <li>keeps means near zero  </li> <li>standard deviation controlled  </li> <li>prevents drift or saturation  </li> <li>stabilizes attention layers and deep MLP stacks  </li> </ul> <p>Transformers rely heavily on LayerNorm to make training depth-independent.</p>"},{"location":"nn_training/2_initial/#72-residual-connections","title":"7.2 Residual Connections","text":"<p>Residual blocks:</p> \\[ x_{l+1} = x_l + f(x_l) \\] <p>This provides:</p> <ul> <li>an identity path for forward activations  </li> <li>a direct gradient path backward  </li> <li>stability for very deep networks  </li> <li>reduced sensitivity to initialization  </li> </ul> <p>Residuals make it possible to train 50\u20131000+ layer networks.</p>"},{"location":"nn_training/2_initial/#73-transformer-specific-initialization","title":"7.3 Transformer-Specific Initialization","text":"<p>Transformers often use:</p> <ul> <li>weight variance similar to He init  </li> <li>embedding scaling by \\(1/\\sqrt{d_{\\text{model}}}\\) </li> <li>residual scaling by \\(1/\\sqrt{L}\\) or similar  </li> <li>pre-LayerNorm to stabilize depth  </li> <li>\u03bc-parameterization for width-scaling consistency  </li> <li>DeepNorm or FixUp for extremely deep models  </li> </ul> <p>Modern LLMs do not use plain Xavier or He initialization alone.</p>"},{"location":"nn_training/2_initial/#8-expected-loss-at-initialization","title":"8. Expected Loss at Initialization","text":"<p>For a softmax classifier with \\(C\\) classes:</p> \\[ \\mathbb{E}[L_{\\text{init}}] = \\log C \\] <p>This baseline helps diagnose early training issues:</p> <ul> <li>higher than expected \u2192 variance too large  </li> <li>lower \u2192 bias in logits or incorrect initialization  </li> </ul>"},{"location":"nn_training/2_initial/#9-diagnosing-initialization-problems-practical","title":"9. Diagnosing Initialization Problems (Practical)","text":""},{"location":"nn_training/2_initial/#symptom-cause-fix","title":"Symptom \u2192 Cause \u2192 Fix","text":""},{"location":"nn_training/2_initial/#1-loss-flatlines-early","title":"1. Loss flatlines early","text":"<ul> <li>Cause: saturation or dead neurons  </li> <li>Fix: use Xavier/He, reduce LR, add normalization  </li> </ul>"},{"location":"nn_training/2_initial/#2-relu-units-all-zero","title":"2. ReLU units all zero","text":"<ul> <li>Cause: dead ReLUs  </li> <li>Fix: He init, LeakyReLU, LayerNorm, smaller LR  </li> </ul>"},{"location":"nn_training/2_initial/#3-tanh-outputs-at-1","title":"3. Tanh outputs at \u00b11","text":"<ul> <li>Cause: activation variance too large  </li> <li>Fix: Xavier init, normalize inputs, reduce bias scale  </li> </ul>"},{"location":"nn_training/2_initial/#4-exploding-loss","title":"4. Exploding loss","text":"<ul> <li>Cause: weight scale too large  </li> <li>Fix: reduce std, residual scaling, gradient clipping  </li> </ul>"},{"location":"nn_training/2_initial/#5-hockey-stick-learning-curve","title":"5. Hockey-stick learning curve","text":"<ul> <li>Cause: poor initialization or poorly scheduled LR warmup  </li> <li>Fix: check activations, add normalization, adjust LR schedule  </li> </ul>"},{"location":"nn_training/2_initial/#10-architecture-specific-recommendations","title":"10. Architecture-Specific Recommendations","text":""},{"location":"nn_training/2_initial/#cnns","title":"CNNs","text":"<ul> <li>Init: He  </li> <li>Norm: BatchNorm  </li> <li>Activation: ReLU or GELU  </li> <li>Notes: BN stabilizes variance, makes init forgiving  </li> </ul>"},{"location":"nn_training/2_initial/#transformers-llms","title":"Transformers / LLMs","text":"<ul> <li>Init: He-like + residual scaling  </li> <li>Norm: LayerNorm or RMSNorm  </li> <li>Activation: GELU  </li> <li>Notes: initialization must consider depth and residual structure  </li> </ul>"},{"location":"nn_training/2_initial/#rnns","title":"RNNs","text":"<ul> <li>Init: orthogonal for recurrent matrices  </li> <li>Activation: tanh or ReLU  </li> <li>Notes: highly sensitive to saturation; normalization helps  </li> </ul>"},{"location":"nn_training/2_initial/#11-summary","title":"11. Summary","text":"<p>Initialization controls:</p> <ul> <li>activation scale  </li> <li>gradient scale  </li> <li>neuron activity  </li> <li>numerical stability  </li> <li>early learning speed  </li> </ul> <p>Modern deep learning relies on three pillars:</p> <ol> <li>Variance-preserving initialization (Xavier, He, scaled residuals)  </li> <li>Normalization layers (BN, LN, RMSNorm)  </li> <li>Residual connections ensuring robust gradient flow  </li> </ol> <p>Together, they make deep networks trainable, stable, and efficient.</p> <p>If your network isn't learning, the first suspects should be:</p> <ul> <li>initialization  </li> <li>activation distributions  </li> <li>normalization  </li> <li>residual pathways  </li> <li>learning rate  </li> </ul> <p>Understanding these fundamentals is essential for building stable, scalable modern neural networks.</p>"},{"location":"nn_training/3_crossentropy/","title":"Understanding Cross-Entropy Loss and Softmax:","text":""},{"location":"nn_training/3_crossentropy/#understanding-cross-entropy-loss-and-softmax","title":"Understanding Cross-Entropy Loss and Softmax:","text":""},{"location":"nn_training/3_crossentropy/#why-only-the-correct-class-appears-yet-every-class-learns","title":"Why Only the Correct Class Appears \u2014 Yet Every Class Learns","text":"<p>Cross-entropy is the default loss function for classification in modern deep learning. But one part often confuses learners:</p> <p>Why does cross-entropy only include the probability of the correct class, yet the network still updates all class logits?</p> <p>This article provides a clean, rigorous, and intuitive explanation.</p>"},{"location":"nn_training/3_crossentropy/#1-what-cross-entropy-actually-measures","title":"1. What Cross-Entropy Actually Measures","text":"<p>For a single sample with true class \\(y\\) and predicted probabilities \\(p_i\\) from softmax:</p> \\[ L = -\\log(p_{\\text{correct}}) = -\\log(p_y) \\] <p>It seems cross-entropy cares only about the correct class. And that part is true.</p> <p>But that\u2019s not the full story.</p>"},{"location":"nn_training/3_crossentropy/#2-softmax-creates-competition-among-classes","title":"2. Softmax Creates Competition Among Classes","text":"<p>Softmax converts logits \\(z_i\\) into probabilities:</p> \\[ p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\] <p>Softmax ensures:</p> \\[ \\sum_i p_i = 1 \\] <p>This couples all classes together. Increasing the logit of any incorrect class automatically reduces the probability of the correct class.</p> <p>Thus, even though the loss formula only includes \\(p_y\\), changing any logit \\(z_i\\) changes \\(p_y\\).</p> <p>This is why incorrect classes still influence the loss.</p>"},{"location":"nn_training/3_crossentropy/#3-the-real-reason-every-class-learns-the-gradient","title":"3. The Real Reason Every Class Learns: The Gradient","text":"<p>The most important fact in cross-entropy + softmax is this gradient:</p> \\[ \\frac{\\partial L}{\\partial z_i} = p_i - Y_i \\] <p>Where:</p> <ul> <li>\\(Y_i = 1\\) for the correct class</li> <li>\\(Y_i = 0\\) for all incorrect classes</li> </ul> <p>This single equation explains everything:</p> <ul> <li>For the correct class \\(i=y\\):</li> </ul> \\[ \\frac{\\partial L}{\\partial z_y} = p_y - 1 \\] <ul> <li>For every incorrect class:</li> </ul> \\[ \\frac{\\partial L}{\\partial z_i} = p_i \\]"},{"location":"nn_training/3_crossentropy/#all-incorrect-classes-get-gradients-proportional-to-their-predicted-probabilities","title":"\u2714 All incorrect classes get gradients proportional to their predicted probabilities","text":""},{"location":"nn_training/3_crossentropy/#the-correct-class-gets-pushed-upward","title":"\u2714 The correct class gets pushed upward","text":""},{"location":"nn_training/3_crossentropy/#incorrect-classes-get-pushed-downward","title":"\u2714 Incorrect classes get pushed downward","text":"<p>This makes the \u201ccompetition\u201d between classes mathematically explicit.</p> <p>Cross-entropy doesn't have to include incorrect probabilities explicitly \u2014 the gradient already penalizes them. -</p>"},{"location":"nn_training/3_crossentropy/#4-why-frameworks-dont-explicitly-apply-softmax","title":"4. Why Frameworks Don\u2019t Explicitly Apply Softmax","text":"<p>In PyTorch / TensorFlow, the loss takes logits, not probabilities:</p> <p>```python CrossEntropyLoss(logits, labels)</p>"},{"location":"nn_training/4_train_llms/","title":"4 train llms","text":"<p>GPT is trained through a multi-stage pipeline designed to gradually improve its knowledge, reasoning, alignment, and usefulness. This process consists of four key stages, each with its own purpose and techniques:</p> <p>Pretraining: The model is trained on vast amounts of raw internet text to learn general language patterns, grammar, facts, and reasoning. At this stage, it becomes good at predicting the next word but is not yet fine-tuned for helpful or safe responses.</p> <p>Supervised Fine-Tuning (SFT): Human experts provide high-quality examples of ideal question-answer pairs. The model learns to imitate helpful, context-aware, and instruction-following responses based on these demonstrations.</p> <p>Reward Modeling (RM): Human trainers compare multiple model-generated responses and select the better one. Using these preferences, a reward model is trained to judge response quality. This model guides future optimization. Reinforcement Learning (RLHF): The model generates responses, and the reward model scores them. Using reinforcement learning, the model improves itself to produce more helpful, safe, and aligned answers, maximizing quality and user satisfaction.</p> Stage Dataset Algorithm Model Notes Pretraining Raw internettext trillions of wordslow-quality, large quantity Language modelingpredict the next token Base model 1000s of GPUsmonths of trainingex: GPT, LLaMA, PaLMcan deploy this model Supervised Finetuning DemonstrationsIdeal Assistant responses,~10-100K (prompt, response)written by contractorslow quantity, high quality Language modelingpredict the next token SFT modelinit from base 1-100 GPUsdays of trainingex: Vicuna-13Bcan deploy this model Reward Modeling Comparisons100K\u20131M comparisonswritten by contractorslow quantity, high quality Binary classificationpredict rewards consistentwith preferences RM modelinit from SFT 1-100 GPUsdays of training Reinforcement Learning Prompts~10K-100K promptswritten by contractorslow quantity, high quality Reinforcement Learninggenerate tokens that maximizethe reward RL modelinit from SFT &amp; RM 1-100 GPUsdays of trainingex: ChatGPT, Claudecan deploy this model"},{"location":"reinforcement/10_offline_rl/","title":"10. Offline Reinforcement Learning","text":""},{"location":"reinforcement/10_offline_rl/#chapter-10-batch-offline-rl-policy-evaluation-optimization","title":"Chapter 10: Batch / Offline RL Policy Evaluation &amp; Optimization","text":"<p>Learning from the Past</p> <ul> <li>Learning from Past Human Demonstrations: Imitation Learning</li> <li>Learning from Past Human Preferences: RLHF and DPO</li> <li>Learning from Past Decisions and Actions: Offline RL</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-reinforcement-learning-a-different-approach","title":"Offline Reinforcement Learning: A Different Approach","text":"<p>Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.</p> <p>In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.</p> <p>Why Can\u2019t We Just Use Q-Learning?</p> <ul> <li>Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values</li> <li>But deadly triad of bootstrapping, function approximation and off policy, and can fail</li> </ul>"},{"location":"reinforcement/10_offline_rl/#batch-policy-evaluation-estimating-the-performance-of-a-policy","title":"Batch Policy Evaluation: Estimating the Performance of a Policy","text":"<p>Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.</p> <ol> <li> <p>Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.</p> <p>Specifically, it learns two main components from data: a reward function \\(\\hat{r}(s,a)\\) and transition dynamics \\(\\hat{P}(s' \\mid s,a)\\). These are learned via supervised learning on the offline dataset \\(D\\) of transitions collected by some behavior policy \\(\\pi_b\\). For example, \\(\\hat{r}(s,a)\\) can be trained by regression to predict the observed reward given state \\(s\\) and action \\(a\\), and \\(\\hat{P}(s' \\mid s,a)\\) can be fit to predict the next-state \\(s'\\) (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in \\(D\\) under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to \u201clearn the environment\u2019s rules\". Once learned, this model \\(\\hat{\\mathcal{M}} = (\\hat{P}, \\hat{r})\\) serves as a proxy for the real environment, which we can use for evaluating any policy \\(\\pi\\) without further real experience.</p> <p>It\u2019s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics \\(\\hat{P}\\) will be reliable only in regions covered by \\(D\\); if \\(\\pi\\) later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).</p> </li> <li> <p>Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.</p> </li> <li> <p>Importance Sampling:     This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy\u2019s value \u2013 assuming coverage (i.e. the target policy doesn\u2019t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithmic-outline-offline-policy-evaluation-via-model","title":"Algorithmic Outline - Offline Policy Evaluation via Model:","text":"<ol> <li> <p>Input: offline dataset \\(D\\) of transitions (from behavior \\(\\pi_b\\)), a policy \\(\\pi\\) to evaluate, discount \\(\\gamma\\).</p> </li> <li> <p>Model Learning: Fit \\(\\hat{P}(s'|s,a)\\) and \\(\\hat{r}(s,a)\\) using \\(D\\) (e.g. maximum likelihood estimation for dynamics, regression for rewards).</p> </li> <li> <p>Policy Evaluation: Initialize \\(V(s)=0\\) for all states (or some initial guess).</p> </li> <li> <p>Loop (Bellman backups using \\(\\hat{P},\\hat{r}\\)): For each state \\(s\\) in the state space (or a representative set of states):</p> </li> <li> <p>Compute \\(\\hat{R}^\\pi(s) = \\sum_a \\pi(a|s)\\hat{r}(s,a)\\).</p> </li> <li> <p>Compute \\(V_{\\text{new}}(s) = \\hat{R}^\\pi(s) + \\gamma \\sum_{s'} \\hat{P}^\\pi(s'\\mid s),V(s')\\).</p> </li> <li> <p>Update \\(V \\leftarrow V_{\\text{new}}\\) and repeat until convergence (the changes in \\(V\\) are below a threshold).</p> </li> <li> <p>Output: \\(V(s)\\) for states of interest (e.g. the estimated value of \\(\\pi\\) under the initial state distribution \\(S_0\\) can be obtained by \\(\\mathbb{E}_{s_0\\sim S_0}[V(s_0)]\\)).</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithm-3-fitted-q-evaluation-fqe-pi-c","title":"Algorithm 3 Fitted Q Evaluation: FQE \\((\\pi, c)\\)","text":"<p>Input: Dataset \\(\\mathcal{D} = \\{(x_i, a_i, x'_i, c_i)\\}_{i=1}^n \\sim \\pi_D\\). Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy \\(\\pi\\) by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state \\(s_i\\), taking action \\(a_i\\), and then following policy \\(\\pi\\) thereafter.At each iteration, we construct a Bellman target:</p> \\[\\tilde{Q}^\\pi(s_i, a_i) = c_i + \\gamma V_\\theta^\\pi(s_{i+1}) \\] <p>where</p> \\[ V_\\theta^\\pi(s_{i+1}) = Q_\\theta^\\pi(s_{i+1}, \\pi(s_{i+1})). \\] <p>The Q-function is parameterized by \\(\\theta\\) (e.g., a neural network), and is learned by solving a supervised regression problem:</p> \\[\\arg\\min_\\theta \\sum_i \\Big( Q_\\theta^\\pi(s_i, a_i) - \\tilde{Q}^\\pi(s_i, a_i) \\Big)^2\\] <p>This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy \\(\\pi\\) under the data distribution induced by \\(\\pi_D\\).</p> <p>Function class \\(F\\) (Let's assume we use a DNN for F). Policy </p> <p>\\(\\pi\\) to be evaluated. 1: Initialize \\(Q_0 \\in F\\) randomly 2: for \\(k = 1, 2, \\dots, K\\) do 3: \\(\\quad\\) Compute target \\(y_i = c_i + \\gamma Q_{k-1}(x'_i, \\pi(x'_i)) \\quad \\forall i\\) 4: \\(\\quad\\) Build training set \\(\\tilde{\\mathcal{D}}_k = \\{(x_i, a_i), y_i\\}_{i=1}^n\\) 5: \\(\\quad\\) Solve a supervised learning problem:  6: end for Output: \\(\\hat{C}^\\pi(x) = Q_K(x, \\pi(x)) \\quad \\forall x\\)</p>"},{"location":"reinforcement/10_offline_rl/#what-is-different-vs-dqn","title":"What is different vs DQN?","text":"<p>DQN learns an optimal policy by interacting with the environment, while FQE evaluates a fixed policy using a fixed offline dataset.</p> Aspect FQE (Fitted Q Evaluation) DQN (Deep Q-Network) Goal Policy evaluation Policy optimization / control Policy Fixed target policy \\(\\pi\\) Implicitly learned via \\(\\max_a Q(s,a)\\) Data Offline, fixed dataset \\(\\mathcal{D}\\) Online, collected during training Bellman target \\(c + \\gamma Q(s', \\pi(s'))\\) \\(r + \\gamma \\max_a Q(s', a)\\) Action at next state From given policy \\(\\pi\\) Greedy over Q-values Exploration None Required (e.g. \\(\\epsilon\\)-greedy) Dataset changes? \u274c No \u2705 Yes Off-policy instability Low High Convergence guarantees Yes (tabular / linear) No (with function approximation)"},{"location":"reinforcement/10_offline_rl/#1-no-maximization-bias-in-fqe","title":"1. No maximization bias in FQE","text":"<ul> <li>DQN suffers from overestimation bias</li> <li>FQE does pure regression, no bootstrapped max</li> </ul>"},{"location":"reinforcement/10_offline_rl/#2-stability","title":"2. Stability","text":"<ul> <li>FQE \u2248 supervised learning  </li> <li>DQN \u2248 bootstrapped + non-stationary targets</li> </ul>"},{"location":"reinforcement/10_offline_rl/#3-offline-vs-online","title":"3. Offline vs Online","text":"<ul> <li>FQE cannot improve the policy  </li> <li>DQN must interact with environment</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-policy-learning-optimization","title":"Offline Policy Learning / Optimization","text":"<p>Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.</p> <ol> <li> <p>Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.</p> </li> <li> <p>Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.</p> </li> <li> <p>Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#challenges-in-offline-policy-optimization","title":"Challenges in Offline Policy Optimization","text":"<ol> <li> <p>Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.</p> </li> <li> <p>Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.</p> </li> <li> <p>Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.</p> </li> </ol> <p>Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.</p>"},{"location":"reinforcement/10_offline_rl/#mental-map","title":"Mental Map","text":"<pre><code>                 Offline / Batch Reinforcement Learning\n        Goal: Learn and evaluate policies from fixed historical data\n           when exploration is unsafe, expensive, or impossible\n                                \u2502\n                                \u25bc\n              Why Online RL Is Not Always Feasible\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Exploration can be dangerous (healthcare, driving, robotics)\u2502\n \u2502 Data already exists from past decisions                     \u2502\n \u2502 Real systems cannot reset or freely experiment              \u2502\n \u2502 We must learn without interacting with the environment      \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Offline RL vs Standard RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Standard RL:                                                \u2502\n \u2502  \u2013 Collect data with current policy                         \u2502\n \u2502  \u2013 Explore \u2192 improve \u2192 repeat                               \u2502\n \u2502                                                             \u2502\n \u2502 Offline RL:                                                 \u2502\n \u2502  \u2013 Fixed dataset D from behavior policy \u03c0_b                 \u2502\n \u2502  \u2013 No new interaction allowed                               \u2502\n \u2502  \u2013 Must generalize only from observed data                  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             Why \u201cJust Use Q-Learning\u201d Fails Offline\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Q-learning is off-policy \u2014 but not offline-safe             \u2502\n \u2502 Deadly triad:                                               \u2502\n \u2502   \u2022 Bootstrapping                                           \u2502\n \u2502   \u2022 Function approximation                                  \u2502\n \u2502   \u2022 Off-policy learning                                     \u2502\n \u2502 Leads to divergence &amp; overestimation                        \u2502\n \u2502 Especially severe with distribution mismatch                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Offline RL Decomposed into Two Core Problems\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 1. Policy Evaluation (OPE)    \u2502 2. Policy Optimization      \u2502\n \u2502    \u201cHow good is this policy?\u201d \u2502    \u201cHow can we improve it?\u201d \u2502\n \u2502    Without running it         \u2502    Without new data         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Batch / Offline Policy Evaluation (OPE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Estimate V^\u03c0 or J(\u03c0) using only dataset D                   \u2502\n \u2502 Three major approaches:                                     \u2502\n \u2502  1. Model-based evaluation                                  \u2502\n \u2502  2. Model-free evaluation (FQE)                             \u2502\n \u2502  3. Importance Sampling                                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        1. Model-Based Offline Policy Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn a model from data:                                    \u2502\n \u2502   \u2022 Reward model: r\u0302(s,a)                                    \u2502\n \u2502   \u2022 Transition model: P\u0302(s'|s,a)                             \u2502\n \u2502 Treat batch data as supervised learning                     \u2502\n \u2502 Then simulate policy \u03c0 inside learned model                 \u2502\n \u2502 Use Bellman backups on (P\u0302, r\u0302)                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Model-Based OPE: Key Limitation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Model is only reliable where data exists                    \u2502\n \u2502 Policy visiting unseen states/actions \u2192 extrapolation error \u2502\n \u2502 Model hallucination \u2192 highly biased value estimates         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Evaluate a *fixed policy* \u03c0                                 \u2502\n \u2502 Learn Q^\u03c0(s,a) from offline data via regression             \u2502\n \u2502 Bellman target:                                             \u2502\n \u2502   y = c + \u03b3 Q(s', \u03c0(s'))                                    \u2502\n \u2502 Pure supervised learning loop                               \u2502\n \u2502 Stable compared to Q-learning / DQN                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             FQE vs DQN (Key Insight)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 DQN                         \u2502 FQE                         \u2502\n \u2502 Learns optimal policy       \u2502 Evaluates fixed policy      \u2502\n \u2502 Uses max over actions       \u2502 Uses given \u03c0(s')            \u2502\n \u2502 Online data collection      \u2502 Fully offline               \u2502\n \u2502 Overestimation bias         \u2502 No max \u2192 more stable        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        3. Importance Sampling (IS) Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat OPE as statistical estimation                         \u2502\n \u2502 Reweight trajectories by \u03c0 / \u03c0_b                            \u2502\n \u2502 Unbiased if coverage holds                                  \u2502\n \u2502 Severe variance for long horizons or policy mismatch        \u2502\n \u2502 Variants:                                                   \u2502\n \u2502   \u2022 Per-decision IS                                         \u2502\n \u2502   \u2022 Weighted IS                                             \u2502\n \u2502   \u2022 Doubly robust estimators                                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Offline Policy Optimization\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Goal: improve policy using only dataset D                   \u2502\n \u2502 Model-free: Fitted Q Iteration (FQI)                        \u2502\n \u2502 Model-based: planning inside learned model                  \u2502\n \u2502 Core challenge: distribution shift                          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        The Central Problem: Distribution Mismatch\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learned policy chooses actions unseen in data               \u2502\n \u2502 Q-values extrapolate \u2192 overly optimistic                    \u2502\n \u2502 Performance collapses at deployment                         \u2502\n \u2502 Offline RL \u2260 just off-policy RL                             \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Conservative / Pessimistic Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Assume unknown actions are risky                            \u2502\n \u2502 Penalize state-action pairs with low data support           \u2502\n \u2502 Prefer policies close to behavior policy                    \u2502\n \u2502 Examples (conceptually):                                    \u2502\n \u2502   \u2022 Conservative Q-Learning (CQL)                           \u2502\n \u2502   \u2022 Regularization toward \u03c0_b                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Key Challenges in Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Coverage / overlap requirement                              \u2502\n \u2502 Model misspecification                                      \u2502\n \u2502 Value overestimation                                        \u2502\n \u2502 Bias\u2013variance tradeoffs                                     \u2502\n \u2502 Safety vs optimality                                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n               Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Offline RL learns entirely from past experience             \u2502\n \u2502 Policy evaluation is foundational before optimization       \u2502\n \u2502 Model-based, FQE, and IS provide OPE tools                  \u2502\n \u2502 Main risk: distribution shift &amp; extrapolation               \u2502\n \u2502 Conservative methods trade performance for safety           \u2502\n \u2502 Offline RL is essential for real-world decision systems     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/11_fast_rl/","title":"11. Data-Efficient Reinforcement Learning","text":""},{"location":"reinforcement/11_fast_rl/#chapter-11-data-efficient-reinforcement-learning-bandit-foundations","title":"Chapter 11: Data-Efficient Reinforcement Learning \u2014 Bandit Foundations","text":"<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>"},{"location":"reinforcement/11_fast_rl/#the-multi-armed-bandit-model","title":"The Multi-Armed Bandit Model","text":"<p>A multi-armed bandit is defined as a tuple \\((\\mathcal{A}, \\mathcal{R})\\), where:</p> <ul> <li>\\(\\mathcal{A} = \\{a_1, \\dots, a_m\\}\\) is a known, finite set of actions (arms),</li> <li>\\(R_a(r) = \\mathbb{P}[r \\mid a]\\) is an unknown probability distribution over rewards for each action.</li> <li>there is no \"state\".</li> </ul> <p>At each timestep \\(t\\), the agent:</p> <ol> <li>Chooses an action \\(a_t \\in \\mathcal{A}\\),</li> <li>Receives a stochastic reward \\(r_t \\sim R_{a_t}\\).</li> </ol> <p>Goal: Maximize cumulative reward: </p> <p>This simple model embodies the core RL challenges\u2014particularly exploration vs. exploitation\u2014in an isolated setting.</p>"},{"location":"reinforcement/11_fast_rl/#evaluating-algorithms-regret-framework","title":"Evaluating Algorithms: Regret Framework","text":"<p>Regret: </p> <ul> <li>\\(Q(a) = \\mathbb{E}[r \\mid a]\\) be the expected reward for action \\(a\\),</li> <li>\\(a^* = \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\),</li> <li>Optimal Value \\(V^* = Q(a^*)\\)</li> </ul> <p>Then regret is the opportunity loss for one step:  </p> <p>Total Regret is the total opportunity loss: Total regret over \\(T\\) timesteps</p> <p>  Where:</p> <ul> <li>\\(N_T(a)\\): Number of times arm \\(a\\) is selected by time \\(T\\),</li> <li>\\(\\Delta_a = V^* - Q(a)\\): Suboptimality gap.</li> </ul> <p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>"},{"location":"reinforcement/11_fast_rl/#baseline-approaches-and-their-regret","title":"Baseline Approaches and Their Regret","text":""},{"location":"reinforcement/11_fast_rl/#greedy-algorithm","title":"Greedy Algorithm","text":"\\[ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\cdot \\mathbb{1}(a_\\tau = a) \\] \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\hat{Q}_t(a) \\]"},{"location":"reinforcement/11_fast_rl/#key-insight","title":"Key Insight:","text":"<ul> <li>Exploits current estimates.</li> <li>May lock onto suboptimal arms due to early bad luck.</li> <li>Linear regret in expectation.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#example","title":"Example:","text":"<p>If \\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\\), and the first sample of \\(a_1\\) yields 0, the greedy agent may ignore it indefinitely.</p>"},{"location":"reinforcement/11_fast_rl/#varepsilon-greedy-algorithm","title":"\\(\\varepsilon\\)-Greedy Algorithm","text":"<p>At each timestep:</p> <ul> <li>With probability \\(1 - \\varepsilon\\): exploit (\\(\\arg\\max \\hat{Q}_t(a)\\)),</li> <li>With probability \\(\\varepsilon\\): explore uniformly at random.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#performance","title":"Performance:","text":"<ul> <li>Guarantees exploration.</li> <li>Linear regret unless \\(\\varepsilon\\) decays over time.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#decaying-varepsilon-greedy","title":"Decaying \\(\\varepsilon\\)-Greedy","text":"<p>Allows \\(\\varepsilon_t \\to 0\\) as \\(t \\to \\infty\\), enabling convergence.</p>"},{"location":"reinforcement/11_fast_rl/#optimism-in-the-face-of-uncertainty","title":"Optimism in the Face of Uncertainty","text":"<p>Prefer actions with uncertain but potentially high value:</p> <p>Why? Two possible outcomes:</p> <ol> <li> <p>Getting a high reward:    If the arm really has a high mean reward.</p> </li> <li> <p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p> </li> </ol> <p>Algorithm: </p> <ul> <li> <p>Estimate an upper confidence bound \\(U_t(a)\\) for each action value, such that   \\(Q(a) \\le U_t(a)\\) with high probability.</p> </li> <li> <p>This depends on the number of times \\(N_t(a)\\) action \\(a\\) has been selected.</p> </li> <li> <p>Select the action maximizing the Upper Confidence Bound (UCB):</p> </li> </ul> \\[a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ U_t(a) \\right]\\] <p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards \\(X_i \\in [0,1]\\),  </p> <p>Setting the right-hand side equal to \\(\\delta\\) and solving for \\(u\\),  Here, \\(\\delta\\) is the failure probability, and the confidence interval holds with probability at least \\(1 - \\delta\\). This means that, with probability at least \\(1 - \\delta\\),  </p> \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ \\hat{Q}_t(a) + \\text{UCB}_t(a) \\right] \\]"},{"location":"reinforcement/11_fast_rl/#ucb1-algorithm","title":"UCB1 Algorithm","text":"\\[ \\text{UCB}_t(a) = \\hat{Q}_t(a) + \\sqrt{\\frac{2 \\log \\frac{1}{\\delta} }{N_t(a)}} \\] <ul> <li>where \\(\\hat{Q}_t(a)\\) is empirical average</li> <li>\\(N_t(a)\\) is number of samples of \\(a\\) after \\(t\\) timesteps.</li> <li>Provable sublinear regret.</li> <li>Balances estimated value and exploration bonus.</li> </ul> <p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p> <p>1: Initialize for each arm \\(a \\in \\mathcal{A}\\):  \\(\\quad N(a) \\leftarrow 0,\\;\\; \\hat{Q}(a) \\leftarrow 0\\) 2: Warm start (sample each arm once): 3: for each arm \\(a \\in \\mathcal{A}\\) do 4: \\(\\quad\\) Pull arm \\(a\\), observe reward \\(r \\in [0,1]\\) 5: \\(\\quad N(a) \\leftarrow 1\\) 6: \\(\\quad \\hat{Q}(a) \\leftarrow r\\) 7: end for 8: Set \\(t \\leftarrow |\\mathcal{A}|\\)</p> <p>9: for \\(t = |\\mathcal{A}|+1, |\\mathcal{A}|+2, \\dots\\) do 10: \\(\\quad\\) Compute UCB for each arm: \\(\\quad \\mathrm{UCB}_t(a) = \\hat{Q}(a) + \\sqrt{\\frac{2\\log t}{N(a)}}\\)</p> <p>11: \\(\\quad\\) Select action:\\(\\quad a_t \\leftarrow \\arg\\max_{a \\in \\mathcal{A}} \\mathrm{UCB}_t(a)\\)</p> <p>12: \\(\\quad\\) Pull arm \\(a_t\\), observe reward \\(r_t\\)</p> <p>13: \\(\\quad\\) Update count: \\(\\quad N(a_t) \\leftarrow N(a_t) + 1\\)</p> <p>14: \\(\\quad\\) Update empirical mean (incremental): </p> <p>15: end for</p>"},{"location":"reinforcement/11_fast_rl/#119-optimistic-initialization-in-greedy-bandit-algorithms","title":"11.9 Optimistic Initialization in Greedy Bandit Algorithms","text":"<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>"},{"location":"reinforcement/11_fast_rl/#motivation","title":"Motivation","text":"<p>Greedy algorithms, by default, select actions with the highest estimated value:</p> \\[ a_t = \\arg\\max_a \\hat{Q}_t(a) \\] <p>If these \\(\\hat{Q}_t(a)\\) estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>"},{"location":"reinforcement/11_fast_rl/#algorithmic-details","title":"Algorithmic Details","text":"<p>We initialize:</p> <ul> <li>\\(\\hat{Q}_0(a) = Q_{\\text{init}}\\) for all \\(a \\in \\mathcal{A}\\), where \\(Q_{\\text{init}}\\) is set higher than any reasonable expected reward (e.g., \\(Q_{\\text{init}} = 1\\) if rewards are bounded in \\([0, 1]\\)).</li> <li>\\(N(a) = 1\\) to ensure initial update is well-defined.</li> </ul> <p>Then we update action values using an incremental Monte Carlo estimate:</p> \\[ \\hat{Q}_{t}(a_t) = \\hat{Q}_{t-1}(a_t) + \\frac{1}{N_t(a_t)} \\left( r_t - \\hat{Q}_{t-1}(a_t) \\right) \\] <p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p> <ul> <li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li> <li>Simple to implement: No need for tuning \\(\\varepsilon\\) or computing uncertainty estimates.</li> <li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#key-design-considerations","title":"Key Design Considerations","text":"<ul> <li>How optimistic is optimistic enough?   If \\(Q_{\\text{init}}\\) is not much larger than the true values, the agent may not explore effectively.</li> <li>What if \\(Q_{\\text{init}}\\) is too high?   Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#function-approximation","title":"Function Approximation","text":"<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>"},{"location":"reinforcement/11_fast_rl/#1110-theoretical-frameworks-regret-and-pac","title":"11.10 Theoretical Frameworks: Regret and PAC","text":""},{"location":"reinforcement/11_fast_rl/#regret-based-evaluation","title":"Regret-Based Evaluation","text":"<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p> <ul> <li>Many small mistakes (frequent near-optimal actions),</li> <li>A few large mistakes (infrequent but very suboptimal actions).</li> </ul> <p>Minimizing regret growth with \\(T\\) is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>"},{"location":"reinforcement/11_fast_rl/#probably-approximately-correct-pac-framework","title":"Probably Approximately Correct (PAC) Framework","text":"<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p> <p>An algorithm is \\((\\varepsilon, \\delta)\\)-PAC if, on each time step \\(t\\), it chooses an action \\(a_t\\) such that:</p> \\[ Q(a_t) \\ge Q(a^*) - \\varepsilon \\quad \\text{with probability at least } 1 - \\delta \\] <p>on all but a polynomial number of time steps (in \\(|\\mathcal{A}|\\), \\(1/\\varepsilon\\), \\(1/\\delta\\), etc). This ensures:</p> <ul> <li>The agent almost always behaves nearly optimally,</li> <li>With high probability, after a reasonable amount of time.</li> </ul> <p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>"},{"location":"reinforcement/11_fast_rl/#comparing-exploration-strategies","title":"Comparing Exploration Strategies","text":"Strategy Regret Behavior Notes Greedy Linear No exploration mechanism Constant \\(\\varepsilon\\)-greedy Linear Fixed chance of exploring Decaying \\(\\varepsilon\\)-greedy Sublinear (if tuned) Requires prior knowledge of reward gaps Optimistic Initialization Sublinear (if optimistic enough) Simple, effective in tabular settings <p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>"},{"location":"reinforcement/11_fast_rl/#bayesian-bandits","title":"Bayesian Bandits","text":"<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in \\([0,1]\\)). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>"},{"location":"reinforcement/11_fast_rl/#key-idea-maintain-beliefs-over-arm-reward-distributions","title":"Key Idea: Maintain Beliefs Over Arm Reward Distributions","text":"<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter \\(\\\\phi_i\\) for arm \\(i\\). Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of \\(\\\\phi_i\\), representing our uncertainty.</p>"},{"location":"reinforcement/11_fast_rl/#prior-and-posterior","title":"Prior and Posterior","text":"<ul> <li>Prior: Our initial belief about \\(\\\\phi_i\\) is encoded in a probability distribution \\(p(\\\\phi_i)\\).</li> <li>Data: After pulling arm \\(i\\) and observing reward \\(r_{i1}\\), we update our belief.</li> <li>Posterior: The new belief is computed using Bayes' rule:</li> </ul> \\[p(\\phi_i \\mid r_{i1}) = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ p(r_{i1}) } = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ \\int p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i)\\, d\\phi_i }\\] <p>This posterior becomes the new prior for future updates as more data arrives.</p>"},{"location":"reinforcement/11_fast_rl/#practical-considerations","title":"Practical Considerations","text":"<p>Computing the posterior \\(p(\\phi_i \\mid D)\\) (where \\(D\\) is the observed data for arm \\(i\\)) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p> <ul> <li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li> <li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li> </ul>"},{"location":"reinforcement/11_fast_rl/#why-use-bayesian-bandits","title":"Why Use Bayesian Bandits?","text":"<ul> <li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li> <li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li> <li>Captures uncertainty in a principled and statistically coherent manner.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#summary","title":"Summary","text":"<ul> <li>Bayesian bandits treat the reward-generating parameters \\(\\phi_i\\) as random variables.</li> <li>We maintain a posterior belief \\(p(\\phi_i \\mid D)\\) using Bayes' rule.</li> <li>When conjugate priors are used, analytical updates are possible.</li> <li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#thompson-sampling","title":"Thompson Sampling:","text":"<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p> <p>This approach is also known as probability matching: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn\u2019t deterministically select the arm with the highest mean\u2014it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p> <p>Algorithm: Thompson Sampling:</p> <p>1: Initialize prior over each arm \\(a\\), \\(p(\\mathcal{R}_a)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) For each arm \\(a\\) sample a reward distribution \\(\\mathcal{R}_a\\) from posterior 4: \\(\\quad\\) Compute action-value function \\(Q(a) = \\mathbb{E}[\\mathcal{R}_a]\\) 5: \\(\\quad a_t \\equiv \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\) 6: \\(\\quad\\) Observe reward \\(r\\) 7: \\(\\quad\\) Update posterior \\(p(\\mathcal{R}_a)\\) using Bayes Rule 8: end for  </p>"},{"location":"reinforcement/11_fast_rl/#contextual-bandits","title":"Contextual Bandits","text":"<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context \\(x_t\\) drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p> <p>In this setting, the goal is to learn a context-dependent policy \\(\\pi(a \\mid x)\\) that maps the observed context \\(x_t\\) to a suitable arm \\(a_t\\), maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p> <p>Formally, the interaction at each time step \\(t\\) is:</p> <ol> <li>Observe context \\(x_t \\in \\mathcal{X}\\)</li> <li>Choose action \\(a_t \\in \\mathcal{A}\\) based on policy \\(\\pi(a \\mid x_t)\\)</li> <li>Receive reward \\(r_t(a_t, x_t)\\)</li> </ol> <p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p> \\[ \\pi^*(x) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}[r(a, x)] \\] <p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>"},{"location":"reinforcement/11_fast_rl/#mental-map","title":"Mental Map","text":"<pre><code>                Bandits: Foundations of Data-Efficient RL\n     Goal: Understand exploration-exploitation in simplest setting\n           Learn to act with minimal data through principled tradeoffs\n                                \u2502\n                                \u25bc\n               What Are Multi-Armed Bandits (MAB)?\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Single-state (stateless) decision problems                  \u2502\n \u2502 Fixed set of actions (arms)                                 \u2502\n \u2502 Unknown reward distribution per arm                         \u2502\n \u2502 Choose an action, receive reward, repeat                    \u2502\n \u2502 No transition dynamics \u2014 unlike full RL                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Objective: Maximize Reward\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Maximize total reward = minimize regret                     \u2502\n \u2502 Regret = missed opportunity vs optimal action               \u2502\n \u2502 Total regret used to evaluate algorithm efficiency          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                  Basic Bandit Algorithms\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Greedy: exploit current best estimates (linear regret)      \u2502\n \u2502 \u03b5-Greedy: random exploration with fixed \u03b5                   \u2502\n \u2502 Decaying \u03b5-Greedy: reduces \u03b5 over time                      \u2502\n \u2502 Optimistic Initialization: set high initial Q\u0302 values        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Principle: Optimism in the Face of Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat unvisited arms as potentially good                    \u2502\n \u2502 Upper Confidence Bound (UCB) algorithms                     \u2502\n \u2502 Tradeoff: mean reward + exploration bonus                   \u2502\n \u2502 Guarantees sublinear regret                                 \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Algorithmic Realization: UCB1\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 UCB_t(a) = Q\u0302_t(a) + \u221a(2 log t / N_t(a))                     \u2502\n \u2502 Encourages pulling uncertain arms early                     \u2502\n \u2502 Regret \u2248 O(\u221a(T log T))                                      \u2502\n \u2502 Theoretically grounded and simple to implement              \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Theoretical Frameworks: Regret vs PAC\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Regret: cumulative gap from always acting optimally         \u2502\n \u2502 PAC: guarantees near-optimal behavior with high probability \u2502\n \u2502 Regret cares about sum of mistakes; PAC focuses on steps    \u2502\n \u2502 Both evaluate quality and efficiency of learning            \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Bayesian Bandits and Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat arm rewards as random variables                       \u2502\n \u2502 Use prior + observed data \u2192 posterior via Bayes rule        \u2502\n \u2502 Conjugate priors simplify computation                       \u2502\n \u2502 Enable principled uncertainty reasoning                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Thompson Sampling (Bayesian)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Sample reward distribution from posterior per arm           \u2502\n \u2502 Pull arm with highest sampled reward                        \u2502\n \u2502 Probabilistic optimism: match probability of being best     \u2502\n \u2502 Natural exploration and strong empirical performance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Probability Matching Perspective\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Thompson Sampling \u2248 sample optimal arm w/ correct frequency \u2502\n \u2502 Avoids hard-coded uncertainty bonuses                       \u2502\n \u2502 Simpler and often better in practice                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       Contextual Bandits\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Input context x_t at each timestep                          \u2502\n \u2502 Reward distribution depends on (action, context)            \u2502\n \u2502 Learn policy \u03c0(a | x): context-aware decision making        \u2502\n \u2502 Real-world applications: ads, medicine, personalization     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n          Summary: Bandits as Foundation for Efficient RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Bandits isolate the exploration-exploitation tradeoff       \u2502\n \u2502 Simpler than full RL, but deeply insightful                 \u2502\n \u2502 Concepts generalize to value estimation, uncertainty        \u2502\n \u2502 Key tools: regret, PAC bounds, posterior reasoning          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/12_fast_mdps/","title":"12. Fast Reinforcement Learning in MDPs and Generalization","text":""},{"location":"reinforcement/12_fast_mdps/#chapter-12-fast-reinforcement-learning-in-mdps-and-generalization","title":"Chapter 12: Fast Reinforcement Learning in MDPs and Generalization","text":"<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p> <ul> <li>Bandits: Single-step decision-making problems.</li> <li>MDPs: Sequential decision-making with transition dynamics.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#evaluation-frameworks","title":"Evaluation Frameworks","text":"<p>To assess learning efficiency, we use:</p> <ul> <li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li> <li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li> <li>PAC (Probably Approximately Correct): Number of steps when the policy is not \\(\\epsilon\\)-optimal is bounded with high probability.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#exploration-approaches","title":"Exploration Approaches","text":"<ul> <li>Optimism under uncertainty (e.g., UCB)</li> <li>Probability matching (e.g., Thompson Sampling)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#pac-framework-for-mdps","title":"PAC Framework for MDPs","text":"<p>A reinforcement learning algorithm \\(A\\) is PAC if with probability at least \\(1 - \\delta\\), it selects an \\(\\epsilon\\)-optimal action on all but a bounded number of time steps \\(N\\), where:</p> \\[ N = \\text{poly} \\left( |S|, |A|, \\frac{1}{1 - \\gamma}, \\frac{1}{\\epsilon}, \\frac{1}{\\delta} \\right) \\]"},{"location":"reinforcement/12_fast_mdps/#mbie-eb-model-based-interval-estimation-with-exploration-bonus","title":"MBIE-EB: Model-Based Interval Estimation with Exploration Bonus","text":"<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function \\(Q(s, a)\\).</p> <p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p> <ul> <li> <p>It estimates \\(\\hat{R}(s, a)\\) and \\(\\hat{T}(s' \\mid s, a)\\) from data using empirical counts.</p> </li> <li> <p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p> </li> </ul> <p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p> <p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p> <p>Algorithm:</p> <p>1: Given \\(\\epsilon\\), \\(\\delta\\), \\(m\\) 2: \\(\\beta = \\dfrac{1}{1-\\gamma}\\sqrt{0.5 \\ln \\!\\left(\\dfrac{2|S||A|m}{\\delta}\\right)}\\) 3: \\(n_{sas}(s,a,s') = 0\\), \\(\\forall s \\in S, a \\in A, s' \\in S\\) 4: \\(rc(s,a) = 0\\), \\(n_{sa}(s,a) = 0\\), \\(\\hat{Q}(s,a) = \\dfrac{1}{1-\\gamma}\\), \\(\\forall s \\in S, a \\in A\\) 5: \\(t = 0\\), \\(s_t = s_{\\text{init}}\\) 6: loop 7: \\(\\quad a_t = \\arg\\max_{a \\in A} \\hat{Q}(s_t, a)\\) 8: \\(\\quad\\) Observe reward \\(r_t\\) and state \\(s_{t+1}\\) 9: \\(\\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\\), \\(\\quad\\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\\) 10: \\(\\quad rc(s_t,a_t) = \\dfrac{rc(s_t,a_t)\\big(n_{sa}(s_t,a_t)-1\\big) + r_t}{n_{sa}(s_t,a_t)}\\) 11: \\(\\quad \\hat{R}(s_t,a_t) = rc(s_t,a_t)\\) and \\(\\quad\\quad \\hat{T}(s' \\mid s_t,a_t) = \\dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\\), \\(\\forall s' \\in S\\) 12: \\(\\quad\\) while not converged do 13: \\(\\quad\\quad \\hat{Q}(s,a) = \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s' \\mid s,a)\\max_{a'} \\hat{Q}(s',a') + \\dfrac{\\beta}{\\sqrt{n_{sa}(s,a)}}\\), \\(\\quad\\quad\\quad \\forall s \\in S, a \\in A\\) 14: \\(\\quad\\) end while 15: end loop</p>"},{"location":"reinforcement/12_fast_mdps/#bayesian-model-based-reinforcement-learning","title":"Bayesian Model-Based Reinforcement Learning","text":"<p>Bayesian RL methods maintain a posterior over MDP models \\((P, R)\\) and sample plausible environments from the posterior to plan and act.</p> <p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p> <p>Formally, the Thompson sampling policy is:</p> \\[ \\pi(s, a \\mid h_t) = \\mathbb{P}\\left(Q(s, a) \\ge Q(s, a'),\\; \\forall a' \\ne a \\;\\middle|\\; h_t \\right) = \\mathbb{E}_{\\mathcal{P}, \\mathcal{R} \\mid h_t} \\left[ \\mathbb{1}\\left(a = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)\\right) \\right] \\] <p>Where: - \\(h_t\\) is the history up to time \\(t\\) (including all observed transitions and rewards), - \\(\\mathcal{P}, \\mathcal{R}\\) are the transition and reward functions respectively, - The expectation is taken over the posterior belief on the MDP \\((\\mathcal{P}, \\mathcal{R})\\).</p>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-algorithm-in-mdps","title":"Thompson Sampling Algorithm in MDPs","text":"<ol> <li>Maintain a posterior \\(p(\\mathcal{P}, \\mathcal{R} \\mid h_t)\\) over the transition and reward models based on all observed data.</li> <li>Sample a model \\((\\mathcal{P}, \\mathcal{R})\\) from the posterior distribution.</li> <li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function \\(Q^*(s, a)\\).</li> <li>Select the action according to the optimal action in the sampled model:     </li> </ol>"},{"location":"reinforcement/12_fast_mdps/#algorithm-thompson-sampling-for-mdps","title":"Algorithm: Thompson Sampling for MDPs","text":"<p>1: Initialize prior over dynamics and reward models for each \\((s, a)\\):  \\(\\quad p(\\mathcal{T}(s' \\mid s, a)), \\quad p(\\mathcal{R}(s, a))\\) 2: Initialize initial state \\(s_0\\) 3: for \\(k = 1\\) to \\(K\\) episodes do 4: \\(\\quad\\) Sample an MDP \\(\\mathcal{M}\\): 5: \\(\\quad\\quad\\) for each \\((s, a)\\) pair do 6: \\(\\quad\\quad\\quad\\) Sample transition model \\(\\mathcal{T}(s' \\mid s, a)\\) from posterior 7: \\(\\quad\\quad\\quad\\) Sample reward model \\(\\mathcal{R}(s, a)\\) from posterior 8: \\(\\quad\\quad\\) end for 9: \\(\\quad\\) Compute optimal value function \\(Q_{\\mathcal{M}}^*\\) for sampled MDP \\(\\mathcal{M}\\) 10: \\(\\quad\\) for \\(t = 1\\) to \\(H\\) do 11: \\(\\quad\\quad a_t = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\mathcal{M}}^*(s_t, a)\\) 12: \\(\\quad\\quad\\) Take action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\) 13: \\(\\quad\\) end for 14: \\(\\quad\\) Update posteriors: \\(\\quad\\quad p(\\mathcal{R}_{s_t, a_t} \\mid r_t), \\quad p(\\mathcal{T}(s' \\mid s_t, a_t) \\mid s_{t+1})\\) using Bayes Rule 15: end for</p>"},{"location":"reinforcement/12_fast_mdps/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li> <li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li> <li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li> </ul> <p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>"},{"location":"reinforcement/12_fast_mdps/#generalization-in-contextual-bandits","title":"Generalization in Contextual Bandits","text":"<p>Contextual bandits generalize standard bandits by associating a context or state \\(s\\) with each decision:</p> <ul> <li>Reward depends on both context and action: \\(r \\sim P[r | s,a]\\)</li> <li>Often model reward as linear: \\(r = \\theta^\\top \\phi(s,a) + \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#benefits-of-generalization","title":"Benefits of Generalization","text":"<ul> <li>Allows learning across states/actions</li> <li>Enables sample-efficient exploration in large state/action spaces</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#strategic-exploration-in-deep-rl","title":"Strategic Exploration in Deep RL","text":"<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>"},{"location":"reinforcement/12_fast_mdps/#optimistic-q-learning-with-function-approximation","title":"Optimistic Q-Learning with Function Approximation","text":"<p>Modified Q-learning update:</p> \\[ \\Delta w = \\alpha \\left( r + r_{\\text{bonus}}(s,a) + \\gamma \\max_{a'} Q(s', a'; w) - Q(s,a;w) \\right) \\nabla_w Q(s,a;w) \\] <p>Bonus \\(r_{\\text{bonus}}\\) reflects novelty or epistemic uncertainty.</p>"},{"location":"reinforcement/12_fast_mdps/#count-based-and-density-based-exploration","title":"Count-Based and Density-Based Exploration","text":"<ul> <li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li> <li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li> <li>Tang et al. (2017) use hashing-based counts.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-for-deep-rl","title":"Thompson Sampling for Deep RL","text":"<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>"},{"location":"reinforcement/12_fast_mdps/#bootstrapped-dqn","title":"Bootstrapped DQN","text":"<ul> <li>Train multiple Q-networks on bootstrapped datasets.</li> <li>Select one head randomly at each episode for exploration.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#bayesian-deep-q-networks","title":"Bayesian Deep Q-Networks","text":"<ul> <li>Bayesian linear regression on final layer</li> <li>Posterior used to sample Q-values, enabling optimism</li> <li>Outperforms naive bootstrapped DQNs in some settings</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#mental-map","title":"Mental Map","text":"<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization\n  Goal: Learn near-optimal policies in MDPs with limited data\n    Extend bandit exploration ideas to sequential decision making\n                            \u2502\n                            \u25bc\n             Why MDPs Are Harder Than Bandits\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 MDPs involve sequential decisions with transitions           \u2502  \u2502 Agent must explore over states and transitions              \u2502  \u2502 Exploration affects future knowledge &amp; rewards              \u2502  \u2502 Sample inefficiency is a major practical bottleneck         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                    Evaluation Frameworks for RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Regret: cumulative gap vs optimal policy over time          \u2502  \u2502 PAC (Probably Approximately Correct):                       \u2502  \u2502   Guarantees \u03b5-optimality with high probability             \u2502  \u2502 Bayesian Regret: expected regret under prior over MDPs      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               PAC Learning in MDPs: Formal Guarantee  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Algorithm is PAC if all but N steps are \u03b5-optimal           \u2502  \u2502 N = poly(|S|, |A|, 1/(1-\u03b3), 1/\u03b5, 1/\u03b4)                        \u2502  \u2502 Ensures high-probability performance bounds                 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                Optimism: MBIE-EB Algorithm (Model-Based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Estimate reward + transitions from data                     \u2502  \u2502 Add bonus to Q-values: encourages actions with high uncertainty \u2502  \u2502 Optimistic model induces exploration                        \u2502  \u2502 Dynamic programming over Q\u0302 + bonus \u2192 exploration policy     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Algorithmic Principle: Optimism Under Uncertainty  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Add uncertainty-driven bonus to reward or Q-value           \u2502  \u2502 Drives exploration to unknown regions                       \u2502  \u2502 Simple but effective in tabular MDPs                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian RL and Posterior Sampling  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maintain belief (posterior) over MDP model (P, R)           \u2502  \u2502 Sample MDP from posterior \u2192 plan optimally in sampled MDP   \u2502  \u2502 Leads to probability matching via Thompson Sampling         \u2502  \u2502 Posterior concentrates with data \u2192 convergence to optimal   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Algorithm: Thompson Sampling in Model-Based RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Sample dynamics + rewards from posterior                    \u2502  \u2502 Solve sampled MDP for optimal Q                            \u2502  \u2502 Act according to Q in sample MDP                           \u2502  \u2502 Update posterior using Bayes rule after each step           \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              Exploration via Posterior Variance (Bayes)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Thompson Sampling \u2248 Probability Matching                    \u2502  \u2502 Probabilistically favors optimal but uncertain policies     \u2502  \u2502 Elegant &amp; adaptive exploration                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Generalization via Contextual Bandits  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Rewards depend on both context and action                   \u2502  \u2502 Learn generalizable function: Q(s,a) or \u03c0(a|s)              \u2502  \u2502 Enables learning across states / actions                    \u2502  \u2502 Use linear models or embeddings: \u03c6(s,a)                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc          Exploration + Generalization in Deep RL Settings  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Optimistic Q-learning: add r_bonus(s,a) in TD target        \u2502  \u2502 r_bonus from novelty, density models, or uncertainty        \u2502  \u2502 Count-based, hashing, or learned density bonuses            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian Deep RL: Posterior Approximation  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Bootstrapped DQN: ensemble of Q-networks for exploration    \u2502  \u2502 Bayesian DQN: sample from approximate Q-posteriors          \u2502  \u2502 Enables implicit Thompson-like behavior                     \u2502  \u2502 Scales to high-dimensional state/action spaces              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                          Chapter Summary  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Strategic exploration = key to fast learning in MDPs        \u2502  \u2502 Optimism (MBIE-EB) and Bayesian methods (Thompson)          \u2502  \u2502 PAC and Bayesian regret are key evaluation tools            \u2502  \u2502 Generalization (via features or deep nets) enables scaling  \u2502  \u2502 Thompson Sampling and bootstrapped approximations bridge gap\u2502  \u2502 Between tabular and high-dimensional RL                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ````</p>"},{"location":"reinforcement/13_montecarlo/","title":"13. Monte Carlo Tree Search and Planning","text":""},{"location":"reinforcement/13_montecarlo/#chapter-13-monte-carlo-tree-search","title":"Chapter 13: Monte Carlo Tree Search","text":"<p>Monte Carlo Tree Search (MCTS) is a powerful planning algorithm that uses simulation-based search to select actions in complex decision-making problems. It is especially effective in large or unknown environments where exact planning is infeasible. MCTS balances exploration and exploitation through sampling and is the backbone of major AI breakthroughs like AlphaGo and AlphaZero.</p>"},{"location":"reinforcement/13_montecarlo/#131-motivation","title":"13.1 Motivation","text":"<p>In classical reinforcement learning (RL), agents often compute policies over the entire state space. MCTS takes a different approach: it performs local search from the current state, using simulated episodes to estimate action values and make near-optimal decisions on the fly.</p> <p>This method is particularly useful in:</p> <ul> <li>Large state/action spaces</li> <li>Games with high branching factor (e.g., Go, Chess)</li> <li>Black-box or simulator-only environments</li> </ul>"},{"location":"reinforcement/13_montecarlo/#132-monte-carlo-search","title":"13.2 Monte Carlo Search","text":"<p>A simple Monte Carlo search uses a model \\(\\mathcal{M}\\) (dynamics and resward model) and a rollout policy \\(\\pi\\) to simulate \\(K\\) trajectories for each action \\(a\\) from the current state \\(s_t\\):</p> <ol> <li>Simulate episodes \\(\\{s_t, a, r_{t+1}^{(k)}, \\ldots, s_T^{(k)}\\}\\) from \\(\\mathcal{M}, \\pi\\).</li> <li>Estimate \\(Q(s_t, a)\\) via sample average:</li> </ol> \\[ Q(s_t, a) = \\frac{1}{K} \\sum_{k=1}^K G_t^{(k)} \\rightarrow q^\\pi(s_t, a) \\] <ol> <li>Select the best action:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\] <p>This performs one-step policy improvement, but does not build deeper search trees.</p>"},{"location":"reinforcement/13_montecarlo/#133-expectimax-search","title":"13.3 Expectimax Search","text":"<p>To go beyond single-step rollouts, expectimax trees compute \\(Q^*(s, a)\\) recursively using the model:</p> <ul> <li>Each node expands by looking ahead using the transition model.</li> <li>Combines maximization (over actions) and expectation (over next states).</li> <li>Forward search avoids solving the entire MDP and focuses only on the subtree starting at \\(s_t\\).</li> </ul> <p>However, the number of nodes grows exponentially with horizon \\(H\\): \\(O(|S||A|)^H\\).</p>"},{"location":"reinforcement/13_montecarlo/#134-monte-carlo-tree-search-mcts","title":"13.4 Monte Carlo Tree Search (MCTS)","text":"<p>MCTS improves on expectimax by sampling rather than fully expanding the tree:</p> <ol> <li>Build a tree rooted at current state \\(s_t\\).</li> <li>Perform \\(K\\) simulations to expand and update parts of the tree.</li> <li>Estimate \\(Q(s, a)\\) using sampled returns.</li> <li>Select the best action at the root:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\]"},{"location":"reinforcement/13_montecarlo/#135-upper-confidence-tree-uct","title":"13.5 Upper Confidence Tree (UCT)","text":"<p>A key challenge in MCTS is deciding which action to simulate at each tree node. UCT addresses this by treating each decision as a multi-armed bandit problem and using an Upper Confidence Bound:</p> \\[ Q(s, a, i) = \\underbrace{\\frac{1}{N(i, a)} \\sum_{k=1}^{N(i,a)} G_k(i,a)}_{\\text{Mean Return}} + \\underbrace{c \\sqrt{\\frac{\\log N(i)}{N(i, a)}}}_{\\text{Exploration Bonus}} \\] <ul> <li>\\(N(i, a)\\): number of times action \\(a\\) taken at node \\(i\\)</li> <li>\\(N(i)\\): total visits to node \\(i\\)</li> <li>\\(c\\): exploration constant</li> <li>\\(G_k(i, a)\\): return from simulation \\(k\\) for \\((i, a)\\)</li> </ul> <p>Action selection:</p> \\[ a_k^i = \\arg\\max_a Q(s, a, i) \\] <p>This balances exploitation of known good actions and exploration of uncertain ones.</p>"},{"location":"reinforcement/13_montecarlo/#136-advantages-of-mcts","title":"13.6 Advantages of MCTS","text":"<ul> <li>Anytime: Can stop search at any time and use the best estimates so far.</li> <li>Model-based or black-box: Only needs sample access to the environment.</li> <li>Best-first: Focuses computation on promising actions.</li> <li>Scalable: Avoids full enumeration of action/state spaces.</li> <li>Parallelizable: Independent simulations can be run in parallel.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#137-alphazero-and-deep-mcts","title":"13.7 AlphaZero and Deep MCTS","text":"<p>AlphaZero revolutionized game-playing AI by combining deep learning with MCTS. Key ideas:</p>"},{"location":"reinforcement/13_montecarlo/#policy-and-value-networks","title":"Policy and Value Networks","text":"<p>A neural network \\(f_\\theta(s)\\) outputs:</p> <ul> <li>\\(P\\): action probabilities</li> <li>\\(V\\): value estimate</li> </ul> \\[ (p, v) = f_\\theta(s) \\]"},{"location":"reinforcement/13_montecarlo/#alphazero-mcts-steps","title":"AlphaZero MCTS Steps","text":"<ol> <li>Select: Traverse tree using \\(Q + U\\) to choose child nodes.</li> <li>Expand: Add a new node, initialized with \\(P\\) from \\(f_\\theta\\).</li> <li>Evaluate: Use \\(v\\) from the network as the value of the leaf.</li> <li>Backup: Propagate value estimates up the tree.</li> <li>Repeat: Perform many rollouts to refine the tree.</li> </ol>"},{"location":"reinforcement/13_montecarlo/#root-action-selection","title":"Root Action Selection","text":"<p>At the root, use visit counts \\(N(s,a)\\) to compute the improved policy:</p> \\[ \\pi(s, a) \\propto N(s, a)^{1/\\tau} \\] <p>where \\(\\tau\\) controls exploration vs exploitation.</p>"},{"location":"reinforcement/13_montecarlo/#138-self-play-and-training","title":"13.8 Self-Play and Training","text":"<p>AlphaZero uses self-play to generate training data:</p> <ol> <li>Play full games using MCTS.</li> <li>Record \\((s, \\pi, z)\\) tuples where:</li> <li>\\(s\\): game state</li> <li>\\(\\pi\\): improved policy from MCTS</li> <li>\\(z\\): final game outcome</li> <li>Train \\(f_\\theta\\) to minimize combined loss:</li> </ol> \\[ \\mathcal{L} = (z - v)^2 - \\pi^\\top \\log p + \\lambda \\|\\theta\\|^2 \\] <p>This allows continual improvement without human supervision.</p>"},{"location":"reinforcement/13_montecarlo/#139-evaluation-and-impact","title":"13.9 Evaluation and Impact","text":"<ul> <li>MCTS dramatically improves performance over raw policy/value networks.</li> <li>Essential to surpassing human performance in Go, Chess, and Shogi.</li> <li>Eliminates the need for human expert data.</li> </ul> <p>Insights:</p> <ul> <li>UCT enables principled tree search with exploration.</li> <li>Neural nets guide and accelerate MCTS.</li> <li>MCTS can be used in any environment where lookahead is possible.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#1310-summary","title":"13.10 Summary","text":"<ul> <li>MCTS uses simulation-based planning with a growing search tree.</li> <li>UCT adds upper confidence bounds to balance exploration/exploitation.</li> <li>AlphaZero combines MCTS with deep learning for superhuman performance.</li> <li>Self-play enables autonomous training without labeled data.</li> </ul> <p>MCTS represents a powerful bridge between planning and learning, enabling agents to make strong decisions under uncertainty in complex domains.</p>"},{"location":"reinforcement/14_final/","title":"14. Summary and Overview","text":""},{"location":"reinforcement/14_final/#chapter-14-reinforcement-learning-review-and-wrap-up","title":"Chapter 14: Reinforcement Learning \u2013 Review and Wrap-Up","text":"<p>In this final chapter, we recap the journey of reinforcement learning (RL) from its foundational ideas in multi-armed bandits through to the cutting-edge of deep RL. Along the way we will revisit key algorithmic concepts \u2013 including Upper Confidence Bounds (UCB), Thompson Sampling, Model-Based Interval Estimation with Exploration Bonus (MBIE-EB), and Monte Carlo Tree Search (MCTS) \u2013 and highlight how different approaches to exploration (optimism vs. probability matching) have shaped the field. We will also emphasize the theoretical foundations of RL (regret minimization, PAC guarantees, Bayesian methods) and illustrate how these principles connect to real-world successes like AlphaTensor and ChatGPT. Throughout, the aim is to provide a high-level summary and synthesis, reinforcing the insights gained across previous chapters.</p>"},{"location":"reinforcement/14_final/#recap-from-bandits-to-deep-reinforcement-learning","title":"Recap: From Bandits to Deep Reinforcement Learning","text":"<p>Reinforcement learning can be defined as learning through experience (data) to make good decisions under uncertainty. In an RL problem, an agent interacts with an environment, observes states \\(s\\), takes actions \\(a\\), and receives rewards \\(r\\), with the goal of learning a policy \\(\\pi(a|s)\\) that maximizes future expected reward. Several core features distinguish RL from other learning paradigms:</p> <ul> <li> <p>Optimization of Long-Term Reward: The agent seeks to maximize cumulative reward, accounting for delayed consequences of actions.</p> </li> <li> <p>Trial-and-Error Learning: The agent learns by exploring different actions and observing outcomes, balancing exploration vs. exploitation.</p> </li> <li> <p>Generalization: The agent must generalize from limited experience to new situations (often via function approximation in large state spaces).</p> </li> <li> <p>Data Distribution Shift: Unlike supervised learning, the agent\u2019s own actions affect the data it collects and the states it visits, creating a feedback loop in the learning process.</p> </li> </ul> <p>We began our journey with multi-armed bandits, the simplest RL setting. In a bandit problem there is a single state (no state transitions); each action (arm) yields a reward drawn from an unknown distribution, and the goal is to maximize reward over repeated plays. A bandit is essentially a stateless decision problem \u2013 the next situation does not depend on the previous action. This contrasts with the general Markov Decision Process (MDP) setting, where each action can change the state and influence future rewards and decisions. Bandits capture the essence of exploration-exploitation without the complication of state transitions, making them a perfect starting point.</p> <p>From bandits we progressed to MDPs and multi-step RL problems, which introduce state dynamics and temporal credit assignment. We studied model-free methods (like Q-learning and policy gradient) and model-based methods (like planning with known models or learned models), as well as combinations thereof. As tasks grew more complex, we incorporated function approximation (e.g. using deep neural networks) to handle large or continuous state spaces. This led us into the realm of deep reinforcement learning, where algorithms like DQN and policy optimization methods (PPO, etc.) leverage deep networks as powerful function approximators. While function approximation enables scaling to complex domains, it also introduced new challenges such as stability of learning (e.g. off-policy learning instability, need for techniques like experience replay, target networks, or trust region methods). In parallel, we discussed how off-policy learning and exploration in large domains remain critical challenges, and saw approaches to address these (from clipped policy optimization (PPO) for stability, to imitation learning like DAGGER to incorporate expert knowledge, to pessimistic value adjustments for safer offline learning).</p> <p>Throughout this journey, a unifying theme has been the exploration-exploitation dilemma and the development of algorithms to efficiently learn optimal strategies. In the following sections, we summarize some key algorithmic ideas for exploration and discuss how they exemplify different strategies to address this core challenge.</p>"},{"location":"reinforcement/14_final/#key-algorithmic-ideas-in-exploration-and-planning","title":"Key Algorithmic Ideas in Exploration and Planning","text":""},{"location":"reinforcement/14_final/#optimistic-exploration-upper-confidence-bounds-ucb","title":"Optimistic Exploration: Upper Confidence Bounds (UCB)","text":"<p>A foundational idea for efficient exploration is optimism in the face of uncertainty. The principle is simple: assume the best about untried actions so that the agent is driven to explore them. The Upper Confidence Bound (UCB) algorithm is a classic realization of this idea for multi-armed bandits. UCB maintains an estimate \\(\\hat{Q}_t(a)\\) for the mean reward of each arm \\(a\\) and an uncertainty interval (confidence bound) around that estimate. At each time \\(t\\), it selects the action maximizing an upper-confidence estimate of the reward:</p> \\[ a_t = \\arg\\max_{a \\in A} \\left[ \\hat{Q}_t(a) + c \\frac{\\ln t}{N_t(a)} \\right], \\] <p>where \\(N_t(a)\\) is the number of times action \\(a\\) has been taken up to time \\(t\\), and \\(c\\) is a constant (e.g. \\(c=\\sqrt{2}\\) for the UCB1 algorithm).</p> <p>This selection rule balances exploitation (the \\(\\hat{Q}_t(a)\\) term) with exploration (the bonus term that is large for rarely-selected actions). Intuitively, UCB explores actions with high potential payoffs or high uncertainty. This approach yields strong theoretical guarantees: for instance, UCB1 achieves sublinear regret on the order of \\(O(\\ln T)\\) for bandits, meaning the gap between the accumulated reward of UCB and that of an oracle choosing the best arm at each play grows only logarithmically with time. Optimistic algorithms like UCB are attractive because they are simple and provide worst-case performance guarantees (they will eventually try everything enough to near-certainty). Variants of UCB and optimism-driven exploration have been extended beyond bandits, for example to MDPs via exploration bonus terms.</p>"},{"location":"reinforcement/14_final/#probability-matching-thompson-sampling","title":"Probability Matching: Thompson Sampling","text":"<p>An alternative approach to exploration comes from a Bayesian perspective. Instead of confidence bounds, the agent maintains a posterior distribution over the reward parameters of each action and samples an action according to the probability it is optimal. This strategy is known as Thompson Sampling (or probability matching). In the multi-armed bandit setting, Thompson Sampling can be implemented by assuming a prior for each arm\u2019s mean reward, updating it with observed rewards, and then at each step sampling a value \\(\\tilde{\\theta}_a\\) from the posterior of each arm\u2019s mean. The agent then plays the arm with the highest sampled value. By randomly exploring according to its uncertainty, Thompson Sampling naturally balances exploration and exploitation in a Bayesian-optimal way for certain problems.</p> <p>For example, if rewards are Bernoulli and a Beta prior is used for each arm\u2019s success probability, Thompson Sampling draws a sample from each arm\u2019s Beta posterior and picks the arm with the largest sample. This probability matching tends to allocate more trials to arms that are likely to be best, yet still occasionally tries others proportional to uncertainty. Empirically, Thompson Sampling often performs exceptionally well, sometimes even outperforming UCB in practice, and it has a Bayesian regret that is optimal in certain settings. The caveat is that analyzing Thompson Sampling\u2019s worst-case performance is more complex; however, theoretical advances have shown Thompson Sampling achieves \\(O(\\ln T)\\) regret for many bandit problems as well. A key appeal of Thompson Sampling is its flexibility \u2013 it can be applied to complex problems if one can sample from a posterior (or an approximate posterior) of the model\u2019s parameters. In modern RL, variants of Thompson Sampling inspire approaches like Bootstrapped DQN (which maintains an ensemble of value networks to generate randomized Q-value estimates for exploration).</p>"},{"location":"reinforcement/14_final/#pac-mdp-algorithms-and-exploration-bonuses-mbie-eb","title":"PAC-MDP Algorithms and Exploration Bonuses (MBIE-EB)","text":"<p>In full reinforcement learning problems (MDPs), the exploration challenge becomes more intricate due to state transitions. PAC-MDP algorithms provide a framework for efficient exploration with theoretical guarantees. PAC stands for \u201cProbably Approximately Correct,\u201d meaning these algorithms guarantee that with high probability (\\(1-\\delta\\)) the agent will behave near-optimally (within \\(\\varepsilon\\) of the optimal return) after a certain number of time steps that is polynomial in relevant problem parameters. In other words, a PAC-MDP algorithm will make only a finite (polynomial) number of suboptimal decisions before it effectively converges to an \\(\\varepsilon\\)-optimal policy.</p> <p>One representative PAC-MDP approach is Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) by Strehl and Littman (2008). This algorithm uses an optimistic model-based strategy: it learns an estimated MDP (transition probabilities \\(\\hat{T}\\) and rewards \\(\\hat{R}\\)) from experience and uses dynamic programming to compute a value function \\(\\tilde{Q}(s,a)\\) for that estimated model. Critically, MBIE-EB adds an exploration bonus term to reward or value updates for state-action pairs that have been infrequently visited. For example, the update might be:</p> \\[ \\tilde{Q}(s,a) \\leftarrow \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s'|s,a) \\max_{a'} \\tilde{Q}(s',a') + \\beta \\frac{1}{\\sqrt{N(s,a)}}, \\] <p>where \\(N(s,a)\\) counts visits to \\((s,a)\\) and \\(\\beta\\) is a bonus scale derived from PAC confidence bounds. The \\(\\sqrt{1/N(s,a)}\\) bonus term is large for rarely tried state-action pairs, injecting optimism that encourages the agent to explore them. MBIE-EB selects actions according to the optimistic \\(\\tilde{Q}\\) values (i.e. optimism under uncertainty in an MDP context). Strehl and Littman proved that MBIE-EB is PAC-MDP: with probability \\(1-\\delta\\), after a number of steps polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta\\), etc., the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal. PAC algorithms like MBIE-EB (and related methods like R-MAX and UCRL) guarantee efficient exploration in theory, though they can be computationally demanding in practice for large domains. They illustrate how theoretical foundations (confidence intervals and PAC guarantees) directly inform algorithm design.</p>"},{"location":"reinforcement/14_final/#monte-carlo-tree-search-mcts-for-planning","title":"Monte Carlo Tree Search (MCTS) for Planning","text":"<p>So far we have discussed exploration in the context of learning unknown values or models. Another key idea in the RL toolkit is planning using simulation, particularly via Monte Carlo Tree Search (MCTS). MCTS is a family of simulation-based search algorithms that became famous through their use in game-playing AI (e.g. AlphaGo and AlphaZero). The idea is to build a partial search tree from the current state by simulating many random play-outs (rollouts) and using the results to gradually refine value estimates for states and actions.</p> <p>One of the most widely used MCTS algorithms is UCT (Upper Confidence Trees), which blends the UCB idea with tree search. In each simulation (from root state until a terminal state or depth limit), UCT traverses the tree by choosing actions that maximize an upper confidence bound: at a state (tree node) \\(s\\), it selects the action \\(a\\) that maximizes</p> \\[ \\frac{w_{s,a}}{n_{s,a}} + c \\sqrt{\\frac{\\ln N_s}{n_{s,a}}}, \\] <p>where \\(w_{s,a}\\) is the total reward accrued from past simulations taking action \\(a\\) in state \\(s\\), \\(n_{s,a}\\) is the number of simulations that took that action, and \\(N_s = \\sum_a n_{s,a}\\) is the total simulations from state \\(s\\). This formula is essentially the UCB1 formula extended to tree nodes: the first term is exploitation (the empirical mean reward), and the second is an exploration bonus that is higher for seldom-tried actions. By using this rule at each step of simulation (Selection phase), MCTS efficiently explores the game tree, focusing on promising moves while still trying less-visited moves once in a while. After selection, a random Simulation (rollout) is played out to the end, and the outcome is backpropagated to update \\(w\\) and \\(n\\) along the path. Repeating thousands or millions of simulations yields increasingly accurate value estimates for the root state and preferred actions.</p> <p>MCTS does not learn parameters from data in the traditional sense; rather it is a planning method that can be applied if we have a generative model of the environment (e.g. a simulator or game rules). However, it connects to our theme as another approach to balancing exploration and exploitation via UCB-like algorithms. In practice, MCTS can be combined with learning. Notably, AlphaGo and AlphaZero combined deep neural networks (for state evaluation and policy guidance) with Monte Carlo Tree Search to achieve superhuman performance in Go, chess, and shogi. In those systems, the neural network\u2019s value estimates guide the rollout, and MCTS provides a powerful lookahead search that complements the learned policy. This combination dramatically improves data efficiency \u2013 for example, AlphaZero uses MCTS to effectively explore the game space instead of needing an exorbitant amount of self-play games, and the knowledge gained from MCTS is distilled back into the network through training. MCTS exemplifies how models and planning can be leveraged in RL: if a model of the environment is available (or learned), one can simulate experience to aid decision-making without direct real-world trial-and-error for every decision. This is crucial in domains where real experiments are costly or limited.</p> <p>Computational vs Data Efficiency: It is worth noting that methods like MCTS (and exhaustive exploration algorithms) tend to be computationally intensive \u2013 they trade computation for reduced real-world data needs. We often face a trade-off: algorithms that are very data-efficient (using fewer environment interactions) are often computationally expensive, whereas simpler algorithms that learn quickly in computation might require more data. In some domains (like games or simulated environments), we can afford massive computation, effectively converting computation into simulated \u201cdata\u201d for learning. In others (like physical systems or online user interactions), data is scarce or expensive, so sample-efficient algorithms (even if computationally heavy) are preferred. This trade-off has been a recurring consideration as we moved from bandits to deep RL.</p>"},{"location":"reinforcement/14_final/#exploration-paradigms-optimism-vs-probability-matching","title":"Exploration Paradigms: Optimism vs. Probability Matching","text":"<p>We have seen two major paradigms for addressing the exploration-exploitation challenge:</p> <ul> <li> <p>Optimism in the face of uncertainty: The agent behaves as if the environment is as rewarding as plausibly possible, given the data. This leads to algorithms like UCB, optimistic initial values, exploration bonuses (e.g. MBIE-EB, optimistic Q-learning), and UCT in MCTS. Optimistic methods systematically encourage trying actions that could be best. They often come with strong theoretical guarantees (UCB\u2019s regret bound, PAC-MDP bounds, etc.) because they ensure sufficient exploration of each alternative. Optimism tends to be a more worst-case (frequentist) approach: it doesn\u2019t assume a prior, just relies on confidence intervals that hold with high probability for any reward distribution.</p> </li> <li> <p>Probability matching (Thompson Sampling and Bayesian methods): The agent maintains a belief (probability distribution) about the environment\u2019s parameters and randomizes its actions according to this belief. Effectively, it samples a hypothesis for the true model and then exploits that hypothesis (e.g., play the best action for that sampled model). Over time, the belief is updated with Bayes\u2019 rule as more data comes in, so the sampling naturally shifts toward optimal actions. This approach is more Bayesian in spirit: it assumes a prior distribution and seeks to maximize performance on average with respect to that prior (i.e., good Bayesian regret). Probability matching can be very effective in practice and can incorporate prior knowledge elegantly. The downside is that providing theoretical guarantees in the worst-case sense can be challenging \u2013 the guarantees are often Bayesian (in expectation over the prior) rather than uniform for all environments. Recent theoretical work, however, has shown that even without a perfect prior, Thompson Sampling performs near-optimally in many settings, and there are ways to bound its regret. In terms of implementation complexity, Thompson Sampling may require the ability to sample from posterior distributions, which can be non-trivial in large-scale problems (though approximate methods exist). Optimistic methods, on the other hand, require confidence bound calculations, which for simple tabular cases are straightforward, but for complex function approximation can be difficult (leading to research on exploration bonuses using predictive models or uncertainty estimates).</p> </li> </ul> <p>In summary, optimism vs. probability matching represents two different philosophies for exploration. Optimistic algorithms behave more deterministically (always picking the current optimistic-best option), ensuring systematic coverage of possibilities, while Thompson-style algorithms inject randomized exploration in proportion to uncertainty. Interestingly, human decision-making experiments suggest people may combine elements of both strategies \u2013 not purely optimistic nor purely Thompson. Both paradigms have influenced modern RL: for example, exploration bonuses (optimism) are commonly used in deep RL (e.g. with bonus rewards from prediction error or curiosity), and Bayesian RL approaches (like posterior sampling for MDPs) are gaining traction for problems where a reasonable prior is available or an ensemble can approximate uncertainty.</p>"},{"location":"reinforcement/14_final/#theoretical-foundations-regret-pac-and-bayesian-optimality","title":"Theoretical Foundations: Regret, PAC, and Bayesian Optimality","text":"<p>Understanding how well an RL algorithm performs relative to an ideal standard is a major theme in RL theory. We revisited two main frameworks for this: regret analysis and PAC (sample complexity) analysis, along with the Bayesian viewpoint.</p> <ul> <li>Regret: Regret measures the opportunity loss from not acting optimally at each time step. Formally, in a bandit with optimal expected reward \\(\\mu^*\\), the regret after \\(T\\) plays is</li> </ul> \\[ R(T) = T\\mu^* - \\sum_{t=1}^T r_t, \\] <p>i.e. the difference between the reward that would be obtained by always executing the optimal arm and the reward actually obtained. Sublinear regret (e.g. \\(R(T) = o(T)\\)) implies the algorithm eventually learns the optimal policy (average regret \\(\\to 0\\) as \\(T\\) grows). We saw that \\(\\varepsilon\\)-greedy exploration can lead to linear regret in the worst case (always pulling some suboptimal arm a constant fraction of the time yields \\(R(T) \\sim \\Omega(T)\\)). In contrast, UCB1 achieves \\(R(T) = O(\\ln T)\\), which is asymptotically optimal up to constant factors (matching the Lai &amp; Robbins lower bound for bandits that \\(R(T) \\ge \\Omega(\\ln T)\\) for any algorithm). Regret analysis can be extended to MDPs (though it becomes more complex). For example, algorithms like UCRL2 (an optimistic tabular RL algorithm) have regret bounds on the order of \\(\\tilde{O}(\\sqrt{T})\\) in an MDP (reflecting the harder challenge of states) under certain assumptions. Regret is a worst-case, online metric \u2013 it asks how well we do even against an adversarially chosen problem (or in the unknown actual environment) without assumptions of a prior, focusing on long-term performance.</p> <ul> <li> <p>PAC (Probably Approximately Correct) guarantees: PAC analysis focuses on sample complexity: how many time steps or episodes are required for the algorithm to achieve near-optimal performance with high probability. A PAC guarantee typically states: for any \\(\\varepsilon, \\delta\\), there exists \\(N(\\varepsilon,\\delta)\\) (poly in relevant parameters) such that with probability at least \\(1-\\delta\\), the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal after \\(N\\) steps (or, equivalently, all but at most \\(N\\) of the steps are \\(\\varepsilon\\)-suboptimal). This is a finite-sample guarantee, giving confidence that the learning will not take too long. We discussed that algorithms like MBIE-EB and R-MAX are PAC-MDP: for a given accuracy \\(\\varepsilon\\) and confidence \\(1-\\delta\\), their sample complexity (number of suboptimal actions) is bounded by a polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta, 1/(1-\\gamma)\\), etc. PAC analysis is particularly useful when we care about guarantees in a learning phase before near-optimal performance is reached (important in safety-critical or costly domains where we need to know learning will be efficient with high probability). While regret goes to zero only asymptotically, PAC gives an explicit bound on how long it takes to be good. Often, achieving PAC guarantees in large-scale problems requires simplifying assumptions or limited function approximation classes, as general function approximation PAC results are quite difficult.</p> </li> <li> <p>Bayesian approaches and Bayes-optimality: In a Bayesian formulation, we assume a prior distribution over environments (bandit reward distributions or MDP dynamics). We can then consider the Bayes-optimal policy, which is the policy that maximizes expected cumulative reward with respect to this prior. This leads to the concept of Bayesian regret \u2013 the expected regret under the prior. A Bayes-optimal algorithm minimizes Bayesian regret and, by definition, will outperform any other algorithm on average if the prior is correct. One famous result in this vein is the Gittins Index for multi-armed bandits, which gives an optimal solution when each arm has independent known priors (casting the problem as a Markov process and solving it via dynamic programming). However, computing Bayes-optimal solutions for general RL (especially with state) is usually intractable \u2013 it involves solving a POMDP (partially observable MDP) where the hidden state is the true environment parameters. Thompson Sampling can be interpreted as an approximation to the Bayes-optimal policy that is much easier to implement. It has low Bayesian regret and in some cases can be shown to be asymptotically Bayes-optimal. The Bayesian view is powerful because it allows incorporation of prior knowledge and gives a normative standard (what should we do if we know what we don\u2019t know, in distribution). But its limitation is the computational difficulty and the dependence on having a reasonable prior. In practice, algorithms inspired by Bayesian ideas (like ensemble sampling or posterior sampling for reinforcement learning) try to capture some of the benefit without solving the full Bayes-optimal policy.</p> </li> </ul> <p>These theoretical frameworks complement each other. Regret and PAC analyses give worst-case performance assurances (no matter what the true environment is, within assumptions) and often inspire optimistic algorithms. Bayesian analysis aims for average-case optimality given prior knowledge and often inspires probability matching or adaptive algorithms. As an RL practitioner or researcher, understanding these foundations helps in choosing and designing algorithms appropriate for the problem at hand \u2013 whether one prioritizes guaranteed efficiency, practical performance with prior info, or a mix of both.</p>"},{"location":"reinforcement/14_final/#from-theory-to-practice-real-world-applications-and-achievements","title":"From Theory to Practice: Real-World Applications and Achievements","text":"<p>One of the most exciting aspects of the recent decade in RL is seeing theoretical ideas translate into real-world (or at least real-problem) successes. In this section, we connect some of the classic algorithms and concepts to notable applications:</p> <ul> <li> <p>Game Mastery and Planning \u2013 AlphaGo, AlphaZero, AlphaTensor: Starting with games, AlphaGo famously combined deep neural networks with MCTS (using UCT) and was trained with reinforcement learning to defeat human Go champions. Its successor AlphaZero took this further by learning from scratch (self-play) for multiple games, using Monte Carlo Tree Search guided by a learned value/policy network. The blend of planning (MCTS) and learning (deep RL) that AlphaZero employs is a direct embodiment of concepts we covered: it uses optimistic simulations (MCTS uses UCB in the tree) and improves data efficiency by leveraging a model (the game simulator) for exploration. The success of AlphaZero demonstrates the power of combining model-based search with model-free function approximation. Recently, these ideas have even extended to domains beyond traditional games. AlphaTensor (DeepMind, 2022) is a system that treated the discovery of new matrix multiplication algorithms as a single-player game, and it applied a variant of AlphaZero\u2019s RL approach to find faster algorithms for matrix multiply. The AlphaTensor agent was trained via self-play reinforcement learning to manipulate tensor representations of matrix multiplication and achieved a breakthrough: it discovered matrix multiplication algorithms that surpass the decades-old human benchmarks in efficiency. This is a striking example of RL not just playing games but discovering algorithms \u2013 essentially using reward signals to guide a search through the space of mathematical formulas. It showcases how MCTS (for planning) and deep RL can work together on combinatorial optimization problems: the agent expands a search tree of partial solutions, guided by value networks and an exploration policy, very much like how it would approach a board game. AlphaTensor\u2019s success underscores the generality of RL methods and how ideas like optimism (self-play explores new moves) and guided search can yield new discoveries.</p> </li> <li> <p>Natural Language and Human Feedback \u2013 ChatGPT: A more recent and widely impactful application of reinforcement learning is in natural language processing \u2013 specifically, training large language models to better align with human intentions. ChatGPT (OpenAI, 2022) is a prime example, where RL was used to fine-tune a pretrained language model using human feedback. The technique, known as Reinforcement Learning from Human Feedback (RLHF), involves first collecting human preference data on model outputs and then training a reward model that predicts human preference. The language model (policy) is then optimized (via a policy gradient method like PPO) to maximize the reward model\u2019s score, i.e. to produce answers humans would rate highly. This is essentially an RL loop on top of the language model, treating the task of generating helpful, correct responses as an MDP (or episodic decision problem) and using the learned reward function as the reward signal. The result, ChatGPT, is notably more aligned with user expectations than its predecessor models. In our context, ChatGPT\u2019s training illustrates several RL ideas in action: offline data (pretraining on text) combined with online RL fine-tuning, and the critical role of a well-shaped reward function for alignment. It also highlights exploration in a different sense \u2013 exploring the space of possible answers to find those that yield high reward according to human feedback. The success of ChatGPT demonstrates that RL is not limited to games or robotics; it can be scaled to very high-dimensional action spaces (like generating entire paragraphs of text) when guided by human-informed rewards. From a theoretical lens, one can view RLHF as optimizing an objective that marries the model\u2019s knowledge (from supervised training) with a policy optimization under a learned reward. While classical exploration algorithms (UCB, Thompson) are not directly apparent in ChatGPT\u2019s training (since the \u201cexploration\u201d comes from the model generating varied outputs and the policy optimization process), the high-level principle remains: use feedback signals to iteratively refine behavior.</p> </li> <li> <p>Scientific and Industrial Applications: Beyond these headline examples, RL is increasingly applied in scientific and industrial domains. The course of our study touched on a few, such as:</p> </li> </ul> <p>Controlling nuclear fusion plasmas: Researchers applied deep RL to control the magnetic coils in a tokamak reactor to sustain plasma configurations. This is a complex continuous control problem with safety constraints, where function approximation and careful exploration (largely in simulations before real experiments) were key.</p> <p>Optimizing public health interventions: An RL approach was used to design efficient COVID-19 border testing policies. Framing the problem as a sequential decision task (who to test and when) and using RL to maximize some health outcome or efficiency metric allowed automating policy design that adapted to data.</p> <p>Robotics and Autonomous Systems: Many advances in robotics have come from RL algorithms that allow robots to learn locomotion, manipulation, or flight. Often these use deep RL and sometimes simulation-to-reality transfer. The exploration techniques we learned (like curiosity-driven bonuses or domain randomization) help address the challenge of learning in these complex environments.</p> <p>Recommender Systems and Online Decision Making: Multi-armed bandit algorithms (including Thompson Sampling and UCB) are widely used in industry for things like A/B testing, website optimization, and personalized recommendations. For example, serving personalized content can be seen as a bandit problem where each content choice is an arm and click-through or engagement is the reward. Companies employ bandit algorithms to balance exploration of new content with exploitation of known user preferences, often in a context of contextual bandits (where the state or context is user features). The theoretical guarantees of bandit algorithms give confidence in their performance, and their simplicity makes them practical at scale.</p> <p>In all these cases, the fundamental concepts from this course appear and validate themselves: whether it\u2019s optimism guiding AlphaZero\u2019s search, or Thompson Sampling driving an online recommendation strategy, or policy gradients tuning ChatGPT using human rewards, the same core ideas of reinforcement learning apply. Modern applications often hybridize approaches \u2013 for instance, using model-based simulations (AlphaTensor, AlphaZero), or combining learning from offline data with online exploration (ChatGPT\u2019s RLHF, or robotics). This underscores the importance of mastering the basics: understanding value functions, policy optimization, exploration mechanisms, and theoretical limits has direct relevance even as we push RL into new territory.</p>"},{"location":"reinforcement/14_final/#final-takeaways","title":"Final Takeaways","text":"<p>In closing, we synthesize a few key insights and lessons from the full RL journey:</p> <ul> <li> <p>Reinforcement Learning Unifies Many Themes: We saw that RL problems range from simple bandits to complex high-dimensional control, but they share the need for sequential decision making under uncertainty. Concepts like state, action, reward, policy, value function, model form a common language to describe problems as diverse as games, robotics, and recommendation systems. Recognizing an appropriate RL formulation (MDP, bandit, etc.) for a given real-world problem is the first step to applying these methods.</p> </li> <li> <p>Exploration vs. Exploitation is Fundamental: The trade-off between trying new actions and leveraging known good actions underpins all of RL. We examined different strategies:</p> <ul> <li> <p>Heuristics like \\(\\epsilon\\)-greedy (simple but can be suboptimal),</p> </li> <li> <p>Optimistic algorithms (UCB, optimism in value iteration,  exploration bonuses) which ensure systematic exploration using confidence bounds,</p> </li> <li> <p>Probabilistic approaches (Thompson Sampling, randomized value functions) which inject randomness based on uncertainty.</p> </li> </ul> </li> </ul> <p>Each approach has its advantages \u2013 optimism often yields strong guarantees and is conceptually straightforward, while Thompson Sampling often gives excellent practical performance and naturally incorporates prior knowledge. In large-scale problems, clever exploration bonuses (intrinsic rewards for novelty) and approximate uncertainty estimates are key to maintaining exploration. The central lesson is that successful RL requires deliberate exploration strategies; naive exploration can lead to poor sample efficiency or getting stuck in suboptimal behaviors.</p> <ul> <li> <p>Theoretical Foundations Guide Algorithm Design: Concepts like regret and PAC provide ways to formally measure learning efficiency. They not only help us compare algorithms (e.g. which has lower regret or better sample complexity) but have directly inspired algorithmic techniques (like UCB from the idea of minimizing regret, or PAC-inspired algorithms like MBIE-EB and R-MAX designed to guarantee learning within polynomial time). Meanwhile, the Bayesian perspective offers a gold-standard for optimal decision-making given prior info, even if it\u2019s often computationally intractable \u2013 it guides us toward algorithms that perform well on average and informs approaches like posterior sampling. As RL practitioners, we should remember:</p> <ul> <li> <p>Regret minimization focuses on not wasting too many opportunities \u2013 it\u2019s about learning as fast as possible in an online sense.</p> </li> <li> <p>PAC guarantees focus on bounding the learning time with high confidence \u2013 giving safety that an algorithm won\u2019t do too poorly for too long.</p> </li> <li> <p>Bayesian optimality focuses on using prior knowledge efficiently \u2013 it\u2019s about doing the best given what you (probabilistically) know.</p> </li> </ul> </li> </ul> <p>All three perspectives are important; balancing them or choosing the right one depends on the application (e.g., in a one-off A/B test you might care about regret, in a lifelong robot learning you care about sample efficiency with high probability, and in a personalized system you might incorporate Bayesian priors about users).</p> <ul> <li> <p>Function Approximation and Deep RL Open New Possibilities (and Challenges): The leap from tabular or small-scale problems to real-world complexity required using function approximation (especially deep neural networks). This enabled RL to handle images, continuous states, and enormous state spaces \u2013 as seen in Atari games, Go, and continuous control benchmarks. The success of deep RL (DQN, policy gradient methods, etc.) comes from blending RL algorithms with powerful representation learning. However, it also brought challenges like stability of training, overfitting, exploration in high dimensions, and reproducibility issues. Key techniques to mitigate these include experience replay, target networks, regularization, large-scale parallel training, and reward shaping. The takeaway is that theoretical convergence guarantees often break down with function approximation, so a lot of practical know-how and experimentation is needed. Yet, the core ideas (Bellman equations, policy improvement, etc.) still apply \u2013 just approximate. The field is actively developing better theories for RL with function approximation (e.g. understanding generalization, error propagation) and techniques for more reliable training.</p> </li> <li> <p>Real-World Impact and Ongoing Research: Reinforcement learning has graduated from textbook problems to impacting real-world systems. Its principles have powered superhuman game AIs, improved scientific research (e.g. algorithm discovery, experiment design), enhanced language models, and optimized business decisions. At the same time, truly robust and general-purpose RL is still an open challenge. Issues of stability, efficiency, and safety remain \u2013 for instance:</p> <ul> <li>Developing algorithms that work out-of-the-box with minimal tuning for any problem (robustness).</li> <li>Improving data efficiency so that RL can be applied with limited real-world interactions (e.g., via model-based methods, better exploration, or transfer learning).</li> <li>Integrating learning and planning seamlessly, and handling settings that mix offline data with online exploration.</li> <li>Expanding the RL framework to account for multiple objectives, collaboration or competition (multi-agent RL), and richer feedback modalities beyond scalar rewards.</li> </ul> </li> </ul> <p>These are active research directions. The skills and concepts acquired \u2013 from understanding theoretical bounds to implementing algorithms \u2013 equip us to tackle these frontiers.</p> <p>In summary, the journey from multi-armed bandits to deep reinforcement learning has taught us not only a catalogue of algorithms, but a way of thinking about sequential decision problems. We learned how to measure learning efficiency and why exploration is hard yet critical. We saw simple ideas like optimism and probability matching scale up to complex systems that play Go or converse in English. As you move forward from this textbook, remember the foundational principles: reward is your guide, value estimation is your tool, policy is your output, and exploration is your catalyst. With these in mind, you are well-prepared to both apply RL to challenging problems and to contribute to the advancing frontier of reinforcement learning research.</p>"},{"location":"reinforcement/1_intro/","title":"1. Introduction to Reinforcement Learning","text":""},{"location":"reinforcement/1_intro/#chapter-1-introduction-to-reinforcement-learning","title":"Chapter 1: Introduction to Reinforcement Learning","text":"<p>Reinforcement Learning is a paradigm in machine learning where an agent learns to make sequential decisions through interaction with an environment. Unlike supervised learning, where the agent learns from labeled examples, or unsupervised learning, where it learns patterns from unlabeled data, reinforcement learning is driven by the goal of maximizing cumulative reward through trial and error. The agent is not told which actions to take but must discover them by exploring the consequences of its actions.</p> <p>Sequential decision-making under uncertainty is at the heart of reinforcement learning. The agent must balance exploration and exploitation. Exploration is needed to gather information about the environment, while exploitation uses this information to select actions that appear best.</p> <pre><code> RL vs Supervised Learning (Key Differences)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Supervised Learning:                                \u2502\n       \u2502   \u2013 Learns from labeled examples (input \u2192 target)   \u2502\n       \u2502   \u2013 Feedback is immediate and correct               \u2502\n       \u2502   \u2013 IID data; no sequential dependence              \u2502\n       \u2502                                                     \u2502\n       \u2502 Reinforcement Learning:                             \u2502\n       \u2502   \u2013 Learns from interaction (trial &amp; error)         \u2502\n       \u2502   \u2013 Feedback (reward) may be delayed or sparse      \u2502\n       \u2502   \u2013 Data depends on agent's actions (non-IID)       \u2502\n       \u2502   \u2013 Must balance exploration vs exploitation        \u2502\n       \u2502   \u2013 Must solve temporal credit assignment           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>A key characteristic of reinforcement learning is that the outcome of an action may not be immediately known. Rewards can be delayed, making it hard to determine which past actions are responsible for future outcomes. This challenge is known as temporal credit assignment. Successful reinforcement learning algorithms must learn to attribute long-term consequences to earlier decisions.</p> <p>At each time step, the agent observes some representation of the world, takes an action, and receives a reward. The world then transitions to a new state. This interaction continues over time, forming an experience trajectory:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, \\dots \\] <p>The agent\u2019s goal is to learn a policy, which is a mapping from states to actions, that maximizes the total reward it collects over time.</p> <p>The total future reward is defined through the notion of return. The most common formulation is the discounted return:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>where \\(0 \\le \\gamma \\le 1\\) is called the discount factor. It determines how much the agent values immediate rewards compared to future rewards. A smaller \\(\\gamma\\) encourages short-term decisions, while a larger \\(\\gamma\\) favors long-term planning.</p> <p>Reinforcement learning involves four fundamental challenges:</p> <ol> <li>Optimization: The agent must find an optimal policy that maximizes expected return.</li> <li>Delayed consequences: Actions can affect rewards far into the future, making credit assignment difficult.</li> <li>Exploration: The agent must try actions to learn their consequences, even though some actions may seem suboptimal in the short term.</li> <li>Generalization: The agent must use limited experience to generalize to states it has never seen before.</li> </ol> <p>The main components of a reinforcement learning system are the agent, the environment, actions, states, and rewards. The agent chooses an action based on its current state. The environment responds with the next state and a reward. From this interaction, the agent must infer how to improve its decisions over time.</p> <p>The concept of state is crucial. A state is a summary of information that can influence future outcomes. In theory, a state is Markov if it satisfies:</p> \\[ p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t) \\] <p>where \\(h_t\\) is the full history of past observations, actions, and rewards. This means that the future depends only on the current state, not on the entire past. The Markov property is important because it simplifies the learning problem and allows powerful mathematical tools to be applied.</p> <p>State (environment)  \u2192  Action (agent)  \u2192  Next State (environment)</p> <p>Reinforcement learning is particularly useful in domains where optimal behavior is not easily specified, data is limited or must be collected through interaction, and long-term consequences matter. Examples include robotics, autonomous vehicles, game playing, resource allocation, recommendation systems, and online decision-making.</p>"},{"location":"reinforcement/1_intro/#key-concepts","title":"Key Concepts:","text":""},{"location":"reinforcement/1_intro/#episodic-vs-continuing","title":"Episodic vs Continuing:","text":"<p>Reinforcement learning problems can be episodic or continuing. In episodic tasks, interactions end after a finite number of steps, and the agent resets for a new episode. In continuing tasks, the interactions never formally end, and the agent must learn to behave well indefinitely. In episodic settings, the return is naturally finite. In continuing tasks, discounting or average reward formulations are used to ensure the return is well-defined.</p>"},{"location":"reinforcement/1_intro/#types-of-rl-tasks","title":"Types of RL tasks:","text":"<p>There are several types of learning tasks in RL:</p> <ol> <li>Prediction/Policy Evaluation: Estimating how good a given policy is.</li> <li>Control: Finding an optimal policy that maximizes expected return.</li> <li>Planning: Computing optimal policies using a known model of the environment.</li> </ol>"},{"location":"reinforcement/1_intro/#model-based-vs-model-free","title":"Model based vs Model-free","text":"<p>Reinforcement learning algorithms can be classified into two major categories: model-based and model-free. Model-based methods assume that the transition dynamics and reward function of the environment are known or learned. They use this information for planning. Model-free methods do not assume access to this knowledge and must learn directly from interaction.</p>"},{"location":"reinforcement/1_intro/#on-policy-vs-off-policy","title":"On-policy vs off-policy","text":"<ul> <li> <p>On-policy learning:</p> <ul> <li>Direct experience.</li> <li>Learn to estimate and evaluate a policy from experience obtained from following that policy.</li> </ul> </li> <li> <p>Off-policy Learning</p> <ul> <li>Learn to estimate and evaluate a policy using experience gathered from following a different policy.</li> </ul> </li> </ul>"},{"location":"reinforcement/1_intro/#tabular-vs-function-approximation","title":"Tabular vs Function Approximation","text":"<p>In small environments with a limited number of states and actions, value functions and policies can be represented using tables. This is known as the tabular setting. However, real-world problems often involve very large or continuous state spaces, where it is impossible to maintain a separate entry for every state or action.</p> <p>In such cases, we approximate the value function or policy using a parameterized function, such as a linear model or neural network. This approach is called function approximation. Function approximation enables generalization: knowledge gained from one state can be applied to many similar states, making learning feasible in large or continuous environments.</p>"},{"location":"reinforcement/2_mdp/","title":"2. MDPs & Dynamic Programming","text":""},{"location":"reinforcement/2_mdp/#chapter-2-markov-decision-processes-and-dynamic-programming","title":"Chapter 2: Markov Decision Processes and Dynamic Programming","text":"<p>Reinforcement Learning relies on the mathematical framework of Markov Decision Processes (MDPs) to formalize sequential decision-making under uncertainty. The key idea is that an agent interacts with an environment, making decisions that influence both immediate and future rewards.</p> <p>Reinforcement Learning is about selecting actions over time to maximize long-term reward.</p>"},{"location":"reinforcement/2_mdp/#the-markovian-hierarchy","title":"The Markovian Hierarchy","text":"<p>The RL framework is built upon three foundational models, each adding complexity and agency.</p>"},{"location":"reinforcement/2_mdp/#the-markov-process","title":"The Markov Process","text":"<p>A Markov Process, or Markov Chain, is the simplest model, concerned only with the flow of states. It is defined by the set of States (\\(S\\)) and the Transition Model (\\(P(s' \\mid s)\\)). The defining characteristic is the Markov Property: the next state is independent of the past states, given only the current state.</p> \\[ P(s_{t+1} \\mid s_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t) \\] <p>The future is conditionally independent of the past given the present. Intuition: MPs describe what happens but do not assign any value to these events.</p>"},{"location":"reinforcement/2_mdp/#the-markov-reward-process-mrp","title":"The Markov Reward Process (MRP)","text":"<p>A Markov Reward Process (MRP) extends an MP by adding rewards and discounting. An MRP is a tuple \\((S, P, R, \\gamma)\\) where \\(R(s)\\) is the expected reward for being in state \\(s\\) and \\(\\gamma\\) is the discount factor. The return is:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>The goal is to compute the value function, which is the expected return starting from a state \\(s\\):</p> \\[ V(s) = \\mathbb{E}[G_t | s_t = s] \\] <p>The value function satisfies the Bellman Expectation Equation:</p> \\[ V(s) = R(s) + \\gamma \\sum_{s'} P(s'|s)V(s') \\] <p>This recursive structure relates the value of a state to the values of its successor states.</p>"},{"location":"reinforcement/2_mdp/#the-markov-decision-process-mdp","title":"The Markov Decision Process (MDP)","text":"<p>An MDP introduces agency. Defined by the tuple \\((S, A, P, R, \\gamma)\\), it extends the MRP by giving the agent a set of Actions (\\(A\\)) to choose from.</p> <ul> <li>Action-Dependent Transition: \\(P(s' \\mid s, a)\\)</li> <li>Action-Dependent Reward: \\(R(s, a)\\)</li> </ul> <p>The agent's strategy is described by a Policy (\\(\\pi(a \\mid s)\\)), the probability of selecting action \\(a\\) in state \\(s\\). A key insight is that fixing any policy \\(\\pi\\) reduces an MDP back into an MRP, allowing all tools developed for MRPs to be applied to the MDP.</p> \\[ R_\\pi(s) = \\sum_a \\pi(a|s) R(s,a) \\] \\[ P_\\pi(s'|s) = \\sum_a \\pi(a|s) P(s'|s,a) \\] <p>Once actions are introduced in an MDP, it becomes useful to evaluate not only how good a state is, but how good a particular action is relative to the policy\u2019s expected behavior. This leads to the advantage function.</p> <p>The state-value function measures how good it is to be in a state: \\(V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\).</p> <p>The action-value function measures how good it is to take action \\(a\\) in state \\(s\\):\\(Q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s,\\; a_t = a]\\)</p> <p>The advantage function compares these two: \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s).\\)</p> <p>\\(V_\\pi(s)\\) is how well the policy performs on average from state \\(s\\).</p> <p>\\(Q_\\pi(s,a)\\) is how well it performs if it specifically takes action \\(a\\).</p> <p>Therefore, the advantage tells us: How much better or worse action \\(a\\) is compared to what the policy would normally do in state \\(s\\).</p>"},{"location":"reinforcement/2_mdp/#value-functions-and-expectation","title":"Value Functions and Expectation","text":"<p>To evaluate a fixed policy \\(\\pi\\), we define two inter-related value functions based on the Bellman Expectation Equations.</p>"},{"location":"reinforcement/2_mdp/#state-value-function-vpis","title":"State Value Function (\\(V^\\pi(s)\\))","text":"<p>\\(V^\\pi(s)\\) quantifies the long-term expected return starting from state \\(s\\) and strictly following policy \\(\\pi\\).  </p> <p>How much total reward should I expect if I start in state s and follow policy \\(\\pi\\): forever?</p>"},{"location":"reinforcement/2_mdp/#state-action-value-function-qpisa","title":"State-Action Value Function (\\(Q^\\pi(s,a)\\))","text":"<p>\\(Q^\\pi(s,a)\\) is a more granular measure, quantifying the expected return if the agent takes action \\(a\\) in state \\(s\\) first, and then follows policy \\(\\pi\\).  </p> <p>Intuition: The \\(Q\\)-function is the value of doing a specific action; the \\(V\\)-function is the value of being in a state (the weighted average of the \\(Q\\)-values offered by the policy \\(\\pi\\) in that state):  </p> <p>The Bellman Expectation Equation for \\(V^\\pi\\) links the value of a state to the values of the actions chosen by \\(\\pi\\) and the resulting future states:  </p>"},{"location":"reinforcement/2_mdp/#optimal-control-finding-pi","title":"Optimal Control: Finding \\(\\pi^*\\)","text":"<p>The ultimate goal of solving an MDP is to find the optimal policy (\\(\\pi^*\\)) that maximizes the expected return from every state \\(s\\).</p> \\[ \\pi^* = \\operatorname*{arg\\,max}_{\\pi} V^\\pi(s) \\quad \\text{for all } s \\in S \\] <p>This optimal policy is characterized by the Optimal Value Functions (\\(V^*\\) and \\(Q^*\\)).</p>"},{"location":"reinforcement/2_mdp/#the-bellman-optimality-equations","title":"The Bellman Optimality Equations","text":"<p>These equations are fundamental, describing the unique value functions that arise when acting optimally. Unlike the expectation equations, they contain a \\(\\max\\) operator, making them non-linear.</p> <ul> <li> <p>Optimal State Value (\\(V^*\\)): The optimal value of a state equals the maximum expected return achievable from any single action \\(a\\) taken from that state:</p> \\[ V^*(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right] \\] </li> <li> <p>Optimal Action-Value (\\(Q^*\\)): The optimal value of taking action \\(a\\) is the immediate reward plus the discounted value of the optimal subsequent actions (\\(\\max_{a'}\\)) in the next state \\(s'\\):</p> \\[ Q^*(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a') \\] </li> </ul> <p>Once \\(Q^*\\) is known, the optimal policy \\(\\pi^*\\) is easily extracted by simply choosing the action that maximizes \\(Q^*(s,a)\\) in every state:  </p> <p>These equations are non-linear due to the max operator and must be solved iteratively.</p>"},{"location":"reinforcement/2_mdp/#dynamic-programming-algorithms","title":"Dynamic Programming Algorithms","text":"<p>For MDPs where the model (\\(P\\) and \\(R\\)) is fully known, Dynamic Programming methods are used to solve the Bellman Optimality Equations iteratively.</p>"},{"location":"reinforcement/2_mdp/#policy-iteration","title":"Policy Iteration","text":"<p>Policy Iteration follows an alternating cycle of Evaluation and Improvement. It takes fewer, but more expensive, iterations to converge.</p> <ol> <li>Policy Evaluation: For the current policy \\(\\pi_k\\), compute \\(V^{\\pi_k}\\) by iteratively applying the Bellman Expectation Equation until full convergence. This is the computationally intensive step.      </li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}\\) by choosing an action that is greedy with respect to the fully converged \\(V^{\\pi_k}\\).      </li> </ol> <p>The process repeats until the policy stabilizes (\\(\\pi_{k+1} = \\pi_k\\)), guaranteeing convergence to \\(\\pi^*\\).</p>"},{"location":"reinforcement/2_mdp/#value-iteration","title":"Value Iteration","text":"<p>Value Iteration is a single, continuous process that combines evaluation and improvement by repeatedly applying the Bellman Optimality Equation. It takes many, but computationally cheap, iterations.</p> <ol> <li>Iterative Update: For every state \\(s\\), update the value function \\(V_k(s)\\) using the \\(\\max\\) operation. This immediately incorporates a greedy improvement step into the value update.      </li> <li>Convergence: The iterations stop when \\(V_{k+1}\\) is sufficiently close to \\(V^*\\).</li> <li>Extraction: The optimal policy \\(\\pi^*\\) is then extracted greedily from the final \\(V^*\\).</li> </ol>"},{"location":"reinforcement/2_mdp/#pi-vs-vi","title":"PI vs VI","text":"Feature Policy Iteration (PI) Value Iteration (VI) Core Idea Evaluate completely, then improve. Greedily improve values in every step. Equation Uses Bellman Expectation (inner loop) Uses Bellman Optimality (max) Convergence Few, large policy steps. Policy guaranteed to stabilize faster. Many, small value steps. Value function converges slowly to \\(V^*\\). Cost High cost per iteration (due to full evaluation). Low cost per iteration (due to one-step backup)."},{"location":"reinforcement/2_mdp/#mdps-mental-map","title":"MDPs Mental Map","text":"<pre><code>                   Markov Decision Processes (MDPs)\n        Formalizing Sequential Decision-Making under Uncertainty\n                                  \u2502\n                                  \u25bc\n                       Progression of Markov Models\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Process (MP): States &amp; Transition Probabilities \u2502\n       \u2502   [S, P(s'|s)] \u2014 No rewards, no decisions               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Reward Process (MRP): MP + Rewards + \u03b3          \u2502\n       \u2502  [S, P(s'|s), R(s), \u03b3]                                  \u2502\n       \u2502    Value Function: V(s) = E[Gt | st = s]                \u2502\n       \u2502     Bellman Expectation Eqn:                            \u2502\n       \u2502     V(s) = R(s) + \u03b3 \u2211 P(s'|s)V(s')                      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Decision Process (MDP): MRP + Actions           \u2502\n       \u2502   [S, A, P(s'|s,a), R(s,a), \u03b3]                          \u2502\n       \u2502    Adds Agency: Agent chooses actions                   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                             Policy \u03c0(a|s)\n                        Agent\u2019s decision strategy\n                                  \u2502\n                                  \u25bc\n                          Value Functions\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 State Value V\u03c0(s): Expected return following \u03c0                 \u2502\n     \u2502 Q\u03c0(s,a): Expected return from (s,a) then follow \u03c0              \u2502\n     \u2502 Relationship: V\u03c0(s) = \u2211 \u03c0(a|s) Q\u03c0(s,a)                         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                    Bellman Expectation Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V\u03c0(s) = \u2211 \u03c0(a|s)[R(s,a) + \u03b3 \u2211 P(s'|s,a)V\u03c0(s')]                 \u2502\n     \u2502 Q\u03c0(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) V\u03c0(s')                        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n               Goal: Find Optimal Policy \u03c0*\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u03c0*(s) = argmax\u2090 Q*(s,a)                                     \u2502\n     \u2502 V*(s): Max possible value from state s under the optimal    |\n     |        policy                                               \u2502\n     \u2502 Q*(s,a): Max possible return state s by taking action a     |\n     |          and thereafter following the optimal policy        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                      Bellman Optimality Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V*(s) = max\u2090 [R(s,a) + \u03b3 \u2211 P(s'|s,a)V*(s')]                 \u2502\n     \u2502 Q*(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) max\u2090' Q*(s',a')            \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                 Solution when Model (P,R) is known:\n                    Dynamic Programming (DP)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Policy Iteration                              \u2502 Value Iteration - \u2502\n     \u2502 (Alternating Evaluation &amp; Improvement)        \u2502 Single update step\u2502\n     \u2502                                               \u2502 repeatedly        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy Eval     \u2502                          \u2502 Bellman Optimality      \u2502\n  \u2502 Using V\u03c0 until  \u2502                          \u2502 Update every iteration  \u2502\n  \u2502 convergence     \u2502                          \u2502 V_(k+1) = max_a[....]   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy          \u2502                          \u2502 After convergence:      \u2502\n  \u2502 Improvement:    \u2502                          \u2502 extract \u03c0* from Q*      \u2502\n  \u2502 \u03c0_(k+1)=argmax Q\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n             Outcome: Optimal Policy and Value Functions\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03c0*(s) \u2014 Best action at each state                   \u2502\n       \u2502 V*(s) \u2014 Max return achievable                       \u2502\n       \u2502 Q*(s,a) \u2014 Max return from (s,a)                     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/3_modelfree/","title":"3. Model-Free Prediction","text":""},{"location":"reinforcement/3_modelfree/#chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy","title":"Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy","text":"<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function \\(V^\\pi(s)\\) or \\(Q^\\pi(s,a)\\) for a given policy \\(\\pi\\) using data of the form:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\dots \\] <p>The true value of a state under policy \\(\\pi\\) is still defined by the expected return:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | s_t = s] \\] <p>but the agent must approximate this expectation using sampled experience.</p> <p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p> <ol> <li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li> <li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li> </ol>"},{"location":"reinforcement/3_modelfree/#monte-carlo-policy-evaluation","title":"Monte Carlo Policy Evaluation","text":"<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function \\(V^\\pi(s)\\) is the expected return, we can approximate it by simply averaging the observed returns (\\(G_t\\)) from many episodes that start at state \\(s\\).</p> \\[ V^\\pi(s) \\approx \\text{Average of observed returns } G_t \\text{ starting from } s \\]"},{"location":"reinforcement/3_modelfree/#key-properties-of-mc","title":"Key Properties of MC","text":"<ol> <li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (\\(s_T\\)) to calculate the full return \\(G_t\\).</li> <li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li> </ol> <p>We can maintain the value estimates \\(V(s)\\) using counts and sums, or through incremental updates.</p>"},{"location":"reinforcement/3_modelfree/#a-first-visit-vs-every-visit-mc","title":"A. First-Visit vs. Every-Visit MC","text":"<p>When computing the return \\(G_t\\) for a state \\(s\\) in a single trajectory, a state might be visited multiple times.</p> <ul> <li>First-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) only the first time state \\(s\\) is visited in an episode.<ul> <li>Properties: First-Visit MC is an unbiased estimator of \\(V^\\pi(s)\\). It is also consistent (converges to the true value as data \\(\\rightarrow \\infty\\)) by the Law of Large Numbers.</li> </ul> </li> <li>Every-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) every time state \\(s\\) is visited in an episode.<ul> <li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li> </ul> </li> </ul>"},{"location":"reinforcement/3_modelfree/#b-incremental-monte-carlo","title":"B. Incremental Monte Carlo","text":"<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p> \\[ V(s) \\leftarrow V(s) + \\alpha \\left[ G_t - V(s) \\right] \\] <p>Where:</p> <ul> <li>\\(G_t\\): The actual observed return (our target).</li> <li>\\(V(s)\\): Our current estimate (our old value).</li> <li>\\(\\alpha\\): The learning rate (\\(\\alpha \\in (0, 1]\\)), which can be fixed or decayed.</li> </ul> <p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (\\(V^\\pi\\)), the learning rate \\(\\alpha_t\\) (which may be \\(1/N(s)\\) or a fixed constant) must satisfy the following conditions:</p> <ol> <li>The sum of all learning rates for state \\(s\\) must diverge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s) = \\infty\\)</li> <li>The sum of the squared learning rates must converge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s)^2 &lt; \\infty\\)</li> </ol>"},{"location":"reinforcement/3_modelfree/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>While MC uses the full return \\(G_t\\), TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>"},{"location":"reinforcement/3_modelfree/#bootstrapping-the-core-idea","title":"Bootstrapping: The Core Idea","text":"<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the next state, \\(V(s_{t+1})\\), to update the value of the current state, \\(V(s_t)\\). The standard TD algorithm is TD(0) (or one-step TD).</p>"},{"location":"reinforcement/3_modelfree/#the-td0-update-rule","title":"The TD(0) Update Rule","text":"<p>The TD(0) update replaces the full return \\(G_t\\) with the TD Target (\\(r_t + \\gamma V(s_{t+1})\\)):</p> \\[ V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ \\underbrace{r_{t+1} + \\gamma V(s_{t+1})}_{\\text{TD Target}} - V(s_t) \\right] \\] <p>The term inside the brackets is the TD Error (\\(\\delta_t\\)):  This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>"},{"location":"reinforcement/3_modelfree/#td-vs-monte-carlo","title":"TD vs. Monte Carlo","text":"<p>The distinction between TD and MC centers on what is used as the target value:</p> Feature Monte Carlo (MC) Temporal Difference (TD) Target \\(G_t\\) (Full observed return to episode end) \\(r_{t+1} + \\gamma V(s_{t+1})\\) (One-step return + estimated future value) Bootstrapping No (waits until episode end) Yes (uses \\(V(s_{t+1})\\)) Bias Unbiased (First-Visit MC) Biased (because \\(V(s_{t+1})\\) is an estimate) Variance High Variance (Return \\(G_t\\) is a sum of many random steps) Low Variance (TD target depends on only one random reward/next state) Convergence Consistent (converges to true \\(V^\\pi\\)) TD(0) converges to true \\(V^\\pi\\) in the tabular case <p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>"},{"location":"reinforcement/3_modelfree/#example-setup","title":"Example Setup","text":""},{"location":"reinforcement/3_modelfree/#parameters","title":"Parameters","text":"<ul> <li>States (\\(S\\)): \\(s_A, s_B, s_C\\)</li> <li>Discount Factor (\\(\\gamma\\)): \\(0.9\\)</li> <li>Learning Rate (\\(\\alpha\\)): \\(0.5\\) (Used for TD updates)</li> <li>Initial Value Estimates (\\(V_0\\)): \\(V(s_A)=0, V(s_B)=0, V(s_C)=0\\)</li> </ul>"},{"location":"reinforcement/3_modelfree/#episodes-and-returns","title":"Episodes and Returns","text":"<p>The full return (\\(G_t\\)) is calculated for every visit in every episode:</p> Episode (E) Trajectory (State \\(\\xrightarrow{r}\\) Next State) Visit Time (\\(t\\)) State (\\(s_t\\)) Full Return (\\(G_t\\)) E1 \\(s_A \\xrightarrow{r=1} s_B \\xrightarrow{r=0} s_C \\xrightarrow{r=5} s_B \\xrightarrow{r=2} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{6.508}\\) 1 \\(s_B\\) (1st) \\(\\mathbf{6.12}\\) 2 \\(s_C\\) \\(\\mathbf{6.8}\\) 3 \\(s_B\\) (2nd) \\(\\mathbf{2.0}\\) E2 \\(s_A \\xrightarrow{r=-2} s_C \\xrightarrow{r=8} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{5.2}\\) 1 \\(s_C\\) \\(\\mathbf{8.0}\\) E3 \\(s_B \\xrightarrow{r=10} s_C \\xrightarrow{r=-5} s_B \\xrightarrow{r=1} \\text{T}\\) 0 \\(s_B\\) (1st) \\(\\mathbf{6.31}\\) 1 \\(s_C\\) \\(\\mathbf{-4.1}\\) 2 \\(s_B\\) (2nd) \\(\\mathbf{1.0}\\)"},{"location":"reinforcement/3_modelfree/#1-first-visit-monte-carlo-mc","title":"1. First-Visit Monte Carlo (MC)","text":"<p>Rule: Only the first return for a state in any given episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508\\) (E1), \\(5.2\\) (E2) \\(11.708\\) 2 \\(s_B\\) \\(6.12\\) (E1), \\(6.31\\) (E3) \\(12.43\\) 2 \\(s_C\\) \\(6.8\\) (E1), \\(8.0\\) (E2), \\(-4.1\\) (E3) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":""},{"location":"reinforcement/3_modelfree/#2-every-visit-monte-carlo-mc","title":"2. Every-Visit Monte Carlo (MC)","text":"<p>Rule: The return from every time a state is encountered in any episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns_1","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508, 5.2\\) \\(11.708\\) 2 \\(s_B\\) \\(6.12, 2.0, 6.31, 1.0\\) \\(15.43\\) 4 \\(s_C\\) \\(6.8, 8.0, -4.1\\) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns_1","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":"\\[ V(s_A) = \\frac{11.708}{2} = \\mathbf{5.854} \\\\ V(s_B) = \\frac{15.43}{4} = \\mathbf{3.858} \\\\ V(s_C) = \\frac{10.7}{3} = \\mathbf{3.567} \\]"},{"location":"reinforcement/3_modelfree/#3-temporal-difference-td0","title":"3. Temporal Difference (TD(0))","text":"<p>Rule: The value is updated after every step using the TD Target (\\(r_{t+1} + \\gamma V(s_{t+1})\\)) and the learning rate \\(\\alpha\\). The updated \\(V(s)\\) estimates are carried over to the next step and episode.</p>"},{"location":"reinforcement/3_modelfree/#a-step-by-step-td-calculation-summary","title":"A. Step-by-Step TD Calculation Summary","text":"Step Transition Old \\(V(s_t)\\) New \\(V(s_t)\\) \\(V(s_A)\\) \\(V(s_B)\\) \\(V(s_C)\\) E1-1 \\(s_A \\xrightarrow{r=1} s_B\\) 0 0.500 0.500 0.000 0.000 E1-2 \\(s_B \\xrightarrow{r=0} s_C\\) 0 0.000 0.500 0.000 0.000 E1-3 \\(s_C \\xrightarrow{r=5} s_B\\) 0 2.500 0.500 0.000 2.500 E1-4 \\(s_B \\xrightarrow{r=2} \\text{T}\\) 0.0 1.000 0.500 1.000 2.500 E2-1 \\(s_A \\xrightarrow{r=-2} s_C\\) 0.500 0.375 0.375 1.000 2.500 E2-2 \\(s_C \\xrightarrow{r=8} \\text{T}\\) 2.500 5.250 0.375 1.000 5.250 E3-1 \\(s_B \\xrightarrow{r=10} s_C\\) 1.000 7.863 0.375 7.863 5.250 E3-2 \\(s_C \\xrightarrow{r=-5} s_B\\) 5.250 3.663 0.375 7.863 3.663 E3-3 \\(s_B \\xrightarrow{r=1} \\text{T}\\) 7.863 4.431 0.375 4.431 3.663"},{"location":"reinforcement/3_modelfree/#b-final-estimates-v_td0","title":"B. Final Estimates (\\(V_{TD(0)}\\))","text":"\\[ V(s_A) = \\mathbf{0.375} \\\\ V(s_B) = \\mathbf{4.431} \\\\ V(s_C) = \\mathbf{3.663} \\]"},{"location":"reinforcement/3_modelfree/#comparison-of-results","title":"Comparison of Results","text":"State First-Visit MC Every-Visit MC TD(0) (\\(\\alpha=0.5, \\gamma=0.9\\)) Note \\(s_A\\) 5.854 5.854 0.375 TD heavily penalized \\(s_A\\) in E2 (Target 0.25), while MC averaged the full observed high returns. \\(s_B\\) 6.215 3.858 4.431 TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping. \\(s_C\\) 3.567 3.567 3.663 All methods are close for \\(s_C\\). <p>This comparison illustrates the bias-variance trade-off: * MC uses the sample return (\\(G_t\\)), which has high variance but is an unbiased target (First-Visit). * TD uses a bootstrapped estimate (\\(r + \\gamma V(s')\\)), which has lower variance but introduces bias by relying on an estimated successor value.</p>"},{"location":"reinforcement/3_modelfree/#model-free-prediction-mental-map","title":"Model Free Prediction Mental Map","text":"<pre><code>            Model-Free Prediction\n     (Policy Evaluation without Model P or R)\n                        \u2502\n                        \u25bc\n                Goal: Estimate\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 State Value: V\u03c0(s)                \u2502\n       \u2502 Action Value: Q\u03c0(s,a)             \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n              Using Sampled Experience\n        (s\u2080,a\u2080,r\u2081,s\u2081,a\u2081,r\u2082,... from \u03c0)\n                        \u2502\n                        \u25bc\n            Two Families of Methods\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Monte Carlo (MC)              \u2502 Temporal Difference (TD)      \u2502\n    \u2502 \"Learn from full episodes\"    \u2502 \"Learn step-by-step\"          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u25bc                           \u25bc\n             Monte Carlo (MC)              Temporal Difference (TD)\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502 Needs full episodes     \u2502       \u2502Works on incomplete episodes\u2502\n      \u2502 No bootstrapping        \u2502       \u2502Uses bootstrapping          \u2502\n      \u2502 High variance           \u2502       \u2502Low variance                \u2502\n      \u2502 Unbiased (first visit)  \u2502       \u2502Biased                      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n        \u2502                               \u2502           \u2502\n        \u25bc                               \u25bc           \u25bc\n First-Visit MC                  Every-Visit MC     TD(0) Update Rule\n (One update per episode          (Multiple updates \u2502 V(s) \u2190 V(s) +\n  per state)                      per episode)      \u2502 \u03b1[ r + \u03b3V(s') \u2212 V(s) ]\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                        Comparison (Bias\u2013Variance)\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 MC: Unbiased, High variance             \u2502\n               \u2502 TD: Biased, Lower variance              \u2502\n               \u2502 MC: Not bootstrapping                   \u2502\n               \u2502 TD: Bootstraps using V(s\u2019)              \u2502\n               \u2502 MC: Episodic only                       \u2502\n               \u2502 TD: Works for continuing tasks          \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                      Outcome: Learned Value Function\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 V\u03c0(s) or Q\u03c0(s,a) from real experience \u2502\n               \u2502 (No model of environment required)    \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/4_model_free_control/","title":"4. Model-Free Control","text":""},{"location":"reinforcement/4_model_free_control/#chapter-4-model-free-control-learning-optimal-behavior-without-a-model","title":"Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model","text":"<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy \\(\\pi^*\\) without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>"},{"location":"reinforcement/4_model_free_control/#from-state-values-to-action-values","title":"From State Values to Action Values","text":"<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p> <p>  This update requires two things:</p> <ul> <li>the transition probabilities \\(P(s'|s,a)\\)</li> <li>the expected reward R(s,a)$</li> </ul> <p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p> <p>Instead of learning the state-value function \\(V^\\pi(s)\\) and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.  </p> <p>The Model-Free Policy Iteration loop:</p> <ol> <li>Policy Evaluation: Compute \\(Q^{\\pi}\\) from experience.</li> <li>Policy Improvement: Update the policy \\(\\pi\\) given the estimated \\(Q^{\\pi}\\).</li> </ol> <p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>"},{"location":"reinforcement/4_model_free_control/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>To learn optimal behavior, the agent must balance two goals:</p> <ol> <li>Exploitation: choose actions believed to yield high rewards.</li> <li>Exploration: try actions whose consequences are uncertain or poorly understood.</li> </ol> <p>A common solution is the \\(\\epsilon\\)-greedy policy:</p> <p>With probability \\(1 - \\epsilon\\), choose the action with the highest estimated value. With probability \\(\\epsilon\\), choose a random action.</p> <p>Formally:</p> \\[ \\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A|} &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\frac{\\epsilon}{|A|} &amp; \\text{otherwise} \\end{cases} \\] <p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>"},{"location":"reinforcement/4_model_free_control/#monte-carlo-control","title":"Monte Carlo Control","text":"<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating \\(V(s)\\), it estimates \\(Q(s,a)\\) using sampled returns.</p> <p>Monte Carlo Policy Evaluation, Now for Q:       1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  \\(k=1\\),  Input \\(\\epsilon=1\\), \\(\\pi\\) 2: loop over epiosdes      3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 4: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 5: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 6: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 7: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 8: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 9: \\(\\quad\\quad\\)       end if 10: \\(\\quad\\)  end for 11: \\(\\quad\\) \\(k = k + 1\\) 12: end loop</p> <p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using \\(\\epsilon\\)-greedy policies for exploration.</p> <ul> <li>Policy Evaluation: \\(Q(s, a)\\) is updated using the full return (\\(G_t\\)) observed after the state-action pair \\((s_t, a_t)\\) has occurred in an episode. The incremental update uses the formula \\(Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\\). </li> <li>Policy Improvement: The new policy \\(\\pi_{k+1}\\) is set to be \\(\\epsilon\\)-greedy with respect to the updated \\(Q\\) function.</li> </ul>"},{"location":"reinforcement/4_model_free_control/#greedy-in-the-limit-of-infinite-exploration-glie","title":"Greedy in the Limit of Infinite Exploration (GLIE)","text":"<p>For Monte Carlo Control to converge to the optimal action-value function \\(Q^*(s, a)\\), the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p> <ol> <li>Infinite Visits: All state-action pairs \\((s, a)\\) must be visited an infinite number of times (\\(\\lim_{i \\rightarrow \\infty} N_i(s, a) \\rightarrow \\infty\\)).</li> <li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li> </ol> <p>A simple strategy to satisfy GLIE is to use an \\(\\epsilon\\)-greedy policy where \\(\\epsilon\\) is decayed over time, such as \\(\\epsilon_i = 1/i\\) (where \\(i\\) is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function \\(Q^*(s, a)\\).</p> <p>Monte Carlo Online Control/On Policy Improvement:    </p> <p>1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  Set \\(k=1\\), \\(\\epsilon=1\\).    2: \\(\\pi_k = \\epsilon - greedy (Q)\\) // Create initial \\(\\epsilon\\) - greedy policy. 3: loop over epiosdes    4: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 5: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 6: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 7: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 8: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 9: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 10: \\(\\quad\\quad\\)       end if  11: \\(\\quad\\)  end for   12:  \\(\\quad\\) \\(\\pi_k = \\epsilon - greedy (Q)\\) //Policy improvement 12: \\(\\quad\\) \\(k = k + 1\\) , \\(\\epsilon = \\frac{1}{k}\\)  13: end loop    </p> <p>This process gradually adjusts the policy and the value estimates until they converge.</p>"},{"location":"reinforcement/4_model_free_control/#iv-temporal-difference-td-control","title":"IV. Temporal Difference (TD) Control","text":"<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>"},{"location":"reinforcement/4_model_free_control/#on-policy-td-control-sarsa","title":"On-Policy TD Control: SARSA","text":"<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy currently being followed (\\(\\pi\\)). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p> <p>The update for the action-value \\(Q(s_t, a_t)\\) uses the value of the next state-action pair, \\((s_{t+1}, a_{t+1})\\), selected by the current policy \\(\\pi\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>The TD Target here is \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\). SARSA learns \\(Q^{\\pi}\\) while \\(\\pi\\) is improved greedily with respect to \\(Q^{\\pi}\\), allowing it to find the optimal policy \\(\\pi^*\\).</p> <p>1: Set initial \\(\\epsilon\\)-greedy policy \\(\\pi\\) randomly, \\(t=0\\), initial state \\(s_t=s_0\\)  2: Take \\(a_t \\sim \\pi(s_t)\\)  3: Observe \\((r_t, s_{t+1})\\)  4: loop        5: \\(\\quad\\) Take action \\(a_{t+1} \\sim \\pi(s_{t+1})\\) // Sample action from policy         6: \\(\\quad\\) Observe \\((r_{t+1}, s_{t+2})\\)  7: \\(\\quad\\) Update \\(Q\\) given \\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\\):     8: \\(\\quad\\) Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p> \\[\\forall s \\in S,\\;\\; \\pi(s) = \\begin{cases} \\arg\\max\\limits_a Q(s,a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{a random action}   &amp; \\text{with probability } \\epsilon \\end{cases}\\] <p>9: \\(\\quad\\) \\(t = t + 1\\) , \\(\\epsilon = \\frac{1}{t}\\)  10: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#b-off-policy-td-control-q-learning","title":"B. Off-Policy TD Control: Q-Learning","text":"<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (\\(\\pi^*\\), the target policy) using experience gathered by a different behavior policy (\\(\\pi_b\\)).</p> <p>In Q-Learning, the agent acts using a soft, exploratory \\(\\pi_b\\) (like \\(\\epsilon\\)-greedy) but the value function update is based on the best possible action from the next state, effectively estimating \\(Q^*\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right] \\] <p>The key difference is the target: Q-Learning uses the value of the max action (\\(\\max_{a'} Q(s_{t+1}, a')\\)), regardless of what action was actually taken in the next step. This makes it a greedy update towards \\(Q^*\\).</p> <p>Q-Learning (Off-Policy TD Control):</p> <p>1: Initialize \\(Q(s,a)=0 \\quad \\forall s \\in S, a \\in A\\), set \\(t = 0\\), initial state \\(s_t = s_0\\)  2: Set \\(\\pi_b\\) to be \\(\\epsilon\\)-greedy w.r.t. \\(Q\\)  3: loop    4: \\(\\quad\\) Take \\(a_t \\sim \\pi_b(s_t)\\) // Sample action from behavior policy    5: \\(\\quad\\) Observe \\((r_t, s_{t+1})\\)  6: \\(\\quad\\) \\(Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\\)  7: \\(\\quad\\) \\(\\pi(s_t) = \\begin{cases} \\arg\\max\\limits_a Q(s_t,a) &amp; \\text{with probability } 1 - \\epsilon \\ \\text{a random action} &amp; \\text{with probability } \\epsilon \\end{cases}\\)  8: \\(\\quad\\) \\(t = t + 1\\)  9: end loop     </p>"},{"location":"reinforcement/4_model_free_control/#value-function-approximation-vfa","title":"Value Function Approximation (VFA)","text":"<p>All methods discussed so far assume a tabular representation, where a separate entry for \\(Q(s, a)\\) is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>"},{"location":"reinforcement/4_model_free_control/#motivation-for-approximation","title":"Motivation for Approximation","text":"<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p> <ol> <li>Memory: Explicitly storing every \\(V\\) or \\(Q\\) value is impossible.</li> <li>Computation: Computing or updating every value is too slow.</li> <li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li> </ol> <p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: \\(\\hat{Q}(s, a; \\mathbf{w}) \\approx Q(s, a)\\). The goal shifts from filling a table to finding the parameter vector \\(\\mathbf{w}\\) that minimizes the error between the true value and the estimate.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\left( Q^{\\pi}(s, a) - \\hat{Q}(s, a; \\mathbf{w}) \\right)^2 \\right] \\] <p>The parameter vector \\(\\mathbf{w}\\) is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function \\(J(\\mathbf{w})\\).</p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-with-vfa-policy-evaluation","title":"Model-Free Control with VFA Policy Evaluation","text":"<p>When using function approximation, we substitute the old \\(Q(s, a)\\) in the update rules (MC, SARSA, Q-Learning) with the function approximator \\(\\hat{Q}(s, a; \\mathbf{w})\\).</p> <ul> <li> <p>MC VFA for Policy Evaluation: </p> <p>The return \\(G_t\\) is used as the target in an SGD update: \\(\\Delta \\mathbf{w} \\propto \\alpha (G_t - \\hat{Q}(s_t, a_t; \\mathbf{w})) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t, a_t; \\mathbf{w})\\).</p> <p>1: Initialize \\(\\mathbf{w}\\), set \\(k = 1\\)  2: loop    3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,L_k})\\) given \\(\\pi\\)  4: \\(\\quad\\) for \\(t = 1, \\dots, L_k\\) do      5: \\(\\quad\\quad\\) if First visit to \\((s,a)\\) in episode \\(k\\) then      6: \\(\\quad\\quad\\quad\\) \\(G_t(s,a) = \\sum_{j=t}^{L_k} r_{k,j}\\)  7: \\(\\quad\\quad\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 \\left[ G_t(s,a) - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w})\\) // Compute Gradient    8: \\(\\quad\\quad\\quad\\) Update weights: \\(\\Delta \\mathbf{w}\\)  9: \\(\\quad\\quad\\) end if 10: \\(\\quad\\) end for  11: \\(\\quad\\) \\(k = k + 1\\)  12: end loop    </p> </li> <li> <p>SARSA with VFA: The TD target is \\(r + \\gamma \\hat{Q}(s', a'; \\mathbf{w})\\), leveraging the current function approximation.</p> <p>1: Initialize \\(\\mathbf{w}\\), \\(s\\)  2: loop    3: \\(\\quad\\) Given \\(s\\), sample \\(a \\sim \\pi(s)\\), observe \\(r(s,a)\\), and \\(s' \\sim p(s'|s,a)\\)  4: \\(\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 [r + \\gamma \\hat{V}(s';\\mathbf{w}) - \\hat{V}(s;\\mathbf{w})] \\nabla_{\\mathbf{w}} \\hat{V}(s;\\mathbf{w})\\)  5: \\(\\quad\\) Update weights \\(\\Delta \\mathbf{w}\\)  6: \\(\\quad\\) if \\(s'\\) is not a terminal state then    7: \\(\\quad\\quad\\) Set \\(s = s'\\)  8: \\(\\quad\\) else        9: \\(\\quad\\quad\\) Restart episode, sample initial state \\(s\\)  10: \\(\\quad\\) end if     11: end loop       * Q-Learning with VFA: The TD target is \\(r + \\gamma \\max_{a'} \\hat{Q}(s', a'; \\mathbf{w})\\).</p> </li> </ul>"},{"location":"reinforcement/4_model_free_control/#control-using-vfa","title":"Control using VFA","text":"<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular \\(Q(s,a)\\) with a function approximator \\(\\hat{Q}(s,a;\\mathbf{w})\\), we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by \\(\\hat{Q}(s,a;\\mathbf{w})\\).</p> <p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each \\((s,a)\\), the agent learns a parameter vector \\(\\mathbf{w}\\) that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function \\(Q^*(s,a)\\).</p> <p>The learning problem becomes:</p> \\[ \\min_{\\mathbf{w}} \\; J(\\mathbf{w}) = \\mathbb{E} \\left[ \\left( Q^*(s,a) - \\hat{Q}(s,a;\\mathbf{w}) \\right)^2 \\right] \\] <p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p> \\[ \\Delta \\mathbf{w} \\propto \\left( \\text{target} - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w}) \\] <p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p> Method Target for updating \\(\\mathbf{w}\\) Monte Carlo \\(G_t\\) SARSA \\(r + \\gamma \\hat{Q}(s',a';\\mathbf{w})\\) Q-Learning \\(r + \\gamma \\max_{a'} \\hat{Q}(s',a';\\mathbf{w})\\) <p>These methods now operate in the same way as before, except instead of updating a single \\(Q(s,a)\\) entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>"},{"location":"reinforcement/4_model_free_control/#challenges-the-deadly-triad","title":"Challenges: The Deadly Triad","text":"<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p> \\[ \\text{Function Approximation} \\;+\\; \\text{Bootstrapping} \\;+\\; \\text{Off-policy Learning} \\] <p>This combination is known as the Deadly Triad .</p> <ul> <li>Function Approximation : Generalizes across states but may introduce bias.</li> <li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li> <li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li> </ul> <p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p> <p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate \\(Q(s,a)\\).</p>"},{"location":"reinforcement/4_model_free_control/#deep-q-networks-dqn","title":"Deep Q-Networks (DQN)","text":"<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function \\(\\hat{Q}(s, a; \\mathbf{w})\\) is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p> <p>DQN stabilizes the non-linear learning process using two critical techniques:</p> <ol> <li> <p>Experience Replay (ER): Transitions \\((s_t, a_t, r_t, s_{t+1})\\) are stored in a replay buffer (\\(\\mathcal{D}\\)). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from \\(\\mathcal{D}\\) for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p> </li> <li> <p>Fixed Q-Targets: The Q-Learning update requires a target value \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w})\\). To prevent the estimate \\(\\hat{Q}(s, a; \\mathbf{w})\\) from chasing its own rapidly changing target, the parameters \\(\\mathbf{w}^{-}\\) used to compute the target are fixed for a period of time, then synchronized with the current parameters \\(\\mathbf{w}\\). This provides a stable target \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w}^{-})\\).</p> </li> </ol> <p>Deep Q-Network (DQN) Algorithm:</p> <p>1: Input \\(C\\), \\(\\alpha\\), \\(D = {}\\), Initialize \\(\\mathbf{w}\\), \\(\\mathbf{w}^- = \\mathbf{w}\\), \\(t = 0\\)  2: Get initial state \\(s_0\\)  3: loop        4: \\(\\quad\\) Sample action \\(a_t\\) using \\(\\epsilon\\)-greedy policy w.r.t. current \\(\\hat{Q}(s_t, a; \\mathbf{w})\\)  5: \\(\\quad\\) Observe reward \\(r_t\\) and next state \\(s_{t+1}\\)  6: \\(\\quad\\) Store transition \\((s_t, a_t, r_t, s_{t+1})\\) in replay buffer \\(D\\)  7: \\(\\quad\\) Sample a random minibatch of tuples \\((s_i, a_i, r_i, s'i)\\) from \\(D\\)  8: \\(\\quad\\) for \\(j\\) in minibatch do     9: \\(\\quad\\quad\\) if episode terminates at step \\(i+1\\) then       10: \\(\\quad\\quad\\quad\\) \\(y_i = r_i\\)  11: \\(\\quad\\quad\\) else      12: \\(\\quad\\quad\\quad\\) \\(y_i = r_i + \\gamma \\max\\limits{a'} \\hat{Q}(s'i, a'; \\mathbf{w}^-)\\)  13: \\(\\quad\\quad\\) end if    14: \\(\\quad\\quad\\) Update \\(\\mathbf{w}\\) using gradient descent:   \\(\\quad\\quad\\quad\\) \\(\\Delta \\mathbf{w} = \\alpha \\left( y_i - \\hat{Q}(s_i, a_i; \\mathbf{w}) \\right) \\nabla{\\mathbf{w}} \\hat{Q}(s_i, a_i; \\mathbf{w})\\)  15: \\(\\quad\\) end for        16: \\(\\quad\\) \\(t = t + 1\\)  17: \\(\\quad\\) if \\(t \\mod C == 0\\) then    18: \\(\\quad\\quad\\) \\(\\mathbf{w}^- \\leftarrow \\mathbf{w}\\)  19: \\(\\quad\\) end if     20: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-mental-map","title":"Model Free Control Mental Map","text":"<pre><code>                     Model-Free Control\n    Goal: Learn the Optimal Policy \u03c0* without knowing P or R\n                               \u2502\n                               \u25bc\n           Key Concept: Action-Value Function Q(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Q\u03c0(s,a) = Expected return by taking action a \u2502\n       \u2502in state s and following policy \u03c0 thereafter \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                      No model \u2192 Learn Q directly\n                               \u2502\n                               \u25bc\n                   Generalized Policy Iteration\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Policy Evaluation       \u2502     Policy Improvement    \u2502\n       \u2502   Learn Q\u03c0(s,a)           \u2502   \u03c0 \u2190 greedy w.r.t Q      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Challenge: Exploration vs. Exploitation\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Greedy policy \u2192 Exploits but stops exploring          \u2502\n       \u2502\u03b5-greedy policy \u2192 Balances exploration &amp; exploitation \u2502\n       \u2502GLIE condition: \u03b5 \u2192 0 and \u221e exploration               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Model-Free Control Families (Tabular)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Monte Carlo Control      \u2502      Temporal Difference   \u2502\n       \u2502   (Episode-based)          \u2502      (Step-based)          \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                        \u25bc\n Monte Carlo Control:                       TD Control:\n Estimates Q from full returns          Estimates Q usingbootstrapped targets\n Uses \u03b5-greedy policy                   Works online, faster, low variance\n Episodic only                          Works for episodic &amp; continuing\n          \u2502                                        \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 GLIE MC Control   \u2502             \u2502 On-Policy TD: SARSA        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 Off-Policy TD: Q-Learning  \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    |\n                                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n| On-Policy TD \u2014 SARSA                     |  Off-Policy TD \u2014 Q-Learning       |\n| Learns Q\u03c0 for the policy being followed  |  Learns Q* while following \u03c0_b    |\n| Update uses next action from \u03c0           |  Update uses max action (greedy)  |\n| Update Target:                           |  Update Target:                   |\n|  r + \u03b3 Q(s',a')                          |  r + \u03b3 max\u2090 Q(s',a)               |\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n      Value Function Approximation (Large/Continuous spaces)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Replace Q(s,a) with Q\u0302(s,a;w) using function approx   \u2502\n       \u2502 Generalization across states                         \u2502\n       \u2502 Gradient-based updates (SGD)                         \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Deep Q-Learning (DQN) \u2014 Stable VFA Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Experience Replay \u2014 decorrelate samples             \u2502\n       \u2502 Target Networks \u2014 stabilize bootstrapped targets    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Final Outcome of Model-Free Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly from experience without model       \u2502\n       \u2502 Learn Q*(s,a) through MC, SARSA, or Q-Learning        \u2502\n       \u2502 Scale to large spaces using function approximation    \u2502\n       \u2502 DQN enables deep RL in complex environments           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/5_policy_gradient/","title":"5. Policy Gradient Methods","text":""},{"location":"reinforcement/5_policy_gradient/#chapter-5-policy-gradient-methods","title":"Chapter 5: Policy Gradient Methods","text":"<p>In previous chapters, we derived policies indirectly from value functions using greedy or \u03b5-greedy strategies. However, value-based RL has several challenges:</p> <ul> <li>Does not naturally support stochastic policies  </li> <li>Struggles in continuous action spaces  </li> <li>Optimizing through value functions is often indirect and unstable</li> </ul> <p>Policy Gradient Methods directly optimize the policy itself:</p> \\[ \\pi_\\theta(a|s) = P(a \\mid s; \\theta) \\] <p>Our goal becomes:</p> \\[ \\theta^* = \\arg\\max_\\theta V(\\theta) \\] <p>That is, learn policy parameters \u03b8 that maximize expected return.</p> <p>Value-based methods struggle in these cases because they do not directly learn the policy. Instead, they estimate action values \\(Q(s,a)\\) and derive a policy using greedy or \u03b5-greedy strategies. This makes the policy indirect and unstable. Small changes in \\(Q(s,a)\\) can suddenly change the best action, making learning discontinuous and erratic \u2014 especially with function approximation like neural networks. Furthermore, value-based methods do not naturally support stochastic or continuous action spaces, since computing \\(\\arg\\max_a Q(s,a)\\) is infeasible when actions are continuous or infinite. Policy-based methods solve this problem by directly modeling and learning the policy, such as using a softmax distribution for discrete actions or Gaussian distributions for continuous actions.</p>"},{"location":"reinforcement/5_policy_gradient/#value-based-vs-policy-based-rl","title":"Value-Based vs Policy-Based RL","text":"Approach What is Learned? Policy Type Works in Continuous Actions? Value-Based \\(V(s)\\) or \\(Q(s,a)\\) Indirect (\u03b5-greedy, greedy) No Policy-Based \\(\\pi_\\theta(a/s)\\) Direct, stochastic Yes Actor-Critic Both Direct &amp; learned Yes"},{"location":"reinforcement/5_policy_gradient/#policy-optimization-objective","title":"Policy Optimization Objective","text":"<p>In policy-based reinforcement learning, the policy itself is directly parameterized as \\(\\pi_\\theta(a|s)\\), and our goal is to find the parameters \\(\\theta\\) that produce the best possible behavior. For episodic tasks starting at initial state \\(s_0\\), the quality of a policy is measured by its expected return:</p> \\[ V(\\theta) = V_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[G_0] \\] <p>Therefore, policy optimization can be formulated as an optimization problem, where the goal is to find the policy parameters \\(\\theta\\) that maximize the expected return:</p> \\[ \\theta^* = \\arg\\max_\\theta V(s_0, \\theta) \\] <p>This optimization does not necessarily require gradients.  We can also use gradient-free (derivative-free) optimization methods such as:</p> <ul> <li>Hill Climbing \u2013 Iteratively adjusts parameters in small random directions and keeps changes that improve performance.</li> <li>Simplex / Amoeba / Nelder-Mead \u2013 Uses a geometric shape (simplex) to explore the parameter space and moves it towards higher-performing regions.</li> <li>Genetic Algorithms \u2013 Evolves a population of candidate policies using selection, crossover, and mutation, inspired by natural evolution.</li> <li>Cross-Entropy Method (CEM) \u2013 Samples multiple policy candidates, selects the top performers, and updates the sampling distribution towards them.</li> <li>Covariance Matrix Adaptation (CMA) \u2013 Adapts both the mean and covariance of a Gaussian distribution to efficiently search complex, high-dimensional policy spaces.</li> </ul> <p>Gradient-free policy optimization methods are often excellent and simple baselines to try.  They are highly flexible, can work with any policy parameterization (including non-differentiable ones), and are easy to parallelize, as policies can be evaluated independently across multiple environments. However, these methods are typically less sample efficient because they treat each policy evaluation as a black box and ignore the temporal structure of trajectories. They do not make use of gradients, value functions, or bootstrapping.</p> <p>To improve efficiency, we can use gradient-based optimization techniques, which exploit  the structure of the return function and update parameters using local information. Common gradient-based optimizers include:</p> <ul> <li>Gradient Descent</li> <li>Conjugate Gradient</li> <li>Quasi-Newton Methods (e.g., BFGS, L-BFGS)</li> </ul> <p>These methods are generally more sample efficient, especially in large or continuous state-action spaces.</p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient","title":"Policy Gradient","text":"<p>The goal is to find parameters \\(\\theta\\) that maximize the expected return. Policy gradient algorithms search for a local maximum of \\(V(\\theta)\\) by performing gradient ascent:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta V(s_0, \\theta) \\] <p>where \\(\\alpha\\) is the step-size (learning rate) and \\(\\nabla_\\theta V(s_0, \\theta)\\) is the policy gradient.</p> <p>Assuming an episodic MDP with discount factor \\(\\gamma = 1\\), the value of a parameterized policy \\(\\pi_\\theta\\) starting from state \\(s_0\\) is</p> \\[ V(s_0,\\theta)  = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} r(s_t, a_t); \\; s_0, \\pi_\\theta \\right], \\] <p>where the expectation is taken over the states and actions visited when following \\(\\pi_\\theta\\). This policy value can be re-expressed in multiple ways.  </p> <ul> <li> <p>First, in terms of the action-value function:      </p> </li> <li> <p>Second, in terms of full trajectories. Let a state\u2013action trajectory be:</p> <p>\\(\\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\\dots,s_{T-1},a_{T-1},r_T),\\)</p> <p>and define</p> <p>  as the sum of rewards of trajectory \\(\\tau\\).  </p> <p>Let \\(P(\\tau;\\theta)\\) denote the probability of trajectory \\(\\tau\\) when starting in \\(s_0\\) and following policy \\(\\pi_\\theta\\). Then:</p> \\[ V(s_0,\\theta) = \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau).\\] </li> </ul> <p>In this trajectory notation, our optimization objective becomes</p> \\[ \\theta^*  = \\arg\\max_{\\theta} V(s_0,\\theta)  = \\arg\\max_{\\theta} \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau). \\] <p>Taking gradient yields:</p> \\[ \\nabla_\\theta V(\\theta) =  \\sum_{\\tau} P(\\tau|\\theta)R(\\tau)  \\nabla_\\theta \\log P(\\tau|\\theta) \\] <p>Using sampled trajectories:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m}  R(\\tau^{(i)}) \\nabla_\\theta \\log P(\\tau^{(i)}|\\theta) \\] <p>Trajectory probability:</p> \\[ P(\\tau|\\theta) = P(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t)\\cdot P(s_{t+1}|s_t,a_t) \\] <p>Since dynamics are independent of \\(\\theta\\):</p> \\[ \\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\] <p>Thus:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m} R(\\tau^{(i)}) \\sum_{t=0}^{T-1}  \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) \\] <p>The term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) is called the score function.  It is the gradient of the log of a parameterized probability distribution and measures how sensitive the policy\u2019s action probability is to changes in the parameters \\(\\theta\\).  It plays a central role in policy gradient methods because it allows us to estimate gradients without knowing the environment dynamics, using only samples from the policy.</p>"},{"location":"reinforcement/5_policy_gradient/#softmax-policy-discrete-action-spaces","title":"Softmax Policy (Discrete Action Spaces)","text":"<p>In discrete action spaces, a common parameterization of the policy is the softmax policy, which assigns probabilities based on exponentiated weighted features. Each action is represented using feature vector \\(\\phi(s,a)\\), and the policy is defined as:</p> \\[ \\pi_\\theta(a|s) =  \\frac{e^{\\phi(s,a)^T \\theta}}      {\\sum_{a'} e^{\\phi(s,a')^T \\theta}} \\] <p>The corresponding score function is:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\phi(s,a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\phi(s,a')] \\] <p>This means the gradient increases the probability of the selected action's features and decreases the probability of competing actions based on their expected feature values.</p>"},{"location":"reinforcement/5_policy_gradient/#gaussian-policy-continuous-action-spaces","title":"Gaussian Policy (Continuous Action Spaces)","text":"<p>For continuous action spaces, the Gaussian policy is a natural choice.  The policy outputs actions by sampling from a normal distribution:</p> \\[ a \\sim \\mathcal{N}(\\mu(s), \\sigma^2) \\] <p>The mean is a linear function of state features:</p> \\[ \\mu(s) = \\phi(s)^T \\theta \\] <p>If we assume a fixed variance \\(\\sigma^2\\), the score function becomes:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\frac{(a - \\mu(s))}{\\sigma^2} \\, \\phi(s) \\] <p>This tells us that the gradient increases the likelihood of actions that are close to the mean \\(\\mu(s)\\) and reduces the probability of actions that deviate from it.</p> <p>Deep neural networks (and other differentiable models) can also be used to represent \\(\\pi_\\theta(a|s)\\), allowing score functions to be computed automatically using backpropagation.</p> <p>Intution:  Think of a sample trajectory \\(\\tau\\) as something we tried \u2014 a sequence of states, actions, and rewards collected during an episode. The return \\(R(\\tau)\\) tells us how good that sample was (higher return means better behavior). The gradient term \\(\\nabla_\\theta \\log P(\\tau|\\theta)\\) tells us how to adjust the policy parameters \\(\\theta\\) to make the trajectory more or less likely. So, when we multiply them:  we are effectively saying: If a trajectory was good, update the policy to make it more likely to occur again. If it was bad, update the policy to make it less likely. This simple idea is the core of policy gradient methods.</p>"},{"location":"reinforcement/5_policy_gradient/#reinforce-algorithm-monte-carlo-policy-gradient","title":"REINFORCE Algorithm (Monte Carlo Policy Gradient)","text":"<p>Update rule:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t \\] <p>Algorithm:</p> <p>1: Initialize policy parameters \\(\\theta\\) 2: loop (for each episode) 3: \\(\\quad\\) Generate a trajectory \\(\\tau = (s_0, a_0, r_1, \\dots, s_T)\\) using \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each time step \\(t\\) in \\(\\tau\\) 5: \\(\\quad\\quad\\) Compute return: \\(\\qquad R_t = \\sum_{k=t}^{T-1} \\gamma^{\\,k-t} r_{k+1}\\) 6: \\(\\quad\\quad\\) Compute policy gradient term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) 7: \\(\\quad\\quad\\) Update policy parameters: \\(\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t\\) 8: \\(\\quad\\) end for 9: end loop  </p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient-methods-mental-map","title":"Policy Gradient Methods \u2014 Mental Map","text":"<pre><code>                     Policy Gradient Methods\n    Goal: Learn the optimal policy \u03c0* directly (no Q or V tables)\n                               \u2502\n                               \u25bc\n         Key Concept: Parameterized Policy \u03c0\u03b8(a|s)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy is a function with parameters \u03b8      \u2502\n       \u2502 \u03c0\u03b8(a|s) gives probability of taking action a\u2502\n       \u2502 Optimization targets J(\u03b8)=Expected Return   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                    Direct Policy Optimization\n                               \u2502\n                               \u25bc\n                 Optimization Objective (J(\u03b8))\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03b8* = argmax\u03b8 V(\u03b8) = argmax\u03b8 E\u03c0\u03b8[G\u2080]         \u2502\n       \u2502 Search in parameter space for best policy   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Two Families of Policy Optimization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Gradient-Free Methods     \u2502   Gradient-Based Methods   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                      \u2502\n                \u2502                                      \u25bc\n                \u2502                          Policy Gradient Methods\n                \u2502                                      \u2502\n                \u25bc                                      \u25bc\n   No gradient needed                     Uses \u2207\u03b8 log \u03c0\u03b8(a|s) * Return\n   \u2013 Hill Climbing                        \u2013 REINFORCE\n   \u2013 CEM, CMA                             \u2013 Actor-Critic\n   \u2013 Genetic Algorithms                   \u2013 Advantage Methods\n   \u2502                                      \u2502\n   \u2514\u2500\u2500\u2500\u2500 Flexible &amp; parallelizable        \u2514\u2500\u2500\u2500\u2500 Sample efficient\n                               \u2502\n                               \u25bc\n                   Policy Gradient Core Idea\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Increase probability of good actions          \u2502\n       \u2502 Decrease probability of poor actions          \u2502\n       \u2502 Gradient term: \u2207\u03b8 log \u03c0\u03b8(a|s)(score function) \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     REINFORCE Algorithm (MC)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Sample full episodes (Monte Carlo)            \u2502\n       \u2502 Compute return Gt at each time step           \u2502\n       \u2502 Update: \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8 log \u03c0\u03b8(a_t|s_t) * G_t    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Policy Parameterization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Softmax Policy (Discrete)  \u2502 Gaussian Policy(Continuous)\u2502\n       \u2502 \u03c0\u03b8(a|s) = exp(...)         \u2502 a ~ N(\u03bc(s), \u03c3\u00b2)            \u2502\n       \u2502 \u2207\u03b8 log \u03c0\u03b8 = \u03c6 - E\u03c6         \u2502 \u2207\u03b8 log \u03c0\u03b8 = (a-\u03bc)/\u03c3\u00b2 * \u03c6   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Final Outcome\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly (no need for Q or V tables)   \u2502\n       \u2502 Works naturally with stochastic &amp; continuous    \u2502\n       \u2502 Supports neural network policy parameterization \u2502\n       \u2502 Foundation of modern deep RL (PPO, A3C, DDPG)   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/6_pg2/","title":"6. Policy Gradient Variance Reduction and Actor-Critic","text":""},{"location":"reinforcement/6_pg2/#chapter-6-policy-gradient-variance-reduction-and-actor-critic","title":"Chapter 6: Policy Gradient Variance Reduction and Actor-Critic","text":"<p>In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy \\(\\pi_\\theta(a|s)\\) in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:</p> <ul> <li> <p>High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return \\(G_t\\) depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.</p> </li> <li> <p>Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.</p> </li> <li> <p>Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.</p> </li> <li> <p>Parameter Space vs Policy Space Mismatch: A small change in policy parameters \\(\\theta\\) does not always translate to a small change in the policy\u2019s behavior. For example, in a two-action policy with probability \\(\\pi_\\theta(a=1)=\\sigma(\\theta)\\) (sigmoid), a slight shift in \\(\\theta\\) can swing the action probabilities significantly if \\(\\theta\\) is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.</p> </li> </ul> <p>The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor\u2013critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.</p>"},{"location":"reinforcement/6_pg2/#policy-gradient-theorem-and-reinforce","title":"Policy Gradient Theorem and REINFORCE","text":"<p>n the previous chapter, we derived an expression for the gradient of the policy objective:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^{m} R(\\tau^{(i)})  \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)}), \\] <p>where each trajectory \\(\\tau^{(i)}\\) is generated by the current policy \\(\\pi_\\theta\\).</p> <p>This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:</p> <ul> <li>The return \\(R(\\tau)\\) depends on the entire trajectory.</li> <li>Different trajectories can have very different returns.</li> <li>Updates become noisy, unstable, and slow to converge.</li> </ul> <p>In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.</p> <p>Goal: Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.</p>"},{"location":"reinforcement/6_pg2/#reducing-variance-with-baselines","title":"Reducing Variance with Baselines","text":"<p>One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action\u2019s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return. Mathematically, we modify the gradient as follows:</p> \\[ \\nabla_\\theta \\mathbb{E}_\\tau [R] \\;=\\; \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; \\left(\\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\\right) \\right]. \\] <p>where \\(b(s_t)\\) is an arbitrary baseline that depends on the state \\(s_t\\) (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, \\(b(s_t)\\) represents a reference level for the return at state \\(s_t\\); the term \\((G_t - b(s_t))\\) is asking: did the action at \\(s_t\\) do better or worse than this baseline expectation?</p> <p>A particularly good choice for the baseline is the value function under the current policy, \\(b(s_t) \\approx V_{\\pi}(s_t)\\). This is the expected return from state \\(s_t\\) if we continue following the current policy. Using \\(V_{\\pi}(s_t)\\) as \\(b(s_t)\\) minimizes variance because it subtracts out the expected part of \\(G_t\\), leaving only the unexpected advantage of the action \\(a_t\\). Using a value function baseline leads to defining the advantage function:  where \\(Q(s_t,a_t)\\) is the expected return for taking action \\(a_t\\) in \\(s_t\\) and following the policy thereafter, and \\(V(s_t)\\) is the expected return from \\(s_t\\) on average. The advantage \\(A(s_t,a_t)\\) tells us how much better or worse the chosen action was compared to the policy\u2019s typical action at that state. If \\(A(s_t,a_t)\\) is positive, the action did better than expected; if negative, it did worse than expected. Using \\(A(s_t,a_t)\\) in the gradient update focuses learning on the deviations from usual outcomes.</p> <p>Benefits of Using a Baseline (Advantage): 1.  Variance Reduction: Subtracting \\(V(s_t)\\) removes the predictable part of the return, reducing the variability of the term \\((G_t - b(s_t))\\). The policy update then depends on advantage, which typically has lower variance than raw returns. 2.  Focused Learning: The update ignores outcomes that are \u201cas expected\u201d and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment. 3.  Unbiased Gradient: Because \\(b(s_t)\\) does not depend on the action, the expected value of \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, b(s_t)\\) is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance</p> <p>Using a baseline in practice usually means we need to estimate the value function \\(V_{\\pi}(s)\\) for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress \\(b(s)\\) toward the observed returns \\(G_t\\) to improve the baseline estimate.</p> <p>Algorithm: Policy Gradient with Baseline (Advantage Estimation)</p> <p>1: Initialize policy parameter \\(\\theta\\), baseline \\(b(s)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) Collect a set of trajectories by executing the current policy \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each trajectory \\(\\tau^{(i)}\\) and each timestep \\(t\\) do 5: \\(\\quad\\quad\\) Compute return: \\(\\quad\\quad\\quad G_t^{(i)} = \\sum_{t'=t}^{T-1} r_{t'}^{(i)}\\) 6: \\(\\quad\\quad\\) Compute advantage estimate: \\(\\quad\\quad\\quad \\hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})\\) 7: \\(\\quad\\) end for 8: \\(\\quad\\) Re-fit baseline by minimizing: \\(\\quad\\quad \\sum_i \\sum_t \\big(b(s_t^{(i)}) - G_t^{(i)}\\big)^2\\) 9: \\(\\quad\\) Update policy parameters using gradient estimate: \\(\\quad\\quad \\theta \\leftarrow \\theta + \\alpha \\sum_{i,t} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\, \\hat{A}_t^{(i)}\\) 10: \\(\\quad\\) (Plug into SGD or Adam optimizer) 11: end for  </p> <p>This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect (\\(b(s)=V_\\pi(s)\\)), then \\(\\hat{A}t = G_t - V\\pi(s_t)\\) is an estimate of the advantage \\(A(s_t,a_t)\\). Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.</p>"},{"location":"reinforcement/6_pg2/#actorcritic-methods","title":"Actor\u2013Critic Methods","text":"<p>Using a learned baseline brings us to the idea of actor\u2013critic algorithms. In the policy gradient with baseline above, the policy is the \"actor\" and the value function baseline is a \"critic\" that evaluates the actor\u2019s decisions. Actor\u2013critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.</p> <ul> <li>Actor: the policy \\(\\pi_\\theta(a|s)\\) that selects actions and is updated by gradient ascent.</li> <li>Critic: a value function \\(V_w(s)\\) (with parameters \\(w\\)) that estimates the return from state \\(s\\) under the current policy. The critic provides the baseline or advantage estimates used in the actor\u2019s update.</li> </ul> <p>Instead of waiting for full episode returns \\(G_t\\), an actor\u2013critic uses the critic\u2019s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic\u2019s current estimate of advantage \\(A(s_t,a_t)\\), and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.</p> <p>Actor update: The policy (actor) update is similar to before, but using the critic\u2019s advantage estimate \\(A_t\\) at time \\(t\\):  Here \\(A_t \\approx Q(s_t,a_t) - V(s_t)\\) is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive \\(A_t\\)) and away from actions that seem worse than expected.</p> <p>Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error \\(\\delta_t\\) to update \\(w\\):  which measures the discrepancy between the predicted value at \\(s_t\\) and the reward plus discounted value of the next state. The critic\u2019s parameters \\(w\\) are updated by a gradient step proportional to \\(\\delta_t \\nabla_w V_w(s_t)\\) (this is essentially a TD(0) update). In practice:  with \\(\\beta\\) a critic learning rate. This update pushes the critic\u2019s value estimate \\(V_w(s_t)\\) toward the observed reward plus the estimated value of \\(s_{t+1}\\). The TD error \\(\\delta_t\\) is also used as an advantage estimate for the actor: notice \\(\\delta_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\) serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.</p> <p>Why Actor\u2013Critic? Actor\u2013critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor\u2013critic algorithms tend to: - Learn faster (lower variance updates thanks to the critic\u2019s guidance) - Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends) - Naturally handle continuing (non-episodic) tasks via the critic\u2019s ongoing value estimates - Still allow stochastic policies and continuous actions (since the actor is explicit)</p> <p>However, actor\u2013critics introduce bias through the critic\u2019s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor\u2013critic; later algorithms address this).</p> <p>Advantage Estimation: In an actor\u2013critic, one often uses n-step returns or more generally \\(\\lambda\\)-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return \\(R^{(3)}t = r_t + \\gamma r{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V(s_{t+3})\\), and define \\(\\hat{A}_t = R^{(3)}_t - V(s_t)\\). Smaller \\(n\\) gives lower variance but more bias; larger \\(n\\) gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.</p> <p>In summary, the actor\u2013critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy\u2019s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.</p>"},{"location":"reinforcement/6_pg2/#limitations-of-vanilla-policy-gradient-and-trust-region-motivation","title":"Limitations of Vanilla Policy Gradient and Trust-Region Motivation","text":"<p>Despite using baselines and even actor\u2013critic methods, vanilla policy gradient algorithms (including basic actor\u2013critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:</p> <ul> <li> <p>On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy \\(\\pi_{\\text{old}}\\) to update the current policy \\(\\pi_{\\text{new}}\\) introduces bias, because the gradient formula assumes data comes from \\(\\pi_{\\text{new}}\\). In short, standard policy gradients waste a lot of data, making them sample-inefficient.</p> </li> <li> <p>Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios \\(r_t = \\frac{\\pi_{\\text{new}}(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}\\). This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if \\(\\pi_{\\text{new}}\\) differs significantly from \\(\\pi_{\\text{old}}\\). A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.</p> </li> <li> <p>Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile \u2013 even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not \u201ctoo large\u201d in terms of its impact on the policy\u2019s behavior.</p> </li> <li> <p>Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in \\(\\theta\\)-space. A small change in parameters can lead to a disproportionate change in the policy\u2019s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes \u2260 small policy changes, it\u2019s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.</p> </li> </ul> <p>These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update \u2013 in other words, to stay within a \u201ctrust region\u201d around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback\u2013Leibler divergence as a distance between \\(\\pi_{\\text{new}}\\) and \\(\\pi_{\\text{old}}\\)), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor\u2013critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.</p>"},{"location":"reinforcement/7_gae/","title":"7. Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":""},{"location":"reinforcement/7_gae/#chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo","title":"Chapter 7: Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":"<p>In the previous chapter, we improved the foundation of policy gradients by reducing variance (using baselines) and introducing actor\u2013critic methods. We also noted that unrestricted policy updates can be unstable and sample-inefficient. In this chapter, we present modern advances in policy optimization that build on those ideas to achieve much better performance in practice. We focus on two main developments: Generalized Advantage Estimation (GAE), which refines how we estimate advantages to balance bias and variance, and trust-region methods (specifically Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)) that ensure updates do not destabilize the policy. These techniques enable more sample-efficient, stable learning by reusing data safely and preventing large detrimental policy shifts.</p>"},{"location":"reinforcement/7_gae/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>Accurate and low-variance advantage estimates are crucial for effective policy gradient updates. Recall that the policy gradient update uses the term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\,A(s_t,a_t)\\) \u2013 if \\(A(s_t,a_t)\\) is noisy or biased, it can severely affect learning. Advantage can be estimated via: - Monte Carlo returns: \\(A_t = G_t - V(s_t)\\) using the full return \\(G_t\\) (summing all future rewards until episode end). This is an unbiased estimator of the true advantage, but it has very high variance because it includes all random future outcomes.</p> <ul> <li> <p>One-step TD returns: \\(A_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\), using the critic\u2019s bootstrapped estimate of the future. This one-step advantage (equivalently the TD error \\(\\delta_t\\)) has much lower variance (it relies on the learned value for the next state) but is biased by function approximation and by truncating the return after one step.</p> </li> <li> <p>n-Step returns:We can also use intermediate approaches, for example a 2-step return \\(R^{(2)}t = r_t + \\gamma r{t+1} + \\gamma^2 V(s_{t+2})\\) giving an advantage \\(\\hat{A}^{(2)}_t = R^{(2)}_t - V(s_t)\\). In general, an n-step advantage estimator can be written as:</p> \\[A^{t}(n) = \\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i+1} + \\gamma^{n} V(s_{t+n}) -V(s_{t})\\] <p>which blends \\(n\\) actual rewards with a bootstrap at time \\(t+n\\). Smaller \\(n\\) (like 1) means more bias (due to heavy reliance on \\(V\\)) but low variance; larger \\(n\\) (approaching the episode length) reduces bias but increases variance.</p> </li> </ul> <p>The pattern becomes clearer if we express these in terms of the TD error \\(\\delta_t\\) (the one-step advantage at \\(t\\)):</p> <p>  - For a 1-step return, \\(\\hat{A}^{(1)}_t = \\delta_t\\). - For a 2-step return, \\(\\hat{A}^{(2)}t = \\delta_t + \\gamma\\,\\delta\\). - For an \\(n\\)-step return, \\(\\hat{A}^{(n)}t = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + \\cdots + \\gamma^{n-1}\\delta_{t+n-1}\\)</p> <p>Each additional term \\(\\gamma^i \\delta_{t+i}\\) extends the return by one more step of real reward before bootstrapping, increasing bias a bit (since it assumes the later \\(\\delta\\) terms are based on an approximate \\(V\\)) but capturing more actual reward outcomes (reducing variance less).</p> <p>Generalized Advantage Estimation (GAE) takes this idea to its logical conclusion by forming a weighted sum of all n-step advantages, with exponentially decreasing weights. Instead of picking a fixed \\(n\\), GAE uses a parameter \\(0 \\le \\lambda \\le 1\\) to blend advantages of different lengths:</p> \\[\\hat{A}^{\\text{GAE}(\\gamma,\\lambda)}_t \\;=\\; (1-\\lambda)\\Big(\\hat{A}^{(1)}_t + \\lambda\\,\\hat{A}^{(2)}_t + \\lambda^2\\,\\hat{A}^{(3)}_t + \\cdots\\Big)\\] <p>This infinite series can be shown to simplify to a very convenient form:</p> \\[\\hat{A}_t^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i\\delta_{t+i}\\] <p>which is an exponentially-weighted sum of the future TD errors. In practice, this is implemented with a simple recursion running backward through each trajectory (since it\u2019s a sum of discounted TD errors).</p> <p>Key intuition: \\(\\lambda\\) controls the bias\u2013variance trade-off in advantage estimation:</p> <ul> <li> <p>\\(\\lambda = 0\\) uses only the one-step TD error: \\(\\hat{A}^{\\text{GAE}(0)}_t = \\delta_t\\). This is the lowest-variance, highest-bias estimator (similar to TD(0) advantage)[21].</p> </li> <li> <p>\\(\\lambda = 1\\) uses an infinitely long sum of un-discounted TD errors, which in theory equals the full Monte Carlo return advantage (since all bootstrapping is deferred to the end). This is unbiased (in the limit of exact \\(V\\)) but highest variance \u2013 essentially Monte Carlo estimation.</p> </li> <li> <p>Intermediate \\(0&lt;\\lambda&lt;1\\) gives a mixture. A typical choice is \\(\\lambda = 0.95\\) in many applications, which provides a good balance (mostly long-horizon returns with a bit of bootstrapping to damp variance).</p> </li> </ul> <p>GAE is not introducing a new kind of return; rather, it generalizes existing returns. It smoothly interpolates between TD and Monte Carlo methods. When \\(\\lambda\\) is low, GAE trusts the critic more (using more bootstrapped estimates); when \\(\\lambda\\) is high, GAE leans toward actual returns over many steps. In modern actor\u2013critic algorithms (including TRPO and PPO), GAE is used to compute the advantage for each state-action in a batch. A typical implementation for each iteration is:</p> <ol> <li>Collect trajectories using the current policy \\(\\pi_{\\theta}\\) (e.g. run \\(N\\) episodes or \\(T\\) time steps of experience).</li> <li>Compute state values \\(V(s_t)\\) for each state visited (using the current value function estimate).</li> <li>Compute TD residuals \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) for each time step.</li> <li> <p>Apply GAE formula: going from \\(t=T-1\\) down to \\(0\\), accumulate \\(\\hat{A}_t = \\delta_t + \\gamma \\lambda, \\hat{A}{t+1}\\), with \\(\\hat{A}_{T} = 0\\). This yields \\(\\hat{A}_t \\approx \\sum{i\\ge0} (\\gamma \\lambda)^i \\delta{t+i}\\).</p> </li> <li> <p>Use Advantages for Update: These \\(\\hat{A}_t\\) values serve as the advantage estimates in the policy gradient update. Simultaneously, you can compute proxy returns for the critic by adding \\(\\hat{A}_t\\) to the baseline \\(V(s_t)\\) (i.e. \\(\\hat{R}_t = \\hat{A}_t + V(s_t)\\), an estimate of the actual return) and use those to update the value function parameters.</p> </li> </ol> <p>The result of GAE is a much smoother, lower-variance advantage signal for the actor, without introducing too much bias. Empirically, this greatly stabilizes training: the policy doesn\u2019t overreact to single high-return episodes, and it doesn\u2019t ignore long-term outcomes either. GAE essentially bridges the gap between the high-variance Monte Carlo world of Chapter 5 and the low-variance TD world of Chapter 3\u20134, and it has become a standard component in virtually all modern policy optimization algorithms.</p>"},{"location":"reinforcement/7_gae/#kl-divergence-constraints-and-surrogate-objectives","title":"KL Divergence Constraints and Surrogate Objectives","text":"<p>We now turn to the question of stable policy updates. As discussed, a major issue with vanilla policy gradient is that a single update can accidentally push the policy into a disastrous region (because the gradient is computed at the current policy but we might step too far). To make updates safer, we want to constrain how much the policy changes at each step. A natural way to measure change between the old policy \\(\\pi_{\\text{old}}\\) and a new policy \\(\\pi_{\\text{new}}\\) is to use the Kullback\u2013Leibler (KL) divergence. For example, we can require:</p> \\[\\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}}(\\cdot \\mid s)\\,\\|\\,\\pi_{\\text{old}}(\\cdot \\mid s)\\bigr) \\right] \\le \\delta\\] <p>for some small \\(\\delta\\). This means that on average over states (under the old policy\u2019s state distribution \\(d_{\\pi_{\\text{old}}}\\)), the new policy\u2019s probability distribution is not too far from the old policy\u2019s distribution. A small KL divergence ensures the policies behave similarly, limiting the \u201csurprise\u201d from one update.</p> <p>But how do we optimize under such a constraint? We need an objective function that tells us whether \\(\\pi_{\\text{new}}\\) is better than \\(\\pi_{\\text{old}}\\). Fortunately, theory provides a useful tool: a surrogate objective that approximates the change in performance if the policy change is small. One version, derived from the policy performance difference lemma and monotonic improvement theorem, is:</p> \\[L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\text{old}}} \\left[ \\frac{\\pi_{\\text{new}}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A_{\\pi_{\\text{old}}}(s,a) \\right]\\] <p>This is an objective functional\u2014it evaluates the new policy using samples from the old policy, weighting rewards by the importance ratio \\(r(s,a) = \\pi_{\\text{new}}(a|s)/\\pi_{\\text{old}}(a|s)\\). Intuitively, \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) is asking: if the old policy visited state \\(s\\) and took action \\(a\\), how good would that decision be under the new policy\u2019s probabilities? Actions that the new policy wants to do more of (\\(r &gt; 1\\)) will contribute their advantage (good or bad) proportionally more.</p> <p>Critically, one can show that if \\(\\pi_{\\text{new}}\\) is very close to \\(\\pi_{\\text{old}}\\) (in KL terms), then improving this surrogate \\(L\\) guarantees an improvement in the true return \\(J(\\pi)\\). Specifically, there is a bound such that:</p> \\[J(\\pi_{\\text{new}}) \\ge J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - C \\, \\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}} \\,\\|\\, \\pi_{\\text{old}}\\bigr)[s] \\right]\\] <p>for some constant \\(C\\) related to horizon and policy support. When the KL divergence is small, the last term is second-order (negligible), so roughly we get \\(J(\\pi_{\\text{new}}) \\gtrapprox J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\). In other words, maximizing \\(L\\) while keeping KL small ensures monotonic improvement: each update should not reduce true performance.</p> <p>This insight leads directly to a constrained optimization formulation for safe policy updates:</p> <ul> <li>Objective: Maximize the surrogate \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) (i.e. maximize expected advantage-weighted probability ratios).</li> <li>Constraint: Limit the policy divergence via \\(D_{\\mathrm{KL}}(\\pi_{\\text{new}}\\Vert \\pi_{\\text{old}}) \\le \\delta\\) (for some small \\(\\delta\\)). Algorithms that implement this idea are called trust-region methods, because they optimize the policy within a trust region of the old policy. Next, we discuss two prominent algorithms: TRPO, which tackles the constrained problem directly (with some approximations), and PPO, which simplifies it into an easier unconstrained loss function.</li> </ul>"},{"location":"reinforcement/7_gae/#trust-region-policy-optimization","title":"Trust Region Policy Optimization","text":"<p>Trust Region Policy Optimization (TRPO) is a seminal algorithm that explicitly embodies the constrained update approach. TRPO chooses a new policy by approximately solving:</p> \\[\\max_{\\theta_{\\text{new}}} \\; L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}}) \\quad \\text{s.t.} \\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\theta_{\\text{new}}} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\bigr) \\right] \\le \\delta\\] <p>where \\(L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}})\\) is the surrogate objective defined above, and \\(\\delta\\) is a small trust-region threshold. In practice, solving this exactly is difficult due to the infinite-dimensional policy space. TRPO makes it tractable by using a few key ideas:</p> <ul> <li> <p>Approximating the constraint via a quadratic expansion of the KL divergence (which yields a Fisher Information Matrix). This turns the problem into something like a second-order update (a natural gradient step). In fact, TRPO\u2019s solution can be shown to correspond to a natural gradient ascent:</p> <p>\\(\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^T F^{-1} g}}\\; F^{-1} g\\)$</p> <p>where \\(g = \\nabla_\\theta L\\) and \\(F\\) is the Fisher matrix. This ensures the KL constraint is satisfied approximately, and is equivalent to scaling the gradient by \\(F^{-1}\\). In simpler terms, TRPO updates \\(\\theta\\) in a direction that accounts for the curvature of the policy space, so that the change in policy (KL) is proportional to the step size.</p> </li> <li> <p>Using a line search to ensure the new policy actually improves \\(J(\\pi)\\). TRPO will back off the step size if the updated policy violates the constraint or fails to achieve a performance improvement. This safeguard maintains the monotonic improvement guarantee in practice.</p> </li> </ul> <p>A simplified outline of TRPO is:</p> <ol> <li>Collect trajectories with the current policy \\(\\pi_{\\theta_{\\text{old}}}\\).</li> <li>Estimate advantages \\(\\hat{A}_t\\) for each time step (using GAE or another method for high-quality advantage estimates).</li> <li>Compute surrogate objective \\(L(\\theta) = \\mathbb{E}[r_t(\\theta), \\hat{A}t]\\) where \\(r_t(\\theta) = \\frac{\\pi{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\).</li> <li>Approximate KL constraint: Compute the policy gradient \\(\\nabla_\\theta L\\) and the Fisher matrix \\(F\\) (via sample-based estimation of the Hessian of the KL divergence). Solve for the update direction \\(p \\approx F^{-1} \\nabla_\\theta L\\) (e.g. using conjugate gradient).</li> <li>Line search: Scale and apply the update step \\(\\theta \\leftarrow \\theta + p\\) gradually, checking the KL and improvement. Stop when the KL constraint or improvement criterion is satisfied.</li> </ol> <p>TRPO\u2019s updates are therefore conservative by design \u2013 they will only take as large a step as can be trusted not to degrade performance. TRPO was influential because it demonstrated much more stable and reliable training on complex continuous control tasks than vanilla policy gradient.</p> <p>Strengths and Weaknesses of TRPO: TRPO offers a theoretical guarantee of non-destructive updates \u2013 under certain assumptions, each iteration is guaranteed to improve or at least not decrease performance. It uses a natural gradient approach that respects the geometry of policy space, which is more effective than an arbitrary gradient in parameter space. However, TRPO comes at a cost: it requires calculating second-order information (the Fisher matrix), and implementing the conjugate gradient solver and line search adds complexity. The algorithm can be slower per iteration and is more complex to code and tune. In practice, TRPO, while effective, proved somewhat cumbersome for large-scale problems due to these complexities.</p>"},{"location":"reinforcement/7_gae/#proximal-policy-optimization","title":"Proximal Policy Optimization","text":"<p>Proximal Policy Optimization (PPO) was introduced as a simpler, more user-friendly variant of TRPO that achieves similar results with only first-order optimization. The core idea of PPO is to keep the spirit of trust-region updates (don\u2019t move the policy too far in one go) but implement it via a relaxed objective that can be optimized with standard stochastic gradient descent. There are two main variants of PPO:</p>"},{"location":"reinforcement/7_gae/#kl-penalty-objective","title":"KL-Penalty Objective:","text":"<p>One version of PPO adds the KL-divergence as a penalty to the objective rather than a hard constraint. The objective becomes:</p> \\[J_{\\text{PPO-KL}}(\\theta) = \\mathbb{E}\\!\\left[ r_t(\\theta)\\, \\hat{A}^t \\right] - \\beta \\,\\mathbb{E}\\!\\left[ D_{\\mathrm{KL}}\\!\\left(\\pi_{\\theta} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\right) \\right]\\] <p>where \\(\\beta\\) is a coefficient determining how strongly to penalize deviation from the old policy. If the KL divergence in an update becomes too large, \\(\\beta\\) can be adjusted (increased) to enforce smaller steps in subsequent updates. This approach maintains a soft notion of a trust region.</p>"},{"location":"reinforcement/7_gae/#algorithm-ppo-with-kl-penalty","title":"Algorithm (PPO with KL Penalty)","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), initial KL penalty \\(\\beta_0\\), target KL-divergence \\(\\delta\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^t_k\\) using any advantage estimation algorithm 5: \\(\\quad\\) Compute policy update by approximately solving \\(\\quad\\quad\\) \\(\\theta_{k+1} = \\arg\\max_\\theta \\; L_{\\theta_k}(\\theta) - \\beta_k \\hat{D}_{KL}(\\theta \\,\\|\\, \\theta_k)\\) 6: \\(\\quad\\) Implement this optimization with \\(K\\) steps of minibatch SGD (e.g., Adam) 7: \\(\\quad\\) Measure actual KL: \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k)\\) 8: \\(\\quad\\) if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\ge 1.5\\delta\\) then 9: \\(\\quad\\quad\\) Increase penalty: \\(\\beta_{k+1} = 2\\beta_k\\) 10: \\(\\quad\\) else if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\le \\delta/1.5\\) then 11: \\(\\quad\\quad\\) Decrease penalty: \\(\\beta_{k+1} = \\beta_k/2\\) 12: \\(\\quad\\) end if 13: end for  </p>"},{"location":"reinforcement/7_gae/#clipped-surrogate-objective-ppo-clip","title":"Clipped Surrogate Objective (PPO-Clip):","text":"<p>The more popular variant of PPO uses a clipped surrogate objective to restrict policy updates:</p> \\[L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t}\\!\\left[ \\min\\!\\Big( r_t(\\theta)\\,\\hat{A}^t,\\; \\text{clip}\\!\\big(r_t(\\theta),\\, 1-\\epsilon,\\, 1+\\epsilon\\big)\\,\\hat{A}^t \\Big) \\right]\\] <p>where \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\) as before, and \\(\\epsilon\\) is a small hyperparameter (e.g. 0.1 or 0.2) that defines the clipping range. This objective says: if the new policy\u2019s probability ratio \\(r_t(\\theta)\\) stays within \\([1-\\epsilon,\\,1+\\epsilon]\\), we use the normal surrogate \\(r_t \\hat{A}_t\\). But if \\(r_t\\) tries to go outside this range (meaning the policy probability for an action has changed dramatically), we clip \\(r_t\\) to either \\(1+\\epsilon\\) or \\(1-\\epsilon\\) before multiplying by \\(\\hat{A}_t\\). Effectively, the advantage contribution is capped once the policy deviates too much from the old policy.</p> <p>The clipped objective is not exactly the original constrained problem, but it serves a similar purpose: it removes the incentive for the optimizer to push \\(r_t\\) outside of \\([1-\\epsilon,1+\\epsilon]\\). If increasing \\(|\\theta|\\) further doesn\u2019t increase the objective (because the min() will select the clipped term), then overly large policy changes are discouraged.</p> <p>Why Clipping Works: Clipping is a simple heuristic, but it has proven extremely effective:</p> <ul> <li> <p>It enforces a soft trust region by preventing extreme updates for any single state-action probability. The policy can still change, but not so much that any one probability ratio blows up.</p> </li> <li> <p>It avoids the complexity of solving a constrained optimization or computing second-order derivatives \u2013 we can just do standard SGD on \\(L^{CLIP}(\\theta)\\).</p> </li> <li> <p>It keeps importance sampling ratios near 1, which means the algorithm can safely perform multiple epochs of updates on the same batch of data without the estimates drifting too far. This directly improves sample efficiency (unlike vanilla policy gradient, PPO typically updates each batch for several epochs).</p> </li> </ul>"},{"location":"reinforcement/7_gae/#ppo-clipped-algorithm","title":"PPO (Clipped) Algorithm","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), clipping threshold \\(\\epsilon\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect a set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^{\\,t}_k\\) using any advantage estimation algorithm (e.g., GAE) 5: \\(\\quad\\) Define the clipped surrogate objective \\(\\quad\\quad\\)  6: \\(\\quad\\) Update policy parameters with several epochs of minibatch SGD to approximately maximize \\(\\mathcal{L}^{\\text{CLIP}}_{\\theta_k}(\\theta)\\) 7: \\(\\quad\\) Set \\(\\theta_{k+1}\\) to the resulting parameters 8: end for  </p> <p>In practice, PPO with clipping has become one of the most widely used RL algorithms because it strikes a good balance between performance and simplicity. It is relatively easy to implement (compared to TRPO) and has been found to be robust across many tasks and hyperparameters. While it doesn\u2019t guarantee monotonic improvement in theory, in practice it achieves stable training behavior very similar to TRPO.</p> <p>In modern practice, PPO is the dominant choice for policy optimization in deep RL, due to its relative simplicity and strong performance across many environments. TRPO is still important conceptually (and sometimes used in scenarios where theoretical guarantees are desired), but PPO\u2019s convenience usually wins out.</p>"},{"location":"reinforcement/7_gae/#putting-it-together-sample-efficiency-stability-and-monotonic-improvement","title":"Putting It Together: Sample Efficiency, Stability, and Monotonic Improvement","text":"<p>The advances covered in this chapter are often used together in state-of-the-art algorithms:</p> <ul> <li> <p>Generalized Advantage Estimation (GAE) provides high-quality advantage estimates that significantly reduce variance without too much bias. This means we can get away with smaller batch sizes or fewer episodes to get a good learning signal \u2013 improving sample efficiency.</p> </li> <li> <p>Trust-region update rules (TRPO/PPO) ensure that each policy update is safe and stable \u2013 the policy doesn\u2019t change erratically, preventing the kind of catastrophic drops in reward that naive policy gradients can suffer. By keeping policy changes small (via KL constraints or clipping), these methods enable multiple updates on the same batch of data (improving data efficiency) and maintain policy monotonicity, i.e. each update is expected to improve or at least not significantly degrade performance.</p> </li> <li> <p>In practice, an algorithm like PPO with GAE is an actor\u2013critic method that uses all these ideas: an actor policy updated with a clipped surrogate objective (making updates stable), a critic to approximate \\(V(s)\\) (enabling advantage estimation), GAE to compute advantages (trading off bias/variance), and typically multiple gradient epochs per batch to squeeze more learning out of each sample. This combination has proven remarkably successful in domains from simulated control tasks to games.</p> </li> </ul> <p>By building on the foundational policy gradient framework and addressing its shortcomings, GAE and trust-region approaches have made deep reinforcement learning much more practical and reliable. They illustrate how theoretical insights (performance bounds, policy geometry) and practical tricks (advantage normalization, clipping) come together to yield algorithms that can solve challenging RL problems while using reasonable amounts of training data and maintaining stability throughout learning. Each component \u2013 be it advantage estimation or constrained updates \u2013 plays a role in ensuring that learning is as efficient, stable, and monotonic as possible. Together, they represent the state-of-the-art toolkit for policy optimization in reinforcement learning.</p> Method Key Idea Pros Cons REINFORCE MC return-based policy gradient Simple, unbiased Very high variance Actor\u2013Critic TD baseline value function More sample-efficient Requires critic Advantage Actor\u2013Critic Uses \\(A(s,a)\\) for updates Best bias\u2013variance trade Needs accurate value est. TRPO Trust-region with KL constraint Strong theory, stable Complex, second-order PPO Clipped/penalized surrogate objective Simple, stable, popular Heuristic, tuning needed"},{"location":"reinforcement/7_gae/#mental-map","title":"Mental Map","text":"<pre><code>                  Advanced Policy Gradient Methods\n     Goal: Fix limitations of vanilla PG (variance, stability, KL control)\n                               \u2502\n                               \u25bc\n             Core Challenges in Policy Gradient Methods\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 High variance (MC returns)                             \u2502\n       \u2502 Poor sample efficiency (on-policy only)                \u2502\n       \u2502 Sensitive to step size \u2192 catastrophic policy collapse  \u2502\n       \u2502 Small \u03b8 change \u2260 small policy change                   \u2502\n       \u2502 Reusing old data is unstable                           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Variance Reduction (Baselines)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Introduce baseline b(s) \u2192 subtract expectation         \u2502\n       \u2502 Keeps estimator unbiased                               \u2502\n       \u2502 Good choice: b(s)= V(s) \u2192 yields Advantage A(s,a)      \u2502\n       \u2502 Update based on: how much action outperformed expected \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                       Advantage Function A(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 A(s,a) = Q(s,a) \u2013 V(s)                                 \u2502\n       \u2502 Measures how much BETTER the action was vs average     \u2502\n       \u2502 Positive \u2192 increase \u03c0\u03b8(a|s); Negative \u2192 decrease it    \u2502\n       \u2502 Major variance reduction \u2013 foundation of Actor\u2013Critic  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Actor\u2013Critic Framework\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Actor: policy \u03c0\u03b8(a|s)                                  \u2502\n       \u2502 Critic: value function V(s;w) estimates baseline       \u2502\n       \u2502 TD error \u03b4t reduces variance (bootstrapping)           \u2502\n       \u2502 Faster, more sample-efficient than REINFORCE           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Target Estimation for the Critic\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Monte Carlo (\u221e-step)       \u2502  TD (1-step)               \u2502\n       \u2502 + Unbiased                 \u2502  + Low variance            \u2502\n       \u2502 \u2013 High variance            \u2502  \u2013 Biased                  \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502 n-Step Returns: Blend of TD and MC                      \u2502\n       \u2502 Control bias\u2013variance by choosing n                     \u2502\n       \u2502 Larger n \u2192 MC-like; smaller n \u2192 TD-like                 \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n             Fundamental Problems with Vanilla Policy Gradient\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Uses each batch for ONE gradient step (on-policy)      \u2502\n       \u2502 Step size is unstable \u2192 huge performance collapse      \u2502\n       \u2502 Small changes in \u03b8 \u2192 large unintended policy changes   \u2502\n       \u2502 Need mechanism to limit POLICY CHANGE, not \u03b8 change    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Safe Policy Improvement Theory \u2192 TRPO &amp; PPO\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy Performance Difference Lemma                    \u2502\n       \u2502   J(\u03c0') \u2212 J(\u03c0) = E\u03c0' [A\u03c0(s,a)]                         \u2502\n       \u2502 KL Divergence as policy distance metric                \u2502\n       \u2502   D_KL(\u03c0'||\u03c0) small \u2192 safe update                      \u2502\n       \u2502 Monotonic Improvement Bound                            \u2502\n       \u2502   Lower bound on J(\u03c0') using surrogate loss L\u03c0(\u03c0')     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                   Surrogate Objective for Safe Updates\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 L\u03c0(\u03c0') = E[ (\u03c0'(a|s)/\u03c0(a|s)) * A\u03c0(s,a) ]               \u2502\n       \u2502 Importance sampling + KL regularization                \u2502\n       \u2502 Foundation of Trust-Region Policy Optimization (TRPO)  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Proximal Policy Optimization (PPO) \u2013 Key Ideas\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 PPO-KL Penalty             \u2502 PPO-Clipped Objective      \u2502\n       \u2502 Adds \u03b2\u00b7KL to loss          \u2502 Clips ratio r_t(\u03b8) to      \u2502\n       \u2502 Adjust \u03b2 adaptively        \u2502 [1\u2212\u03b5, 1+\u03b5] to prevent      \u2502\n       \u2502 Prevents large updates     \u2502 destructive policy jumps   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         PPO Algorithm Summary\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 1. Collect trajectories from old policy                \u2502\n       \u2502 2. Estimate advantages A\u0302_t (GAE, TD, etc.)            \u2502\n       \u2502 3. Optimize clipped surrogate for many epochs          \u2502\n       \u2502 4. Update parameters safely                            \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                          Final Outcome (Chapter 6)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Stable and efficient policy optimization               \u2502\n       \u2502 Reuse data safely across multiple updates              \u2502\n       \u2502 Avoid catastrophic policy collapse                     \u2502\n       \u2502 Foundation of modern deep RL algorithms                \u2502\n       \u2502 (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/8_imitation_learning/","title":"8. Imitation Learning","text":""},{"location":"reinforcement/8_imitation_learning/#chapter-8-imitation-learning","title":"Chapter 8: Imitation Learning","text":"<p>In previous chapters, we focused on reinforcement learning with explicit reward signals guiding the agent's behavior. We assumed that a well-defined reward function \\(R(s,a)\\) was provided as part of the MDP, and the agent\u2019s goal was to learn a policy that maximizes cumulative reward. But what if specifying the reward is difficult or the agent cannot safely explore to learn from reward? Imitation Learning (IL) addresses these scenarios by leveraging expert demonstrations instead of explicit rewards.</p> <p>Imitation Learning allows an agent to learn how to act by mimicking an expert\u2019s behavior, rather than by maximizing a hand-crafted reward.</p>"},{"location":"reinforcement/8_imitation_learning/#motivation-the-case-for-learning-from-demonstrations","title":"Motivation: The Case for Learning from Demonstrations","text":"<p>Designing a reward function that truly captures the desired behavior can be extremely challenging. A misspecified reward can lead to unintended behaviors (reward hacking) or require exhaustive tuning. Even with a good reward, some environments present sparse rewards (e.g. only a success/failure signal at the very end of an episode) \u2013 making pure trial-and-error learning inefficient. In other cases, unsafe exploration is a concern: letting an agent freely explore (as classic RL would) could be dangerous or costly (imagine a self-driving car learning by crashing to discover that crashing is bad).</p> <p>However, in many of these settings expert behavior is available: we might have logs of human drivers driving safely, or demonstrations of a robot performing the task. Imitation Learning leverages this data. Instead of specifying what to do via a reward function, we show the agent how to do it via example trajectories. The agent's objective is then to imitate the expert as closely as possible.</p> <p>This paradigm contrasts with reward-based RL in key ways:</p> <ul> <li> <p>Reward-Based RL: The agent explores and learns by trial-and-error, guided by a numeric reward signal for feedback. It requires careful reward design and often extensive exploration.</p> </li> <li> <p>Imitation Learning: The agent learns from demonstrations of the desired behavior, treating the expert\u2019s actions as ground truth. No explicit reward is needed to train; learning is driven by matching the expert's behavior.</p> </li> </ul> <p>By learning from an expert, IL can produce competent policies much faster and safer in these scenarios. It essentially sidesteps the credit assignment problem of RL (because the \"right\" action is directly provided by the expert) and avoids dangerous exploration. In domains like autonomous driving, robotics, or any task where a human can demonstrate the skill, IL offers a powerful shortcut to get an agent up to a reasonable performance.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-problem-setup","title":"Imitation Learning Problem Setup","text":"<p>Formally, we can describe the imitation learning scenario using the same environment structure as an MDP \\((S, A, P, R, \\gamma)\\) except that the reward function \\(R\\) is unknown or not used. The agent still has a state space \\(S\\), an action space \\(A\\), and the environment transition dynamics \\(P(s' \\mid s, a)\\). What we do have, instead of \\(R\\), is access to expert demonstrations. An expert (which could be a human or a pre-trained optimal agent) provides example trajectories:</p> \\[ \\tau_E = (s_0, a_0, s_1, a_1, \\dots , s_T) \\] <p>collected by following the expert\u2019s policy \\(\\pi_E\\) in the environment. We may have a dataset \\(D\\) of these expert trajectories (or simply a set of state-action pairs drawn from expert behavior). The key point is that in IL, the agent does not receive numeric rewards from the environment. Instead, success is measured by how well the agent\u2019s behavior matches the expert\u2019s behavior.</p> <p>The goal of imitation learning can be stated as: find a policy \\(\\pi\\) for the agent that reproduces the expert's behavior (and ideally, achieves similar performance on the task). If the expert is optimal or highly skilled, we hope \\(\\pi\\) will achieve near-optimal results as well. This is an alternative path to finding a good policy without ever specifying a reward function explicitly or performing unguided exploration.</p> <p>(If we imagine there was some true but unknown reward \\(R\\) the expert is optimizing, then ideally \\(\\pi\\) should perform nearly as well as \\(\\pi_E\\) on that reward. IL attempts to reach that outcome via demonstrations rather than explicit reward feedback.)</p>"},{"location":"reinforcement/8_imitation_learning/#3-behavioral-cloning-learning-by-supervised-imitation","title":"3. Behavioral Cloning: Learning by Supervised Imitation","text":"<p>The most direct approach to imitation learning is Behavioral Cloning. Behavioral cloning treats imitation as a pure supervised learning problem: we train a policy to map states to the expert\u2019s actions, using the expert demonstrations as labeled examples. In essence, the agent \"clones\" the expert's behavior by learning to predict the expert's action in any given state.</p> <p>BC: Learn state to action mappings using expert demonstrations.</p> <p>In practice, we parameterize a policy \\(\\pi_\\theta(a\\mid s)\\) (e.g. a neural network with parameters \\(\\theta\\)) and adjust \\(\\theta\\) so that \\(\\pi_\\theta(\\cdot\\mid s)\\) is as close as possible to the expert\u2019s action choice in state \\(s\\). We define a loss function on the dataset of state-action pairs. For example:</p> <ul> <li>Discrete actions: Use cross-entropy (negative log-likelihood) of the expert\u2019s action.</li> </ul> \\[L(\\theta) = - \\mathbb{E}_{(s,a)\\sim D}\\left[ \\log \\pi_{\\theta}(a \\mid s) \\right]\\] <ul> <li>Continuous actions: Use mean squared error (regression loss).</li> </ul> \\[L(\\theta) = \\mathbb{E}_{(s,a)\\sim D} \\left[ \\left( \\pi_\\theta(s) - a \\right)^2 \\right]\\] <p>Minimizing these losses drives the policy to imitate the expert decisions on the training set.</p> <p>Training a behavioral cloning agent typically involves three steps:</p> <ol> <li> <p>Collect demonstrations: Gather a dataset \\(D = {(s_i, a_i)}\\) of expert state-action examples by observing the expert \\(\\pi_E\\) in the environment.</p> </li> <li> <p>Supervised learning on \\((s, a)\\) pairs: Choose a policy representation for \\(\\pi_\\theta\\) and use the collected data to adjust \\(\\theta\\). For each example \\((s_i, a_i)\\), we update \\(\\pi_\\theta\\) to reduce the error between its prediction \\(\\pi_\\theta(s_i)\\) and the expert\u2019s action \\(a_i\\). (For instance, if actions are discrete, we increase the probability \\(\\pi_\\theta(a_i \\mid s_i)\\) for the expert\u2019s action; if continuous, we move \\(\\pi_\\theta(s_i)\\) closer to \\(a_i\\) in value.)</p> </li> <li> <p>Deployment: Once the policy is trained (approximating \\(\\pi_E\\)), we fix \\(\\theta\\). The agent then acts autonomously: at each state \\(s\\), it outputs \\(a = \\pi_\\theta(s)\\) as its action. Ideally, this learned policy will behave similarly to the expert in the environment.</p> </li> </ol> <p>If the expert demonstrations are representative of the situations the agent will face, behavioral cloning can yield a policy that mimics the expert\u2019s behavior effectively. BC has some clear advantages:</p> <ul> <li> <p>Simplicity: It reduces policy learning to standard supervised learning, for which many stable algorithms and optimizations exist.</p> </li> <li> <p>Offline training: The model can be trained entirely from pre-recorded expert data, without requiring interactive environment feedback. This makes it data-efficient in terms of environment interactions.</p> </li> <li> <p>Safety: No random exploration is needed. The agent never tries highly suboptimal actions during training, since it always learns from demonstrated good behavior (critical in safety-sensitive domains).</p> </li> </ul> <p>However, purely copying the expert also comes with important limitations.</p>"},{"location":"reinforcement/8_imitation_learning/#covariate-shift-and-compounding-errors","title":"Covariate Shift and Compounding Errors","text":"<p>The main problem with behavioral cloning is that the training distribution of states can differ from the test distribution when the agent actually runs. During training, \\(\\pi_\\theta\\) is only exposed to states that the expert visited. But once the agent is deployed, if it ever deviates even slightly from the expert\u2019s trajectory, it may enter states not seen in the training data. In those unfamiliar states, the policy\u2019s predictions may be unreliable, leading to errors that cause it to drift further from expert-like behavior.</p> <p>A small mistake can snowball: once the agent strays from what the expert would do, it encounters novel situations where its learned policy might be very poor. One error leads to another, and the agent can cascade into failure because it was never taught how to recover.</p> <p>This phenomenon is known as covariate shift or distributional shift. The learner is trained on the state distribution induced by the expert policy \\(\\pi_E\\), but it is testing on the state distribution induced by its own policy \\(\\pi_\\theta\\). Unless \\(\\pi_\\theta\\) is perfect, these distributions will diverge over time, and the divergence can grow unchecked. In other words, the agent might handle situations similar to the expert's trajectories well, but if it finds itself in a situation the expert never encountered (often a result of a prior mistake), it has no guidance on what to do and can rapidly veer off course. This is often illustrated by the example of a self-driving car learned by BC: if it slightly misjudges a turn and drifts, it may end up in a part of the road it never saw during training, leading to more errors (compounding until possibly a crash).</p> <p>Another limitation is that BC does not inherently guarantee optimality or improvement beyond the expert: the policy is only as good as the demonstration data. If the expert is suboptimal or the dataset doesn\u2019t cover certain scenarios, the cloned policy will reflect those shortcomings and cannot improve by itself (since it has no feedback signal like reward to further refine its behavior). In reinforcement learning terms, BC has no notion of feedback for success or failure; it merely apes the expert, so it cannot discover better strategies or correct mistakes outside the expert's shadow.</p> <p>Researchers have developed strategies to mitigate the covariate shift problem. One approach is Dataset Aggregation (DAgger), which is an iterative algorithm: after training an initial policy via BC, let the policy interact with the environment and observe where it makes mistakes or visits unseen states; then have the expert provide the correct actions for those states, add these state-action pairs to the training set, and retrain the policy. By repeating this process, the policy\u2019s training distribution is gradually brought closer to the distribution it will encounter when it controls the agent. DAgger can significantly reduce compounding errors, but it requires ongoing access to an expert for feedback during training.</p> <p>In summary, behavioral cloning is a powerful first step for imitation learning\u2014it's straightforward and avoids many challenges of pure RL. But one must be mindful of its limitations: a blindly cloned policy can fail catastrophically when it encounters situations outside the expert\u2019s experience. This motivates more sophisticated imitation learning methods that incorporate the dynamics of the environment and attempt to infer the intent behind expert actions, rather than just copying them. We turn to those next.</p>"},{"location":"reinforcement/8_imitation_learning/#inverse-reinforcement-learning-learning-the-why","title":"Inverse Reinforcement Learning: Learning the \"Why\"","text":"<p>Behavioral cloning directly learns what to do (mapping states to actions) but does not capture why those actions are desirable. Inverse Reinforcement Learning (IRL) instead asks: Given expert behavior, what underlying reward function \\(R\\) could explain it? In other words, IRL attempts to reverse-engineer the expert's objectives from its observed behavior.</p> <p>In IRL, we assume that the expert \\(\\pi_E\\) is (approximately) optimal for some unknown reward function \\(R^*\\). The goal is to infer a reward function \\(\\hat{R}\\) such that, if an agent were to optimize \\(\\hat{R}\\), it would reproduce the expert\u2019s behavior. Formally, we want \\(\\pi_E\\) to be the optimal policy under the learned reward:</p> \\[\\pi_E = \\arg\\max_{\\pi} \\, V_R^{\\pi}\\] <p>where \\(V^{\\pi}_{\\hat{R}}\\) is the expected return of policy \\(\\pi\\) under the reward function \\(\\hat{R}\\). In words, the expert should have higher cumulative reward (according to \\(\\hat{R}\\)) than any other policy. If we can find such an \\(\\hat{R}\\), we have explained the expert\u2019s behavior in terms of incentives.</p> <p>Intuition: IRL flips the reinforcement learning problem on its head. Rather than starting with a reward and finding a policy, we start with a policy (the expert's) and try to find a reward that this policy optimizes. It's like observing an expert driver and deducing that they must be implicitly trading off goals like \"reach the destination quickly\" and \"avoid collisions\" because their driving balances speed and safety.</p> <p>One challenge is that IRL is inherently an under-defined (ill-posed) problem: many possible reward functions might make \\(\\pi_E\\) appear optimal. To resolve this ambiguity, IRL algorithms introduce additional criteria or regularization. For example, they might prefer the simplest reward function that explains the behavior, or in the case of maximum entropy IRL, prefer a reward that leads to the most random (maximally entropic) policy among those that match the expert's behavior \u2013 this avoids overly narrow explanations and spreads probability over possible behaviors unless forced by data.</p> <p>Once a candidate reward function \\(\\hat{R}(s,a)\\) is learned through IRL, the process typically continues as follows: we plug \\(\\hat{R}\\) back into the environment and solve a forward RL problem (using any suitable algorithm from earlier chapters) to obtain a policy \\(\\pi_{\\hat{R}}\\) that maximizes this recovered reward. Ideally, \\(\\pi_{\\hat{R}}\\) will then behave similarly to the expert's policy \\(\\pi_E\\) (since \\(\\hat{R}\\) was chosen to explain \\(\\pi_E\\)). The end result is an agent that not only imitates the expert, but also has an explicit reward model of the task it is performing.</p> <p>IRL is usually more complex and computationally expensive than behavioral cloning, because it often involves a nested loop: for each candidate reward function, the algorithm may need to perform an inner optimization (solving an MDP) to evaluate how well that reward explains the expert. However, IRL provides several potential benefits:</p> <ul> <li> <p>It yields a reward function, which is a portable definition of the task. This inferred reward can then be reused: for example, to train new agents from scratch, to evaluate different policies, or to modify the task (by tweaking the reward) in a principled way.</p> </li> <li> <p>It can generalize better to new situations. If the environment changes in dynamics or constraints, having \\(\\hat{R}\\) allows us to re-optimize and find a new optimal policy for the new conditions. A policy learned by pure BC might not adapt well beyond the situations it was shown, whereas a reward captures the goal and can be re-optimized.</p> </li> <li> <p>It may allow the agent to exceed the demonstrator\u2019s performance. Since IRL ultimately produces a reward function, an agent can continue to improve with further RL optimization. If the expert was suboptimal or noisy, a sufficiently good RL algorithm might find a policy that achieves an even higher reward (i.e. fine-tunes the behavior) while still aligning with the expert\u2019s intent encoded in \\(\\hat{R}\\).</p> </li> </ul> <p>In summary, IRL shifts the imitation learning problem from policy regression to reward inference. It answers a fundamentally different question: instead of directly cloning actions, infer the hidden goals that the expert is pursuing. With \\(\\hat{R}\\) in hand, we then fall back on standard RL techniques (like those from Chapters 4\u20138) to derive a policy. IRL is especially appealing in scenarios where we suspect the expert\u2019s behavior is optimizing some elegant underlying objective, and we want to uncover that objective for reuse or interpretation. The cost of IRL is the added complexity of the learning process, but the payoff is a deeper understanding of the task and potentially greater robustness and optimality of the learned policy.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-reinforcement-learning","title":"Maximum Entropy Inverse Reinforcement Learning","text":""},{"location":"reinforcement/8_imitation_learning/#principle-of-maximum-entropy","title":"Principle of Maximum Entropy","text":"<p>The entropy of a distribution \\(p(s)\\) is defined as:</p> \\[H(p) = -\\sum_{s} p(s)\\log p(s)\\] <p>The principle of maximum entropy states: The probability distribution that best represents our state of knowledge is the one with the largest entropy, given the constraints of precisely stated prior data. Consider all probability distributions consistent with the observed data. Select the one with maximum entropy\u2014i.e., the least biased distribution that fits what we know while assuming nothing extra.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-applied-to-irl","title":"Maximum Entropy Applied to IRL","text":"<p>We seek a distribution over trajectories \\(P(\\tau)\\) that:</p> <ol> <li>Has maximum entropy, and</li> <li>Matches expert feature expectations.</li> </ol> <p>Formally, we maximize:</p> \\[\\max_{P} -\\sum_{\\tau} P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)\\mu(\\tau) = \\frac{1}{|D|}\\sum_{\\tau_i \\in D} \\mu(\\tau_i)\\] \\[\\sum_{\\tau} P(\\tau) = 1\\] <p>Here:</p> <ul> <li>\\(\\mu(\\tau)\\) represents feature counts for trajectory \\(\\tau\\)</li> <li>\\(D\\) is the expert demonstration set</li> </ul> <p>This says: among all possible distributions consistent with observed expert feature averages, choose the one with maximum uncertainty.</p>"},{"location":"reinforcement/8_imitation_learning/#matching-rewards","title":"Matching Rewards","text":"<p>In linear reward IRL, we assume rewards take the form:</p> \\[r_\\phi(\\tau) = \\phi^\\top \\mu(\\tau)\\] <p>We want a policy \\(\\pi\\) that induces a trajectory distribution \\(P(\\tau)\\) matching the expert\u2019s expected reward under \\(r_\\phi\\):</p> \\[\\max_{P(\\tau)} -\\sum_{\\tau}P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)r_\\phi(\\tau) = \\sum_{\\tau} \\hat{P}(\\tau)r_\\phi(\\tau)\\] \\[\\sum_{\\tau}P(\\tau)=1\\] <p>This aligns the learner\u2019s expected reward with the expert\u2019s reward estimate.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-exponential-family-distributions","title":"Maximum Entropy \u21d2 Exponential Family Distributions","text":"<p>Using constrained optimization (Lagrangians), we obtain:</p> \\[\\log P(\\tau) = \\lambda_1 r_\\phi(\\tau) - 1 - \\lambda_0\\] <p>Thus:</p> \\[P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\] <p>This reveals a key result: The maximum entropy distribution consistent with constraints belongs to the exponential family.</p> <p>That is,</p> \\[p(\\tau|\\phi) = \\frac{1}{Z(\\phi)}\\exp(r_\\phi(\\tau))\\] <p>where</p> \\[Z(\\phi)=\\sum_{\\tau}\\exp(r_\\phi(\\tau))\\] <p>This means we can now learn \\(\\phi\\) by maximizing likelihood of observed expert data, because the trajectory distribution becomes a normalized exponential model.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution","title":"Maximum Entropy Over \\(\\tau\\) Equals Maximum Likelihood of Observed Data Under Max Entropy (Exponential Family) Distribution","text":"<p>Jaynes (1957) showed: Maximizing entropy over trajectories = maximizing likelihood of data under the maximum-entropy distribution.</p> <p>So we:</p> <ol> <li>Assume \\(p(\\tau|\\phi)\\) has exponential form</li> <li>Learn \\(\\phi\\) by maximizing:</li> </ol> \\[\\max_{\\phi} \\prod_{\\tau \\in D} p(\\tau|\\phi)\\] <p>This allows IRL to treat expert demonstrations as data to be probabilistically explained.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-rl-algorithm","title":"Maximum Entropy Inverse RL Algorithm","text":"<p>Assuming known dynamics and linear rewards:</p> <ol> <li>Input: expert demonstrations \\(\\mathcal{D}\\)</li> <li>Initialize reward weights \\(r_\\phi\\)</li> <li>Compute optimal policy \\(\\pi(a|s)\\) given \\(r_\\phi\\) (via dynamic programming / value iteration)</li> <li>Compute state visitation frequencies \\(\\rho(s|\\phi,T)\\)</li> <li> <p>Compute gradient on reward parameters:</p> <p>\\(\\nabla J(\\phi) = \\frac{1}{N}\\sum_{\\tau_i \\in \\mathcal{D}} \\mu(\\tau_i) - \\sum_{s}\\rho(s|\\phi,T)\\mu(s)\\)</p> </li> <li> <p>Update \\(\\phi\\) via gradient step</p> </li> <li>Repeat from Step 3</li> </ol> <p>Maximum Entropy IRL assumes experts act stochastically but optimally. Instead of selecting a single best policy, it finds a distribution over trajectories consistent with expert behavior. The resulting trajectory probabilities follow: \\(\\(P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\)\\)</p> <p>Learning becomes maximum likelihood estimation: find reward parameters \\(\\phi\\) that best explain expert demonstrations.</p>"},{"location":"reinforcement/8_imitation_learning/#apprenticeship-learning","title":"Apprenticeship Learning","text":"<p>Apprenticeship Learning usually refers to the scenario where an agent learns to perform a task by iteratively improving its policy using expert demonstrations as a reference. In many contexts, this term is used when an IRL algorithm is combined with policy learning: the agent behaves as an apprentice to the expert, gradually mastering the task. The classic formulation by Abbeel and Ng (2004) introduced apprenticeship learning via IRL, which guarantees that the learner\u2019s policy will perform nearly as well as the expert\u2019s, given enough demonstration data.</p> <p>One way to think of apprenticeship learning is as follows: rather than directly cloning actions, we try to match the feature expectations of the expert. Suppose we have some features \\(\\phi(s)\\) of states (or state-action pairs) that capture what we care about in the task (for example, in driving, features might include lane deviation, speed, collision count, etc.). The expert will have some expected cumulative feature values . Apprenticeship learning methods aim for the learner to achieve similar feature expectations.</p> <p>A prototypical apprenticeship learning algorithm proceeds like this:</p> <ol> <li> <p>Initialize a candidate policy (it could even start random).</p> </li> <li> <p>Evaluate how this policy behaves in terms of features (run it in simulation to estimate \\(\\mathbb{E}_{\\pi}\\left[\\sum_t \\phi(s_t)\\right]\\)).</p> </li> <li> <p>Compare the policy\u2019s behavior to the expert\u2019s behavior. Identify the biggest discrepancy in feature expectations.</p> </li> <li> <p>Adjust the reward (implicitly defined as a weighted sum of features) to penalize the discrepancy. In other words, find reward weights \\(w\\) such that the expert\u2019s advantage over the apprentice in those feature dimensions is highlighted.</p> </li> <li> <p>Optimize a new policy for this updated reward function (solve the MDP with the new \\(w\\) to get \\(\\pi_{\\text{new}}\\) that maximizes \\(w \\cdot \\phi\\)).</p> </li> <li> <p>Set this \\(\\pi_{\\text{new}}\\) as the apprentice\u2019s policy and repeat the evaluation -&gt; comparison -&gt; reward adjustment cycle.</p> </li> </ol> <p>Each iteration pushes the apprentice to close the gap on the feature that most distinguishes it from the expert. After a few iterations, this process yields a policy that matches the expert on all key feature dimensions within some tolerance. At that point, the apprentice is essentially as good as the expert with respect to any reward expressible as a combination of those features.</p> <p>The term apprenticeship learning highlights that the agent is not just mimicking blindly but is engaged in a process of improvement guided by the expert\u2019s example. Importantly, the focus is on achieving at least the expert\u2019s level of performance. We don\u2019t necessarily care about identifying the exact reward the expert had; we care that our apprentice\u2019s policy is successful. In fact, in the algorithm above, the reward weights \\(w\\) found in each iteration are intermediate tools \u2013 at the end, one can take the final policy and deploy it, without needing to stick to a single explicit reward interpretation.</p> <p>In relation to IRL, apprenticeship learning can be seen as a practical approach to use IRL for control: IRL finds a reward that explains the expert, and then the agent learns a policy for that reward; if it\u2019s not yet good enough, adjust and repeat. Modern developments in imitation learning often follow this spirit. For example, Generative Adversarial Imitation Learning (GAIL) is a more recent technique where the agent learns a policy by trying to fool a discriminator into thinking the agent\u2019s trajectories are from the expert \u2013 conceptually, the discriminator\u2019s judgment provides a sort of reward signal telling the agent how \"expert-like\" its behavior is. This can be viewed as a form of apprenticeship learning, since the agent is iteratively tweaking its policy to become indistinguishable from the expert.</p> <p>In summary, apprenticeship learning is about learning by iteratively comparing to an expert and closing the gap. It often uses IRL under the hood, but its end goal is the policy (the apprentice\u2019s skill), not necessarily the reward. It underscores a key point: in imitation learning, sometimes we care more about performing as well as the expert (a direct goal), and sometimes we care about understanding the expert\u2019s intentions (the indirect goal via IRL). Apprenticeship learning emphasizes the former.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-in-the-rl-landscape","title":"Imitation Learning in the RL Landscape","text":"<p>Imitation learning fills an important niche in the overall reinforcement learning framework. It is especially useful when:</p> <ol> <li> <p>Rewards are difficult to specify: If it's unclear how to craft a reward that captures all aspects of the desired behavior, providing demonstrations can bypass this. IL shines in complex tasks (e.g. high-level driving maneuvers, dexterous robot manipulation) where manually writing a reward function would be cumbersome or prone to error.</p> </li> <li> <p>Rewards are sparse or delayed: When reward feedback is very rare or only given at the end of an episode, a pure RL agent might struggle to get enough signal to learn. An expert trajectory provides dense guidance at every time step (state-action pairs), effectively providing a shaped signal through imitation. This can jump-start learning in tasks that are otherwise too sparse for RL to crack (Chapter 4 discussed how sparse rewards make value estimation difficult \u2013 IL sidesteps that by using expert knowledge).</p> </li> <li> <p>Exploration is risky or expensive: In real-world environments like robotics, autonomous driving, or healthcare, exploring with random or untrained policies can be dangerous or costly. IL allows learning a policy without the agent ever taking unguided actions in the real environment; it learns from safe, successful behaviors demonstrated by the expert. This makes it an attractive approach when safety is a hard constraint.</p> </li> </ol> <p>It\u2019s important to note that IL is not necessarily a replacement for reward-based RL, but rather a complement to it. A common practical approach is to bootstrap an agent with imitation learning and then fine-tune it with reinforcement learning. For example, one might first use behavioral cloning to teach a robot arm the basics of a task from human demonstrations, getting it into a reasonable regime of behavior; then, if a reward function is available (even a sparse one for success), use RL to further improve the policy, possibly surpassing the human expert's performance or adapting to slight changes in the task. The initial IL phase provides a good policy prior (saving time and avoiding dangerous exploration), and the subsequent RL phase lets the agent optimize and explore around that policy to refine skills.</p> <p>On the flip side, imitation learning does require expert data. If obtaining demonstrations is hard (or if no expert exists for a brand-new task), IL might not be applicable. Moreover, if the expert demonstrations are of varying quality or contain noise, the agent will faithfully learn those imperfections unless additional measures (like filtering data or combining with RL optimization) are taken. In contrast, a pure RL approach, given a well-defined reward and enough exploration, can in principle discover superior strategies that no demonstrator provided. Thus, in practice, there is a trade-off: IL can dramatically speed up learning and improve safety given an expert, whereas RL remains the go-to when we only have a reward signal and the freedom to explore.</p> <p>Imitation learning has become a critical part of the toolbox for solving real-world sequential decision problems. It enables success in domains that might be intractable for pure reinforcement learning by providing an external source of guidance. By learning directly from expert behavior \u2013 through methods like behavioral cloning (learning the policy directly) or inverse reinforcement learning (learning the underlying reward and then the policy) \u2013 an agent can shortcut the trial-and-error process. Of course, IL introduces its own challenges (distribution shift, reliance on demonstration coverage, potential suboptimality of the expert), but these can often be managed with algorithmic innovations (DAgger, combining IL with RL, etc.). In summary, imitation learning serves as a powerful paradigm for training agents in cases where designing rewards or allowing extensive exploration is impractical, and it often works hand-in-hand with traditional RL to achieve the best results in complex environments.</p>"},{"location":"reinforcement/8_imitation_learning/#mental-map","title":"Mental map","text":"<pre><code>                    Imitation Learning (IL)\n      Goal: Learn behavior from expert demonstrations\n                     instead of explicit rewards\n                                \u2502\n                                \u25bc\n             Why Imitation Learning? (Motivation)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Hard to design rewards \u2192 reward hacking, tuning           \u2502\n \u2502 Sparse rewards \u2192 inefficient trial &amp; error                \u2502\n \u2502 Unsafe exploration (robots, driving, healthcare)          \u2502\n \u2502 Expert data available \u2192 demonstrations as guidance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   IL vs Reward-Based RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Reward-Based RL             \u2502 Imitation Learning           \u2502\n \u2502 + Explores actively         \u2502 + Learns from expert         \u2502\n \u2502 + Needs reward design       \u2502 + No explicit reward         \u2502\n \u2502 \u2013 Unsafe / inefficient      \u2502 \u2013 Depends on demo quality   \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                         IL Problem Setup\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 MDP without reward function                                \u2502\n \u2502 Access to expert trajectories \u03c4E (s,a pairs)               \u2502\n \u2502 Goal \u2192 Learn policy \u03c0 that mimics \u03c0E                       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Core Method 1: Behavioral Cloning (BC)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat imitation as supervised learning                    \u2502\n \u2502 Train \u03c0\u03b8(s) \u2192 aE using dataset D                          \u2502\n \u2502 Discrete: cross-entropy loss                              \u2502\n \u2502 Continuous: mean squared error                            \u2502\n \u2502 Advantages: simple, offline, safe                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Key BC Problem: Covariate / Distribution Shift\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Trained only on expert states                             \u2502\n \u2502 When deployed, policy errors lead to unseen states        \u2502\n \u2502 \u2192 Poor decisions \u2192 more drift \u2192 compounding failure       \u2502\n \u2502 BC cannot recover or improve beyond expert                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                    Fixing BC: DAgger (Idea)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Let policy act, collect mistakes                          \u2502\n \u2502 Ask expert for correct action                             \u2502\n \u2502 Add to dataset and retrain                                \u2502\n \u2502 \u2192 brings training data closer to deployment distribution  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n       Core Method 2: Inverse Reinforcement Learning (IRL)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn the \u201cwhy\u201d behind actions \u2192 infer hidden reward R*   \u2502\n \u2502 Expert assumed optimal                                     \u2502\n \u2502 Solve inverse problem: \u03c0E \u2248 optimal for R*                 \u2502\n \u2502 After reward recovered \u2192 run normal RL to learn policy     \u2502\n \u2502 Benefits: generalization, interpretability, improve expert \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Method 3: Apprenticeship Learning\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Iteratively improve policy via comparing to expert        \u2502\n \u2502 Match feature expectations \u03c6(s)                           \u2502\n \u2502 Reweights reward \u2192 optimize \u2192 evaluate \u2192 repeat           \u2502\n \u2502 Goal: perform at least as well as expert                  \u2502\n \u2502 Often implemented via IRL (e.g., GAIL conceptually)       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Role of IL within broader RL landscape\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 When IL is useful:                                        \u2502\n \u2502 - Reward hard to design                                   \u2502\n \u2502 - Unsafe or costly to explore                             \u2502\n \u2502 - Sparse reward tasks                                     \u2502\n \u2502 IL + RL hybrid: BC warm-start \u2192 RL fine-tune beyond expert\u2502\n \u2502 Limitations: need expert, demos may be suboptimal         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 IL bypasses reward engineering &amp; risky exploration         \u2502\n \u2502 BC learns \u201cwhat,\u201d IRL learns \u201cwhy,\u201d apprenticeship learns  \u2502\n \u2502 \u201chow to get as good as expert.\u201d                           \u2502\n \u2502 IL often combined with RL for best performance.           \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/9_rlhf/","title":"9. RLHF","text":""},{"location":"reinforcement/9_rlhf/#chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment","title":"Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment","text":"<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for \u201cgood\u201d behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment \u2013 ensuring an AI\u2019s behavior aligns with human preferences and values \u2013 by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute \u201cscore\u201d to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p> <p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>"},{"location":"reinforcement/9_rlhf/#bradleyterry-preference-modeling-in-rlhf","title":"Bradley\u2013Terry Preference Modeling in RLHF","text":"<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley\u2013Terry model, a probabilistic model for noisy comparisons. </p> <p>Consider a \\(K\\)-armed bandit with actions \\(b_1, b_2, \\dots, b_K\\), and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action \\(b_i\\) over \\(b_j\\) is modeled as:</p> \\[ P(b_i \\succ b_j) = \\frac{\\exp(r(b_i))}{\\exp(r(b_i)) + \\exp(r(b_j))} = p_{ij} \\] <p>where \\(r(b)\\) is an unobserved scalar reward associated with action \\(b\\). Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p> <p>Assume we collect a dataset \\(\\mathcal{D}\\) of \\(N\\) comparisons of the form \\((b_i, b_j, \\mu)\\), where:</p> <ul> <li>\\(\\mu(1) = 1\\) if the human marked \\(b_i \\succ b_j\\)</li> <li>\\(\\mu(1) = 0.5\\) if the human marked \\(b_i = b_j\\)</li> <li>\\(\\mu(1) = 0\\) if the human marked \\(b_j \\succ b_i\\)</li> </ul> <p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p> \\[ \\mathcal{L} = - \\sum_{(b_i,b_j,\\mu)\\in\\mathcal{D}} \\left[ \\mu(1)\\log P(b_i \\succ b_j) + \\mu(2)\\log P(b_j \\succ b_i) \\right] \\] <p>Optimizing this loss adjusts the reward function \\(r(\\cdot)\\) so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p> <p>Once the reward model is trained using the Bradley\u2013Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley\u2013Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>"},{"location":"reinforcement/9_rlhf/#the-rlhf-training-pipeline","title":"The RLHF Training Pipeline","text":"<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT) \u2013 Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p> </li> <li> <p>Reward Model Training from Human Preferences \u2013 Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function \\(r_\\phi(x,y)\\) (parameterized by \\(\\phi\\)) that predicts which response is more preferable for a given input x using Bradley\u2013Terry model.</p> </li> <li> <p>Reinforcement Learning Fine-Tuning \u2013 In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy \\(\\pi_\\theta(y|x)\\) (with parameters \\(\\theta\\)) is updated to maximize the expected reward \\(r_\\phi(x,y)\\) of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model\u2019s score, the policy might exploit flaws in \\(r_\\phi\\) (a form of \u201creward hacking\u201d) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback\u2013Leibler (KL) penalty that keeps the new policy \\(\\pi_\\theta\\) close to the reference policy \\(\\pi_{\\text{ref}}\\) (often the SFT model). In summary, the RL objective can be written as:</p> \\[\\max_{\\pi_\\theta} ( \\underbrace{ \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi_\\theta(y \\mid x)} }_{\\text{Sample from policy}} \\left[ \\underbrace{ r_\\phi(x,y) }_{\\text{Want high reward}} \\right] - \\underbrace{ \\beta \\, \\mathbb{D}_{\\mathrm{KL}} \\left[ \\pi_\\theta(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\right] }_{\\text{Keep KL to original model small}}) \\] <p>where \\(\\beta&gt;0\\) controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if \\(\\pi_\\theta\\) deviates too much from the original model\u2019s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p> </li> </ol> <p>Through this pipeline \u2013 SFT, reward modeling, and RL fine-tuning \u2013 we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like \\(\\beta\\) and PPO clip thresholds). The process can be unstable; for instance, if \\(\\beta\\) is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if \\(\\beta\\) is too high, the policy might hardly improve at all. Researchers have described RLHF as a \u201ccomplex and often unstable procedure\u201d that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>"},{"location":"reinforcement/9_rlhf/#direct-preference-optimization-rlhf-without-rl","title":"Direct Preference Optimization: RLHF without RL?","text":"<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p> <p>Recall that in the RLHF setting, our goal is to find a policy \\(\\pi^*(y|x)\\) that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of \\(J(\\pi)\\) occurs when \\(\\pi\\) is proportional to the reference policy times an exponential of the reward:</p> \\[\\pi^*(y \\mid x) \\propto \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{1}{\\beta}\\, r_\\phi(x, y)\\right)\\] <p>This equation gives a closed-form solution for the optimal policy in terms of the reward function \\(r_\\phi\\). It makes sense: actions \\(y\\) that have higher human-derived reward should be taken with higher probability, but we temper this by \\(\\beta\\) and weight by the reference probabilities \\(\\pi_{\\text{ref}}(y|x)\\) so that we don\u2019t stray too far. If we were to normalize the right-hand side, we\u2019d write:</p> \\[\\pi^*(y \\mid x) = \\frac{ \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y)}{\\beta}\\right) }{ \\sum_{y'} \\pi_{\\text{ref}}(y' \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y')}{\\beta}\\right) }\\] <p>Here the denominator is a partition functionsumming over all possible responses \\(y'\\) for input \\(x\\). This normalization involves a sum over the entire response space, which is astronomically large for language models \u2013 hence we cannot directly compute \\(\\pi^*(y|x)\\) in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p> <p>DPO\u2019s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses \\(y_+\\) (preferred) and \\(y_-\\) (dispreferred) for the same context \\(x\\), the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p> \\[\\frac{\\pi^*(y^+ \\mid x)}{\\pi^\\ast(y^- \\mid x)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^+)}{\\beta}\\right)} {\\pi_{\\text{ref}}(y^- \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^-)}{\\beta}\\right)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)} {\\pi_{\\text{ref}}(y^- \\mid x)} \\exp\\!\\left( \\frac{1}{\\beta} \\big[ r_\\phi(x, y^+) - r_\\phi(x, y^-) \\big] \\right)\\] <p>Taking the log of both sides, we get a neat relationship:</p> \\[\\frac{1}{\\beta} \\big( r_\\phi(x, y^{+}) - r_\\phi(x, y^{-}) \\big) = \\big[ \\log \\pi^\\ast(y^{+} \\mid x) - \\log \\pi^\\ast(y^{-} \\mid x) \\big] - \\big[ \\log \\pi_{\\text{ref}}(y^{+} \\mid x) - \\log \\pi_{\\text{ref}}(y^{-} \\mid x) \\big]\\] <p>The term in brackets on the right is the difference in log-probabilities that the optimal policy \\(\\pi^*\\) assigns to the two responses (which in turn would equal the difference in our learned policy\u2019s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if \\(y_+\\) is better than \\(y_-\\) by some amount of reward, then the optimal policy should tilt its probabilities in favor of \\(y_+\\) by a corresponding factor.</p> <p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts \\(\\pi_\\theta\\) to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context \\(x\\) and two candidate outputs \\(y_+\\) (human-preferred) and \\(y_-\\) (human-dispreferred), we want the model to assign a higher probability to \\(y_+\\) than to \\(y_-\\), with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p> <p>In practice, the DPO loss for a pair \\((x, y_+, y_-)\\) is something like:</p> \\[\\ell_{\\text{DPO}}(\\theta) = - \\log \\sigma \\!\\left( \\beta\\, \\big[ \\log \\pi_\\theta(y^{+} \\mid x) - \\log \\pi_\\theta(y^{-} \\mid x) \\big] \\right)\\] <p>where \\(\\sigma\\) is the sigmoid function. This loss is low (i.e. good) when \\(\\log \\pi_\\theta(y_+|x) \\gg \\log \\pi_\\theta(y_-|x)\\), meaning the model assigns much higher probability to the preferred outcome \u2013 which is what we want. If the model hasn\u2019t yet learned the preference, the loss will be higher, and gradient descent on this loss will push \\(\\pi_\\theta\\) to increase the probability of \\(y_+\\) and decrease that of \\(y_-\\). Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy\u2019s logits: effectively, \\(\\log \\pi_\\theta(y|x)\\) plays the role of a reward score for how good \\(y\\) is, up to the scaling factor \\(1/\\beta\\). In fact, the DPO derivation can be seen as combining the preference loss on \\(r_\\phi\\) with the \\(\\pi^*\\) solution formula to produce a preference loss on \\(\\pi_\\theta\\). The original DPO paper calls this approach \u201cyour language model is secretly a reward model\u201d \u2013 by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p> <p>## Mental map</p> <p><code>text          Reinforcement Learning from Human Feedback (RLHF)    Goal: Align model behavior with human preferences and values           when explicit reward design is impractical                                 \u2502                                 \u25bc            Why Dense Rewards Are Hard for Language Models  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Open-ended tasks (dialogue, summarization, reasoning)     \u2502  \u2502 No clear numeric notion of \u201cgood\u201d behavior                \u2502  \u2502 Hand-crafted dense rewards \u2192 miss nuance, reward hacking  \u2502  \u2502 Metrics (BLEU, ROUGE, length) poorly reflect human values \u2502  \u2502 Human values are subjective, contextual, and fuzzy        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc             From Imitation Learning to Human Preferences  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Behavioral Cloning (IL): imitate demonstrations           \u2502  \u2502 + Simple, safe, no reward needed                          \u2502  \u2502 \u2013 Cannot exceed expert, sensitive to distribution shift   \u2502  \u2502 DAgger: fixes BC but requires step-by-step human labeling \u2502  \u2502 Pairwise preferences = middle ground                      \u2502  \u2502 \u2192 no dense rewards, no per-step supervision               \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Pairwise Preference Feedback (Key Idea)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Humans compare two outputs and choose the better one      \u2502  \u2502 Easier than assigning numeric rewards                     \u2502  \u2502 More informative than raw demonstrations                  \u2502  \u2502 Scales to complex, open-ended behaviors                   \u2502  \u2502 Forms basis of reward learning in RLHF                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc         Bradley\u2013Terry Model: Preferences \u2192 Reward Signal  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Model noisy human comparisons probabilistically           \u2502  \u2502 P(b_i \u227b b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    \u2502  \u2502 r(b): latent scalar reward                                \u2502  \u2502 Fit r(\u00b7) by maximizing likelihood / cross-entropy         \u2502  \u2502 Preferred outputs get higher reward scores                \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              RLHF Training Pipeline (3 Stages)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 1. Supervised Fine-Tuning (SFT)                           \u2502  \u2502    \u2013 Behavioral cloning on human-written demos            \u2502  \u2502    \u2013 Produces reference policy \u03c0_ref                      \u2502  \u2502                                                           \u2502  \u2502 2. Reward Model Training                                  \u2502  \u2502    \u2013 Human pairwise preferences                           \u2502  \u2502    \u2013 Train r_\u03c6(x,y) via Bradley\u2013Terry loss                \u2502  \u2502                                                           \u2502  \u2502 3. RL Fine-Tuning (PPO)                                   \u2502  \u2502    \u2013 Maximize reward r_\u03c6(x,y)                             \u2502  \u2502    \u2013 KL penalty keeps \u03c0_\u03b8 close to \u03c0_ref                  \u2502  \u2502    \u2013 Prevents reward hacking &amp; language drift             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               RLHF Objective (KL-Regularized RL)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maximize:                                                 \u2502  \u2502   E[r_\u03c6(x,y)] \u2212 \u03b2 \u00b7 KL(\u03c0_\u03b8 || \u03c0_ref)                      \u2502  \u2502 \u03b2 controls tradeoff:                                      \u2502  \u2502   Low \u03b2 \u2192 reward hacking / mode collapse                  \u2502  \u2502   High \u03b2 \u2192 little improvement over SFT                    \u2502  \u2502 PPO provides stable policy updates                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Limitations of Standard RLHF (PPO-based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Requires training multiple models                         \u2502  \u2502 Many hyperparameters (\u03b2, PPO clip, value loss, etc.)      \u2502  \u2502 Sampling-based RL can be unstable                         \u2502  \u2502 Expensive and complex pipeline                            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc       Direct Preference Optimization (DPO): RLHF without RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Solve RLHF objective in closed form                       \u2502  \u2502 Optimal policy:                                           \u2502  \u2502   \u03c0*(y|x) \u221d \u03c0_ref(y|x) \u00b7 exp(r_\u03c6(x,y)/\u03b2)                  |  \u2502 Use probability ratios \u2192 normalization cancels            \u2502  \u2502 Train \u03c0_\u03b8 directly on preference pairs                    \u2502  \u2502 Loss: sigmoid on log-prob difference                      \u2502  \u2502 \u201cYour LM is secretly a reward model\u201d                      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               DPO vs PPO-based RLHF  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 RLHF (PPO)                  \u2502 DPO                         \u2502  \u2502 + Explicit RL optimization  \u2502 + Pure supervised learning  \u2502  \u2502 \u2013 Complex &amp; unstable        \u2502 \u2013 Assumes KL-optimal form   \u2502  \u2502 \u2013 Many hyperparameters      \u2502 + Simple, stable, efficient \u2502  \u2502                             \u2502 + No separate reward model  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Final Takeaway (Chapter Summary)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Dense rewards are hard for language tasks                 \u2502  \u2502 Pairwise preferences provide natural human feedback       \u2502  \u2502 RLHF learns rewards from preferences + optimizes policy   \u2502  \u2502 DPO simplifies RLHF by removing explicit RL               \u2502  \u2502 Together, they extend imitation learning toward           \u2502  \u2502 scalable value alignment for modern language models       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</code></p>"},{"location":"reinforcement/readme/","title":"Readme","text":"<p>https://www.youtube.com/watch?v=L6OVEmV3NcE&amp;list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX&amp;index=5</p> <p>https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=1</p> <p>https://www.youtube.com/watch?v=WxRDyObrm_M</p>"},{"location":"statistics/introd/","title":"Introd","text":"<p>Variational Inference Exact Inference Monte Carlo  Markov Chain MCMC Sampling vs Optimization</p>"},{"location":"tools/intro/","title":"Intro","text":"<p>Snowflake</p> <p>Agentic frameworks (using CruxAI, Google ADK, LangGraph).</p> <p>Multi-agent systems using frameworks such as LangChain, LangGraph, AutoGen, or CrewAI, with practical understanding of LLM orchestration, retrieval augmentation (RAG), tool calling, and dynamic reasoning.</p> <p>Docker, Kubernetes, and microservices integration.</p> <p>Object-oriented programming skills and experience working with Python, PyTorch and NumPy are desirable Advanced optimisation methods, modern ML techniques, HPC, profiling, model inference; you don\u2019t need to have all of the above Big-data technologies such as Spark, KDB</p>"},{"location":"tools/coding%20practices/1_env_setup/","title":"1 env setup","text":""},{"location":"tools/coding%20practices/1_env_setup/#setting-up-a-python-virtual-environment","title":"Setting Up a Python Virtual Environment","text":""},{"location":"tools/coding%20practices/1_env_setup/#1-create-a-separate-python-virtual-environment","title":"1. Create a Separate Python Virtual Environment","text":"<p>Use the following command to create a virtual environment named <code>.venv</code>:</p> <pre><code>python -m venv .venv\n</code></pre>"},{"location":"tools/coding%20practices/1_env_setup/#2-activate-the-virtual-environment","title":"2. Activate the Virtual Environment","text":"<ul> <li> <p>On Windows (Git Bash / PowerShell): </p><pre><code>source .venv/Scripts/activate\n</code></pre><p></p> </li> <li> <p>On macOS / Linux: </p><pre><code>source .venv/bin/activate\n</code></pre><p></p> </li> </ul> <p>Once activated, your terminal prompt will change toindicate that the virtual environment is active.</p>"},{"location":"tools/gRPC/gRPC/","title":"gRPC","text":""},{"location":"tools/gRPC/gRPC/#api-technologies-overview","title":"API Technologies Overview","text":"API Style Communication Style Core Technology Primary Data Format Best For REST Stateless, Request/Response HTTP/HTTPS JSON, XML, HTML General-purpose web services, public APIs, mobile apps, and simple resource management (CRUD). SOAP Protocol-agnostic, Structured HTTP, SMTP, TCP XML Enterprise-level services, financial/banking, and highly secure or reliable transactions with strict contracts. gRPC RPC, Binary Streaming HTTP/2 Protocol Buffers (Protobuf) High-performance microservices communication, internal APIs, and streaming data where speed is critical. GraphQL Request/Response (Single Endpoint) HTTP/HTTPS JSON Modern web/mobile frontends that need flexible and precise data fetching to avoid over/under-fetching. Webhook Event-Driven (Server to Client) HTTP POST JSON, XML Real-time notifications, system integrations, and subscribing to external events (e.g., payment status change). WebSocket Stateful, Full-Duplex TCP/WebSocket Protocol Binary, Text, JSON Real-time interactive applications like chat, live dashboards, gaming, and collaborative editing. WebRTC Peer-to-Peer Various Protocols (e.g., STUN, TURN) Media Streams Direct, low-latency browser-to-browser communication for video calls, voice chat, and screen sharing."},{"location":"tools/gRPC/gRPC/#detailed-breakdown-use-cases","title":"Detailed Breakdown &amp; Use Cases","text":""},{"location":"tools/gRPC/gRPC/#1-rest-representational-state-transfer","title":"1. REST (Representational State Transfer)","text":"<ul> <li>Basic Details: Architectural style, stateless. Uses standard HTTP methods (GET, POST, PUT, DELETE) to operate on resources identified by URLs.</li> <li>When to Use:<ul> <li>\u2705 Building a public-facing API where simplicity, broad browser support, and a stateless, cacheable design are priorities.</li> <li>\u2705 For simple CRUD (Create, Read, Update, Delete) operations.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#2-soap-simple-object-access-protocol","title":"2. SOAP (Simple Object Access Protocol)","text":"<ul> <li>Basic Details: Formal, XML-based messaging protocol. Protocol-independent (HTTP, SMTP, etc.) with built-in standards for security and reliability (WS-Security).</li> <li>When to Use:<ul> <li>\u2705 In enterprise environments, such as banking or healthcare, where strict contracts (WSDL), security, and transaction reliability are non-negotiable.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#3-grpc-google-remote-procedure-call","title":"3. gRPC (Google Remote Procedure Call)","text":"<ul> <li>Basic Details: High-performance RPC framework using HTTP/2 and Protocol Buffers (Protobuf) for efficient, binary data serialization. Supports multiple types of streaming.</li> <li>When to Use:<ul> <li>\u2705 For internal microservices communication where speed, efficiency, and strongly-typed contracts across multiple languages are essential.</li> <li>\u2705 When high-throughput streaming of data is a core requirement.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#4-graphql-graph-query-language","title":"4. GraphQL (Graph Query Language)","text":"<ul> <li>Basic Details: A query language where clients request exactly the data they need from a single endpoint, preventing over/under-fetching.</li> <li>When to Use:<ul> <li>\u2705 For applications with complex, interconnected data or multiple clients (web, mobile) with varying data needs.</li> <li>\u2705 To improve performance on mobile clients by minimizing payload size.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#5-webhook","title":"5. Webhook","text":"<ul> <li>Basic Details: A \"reverse API\" that acts as an event-driven push notification. The server makes an HTTP POST request to a client-defined URL (callback) when an event occurs.</li> <li>When to Use:<ul> <li>\u2705 When you need real-time alerts for events (e.g., \"a payment was processed\") without continuous polling.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#6-websocket","title":"6. WebSocket","text":"<ul> <li>Basic Details: A communication protocol that provides a persistent, two-way (full-duplex) connection over a single TCP connection, enabling both client and server to send data at any time.</li> <li>When to Use:<ul> <li>\u2705 For real-time interactive applications where sustained, low-latency, two-way communication is necessary (e.g., chat applications, live data feeds).</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#7-webrtc-web-real-time-communication","title":"7. WebRTC (Web Real-Time Communication)","text":"<ul> <li>Basic Details: Enables real-time, peer-to-peer (P2P) communication directly between browsers for streaming audio, video, and arbitrary data.</li> <li>When to Use:<ul> <li>\u2705 For building live video/audio conferencing, voice-over-IP (VoIP), or P2P data sharing.</li> </ul> </li> </ul>"},{"location":"tools/kafka/kafka/","title":"Apache Kafka","text":""},{"location":"tools/kafka/kafka/#apache-kafka","title":"Apache Kafka","text":""},{"location":"tools/kafka/kafka/#1-what-is-kafka","title":"1. What Is Kafka?","text":"<p>Apache Kafka is a distributed event streaming platform designed for: - High-throughput data ingestion - Real-time analytics - Decoupling microservices - Reliable event storage and replay  </p> <p>Kafka acts like a conveyor belt for data: producers place events on the belt; consumers pick them up independently.</p> <p>fundamental ideas behind Kafka: messages sent and received through Kafka require a user specified distribution strategy</p>"},{"location":"tools/kafka/kafka/#2-why-kafka-problems-it-solves","title":"2. Why Kafka? (Problems It Solves)","text":""},{"location":"tools/kafka/kafka/#a-the-problem-with-tightly-coupled-microservices","title":"A. The Problem With Tightly Coupled Microservices","text":"<p>Traditional microservices call each other directly (REST, RPC).</p> <p>Issues: 1. Tight Coupling    If Service A depends on Service B, and B slows or fails \u2192 A also fails.</p> <ol> <li> <p>Single Point of Failure    One service outage disrupts the entire chain.</p> </li> <li> <p>Scalability Limitations    Upstream must handle downstream load directly.</p> </li> <li> <p>Lost Analytics Data    If analytics is down, events are lost.</p> </li> </ol> <p>Kafka solves this by decoupling communication and introducing an event broker.</p>"},{"location":"tools/kafka/kafka/#3-kafka-as-an-event-broker","title":"3. Kafka as an Event Broker","text":"<p>Microservices no longer call each other. Instead:</p> <ul> <li>Producers publish events to Kafka.</li> <li>Topics organize those events.</li> <li>Consumers independently subscribe and process them.</li> </ul> <p>This supports: - Asynchronous communication - Fault isolation - Independent scaling - Reliable buffering - Replay of past events  </p>"},{"location":"tools/kafka/kafka/#4-core-concepts","title":"4. Core Concepts","text":""},{"location":"tools/kafka/kafka/#a-events","title":"A. Events","text":"<p>An event records that something happened.</p> <p>Structure: - Key - Value - Timestamp - Optional metadata (headers)</p> <p>Example: </p><pre><code>Key: orderId=123\nValue: {\"status\": \"CREATED\", \"amount\": 200}\n</code></pre><p></p>"},{"location":"tools/kafka/kafka/#b-topics","title":"B. Topics","text":"<p>A topic is a category for events of the same type (e.g., <code>orders</code>, <code>payments</code>).</p>"},{"location":"tools/kafka/kafka/#where-are-topics-stored","title":"Where Are Topics Stored?","text":"<p>On Kafka brokers, distributed into partitions.</p>"},{"location":"tools/kafka/kafka/#c-partitions","title":"C. Partitions","text":"<p>Partition = append-only, ordered event log.</p> <p>Benefits: - Scalability (multiple partitions) - High throughput - Parallel processing - Ordering (guaranteed only within a partition)</p> <p>A topic is a logical grouping of messages. A partition is a physical grouping of messages. A topic can have multiple partitions, and each partition can be on a different broker. Topics are just a way to organize your data, while partitions are a way to scale your data.</p>"},{"location":"tools/kafka/kafka/#d-kafka-broker","title":"D. Kafka Broker","text":"<p>A broker is a Kafka server.</p> <p>Responsibilities: - Store topic partitions - Handle reads/writes from producers/consumers - Replicate data for fault tolerance  </p> <p>Replication: - Each partition has:   - Leader (handles requests)   - Followers (replicas for failover)</p>"},{"location":"tools/kafka/kafka/#e-consumers-consumer-groups","title":"E. Consumers &amp; Consumer Groups","text":""},{"location":"tools/kafka/kafka/#consumer","title":"Consumer","text":"<p>Reads records from topics.</p>"},{"location":"tools/kafka/kafka/#consumer-group","title":"Consumer Group","text":"<p>A group of consumers sharing the work of reading a topic.</p> <p>Rules: - One consumer per partition (within a group) - But multiple groups can read the same topic independently</p> <p>Examples: - One group processes orders - Another group replays events for analytics - Another group trains ML models  </p>"},{"location":"tools/kafka/kafka/#5-kafka-vs-traditional-message-queues-rabbitmq-activemq","title":"5. Kafka vs Traditional Message Queues (RabbitMQ, ActiveMQ)","text":"Feature Kafka Traditional MQ Message deletion Stored for retention period Deleted after consumption Replay messages Yes No Storage Disk-backed log Often in-memory Primary use Event streaming &amp; analytics Task queues <p>Kafka is not a database, but it reliably stores events and allows consumers to replay them.</p>"},{"location":"tools/kafka/kafka/#6-kafka-for-real-time-processing","title":"6. Kafka for Real-Time Processing","text":""},{"location":"tools/kafka/kafka/#a-kafka-streams-api","title":"A. Kafka Streams API","text":"<p>A powerful library for real-time event processing.</p> <p>Features: - Continuous streaming (no polls) - Functional transformations:   - map   - filter   - join   - aggregate - Windowing - Stateful processing  </p> <p>Use cases: - Fraud detection - Real-time dashboards - Metric aggregation - Data enrichment  </p>"},{"location":"tools/kafka/kafka/#7-kafka-architecture-components-summary-table","title":"7. Kafka Architecture Components (Summary Table)","text":"Component Role Producer Writes events to topics Topic Category of events Partition Ordered log segment for scalability Consumer Reads events Consumer Group Enables parallel processing Broker Stores partitions and handles traffic Cluster Multiple brokers working together"},{"location":"tools/kafka/kafka/#8-kafka-coordination-zookeeper-vs-kafka-raft-kraft","title":"8. Kafka Coordination: Zookeeper vs Kafka Raft (KRaft)","text":""},{"location":"tools/kafka/kafka/#originally-zookeeper","title":"Originally: Zookeeper","text":"<p>Used for: - Metadata management - Leader election - Cluster configuration  </p>"},{"location":"tools/kafka/kafka/#now-kafka-kraft-kafka-raft","title":"Now: Kafka KRaft (Kafka Raft)","text":"<ul> <li>Built-in consensus protocol  </li> <li>No Zookeeper dependency  </li> <li>Simplifies deployment  </li> <li>Better scalability and reliability  </li> </ul> <p>Zookeeper is being phased out.</p>"},{"location":"tools/kafka/kafka/#9-key-benefits-of-kafka","title":"9. Key Benefits of Kafka","text":"<ul> <li>High throughput  </li> <li>Low latency  </li> <li>Durable storage  </li> <li>Scalable horizontally  </li> <li>Fault-tolerant  </li> <li>Can replay historical events  </li> <li>Decouples microservices  </li> <li>Supports real-time analytics  </li> </ul> <p>https://www.youtube.com/watch?v=QkdkLdMBuL0</p> <p>https://www.youtube.com/watch?v=B7CwU_tNYIE</p>"},{"location":"tools/kafka/kafka_rabbitmq/","title":"Kafka rabbitmq","text":"<p>Kafka vs RabbitMQ</p> <p>Kafka - Extremely high trhouhou - Rplay - Data Retention - Fan out</p> <p>Tranditional Queues - COmplex message routing - message indended for one consumer - modeerate data volume</p>"},{"location":"tools/mcp/1_mcp_architecture/","title":"Understanding MCP Client-Server Architecture (Step by Step)","text":""},{"location":"tools/mcp/1_mcp_architecture/#understanding-mcp-client-server-architecture-step-by-step","title":"Understanding MCP Client-Server Architecture (Step by Step)","text":""},{"location":"tools/mcp/1_mcp_architecture/#1-mcp-uses-a-client-server-model","title":"1. MCP Uses a \u201cClient-Server\u201d Model","text":"<ul> <li>Think of client-server like a restaurant:</li> <li>The server is the kitchen, which prepares food.</li> <li>The client is the customer, who asks for food.</li> <li>In MCP, the server provides AI functionality (like responding to code or commands), and the client asks for it.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#2-what-is-an-mcp-host","title":"2. What is an MCP Host?","text":"<ul> <li>The MCP host is like the manager of the restaurant.  </li> <li>It\u2019s the AI application you\u2019re using, e.g., Claude Code or Claude Desktop.  </li> <li>Its job is to connect to MCP servers so it can get AI services.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#3-mcp-host-creates-mcp-clients","title":"3. MCP Host Creates MCP Clients","text":"<ul> <li>For each server the host wants to talk to, it makes a client.  </li> <li>Imagine this as the manager sending one waiter per kitchen to get the food.  </li> <li>Each client talks only to its assigned server.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#4-local-vs-remote-servers","title":"4. Local vs Remote Servers","text":"<ul> <li>Local MCP servers:</li> <li>Run on your own computer.</li> <li>Usually only serve one client (your AI app).  </li> <li>They communicate using STDIO (sending text back and forth like typing into a console).</li> <li>Remote MCP servers:</li> <li>Run somewhere else on the internet.</li> <li>Can serve many clients at the same time.  </li> <li>They use Streamable HTTP, which sends data efficiently over the internet.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#5-summary-with-a-simple-analogy","title":"5. Summary with a Simple Analogy","text":"<p>Imagine you have a chain of kitchens (servers) and a restaurant manager (host):</p> <ol> <li>The manager (host) wants dishes from multiple kitchens.</li> <li>For each kitchen, the manager sends a waiter (client) to place orders.</li> <li>Each waiter only talks to their assigned kitchen.</li> <li>Some kitchens are nearby (local) \u2192 serve one waiter at a time.</li> <li>Some kitchens are far away (remote) \u2192 serve many waiters at once.</li> </ol>"},{"location":"tools/mcp/1_mcp_architecture/#understanding-mcp-layers-data-layer-and-transport-layer-analogy-based","title":"Understanding MCP Layers: Data Layer and Transport Layer (Analogy-Based)","text":""},{"location":"tools/mcp/1_mcp_architecture/#mcp-layers","title":"MCP Layers","text":"<p>MCP Has Two Layers</p> <ol> <li>Data Layer (Inner Layer \u2013 the menu &amp; recipes)  </li> <li>Defines what can be ordered and how it\u2019s prepared.  </li> <li>Includes the structure of orders, cooking steps, and communication rules between the waiter and the kitchen.  </li> <li> <p>Key responsibilities:</p> <ul> <li>Lifecycle Management \u2192 Setting up the kitchen and waiters, making sure they can communicate, and closing the kitchen at the end of the day.</li> <li>Server Features \u2192 Kitchen provides tools (utensils), resources (ingredients), and prompts (recipe instructions) to make dishes.</li> <li>Client Features \u2192 Waiters (clients) can ask the kitchen to sample dishes, get input from customers, and log orders.</li> <li>Utility Features \u2192 Notifications (like \u201corder ready\u201d) and tracking long cooking times.</li> </ul> </li> <li> <p>Transport Layer (Outer Layer \u2013 the delivery system)  </p> </li> <li>Handles how orders are sent and delivered between waiters and kitchens.  </li> <li> <p>Key responsibilities:</p> <ul> <li>Ensures messages (orders) are framed correctly.</li> <li>Manages secure communication.</li> <li>Handles authentication (who is allowed to place orders).</li> </ul> </li> <li> <p>Transport Methods:</p> </li> <li>Stdio Transport \u2192 Waiter walks directly to a local kitchen in the same building. Fast, simple, no network needed.</li> <li>Streamable HTTP Transport \u2192 Waiter sends orders over the internet to a distant kitchen, possibly streaming updates. Supports authentication like ID badges, keys, or OAuth tokens.</li> </ol>"},{"location":"tools/mcp/1_mcp_architecture/#putting-it-together","title":"Putting It Together","text":"<ul> <li>Data Layer = the menu, recipes, and instructions (defines what can be done).  </li> <li>Transport Layer = the delivery system (defines how the orders travel).  </li> <li>This separation allows the same recipes (JSON-RPC messages) to work whether the kitchen is next door (local) or across town (remote).</li> </ul> <p>Analogy Summary: </p> <ul> <li>Kitchen = MCP Server  </li> <li>Waiter = MCP Client  </li> <li>Manager = MCP Host  </li> <li>Menu &amp; recipes = Data Layer  </li> <li>Delivery system = Transport Layer</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#mcp-data-layer-protocol-analogy-based","title":"MCP Data Layer Protocol (Analogy-Based)","text":""},{"location":"tools/mcp/1_mcp_architecture/#mcp-data-layer-overview","title":"MCP Data Layer Overview","text":"<p>Think of the data layer as the menu, recipes, and instructions in a restaurant:</p> <ul> <li>It defines what can be done between waiters (clients) and kitchens (servers).  </li> <li>This is the part developers interact with most, because it specifies the context and actions that can be shared.  </li> <li>MCP uses JSON-RPC 2.0, which is like a standard way for waiters and kitchens to send orders and responses:</li> <li>Requests = ordering a dish  </li> <li>Responses = kitchen replies with the dish  </li> <li>Notifications = announcements like \"special of the day\" (no reply needed)</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#lifecycle-management","title":"Lifecycle Management","text":"<ul> <li>MCP is stateful \u2014 meaning it remembers who\u2019s talking to whom.  </li> <li>Lifecycle management ensures the waiter and kitchen agree on capabilities before starting work:</li> <li>Example: Which tools, ingredients, or prompts each side supports.  </li> <li>This is like a waiter confirming the kitchen can handle gluten-free or vegan orders before taking the order.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#mcp-primitives-the-menu-items","title":"MCP Primitives (The \u201cMenu Items\u201d)","text":"<p>Primitives are the core items that can be shared between client and server \u2014 think of them as the dishes on the menu.</p>"},{"location":"tools/mcp/1_mcp_architecture/#1-server-primitives-kitchen-offerings","title":"1. Server Primitives (Kitchen Offerings)","text":"<ul> <li>Tools \u2192 Executable functions the kitchen can perform (e.g., make a pizza, run a database query, call an API).  </li> <li>Resources \u2192 Data or ingredients provided to the waiter (e.g., database schema, file contents, API responses).  </li> <li>Prompts \u2192 Reusable templates to help structure interactions (e.g., recipes, system prompts, few-shot examples).</li> </ul> <p>How it works: 1. Waiter lists available dishes: <code>tools/list</code>, <code>resources/list</code>, <code>prompts/list</code>. 2. Waiter gets details or executes actions: <code>tools/call</code>, <code>resources/get</code>, etc.  </p> <p>Analogy Example: - Kitchen offers a database context:   - Tool: query the database   - Resource: database schema   - Prompt: few-shot instructions for querying</p>"},{"location":"tools/mcp/1_mcp_architecture/#2-client-primitives-waiter-capabilities","title":"2. Client Primitives (Waiter Capabilities)","text":"<p>These let kitchens ask the waiter to do things:</p> <ul> <li>Sampling \u2192 Kitchen asks waiter to generate AI completions (like letting the waiter suggest a dish from another kitchen).  </li> <li>Elicitation \u2192 Kitchen asks waiter to get more info from the customer (user input or confirmation).  </li> <li>Logging \u2192 Kitchen sends logs to the waiter for monitoring or debugging.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#3-utility-primitives-special-services","title":"3. Utility Primitives (Special Services)","text":"<ul> <li>Tasks (Experimental) \u2192 Wrappers for long-running or deferred actions (like pre-orders, batch cooking, or multi-step dishes).  </li> <li>These allow kitchens to track progress and retrieve results later.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#notifications-real-time-updates","title":"Notifications (Real-Time Updates)","text":"<ul> <li>Servers can send updates to clients without expecting a response:  </li> <li>Example: A new tool becomes available, or an existing one is modified.  </li> <li>Analogy: Kitchen announces \u201cNew seasonal dish available!\u201d to all connected waiters.  </li> <li>Notifications are sent as JSON-RPC 2.0 notification messages.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#summary-analogy-table","title":"Summary Analogy Table","text":"MCP Concept Restaurant Analogy Server (MCP Server) Kitchen Client (MCP Client) Waiter Host (MCP Host) Restaurant Manager Data Layer Menu &amp; Recipes (defines what can be done) Tools Kitchen actions (make dishes, run queries) Resources Ingredients / data for dishes Prompts Recipes / instructions Client Primitives Waiter capabilities (ask for help, log info) Utility Primitives Special services like batch orders or long tasks Notifications Announcements like \u201cspecials of the day\u201d"},{"location":"tools/pytorch/1_pytorch/","title":"PyTorch","text":""},{"location":"tools/pytorch/1_pytorch/#pytorch","title":"PyTorch","text":"<pre><code>import torch\nimport numpy as np\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#1-creating-tensors","title":"1. Creating Tensors","text":""},{"location":"tools/pytorch/1_pytorch/#torchzeros","title":"<code>torch.zeros</code>","text":"<pre><code>x_zeros = torch.zeros(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#torchones","title":"<code>torch.ones</code>","text":"<pre><code>x_ones = torch.ones(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#torchrand","title":"<code>torch.rand</code>","text":"<pre><code>x_rand = torch.rand(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#2-tensor-from-a-python-list","title":"2. Tensor from a Python List","text":"<pre><code>data = [[1,2,3],[4,5,6]]\nx_from_list = torch.tensor(data)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#3-tensor-from-numpy-array","title":"3. Tensor from NumPy Array","text":"<pre><code>np_array = np.array([[1,2,3],[4,5,6]])\nx_from_np = torch.from_numpy(np_array)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#4-shape-batch-dimension","title":"4. Shape &amp; Batch Dimension","text":"<pre><code>images = torch.rand(32,3,64,64)\nbatch = images.shape[0]\nsample = images.shape[1:]\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#5-dtype","title":"5. Dtype","text":"<pre><code>torch.zeros(2,2,dtype=torch.float32)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#6-reshaping","title":"6. Reshaping","text":""},{"location":"tools/pytorch/1_pytorch/#unsqueeze","title":"Unsqueeze","text":"<pre><code>x = torch.tensor([1,2,3,4])\nx.unsqueeze(0)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#squeeze","title":"Squeeze","text":"<pre><code>y = torch.rand(1,3,1,4)\ny.squeeze()\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#7-slicing","title":"7. Slicing","text":"<pre><code>x = torch.arange(12).reshape(3,4)\nx[0]\nx[:,1]\nx[0:2,1:3]\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#8-item","title":"8. .item()","text":"<pre><code>loss = torch.tensor(3.14)\nloss.item()\n</code></pre>"},{"location":"tools/pytorch/2_broadcasting/","title":"Broadcasting in NumPy","text":""},{"location":"tools/pytorch/2_broadcasting/#broadcasting-in-numpy","title":"Broadcasting in NumPy","text":"<p>Broadcasting is a set of rules that NumPy uses to let arrays with different shapes work together in arithmetic operations. When shapes don't match, NumPy aligns dimensions from the right (the trailing dimensions) and tries to stretch dimensions of size <code>1</code> so the arrays become compatible.</p> <ul> <li>A dimension can broadcast if it matches or is <code>1</code>.</li> <li>If two dimensions differ and neither is <code>1</code>, broadcasting fails.</li> </ul>"},{"location":"tools/pytorch/2_broadcasting/#why-align-from-the-right","title":"Why \"align from the right\"?","text":"<p>NumPy compares array shapes starting from the rightmost dimension, since those describe the element-level structure.</p> <p>Example of right alignment:</p> <pre><code>Array A shape:      (5, 1)\nArray B shape:          (5,)\n                     --------\nAligned shapes:    (5, 1)\n                    (1, 5)\n</code></pre>"},{"location":"tools/pytorch/2_broadcasting/#simple-example","title":"Simple Example","text":"<pre><code>import numpy as np\n\nA = np.array([[10],\n              [20],\n              [30]])   # shape (3,1)\n\nB = np.array([1, 2, 3])  # shape (3,)\n\n# Broadcasting:\n# A becomes (3,3) by repeating its single column\n# B becomes (1,3) by repeating its single row\nprint(A + B)\n</code></pre> <p>Output:</p> <pre><code>[[11 12 13]\n [21 22 23]\n [31 32 33]]\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/","title":"PyTorch Data Utilities: Transforms, Datasets, and DataLoaders","text":""},{"location":"tools/pytorch/3_pytorch_datautilities/#pytorch-data-utilities-transforms-datasets-and-dataloaders","title":"PyTorch Data Utilities: Transforms, Datasets, and DataLoaders","text":"<p>Efficient data handling is a core part of every machine learning workflow. PyTorch provides a clean, modular data pipeline built around three utilities: Transforms, Datasets, and DataLoaders. Together, they make it easy to prepare data, apply preprocessing, and feed batches to your model during training.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#1-transforms","title":"1. Transforms","text":"<p>Transforms perform preprocessing operations on input data. They are typically used to convert raw samples (images, text, audio, etc.) into tensors and normalize them for training.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#common-uses","title":"Common Uses","text":"<ul> <li>Convert images to tensors\\</li> <li>Normalize pixel values\\</li> <li>Data augmentation (flip, crop, rotate)\\</li> <li>Compose multiple preprocessing steps</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#underlying-idea","title":"Underlying Idea","text":"<p>Transforms are callable objects. A transform takes one sample as input and returns a modified sample.</p> <pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips","title":"Practitioner Tips","text":"<ul> <li>Use <code>transforms.Compose()</code> to chain steps.</li> <li>Apply data augmentation only on the training set.</li> <li>Keep normalization consistent with the pretrained models you use.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#2-datasets","title":"2. Datasets","text":"<p>A Dataset is a wrapper around your data. It tells PyTorch how to access one sample at a time. Every custom dataset must implement two methods:</p> <ul> <li><code>__len__</code> \u2192 returns dataset size\\</li> <li><code>__getitem__</code> \u2192 returns one sample at index i</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#example-custom-image-dataset","title":"Example: Custom Image Dataset","text":"<pre><code>from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass MyImages(Dataset):\n    def __init__(self, filepaths, labels, transform=None):\n        self.filepaths = filepaths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.filepaths[idx])\n        label = self.labels[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips_1","title":"Practitioner Tips","text":"<ul> <li>Use <code>torchvision.datasets</code> for standard datasets (CIFAR, MNIST,     ImageNet).</li> <li>Load files inside <code>__getitem__</code>, not beforehand.</li> <li>Keep transforms inside the dataset to ensure consistent     preprocessing.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#3-dataloader","title":"3. DataLoader","text":"<p>A DataLoader wraps a Dataset and helps you iterate through the data efficiently by:</p> <ul> <li>batching samples\\</li> <li>shuffling\\</li> <li>loading data in parallel with worker processes</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#example","title":"Example","text":"<pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4\n)\n\nfor images, labels in train_loader:\n    pass\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#underlying-idea_1","title":"Underlying Idea","text":"<p>The DataLoader retrieves indices from the Dataset, loads samples, applies batching, and returns them in iterable form.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips_2","title":"Practitioner Tips","text":"<ul> <li>Set <code>shuffle=True</code> for training; <code>False</code> for evaluation.</li> <li>Increase <code>num_workers</code> to speed up loading (based on CPU cores).</li> <li>Use <code>pin_memory=True</code> when training on GPU.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#putting-it-all-together","title":"Putting It All Together","text":"<pre><code>transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()\n])\n\ndataset = MyImages(filepaths, labels, transform=transform)\n\nloader = DataLoader(dataset, batch_size=64, shuffle=True)\n</code></pre> <p>This structure cleanly separates: - how data is processed (Transforms) - how data is accessed (Dataset) - how data is delivered to the model (DataLoader)</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#conclusion","title":"Conclusion","text":"<p>PyTorch's data utilities provide a flexible and powerful way to prepare data for deep learning workflows. Understanding these components enables practitioners to build efficient, scalable training pipelines.</p>"},{"location":"tools/pytorch/4_pytorch_device_management/","title":"Understanding Device Management in PyTorch: A Practical Guide","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#understanding-device-management-in-pytorch-a-practical-guide","title":"Understanding Device Management in PyTorch: A Practical Guide","text":"<p>When working with tensors and neural networks in PyTorch, understanding device management is essential. Every tensor and model parameter lives on a device---either the CPU or an accelerator like a GPU. PyTorch does not automatically move your data or model between devices. If your tensors and model are not on the same device, your program may crash with errors such as:</p> <p>RuntimeError: Expected all tensors to be on the same device...</p>"},{"location":"tools/pytorch/4_pytorch_device_management/#in-this-article-youll-learn-how-to-properly-manage-devices-avoid-common-beginner-errors-and-build-reliable-training-loops","title":"In this article, you'll learn how to properly manage devices, avoid common beginner errors, and build reliable training loops.","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#why-devices-matter","title":"Why Devices Matter","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#cpu","title":"CPU","text":"<ul> <li>Default device in PyTorch.</li> <li>Handles general-purpose computations.</li> <li>Runs operations sequentially.</li> <li>Slower for large deep learning workloads.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#gpu-cuda","title":"GPU (CUDA)","text":"<ul> <li>Parallel accelerator.</li> <li>Can train models 10--15\u00d7 faster than CPU.</li> <li>Essential for scaling deep learning.</li> <li>Must explicitly move data and models to this device.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#checking-for-gpu-availability","title":"Checking for GPU Availability","text":"<p>PyTorch provides a simple API to verify whether a GPU is available:</p> <pre><code>torch.cuda.is_available()\n</code></pre> <p>A common pattern for selecting a device is:</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <ul> <li><code>\"cuda\"</code> \u2192 NVIDIA GPU.</li> <li><code>\"cpu\"</code> \u2192 fallback if no GPU\\</li> <li>Other options exist (e.g., <code>\"mps\"</code> for Apple Silicon), but CUDA is still the standard in most PyTorch workflows.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#moving-models-and-data-to-a-device","title":"Moving Models and Data to a Device","text":"<p>Once you choose a device, you must manually move:</p> <ol> <li>Your model</li> <li>Your input tensors</li> <li>Your target labels</li> </ol>"},{"location":"tools/pytorch/4_pytorch_device_management/#move-the-model","title":"Move the model:","text":"<pre><code>model = MyModel().to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#move-the-batch-data-inside-the-training-loop","title":"Move the batch data inside the training loop:","text":"<pre><code>inputs = inputs.to(device)\ntargets = targets.to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#check-where-something-lives","title":"Check where something lives","text":"<pre><code>tensor.device\nnext(model.parameters()).device\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#important-to-does-not-modify-in-place","title":"Important: <code>.to()</code> Does NOT Modify in Place","text":"<p>A very common mistake is:</p> <pre><code>inputs.to(device)  # WRONG \u2192 result is discarded\n</code></pre> <p>You must assign it:</p> <pre><code>inputs = inputs.to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#training-loop-with-proper-device-management","title":"Training Loop With Proper Device Management","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MyModel().to(device)\noptimizer = optim.Adam(model.parameters())\nloss_function = nn.CrossEntropyLoss()\n\nfor inputs, targets in dataloader:\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#avoiding-gpu-memory-errors","title":"Avoiding GPU Memory Errors","text":"<p>Even when everything is set correctly, you might encounter:</p> <p>RuntimeError: CUDA out of memory</p> <p>This occurs when your model or batch size requires more memory than the GPU has.</p>"},{"location":"tools/pytorch/4_pytorch_device_management/#how-to-fix-it","title":"How to fix it","text":"<ul> <li>Lower your batch size first (the most common fix)</li> <li>Reduce image resolution\\</li> <li>Use <code>torch.cuda.empty_cache()</code> if necessary\\</li> <li>Try gradient accumulation</li> </ul> <p>For many systems: - Batch size 32--64 is a good starting point</p>"},{"location":"tools/pytorch/5_datapipelines/","title":"Building a Custom PyTorch Dataset for Oxford Flowers","text":""},{"location":"tools/pytorch/5_datapipelines/#building-a-custom-pytorch-dataset-for-oxford-flowers","title":"Building a Custom PyTorch Dataset for Oxford Flowers","text":"<p>Real-world datasets rarely come neatly packaged like MNIST or CIFAR. The Oxford Flowers dataset is a perfect example of messy data:</p> <ul> <li>Images are stored in a flat folder: <code>image0001.jpg</code>,     <code>image0002.jpg</code>, ...</li> <li>Labels are stored separately in a MATLAB <code>.mat</code> file</li> <li>Labels start at 1, but PyTorch expects 0-based labels</li> <li>There's no built-in PyTorch dataset for it</li> </ul> <p>To make this dataset usable for training, we need to build a custom PyTorch <code>Dataset</code> class.</p> <p>A PyTorch <code>Dataset</code> only needs to answer three questions:</p> <ol> <li><code>__init__</code> --- How do I set up the dataset?</li> <li><code>__len__</code> --- How many samples are there?</li> <li><code>__getitem__</code> --- Given an index <code>i</code>, what image and label     should I return? --</li> </ol>"},{"location":"tools/pytorch/5_datapipelines/#key-concepts","title":"Key Concepts","text":""},{"location":"tools/pytorch/5_datapipelines/#light-setup-__init__","title":"\u2714 Light setup (<code>__init__</code>)","text":"<ul> <li>Store image folder path</li> <li>Load labels from <code>.mat</code></li> <li>Fix label indexing from 1--102 to 0--101</li> <li>But do not load images here</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#lazy-loading-__getitem__","title":"\u2714 Lazy loading (<code>__getitem__</code>)","text":"<ul> <li>Load only the image you need</li> <li>Convert it with PIL</li> <li>Return image + label</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#simple-length-__len__","title":"\u2714 Simple length (<code>__len__</code>)","text":"<ul> <li>Just return number of samples (labels)</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#example-implementation-oxfordflowersdataset","title":"Example Implementation: <code>OxfordFlowersDataset</code>","text":"<pre><code>import os\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport scipy.io as sio   # pip install scipy\n\n\nclass OxfordFlowersDataset(Dataset):\n    def __init__(self, root_dir, labels_mat_file, transform=None):\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n\n        mat = sio.loadmat(labels_mat_file)\n        raw_labels = mat[\"labels\"].squeeze()\n        self.labels = raw_labels.astype(\"int64\") - 1\n        self.num_samples = len(self.labels)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.item()\n\n        filename = f\"image{idx + 1:04d}.jpg\"\n        img_path = self.root_dir / filename\n\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n</code></pre>"},{"location":"tools/pytorch/5_datapipelines/#using-the-dataset-with-a-dataloader","title":"Using the Dataset with a DataLoader","text":"<pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndataset = OxfordFlowersDataset(\n    root_dir=\"path/to/jpg\",\n    labels_mat_file=\"path/to/labels.mat\",\n    transform=transform,\n)\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n)\n\nimages, labels = next(iter(dataloader))\n\nprint(\"Batch image shape:\", images.shape)\nprint(\"Batch labels:\", labels[:10])\n</code></pre>"},{"location":"tools/pytorch/6_transform/","title":"PyTorch Image Transformations: Fixing Real-World Data Quality Issues","text":""},{"location":"tools/pytorch/6_transform/#pytorch-image-transformations-fixing-real-world-data-quality-issues","title":"PyTorch Image Transformations: Fixing Real-World Data Quality Issues","text":"<p>In the previous article, you built a custom <code>Dataset</code> class for the Oxford Flowers dataset. Now it's time to deal with the second major challenge: raw images are messy.</p> <p>Real-world image datasets rarely come in the exact format a model expects. Common problems:</p> <ul> <li>Images have different sizes</li> <li>Images come as PIL objects, but PyTorch models expect     tensors</li> <li>Pixel values range from 0--255, which can destabilize training</li> <li>Color distributions vary dramatically, requiring normalization</li> </ul> <p>This article walks through PyTorch's powerful transformation pipeline, explaining how each transform works, why order matters, and how to debug issues.</p>"},{"location":"tools/pytorch/6_transform/#why-your-dataset-breaks-without-transforms","title":"Why Your Dataset Breaks Without Transforms","text":"<p>If you try to load Oxford Flowers with a DataLoader as-is, you'll likely see this error:</p> <pre><code>RuntimeError: stack expects each tensor to be equal size...\n</code></pre> <p>Why?</p>"},{"location":"tools/pytorch/6_transform/#images-have-different-sizes","title":"\u2714 Images have different sizes","text":"<p>PyTorch wants batches shaped like:</p> <pre><code>(batch_size, channels, height, width)\n</code></pre> <p>If height/width differ, tensors can't be stacked \u2192 crash.</p>"},{"location":"tools/pytorch/6_transform/#images-are-pil-objects","title":"\u2714 Images are PIL objects","text":"<p>Your model expects tensors, not PIL images.</p> <p>So you need to fix both format and shape.</p>"},{"location":"tools/pytorch/6_transform/#pytorch-transformations-to-the-rescue","title":"PyTorch Transformations to the Rescue","text":"<p>PyTorch's <code>torchvision.transforms</code> lets you chain image processing stepscusing <code>Compose</code>.</p> <p>Two major issues to solve:</p> <ol> <li>Different image sizes</li> <li>Wrong data format (PIL instead of tensor)</li> </ol>"},{"location":"tools/pytorch/6_transform/#1-fixing-size-mismatches","title":"1. Fixing Size Mismatches","text":""},{"location":"tools/pytorch/6_transform/#bad-approach","title":"Bad approach","text":"<pre><code>transforms.Resize((224, 224))\n</code></pre> <p>This forces the image to exactly 224\u00d7224 \u2192 stretches rectangular photos.</p>"},{"location":"tools/pytorch/6_transform/#better-approach","title":"Better approach","text":"<pre><code>transforms.Resize(256)       # resizes shortest edge, preserves aspect ratio\ntransforms.CenterCrop(224)   # extract a clean square\n</code></pre> <p>This avoids distortion.</p>"},{"location":"tools/pytorch/6_transform/#2-converting-images-to-tensors","title":"2. Converting Images to Tensors","text":"<p><code>ToTensor()</code> does more than people realize.</p>"},{"location":"tools/pytorch/6_transform/#what-totensor-does","title":"What <code>ToTensor()</code> does:","text":"<ul> <li>Converts PIL \u2192 Tensor\\</li> <li>Rearranges dimensions to <code>(C, H, W)</code></li> <li>Scales pixel values from 0--255 \u2192 0--1</li> </ul> <p>Example:</p> <pre><code>img = Image.open(\"flower.jpg\")\nt = transforms.ToTensor()(img)\n\n# PIL: (224, 224)\n# Tensor: (3, 224, 224)\n# Values: 0.0\u20131.0\n</code></pre> <p>This scaling stabilizes training because:</p> <ul> <li>Values share a consistent scale\\</li> <li>Prevents exploding activations</li> </ul>"},{"location":"tools/pytorch/6_transform/#3-normalization","title":"3. Normalization","text":"<p>Even after scaling to 0--1, your dataset may have:</p> <ul> <li>mostly bright flowers \u2192 values cluster near 1\\</li> <li>mostly dark backgrounds \u2192 values cluster near 0</li> </ul> <p>Normalization spreads values out and helps the model learn subtlecdetail.</p> <p>Typical ImageNet-style normalization:</p> <pre><code>transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n)\n</code></pre> <p>\u26a0 Normalize only works on tensors, never PIL images.</p>"},{"location":"tools/pytorch/6_transform/#understanding-transform-order","title":"Understanding Transform Order","text":"<p>Transformation pipelines happen in sequence.</p> <p>Think of <code>ToTensor</code> as a bridge:</p> <p>Before <code>ToTensor</code>     After <code>ToTensor</code></p> <p>Works on PIL images   Works on tensors   Resize, crop, flip    Normalize</p> <p>In modern TorchVision, many image ops work on both sides, but some don't.</p>"},{"location":"tools/pytorch/6_transform/#complete-transform-pipeline-for-oxford-flowers","title":"Complete Transform Pipeline for Oxford Flowers","text":"<pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(256),            # keep aspect ratio\n    transforms.CenterCrop(224),        # square crop\n    transforms.ToTensor(),             # PIL \u2192 Tensor, scale to 0\u20131\n    transforms.Normalize(               # improve training stability\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n</code></pre> <p>Add this when creating your dataset:</p> <pre><code>dataset = OxfordFlowersDataset(\n    root_dir=\"path/to/jpg\",\n    labels_mat_file=\"path/to/labels.mat\",\n    transform=transform\n)\n</code></pre>"},{"location":"tools/pytorch/6_transform/#debugging-transform-pipelines","title":"Debugging Transform Pipelines","text":"<p>A life-saving technique:</p>"},{"location":"tools/pytorch/6_transform/#1-pull-a-single-transformed-image","title":"1. Pull a single transformed image","text":"<pre><code>img, label = dataset[42]\nprint(img.shape)\n</code></pre>"},{"location":"tools/pytorch/6_transform/#2-pull-a-raw-image","title":"2. Pull a raw image","text":"<pre><code>raw_img = Image.open(\"path/to/image0043.jpg\")\n</code></pre>"},{"location":"tools/pytorch/6_transform/#3-apply-transforms-one-by-one","title":"3. Apply transforms one by one","text":"<pre><code>step1 = transforms.Resize(256)(raw_img)\nstep2 = transforms.CenterCrop(224)(step1)\nstep3 = transforms.ToTensor()(step2)\nstep4 = transforms.Normalize(...)(step3)\n</code></pre> <p>This lets you catch:</p> <ul> <li>Distorted images\\</li> <li>Wrong order bugs\\</li> <li>Type mismatches (e.g., Normalize on PIL)</li> </ul> <p>Debugging transforms this way saves hours.</p>"},{"location":"tools/pytorch/7_dataloader/","title":"PyTorch Data Splitting &amp; DataLoader: Building a Reliable Training Pipeline","text":""},{"location":"tools/pytorch/7_dataloader/#pytorch-data-splitting-dataloader-building-a-reliable-training-pipeline","title":"PyTorch Data Splitting &amp; DataLoader: Building a Reliable Training Pipeline","text":"<p>In the previous article, we transformed and prepared your images for model consumption. Now, it's time to tackle two critical steps in every machine learning workflow:</p> <ul> <li>Splitting  dataset into  training ,  validation , and  test  sets  </li> <li>Using  DataLoader  to efficiently serve your data in  batches </li> </ul> <p>These steps may feel like simple logistics, but they directly affect model performance, fairness, and reliability.</p>"},{"location":"tools/pytorch/7_dataloader/#why-not-train-on-the-whole-dataset","title":"Why Not Train on the Whole Dataset?","text":"<p>If more data is better, why don\u2019t we train using  all  flower images?</p> <p>Because training accuracy tells you only how well your model memorizes. It does  not  tell you how well it  generalizes  to new images.</p> <p>That\u2019s why we split the data:</p> Split Purpose Training set Used to teach the model Validation set Used during training to tune hyperparameters &amp; detect overfitting Test set Used once at the very end to report final unbiased performance <p>This split ensures your model learns  patterns , not  memorizes answers .</p>"},{"location":"tools/pytorch/7_dataloader/#splitting-the-dataset-in-pytorch","title":"Splitting the Dataset in PyTorch","text":"<p>Use <code>random_split()</code> to ensure a  random and balanced  distribution of flower types.</p> <p>Example for Oxford Flowers (8,189 images):</p> <pre><code>train_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_set, val_set, test_set = torch.utils.data.random_split(\n    dataset, [train_size, val_size, test_size]\n)\n</code></pre> <p>This ensures: - No rounding errors - All images are used exactly once  </p> <p>Your  original dataset remains unchanged  \u2014 you\u2019re only creating views.</p>"},{"location":"tools/pytorch/7_dataloader/#dataloader-efficiently-serving-data-in-batches","title":"DataLoader: Efficiently Serving Data in Batches","text":"<p>A <code>Dataset</code> gives access to  one sample at a time . A <code>DataLoader</code> groups data into  batches , making training much faster on GPU.</p> <pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_set, batch_size=32, shuffle=False)\ntest_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n</code></pre>"},{"location":"tools/pytorch/7_dataloader/#why-shuffle-only-in-training","title":"Why Shuffle Only in Training?","text":"Split shuffle? Why? Training \u2714 Yes Mixes classes to avoid bias &amp; forgetting Validation \u274c No Does not affect evaluation Test \u274c No Ensures deterministic, repeatable results <p>Shuffling prevents your model from mistakenly learning:</p> <p>\"Daisies always come first, then roses.\"</p> <p>Instead, it learns image  patterns , not  order .</p>"},{"location":"tools/pytorch/7_dataloader/#inspecting-batches","title":"Inspecting Batches","text":"<p>A single batch contains:</p> <ul> <li>A tensor of 32 images \u2192 <code>(32, 3, 224, 224)</code></li> <li>A tensor of 32 labels \u2192 <code>(32,)</code></li> </ul> <p>Verify:</p> <pre><code>images, labels = next(iter(train_loader))\nprint(images.shape)   # torch.Size([32, 3, 224, 224])\nprint(labels.shape)   # torch.Size([32])\n</code></pre>"},{"location":"tools/pytorch/7_dataloader/#batches-remainders-epochs-explained","title":"Batches, Remainders &amp; Epochs Explained","text":"<p>If you have 5,732 training images and a batch size of 32:</p> <pre><code>5732 / 32 = 179 full batches + 1 partial batch (4 images)\n</code></pre> <p>So an  epoch  = 180 batches.</p> <p>PyTorch still uses that last small batch \u2014 that\u2019s  normal and expected .</p>"},{"location":"tools/pytorch/7_dataloader/#common-mistake-reloading-data-in-__getitem__","title":"Common Mistake: Reloading Data in <code>__getitem__</code>","text":"<pre><code>def __getitem__(self, idx):\n    df = pd.read_csv(\"labels.csv\")  # \u274c Very expensive!\n</code></pre> <p>This reloads the whole file  thousands of times per epoch .</p> <p>\u2714 Instead: Load once in <code>__init__</code></p> <pre><code>def __init__(self, csv_path):\n    self.labels = pd.read_csv(csv_path)  # loaded once\n</code></pre> <p>This change alone can speed training up by  10\u00d7 or more .</p>"},{"location":"tools/pytorch/7_dataloader/#if-you-get-cuda-out-of-memory-errors","title":"If You Get CUDA Out of Memory Errors\u2026","text":"<p>Try reducing batch size before anything else:</p> <pre><code>train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n</code></pre> <p>Then gradually increase until you find the optimal value for your GPU.</p>"},{"location":"tools/pytorch/7_dataloader/#complete-data-pipeline-example","title":"Complete Data Pipeline Example","text":"<pre><code>train_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\nfor images, labels in train_loader:\n    print(images.shape, labels.shape)\n    break\n</code></pre>"},{"location":"tools/pytorch/8_augmentation/","title":"Making Your PyTorch Pipeline Robust: Augmentation, Error Handling &amp; Monitoring","text":""},{"location":"tools/pytorch/8_augmentation/#making-your-pytorch-pipeline-robust-augmentation-error-handling-monitoring","title":"Making Your PyTorch Pipeline Robust: Augmentation, Error Handling &amp; Monitoring","text":"<p>Real-world datasets are messy, unpredictable, and full of surprises. In this article, you'll learn how to make your pipeline:</p> <ul> <li>More  robust  (able to handle corrupted or problematic images)  </li> <li>More  generalizable  (able to perform well in real-world conditions)  </li> <li>More  visible  (able to monitor access patterns and detect issues early)</li> </ul>"},{"location":"tools/pytorch/8_augmentation/#1-making-your-model-more-robust-with-data-augmentation","title":"1. Making Your Model More Robust with Data Augmentation","text":"<p>So far, your model has only seen ideal flower images\u2014centered, well-lit, and clean. But real-world images vary in:</p> <ul> <li>Lighting  </li> <li>Orientation  </li> <li>Backgrounds  </li> <li>Perspective  </li> <li>Positioning  </li> </ul> <p>That's where  data augmentation  helps.</p>"},{"location":"tools/pytorch/8_augmentation/#old-approach-inefficient","title":"Old Approach (Inefficient)","text":"<p>Save rotated, flipped, or brightened copies as physical files.</p>"},{"location":"tools/pytorch/8_augmentation/#smarter-solution-on-the-fly-augmentation","title":"Smarter Solution: On-the-Fly Augmentation","text":"<p>PyTorch applies random transformations  each time  an image is loaded.</p> <pre><code>from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n</code></pre>"},{"location":"tools/pytorch/8_augmentation/#why-no-augmentation-in-validation","title":"Why no augmentation in validation?","text":"<p>Because validation should reflect  real-world performance , not random alterations.</p>"},{"location":"tools/pytorch/8_augmentation/#single-batch-test-catch-problems-before-training","title":"Single Batch Test \u2014 Catch Problems Before Training","text":"<p>Before training for hours, verify that:</p> <ul> <li>Shapes are correct  </li> <li>Transformations work  </li> <li>Labels are valid  </li> <li>No file crashes the loader  </li> </ul> <pre><code>images, labels = next(iter(train_loader))\nprint(\"Images shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)\n</code></pre> <p>Expected output:</p> <pre><code>Images shape: torch.Size([32, 3, 224, 224])\nLabels shape: torch.Size([32])\n</code></pre> <p>Tip: Test this before every major model change.</p>"},{"location":"tools/pytorch/8_augmentation/#3-error-handling-dont-let-one-bad-image-crash-training","title":"3. Error Handling \u2014 Don\u2019t Let One Bad Image Crash Training","text":"<p>Real-world datasets contain:</p> <ul> <li>Corrupted files  </li> <li>Extremely small images  </li> <li>Grayscale images  </li> <li>Unsupported formats  </li> </ul> <p>Here\u2019s how to make <code>__getitem__</code> resilient:</p> <pre><code>from PIL import Image\n\ndef __getitem__(self, idx):\n    img_path = self.image_paths[idx]\n\n    try:\n        img = Image.open(img_path)\n        img.verify()        # file is not corrupted\n        img = Image.open(img_path)  # reopen\n        img = img.convert(\"RGB\")\n\n        if img.size[0] &lt; 100 or img.size[1] &lt; 100:\n            raise ValueError(\"Image too small\")\n\n        if self.transform:\n            img = self.transform(img)\n\n    except Exception as e:\n        print(f\"\u26a0 Error loading {img_path}: {e}\")\n        return self.__getitem__((idx + 1) % len(self))\n\n    return img, self.labels[idx]\n</code></pre> <p>\ud83d\udee1 Your pipeline now  skips  invalid files instead of crashing.</p>"},{"location":"tools/pytorch/8_augmentation/#4-visualizing-augmentations-are-they-too-strong","title":"4. Visualizing Augmentations \u2014 Are They Too Strong?","text":"<p>Overly aggressive augmentations can destroy key features.</p> <pre><code>def reverse_normalize(tensor):\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    return tensor * std + mean\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 4, figsize=(12,6))\nfor i in range(8):\n    img, _ = dataset[i]\n    img = reverse_normalize(img).permute(1,2,0)\n    axes[i//4, i%4].imshow(img)\n    axes[i//4, i%4].axis('off')\nplt.show()\n</code></pre> <p>Look for: | Result | Meaning | |--------|---------| | Recognizable flowers | \ud83d\udc4d Good augmentation | | No change | \u274c Too weak | | Abstract blobs | \u26a0 Too strong | | Black/wild colors | \u26a0 Normalization mistake |</p>"},{"location":"tools/pytorch/8_augmentation/#5-monitoring-dataset-usage-see-whats-happening-during-training","title":"5. Monitoring Dataset Usage \u2014 See What's Happening During Training","text":"<p>Your data might  look fine , but hidden problems include:</p> <ul> <li>Some images never used (shuffling bug)  </li> <li>Certain images used too often (bias)  </li> <li>Extremely slow load times  </li> <li>Poor variability in samples  </li> </ul>"},{"location":"tools/pytorch/8_augmentation/#add-lightweight-tracking","title":"Add lightweight tracking:","text":"<pre><code>from time import time\n\nself.access_count = {}\nself.load_times = []\n\ndef __getitem__(self, idx):\n    start = time()\n    img, label = load_image(idx)  # existing logic\n    self.load_times.append(time() - start)\n\n    self.access_count[idx] = self.access_count.get(idx, 0) + 1\n    return img, label\n\ndef print_stats(self):\n    print(\"Average load time:\", sum(self.load_times)/len(self.load_times))\n    print(\"Most accessed:\", sorted(self.access_count.items(), key=lambda x: x[1], reverse=True)[:5])\n</code></pre> <p>Call <code>print_stats()</code> at the end of each epoch to track issues early.</p>"},{"location":"tools/pytorch/9_hyperparamtersearch/","title":"9 hyperparamtersearch","text":""},{"location":"tools/pytorch/9_hyperparamtersearch/#optuna-tree-structured-parzen-estimator","title":"Optuna - Tree Structured Parzen Estimator","text":""}]}