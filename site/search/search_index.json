{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p> <ul> <li>Concise derivations of essential results</li> <li>Geometric intuition and figures where helpful</li> <li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li> <li>Appendices that extend into more advanced or specialized topics</li> </ul> <p>Whether you\u2019re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p>"},{"location":"appendices/120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"appendices/120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"appendices/120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"appendices/120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"appendices/120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"appendices/120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"appendices/120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"appendices/120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"appendices/120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"appendices/120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"appendices/130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"appendices/140_support/","title":"Appendix C - Support Functions and Dual Geometry (Advanced)","text":""},{"location":"appendices/140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"appendices/140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"appendices/140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"appendices/140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"appendices/140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"appendices/140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"appendices/140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"appendices/140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"appendices/160_conjugates/","title":"Appendix D - Convex Conjugates and Fenchel Duality (Advanced)","text":""},{"location":"appendices/160_conjugates/#appendix-d-convex-conjugates-and-fenchel-duality","title":"Appendix D: Convex Conjugates and Fenchel Duality","text":"<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis. They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p> <ul> <li>Convex conjugates convert a function into its \u201cslope-space\u201d representation \u2014 capturing its tightest linear overestimates.  </li> <li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li> </ul> <p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>"},{"location":"appendices/160_conjugates/#d1-intuitive-picture","title":"D.1 Intuitive Picture","text":"<p>Imagine a convex function \\(f(x)\\) drawn as a bowl in space. Each point \\(y\\) defines a line (or hyperplane) of slope \\(y\\):  The convex conjugate \\(f^*(y)\\) is the smallest height \\(b\\) such that this line always stays above \\(f(x)\\). In other words:</p> <p>\\(f^*(y)\\) measures the tightest linear overestimate of \\(f\\) in direction \\(y\\).</p> <p>So \\(f^*\\) encodes how \u201csteep\u201d \\(f\\) can be in every direction \u2014 it transforms the geometry of \\(f\\) into a new convex function on slope-space.</p>"},{"location":"appendices/160_conjugates/#d2-definition-and-key-properties","title":"D.2 Definition and Key Properties","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}\\) be a proper convex function. Its convex (Fenchel) conjugate is  </p> <p>Interpretation - \\(y\\): a slope or linear functional. - The supremum seeks the largest gap between the linear function \\(\\langle y,x\\rangle\\) and the graph of \\(f\\). - \\(f^*(y)\\) is always convex, even if \\(f\\) isn\u2019t strictly convex.</p>"},{"location":"appendices/160_conjugates/#fundamental-identities","title":"Fundamental Identities","text":"<ol> <li> <p>Fenchel\u2013Young inequality        with equality iff \\(y \\in \\partial f(x)\\).</p> </li> <li> <p>Biconjugation        This tells us the conjugate transform loses no information for convex functions.</p> </li> <li> <p>Order reversal    \\(f \\le g \\;\\Rightarrow\\; f^* \\ge g^*\\).</p> </li> <li> <p>Scaling and shift</p> </li> <li>\\((f + a)^*(y) = f^*(y) - a\\),</li> <li>\\((\\alpha f)^*(y) = \\alpha f^*(y/\\alpha)\\) for \\(\\alpha&gt;0.\\)</li> </ol>"},{"location":"appendices/160_conjugates/#d3-canonical-examples","title":"D.3 Canonical Examples","text":"Function \\(f(x)\\) Conjugate \\(f^*(y)\\) Notes \\( \\tfrac{1}{2}\\|x\\|_2^2 \\) \\( \\tfrac{1}{2}\\|y\\|_2^2 \\) Self-conjugate quadratic \\( \\|x\\|_1 \\) \\( \\delta_{\\{\\|y\\|_\\infty \\le 1\\}}(y) \\) Dual norm indicator \\( \\delta_C(x) \\) \\( \\sigma_C(y)=\\sup_{x\\in C}\\langle y,x\\rangle \\) Support function of set \\(C\\) \\( e^x \\) \\( y\\log y - y,\\, y&gt;0 \\) Appears in entropy and KL-divergence <p>These examples illustrate how conjugation connects: - Norms \u2194 dual norms, - Sets \u2194 support functions, - Exponentials \u2194 entropy, - Quadratics \u2194 themselves.</p>"},{"location":"appendices/160_conjugates/#d4-geometric-interpretation","title":"D.4 Geometric Interpretation","text":"<ul> <li>Each point on \\(f\\) has a tangent hyperplane whose slope is a subgradient.  </li> <li>The collection of all such hyperplanes forms the epigraph of \\(f^*\\).  </li> <li>The transformation \\(f \\mapsto f^*\\) swaps the roles of \u201cposition\u201d and \u201cslope\u201d:   convex geometry \u2194 supporting hyperplanes.</li> </ul> <p>Visually: - \\(f\\) describes a bowl in \\((x,t)\\)-space. - \\(f^*\\) describes the envelope of tangent planes to that bowl.</p>"},{"location":"appendices/160_conjugates/#d5-from-conjugates-to-duality-fenchel-duality","title":"D.5 From Conjugates to Duality \u2014 Fenchel Duality","text":"<p>Many convex optimization problems can be written as  where \\(f,g\\) are convex and \\(A\\) is linear. Fenchel duality uses conjugates to build a dual problem in terms of \\(f^*\\) and \\(g^*\\).</p>"},{"location":"appendices/160_conjugates/#the-fenchel-dual-problem","title":"The Fenchel Dual Problem","text":"\\[ \\max_y \\; -f^*(A^\\top y) - g^*(-y). \\] <p>Interpretation - \\(y\\) is the dual variable (similar to Lagrange multipliers). - The dual objective collects the best linear lower bounds on the primal cost.</p>"},{"location":"appendices/160_conjugates/#d6-weak-and-strong-duality","title":"D.6 Weak and Strong Duality","text":"<ul> <li> <p>Weak duality: For any \\(x,y\\),      So the dual value always underestimates the primal value.</p> </li> <li> <p>Strong duality:   If \\(f,g\\) are closed convex and a mild constraint qualification holds (e.g. Slater\u2019s condition \u2014 existence of strictly feasible \\(x\\)), then    </p> </li> </ul> <p>At the optimum:  These are the Fenchel\u2013KKT conditions, directly linking primal and dual subgradients.</p>"},{"location":"appendices/160_conjugates/#d7-illustrative-examples","title":"D.7 Illustrative Examples","text":""},{"location":"appendices/160_conjugates/#a-linear-programming","title":"(a) Linear Programming","text":"<p>Primal:  </p> <p>Take \\(f(x) = c^\\top x + \\delta_{\\{x\\ge0\\}}(x)\\), \\(g(z)=\\delta_{\\{z=b\\}}(z)\\).</p> <p>Then  </p> <p>Dual:  which is the standard LP dual.</p>"},{"location":"appendices/160_conjugates/#b-quadratic-set-constraint","title":"(b) Quadratic + Set Constraint","text":"<p>Primal:  </p> <p>Then  so the dual is  Optimality gives \\(x^*=y^*\\), the projection condition in Euclidean geometry.</p>"},{"location":"appendices/160_conjugates/#d8-practical-significance","title":"D.8 Practical Significance","text":"Area How Fenchel Duality Appears Optimization theory Derives general dual problems beyond inequality constraints. Algorithm design Basis for primal\u2013dual and splitting methods (ADMM, Chambolle\u2013Pock, Mirror Descent). Geometry Dual problem finds the \u201cbest supporting hyperplane\u201d to the primal epigraph. Machine Learning Loss\u2013regularizer pairs (hinge \u2194 clipped loss, logistic \u2194 log-sum-exp) often form conjugate pairs. Proximal operators Linked via Moreau identity:  \\(\\mathrm{prox}_{f^*}(y) = y - \\mathrm{prox}_f(y)\\)."},{"location":"appendices/160_conjugates/#d9-conceptual-unification","title":"D.9 Conceptual Unification","text":"<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p> <ul> <li>From geometry: support functions, projections, subgradients (Appendices B\u2013C).  </li> <li>From analysis: inequalities like Fenchel\u2019s and Jensen\u2019s (Appendix A).  </li> <li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7\u20138).  </li> <li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9\u201310).</li> </ul> <p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>"},{"location":"appendices/160_conjugates/#d10-summary-and-takeaways","title":"D.10 Summary and Takeaways","text":"<ul> <li>The convex conjugate \\(f^*\\) expresses \\(f\\) through its linear support planes.  </li> <li>The Fenchel\u2013Young inequality connects primal variables and dual slopes.  </li> <li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li> <li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li> <li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li> </ul> <p>Further Reading</p> <ul> <li>Rockafellar, R. T. (1970). Convex Analysis. Princeton UP.  </li> <li>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization, Chs. 3 &amp; 5.  </li> <li>Bauschke, H. H., &amp; Combettes, P. L. (2017). Convex Analysis and Monotone Operator Theory.  </li> <li>Hiriart-Urruty, J.-B., &amp; Lemar\u00e9chal, C. (2001). Fundamentals of Convex Analysis.  </li> </ul>"},{"location":"appendices/170_probability/","title":"Appendix E - Convexity in Probability and Statistics (Advanced)","text":""},{"location":"appendices/170_probability/#appendix-e-convexity-in-probability-and-statistics","title":"Appendix E : Convexity in Probability and Statistics","text":"<p>Convex analysis is not just geometry and optimization \u2014 it is deeply woven into probability, statistics, and information theory. Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p> <p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>"},{"location":"appendices/170_probability/#e1-convexity-of-expectations","title":"E.1 Convexity of Expectations","text":"<p>Let \\(f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}\\) be convex and \\(X\\) a random vector. Then by Jensen\u2019s inequality (Appendix A):</p> \\[ f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]. \\]"},{"location":"appendices/170_probability/#consequences","title":"Consequences","text":"<ul> <li>Expectations preserve convexity:   if each \\(f(\\cdot,\\xi)\\) is convex, then \\(F(x)=\\mathbb{E}_\\xi[f(x,\\xi)]\\) is convex.</li> <li>Stochastic objectives in ML \u2014 e.g. expected loss \\(\\mathbb{E}_{(a,b)}[\\ell(a^\\top x,b)]\\) \u2014 are convex when the sample-wise loss is convex.</li> </ul> <p>Hence almost all empirical risk minimization problems are discrete approximations of convex expectations.</p>"},{"location":"appendices/170_probability/#e2-convexity-of-log-partition-and-moment-generating-functions","title":"E.2 Convexity of Log-Partition and Moment-Generating Functions","text":"<p>For a random variable \\(X\\), the moment-generating function (MGF) and cumulant-generating function (CGF) are</p> \\[ M_X(t)=\\mathbb{E}[e^{tX}], \\qquad K_X(t)=\\log M_X(t). \\] <p>Fact: \\(K_X(t)\\) is always convex in \\(t\\).</p> <p>Reason: \\(K_X''(t)=\\mathrm{Var}_t(X)\\ge0\\); variance is nonnegative.  </p>"},{"location":"appendices/170_probability/#implications","title":"Implications","text":"<ul> <li>\\(K_X(t)\\) acts as a convex \u201cpotential\u201d controlling exponential families.</li> <li>The log-partition function in statistics,      is convex in \\(\\theta\\) (strictly convex for full exponential families).</li> <li>Its gradient gives the mean parameter: \\(\\nabla A(\\theta)=\\mathbb{E}_\\theta[T(X)]\\).</li> </ul> <p>Thus convexity of \\(A\\) guarantees a one-to-one mapping between natural and mean parameters \u2014 a foundation of exponential-family inference.</p>"},{"location":"appendices/170_probability/#e3-exponential-families-and-dual-convexity","title":"E.3 Exponential Families and Dual Convexity","text":"<p>An exponential-family density has the form  </p> <p>Properties:</p> <ol> <li>\\(A(\\theta)\\) is convex, smooth, and serves as a potential function.</li> <li>Its convex conjugate \\(A^*(\\mu)\\) defines the entropy of the family:        where \\(H\\) is the Shannon entropy of the distribution with mean \\(\\mu\\).</li> </ol> <p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>"},{"location":"appendices/170_probability/#e4-convex-divergences-and-information-measures","title":"E.4 Convex Divergences and Information Measures","text":""},{"location":"appendices/170_probability/#a-kullbackleibler-kl-divergence","title":"(a) Kullback\u2013Leibler (KL) Divergence","text":"<p>For densities \\(p,q\\),  </p> <ul> <li>\\(D_{\\mathrm{KL}}\\) is jointly convex in \\((p,q)\\).  </li> <li>Proof: the function \\((u,v)\\mapsto u\\log(u/v)\\) is convex on \\(\\mathbb{R}_+^2\\).  </li> <li>Consequently, mixtures of distributions cannot increase KL divergence \u2014 a key fact in variational inference and EM.</li> </ul>"},{"location":"appendices/170_probability/#b-bregman-divergences","title":"(b) Bregman Divergences","text":"<p>Given a differentiable convex \\(\\phi\\), define  KL divergence is a Bregman divergence for \\(\\phi(p)=\\sum_i p_i\\log p_i\\). Thus information-theoretic distances are geometric shadows of convex functions.</p>"},{"location":"appendices/170_probability/#c-f-divergences","title":"(c) f-Divergences","text":"<p>A general convex generator \\(f\\) with \\(f(1)=0\\) yields  Convexity of \\(f\\) \u21d2 convexity of \\(D_f\\). Common choices recover KL, \u03c7\u00b2, Hellinger, and Jensen\u2013Shannon divergences.</p>"},{"location":"appendices/170_probability/#e5-convex-loss-functions-in-statistics-and-machine-learning","title":"E.5 Convex Loss Functions in Statistics and Machine Learning","text":"<p>Convexity ensures estimators are globally optimal and algorithms converge.</p> Setting Loss / Negative Log-Likelihood Convexity Gaussian noise \\(\\tfrac12\\|Ax-b\\|_2^2\\) quadratic, strongly convex Laplace noise \\(\\|Ax-b\\|_1\\) convex, nonsmooth Logistic regression \\(\\log(1+e^{-y a^\\top x})\\) convex, smooth Poisson regression \\(e^{a^\\top x}-y a^\\top x\\) convex, exponential Huber loss piecewise quadratic/linear convex, robust <p>Convexity of the negative log-likelihood follows from convexity of the log-partition function \\(A(\\theta)\\) in exponential families.</p>"},{"location":"appendices/170_probability/#e6-convexity-and-bayesian-inference","title":"E.6 Convexity and Bayesian Inference","text":"<p>In Bayesian inference, convexity appears in:</p> <ul> <li> <p>Log-concave posteriors:   If the likelihood and prior are log-concave, the posterior \\(p(x|y)\\propto \\exp(-f(x))\\) is also log-concave \u21d2 \\(\\log p(x|y)\\) concave, \\(f(x)\\) convex.</p> </li> <li> <p>MAP estimation:   Maximizing \\(\\log p(x|y)\\) \u2261 minimizing a convex function when \\(p(x|y)\\) is log-concave \u21d2 global optimum guaranteed.</p> </li> <li> <p>Variational inference:   The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).   Optimizing it is equivalent to minimizing a convex divergence.</p> </li> </ul> <p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>"},{"location":"appendices/170_probability/#e7-statistical-risk-and-convex-surrogates","title":"E.7 Statistical Risk and Convex Surrogates","text":"<p>Convex surrogate losses replace nonconvex 0\u20131 loss with convex approximations:</p> <ul> <li>Hinge loss (\\(\\max(0,1-y a^\\top x)\\)) \u2192 support-vector machines.  </li> <li>Logistic loss \u2192 probabilistic classification (cross-entropy).  </li> <li>Exponential loss \u2192 AdaBoost.</li> </ul> <p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p>"},{"location":"appendices/180_subgradient_methods/","title":"Appendix F - Subgradient Method and Variants (Advanced)","text":""},{"location":"appendices/180_subgradient_methods/#appendix-f-subgradient-method-derivation-geometry-and-convergence","title":"Appendix F: Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>This appendix presents the subgradient method\u2014the fundamental algorithm for minimizing nonsmooth convex functions. It generalizes gradient descent to functions such as the \\(\\ell_1\\) norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>"},{"location":"appendices/180_subgradient_methods/#f1-problem-setup","title":"F.1 Problem Setup","text":"<p>We consider</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but possibly nondifferentiable and \\(\\mathcal{X}\\) is a convex feasible set.</p>"},{"location":"appendices/180_subgradient_methods/#f2-subgradients-and-geometry","title":"F.2 Subgradients and Geometry","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) satisfies</p> \\[ f(y) \\ge f(x_t) + \\langle g_t,\\, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>If \\(f\\) is differentiable, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).  </li> <li>At a nonsmooth point (e.g. \\(|x|\\) at \\(x=0\\)), \\(\\partial f(x_t)\\) is a set of supporting slopes.  </li> <li>Each subgradient defines a supporting hyperplane below the graph of \\(f\\).</li> </ul> <p>Hence a subgradient gives a descent direction even when \\(f\\) lacks a unique gradient.</p>"},{"location":"appendices/180_subgradient_methods/#f3-update-rule-and-projection-view","title":"F.3 Update Rule and Projection View","text":"<p>The projected subgradient step is</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}}\\!\\big(x_t - \\eta_t g_t\\big), \\] <p>where - \\(g_t \\in \\partial f(x_t)\\), - \\(\\eta_t&gt;0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) projects onto \\(\\mathcal{X}\\).</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\), projection disappears:  </p> <p>Geometric view: move in a subgradient direction, then project back to feasibility. The method \u201cslides\u201d along the edges of \\(f\\)\u2019s epigraph.</p>"},{"location":"appendices/180_subgradient_methods/#f4-distance-analysis","title":"F.4 Distance Analysis","text":"<p>Let \\(x^\\star\\) be an optimal solution. Expanding the squared distance:</p> \\[ \\|x_{t+1}-x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t\\langle g_t, x_t - x^\\star\\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity,  </p> <p>Substitute to get</p> \\[ \\|x_{t+1}-x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t\\big(f(x_t)-f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"appendices/180_subgradient_methods/#f5-bounding-suboptimality","title":"F.5 Bounding Suboptimality","text":"<p>Rearranging:</p> \\[ f(x_t)-f(x^\\star) \\le \\frac{\\|x_t-x^\\star\\|^2 - \\|x_{t+1}-x^\\star\\|^2}{2\\eta_t} + \\frac{\\eta_t}{2}\\|g_t\\|^2. \\] <p>This shows a trade-off:</p> <ul> <li>Large \\(\\eta_t\\) \u2192 faster steps but higher error term.  </li> <li>Small \\(\\eta_t\\) \u2192 more precise but slower progress.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f6-convergence-rate","title":"F.6 Convergence Rate","text":"<p>Assume \\(\\|g_t\\| \\le G\\). Summing over \\(t=0,\\dots,T-1\\):</p> \\[ \\sum_{t=0}^{T-1}\\!\\big(f(x_t)-f(x^\\star)\\big) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Define \\(\\bar{x}_T = \\tfrac{1}{T}\\sum_{t=0}^{T-1} x_t\\). By convexity of \\(f\\),</p> \\[ f(\\bar{x}_T)-f(x^\\star) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing \\(\\eta_t = \\tfrac{R}{G\\sqrt{T}}\\) with \\(R=\\|x_0-x^\\star\\|\\) yields</p> <p>  i.e. a sublinear rate \\(O(1/\\sqrt{T})\\).</p>"},{"location":"appendices/180_subgradient_methods/#f7-interpretation-and-practice","title":"F.7 Interpretation and Practice","text":"<ul> <li>Works for any convex function, smooth or not.  </li> <li>Converges slower than smooth-gradient methods (\\(O(1/T)\\) or linear), but applies more generally.  </li> <li>Step size schedule is crucial: \\(\\eta_t \\!\\downarrow 0\\) for convergence, or fixed \\(\\eta\\) for steady error.  </li> <li>Averaging \\(\\bar{x}_T\\) improves stability.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#typical-ml-uses","title":"Typical ML Uses","text":"Model Objective Nonsmooth Term LASSO \\(\\tfrac12\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1\\) \\(\\ell_1\\) penalty SVM \\(\\tfrac12\\|w\\|^2 + C\\sum_i \\max(0,1-y_i w^\\top x_i)\\) hinge loss Robust regression $\\sum_i a_i^\\top x - b_i Neural nets \\(\\|w\\|_1\\) or ReLU activations piecewise linear"},{"location":"appendices/180_subgradient_methods/#f8-beyond-basic-subgradients","title":"F.8 Beyond Basic Subgradients","text":"<p>Many advanced methods refine or accelerate the basic idea:</p> <ul> <li>Stochastic subgradients: sample-based updates for large-scale ML.  </li> <li>Mirror descent: adapt geometry via Bregman divergences.  </li> <li>Proximal methods: replace step with proximal operator (see Appendix B).  </li> <li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f9-summary","title":"F.9 Summary","text":"<ul> <li>Subgradients generalize gradients to nondifferentiable convex functions.  </li> <li>The projected subgradient method provides a universal, robust minimization algorithm.  </li> <li>Achieves \\(O(1/\\sqrt{T})\\) convergence under bounded subgradients.  </li> <li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9\u201310.</li> </ul>"},{"location":"appendices/190_proximal/","title":"Appendix G - Proximal Operators","text":""},{"location":"appendices/190_proximal/#appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization","title":"Appendix G | Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>Many convex optimization problems involve constraints or nonsmooth penalties. This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>"},{"location":"appendices/190_proximal/#g1-problem-setup","title":"G.1 Problem Setup","text":"<p>We wish to minimize a convex, differentiable function \\( f(x) \\) subject to a convex feasible set \\( \\mathcal{X} \\subseteq \\mathbb{R}^n \\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>A plain gradient step,</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>may leave \\( x_{t+1} \\notin \\mathcal{X} \\). We fix this by projecting the iterate back into the feasible region.</p>"},{"location":"appendices/190_proximal/#g2-projection-operator","title":"G.2 Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2. \\] <p>Hence, the projected gradient descent update is</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\]"},{"location":"appendices/190_proximal/#geometric-insight","title":"Geometric Insight","text":"<ul> <li>Take a descent step possibly outside the feasible set.  </li> <li>Project back to the closest feasible point.  </li> <li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li> </ul> <p>Example \u2014 Euclidean ball: If \\( \\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\} \\), then</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)}. \\] <ul> <li>Inside the ball \u2192 unchanged.  </li> <li>Outside \u2192 scaled back to the boundary.</li> </ul>"},{"location":"appendices/190_proximal/#g3-from-projections-to-proximal-operators","title":"G.3 From Projections to Proximal Operators","text":"<p>Projections handle explicit constraints, but many problems use implicit penalties \u2014 e.g. sparsity (\\(\\|x\\|_1\\)), total variation, or nonnegativity penalties.</p> <p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>"},{"location":"appendices/190_proximal/#definition","title":"Definition","text":"<p>For a convex (possibly nondifferentiable) function \\( g(x) \\),</p> <p>  where \\( \\lambda &gt; 0 \\) balances regularization vs. proximity.</p>"},{"location":"appendices/190_proximal/#interpretation","title":"Interpretation","text":"<ul> <li>The quadratic term \\( \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\) keeps \\(x\\) close to \\(y\\).  </li> <li>The function \\( g(x) \\) encourages structure (sparsity, smoothness, feasibility).  </li> <li>Small \\(\\lambda\\): conservative correction; large \\(\\lambda\\): stronger regularization.</li> </ul> <p>The proximal step acts as a soft correction after a gradient step.</p>"},{"location":"appendices/190_proximal/#g4-projection-as-a-special-case","title":"G.4 Projection as a Special Case","text":"<p>Define the indicator function of a convex set \\(\\mathcal{X}\\):</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X}, \\\\[4pt] +\\infty, &amp; x \\notin \\mathcal{X}. \\end{cases} \\] <p>Substitute \\(g(x)=I_{\\mathcal{X}}(x)\\) into the proximal definition:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\Big) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y). \\] <p>\u2705 Projection is just a proximal operator for an indicator function.</p>"},{"location":"appendices/190_proximal/#g5-proximal-gradient-method","title":"G.5 Proximal Gradient Method","text":"<p>When minimizing a composite convex objective  where \\(f\\) is smooth and \\(g\\) convex (possibly nonsmooth), the proximal gradient method updates:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\] <ul> <li>The gradient step reduces the smooth part \\(f(x)\\).  </li> <li>The proximal step enforces structure via \\(g(x)\\). This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li> </ul>"},{"location":"appendices/190_proximal/#g6-example-proximal-operator-of-the-ell_1-norm","title":"G.6 Example: Proximal Operator of the \\(\\ell_1\\)-Norm","text":"<p>We seek</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda\\|x\\|_1 + \\tfrac{1}{2}\\|x - y\\|^2 \\right). \\]"},{"location":"appendices/190_proximal/#step-1-coordinate-separation","title":"Step 1. Coordinate Separation","text":"<p>The problem is separable across coordinates:  so each coordinate solves  </p>"},{"location":"appendices/190_proximal/#step-2-subgradient-optimality","title":"Step 2. Subgradient Optimality","text":"<p>Optimality condition:  Thus,  </p>"},{"location":"appendices/190_proximal/#step-3-case-analysis","title":"Step 3. Case Analysis","text":"Case Condition Solution \\(x^\\star&gt;0\\) \\(y&gt;\\lambda\\) \\(x^\\star = y - \\lambda\\) \\(x^\\star&lt;0\\) \\(y&lt;-\\lambda\\) \\(x^\\star = y + \\lambda\\) \\(x^\\star=0\\) ( y"},{"location":"appendices/190_proximal/#step-4-compact-form","title":"Step 4. Compact Form","text":"\\[ \\boxed{ \\text{prox}_{\\lambda|\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda,\\, 0) } \\] <p>This is the soft-thresholding operator.</p>"},{"location":"appendices/190_proximal/#step-5-vector-case","title":"Step 5. Vector Case","text":"<p>For \\(y \\in \\mathbb{R}^n\\),</p> \\[ \\big(\\text{prox}_{\\lambda\\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i)\\cdot\\max(|y_i| - \\lambda, 0). \\] <p>Each coordinate is independently shrunk toward zero \u2014 producing sparse solutions.</p>"},{"location":"appendices/190_proximal/#step-6-interpretation","title":"Step 6. Interpretation","text":"<ul> <li>Coordinates with \\(|y_i| \\le \\lambda\\) \u2192 set to zero (promotes sparsity).  </li> <li>Coordinates with \\(|y_i| &gt; \\lambda\\) \u2192 shrink by \\(\\lambda\\).  </li> <li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li> </ul>"},{"location":"appendices/190_proximal/#g7-geometry-and-connection-to-algorithms","title":"G.7 Geometry and Connection to Algorithms","text":"<ul> <li>Projection = nearest feasible point \u2192 handles hard constraints.  </li> <li>Proximal operator = nearest structured point \u2192 handles soft regularization.  </li> <li>Proximal gradient = combines both, yielding algorithms like:</li> <li>ISTA / FISTA (sparse recovery, LASSO),</li> <li>Projected gradient (feasibility),</li> <li>ADMM (splitting into subproblems).</li> </ul> <p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>"},{"location":"appendices/190_proximal/#g8-summary","title":"G.8 Summary","text":"<ul> <li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li> <li>Projection is a special case of the proximal operator for an indicator function.  </li> <li>Proximal mappings handle nonsmooth regularizers (e.g., \\(\\ell_1\\)-norm).  </li> <li>The proximal gradient method unifies constrained and regularized optimization.  </li> <li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li> </ul>"},{"location":"appendices/200_mirror/","title":"Appendix H - Mirror Descent and Bregman Geometry","text":""},{"location":"appendices/200_mirror/#appendix-h-mirror-descent-and-bregman-geometry","title":"Appendix H: Mirror Descent and Bregman Geometry","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry. In many structured domains\u2014such as probability simplices or sparse models\u2014Euclidean updates can destroy problem structure or cause instability.  </p> <p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence. It performs gradient-like updates in a dual space, respecting the intrinsic geometry of the domain.</p>"},{"location":"appendices/200_mirror/#h1-motivation-and-limitations-of-euclidean-gd","title":"H.1 Motivation and Limitations of Euclidean GD","text":"<p>Standard GD update:  assumes Euclidean distance  </p> <p>This works well in \\(\\mathbb{R}^n\\) without structure, but fails to respect constraints or sparsity.</p> <p>In practice:</p> <ul> <li>Many parameters are nonnegative or normalized (probabilities, weights).  </li> <li>Euclidean steps can violate constraints or zero out coordinates.  </li> <li>The \u201cflat\u201d \\(\\ell_2\\) geometry treats all directions equally.</li> </ul> <p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the metric via a mirror map.</p>"},{"location":"appendices/200_mirror/#h2-geometry-in-optimization","title":"H.2 Geometry in Optimization","text":"<p>The \u201csteepest descent\u201d direction depends on the notion of distance. GD implicitly minimizes a linearized loss plus a Euclidean proximity term.</p> Scenario Natural Constraint Appropriate Geometry Probability vectors \\(x_i\\ge0, \\sum_i x_i=1\\) KL / entropy geometry Sparse models \\(\\|x\\|_1\\)-structured \\(\\ell_1\\) geometry Online learning multiplicative updates log-space geometry <p>Using Euclidean projections in these domains can cause:</p> <ul> <li>abrupt projection onto boundaries,</li> <li>loss of positivity or sparsity,</li> <li>geometric inconsistency.</li> </ul>"},{"location":"appendices/200_mirror/#h3-mirror-descent-framework","title":"H.3 Mirror Descent Framework","text":"<p>Let \\(\\psi(x)\\) be a mirror map \u2014 a strictly convex, differentiable potential encoding the geometry.</p> <p>Define the dual coordinate:  and its inverse mapping through the convex conjugate \\(\\psi^*\\):  </p>"},{"location":"appendices/200_mirror/#bregman-divergence","title":"Bregman Divergence","text":"<p>The geometry is quantified by the Bregman divergence:  </p> <ul> <li>Measures how nonlinear \\(\\psi\\) is between \\(x\\) and \\(y\\).  </li> <li>When \\(\\psi(x)=\\tfrac12\\|x\\|_2^2\\), \\(D_\\psi\\) becomes \\(\\tfrac12\\|x-y\\|_2^2\\).  </li> <li>When \\(\\psi(x)=\\sum_i x_i\\log x_i\\), \\(D_\\psi\\) becomes KL divergence.</li> </ul>"},{"location":"appendices/200_mirror/#h4-mirror-descent-update-rule","title":"H.4 Mirror Descent Update Rule","text":"<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:  </p> <p>Equivalent dual-space form:  </p> <p>\u2705 MD is gradient descent in dual coordinates, where distances are measured by \\(D_\\psi\\) instead of \\(\\|x-y\\|_2\\).</p>"},{"location":"appendices/200_mirror/#h5-comparing-gd-projected-gd-and-md","title":"H.5 Comparing GD, Projected GD, and MD","text":"Method Update Rule Geometry Comments Gradient Descent \\(x - \\eta\\nabla f\\) Euclidean may leave feasible set Projected GD \\(\\text{Proj}(x - \\eta\\nabla f)\\) Euclidean + projection can cause discontinuous jumps Mirror Descent \\(\\arg\\min_x \\langle\\nabla f, x - x_t\\rangle + \\frac{1}{\\eta}D_\\psi(x\\|x_t)\\) Bregman smooth, structure-preserving"},{"location":"appendices/200_mirror/#h6-simplex-example-kl-geometry","title":"H.6 Simplex Example (KL Geometry)","text":"<p>Let \\(x\\in\\Delta^2=\\{x\\ge0, x_1+x_2=1\\}\\), objective \\(f(x)=x_1^2+2x_2\\), \\(\\eta=0.3\\).</p>"},{"location":"appendices/200_mirror/#euclidean-gd-projection","title":"Euclidean GD + Projection","text":"<ol> <li>\\(\\nabla f=(2x_1,2)=(1,2)\\),  </li> <li>\\(y=x-\\eta\\nabla f=(0.2,-0.1)\\),  </li> <li>Project \u2192 \\(x_{new}=(1,0)\\).</li> </ol> <p>\u2192 Projection kills one coordinate \u21d2 lost smoothness.</p>"},{"location":"appendices/200_mirror/#mirror-descent-with-negative-entropy","title":"Mirror Descent with Negative Entropy","text":"<p>Mirror map \\(\\psi(x)=\\sum_i x_i\\log x_i\\). Update:  Gives \\(x\\approx(0.57,0.43)\\) \u2014 smooth, positive, stays in simplex.</p> <p>MD follows the manifold of the simplex naturally\u2014no harsh projection.</p>"},{"location":"appendices/200_mirror/#h7-choosing-the-mirror-map","title":"H.7 Choosing the Mirror Map","text":"Mirror Map \\(\\psi(x)\\) Bregman Divergence \\(D_\\psi\\) Typical Domain / Application \\(\\tfrac12\\|x\\|_2^2\\) Euclidean distance unconstrained \\(\\mathbb{R}^n\\) \\(\\sum_i x_i\\log x_i\\) KL divergence simplex, probabilities \\(\\|x\\|_1\\) or variants \\(\\ell_1\\) geometry sparse models log-barrier \\(\\sum_i -\\log x_i\\) barrier divergence positive orthant <p>Mirror maps act as design choices defining the optimization geometry.</p>"},{"location":"appendices/200_mirror/#h8-practical-remarks","title":"H.8 Practical Remarks","text":"<p>When to prefer Mirror Descent:</p> <ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates desired</li> <li>Avoiding discontinuous projections</li> </ul> <p>Computational notes:</p> <ul> <li>Some \\(\\psi\\) yield closed-form updates (e.g. multiplicative weights).  </li> <li>Works with adaptive or momentum step-size schemes.  </li> <li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li> </ul>"},{"location":"appendices/200_mirror/#h9-convergence-at-a-glance","title":"H.9 Convergence at a Glance","text":"<p>For convex \\(f\\) with bounded gradients \\(\\|\\nabla f\\|\\le G\\) and strong convex mirror map \\(\\psi\\), Mirror Descent achieves the same sublinear rate as projected subgradient methods:  but with improved geometry-adapted constants that exploit curvature of \\(\\psi\\).</p>"},{"location":"appendices/300_matrixfactorization/","title":"Appendix I - Matrix Factorization","text":""},{"location":"appendices/300_matrixfactorization/#numerical-linear-algebra-for-convex-optimization","title":"Numerical Linear Algebra for Convex Optimization","text":"<p>Numerical linear algebra is the computational foundation of convex optimization. Every modern optimization algorithm \u2014 from Newton\u2019s method to interior-point or proximal algorithms \u2014 ultimately requires solving a structured linear system:  where \\(H\\) may represent a Hessian, a normal equations matrix, or a KKT (Karush\u2013Kuhn\u2013Tucker) system.</p> <p>In practice, we never compute \\(H^{-1}\\) directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>"},{"location":"appendices/300_matrixfactorization/#1-why-linear-algebra-matters-in-convex-optimization","title":"1. Why Linear Algebra Matters in Convex Optimization","text":"<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p> <ul> <li> <p>Newton\u2019s method:    </p> </li> <li> <p>Interior-point methods (KKT systems):</p> </li> </ul> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\[3pt] \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -r_d \\\\[3pt] -r_p \\end{bmatrix} \\] <ul> <li>Least-squares problems: \\(A^T A x = A^T b\\)</li> </ul> <p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>"},{"location":"appendices/300_matrixfactorization/#2-the-matrix-factorization-toolbox","title":"2. The Matrix Factorization Toolbox","text":"<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure. They enable efficient triangular solves instead of direct inversion.</p> Factorization Applies To Form Common Use Key Notes LU Any nonsingular matrix \\(A = L U\\) General linear systems Requires pivoting for stability QR Any (rectangular) matrix \\(A = Q R\\) Least-squares Orthogonal, stable Cholesky Symmetric positive definite \\(A = L L^T\\) SPD systems, normal equations Fastest for SPD \\(LDL^T\\) Symmetric indefinite \\(A = L D L^T\\) KKT systems Handles indefiniteness Eigen Symmetric/Hermitian \\(A = Q \\Lambda Q^T\\) Curvature, convexity checks Diagonalizes \\(A\\) SVD Any matrix \\(A = U \\Sigma V^T\\) Rank, conditioning, pseudoinverse Most stable, expensive <p>Each factorization corresponds to a numerically preferred strategy for certain classes of problems.</p>"},{"location":"appendices/300_matrixfactorization/#3-lu-factorization-the-general-purpose-workhorse","title":"3. LU Factorization \u2014 The General-Purpose Workhorse","text":"<p>Form:  where \\(P\\) is a permutation matrix ensuring stability.</p> <ul> <li>Used for: General linear systems, nonsymmetric matrices.</li> <li>Cost: \\(\\approx \\tfrac{2}{3}n^3\\) (dense).</li> <li>Stability: Requires partial pivoting (\\(PA=LU\\)) to avoid numerical blow-up.</li> </ul> <p>Example use case:</p> <ul> <li>Solving KKT systems in linear programming (LP simplex tableau).</li> <li>Small dense systems with no symmetry or SPD property.</li> </ul> <p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or \\(LDL^T\\).</p>"},{"location":"appendices/300_matrixfactorization/#4-qr-factorization-orthogonal-and-stable","title":"4. QR Factorization \u2014 Orthogonal and Stable","text":"<p>Form:  </p> <ul> <li>Used for: Least-squares problems      Instead of forming normal equations (\\(A^T A x = A^T b\\)), we solve:    </li> <li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li> </ul> <p>Example use cases:</p> <ul> <li>Linear regression via least squares.</li> <li>ADMM and proximal steps with overdetermined systems.</li> <li>Orthogonal projections in signal processing.</li> </ul> <p>Variants:</p> <ul> <li>Householder QR: numerically robust, used in LAPACK.</li> <li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#5-cholesky-factorization-fastest-for-spd-systems","title":"5. Cholesky Factorization \u2014 Fastest for SPD Systems","text":"<p>Form:  Applicable when \\(A\\) is symmetric positive definite (SPD) \u2014 common in convex problems.</p> <p>Why it\u2019s central: Convexity ensures \\(A \\succeq 0\\). For strictly convex problems, \\(A \\succ 0\\) and Cholesky is the most efficient and stable method.</p> <p>Cost: \\(\\tfrac{1}{3}n^3\\) operations \u2014 half of LU.</p> <p>Example use cases:</p> <ul> <li>Newton\u2019s method on unconstrained convex functions.</li> <li>Solving normal equations \\(A^T A x = A^T b\\).</li> <li>QP subproblems and ridge regression.</li> </ul> <p>Implementation detail: No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>"},{"location":"appendices/300_matrixfactorization/#6-ldlt-factorization-for-indefinite-symmetric-systems","title":"6. LDL\u1d40 Factorization \u2014 For Indefinite Symmetric Systems","text":"<p>Form:  where \\(D\\) is block diagonal (1\u00d71 or 2\u00d72 blocks), and \\(L\\) is unit lower triangular.</p> <p>Used when \\(A\\) is symmetric but not SPD (e.g., KKT systems).</p> <p>Example use cases:</p> <ul> <li> <p>Interior-point methods for QPs and SDPs:    </p> </li> <li> <p>Equality-constrained least-squares.</p> </li> <li>Sparse symmetric indefinite systems in primal-dual algorithms.</li> </ul> <p>Algorithmic note: Uses Bunch\u2013Kaufman pivoting to maintain numerical stability. In practice, LDL\u1d40 is used with sparse reordering and partial elimination.</p>"},{"location":"appendices/300_matrixfactorization/#7-block-systems-and-the-schur-complement","title":"7. Block Systems and the Schur Complement","text":"<p>Many KKT or structured systems naturally appear in block form:  </p> <p>Assuming \\(A_{11}\\) is invertible:</p> <ol> <li>Eliminate \\(x_1\\):     </li> <li>Substitute into the second block:     </li> </ol> <p>The matrix  is the Schur complement of \\(A_{11}\\) in \\(A\\).</p>"},{"location":"appendices/300_matrixfactorization/#schur-complement-in-optimization","title":"Schur Complement in Optimization","text":"<ul> <li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li> <li>Preserves symmetry and often positive definiteness.</li> <li>Foundation of block elimination and reduced Hessian methods.</li> </ul> <p>Example use cases:</p> <ul> <li>Interior-point Newton systems (eliminate \\(\\Delta x\\) to get a system in \\(\\Delta \\lambda\\)).</li> <li>Partial elimination in sequential quadratic programming (SQP).</li> <li>Covariance conditioning and Gaussian marginalization.</li> </ul> <p>Numerical caution: Never form \\(A_{11}^{-1}\\) explicitly \u2014 use triangular solves via Cholesky or LU.</p>"},{"location":"appendices/300_matrixfactorization/#8-block-elimination-algorithm","title":"8. Block Elimination Algorithm","text":"<p>Given a nonsingular \\(A_{11}\\):</p> <ol> <li>Compute \\(A_{11}^{-1}A_{12}\\) and \\(A_{11}^{-1}b_1\\) by solving triangular systems.</li> <li>Form \\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\\), \\(\\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\\).</li> <li>Solve \\(Sx_2 = \\tilde{b}\\).</li> <li>Recover \\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\\).</li> </ol> <p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p> <p>Example use case:</p> <ul> <li>Partitioned least-squares with fixed and variable parameters.</li> <li>Constrained optimization where some variables can be analytically eliminated.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#9-structured-plus-low-rank-matrices","title":"9. Structured Plus Low-Rank Matrices","text":"<p>Suppose we need to solve:  where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{n \\times n}\\) is structured or easily invertible (e.g., diagonal or sparse),</li> <li>\\(B \\in \\mathbb{R}^{n \\times p}\\), \\(C \\in \\mathbb{R}^{p \\times n}\\) are low rank.</li> </ul> <p>This situation arises when updating an existing system with a small modification.</p>"},{"location":"appendices/300_matrixfactorization/#block-reformulation","title":"Block Reformulation","text":"<p>Introduce \\(y = Cx\\), yielding:</p> <p>$$   =</p> <p> . $$</p> <p>Block elimination gives:  </p>"},{"location":"appendices/300_matrixfactorization/#matrix-inversion-lemma-woodbury-identity","title":"Matrix Inversion Lemma (Woodbury Identity)","text":"<p>If \\(A\\) and \\(A + BC\\) are nonsingular:  </p> <p>Example use cases:</p> <ul> <li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li> <li>Ridge regression / kernel methods: low-rank updates to \\((X^T X + \\lambda I)^{-1}\\).</li> <li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li> </ul> <p>Numerical note: Avoid explicit inversion; use solves with \\(A\\) and small dense matrices.</p>"},{"location":"appendices/300_matrixfactorization/#10-conditioning-stability-and-sparsity","title":"10. Conditioning, Stability, and Sparsity","text":""},{"location":"appendices/300_matrixfactorization/#conditioning","title":"Conditioning","text":"<ul> <li>Condition number: \\(\\kappa(A) = |A||A^{-1}|\\) measures sensitivity to perturbations.</li> <li>High \\(\\kappa(A)\\) \u21d2 round-off errors amplified \u21d2 ill-conditioning.</li> <li>Regularization (adding \\(\\lambda I\\)) improves numerical stability.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#stability","title":"Stability","text":"<ul> <li>Orthogonal transformations (QR, SVD) are backward stable.</li> <li>LU needs partial pivoting.</li> <li>LDL\u1d40 needs symmetric pivoting (Bunch\u2013Kaufman).</li> <li>Cholesky is stable for SPD matrices.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#sparsity-and-fill-in","title":"Sparsity and Fill-In","text":"<ul> <li>Large convex solvers exploit sparse Cholesky / LDL\u1d40.</li> <li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li> <li>Symbolic factorization determines the pattern before numeric factorization.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#11-iterative-solvers-and-preconditioning","title":"11. Iterative Solvers and Preconditioning","text":"<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>"},{"location":"appendices/300_matrixfactorization/#common-iterative-methods","title":"Common Iterative Methods","text":"Method For Description CG SPD systems Uses matrix\u2013vector products; converges in \u2264 n steps MINRES / SYMMLQ Symmetric indefinite Handles KKT and saddle-point systems GMRES / BiCGSTAB Nonsymmetric General-purpose Krylov solvers"},{"location":"appendices/300_matrixfactorization/#preconditioning","title":"Preconditioning","text":"<p>Preconditioners \\(M \\approx A^{-1}\\) improve convergence:</p> <ul> <li>Jacobi (diagonal): \\(M = \\text{diag}(A)^{-1}\\)</li> <li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li> <li>Block preconditioners: use Schur complement approximations for KKT systems</li> </ul> <p>Example use case:</p> <ul> <li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li> <li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#12-eigenvalue-and-svd-decompositions","title":"12. Eigenvalue and SVD Decompositions","text":""},{"location":"appendices/300_matrixfactorization/#eigenvalue-decomposition","title":"Eigenvalue Decomposition","text":"<p>  Reveals curvature, stability, and definiteness:</p> <ul> <li>Convexity \u21d4 \\(\\Lambda \\ge 0\\).</li> <li>Used in semidefinite programming (SDP) and spectral analysis.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>  with \\(\\Sigma = \\text{diag}(\\sigma_i) \\ge 0\\).</p> <p>Applications:</p> <ul> <li>Rank and condition number estimation (\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\)).</li> <li>Low-rank approximation (\\(A_k = U_k \\Sigma_k V_k^T\\)).</li> <li>Pseudoinverse: \\(A^+ = V \\Sigma^{-1} U^T\\).</li> <li>Convex relaxations: nuclear-norm minimization (matrix completion).</li> </ul>"},{"location":"appendices/300_matrixfactorization/#13-computational-complexity-summary","title":"13. Computational Complexity Summary","text":"Factorization Dense Cost Notes LU \\(\\frac{2}{3}n^3\\) Needs pivoting Cholesky \\(\\frac{1}{3}n^3\\) Fastest for SPD QR \\(\\approx \\frac{2}{3}n^3\\) Stable, more memory LDL\u1d40 \\(\\approx \\frac{2}{3}n^3\\) For indefinite SVD \\(\\approx \\frac{4}{3}n^3\\) Most accurate CG / MINRES Variable Depends on condition number and preconditioning <p>Sparse systems reduce cost to roughly \\(O(n^{1.5})\\)\u2013\\(O(n^2)\\) depending on fill-in.</p>"},{"location":"appendices/300_matrixfactorization/#14-example-applications-overview","title":"14. Example Applications Overview","text":"Problem Type Typical Matrix Solver / Factorization Example Unconstrained Newton step SPD Hessian Cholesky Convex quadratic, ridge regression Equality-constrained QP Symmetric indefinite KKT LDL\u1d40 Interior-point QP solver Overdetermined LS Rectangular \\(A\\) QR Linear regression, ADMM subproblem KKT block system Block-symmetric Schur complement Primal-dual method Low-rank correction \\(A + U U^T\\) Woodbury Kalman filter, online update Rank-deficient system Any SVD Matrix completion, regularization Large-scale Hessian SPD CG + preconditioner Logistic regression, large ML models"},{"location":"cheatsheets/20a_cheatsheet/","title":"Optimization Algos - Cheat Sheet","text":""},{"location":"cheatsheets/20a_cheatsheet/#first-order-methods","title":"First-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Applications Gradient Descent (GD) Unconstrained smooth (convex/nonconvex) Differentiable, L-smooth (Lipschitz \u2207) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k)\\) Logistic regression, deep neural networks Nesterov\u2019s Accelerated GD Smooth convex (fast convergence) Convex, L-smooth \\(y_k = x_k + \\frac{k-1}{k+2}(x_k - x_{k-1})\\), \\(x_{k+1} = y_k - \\eta \\nabla f(y_k)\\) (momentum-based) Same as GD (e.g. convex regression, accelerating training) (Polyak) Heavy-ball Momentum Unconstrained smooth (accelerated) f convex, differentiable, \u03b2 \u2208 (0,1) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k) + \\beta(x_k - x_{k-1})\\) (momentum) Deep learning (momentum SGD), convex optimization"},{"location":"cheatsheets/20a_cheatsheet/#second-order-methods","title":"Second-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Applications Newton\u2019s Method Unconstrained smooth (convex) Twice-differentiable, Hessian\u00a0PD \\(x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\\) Logistic regression (IRLS), convex optimization BFGS (Quasi-Newton) Unconstrained smooth (convex) f twice-differentiable (no exact Hessian) Solve \\(B_k p_k = -\\nabla f(x_k)\\) then \\(x_{k+1}=x_k+p_k\\); update \\(B_k\\) by the secant rule Large-scale learning (L-BFGS for ML models)"},{"location":"cheatsheets/20a_cheatsheet/#proximal-projected-methods","title":"Proximal &amp; Projected Methods","text":"Method Problem Type Assumptions Core Update Rule Applications Proximal Gradient (ISTA) Convex composite (smooth + nonsmooth) \\(f(x)=g(x)+h(x)\\) where \\(g\\) smooth, \\(h\\) convex \\(x_{k+1} = \\mathrm{prox}_{\\alpha h}(x_k - \\alpha \\nabla g(x_k))\\) LASSO, elastic net, regularized regression Fast Proximal (FISTA) Convex composite (accelerated) same as ISTA, with momentum Like ISTA but with Nesterov momentum (e.g. update \\(y_k\\) as above) Same as ISTA (faster convergence) Projected Gradient (PG) Convex constrained (onto set \\(C\\)) \\(f\\) L-smooth; \\(C\\) convex (efficient proj) \\(x_{k+1} = \\Pi_C!\\bigl(x_k - \\eta \\nabla f(x_k)\\bigr)\\) (project onto \\(C\\)) Constrained problems (e.g. box/QP)"},{"location":"cheatsheets/20a_cheatsheet/#interior-point-barrier-methods","title":"Interior-Point &amp; Barrier Methods","text":"Method Problem Type Assumptions Core Update Rule Applications Interior-Point Convex with inequality constraints Convex problem, self-concordant barriers Solve Newton step on barrier-augmented objective \\(\\phi(x)=f(x)+\\frac{1}{t}\\sum_i -\\log(-g_i(x))\\) Linear/Quadratic programming, SDPs"},{"location":"cheatsheets/20a_cheatsheet/#stochastic-mini-batch-methods","title":"Stochastic &amp; Mini-Batch Methods","text":"Method Problem Type Assumptions Core Update Rule Applications Stochastic Gradient (SGD) Stochastic (often nonconvex) Unbiased gradient samples, decaying step-size \\(x_{k+1} = x_k - \\eta ,\\nabla f_{i_k}(x_k)\\) (use random sample or mini-batch) Online learning, deep networks Adaptive SGD (Adam) Stochastic nonconvex (NN training) Bounded variances; (\u03b2<sub>1</sub>,\u03b2<sub>2</sub>\u2208[0,1)) \\(m_k=\u03b2_1 m_{k-1}+(1-\u03b2_1)g_k\\), \\(v_k=\u03b2_2 v_{k-1}+(1-\u03b2_2)g_k^2\\); \\(\\hat m_k,\\hat v_k\\) bias-corrected, \\(x_{k+1}=x_k - \\eta,\\frac{\\hat m_k}{\\sqrt{\\hat v_k}+\u03b5}\\) Deep learning (CNNs, RNNs)"},{"location":"cheatsheets/20a_cheatsheet/#admm-alternating-direction-method-of-multipliers","title":"ADMM (Alternating Direction Method of Multipliers)","text":"Method Problem Type Assumptions Core Update Rule Applications ADMM Split convex with linear constraints \\(f,g\\) convex; equality constraints \\(Ax+Bz=c\\) $x^{k+1}=\\arg\\min_x\\Bigl{f(x)+\\frac{\\rho}{2} Ax+Bz^k-c+w^k"},{"location":"cheatsheets/20a_cheatsheet/#majorizationminimization-mm","title":"Majorization\u2013Minimization (MM)","text":"Method Problem Type Assumptions Core Update Rule Applications Majorization\u2013Minimization General nonconvex (or convex) Surrogate $g(x x_k)$ majorizes \\(f(x)\\) at \\(x_k\\) $x_{k+1} = \\arg\\min_x,g(x x_k)g(x x_k)\\ge f(x)g(x_k x_k)=f(x_k)$ EM algorithm (mixture models), IRLS for \u2113<sub>p</sub> regression"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1: Introduction and Overview","text":"<p>Optimization is the mathematical language of decision-making and learning. In machine learning and data science we constantly fit, estimate, or infer by minimizing a loss. Yet most real-world objectives are nonlinear and messy; solving them globally can be hard.</p> <p>Convex optimization is the tractable core of this landscape. A convex problem has a single \u201cbowl-shaped\u201d valley, no false minima, so any local optimum is automatically global. That geometric simplicity gives us:</p> <ul> <li>mathematical guarantees of optimality,  </li> <li>efficient algorithms that scale,  </li> <li>robustness to noise and initialization.</li> </ul> <p>Convexity leads to robustness. Small perturbations to problem data typically result in small changes to the solution in convex optimization, a desirable feature in practice. Moreover, many approximation techniques (like convex relaxations of NP-hard problems) rely on solving a convex problem to get bounds or heuristic solutions for the original task. </p>"},{"location":"convex/11_intro/#12-canonical-convex-problem-form","title":"1.2 Canonical Convex Problem Form","text":"<p>A generic convex optimization problem can be written as:</p> \\[ \\begin{array}{ll} \\text{minimize}_{x} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i = 1,\\dots,m, \\\\                   &amp; h_j(x) = 0,\\quad j = 1,\\dots,p, \\end{array} \\] <p>where each \\(f\\) and \\(g_i\\) is convex, and each \\(h_j\\) is affine (affine functions are both convex and concave).</p>"},{"location":"convex/11_intro/#13-how-the-web-book-is-structured","title":"1.3 How the Web-Book Is Structured","text":"<p>This Web-book builds step-by-step from mathematics to algorithms:</p> <ol> <li>Linear Algebra Foundations (Ch 2) \u2014 geometry of vectors, subspaces, and positive-semidefinite matrices.  </li> <li>Multivariable Calculus (Ch 3) \u2014 gradients, Hessians, and first-/second-order optimality.  </li> <li>Convex Sets (Ch 4) \u2014 feasible regions and geometric intuition.  </li> <li>Convex Functions (Ch 5) \u2014 what makes an objective convex.  </li> <li>Subgradients (Ch 6) \u2014 handling nondifferentiable convex functions (e.g., \\(\\lvert x \\rvert\\), \\(\\max\\)).  </li> <li>KKT Conditions (Ch 7) \u2014 first-order optimality for constrained problems.  </li> <li>Duality (Ch 8) \u2014 lower bounds, certificates of optimality, and geometric interpretation.  </li> <li>Algorithms (Ch 9\u201310) \u2014 gradient, proximal, Newton, stochastic, and ADMM methods.  </li> <li>Modeling and Practice (Ch 11) \u2014 convex modeling patterns, solver selection, ML applications.  </li> <li>Appendices \u2014 common inequalities, projections, and support-function geometry.</li> </ol>"},{"location":"convex/11_intro/#missing-elements","title":"MIssing elements","text":"<ol> <li>Subgradient descent</li> </ol>"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Convex optimisation is geometric. To talk about convex sets, supporting hyperplanes, projections, and quadratic forms, we need linear algebra. This chapter reviews the specific linear algebra tools we will use throughout: vector spaces, inner products, norms, projections, eigenvalues, and positive semidefinite matrices.</p>"},{"location":"convex/12_vector/#21-vector-spaces-subspaces-and-affine-sets","title":"2.1 Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set \\(V\\) equipped with addition and scalar multiplication satisfying the usual axioms: closure, associativity, distributivity, etc. In this book we mostly work with \\(V = \\mathbb{R}^n\\).</p> <p>A subspace \\(S \\subseteq \\mathbb{R}^n\\) is a subset that:</p> <ol> <li>contains \\(0\\),</li> <li>is closed under addition,</li> <li>is closed under scalar multiplication.</li> </ol> <p>For example, the set of all solutions to \\(Ax = 0\\) is a subspace, called the nullspace or kernel of \\(A\\).</p> <p>An affine set is a translated subspace. A set \\(A\\) is affine if for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  Every affine set can be written as  where \\(S\\) is a subspace. Affine sets appear as the solution sets to linear equality constraints \\(Ax = b\\).</p> <p>Affine sets are important in optimisation because:</p> <ul> <li>Feasible sets defined by equality constraints are affine.</li> <li>Affine functions preserve convexity.</li> </ul>"},{"location":"convex/12_vector/#22-linear-combinations-span-basis-dimension","title":"2.2 Linear combinations, span, basis, dimension","text":"<p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form  is a linear combination. The set of all linear combinations is called the span:  </p> <p>A list of vectors is linearly independent if no nontrivial linear combination gives \\(0\\). A basis of a subspace \\(S\\) is a set of linearly independent vectors whose span is \\(S\\). The number of vectors in a basis is the dimension of \\(S\\).</p> <p>Rank and nullity facts:</p> <ul> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states:  where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>In constrained optimisation, \\(\\mathrm{rank}(A)\\) encodes the \u201cnumber of independent constraints\u201d, and the nullspace encodes feasible directions that do not violate certain constraints.</p> <p>Column Space: The column space of a matrix \\( A \\), denoted \\( C(A) \\), is the set of all possible output vectors \\( b \\) that can be written as \\( Ax \\) for some \\( x \\). In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system \\( Ax = b \\) have a solution?\u201d is equivalent to asking whether \\( b \\in C(A) \\). If \\( b \\) lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of \\( A \\), denoted \\( N(A) \\), is the set of all input vectors \\( x \\) that are mapped to zero: \\( N(A) = \\{ x : Ax = 0 \\} \\). It answers a different question: If a solution to \\( Ax = b \\) exists, is it unique? If the null space contains only the zero vector (\\( \\mathrm{nullity}(A) = 0 \\)), the solution is unique. But if \\( N(A) \\) contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix \\( A \\) is a linear combination of others\u2014for example, \\( \\text{feature}_3 = 2 \\times \\text{feature}_1 + \\text{feature}_2 \\)\u2014the columns of \\( A \\) become linearly dependent. This creates a nonzero vector in the null space of \\( A \\), meaning multiple weight vectors \\( x \\) can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and \\( A^\\top A \\) becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Feasible Directions: In a constrained optimization problem of the form \\( Ax = b \\), the null space of \\( A \\) characterizes the directions along which one can move without violating the constraints. If \\( d \\in N(A) \\), then moving from a feasible point \\( x \\) to \\( x + d \\) preserves feasibility, since \\( A(x + d) = Ax + Ad = b + 0 = b \\). Thus, the null space defines the space of free movement\u2014directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of \\( A \\), denoted \\( R(A) \\), is the span of the rows of \\( A \\) (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to \\( \\mathrm{rank}(A) \\). The row space is orthogonal to the null space of \\( A \\): \\( R(A) \\perp N(A) \\). In optimization, the row space corresponds to the set of active constraints or the directions along which changes in \\( x \\) affect the constraints.</p> <p>Left Null Space: The left null space, denoted \\( N(A^\\top) \\), is the set of all vectors \\( y \\) such that \\( A^\\top y = 0 \\). These vectors are orthogonal to the columns of \\( A \\), and therefore orthogonal to the column space itself. In least squares problems, \\( N(A^\\top) \\) represents residual directions\u2014components of \\( b \\) that cannot be explained by the model \\( Ax = b \\).</p> <p>Orthogonality Relationships (Fundamental Theorem of Linear Algebra): For any matrix \\( A \\), there are four key subspaces:  These satisfy the orthogonality relationships:  Together, these subspaces form a complete and mutually orthogonal decomposition of the input and output spaces.</p> <p>Projection Interpretation (Least Squares): When \\( Ax = b \\) has no exact solution (as in overdetermined systems), the least squares solution finds \\( x \\) such that \\( Ax \\) is the projection of \\( b \\) onto the column space of \\( A \\):  and the residual  lies in the left null space \\( N(A^\\top) \\). This provides a geometric view: the solution projects \\( b \\) onto the closest point in the subspace that \\( A \\) can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of \\( A \\) is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where \\( n \\) is the number of columns of \\( A \\). This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in \\( x \\).</p> <p>Geometric Interpretation: - The column space represents all reachable outputs. - The null space represents all indistinguishable inputs that map to zero. - The row space represents all independent constraints imposed by \\( A \\). - The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </p> <p>Together, these four subspaces define the complete geometry of the linear map \\( A: \\mathbb{R}^n \\to \\mathbb{R}^m \\).</p>"},{"location":"convex/12_vector/#23-inner-products-and-orthogonality","title":"2.3 Inner products and orthogonality","text":"<p>An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\):  with equality iff \\(x\\) and \\(y\\) are linearly dependent Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#24-norms-and-distances","title":"2.4 Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms: Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), and the dual of \\(\\ell_2\\) is itself.</p> <p>Imagine the vector \\(x\\) lives inside the original norm ball (\\(\\|x\\| \\le 1\\)). The term \\(x^\\top y\\) is the dot product, which measures the alignment between \\(x\\) and \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible value you can get by taking the dot product of \\(y\\) with any vector \\(x\\) that fits inside the original norm ball.If the dual norm \\(\\|y\\|_*\\) is large, it means \\(y\\) is strongly aligned with a direction \\(x\\) that is \"small\" (size \\(\\le 1\\)) according to the original norm.If the dual norm is small, \\(y\\) must be poorly aligned with all vectors \\(x\\) in the ball.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not (Boyd and Vandenberghe, 2004). This is the algebraic test for convexity of quadratic forms.</p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p>"},{"location":"convex/12_vector/#26-orthogonal-projections-and-least-squares","title":"2.6 Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\).</p> <p>If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#27-advanced-concepts","title":"2.7 Advanced Concepts","text":"<p>Operator norm: Given a matrix (linear map) \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and given a choice of vector norms on input and output, one can define the induced operator norm. If we use \\(|\\cdot|_p\\) on \\(\\mathbb{R}^n\\) and \\(|\\cdot|_q\\) on \\(\\mathbb{R}^m\\), the operator norm is</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q \\] <p>This gives the maximum factor by which \\(A\\) can stretch a vector (measuring \\(x\\) in norm \\(p\\) and \\(Ax\\) in norm \\(q\\)).pecial cases are common: with \\(p = q = 2\\), \\(|A|_{2 \\to 2}\\) (often just written \\(|A|_2\\)) is the spectral norm, which equals the largest singular value of \\(A\\) (more on singular values below). If \\(p = q = 1\\), \\(|A|_{1 \\to 1}\\) is the maximum absolute column sum of \\(A\\). If \\(p = q = \\infty\\), \\(|A|{\\infty \\to \\infty}\\) is the maximum absolute row sum.</p> <p>Operator norms tell us the worst-case amplification of signals by \\(A\\). In gradient descent on \\(f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x\\) (a quadratic form), the step size must be \\(\\le \\tfrac{2}{|A|_2}\\) for convergence; here \\(|A|_2 = \\lambda_{\\max}(A)\\) if \\(A\\) is symmetric (it\u2019s related to Hessian eigenvalues, Chapter 5). In general, controlling \\(|A|\\) controls stability: if \\(|A| &lt; 1\\), the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p> <p>Singular Value Decomposition (SVD): Any matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as</p> \\[ A = U \\Sigma V^\\top \\] <p>where \\(U \\in \\mathbb{R}^{m\\times m}\\) and \\(V \\in \\mathbb{R}^{n\\times n}\\) are orthogonal matrices (their columns are orthonormal bases of \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\), respectively), and \\(\\Sigma\\) is an \\(m\\times n\\) diagonal matrix with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0\\) on the diagonal. The \\(\\sigma_i\\) are the singular values of \\(A\\). Geometrically, \\(A\\) sends the unit ball in \\(\\mathbb{R}^n\\) to an ellipsoid in \\(\\mathbb{R}^m\\) whose principal semi-axes lengths are the singular values and directions are the columns of \\(V\\) (mapped to columns of \\(U\\)). The largest singular value \\(\\sigma_{\\max} = |A|_2\\) is the spectral norm. The smallest (if \\(n \\le m\\), \\(\\sigma{\\min}\\) of those \\(n\\)) indicates how \\(A\\) contracts the least \u2013 if \\(\\sigma_{\\min} = 0\\), \\(A\\) is rank-deficient.</p> <p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number \\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\) (when \\(\\sigma_{\\min}&gt;0\\)), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in \\(x\\)-space hardly change \\(Ax\\) (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means \\(A\\) is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of \\(A\\) (Eckart\u2013Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p> <p>Low-rank structure: The rank of \\(A\\) equals the number of nonzero singular values. If \\(A\\) has rank \\(r \\ll \\min(n,m)\\), it means \\(A\\) effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem\u2019s data matrix \\(A\\) is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p> <p>Operator norm in optimization: Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\), the Hessian is \\(Q\\) and gradient descent converges if \\(\\alpha &lt; 2/\\lambda_{\\max}(Q)\\). Preconditioning aims to transform \\(Q\\) into one with a smaller condition number by multiplying by some \\(P\\) (like using \\(P^{-1}Q\\)) \u2014 effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex \\(f\\), the Lipschitz constant of \\(\\nabla f\\) (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p> <p>Summary of spectral properties:</p> <ul> <li> <p>The spectral norm \\(|A|_2 = \\sigma_{\\max}(A)\\) quantifies the largest stretching. It determines stability and step sizes.</p> </li> <li> <p>The smallest singular value \\(\\sigma_{\\min}\\) (if \\(A\\) is tall full-rank) tells if \\(A\\) is invertible and how sensitive the inverse is. If \\(\\sigma_{\\min}\\) is tiny, small changes in output cause huge changes in solving \\(Ax=b\\).</p> </li> <li> <p>The condition number \\(\\kappa = \\sigma_{\\max}/\\sigma_{\\min}\\) is a figure of merit for algorithms: gradient descent iterations needed often scale with \\(\\kappa\\) (worse conditioning = slower). Regularization like adding \\(\\mu I\\) increases \\(\\sigma_{\\min}\\), thereby reducing \\(\\kappa\\) and accelerating convergence (at the expense of bias).</p> </li> <li> <p>Nuclear norm (sum of singular values) and spectral norm often appear in optimization as convex surrogates for rank and as constraints to limit the operator\u2019s impact.</p> </li> </ul> <p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimisation is about finding points that minimise (or maximise) a function. To do that analytically, we need to understand gradients, Hessians, Taylor expansions, and first-/second-order optimality conditions.</p> <p>We work in \\(\\mathbb{R}^n\\).</p>"},{"location":"convex/13_calculus/#31-gradients-jacobians-and-hessians","title":"3.1 Gradients, Jacobians, and Hessians","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The gradient of \\(f\\) at \\(x\\) is the column vector  It points in the direction of steepest increase of \\(f\\).</p> <p>The directional derivative in direction \\(u\\) is \\(D_u f(x) = \\lim_{t\\to0} \\frac{f(x+tu)-f(x)}{t} = \\langle \\nabla f(x), u\\rangle\\). This shows how the gradient inner product with \\(u\\) gives the instantaneous rate of change of \\(f\\) along \\(u\\). In particular, \\(D_u f(x)\\) is maximized when \\(u\\) points along \\(\\nabla f(x)\\) (steepest ascent) and minimized when \\(u\\) is opposite.</p> <p>If \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian \\(J_F(x)\\) is the \\(m \\times n\\) matrix of partial derivatives.</p> <p>Gradient Lipschitz continuity: A concept often used in convergence analysis is Lipschitz continuity of the gradient. If there exists \\(L\\) such that \\(|\\nabla f(x) - \\nabla f(y)| \\le L |x-y|\\) for all \\(x,y\\), we say the gradient is \\(L\\)-Lipschitz (or \\(f\\) is \\(L\\)-smooth). \\(L\\) is essentially an upper bound on the Hessian eigenvalues (for \\(\\ell_2\\) norm): \\(L \\ge \\lambda_{\\max}(\\nabla^2 f(x))\\) for all \\(x\\). Smoothness is important because it ensures gradient descent with step \\(\\alpha = 1/L\\) converges, and it gives a bound \\(f(x_{k+1}) \\le f(x_k) - \\frac{1}{2L}|\\nabla f(x_k)|^2\\) (so the function value decreases at least proportionally to the squared gradient norm). Many convex functions in optimization are \\(L\\)-smooth (e.g. quadratic forms with \\(\\lambda_{\\max}(Q)=L\\)). Smoothness together with strong convexity (defined shortly) yields linear convergence rates for gradient descent.</p> <p>Strong convexity: A differentiable function \\(f\\) is \\(\\mu\\)-strongly convex if \\(f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2}|y-x|^2\\) for all \\(x,y\\). Equivalently, \\(f(x) - \\frac{\\mu}{2}|x|^2\\) is convex, which implies \\(\\nabla^2 f(x) \\succeq \\mu I\\) (Hessian bounded below by \\(\\mu\\)) when \\(f\\) is twice differentiable. Strong convexity means \\(f\\) has a quadratic curvature of at least \\(\\mu\\) \u2013 it grows at least as fast as a parabola. Strongly convex functions have unique minimizers (the bowl can\u2019t flatten out). They also yield much faster convergence: for \\(\\mu\\)-strongly convex and \\(L\\)-smooth \\(f\\), gradient descent with \\(\\alpha=1/L\\) converges like \\((1-\\mu/L)^k\\) (linear rate). Intuitively, the condition number \\(\\kappa = L/\\mu\\) comes into play. Examples: the quadratic form above is strongly convex with \\(\\mu = \\lambda_{\\min}(Q)\\). Adding a small ridge term \\(\\frac{\\mu}{2}|x|^2\\) to any convex \\(f\\) makes it \\(\\mu\\)-strongly convex and improves conditioning at the cost of bias.</p> <p>The Hessian of \\(f\\) is the \\(n \\times n\\) matrix of second partial derivatives:  </p> <p>If \\(f\\) is twice continuously differentiable, then \\(\\nabla^2 f(x)\\) is symmetric (Clairaut\u2019s theorem).</p> <p>Example \u2013 quadratic function: \\(f(x) = \\frac{1}{2}x^TQx - b^T x\\). Here \\(\\nabla f(x) = Qx - b\\) (linear), and \\(\\nabla^2 f(x) = Q\\). Solving \\(\\nabla f=0\\) yields \\(Qx=b\\), so if \\(Q \\succ 0\\) the unique minimizer is \\(x^* = Q^{-1}b\\). The Hessian being \\(Q \\succ 0\\) confirms convexity. If \\(Q\\) has large eigenvalues, gradient \\(Qx - b\\) changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of \\(Q\\) controls difficulty of minimizing \\(f\\).</p>"},{"location":"convex/13_calculus/#32-first-order-taylor-approximation","title":"3.2 First-order Taylor approximation","text":"<p>For differentiable \\(f\\), we have the first-order Taylor expansion around \\(x\\):  </p> <p>Interpretation:</p> <ul> <li>\\(\\nabla f(x)\\) gives the best linear approximation.</li> <li>The linear model predicts how \\(f\\) changes if we move by \\(d\\).</li> </ul> <p>This is the basis of first-order optimisation methods like gradient descent.</p>"},{"location":"convex/13_calculus/#33-second-order-taylor-approximation","title":"3.3 Second-order Taylor approximation","text":"<p>If \\(f\\) is twice differentiable, then  </p> <p>If \\(\\nabla^2 f(x)\\) is positive semidefinite, the quadratic term is always \\(\\ge 0\\). Locally, \\(x\\) is in a \u201cbowl\u201d. If \\(\\nabla^2 f(x)\\) is indefinite, the landscape can curve up in some directions and down in others \u2014 typical of saddle points.</p>"},{"location":"convex/13_calculus/#34-unconstrained-optimality-conditions","title":"3.4 Unconstrained optimality conditions","text":"<p>Suppose we want to solve  with no constraints.</p> <p>A point \\(x^*\\) is called a critical point if \\(\\nabla f(x^*) = 0\\).</p> <ul> <li> <p>First-order necessary condition:   If \\(x^*\\) is a (local) minimiser and \\(f\\) is differentiable, then    </p> </li> <li> <p>Second-order necessary condition:   If \\(x^*\\) is a (local) minimiser and \\(f\\) is twice differentiable,    </p> </li> <li> <p>Second-order sufficient condition:   If      then \\(x^*\\) is a strict local minimiser.</p> </li> </ul> <p>Now, here is where convexity changes everything.</p> <p>If \\(f\\) is convex, then any point \\(x^*\\) with \\(\\nabla f(x^*) = 0\\) is not just a local minimiser \u2014 it is a global minimiser (Boyd and Vandenberghe, 2004). No second-order check is even needed.</p>"},{"location":"convex/13_calculus/#35-gradients-as-normals-to-level-sets","title":"3.5 Gradients as normals to level sets","text":"<p>A level set of a differentiable function \\(f\\) is  </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Intuitively, the level set is like a contour line, and the gradient is perpendicular to it, pointing toward larger values of \\(f\\).</p> <p>In optimisation, if we want to decrease \\(f\\), we move roughly in direction \\(-\\nabla f(x)\\).</p> <p>This geometric fact recurs later in constrained optimisation and KKT: at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#36-convexity-and-the-hessian","title":"3.6 Convexity and the Hessian","text":"<p>If \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is twice differentiable, then:</p> <ul> <li>\\(f\\) is convex if and only if \\(\\nabla^2 f(x)\\) is positive semidefinite for all \\(x\\) in its domain (Boyd and Vandenberghe, 2004).</li> <li>\\(f\\) is strictly convex if \\(\\nabla^2 f(x)\\) is positive definite for all \\(x\\).</li> </ul>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Optimisation problems almost always include constraints. The feasible region, the set of points allowed by those constraints, is often a convex set. This chapter builds geometric intuition for convex sets, affine sets, hyperplanes, polyhedra, and supporting hyperplanes.</p>"},{"location":"convex/14_convexsets/#41-convex-sets","title":"4.1 Convex sets","text":"<p>A set \\(C \\subseteq \\mathbb{R}^n\\) is convex if for any \\(x,y \\in C\\) and any \\(\\theta \\in [0,1]\\),  </p> <p>For any two feasible points, the entire line segment between them is also feasible. No \u201cindentations.\u201d</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>An affine subspace: \\(\\{ x : Ax = b \\}\\).</li> <li>A halfspace: \\(\\{ x : a^\\top x \\le b \\}\\).</li> <li>An \\(\\ell_2\\) ball: \\(\\{ x : \\|x\\|_2 \\le r \\}\\).</li> <li>An \\(\\ell_\\infty\\) ball: \\(\\{ x : \\|x\\|_\\infty \\le r \\}\\), which is a box.</li> <li>The probability simplex: \\(\\{ x \\in \\mathbb{R}^n : x \\ge 0, \\sum_i x_i = 1 \\}\\).</li> </ul> <p>A set that is not convex: a crescent shape or annulus. The defining failure is: there exist \\(x,y\\) in the set such that some convex combination leaves the set.</p>"},{"location":"convex/14_convexsets/#42-affine-sets-hyperplanes-and-halfspaces","title":"4.2 Affine sets, hyperplanes, and halfspaces","text":"<p>An affine set is a translate of a subspace:  where \\(S\\) is a subspace. Affine sets are convex.</p> <p>A hyperplane in \\(\\mathbb{R}^n\\) is a set of the form  for some nonzero \\(a \\in \\mathbb{R}^n\\).</p> <p>A halfspace is  </p> <p>Halfspaces are convex; intersections of halfspaces are convex. Linear inequality constraints define intersections of halfspaces, and therefore give convex feasible regions.</p>"},{"location":"convex/14_convexsets/#43-convex-combinations-convex-hulls","title":"4.3 Convex combinations, convex hulls","text":"<p>A convex combination of points \\(x_1,\\dots,x_k\\) is  </p> <p>The convex hull of a set \\(S\\) is the set of all convex combinations of finitely many points of \\(S\\). It is the \u201csmallest\u201d convex set containing \\(S\\).</p> <p>Convex hulls matter because:</p> <ul> <li>Polytopes (bounded polyhedra) can be described as convex hulls of finitely many points (their vertices).</li> <li>Many relaxations in optimisation replace a complicated nonconvex feasible set by its convex hull.</li> </ul>"},{"location":"convex/14_convexsets/#44-polyhedra-and-polytopes","title":"4.4 Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are convex and cane be unbounded. If \\(P\\) is also bounded, it is called a polytope.</p> <p>In linear programming, we minimise a linear objective \\(c^\\top x\\) over a polyhedron. The optimal solution, if it exists, is always attained at an extreme point (vertex) of the feasible polyhedron.</p>"},{"location":"convex/14_convexsets/#45-extreme-points","title":"4.5 Extreme points","text":"<p>Let \\(C\\) be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be expressed as a strict convex combination of two other distinct points in \\(C\\). Formally, \\(x\\) is extreme in \\(C\\) if whenever  then \\(y = z = x\\).</p> <p>Geometric meaning:</p> <ul> <li>Extreme points are the \u201ccorners.\u201d</li> <li>In a polytope, extreme points are precisely the vertices.</li> </ul> <p>This is why linear programming solutions are found at vertices.</p>"},{"location":"convex/14_convexsets/#46-cones","title":"4.6 Cones","text":"<p>A set \\(K\\) is a cone if for any \\(x \\in K\\) and \\(\\alpha \\ge 0\\), \\(\\alpha x \\in K\\). A cone is convex if additionally \\(x,y\\in K\\) implies \\(x+y \\in K\\). Convex cones are important (e.g. nonnegative orthant, PSD matrices cone) because many optimization problems can be cast as cone programs. Cones have extreme rays instead of points (directions that generate edges of the cone). For instance, the extreme rays of the positive orthant in \\(\\mathbb{R}^n\\) are the coordinate axes (each axis direction can\u2019t be formed by positive combos of others).</p> <ul> <li>Cones are closed under nonnegative scaling, but not necessarily addition.  </li> <li>Conic hull (convex cone): Collection of all conic combinations of points in \\(S\\).</li> <li>A cone is not necessarily a subspace (negative multiples may not be included).  </li> <li>A convex cone is closed under addition and nonnegative scaling.  </li> <li> <p>Polar Cones: Given a cone \\(K \\subseteq \\mathbb{R}^n\\), the polar cone is:</p> \\[ K^\\circ = \\{ y \\in \\mathbb{R}^n \\mid \\langle y, x \\rangle \\le 0, \\; \\forall x \\in K \\}. \\] <ul> <li>Intuition: polar cone vectors form non-acute angles with every vector in \\(K\\).  </li> <li>Properties:  <ul> <li>Always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, \\(K^\\circ\\) is the orthogonal complement.  </li> <li>Duality: \\((K^\\circ)^\\circ = K\\) for closed convex cones.  </li> </ul> </li> </ul> </li> <li> <p>Tangent Cone: For a set \\(C\\) and point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) contains all directions in which one can \u201cmove infinitesimally\u201d while remaining in \\(C\\):</p> \\[ T_C(x) = \\Big\\{ d \\in \\mathbb{R}^n \\;\\Big|\\; \\exists t_k \\downarrow 0, \\; x_k \\in C, \\; x_k \\to x, \\; \\frac{x_k - x}{t_k} \\to d \\Big\\}. \\] <ul> <li>Interior point: \\(T_C(x) = \\mathbb{R}^n\\).  </li> <li>Boundary point: \\(T_C(x)\\) restricts movement to directions staying inside \\(C\\). </li> <li>Tangent cones define feasible directions for projected gradient steps or constrained optimization.</li> </ul> </li> <li> <p>Normal Cone: For a convex set \\(C\\) at point \\(x \\in C\\):      </p> <ul> <li>Each \\(v \\in N_C(x)\\) defines a supporting hyperplane at \\(x\\).  </li> <li>Relation: \\(N_C(x) = \\big(T_C(x)\\big)^\\circ\\) \u2014 polar of tangent cone.  </li> <li>Interior point: \\(N_C(x) = \\{0\\}\\).  </li> <li>Boundary/corner: \\(N_C(x)\\) is a cone of outward normals.- Appears in first-order optimality conditions:  where the subgradient of \\(f\\) is balanced by the \u201cpush-back\u201d of constraints.</li> </ul> </li> </ul>"},{"location":"convex/14_convexsets/#47-supporting-hyperplanes-and-separation","title":"4.7 Supporting hyperplanes and separation","text":"<p>Convex sets can be \u201ctouched\u201d or \u201cseparated\u201d by hyperplanes.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting hyperplane theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty closed convex set, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero \\(a\\) such that  In words, there is a hyperplane \\(a^\\top x = a^\\top x_0\\) that \u201csupports\u201d \\(C\\) at \\(x_0\\): it touches \\(C\\) but does not cut through it.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating hyperplane theorem","text":"<p>If \\(C\\) and \\(D\\) are two disjoint nonempty convex sets, then there exists a hyperplane that separates them: some nonzero \\(a\\) and scalar \\(b\\) such that  </p> <p>Why do we care?</p> <ul> <li>These theorems are the geometric heart of duality.</li> <li>KKT conditions can be interpreted as existence of a supporting hyperplane that is simultaneously aligned with objective and constraints.</li> <li>Subgradients of convex functions correspond to supporting hyperplanes of epigraphs.</li> </ul>"},{"location":"convex/14_convexsets/#47-feasible-regions-in-convex-optimisation","title":"4.7 Feasible regions in convex optimisation","text":"<p>In convex optimisation, the feasible set is typically something like  </p> <ul> <li>If each \\(g_i\\) is convex and each \\(h_j\\) is affine, then the feasible set is convex.</li> <li>If \\(f\\) is also convex, then the entire problem is a convex optimisation problem.</li> </ul> <p>Thus, convex sets formalise \u201cwhat it means for feasible directions to be allowed,\u201d and this is what gives us global optimality guarantees later.</p>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>Convex functions are the objectives we minimise. Understanding them is essential, because convexity of the objective is what turns an optimisation problem from \"possibly impossible\" to \"provably solvable\".</p>"},{"location":"convex/15_convexfunctions/#51-definitions-of-convexity","title":"5.1 Definitions of convexity","text":""},{"location":"convex/15_convexfunctions/#511-basic-definition","title":"5.1.1 Basic definition","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),</p> \\[ f(\\theta x + (1-\\theta) y) \\le \\theta f(x) + (1-\\theta) f(y)~. \\] <p>If the inequality is strict whenever \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\), then \\(f\\) is strictly convex.</p>"},{"location":"convex/15_convexfunctions/#512-epigraph-definition","title":"5.1.2 Epigraph definition","text":"<p>The epigraph of \\(f\\) is  </p> <p>\\(f\\) is convex if and only if \\(\\mathrm{epi}(f)\\) is a convex set. This links convex functions to convex sets, and unlocks the geometry: tangent hyperplanes to \\(\\mathrm{epi}(f)\\) correspond to subgradients (Chapter 6).</p>"},{"location":"convex/15_convexfunctions/#52-first-order-characterisation","title":"5.2 First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y - x) \\quad \\text{for all } x,y. \\] <p>Interpretation:</p> <ul> <li>The first-order Taylor approximation is always a global underestimator.</li> <li>The gradient at \\(x\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x, f(x))\\).</li> </ul> <p>This inequality is sometimes called the first-order condition for convexity.</p> <p>This is a powerful characterization: it says the tangent hyperplane at any point \\(x\\) lies below the graph of \\(f\\) everywhere. In other words, the gradient at \\(x\\) provides a global underestimator of \\(f\\) (supporting hyperplane to epigraph). Geometrically, this means no tangent line ever goes above the function. </p> <p>For a convex differentiable \\(f\\), we have \\(f(y) - f(x) \\ge \\nabla f(x)^T (y-x)\\), so moving from \\(x\\) in any direction, the actual increase in \\(f\\) is at least as large as the linear prediction by \\(\\nabla f(x)\\) (since the function bends upward or straight). At optimum \\(\\hat{x}\\), a necessary and sufficient condition (for convex differentiable \\(f\\)) is \\(\\nabla f(\\hat{x}) = 0\\). This ties to optimality: \\(\\nabla f(\\hat{x})=0\\) means \\(f(y)\\ge f(\\hat{x}) + \\nabla f(\\hat{x})^T (y-\\hat{x}) = f(\\hat{x})\\) for all \\(y\\), so \\(\\hat{x}\\) is global minimizer.</p> <p>If \\(f\\) is not differentiable, a similar condition holds with subgradients: \\(f\\) is convex iff for all \\(x,y\\) there exists a (sub)gradient \\(g \\in \\partial f(x)\\) such that \\(f(y) \\ge f(x) + g^T(y-x)\\). The set of all subgradients \\(\\partial f(x)\\) is a convex set (the subdifferential). At optimum, \\(0 \\in \\partial f(\\hat{x})\\) is the condition. For example, \\(f(x)=|x|\\) has subgradient \\(g=\\mathrm{sign}(x)\\) (multivalued at 0, where any \\(g\\in[-1,1]\\) is subgradient). Setting \\(0\\) in subdifferential yields \\(0\\in[-1,1]\\), so indeed \\(x^=0\\) is minimizer.</p>"},{"location":"convex/15_convexfunctions/#53-second-order-characterisation","title":"5.3 Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then \\(f\\) is convex if and only if its Hessian is positive semidefinite everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{for all } x~. \\] <p>If \\(\\nabla^2 f(x) \\succ 0\\) for all \\(x\\), then \\(f\\) is strictly convex.</p>"},{"location":"convex/15_convexfunctions/#54-examples-of-convex-functions","title":"5.4 Examples of convex functions","text":"<ol> <li> <p>Affine functions: \\(f(x) = a^\\top x + b\\).    Always convex (and concave).</p> </li> <li> <p>Quadratic functions with PSD Hessian: \\(f(x) = \\tfrac12 x^\\top Q x + c^\\top x + d\\),    where \\(Q \\succeq 0\\) (symmetric positive semidefinite).    Convex because \\(\\nabla^2 f(x) = Q \\succeq 0\\).</p> </li> <li> <p>Norms: \\(f(x) = \\|x\\|\\) for any norm.    All norms are convex.</p> </li> <li> <p>Maximum of affine functions: \\(f(x) = \\max_i (a_i^\\top x + b_i)\\).    Convex because it is the pointwise maximum of convex functions.</p> </li> <li> <p>Log-sum-exp function: \\(f(x) = \\log \\left( \\sum_{i=1}^k \\exp(a_i^\\top x + b_i) \\right)\\).    This function is convex and is ubiquitous in statistics and machine learning (softmax, logistic regression). The convexity follows from Jensen\u2019s inequality and properties of the exponential (Boyd and Vandenberghe, 2004).</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#55-jensens-inequality","title":"5.5 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable taking values in the domain of \\(f\\). Then  </p> <p>This is Jensen\u2019s inequality. It generalises the definition of convexity from two-point averages to arbitrary expectations. As a special case, for scalars \\(x_1,\\dots,x_n\\) and weights \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality has many uses: in machine learning, it justifies algorithms like EM (which use the inequality to create surrogate objectives), and it provides bounds like \\(\\log(\\mathbb{E}[e^X]) \\ge \\mathbb{E}[X]\\) (by convexity of \\(\\log\\) or \\(e^x\\)). As a simple example, taking \\(f(x)=x^2\\) and \\(X\\) uniform in \\({-1,1}\\), Jensen says \\((\\mathbb{E}[X])^2 = 0^2 \\le \\mathbb{E}[X^2] = 1\\), which is true. Or \\(f(x)=\\frac{1}{x}\\) convex on \\((0,\\infty)\\) implies \\(\\frac{1}{\\mathbb{E}[X]} \\le \\mathbb{E}[\\frac{1}{X}]\\) for positive \\(X\\). In optimization, Jensen\u2019s inequality often helps in proving convexity of expectations: if you mix some distributions or uncertain inputs, the expected loss is convex if the loss function is convex.</p>"},{"location":"convex/15_convexfunctions/#56-operations-that-preserve-convexity","title":"5.6 Operations that preserve convexity","text":"<p>If \\(f\\) and \\(g\\) are convex, then:</p> <ul> <li>\\(f + g\\) is convex.</li> <li>\\(\\alpha f\\) is convex for any \\(\\alpha \\ge 0\\).</li> <li>\\(\\max\\{f,g\\}\\) is convex.</li> <li>Composition with an affine map preserves convexity:   If \\(A\\) is a matrix and \\(b\\) a vector, then \\(x \\mapsto f(Ax + b)\\) is convex.</li> </ul> <p>If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex, then the composition \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex. This helps you build new convex functions from known ones.</p>"},{"location":"convex/15_convexfunctions/#57-level-sets-of-convex-functions","title":"5.7 Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), define the sublevel set  </p> <p>If \\(f\\) is convex, then every sublevel set is convex (Boyd and Vandenberghe, 2004). This is crucial: constraints of the form \\(f(x) \\le \\alpha\\) are convex constraints.</p> <p>For example, the set  is convex because \\(x \\mapsto \\|Ax-b\\|_2\\) is convex.</p>"},{"location":"convex/15_convexfunctions/#58-strict-and-strong-convexity","title":"5.8 Strict and strong convexity","text":"<ul> <li> <p>\\(f\\) is strictly convex if  for all \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\).</p> </li> <li> <p>\\(f\\) is strongly convex with parameter \\(m&gt;0\\) if  </p> </li> </ul> <p>Strong convexity implies a unique minimiser and gives quantitative convergence rates for gradient methods.</p>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many of the most important convex functions are not differentiable everywhere:</p> <ul> <li>\\(\\|x\\|_1 = \\sum_i |x_i|\\) has corners at \\(x_i = 0\\),</li> <li>\\(f(x) = \\max\\{a_1^\\top x + b_1, \\dots, a_k^\\top x + b_k\\}\\) is piecewise affine,</li> <li>the hinge loss \\(\\max\\{0, 1 - y w^\\top x\\}\\) (used in SVMs) is not smooth at the kink.</li> </ul> <p>For a convex but nonsmooth \\(f\\), the usual condition \u201c\\(\\nabla f(x^*) = 0\\)\u201d may not make sense, because \\(\\nabla f(x^*)\\) may not exist. But geometrically, convex functions still have supporting hyperplanes at every point. That is the key.</p>"},{"location":"convex/16_subgradients/#61-subgradients-and-the-subdifferential","title":"6.1 Subgradients and the subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex. A vector \\(g \\in \\mathbb{R}^n\\) is called a subgradient of \\(f\\) at \\(x\\) if, for all \\(y\\),  </p> <p>Interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top (y-x)\\) is a global underestimator of \\(f\\).</li> <li>\\(g\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x,f(x))\\).</li> </ul> <p>The set of all subgradients of \\(f\\) at \\(x\\) is called the subdifferential of \\(f\\) at \\(x\\):  </p> <p>If \\(f\\) is differentiable at \\(x\\), then  </p> <p>If \\(f\\) is not differentiable at \\(x\\), \\(\\partial f(x)\\) is typically a nonempty convex set.</p>"},{"location":"convex/16_subgradients/#62-examples","title":"6.2 Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\).</p> <ul> <li>For \\(t&gt;0\\), \\(\\partial f(t) = \\{1\\}\\).</li> <li>For \\(t&lt;0\\), \\(\\partial f(t) = \\{-1\\}\\).</li> <li>For \\(t=0\\),     </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) is a valid supporting line from below.</p>"},{"location":"convex/16_subgradients/#ell_1-norm","title":"\\(\\ell_1\\) norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\), we have  So</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This is exactly what shows up in LASSO optimality conditions in statistics.</p>"},{"location":"convex/16_subgradients/#pointwise-max-of-affine-functions","title":"Pointwise max of affine functions","text":"<p>Let  </p> <p>If a single index \\(i^*\\) achieves the max at \\(x\\), then  </p> <p>If multiple \\(i\\) are tied at the max, then  the convex hull of all active slopes.</p>"},{"location":"convex/16_subgradients/#63-subgradient-optimality-condition","title":"6.3 Subgradient optimality condition","text":"<p>Suppose we want to solve the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>Then a point \\(x^*\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^*). \\] <p>At the minimiser, there is no subgradient pointing into a direction that would reduce \\(f\\).</p>"},{"location":"convex/16_subgradients/#64-subgradient-calculus-useful-rules","title":"6.4 Subgradient calculus (useful rules)","text":"<p>If \\(f\\) and \\(g\\) are convex:</p> <ul> <li> <p>\\(\\partial (f+g)(x) \\subseteq \\partial f(x) + \\partial g(x)\\), i.e.    </p> </li> <li> <p>If \\(A\\) is a matrix and \\(h(x) = f(Ax)\\), then    </p> </li> <li> <p>If \\(f(x) = \\max_i f_i(x)\\) and each \\(f_i\\) is convex, then    </p> </li> </ul> <p>These rules make it possible to compute subgradients of complicated nonsmooth objectives.</p>"},{"location":"convex/16_subgradients/#65-applications-in-machine-learning","title":"6.5 Applications in Machine Learning","text":"<ul> <li> <p>Nonsmooth Optimization:   Subgradient descent and proximal algorithms solve problems with nonsmooth losses (hinge, absolute error, \\(\\ell_1\\) penalties).  </p> <ul> <li>Lasso (\\(\\ell_1\\) regularization): \\(\\min \\|Xw - y\\|_2^2 + \\lambda \\|w\\|_1\\)</li> <li>SVM (hinge loss): \\(\\min \\frac{1}{2}\\|w\\|^2 + C \\sum_i \\max(0, 1 - y_i w^T x_i)\\)</li> </ul> </li> <li> <p>Regularization and Sparsity:   Subgradients characterize the behavior of norms at the origin \u2013 leading to sparsity in optimal solutions (especially \\(\\ell_1\\) norms).</p> </li> <li> <p>Duality and Proximal Methods:   Many primal-dual algorithms rely on subgradients for deriving dual objectives and implementing updates via proximal operators.</p> </li> <li> <p>Composite Optimization:   Problems of the form \\(f(x) + g(x)\\), where \\(f\\) is smooth and \\(g\\) is nonsmooth (e.g. Lasso), rely on subgradients of \\(g\\).</p> </li> </ul>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-optimality-conditions-in-convex-optimization","title":"Chapter 7: First-Order Optimality Conditions in Convex Optimization","text":"<p>Convex optimization enjoys a remarkable property: every local minimum is also a global minimum. This chapter develops the unified first-order conditions that determine whether a point is optimal \u2014 in both unconstrained and constrained convex problems, smooth or nonsmooth.</p> <p>These conditions form the conceptual bridge between the subgradient theory of Chapter 6 and the KKT framework of Chapter 8.</p>"},{"location":"convex/16a_optimality_conditions/#71-why-optimality-conditions-matter","title":"7.1 Why Optimality Conditions Matter","text":"<p>Knowing when a point is optimal is essential for:</p> <ul> <li>Certifying convergence of algorithms (e.g., gradient or proximal methods),</li> <li>Understanding how constraints affect the solution,</li> <li>Handling nonsmooth objectives (e.g., hinge loss, \\(\\ell_1\\) regularization),</li> <li>Building geometric intuition about \u201cstationarity.\u201d</li> </ul>"},{"location":"convex/16a_optimality_conditions/#72-unconstrained-convex-problems","title":"7.2 Unconstrained Convex Problems","text":"<p>We start with the simplest case:  where \\( f \\) is convex.</p>"},{"location":"convex/16a_optimality_conditions/#a-smooth-case","title":"(a) Smooth Case","text":"<p>If \\( f \\) is differentiable, a point \\(\\hat{x}\\) is optimal iff  </p> <p>This is the classical first-order condition.</p> <p>Examples</p> Function Gradient Optimum \\(f(x)=x^2-4x+7\\) \\(2x-4\\) \\(\\hat{x}=2\\) \\(f(x)=\\sum_i (x-y_i)^2\\) \\(2n(x-\\bar{y})\\) \\(\\hat{x}=\\bar{y}\\) (the mean)"},{"location":"convex/16a_optimality_conditions/#b-nonsmooth-case","title":"(b) Nonsmooth Case","text":"<p>When \\(f\\) is convex but nondifferentiable, we use subgradients (Chapter 6).</p> <p>Optimality condition:  where \\(\\partial f(x)\\) is the subdifferential set.</p> <p>Examples</p> Function Subgradient Optimum (f(x)= x ) (f(x)=\\sum_i x-y_i ) <p>For convex \\(f\\), any \\(\\hat{x}\\) satisfying \\(0\\in\\partial f(\\hat{x})\\) is globally optimal.</p>"},{"location":"convex/16a_optimality_conditions/#73-constrained-convex-problems","title":"7.3 Constrained Convex Problems","text":"<p>Let \\( f:\\mathbb{R}^n\\to\\mathbb{R} \\) be convex and \\( \\mathcal{X}\\subseteq\\mathbb{R}^n \\) be a convex feasible set. We consider:  </p> <p>The geometry of \\( \\mathcal{X} \\) now interacts with the stationarity condition.</p>"},{"location":"convex/16a_optimality_conditions/#a-interior-points","title":"(a) Interior Points","text":"<p>If \\(\\hat{x}\\in\\operatorname{int}(\\mathcal{X})\\), the constraint is inactive locally, so  Same as the unconstrained case.</p>"},{"location":"convex/16a_optimality_conditions/#b-boundary-points-and-normal-cones","title":"(b) Boundary Points and Normal Cones","text":"<p>If \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\), feasible motion is restricted.</p> <p>A point \\(\\hat{x}\\) is optimal iff  where \\( N_{\\mathcal{X}}(\\hat{x}) \\) is the normal cone to \\(\\mathcal{X}\\) at \\(\\hat{x}\\):  </p> <p>Geometric form: for every feasible direction \\(d\\) in the tangent cone \\(T_{\\mathcal{X}}(\\hat{x})\\),  The gradient points outward\u2014any move within \\(\\mathcal{X}\\) would not decrease \\(f\\).</p>"},{"location":"convex/16a_optimality_conditions/#c-unified-compact-form","title":"(c) Unified Compact Form","text":"<p>Both interior and boundary cases can be written as:  </p> <p>This is the general first-order optimality condition for convex problems with convex constraints.</p>"},{"location":"convex/16a_optimality_conditions/#74-tangent-and-normal-cones-intuition","title":"7.4 Tangent and Normal Cones \u2014 Intuition","text":"<ul> <li>Tangent cone \\(T_{\\mathcal{X}}(x)\\): directions one can move within \\(\\mathcal{X}\\).</li> <li>Normal cone \\(N_{\\mathcal{X}}(x)\\): directions pointing outward, orthogonal to all feasible directions.</li> </ul> <p>At an optimum: - Interior point \u2192 gradient = 0. - Boundary point \u2192 gradient points into the normal cone (outward from feasible region).</p> <p>The gradient (or subgradient) must \u201cbalance\u201d the boundary forces \u2014 this geometric picture underlies KKT conditions (next chapter).</p>"},{"location":"convex/16a_optimality_conditions/#75-worked-example","title":"7.5 Worked Example","text":"<p>Minimize  </p> <ul> <li>\\(f'(x)=2x\\)</li> <li>Feasible set \\(\\mathcal{X}=[1,\\infty)\\).</li> </ul> <p>Unconstrained optimum \\(x=0\\) is infeasible. At \\(x=1\\):  \u2705 Condition satisfied \u2192 \\(x^*=1\\) is optimal.</p>"},{"location":"convex/16a_optimality_conditions/#76-summary-table","title":"7.6 Summary Table","text":"Setting Condition for Optimality Unconstrained, smooth \\( \\nabla f(\\hat{x}) = 0 \\) Unconstrained, nonsmooth \\( 0 \\in \\partial f(\\hat{x}) \\) Constrained, general \\( 0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x}) \\)"},{"location":"convex/16a_optimality_conditions/#77-connections-and-outlook","title":"7.7 Connections and Outlook","text":"<ul> <li>The condition \\(0\\in\\partial f(\\hat{x})+N_{\\mathcal{X}}(\\hat{x})\\) is the foundation of projected and proximal methods (see Appendix G).  </li> <li>It generalizes smoothly to inequality/equality constraints, leading to KKT conditions (Chapter 8).  </li> <li>It underlies duality theory (Chapter 9), where these stationarity conditions appear as primal\u2013dual relationships.</li> </ul> <p>Next: In Chapter 8 we extend these first-order ideas to handle general constrained problems via Karush\u2013Kuhn\u2013Tucker (KKT) conditions and Lagrange multipliers.</p>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-optimization-principles-from-gradient-descent-to-kkt","title":"Chapter 8: Optimization Principles \u2013 From Gradient Descent to KKT","text":"<p>At this point we understand:</p> <ul> <li>how to recognise convex functions,</li> <li>how to talk about feasible sets,</li> <li>how to describe optimality with gradients or subgradients.</li> </ul> <p>Now we turn to constrained optimisation. We first recall unconstrained optimisation and gradient descent, then develop the Karush\u2013Kuhn\u2013Tucker (KKT) conditions, which are the first-order optimality conditions for constrained convex optimisation (Boyd and Vandenberghe, 2004).</p>"},{"location":"convex/17_kkt/#81-unconstrained-convex-minimisation","title":"8.1 Unconstrained convex minimisation","text":"<p>Consider  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent is the iterative method:  for some step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Move opposite the gradient to reduce \\(f\\).</li> <li>Under suitable conditions on step size (e.g. Lipschitz gradient), this converges to a global minimiser if \\(f\\) is convex.</li> </ul> <p>If \\(f\\) is strongly convex, we get uniqueness of the minimiser and faster convergence.</p>"},{"location":"convex/17_kkt/#82-equality-constrained-optimisation-and-lagrange-multipliers","title":"8.2 Equality-constrained optimisation and Lagrange multipliers","text":"<p>Consider  where \\(f\\) and each \\(h_j\\) are differentiable.</p> <p>We define the Lagrangian  where \\(\\lambda_j\\) are the Lagrange multipliers.</p> <p>A necessary condition for \\(x^*\\) to be optimal (under suitable regularity assumptions) is:</p> <ol> <li>Stationarity:     </li> <li>Primal feasibility:     </li> </ol> <p>Geometrically, stationarity says: the gradient of \\(f\\) at \\(x^*\\) lies in the span of the gradients of the active constraints. In words, you cannot move in any feasible direction without increasing \\(f\\).</p>"},{"location":"convex/17_kkt/#83-inequality-constraints-and-kkt","title":"8.3 Inequality constraints and KKT","text":"<p>Now consider the general problem:  </p> <p>We form the Lagrangian  with multipliers \\(\\lambda \\in \\mathbb{R}^p\\) (unrestricted) and \\(\\mu \\in \\mathbb{R}^m\\) with \\(\\mu_i \\ge 0\\).</p> <p>The Karush\u2013Kuhn\u2013Tucker (KKT) conditions consist of:</p> <ol> <li> <p>Primal feasibility:     </p> </li> <li> <p>Dual feasibility:     </p> </li> <li> <p>Stationarity:</p> <p>\\(\\nabla f(x^*)    + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*)   + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*)   = 0\\)</p> </li> <li> <p>Complementary slackness:     </p> </li> </ol> <p>Complementary slackness means:</p> <ul> <li>If a constraint \\(g_i(x) \\le 0\\) is strictly inactive at \\(x^*\\) (i.e. \\(g_i(x^*) &lt; 0\\)), then \\(\\mu_i^* = 0\\).</li> <li>If \\(\\mu_i^* &gt; 0\\), then the constraint is tight: \\(g_i(x^*) = 0\\).</li> </ul> <p>This matches geometric intuition: only active constraints can \u201cpush back\u201d on the optimiser.</p>"},{"location":"convex/17_kkt/#84-kkt-and-convexity","title":"8.4 KKT and convexity","text":"<p>For general nonlinear problems, KKT conditions are necessary under regularity assumptions. For convex problems, KKT conditions are often necessary and sufficient for optimality. In other words, if the problem is convex and a point satisfies KKT, that point is globally optimal.</p> <p>This is extremely powerful:</p> <ul> <li>You can certify optimality (and thus global optimality) just by finding multipliers \\(\\lambda^*, \\mu^*\\) that satisfy KKT.</li> <li>KKT conditions are constructive: they are what solvers try to satisfy.</li> </ul>"},{"location":"convex/17_kkt/#85-geometric-picture","title":"8.5 Geometric picture","text":"<p>At the optimal point \\(x^*\\):</p> <ul> <li>\\(\\nabla f(x^*)\\) is balanced by a conic combination of the normals of the active inequality constraints plus a linear combination of the equality constraint normals.</li> <li>The objective cannot be decreased by moving in any feasible direction.</li> </ul> <p>Visually: the contour of \\(f\\) is \u201ctangent\u201d to the feasible region. The Lagrange multipliers encode the direction and strength of that tangency.</p>"},{"location":"convex/17_kkt/#86-constraint-qualifications","title":"8.6 Constraint qualifications","text":"<p>To guarantee that KKT multipliers exist and KKT conditions apply cleanly, we usually need a mild regularity condition called a constraint qualification. The most common is Slater\u2019s condition for convex problems:</p> <p>If the problem is convex and there exists a strictly feasible point \\(\\tilde{x}\\) such that \\(g_i(\\tilde{x}) &lt; 0\\) for all \\(i\\) and \\(h_j(\\tilde{x}) = 0\\) for all \\(j\\), then strong duality holds and KKT conditions are necessary and sufficient (Boyd and Vandenberghe, 2004).</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the most beautiful and useful ideas in convex optimisation. Every constrained optimisation problem (the primal) has an associated dual problem. The dual problem:</p> <ul> <li>provides a lower bound on the optimal primal value,</li> <li>often has structure that is easier to analyse,</li> <li>gives certificates of optimality,</li> <li>interprets multipliers as \u201cprices\u201d of constraints.</li> </ul> <p>In convex optimisation, under mild assumptions, the primal and dual optimal values are equal.</p>"},{"location":"convex/18_duality/#91-the-primal-problem","title":"9.1 The primal problem","text":"<p>We consider the general problem:  </p> <p>Assume \\(f\\) and the \\(g_i\\) are convex, and \\(h_j\\) are affine. This is a convex optimisation problem.</p> <p>We call \\(f^\\star\\) the optimal value:  </p> <p>Infimum (inf): the greatest lower bound of a set \u2014 the smallest value a function can approach, even if it is not attained.</p>"},{"location":"convex/18_duality/#92-the-lagrangian","title":"9.2 The Lagrangian","text":"<p>We define the Lagrangian:  with multipliers \\(\\mu \\in \\mathbb{R}^m\\) and \\(\\lambda \\in \\mathbb{R}^p\\). For inequality constraints, we will later require \\(\\mu_i \\ge 0\\).</p> <p>Think of \\(\\mu_i\\) and \\(\\lambda_j\\) as \u201cpenalties\u201d for violating the constraints.</p>"},{"location":"convex/18_duality/#93-the-dual-function","title":"9.3 The dual function","text":"<p>For fixed multipliers \\((\\lambda,\\mu)\\), define the dual function:  </p> <p>Important:</p> <ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\), even if \\(f\\) is not convex.</li> <li>For any \\(\\mu \\ge 0\\),  </li> </ul> <p>This last fact is called weak duality:</p> <p>The dual function gives lower bounds on the primal optimum.</p> <p>Proof sketch of weak duality: For any feasible \\(x\\) (i.e. satisfying \\(g_i(x) \\le 0\\), \\(h_j(x) = 0\\)) and any \\(\\mu \\ge 0\\),  because \\(g_i(x) \\le 0\\) and \\(\\mu_i \\ge 0\\). So \\(\\theta(\\lambda,\\mu) = \\inf_x L(x,\\lambda,\\mu) \\le f(x)\\) for all feasible \\(x\\). Taking the infimum over feasible \\(x\\) gives \\(\\theta(\\lambda,\\mu) \\le f^\\star\\).</p>"},{"location":"convex/18_duality/#94-the-dual-problem","title":"9.4 The dual problem","text":"<p>We now maximise the lower bound. The Lagrange dual problem is:  </p> <p>Because \\(\\theta\\) is concave and we are maximising it, the dual problem is always a concave maximisation problem (i.e. a convex optimisation problem in standard form).</p> <p>Let \\(d^\\star\\) denote the optimal dual value. From weak duality, \\(d^\\star \\le f^\\star\\) always.</p>"},{"location":"convex/18_duality/#95-strong-duality-and-slaters-condition","title":"9.5 Strong duality and Slater\u2019s condition","text":"<p>If \\(d^\\star = f^\\star\\), we say strong duality holds.</p> <p>For convex problems, strong duality typically holds under a mild regularity condition known as Slater\u2019s condition (Boyd and Vandenberghe, 2004):</p> <p>If the problem is convex and there exists a strictly feasible point \\(\\tilde{x}\\) such that \\(g_i(\\tilde{x}) &lt; 0\\) for all \\(i\\) and \\(h_j(\\tilde{x}) = 0\\) for all \\(j\\), then strong duality holds.</p> <p>Consequences of strong duality:</p> <ul> <li>The gap \\(f^\\star - d^\\star\\) is zero.</li> <li>There exist optimal multipliers \\((\\lambda^*, \\mu^*)\\).</li> <li>KKT conditions hold and characterise optimality.</li> </ul>"},{"location":"convex/18_duality/#96-kkt-revisited-via-duality","title":"9.6 KKT revisited via duality","text":"<p>The Karush\u2013Kuhn\u2013Tucker (KKT) conditions from Chapter 7 can also be seen as the conditions under which:</p> <ol> <li>\\(x^*\\) minimises the Lagrangian over \\(x\\),</li> <li>\\((\\lambda^*, \\mu^*)\\) maximises \\(\\theta(\\lambda,\\mu)\\),</li> <li>complementary slackness holds,</li> <li>primal feasibility and dual feasibility hold.</li> </ol> <p>Under convexity + Slater, a point is optimal if and only if it satisfies KKT (Boyd and Vandenberghe, 2004). So KKT is both necessary and sufficient.</p> <p>This is the unification:</p> <ul> <li>primal feasibility,</li> <li>dual feasibility,</li> <li>complementary slackness,</li> <li>stationarity (zero subgradient of \\(L\\) w.r.t. \\(x\\)).</li> </ul>"},{"location":"convex/18_duality/#97-interpretation-of-multipliers","title":"9.7 Interpretation of multipliers","text":"<p>The dual variables \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) have interpretations:</p> <ul> <li>\\(\\mu_i^*\\) can be seen as the \u201cshadow price\u201d of relaxing constraint \\(g_i(x) \\le 0\\). If \\(\\mu_i^*\\) is large, then constraint \\(i\\) is \u201cexpensive\u201d to satisfy \u2014 it is strongly active.</li> <li>\\(\\lambda_j^*\\) plays a similar role for equality constraints.</li> </ul> <p>In resource allocation problems, these multipliers act like market prices. In regularised estimation, they act like trade-off coefficients chosen by the optimisation itself.</p>"},{"location":"convex/18_duality/#98-example-linear-programming-dual","title":"9.8 Example: Linear programming dual","text":"<p>Consider a linear program in standard form:  </p> <p>Its dual is  </p> <p>This is a classical primal\u2013dual pair. Linear programming is convex, Slater\u2019s condition typically holds (assuming strict feasibility), and therefore strong duality holds. The LP duality theory you may have seen in undergraduate optimisation is just a special case of Lagrange duality (Boyd and Vandenberghe, 2004; Rockafellar, 1970).</p>"},{"location":"convex/18_duality/#99-duality-as-geometry","title":"9.9 Duality as geometry","text":"<p>Duality is geometry in disguise. The dual problem is finding the \u201cbest supporting hyperplane\u201d that underestimates the primal objective over the feasible set. This is exactly the picture of supporting hyperplanes from Chapter 4, and exactly the subgradient picture from Chapter 6. Appendix B makes this geometric relationship precise in terms of support functions.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-pareto-optimality-and-multi-objective-convex-optimization","title":"Chapter 10: Pareto Optimality and Multi-Objective Convex Optimization","text":"<p>Optimization often focuses on a single objective function \u2014 minimizing one measure of performance. However, real-world problems rarely involve a single criterion. In practice, we must balance multiple conflicting goals: accuracy vs. complexity, fairness vs. utility, return vs. risk, etc.  </p> <p>This chapter introduces Pareto optimality, which generalizes classical convex optimization to the multi-objective setting, and explores how scalarisation connects multi-objective problems to duality and regularisation.</p>"},{"location":"convex/18a_pareto/#101-classical-optimality","title":"10.1 Classical Optimality","text":"<p>In standard convex optimization, we minimize one function:  where \\(\\mathcal{X}\\) is a convex feasible set.</p> <p>Here, optimality is absolute \u2014 there exists a single best point (or equivalence class) minimizing one measure of performance. The theory of Chapters 7\u20139 applies directly: gradients, subgradients, and KKT conditions fully characterize optimality.</p> <p>But what if we have multiple objectives that cannot all be minimized simultaneously?</p>"},{"location":"convex/18a_pareto/#102-multi-objective-convex-optimization","title":"10.2 Multi-Objective Convex Optimization","text":"<p>In many learning and design problems, several objectives compete:</p> Application Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs complexity Fair ML Prediction loss Fairness metric Accuracy vs fairness Portfolio design Return Risk Profit vs stability Information theory Accuracy Compression Fit vs simplicity <p>Formally, we write:  where each \\(f_i(x)\\) is convex. The solution concept changes: there is no single global minimum. Instead, there is a set of trade-off solutions.</p>"},{"location":"convex/18a_pareto/#103-pareto-optimality","title":"10.3 Pareto Optimality","text":""},{"location":"convex/18a_pareto/#a-strong-pareto-optimality","title":"(a) Strong Pareto Optimality","text":"<p>A point \\(x^* \\in \\mathcal{X}\\) is Pareto optimal if no other \\(x \\in \\mathcal{X}\\) satisfies:  with strict inequality for at least one \\(j\\).</p> <p>Intuitively: no feasible solution can improve one objective without worsening another.</p>"},{"location":"convex/18a_pareto/#b-weak-pareto-optimality","title":"(b) Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no \\(x\\) satisfies:  That is, no feasible solution strictly improves all objectives simultaneously.</p>"},{"location":"convex/18a_pareto/#c-geometric-intuition","title":"(c) Geometric Intuition","text":"<p>In two dimensions \\((f_1, f_2)\\), the Pareto frontier forms the lower-left boundary of the feasible region (for minimization):</p> <ul> <li>Points on the frontier are Pareto optimal \u2014 non-dominated.  </li> <li>Points above or inside are dominated (inferior in all respects).</li> </ul> <p>The frontier visualizes the fundamental trade-offs in the problem.</p>"},{"location":"convex/18a_pareto/#104-scalarisation-reducing-many-objectives-to-one","title":"10.4 Scalarisation: Reducing Many Objectives to One","text":"<p>Since multi-objective problems rarely have a unique minimizer, we often scalarise them \u2014 combine all objectives into a single composite scalar that we can minimize using standard methods.</p>"},{"location":"convex/18a_pareto/#a-weighted-sum-scalarisation","title":"(a) Weighted Sum Scalarisation","text":"\\[ \\min_{x \\in \\mathcal{X}} \\; \\sum_{i=1}^k w_i f_i(x), \\quad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights \\(w_i\\) encode relative importance of objectives.</li> <li>Each choice of \\(w\\) yields a different point on the Pareto frontier.</li> <li>Larger \\(w_i\\) emphasizes objective \\(f_i\\).</li> </ul> <p>Convexity caveat: If the objectives and feasible set are convex, the weighted-sum method recovers the convex portion of the Pareto frontier. Nonconvex parts cannot be reached with simple weights.</p>"},{"location":"convex/18a_pareto/#b-varepsilon-constraint-scalarisation","title":"(b) \\(\\varepsilon\\)-Constraint Scalarisation","text":"<p>Alternatively, minimize one objective while turning others into constraints:  </p> <ul> <li>The tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Varying them explores different Pareto-optimal trade-offs.  </li> </ul> <p>Example connection: Ridge regression minimizes fit error subject to a bound on model complexity:  The Lagrangian form  is a weighted-sum scalarisation \u2014 \\(\\lambda\\) acts as a trade-off parameter.</p>"},{"location":"convex/18a_pareto/#c-duality-and-scalarisation","title":"(c) Duality and Scalarisation","text":"<p>Scalarisation is closely related to duality (Chapter 9):</p> <ul> <li>The weights \\(w_i\\) or Lagrange multipliers \\(\\lambda_i\\) act as dual variables.  </li> <li>Changing them selects different Pareto-optimal points on the frontier.  </li> <li>Regularisation parameters in ML (like \\(\\lambda\\)) are dual to constraint levels \u2014 they move along the Pareto frontier.</li> </ul>"},{"location":"convex/18a_pareto/#105-examples","title":"10.5 Examples","text":""},{"location":"convex/18a_pareto/#example-1-regularised-least-squares","title":"Example 1 \u2013 Regularised Least Squares","text":"<p>Objectives:  </p> <p>Two equivalent formulations:</p> <ol> <li>Weighted sum:     </li> <li>\\(\\varepsilon\\)-constraint:     </li> </ol> <p>Both yield Pareto optimal solutions; \\(\\lambda\\) and \\(\\tau\\) parameterize the same trade-off curve.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2 \u2013 Portfolio Optimization (Risk\u2013Return)","text":"<p>Decision variable: portfolio weights \\( w \\in \\mathbb{R}^n \\). Objectives:  </p> <p>Weighted sum formulation:  </p> <ul> <li>Varying \\(\\alpha\\) traces the efficient frontier in risk\u2013return space.</li> <li>This is the foundation of Modern Portfolio Theory (Markowitz).</li> </ul>"},{"location":"convex/18a_pareto/#example-3-probabilistic-modelling-elbo-and-beta-vae","title":"Example 3 \u2013 Probabilistic Modelling (ELBO and \\(\\beta\\)-VAE)","text":"<p>The Evidence Lower Bound (ELBO) in variational inference:  </p> <p>Two competing objectives: - Data fit: maximize expected log-likelihood. - Simplicity: minimize KL divergence.</p> <p>Scalarised form (\\(\\beta\\)-VAE):  </p> <p>Parameter \\(\\beta\\) controls the trade-off \u2014 different \\(\\beta\\) yield different Pareto-optimal points between reconstruction accuracy and disentanglement.</p>"},{"location":"convex/18a_pareto/#106-broader-connections-in-ml-and-ai","title":"10.6 Broader Connections in ML and AI","text":"Domain Competing Objectives Pareto/Scalarised Form Fairness-aware learning Accuracy vs fairness Weighted or constrained objectives Regularisation Fit vs complexity \\(\\|Ax-b\\|^2 + \\lambda R(x)\\) Generalisation Empirical vs expected risk Regularised training Information bottleneck Accuracy vs compression Lagrangian form with \\(\\beta\\) Reinforcement learning Reward vs risk Multi-objective policies <p>Scalarisation explains why hyperparameters like \\(\\lambda\\) or \\(\\beta\\) shape trade-offs in modern learning systems \u2014 they represent weights on a hidden multi-objective frontier.</p>"},{"location":"convex/18a_pareto/#107-geometry-and-visualization","title":"10.7 Geometry and Visualization","text":"<ul> <li>The Pareto frontier is the image of optimal trade-offs in objective space.  </li> <li>Scalarisation corresponds to taking supporting hyperplanes with normal vector \\(w\\).  </li> <li>The shape of the frontier reveals whether the problem is convex:</li> <li>Convex frontier \u2192 smooth trade-offs, all reachable by weighted sums.  </li> <li>Nonconvex frontier \u2192 gaps requiring \\(\\varepsilon\\)-constraints or evolutionary methods.</li> </ul>"},{"location":"convex/18a_pareto/#108-summary-and-outlook","title":"10.8 Summary and Outlook","text":"Concept Interpretation Pareto optimality No objective can improve without worsening another Weighted-sum scalarisation Lagrangian combination of convex objectives \\(\\varepsilon\\)-constraint One objective minimized, others bounded Regularisation Hidden scalarisation in ML Duality connection Weights = Lagrange multipliers Pareto frontier Boundary of optimal trade-offs <p>Key insight: Pareto optimality unifies diverse ML techniques \u2014 from regularisation to variational inference \u2014 under one geometric lens: balancing competing convex objectives.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-regularized-approximation-balancing-fit-and-complexity","title":"Chapter 11: Regularized Approximation \u2013 Balancing Fit and Complexity","text":"<p>Many practical optimization problems involve a trade-off between fitting observed data and controlling model complexity. Regularization formalizes this trade-off as a convex optimization problem that balances these two competing goals.  </p> <p>Building on Chapter 10 (Pareto Optimality), this chapter shows that regularized models correspond to specific points on a Pareto frontier between data fidelity and simplicity. We also connect regularization to duality (Chapter 9), KKT conditions, and probabilistic interpretations.</p>"},{"location":"convex/18b_regularization/#111-motivation-fit-vs-complexity","title":"11.1 Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we want both: 1. Data fidelity: minimize the loss or error \\(f(x)\\), 2. Model simplicity: penalize unnecessary complexity \\(R(x)\\).</p> <p>This yields a bicriterion problem:  </p> <p>Since improving both simultaneously is typically impossible, we form a scalarized problem:  </p> <ul> <li>\\(f(x)\\) \u2014 data-fitting term (e.g. \\(\\|Ax-b\\|_2^2\\))  </li> <li>\\(R(x)\\) \u2014 regularizer (e.g. \\(\\|x\\|_1\\), \\(\\|x\\|_2^2\\), total variation)  </li> <li>\\(\\lambda\\) \u2014 trade-off parameter controlling bias\u2013variance or fit\u2013complexity balance.</li> </ul> <p>Small \\(\\lambda\\) \u2192 better fit, possible overfitting. Large \\(\\lambda\\) \u2192 simpler, possibly underfit model.</p>"},{"location":"convex/18b_regularization/#112-bicriterion-optimization-and-the-pareto-frontier","title":"11.2 Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization is a scalarised multi-objective problem (Chapter 10). A solution \\(x^*\\) is Pareto optimal if no other feasible \\(x\\) can reduce \\(f(x)\\) without increasing \\(R(x)\\).</p> <ul> <li>For convex \\(f\\) and \\(R\\), every Pareto-optimal solution can be obtained for some \\(\\lambda \\ge 0\\).  </li> <li>The mapping between \\(\\lambda\\) and the constraint level \\(R(x) \\le t\\) is monotonic (though not one-to-one).  </li> <li>Nonconvex objectives may yield Pareto frontiers that cannot be fully recovered by weighted sums.</li> </ul> <p>Regularization thus selects one point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#113-why-keep-x-small","title":"11.3 Why Keep \\(x\\) Small?","text":"<p>Ill-posed or noisy inverse problems (\\(Ax \\approx b\\) with ill-conditioned \\(A\\)) often admit many unstable solutions. Large \\(\\|x\\|\\) values tend to overfit noise.</p> <p>Regularization \u2014 especially \\(\\ell_2\\) \u2014 stabilizes the solution by shrinking \\(x\\).</p> <p>Example: Ridge Regression  The optimality condition (normal equations):  - The matrix \\(A^\\top A + \\lambda I\\) is positive definite for \\(\\lambda&gt;0\\). - Even if \\(A\\) is rank-deficient, the solution is unique and stable. - Larger \\(\\lambda\\) improves conditioning but increases bias.</p>"},{"location":"convex/18b_regularization/#114-constrained-and-lagrangian-forms","title":"11.4 Constrained and Lagrangian Forms","text":"<p>Regularized problems can be equivalently written as constrained convex programs:  </p>"},{"location":"convex/18b_regularization/#lagrangian-formulation","title":"Lagrangian Formulation","text":"<p>The Lagrangian is  The associated penalized form  corresponds to solving the constrained problem for some \\(t&gt;0\\).</p>"},{"location":"convex/18b_regularization/#kkt-and-duality-connection","title":"KKT and Duality Connection","text":"<p>Under convexity and Slater\u2019s condition:  </p> <ul> <li>Penalized and constrained forms yield the same optimality structure.  </li> <li>The mapping \\(\\lambda \\leftrightarrow t\\) is monotonic but not bijective.  </li> <li>Regularization parameters thus act as Lagrange multipliers, weighting one objective against another (Chapter 10).</li> </ul>"},{"location":"convex/18b_regularization/#115-common-regularizers","title":"11.5 Common Regularizers","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"<p>  - Smooth, strongly convex \u2192 unique minimizer. - Shrinks coefficients uniformly; improves numerical conditioning. - Bayesian view: corresponds to Gaussian prior \\(x\\sim \\mathcal{N}(0,\\tau^2I)\\).</p>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"<p>  - Convex but not smooth \u2192 promotes sparsity. - The \\(\\ell_1\\) ball\u2019s corners align with coordinate axes, leading to zeros in the solution. - Proximal operator (soft-thresholding):    - Bayesian view: Laplace prior \\(\\sim e^{-|x_i|/\\tau}\\).</p>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"<p>  - Combines sparsity (L1) with stability (L2). - Ensures uniqueness under correlated features.</p>"},{"location":"convex/18b_regularization/#d-beyond-l1l2","title":"(d) Beyond L1/L2","text":"Regularizer Definition Effect General Tikhonov \\(R(x)=\\|Lx\\|_2^2\\) smoothness via linear operator \\(L\\) Total Variation (TV) \\(R(x)=\\|\\nabla x\\|_1\\) piecewise-constant signals Group Lasso \\(R(x)=\\sum_g \\|x_g\\|_2\\) structured sparsity Nuclear Norm \\(R(X)=\\|X\\|_* = \\sum_i \\sigma_i(X)\\) low-rank matrix recovery <p>Each regularizer defines a geometry of simplicity, shaping the solution\u2019s structure.</p>"},{"location":"convex/18b_regularization/#116-choosing-the-regularization-parameter-lambda","title":"11.6 Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-off Behavior","text":"<ul> <li>Small \\(\\lambda\\) \u2192 high fit, high variance.  </li> <li>Large \\(\\lambda\\) \u2192 smoother, more biased solution. \\(\\lambda\\) determines the location on the Pareto frontier.</li> </ul>"},{"location":"convex/18b_regularization/#b-cross-validation-cv","title":"(b) Cross-Validation (CV)","text":"<p>Most common selection strategy: 1. Split data into \\(k\\) folds. 2. Train on \\(k-1\\) folds, validate on the remaining one. 3. Average validation error, choose \\(\\lambda\\) minimizing it.</p> <p>Best practices - Standardize features for L1/Elastic Net. - For time series: use blocked or rolling CV. - Use nested CV for fair model comparison. - One-standard-error rule: choose simplest model within 1 SE of best error.</p>"},{"location":"convex/18b_regularization/#c-analytical-heuristic-alternatives","title":"(c) Analytical / Heuristic Alternatives","text":"<ul> <li>Closed-form rules (ridge shrinkage factor).  </li> <li>Information criteria (AIC/BIC for Lasso).  </li> <li>Regularization paths: trace \\(x^*(\\lambda)\\) as \\(\\lambda\\) varies.  </li> <li>Inverse problems: discrepancy principle or L-curve method.</li> </ul>"},{"location":"convex/18b_regularization/#117-algorithmic-perspective","title":"11.7 Algorithmic Perspective","text":"<p>Regularized convex problems typically take the form  where \\(f\\) is smooth convex and \\(R\\) convex, possibly nonsmooth.</p> <p>Key algorithms:</p> Method Idea Suitable For Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), prox step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update one coordinate at a time Lasso, Elastic Net ADMM Split \\(f\\) and \\(R\\) for parallel structure Large-scale structured problems <p>Proximal operators (Appendix G) handle the nonsmooth term efficiently: - L2 \u2192 scaling (shrinkage) - L1 \u2192 soft-thresholding - TV/Nuclear \u2192 more advanced proximal maps</p>"},{"location":"convex/18b_regularization/#118-bayesian-interpretation","title":"11.8 Bayesian Interpretation","text":"<p>Regularization corresponds to MAP estimation in probabilistic models.</p> <p>Assume data model \\( b = A x + \\varepsilon \\) with Gaussian noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)\\), and prior \\( x \\sim \\mathcal{N}(0,\\tau^2 I) \\). Then:  is the MAP estimator, with \\(\\lambda = \\sigma^2/(2\\tau^2)\\). - Gaussian prior \u2192 L2 penalty - Laplace prior \u2192 L1 penalty (sparse MAP estimate)</p>"},{"location":"convex/18b_regularization/#119-summary-and-outlook","title":"11.9 Summary and Outlook","text":"Concept Key Idea Regularization scalarisation of fit\u2013complexity trade-off Penalized vs constrained linked by duality and KKT Ridge / Lasso / Elastic Net canonical convex regularizers Proximal gradient, ADMM scalable solution methods \\(\\lambda\\) selection via CV or analytical heuristics Bayesian view priors \u2194 regularizers <p>Summary: Regularized approximation formalizes the balance between data fit and model simplicity \u2014 a practical manifestation of Pareto optimality. Its convex structure enables efficient algorithms and strong theoretical guarantees.</p> <p>Next: In Chapter 12, we study first-order algorithms \u2014 gradient, proximal, and accelerated methods \u2014 that efficiently solve large-scale regularized convex problems.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In Chapters 2\u20138 we built the mathematics of convex optimization: linear algebra (Chapter 2), gradients and Hessians (Chapter 3), convex sets (Chapter 4), convex functions (Chapter 5), subgradients (Chapter 6), KKT conditions (Chapter 7), and duality (Chapter 8). </p> <p>Now we answer the practical question:</p> <p>How do we actually solve convex optimization problems in practice?</p> <p>This chapter develops the major algorithmic families used to solve convex problems. Our goal is not only to describe each method, but to explain:</p> <ul> <li>what class of problem it solves,</li> <li>what information it needs (gradient, Hessian, projection, etc.),</li> <li>when you should use it,</li> <li>how it connects to the modelling choices you make.</li> </ul>"},{"location":"convex/19_optimizationalgo/#91-problem-classes-vs-method-classes","title":"9.1 Problem classes vs method classes","text":"<p>Before we dive into algorithms, we need a map. Different algorithms are natural for different convex problem structures.</p>"},{"location":"convex/19_optimizationalgo/#911-smooth-unconstrained-convex-minimisation","title":"9.1.1 Smooth unconstrained convex minimisation","text":"<p>We want  where \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is convex and differentiable.</p> <p>Typical methods:</p> <ul> <li>Gradient descent (first-order),</li> <li>Accelerated gradient,</li> <li>Newton / quasi-Newton (second-order).</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\), sometimes \\(\\nabla^2 f(x)\\).</li> </ul>"},{"location":"convex/19_optimizationalgo/#912-smooth-convex-minimisation-with-simple-constraints","title":"9.1.2 Smooth convex minimisation with simple constraints","text":"<p>We want  where \\(\\mathcal{X}\\) is a \u201csimple\u201d closed convex set such as a box, a norm ball, or a simplex (Chapter 4).</p>"},{"location":"convex/19_optimizationalgo/#practical-examples-of-simple-constraints","title":"Practical Examples of Simple Constraints","text":"Constraint Type Explanation Example Meaning Box Each variable is bounded independently within lower and upper limits. \\( 0 \\le x_i \\le 1 \\) Parameters are restricted to a fixed range (e.g., pixel intensities, control limits). Norm Ball All feasible points lie within a fixed radius from a center under some norm. \\( \\|x - x_0\\|_2 \\le 1 \\) Keeps the solution close to a reference point \u2014 controls total magnitude or deviation. Simplex Nonnegative variables that sum to one. \\( x_i \\ge 0,\\ \\sum_i x_i = 1 \\) Represents valid probability distributions or normalized weights (e.g., portfolio allocations). <p>Typical method:</p> <ul> <li>Projected gradient descent, which alternates a gradient step and Euclidean projection back to \\(\\mathcal{X}\\).</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\),</li> <li>the ability to compute \\(\\Pi_\\mathcal{X}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x-y\\|_2^2\\) efficiently.</li> </ul>"},{"location":"convex/19_optimizationalgo/#913-composite-convex-minimisation-smooth-nonsmooth","title":"9.1.3 Composite convex minimisation (smooth + nonsmooth)","text":"<p>We want  where \\(f\\) is convex and differentiable with Lipschitz gradient, and \\(R\\) is convex but possibly nonsmooth (Chapter 6). Examples:</p> <ul> <li>\\(f(x)=\\|Ax-b\\|_2^2\\), \\(R(x)=\\lambda\\|x\\|_1\\) (LASSO),</li> <li>\\(R(x)\\) is the indicator of a convex set, enforcing a hard constraint.</li> </ul> <p>Typical method:</p> <ul> <li>Proximal gradient / forward\u2013backward splitting,</li> <li>Projected gradient as a special case.</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\),</li> <li>the proximal operator of \\(R\\).</li> </ul>"},{"location":"convex/19_optimizationalgo/#914-general-convex-programs-with-inequality-constraints","title":"9.1.4 General convex programs with inequality constraints","text":"<p>We want  where \\(f\\) and \\(g_i\\) are convex, \\(h_j\\) are affine.  </p> <p>Typical method:</p> <ul> <li>Interior-point (barrier) methods.</li> </ul> <p>Information required:</p> <ul> <li>Gradients and Hessians of the barrier-augmented objective,</li> <li>ability to solve linear systems arising from Newton steps.</li> </ul>"},{"location":"convex/19_optimizationalgo/#915-the-moral","title":"9.1.5 The moral","text":"<p>There is no single \u201cbest\u201d algorithm. There is a best algorithm for the structure you have.</p> <ul> <li>First-order methods scale to huge problems but converge relatively slowly.</li> <li>Newton and interior-point methods converge extremely fast in iterations but each iteration is more expensive (they solve linear systems involving Hessians).</li> <li>Proximal methods are designed for nonsmooth regularisers and constraints that appear everywhere in statistics and machine learning.</li> <li>Interior-point methods are the workhorse for general convex programs (including linear programs, quadratic programs, conic programs) and deliver high-accuracy solutions with strong certificates of optimality </li> </ul>"},{"location":"convex/19_optimizationalgo/#92-first-order-methods-gradient-descent","title":"9.2 First-order methods: Gradient descent","text":""},{"location":"convex/19_optimizationalgo/#921-setting","title":"9.2.1 Setting","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  Smoothness lets us control step sizes.</p>"},{"location":"convex/19_optimizationalgo/#922-algorithm","title":"9.2.2 Algorithm","text":"<p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). A common choice is a constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known, or a backtracking line search when it is not.</p> <p>Derivation: Around \\(x_t\\), we approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <ul> <li>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  </li> <li>If we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable.</li> </ul> <p>This motivates adding a locality restriction \u2014 we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\]"},{"location":"convex/19_optimizationalgo/#923-geometric-meaning","title":"9.2.3 Geometric meaning","text":"<p>From Chapter 3, the first-order Taylor model is  This is minimised (under a step length constraint) by taking \\(d\\) in the direction \\(-\\nabla f(x)\\). So gradient descent is just \u201ctake a cautious step downhill\u201d.</p>"},{"location":"convex/19_optimizationalgo/#924-convergence","title":"9.2.4 Convergence","text":"<p>For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p>"},{"location":"convex/19_optimizationalgo/#925-when-to-use-gradient-descent","title":"9.2.5 When to use gradient descent","text":"<ul> <li>Problems with millions of variables (large-scale ML).</li> <li>You can afford many cheap iterations.</li> <li>You only have access to gradients (or stochastic gradients).</li> <li>You do not need very high precision.</li> </ul> <p>Gradient descent is the baseline first-order method. But we can do better.</p>"},{"location":"convex/19_optimizationalgo/#93-accelerated-first-order-methods","title":"9.3 Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#931-nesterov-acceleration","title":"9.3.1 Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common presentation is:</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra \\(\\beta_k\\) term \u201clooks ahead,\u201d helping the method exploit curvature better than plain gradient descent.</p>"},{"location":"convex/19_optimizationalgo/#932-optimal-first-order-rate","title":"9.3.2 Optimal first-order rate","text":"<p>For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives. In other words, you cannot beat \\(O(1/k^2)\\) in the worst case using only first-order oracle calls.</p>"},{"location":"convex/19_optimizationalgo/#933-when-to-use-acceleration","title":"9.3.3 When to use acceleration","text":"<ul> <li>Same setting as gradient descent (large-scale smooth convex problems),</li> <li>but you want to converge in fewer iterations.</li> <li>You can tolerate a little more instability/parameter tuning (acceleration can overshoot if step sizes are not chosen carefully).</li> </ul> <p>Acceleration is the default upgrade from vanilla gradient descent in many smooth convex machine learning problems.</p> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical)\u2014the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence.</p> <p>In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#94-steepest-descent-method","title":"9.4 Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization. Gradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>"},{"location":"convex/19_optimizationalgo/#941-mathematical-properties","title":"9.4.1. Mathematical Properties","text":""},{"location":"convex/19_optimizationalgo/#a-normalized-direction","title":"(a) Normalized direction","text":"<p>  \u2192 unit vector with the most negative directional derivative.</p>"},{"location":"convex/19_optimizationalgo/#b-unnormalized-direction","title":"(b) Unnormalized direction","text":"<p>  This gives the actual direction and magnitude used in updates.</p>"},{"location":"convex/19_optimizationalgo/#c-key-identity","title":"(c) Key identity","text":"<p>  The directional derivative equals the negative squared dual norm of the gradient.</p>"},{"location":"convex/19_optimizationalgo/#942-the-steepest-descent-method","title":"9.4.2. The Steepest Descent Method","text":"<p>The iterative update rule is:  where \\(t_k &gt; 0\\) is a step size (from line search or a fixed rule).</p> <ul> <li>For the Euclidean norm, this reduces to ordinary gradient descent.</li> <li>For other norms, it adapts the search direction to the geometry of the problem.</li> </ul> <p>Convergence: Similar to gradient descent \u2014 linear for general convex functions, potentially faster when level sets are well-conditioned.</p>"},{"location":"convex/19_optimizationalgo/#943-role-of-the-norm-and-its-influence","title":"9.4.3. Role of the Norm and Its Influence","text":"<p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#a-euclidean-norm-v_2","title":"(a) Euclidean Norm \\(|v|_2\\)","text":"\\[ \\Delta x_{\\text{nsd}} = -\\frac{\\nabla f(x)}{|\\nabla f(x)|*2}, \\quad \\Delta x*{\\text{sd}} = -\\nabla f(x) \\] <p>This is standard gradient descent. The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>"},{"location":"convex/19_optimizationalgo/#b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0","title":"(b) Quadratic Norm \\(|v|_P = (v^T P v)^{1/2}\\), with \\(P \\succ 0\\)","text":"<p>Here, \\(P\\) defines an ellipsoidal metric. The dual norm is \\(|y|_* = (y^T P^{-1} y)^{1/2}\\).</p> \\[ \\Delta x_{\\text{sd}} = -P^{-1}\\nabla f(x) \\] <p>This corresponds to preconditioned gradient descent, where \\(P\\) rescales directions to counter anisotropy in level sets.</p> <p>Interpretation:</p> <ul> <li>If \\(P\\) approximates the Hessian, this becomes Newton\u2019s method.</li> <li>If \\(P\\) is diagonal, it acts like an adaptive step size per coordinate.</li> </ul>"},{"location":"convex/19_optimizationalgo/#c-ell_1-norm","title":"(c) \\(\\ell_1\\)-Norm","text":"\\[ \\Delta x_{\\text{nsd}} = -e_i, \\quad i = \\arg\\max_j \\left|\\frac{\\partial f}{\\partial x_j}\\right| $$ and $$ \\Delta x_{\\text{sd}} = -|\\nabla f(x)|_\\infty e_i \\] <p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p> <p>Geometric intuition: The \\(\\ell_1\\)-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p> <ul> <li>In \\(\\ell_2\\)-norm: the unit ball is a circle \u2192 the steepest direction is exactly opposite the gradient.</li> <li>In \\(\\ell_1\\)-norm: the unit ball is a diamond \u2192 the steepest direction points to a corner (one coordinate).</li> <li>In quadratic norms: the unit ball is an ellipsoid \u2192 the steepest direction follows the metric-adjusted gradient.</li> </ul> <p>Hence, the norm defines the geometry of what \u201csteepest\u201d means.</p>"},{"location":"convex/19_optimizationalgo/#95-newtons-method-and-second-order-methods","title":"9.5 Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p>"},{"location":"convex/19_optimizationalgo/#951-local-quadratic-model","title":"9.5.1 Local quadratic model","text":"<p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step points toward the minimizer of the local quadratic model, and near the true minimizer, Newton\u2019s method exhibits quadratic convergence.</p>"},{"location":"convex/19_optimizationalgo/#952-convergence-behaviour","title":"9.5.2 Convergence behaviour","text":"<ul> <li>Near the minimiser of a strictly convex, twice-differentiable \\(f\\), Newton\u2019s method converges quadratically: roughly, the number of correct digits doubles every iteration.  </li> <li>This is dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order methods \u2014 but only once the iterates enter the basin of attraction.  </li> <li>Far from the minimiser, Newton\u2019s method can behave erratically or even diverge.   To stabilise it, we typically pair it with a line search or trust region strategy to control step size.</li> </ul>"},{"location":"convex/19_optimizationalgo/#953-implementation","title":"9.5.3 Implementation","text":"<p>The main computational effort in each iteration lies in evaluating derivatives and solving the Newton system:</p> \\[ H \\, \\Delta x = -g, \\] <p>where</p> \\[ H = \\nabla^2 f(x), \\quad g = \\nabla f(x). \\]"},{"location":"convex/19_optimizationalgo/#solving-via-cholesky-factorization","title":"Solving via Cholesky factorization","text":"<p>If \\(H\\) is symmetric and positive definite, we can efficiently solve this system using a Cholesky factorization:</p> \\[ H = L L^{\\top}, \\] <p>where \\(L\\) is lower triangular. The Newton step is then:</p> \\[ \\Delta x_{\\text{nt}} = -L^{-\\top} L^{-1} g. \\] <p>This involves two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^{\\top} \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids explicitly computing \\(H^{-1}\\) and ensures numerical stability.</p>"},{"location":"convex/19_optimizationalgo/#newton-decrement","title":"Newton decrement","text":"<p>A useful measure of progress is the Newton decrement:</p> \\[ \\lambda(x) = \\| L^{-1} g \\|_2, \\] <p>which approximates how far we are from the optimum. A common stopping criterion is \\(\\lambda(x)^2 / 2 &lt; \\varepsilon\\).</p>"},{"location":"convex/19_optimizationalgo/#954-computational-cost","title":"9.5.4 Computational cost","text":"<p>Each Newton step requires solving a linear system involving \\(\\nabla^2 f(x_k)\\), which costs about as much as factoring the Hessian (or an approximation).</p> <ul> <li>For an unstructured, dense Hessian, Cholesky factorization requires approximately \\((1/3) n^3\\) floating-point operations.  </li> <li>If \\(H\\) is sparse, banded, or has special structure, the cost can be much lower.  </li> <li>Because of this cubic scaling, Newton\u2019s method is most attractive for medium-scale problems where high accuracy is required.</li> </ul>"},{"location":"convex/19_optimizationalgo/#955-why-convexity-helps","title":"9.5.5 Why convexity helps","text":"<p>If \\(f\\) is convex, then \\(\\nabla^2 f(x_k)\\) is positive semidefinite (Chapter 5). This has two important implications:</p> <ul> <li>The local quadratic model is bowl-shaped, so the Newton direction points toward a minimiser.  </li> <li>Regularised Newton steps (e.g. using \\(H + \\mu I\\) for small \\(\\mu &gt; 0\\)) are guaranteed to be descent directions and behave predictably.</li> </ul>"},{"location":"convex/19_optimizationalgo/#956-quasi-newton-methods","title":"9.5.6 Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods.</p> <p>For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p>"},{"location":"convex/19_optimizationalgo/#957-when-to-use-newton-or-quasi-newton-methods","title":"9.5.7 When to use Newton or quasi-Newton methods","text":"<p>Use Newton or quasi-Newton methods when:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#96-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"9.6 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex optimization problems are not purely smooth. They often include:</p> <ul> <li>Constraints: \\(x \\in \\mathcal{X}\\),</li> <li>Nonsmooth regularisers: such as \\(\\|x\\|_1\\),</li> <li>Penalties: promoting robustness or sparsity (see Chapter 6).</li> </ul> <p>Two core strategies handle such settings:</p> <ol> <li>Projected gradient methods \u2014 where we project each iterate back into the feasible set \\(\\mathcal{X}\\).  </li> <li>Proximal gradient methods \u2014 which generalize projection to handle nonsmooth but structured terms.</li> </ol> <p>These methods extend the ideas of gradient and Newton updates to the broader world of constrained and composite optimization.</p>"},{"location":"convex/19_optimizationalgo/#952-convergence-behaviour_1","title":"9.5.2 Convergence behaviour","text":"<ul> <li>Near the minimiser of a strictly convex, twice-differentiable \\(f\\), Newton\u2019s method converges quadratically: roughly, the number of correct digits doubles every iteration.</li> <li>This is dramatically faster than \\(O(1/k)\\) or \\(O(1/k^2)\\), but only once you\u2019re in the \u201cbasin of attraction.\u201d</li> <li>Far from the minimiser, Newton can misbehave, so we pair it with a line search or trust region.</li> </ul>"},{"location":"convex/19_optimizationalgo/#953-computational-cost","title":"9.5.3 Computational cost","text":"<p>Each Newton step requires solving a linear system involving \\(\\nabla^2 f(x_k)\\), which costs about as much as factoring the Hessian (or an approximation). This is expensive in very high dimensions, which is why Newton is most attractive for medium-scale problems where high accuracy matters.</p>"},{"location":"convex/19_optimizationalgo/#954-why-convexity-helps","title":"9.5.4 Why convexity helps","text":"<p>If \\(f\\) is convex, then \\(\\nabla^2 f(x_k)\\) is positive semidefinite (Chapter 5). This means:</p> <ul> <li>The quadratic model is bowl-shaped, so the Newton step makes sense.</li> <li>Regularised Newton steps (adding a multiple of the identity to the Hessian) behave very predictably.</li> </ul>"},{"location":"convex/19_optimizationalgo/#955-quasi-newton","title":"9.5.5 Quasi-Newton","text":"<p>When Hessians are too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. Famous examples include BFGS and L-BFGS. These methods keep much of Newton\u2019s fast local convergence but with per-iteration cost closer to first-order methods.</p>"},{"location":"convex/19_optimizationalgo/#956-when-to-use-newton-quasi-newton","title":"9.5.6 When to use Newton / quasi-Newton","text":"<ul> <li>You need high-accuracy solutions.</li> <li>The problem is smooth and reasonably well-conditioned.</li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g. via sparse linear algebra).</li> </ul>"},{"location":"convex/19_optimizationalgo/#95-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"9.5 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#961-projected-gradient-descent","title":"9.6.1 Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#962-proximal-gradient-forwardbackward-splitting","title":"9.6.2 Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#97-penalties-barriers-and-interior-point-methods","title":"9.7 Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\)? Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#971-penalty-methods","title":"9.7.1 Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints.</p> <p>Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul> <p>Penalty methods are closely linked to robust formulations and Huber-like losses: you replace a hard requirement by a soft cost. This is exactly what you do in robust regression and in \\(\\epsilon\\)-insensitive / Huber losses (see Section 9.7).</p>"},{"location":"convex/19_optimizationalgo/#972-barrier-methods","title":"9.7.2 Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary.</p> <p>For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul> <p>This is the core idea of interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#973-interior-point-methods-in-practice","title":"9.7.3 Interior-point methods in practice","text":"<p>Interior-point methods:</p> <ul> <li>Are globally convergent for convex problems under mild assumptions (Slater\u2019s condition; see Chapter 8).</li> <li>Solve a series of smooth, strictly feasible subproblems.</li> <li>Use Newton-like steps to update primal (and, implicitly, dual) variables.</li> <li>Produce both primal and dual iterates \u2014 so they naturally produce a duality gap, which certifies how close you are to optimality (Chapter 8).</li> </ul> <p>Interior-point methods are the engine behind modern general-purpose convex solvers for:</p> <ul> <li>linear programs (LP),</li> <li>quadratic programs (QP),</li> <li>second-order cone programs (SOCP),</li> <li>semidefinite programs (SDP).</li> </ul> <p>They give high-accuracy answers and KKT-based optimality certificates. They are more expensive per iteration than gradient methods, but need far fewer iterations, and they handle fully general convex constraints.</p> <p>Summary: Penalty vs Barrier vs Interior-Point</p> Method Feasibility During Iteration Mechanism Typical Behavior Penalty May violate constraints Adds large penalty outside feasible region Easy to implement but can be ill-conditioned Barrier Stays strictly feasible Adds infinite cost near constraint boundary Smooth approximation to constrained problem Interior-Point Always feasible (uses barrier) Solves a sequence of barrier problems with increasing precision Follows central path to true optimum"},{"location":"convex/19_optimizationalgo/#98-choosing-the-right-method-in-practice","title":"9.8 Choosing the right method in practice","text":"<p>Let\u2019s summarise the chapter in the form of a decision guide.</p> <p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>In many optimization problems, we are not free to choose any \\(x \\in \\mathbb{R}^n\\); instead, we must satisfy a set of equality constraints of the form:</p> \\[ A x = b, \\] <p>where \\(A \\in \\mathbb{R}^{p \\times n}\\) and \\(b \\in \\mathbb{R}^p\\). Such problems arise naturally in engineering design, resource allocation, and systems governed by conservation laws.</p>"},{"location":"convex/19a_optimization_constraints/#131-equality-constrained-minimization","title":"13.1 Equality-Constrained Minimization","text":"<p>We consider the general equality-constrained smooth optimization problem:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f(x) \\\\ \\text{subject to} \\quad &amp; A x = b, \\end{aligned} \\] <p>where:</p> <ul> <li>\\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex and twice continuously differentiable,  </li> <li>\\(A\\) has full row rank, i.e., \\(\\text{rank}(A) = p\\),  </li> <li>and the optimum value \\(p^*\\) is finite and attained.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#optimality-conditions","title":"Optimality Conditions","text":"<p>The point \\(x^*\\) is optimal if and only if there exists a Lagrange multiplier vector \\(v^*\\) such that:</p> \\[ \\nabla f(x^*) + A^T v^* = 0, \\qquad A x^* = b. \\] <p>These are the Karush\u2013Kuhn\u2013Tucker (KKT) conditions for equality-constrained optimization.</p> <p>Intuitively:</p> <ul> <li>\\(\\nabla f(x^*)\\) represents the gradient (direction of steepest ascent) of the objective.</li> <li>\\(A^T v^*\\) represents a correction term ensuring that we stay within the feasible set \\(A x = b\\).</li> <li>At the optimum, the projected gradient along feasible directions is zero.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#132-equality-constrained-quadratic-minimization","title":"13.2 Equality-Constrained Quadratic Minimization","text":"<p>A particularly important case is when \\(f\\) is quadratic:</p> \\[ f(x) = \\tfrac{1}{2} x^T P x + q^T x + r, \\] <p>where \\(P \\in \\mathbb{S}_+^n\\) (symmetric positive semidefinite). Then \\(\\nabla f(x) = P x + q\\), and the optimality conditions become the linear system:</p> \\[ \\begin{bmatrix} P &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^* \\\\ v^* \\end{bmatrix} = \\begin{bmatrix} - q \\\\ b \\end{bmatrix}. \\] <p>This is known as the KKT system for the quadratic program.</p>"},{"location":"convex/19a_optimization_constraints/#the-kkt-matrix","title":"The KKT Matrix","text":"<p>The block matrix</p> \\[ K =  \\begin{bmatrix} P &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\] <p>is called the KKT matrix. It is nonsingular (and thus the problem has a unique solution) if and only if:</p> \\[ A x = 0, \\, x \\neq 0 \\quad \\Rightarrow \\quad x^T P x &gt; 0, \\] <p>i.e., \\(P\\) is positive definite on the nullspace of \\(A\\). An equivalent condition is that:</p> \\[ P + A^T A \\succ 0. \\]"},{"location":"convex/19a_optimization_constraints/#133-eliminating-equality-constraints","title":"13.3 Eliminating Equality Constraints","text":"<p>Sometimes it is convenient to eliminate the equality constraints by parameterizing the feasible set.</p> <p>The feasible set \\(\\{x \\mid A x = b\\}\\) can be expressed as:</p> \\[ x = F z + \\hat{x}, \\quad z \\in \\mathbb{R}^{n-p}, \\] <p>where:</p> <ul> <li>\\(\\hat{x}\\) is a particular solution of \\(A x = b\\),</li> <li>\\(F \\in \\mathbb{R}^{n \\times (n-p)}\\) is a matrix whose columns form a basis for the nullspace of \\(A\\) (so \\(A F = 0\\)).</li> </ul> <p>Substituting this into the objective gives an unconstrained problem:</p> \\[ \\min_z \\; f(F z + \\hat{x}). \\] <p>Once we solve for \\(z^*\\), we recover:</p> \\[ x^* = F z^* + \\hat{x}, \\quad v^* = - (A A^T)^{-1} A \\nabla f(x^*). \\]"},{"location":"convex/19a_optimization_constraints/#example-optimal-resource-allocation","title":"Example: Optimal Resource Allocation","text":"<p>Suppose we allocate resources \\(x_i \\in \\mathbb{R}\\) to \\(n\\) agents, with cost functions \\(f_i(x_i)\\) for each agent \\(i\\).</p> <p>We have a total resource constraint:</p> \\[ x_1 + x_2 + \\dots + x_n = b. \\] <p>The problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_1(x_1) + \\cdots + f_n(x_n) \\\\ \\text{subject to} \\quad &amp; x_1 + \\cdots + x_n = b. \\end{aligned} \\] <p>To eliminate the constraint, we express \\(x_n\\) as:</p> \\[ x_n = b - (x_1 + \\cdots + x_{n-1}), \\] <p>and define:</p> \\[ \\hat{x} = b e_n, \\quad F =  \\begin{bmatrix} I_{n-1} \\\\ - \\mathbf{1}^T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (n-1)}. \\] <p>The reduced problem is:</p> \\[ \\min_{x_1, \\ldots, x_{n-1}} f_1(x_1) + \\cdots + f_{n-1}(x_{n-1}) + f_n(b - x_1 - \\cdots - x_{n-1}). \\]"},{"location":"convex/19a_optimization_constraints/#134-newtons-method-for-equality-constrained-problems","title":"13.4 Newton\u2019s Method for Equality-Constrained Problems","text":"<p>We now extend Newton\u2019s method to handle equality constraints.</p> <p>At a feasible point \\(x\\) (i.e., \\(A x = b\\)), the Newton step \\(\\Delta x_{\\text{nt}}\\) is obtained by solving:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} v \\\\ w \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>The Newton step is \\(\\Delta x_{\\text{nt}} = v\\).</p>"},{"location":"convex/19a_optimization_constraints/#interpretation","title":"Interpretation","text":"<p>The step \\(\\Delta x_{\\text{nt}}\\) solves the second-order approximation of the equality-constrained problem:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\hat{f}(x + v) = f(x) + \\nabla f(x)^T v + \\tfrac{1}{2} v^T \\nabla^2 f(x) v \\\\ \\text{subject to} \\quad &amp; A(x + v) = b. \\end{aligned} \\] <p>The equations follow from linearizing the KKT conditions:</p> \\[ \\nabla f(x + v) + A^T w \\approx \\nabla f(x) + \\nabla^2 f(x) v + A^T w = 0, \\quad A(x + v) = b. \\]"},{"location":"convex/19a_optimization_constraints/#135-newton-decrement-and-stopping-criterion","title":"13.5 Newton Decrement and Stopping Criterion","text":"<p>The Newton decrement measures proximity to the optimum:</p> \\[ \\lambda(x) = (\\Delta x_{\\text{nt}}^T \\nabla^2 f(x) \\Delta x_{\\text{nt}})^{1/2} = (-\\nabla f(x)^T \\Delta x_{\\text{nt}})^{1/2}. \\] <p>It provides a local estimate of suboptimality:</p> \\[ f(x) - f^* \\approx \\frac{\\lambda(x)^2}{2}. \\] <p>The directional derivative along the Newton direction is:</p> \\[ \\left.\\frac{d}{dt} f(x + t \\Delta x_{\\text{nt}})\\right|_{t=0} = -\\lambda(x)^2. \\] <p>A common stopping condition is \\(\\lambda(x)^2 / 2 \\leq \\varepsilon\\).</p>"},{"location":"convex/19a_optimization_constraints/#136-newtons-method-with-equality-constraints","title":"13.6 Newton\u2019s Method with Equality Constraints","text":"<p>A practical algorithm:</p> <p>Given: feasible starting point \\(x \\in \\text{dom } f\\) with \\(A x = b\\), and tolerance \\(\\varepsilon &gt; 0\\).</p> <p>Repeat: 1. Compute the Newton step and decrement \\(\\Delta x_{\\text{nt}}, \\lambda(x)\\). 2. Stopping criterion: if \\(\\lambda^2 / 2 \\leq \\varepsilon\\), quit. 3. Line search: choose step size \\(t\\) via backtracking line search. 4. Update: \\(x := x + t \\Delta x_{\\text{nt}}\\).</p> <p>This is a feasible descent method: all iterates satisfy \\(A x = b\\) and \\(f(x^{(k+1)}) &lt; f(x^{(k)})\\). It is also affine invariant \u2014 the algorithm behaves identically under affine transformations of the variables.</p>"},{"location":"convex/19a_optimization_constraints/#137-newtons-method-and-elimination","title":"13.7 Newton\u2019s Method and Elimination","text":"<p>If we have already eliminated the constraints, Newton\u2019s method can be directly applied to the reduced function:</p> \\[ \\tilde{f}(z) = f(F z + \\hat{x}), \\quad A \\hat{x} = b, \\quad A F = 0. \\] <p>Unconstrained Newton\u2019s method for \\(\\tilde{f}(z)\\) generates iterates \\(z^{(k)}\\), and corresponding:</p> \\[ x^{(k)} = F z^{(k)} + \\hat{x}. \\] <p>Thus, the equality-constrained Newton iterates correspond exactly to unconstrained Newton iterates on the reduced problem.</p>"},{"location":"convex/19a_optimization_constraints/#138-newton-step-at-infeasible-points","title":"13.8 Newton Step at Infeasible Points","text":"<p>If \\(x\\) is not feasible (i.e., \\(A x \\neq b\\)), we can generalize to the infeasible Newton step.</p> <p>Define the primal-dual residual:</p> \\[ r(y) =  \\begin{bmatrix} \\nabla f(x) + A^T v \\\\ A x - b \\end{bmatrix}, \\quad y = (x, v). \\] <p>Linearizing \\(r(y) = 0\\) around \\(y\\) gives:</p> \\[ r(y + \\Delta y) \\approx r(y) + D r(y) \\Delta y = 0. \\] <p>Hence, the Newton step \\(\\Delta y = (\\Delta x_{\\text{nt}}, \\Delta v_{\\text{nt}})\\) satisfies:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x_{\\text{nt}} \\\\ \\Delta v_{\\text{nt}} \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^T v \\\\ A x - b \\end{bmatrix}. \\] <p>This is known as the primal-dual Newton step.</p>"},{"location":"convex/19a_optimization_constraints/#139-infeasible-start-newton-method","title":"13.9 Infeasible-Start Newton Method","text":"<p>A more general algorithm that does not require feasibility at the start.</p> <p>Given: initial \\((x, v)\\), tolerance \\(\\varepsilon &gt; 0\\), constants \\(\\alpha \\in (0, 1/2)\\), \\(\\beta \\in (0, 1)\\).</p> <p>Repeat: 1. Compute primal-dual Newton step \\((\\Delta x_{\\text{nt}}, \\Delta v_{\\text{nt}})\\). 2. Backtracking line search on \\(\\|r(y)\\|_2\\):    - set \\(t = 1\\)    - while \\(\\|r(y + t \\Delta y)\\|_2 &gt; (1 - \\alpha t)\\|r(y)\\|_2\\), set \\(t := \\beta t\\). 3. Update: \\(x := x + t \\Delta x_{\\text{nt}}, \\quad v := v + t \\Delta v_{\\text{nt}}\\).</p> <p>Continue until \\(\\|r(y)\\|_2 \\leq \\varepsilon\\).</p> <p>Although this is not a descent method for \\(f(x)\\), it ensures that \\(\\|r(y)\\|_2\\) decreases at each iteration.</p> <p>Directional derivative of \\(\\|r(y)\\|_2\\) along the Newton direction is:</p> \\[ \\left.\\frac{d}{dt} \\|r(y + t \\Delta y)\\|_2\\right|_{t=0} = -\\|r(y)\\|_2. \\]"},{"location":"convex/19a_optimization_constraints/#1310-solving-kkt-systems-efficiently","title":"13.10 Solving KKT Systems Efficiently","text":"<p>Both feasible and infeasible Newton methods require solving KKT systems of the form:</p> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} v \\\\ w \\end{bmatrix} = - \\begin{bmatrix} g \\\\ h \\end{bmatrix}, \\]"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In many real-world optimization problems, not all solutions are allowed \u2014 we often have both equality and inequality constraints that define a feasible region. In this chapter, we study algorithms that handle inequality constraints efficiently, focusing on the logarithmic barrier method, which is foundational for interior-point methods.</p>"},{"location":"convex/19b_optimization_constraints/#141-inequality-constrained-minimization","title":"14.1 Inequality-Constrained Minimization","text":"<p>We consider the general inequality- and equality-constrained optimization problem:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_0(x) \\\\ \\text{subject to} \\quad &amp; f_i(x) \\leq 0, \\quad i = 1, \\dots, m, \\\\ &amp; A x = b. \\end{aligned} \\] <p>We assume: - Each \\(f_i\\) is convex and twice continuously differentiable. - \\(A \\in \\mathbb{R}^{p \\times n}\\) has full rank, \\(\\text{rank}(A) = p\\). - The optimal value \\(p^*\\) is finite and attained. - The problem is strictly feasible, meaning there exists a point \\(\\bar{x}\\) satisfying:</p> \\[ \\bar{x} \\in \\text{dom } f_0, \\quad f_i(\\bar{x}) &lt; 0, \\; i=1,\\dots,m, \\quad A \\bar{x} = b. \\] <p>Under these assumptions, strong duality holds and the dual optimum is attained.</p>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"<p>Many common optimization problems fall under this framework:</p> <ul> <li>LP (Linear Program): \\(f_0(x) = c^T x\\), \\(f_i(x) = a_i^T x - b_i\\) </li> <li>QP (Quadratic Program): \\(f_0(x) = \\tfrac{1}{2} x^T P x + q^T x\\) </li> <li>QCQP (Quadratically Constrained Quadratic Program)  </li> <li>GP (Geometric Program)  </li> <li>Entropy maximization with linear inequality constraints:</li> </ul> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\sum_{i=1}^n x_i \\log x_i \\\\ \\text{subject to} \\quad &amp; F x \\leq g, \\\\ &amp; A x = b, \\end{aligned} \\] <p>where \\(\\text{dom } f_0 = \\mathbb{R}_{++}^n\\).</p>"},{"location":"convex/19b_optimization_constraints/#142-reformulation-via-indicator-functions","title":"14.2 Reformulation via Indicator Functions","text":"<p>We can reformulate the constrained problem as an unconstrained one using an indicator function for the feasible set.</p> <p>Define the indicator of the nonpositive orthant:</p> \\[ I_-(u) = \\begin{cases} 0, &amp; u \\leq 0, \\\\ +\\infty, &amp; u &gt; 0. \\end{cases} \\] <p>Then the constrained problem is equivalent to:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_0(x) + \\sum_{i=1}^m I_-(f_i(x)) \\\\ \\text{subject to} \\quad &amp; A x = b. \\end{aligned} \\] <p>However, this formulation is not smooth \u2014 \\(I_-(u)\\) is discontinuous. To apply smooth optimization methods, we approximate \\(I_-\\) using a logarithmic barrier.</p>"},{"location":"convex/19b_optimization_constraints/#143-logarithmic-barrier-approximation","title":"14.3 Logarithmic Barrier Approximation","text":"<p>We replace \\(I_-(u)\\) by a smooth function:</p> \\[ \\Phi(u) = -\\frac{1}{t} \\log(-u), \\] <p>where \\(t &gt; 0\\) is a barrier parameter controlling the accuracy of the approximation. The resulting logarithmic barrier problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f_0(x) - \\frac{1}{t} \\sum_{i=1}^m \\log(-f_i(x)) \\\\ \\text{subject to} \\quad &amp; A x = b. \\end{aligned} \\] <ul> <li>For small \\(t\\), the barrier term is significant \u2014 it keeps iterates strictly inside the feasible region.  </li> <li>As \\(t \\to \\infty\\), the approximation becomes exact, and the solution approaches the true constrained optimum.</li> </ul> <p>Thus, the original constrained problem is transformed into a sequence of smooth equality-constrained problems.</p>"},{"location":"convex/19b_optimization_constraints/#144-logarithmic-barrier-function","title":"14.4 Logarithmic Barrier Function","text":"<p>Define the logarithmic barrier function:</p> \\[ \\phi(x) = -\\sum_{i=1}^m \\log(-f_i(x)), \\] <p>with domain</p> \\[ \\text{dom } \\phi = \\{ x \\mid f_i(x) &lt; 0, \\; i = 1, \\dots, m \\}. \\]"},{"location":"convex/19b_optimization_constraints/#properties","title":"Properties","text":"<ul> <li>\\(\\phi(x)\\) is convex, since it is a composition of convex and decreasing functions.  </li> <li>It is twice continuously differentiable, with:</li> </ul> \\[ \\nabla \\phi(x) = \\sum_{i=1}^m \\frac{1}{-f_i(x)} \\nabla f_i(x), \\] <p>and</p> \\[ \\nabla^2 \\phi(x) = \\sum_{i=1}^m \\frac{1}{f_i(x)^2} \\nabla f_i(x) \\nabla f_i(x)^T + \\sum_{i=1}^m \\frac{1}{-f_i(x)} \\nabla^2 f_i(x). \\]"},{"location":"convex/19b_optimization_constraints/#145-central-path","title":"14.5 Central Path","text":"<p>For each \\(t &gt; 0\\), define \\(x^*(t)\\) as the minimizer of the barrier problem:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; t f_0(x) + \\phi(x) \\\\ \\text{subject to} \\quad &amp; A x = b. \\end{aligned} \\] <p>The set \\(\\{ x^*(t) \\mid t &gt; 0 \\}\\) is called the central path.</p> <p>As \\(t \\to \\infty\\), \\(x^*(t)\\) approaches the solution \\(x^*\\) of the original constrained problem.</p>"},{"location":"convex/19b_optimization_constraints/#example-central-path-for-a-linear-program","title":"Example: Central Path for a Linear Program","text":"<p>For an LP:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; c^T x \\\\ \\text{subject to} \\quad &amp; a_i^T x \\leq b_i, \\quad i = 1, \\dots, m, \\end{aligned} \\] <p>the central path consists of points \\(x^*(t)\\) where hyperplanes \\(c^T x = c^T x^*(t)\\) are tangent to the level sets of the barrier \\(\\phi(x)\\).</p>"},{"location":"convex/19b_optimization_constraints/#146-interpretation-via-kkt-conditions","title":"14.6 Interpretation via KKT Conditions","text":"<p>At any point on the central path \\((x^*(t), \\lambda^*(t), v^*(t))\\), the following hold:</p> <ol> <li> <p>Primal feasibility: \\(f_i(x^*(t)) \\leq 0, \\; i = 1, \\dots, m\\), and \\(A x^*(t) = b\\).</p> </li> <li> <p>Dual feasibility: \\(\\lambda_i^*(t) \\geq 0\\).</p> </li> <li> <p>Approximate complementary slackness: \\(-\\lambda_i^*(t) f_i(x^*(t)) = \\frac{1}{t}\\), for \\(i = 1, \\dots, m\\).</p> </li> <li> <p>Stationarity (gradient condition):     </p> </li> </ol> <p>The only difference between these and the exact KKT conditions is the relaxation of complementary slackness \u2014 instead of \\(\\lambda_i f_i(x) = 0\\), we have \\(\\lambda_i f_i(x) = -1/t\\).</p> <p>As \\(t \\to \\infty\\), the approximate KKT conditions converge to the true KKT conditions.</p>"},{"location":"convex/19b_optimization_constraints/#147-force-field-interpretation","title":"14.7 Force Field Interpretation","text":"<p>To build geometric intuition, consider the centering problem (no equality constraints):</p> \\[ \\text{minimize} \\quad t f_0(x) - \\sum_{i=1}^m \\log(-f_i(x)). \\] <ul> <li> <p>The term \\(f_0(x)\\) represents a \u201cpotential energy\u201d whose gradient is the external force:    </p> </li> <li> <p>Each constraint contributes a repulsive force preventing violation:    </p> </li> </ul> <p>At equilibrium (the central point \\(x^*(t)\\)), forces balance perfectly:</p> \\[ F_0(x^*(t)) + \\sum_{i=1}^m F_i(x^*(t)) = 0. \\] <p>The solution thus represents a balance between the pull of minimizing \\(f_0\\) and the repulsion from the boundaries \\(f_i(x) = 0\\).</p>"},{"location":"convex/19b_optimization_constraints/#148-barrier-method-algorithm","title":"14.8 Barrier Method Algorithm","text":"<p>The barrier method solves a sequence of equality-constrained barrier subproblems, gradually increasing \\(t\\) to approach the true optimum.</p> <p>Given: strictly feasible \\(x\\), initial barrier parameter \\(t := t^{(0)} &gt; 0\\), multiplier \\(\\mu &gt; 1\\), tolerance \\(\\varepsilon &gt; 0\\).</p> <p>Repeat: 1. Centering Step:    Compute \\(x^*(t)\\) by minimizing     subject to \\(A x = b\\).    (This is typically done with Newton\u2019s method.)</p> <ol> <li> <p>Update: \\(x := x^*(t)\\).</p> </li> <li> <p>Stopping Criterion:    Quit if \\(\\frac{m}{t} &lt; \\varepsilon\\).</p> </li> <li> <p>Increase Barrier Parameter: \\(t := \\mu t\\).</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#notes-on-practical-implementation","title":"Notes on Practical Implementation","text":"<ul> <li>Each centering step (inner loop) is an equality-constrained Newton method.  </li> <li>The outer loop gradually increases \\(t\\), making the barrier steeper.  </li> <li>A larger \\(\\mu\\) means fewer outer iterations but more Newton steps per inner problem.   Typical values: \\(\\mu = 10\\) or \\(\\mu = 20\\).</li> </ul> <p>The method terminates when:</p> \\[ f_0(x) - p^* \\leq \\varepsilon, \\] <p>which follows directly from the bound \\(f_0(x^*(t)) - p^* \\leq \\frac{m}{t}\\).</p>"},{"location":"convex/19b_optimization_constraints/#convergence-and-intuition","title":"Convergence and Intuition","text":"<ul> <li>Early iterations focus on feasibility and centrality \u2014 staying deep within the feasible region.  </li> <li>Later iterations approach the true optimum, as the barrier term becomes negligible.  </li> <li>The iterates \\(x^*(t)\\) always remain strictly feasible, hence the method name: interior-point.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#summary-of-key-ideas","title":"Summary of Key Ideas","text":"Concept Description Indicator function Encodes inequality constraints via \\(I_-(f_i(x))\\). Logarithmic barrier Smooth approximation of \\(I_-\\), prevents boundary crossing. Central path Sequence of minimizers for increasing \\(t\\), approaching the optimum. Approximate KKT \\(\\lambda_i f_i(x) = -1/t\\) replaces exact complementarity. Barrier method Iterative scheme using inner Newton steps and outer \\(t\\) updates. <p>In summary: The barrier method transforms inequality constraints into smooth penalties that \u201crepel\u201d the optimizer from infeasible regions. As \\(t\\) grows, the optimizer moves along the central path, converging to the true solution while maintaining strict feasibility. This approach forms the foundation for modern interior-point methods used in convex optimization, linear programming, and semidefinite programming.</p>"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-14-advanced-large-scale-and-structured-methods","title":"Chapter 14: Advanced Large-Scale and Structured Methods","text":"<p>In Chapter 9 we focused on \u201cclassical convex solvers\u201d: gradient methods, accelerated methods, Newton and quasi-Newton methods, projected/proximal methods, and interior-point methods. Those are the canonical tools of convex optimisation.</p> <p>This chapter moves one step further.</p> <p>Here we study methods that:</p> <ul> <li>exploit problem structure (sparsity, separability, block structure),</li> <li>scale to extremely high dimensions,</li> <li>or are widely used in practice for machine learning and signal processing \u2014 including in problems that are not convex.</li> </ul> <p>Some of these methods were first analysed in the convex setting (often with strong guarantees), and then adopted \u2014 sometimes recklessly \u2014 in the nonconvex world (training neural nets, matrix factorisation, etc.). You\u2019ll absolutely see them in modern optimisation and ML code.</p> <p>We\u2019ll cover:</p> <ol> <li>Coordinate (block) descent,</li> <li>Stochastic gradient and mini-batch methods,</li> <li>ADMM (Alternating Direction Method of Multipliers),</li> <li>Proximal coordinate / coordinate proximal variants,</li> <li>Majorization\u2013minimization and iterative reweighted schemes.</li> </ol> <p>Throughout we\u2019ll emphasise: - When they are provably correct for convex problems, - Why people also use them in nonconvex problems.</p>"},{"location":"convex/20_advanced/#101-coordinate-descent-and-block-coordinate-descent","title":"10.1 Coordinate descent and block coordinate descent","text":""},{"location":"convex/20_advanced/#1011-idea","title":"10.1.1 Idea","text":"<p>Instead of updating all coordinates of \\(x\\) at once using a full gradient or Newton direction, we update one coordinate (or one block of coordinates) at a time, holding the others fixed.</p> <p>Suppose we want to minimise a convex function  and write \\(x = (x_1, x_2, \\dots, x_p)\\) in coordinates or blocks. Coordinate descent cycles through \\(i = 1,2,\\dots,p\\) and solves (or approximately solves)  </p> <p>In other words: update coordinate \\(i\\) by optimising over just that coordinate (or block), treating the rest as constants.</p>"},{"location":"convex/20_advanced/#1012-why-this-can-be-fast","title":"10.1.2 Why this can be fast","text":"<ul> <li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li> <li>For problems with separable structure \u2014 e.g. sums over features, or regularisers like \\(\\|x\\|_1 = \\sum_i |x_i|\\) \u2014 the coordinate update is extremely cheap.</li> <li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li> </ul> <p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>"},{"location":"convex/20_advanced/#1013-convergence-in-convex-problems","title":"10.1.3 Convergence in convex problems","text":"<p>For many convex, continuously differentiable problems with certain regularity (e.g. strictly convex objective, or convex plus separable nonsmooth terms), cyclic coordinate descent is guaranteed to converge to the global minimiser. There are also randomized versions that pick a coordinate uniformly at random, which often give cleaner expected-rate guarantees.</p> <p>For \\(\\ell_1\\)-regularised least squares, i.e.  each coordinate update becomes a scalar soft-thresholding step \u2014 so coordinate descent becomes an extremely efficient sparse regression solver.</p>"},{"location":"convex/20_advanced/#1014-block-coordinate-descent","title":"10.1.4 Block coordinate descent","text":"<p>When coordinates are naturally grouped (for example, \\(x\\) is really \\((x^{(1)}, x^{(2)}, \\dots)\\) where each \\(x^{(j)}\\) is a vector of parameters for a submodule or layer), we generalise to block coordinate descent. Each step solves  </p> <p>Block coordinate descent is the backbone of many alternating minimisation schemes in signal processing, matrix factorisation, dictionary learning, etc.</p>"},{"location":"convex/20_advanced/#1015-use-in-nonconvex-problems","title":"10.1.5 Use in nonconvex problems","text":"<p>Even when \\(F\\) is not convex, people still run block coordinate descent (under names like \u201calternating minimisation\u201d or \u201calternating least squares\u201d), because:</p> <ul> <li>each block subproblem might be convex even if the joint problem isn\u2019t,</li> <li>it is easy to implement,</li> <li>it often works \u201cwell enough\u201d in practice.</li> </ul> <p>You see this in low-rank matrix factorisation (recommender systems), where you fix all user factors and update item factors, then swap. There are no global guarantees in general (no convexity), but empirically it converges to useful solutions.</p> <p>So:  </p> <ul> <li>In convex settings \u2192 provable global convergence.  </li> <li>In nonconvex settings \u2192 heuristic that often finds acceptable stationary points.</li> </ul>"},{"location":"convex/20_advanced/#102-stochastic-gradient-and-mini-batch-methods","title":"10.2 Stochastic gradient and mini-batch methods","text":""},{"location":"convex/20_advanced/#1021-full-gradient-vs-stochastic-gradient","title":"10.2.1 Full gradient vs stochastic gradient","text":"<p>In Chapter 9, gradient descent uses the full gradient \\(\\nabla f(x)\\) at each step. In large-scale learning problems, \\(f\\) is almost always an average over data:  where \\(\\ell_i\\) is the loss on sample \\(i\\).</p> <p>Computing \\(\\nabla f(x)\\) exactly costs \\(O(N)\\) per step, which is huge.</p> <p>Stochastic Gradient Descent (SGD) replaces \\(\\nabla f(x)\\) with an unbiased estimate. At each iteration we:</p> <ol> <li>Sample \\(i\\) uniformly from \\(\\{1,\\dots,N\\}\\),</li> <li>Use \\(g_k = \\nabla \\ell_i(x_k)\\),</li> <li>Update     </li> </ol> <p>This is extremely cheap: one data point (or a small mini-batch) per step.</p>"},{"location":"convex/20_advanced/#1022-convergence-in-convex-problems","title":"10.2.2 Convergence in convex problems","text":"<p>For convex problems, with diminishing step sizes \\(\\alpha_k\\), SGD converges to the global optimum in expectation, and more refined analyses show \\(O(1/\\sqrt{k})\\) suboptimality rates for general convex Lipschitz losses, improving to \\(O(1/k)\\) in strongly convex smooth cases with appropriate averaging.</p> <p>That is slower (per iteration) than deterministic gradient descent in theory, but each iteration is much cheaper. So SGD wins in wall-clock time for huge \\(N\\).</p>"},{"location":"convex/20_advanced/#1023-momentum-adam-rmsprop-nonconvex-practice-convex-roots","title":"10.2.3 Momentum, Adam, RMSProp (nonconvex practice, convex roots)","text":"<p>In modern machine learning, methods like momentum SGD, Adam, RMSProp, Adagrad, etc., are used routinely to train enormous nonconvex models (deep networks). These are variations of first-order methods with:</p> <ul> <li>adaptive step sizes,</li> <li>running averages of squared gradients,</li> <li>momentum terms.</li> </ul> <p>While the most common use is for nonconvex problems, many of these methods (e.g. Adagrad-type adaptive steps, momentum acceleration) have their theoretical roots in convex optimisation and mirror-descent style analyses.</p> <p>So stochastic first-order methods are:</p> <ul> <li>rigorous for convex problems,</li> <li>widely used heuristically for nonconvex problems.</li> </ul>"},{"location":"convex/20_advanced/#103-admm-alternating-direction-method-of-multipliers","title":"10.3 ADMM: Alternating Direction Method of Multipliers","text":"<p>ADMM is one of the most important algorithms in modern convex optimisation for structured problems. It is used constantly in signal processing, sparse learning, distributed optimisation, and large-scale statistical estimation.</p>"},{"location":"convex/20_advanced/#1031-problem-form","title":"10.3.1 Problem form","text":"<p>ADMM solves problems of the form  where \\(f\\) and \\(g\\) are convex.</p> <p>This form appears everywhere:</p> <ul> <li>\\(f\\) is a data-fit term,</li> <li>\\(g\\) is a regulariser or constraint indicator,</li> <li>\\(Ax + Bz = c\\) ties them together.</li> </ul> <p>For example, LASSO can be written by introducing a copy variable and enforcing \\(x=z\\).</p>"},{"location":"convex/20_advanced/#1032-augmented-lagrangian","title":"10.3.2 Augmented Lagrangian","text":"<p>ADMM applies the augmented Lagrangian method, which is like dual ascent but with a quadratic penalty on constraint violation. The augmented Lagrangian is  with dual variable (Lagrange multiplier) \\(y\\) and penalty parameter \\(\\rho&gt;0\\).</p>"},{"location":"convex/20_advanced/#1033-the-admm-updates-two-block-case","title":"10.3.3 The ADMM updates (two-block case)","text":"<p>Iterate the following: 1. \\(x\\)-update:        (holding \\(z,y\\) fixed). 2. \\(z\\)-update:     3. Dual update:     </p> <p>That is: optimise \\(x\\) given \\(z\\), optimise \\(z\\) given \\(x\\), then update the multiplier.</p>"},{"location":"convex/20_advanced/#1034-why-admm-is-powerful","title":"10.3.4 Why ADMM is powerful","text":"<ul> <li>Each subproblem often becomes simple and separable:</li> <li>The \\(x\\)-update might be a least-squares or a smooth convex minimisation,</li> <li>The \\(z\\)-update might be a proximal operator (soft-thresholding, projection, etc.).</li> <li>You never have to solve the full coupled problem in one shot.</li> <li>ADMM is embarrassingly parallel / distributable: different blocks can be solved on different machines then averaged via the multiplier step.</li> </ul>"},{"location":"convex/20_advanced/#1035-convergence","title":"10.3.5 Convergence","text":"<p>For convex \\(f\\) and \\(g\\), under mild assumptions (closed proper convex functions, some regularity), ADMM converges to a solution of the primal problem, and the dual variable \\(y^k\\) converges to an optimal dual multiplier (Boyd and Vandenberghe, 2004, Ch. 5; also classical ADMM literature).</p> <p>This is deeply tied to duality (Chapter 8): ADMM is best understood as a method of solving the dual with decomposability, but returning primal iterates along the way.</p>"},{"location":"convex/20_advanced/#1036-use-in-nonconvex-problems","title":"10.3.6 Use in nonconvex problems","text":"<p>In practice, ADMM is often extended to nonconvex problems by simply \u201cpretending it\u2019s fine.\u201d Each subproblem is solved anyway, and the dual variable is updated the same way. The method is no longer guaranteed to find a global minimiser \u2014 but it often finds a stationary point that is good enough (e.g. in nonconvex regularised matrix completion, dictionary learning, etc.).</p> <p>You will see ADMM used in imaging, sparse coding, variational inference, etc., even when parts of the model are not convex.</p>"},{"location":"convex/20_advanced/#104-proximal-coordinate-and-coordinate-prox-methods","title":"10.4 Proximal coordinate and coordinate-prox methods","text":"<p>There\u2019s a natural fusion of the ideas in Sections 10.1 (coordinate descent) and 9.5 (proximal methods): proximal coordinate descent.</p>"},{"location":"convex/20_advanced/#1041-problem-form","title":"10.4.1 Problem form","text":"<p>Consider composite convex objectives  with \\(f\\) smooth convex and \\(R\\) convex, possibly nonsmooth and separable across coordinates or blocks:  </p>"},{"location":"convex/20_advanced/#1042-algorithm-sketch","title":"10.4.2 Algorithm sketch","text":"<p>At each iteration, pick coordinate (or block) \\(j\\), and update only \\(x_j\\) by solving the 1D (or low-dim) proximal subproblem:  </p> <p>Often we linearise \\(f\\) around the current point in that block and add a quadratic term, just like a proximal gradient step but on one coordinate at a time.</p>"},{"location":"convex/20_advanced/#1043-why-its-useful","title":"10.4.3 Why it\u2019s useful","text":"<ul> <li>When \\(R\\) is separable (e.g. \\(\\ell_1\\) sparsity penalties), each coordinate subproblem becomes a scalar shrinkage / thresholding step.</li> <li>Memory footprint is tiny.</li> <li>You get sparsity \u201cfor free\u201d as many coordinates get driven to zero and stay there.</li> <li>Randomised versions (pick a coordinate at random) are simple and have good expected convergence guarantees in convex problems.</li> </ul>"},{"location":"convex/20_advanced/#1044-use-in-nonconvex-settings","title":"10.4.4 Use in nonconvex settings","text":"<p>People run proximal coordinate descent in nonconvex sparse learning (e.g. \\(\\ell_0\\)-like surrogates, nonconvex penalties for variable selection). The convex convergence guarantees are gone, but empirically the method still often converges to a structured, interpretable solution.</p>"},{"location":"convex/20_advanced/#105-majorizationminimization-mm-and-reweighted-schemes","title":"10.5 Majorization\u2013minimization (MM) and reweighted schemes","text":"<p>Majorization\u2013minimization (MM) is a general pattern:</p> <ol> <li>Build a simple convex surrogate that upper-bounds (majorises) your objective at the current iterate,</li> <li>Minimise the surrogate,</li> <li>Repeat.</li> </ol> <p>It is sometimes called \u201citerative reweighted\u201d or \u201csuccessive convex approximation.\u201d</p>"},{"location":"convex/20_advanced/#1051-mm-template","title":"10.5.1 MM template","text":"<p>Suppose we want to minimise \\(F(x)\\) (convex or not). We construct \\(G(x \\mid x^{(k)})\\) such that:</p> <ul> <li>\\(G(x^{(k)} \\mid x^{(k)}) = F(x^{(k)})\\) (touches at current iterate),</li> <li>\\(G(x \\mid x^{(k)}) \\ge F(x)\\) for all \\(x\\) (majorises \\(F\\)),</li> <li>\\(G(\\cdot \\mid x^{(k)})\\) is easy to minimise (often convex, often separable).</li> </ul> <p>Then we set  </p> <p>This guarantees \\(F(x^{(k+1)}) \\le F(x^{(k)})\\). So the objective is monotonically nonincreasing.</p>"},{"location":"convex/20_advanced/#1052-iterative-reweighted-ell_1-ell_2","title":"10.5.2 Iterative reweighted \\(\\ell_1\\) / \\(\\ell_2\\)","text":"<p>A classical example: to promote sparsity or robustness, you might want to minimise something like  or a concave penalty on residuals. You replace that concave / nonconvex penalty with a weighted convex penalty that depends on the previous iterate. Then you update the weights and solve again.</p> <p>In the convex world, MM is just another way to design descent methods. In the nonconvex world, MM is a way to attack nonconvex penalties using a sequence of convex subproblems.</p> <p>This is extremely common in robust regression, compressed sensing with nonconvex sparsity surrogates, and low-rank matrix recovery.</p>"},{"location":"convex/20_advanced/#1053-relation-to-proximal-methods","title":"10.5.3 Relation to proximal methods","text":"<p>MM can often be interpreted as doing a proximal step on a locally quadratic or linearised upper bound. In that sense, it is philosophically close to proximal gradient (Chapter 9) and to Newton-like local quadratic approximation (Chapter 9), but with the additional twist that we are allowed to handle nonconvex \\(F\\) as long as we majorise it with something convex.</p>"},{"location":"convex/20_advanced/#106-summary-and-perspective","title":"10.6 Summary and perspective","text":"<p>We\u2019ve now seen several algorithmic families that are particularly important at large scale and/or under structural constraints:</p> <ol> <li> <p>Coordinate descent / block coordinate descent </p> <ul> <li>Updates one coordinate block at a time.  </li> <li>Converges globally for many convex problems.  </li> <li>Scales extremely well in high dimensions.  </li> <li>Used heuristically in nonconvex alternating minimisation.</li> </ul> </li> <li> <p>Stochastic and mini-batch gradient methods  </p> <ul> <li>Use noisy gradient estimates to get cheap iterations.  </li> <li>Converge (in expectation) for convex problems.  </li> <li>Power all of modern large-scale ML, including nonconvex deep learning.</li> </ul> </li> <li> <p>ADMM (Alternating Direction Method of Multipliers)  </p> <ul> <li>Splits a problem into simpler subproblems linked by linear constraints.  </li> <li>Closely tied to duality and KKT (Chapters 7\u20138).  </li> <li>Converges for convex problems.  </li> <li>Used everywhere, including nonconvex settings, due to its modularity and parallelisability.</li> </ul> </li> <li> <p>Proximal coordinate / coordinate-prox methods  </p> <ul> <li>Merge sparsity-inducing penalties (Chapter 6) with blockwise updates.  </li> <li>Ideal for \\(\\ell_1\\)-type structure, group lasso, etc.  </li> <li>Often extended to nonconvex penalties for even \u201cmore sparse\u201d solutions.</li> </ul> </li> <li> <p>Majorization\u2013minimization (MM)  </p> <ul> <li>Iteratively builds and minimises convex surrogates.  </li> <li>Guarantees monotone descent of the true objective.  </li> <li>Provides a clean bridge from convex optimisation theory into heuristic nonconvex optimisation.</li> </ul> </li> </ol>"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-15-modelling-patterns-and-algorithm-selection","title":"Chapter 15: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#111-regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"11.1 Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithmic pointers for 11.1:  </p> <ul> <li>Smooth+\u2113\u2082 (strongly convex) \u2192 Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li> <li>Mixed penalties (\u2113\u2081+\u2113\u2082) \u2192 Still convex; often handle like \u2113\u2081 case since smooth part dominates curvature.  </li> <li>Large-scale data \u2192 Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li> </ul> <p>Remarks:  Choose \\(\\lambda\\) via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (\\(n\\) large), coordinate or stochastic methods often outperform direct second-order methods.</p>"},{"location":"convex/21_models/#112-robust-regression-and-outlier-resistance","title":"11.2 Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#1121-least-absolute-deviations-l1-loss","title":"11.2.1 Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#1122-huber-loss","title":"11.2.2 Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints.     That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.     This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#1123-worst-case-robust-regression","title":"11.2.3 Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#113-maximum-likelihood-and-loss-design","title":"11.3 Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#114-structured-constraints-in-engineering-and-design","title":"11.4 Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers for 11.4:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#115-linear-and-conic-programming-the-canonical-models","title":"11.5 Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#116-risk-safety-margins-and-robust-design","title":"11.6 Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#117-cheat-sheet-if-your-problem-looks-like-this-use-that","title":"11.7 Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-15-canonical-problems-in-convex-optimization","title":"Chapter 15: Canonical Problems in Convex Optimization","text":"<p>The table below gives canonical convex problem classes, with special cases listed inside their parent class to avoid duplication. Key hierarchy: LP \u2282 QP \u2282 SOCP \u2282 SDP (all are conic programs).</p> Problem Type Canonical Form Special Cases  Examples Typical Applications Common Algorithms Linear Program \\(\\displaystyle \\min_x\\; c^T x  \\\\ \\text{s.t.}\\; A x = b,\\; x \\ge 0\\) - Resource allocation, scheduling, routing, relaxations Simplex  Dual simplex, Interior-point (barrier), Decomposition &amp; first-order for very large-scale Quadratic Program (convex if \\(Q \\succeq 0\\)) \\(\\displaystyle \\min_x\\; \\tfrac12 x^T Q x + c^T x \\\\ \\text{s.t.}\\; A x \\le b,\\; F x = g\\) Least Squares \\(\\min\\|Ax-b\\|_2^2\\)   Ridge:\\(\\min\\|Ax-b\\|_2^2+\\lambda\\|x\\|_2^2\\)  LASSO (via variable splitting \u2192 QP  Box-QP  Trust-region (QP with ball) Portfolio optimization, MPC, regression &amp; sparse estimation, large-scale ML fitting Interior-point, Active-set, ProjectedProximal gradient, Conjugate gradient (unconstrained), Coordinate descentADMM (sparsestructured) Second-Order Cone Program \\(\\displaystyle \\min_x\\; f^T x \\\\ \\text{s.t.}\\; \\|A_i x + b_i\\|_2 \\le c_i^T x + d_i,\\; F x = g\\) Robust least squares  Norm constraints (\\(\\ell_2\\))  Quadratic constraints representable as SOC  Chebyshev center  Portfolio with varianceVaR surrogates Robust regression, antennabeamforming, control, risk-constrained finance Conic interior-point (barrier), Primal-dual first-order splitting for large-scale Semidefinite Program \\(\\displaystyle \\min_X\\; \\mathrm{Tr}(C^T X) \\\\ \\text{s.t.}\\; \\mathrm{Tr}(A_i^T X)=b_i,\\; X \\succeq 0\\) QCQP relaxations; Max-cutGoemans\u2013Williamson; Covariance selection; Lyapunovcontrol synthesis; Sum-of-squares (SOS) relaxations Control &amp; systems, combinatorial relaxations, covariancecorrelation modeling, quantum information Interior-point (medium-scale), Low-rank first-order (proxFrank\u2013Wolfe), ADMMsplitting Quadratically Constrained QP (convex if all \\(P_i \\succeq 0\\); otherwise generally NP-hard) \\(\\displaystyle \\min_x\\; \\tfrac12 x^T P_0 x + q_0^T x + d_0 \\\\ \\text{s.t.}\\ \\tfrac12 x^T P_i x + q_i^T x + d_i \\le 0\\) Trust-region subproblems; Ellipsoidal constraints; Robust fitting; Many admit SOCPSDP reformulations Robust control, beamforming, filter design, robust estimation Interior-point (convex case), SDPSOCP relaxations, Convex\u2013concave proceduresheuristics GP (Geometric Program) (convex in log-space) \\(\\displaystyle \\min_x\\; f_0(x)\\ \\text{s.t.}\\ f_i(x)\\le 1,\\ g_j(x)=1\\) (posynomial \\(f_i\\), monomial \\(g_j\\)) After \\(y=\\log x\\): convex; posynomial circuit sizing; powerenergy scaling laws; resource trade-offs CircuitIC design, communication &amp; power control, engineering tuning Log-transform \u2192 Interior-point; Primal-dual methods; First-order for large sparse instances MLE  GLM (Likelihood-based convex models) \\(\\displaystyle \\min_x\\; -\\sum_i \\log p(b_i\\mid a_i^T x)\\ +\\ \\mathcal{R}(x)\\) where \\(\\mathcal{R}\\) is convex (e.g., \\(\\ell_1,\\ell_2\\)) Classification: Logisticsoftmax (GLMs); SVM as convex risk minimization (hinge loss); PoissonExponential family GLMs; Elastic-net regularized GLMs Predictive modeling, classification, count modeling, calibration Newton  (L-)BFGS, Acceleratedproximal gradient, Coordinate descent, Stochastic variants (SGDSAGASVRG), ADMM"},{"location":"convex/30_canonical_problems/#linear-programming-lp","title":"Linear Programming (LP)","text":"<p>A standard LP has a linear objective and linear constraints:</p> \\[ minimize_{x}\\; c^T x \\quad\\text{s.t.}\\; A x = b,\\; x \\ge 0. \\] <p>with decision variable \\(x\\in\\mathbb R^n\\). Geometrically, the feasible set is a polyhedron (intersection of halfspaces), and optimal solutions lie at extreme points (vertices) of this polyhedron. Intuitively, imagine a flat objective plane being \u201cpushed\u201d until it first touches the polyhedron at a vertex.</p>"},{"location":"convex/30_canonical_problems/#algorithms","title":"Algorithms:","text":"<p>At a high level, LP algorithms fall into three families:</p> <ol> <li>Vertex-based (Simplex)</li> <li>Interior-based (Barrier  IPM)</li> <li>Decomposition and First-Order (Large-Scale  Structured)</li> </ol> <p>Each family explores the polyhedral geometry of the feasible set in a different way.</p>"},{"location":"convex/30_canonical_problems/#1-simplex-method-walking-the-edges-of-the-polyhedron","title":"1. Simplex Method \u2014 Walking the edges of the polyhedron","text":"<p>The Simplex method moves from vertex to vertex (corner to corner) of the feasible polyhedron, improving the objective value at each step. Intuitively, the LP feasible region is a polyhedron (a high-dimensional \u201cflat-sided\u201d shape) and the linear objective defines a tilted plane sweeping across it. The optimum is always attained at a vertex, so Simplex efficiently hops between adjacent vertices along edges until it reaches the best one.</p>"},{"location":"convex/30_canonical_problems/#2-interior-point-methods-gliding-through-the-interior","title":"2. Interior-Point Methods \u2014 Gliding through the interior","text":"<p>Interior-point methods (IPMs) take a fundamentally different route: Instead of crawling along edges, they glide through the interior of the feasible region, guided by a barrier function that prevents them from hitting the boundaries.</p> <p>They solve a sequence of barrier problems of the form:  where the logarithmic term keeps \\(x_i &gt; 0\\).  </p> <p>As the barrier parameter \\(\\mu \\to 0\\), the iterates approach the true boundary optimum along a central path.  Each step requires solving a Newton system that couples all variables, making IPMs especially efficient for dense, moderately sized LPs.</p>"},{"location":"convex/30_canonical_problems/#3-decomposition-methods-divide-and-conquer-for-large-scale-structure","title":"3. Decomposition Methods \u2014 Divide and conquer for large-scale structure","text":"<p>When an LP is too large to solve as a single system, or its constraint matrix \\(A\\) has block structure, decomposition methods exploit separability by breaking the problem into smaller subproblems that can be solved independently.</p> <p>The basic idea:  </p> <ul> <li>Identify coupling constraints or variables that link otherwise independent subsystems.  </li> <li>Solve each subsystem separately, and coordinate them via dual variables or cutting planes until consistency and optimality are achieved.</li> </ul> <p>Decomposition turns \u201cone huge LP\u201d into \u201cmany small LPs talking to each other.\u201d</p>"},{"location":"convex/30_canonical_problems/#applications","title":"Applications","text":"<p>LPs model many resource-allocation and network-flow problems (e.g.\\ transportation, scheduling, blending). They also capture piecewise-linear loss minimization. For example, the minimax (Chebyshev) regression \\(\\min_x\\max_i|a_i^T x - b_i|\\) can be written as an LP. LPs appear in operations research, control (robust linear design), and in approximations of more complex problems.</p>"},{"location":"convex/30_canonical_problems/#quadratic-programming-qp","title":"Quadratic Programming (QP)","text":"<p>A convex QP has a quadratic objective and linear constraints:</p> \\[ minimize_{x}\\;\\tfrac12 x^T Q x + c^T x \\quad\\text{s.t.}\\; A x = b,\\;x\\ge0, \\] <p>where \\(Q\\succeq0\\) (positive semidefinite) makes the problem convex. Here \\(x\\in\\mathbb R^n\\). Equivalently, one may have inequality constraints \\(A x\\le b\\). Level sets of the objective are ellipsoids (quadratic bowls), so the optimum may lie on a boundary or in the interior of the feasible set.</p> <p>Intuition &amp; geometry: The positive-definite part of \\(Q\\) defines an \u201cellipsoidal\u201d bowl of the objective. At optimum, the gradient \\(\\nabla(\\tfrac12 x^T Q x + c^T x) = Qx+c\\) is orthogonal to the active constraint surfaces (KKT stationarity). Unlike LP, an unconstrained QP\u2019s minimizer is at \\(x=-Q^{-1}c\\) if \\(Q\\succ0\\). With constraints, the optimum occurs where the ellipsoid just touches the feasible set (which is a polyhedron or affine subspace).</p> <p>Applications: QPs model many smooth convex problems. A classic example is Markowitz portfolio optimization (minimize variance \\(x^T\\Sigma x\\) plus a linear return term). QPs arise in ridge regression (least-squares with \\(\\ell_2\\) penalty), support-vector machines (SVMs with quadratic soft-margin), and model-predictive control (convex quadratic cost). Any least-squares problem with linear constraints is a QP. Many robust or regularized designs (elastic net regression, norm-constrained classification) also lead to QPs.</p> <p>Example Risk-Adjusted Quadratic Program (Mean\u2013Variance Portfolio Optimization)</p> <p>We want to allocate portfolio weights ( x \\in \\mathbb{R}^n ) across ( n ) assets.</p> <ul> <li>\\(\\mu \\in \\mathbb{R}^n\\): expected returns of each asset</li> <li>\\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\): covariance matrix of returns (symmetric, positive semidefinite)</li> <li>\\(x\\): portfolio weights (fractions of capital invested in each asset)</li> </ul> <p>The mean\u2013variance trade-off says we want to:</p> <ul> <li>maximize expected return \\(\\mu^T x\\),</li> <li>minimize risk (variance) \\(x^T \\Sigma x\\).</li> </ul> <p>Combining them gives the standard convex QP form:</p> \\[\\begin{aligned} \\min_x \\quad &amp; \\tfrac{1}{2} x^T \\Sigma x - \\lambda \\mu^T x \\ \\text{s.t.} \\quad &amp; \\mathbf{1}^T x = 1, \\ &amp; x \\ge 0, \\end{aligned}\\] <p>where \\(\\lambda &gt; 0\\) is the risk\u2013return trade-off parameter.</p> <ul> <li>The quadratic term \\(\\tfrac{1}{2} x^T \\Sigma x\\) penalizes portfolio variance (risk).</li> <li>The linear term \\(-\\lambda \\mu^T x\\) rewards expected return.</li> <li>The equality constraint \\(\\mathbf{1}^T x = 1\\) ensures all capital is invested (weights sum to 1).</li> <li>The inequality \\(x \\ge 0\\) enforces no short selling.</li> </ul>"},{"location":"convex/30_canonical_problems/#algorithms_1","title":"Algorithms","text":""},{"location":"convex/30_canonical_problems/#1-interior-point-methods-smooth-paths-through-the-interior","title":"1. Interior-Point Methods \u2014 Smooth paths through the interior","text":"<p>Interior-point methods extend the same idea used for LPs. They replace inequality constraints with barrier terms, e.g.  where the logarithmic barrier keeps \\(s_i &gt; 0\\).</p> <p>Each iteration solves a Newton system derived from the Karush\u2013Kuhn\u2013Tucker (KKT) conditions, coupling primal and dual variables. The method follows a central path toward the optimum as \\(\\mu \\to 0\\).</p>"},{"location":"convex/30_canonical_problems/#2-active-set-methods-walking-along-faces","title":"2. Active-Set Methods \u2014 Walking along faces","text":"<p>Active-set methods generalize the Simplex idea to QPs.</p> <ol> <li>Guess which constraints are active (tight) at the solution.  </li> <li>Solve the resulting equality-constrained QP:     </li> <li>Check which inactive constraints become violated; update the working set and repeat. Each iteration moves along the face of the feasible region, turning when constraints become active or inactive.</li> </ol>"},{"location":"convex/30_canonical_problems/#3-newtons-method-one-shot-solution-when-unconstrained","title":"3. Newton\u2019s Method \u2014 One-shot solution when unconstrained","text":"<p>If there are no constraints, the QP reduces to:  </p> <p>Setting the gradient to zero gives:  </p> <p>Since the Hessian \\(Q\\) is constant, Newton\u2019s method reaches the optimum in one iteration.</p>"},{"location":"convex/30_canonical_problems/#4-conjugate-gradient-cg-iterative-linear-solver-for-large-problems","title":"4. Conjugate Gradient (CG) \u2014 Iterative linear solver for large problems","text":"<p>When \\(Q\\) is large and sparse, inverting or factorizing it is too expensive. Instead, Conjugate Gradient (CG) solves \\(Qx = -c\\) iteratively using only matrix\u2013vector products. If \\(Q = A^T A\\), the QP corresponds to a least-squares problem:  and CG can efficiently find the minimizer without forming \\(A^T A\\).</p>"},{"location":"convex/30_canonical_problems/#5-accelerated-gradient-methods-first-order-scalability","title":"5. Accelerated Gradient Methods \u2014 First-order scalability","text":"<p>For extremely large QPs with simple constraints (\\(x \\ge 0\\) or box bounds), first-order methods are used:  where \\(\\Pi_{\\mathcal{C}}\\) projects onto the feasible region.</p>"},{"location":"convex/30_canonical_problems/#quadratically-constrained-qp","title":"Quadratically Constrained QP","text":"\\[ minimize_{x}\\;\\tfrac12x^T P_0 x + q_0^T x + r_0 \\quad\\text{s.t.}\\quad \\tfrac12x^T P_i x + q_i^T x + r_i \\le 0. \\] <p>where each \\(P_i\\in\\mathbb R^{n\\times n}\\) and \\(x\\in\\mathbb R^n\\). If all \\(P_i\\succeq0\\), the feasible set is convex, and the QCQP is convex. (If any \\(P_i\\) is indefinite, the problem is generally nonconvex and NP-hard.) A special case is QP (no quadratic constraints). A common simpler form is a single quadratic (ellipsoidal) constraint, as in the trust-region problem.</p> <p>Intuition &amp; geometry: Convex QCQPs describe intersections of \u201cellipsoidal\u201d regions (and possibly affine sets). For example, the constraint \\(\\tfrac12x^TPx + q^Tx + r\\le0\\) defines a (possibly unbounded) ellipsoid or paraboloid region. Geometrically, a convex QCQP feasible set can be an ellipsoid, paraboloid, or their intersections. The optimum lies where the objective\u2019s ellipsoidal level set first touches this intersection. When \\(P_0\\succeq0\\), the objective contours are convex (ellipsoids); if \\(P_i\\succ0\\), constraint boundaries are convex surfaces.</p> <p>Applications: QCQPs appear in robust design and engineering. For instance, in robust beamforming or filter design one often imposes constraints on quadratic forms of \\(x\\). Sensor network localization and trust-region subproblems are QCQPs. Robust linear regression against ellipsoidal noise uncertainty is a QCQP. Any problem with norms or quadratic inequalities (e.g.\\ \\(|Fx-c|_2^2\\le d^2\\)) can be written as a QCQP.</p>"},{"location":"convex/30_canonical_problems/#algorithms_2","title":"Algorithms","text":"<p>Convex Quadratically Constrained Quadratic Programs (QCQPs) where all \\(P_i \\succeq 0\\) can be solved efficiently using interior-point methods, which provide polynomial-time convergence. Because QCQPs can be expressed as second-order cone programs (SOCPs) or more generally as semidefinite programs (SDPs), modern conic solvers (e.g., MOSEK, SCS, SDPT3) handle these problems robustly for moderate dimensions.</p>"},{"location":"convex/30_canonical_problems/#second-order-cone-programming-socp","title":"Second-Order Cone Programming (SOCP)","text":"\\[ minimize_{x}\\; f^T x \\quad\\text{s.t.}\\quad \\|A_i x + b_i\\|_2 \\le c_i^T x + d_i,\\; F x = g. \\] <p>where each \\(|A_i x + b_i|_2\\le c_i^T x + d_i\\) is a second-order cone constraint. Equivalently, each constraint says the affine function \\((x,t)\\mapsto (A_i x + b_i, c_i^T x + d_i)\\) lies in the norm cone {}. Thus the feasible set is the intersection of affine spaces and cones (a convex cone itself).</p> <p>Intuition &amp; geometry: Each second-order constraint carves out a convex \u201ccone\u201d in \\((x,t)\\)-space. Geometrically, a 2-norm constraint \\(|u|\\le t\\) forms a (rotated) quadratic cone. The feasible set is the intersection of these cones with any affine constraints. For example, \\(|x|_2 \\le 1\\) is a unit ball (an ellipsoid), which is a simple special case of an SOCP. SOCPs generalize linear constraints (\\(\\ell_1\\) norm cones) and some QCQPs.</p> <p>Applications: SOCPs are widely used in engineering and finance. Examples include robust least-squares (using Huber or \\(\\ell_2\\) penalties), design of filters and antennas, truss or structural design under stress norms, and certain portfolio models with risk measures. In statistics and ML, enforcing \\(|w|_2\\le R\\) is an SOCP constraint. Many chance-constrained or variance-based optimization problems lead to SOCPs. Notably, portfolio optimization with a value-at-risk (VaR) constraint can be cast as an SOCP en.wikipedia.org</p> <p>Algorithms: Interior-point methods are the standard solvers for SOCPs. Off-the-shelf conic solvers (e.g.\\ MOSEK, ECOS) efficiently handle moderate-size SOCPs. Compared to SDP, SOCPs are usually faster to solve for similar problem sizes. First-order or decomposition methods (e.g.\\ ADMM) can scale to larger SOCPs if needed.</p>"},{"location":"convex/30_canonical_problems/#geometric-programming-gp","title":"Geometric Programming (GP)","text":"<p>Original (nonconvex) form:</p> \\[ minimize_{x&gt;0}\\; f_0(x) \\quad\\text{s.t.}\\; f_i(x)\\le1,\\quad g_j(x)=1, \\] <p>where each \\(f_i\\) is a posynomial (a sum of monomials) and each \\(g_j\\) is a monomial. In coordinates, a monomial has the form \\(c,x_1^{a_1}x_2^{a_2}\\cdots x_n^{a_n}\\) with \\(c&gt;0\\), and a posynomial is a sum of such terms.</p> <p>Intuition &amp; convexity: Though GPs are not convex in the original variables, they can be made convex by the change of variables \\(y_i=\\log x_i\\) and taking logs of functions. Under this log\u2013log transformation, each monomial becomes an affine function of \\(y\\), and each posynomial becomes a log-sum-exp (convex) function. Thus every GP can be converted to a convex problem (a log-sum-exp minimization). Geometrically, GPs operate on positive variables with multiplicative relationships; in the log-domain, they become standard convex programs.</p> <p>Applications: GPs have many applications in engineering design. For example, circuit and analog IC component sizing (transistors, amplifiers) is often cast as a GP. Other examples include aircraft design, power system design, and network flow with multiplicative constraints. In statistics, certain inference problems (e.g. fitting log-linear models) can be written as GPs. Any problem where costs and constraints are products of powers of variables (posynomials) is a candidate for geometric programming en.wikipedia.org</p> <p>Algorithms: The usual approach is to transform the GP into a convex form and apply convex solvers. After \\(y=\\log x\\), one solves a convex program using interior-point methods on the log-transformed problem. Off-the-shelf convex solvers (e.g.\\ CVXOPT, MOSEK) now directly support GPs by performing this transformation internally. Specialized GP toolkits (e.g.\\ GPkit) also exist. For sequential design, one can iteratively solve approximate GPs. Because GPs become convex, KKT conditions and duality apply after transformation.</p>"},{"location":"convex/30_canonical_problems/#maximum-likelihood-and-generalized-linear-models-glm","title":"Maximum Likelihood and Generalized Linear Models (GLM)","text":""},{"location":"convex/30_canonical_problems/#1-maximum-likelihood-estimation-mle","title":"1. Maximum Likelihood Estimation (MLE)","text":"<ul> <li>Parametric distribution estimation: Choose from a family of densities \\(p_x(y)\\), indexed by a parameter \\(x\\) (often denoted \\(\\theta\\)).</li> <li>We take \\(p_x(y) = 0\\) for invalid values of \\(x\\).</li> <li>The likelihood function is \\(p_x(y)\\) as a function of \\(x\\).</li> <li>The log-likelihood function is \\(l(x) = \\log p_x(y)\\).</li> </ul> <p>Maximum Likelihood Estimation (MLE):  </p> <p>This is often a convex optimization problem when \\(\\log p_x(y)\\) is concave in \\(x\\) for fixed \\(y\\).</p>"},{"location":"convex/30_canonical_problems/#2-linear-measurements-with-iid-noise","title":"2. Linear Measurements with IID Noise","text":"<p>Consider the linear model:  </p> <p>where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\): unknown parameter vector,</li> <li>\\(\\nu_i\\): IID measurement noise with density \\(p(z)\\),</li> <li>\\(y_i\\): observed measurements.</li> </ul> <p>Then the joint density is:  </p> <p>and the log-likelihood becomes:  </p> <p>The MLE is any \\(x\\) that maximizes \\(l(x)\\).</p>"},{"location":"convex/30_canonical_problems/#3-common-noise-models-and-corresponding-mles","title":"3. Common Noise Models and Corresponding MLEs","text":""},{"location":"convex/30_canonical_problems/#a-gaussian-noise-mathcaln0-sigma2","title":"(a) Gaussian Noise \\(\\mathcal{N}(0, \\sigma^2)\\)","text":"\\[p(z) = (2\\pi\\sigma^2)^{-1/2} e^{-z^2 / (2\\sigma^2)}\\] \\[l(x) = -\\frac{m}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^m (a_i^T x - y_i)^2\\] <p>MLE: Least-squares solution  </p>"},{"location":"convex/30_canonical_problems/#b-laplacian-noise","title":"(b) Laplacian Noise","text":"\\[p(z) = \\frac{1}{2a} e^{-|z|/a}\\] \\[l(x) = -m\\log(2a) - \\frac{1}{a} \\sum_{i=1}^m |a_i^T x - y_i|\\] <p>MLE: \\(\\ell_1\\)-norm solution (least absolute deviations)  </p> <p>This is convex but non-smooth; it can be solved via LP, proximal gradient, or robust convex solvers.</p>"},{"location":"convex/30_canonical_problems/#c-uniform-noise-on-a-a","title":"(c) Uniform Noise on \\([-a, a]\\)","text":"\\[ p(z) = \\begin{cases} \\frac{1}{2a}, &amp; |z| \\le a \\ 0, &amp; \\text{otherwise} \\end{cases} \\] \\[ l(x) = \\begin{cases} -m \\log(2a), &amp; |a_i^T x - y_i| \\le a, \\ \\forall i \\ -\\infty, &amp; \\text{otherwise} \\end{cases} \\] <p>MLE: Any \\(x\\) satisfying  </p> <p>This defines a feasible region (a convex polyhedron).</p>"},{"location":"convex/30_canonical_problems/#4-logistic-regression-bernoulli-likelihood","title":"4. Logistic Regression (Bernoulli Likelihood)","text":"<p>For binary labels \\(y \\in {0,1}\\):  where \\(a, b\\) are parameters and \\(u \\in \\mathbb{R}^n\\) are observed features.</p> <p>For \\(m\\) samples \\((u_i, y_i)\\), the log-likelihood is:  </p> <p>and the negative log-likelihood (to minimize) is:  </p> <p>This function is convex and smooth in \\(a, b\\).</p>"},{"location":"convex/30_canonical_problems/#5-softmax-multiclass-logistic-regression","title":"5. Softmax (Multiclass Logistic Regression)","text":"<p>For \\(K\\) classes with one-hot encoded labels \\(y_i\\) and parameters \\(x_k\\) for each class:  </p> <p>Negative log-likelihood:  </p> <p>Convex in all \\(x_k\\).</p>"},{"location":"convex/30_canonical_problems/#algorithms-for-mle-and-glms","title":"Algorithms for MLE and GLMs","text":"<ul> <li>LS/Gaussian: Closed-form or CG/Newton as above.</li> <li>\u2113\u2081 regression (Laplace): Convert to LP or use interior-point, or first-order methods (subgradient/proximal) for large-scale. </li> <li>Logistic/softmax: Use Newton or quasi-Newton (L-BFGS) when moderate-sized, since the loss is smooth. Accelerated gradient or stochastic gradient (SGD/Adam) is common for large datasets. With \\(\\ell_2\\)-regularization the problem is strongly convex, ensuring fast convergence. With \\(\\ell_1\\)-regularization one uses proximal-gradient or coordinate descent to handle the non-smooth part. </li> <li>GLMs: Iteratively Reweighted Least Squares (IRLS, a Newton method) often solves canonical-link GLMs efficiently.</li> </ul>"},{"location":"convex/30_canonical_problems/#support-vector-machines","title":"Support Vector Machines","text":"\\[ \\min_w\\;\\tfrac12\\|w\\|_2^2 + C\\sum_i \\max(0,1 - y_i w^T a_i). \\] <p>The SVM primal problem is a QP of the form $, which is convex but non-smooth. This QP is typically solved via its dual or by specialized coordinate-descent.</p>"},{"location":"nonconvex/41_intro/","title":"1. Introduction","text":""},{"location":"nonconvex/41_intro/#chapter-1-non-convex-optimization-fundamentals","title":"Chapter 1: Non-Convex Optimization Fundamentals","text":""},{"location":"nonconvex/41_intro/#11-why-non-convexity-matters","title":"1.1 Why Non-Convexity Matters","text":"<p>Convex optimization ensures a unique global minimum and strong theoretical guarantees. However, many practical problems in machine learning, deep learning, control, and physics are non-convex:</p> <ul> <li>Neural network loss surfaces</li> <li>Reinforcement learning value functions</li> <li>Matrix factorization</li> <li>Clustering and combinatorial tasks</li> </ul> <p>These landscapes cannot be handled efficiently with traditional convex methods.</p>"},{"location":"nonconvex/41_intro/#12-characteristics-of-non-convex-landscapes","title":"1.2 Characteristics of Non-Convex Landscapes","text":"Property Description Consequence Multiple local minima Many suboptimal valleys Gradient descent may get trapped Saddle points Flat or neutral zones Slow or no convergence Discontinuities Non-differentiable regions Gradients undefined Non-linearity Coupled variables Non-trivial curvature and topology <p>Visualization of 2D loss surfaces often reveals chaotic or fractal-like geometry.</p>"},{"location":"nonconvex/41_intro/#13-gradient-based-methods-and-their-limits","title":"1.3 Gradient-Based Methods and Their Limits","text":"<p>Even though SGD, Adam, and similar methods dominate deep learning, they:</p> <ul> <li>Depend heavily on initialization</li> <li>May converge to poor local minima or plateaus</li> <li>Are sensitive to learning rate and batch size</li> <li>Cannot handle discrete or combinatorial variables</li> </ul> <p>This motivates global search strategies that can explore the space more broadly.</p>"},{"location":"nonconvex/41_intro/#14-toward-global-optimization","title":"1.4 Toward Global Optimization","text":"<p>Global optimization aims to find near-optimal solutions without convexity assumptions. Two main families exist:</p> <ol> <li>Deterministic global methods \u2014 exhaustive, branch-and-bound, interval analysis</li> <li>Stochastic and metaheuristic methods \u2014 probabilistic, adaptive, and nature-inspired</li> </ol> <p>The remainder of this book focuses on the latter, due to their flexibility and robustness in black-box settings.</p>"},{"location":"nonconvex/42_meta/","title":"2. Metaheuristic Optimization Methods","text":""},{"location":"nonconvex/42_meta/#chapter-2-metaheuristic-optimization-methods","title":"Chapter 2: Metaheuristic Optimization Methods","text":"<p>Metaheuristics are general-purpose stochastic search algorithms inspired by natural or social processes. They do not require gradient information and are particularly powerful for non-convex, discrete, and black-box problems.</p>"},{"location":"nonconvex/42_meta/#21-core-principles","title":"2.1 Core Principles","text":"<p>Metaheuristics operate by balancing two key dynamics:</p> <ul> <li>Exploration: Searching new, unvisited regions of the solution space</li> <li>Exploitation: Refining promising areas to improve solution quality</li> </ul> <p>Algorithms differ in how they maintain this balance \u2014 through temperature schedules, populations, or probabilistic moves.</p>"},{"location":"nonconvex/42_meta/#22-major-families-of-metaheuristics","title":"2.2 Major Families of Metaheuristics","text":"Category Examples Inspiration Trajectory-based Simulated Annealing (SA) Thermodynamics Evolutionary algorithms Genetic Algorithms (GA), Differential Evolution (DE) Natural selection Swarm intelligence Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO) Collective animal behavior Physics/chemistry inspired Harmony Search, Firefly Algorithm, Gravitational Search Physical processes"},{"location":"nonconvex/42_meta/#23-simulated-annealing-sa","title":"2.3 Simulated Annealing (SA)","text":"<ul> <li>Mimics the cooling of metals.</li> <li>Accepts worse moves with a probability <code>exp(-\u0394E/T)</code>, allowing escape from local minima.</li> <li>Temperature <code>T</code> decreases over time.</li> </ul> <p>Key idea: Controlled randomness to avoid premature convergence.</p>"},{"location":"nonconvex/42_meta/#24-genetic-algorithms-ga","title":"2.4 Genetic Algorithms (GA)","text":"<ul> <li>Maintain a population of solutions.</li> <li>Apply selection, crossover, and mutation to evolve toward better candidates.</li> <li>Works well for discrete, combinatorial, or mixed-variable problems.</li> </ul> <p>Mathematical insight: Balances exploitation (selection) and exploration (mutation).</p>"},{"location":"nonconvex/42_meta/#25-swarm-based-methods","title":"2.5 Swarm-Based Methods","text":"<p>Inspired by collective behaviors in nature:</p> <ul> <li>PSO: Particles move based on personal and social bests</li> <li>ACO: Agents deposit pheromones to guide search</li> <li>Firefly Algorithm: Movement toward brighter (better) peers</li> </ul> <p>These methods excel in continuous search spaces and dynamic environments.</p>"},{"location":"nonconvex/42_meta/#26-theoretical-considerations","title":"2.6 Theoretical Considerations","text":"<p>Although metaheuristics lack strong convex guarantees, their stochastic convergence can be studied via:</p> <ul> <li>Markov chain analysis</li> <li>Expected improvement over iterations</li> <li>Diversity measures within populations</li> </ul> <p>They often converge probabilistically to a near-optimal region rather than a single guaranteed optimum.</p>"},{"location":"nonconvex/43_hybrid/","title":"3. Hybrid and Modern Optimization Methods","text":""},{"location":"nonconvex/43_hybrid/#chapter-3-hybrid-and-modern-optimization-methods","title":"Chapter 3: Hybrid and Modern Optimization Methods","text":"<p>Modern optimization integrates heuristic exploration with mathematical precision. Hybrid and adaptive approaches leverage the strengths of both global and local methods.</p>"},{"location":"nonconvex/43_hybrid/#31-hybrid-metaheuristics","title":"3.1 Hybrid Metaheuristics","text":"<p>Combine metaheuristics with classical optimization to accelerate convergence:</p> <ul> <li>Memetic algorithms: GA + local search refinement</li> <li>Hybrid PSO: PSO with gradient descent fine-tuning</li> <li>Adaptive Simulated Annealing: Dynamic temperature and step size</li> </ul> <p>The goal: exploit global search for exploration, and analytical methods for exploitation.</p>"},{"location":"nonconvex/43_hybrid/#32-multi-objective-optimization","title":"3.2 Multi-Objective Optimization","text":"<p>Many real-world problems have conflicting objectives, e.g., accuracy vs interpretability.</p> <ul> <li>Represent trade-offs via the Pareto front</li> <li>Search for non-dominated solutions using evolutionary multi-objective algorithms (e.g., NSGA-II, MOEA/D)</li> <li>Use crowding distance and rank-based selection for diversity</li> </ul> <p>These methods underpin design trade-offs in engineering and AutoML pipelines.</p>"},{"location":"nonconvex/43_hybrid/#33-constraint-handling","title":"3.3 Constraint Handling","text":"<p>Constraints are incorporated via:</p> <ul> <li>Penalty functions: Add cost for violations</li> <li>Repair mechanisms: Project invalid solutions back into feasible space</li> <li>Decoders: Convert unconstrained representations into feasible solutions</li> </ul> <p>These are essential for optimization in robotics, control, and combinatorial planning.</p>"},{"location":"nonconvex/43_hybrid/#34-modern-directions","title":"3.4 Modern Directions","text":""},{"location":"nonconvex/43_hybrid/#1-reinforcement-learning-and-evolution","title":"1. Reinforcement Learning and Evolution","text":"<ul> <li>Neuroevolution (e.g., NEAT)</li> <li>Policy optimization via evolutionary strategies</li> </ul>"},{"location":"nonconvex/43_hybrid/#2-bayesian-and-surrogate-optimization","title":"2. Bayesian and Surrogate Optimization","text":"<ul> <li>Gaussian processes + exploration policies</li> <li>Efficient black-box optimization (used in hyperparameter tuning)</li> </ul>"},{"location":"nonconvex/43_hybrid/#3-quantum-inspired-and-neuro-symbolic-search","title":"3. Quantum-Inspired and Neuro-symbolic Search","text":"<ul> <li>Quantum annealing analogies</li> <li>Neural controllers guiding metaheuristics</li> <li>AutoML as meta-level optimization</li> </ul>"},{"location":"nonconvex/43_hybrid/#35-summary","title":"3.5 Summary","text":"<p>Hybrid and modern metaheuristics represent a convergence of mathematics, biology, and computation. They embrace stochasticity not as noise, but as a powerful tool for discovering high-quality solutions in complex, non-convex landscapes.</p>"}]}