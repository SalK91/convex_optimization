{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository is a curated collection of concepts, algorithms, and case studies in convex optimization \u2014 a unifying framework that sits at the intersection of applied mathematics, computer science, and engineering.</p> <p>We focus on: - Theoretical foundations \u2014 understanding what makes a problem convex and why convexity matters. - Practical algorithms \u2014 from classical methods like simplex and gradient descent to modern interior-point and first-order methods. - Real-world case studies \u2014 demonstrating convex optimization in machine learning, control, finance, and beyond.</p>"},{"location":"#why-convex-optimization","title":"Why Convex Optimization?","text":"<p>Convex optimisation is one of the central pillars of modern applied mathematics, machine learning, and artificial intelligence. It provides a rich framework in which we can model problems, design algorithms, and guarantee performance. Unlike general non-convex optimisation, convex optimisation problems enjoy the key property that any local solution is also global. This makes them especially important in practice, where reliability, interpretability, and theoretical guarantees are valued.</p>"},{"location":"0%20d%20Algos/","title":"0 d Algos","text":""},{"location":"0%20d%20Algos/#algorithms-for-convex-optimisation","title":"Algorithms for Convex Optimisation","text":"<p>Convex optimization algorithms exploit the geometry of convex sets and functions. Because every local minimum is global, algorithms can converge reliably without worrying about bad local minima.</p> <p>We divide algorithms into three main families:</p> <ol> <li>First-order methods \u2013 use gradients (scalable, but slower convergence).  </li> <li>Second-order methods \u2013 use Hessians (faster convergence, more expensive).  </li> <li>Interior-point methods \u2013 general-purpose, highly accurate solvers.</li> </ol>"},{"location":"0%20d%20Algos/#gradient-descent-first-order-method","title":"Gradient Descent (First-Order Method)","text":""},{"location":"0%20d%20Algos/#algorithm","title":"Algorithm","text":"<p>For step size :  </p>"},{"location":"0%20d%20Algos/#convergence","title":"Convergence","text":"<ul> <li>If  is convex and  is Lipschitz continuous with constant :</li> <li>With fixed step , we have:      </li> <li>If  is -strongly convex:          (linear convergence).</li> </ul>"},{"location":"0%20d%20Algos/#pros-cons","title":"Pros &amp; Cons","text":"<ul> <li>\u2705 Simple, scalable to very high dimensions.  </li> <li>\u274c Slow convergence compared to higher-order methods.</li> </ul>"},{"location":"0%20d%20Algos/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<ul> <li>Nesterov\u2019s Accelerated Gradient (NAG): Improves convergence rate from  to .  </li> <li>Widely used in machine learning (e.g., training deep neural networks).  </li> </ul>"},{"location":"0%20d%20Algos/#newtons-method-second-order-method","title":"Newton\u2019s Method (Second-Order Method)","text":""},{"location":"0%20d%20Algos/#algorithm_1","title":"Algorithm","text":"<p>Update rule:  </p>"},{"location":"0%20d%20Algos/#convergence_1","title":"Convergence","text":"<ul> <li>Quadratic near optimum: </li> <li>Very fast, but requires Hessian and solving linear systems.</li> </ul>"},{"location":"0%20d%20Algos/#damped-newton","title":"Damped Newton","text":"<p>To maintain global convergence:  </p>"},{"location":"0%20d%20Algos/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Approximate the Hessian to reduce cost.</p> <ul> <li>BFGS and L-BFGS (limited memory version).  </li> <li>Used in large-scale optimization (e.g., machine learning, statistics).  </li> <li>Convergence: superlinear.  </li> </ul>"},{"location":"0%20d%20Algos/#subgradient-methods","title":"Subgradient Methods","text":"<p>For nondifferentiable convex functions (e.g., ).</p> <p>Update rule:  </p> <ul> <li> : subdifferential (set of all subgradients).  </li> <li>Convergence:  with diminishing step sizes.  </li> <li>Useful in large-scale, nonsmooth optimization.</li> </ul>"},{"location":"0%20d%20Algos/#proximal-methods","title":"Proximal Methods","text":"<p>For composite problems:  where  is smooth convex,  convex but possibly nonsmooth.</p> <p>Proximal operator: </p> <ul> <li>Proximal gradient descent: </li> <li>Widely used in sparse optimization (e.g., Lasso).</li> </ul>"},{"location":"0%20d%20Algos/#interior-point-methods","title":"Interior-Point Methods","text":"<p>Transform constrained problem into a sequence of unconstrained problems using barrier functions.</p> <p>For constraint , replace with barrier:  </p> <p>Solve:  for increasing .</p>"},{"location":"0%20d%20Algos/#properties","title":"Properties","text":"<ul> <li>Polynomial-time complexity for convex problems.  </li> <li>Extremely accurate solutions.  </li> <li>Basis of general-purpose solvers (e.g., CVX, MOSEK, Gurobi).  </li> </ul>"},{"location":"0%20d%20Algos/#coordinate-descent","title":"Coordinate Descent","text":"<p>At each iteration, optimize w.r.t. one coordinate (or block of coordinates):</p> <p> </p> <ul> <li>Works well for high-dimensional problems.  </li> <li>Used in Lasso, logistic regression, and large-scale ML problems.</li> </ul>"},{"location":"0%20d%20Algos/#primal-dual-and-splitting-methods","title":"Primal-Dual and Splitting Methods","text":"<ul> <li> <p>ADMM (Alternating Direction Method of Multipliers):   Splits problem into subproblems, solves in parallel.   Popular in distributed optimization and ML.  </p> </li> <li> <p>Primal-dual interior-point methods:   Solve both primal and dual simultaneously.  </p> </li> </ul>"},{"location":"0%20d%20Algos/#summary-of-convergence-rates","title":"Summary of Convergence Rates","text":"Method Smooth Convex Strongly Convex Gradient Descent Linear Accelerated Gradient Linear Subgradient \u2013 Newton\u2019s Method Quadratic (local) Quadratic Interior-Point Polynomial-time Polynomial-time"},{"location":"0%20d%20Algos/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<ul> <li>Small problems, high accuracy: Newton, Interior-point.  </li> <li>Large-scale smooth problems: Gradient descent, Nesterov acceleration, L-BFGS.  </li> <li>Large-scale nonsmooth problems: Subgradient, Proximal, ADMM.  </li> <li>Sparse / structured constraints: Coordinate descent, Proximal methods.  </li> </ul>"},{"location":"0%20d%20Algos/#log-concavity-and-log-convexity","title":"Log-concavity and Log-convexity","text":"<p>A function  is:</p> <ul> <li>Log-concave if  is concave.</li> <li>Log-convex if  is convex.</li> </ul>"},{"location":"0%20d%20Algos/#relevance","title":"Relevance","text":"<ul> <li>Log-concave functions appear in probability (many common distributions have log-concave densities, such as Gaussian, exponential, and uniform). This ensures tractability of maximum likelihood estimation.</li> <li>Log-convexity is useful in geometric programming, where monomials and posynomials are log-convex.</li> </ul>"},{"location":"0%20d%20Algos/#examples","title":"Examples","text":"<ul> <li>Gaussian density is log-concave.</li> <li>Exponential function is log-convex.</li> </ul>"},{"location":"0%20d%20Algos/#geometric-programming","title":"Geometric Programming","text":"<p>Geometric programming (GP) is a class of problems of the form:  where each  is a posynomial.</p> <ul> <li>Monomial: , with , exponents real.</li> <li>Posynomial: Sum of monomials.</li> </ul> <p>By applying the log transformation , the problem becomes convex.</p>"},{"location":"0a%20LA/","title":"LA Prerequisites","text":""},{"location":"0a%20LA/#linear-algebra-prerequisites","title":"Linear Algebra Prerequisites","text":"<ul> <li> <p>Vector spaces and norms: We work primarily in , the -dimensional Euclidean space. The Euclidean norm is , but other norms, such as  or , are also important.</p> </li> <li> <p>Inner products: An inner product in  is .</p> </li> <li> <p>Angle in inner product: The angle  between two nonzero vectors  is defined via .</p> </li> <li> <p>Affine sets: A set of the form  , where  is a matrix and  is a vector. Affine sets are the natural generalisation of lines and planes.</p> </li> <li> <p>Linear independence: A set of vectors  is linearly independent if        implies .</p> </li> <li> <p>Outer product: Given vectors , their outer product is   , which is an  matrix.  (Contrast with the inner product , which is a scalar.)</p> </li> <li> <p>Cauchy\u2013Schwarz inequality: For any , .   Equality holds iff  and  are linearly dependent.</p> </li> <li> <p>Projection onto a vector: The projection of  onto  (with ) is  . This gives the component of  in the direction of .</p> </li> <li> <p>Subspace spanned by  perpendicular (orthogonal) vectors: If  are mutually perpendicular (orthogonal) and nonzero, then they form an orthogonal basis for their span. The subspace is ,  and any  can be uniquely written as  with coefficients .</p> </li> <li> <p>Projection onto a subspace:  Let , where the vectors are linearly independent. If  has orthonormal basis , then the projection of  onto  is   . In matrix form, if , then .</p> </li> <li> <p>Projection onto a convex set:  Projecting two points on convex set is not expansive i.e. the distance between projection of points is always less than equal to the original distance between the points.</p> </li> </ul>"},{"location":"0a%20LA/#matrix-concepts","title":"Matrix Concepts","text":"<ul> <li> <p>Rank:  The rank of a matrix , denoted , is the dimension of its column space (or equivalently, row space). It is the number of linearly independent columns (or rows). .  </p> </li> <li> <p>Null space (kernel): The null space of  is /  It contains all solutions to the homogeneous system . Its dimension is called the nullity of . By the rank\u2013nullity theorem </p> </li> <li> <p>Determinant:  For a square matrix , the determinant  is a scalar with these properties:  </p> <ul> <li>  is singular (non-invertible).  </li> <li> .  </li> <li> .  </li> <li>Geometric meaning:  gives the volume scaling factor of the linear map .</li> </ul> </li> <li> <p>Range (column space): The range of  is  It is the span of the columns of . .  </p> </li> </ul>"},{"location":"0a%20LA/#subspaces-and-related-concepts","title":"Subspaces and Related Concepts","text":"<ul> <li> <p>Subspace: A subset  is a subspace if it satisfies:  </p> <ol> <li>  (contains the zero vector)  </li> <li>Closed under addition:  </li> <li>Closed under scalar multiplication:  </li> </ol> <p>Examples: column space of a matrix, null space of a matrix,  itself.  </p> </li> <li> <p>Orthogonal complement: For a subspace , the orthogonal complement  is </p> <ul> <li>  is a subspace.  </li> <li> .  </li> <li> .  </li> </ul> </li> <li> <p>Affine subspace:  An affine subspace is a translation of a subspace. Formally,    where  is a subspace and  is a fixed point.  </p> <ul> <li>Examples: lines or planes that do not pass through the origin.  </li> <li>If , the affine subspace is just a subspace.</li> </ul> </li> </ul>"},{"location":"0a%20LA/#eigenvalues-eigenvectors-and-symmetric-matrices","title":"Eigenvalues, Eigenvectors, and Symmetric Matrices","text":"<ul> <li> <p>Eigenvalue and Eigenvector: For a square matrix , a scalar  (or ) is an eigenvalue if there exists a nonzero vector  such that    The vector  is called an eigenvector corresponding to .  </p> <ul> <li> </li> <li>  gives the characteristic equation to find eigenvalues.  </li> </ul> </li> <li> <p>Symmetric matrix:  A matrix  is symmetric if </p> <ol> <li>All eigenvalues of a symmetric matrix are real.  Eigenvectors of symmetric matrices are orthogonal. </li> <li>There exists an orthonormal set of eigenvectors that spans .  </li> <li>  can be diagonalized as    where  is an orthogonal matrix of eigenvectors () and  is a diagonal matrix of eigenvalues.  </li> </ol> </li> <li> <p>Positive Semidefinite Matrices   A symmetric matrix  is positive semidefinite (PSD) if </p> <ol> <li>  is symmetric: .  </li> <li>All eigenvalues of  are non-negative: .  </li> <li>If  for all , then  is positive definite (PD).  </li> <li>  can be diagonalized as    where  is orthogonal and  is a diagonal matrix with .  </li> <li>  if A is PSD. </li> <li>for any matrix A,  and  are PSD.</li> </ol> </li> </ul>"},{"location":"0a%20LA/#continuity-and-lipschitz-continuity","title":"Continuity and Lipschitz Continuity","text":"<ul> <li> <p>Continuity: A function  is continuous at  if    Equivalently, for every  there exists  such that    If this holds for all , then  is continuous on its domain.  </p> </li> <li> <p>Lipschitz continuity:  A function  is Lipschitz continuous if there exists a constant  such that </p> <ul> <li>The smallest such  is called the Lipschitz constant.  </li> <li>Lipschitz continuity  continuity (but not vice versa).  </li> <li>If  is differentiable and  for all , then  is Lipschitz continuous with constant .  </li> </ul> </li> </ul>"},{"location":"0a%20LA/#gradient-and-hessian","title":"Gradient and Hessian","text":"<ul> <li> <p>Gradient:  Let  be differentiable.   The gradient of  at  is the vector of partial derivatives: </p> <ul> <li>It points in the direction of steepest ascent of .  </li> <li>The magnitude  gives the rate of increase in that direction.  </li> </ul> </li> <li> <p>Hessian:  If  is twice differentiable, the Hessian matrix at  is the matrix of second-order partial derivatives: </p> <ul> <li>The Hessian is symmetric if  is twice continuously differentiable.  </li> <li>It describes the curvature of :  <ul> <li>  (PSD)  is locally convex.  </li> <li>  (PD)  is locally strictly convex.  </li> </ul> </li> </ul> </li> </ul>"},{"location":"0b%20Convex%20Sets/","title":"Convex Sets","text":"<ul> <li>Convexity of sets: A set  is convex if for any  and , we have .</li> <li>Closed sets: A set is closed if it contains all its limit points. The closure of a set is the smallest closed set containing it.</li> <li>Extreme points: A point in a convex set is extreme if it cannot be expressed as a convex combination of two other distinct points in the set. For polyhedra, extreme points correspond to vertices.</li> </ul>"},{"location":"0b%20Convex%20Sets/#convex-combination","title":"Convex Combination","text":"<p>A convex combination of  is  </p> <p>This is simply a weighted average where weights are nonnegative and sum to 1.</p>"},{"location":"0b%20Convex%20Sets/#convex-hull","title":"Convex Hull","text":"<p>The convex hull of a set  is the collection of all convex combinations of points in . It is the smallest convex set containing .</p> <p><code>Geometric intuition: Imagine stretching a rubber band around the points; the enclosed region is the convex hull.</code></p>"},{"location":"0b%20Convex%20Sets/#cones","title":"Cones","text":"<ul> <li> <p>A  cone is a set  such that if  and , then .   In words: a cone is closed under nonnegative scalar multiplication.  </p> </li> <li> <p>The conic hull (or convex cone) of a set  is the collection of all conic combinations of points in : </p> </li> <li> <p>A cone is not necessarily a subspace (since a subspace allows all linear combinations, including negative multiples).   However, every subspace is a cone (because it is closed under nonnegative scaling).  </p> </li> <li> <p>A cone is not necessarily convex. To be convex, a set must be closed under addition and convex combinations, which is not guaranteed for a general cone. A convex cone is a cone that is also convex, i.e., closed under nonnegative linear combinations.</p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#polar-cones","title":"Polar Cones","text":"<ul> <li> <p>Given a cone , the polar cone of  is defined as </p> </li> <li> <p>Intuitively, the polar cone consists of all vectors that form a non-acute angle (inner product ) with every vector in .  </p> </li> <li> <p>The polar of any cone is always a closed convex cone, even if the original cone  is not convex or not closed.  </p> </li> <li> <p>If  is a subspace, then its polar cone  is the orthogonal complement of .  </p> </li> <li> <p>A useful duality property: for a closed convex cone , </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#tangent-cone","title":"Tangent Cone","text":"<ul> <li>Given a set  and a point , the tangent cone (also called the contingent cone) at  is defined as </li> <li>Intuitively:  is the set of all directions  in which you can \u201cmove infinitesimally\u201d inside  starting from .</li> <li>If  is in the interior of , then .  </li> <li>If  is on the boundary,  consists of directions that keep you inside  locally.</li> </ul>"},{"location":"0b%20Convex%20Sets/#normal-cone","title":"Normal Cone","text":"<ul> <li> <p>For a convex set  and a point , the normal cone at  is defined as </p> </li> <li> <p>Each  defines a supporting hyperplane to  at :  </p> </li> </ul> <p> </p> <ul> <li> <p>Relationship: the normal cone is the polar cone of the tangent cone: </p> </li> <li> <p>  is always a closed convex cone.</p> </li> <li>Intuitively:  is the set of vectors pointing \u201coutward\u201d and supporting the set  at .  </li> <li>Interior point of :  (no outward direction, since  is surrounded).  </li> <li>Boundary point:  contains outward-pointing directions normal to the boundary.  </li> <li>Corner/vertex:  is a cone of outward normals, capturing multiple \u201cfaces\u201d meeting at .  </li> </ul>"},{"location":"0b%20Convex%20Sets/#why-is-the-normal-cone-important","title":"Why is the normal cone important?","text":""},{"location":"0b%20Convex%20Sets/#first-order-optimality-condition","title":"First-order optimality condition","text":"<ul> <li> <p>For the problem  subject to , a point  is optimal if:  </p> <p> </p> <p>This says the slope (subgradient) of  is exactly balanced by the \u201cpush back\u201d of the constraint set.  </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#connection-to-tangent-cone","title":"Connection to tangent cone","text":"<ul> <li> <p>The normal cone is the polar cone of the tangent cone:  </p> <p> </p> <p>This duality links feasible directions (tangent cone) with blocking directions (normal cone).  </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#geometric-interpretation","title":"Geometric interpretation","text":"<ul> <li>Normals describe which directions leave the set if you try to move outward.  </li> <li>They capture the \u201cboundary geometry\u201d of , generalizing the idea of perpendicular vectors to surfaces. </li> </ul>"},{"location":"0b%20Convex%20Sets/#comparison-of-tangent-normal-and-polar-cones","title":"Comparison of Tangent, Normal, and Polar Cones","text":"Cone Applies To Meaning Interior Point Boundary Point Key Facts Tangent Any set (convex nicer) Feasible move directions Restricted to stay in  Local geometry of  Normal Convex sets (gen. exist) Outward blocking dirs Outward rays/cones Closed, convex;  Polar Any cone  Non-acute dirs wrt  Not point-specific Not point-specific Closed, convex;  if closed convex"},{"location":"0b%20Convex%20Sets/#hyperplanes-and-half-spaces","title":"Hyperplanes and Half-spaces","text":"<ul> <li>A hyperplane is the solution set of .</li> <li>A half-space is one side of a hyperplane, defined as  or .</li> <li>These objects are convex and serve as building blocks in constraints.</li> </ul> <p>Separation and Supporting Hyperplanes: One of the most powerful results in convex geometry is the separating hyperplane theorem: two disjoint convex sets can be separated by a hyperplane. For a convex set  and a point , there exists a hyperplane that separates  from . This underpins duality theory in optimisation. A supporting hyperplane touches a convex set at one or more points but does not cut through it.</p>"},{"location":"0c%20Convex%20Functions/","title":"Convex Functions","text":"<p>A function  is convex if its domain  is convex and, for all  and all :</p> <p> </p> <ul> <li>Convex domain: For any , the line segment connecting them lies entirely in .  </li> <li>The graph of  lies below or on the straight line connecting any two points on it (\u201cbowl-shaped\u201d).</li> </ul>"},{"location":"0c%20Convex%20Functions/#first-order-condition","title":"First-order condition:","text":"<p>For a convex function  defined on a convex domain :</p>"},{"location":"0c%20Convex%20Functions/#1-differentiable-case","title":"1. Differentiable case","text":"<p>If  is differentiable at , the gradient  satisfies, for all :</p> <p> </p> <ul> <li>Geometric meaning: The tangent hyperplane at  lies below the graph of  at all points.  </li> <li>Domain of gradient:  exists for all .</li> </ul>"},{"location":"0c%20Convex%20Functions/#2-non-differentiable-case-subgradients","title":"2. Non-differentiable case (subgradients)","text":"<p>If  is convex but not differentiable at , a subgradient  satisfies:</p> <p> </p> <ul> <li>The set of all such  is called the subdifferential at :  </li> </ul> <p> </p> <ul> <li>Geometric meaning: Even at a \"kink,\" there exists a hyperplane (with slope ) that lies below the graph at all points.  </li> <li>If  is differentiable at , .</li> </ul>"},{"location":"0c%20Convex%20Functions/#second-order-hessian-condition","title":"Second-order (Hessian) condition:","text":"<p>If  is twice differentiable,  is convex if and only if its Hessian matrix  is positive semidefinite for all :</p> <p> </p> <p>Positive semidefinite Hessian means the function curves upward or is flat in all directions, never downward.  </p> <p><code>\u201cCurvature is nonnegative in all directions.\u201d</code></p>"},{"location":"0c%20Convex%20Functions/#examples-of-convex-functions","title":"Examples of Convex Functions","text":"<ol> <li>Quadratic functions: , where  (positive semidefinite).  </li> <li>Norms:  for .  </li> <li>Exponential function: .  </li> <li>Negative logarithm:  on .  </li> <li>Linear functions: .  </li> </ol>"},{"location":"0c%20Convex%20Functions/#subgradients-proximal-operators","title":"Subgradients &amp; Proximal Operators","text":"<p>Modern optimization in machine learning often deals with nonsmooth functions e.g.,  regularization, hinge loss in SVMs, indicator constraints. Gradients are not always defined at these nonsmooth points, so we need subgradients and proximal operators. For a differentiable convex function , the gradient  provides the slope for descent. But many convex functions are not differentiable everywhere:</p> <ul> <li>Absolute value:     (non-differentiable at )  </li> <li>Hinge loss:  </li> <li>  norm:  </li> </ul> <p>At kinks/corners, derivatives don\u2019t exist.  </p> <p>A vector  is a subgradient of a convex function  at point  if:</p> <p> </p> <ul> <li>Geometric meaning:  defines a supporting hyperplane at  that lies below the function everywhere.  </li> <li>The set of all subgradients at  is called the subdifferential, written:    </li> </ul>"},{"location":"0c%20Convex%20Functions/#example-1-absolute-value","title":"Example 1: Absolute value","text":"<p>Take .  </p> <ul> <li>If : .  </li> <li>If : .  </li> <li>If : derivative doesn\u2019t exist. But  Any slope between  and  is a valid subgradient at the kink.  Intuition: At , instead of one tangent line, there\u2019s a whole fan of supporting lines.</li> </ul>"},{"location":"0c%20Convex%20Functions/#example-2-hinge-loss","title":"Example 2: Hinge loss","text":"<p> .  </p> <ul> <li>If : .  </li> <li>If : .  </li> <li>If : </li> </ul>"},{"location":"0c%20Convex%20Functions/#why-subgradients-matter","title":"Why subgradients matter","text":"<ul> <li>They generalize gradients to nonsmooth convex functions.  </li> <li>Subgradient descent update:    </li> <li>Convergence is guaranteed, though slower than gradient descent:</li> <li>Smooth case:  rate  </li> <li>Nonsmooth case:  rate  </li> </ul>"},{"location":"0c%20Convex%20Functions/#proximal-operators","title":"Proximal Operators","text":"<p>Nonsmooth penalties (like  norm, indicator functions) appear frequently: - Lasso:   (L1 norm is nonsmooth) - SVM: hinge loss   - Constraints: e.g.,  for some convex set  </p> <p>Plain gradient descent cannot directly handle the nonsmooth part.</p> <p>The proximal operator of a function  with step size  is:</p> <p> </p> <ul> <li>Interpretation:  </li> <li>Stay close to  (the quadratic term)  </li> <li> <p>While reducing the penalty  </p> </li> <li> <p>Geometric meaning: A regularized projection of  onto a region encouraged by .  </p> </li> </ul>"},{"location":"0c%20Convex%20Functions/#example-1-l_1-norm-soft-thresholding","title":"Example 1:  norm (soft-thresholding)","text":"<p>Let . Then:</p> <p> </p> <p>This is the soft-thresholding operator:</p> <ul> <li>Shrinks small entries of  to zero \u2192 sparsity.  </li> <li>Reduces magnitude of large entries but keeps their sign.  </li> </ul> <p>\ud83d\udc49 This is the key step in Lasso regression and compressed sensing.</p>"},{"location":"0c%20Convex%20Functions/#example-2-indicator-function","title":"Example 2: Indicator function","text":"<p>Let , where  if , and  otherwise. Then:</p> <p> </p> <p>the Euclidean projection of  onto .  </p> <p>Example: if  is the unit ball , prox just normalizes  if it\u2019s outside.</p>"},{"location":"0c%20Convex%20Functions/#example-3-squared-ell_2-norm","title":"Example 3: Squared  norm","text":"<p>If , then</p> <p> </p> <p>This is just a shrinkage toward the origin.</p>"},{"location":"0c%20Convex%20Functions/#why-proximal-operators-matter","title":"Why proximal operators matter","text":"<p>They allow efficient algorithms for composite objectives:</p> <p> </p> <p>where: -  is smooth (differentiable with Lipschitz gradient) -  is convex but possibly nonsmooth  </p> <p>Proximal gradient method (ISTA):  </p> <p>This generalizes gradient descent by replacing the plain update with a proximal step that handles .</p> <ul> <li>If : reduces to gradient descent  </li> <li>If : reduces to proximal operator (e.g. projection, shrinkage)  </li> </ul>"},{"location":"0c%20Convex%20Functions/#3-intuition-summary","title":"3. Intuition Summary","text":"<ul> <li>Subgradients:  </li> <li>Generalized \u201cslopes\u201d for nonsmooth convex functions.  </li> <li>At corners, we have a set of possible slopes (subdifferential).  </li> <li> <p>Enable subgradient descent with convergence guarantees.  </p> </li> <li> <p>Proximal operators:  </p> </li> <li>Generalized update steps for nonsmooth regularizers.  </li> <li>Combine a gradient-like move with a \u201ccorrection\u201d that enforces structure (sparsity, constraints).  </li> <li>Core of algorithms like ISTA, FISTA, ADMM.  </li> </ul>"},{"location":"0c%20Convex%20Functions/#4-big-picture-in-ml","title":"4. Big Picture in ML","text":"<ul> <li>Subgradients: Let us train models with nonsmooth losses (SVM hinge loss, ).  </li> <li>Proximal operators: Let us efficiently solve regularized problems (Lasso, group sparsity, constrained optimization).  </li> <li>Intuition:  </li> <li>Subgradient = \"any slope that supports the function\"  </li> <li>Proximal = \"soft move toward minimizing the nonsmooth part\"  </li> </ul>"},{"location":"0c%20Convex%20Functions/#subgradients-proximal-operators_1","title":"Subgradients &amp; Proximal Operators","text":"<ul> <li>Subgradient:  is a subgradient if </li> <li>Proximal operator: </li> <li>Context: Needed for nonsmooth functions (e.g., L1-regularization, hinge loss).  </li> <li>ML relevance: SVM hinge loss, Lasso, sparse dictionary learning. Proximal methods handle shrinkage or projection efficiently.  </li> <li>Intuition:  </li> <li>Subgradient: Like a tangent for a function that isn\u2019t smooth\u2014provides a direction to descend.  </li> <li>Proximal operator: Think of it as a \u201csoft step\u201d toward minimizing a nonsmooth function, like gently nudging a point toward a feasible or sparse region.</li> </ul>"},{"location":"0c%20Convex%20Functions/#convex-optimisation-problems","title":"Convex Optimisation Problems","text":"<p>A convex optimisation problem has the form:</p> <p>  where  and  are convex functions, and  are affine. The feasible set is convex, and any local minimum is a global minimum.</p>"},{"location":"0d%20Optimality%20Conditions/","title":"Optimality Conditions","text":""},{"location":"0d%20Optimality%20Conditions/#optimality-conditions-for-convex-optimization-problems","title":"Optimality Conditions for Convex Optimization Problems","text":"<p>Convex optimization problems have the important property that any local minimum is also a global minimum. Depending on whether the problem is unconstrained or constrained, and whether the function is differentiable or not, the optimality conditions differ.</p>"},{"location":"0d%20Optimality%20Conditions/#1-unconstrained-problems","title":"1. Unconstrained Problems","text":""},{"location":"0d%20Optimality%20Conditions/#11-differentiable-convex-functions","title":"1.1 Differentiable Convex Functions","text":"<p>For a differentiable convex function :</p> <p> </p> <p>Optimality condition:</p> <p> </p> <p>Intuition: The gradient points in the direction of steepest increase, so a zero gradient indicates a flat spot (minimum).</p> <p>Examples:</p> <p>a) Quadratic function:   \u2192 set to 0 \u2192  </p> <p>b) Sum of Squared Errors   \u2192 set to 0 \u2192  (the mean)</p>"},{"location":"0d%20Optimality%20Conditions/#12-non-differentiable-convex-functions","title":"1.2 Non-Differentiable Convex Functions","text":"<p>For convex but non-differentiable functions:</p> <p> </p> <p>Optimality condition:</p> <p> </p> <p>Intuition: The subgradient generalizes the derivative for functions with \"kinks.\"</p> <p>Examples:</p> <p>a) Sum of Absolute Errors (SAE):   = median of the data points.</p> <p>b) Maximum function:   Subgradient:  \u2192 minimum at  </p>"},{"location":"0d%20Optimality%20Conditions/#2-constrained-problems","title":"2. Constrained Problems","text":"<p>Consider a convex optimization problem:</p> <p> </p> <p>where  is convex and  is a convex feasible set.</p>"},{"location":"0d%20Optimality%20Conditions/#interior-point","title":"Interior Point:","text":"<p>If  lies strictly inside the feasible set, then the unconstrained condition applies:</p> <p> </p> <p>Intuition: There are no boundary restrictions, so the gradient (or subgradient) must vanish.</p>"},{"location":"0d%20Optimality%20Conditions/#boundary-point","title":"Boundary Point:","text":"<p>If  lies on the boundary of , then for  to be optimal:</p> <ul> <li>The negative gradient must lie in the normal cone of  at :</li> </ul> <p> </p> <ul> <li>Equivalently, the gradient must form an angle of at least  with any feasible direction  inside :</li> </ul> <p> </p> <p>where  is the tangent (feasible) cone at , and  is the normal cone at .</p> <p>Intuition: At the boundary, the optimal direction cannot point into the feasible set because any movement along a feasible direction increases the objective.</p>"},{"location":"0d%20Optimality%20Conditions/#compact-form","title":"Compact Form","text":"<p>Combining interior and boundary cases:</p> <p> </p> <p>where:</p> <ul> <li>  for interior points  </li> <li>  is the normal cone for boundary points  </li> </ul> <p>This is a general convex optimality condition for constrained problems, valid for both differentiable and non-differentiable .</p>"},{"location":"0d%20Optimality%20Conditions/#intution","title":"Intution","text":"<p>Imagine a region of allowed points, called the feasible set  \ud835\udc4b X. Points strictly inside the region form the interior, where movement in any direction is possible without leaving the set. The edges and corners of the region form the boundary, where movement is restricted because you can only move along directions that remain feasible. Consider standing at a point  \ud835\udc65 ^ x ^  on this boundary. From here, you cannot move freely in all directions; you can only move along directions that stay inside the feasible set. These allowable directions form what is called the tangent cone at  \ud835\udc65 ^ x ^ , encompassing movements along the boundary or slightly into the interior.</p> <p>Opposing these feasible directions is the normal cone, which consists of vectors that point outward from the feasible region, effectively \u201cblocking\u201d any movement that would stay inside. At an optimal boundary point, the gradient of the objective function points outward, lying within the normal cone. This means that moving along any feasible direction \u2014 whether along the boundary or slightly into the interior \u2014 cannot decrease the objective function. The gradient \u201cpushes against\u201d all allowable moves, so any small displacement that respects the constraints either increases the objective or leaves it unchanged.</p> <p>This behavior contrasts with an interior optimum, where the gradient is zero and movement in any direction does not change the objective. At a boundary optimum, the gradient is non-zero but oriented such that all feasible directions are blocked from reducing the objective. Even though the gradient is not zero, the point is still optimal because the boundary restricts movement: every allowed step either raises the objective or keeps it the same. In this way, a boundary point can be a true optimum, and the outward-pointing gradient is the formal expression of the intuitive idea that you cannot \u201cgo downhill\u201d without leaving the feasible region.</p>"},{"location":"0d%20Optimality%20Conditions/#23-example-quadratic-with-constraint","title":"2.3 Example: Quadratic with Constraint","text":"<ul> <li>Feasible set:  </li> <li>Gradient:  </li> </ul> <p>Check optimality:</p> <ul> <li>Interior check ():  \u2192 infeasible  </li> <li>Boundary check ():  \u2192 satisfied  </li> </ul> <p>Solution: </p>"},{"location":"0e%20Optimization%20Algos/","title":"Optimization Algos","text":""},{"location":"0e%20Optimization%20Algos/#optimization-algos","title":"Optimization:  Algos","text":"<p>Optimization is the core of machine learning: training a model is an optimization problem where we search for parameters  that minimize a loss or maximize a likelihood. The choice of optimization algorithm depends on problem type, scale, constraints, smoothness, and stochasticity. </p>"},{"location":"0e%20Optimization%20Algos/#gradient-descent","title":"Gradient descent","text":"<p>We want to minimize a function over a feasible set:</p> <p> </p> <p>At iteration , given the current point , we approximate  around  using a first-order Taylor expansion plus a quadratic regularization term. The square term penalizes moving too far from the current point . You can think of it as saying: \u201cI trust my first-order approximation, but only locally. If I move too far, the approximation might be bad \u2014 so I add a cost for large steps.\u201d</p> <p> </p> <p>where  is the step size (also called the learning rate).</p> <p>Thus, the update rule is defined as the solution of:</p> <p> </p> <p>To find , take the derivative of the objective inside the brackets with respect to :</p> <p> </p> <p>Rearranging gives:</p> <p> </p> <p>Therefore, the update rule is:</p> <p>  - Under smoothness and strong convexity, gradient descent converges linearly, improving by a fixed fraction at each iteration.</p>"},{"location":"0e%20Optimization%20Algos/#subgradient-method","title":"Subgradient Method","text":"<p>We want to minimize a convex (possibly nonsmooth) function:</p> <p> </p> <p>where  is convex but may not be differentiable everywhere.</p> <p>At iteration , we compute a subgradient , where  is the set of subgradients at .</p> <p>The update rule is:</p> <p> </p> <p>where: -  is a subgradient of  at , -  is the step size (which may change with ), -  is the projection onto the feasible set .</p> <p>If , the update reduces to:</p> <p> </p> <p>Unlike gradient descent, the subgradient method does not achieve linear convergence. Instead:</p> <ul> <li>If  is convex (not strongly convex), with a diminishing step size such as , we obtain a sublinear convergence rate:</li> </ul> <p> </p> <p>where: -  is an optimal solution, -  is the average iterate, -  is the distance from the initial point to the optimum, -  is a bound on the subgradients ().</p> <p>Thus the convergence rate is:</p> <p> </p>"},{"location":"0e%20Optimization%20Algos/#accelerated-gradient-descent-momentum","title":"Accelerated Gradient Descent: Momentum","text":"<p>The standard gradient descent update is:</p> <p> </p> <p>where: -  is the current point -  is the step size (learning rate) -  is the gradient at  </p> <p>Intuition: Imagine rolling a ball on a hill: the ball moves in the steepest downhill direction at each step. This works, but can be slow if the valley is long and narrow: the ball zig-zags and takes many small steps to reach the bottom.</p> <p>Momentum adds the idea of inertia:</p> <p> </p> <ul> <li>  represents the velocity of the ball  </li> <li>Instead of reacting only to the current slope, the ball remembers its previous speed and direction, helping it roll faster through shallow or consistent slopes  </li> <li>Momentum reduces zig-zagging in steep valleys and accelerates movement along flat directions</li> </ul> <p>Analogy: - No momentum \u2192 ball stops after each step, carefully following the slope - With momentum \u2192 ball keeps moving, building speed along the valley, only slowing when gradients push against it</p>"},{"location":"0e%20Optimization%20Algos/#gradient-descent-momentum","title":"Gradient Descent + Momentum","text":"<p>The update rule with momentum:</p> <p> </p> <p>where: -  controls how much past velocity is retained  </p> <p>Breakdown: 1. Gradient step:  \u2192 move downhill 2. Momentum step:  \u2192 continue moving along previous direction  </p> <p>Intuition: - If gradients keep pointing roughly in the same direction, momentum accelerates the steps - If gradients oscillate, momentum smooths the path, reducing overshooting</p>"},{"location":"0e%20Optimization%20Algos/#convergence-intuition","title":"Convergence Intuition","text":"<ul> <li>For convex and smooth functions, momentum accelerates convergence:  </li> <li>Standard GD:  </li> <li> <p>GD + Momentum / Nesterov:  </p> </li> <li> <p>Each step combines the current slope and accumulated speed from past steps  </p> </li> <li>Momentum acts like a rolling ball in a frictionless valley: the more it rolls in the right direction, the faster it reaches the minimum</li> </ul>"},{"location":"0e%20Optimization%20Algos/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Momentum is memory: it remembers the direction of previous steps  </li> <li>It smooths oscillations in narrow valleys  </li> <li>It accelerates convergence along consistent gradient directions  </li> <li>Hyperparameter  controls the inertia: higher  \u2192 longer memory, faster but riskier steps</li> </ol>"},{"location":"0f%20Convergence/","title":"Properties","text":""},{"location":"0f%20Convergence/#function-properties-for-optimization-strong-convexity-smoothness-and-conditioning","title":"Function Properties for Optimization: Strong Convexity, Smoothness, and Conditioning","text":""},{"location":"0f%20Convergence/#strong-convexity","title":"Strong Convexity","text":"<p>A function  is -strongly convex if</p> <p> </p> <p>If  is twice differentiable, this is equivalent to</p> <p> </p> <ul> <li>Guarantees a unique minimizer.  </li> <li>Gradient-based methods achieve linear convergence.  </li> <li>Prevents flat regions where optimization would stall.  </li> </ul>"},{"location":"0f%20Convergence/#why-convergence-may-be-slow-without-strong-convexity","title":"Why Convergence May Be Slow Without Strong Convexity","text":"<ul> <li>If  is convex but not strongly convex, it can have flat regions (zero curvature).  </li> <li>Gradients may be very small in these directions \u2192 gradient steps shrink, and convergence becomes sublinear:  </li> </ul> <ul> <li> <p>Example:  is convex but not strongly convex near . Gradient descent steps become tiny near the minimum \u2192 slow convergence.  </p> </li> <li> <p>Contrast:  is strongly convex () \u2192 linear convergence.  </p> </li> </ul>"},{"location":"0f%20Convergence/#examples","title":"Examples","text":"<ol> <li> <p>Quadratic function:  \u2192 , strongly convex \u2192 fast convergence.  </p> </li> <li> <p>Quartic function:  \u2192 convex but not strongly convex near  \u2192 slow convergence.  </p> </li> <li> <p>Ridge Regression (L2 Regularization): </p> <ul> <li>The first term  is convex.  </li> <li>The L2 term  is strongly convex ().  </li> <li>Adding the L2 penalty makes the entire objective strongly convex with .  </li> <li>Implications: <ul> <li>Unique solution:  even if  is singular or ill-conditioned.  </li> <li>Stable optimization: gradient-based methods converge linearly.  </li> <li>Prevents overfitting by controlling the size of weights.  </li> </ul> </li> </ul> </li> </ol>"},{"location":"0f%20Convergence/#smoothness-l-smoothness","title":"Smoothness (L-smoothness)","text":"<p>A function  is -smooth if</p> <p> </p> <p>If twice differentiable:</p> <p> </p> <ul> <li>Limits how steep  can be.  </li> <li>Ensures gradients change gradually \u2192 stable gradient steps.  </li> <li>Guarantees safe step sizes:  for gradient descent.  </li> </ul>"},{"location":"0f%20Convergence/#why-smoothness-matters-for-convergence","title":"Why Smoothness Matters for Convergence","text":"<ul> <li>Without smoothness, the gradient can change abruptly.  </li> <li>A large gradient could lead to overshooting, oscillation, or divergence.  </li> <li>Smoothness ensures predictable, stable progress along the gradient.  </li> </ul> <p>Examples: - Quadratic : . - Logistic regression loss: smooth with  depending on . - Non-smooth case:  \u2192 gradient jumps at , cannot guarantee smooth progress \u2192 need subgradient methods.  </p>"},{"location":"0f%20Convergence/#condition-number","title":"Condition Number","text":"<p>The condition number is defined as</p> <p> </p> <ul> <li>Measures how \u201cstretched\u201d the optimization landscape is.  </li> <li>High  \u2192 narrow, elongated valleys \u2192 gradient descent zig-zags, converges slowly.  </li> <li>Low  \u2192 round bowl \u2192 fast convergence.  </li> </ul> <p>Examples: - : ,  \u2192 fastest convergence. - : , ,  \u2192 ill-conditioned, very slow. - In ML, normalization (batch norm, feature scaling, whitening) reduces , improving training speed.  </p>"},{"location":"0f%20Convergence/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"0f%20Convergence/#strong-convexity_1","title":"Strong Convexity","text":"<ul> <li>Unique solution (ridge regression).  </li> <li>Linear convergence of gradient-based methods.  </li> <li>Stabilizes optimization by avoiding flatness.  </li> </ul>"},{"location":"0f%20Convergence/#smoothness","title":"Smoothness","text":"<ul> <li>Ensures safe and predictable step sizes.  </li> <li>Avoids overshooting or divergence.  </li> <li>Justifies constant learning rates for many ML losses.  </li> </ul>"},{"location":"0f%20Convergence/#condition-number_1","title":"Condition Number","text":"<ul> <li>Predicts convergence speed.  </li> <li>Guides preprocessing: scaling, normalization, whitening.  </li> <li>Central in designing adaptive optimizers and preconditioning methods.  </li> </ul>"},{"location":"0f%20Convergence/#convergence-rates-of-first-order-methods","title":"Convergence Rates of First-Order Methods","text":"Function Property Gradient Descent Rate Accelerated Gradient (Nesterov) Subgradient Method Rate Convex (not strongly convex) -Strongly Convex Linear:  Linear: faster than GD Condition Number  Iterations  Iterations  \u2013"},{"location":"0f%20Convergence/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Strong convexity: bowl is always curved enough \u2192 unique and fast convergence.  </li> <li>Smoothness: bowl is not too steep \u2192 safe steps, avoids overshooting.  </li> <li>Condition number: how round vs stretched the bowl is \u2192 dictates optimization difficulty.  </li> <li>Without strong convexity \u2192 flat regions \u2192 slow sublinear convergence.  </li> <li>Without smoothness \u2192 steep gradient changes \u2192 possible divergence or oscillations.</li> </ul>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/","title":"Projections and Proximal Operators","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projections-and-proximal-operators-in-constrained-convex-optimization","title":"Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>In many convex optimization problems, we aim to minimize a differentiable convex function  over a closed convex set :</p> <p> </p> <p>A simple gradient step of the form</p> <p> </p> <p>may produce a point that lies outside the feasible region . To ensure feasibility, we introduce the projection operator onto :</p> <p> </p> <p>The projected gradient descent update is then written as:</p> <p> </p> <p>This projection step ensures that each iterate remains within  while still following the descent direction of . Geometrically, it \u201cpulls\u201d the point back to the nearest feasible position in the set after taking a gradient step. This is especially important when  encodes constraints such as non-negativity, norm bounds, or affine restrictions.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#from-projections-to-proximal-operators","title":"From Projections to Proximal Operators","text":"<p>While projection handles explicit set constraints, many optimization problems involve non-smooth regularization terms instead. For such cases, the idea of projection generalizes to the proximal operator.</p> <p>For a convex (possibly non-differentiable) function , the proximal operator is defined as:</p> <p> </p> <p>If  is the indicator function of a convex set \u2014that is,  for  and  otherwise\u2014then the proximal operator reduces exactly to the projection operator:</p> <p> </p> <p>Hence, projections are a special case of proximal operators.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#intuition-and-role-in-optimization","title":"Intuition and Role in Optimization","text":"<p>Both operators serve as correction mechanisms during iterative optimization:</p> <ul> <li>The projection operator ensures feasibility with respect to constraints.  </li> <li>The proximal operator enforces regularization or penalty structure when dealing with non-smooth functions.</li> </ul> <p>Intuitively, these operators help balance descent direction with constraint or regularization structure, maintaining the geometry required for convergence in convex optimization.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#example","title":"Example","text":"<p>If , the projection onto  is:</p> <p> </p> <p>Similarly, if , then the proximal operator corresponds to the soft-thresholding function:</p> <p> </p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#summary","title":"Summary","text":"Concept Mathematical Definition Purpose Projection  Keeps iterates within the feasible set Proximal operator  Handles non-smooth penalties or implicit constraints <p>Figure: Illustration of a projected gradient step. After taking a gradient descent step that moves  outside the feasible set , the projection operator maps it back to the nearest point in , ensuring feasibility of the next iterate.</p>"},{"location":"0h%20AdvancedAlgos/","title":"Advanced Algos","text":""},{"location":"0h%20AdvancedAlgos/#first-order-gradient-based-methods","title":"First-Order Gradient-Based Methods","text":"<p>Used when: Only gradient information is available; scalable to high-dimensional problems.</p>"},{"location":"0h%20AdvancedAlgos/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<ul> <li>Problem: Minimize smooth or convex functions.  </li> <li>Update: </li> <li>Convergence: Convex \u2192 ; Strongly convex \u2192 linear.  </li> <li>Use case: Small convex problems, theoretical baseline.  </li> <li>Pitfalls: Step size too small \u2192 slow; too large \u2192 divergence.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<ul> <li>Problem: Minimize empirical risk over large datasets.  </li> <li>Update:   (mini-batch gradient)  </li> <li>Pros: Scales to huge datasets; cheap per iteration.  </li> <li>Cons: Noisy updates \u2192 requires learning rate schedules.  </li> <li>ML use: Deep learning, large-scale logistic regression.  </li> </ul> <p>Best Practices: Learning rate warmup, linear scaling with batch size, momentum to stabilize updates, cyclic learning rates for exploration.</p>"},{"location":"0h%20AdvancedAlgos/#momentum-nesterov-accelerated-gradient","title":"Momentum &amp; Nesterov Accelerated Gradient","text":"<ul> <li>Problem: Reduce oscillations and accelerate convergence in ill-conditioned problems.  </li> <li>Momentum: </li> <li>Nesterov: Gradient computed at lookahead point \u2192 theoretically optimal for convex problems.  </li> <li>ML use: CNNs, ResNets, EfficientNet.  </li> <li>Pitfalls: High momentum \u2192 oscillations; careful learning rate tuning required.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#adaptive-methods-adagrad-rmsprop-adam-adamw","title":"Adaptive Methods (AdaGrad, RMSProp, Adam, AdamW)","text":"<ul> <li>Problem: Adjust learning rate per parameter for fast/stable convergence.  </li> <li>Behavior: </li> <li>AdaGrad \u2192 aggressive decay, good for sparse features.  </li> <li>RMSProp \u2192 fixes AdaGrad\u2019s rapid decay.  </li> <li>Adam \u2192 RMSProp + momentum.  </li> <li>AdamW \u2192 decouples weight decay for better generalization.  </li> <li>ML use: Transformers, NLP, sparse models.  </li> <li>Pitfalls: Adam may converge to sharp minima \u2192 worse generalization than SGD in CNNs.  </li> <li>Best Practices: Warmup, cosine LR decay, weight decay with AdamW.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#second-order-curvature-aware-methods","title":"Second-Order &amp; Curvature-Aware Methods","text":"<p>Used when: Hessian or curvature information improves convergence; mostly for small/medium models.</p>"},{"location":"0h%20AdvancedAlgos/#newtons-method","title":"Newton\u2019s Method","text":"<ul> <li>Problem: Solve  with smooth Hessian.  </li> <li>Update: </li> <li>Pros: Quadratic convergence.  </li> <li>Cons: Hessian expensive in high dimensions.  </li> <li>ML use: GLMs, small convex models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#quasi-newton-bfgs-l-bfgs","title":"Quasi-Newton (BFGS, L-BFGS)","text":"<ul> <li>Problem: Approximate Hessian using low-rank updates.  </li> <li>Pros: Efficient for medium-scale problems.  </li> <li>Cons: BFGS memory-heavy; L-BFGS preferred.  </li> <li>ML use: Logistic regression, Cox models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#conjugate-gradient","title":"Conjugate Gradient","text":"<ul> <li>Problem: Solve large linear/quadratic problems efficiently.  </li> <li>ML use: Hessian-free optimization; combined with Pearlmutter trick for Hessian-vector products.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#natural-gradient-k-fac","title":"Natural Gradient &amp; K-FAC","text":"<ul> <li>Problem: Precondition gradients using Fisher Information \u2192 invariant to parameterization.  </li> <li>ML use: Large CNNs, transformers; improves convergence in distributed training.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#constrained-specialized-optimization","title":"Constrained &amp; Specialized Optimization","text":""},{"location":"0h%20AdvancedAlgos/#interior-point","title":"Interior-Point","text":"<ul> <li>Problem: Constrained optimization via barrier functions.  </li> <li>ML use: Structured convex problems, LP/QP.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#admm-augmented-lagrangian","title":"ADMM / Augmented Lagrangian","text":"<ul> <li>Problem: Split constraints into easier subproblems with dual updates.  </li> <li>ML use: Distributed optimization, structured sparsity.  </li> </ul>"},{"location":"0h%20AdvancedAlgos/#frankwolfe","title":"Frank\u2013Wolfe","text":"<ul> <li>Problem: Projection-free constrained optimization; linear subproblem instead of projection.  </li> <li>ML use: Simplex, nuclear norm problems.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#coordinate-descent","title":"Coordinate Descent","text":"<ul> <li>Problem: Update one variable at a time.  </li> <li>ML use: Lasso, GLMs, sparse regression.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#proximal-methods","title":"Proximal Methods","text":"<ul> <li>Problem: Efficiently handle nonsmooth penalties.  </li> <li>Algorithms: ISTA (), FISTA ()  </li> <li>ML use: Sparse coding, Lasso, elastic net.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#derivative-free-black-box","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Optimize when gradients unavailable or unreliable.  </li> <li>Algorithms: Nelder\u2013Mead, CMA-ES, Bayesian Optimization  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#optimization-problem-styles","title":"Optimization Problem Styles","text":""},{"location":"0h%20AdvancedAlgos/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<ul> <li>Problem: Maximize likelihood or minimize negative log-likelihood: </li> <li>Algorithms: Newton/Fisher scoring, L-BFGS, SGD, EM, Proximal/Coordinate.  </li> <li>ML use: Logistic regression, GLMs, Gaussian mixture models, HMMs.  </li> <li>Notes: EM guarantees monotonic likelihood increase; Fisher scoring uses expected curvature \u2192 stable.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#empirical-risk-minimization-erm","title":"Empirical Risk Minimization (ERM)","text":"<ul> <li>Problem: Minimize average loss with optional regularization: </li> <li>Algorithms: GD, SGD, Momentum, Adam, L-BFGS, Proximal.  </li> <li>ML use: Regression, classification, deep learning.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#regularized-penalized-optimization","title":"Regularized / Penalized Optimization","text":"<ul> <li>Problem: Add penalties to encourage sparsity or smoothness: </li> <li>Algorithms: Proximal gradient, ADMM, Coordinate Descent, ISTA/FISTA.  </li> <li>ML use: Lasso, Elastic Net, sparse dictionary learning.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#constrained-optimization","title":"Constrained Optimization","text":"<ul> <li>Problem: Minimize with equality/inequality constraints.  </li> <li>Algorithms: Interior-point, ADMM, Frank\u2013Wolfe, penalty/barrier methods.  </li> <li>ML use: Fairness constraints, structured prediction.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#bayesian-map-optimization","title":"Bayesian / MAP Optimization","text":"<ul> <li>Problem: Maximize posterior: </li> <li>Algorithms: Gradient-based, Laplace approximation, Variational Inference, MCMC.  </li> <li>ML use: Bayesian neural networks, probabilistic models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#minimax-adversarial-optimization","title":"Minimax / Adversarial Optimization","text":"<ul> <li>Problem: </li> <li>Algorithms: Gradient descent/ascent, extragradient, mirror descent.  </li> <li>ML use: GANs, adversarial training, robust optimization.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#reinforcement-learning-policy-optimization","title":"Reinforcement Learning / Policy Optimization","text":"<ul> <li>Problem: Maximize expected cumulative reward: </li> <li>Algorithms: Policy gradient, Actor-Critic, Natural Gradient.  </li> <li>ML use: RL agents, sequential decision-making.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<ul> <li>Problem: Optimize multiple competing objectives \u2192 Pareto front.  </li> <li>Algorithms: Scalarization, weighted sum, evolutionary algorithms.  </li> <li>ML use: Multi-task learning, accuracy vs fairness trade-offs.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#metric-embedding-learning","title":"Metric / Embedding Learning","text":"<ul> <li>Problem: Learn embeddings preserving similarity/distance: </li> <li>Algorithms: SGD/Adam with careful sampling.  </li> <li>ML use: Contrastive learning, triplet loss, Siamese networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#combinatorial-discrete-optimization","title":"Combinatorial / Discrete Optimization","text":"<ul> <li>Problem: Optimize discrete/integer variables.  </li> <li>Algorithms: Branch-and-bound, integer programming, RL-based relaxation, Gumbel-softmax.  </li> <li>ML use: Feature selection, neural architecture search, graph matching.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#derivative-free-black-box_1","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Gradients unavailable or noisy.  </li> <li>Algorithms: Bayesian Optimization, CMA-ES, Nelder\u2013Mead.  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#learning-rate-practical-tips","title":"Learning Rate &amp; Practical Tips","text":"<ul> <li>Step decay, cosine annealing, OneCycle, warmup.  </li> <li>Gradient clipping (global norm 1\u20135), batch/layer normalization, FP16 mixed precision.  </li> <li>Decouple weight decay from Adam (AdamW).</li> </ul>"},{"location":"0h%20AdvancedAlgos/#summary","title":"Summary","text":"Algorithm Problem Type ML / AI Use Case GD Smooth / convex Small convex models, baseline SGD Large-scale ERM Deep learning, logistic regression SGD + Momentum Ill-conditioned / deep nets CNNs (ResNet, EfficientNet) Nesterov Accelerated GD Convex / ill-conditioned CNNs, small convex models AdaGrad Sparse features NLP, sparse embeddings RMSProp Stabilized adaptive LR RNNs, sequence models Adam Adaptive large-scale Transformers, small nets AdamW Adaptive + weight decay Transformers, NLP Newton / Fisher Scoring Smooth convex GLMs, small MLE BFGS / L-BFGS Medium convex Logistic regression, Cox models Conjugate Gradient Linear / quadratic Hessian-free optimization, linear regression Natural Gradient / K-FAC Deep nets CNNs, transformers Proximal / ISTA / FISTA Nonsmooth / sparse Lasso, sparse coding, elastic net Coordinate Descent Separable / sparse Lasso, GLMs Interior-Point Constrained convex LP/QP problems ADMM Distributed convex Sparse or structured optimization Frank\u2013Wolfe Projection-free constraints Simplex, nuclear norm problems EM Algorithm Latent variable MLE GMM, HMM, LDA Policy Gradient / Actor-Critic Sequential / RL RL agents Bayesian Optimization Black-box / derivative-free Hyperparameter tuning, NAS CMA-ES / Nelder-Mead Black-box Small networks, continuous black-box Minimax / Gradient Ascent-Descent Adversarial GANs, robust optimization Multi-Objective / Evolutionary Multiple objectives Multi-task learning, fairness Metric Learning / Triplet Loss Similarity embedding Contrastive learning, Siamese nets"},{"location":"0ssc%20Epigraphs/","title":"Epigraphs","text":""},{"location":"0ssc%20Epigraphs/#epigraphs-and-convex-optimization","title":"Epigraphs and Convex Optimization","text":""},{"location":"0ssc%20Epigraphs/#1-definition-of-an-epigraph","title":"1. Definition of an Epigraph","text":"<p>For a function , the epigraph is the set of points lying on or above its graph:</p> <p> </p> <ul> <li>  is a point in -dimensional space.  </li> <li>For each , the condition  means  is at or above the function value.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#2-intuition","title":"2. Intuition","text":"<ul> <li>If you draw a 2D function , the epigraph is the region above the curve.  </li> <li>For example, if , then the epigraph is everything above the parabola.  </li> </ul> <p>So:  </p> <ul> <li>Graph = the curve itself.  </li> <li>Epigraph = the curve + everything above it.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#3-convexity-via-epigraphs","title":"3. Convexity via Epigraphs","text":"<p>A function  is convex if and only if its epigraph is a convex set.  </p> <ul> <li>A set is convex if the line segment between any two points in the set lies entirely within the set.  </li> <li>Geometrically: the \"roof\" (epigraph) above the function must form a bowl-shaped region, not a cave.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#4-examples","title":"4. Examples","text":"<ol> <li>Convex function:  </li> <li>Epigraph = everything above the parabola.  </li> <li>This region is convex: if you connect any two points above the parabola, the line stays above the parabola.  </li> <li> <p>\u21d2  is convex.</p> </li> <li> <p>Non-convex function:  </p> </li> <li>Epigraph = everything above an upside-down parabola.  </li> <li>This region is not convex: connecting two points above the parabola can dip below it.  </li> <li>\u21d2  is not convex.</li> </ol>"},{"location":"0ssc%20Epigraphs/#5-why-epigraphs-matter-in-optimization","title":"5. Why Epigraphs Matter in Optimization","text":"<p>Many optimization problems can be written in epigraph form:</p> <p> </p> <ul> <li>We \"lift\" the problem into one extra dimension.  </li> <li>The feasible region is the epigraph of .  </li> <li>Optimization over convex sets (epigraphs) is much more tractable.</li> </ul>"},{"location":"0ssc%20Epigraphs/#summary","title":"\u2705 Summary","text":"<ul> <li>The epigraph of a function is the region above its graph.  </li> <li>A function is convex iff its epigraph is convex.  </li> <li>Epigraphs let us reformulate optimization problems in a way that makes convexity clear and usable.  </li> </ul>"},{"location":"1a%20LP/","title":"LP","text":""},{"location":"1a%20LP/#linear-programming-lp-problem","title":"Linear Programming (LP) Problem","text":"<p>Linear Programming (LP) is a cornerstone of convex optimization. It is used to find a decision vector  that minimizes a linear objective function subject to linear constraints:</p> <p> </p> <p>Where: -  \u2014 decision variables, -  \u2014 cost vector, -  \u2014 constant offset (shifts objective but does not affect optimizer), -  \u2014 inequality constraints, -  \u2014 equality constraints.  </p>"},{"location":"1a%20LP/#why-lps-are-convex-optimization-problems","title":"Why LPs Are Convex Optimization Problems","text":"<p>A problem is convex if: 1. The objective is convex, 2. The feasible region is convex.  </p>"},{"location":"1a%20LP/#convexity-of-the-objective","title":"Convexity of the Objective","text":"<p>The LP objective is affine:</p> <p> </p> <ul> <li>Affine functions are both convex and concave (zero curvature).  </li> <li>Thus, no spurious local minima: every local optimum is global.  </li> </ul>"},{"location":"1a%20LP/#convexity-of-the-feasible-set","title":"Convexity of the Feasible Set","text":"<ul> <li>Each inequality  defines a half-space \u2014 convex.  </li> <li>Each equality  defines a hyperplane \u2014 convex.  </li> <li>The intersection of convex sets is convex.  </li> </ul> <p>Hence, the feasible region is a convex polyhedron.</p>"},{"location":"1a%20LP/#geometric-intuition","title":"\ud83d\udcd0 Geometric Intuition","text":"<ul> <li>Inequalities act like flat walls, keeping feasible points on one side.  </li> <li>Equalities act like flat sheets, slicing through space.  </li> <li>The feasible region is a polyhedron (possibly unbounded).  </li> <li>LP solutions always occur at a vertex (extreme point) of this polyhedron \u2014 this fact powers the simplex algorithm.  </li> </ul>"},{"location":"1a%20LP/#canonical-and-standard-forms","title":"Canonical and Standard Forms","text":"<p>LPs are often reformulated for theory and solvers:</p> <ul> <li>Canonical form (minimization):</li> </ul> <p> </p> <ul> <li>Standard form (maximization):</li> </ul> <p> </p> <p>Any LP can be transformed into one of these forms via slack variables and variable splitting.</p>"},{"location":"1a%20LP/#duality-in-linear-programming","title":"\u2696\ufe0f Duality in Linear Programming","text":"<p>Every LP has a dual problem:</p> <ul> <li>Primal (minimization):</li> </ul> <p> </p> <ul> <li>Dual:</li> </ul> <p> </p>"},{"location":"1a%20LP/#properties","title":"Properties:","text":"<ul> <li>Weak duality: Dual objective  Primal objective.  </li> <li>Strong duality: Holds under mild conditions (Slater\u2019s condition).  </li> <li>Complementary slackness provides optimality certificates.  </li> </ul> <p>Duality underpins modern algorithms like interior-point methods.</p>"},{"location":"1a%20LP/#robust-linear-programming-rlp","title":"Robust Linear Programming (RLP)","text":"<p>In many applications, the LP data () are uncertain due to noise, estimation errors, or worst-case planning requirements. Robust Optimization handles this by requiring constraints to hold for all possible realizations of the uncertain parameters within a given uncertainty set.</p>"},{"location":"1a%20LP/#general-robust-lp-formulation","title":"General Robust LP Formulation","text":"<p>Consider an uncertain LP:</p> <p> </p> <ul> <li> : uncertainty set (polyhedron, ellipsoid, box, etc.)  </li> <li> : uncertain parameters affecting .  </li> </ul> <p>The robust counterpart requires feasibility for all .</p>"},{"location":"1a%20LP/#box-uncertainty-interval-uncertainty","title":"Box Uncertainty (Interval Uncertainty)","text":"<p>Suppose , with each row uncertain in a box set:</p> <p> </p> <p>Robust constraint:</p> <p> </p> <p>This is equivalent to:</p> <p> </p> <p>Thus, a robust LP under box uncertainty is still a deterministic convex program (LP with additional  terms).</p>"},{"location":"1a%20LP/#ellipsoidal-uncertainty","title":"Ellipsoidal Uncertainty","text":"<p>If uncertainty lies in an ellipsoid:</p> <p> </p> <p>then the robust counterpart becomes:</p> <p> </p> <p>This is a Second-Order Cone Program (SOCP), still convex but more general than LP.</p>"},{"location":"1a%20LP/#robust-objective","title":"Robust Objective","text":"<p>When cost vector  is uncertain in :</p> <p> </p> <ul> <li>If  is a box: inner max =  </li> <li>If  is ellipsoidal: inner max =  </li> </ul> <p>Thus, robust objectives often introduce regularization-like terms.  </p>"},{"location":"1a%20LP/#applications-of-robust-lp","title":"Applications of Robust LP","text":"<ul> <li>Supply chain optimization: demand uncertainty \u2192 robust inventory policies.  </li> <li>Finance: portfolio selection under uncertain returns.  </li> <li>Energy systems: robust scheduling under uncertain loads.  </li> <li>AI/ML: adversarial optimization, distributionally robust ML training.  </li> </ul>"},{"location":"1a%20LP/#how-lp-scales-in-practice","title":"How LP Scales in Practice","text":""},{"location":"1a%20LP/#polynomial-time-solvability","title":"Polynomial-Time Solvability","text":"<ul> <li>LPs can be solved in polynomial time using Interior-Point Methods (IPMs).  </li> <li>For an LP with  variables and  constraints, classical IPM complexity is roughly:</li> </ul> <ul> <li>But real-world performance depends on sparsity and problem structure. Sparse LPs are often solved in nearly linear time with specialized solvers.</li> </ul>"},{"location":"1a%20LP/#solver-ecosystem","title":"Solver Ecosystem","text":"<ul> <li>Commercial solvers: Gurobi, CPLEX, Mosek \u2014 highly optimized, exploit sparsity and parallelism, support warm-starts. These dominate large-scale industrial and financial problems.  </li> <li>Open-source solvers: HiGHS, GLPK, SCIP \u2014 robust for moderate problems, widely integrated into Python/Julia (via PuLP, Pyomo, CVXPY).  </li> <li>ML integration: CVXPY and PyTorch integrations make LP-based optimization easily callable inside ML pipelines.  </li> </ul>"},{"location":"1a%20LP/#algorithmic-tradeoffs","title":"Algorithmic Tradeoffs","text":"<ul> <li>Simplex method: moves along vertices of the feasible polyhedron.  </li> <li>Often very fast in practice, though exponential in theory.  </li> <li>Warm-starts make it excellent for iterative ML problems.  </li> <li>Interior-Point Methods (IPMs): follow a central path through the feasible region.  </li> <li>Polynomial worst-case guarantees.  </li> <li>Very robust to degeneracy, well-suited to dense problems.  </li> <li>First-order and decomposition methods:  </li> <li>ADMM, primal-dual splitting, stochastic coordinate descent.  </li> <li>Scale to massive LPs with billions of variables.  </li> <li>Sacrifice exactness for approximate but usable solutions.  </li> </ul>"},{"location":"1a%20LP/#comparison-of-lp-solvers","title":"Comparison of LP Solvers","text":"Method Complexity (theory) Scaling in practice Strengths Weaknesses ML/Engineering Use Cases Simplex Worst-case exponential Very fast in practice (near-linear for sparse LPs) Supports warm-starts, excellent for re-solving May stall on degenerate problems Iterative ML models, resource allocation, network flow Interior-Point (IPM) Handles millions of variables if sparse Polynomial guarantees, robust, finds central solutions Memory-heavy (factorization of large matrices) Large dense LPs, convex relaxations in ML, finance First-order methods Sublinear (per iteration) Scales to billions of variables Memory-efficient, parallelizable Only approximate solutions MAP inference in CRFs, structured SVMs, massive embeddings Decomposition methods Problem-dependent Linear or near-linear scaling when structure exploited Breaks huge problems into smaller ones Requires separable structure Supply chain optimization, distributed training, scheduling"},{"location":"1a%20LP/#solving-large-scale-lps-in-ml-and-engineering","title":"Solving Large-Scale LPs in ML and Engineering","text":"<p>When problem sizes explode (e.g.,  variables in embeddings or large-scale resource scheduling), standard solvers may fail due to memory or time.</p>"},{"location":"1a%20LP/#strategies","title":"Strategies","text":"<ul> <li>Decomposition methods:  </li> <li>Dantzig\u2013Wolfe, Benders, Lagrangian relaxation break problems into subproblems solved iteratively.  </li> <li>Column generation:  </li> <li>Introduces only a subset of variables initially, generating new ones as needed.  </li> <li>Stochastic and online optimization:  </li> <li>Replaces full LP solves with SGD-like updates, used in ML training pipelines.  </li> <li>Approximate relaxations:  </li> <li>In structured ML, approximate LP solutions often suffice (e.g., in structured prediction tasks).  </li> </ul>"},{"location":"1a%20LP/#ml-perspective","title":"ML Perspective","text":"<ul> <li>Structured prediction: LP relaxations approximate inference in CRFs, structured SVMs.  </li> <li>Adversarial robustness: Worst-case perturbation problems often reduce to LP relaxations, especially under  constraints.  </li> <li>Fairness: Linear constraints encode fairness requirements inside risk minimization objectives.  </li> <li>Large-scale systems: Recommender systems, resource allocation, energy scheduling \u2192 decomposition + approximate LP solvers.  </li> </ul>"},{"location":"1a%20LP/#where-lp-struggles-failure-modes","title":"Where LP Struggles (Failure Modes)","text":"<p>Despite its power, LPs face limitations:</p> <ol> <li>Nonlinearities </li> <li>Many ML objectives (e.g., log-likelihood, quadratic loss) are nonlinear.  </li> <li> <p>LP relaxations may be loose, requiring QP, SOCP, or nonlinear solvers.  </p> </li> <li> <p>Integrality </p> </li> <li>LP cannot enforce discrete decisions.  </li> <li> <p>Mixed-Integer Linear Programs (MILPs) are NP-hard, limiting scalability.  </p> </li> <li> <p>Uncertainty </p> </li> <li>Classical LP assumes perfect knowledge of data.  </li> <li> <p>Real problems require Robust LP or Stochastic LP.  </p> </li> <li> <p>Numerical conditioning </p> </li> <li>Poorly scaled coefficients lead to solver instability.  </li> <li> <p>Always normalize inputs for ML-scale LPs.  </p> </li> <li> <p>Memory bottlenecks </p> </li> <li>IPMs require factorizing large dense matrices \u2014 infeasible for extremely large-scale ML problems.  </li> </ol>"},{"location":"1b%20QP/","title":"QP","text":""},{"location":"1b%20QP/#quadratic-programming-qp-problem","title":"Quadratic Programming (QP) Problem","text":"<p>Quadratic Programming (QP) is an optimization framework that generalizes Linear Programming by allowing a quadratic objective function, while keeping the constraints linear. Formally, the problem is:</p> <p> </p> <p>Where:</p> <ul> <li>  \u2014 the decision vector to be optimized.  </li> <li>  \u2014 the Hessian matrix defining the curvature of the objective.  </li> <li>  \u2014 the linear cost term.  </li> <li>  \u2014 a constant offset (does not affect the minimizer\u2019s location).  </li> <li> ,  \u2014 inequality constraints .  </li> <li> ,  \u2014 equality constraints .  </li> </ul>"},{"location":"1b%20QP/#why-qps-can-be-convex-optimization-problems","title":"Why QPs Can Be Convex Optimization Problems","text":"<p>Whether a QP is convex depends on one key condition:</p> <ul> <li>Convex QP: The Hessian  is positive semidefinite ().  </li> <li>Nonconvex QP: The Hessian has negative eigenvalues (some directions curve downward).</li> </ul>"},{"location":"1b%20QP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QP objective is:</p> <p> </p> <p>This is a quadratic function, which is:</p> <ul> <li>Convex if  (all curvature is flat or bowl-shaped).</li> <li>Strictly convex if  (curvature is strictly bowl-shaped, ensuring a unique minimizer).</li> <li>Nonconvex if  has negative eigenvalues (some directions slope downward).</li> </ul> <p>The gradient and Hessian are:</p> <p> </p> <p>Since the Hessian is constant in QPs, checking convexity is straightforward:  </p> <p>Positive semidefinite Hessian \u2192 Convex objective \u2192 No spurious local minima.</p>"},{"location":"1b%20QP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>Exactly as in LPs:</p> <ul> <li>Each inequality  is a half-space (convex).  </li> <li>Each equality  is a hyperplane (convex).  </li> </ul> <p>The feasible set:</p> <p> </p> <p>is the intersection of convex sets, hence convex.</p>"},{"location":"1b%20QP/#the-feasible-set-is-a-convex-polyhedron","title":"The Feasible Set is a Convex Polyhedron","text":"<p>For convex QPs:</p> <ul> <li>The feasible region  is still a convex polyhedron (because constraints are the same as in LPs).  </li> <li>The objective is a convex quadratic \"bowl\" sitting over that polyhedron.  </li> <li>The optimal solution is where the bowl\u2019s lowest point touches the feasible polyhedron.</li> </ul>"},{"location":"1b%20QP/#geometric-intuition-visualizing-qp","title":"Geometric Intuition: Visualizing QP","text":"<ul> <li>In LP, the objective is a flat plane sliding over a polyhedron.  </li> <li>In convex QP, the objective is a curved bowl sliding over the same polyhedron.  </li> <li>If the bowl\u2019s center lies inside the feasible region, the optimum is at that center.  </li> <li>If not, the bowl \u201cleans\u201d against the polyhedron\u2019s faces, edges, or vertices \u2014 which is where the optimal solution lies.</li> </ul> <p>\u2705 Summary: A QP is a convex optimization problem if and only if . In that case:</p> <ul> <li>Objective: Convex quadratic.  </li> <li>Constraints: Linear, hence convex.  </li> <li>Feasible set: Convex polyhedron.  </li> <li>Solution: Found at the point in the feasible set where the quadratic surface reaches its lowest value.</li> </ul>"},{"location":"1c%20Least%20Square/","title":"Least Squares","text":""},{"location":"1c%20Least%20Square/#least-squares-ls-problem","title":"\ud83d\udd39 Least Squares (LS) Problem","text":"<p>Least Squares (LS) is one of the canonical convex optimization problems in statistics, machine learning, and signal processing. It seeks the vector  that minimizes the sum of squared errors:</p> <p> </p> <p>Where: -  \u2014 data or measurement matrix, -  \u2014 observation or target vector, -  \u2014 decision vector (unknowns to estimate).  </p>"},{"location":"1c%20Least%20Square/#objective-expansion","title":"Objective Expansion","text":"<p>Expanding the squared norm:</p> <p> </p> <p>This is a quadratic convex function. In standard quadratic form:</p> <p> </p> <p>with</p> <p> </p>"},{"location":"1c%20Least%20Square/#convexity","title":"\u2705 Convexity","text":"<ul> <li>  since for any :  </li> </ul> <ul> <li>Hence LS is convex.  </li> <li>If  has full column rank, then , making the problem strictly convex with a unique minimizer.  </li> </ul>"},{"location":"1c%20Least%20Square/#geometric-intuition","title":"\ud83d\udcd0 Geometric Intuition","text":"<ul> <li>When  (overdetermined system), the equations  may not be solvable.  </li> <li>LS finds  such that  is the orthogonal projection of  onto the column space of .  </li> <li>The residual  is orthogonal to :</li> </ul>"},{"location":"1c%20Least%20Square/#solutions","title":"\ud83e\uddee Solutions","text":"<ul> <li>Normal Equations (full-rank case):</li> </ul> <ul> <li>General Case (possibly rank-deficient): The solution set is affine. The minimum-norm solution is given by the Moore\u2013Penrose pseudoinverse:</li> </ul> <ul> <li>Numerical Considerations: Normal equations can be ill-conditioned. In practice:</li> <li>Use QR decomposition or  </li> <li>SVD (stable, gives pseudoinverse).  </li> </ul>"},{"location":"1c%20Least%20Square/#constrained-least-squares-cls","title":"\ud83d\udd12 Constrained Least Squares (CLS)","text":"<p>Many practical problems require constraints on the solution. A general CLS formulation is:</p> <p> </p> <ul> <li>Objective: convex quadratic.  </li> <li>Constraints: linear.  </li> <li>Therefore: CLS is always a Quadratic Program (QP).</li> </ul>"},{"location":"1c%20Least%20Square/#example-1-wear-and-tear-allocation-cls-with-inequalities","title":"Example 1: Wear-and-Tear Allocation (CLS with Inequalities)","text":"<p>Suppose a landlord models annual apartment wear-and-tear costs as:</p> <p> </p> <p>with parameters .  </p> <p>CLS formulation:</p> <p> </p> <p>Constraints (practical feasibility):</p> <ul> <li>Costs cannot be negative: </li> </ul> <p>This yields a CLS problem with linear inequality constraints, hence a QP.  </p>"},{"location":"1c%20Least%20Square/#example-2-energy-consumption-fitting-cls-with-box-constraints","title":"\ud83d\udca1 Example 2: Energy Consumption Fitting (CLS with Box Constraints)","text":"<p>Suppose we fit energy usage from appliance data:  </p> <ul> <li>  usage matrix,  </li> <li>  observed energy bills.  </li> </ul> <p>CLS formulation:</p> <p> </p> <p>Constraints: each appliance has a usage cap:  </p> <p> </p> <p>This is a QP with box constraints, often solved efficiently by projected gradient or interior-point methods.  </p>"},{"location":"1c%20Least%20Square/#regularized-least-squares-ridge-regression","title":"\ud83d\udd27 Regularized Least Squares (Ridge Regression)","text":"<p>A common extension in ML is regularized LS, e.g., ridge regression:</p> <p> </p> <ul> <li>Equivalent to CLS with a quadratic penalty on .  </li> <li>Ensures uniqueness even if  is rank-deficient.  </li> <li>Solution:</li> </ul> <p> </p>"},{"location":"1c%20Least%20Square/#summary","title":"\ud83d\udcca Summary","text":"<ul> <li>Unconstrained LS: convex quadratic, closed form via pseudoinverse.  </li> <li>CLS: convex quadratic + linear constraints \u2192 QP.  </li> <li>Regularized LS: stabilizes solution, improves generalization.  </li> <li>Geometry: LS = orthogonal projection; CLS = projection with constraints.  </li> <li>Solvers: </li> <li>Small problems: QR/SVD (LS) or active-set (CLS).  </li> <li>Large problems: iterative methods (CG, projected gradient).  </li> </ul>"},{"location":"1d%20QCQP/","title":"QCQP","text":""},{"location":"1d%20QCQP/#quadratically-constrained-quadratic-programming-qcqp-problem","title":"Quadratically Constrained Quadratic Programming (QCQP) Problem","text":"<p>A Quadratically Constrained Quadratic Program (QCQP) is an optimization problem in which both the objective and the constraints can be quadratic functions. Formally, the problem is:</p> <p> </p> <p>Where:</p> <ul> <li>  \u2014 the decision vector.  </li> <li>  \u2014 symmetric matrices defining curvature of the objective and constraints.  </li> <li>  \u2014 linear terms in the objective and constraints.  </li> <li>  \u2014 constant offsets.  </li> <li> ,  \u2014 equality constraints (linear).  </li> </ul>"},{"location":"1d%20QCQP/#why-qcqps-can-be-convex-optimization-problems","title":"Why QCQPs Can Be Convex Optimization Problems","text":"<p>QCQPs are not automatically convex \u2014 convexity requires specific conditions:</p> <ol> <li> <p>Objective convexity:  (positive semidefinite Hessian for the objective).</p> </li> <li> <p>Constraint convexity:    For each inequality constraint ,  so that  defines a convex set.</p> </li> <li> <p>Equality constraints:    Must be affine (linear), e.g., .</p> </li> </ol>"},{"location":"1d%20QCQP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QCQP objective:</p> <p> </p> <p>is convex iff .</p>"},{"location":"1d%20QCQP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>A single quadratic constraint:</p> <p> </p> <p>defines a convex feasible set iff .  </p> <p>If any  is not positive semidefinite, the constraint set becomes nonconvex, and the overall problem is nonconvex.</p>"},{"location":"1d%20QCQP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>If all , inequality constraints define convex quadratic regions (ellipsoids, elliptic cylinders, or half-spaces).</li> <li>Equality constraints  cut flat slices through these regions.</li> <li>The feasible set is the intersection of convex quadratic sets and affine sets \u2014 hence convex.</li> </ul>"},{"location":"1d%20QCQP/#geometric-intuition-visualizing-qcqp","title":"Geometric Intuition: Visualizing QCQP","text":"<ul> <li>In QP, only the objective is curved; constraints are flat.  </li> <li>In QCQP, constraints can also be curved \u2014 forming shapes like ellipsoids or paraboloids.  </li> <li>Convex QCQPs look like a \u201cbowl\u201d objective contained within (or pressed against) curved convex walls.  </li> <li>Nonconvex QCQPs can have holes or disconnected regions, making them much harder to solve.</li> </ul> <p>\u2705 Summary: A QCQP is a convex optimization problem if and only if:</p> <ul> <li>  (objective convexity), and  </li> <li>  for all  (each quadratic inequality constraint convex), and  </li> <li>All equality constraints are affine.  </li> </ul> <p>When these hold: - Objective: Convex quadratic. - Constraints: Convex quadratic or affine. - Feasible set: Intersection of convex sets (can be curved). - Solution: Found where the objective\u2019s minimum touches the convex feasible region.</p>"},{"location":"1e%20SOCP/","title":"SOCP","text":""},{"location":"1e%20SOCP/#second-order-cone-programming-socp-problem","title":"Second-Order Cone Programming (SOCP) Problem","text":"<p>Second-Order Cone Programming (SOCP) is a class of convex optimization problems that generalizes Linear and (certain) Quadratic Programs by allowing constraints involving second-order (quadratic) cones.  </p> <p>Formally, the problem is:</p> <p> </p> <p>Where:</p> <ul> <li>  \u2014 the decision vector.  </li> <li>  \u2014 the linear objective coefficients.  </li> <li> ,  \u2014 define the affine transformation inside the norm for cone .  </li> <li> ,  \u2014 define the affine term on the right-hand side.  </li> <li> ,  \u2014 define linear equality constraints.</li> </ul>"},{"location":"1e%20SOCP/#the-second-order-quadratic-cone","title":"The Second-Order (Quadratic) Cone","text":"<p>A second-order cone in  is:</p> <p> </p> <p>Key properties: - Convex set. - Rotationally symmetric around the -axis. - Contains all rays pointing \u201cupward\u201d inside the cone.</p>"},{"location":"1e%20SOCP/#why-socp-is-a-convex-optimization-problem","title":"Why SOCP is a Convex Optimization Problem","text":""},{"location":"1e%20SOCP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<ul> <li>The SOCP objective  is affine.</li> <li>Affine functions are both convex and concave \u2014 no curvature.</li> </ul>"},{"location":"1e%20SOCP/#2-convexity-of-the-constraints","title":"2. Convexity of the Constraints","text":"<p>Each second-order cone constraint:</p> <p> </p> <p>is convex because: - The left-hand side  is a convex function of . - The right-hand side  is affine. - The set  is a convex set.</p> <p>Equality constraints  define a hyperplane, which is convex.</p> <p>Since the feasible region is the intersection of convex sets, it is convex.</p>"},{"location":"1e%20SOCP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>Each SOCP constraint defines a rotated or shifted cone in -space.</li> <li>Equality constraints slice the space with flat hyperplanes.</li> <li>The feasible set is the intersection of these cones and hyperplanes.</li> </ul>"},{"location":"1e%20SOCP/#special-cases-of-socp","title":"Special Cases of SOCP","text":"<ul> <li>Linear Programs (LP): If all , cone constraints reduce to linear inequalities.</li> <li>Certain Quadratic Programs (QP): Quadratic inequalities of the form  can be rewritten as SOCP constraints.</li> <li>Norm Constraints: Bounds on -norms (e.g., ) are directly SOCP constraints.</li> </ul>"},{"location":"1e%20SOCP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>In LP, constraints are flat walls.</li> <li>In QP, objective is curved but constraints are flat.</li> <li>In SOCP, constraints themselves are curved (cone-shaped), allowing more modeling flexibility.</li> <li>The optimal solution is where the objective plane just \u201ctouches\u201d the feasible cone-shaped region.</li> </ul> <p>\u2705 Summary: - Objective: Affine (linear) \u2192 convex. - Constraints: Intersection of affine equalities and convex second-order cone inequalities. - Feasible set: Convex \u2014 shaped by cones and hyperplanes. - Power: Captures LPs, norm minimization, robust optimization, and some QCQPs. - Solution: Found efficiently by interior-point methods specialized for conic programming.</p>"},{"location":"1f%20GeometricInterpretation/","title":"Geometric Interpretation","text":"Feature LP QP QCQP SOCP Objective Linear:  Quadratic: , convex if , indefinite  \u2192 nonconvex Quadratic: , convex if , indefinite \u2192 nonconvex Linear: , always convex Objective level sets Hyperplanes (flat, parallel) Quadrics: ellipsoids/paraboloids if , hyperboloids if indefinite Quadrics: ellipsoids/paraboloids; shape depends on  definiteness Hyperplanes (flat, parallel) Constraints Linear:  \u2192 half-spaces, flat Linear:  \u2192 half-spaces, flat Quadratic:  \u2192 curved surfaces; convex if , otherwise possibly nonconvex Second-order cone:  \u2192 convex, curved conic surfaces Feasible region Polyhedron (flat faces, convex) Polyhedron (flat faces, convex) Curved region; convex if all , otherwise possibly nonconvex Intersection of convex cones; curved, convex region Optimum location Vertex (extreme point of polyhedron) Face, edge, vertex, or interior if unconstrained minimizer feasible Boundary or interior; multiple local minima possible if nonconvex Boundary or interior; linear objective touches cone tangentially 2D Example Max , s.t. , ,  \u2192 polygon Min , s.t. , ,  \u2192 polygon feasible, elliptical contours Min , s.t. ,  \u2192 circular + linear \u2192 curved feasible Min , s.t.  \u2192 tilted cone 3D Example Max , s.t. ,  \u2192 polyhedron Min , s.t. ,  \u2192 polyhedron + ellipsoid Min , s.t. ,  \u2192 spherical + plane \u2192 curved feasible Min , s.t.  \u2192 3D cone 4D Intuition Polytope + 3D hyperplane Polytope + 4D ellipsoid Curved 4D region + 4D ellipsoid; convex if  4D cone + 3D hyperplane Curvature Hint Flat objective / flat constraints Curved objective / flat constraints Curved objective / curved constraints Flat objective / curved constraints"},{"location":"1g%20GP/","title":"GP","text":""},{"location":"1g%20GP/#geometric-programming-gp","title":"\ud83d\udcd8 Geometric Programming (GP)","text":"<p>Geometric Programming (GP) is a flexible, widely used optimization class (communications, circuit design, resource allocation, control, ML model fitting). In its natural variable form it looks nonconvex, but \u2014 crucially \u2014 there is a canonical change of variables and a monotone transformation that converts a GP into a convex optimization problem.</p>"},{"location":"1g%20GP/#definitions-monomials-posynomials-and-the-standard-gp","title":"Definitions: monomials, posynomials, and the standard GP","text":"<p>Let  with .</p> <ul> <li> <p>A monomial (in GP terminology) is a function of the form  where  and the exponents  (real exponents allowed).  </p> <p>Note: in GP literature \"monomial\" means positive coefficient times a power product (not to be confused with polynomial monomial which has nonnegative integer powers).</p> </li> <li> <p>A posynomial is a sum of monomials:  </p> </li> <li> <p>Standard (inequality) form of a geometric program:  where each  is a posynomial and each  is a monomial. (Any GP with other RHS values can be normalized to this form by dividing.)</p> </li> </ul>"},{"location":"1g%20GP/#why-gp-in-the-original-x-variables-is-not-convex","title":"\u2757 Why GP (in the original  variables) is not convex","text":"<ul> <li> <p>A monomial  (single variable) is convex on  only for certain exponent ranges (e.g.  or ). For  it is concave; for general real  it can be neither globally convex nor concave over .   Example:  () is concave on  (second derivative ? \u2014 check sign; in fact  showing concavity).</p> </li> <li> <p>A posynomial is a sum of monomials. Sums of nonconvex (or concave) terms are generally nonconvex. There is no general convexity guarantee for posynomials in the original variables .</p> </li> <li> <p>Therefore the objective  and constraints  are not convex functions/constraints in , so the GP is not a convex program in the -space.</p> </li> </ul> <p>Concrete counterexample (1D): take . The term  is concave on ,  is convex, and the sum is neither convex nor concave. One can find points  and  that violate the convexity inequality.</p>"},{"location":"1g%20GP/#how-to-make-gp-convex-the-log-change-of-variables-and-log-transformation","title":"\u2705 How to make GP convex: the log-change of variables and log transformation","text":"<p>Key facts that enable convexification:</p> <ul> <li> <p>Monomials become exponentials of affine functions in log-variables.   Define  (so ) and write . For a monomial      we have      which is affine in .</p> </li> <li> <p>Posynomials become sums of exponentials of affine functions. For a posynomial      where  is the exponent-vector for the th monomial and .</p> </li> <li> <p>Taking the log of a posynomial yields a log-sum-exp function, i.e.      where .</p> </li> <li> <p>The log-sum-exp function is convex. Hence constraints of the form  become      i.e. a convex constraint in  because  is convex.</p> </li> <li> <p>Since  is monotone, minimizing  is equivalent to minimizing . Therefore one may transform the GP to the equivalent convex program in :</p> </li> </ul> <p> </p> <p>This -problem is convex: log-sum-exp objective/constraints are convex; monomial equalities are affine in .</p>"},{"location":"1g%20GP/#why-log-sum-exp-is-convex-brief-proof-via-hessian","title":"\ud83d\udd0d Why log-sum-exp is convex (brief proof via Hessian)","text":"<p>Let  with  (affine). Define  - Gradient:  - Hessian:  where . The Hessian is a weighted covariance matrix of the vectors  (weights ), hence PSD. Thus  is convex.</p>"},{"location":"1g%20GP/#monomials-as-affine-constraints-in-y","title":"\u2733\ufe0f Monomials as affine constraints in","text":"<p>A monomial equality  becomes  an affine equality in . So monomial equality constraints become linear equalities after the log change.</p>"},{"location":"1g%20GP/#equivalence-and-solving-workflow","title":"\ud83d\udd01 Equivalence and solving workflow","text":"<ol> <li>Start with GP in : minimize  subject to posynomial constraints and monomial equalities.  </li> <li>Change variables:  (domain becomes all ).  </li> <li>Apply log to posynomials (objective + inequality LHS). Because  is monotone increasing, inequalities maintain direction.  </li> <li>Solve the convex problem in  (log-sum-exp objective, convex constraints). Use interior-point or other convex solvers.  </li> <li>Recover .</li> </ol> <p>Because  is a bijection for , solutions correspond exactly.</p>"},{"location":"1g%20GP/#worked-out-simple-example-2-variables","title":"\ud83d\udd27 Worked-out simple example (2 variables)","text":"<p>Original GP (standard form): </p> <p>Change variables: .</p> <ul> <li>Transform terms:</li> <li>  with  term .</li> <li>  with  term .</li> <li>Constraint posynomial: .</li> </ul> <p>Convex form (in ): </p> <p>Both objective and constraint are log-sum-exp functions (convex). Solve for  with a convex solver; then .</p>"},{"location":"1g%20GP/#numerical-implementation-remarks","title":"\u2699\ufe0f Numerical &amp; implementation remarks","text":"<ul> <li> <p>Domain requirement: GP requires . The log transform only works on the positive orthant. If some variables can be zero, model reformulation (introducing small positive lower bounds) may be necessary.</p> </li> <li> <p>Normalization: Standard GPs use constraints . If you have , divide by  to normalize.</p> </li> <li> <p>Numerical stability: Use the stable log-sum-exp implementation:      to avoid overflow/underflow.</p> </li> <li> <p>Solvers: After convexification the problem can be passed to generic convex solvers (CVX, CVXOPT, MOSEK, SCS, ECOS). Many solvers accept the log-sum-exp cone directly. Interior-point methods are effective on moderate-size GPs.</p> </li> <li> <p>Interpretation: The convexified problem is not an LP; it is a convex program with log-sum-exp terms (equivalently representable using exponential/relative entropy cones or by second-order cone approximations in some cases).</p> </li> </ul>"},{"location":"1g%20GP/#limitations-and-extensions","title":"\ud83d\udeab Limitations and extensions","text":"<ul> <li> <p>Signomials: If the problem contains negative coefficients (e.g. sums of monomials with arbitrary signs), it is a signomial program and the log transform does not yield a convex problem. Signomial programs are generally nonconvex and require local optimization or sequential convex approximations (e.g., successive convex approximation / condensation, branch-and-bound heuristics).</p> </li> <li> <p>Robust GP: Uncertainty in coefficients  or exponents  can sometimes be handled by robustification. If uncertainty is modeled multiplicatively (log-convex uncertainty sets), the robust counterpart often remains convex in ; specifics depend on the uncertainty set shape (box \u2192 additional terms, ellipsoidal \u2192 conic forms). Robust GP is a rich topic \u2014 it frequently yields convex reformulations after log-transform if the uncertainty preserves log-convexity.</p> </li> <li> <p>Sparsity/exponents: Exponents  need not be integers \u2014 positive real exponents are allowed. Sparsity in exponent matrix () often leads to efficient implementations.</p> </li> </ul>"},{"location":"1g%20GP/#takeaway-summary-concise","title":"\u2705 Takeaway / Summary (concise)","text":"<ul> <li>GP in  is typically nonconvex because posynomials are not convex in .  </li> <li>Key transformation: , and take  of posynomials \u2192 posynomial  log-sum-exp of affine functions in .  </li> <li>Result: The GP becomes a convex optimization problem in : log-sum-exp objective/constraints (convex), and monomial equalities become linear equalities.  </li> <li>Solve in , then exponentiate to get .  </li> <li>Caveats: requires ; signomials (negative coefficients) remain nonconvex and need other techniques.</li> </ul> <p>If you want, I can: - provide a numerical, step-by-step example solving the worked GP above (with numbers) and show the solver output and ; - show how robust multiplicative uncertainty in coefficients is handled in log domain; or - give a short code snippet (CVX/MOSEK or CVXPY) that constructs and solves the convexified GP.</p> <p>Which of those would you like next?</p>"},{"location":"2a%20Duality/","title":"Duality","text":""},{"location":"2a%20Duality/#duality-theory-kkt-conditions-and-duality-gap","title":"Duality Theory, KKT Conditions, and Duality Gap","text":"<p>Duality theory is a central tool in optimization and machine learning. It provides alternative perspectives on problems, certificates of optimality, and insights into algorithm design. Applications include Support Vector Machines (SVMs), Lasso, and ridge regression.</p>"},{"location":"2a%20Duality/#1-convex-optimization-problem","title":"1. Convex Optimization Problem","text":"<p>Consider the standard convex optimization problem:</p> <p> </p> <ul> <li> : objective function.  </li> <li> : convex inequality constraints.  </li> <li> : affine equality constraints.  </li> </ul> <p>Feasible set:</p> <p> </p>"},{"location":"2a%20Duality/#2-lagrangian-function","title":"2. Lagrangian Function","text":"<p>The Lagrangian incorporates constraints into the objective:</p> <p> </p> <ul> <li> : dual variables for inequalities.  </li> <li> : dual variables for equalities.  </li> </ul> <p>Intuition:  represents the \u201cprice\u201d of violating constraint . Larger  penalizes violations more.</p>"},{"location":"2a%20Duality/#3-dual-function-and-infimum","title":"3. Dual Function and Infimum","text":"<p>The dual function is:</p> <p> </p>"},{"location":"2a%20Duality/#31-infimum","title":"3.1 Infimum","text":"<ul> <li>The infimum (inf) of a function is its greatest lower bound: the largest number that is less than or equal to all function values.  </li> <li>Formally, for :</li> </ul> <ul> <li>Intuition: </li> <li>If  has a minimum, the infimum equals the minimum.  </li> <li>If no minimum exists, the infimum is the value approached but never reached.</li> </ul> <p>Examples:</p> <ol> <li> ,  \u2192  at .  </li> <li> ,  \u2192 , never attained.  </li> </ol>"},{"location":"2a%20Duality/#32-supremum","title":"3.2 Supremum","text":"<ul> <li>The supremum (sup) of a set  is the least upper bound: the smallest number greater than or equal to all elements of .</li> </ul> <p>Example:  \u2192 , although no maximum exists.</p>"},{"location":"2a%20Duality/#33-why-the-dual-function-provides-a-lower-bound","title":"3.3 Why the Dual Function Provides a Lower Bound","text":"<p>For any feasible  and , :</p> <p> </p> <p>Taking the infimum over all :</p> <p> </p> <p>Thus, for any dual variables:</p> <p> </p> <ul> <li>Interpretation: The dual function is always a lower bound on the primal optimum.  </li> <li>Geometric intuition: Think of the dual as the highest \u201cfloor\u201d under the primal objective that supports the feasible region.  </li> </ul>"},{"location":"2a%20Duality/#4-the-dual-problem","title":"4. The Dual Problem","text":"<p>The dual problem seeks the tightest lower bound:</p> <p> </p> <ul> <li>Dual optimal value: .  </li> <li>Always satisfies weak duality: .  </li> <li>If convexity + Slater's condition hold, strong duality: .</li> </ul>"},{"location":"2a%20Duality/#5-duality-gap","title":"5. Duality Gap","text":"<p>The duality gap measures the difference between primal and dual optima:</p> <p> </p> <ul> <li>Zero gap: strong duality (common in convex ML problems).  </li> <li>Positive gap: weak duality only; dual provides only a lower bound.  </li> </ul>"},{"location":"2a%20Duality/#51-causes-of-positive-gap","title":"5.1 Causes of Positive Gap","text":"<ol> <li>Nonconvex objective.  </li> <li>Constraint qualification fails (e.g., Slater\u2019s condition not satisfied).  </li> <li>Dual problem is infeasible or unbounded.</li> </ol>"},{"location":"2a%20Duality/#52-example","title":"5.2 Example","text":"<p>Primal problem:</p> <p> </p> <ul> <li>Primal optimum:  at  </li> <li>Dual problem:  (unbounded below)  </li> <li>Gap:  \u2192 positive duality gap.</li> </ul> <p>Interpretation: dual gives a guaranteed lower bound but may not achieve the primal optimum.</p>"},{"location":"2a%20Duality/#6-karushkuhntucker-kkt-conditions","title":"6. Karush\u2013Kuhn\u2013Tucker (KKT) Conditions","text":"<p>For convex problems with strong duality, KKT conditions fully characterize optimality.</p> <p>Let  be primal optimal and  dual optimal:</p> <ol> <li> <p>Primal feasibility: </p> </li> <li> <p>Dual feasibility: </p> </li> <li> <p>Stationarity: </p> </li> <li> <p>Use subgradients for nonsmooth problems (e.g., Lasso).  </p> </li> <li> <p>Complementary slackness: </p> </li> </ol> <p>Intuition: Only active constraints contribute; the \u201cforces\u201d of objective and constraints balance.</p>"},{"location":"2a%20Duality/#7-applications-in-machine-learning","title":"7. Applications in Machine Learning","text":""},{"location":"2a%20Duality/#71-ridge-regression","title":"7.1 Ridge Regression","text":"<ul> <li>Smooth shrinkage, unique solution.  </li> <li>Dual view useful in kernelized ridge regression.</li> </ul>"},{"location":"2a%20Duality/#72-lasso-regression","title":"7.2 Lasso Regression","text":"<ul> <li>KKT conditions explain sparsity:</li> </ul> <ul> <li>Basis for coordinate descent and soft-thresholding algorithms.</li> </ul>"},{"location":"2a%20Duality/#73-support-vector-machines-svms","title":"7.3 Support Vector Machines (SVMs)","text":"<ul> <li>Dual depends only on inner products , enabling kernel methods.  </li> <li>Often more efficient if number of features  exceeds number of data points .</li> </ul>"},{"location":"2a%20Duality/#8-constrained-vs-penalized-optimization","title":"8. Constrained vs. Penalized Optimization","text":"<ul> <li>Constrained form:</li> </ul> <ul> <li>Penalized form:</li> </ul> <ul> <li>Lagrange multiplier  acts as a \u201cprice\u201d on the constraint.  </li> <li>Equivalence holds for convex problems, but mapping  may be non-unique.</li> </ul>"},{"location":"2a%20Duality/#9-summary","title":"9. Summary","text":"<ol> <li>Infimum and supremum: greatest lower bound and least upper bound.  </li> <li>Dual function:  always provides a lower bound.  </li> <li>Duality gap: , zero under strong duality, positive when dual does not attain primal optimum.  </li> <li>KKT conditions: necessary and sufficient for convex problems with strong duality.  </li> <li>ML connections: Ridge, Lasso, and SVM exploit duality for computation, sparsity, and kernelization.</li> </ol> <p>Key intuition: The dual function can be visualized as the highest supporting \u201cfloor\u201d under the primal objective. Maximizing it gives the tightest lower bound, and when strong duality holds, it meets the primal optimum exactly.</p>"},{"location":"3a%20Huber/","title":"Huber","text":""},{"location":"3a%20Huber/#huber-penalty-loss","title":"Huber Penalty Loss","text":"<p>The Huber loss is a robust loss function that combines the advantages of squared loss and absolute loss, making it less sensitive to outliers while remaining convex. It is defined as:</p> <p> </p> <p>where  is the residual, and  is a threshold parameter.  </p>"},{"location":"3a%20Huber/#key-properties","title":"Key Properties","text":"<ul> <li>Quadratic for small residuals () \u2192 behaves like least squares.  </li> <li>Linear for large residuals () \u2192 reduces the influence of outliers.  </li> <li>Convex, so standard convex optimization techniques apply.  </li> </ul>"},{"location":"3a%20Huber/#use","title":"Use","text":"<ul> <li>Commonly used in robust regression to estimate parameters in the presence of outliers.</li> <li>Balances efficiency (like least squares) and robustness (like absolute loss).</li> </ul>"},{"location":"3b%20Penalty%20Functions/","title":"Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#convex-optimization-notes-penalty-function-approximation","title":"Convex Optimization Notes: Penalty Function Approximation","text":""},{"location":"3b%20Penalty%20Functions/#penalty-function-approximation","title":"Penalty Function Approximation","text":"<p>We solve:</p> <p> </p> <p>where: -  -  is a convex penalty function</p> <p>The choice of  determines how residuals are penalized.</p>"},{"location":"3b%20Penalty%20Functions/#common-penalty-functions","title":"Common Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#1-quadratic-least-squares","title":"1. Quadratic (Least Squares)","text":"<ul> <li>Strongly convex, smooth.  </li> <li>Penalizes large residuals heavily.  </li> <li>Equivalent to Gaussian noise model in statistics.  </li> <li>Leads to unique minimizer.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#2-absolute-value-least-absolute-deviations","title":"2. Absolute Value (Least Absolute Deviations)","text":"<ul> <li>Convex but nonsmooth at  (subgradient methods needed).  </li> <li>Robust to outliers compared to quadratic.  </li> <li>Equivalent to Laplace noise model in statistics.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#why-does-it-lead-to-sparsity","title":"Why does it lead to sparsity?","text":"<ul> <li>The sharp corner at  makes it favorable for optimization to set many residuals (or coefficients) exactly to zero.  </li> <li>In contrast, quadratic penalties () only shrink values toward zero but rarely make them exactly zero.  </li> <li>Geometric intuition: the  ball has corners aligned with coordinate axes \u2192 solutions land on axes \u2192 sparse.  </li> <li>Statistical interpretation: corresponds to a Laplace prior, which induces sparsity, whereas  corresponds to a Gaussian prior (no sparsity).  </li> </ul> <p>\ud83d\udc49 This property is the foundation of Lasso regression and many compressed sensing methods.  </p>"},{"location":"3b%20Penalty%20Functions/#3-deadzone-linear","title":"3. Deadzone-Linear","text":"<p> , where  </p> <ul> <li>Ignores small deviations ().  </li> <li>Linear growth outside the \u201cdeadzone.\u201d  </li> <li>Used in support vector regression (SVR) with -insensitive loss.  </li> <li>Convex, but not strictly convex \u2192 possibly multiple minimizers.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#4-log-barrier","title":"4. Log-Barrier","text":"<ul> <li>Smooth, convex inside domain .  </li> <li>Grows steeply as .  </li> <li>Effectively enforces constraint .  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#histograms-of-residuals-effect-of-penalty-choice","title":"Histograms of Residuals (Effect of Penalty Choice)","text":"<p>For , the residual distribution  depends on :</p> <ul> <li>Quadratic (): residuals spread out (Gaussian-like).  </li> <li>Absolute value (): sharper peak at 0, heavier tails (Laplace-like).  </li> <li>Deadzone: many residuals exactly at 0 (ignored region).  </li> <li>Log-barrier: residuals concentrate away from the boundary .  </li> </ul> <p>\ud83d\udc49 Takeaway: Choice of  directly shapes residual distribution.</p>"},{"location":"3b%20Penalty%20Functions/#huber-penalty-function","title":"Huber Penalty Function","text":"<p>The Huber penalty combines quadratic and linear growth:</p> <p> </p>"},{"location":"3b%20Penalty%20Functions/#properties","title":"Properties","text":"<ul> <li>Quadratic near 0 () \u2192 efficient for small noise.  </li> <li>Linear for large  \u2192 robust to outliers.  </li> <li>Smooth, convex.  </li> <li>Interpolates between least squares and least absolute deviations.  </li> </ul> <p>\ud83d\udc49 Called a robust penalty, widely used in robust regression.</p>"},{"location":"3b%20Penalty%20Functions/#summary-choosing-a-penalty-function","title":"Summary: Choosing a Penalty Function","text":"<ul> <li>Quadratic: efficient, but sensitive to outliers.  </li> <li>Absolute value: robust, but nonsmooth.  </li> <li>Deadzone: ignores small errors, good for sparse modeling (e.g., SVR).  </li> <li>Log-barrier: enforces domain constraints smoothly.  </li> <li>Huber: best of both worlds \u2192 quadratic for small residuals, linear for large ones.  </li> </ul>"},{"location":"3c%20Regularized/","title":"Regularized","text":""},{"location":"3c%20Regularized/#regularized-approximation","title":"Regularized Approximation","text":""},{"location":"3c%20Regularized/#1-motivation-fit-vs-complexity","title":"1. Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we often want to balance two competing goals:</p> <ol> <li>Data fidelity: minimize how poorly the model fits the observed data ().  </li> <li>Model simplicity: discourage overly complex solutions ().</li> </ol> <p>This is naturally a bicriterion optimization problem:</p> <ul> <li>Criterion 1:  = data-fitting term (e.g., least squares loss ).  </li> <li>Criterion 2:  = regularization term (e.g., , , TV).  </li> </ul> <p>Since minimizing both simultaneously is usually impossible, we form the scalarized problem:</p> <p> </p> <p>Here,  controls the trade-off: small  emphasizes fit, large  emphasizes simplicity.</p>"},{"location":"3c%20Regularized/#2-bicriterion-and-pareto-frontier","title":"2. Bicriterion and Pareto Frontier","text":"<ul> <li>Pareto optimality: a solution  is Pareto optimal if no other  improves one criterion without worsening the other.  </li> <li>Weighted sum method:  </li> <li>For convex  and , every Pareto optimal solution can be obtained from some .  </li> <li>For nonconvex problems, weighted sums may miss parts of the frontier.  </li> </ul> <p>Thus, regularization is a way of choosing a point on the Pareto frontier between fit and complexity.</p>"},{"location":"3c%20Regularized/#3-why-keep-x-small","title":"3. Why Keep  Small?","text":"<p>Ill-posed or noisy problems (e.g.,  with ill-conditioned ) often admit solutions with very large . - These large values overfit noise and are unstable. - Regularization (especially ) controls the size of , yielding stable and robust solutions.  </p> <p>Example (Ridge regression):</p> <p> </p> <p>Leads to the normal equations:</p> <p> </p> <ul> <li>  is always positive definite.  </li> <li>Even if  is rank-deficient, the solution is unique and stable.</li> </ul>"},{"location":"3c%20Regularized/#4-lagrangian-interpretation","title":"4. Lagrangian Interpretation","text":"<p>Regularized approximation is equivalent to a constrained optimization formulation:</p> <p> </p> <p>for some bound .  </p>"},{"location":"3c%20Regularized/#kkt-and-duality","title":"KKT and Duality","text":"<ul> <li>The Lagrangian is:</li> </ul> <ul> <li>Under convexity and Slater\u2019s condition, strong duality holds.  </li> <li>KKT conditions:</li> </ul> <ul> <li>The penalized form:</li> </ul> <p>has the same optimality condition:</p> <p> </p> <p>Hence, solving the penalized problem corresponds to solving the constrained one for some , though the  mapping is monotone but not one-to-one.</p>"},{"location":"3c%20Regularized/#5-common-regularizers","title":"5. Common Regularizers","text":""},{"location":"3c%20Regularized/#l2-ridge","title":"L2 (Ridge)","text":"<p>  - Strongly convex \u2192 unique solution. - Encourages small coefficients, smooth solutions. - Bayesian view: Gaussian prior on . - Improves conditioning of .  </p>"},{"location":"3c%20Regularized/#l1-lasso","title":"L1 (Lasso)","text":"<p>  - Convex, but not strongly convex \u2192 solutions may be non-unique. - Promotes sparsity: many coefficients exactly zero. - Geometric view:  ball has corners aligned with coordinate axes; intersections often occur at corners \u2192 sparse solutions. - Bayesian view: Laplace prior on . - Proximal operator: soft-thresholding </p>"},{"location":"3c%20Regularized/#elastic-net","title":"Elastic Net","text":"<p>  - Combines L1 sparsity with L2 stability. - Ensures uniqueness even when features are correlated.  </p>"},{"location":"3c%20Regularized/#beyond-l1l2","title":"Beyond L1/L2","text":"<ul> <li>General Tikhonov: , where  encodes smoothness (e.g., derivative operator).  </li> <li>Total Variation (TV): , promotes piecewise-constant signals.  </li> <li>Group Lasso: , induces structured sparsity.  </li> <li>Nuclear Norm:  (sum of singular values), promotes low-rank matrices.  </li> </ul>"},{"location":"3c%20Regularized/#6-choosing-the-regularization-parameter-lambda","title":"6. Choosing the Regularization Parameter","text":""},{"location":"3c%20Regularized/#trade-off","title":"Trade-off","text":"<ul> <li>Too small : weak regularization \u2192 overfitting, unstable solutions.  </li> <li>Too large : strong regularization \u2192 underfitting, biased solutions.  </li> </ul> <p>  determines where on the Pareto frontier the solution lies.</p>"},{"location":"3c%20Regularized/#practical-selection","title":"Practical Selection","text":"<ul> <li>Cross-validation (CV): </li> <li>Split data into  folds.  </li> <li>Train on  folds, validate on the held-out fold.  </li> <li>Average validation error across folds.  </li> <li> <p>Choose  minimizing average error.  </p> </li> <li> <p>Best practices: </p> </li> <li>Standardize features before using L1/Elastic Net.  </li> <li>For time series, use blocked or rolling CV (avoid leakage).  </li> <li>Use nested CV for model comparison.  </li> <li>One-standard-error rule: prefer larger  within one SE of min error \u2192 simpler model.  </li> </ul>"},{"location":"3c%20Regularized/#alternatives","title":"Alternatives","text":"<ul> <li>Analytical rules (ridge regression has closed-form shrinkage).  </li> <li>Information criteria (AIC/BIC; heuristic for Lasso).  </li> <li>Regularization path (trace solutions as  varies, pick best by validation error).  </li> <li>Inverse problems: discrepancy principle, L-curve, generalized CV.  </li> </ul>"},{"location":"3c%20Regularized/#7-algorithmic-perspective","title":"7. Algorithmic Perspective","text":"<p>Regularized problems often have the form:</p> <p> </p> <p>where  is smooth convex and  is convex but possibly nonsmooth.</p> <ul> <li>Proximal Gradient (ISTA, FISTA):   Iterative updates using gradient of  and prox of .  </li> <li>Coordinate Descent: very effective for Lasso/Elastic Net.  </li> <li>ADMM: handles separable structures and constraints well.  </li> </ul> <p>Proximal operators are key: - L2: shrinkage (scaling). - L1: soft-thresholding. - TV/nuclear norm: more advanced proximal maps.  </p>"},{"location":"3c%20Regularized/#8-bayesian-interpretation","title":"8. Bayesian Interpretation","text":"<ul> <li>Regularization corresponds to MAP estimation.  </li> <li>Example: Gaussian noise  and Gaussian prior  yields:</li> </ul> <p>So  (up to scaling). - L1 corresponds to a Laplace prior, inducing sparsity.</p>"},{"location":"3c%20Regularized/#9-key-takeaways","title":"9. Key Takeaways","text":"<ul> <li>Regularized approximation = bicriterion optimization (fit vs. complexity).  </li> <li>Penalized and constrained forms are connected via duality and KKT.  </li> <li>Regularization stabilizes ill-posed problems and improves generalization.  </li> <li>Choice of regularizer shapes the solution (small , sparse , structured TV/group/nuclear).  </li> <li>  is critical \u2014 usually chosen by cross-validation or problem-specific heuristics.  </li> <li>Proximal algorithms make regularized optimization scalable.  </li> <li>Bayesian view ties  to prior assumptions and noise models.</li> </ul>"},{"location":"3d%20Robust%20Approximation/","title":"Robust Approximation","text":""},{"location":"3d%20Robust%20Approximation/#robust-regression-stochastic-vs-worst-case-formulations","title":"Robust Regression: Stochastic vs. Worst-Case Formulations","text":""},{"location":"3d%20Robust%20Approximation/#setup","title":"Setup","text":"<p>We study linear regression with uncertain design matrix:</p> <p> </p> <p>where  </p> <ul> <li>  is the decision variable,  </li> <li>  is the observed response,  </li> <li>  is the nominal design matrix,  </li> <li>  is an uncertainty term.  </li> </ul> <p>The treatment of  gives rise to two main formulations: stochastic (probabilistic uncertainty) and worst-case (deterministic uncertainty).</p>"},{"location":"3d%20Robust%20Approximation/#1-stochastic-formulation","title":"1. Stochastic Formulation","text":"<p>Assume  is random with  </p> <ul> <li> ,  </li> <li> ,  </li> <li>finite second moment,  </li> <li>independent of .  </li> </ul> <p>We minimize the expected squared residual:</p> <p> </p>"},{"location":"3d%20Robust%20Approximation/#expansion","title":"Expansion","text":"<ul> <li>Cross-term vanishes since  and  is independent of :  </li> </ul> <ul> <li>Variance term simplifies:  </li> </ul>"},{"location":"3d%20Robust%20Approximation/#resulting-problem","title":"Resulting Problem","text":"<ul> <li>If : ridge regression (L2 regularization).  </li> <li>If  general: generalized Tikhonov regularization, with anisotropic penalty .  </li> </ul>"},{"location":"3d%20Robust%20Approximation/#convexity","title":"Convexity","text":"<p>The Hessian is  </p> <p> </p> <p>Thus the problem is convex. If , it is strongly convex and the minimizer is unique.  </p>"},{"location":"3d%20Robust%20Approximation/#2-worst-case-formulation","title":"2. Worst-Case Formulation","text":"<p>Suppose  is unknown but bounded:</p> <p> </p> <p>where  is the spectral norm (largest singular value). We minimize the worst-case squared residual:</p> <p> </p>"},{"location":"3d%20Robust%20Approximation/#expansion-via-spectral-norm-bound","title":"Expansion via Spectral Norm Bound","text":"<p>For spectral norm uncertainty:</p> <p> </p> <p>This identity uses the fact that  can align with the residual direction when . Note: If a different norm bound is used (Frobenius, , etc.), the expression changes.</p>"},{"location":"3d%20Robust%20Approximation/#resulting-problem_1","title":"Resulting Problem","text":"<p>This is convex but not quadratic. Unlike ridge regression, the regularization is coupled inside the residual norm, making the solution more conservative.</p>"},{"location":"3d%20Robust%20Approximation/#3-comparison","title":"3. Comparison","text":"Aspect Stochastic Formulation Worst-Case Formulation Model of  Random, mean zero, finite variance Deterministic, bounded  Objective Regularization Quadratic penalty (ellipsoidal shrinkage) Norm inflation coupled with residual Geometry Ellipsoidal shrinkage of  (Mahalanobis norm) Inflated residual tube, more conservative Convexity Convex quadratic; strongly convex if  Convex but non-quadratic"},{"location":"3d%20Robust%20Approximation/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Stochastic robust regression \u2192 ridge/Tikhonov regression (quadratic L2 penalty).  </li> <li>Worst-case robust regression \u2192 inflated residual norm with L2 penalty inside the loss, more conservative than ridge.  </li> <li>Both are convex, but their geometry differs:  </li> <li>Stochastic: smooth ellipsoidal shrinkage of coefficients.  </li> <li>Worst-case: enlarged residual \u201ctube\u201d that hedges against adversarial perturbations.  </li> </ul>"},{"location":"3e%20MLE/","title":"MLE","text":""},{"location":"3e%20MLE/#statistical-estimation-and-maximum-likelihood","title":"Statistical Estimation and Maximum Likelihood","text":""},{"location":"3e%20MLE/#1-maximum-likelihood-estimation-mle","title":"1. Maximum Likelihood Estimation (MLE)","text":"<p>Suppose we have a family of probability densities</p> <p> </p> <p>where  (often written as  in statistics) is the parameter to be estimated.  </p> <ul> <li>  for invalid parameter values .  </li> <li>The function , viewed as a function of  with  fixed, is called the likelihood function.  </li> <li>The log-likelihood is defined as  </li> </ul> <p> </p> <ul> <li>The maximum likelihood estimate (MLE) is  </li> </ul> <p> </p>"},{"location":"3e%20MLE/#convexity-perspective","title":"Convexity Perspective","text":"<ul> <li>If  is concave in  for each fixed , then the MLE problem is a convex optimization problem.  </li> <li>Important distinction: this requires concavity in , not in .  </li> <li>Example:  may be a log-concave density in  (common in statistics),  </li> <li>but this does not imply that  is concave in .  </li> </ul> <p>Thus, convexity of the MLE depends on the parameterization of the distribution family.  </p>"},{"location":"3e%20MLE/#2-linear-measurements-with-iid-noise","title":"2. Linear Measurements with IID Noise","text":"<p>Consider the linear measurement model:</p> <p> </p> <p>where  </p> <ul> <li>  is the unknown parameter vector,  </li> <li>  are known measurement vectors,  </li> <li>  are i.i.d. noise variables with density ,  </li> <li>  is the vector of observed measurements.  </li> </ul>"},{"location":"3e%20MLE/#likelihood-function","title":"Likelihood Function","text":"<p>Since the noise terms are independent:</p> <p> </p> <p>Taking logs:</p> <p> </p>"},{"location":"3e%20MLE/#mle-problem","title":"MLE Problem","text":"<p>The MLE is any solution to:</p> <p> </p>"},{"location":"3e%20MLE/#convexity-note","title":"Convexity Note","text":"<ul> <li>If  is log-concave in , then  is concave in .  </li> <li>Therefore, under log-concave noise distributions (e.g. Gaussian, Laplace, logistic), the MLE problem is a concave maximization problem, hence equivalent to a convex optimization problem after sign change:</li> </ul>"},{"location":"4a%20Linear%20Discrimination/","title":"Linear Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#1-linear-discrimination-lp-feasibility","title":"1. Linear Discrimination (LP Feasibility)","text":""},{"location":"4a%20Linear%20Discrimination/#problem-setup","title":"Problem Setup","text":"<ul> <li>Variables:  </li> <li>Constraints:</li> <li> </li> <li> </li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity","title":"Convexity","text":"<ul> <li>Each constraint is affine in .</li> <li>Affine inequalities define convex half-spaces.</li> <li>Intersection of half-spaces = convex polyhedron.</li> <li>No objective \u2192 pure LP feasibility.</li> </ul> <p>Type: Convex LP feasibility problem.</p>"},{"location":"4a%20Linear%20Discrimination/#2-robust-linear-discrimination-hard-margin-svm","title":"2. Robust Linear Discrimination (Hard-Margin SVM)","text":""},{"location":"4a%20Linear%20Discrimination/#problem","title":"Problem","text":""},{"location":"4a%20Linear%20Discrimination/#convexity_1","title":"Convexity","text":"<ul> <li>Objective:  is convex quadratic (strictly convex in ).</li> <li>Constraints: Affine  convex feasible set.</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#3-soft-margin-svm","title":"3. Soft-Margin SVM","text":""},{"location":"4a%20Linear%20Discrimination/#problem_1","title":"Problem","text":"<p>  subject to  </p>"},{"location":"4a%20Linear%20Discrimination/#convexity_2","title":"Convexity","text":"<ul> <li>Objective: Sum of convex quadratic () and linear ().</li> <li>Constraints: Affine in .</li> <li>Feasible set = intersection of half-spaces (convex).</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#4-hinge-loss-formulation","title":"4. Hinge Loss Formulation","text":""},{"location":"4a%20Linear%20Discrimination/#problem_2","title":"Problem","text":""},{"location":"4a%20Linear%20Discrimination/#convexity_3","title":"Convexity","text":"<ul> <li> : convex quadratic.</li> <li>Inside hinge:  is affine.</li> <li>  = convex function.</li> <li>Sum of convex functions = convex.</li> </ul> <p>Type: Unconstrained convex optimization problem.</p>"},{"location":"4a%20Linear%20Discrimination/#5-dual-svm-problem","title":"5. Dual SVM Problem","text":""},{"location":"4a%20Linear%20Discrimination/#problem_3","title":"Problem","text":"<p>  subject to  </p>"},{"location":"4a%20Linear%20Discrimination/#convexity_4","title":"Convexity","text":"<ul> <li>Quadratic form has negative semidefinite Hessian (concave).</li> <li>Maximization of concave function over convex set \u2192 convex optimization.</li> </ul> <p>Type: Convex quadratic program in dual variables.</p>"},{"location":"4a%20Linear%20Discrimination/#6-nonlinear-discrimination-with-kernels","title":"6. Nonlinear Discrimination with Kernels","text":""},{"location":"4a%20Linear%20Discrimination/#problem-dual-with-kernel","title":"Problem (Dual with Kernel)","text":"<p>  subject to  </p>"},{"location":"4a%20Linear%20Discrimination/#convexity_5","title":"Convexity","text":"<ul> <li>If  is positive semidefinite (Mercer kernel), quadratic form is convex.</li> <li>Maximization remains convex program.</li> </ul> <p>Type: Convex QP with kernel matrix.</p>"},{"location":"4a%20Linear%20Discrimination/#7-quadratic-discrimination","title":"7. Quadratic Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#problem_4","title":"Problem","text":"<p>Classifier:  with variables .</p> <p>Constraints: -  -  </p>"},{"location":"4a%20Linear%20Discrimination/#convexity_6","title":"Convexity","text":"<ul> <li> , affine in .</li> <li>Constraints affine in .</li> <li>If additional constraint , this is convex (semidefinite cone).</li> </ul> <p>Type: LP feasibility or SDP (semidefinite program).</p>"},{"location":"4a%20Linear%20Discrimination/#8-polynomial-feature-maps","title":"8. Polynomial Feature Maps","text":""},{"location":"4a%20Linear%20Discrimination/#setup","title":"Setup","text":"<ul> <li>Map  with monomials up to degree .</li> <li>Classifier: .</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity_7","title":"Convexity","text":"<ul> <li>Constraints: , affine in .</li> <li>Margin maximization objective: , convex quadratic.</li> </ul> <p>Type: LP feasibility or convex QP.</p>"},{"location":"4a%20Linear%20Discrimination/#9-summary-of-convex-structures","title":"9. Summary of Convex Structures","text":"<ul> <li>LP feasibility: Linear separation.  </li> <li>QP: Hard-margin and soft-margin SVM.  </li> <li>Unconstrained convex problem: Hinge loss.  </li> <li>Dual SVM: Convex QP in dual variables.  </li> <li>Kernel SVM: Convex QP with PSD kernel.  </li> <li>Quadratic/Polynomial discrimination: LP or SDP, depending on constraints.  </li> </ul>"},{"location":"6a%20First%20Order%20Optimization/","title":"First Order Optimization","text":""},{"location":"6a%20First%20Order%20Optimization/#first-order-optimization-methods","title":"First-Order Optimization Methods","text":"<p>In machine learning, especially at large scale, we often cannot afford to solve convex problems using heavy, exact solvers (like simplex or interior-point methods). Instead, we rely on first-order methods \u2014 algorithms that use only function values and gradients to iteratively approach the solution.</p>"},{"location":"6a%20First%20Order%20Optimization/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<p>Gradient descent is the most fundamental algorithm for minimizing differentiable convex functions.</p> <p>Basic Idea: At each iteration, move in the direction opposite to the gradient (the steepest descent direction), because it points toward lower values of the function.</p> <p>Update Rule:  where: -  \u2014 gradient at the current point. -  \u2014 step size (learning rate).</p> <p>Convergence (Convex Case): - If  is convex and has Lipschitz-continuous gradients, gradient descent converges at rate  for constant step size. - If  is also strongly convex, the rate improves to linear convergence.</p> <p>Step Size Selection: - Constant: simple but requires tuning. - Diminishing:  ensures convergence but may be slow. - Backtracking Line Search: adaptively chooses  for efficiency.</p>"},{"location":"6a%20First%20Order%20Optimization/#subgradient-methods","title":"Subgradient Methods","text":"<p>Many important convex functions in ML are non-differentiable (e.g., ). The subgradient generalizes the gradient for such functions.</p> <p>Subgradient Definition: A vector  is a subgradient of  at  if:  </p> <p>Update Rule: </p> <p>Key Trade-Off: Subgradient methods are robust but converge more slowly ( in general).</p>"},{"location":"6a%20First%20Order%20Optimization/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>SGD is the workhorse of large-scale machine learning.</p> <p>When to Use: When the objective is a sum over many data points:  </p> <p>Update Rule: - Pick a random index  - Use  as an unbiased estimate of the full gradient:  </p> <p>Advantages: - Much faster per iteration for large . - Enables online learning.</p> <p>Disadvantages: - Introduces variance; iterates \u201cbounce\u201d around the optimum. - Requires careful learning rate schedules.</p>"},{"location":"6a%20First%20Order%20Optimization/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<p>Nesterov\u2019s Accelerated Gradient (NAG) achieves the optimal convergence rate for smooth convex functions: .</p> <p>Key Idea: Introduce a momentum term that anticipates the next position, correcting the gradient direction.</p> <p>Update: </p> <p>When Useful: - Smooth convex problems where plain gradient descent is too slow. - Large-scale ML tasks with batch updates.</p>"},{"location":"6a%20First%20Order%20Optimization/#why-first-order-methods-matter-for-ml","title":"Why First-Order Methods Matter for ML","text":"<ul> <li>Handle huge datasets efficiently.</li> <li>Require only gradient information, which is cheap for many models.</li> <li>Naturally fit into streaming and online learning setups.</li> <li>Form the backbone of deep learning optimizers (SGD, Adam, RMSProp \u2014 though deep nets are non-convex).</li> </ul>"},{"location":"7a%20pareto%20optimal/","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#classical-optimality","title":"Classical Optimality","text":"<p>In standard convex optimisation, we consider a single objective function  and aim to find a globally optimal solution:  where  is the feasible set.  </p> <p>Here, optimality is absolute: there exists a single best point (or set of equivalent best points) with respect to one measure of performance.</p>"},{"location":"7a%20pareto%20optimal/#multi-objective-optimisation","title":"Multi-objective Optimisation","text":"<p>Many practical problems in machine learning and optimisation involve multiple competing objectives. For instance:</p> <ul> <li>In supervised learning, one wishes to minimise prediction error while also controlling model complexity.  </li> <li>In fairness-aware learning, we want high accuracy while limiting demographic disparity.  </li> <li>In finance, an investor balances expected return against risk.  </li> </ul> <p>Formally, a multi-objective optimisation problem is written as:  where  are the competing objectives.</p>"},{"location":"7a%20pareto%20optimal/#pareto-optimality_1","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A solution  is Pareto optimal if there is no  such that:  with strict inequality for at least one objective .  </p> <p>Intuitively, no feasible point strictly improves one objective without worsening another.</p>"},{"location":"7a%20pareto%20optimal/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A solution  is weakly Pareto optimal if there is no  such that:  </p> <p>In other words, no solution improves every objective simultaneously.</p>"},{"location":"7a%20pareto%20optimal/#geometric-intuition","title":"Geometric Intuition","text":"<p>If we plot feasible solutions in the objective space , the Pareto frontier is the lower-left boundary for minimisation problems. - Points on the frontier are non-dominated (Pareto optimal). - Points inside the feasible region but above the boundary are dominated.  </p>"},{"location":"7a%20pareto%20optimal/#scalarisation","title":"Scalarisation","text":"<p>Since multi-objective optimisation problems usually admit a set of Pareto optimal solutions rather than a single best point, practitioners use scalarisation. This reduces multiple objectives to a single scalar objective that can be optimised with standard methods.</p>"},{"location":"7a%20pareto%20optimal/#weighted-sum-scalarisation","title":"Weighted Sum Scalarisation","text":"<p>The most common approach is the weighted sum:  </p> <ul> <li>Each choice of weights  corresponds to a different point on the Pareto frontier.  </li> <li>Larger  prioritises objective  relative to others.  </li> </ul> <p>Convexity caveat: If the feasible set and objectives are convex, weighted sum scalarisation can recover the convex part of the Pareto frontier. Non-convex regions of the frontier may not be attainable using weighted sums alone.</p>"},{"location":"7a%20pareto%20optimal/#varepsilon-constraint-method","title":"-Constraint Method","text":"<p>Another approach is to optimise one objective while converting others into constraints:  Here  are tolerance levels. By adjusting them, we can explore different trade-offs.  </p> <p>This connects directly to regularisation in machine learning: - In ridge regression, we minimise data fit subject to a complexity budget . - The equivalent penalised form  is obtained via Lagrangian duality, where  is the multiplier associated with .  </p>"},{"location":"7a%20pareto%20optimal/#duality-and-scalarisation","title":"Duality and Scalarisation","text":"<p>Scalarisation is deeply connected to duality in convex optimisation: - The weights  or multipliers  can be interpreted as Lagrange multipliers balancing objectives. - Adjusting these parameters changes the point on the Pareto frontier that is selected. - This explains why hyperparameters like  in regularisation are so influential: they represent trade-offs in a hidden multi-objective problem.</p>"},{"location":"7a%20pareto%20optimal/#example-1-regularised-least-squares","title":"Example 1: Regularised Least Squares","text":"<p>Consider the regression problem with data matrix  and target .  </p> <p>We want to minimise: 1. Prediction error:   2. Model complexity:  </p> <p>This is a two-objective optimisation problem.  </p> <ul> <li> <p>Using the weighted sum:  where  determines the trade-off.  </p> </li> <li> <p>Alternatively, using the -constraint:  </p> </li> </ul> <p>Both formulations yield Pareto optimal solutions, with  and  providing different parametrisations of the frontier.</p>"},{"location":"7a%20pareto%20optimal/#example-2-portfolio-optimisation-riskreturn","title":"Example 2: Portfolio Optimisation (Risk\u2013Return)","text":"<p>In finance, suppose an investor chooses portfolio weights .</p> <ul> <li>Expected return:  (we minimise negative return).  </li> <li>Risk:  (variance of returns, a convex function).  </li> </ul> <p>The problem is:  </p> <ul> <li>Using weighted sum scalarisation:  </li> <li>Different  values give different points on the efficient frontier.  </li> </ul> <p>This convex formulation underpins modern portfolio theory.</p>"},{"location":"7a%20pareto%20optimal/#example-3-probabilistic-modelling-elbo","title":"Example 3: Probabilistic Modelling (ELBO)","text":"<p>In variational inference, the Evidence Lower Bound (ELBO) is:  </p> <p>This can be seen as a scalarisation of two competing objectives: 1. Data fit (reconstruction term). 2. Simplicity or prior adherence (KL divergence).  </p> <p>By weighting the KL divergence with a parameter , we obtain the -VAE:  </p> <p>Here,  plays the role of a scalarisation weight, selecting different Pareto optimal trade-offs between reconstruction accuracy and disentanglement.</p>"},{"location":"7a%20pareto%20optimal/#broader-connections-in-ai-and-ml","title":"Broader Connections in AI and ML","text":"<ul> <li>Fairness vs accuracy: Balancing accuracy with fairness metrics is a multi-objective problem often approached via scalarisation.  </li> <li>Generalisation vs training error: Regularisation is a scalarisation of fit versus complexity.  </li> <li>Compression vs performance: The information bottleneck principle is a Pareto trade-off between accuracy and representation complexity.  </li> <li>Inference vs divergence: Variational inference (ELBO) is naturally a scalarised multi-objective problem.  </li> </ul>"},{"location":"7a%20pareto%20optimal/#summary","title":"Summary","text":"<ul> <li>Classical optimisation yields a single best solution.  </li> <li>Multi-objective optimisation gives a set of non-dominated (Pareto optimal) solutions.  </li> <li>Scalarisation provides practical methods to compute Pareto optimal solutions.  </li> <li>Weighted sums recover convex parts of the frontier, while -constraints provide flexibility.  </li> <li>Scalarisation connects directly to duality, where weights act as Lagrange multipliers.  </li> <li>Examples in ML (ridge regression, ELBO) and finance (portfolio optimisation) demonstrate its wide relevance.  </li> </ul> <p>Scalarisation is not only a mathematical device but the foundation for understanding regularisation, fairness, generalisation, and many practical trade-offs in machine learning.</p>"},{"location":"Example/","title":"Example","text":""},{"location":"Example/#galactic-cargo-delivery-optimization-lp-formulation","title":"Galactic Cargo Delivery Optimization \u2014 LP Formulation","text":"<p>You are the logistics commander of an interstellar fleet tasked with delivering vital supplies across the galaxy. Your fleet consists of  starship pilots, and you have  distinct types of cargo crates to deliver. Each cargo type  has a known volume , representing the number of crates that must reach their destinations.</p> <p>To maintain fleet balance and operational efficiency, each pilot must carry the same total number of crates. Your mission is to assign crates to pilots to minimize the total expected delivery time, accounting for each pilot\u2019s unique speed and proficiency with different cargo types.</p>"},{"location":"Example/#notation","title":"Notation","text":"<ul> <li> : indices for starship pilots  </li> <li> : indices for cargo types  </li> <li> : volume (number of crates) of cargo type  </li> <li> : estimated delivery time for pilot  to deliver one crate of type  </li> </ul>"},{"location":"Example/#decision-variables","title":"Decision Variables","text":"<p>Number of crates of cargo type  assigned to pilot .</p>"},{"location":"Example/#objective-function","title":"Objective Function","text":"<p>Minimize the total delivery time across all pilots:</p> <p> </p> <p>Or equivalently, in vector form:</p> <p> </p> <p>where</p> <p> </p>"},{"location":"Example/#constraints","title":"Constraints","text":"<ol> <li>All crates must be delivered:</li> </ol> <ol> <li>Each pilot carries the same total number of crates:</li> </ol> <ol> <li>Non-negativity: </li> </ol>"},{"location":"Example/#lp-formulation","title":"LP Formulation","text":"<p>Where:</p> <ul> <li>  encodes the equality constraints for cargo delivery and load balancing.</li> <li>  combines the crate volumes  and equal load targets .</li> </ul>"}]}