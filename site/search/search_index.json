{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p> <ul> <li>Concise derivations of essential results</li> <li>Geometric intuition and figures where helpful</li> <li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li> <li>Appendices that extend into more advanced or specialized topics</li> </ul> <p>Whether you\u2019re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p>"},{"location":"appendices/120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"appendices/120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"appendices/120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"appendices/120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"appendices/120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"appendices/120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"appendices/120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"appendices/120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"appendices/120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"appendices/120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"appendices/130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"appendices/140_support/","title":"Appendix C - Support Functions and Dual Geometry (Advanced)","text":""},{"location":"appendices/140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"appendices/140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"appendices/140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"appendices/140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"appendices/140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"appendices/140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"appendices/140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"appendices/140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"appendices/160_conjugates/","title":"Appendix D - Convex Conjugates and Fenchel Duality (Advanced)","text":""},{"location":"appendices/160_conjugates/#appendix-d-convex-conjugates-and-fenchel-duality","title":"Appendix D: Convex Conjugates and Fenchel Duality","text":"<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis. They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p> <ul> <li>Convex conjugates convert a function into its \u201cslope-space\u201d representation \u2014 capturing its tightest linear overestimates.  </li> <li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li> </ul> <p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>"},{"location":"appendices/160_conjugates/#d1-intuitive-picture","title":"D.1 Intuitive Picture","text":"<p>Imagine a convex function \\(f(x)\\) drawn as a bowl in space. Each point \\(y\\) defines a line (or hyperplane) of slope \\(y\\):  The convex conjugate \\(f^*(y)\\) is the smallest height \\(b\\) such that this line always stays above \\(f(x)\\). In other words:</p> <p>\\(f^*(y)\\) measures the tightest linear overestimate of \\(f\\) in direction \\(y\\).</p> <p>So \\(f^*\\) encodes how \u201csteep\u201d \\(f\\) can be in every direction \u2014 it transforms the geometry of \\(f\\) into a new convex function on slope-space.</p>"},{"location":"appendices/160_conjugates/#d2-definition-and-key-properties","title":"D.2 Definition and Key Properties","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}\\) be a proper convex function. Its convex (Fenchel) conjugate is  </p> <p>Interpretation - \\(y\\): a slope or linear functional. - The supremum seeks the largest gap between the linear function \\(\\langle y,x\\rangle\\) and the graph of \\(f\\). - \\(f^*(y)\\) is always convex, even if \\(f\\) isn\u2019t strictly convex.</p>"},{"location":"appendices/160_conjugates/#fundamental-identities","title":"Fundamental Identities","text":"<ol> <li> <p>Fenchel\u2013Young inequality        with equality iff \\(y \\in \\partial f(x)\\).</p> </li> <li> <p>Biconjugation        This tells us the conjugate transform loses no information for convex functions.</p> </li> <li> <p>Order reversal    \\(f \\le g \\;\\Rightarrow\\; f^* \\ge g^*\\).</p> </li> <li> <p>Scaling and shift</p> </li> <li>\\((f + a)^*(y) = f^*(y) - a\\),</li> <li>\\((\\alpha f)^*(y) = \\alpha f^*(y/\\alpha)\\) for \\(\\alpha&gt;0.\\)</li> </ol>"},{"location":"appendices/160_conjugates/#d3-canonical-examples","title":"D.3 Canonical Examples","text":"Function \\(f(x)\\) Conjugate \\(f^*(y)\\) Notes \\( \\tfrac{1}{2}\\|x\\|_2^2 \\) \\( \\tfrac{1}{2}\\|y\\|_2^2 \\) Self-conjugate quadratic \\( \\|x\\|_1 \\) \\( \\delta_{\\{\\|y\\|_\\infty \\le 1\\}}(y) \\) Dual norm indicator \\( \\delta_C(x) \\) \\( \\sigma_C(y)=\\sup_{x\\in C}\\langle y,x\\rangle \\) Support function of set \\(C\\) \\( e^x \\) \\( y\\log y - y,\\, y&gt;0 \\) Appears in entropy and KL-divergence <p>These examples illustrate how conjugation connects: - Norms \u2194 dual norms, - Sets \u2194 support functions, - Exponentials \u2194 entropy, - Quadratics \u2194 themselves.</p>"},{"location":"appendices/160_conjugates/#d4-geometric-interpretation","title":"D.4 Geometric Interpretation","text":"<ul> <li>Each point on \\(f\\) has a tangent hyperplane whose slope is a subgradient.  </li> <li>The collection of all such hyperplanes forms the epigraph of \\(f^*\\).  </li> <li>The transformation \\(f \\mapsto f^*\\) swaps the roles of \u201cposition\u201d and \u201cslope\u201d:   convex geometry \u2194 supporting hyperplanes.</li> </ul> <p>Visually: - \\(f\\) describes a bowl in \\((x,t)\\)-space. - \\(f^*\\) describes the envelope of tangent planes to that bowl.</p>"},{"location":"appendices/160_conjugates/#d5-from-conjugates-to-duality-fenchel-duality","title":"D.5 From Conjugates to Duality \u2014 Fenchel Duality","text":"<p>Many convex optimization problems can be written as  where \\(f,g\\) are convex and \\(A\\) is linear. Fenchel duality uses conjugates to build a dual problem in terms of \\(f^*\\) and \\(g^*\\).</p>"},{"location":"appendices/160_conjugates/#the-fenchel-dual-problem","title":"The Fenchel Dual Problem","text":"\\[ \\max_y \\; -f^*(A^\\top y) - g^*(-y). \\] <p>Interpretation - \\(y\\) is the dual variable (similar to Lagrange multipliers). - The dual objective collects the best linear lower bounds on the primal cost.</p>"},{"location":"appendices/160_conjugates/#d6-weak-and-strong-duality","title":"D.6 Weak and Strong Duality","text":"<ul> <li> <p>Weak duality: For any \\(x,y\\),      So the dual value always underestimates the primal value.</p> </li> <li> <p>Strong duality:   If \\(f,g\\) are closed convex and a mild constraint qualification holds (e.g. Slater\u2019s condition \u2014 existence of strictly feasible \\(x\\)), then    </p> </li> </ul> <p>At the optimum:  These are the Fenchel\u2013KKT conditions, directly linking primal and dual subgradients.</p>"},{"location":"appendices/160_conjugates/#d7-illustrative-examples","title":"D.7 Illustrative Examples","text":""},{"location":"appendices/160_conjugates/#a-linear-programming","title":"(a) Linear Programming","text":"<p>Primal:  </p> <p>Take \\(f(x) = c^\\top x + \\delta_{\\{x\\ge0\\}}(x)\\), \\(g(z)=\\delta_{\\{z=b\\}}(z)\\).</p> <p>Then  </p> <p>Dual:  which is the standard LP dual.</p>"},{"location":"appendices/160_conjugates/#b-quadratic-set-constraint","title":"(b) Quadratic + Set Constraint","text":"<p>Primal:  </p> <p>Then  so the dual is  Optimality gives \\(x^*=y^*\\), the projection condition in Euclidean geometry.</p>"},{"location":"appendices/160_conjugates/#d8-practical-significance","title":"D.8 Practical Significance","text":"Area How Fenchel Duality Appears Optimization theory Derives general dual problems beyond inequality constraints. Algorithm design Basis for primal\u2013dual and splitting methods (ADMM, Chambolle\u2013Pock, Mirror Descent). Geometry Dual problem finds the \u201cbest supporting hyperplane\u201d to the primal epigraph. Machine Learning Loss\u2013regularizer pairs (hinge \u2194 clipped loss, logistic \u2194 log-sum-exp) often form conjugate pairs. Proximal operators Linked via Moreau identity:  \\(\\mathrm{prox}_{f^*}(y) = y - \\mathrm{prox}_f(y)\\)."},{"location":"appendices/160_conjugates/#d9-conceptual-unification","title":"D.9 Conceptual Unification","text":"<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p> <ul> <li>From geometry: support functions, projections, subgradients (Appendices B\u2013C).  </li> <li>From analysis: inequalities like Fenchel\u2019s and Jensen\u2019s (Appendix A).  </li> <li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7\u20138).  </li> <li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9\u201310).</li> </ul> <p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>"},{"location":"appendices/160_conjugates/#d10-summary-and-takeaways","title":"D.10 Summary and Takeaways","text":"<ul> <li>The convex conjugate \\(f^*\\) expresses \\(f\\) through its linear support planes.  </li> <li>The Fenchel\u2013Young inequality connects primal variables and dual slopes.  </li> <li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li> <li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li> <li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li> </ul> <p>Further Reading</p> <ul> <li>Rockafellar, R. T. (1970). Convex Analysis. Princeton UP.  </li> <li>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization, Chs. 3 &amp; 5.  </li> <li>Bauschke, H. H., &amp; Combettes, P. L. (2017). Convex Analysis and Monotone Operator Theory.  </li> <li>Hiriart-Urruty, J.-B., &amp; Lemar\u00e9chal, C. (2001). Fundamentals of Convex Analysis.  </li> </ul>"},{"location":"appendices/170_probability/","title":"Appendix E - Convexity in Probability and Statistics (Advanced)","text":""},{"location":"appendices/170_probability/#appendix-e-convexity-in-probability-and-statistics","title":"Appendix E : Convexity in Probability and Statistics","text":"<p>Convex analysis is not just geometry and optimization \u2014 it is deeply woven into probability, statistics, and information theory. Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p> <p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>"},{"location":"appendices/170_probability/#e1-convexity-of-expectations","title":"E.1 Convexity of Expectations","text":"<p>Let \\(f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}\\) be convex and \\(X\\) a random vector. Then by Jensen\u2019s inequality (Appendix A):</p> \\[ f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]. \\]"},{"location":"appendices/170_probability/#consequences","title":"Consequences","text":"<ul> <li>Expectations preserve convexity:   if each \\(f(\\cdot,\\xi)\\) is convex, then \\(F(x)=\\mathbb{E}_\\xi[f(x,\\xi)]\\) is convex.</li> <li>Stochastic objectives in ML \u2014 e.g. expected loss \\(\\mathbb{E}_{(a,b)}[\\ell(a^\\top x,b)]\\) \u2014 are convex when the sample-wise loss is convex.</li> </ul> <p>Hence almost all empirical risk minimization problems are discrete approximations of convex expectations.</p>"},{"location":"appendices/170_probability/#e2-convexity-of-log-partition-and-moment-generating-functions","title":"E.2 Convexity of Log-Partition and Moment-Generating Functions","text":"<p>For a random variable \\(X\\), the moment-generating function (MGF) and cumulant-generating function (CGF) are</p> \\[ M_X(t)=\\mathbb{E}[e^{tX}], \\qquad K_X(t)=\\log M_X(t). \\] <p>Fact: \\(K_X(t)\\) is always convex in \\(t\\).</p> <p>Reason: \\(K_X''(t)=\\mathrm{Var}_t(X)\\ge0\\); variance is nonnegative.  </p>"},{"location":"appendices/170_probability/#implications","title":"Implications","text":"<ul> <li>\\(K_X(t)\\) acts as a convex \u201cpotential\u201d controlling exponential families.</li> <li>The log-partition function in statistics,      is convex in \\(\\theta\\) (strictly convex for full exponential families).</li> <li>Its gradient gives the mean parameter: \\(\\nabla A(\\theta)=\\mathbb{E}_\\theta[T(X)]\\).</li> </ul> <p>Thus convexity of \\(A\\) guarantees a one-to-one mapping between natural and mean parameters \u2014 a foundation of exponential-family inference.</p>"},{"location":"appendices/170_probability/#e3-exponential-families-and-dual-convexity","title":"E.3 Exponential Families and Dual Convexity","text":"<p>An exponential-family density has the form  </p> <p>Properties:</p> <ol> <li>\\(A(\\theta)\\) is convex, smooth, and serves as a potential function.</li> <li>Its convex conjugate \\(A^*(\\mu)\\) defines the entropy of the family:        where \\(H\\) is the Shannon entropy of the distribution with mean \\(\\mu\\).</li> </ol> <p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>"},{"location":"appendices/170_probability/#e4-convex-divergences-and-information-measures","title":"E.4 Convex Divergences and Information Measures","text":""},{"location":"appendices/170_probability/#a-kullbackleibler-kl-divergence","title":"(a) Kullback\u2013Leibler (KL) Divergence","text":"<p>For densities \\(p,q\\),  </p> <ul> <li>\\(D_{\\mathrm{KL}}\\) is jointly convex in \\((p,q)\\).  </li> <li>Proof: the function \\((u,v)\\mapsto u\\log(u/v)\\) is convex on \\(\\mathbb{R}_+^2\\).  </li> <li>Consequently, mixtures of distributions cannot increase KL divergence \u2014 a key fact in variational inference and EM.</li> </ul>"},{"location":"appendices/170_probability/#b-bregman-divergences","title":"(b) Bregman Divergences","text":"<p>Given a differentiable convex \\(\\phi\\), define  KL divergence is a Bregman divergence for \\(\\phi(p)=\\sum_i p_i\\log p_i\\). Thus information-theoretic distances are geometric shadows of convex functions.</p>"},{"location":"appendices/170_probability/#c-f-divergences","title":"(c) f-Divergences","text":"<p>A general convex generator \\(f\\) with \\(f(1)=0\\) yields  Convexity of \\(f\\) \u21d2 convexity of \\(D_f\\). Common choices recover KL, \u03c7\u00b2, Hellinger, and Jensen\u2013Shannon divergences.</p>"},{"location":"appendices/170_probability/#e5-convex-loss-functions-in-statistics-and-machine-learning","title":"E.5 Convex Loss Functions in Statistics and Machine Learning","text":"<p>Convexity ensures estimators are globally optimal and algorithms converge.</p> Setting Loss / Negative Log-Likelihood Convexity Gaussian noise \\(\\tfrac12\\|Ax-b\\|_2^2\\) quadratic, strongly convex Laplace noise \\(\\|Ax-b\\|_1\\) convex, nonsmooth Logistic regression \\(\\log(1+e^{-y a^\\top x})\\) convex, smooth Poisson regression \\(e^{a^\\top x}-y a^\\top x\\) convex, exponential Huber loss piecewise quadratic/linear convex, robust <p>Convexity of the negative log-likelihood follows from convexity of the log-partition function \\(A(\\theta)\\) in exponential families.</p>"},{"location":"appendices/170_probability/#e6-convexity-and-bayesian-inference","title":"E.6 Convexity and Bayesian Inference","text":"<p>In Bayesian inference, convexity appears in:</p> <ul> <li> <p>Log-concave posteriors:   If the likelihood and prior are log-concave, the posterior \\(p(x|y)\\propto \\exp(-f(x))\\) is also log-concave \u21d2 \\(\\log p(x|y)\\) concave, \\(f(x)\\) convex.</p> </li> <li> <p>MAP estimation:   Maximizing \\(\\log p(x|y)\\) \u2261 minimizing a convex function when \\(p(x|y)\\) is log-concave \u21d2 global optimum guaranteed.</p> </li> <li> <p>Variational inference:   The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).   Optimizing it is equivalent to minimizing a convex divergence.</p> </li> </ul> <p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>"},{"location":"appendices/170_probability/#e7-statistical-risk-and-convex-surrogates","title":"E.7 Statistical Risk and Convex Surrogates","text":"<p>Convex surrogate losses replace nonconvex 0\u20131 loss with convex approximations:</p> <ul> <li>Hinge loss (\\(\\max(0,1-y a^\\top x)\\)) \u2192 support-vector machines.  </li> <li>Logistic loss \u2192 probabilistic classification (cross-entropy).  </li> <li>Exponential loss \u2192 AdaBoost.</li> </ul> <p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p>"},{"location":"appendices/180_subgradient_methods/","title":"Appendix F - Subgradient Method and Variants (Advanced)","text":""},{"location":"appendices/180_subgradient_methods/#appendix-f-subgradient-method-derivation-geometry-and-convergence","title":"Appendix F: Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>This appendix presents the subgradient method\u2014the fundamental algorithm for minimizing nonsmooth convex functions. It generalizes gradient descent to functions such as the \\(\\ell_1\\) norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>"},{"location":"appendices/180_subgradient_methods/#f1-problem-setup","title":"F.1 Problem Setup","text":"<p>We consider</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but possibly nondifferentiable and \\(\\mathcal{X}\\) is a convex feasible set.</p>"},{"location":"appendices/180_subgradient_methods/#f2-subgradients-and-geometry","title":"F.2 Subgradients and Geometry","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) satisfies</p> \\[ f(y) \\ge f(x_t) + \\langle g_t,\\, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>If \\(f\\) is differentiable, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).  </li> <li>At a nonsmooth point (e.g. \\(|x|\\) at \\(x=0\\)), \\(\\partial f(x_t)\\) is a set of supporting slopes.  </li> <li>Each subgradient defines a supporting hyperplane below the graph of \\(f\\).</li> </ul> <p>Hence a subgradient gives a descent direction even when \\(f\\) lacks a unique gradient.</p>"},{"location":"appendices/180_subgradient_methods/#f3-update-rule-and-projection-view","title":"F.3 Update Rule and Projection View","text":"<p>The projected subgradient step is</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}}\\!\\big(x_t - \\eta_t g_t\\big), \\] <p>where - \\(g_t \\in \\partial f(x_t)\\), - \\(\\eta_t&gt;0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) projects onto \\(\\mathcal{X}\\).</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\), projection disappears:  </p> <p>Geometric view: move in a subgradient direction, then project back to feasibility. The method \u201cslides\u201d along the edges of \\(f\\)\u2019s epigraph.</p>"},{"location":"appendices/180_subgradient_methods/#f4-distance-analysis","title":"F.4 Distance Analysis","text":"<p>Let \\(x^\\star\\) be an optimal solution. Expanding the squared distance:</p> \\[ \\|x_{t+1}-x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t\\langle g_t, x_t - x^\\star\\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity,  </p> <p>Substitute to get</p> \\[ \\|x_{t+1}-x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t\\big(f(x_t)-f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"appendices/180_subgradient_methods/#f5-bounding-suboptimality","title":"F.5 Bounding Suboptimality","text":"<p>Rearranging:</p> \\[ f(x_t)-f(x^\\star) \\le \\frac{\\|x_t-x^\\star\\|^2 - \\|x_{t+1}-x^\\star\\|^2}{2\\eta_t} + \\frac{\\eta_t}{2}\\|g_t\\|^2. \\] <p>This shows a trade-off:</p> <ul> <li>Large \\(\\eta_t\\) \u2192 faster steps but higher error term.  </li> <li>Small \\(\\eta_t\\) \u2192 more precise but slower progress.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f6-convergence-rate","title":"F.6 Convergence Rate","text":"<p>Assume \\(\\|g_t\\| \\le G\\). Summing over \\(t=0,\\dots,T-1\\):</p> \\[ \\sum_{t=0}^{T-1}\\!\\big(f(x_t)-f(x^\\star)\\big) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Define \\(\\bar{x}_T = \\tfrac{1}{T}\\sum_{t=0}^{T-1} x_t\\). By convexity of \\(f\\),</p> \\[ f(\\bar{x}_T)-f(x^\\star) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing \\(\\eta_t = \\tfrac{R}{G\\sqrt{T}}\\) with \\(R=\\|x_0-x^\\star\\|\\) yields</p> <p>  i.e. a sublinear rate \\(O(1/\\sqrt{T})\\).</p>"},{"location":"appendices/180_subgradient_methods/#f7-interpretation-and-practice","title":"F.7 Interpretation and Practice","text":"<ul> <li>Works for any convex function, smooth or not.  </li> <li>Converges slower than smooth-gradient methods (\\(O(1/T)\\) or linear), but applies more generally.  </li> <li>Step size schedule is crucial: \\(\\eta_t \\!\\downarrow 0\\) for convergence, or fixed \\(\\eta\\) for steady error.  </li> <li>Averaging \\(\\bar{x}_T\\) improves stability.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#typical-ml-uses","title":"Typical ML Uses","text":"Model Objective Nonsmooth Term LASSO \\(\\tfrac12\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1\\) \\(\\ell_1\\) penalty SVM \\(\\tfrac12\\|w\\|^2 + C\\sum_i \\max(0,1-y_i w^\\top x_i)\\) hinge loss Robust regression $\\sum_i a_i^\\top x - b_i Neural nets \\(\\|w\\|_1\\) or ReLU activations piecewise linear"},{"location":"appendices/180_subgradient_methods/#f8-beyond-basic-subgradients","title":"F.8 Beyond Basic Subgradients","text":"<p>Many advanced methods refine or accelerate the basic idea:</p> <ul> <li>Stochastic subgradients: sample-based updates for large-scale ML.  </li> <li>Mirror descent: adapt geometry via Bregman divergences.  </li> <li>Proximal methods: replace step with proximal operator (see Appendix B).  </li> <li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f9-summary","title":"F.9 Summary","text":"<ul> <li>Subgradients generalize gradients to nondifferentiable convex functions.  </li> <li>The projected subgradient method provides a universal, robust minimization algorithm.  </li> <li>Achieves \\(O(1/\\sqrt{T})\\) convergence under bounded subgradients.  </li> <li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9\u201310.</li> </ul>"},{"location":"appendices/190_proximal/","title":"Appendix G - Proximal Operators","text":""},{"location":"appendices/190_proximal/#appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization","title":"Appendix G | Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>Many convex optimization problems involve constraints or nonsmooth penalties. This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>"},{"location":"appendices/190_proximal/#g1-problem-setup","title":"G.1 Problem Setup","text":"<p>We wish to minimize a convex, differentiable function \\( f(x) \\) subject to a convex feasible set \\( \\mathcal{X} \\subseteq \\mathbb{R}^n \\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>A plain gradient step,</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>may leave \\( x_{t+1} \\notin \\mathcal{X} \\). We fix this by projecting the iterate back into the feasible region.</p>"},{"location":"appendices/190_proximal/#g2-projection-operator","title":"G.2 Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2. \\] <p>Hence, the projected gradient descent update is</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\]"},{"location":"appendices/190_proximal/#geometric-insight","title":"Geometric Insight","text":"<ul> <li>Take a descent step possibly outside the feasible set.  </li> <li>Project back to the closest feasible point.  </li> <li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li> </ul> <p>Example \u2014 Euclidean ball: If \\( \\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\} \\), then</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)}. \\] <ul> <li>Inside the ball \u2192 unchanged.  </li> <li>Outside \u2192 scaled back to the boundary.</li> </ul>"},{"location":"appendices/190_proximal/#g3-from-projections-to-proximal-operators","title":"G.3 From Projections to Proximal Operators","text":"<p>Projections handle explicit constraints, but many problems use implicit penalties \u2014 e.g. sparsity (\\(\\|x\\|_1\\)), total variation, or nonnegativity penalties.</p> <p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>"},{"location":"appendices/190_proximal/#definition","title":"Definition","text":"<p>For a convex (possibly nondifferentiable) function \\( g(x) \\),</p> <p>  where \\( \\lambda &gt; 0 \\) balances regularization vs. proximity.</p>"},{"location":"appendices/190_proximal/#interpretation","title":"Interpretation","text":"<ul> <li>The quadratic term \\( \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\) keeps \\(x\\) close to \\(y\\).  </li> <li>The function \\( g(x) \\) encourages structure (sparsity, smoothness, feasibility).  </li> <li>Small \\(\\lambda\\): conservative correction; large \\(\\lambda\\): stronger regularization.</li> </ul> <p>The proximal step acts as a soft correction after a gradient step.</p>"},{"location":"appendices/190_proximal/#g4-projection-as-a-special-case","title":"G.4 Projection as a Special Case","text":"<p>Define the indicator function of a convex set \\(\\mathcal{X}\\):</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X}, \\\\[4pt] +\\infty, &amp; x \\notin \\mathcal{X}. \\end{cases} \\] <p>Substitute \\(g(x)=I_{\\mathcal{X}}(x)\\) into the proximal definition:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\Big) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y). \\] <p>\u2705 Projection is just a proximal operator for an indicator function.</p>"},{"location":"appendices/190_proximal/#g5-proximal-gradient-method","title":"G.5 Proximal Gradient Method","text":"<p>When minimizing a composite convex objective  where \\(f\\) is smooth and \\(g\\) convex (possibly nonsmooth), the proximal gradient method updates:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\] <ul> <li>The gradient step reduces the smooth part \\(f(x)\\).  </li> <li>The proximal step enforces structure via \\(g(x)\\). This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li> </ul>"},{"location":"appendices/190_proximal/#g6-example-proximal-operator-of-the-ell_1-norm","title":"G.6 Example: Proximal Operator of the \\(\\ell_1\\)-Norm","text":"<p>We seek</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda\\|x\\|_1 + \\tfrac{1}{2}\\|x - y\\|^2 \\right). \\]"},{"location":"appendices/190_proximal/#step-1-coordinate-separation","title":"Step 1. Coordinate Separation","text":"<p>The problem is separable across coordinates:  so each coordinate solves  </p>"},{"location":"appendices/190_proximal/#step-2-subgradient-optimality","title":"Step 2. Subgradient Optimality","text":"<p>Optimality condition:  Thus,  </p>"},{"location":"appendices/190_proximal/#step-3-case-analysis","title":"Step 3. Case Analysis","text":"Case Condition Solution \\(x^\\star&gt;0\\) \\(y&gt;\\lambda\\) \\(x^\\star = y - \\lambda\\) \\(x^\\star&lt;0\\) \\(y&lt;-\\lambda\\) \\(x^\\star = y + \\lambda\\) \\(x^\\star=0\\) ( y"},{"location":"appendices/190_proximal/#step-4-compact-form","title":"Step 4. Compact Form","text":"\\[ \\boxed{ \\text{prox}_{\\lambda|\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda,\\, 0) } \\] <p>This is the soft-thresholding operator.</p>"},{"location":"appendices/190_proximal/#step-5-vector-case","title":"Step 5. Vector Case","text":"<p>For \\(y \\in \\mathbb{R}^n\\),</p> \\[ \\big(\\text{prox}_{\\lambda\\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i)\\cdot\\max(|y_i| - \\lambda, 0). \\] <p>Each coordinate is independently shrunk toward zero \u2014 producing sparse solutions.</p>"},{"location":"appendices/190_proximal/#step-6-interpretation","title":"Step 6. Interpretation","text":"<ul> <li>Coordinates with \\(|y_i| \\le \\lambda\\) \u2192 set to zero (promotes sparsity).  </li> <li>Coordinates with \\(|y_i| &gt; \\lambda\\) \u2192 shrink by \\(\\lambda\\).  </li> <li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li> </ul>"},{"location":"appendices/190_proximal/#g7-geometry-and-connection-to-algorithms","title":"G.7 Geometry and Connection to Algorithms","text":"<ul> <li>Projection = nearest feasible point \u2192 handles hard constraints.  </li> <li>Proximal operator = nearest structured point \u2192 handles soft regularization.  </li> <li>Proximal gradient = combines both, yielding algorithms like:</li> <li>ISTA / FISTA (sparse recovery, LASSO),</li> <li>Projected gradient (feasibility),</li> <li>ADMM (splitting into subproblems).</li> </ul> <p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>"},{"location":"appendices/190_proximal/#g8-summary","title":"G.8 Summary","text":"<ul> <li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li> <li>Projection is a special case of the proximal operator for an indicator function.  </li> <li>Proximal mappings handle nonsmooth regularizers (e.g., \\(\\ell_1\\)-norm).  </li> <li>The proximal gradient method unifies constrained and regularized optimization.  </li> <li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li> </ul>"},{"location":"appendices/200_mirror/","title":"Appendix H - Mirror Descent and Bregman Geometry","text":""},{"location":"appendices/200_mirror/#appendix-h-mirror-descent-and-bregman-geometry","title":"Appendix H: Mirror Descent and Bregman Geometry","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry. In many structured domains\u2014such as probability simplices or sparse models\u2014Euclidean updates can destroy problem structure or cause instability.  </p> <p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence. It performs gradient-like updates in a dual space, respecting the intrinsic geometry of the domain.</p>"},{"location":"appendices/200_mirror/#h1-motivation-and-limitations-of-euclidean-gd","title":"H.1 Motivation and Limitations of Euclidean GD","text":"<p>Standard GD update:  assumes Euclidean distance  </p> <p>This works well in \\(\\mathbb{R}^n\\) without structure, but fails to respect constraints or sparsity.</p> <p>In practice:</p> <ul> <li>Many parameters are nonnegative or normalized (probabilities, weights).  </li> <li>Euclidean steps can violate constraints or zero out coordinates.  </li> <li>The \u201cflat\u201d \\(\\ell_2\\) geometry treats all directions equally.</li> </ul> <p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the metric via a mirror map.</p>"},{"location":"appendices/200_mirror/#h2-geometry-in-optimization","title":"H.2 Geometry in Optimization","text":"<p>The \u201csteepest descent\u201d direction depends on the notion of distance. GD implicitly minimizes a linearized loss plus a Euclidean proximity term.</p> Scenario Natural Constraint Appropriate Geometry Probability vectors \\(x_i\\ge0, \\sum_i x_i=1\\) KL / entropy geometry Sparse models \\(\\|x\\|_1\\)-structured \\(\\ell_1\\) geometry Online learning multiplicative updates log-space geometry <p>Using Euclidean projections in these domains can cause:</p> <ul> <li>abrupt projection onto boundaries,</li> <li>loss of positivity or sparsity,</li> <li>geometric inconsistency.</li> </ul>"},{"location":"appendices/200_mirror/#h3-mirror-descent-framework","title":"H.3 Mirror Descent Framework","text":"<p>Let \\(\\psi(x)\\) be a mirror map \u2014 a strictly convex, differentiable potential encoding the geometry.</p> <p>Define the dual coordinate:  and its inverse mapping through the convex conjugate \\(\\psi^*\\):  </p>"},{"location":"appendices/200_mirror/#bregman-divergence","title":"Bregman Divergence","text":"<p>The geometry is quantified by the Bregman divergence:  </p> <ul> <li>Measures how nonlinear \\(\\psi\\) is between \\(x\\) and \\(y\\).  </li> <li>When \\(\\psi(x)=\\tfrac12\\|x\\|_2^2\\), \\(D_\\psi\\) becomes \\(\\tfrac12\\|x-y\\|_2^2\\).  </li> <li>When \\(\\psi(x)=\\sum_i x_i\\log x_i\\), \\(D_\\psi\\) becomes KL divergence.</li> </ul>"},{"location":"appendices/200_mirror/#h4-mirror-descent-update-rule","title":"H.4 Mirror Descent Update Rule","text":"<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:  </p> <p>Equivalent dual-space form:  </p> <p>\u2705 MD is gradient descent in dual coordinates, where distances are measured by \\(D_\\psi\\) instead of \\(\\|x-y\\|_2\\).</p>"},{"location":"appendices/200_mirror/#h5-comparing-gd-projected-gd-and-md","title":"H.5 Comparing GD, Projected GD, and MD","text":"Method Update Rule Geometry Comments Gradient Descent \\(x - \\eta\\nabla f\\) Euclidean may leave feasible set Projected GD \\(\\text{Proj}(x - \\eta\\nabla f)\\) Euclidean + projection can cause discontinuous jumps Mirror Descent \\(\\arg\\min_x \\langle\\nabla f, x - x_t\\rangle + \\frac{1}{\\eta}D_\\psi(x\\|x_t)\\) Bregman smooth, structure-preserving"},{"location":"appendices/200_mirror/#h6-simplex-example-kl-geometry","title":"H.6 Simplex Example (KL Geometry)","text":"<p>Let \\(x\\in\\Delta^2=\\{x\\ge0, x_1+x_2=1\\}\\), objective \\(f(x)=x_1^2+2x_2\\), \\(\\eta=0.3\\).</p>"},{"location":"appendices/200_mirror/#euclidean-gd-projection","title":"Euclidean GD + Projection","text":"<ol> <li>\\(\\nabla f=(2x_1,2)=(1,2)\\),  </li> <li>\\(y=x-\\eta\\nabla f=(0.2,-0.1)\\),  </li> <li>Project \u2192 \\(x_{new}=(1,0)\\).</li> </ol> <p>\u2192 Projection kills one coordinate \u21d2 lost smoothness.</p>"},{"location":"appendices/200_mirror/#mirror-descent-with-negative-entropy","title":"Mirror Descent with Negative Entropy","text":"<p>Mirror map \\(\\psi(x)=\\sum_i x_i\\log x_i\\). Update:  Gives \\(x\\approx(0.57,0.43)\\) \u2014 smooth, positive, stays in simplex.</p> <p>MD follows the manifold of the simplex naturally\u2014no harsh projection.</p>"},{"location":"appendices/200_mirror/#h7-choosing-the-mirror-map","title":"H.7 Choosing the Mirror Map","text":"Mirror Map \\(\\psi(x)\\) Bregman Divergence \\(D_\\psi\\) Typical Domain / Application \\(\\tfrac12\\|x\\|_2^2\\) Euclidean distance unconstrained \\(\\mathbb{R}^n\\) \\(\\sum_i x_i\\log x_i\\) KL divergence simplex, probabilities \\(\\|x\\|_1\\) or variants \\(\\ell_1\\) geometry sparse models log-barrier \\(\\sum_i -\\log x_i\\) barrier divergence positive orthant <p>Mirror maps act as design choices defining the optimization geometry.</p>"},{"location":"appendices/200_mirror/#h8-practical-remarks","title":"H.8 Practical Remarks","text":"<p>When to prefer Mirror Descent:</p> <ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates desired</li> <li>Avoiding discontinuous projections</li> </ul> <p>Computational notes:</p> <ul> <li>Some \\(\\psi\\) yield closed-form updates (e.g. multiplicative weights).  </li> <li>Works with adaptive or momentum step-size schemes.  </li> <li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li> </ul>"},{"location":"appendices/200_mirror/#h9-convergence-at-a-glance","title":"H.9 Convergence at a Glance","text":"<p>For convex \\(f\\) with bounded gradients \\(\\|\\nabla f\\|\\le G\\) and strong convex mirror map \\(\\psi\\), Mirror Descent achieves the same sublinear rate as projected subgradient methods:  but with improved geometry-adapted constants that exploit curvature of \\(\\psi\\).</p>"},{"location":"appendices/300_matrixfactorization/","title":"Appendix I - Matrix Factorization","text":""},{"location":"appendices/300_matrixfactorization/#numerical-linear-algebra-for-convex-optimization","title":"Numerical Linear Algebra for Convex Optimization","text":"<p>Numerical linear algebra is the computational foundation of convex optimization. Every modern optimization algorithm \u2014 from Newton\u2019s method to interior-point or proximal algorithms \u2014 ultimately requires solving a structured linear system:  where \\(H\\) may represent a Hessian, a normal equations matrix, or a KKT (Karush\u2013Kuhn\u2013Tucker) system.</p> <p>In practice, we never compute \\(H^{-1}\\) directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>"},{"location":"appendices/300_matrixfactorization/#1-why-linear-algebra-matters-in-convex-optimization","title":"1. Why Linear Algebra Matters in Convex Optimization","text":"<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p> <ul> <li> <p>Newton\u2019s method:    </p> </li> <li> <p>Interior-point methods (KKT systems):</p> </li> </ul> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\[3pt] \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -r_d \\\\[3pt] -r_p \\end{bmatrix} \\] <ul> <li>Least-squares problems: \\(A^T A x = A^T b\\)</li> </ul> <p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>"},{"location":"appendices/300_matrixfactorization/#2-the-matrix-factorization-toolbox","title":"2. The Matrix Factorization Toolbox","text":"<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure. They enable efficient triangular solves instead of direct inversion.</p> Factorization Applies To Form Common Use Key Notes LU Any nonsingular matrix \\(A = L U\\) General linear systems Requires pivoting for stability QR Any (rectangular) matrix \\(A = Q R\\) Least-squares Orthogonal, stable Cholesky Symmetric positive definite \\(A = L L^T\\) SPD systems, normal equations Fastest for SPD \\(LDL^T\\) Symmetric indefinite \\(A = L D L^T\\) KKT systems Handles indefiniteness Eigen Symmetric/Hermitian \\(A = Q \\Lambda Q^T\\) Curvature, convexity checks Diagonalizes \\(A\\) SVD Any matrix \\(A = U \\Sigma V^T\\) Rank, conditioning, pseudoinverse Most stable, expensive <p>Each factorization corresponds to a numerically preferred strategy for certain classes of problems.</p>"},{"location":"appendices/300_matrixfactorization/#3-lu-factorization-the-general-purpose-workhorse","title":"3. LU Factorization \u2014 The General-Purpose Workhorse","text":"<p>Form:  where \\(P\\) is a permutation matrix ensuring stability.</p> <ul> <li>Used for: General linear systems, nonsymmetric matrices.</li> <li>Cost: \\(\\approx \\tfrac{2}{3}n^3\\) (dense).</li> <li>Stability: Requires partial pivoting (\\(PA=LU\\)) to avoid numerical blow-up.</li> </ul> <p>Example use case:</p> <ul> <li>Solving KKT systems in linear programming (LP simplex tableau).</li> <li>Small dense systems with no symmetry or SPD property.</li> </ul> <p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or \\(LDL^T\\).</p>"},{"location":"appendices/300_matrixfactorization/#4-qr-factorization-orthogonal-and-stable","title":"4. QR Factorization \u2014 Orthogonal and Stable","text":"<p>Form:  </p> <ul> <li>Used for: Least-squares problems      Instead of forming normal equations (\\(A^T A x = A^T b\\)), we solve:    </li> <li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li> </ul> <p>Example use cases:</p> <ul> <li>Linear regression via least squares.</li> <li>ADMM and proximal steps with overdetermined systems.</li> <li>Orthogonal projections in signal processing.</li> </ul> <p>Variants:</p> <ul> <li>Householder QR: numerically robust, used in LAPACK.</li> <li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#5-cholesky-factorization-fastest-for-spd-systems","title":"5. Cholesky Factorization \u2014 Fastest for SPD Systems","text":"<p>Form:  Applicable when \\(A\\) is symmetric positive definite (SPD) \u2014 common in convex problems.</p> <p>Why it\u2019s central: Convexity ensures \\(A \\succeq 0\\). For strictly convex problems, \\(A \\succ 0\\) and Cholesky is the most efficient and stable method.</p> <p>Cost: \\(\\tfrac{1}{3}n^3\\) operations \u2014 half of LU.</p> <p>Example use cases:</p> <ul> <li>Newton\u2019s method on unconstrained convex functions.</li> <li>Solving normal equations \\(A^T A x = A^T b\\).</li> <li>QP subproblems and ridge regression.</li> </ul> <p>Implementation detail: No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>"},{"location":"appendices/300_matrixfactorization/#6-ldlt-factorization-for-indefinite-symmetric-systems","title":"6. LDL\u1d40 Factorization \u2014 For Indefinite Symmetric Systems","text":"<p>Form:  where \\(D\\) is block diagonal (1\u00d71 or 2\u00d72 blocks), and \\(L\\) is unit lower triangular.</p> <p>Used when \\(A\\) is symmetric but not SPD (e.g., KKT systems).</p> <p>Example use cases:</p> <ul> <li> <p>Interior-point methods for QPs and SDPs:    </p> </li> <li> <p>Equality-constrained least-squares.</p> </li> <li>Sparse symmetric indefinite systems in primal-dual algorithms.</li> </ul> <p>Algorithmic note: Uses Bunch\u2013Kaufman pivoting to maintain numerical stability. In practice, LDL\u1d40 is used with sparse reordering and partial elimination.</p>"},{"location":"appendices/300_matrixfactorization/#7-block-systems-and-the-schur-complement","title":"7. Block Systems and the Schur Complement","text":"<p>Many KKT or structured systems naturally appear in block form:  </p> <p>Assuming \\(A_{11}\\) is invertible:</p> <ol> <li>Eliminate \\(x_1\\):     </li> <li>Substitute into the second block:     </li> </ol> <p>The matrix  is the Schur complement of \\(A_{11}\\) in \\(A\\).</p>"},{"location":"appendices/300_matrixfactorization/#schur-complement-in-optimization","title":"Schur Complement in Optimization","text":"<ul> <li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li> <li>Preserves symmetry and often positive definiteness.</li> <li>Foundation of block elimination and reduced Hessian methods.</li> </ul> <p>Example use cases:</p> <ul> <li>Interior-point Newton systems (eliminate \\(\\Delta x\\) to get a system in \\(\\Delta \\lambda\\)).</li> <li>Partial elimination in sequential quadratic programming (SQP).</li> <li>Covariance conditioning and Gaussian marginalization.</li> </ul> <p>Numerical caution: Never form \\(A_{11}^{-1}\\) explicitly \u2014 use triangular solves via Cholesky or LU.</p>"},{"location":"appendices/300_matrixfactorization/#8-block-elimination-algorithm","title":"8. Block Elimination Algorithm","text":"<p>Given a nonsingular \\(A_{11}\\):</p> <ol> <li>Compute \\(A_{11}^{-1}A_{12}\\) and \\(A_{11}^{-1}b_1\\) by solving triangular systems.</li> <li>Form \\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\\), \\(\\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\\).</li> <li>Solve \\(Sx_2 = \\tilde{b}\\).</li> <li>Recover \\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\\).</li> </ol> <p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p> <p>Example use case:</p> <ul> <li>Partitioned least-squares with fixed and variable parameters.</li> <li>Constrained optimization where some variables can be analytically eliminated.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#9-structured-plus-low-rank-matrices","title":"9. Structured Plus Low-Rank Matrices","text":"<p>Suppose we need to solve:  where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{n \\times n}\\) is structured or easily invertible (e.g., diagonal or sparse),</li> <li>\\(B \\in \\mathbb{R}^{n \\times p}\\), \\(C \\in \\mathbb{R}^{p \\times n}\\) are low rank.</li> </ul> <p>This situation arises when updating an existing system with a small modification.</p>"},{"location":"appendices/300_matrixfactorization/#block-reformulation","title":"Block Reformulation","text":"<p>Introduce \\(y = Cx\\), yielding:</p> <p>$$   =</p> <p> . $$</p> <p>Block elimination gives:  </p>"},{"location":"appendices/300_matrixfactorization/#matrix-inversion-lemma-woodbury-identity","title":"Matrix Inversion Lemma (Woodbury Identity)","text":"<p>If \\(A\\) and \\(A + BC\\) are nonsingular:  </p> <p>Example use cases:</p> <ul> <li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li> <li>Ridge regression / kernel methods: low-rank updates to \\((X^T X + \\lambda I)^{-1}\\).</li> <li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li> </ul> <p>Numerical note: Avoid explicit inversion; use solves with \\(A\\) and small dense matrices.</p>"},{"location":"appendices/300_matrixfactorization/#10-conditioning-stability-and-sparsity","title":"10. Conditioning, Stability, and Sparsity","text":""},{"location":"appendices/300_matrixfactorization/#conditioning","title":"Conditioning","text":"<ul> <li>Condition number: \\(\\kappa(A) = |A||A^{-1}|\\) measures sensitivity to perturbations.</li> <li>High \\(\\kappa(A)\\) \u21d2 round-off errors amplified \u21d2 ill-conditioning.</li> <li>Regularization (adding \\(\\lambda I\\)) improves numerical stability.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#stability","title":"Stability","text":"<ul> <li>Orthogonal transformations (QR, SVD) are backward stable.</li> <li>LU needs partial pivoting.</li> <li>LDL\u1d40 needs symmetric pivoting (Bunch\u2013Kaufman).</li> <li>Cholesky is stable for SPD matrices.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#sparsity-and-fill-in","title":"Sparsity and Fill-In","text":"<ul> <li>Large convex solvers exploit sparse Cholesky / LDL\u1d40.</li> <li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li> <li>Symbolic factorization determines the pattern before numeric factorization.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#11-iterative-solvers-and-preconditioning","title":"11. Iterative Solvers and Preconditioning","text":"<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>"},{"location":"appendices/300_matrixfactorization/#common-iterative-methods","title":"Common Iterative Methods","text":"Method For Description CG SPD systems Uses matrix\u2013vector products; converges in \u2264 n steps MINRES / SYMMLQ Symmetric indefinite Handles KKT and saddle-point systems GMRES / BiCGSTAB Nonsymmetric General-purpose Krylov solvers"},{"location":"appendices/300_matrixfactorization/#preconditioning","title":"Preconditioning","text":"<p>Preconditioners \\(M \\approx A^{-1}\\) improve convergence:</p> <ul> <li>Jacobi (diagonal): \\(M = \\text{diag}(A)^{-1}\\)</li> <li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li> <li>Block preconditioners: use Schur complement approximations for KKT systems</li> </ul> <p>Example use case:</p> <ul> <li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li> <li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#12-eigenvalue-and-svd-decompositions","title":"12. Eigenvalue and SVD Decompositions","text":""},{"location":"appendices/300_matrixfactorization/#eigenvalue-decomposition","title":"Eigenvalue Decomposition","text":"<p>  Reveals curvature, stability, and definiteness:</p> <ul> <li>Convexity \u21d4 \\(\\Lambda \\ge 0\\).</li> <li>Used in semidefinite programming (SDP) and spectral analysis.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>  with \\(\\Sigma = \\text{diag}(\\sigma_i) \\ge 0\\).</p> <p>Applications:</p> <ul> <li>Rank and condition number estimation (\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\)).</li> <li>Low-rank approximation (\\(A_k = U_k \\Sigma_k V_k^T\\)).</li> <li>Pseudoinverse: \\(A^+ = V \\Sigma^{-1} U^T\\).</li> <li>Convex relaxations: nuclear-norm minimization (matrix completion).</li> </ul>"},{"location":"appendices/300_matrixfactorization/#13-computational-complexity-summary","title":"13. Computational Complexity Summary","text":"Factorization Dense Cost Notes LU \\(\\frac{2}{3}n^3\\) Needs pivoting Cholesky \\(\\frac{1}{3}n^3\\) Fastest for SPD QR \\(\\approx \\frac{2}{3}n^3\\) Stable, more memory LDL\u1d40 \\(\\approx \\frac{2}{3}n^3\\) For indefinite SVD \\(\\approx \\frac{4}{3}n^3\\) Most accurate CG / MINRES Variable Depends on condition number and preconditioning <p>Sparse systems reduce cost to roughly \\(O(n^{1.5})\\)\u2013\\(O(n^2)\\) depending on fill-in.</p>"},{"location":"appendices/300_matrixfactorization/#14-example-applications-overview","title":"14. Example Applications Overview","text":"Problem Type Typical Matrix Solver / Factorization Example Unconstrained Newton step SPD Hessian Cholesky Convex quadratic, ridge regression Equality-constrained QP Symmetric indefinite KKT LDL\u1d40 Interior-point QP solver Overdetermined LS Rectangular \\(A\\) QR Linear regression, ADMM subproblem KKT block system Block-symmetric Schur complement Primal-dual method Low-rank correction \\(A + U U^T\\) Woodbury Kalman filter, online update Rank-deficient system Any SVD Matrix completion, regularization Large-scale Hessian SPD CG + preconditioner Logistic regression, large ML models"},{"location":"cheatsheets/20a_cheatsheet/","title":"Optimization Algos - Cheat Sheet","text":""},{"location":"cheatsheets/20a_cheatsheet/#comprehensive-optimization-algorithm-cheat-sheet","title":"Comprehensive Optimization Algorithm Cheat Sheet","text":"<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search. It balances theoretical precision with practical intuition\u2014from gradient-based solvers to black-box evolutionary methods.</p>"},{"location":"cheatsheets/20a_cheatsheet/#how-to-read-this-table","title":"\ud83e\udded How to Read This Table","text":"<p>Each method lists: - Problem Type \u2014 the class of objectives it applies to. - Assumptions \u2014 smoothness, convexity, or structural conditions. - Core Update Rule \u2014 canonical iteration. - Scalability \u2014 computational feasibility. - Per-Iteration Cost \u2014 approximate computational complexity. - Applications \u2014 typical ML or engineering use cases.</p>"},{"location":"cheatsheets/20a_cheatsheet/#first-order-methods","title":"\ud83d\ude80 First-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Gradient Descent (GD) Unconstrained smooth (convex/nonconvex) Differentiable; \\(L\\)-smooth \\(x_{k+1} = x_k - \\eta \\nabla f(x_k)\\) Medium \\(O(nd)\\) Logistic regression, least squares Nesterov\u2019s Accelerated GD Smooth convex (fast rate) Convex, \\(L\\)-smooth \\(y_k = x_k + \\frac{k-1}{k+2}(x_k - x_{k-1})\\); \\(x_{k+1} = y_k - \\eta \\nabla f(y_k)\\) Medium \\(O(nd)\\) Accelerated convex models (Polyak) Heavy-Ball Momentum Unconstrained smooth Differentiable, \\(\\beta \\in (0,1)\\) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k) + \\beta(x_k - x_{k-1})\\) Large \\(O(nd)\\) Deep networks, convex smooth losses Conjugate Gradient (CG) Quadratic or linear systems \\(Ax=b\\) \\(A\\) symmetric positive definite \\(p_{k+1}=r_{k+1}+\\beta_k p_k\\), \\(x_{k+1}=x_k+\\alpha_k p_k\\) Large \\(O(nd)\\) Large-scale least squares, implicit Newton steps Mirror Descent Non-Euclidean geometry Convex; mirror map \\(\\psi\\) strongly convex \\(x_{k+1} = \\nabla \\psi^*(\\nabla \\psi(x_k) - \\eta \\nabla f(x_k))\\) Medium \\(O(nd)\\) Probability simplex, online learning <p>Conjugate Gradient (CG) bridges first- and second-order methods: it achieves exact convergence in at most \\(d\\) steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>"},{"location":"cheatsheets/20a_cheatsheet/#second-order-methods","title":"\u2699\ufe0f Second-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Newton\u2019s Method Smooth convex Twice differentiable; \\(\\nabla^2 f(x)\\) PD \\(x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\\) Small\u2013Medium \\(O(d^3)\\) Logistic regression (IRLS), convex solvers BFGS / L-BFGS Smooth convex Differentiable, approximate Hessian Solve \\(B_k p_k=-\\nabla f(x_k)\\); update \\(B_k\\) via secant rule Medium \\(O(d^2)\\) GLMs, medium ML models Trust-Region Smooth convex/nonconvex Twice differentiable \\(\\min_p \\tfrac{1}{2}p^\\top \\nabla^2 f(x_k)p + \\nabla f(x_k)^\\top p\\) s.t. \\(\\|p\\|\\le\\Delta_k\\) Medium \\(O(d^2)\\) TRPO, physics-based ML"},{"location":"cheatsheets/20a_cheatsheet/#proximal-projected-splitting-methods","title":"\ud83e\uddee Proximal, Projected &amp; Splitting Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Proximal Gradient (ISTA) Composite \\(f=g+h\\) \\(g\\) smooth, \\(h\\) convex \\(x_{k+1}=\\operatorname{prox}_{\\alpha h}(x_k-\\alpha\\nabla g(x_k))\\) Medium \\(O(nd)\\) LASSO, sparse recovery FISTA Same as ISTA Convex, \\(L\\)-smooth \\(g\\) Like ISTA with momentum Medium \\(O(nd)\\) Compressed sensing Projected Gradient (PG) Convex constrained \\(f\\) smooth; easy projection \\(x_{k+1}=\\Pi_C(x_k-\\eta\\nabla f(x_k))\\) Medium \\(O(nd)\\) + projection Box/simplex constraints ADMM Separable convex + linear constraints \\(f,g\\) convex Alternating minimization + dual update Medium \\(O(nd)\\) per block Distributed ML, consensus Majorization\u2013Minimization (MM) Convex/nonconvex $g(x x_k)\\ge f(x)$ $x_{k+1}=\\arg\\min g(x x_k)$ Medium"},{"location":"cheatsheets/20a_cheatsheet/#coordinate-block-methods","title":"\ud83e\udde9 Coordinate &amp; Block Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Coordinate Descent (CD) Separable convex Convex, differentiable Update one coordinate: \\(x_{i}^{k+1}=x_i^k-\\eta\\partial_i f(x^k)\\) Large \\(O(d)\\) LASSO, SVM duals Block Coordinate Descent (BCD) Block separable Convex per block Minimize over \\(x^{(j)}\\) while fixing others Large \\(O(nd_j)\\) Matrix factorization, alternating minimization <p>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</p>"},{"location":"cheatsheets/20a_cheatsheet/#stochastic-mini-batch-methods","title":"\ud83c\udfb2 Stochastic &amp; Mini-Batch Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Stochastic Gradient Descent (SGD) Large-scale / streaming Unbiased stochastic gradients \\(x_{k+1}=x_k-\\eta_t\\nabla f_{i_k}(x_k)\\) Very Large \\(O(bd)\\) Deep learning, online learning Variance-Reduced (SVRG/SAGA/SARAH) Finite-sum convex Smooth, strongly convex \\(v_k=\\nabla f_{i_k}(x_k)-\\nabla f_{i_k}(\\tilde{x})+\\nabla f(\\tilde{x})\\) Large \\(O(bd)\\) Logistic regression, GLMs Adaptive SGD (Adam/RMSProp/Adagrad) Nonconvex stochastic Bounded variance \\(m_k=\\beta_1m_{k-1}+(1-\\beta_1)g_k\\), \\(v_k=\\beta_2v_{k-1}+(1-\\beta_2)g_k^2\\) Very Large \\(O(bd)\\) Neural networks Proximal Stochastic (Prox-SGD / Prox-SAGA) Nonsmooth stochastic \\(f=g+h\\) with prox of \\(h\\) known \\(x_{k+1}=\\operatorname{prox}_{\\eta h}(x_k-\\eta\\widehat{\\nabla g}(x_k))\\) Large \\(O(bd)\\) Sparse online learning"},{"location":"cheatsheets/20a_cheatsheet/#interior-point-augmented-methods","title":"\ud83e\uddf1 Interior-Point &amp; Augmented Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Interior-Point Convex with inequalities Slater\u2019s condition, self-concordant barrier Solve \\(\\min f_0(x)-\\tfrac{1}{t}\\sum_i\\log(-g_i(x))\\) Small\u2013Medium \\(O(d^3)\\) LP, QP, SDP Augmented Lagrangian (ALM) Constrained convex \\(f,g\\) convex; equality constraints \\(L_\\rho(x,\\lambda)=f(x)+\\lambda^T g(x)+\\tfrac{\\rho}{2}\\|g(x)\\|^2\\) Medium \\(O(nd)\\) Penalty methods, PDEs"},{"location":"cheatsheets/20a_cheatsheet/#derivative-free-black-box-optimization","title":"\ud83c\udf10 Derivative-Free &amp; Black-Box Optimization","text":"Method Problem Type Assumptions Core Idea Scalability Cost Applications Nelder\u2013Mead Simplex Low-dimensional, smooth or noisy No gradients; continuous \\(f\\) Maintain simplex of \\(d+1\\) points; reflect\u2013expand\u2013contract\u2013shrink operations Small \\(O(d^2)\\) Parameter tuning, physics models Simulated Annealing Nonconvex, global Stochastic exploration via temperature Random perturbations accepted w.p. \\(\\exp(-\\Delta f/T)\\); \\(T\\downarrow\\) Medium High (many samples) Hyperparameter tuning, design optimization Multi-start Local Search Nonconvex None; relies on restart diversity Run local solver from multiple random inits, pick best result Medium \\(k\\times\\) local solver Avoids local minima; cheap global heuristic Evolutionary Algorithms (EA) Black-box, global Population-based; fitness function only Mutate, select, recombine candidates Large \\(O(Pd)\\) per gen Global optimization, control, AutoML Genetic Algorithms (GA) Combinatorial / continuous Chromosomal encoding of solutions Apply selection, crossover, mutation; evolve over generations Medium\u2013Large \\(O(Pd)\\) Feature selection, neural architecture search Evolution Strategies (ES) Continuous, black-box Gaussian mutation around mean \\(\\theta_{k+1} = \\theta_k + \\eta \\sum_i w_i \\epsilon_i f(\\theta_k+\\sigma \\epsilon_i)\\) Large \\(O(Pd)\\) Reinforcement learning, black-box control Derivative-Free Optimization (DFO) Black-box, noisy \\(f\\) Only function values available Gradient estimated via random perturbations: \\(g\\approx\\frac{f(x+hu)-f(x)}{h}u\\) Medium \\(O(d)\\)\u2013\\(O(d^2)\\) Robotics, policy search, design Black-Box Optimization Framework General No analytical gradients; often stochastic Unified term covering EA, GA, ES, and DFO Medium\u2013Large varies Hyperparameter search, AutoML, reinforcement learning Numerical Encodings Used in GA/EA Represents variables in binary, integer, or floating-point form Choice of encoding impacts mutation/crossover behavior N/A negligible Optimization of mixed or discrete variables <p>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</p>"},{"location":"cheatsheets/20a_cheatsheet/#convergence-complexity-snapshot","title":"\ud83d\udcc8 Convergence &amp; Complexity Snapshot","text":"Method Type Convergence (Convex) Notes Subgradient \\(O(1/\\sqrt{k})\\) Nonsmooth convex Gradient Descent \\(O(1/k)\\) Smooth convex Accelerated Gradient \\(O(1/k^2)\\) Optimal first-order Newton / Quasi-Newton Quadratic / Superlinear Local only Strongly Convex \\((1-\\mu/L)^k\\) Linear rate Variance-Reduced Linear (strongly convex) Finite-sum optimization ADMM / Proximal \\(O(1/k)\\) Composite convex Interior-Point Polynomial time High-accuracy convex Derivative-Free / Heuristics No formal bound Empirical convergence only"},{"location":"cheatsheets/20a_cheatsheet/#practitioner-summary","title":"\ud83e\udde0 Practitioner Summary","text":"Situation Recommended Methods Gradients available, smooth convex Gradient Descent, Nesterov Curvature matters, moderate scale Newton, BFGS, Conjugate Gradient Nonsmooth regularizer Proximal Gradient, ADMM Simple constraints Projected Gradient Large-scale / streaming SGD, Adam, RMSProp Finite-sum convex SVRG, SAGA Online / adaptive Mirror Descent, FTRL No gradients (black-box) DFO, Nelder\u2013Mead, ES, GA Global nonconvex search Simulated Annealing, Multi-starts, Evolutionary Algorithms Distributed / separable ADMM, ALM High-precision convex programs Interior-Point, Trust-Region"},{"location":"cheatsheets/20a_cheatsheet/#notes-on-global-black-box-optimization","title":"\ud83e\udde9 Notes on Global &amp; Black-Box Optimization","text":"<ul> <li>Conjugate Gradient: memory-efficient quasi-second-order method for large convex quadratics.  </li> <li>Nelder\u2013Mead: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li> <li>Simulated Annealing: probabilistic global search inspired by thermodynamics.  </li> <li>Multi-Starts: pragmatic global exploration by repeated local optimization.  </li> <li>Evolutionary / Genetic / ES: population-based global heuristics; robust to noise and discontinuity.  </li> <li>Derivative-Free Optimization (DFO): umbrella for random, surrogate-based, or adaptive black-box methods.  </li> <li>Numerical Encoding: crucial in discrete search\u2014how real or binary variables are represented determines performance.</li> </ul> <p>Summary Insight: - Convex + differentiable \u2192 use gradient-based or Newton-type methods. - Convex + nonsmooth \u2192 use proximal, ADMM, or coordinate descent. - Large-scale or stochastic \u2192 use SGD or adaptive variants. - No gradients or nonconvex \u2192 use derivative-free or evolutionary methods. - The structure of the objective, not its size alone, determines the optimal solver family.</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1:  Introduction and Overview","text":"<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p> <ol> <li>No bad local minima: any local minimum is also the global minimum.  </li> <li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li> <li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li> </ol> <p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p> <p>This web-book is written for practitioners who have basic familiarity with optimization, especially gradient-based methods, and want to understand how convex optimization principles help guide reliable machine-learning practice.</p>"},{"location":"convex/11_intro/#11-motivation-optimization-in-machine-learning","title":"1.1 Motivation: Optimization in Machine Learning","text":"<p>Many supervised learning problems can be written in a common form:</p> \\[ \\min_{x \\in \\mathcal{X}}  \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i)  + \\lambda R(x), \\] <p>where</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function that measures how well the model predicts \\(b_i\\) from \\(a_i\\),  </li> <li>\\(R(x)\\) is a regularizer that encourages certain structure (such as sparsity or small weights),  </li> <li>\\(\\mathcal{X}\\) is a set of allowed parameter values, often simple and convex.</li> </ul> <p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the \\(\\ell_1\\) norm, and the \\(\\ell_2\\) norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>"},{"location":"convex/11_intro/#12-convex-sets-and-convex-functions-first-intuition","title":"1.2 Convex Sets and Convex Functions \u2014 First Intuition","text":"<p>A set \\(\\mathcal{C}\\) is convex if, whenever you pick two points in the set, the line segment between them stays entirely inside the set:</p> \\[ \\theta x + (1-\\theta)y \\in \\mathcal{C}  \\quad \\text{for all } x,y \\in \\mathcal{C},\\; \\theta \\in [0,1]. \\] <p>Convex functions follow a similar idea. A function \\(f\\) is convex if its graph never dips below the straight line connecting two points on the function:</p> \\[ f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta) f(y). \\] <p>Intuitively, convex functions look like bowls: they curve upward and have at most one global minimum. Affine functions are both convex and concave, and quadratics with positive semidefinite Hessians are convex. Many ML loss functions share this shape, which makes them easy to optimize.</p>"},{"location":"convex/11_intro/#13-why-convex-optimization-remains-central-in-ml","title":"1.3 Why Convex Optimization Remains Central in ML","text":"<p>Although many modern models are nonconvex, convex optimization continues to play a major role in three ways:</p> <ol> <li> <p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the \\(0\\text{\u2013}1\\) loss. They make optimization practical while still leading to models that generalize well.</p> </li> <li> <p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p> </li> <li> <p>Implicit bias in linear models:  In overparameterized linear least-squares problems, gradient descent starting from zero converges to the minimum-norm solution. This phenomenon helps explain generalization and implicit regularization in linear and kernel models.</p> </li> </ol> <p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>"},{"location":"convex/11_intro/#14-from-global-optima-to-algorithms","title":"1.4 From Global Optima to Algorithms","text":"<p>A major advantage of convex optimization is that it eliminates the possibility of non-global local minima. For a differentiable convex function on an open domain:</p> \\[ \\nabla f(x^*) = 0  \\quad \\Rightarrow \\quad x^* \\text{ is a global minimizer}. \\] <p>This means that simply finding a point where the gradient is zero is enough. For constrained or nondifferentiable problems, optimality is checked using subgradients or KKT conditions:</p> \\[ 0 \\in \\partial f(x^*) + N_{\\mathcal{X}}(x^*), \\] <p>where \\(N_{\\mathcal{X}}(x^*)\\) represents the outward directions that are blocked by the constraint set. These conditions are useful because many iterative algorithms aim to drive the gradient or subgradient toward zero.</p>"},{"location":"convex/11_intro/#15-canonical-convex-ml-problems-at-a-glance","title":"1.5 Canonical Convex ML Problems at a Glance","text":"Problem Objective Typical Solver Least squares \\(\\|A x - b\\|_2^2\\) Gradient descent, conjugate gradient Ridge regression \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_2^2\\) Closed form, gradient methods LASSO \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_1\\) Proximal gradient (ISTA/FISTA) Logistic regression \\(\\sum_i \\log(1+\\exp(-y_i a_i^\\top x)) + \\lambda\\|x\\|_2^2\\) Newton, quasi-Newton, SGD SVM (hinge loss) \\(\\tfrac{1}{2}\\|x\\|^2 + C\\sum_i \\max(0,1-y_i a_i^\\top x)\\) Subgradient, coordinate methods, SMO Robust regression \\(\\|A x - b\\|_1\\) Linear programming Elastic Net \\(\\|A x-b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|x\\|_2^2\\) Coordinate descent <p>These problems illustrate how convex models appear throughout ML.</p>"},{"location":"convex/11_intro/#16-web-book-roadmap-and-how-to-use-it","title":"1.6 Web-Book Roadmap and How to Use It","text":"Question Where to Look Key Idea What makes a function or set convex? Chapters 2\u20135 Geometry and basic properties of convexity How do gradients, subgradients, and KKT conditions define optimality? Chapters 6\u20139 Optimality conditions and duality How are convex problems solved in practice? Chapters 10\u201314 First-order, second-order, and interior-point methods How to choose an algorithm for a given optimization problem? Chapters 15\u201317 Large-scale and structured optimization techniques"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model \\(x\\) to data \\((A, b)\\) takes the form:</p> \\[ \\min_x \\ \\|A x - b\\|_2^2. \\] <p>Later in this chapter, we will see that this objective finds the point in the column space of \\(A\\) that is closest to \\(b\\). Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p> <p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas \u2014 projections, subspaces, orthogonality, eigenvalues, singular values, and norms \u2014 because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>"},{"location":"convex/12_vector/#21-vector-spaces-subspaces-and-affine-sets","title":"2.1 Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set of vectors that can be added and scaled without leaving the set. The familiar example is \\(\\mathbb{R}^n\\), where operations like \\(\\alpha x + \\beta y\\) keep us within the same space.</p> <p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are \u201cflat\u201d objects that always pass through the origin, such as lines or planes in \\(\\mathbb{R}^3\\). </p> <p>Affine sets extend this idea by allowing a shift away from the origin. A set \\(A\\) is affine if it contains the entire line passing through any two of its points. Equivalently, for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  \\(\\theta x + (1 - \\theta) y \\in A.\\) That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\). By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set. </p> <p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: \\(A = x_0 + S = \\{\\, x_0 + s : s \\in S \\,\\},\\) where \\(S\\) is a subspace and \\(x_0\\) is any point in the set. This representation is extremely useful in optimization. If \\(Ax = b\\) is a linear constraint, then its solution set is an affine set. A single particular solution \\(x_0\\) gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of \\(A\\). Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p> <p>Finally, affine transformations play a central role in both machine learning and optimization. A mapping of the form</p> <p>Affine Transformations: An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as \\(f(x) = A x + b,\\) where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex. is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p> <p>In summary, vector spaces describe the ambient space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the book.</p>"},{"location":"convex/12_vector/#22-linear-combinations-span-basis-dimension","title":"2.2 Linear combinations, span, basis, dimension","text":"<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p> <p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form is a linear combination. The set of all linear combinations is called the span:  The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p> <p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p> <p>A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). The number of basis vectors is the dimension \\(\\dim(V)\\).</p> <p>Rank and nullity facts:</p> <ul> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states: \\(\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n,\\) where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>Column Space: The column space of a matrix \\( A \\), denoted \\( C(A) \\), is the set of all possible output vectors \\( b \\) that can be written as \\( Ax \\) for some \\( x \\). In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system \\( Ax = b \\) have a solution?\u201d is equivalent to asking whether \\( b \\in C(A) \\). If \\( b \\) lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of \\( A \\), denoted \\( N(A) \\), is the set of all input vectors \\( x \\) that are mapped to zero:  \\( N(A) = \\{ x : Ax = 0 \\} \\). It answers a different question: If a solution to \\( Ax = b \\) exists, is it unique? If the null space contains only the zero vector (\\( \\mathrm{nullity}(A) = 0 \\)), the solution is unique. But if \\( N(A) \\) contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix \\( A \\) is a linear combination of others for example, \\( \\text{feature}_3 = 2 \\times \\text{feature}_1 + \\text{feature}_2 \\)\u2014the columns of \\( A \\) become linearly dependent. This creates a nonzero vector in the null space of \\( A \\), meaning multiple weight vectors \\( x \\) can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and \\( A^\\top A \\) becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p> <p>Feasible Directions: In a constrained optimization problem of the form \\( Ax = b \\), the null space of \\( A \\) characterizes the directions along which one can move without violating the constraints. If \\( d \\in N(A) \\), then moving from a feasible point \\( x \\) to \\( x + d \\) preserves feasibility, since  \\( A(x + d) = Ax + Ad = b \\). Thus, the null space defines the space of free movement directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of \\( A \\), denoted \\( R(A) \\), is the span of the rows of \\( A \\) (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to \\( \\mathrm{rank}(A) \\). The row space is orthogonal to the null space of \\( A \\):  \\( R(A) \\perp N(A) \\).  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in \\( x \\) affect the constraints.</p> <p>Left Null Space: The left null space, denoted \\( N(A^\\top) \\), is the set of all vectors \\( y \\) such that \\( A^\\top y = 0 \\). These vectors are orthogonal to the columns of \\( A \\), and therefore orthogonal to the column space itself. In least squares problems, \\( N(A^\\top) \\) represents residual directions\u2014components of \\( b \\) that cannot be explained by the model \\( Ax = b \\).</p> <p>Projection Interpretation (Least Squares):  When \\( Ax = b \\) has no exact solution (as in overdetermined systems), the least squares solution finds \\( x \\) such that \\( Ax \\) is the projection of \\( b \\) onto the column space of \\( A \\):  and the residual  lies in the left null space \\( N(A^\\top) \\). This provides a geometric view: the solution projects \\( b \\) onto the closest point in the subspace that \\( A \\) can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of \\( A \\) is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where \\( n \\) is the number of columns of \\( A \\). This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in \\( x \\).</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by \\( A \\).  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map \\( A: \\mathbb{R}^n \\to \\mathbb{R}^m \\).</p>"},{"location":"convex/12_vector/#23-inner-products-and-orthogonality","title":"2.3 Inner products and orthogonality","text":"<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality\u2014concepts that appear repeatedly in convex optimization and machine learning.</p> <p>An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\):  with equality iff \\(x\\) and \\(y\\) are linearly dependent Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#24-norms-and-distances","title":"2.4 Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms: Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), and the dual of \\(\\ell_2\\) is itself.</p> <p>Imagine the vector \\(x\\) lives inside the original norm ball (\\(\\|x\\| \\le 1\\)). The term \\(x^\\top y\\) is the dot product, which measures the alignment between \\(x\\) and \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible value you can get by taking the dot product of \\(y\\) with any vector \\(x\\) that fits inside the original norm ball.If the dual norm \\(\\|y\\|_*\\) is large, it means \\(y\\) is strongly aligned with a direction \\(x\\) that is \"small\" (size \\(\\le 1\\)) according to the original norm.If the dual norm is small, \\(y\\) must be poorly aligned with all vectors \\(x\\) in the ball.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>"},{"location":"convex/12_vector/#26-orthogonal-projections-and-least-squares","title":"2.6 Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\). If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#27-operator-norms-singular-values-and-spectral-structure","title":"2.7 Operator norms, singular values, and spectral structure","text":"<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>"},{"location":"convex/12_vector/#operator-norms","title":"Operator norms","text":"<p>Given a matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and norms \\(\\|\\cdot\\|_p\\) on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_q\\) on \\(\\mathbb{R}^m\\), the induced operator norm is defined as  This quantity measures the largest amount by which \\(A\\) can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p> <ul> <li>\\(\\|A\\|_{2 \\to 2}\\), the spectral norm, equals the largest singular value of \\(A\\).</li> <li>\\(\\|A\\|_{1 \\to 1}\\) is the maximum absolute column sum.</li> <li>\\(\\|A\\|_{\\infty \\to \\infty}\\) is the maximum absolute row sum.</li> </ul> <p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function  converges for step sizes \\(\\alpha &lt; 2 / \\|Q\\|_2\\). This shows that controlling the operator norm of the Hessian\u2014or a Lipschitz constant of the gradient\u2014directly governs how aggressively an algorithm can move.</p>"},{"location":"convex/12_vector/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a factorization  where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is diagonal with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots\\). The \\(\\sigma_i\\) are the singular values of \\(A\\).</p> <p>Geometrically, the SVD shows how \\(A\\) transforms the unit ball into an ellipsoid. The columns of \\(V\\) give the principal input directions, the singular values are the lengths of the ellipsoid\u2019s axes, and the columns of \\(U\\) give the output directions. The largest singular value \\(\\sigma_{\\max}\\) equals the spectral norm \\(\\|A\\|_2\\), while the smallest \\(\\sigma_{\\min}\\) describes the least expansion (or exact flattening if \\(\\sigma_{\\min} = 0\\)).</p> <p>SVD is a powerful diagnostic tool in optimization. The ratio  is the condition number of \\(A\\). A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means \\(A\\) behaves more like a uniform scaling, which is ideal for optimization.</p>"},{"location":"convex/12_vector/#low-rank-structure","title":"Low-rank structure","text":"<p>The rank of \\(A\\) is the number of nonzero singular values. When \\(A\\) has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p> <p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>"},{"location":"convex/12_vector/#operator-norms-and-optimization-algorithms","title":"Operator norms and optimization algorithms","text":"<p>Operator norms help determine step sizes, convergence rates, and preconditioning strategies. For a general smooth convex function, the Lipschitz constant of its gradient often corresponds to a spectral norm of a Hessian or Jacobian, and this constant controls the safe step size for gradient descent. Preconditioning modifies the geometry of the problem\u2014changing the inner product or scaling the variables\u2014in order to reduce the effective operator norm and improve conditioning.</p> <p>These spectral considerations appear in both first-order and second-order methods. Newton\u2019s method, for example, implicitly rescales the space using the inverse Hessian, which equalizes curvature by transforming eigenvalues toward 1. This explains its rapid local convergence when the Hessian is well behaved.</p>"},{"location":"convex/12_vector/#summary","title":"Summary","text":"<ul> <li>The operator norm measures the maximum stretching effect of a matrix.</li> <li>Singular values give a complete geometric description of this stretching.</li> <li>The condition number captures how unevenly the matrix acts in different directions.</li> <li>Low-rank structure reveals underlying dimension and enables efficient computation.</li> <li>All of these properties strongly influence the behavior and design of optimization algorithms.</li> </ul> <p>Understanding operator norms and singular values provides valuable insight into when optimization problems are well conditioned, how algorithms will behave, and how to modify a problem to improve performance.</p>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p> <p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton\u2019s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p> <p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>"},{"location":"convex/13_calculus/#31-gradients-and-directional-derivatives","title":"3.1 Gradients and Directional Derivatives","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The function is differentiable at a point \\(x\\) if there exists a vector \\(\\nabla f(x)\\) such that  meaning that the linear function \\(h \\mapsto \\nabla f(x)^\\top h\\) provides the best local approximation to \\(f\\) near \\(x\\). The gradient is the unique vector with this property.</p> <p>A closely related concept is the directional derivative. For any direction \\(v \\in \\mathbb{R}^n\\), the directional derivative of \\(f\\) at \\(x\\) in the direction \\(v\\) is  If \\(f\\) is differentiable, then  Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction \\(v\\) tells us how rapidly \\(f\\) increases when we move infinitesimally along \\(v\\).</p> <p>This immediately yields an important geometric fact. Among all unit directions \\(u\\),  is maximized when \\(u\\) points in the direction of \\(\\nabla f(x)\\), the direction of steepest ascent. The steepest descent direction is therefore \\(-\\nabla f(x)\\), which motivates gradient-descent algorithms for minimizing functions.</p> <p>For any real number \\(c\\), the level set of \\(f\\) is   </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#32-jacobians","title":"3.2  Jacobians","text":"<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>"},{"location":"convex/13_calculus/#from-derivative-to-gradient","title":"From derivative to gradient","text":"<p>For a scalar function , differentiability means that near any point ,  The gradient vector  collects all partial derivatives. Each component measures how sensitive \\(f\\) is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>"},{"location":"convex/13_calculus/#from-gradient-to-jacobian","title":"From gradient to Jacobian","text":"<p>Now consider a vector-valued function \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\),  Each output \\(F_i\\) has its own gradient. Stacking these row vectors yields the Jacobian matrix:  </p> <p>The Jacobian provides the best linear approximation of \\(F\\) near \\(x\\):  Thus, locally, the nonlinear map \\(F\\) behaves like the linear map \\(h \\mapsto J_F(x)h\\). A small displacement \\(h\\) in input space is transformed into an output change governed by the Jacobian.</p>"},{"location":"convex/13_calculus/#interpreting-the-jacobian","title":"Interpreting the Jacobian","text":"Component of \\(J_F(x)\\) Meaning Row \\(i\\) Gradient of output \\(F_i(x)\\): how the \\(i\\)-th output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to \\(x_j\\): how varying input \\(x_j\\) affects the entire output vector. Determinant (when \\(m=n\\)) Local volume scaling: how \\(F\\) expands or compresses space near \\(x\\). Rank Local dimension of the image: whether any input directions are lost or collapsed. <p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>"},{"location":"convex/13_calculus/#33-the-hessian-and-curvature","title":"3.3 The Hessian and Curvature","text":"<p>For a twice\u2013differentiable function , the Hessian matrix collects all second-order partial derivatives:  </p> <p>The Hessian describes the local curvature of the function. While the gradient indicates the direction of steepest change, the Hessian tells us how that directional change itself varies\u2014whether the surface curves upward, curves downward, or remains nearly flat.</p>"},{"location":"convex/13_calculus/#curvature-and-positive-definiteness","title":"Curvature and positive definiteness","text":"<p>The eigenvalues of the Hessian determine its geometric behavior:</p> <ul> <li>If  (all eigenvalues nonnegative), the function is locally convex near \\(x\\).  </li> <li>If , the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li> <li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li> </ul> <p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>"},{"location":"convex/13_calculus/#example-quadratic-functions","title":"Example: Quadratic functions","text":"<p>Consider the quadratic function  where \\(Q\\) is symmetric. The gradient and Hessian are  Setting the gradient to zero gives the stationary point  If \\(Q \\succ 0\\), the solution  is the unique minimizer. The Hessian \\(Q\\) being positive definite confirms strict convexity.</p> <p>The eigenvalues of \\(Q\\) also explain the difficulty of minimizing \\(f\\):</p> <ul> <li>Large eigenvalues produce very steep, narrow directions\u2014optimization methods must take small steps.  </li> <li>Small eigenvalues produce flat directions\u2014progress is slow, especially for gradient descent.  </li> </ul> <p>The ratio of largest to smallest eigenvalue, the condition number, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>"},{"location":"convex/13_calculus/#why-the-hessian-matters-in-optimization","title":"Why the Hessian matters in optimization","text":"<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p> <ul> <li>Newton\u2019s method uses the Hessian to rescale directions, effectively \u201cwhitening\u2019\u2019 curvature and often converging rapidly.  </li> <li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li> <li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li> </ul> <p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>"},{"location":"convex/13_calculus/#34-taylor-approximation","title":"3.4 Taylor approximation","text":"<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>"},{"location":"convex/13_calculus/#first-order-approximation","title":"First-order approximation","text":"<p>If \\(f\\) is differentiable at \\(x\\), then for small steps \\(d\\),  The gradient gives the best linear model of the function near \\(x\\). This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>"},{"location":"convex/13_calculus/#second-order-approximation","title":"Second-order approximation","text":"<p>If \\(f\\) is twice differentiable, we can include curvature information:  The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p> <ul> <li>If , the quadratic term is nonnegative and the function curves upward\u2014locally bowl-shaped.</li> <li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others\u2014characteristic of saddle points.</li> </ul>"},{"location":"convex/13_calculus/#role-in-optimization-algorithms","title":"Role in optimization algorithms","text":"<p>Second-order Taylor models are the basis of Newton-type methods. Newton\u2019s method chooses \\(d\\) by approximately minimizing the quadratic model,  which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p> <p>Thus, Taylor expansions connect a function\u2019s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>"},{"location":"convex/13_calculus/#35-smoothness-and-strong-convexity","title":"3.5 Smoothness and Strong Convexity","text":"<p>In optimization, the behavior of a function\u2019s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>"},{"location":"convex/13_calculus/#lipschitz-continuous-gradients-l-smoothness","title":"Lipschitz continuous gradients (L-smoothness)","text":"<p>A differentiable function  has an \\(L\\)-Lipschitz continuous gradient if  This condition limits how quickly the gradient can change. Intuitively, an \\(L\\)-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:  This inequality states that every \\(L\\)-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p> <p>In gradient descent, smoothness directly determines a safe step size: choosing  ensures that each update decreases the function value for convex objectives. In machine learning, the constant \\(L\\) effectively controls how large the learning rate can be before training becomes unstable.</p>"},{"location":"convex/13_calculus/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function  is -strongly convex if, for some ,  This condition guarantees that \\(f\\) has at least \\(\\mu\\) amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p> <p>Strong convexity has major optimization implications:</p> <ul> <li>The minimizer is unique.  </li> <li>Gradient descent converges linearly with step size \\(\\eta \\le 1/L\\).  </li> <li>The ratio \\(L / \\mu\\) (the condition number) dictates convergence speed.</li> </ul>"},{"location":"convex/13_calculus/#curvature-in-both-directions","title":"Curvature in both directions","text":"<p>Together, smoothness and strong convexity bound the curvature of \\(f\\):  Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p> <p>These concepts\u2014, imiting curvature from above via \\(L\\) and from below via \\(\\mu\\), form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p> <p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>"},{"location":"convex/14_convexsets/#41-convex-sets","title":"4.1 Convex sets","text":"<p>A set  is convex if for any two points  and any ,  That is, the entire line segment between \\(x\\) and \\(y\\) lies inside the set. Convex sets have no \u201choles\u201d or \u201cindentations,\u201d and this geometric regularity is what makes optimization over them tractable.</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>Affine subspaces: .  </li> <li>Halfspaces: .  </li> <li>Euclidean balls: .  </li> <li>  balls (axis-aligned boxes): .  </li> <li>Probability simplex: .  </li> </ul> <p>A set fails to be convex whenever some segment between two feasible points leaves the set\u2014for example, a crescent or an annulus.</p>"},{"location":"convex/14_convexsets/#42-affine-sets-hyperplanes-and-halfspaces","title":"4.2 Affine sets, hyperplanes, and halfspaces","text":"<p>Affine sets generalize linear subspaces by allowing a shift. A set \\(A\\) is affine if for some point \\(x_0\\) and subspace \\(S\\),  Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p> <p>A hyperplane is an affine set defined by a single linear equation:  Hyperplanes act as the \u201cflat boundaries\u201d of higher-dimensional space and are the fundamental building blocks of polyhedra.</p> <p>A halfspace is one side of a hyperplane:  Halfspaces are convex and serve as basic local approximations to general convex sets.</p>"},{"location":"convex/14_convexsets/#43-convex-combinations-and-convex-hulls","title":"4.3 Convex combinations and convex hulls","text":"<p>A convex combination of points  is a weighted average  Convex sets are precisely those that contain all convex combinations of their points.</p> <p>The convex hull of a set \\(S\\), denoted \\(\\operatorname{conv}(S)\\), is the set of all convex combinations of finitely many points in \\(S\\). It is the smallest convex set containing \\(S\\). Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p> <p>Convex hulls are important because:</p> <ul> <li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li> <li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li> </ul>"},{"location":"convex/14_convexsets/#44-polyhedra-and-polytopes","title":"4.4 Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are always convex; they may be bounded or unbounded.</p> <p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>"},{"location":"convex/14_convexsets/#45-extreme-points","title":"4.5 Extreme points","text":"<p>Let  be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if  implies .</p> <p>Geometrically, extreme points are the \u201ccorners\u201d of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems\u2014such as linear programs\u2014achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>"},{"location":"convex/14_convexsets/#46-cones","title":"4.6 Cones","text":"<p>Cones generalize the idea of \u201cdirections\u201d in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>"},{"location":"convex/14_convexsets/#basic-definition","title":"Basic definition","text":"<p>A set \\(K \\subseteq \\mathbb{R}^n\\) is a cone if  A cone is convex if it is also closed under addition:  </p> <p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant , each coordinate axis direction is an extreme ray.</p>"},{"location":"convex/14_convexsets/#conic-hull","title":"Conic hull","text":"<p>Given any set \\(S\\), its conic hull is the set of all conic combinations:  This is the smallest convex cone containing \\(S\\). Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>"},{"location":"convex/14_convexsets/#polar-cones","title":"Polar cones","text":"<p>For a cone \\(K\\), the polar cone is defined as  </p> <p>Intuition:</p> <ul> <li>Polar vectors make a nonacute angle with every vector in \\(K\\).  </li> </ul> <p>Key properties:</p> <ul> <li>\\(K^\\circ\\) is always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, then \\(K^\\circ\\) is the orthogonal complement.  </li> <li>For any closed convex cone, </li> </ul> <p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>"},{"location":"convex/14_convexsets/#tangent-cones","title":"Tangent cones","text":"<p>For a set \\(C\\) and a point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) consists of all feasible \u201cinfinitesimal directions\u201d from \\(x\\):  </p> <p>Intuition:</p> <ul> <li>At an interior point, \\(T_C(x) = \\mathbb{R}^n\\): all small moves are allowed.  </li> <li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li> </ul> <p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>"},{"location":"convex/14_convexsets/#normal-cones","title":"Normal cones","text":"<p>For a convex set \\(C\\), the normal cone at a point \\(x \\in C\\) is  </p> <p>Interpretation:</p> <ul> <li>Every \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\).  </li> <li>At interior points, the normal cone is \\(\\{0\\}\\).  </li> <li>At boundary or corner points, it becomes a pointed cone of outward normals.</li> </ul> <p>A fundamental relationship ties tangent and normal cones together:  </p> <p>Normal cones appear directly in first-order optimality conditions. For a constrained problem  a point \\(x^*\\) is optimal only if  This expresses a balance between the objective\u2019s slope and the \u201cpushback\u2019\u2019 from the constraint set.</p> <p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>"},{"location":"convex/14_convexsets/#47-supporting-hyperplanes-and-separation","title":"4.7 Supporting Hyperplanes and Separation","text":"<p>One of the most important geometric facts about convex sets is that they can be supported or separated by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting Hyperplane Theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be nonempty, closed, and convex, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero vector \\(a\\) such that</p> \\[ a^\\top x \\le a^\\top x_0 \\qquad \\forall x \\in C. \\] <p>This means that the hyperplane</p> \\[ a^\\top x = a^\\top x_0 \\] <p>touches \\(C\\) at \\(x_0\\) but does not cut through it. The vector \\(a\\) is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating Hyperplane Theorem","text":"<p>If \\(C\\) and \\(D\\) are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector \\(a\\) and scalar \\(b\\) such that</p> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\qquad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <p>The hyperplane \\(a^\\top x = b\\) places all points of \\(C\\) on one side and all points of \\(D\\) on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p>"},{"location":"convex/14_convexsets/#why-this-matters-for-optimisation","title":"Why This Matters for Optimisation","text":"<p>These geometric results are central to convex optimisation:</p> <ul> <li>Subgradients correspond to supporting hyperplanes of the epigraph of a convex function.</li> <li>Dual variables arise from separating infeasible points from the feasible region.</li> <li>KKT conditions express the balance between the gradient of the objective and the normals of active constraints.</li> <li>Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.</li> </ul> <p>Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>Convex functions play a central role in optimisation and machine learning. When the objective function is convex, the optimisation landscape has a single global minimum, gradient-based algorithms behave predictably, and optimality conditions have clean geometric interpretations. Many common ML losses\u2014least squares, logistic loss, hinge loss, Huber loss\u2014are convex precisely for these reasons.</p> <p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>"},{"location":"convex/15_convexfunctions/#51-definitions-of-convexity","title":"5.1 Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),  </p> <p>The graph of \\(f\\) never dips below the straight line between \\((x,f(x))\\) and \\((y,f(y))\\). If the inequality is strict whenever \\(x \\neq y\\), the function is strictly convex.</p> <p>A powerful geometric viewpoint comes from the epigraph:  The function \\(f\\) is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>"},{"location":"convex/15_convexfunctions/#52-first-order-characterisation","title":"5.2 First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if  </p> <p>Interpretation:</p> <ul> <li>The tangent plane at any point \\(x\\) lies below the function everywhere.</li> <li>\\(\\nabla f(x)\\) defines a supporting hyperplane to the epigraph.</li> <li>The gradient provides a global linear underestimator of \\(f\\).</li> </ul> <p>This geometric picture is crucial in optimisation: at a minimiser \\(x^\\star\\), convexity implies </p> <p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>"},{"location":"convex/15_convexfunctions/#53-second-order-characterisation","title":"5.3 Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then convexity can be checked via curvature:</p> \\[ f \\text{ is convex } \\iff \\nabla^2 f(x) \\succeq 0 \\text{ for all } x. \\] <ul> <li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li> <li>If \\(\\nabla^2 f(x) \\succ 0\\) everywhere, the function is strictly convex.  </li> <li>Negative eigenvalues indicate directions of negative curvature \u2014 impossible for convex functions.</li> </ul> <p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>"},{"location":"convex/15_convexfunctions/#54-examples-of-convex-functions","title":"5.4 Examples of convex functions","text":"<ol> <li> <p>Affine functions:     Always convex (and concave). They define supporting hyperplanes.</p> </li> <li> <p>Quadratic functions with PSD Hessian:     Convex because the curvature matrix \\(Q\\) is PSD.</p> </li> <li> <p>Norms:     All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p> </li> <li> <p>Maximum of affine functions:     Convex because the maximum of convex functions is convex.    (Important in SVM hinge loss.)</p> </li> <li> <p>Log-sum-exp:     A smooth approximation to the max; convex by Jensen\u2019s inequality. Appears in softmax, logistic regression, partition functions.</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#55-jensens-inequality","title":"5.5 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex and \\(X\\) a random variable in its domain. Then:  </p> <p>This generalises the definition of convexity from finite averages to expectations. Practically:</p> <ul> <li>convex functions \u201cpull upward\u201d under averaging,</li> <li>log-sum-exp is convex because exponential is convex,</li> <li>EM and variational methods rely on Jensen to construct lower bounds.</li> </ul> <p>As a finite form, for \\(\\theta_i \\ge 0\\) with \\(\\sum \\theta_i = 1\\),  </p>"},{"location":"convex/15_convexfunctions/#56-operations-that-preserve-convexity","title":"5.6 Operations that preserve convexity","text":"<p>Convexity is preserved under many natural constructions:</p> <ul> <li> <p>Nonnegative scaling:   If \\(f\\) is convex and \\(\\alpha \\ge 0\\), then \\(\\alpha f\\) is convex.</p> </li> <li> <p>Addition:   If \\(f\\) and \\(g\\) are convex, then \\(f+g\\) is convex.</p> </li> <li> <p>Maximum: \\(\\max\\{f,g\\}\\) is convex.</p> </li> <li> <p>Affine pre-composition:   If \\(A\\) is a matrix,      is convex.</p> </li> <li> <p>Monotone composition rule:   If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex,   then \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex.</p> </li> </ul> <p>These rules allow construction of complex convex models from simple building blocks.</p>"},{"location":"convex/15_convexfunctions/#57-level-sets-of-convex-functions","title":"5.7 Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), the sublevel set is  </p> <p>If \\(f\\) is convex, every sublevel set is convex. This property is crucial because inequalities \\(f(x) \\le \\alpha\\) are ubiquitous in constraints.</p> <p>Examples:</p> <ul> <li>Norm balls: \\(\\{ x : \\|x\\|_2 \\le r \\}\\) </li> <li>Linear regression confidence ellipsoids: \\(\\{ x : \\|Ax - b\\|_2 \\le \\epsilon \\}\\)</li> </ul> <p>These sets enable convex constrained optimisation formulations.</p>"},{"location":"convex/15_convexfunctions/#58-strict-and-strong-convexity","title":"5.8 Strict and strong convexity","text":""},{"location":"convex/15_convexfunctions/#strict-convexity","title":"Strict convexity","text":"<p>A function is strictly convex if  for all \\(x \\neq y\\) and \\(\\theta \\in (0,1)\\).</p> <p>Strict convexity implies unique minimisers.</p>"},{"location":"convex/15_convexfunctions/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function is \\(\\mu\\)-strongly convex if  </p> <p>Strong convexity adds quantitative curvature: the function grows at least quadratically away from its minimiser.</p> <p>Consequences:</p> <ul> <li>unique minimiser,</li> <li>gradient descent achieves linear convergence rate,   error shrinks as </li> <li>conditioning (\\(\\kappa = L/\\mu\\)) governs algorithmic difficulty.</li> </ul> <p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds \\(\\tfrac{\\lambda}{2}\\|x\\|_2^2\\)).</p>"},{"location":"convex/15_convexfunctions/#summary","title":"Summary","text":"<p>Convex functions form the analytical backbone of convex optimisation. They provide:</p> <ul> <li>predictable geometry,</li> <li>clean gradient conditions,</li> <li>reliable convergence behaviour,</li> <li>tractable constraints via convex sublevel sets,</li> <li>stability under composition and modelling operations.</li> </ul> <p>These properties make convex objectives indispensable across machine learning, signal processing, and optimisation theory.</p>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p> <ul> <li>the  norm  (nondifferentiable at zero),</li> <li>pointwise-max functions such as ,</li> <li>the hinge loss  used in SVMs,</li> <li>regularisers like total variation or indicator functions of convex sets.</li> </ul> <p>Although these functions have \u201ckinks\u201d, they remain convex\u2014and convexity guarantees the existence of supporting hyperplanes at every point. Subgradients formalise this idea and allow optimisation algorithms to operate even when no derivative exists.</p> <p>This chapter introduces subgradients, subdifferentials, subgradient calculus, and the basic subgradient method.</p>"},{"location":"convex/16_subgradients/#61-subgradients-and-the-subdifferential","title":"6.1 Subgradients and the Subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex.  A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if</p> \\[ f(y) \\ge f(x) + g^\\top (y - x) \\quad \\text{for all } y. \\] <p>Geometric interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top(y-x)\\) is a global underestimator of \\(f\\).</li> <li>Each subgradient defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).</li> <li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li> <li>At kinks, there may be infinitely many supporting hyperplanes.</li> </ul> <p>The subdifferential of \\(f\\) at \\(x\\) is the set  </p> <p>Properties:</p> <ul> <li>  is always a nonempty convex set (if \\(x\\) is in the interior of the domain).</li> <li>If \\(f\\) is differentiable at \\(x\\), then </li> <li>If \\(f\\) is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li> </ul> <p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>"},{"location":"convex/16_subgradients/#62-examples","title":"6.2 Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\). Then:</p> <ul> <li>If \\(t &gt; 0\\),  \\(\\partial f(t) = \\{1\\}\\).</li> <li>If \\(t &lt; 0\\),  \\(\\partial f(t) = \\{-1\\}\\).</li> <li>If \\(t = 0\\), </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) supports the graph from below.</p>"},{"location":"convex/16_subgradients/#the-ell_1-norm","title":"The  norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\):</p> \\[ g \\in \\partial \\|x\\|_1 \\quad\\Longleftrightarrow\\quad g_i \\in \\partial |x_i|. \\] <p>Thus:</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>"},{"location":"convex/16_subgradients/#pointwise-maximum-of-affine-functions","title":"Pointwise maximum of affine functions","text":"<p>Let </p> <ul> <li> <p>If only one index \\(i^\\star\\) achieves the maximum at \\(x\\), then </p> </li> <li> <p>If multiple indices are tied, then    the convex hull of the active slopes.</p> </li> </ul> <p>This structure underlies SVM hinge loss and ReLU-type functions.</p>"},{"location":"convex/16_subgradients/#63-subgradient-optimality-condition","title":"6.3 Subgradient Optimality Condition","text":"<p>For the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>a point \\(x^\\star\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^\\star). \\] <p>Interpretation:</p> <ul> <li>At optimality, no subgradient points to a direction that would decrease \\(f\\).</li> <li>Geometrically, the supporting hyperplane at \\(x^\\star\\) is horizontal, forming the flat bottom of the convex bowl.</li> <li>This generalises the smooth condition .</li> </ul>"},{"location":"convex/16_subgradients/#64-subgradient-calculus-useful-rules","title":"6.4 Subgradient Calculus (Useful Rules)","text":"<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let \\(f\\) and \\(g\\) be convex.</p>"},{"location":"convex/16_subgradients/#sum-rule","title":"Sum rule","text":"\\[ \\partial(f+g)(x) \\subseteq \\partial f(x) + \\partial g(x) = \\{ u+v : u \\in \\partial f(x),\\ v \\in \\partial g(x) \\}. \\] <p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>"},{"location":"convex/16_subgradients/#affine-composition","title":"Affine composition","text":"<p>If \\(h(x) = f(Ax + b)\\), then  </p> <p>This rule is heavily used in machine learning models, where losses depend on linear predictions \\(Ax\\).</p>"},{"location":"convex/16_subgradients/#maximum-of-convex-functions","title":"Maximum of convex functions","text":"<p>If \\(f(x) = \\max_i f_i(x)\\), then  </p> <p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>"},{"location":"convex/16_subgradients/#65-subgradient-methods","title":"6.5 Subgradient Methods","text":"<p>Even when \\(f\\) is not differentiable, we can minimise it using subgradient descent:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k, \\qquad g_k \\in \\partial f(x_k). \\] <p>Key features:</p> <ul> <li>Requires only a subgradient (no differentiability needed).</li> <li>Works for any convex function.</li> <li>Stepsizes must typically decrease (e.g. , ).</li> <li>Guaranteed convergence for convex \\(f\\), but generally slow.</li> </ul>"},{"location":"convex/16_subgradients/#convergence-rates-worst-case","title":"Convergence rates (worst case)","text":"<ul> <li>Smooth convex gradient descent: \\(O(1/k)\\) or \\(O(1/k^2)\\).  </li> <li>Nonsmooth subgradient descent: </li> </ul> <p>This slower rate reflects the lack of curvature information at kinks.</p>"},{"location":"convex/16_subgradients/#why-it-still-matters-in-ml","title":"Why it still matters in ML","text":"<p>Many training objectives behave nonsmoothly:</p> <ul> <li>SVM hinge loss  </li> <li> -regularised models (sparse optimisation)  </li> <li>ReLUs and piecewise-linear networks  </li> <li>Projections onto convex sets  </li> </ul> <p>Even modern deep-learning optimisers operate as subgradient methods whenever the network contains nonsmooth operations.</p>"},{"location":"convex/16_subgradients/#66-proximal-and-smoothed-alternatives","title":"6.6 Proximal and Smoothed Alternatives","text":"<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>"},{"location":"convex/16_subgradients/#1-proximal-methods","title":"(1) Proximal methods","text":"<p>For a convex function \\(f\\), the proximal operator is  </p> <p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p> <ul> <li>  regularisation,</li> <li>indicator functions of convex sets,</li> <li>total variation penalties.</li> </ul> <p>They achieve faster and more stable convergence than basic subgradient descent.</p>"},{"location":"convex/16_subgradients/#2-smoothing-techniques","title":"(2) Smoothing techniques","text":"<p>Many nonsmooth convex functions have smooth approximations:</p> <ul> <li>Replace  with the Huber loss.</li> <li>Replace  with softplus.</li> <li>Replace  with log-sum-exp, a smooth convex approximation.</li> </ul> <p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>"},{"location":"convex/16_subgradients/#summary","title":"Summary","text":"<ul> <li>Nonsmooth convex functions arise naturally in ML.  </li> <li>Subgradients generalise gradients: they give supporting hyperplanes.  </li> <li>Optimality: \\(0 \\in \\partial f(x^\\star)\\).  </li> <li>Subgradient calculus enables reasoning about complex nonsmooth models.  </li> <li>Subgradient descent converges globally but slowly.  </li> <li>Proximal and smoothing methods yield faster practical algorithms.</li> </ul> <p>Subgradients complete the picture of convex analysis by extending optimisation tools beyond differentiable functions, setting the stage for modern first-order methods.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar \u201czero gradient\u201d condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) framework developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization","title":"7.1 Orders of Optimality: Why First Order is Enough in Convex Optimization","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer \\(x^\\star\\):</p> Order Object inspected Role First-order \\(\\nabla f(x^\\star)\\) or subgradients Detects existence of a local descent direction Second-order Hessian \\(\\nabla^2 f(x^\\star)\\) Examines curvature (minimum vs saddle vs maximum) Higher-order Third derivative and beyond Rarely used; only for degenerate cases with vanishing curvature <p>In general nonconvex optimization, these conditions are used together: a point may have \\(\\nabla f(x^\\star) = 0\\) but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p> <p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{whenever the Hessian exists}. \\] <p>Therefore:</p> <ul> <li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li> <li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li> </ul> <p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>"},{"location":"convex/16a_optimality_conditions/#72-motivation","title":"7.2 Motivation","text":"<p>Consider the basic convex problem  where \\(f\\) is convex and \\(\\mathcal{X}\\) is a convex set.</p> <p>Intuitively, a point \\(\\hat{x}\\) is optimal if there is no feasible direction in which we can move and strictly decrease \\(f\\). In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside \\(\\mathcal{X}\\) are allowed.</p> <p>Thus, optimality can be seen as an equilibrium:</p> <ul> <li>the objective\u2019s tendency to decrease (captured by its gradient or subgradient)  </li> <li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li> </ul> <p>In machine learning, this appears as:</p> <ul> <li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li> <li>training until the force from regularization/constraints balances the data fit term (e.g., in \\(\\ell_1\\)-regularized models).</li> </ul> <p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>"},{"location":"convex/16a_optimality_conditions/#73-unconstrained-convex-problems","title":"7.3 Unconstrained Convex Problems","text":"<p>For the unconstrained problem  with \\(f\\) convex, the optimality conditions are especially simple.</p>"},{"location":"convex/16a_optimality_conditions/#smooth-case","title":"Smooth case","text":"<p>If \\(f\\) is differentiable, then a point \\(\\hat{x}\\) is optimal if and only if  </p> <p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>"},{"location":"convex/16a_optimality_conditions/#nonsmooth-case","title":"Nonsmooth case","text":"<p>If \\(f\\) is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes  </p> <p>Interpretation:</p> <ul> <li>The origin lies in the set of all subgradients at \\(\\hat{x}\\).  </li> <li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\).  </li> <li>No direction in \\(\\mathbb{R}^n\\) gives a first-order improvement in the objective.</li> </ul> <p>For smooth \\(f\\), this reduces to the usual condition \\(\\nabla f(\\hat{x}) = 0\\).</p>"},{"location":"convex/16a_optimality_conditions/#74-constrained-convex-problems","title":"7.4 Constrained Convex Problems","text":"<p>Now consider the constrained problem  where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a nonempty closed convex set.</p> <p>If \\(\\hat{x}\\) lies strictly inside \\(\\mathcal{X}\\), then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,  remains the necessary and sufficient condition for optimality.</p> <p>The interesting case is when \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\).</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-order condition with constraints","text":"<p>The general first-order optimality condition for the constrained convex problem is:  </p> <p>That is, there exist</p> <ul> <li>a subgradient \\(g \\in \\partial f(\\hat{x})\\), and  </li> <li>a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\)</li> </ul> <p>such that  </p> <p>Interpretation:</p> <ul> <li>The objective\u2019s slope \\(g\\) is exactly balanced by a normal vector \\(v\\) coming from the constraint set.  </li> <li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which \\(f\\) can decrease.  </li> <li>Geometrically, the epigraph of \\(f\\) and the feasible set meet with aligned supporting hyperplanes at \\(\\hat{x}\\).</li> </ul> <p>Special cases:</p> <ul> <li>If \\(\\hat{x}\\) is an interior point, then \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so we recover the unconstrained condition \\(0 \\in \\partial f(\\hat{x})\\).  </li> <li>If \\(\\mathcal{X}\\) is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li> </ul>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-the-kkt-framework","title":"Chapter 8: Lagrange Multipliers and the KKT Framework","text":"<p>We now have the ingredients for understanding optimality in convex optimization:</p> <ul> <li>convex functions define well-behaved objectives,</li> <li>convex sets describe feasible regions,</li> <li>gradients and subgradients encode descent directions.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions\u2014the Karush\u2013Kuhn\u2013Tucker (KKT) conditions\u2014is the central optimality framework for constrained convex optimization.</p> <p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by \u201cforces\u2019\u2019 coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>"},{"location":"convex/17_kkt/#81-unconstrained-convex-minimization","title":"8.1 Unconstrained Convex Minimization","text":"<p>Consider the problem  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent iteratively updates  with step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Moving opposite the gradient decreases \\(f\\).</li> <li>If the gradient is Lipschitz continuous and the step size is small enough (\\(\\alpha_k \\le 1/L\\)), then gradient descent converges to a global minimizer.</li> <li>If \\(f\\) is strongly convex, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li> </ul> <p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>"},{"location":"convex/17_kkt/#82-equality-constrained-problems-and-lagrange-multipliers","title":"8.2 Equality-Constrained Problems and Lagrange Multipliers","text":"<p>Now consider minimizing \\(f\\) subject to equality constraints:  </p> <p>Define the Lagrangian  where \\(\\lambda = (\\lambda_1,\\dots,\\lambda_p)\\) are the Lagrange multipliers.</p> <p>Under differentiability and regularity assumptions, a point \\(x^*\\) is optimal only if:</p> <ol> <li> <p>Primal feasibility     </p> </li> <li> <p>Stationarity     </p> </li> </ol> <p>Geometric meaning:</p> <ul> <li>The feasible set  is typically a smooth manifold.</li> <li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li> <li>The multipliers \\(\\lambda_j^*\\) weight the constraint normals to exactly cancel the objective\u2019s gradient.</li> </ul> <p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>"},{"location":"convex/17_kkt/#83-inequality-constraints-and-the-kkt-conditions","title":"8.3 Inequality Constraints and the KKT Conditions","text":"<p>Now consider the general convex problem:  </p> <p>Form the Lagrangian  with:</p> <ul> <li>  (equality multipliers),</li> <li>  (inequality multipliers).</li> </ul> <p>A point \\(x^*\\) with multipliers \\((\\lambda^*,\\mu^*)\\) satisfies the KKT conditions:</p>"},{"location":"convex/17_kkt/#1-primal-feasibility","title":"1. Primal feasibility","text":"\\[ g_i(x^*) \\le 0,\\quad \\forall i, \\qquad h_j(x^*) = 0,\\quad \\forall j. \\]"},{"location":"convex/17_kkt/#2-dual-feasibility","title":"2. Dual feasibility","text":"\\[ \\mu_i^* \\ge 0,\\quad \\forall i. \\]"},{"location":"convex/17_kkt/#3-stationarity","title":"3. Stationarity","text":"\\[ \\nabla f(x^*)  + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*) = 0. \\]"},{"location":"convex/17_kkt/#4-complementary-slackness","title":"4. Complementary slackness","text":"\\[ \\mu_i^*\\, g_i(x^*) = 0, \\quad i=1,\\dots,m. \\] <p>Complementary slackness expresses a clear dichotomy:</p> <ul> <li>If constraint \\(g_i(x) \\le 0\\) is inactive (strictly \\(&lt;0\\)), then it applies no force: \\(\\mu_i^* = 0\\).</li> <li>If a constraint is active at the boundary, it may exert a force: \\(\\mu_i^* &gt; 0\\), and then \\(g_i(x^*) = 0\\).</li> </ul> <p>Only active constraints can push back against the objective.</p>"},{"location":"convex/17_kkt/#84-slaters-condition-guaranteeing-strong-duality","title":"8.4 Slater\u2019s Condition \u2014 Guaranteeing Strong Duality","text":"<p>The KKT conditions always provide necessary conditions for optimality. For them to also be sufficient (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p> <p>For convex problems with convex \\(g_i\\) and affine \\(h_j\\), Slater\u2019s condition holds if there exists a strictly feasible point:  </p> <p>Interpretation:</p> <ul> <li>The feasible region contains an interior point.</li> <li>The constraints are not \u201ctight\u201d everywhere.</li> <li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li> </ul> <p>When Slater\u2019s condition holds:</p> <ol> <li> <p>Strong duality holds: </p> </li> <li> <p>The dual optimum is attained.</p> </li> <li> <p>The KKT conditions are both necessary and sufficient for optimality.</p> </li> </ol>"},{"location":"convex/17_kkt/#duality-gap","title":"Duality gap","text":"<p>For a primal problem with optimum \\(p^*\\) and its dual with optimum \\(d^*\\), the duality gap is  </p> <ul> <li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li> <li>Slater\u2019s condition ensures the gap is zero.</li> </ul> <p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>"},{"location":"convex/17_kkt/#85-geometric-and-physical-interpretation","title":"8.5 Geometric and Physical Interpretation","text":"<p>The KKT conditions describe an equilibrium of forces:</p> <ul> <li>The objective gradient pushes the point in the direction of steepest decrease.</li> <li>Active constraints push back through normal vectors scaled by multipliers.</li> <li>At optimality, these forces exactly cancel.</li> </ul> <p>Physically:</p> <ul> <li>Lagrange multipliers are \u201creaction forces\u2019\u2019 keeping a system on the constraint surface.</li> <li>In economics, they are \u201cshadow prices\u2019\u2019 indicating how much the objective would improve if a constraint were relaxed.</li> <li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li> </ul> <p>KKT theory unifies all earlier ideas\u2014convexity, gradients/subgradients, feasible regions, tangent and normal cones\u2014into one clean, general optimality framework.</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p> <ul> <li>lower bounds on the primal optimal value,</li> <li>certificates of optimality,</li> <li>interpretations of constraint \u201cprices,\u201d</li> <li>and alternative algorithmic routes to solutions.</li> </ul> <p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality \u2014 strong duality \u2014 lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>"},{"location":"convex/18_duality/#91-the-primal-problem","title":"9.1 The Primal Problem","text":"<p>Consider the general convex problem</p> \\[ \\begin{array}{ll} \\text{minimize} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i=1,\\dots,m, \\\\  &amp; h_j(x) = 0,\\quad j=1,\\dots,p, \\end{array} \\] <p>where:</p> <ul> <li>\\(f\\) and each \\(g_i\\) are convex,</li> <li>each equality constraint \\(h_j\\) is affine.</li> </ul> <p>The optimal value is</p> \\[ f^\\star = \\inf\\{ f(x) : g_i(x) \\le 0,\\ h_j(x)=0 \\}. \\] <p>The infimum allows for the possibility that the best value is approached but not attained.</p>"},{"location":"convex/18_duality/#92-why-duality","title":"9.2 Why Duality?","text":"<p>A constrained problem can be viewed as:</p> <p>minimize \\(f(x)\\) but pay a penalty whenever constraints are violated.</p> <p>If the penalties are chosen \u201ccorrectly,\u201d one can recover the original constrained problem from an unconstrained penalized problem. Dual variables \u2014 \\(\\mu_i\\) for inequalities and \\(\\lambda_j\\) for equalities \u2014 precisely encode these penalties:</p> <ul> <li>\\(\\mu_i\\) measures how costly it is to violate \\(g_i(x)\\le 0\\),</li> <li>\\(\\lambda_j\\) measures the sensitivity of the objective to relaxing \\(h_j(x)=0\\).</li> </ul> <p>Duality converts constraints into prices, and transforms geometry into algebra.</p>"},{"location":"convex/18_duality/#93-the-lagrangian","title":"9.3 The Lagrangian","text":"<p>The Lagrangian function is</p> \\[ L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^p \\lambda_j h_j(x), \\] <p>with:</p> <ul> <li>\\(\\mu_i \\ge 0\\) for inequality constraints,</li> <li>\\(\\lambda_j \\in \\mathbb{R}\\) unrestricted for equalities.</li> </ul> <p>Interpretation:</p> <ul> <li>If \\(\\mu_i &gt; 0\\), violating \\(g_i(x)\\le 0\\) incurs a penalty proportional to \\(\\mu_i\\).</li> <li>If \\(\\mu_i = 0\\), that constraint does not influence the Lagrangian at that point.</li> </ul>"},{"location":"convex/18_duality/#94-the-dual-function-lower-bounds-from-penalties","title":"9.4 The Dual Function: Lower Bounds from Penalties","text":"<p>Fix \\((\\lambda,\\mu)\\) and minimize the Lagrangian with respect to \\(x\\):</p> \\[ \\theta(\\lambda, \\mu) = \\inf_x L(x,\\lambda,\\mu). \\] <p>Because \\(g_i(x) \\le 0\\) for feasible \\(x\\) and \\(\\mu_i \\ge 0\\),</p> \\[ L(x,\\lambda,\\mu) \\le f(x), \\] <p>so taking the infimum over all \\(x\\) yields</p> \\[ \\theta(\\lambda,\\mu) \\le f^\\star. \\] <p>Thus \\(\\theta\\) always produces lower bounds on the true optimal value (weak duality).</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\) (infimum of affine functions).</li> <li>It may be \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul>"},{"location":"convex/18_duality/#95-the-dual-problem","title":"9.5 The Dual Problem","text":"<p>The dual problem maximizes these lower bounds:</p> \\[ \\begin{array}{ll} \\text{maximize}_{\\lambda,\\mu} &amp; \\theta(\\lambda,\\mu) \\\\ \\text{subject to} &amp; \\mu \\ge 0. \\end{array} \\] <p>Let \\(d^\\star\\) be the optimal dual value. Weak duality guarantees:</p> \\[ d^\\star \\le f^\\star. \\] <p>The dual problem is always a concave maximization, i.e., a convex optimization problem in \\((\\lambda,\\mu)\\).</p>"},{"location":"convex/18_duality/#96-strong-duality-and-the-duality-gap","title":"9.6 Strong Duality and the Duality Gap","text":"<p>If</p> \\[ d^\\star = f^\\star, \\] <p>we say strong duality holds. The duality gap is zero.</p>"},{"location":"convex/18_duality/#slaters-condition","title":"Slater\u2019s Condition","text":"<p>If:</p> <ul> <li>\\(g_i\\) are convex,</li> <li>\\(h_j\\) are affine,</li> <li>and there exists a \\(\\tilde{x}\\) such that </li> </ul> <p>then:</p> <ul> <li>strong duality holds (\\(f^\\star = d^\\star\\)),</li> <li>dual maximizers exist,</li> <li>KKT conditions fully characterize primal\u2013dual optimality.</li> </ul> <p>Slater\u2019s condition ensures the feasible region has interior \u2014 the constraints are not tight everywhere.</p>"},{"location":"convex/18_duality/#97-duality-and-the-kkt-conditions","title":"9.7 Duality and the KKT Conditions","text":"<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>"},{"location":"convex/18_duality/#1-primal-feasibility","title":"1. Primal feasibility","text":"\\[ g_i(x^\\star) \\le 0,\\qquad h_j(x^\\star)=0. \\]"},{"location":"convex/18_duality/#2-dual-feasibility","title":"2. Dual feasibility","text":"\\[ \\mu_i^\\star \\ge 0. \\]"},{"location":"convex/18_duality/#3-stationarity","title":"3. Stationarity","text":"\\[ \\nabla f(x^\\star) + \\sum_{i=1}^m \\mu_i^\\star \\nabla g_i(x^\\star) + \\sum_{j=1}^p \\lambda_j^\\star \\nabla h_j(x^\\star) = 0. \\]"},{"location":"convex/18_duality/#4-complementary-slackness","title":"4. Complementary slackness","text":"\\[ \\mu_i^\\star g_i(x^\\star) = 0,\\qquad \\forall i. \\] <p>Together these conditions ensure:</p> \\[ f(x^\\star) = \\theta(\\lambda^\\star,\\mu^\\star) = f^\\star = d^\\star. \\] <p>Geometrically, the gradients of the active constraints form a supporting hyperplane that \u201ctouches\u2019\u2019 the objective exactly at the optimum.</p>"},{"location":"convex/18_duality/#98-interpretation-of-dual-variables","title":"9.8 Interpretation of Dual Variables","text":"<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>"},{"location":"convex/18_duality/#shadow-prices-constraint-forces","title":"Shadow Prices / Constraint Forces","text":"<ul> <li> <p>\\(\\mu_i^\\star\\): the shadow price for relaxing \\(g_i(x)\\le 0\\).   Large \\(\\mu_i^\\star\\) means the constraint is tight and costly to relax.</p> </li> <li> <p>\\(\\lambda_j^\\star\\): sensitivity of the optimal value to perturbations of \\(h_j(x)=0\\).</p> </li> </ul>"},{"location":"convex/18_duality/#ml-interpretations","title":"ML Interpretations","text":"<ul> <li>Support Vector Machines: dual variables select support vectors (only points with \\(\\mu_i^\\star &gt; 0\\) matter).</li> <li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li> <li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li> </ul> <p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-multi-objective-convex-optimization","title":"Chapter 10: Multi-Objective Convex Optimization","text":"<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p> <ul> <li>accuracy vs. regularity,</li> <li>loss vs. fairness,</li> <li>return vs. risk,</li> <li>reconstruction vs. compression,</li> <li>energy use vs. performance.</li> </ul> <p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p> <p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>"},{"location":"convex/18a_pareto/#101-classical-optimality-one-objective","title":"10.1 Classical Optimality (One Objective)","text":"<p>In standard convex optimization, we solve:</p> \\[ x^* \\in \\arg\\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex and \\(\\mathcal{X}\\) is convex. In this setting, it is natural to speak of the minimizer \u2014 or set of minimizers \u2014 because the task is governed by a single quantitative measure.</p> <p>However, when multiple objectives \\((f_1,\\dots,f_k)\\) must be minimized simultaneously, a single \u201cbest\u201d point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#102-multi-objective-convex-optimization","title":"10.2 Multi-Objective Convex Optimization","text":"<p>A multi-objective optimization problem takes the form</p> \\[ \\min_{x \\in \\mathcal{X}} F(x) = (f_1(x), \\dots, f_k(x)), \\] <p>where each \\(f_i\\) is convex. This framework appears in many ML and statistical tasks:</p> Domain Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs. complexity Fair ML Loss Fairness metric Utility vs. fairness Portfolio Return Risk Profit vs. stability Autoencoders Reconstruction KL divergence Fidelity vs. disentanglement <p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes Pareto efficiency.</p>"},{"location":"convex/18a_pareto/#103-pareto-optimality","title":"10.3 Pareto Optimality","text":""},{"location":"convex/18a_pareto/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A point \\(x^*\\) is Pareto optimal if there is no other \\(x\\) such that</p> \\[ f_i(x) \\le f_i(x^*)\\quad \\forall i, \\] <p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>"},{"location":"convex/18a_pareto/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no feasible point satisfies</p> \\[ f_i(x) &lt; f_i(x^*)\\quad \\forall i. \\] <p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>"},{"location":"convex/18a_pareto/#geometric-view","title":"Geometric View","text":"<p>For two objectives \\((f_1, f_2)\\), the feasible set in objective space is a region in \\(\\mathbb{R}^2\\). Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p> <ul> <li>Points on the frontier are the best achievable trade-offs.</li> <li>Points above or inside the region are dominated and thus suboptimal.</li> </ul> <p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>"},{"location":"convex/18a_pareto/#104-scalarization-turning-many-objectives-into-one","title":"10.4 Scalarization: Turning Many Objectives into One","text":"<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>"},{"location":"convex/18a_pareto/#weighted-sum-scalarization","title":"Weighted-Sum Scalarization","text":"\\[ \\min_{x \\in \\mathcal{X}} \\sum_{i=1}^k w_i f_i(x), \\qquad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights encode relative importance.  </li> <li>Varying \\(w\\) traces (part of) the Pareto frontier.  </li> <li>When \\(f_i\\) and \\(\\mathcal{X}\\) are convex, this method recovers the convex portion of the frontier.</li> </ul>"},{"location":"convex/18a_pareto/#-constraint-method","title":"\u03b5-Constraint Method","text":"\\[ \\min_{x} \\ f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\ \\ i = 2,\\dots,k. \\] <ul> <li>Here the tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Each choice of \\(\\varepsilon\\) yields a different Pareto-efficient point.</li> </ul> <p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>"},{"location":"convex/18a_pareto/#duality-connection","title":"Duality Connection","text":"<p>Scalarization has a tight relationship with duality (Chapter 9):</p> <ul> <li>Weights \\(w_i\\) in a weighted sum act like dual variables.</li> <li>Regularization parameters (e.g., the \\(\\lambda\\) in L2 or L1 regularization) correspond to dual multipliers.</li> <li>Moving along \\(\\lambda\\) traces the Pareto frontier between data fit and model complexity.</li> </ul> <p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>"},{"location":"convex/18a_pareto/#105-examples-and-applications","title":"10.5 Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1: Regularized Least Squares","text":"<p>Consider</p> \\[ f_1(x) = \\|Ax - b\\|_2^2,\\qquad  f_2(x) = \\|x\\|_2^2. \\] <p>Two scalarizations:</p> <ol> <li> <p>Weighted:     </p> </li> <li> <p>\u03b5-constraint:     </p> </li> </ol> <p>\\(\\lambda\\) and \\(\\tau\\) trace the same Pareto curve \u2014 the classical bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2: Portfolio Optimization (Risk\u2013Return)","text":"<p>Let \\(w\\) be portfolio weights, \\(\\mu\\) expected returns, and \\(\\Sigma\\) the covariance matrix. Objectives:</p> \\[ f_1(w) = -\\mu^\\top w, \\qquad f_2(w) = w^\\top \\Sigma w. \\] <p>Weighted scalarization:</p> \\[ \\min_w \\ -\\alpha \\mu^\\top w + (1-\\alpha) w^\\top \\Sigma w, \\quad 0 \\le \\alpha \\le 1. \\] <p>Varying \\(\\alpha\\) recovers the efficient frontier of Modern Portfolio Theory.</p>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-in-ml","title":"Example 3: Fairness\u2013Accuracy in ML","text":"\\[ \\min_\\theta \\ \\mathbb{E}[\\ell(y, f_\\theta(x))] \\quad \\text{s.t. } D(f_\\theta(x),y) \\le \\varepsilon, \\] <p>where \\(D\\) is a fairness metric. Scalarized form:</p> \\[ \\min_\\theta\\  \\mathbb{E}[\\ell(y, f_\\theta(x))] + \\lambda D(f_\\theta(x), y). \\] <p>Tuning \\(\\lambda\\) walks across the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-vae","title":"Example 4: Variational Autoencoders and \u03b2-VAE","text":"<p>The ELBO is:</p> \\[ \\mathbb{E}_{q(z)}[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z)). \\] <p>Objectives:</p> <ul> <li>Reconstruction fidelity,</li> <li>Latent simplicity.</li> </ul> <p>\u03b2-VAE scalarization:</p> \\[ \\max_q \\ \\mathbb{E}[\\log p(x|z)] - \\beta \\,\\mathrm{KL}(q(z)\\|p(z)). \\] <p>\\(\\beta\\) controls the trade-off between reconstruction and disentanglement \u2014 a Pareto frontier in latent space.</p> <p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-balancing-fit-and-complexity","title":"Chapter 11:  Balancing Fit and Complexity","text":"<p>Most real-world learning and estimation problems must balance two competing goals:</p> <ol> <li>Fit the observed data well, and  </li> <li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li> </ol> <p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>"},{"location":"convex/18b_regularization/#111-motivation-fit-vs-complexity","title":"11.1 Motivation: Fit vs. Complexity","text":"<p>Suppose we wish to estimate parameters \\(x\\) from data via a loss function \\(f(x)\\). If the data are noisy or the model is high-dimensional, solutions minimizing \\(f\\) alone may be unstable or overly complex. We introduce a regularizer \\(R(x)\\), typically convex, to encourage desirable structure:</p> \\[ \\min_{x} \\; f(x) + \\lambda R(x), \\qquad \\lambda &gt; 0. \\] <ul> <li>\\(f(x)\\): measures data misfit (e.g., squared loss, logistic loss).  </li> <li>\\(R(x)\\): penalizes complexity (e.g., \\(\\ell_1\\) norm for sparsity, \\(\\ell_2\\) norm for smoothness).  </li> <li>\\(\\lambda\\): controls the trade-off.<ul> <li>Small \\(\\lambda\\): excellent data fit, potentially overfitting.  </li> <li>Large \\(\\lambda\\): simpler model, potentially underfitting.</li> </ul> </li> </ul> <p>This is a scalarized multi-objective optimization problem of \\((f, R)\\).</p>"},{"location":"convex/18b_regularization/#112-bicriterion-optimization-and-the-pareto-frontier","title":"11.2 Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization corresponds to the bicriterion objective:</p> \\[ \\min_{x} \\; (f(x), R(x)). \\] <p>A point \\(x^*\\) is Pareto optimal if there is no feasible \\(x\\) such that:  with strict inequality in at least one component.</p> <p>For convex \\(f\\) and \\(R\\):</p> <ul> <li>Every \\(\\lambda \\ge 0\\) yields a Pareto-optimal point,</li> <li>The mapping from \\(\\lambda\\) to constraint level \\(R(x^*)\\) is monotone,</li> <li>The Pareto frontier is convex and can be traced continuously by varying \\(\\lambda\\).</li> </ul> <p>Thus, tuning \\(\\lambda\\) moves the solution along the fit\u2013complexity frontier.</p>"},{"location":"convex/18b_regularization/#113-why-control-the-size-of-the-solution","title":"11.3 Why Control the Size of the Solution?","text":"<p>Inverse problems such as \\(Ax \\approx b\\) are often ill-posed or ill-conditioned:</p> <ul> <li>Small noise in \\(b\\) may cause large variability in the solution \\(x\\).  </li> <li>If \\(A\\) is rank-deficient or nearly singular, infinitely many solutions exist.</li> </ul> <p>Example: ridge regression</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2. \\] <p>The optimality condition is</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b. \\] <p>Benefits of L2 regularization:</p> <ul> <li>\\(A^\\top A + \\lambda I\\) becomes positive definite for any \\(\\lambda &gt; 0\\),  </li> <li>the solution becomes unique and stable,  </li> <li>small singular directions of \\(A\\) are suppressed.</li> </ul> <p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>"},{"location":"convex/18b_regularization/#114-constrained-vs-penalized-formulations","title":"11.4 Constrained vs. Penalized Formulations","text":"<p>Regularized problems can be expressed equivalently as constrained problems:</p> \\[ \\min_x f(x)  \\quad \\text{s.t. } R(x) \\le t. \\] <p>The Lagrangian is</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda (R(x) - t), \\qquad \\lambda \\ge 0. \\] <p>The penalized form</p> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>is the dual of the constrained form. Under convexity and Slater\u2019s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*),  \\] \\[ R(x^*) \\le t,\\qquad \\lambda^* \\ge 0,\\qquad \\lambda^*(R(x^*) - t) = 0. \\] <p>Here:</p> <ul> <li>If \\(R(x^*) &lt; t\\), then \\(\\lambda^* = 0\\).  </li> <li>If \\(\\lambda^* &gt; 0\\), then \\(R(x^*) = t\\) (constraint active).</li> </ul> <p>Thus \\(\\lambda\\) is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>"},{"location":"convex/18b_regularization/#115-common-regularizers-and-their-effects","title":"11.5 Common Regularizers and Their Effects","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"\\[ R(x) = \\|x\\|_2^2. \\] <ul> <li>Smooth and strongly convex.  </li> <li>Shrinks coefficients uniformly.  </li> <li>Improves conditioning.  </li> <li>MAP interpretation: Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"\\[ R(x) = \\|x\\|_1 = \\sum_i |x_i|. \\] <ul> <li>Convex but not differentiable \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball has corners aligned with coordinate axes, encouraging zeros in \\(x\\).  </li> <li>Proximal operator (soft-thresholding):</li> </ul> \\[ \\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(v) = \\operatorname{sign}(v)\\,\\max(|v|-\\tau, 0). \\] <ul> <li>MAP interpretation: Laplace prior.</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"\\[ R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2. \\] <ul> <li>Combines sparsity with numerical stability.  </li> <li>Useful with correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2-structured-regularizers","title":"(d) Beyond L1/L2: Structured Regularizers","text":"Regularizer Formula Effect Tikhonov \\(\\|Lx\\|_2^2\\) smoothness via operator \\(L\\) Total Variation \\(\\|\\nabla x\\|_1\\) piecewise-constant signals/images Group Lasso \\(\\sum_g \\|x_g\\|_2\\) structured sparsity across groups Nuclear Norm \\(\\|X\\|_* = \\sum_i \\sigma_i\\) low-rank matrices <p>Each regularizer defines a geometry for the solution \u2014 ellipsoids, diamonds, polytopes, or spectral shapes.</p>"},{"location":"convex/18b_regularization/#116-choosing-the-regularization-parameter-lambda","title":"11.6 Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-Off Behavior","text":"<ul> <li>\\(\\lambda \\downarrow\\): favors small training error, high variance.  </li> <li>\\(\\lambda \\uparrow\\): favors simplicity, higher bias.  </li> </ul> <p>\\(\\lambda\\) selects a point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#b-cross-validation","title":"(b) Cross-Validation","text":"<p>The most common practice:</p> <ol> <li>Split data into folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining fold.  </li> <li>Choose \\(\\lambda\\) minimizing average validation error.</li> </ol> <p>Guidelines:</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>Use time-aware CV for dependent data.  </li> <li>Use the \u201cone-standard-error rule\u201d for simpler models.</li> </ul>"},{"location":"convex/18b_regularization/#c-other-selection-methods","title":"(c) Other Selection Methods","text":"<ul> <li>Information criteria (AIC, BIC) for sparsity.  </li> <li>L-curve or discrepancy principle in inverse problems.  </li> <li>Regularization paths: computing \\(x^*(\\lambda)\\) for many \\(\\lambda\\).</li> </ul>"},{"location":"convex/18b_regularization/#117-algorithmic-view","title":"11.7 Algorithmic View","text":"<p>Most regularized problems have the form:</p> \\[ \\min_x \\ f(x) + R(x), \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex (possibly nonsmooth).</p> <p>Common algorithms:</p> Method Idea When Useful Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), proximal step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update coordinates cyclically Lasso, Elastic Net ADMM Split problem to exploit structure Large-scale or distributed settings <p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal \\(O(1/k^2)\\) rate for smooth+convex problems.</p>"},{"location":"convex/18b_regularization/#118-bayesian-interpretation","title":"11.8 Bayesian Interpretation","text":"<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p> <p>Linear model:</p> \\[ b = Ax + \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I). \\] <p>With prior \\(x \\sim p(x)\\), MAP estimation solves:</p> \\[ \\min_x \\ \\frac{1}{2\\sigma^2}\\|Ax - b\\|_2^2 - \\log p(x). \\] <p>Examples:</p> <ul> <li>Gaussian prior \\(p(x) \\propto e^{-\\|x\\|_2^2 / (2\\tau^2)}\\)   \u2192 L2 penalty with \\(\\lambda = \\sigma^2/(2\\tau^2)\\).  </li> <li>Laplace prior   \u2192 L1 penalty and sparse MAP estimate.</li> </ul> <p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p> <p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#121-problem-classes-vs-method-classes","title":"12.1 Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#122-first-order-methods-gradient-descent","title":"12.2 First-order methods: Gradient descent","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  </p> <p>Smoothness lets us control step sizes.</p> <p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). Typical choices:</p> <ul> <li>constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known,</li> <li>backtracking line search when \\(L\\) is unknown,</li> <li>diminishing step sizes in some settings.</li> </ul> <p>Derivation: </p> <p>Around \\(x_t\\), we can approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Convergence: For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p> <p>When to use gradient descent:</p> <ul> <li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li> <li>You can compute gradients cheaply.</li> <li>You only need moderate accuracy.</li> <li>Memory constraints rule out storing or factoring Hessians.</li> </ul>"},{"location":"convex/19_optimizationalgo/#123-accelerated-first-order-methods","title":"12.3 Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#1231-nesterov-acceleration","title":"12.3.1 Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences \\(x_k\\) and \\(y_k\\):</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra momentum term \\(\\beta_k (x_{k+1}-x_k)\\) uses past iterates to \u201clook ahead\u201d and can significantly accelerate convergence.</p> <p>Convergece: For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives.</p> <ul> <li>Acceleration is effective for well-behaved smooth convex problems.</li> <li>It can be more sensitive to step size and noise than plain gradient descent.</li> <li>Variants such as FISTA apply acceleration in the composite setting \\(f + R\\).</li> </ul> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#124-steepest-descent-method","title":"12.4 Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization.cGradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>"},{"location":"convex/19_optimizationalgo/#1241-mathematical-properties","title":"12.4.1. Mathematical Properties","text":""},{"location":"convex/19_optimizationalgo/#a-normalized-direction","title":"(a) Normalized direction","text":"<p>  \u2192 unit vector with the most negative directional derivative.</p>"},{"location":"convex/19_optimizationalgo/#b-unnormalized-direction","title":"(b) Unnormalized direction","text":"<p>  This gives the actual direction and magnitude used in updates.</p>"},{"location":"convex/19_optimizationalgo/#c-key-identity","title":"(c) Key identity","text":"<p>  The directional derivative equals the negative squared dual norm of the gradient.</p>"},{"location":"convex/19_optimizationalgo/#1242-the-steepest-descent-method","title":"12.4.2. The Steepest Descent Method","text":"<p>The iterative update rule is:  where \\(t_k &gt; 0\\) is a step size (from line search or a fixed rule).</p> <ul> <li>For the Euclidean norm, this reduces to ordinary gradient descent.</li> <li>For other norms, it adapts the search direction to the geometry of the problem.</li> </ul> <p>Convergence: Similar to gradient descent \u2014 linear for general convex functions, potentially faster when level sets are well-conditioned.</p>"},{"location":"convex/19_optimizationalgo/#1243-role-of-the-norm-and-its-influence","title":"12.4.3. Role of the Norm and Its Influence","text":"<p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#a-euclidean-norm-v_2","title":"(a) Euclidean Norm \\(|v|_2\\)","text":"\\[ \\Delta x_{\\text{nsd}} = -\\frac{\\nabla f(x)}{|\\nabla f(x)|*2}, \\quad \\Delta x*{\\text{sd}} = -\\nabla f(x) \\] <p>This is standard gradient descent. The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>"},{"location":"convex/19_optimizationalgo/#b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0","title":"(b) Quadratic Norm \\(|v|_P = (v^T P v)^{1/2}\\), with \\(P \\succ 0\\)","text":"<p>Here, \\(P\\) defines an ellipsoidal metric. The dual norm is \\(|y|_* = (y^T P^{-1} y)^{1/2}\\).</p> \\[ \\Delta x_{\\text{sd}} = -P^{-1}\\nabla f(x) \\] <p>This corresponds to preconditioned gradient descent, where \\(P\\) rescales directions to counter anisotropy in level sets.</p> <p>Interpretation:</p> <ul> <li>If \\(P\\) approximates the Hessian, this becomes Newton\u2019s method.</li> <li>If \\(P\\) is diagonal, it acts like an adaptive step size per coordinate.</li> </ul>"},{"location":"convex/19_optimizationalgo/#c-ell_1-norm","title":"(c) \\(\\ell_1\\)-Norm","text":"\\[ \\Delta x_{\\text{nsd}} = -e_i, \\quad i = \\arg\\max_j \\left|\\frac{\\partial f}{\\partial x_j}\\right| \\] <p>and</p> \\[ \\Delta x_{\\text{sd}} = -|\\nabla f(x)|_\\infty e_i \\] <p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p> <p>Geometric intuition: The \\(\\ell_1\\)-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p> <ul> <li>In \\(\\ell_2\\)-norm: the unit ball is a circle \u2192 the steepest direction is exactly opposite the gradient.</li> <li>In \\(\\ell_1\\)-norm: the unit ball is a diamond \u2192 the steepest direction points to a corner (one coordinate).</li> <li>In quadratic norms: the unit ball is an ellipsoid \u2192 the steepest direction follows the metric-adjusted gradient.</li> </ul> <p>Hence, the norm defines the geometry of what \u201csteepest\u201d means.</p>"},{"location":"convex/19_optimizationalgo/#125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives","title":"12.5 Conjugate Gradient Method \u2014 Fast Optimization for Quadratic Objectives","text":"<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p> <p>CG is a first-order method that behaves like a second-order method for quadratics.</p> <p>For a quadratic objective function:</p> \\[ f(x) = \\tfrac12 x^\\top A x - b^\\top x  \\] <p>with \\(A \\succ 0\\), the level sets are ellipses shaped by the eigenvalues of \\(A\\). If \\(A\\) is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p> <p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors \\(p_i, p_j\\) are said to be A-conjugate if</p> \\[ p_i^\\top A p_j = 0. \\] <p>This is orthogonality measured in the geometry induced by the Hessian \\(A\\). Why is this useful?</p> <ul> <li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of \\(A\\).</li> <li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li> <li>After \\(n\\) mutually A-conjugate directions, all curvature directions are resolved \u2192 exact solution.</li> </ul> <p>In contrast, gradient descent repeatedly re-corrects previous progress.</p> <p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system \\(Ax = b\\). Let</p> \\[ r_0 = b - A x_0, \\qquad p_0 = r_0. \\] <p>For \\(k = 0,1,2,\\dots\\):</p> <ol> <li> <p>Step size     </p> </li> <li> <p>Update iterate     </p> </li> <li> <p>Update residual (negative gradient)     </p> </li> <li> <p>Direction scaling     </p> </li> <li> <p>New conjugate direction     </p> </li> </ol> <p>Stop when \\(\\|r_k\\|\\) is below tolerance.</p> <p>Every new direction \\(p_{k+1}\\) is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p> <p>Why CG Is Fast: For an \\(n\\)-dimensional quadratic, CG solves the problem in at most \\(n\\) iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in \\(O(\\sqrt{\\kappa})\\) iterations, where \\(\\kappa = \\lambda_{\\max}/\\lambda_{\\min}\\) is the condition number. The convergence bound in the A-norm is:</p> \\[ \\|x_k - x^\\star\\|_A \\le  2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k  \\|x_0 - x^\\star\\|_A. \\] <p>This is dramatically better than the \\(O(1/k)\\) rate of gradient descent.</p> <p>CG is ideal when:</p> <ul> <li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix \\(A\\).</li> <li>\\(A\\) is large and sparse or available as a matrix\u2013vector product.</li> <li>You cannot form or store \\(A^{-1}\\) or even the full matrix \\(A\\).</li> <li>You want a Hessian-aware method but cannot afford Newton\u2019s method.</li> </ul> <p>Typical scenarios:</p> Application Why CG fits Large linear systems \\(A x = b\\) Only requires \\(A p\\), not factorization. Ridge regression Normal equations form an SPD matrix. Kernel ridge regression Solves \\((K+\\lambda I)\\alpha = y\\) efficiently. Newton steps in ML Inner solver for Hessian systems without forming Hessian. PDEs and scientific computing Sparse SPD matrices, ideal for CG. <p>Assumptions Required for CG: To guarantee correctness of linear CG, we require:</p> <ul> <li>\\(A\\) is symmetric</li> <li>\\(A\\) is positive definite</li> <li>Objective is strictly convex quadratic</li> <li>Arithmetic is exact (for the finite-step guarantee)</li> </ul> <p>If the function is not quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p> <p>Practical Notes:</p> <ul> <li>You only need matrix\u2013vector products \\(Ap\\).  </li> <li>Storage cost is \\(O(n)\\).  </li> <li>Preconditioning (replacing the system with \\(M^{-1} A\\)) improves conditioning and accelerates convergence dramatically.  </li> <li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li> </ul> <p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>"},{"location":"convex/19_optimizationalgo/#126-newtons-method-and-second-order-methods","title":"12.6 Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p> <p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex \\(f\\), Newton\u2019s method achieves quadratic convergence\u2014dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order algorithms.</p> <p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton\u2019s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>"},{"location":"convex/19_optimizationalgo/#solving-the-newton-system","title":"Solving the Newton System","text":"<p>Each iteration requires solving</p> \\[ H \\,\\Delta x = -g, \\qquad H = \\nabla^2 f(x), \\;\\; g = \\nabla f(x). \\] <p>If \\(H\\) is symmetric positive definite, a Cholesky factorization</p> \\[ H = L L^\\top \\] <p>allows efficient and numerically stable solution via two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^\\top \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids forming \\(H^{-1}\\) explicitly.</p> <p>The Newton decrement:</p> \\[ \\lambda(x) = \\|L^{-1} g\\|_2 \\] <p>gauges proximity to the optimum and provides a natural stopping criterion: \\(\\lambda(x)^2/2 &lt; \\varepsilon\\).</p> <p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs \\(\\approx (1/3)n^3\\) operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton\u2019s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix\u2013free iterative methods.</p>"},{"location":"convex/19_optimizationalgo/#gaussnewton-method","title":"Gauss\u2013Newton Method","text":"<p>The Gauss\u2013Newton method is a specialization of Newton\u2019s method for nonlinear least squares problems</p> \\[ f(x) = \\tfrac12 \\| r(x) \\|^2, \\] <p>where \\(r(x)\\) is a vector of residual functions and a nonlinear function of \\(x\\) and \\(J\\) is its Jacobian. Newton\u2019s Hessian decomposes as</p> \\[ \\nabla^2 f(x) = J^\\top J \\;+\\; \\sum_i r_i(x)\\, \\nabla^2 r_i(x). \\] <p>The second term involves the curvature of the residuals. When \\(r(x)\\) is approximately linear near the optimum, this term is small. Gauss\u2013Newton drops it, giving the approximation</p> \\[ \\nabla^2 f(x) \\approx J^\\top J, \\] <p>leading to the Gauss\u2013Newton step:</p> \\[ (J^\\top J)\\, \\Delta = -J^\\top r. \\] <p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg\u2013Marquardt method adds a damping term,</p> \\[ (J^\\top J + \\lambda I)\\, \\Delta = -J^\\top r, \\] <p>which interpolates smoothly between  </p> <ul> <li>gradient descent (large \\(\\lambda\\)), and  </li> <li>Gauss\u2013Newton (small \\(\\lambda\\)).</li> </ul> <p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss\u2013Newton and Levenberg\u2013Marquardt are highly effective when the residuals are nearly linear\u2014common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning\u2014yielding fast convergence without the expense of full second derivatives.</p>"},{"location":"convex/19_optimizationalgo/#quasi-newton-methods","title":"Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p> <p>When to use Newton or quasi-Newton methods:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#128-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"12.8 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#1281-projected-gradient-descent","title":"12.8.1 Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#1282-proximal-gradient-forwardbackward-splitting","title":"12.8.2 Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#129-penalties-barriers-and-interior-point-methods","title":"12.9 Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\): Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#1291-penalty-methods","title":"12.9.1 Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-basic-penalty-method-quadratic-or-general-penalization","title":"Algorithm: Basic Penalty Method (Quadratic or General Penalization)","text":"<p>Goal:  Solve </p> <p>Penalty formulation:  where  </p> <ul> <li>\\(\\phi(r) = 0\\) if \\(r \\le 0\\),  </li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (e.g., \\(\\phi(r)=\\max\\{0,r\\}^2\\)),  </li> <li>\\(\\rho &gt; 0\\) is the penalty weight.</li> </ul> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>constraints \\(g_i(x)\\) </li> <li>penalty function \\(\\phi\\) </li> <li>initial point \\(x_0\\) </li> <li>initial penalty parameter \\(\\rho_0 &gt; 0\\) </li> <li>penalty update factor \\(\\gamma &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose \\(x_0\\), \\(\\rho_0 &gt; 0\\).  </li> <li>For \\(k = 0, 1, 2, \\dots\\):  <ol> <li>Solve the penalized subproblem  \\(x_{k+1} = \\arg\\min_x F_{\\rho_k}(x)\\) using Newton\u2019s method, gradient descent, quasi-Newton, etc.  </li> <li>Check feasibility / stopping:  If \\(\\max_i g_i(x_{k+1}) \\le \\varepsilon, \\quad   \\|x_{k+1} - x_k\\| \\le \\varepsilon\\)  stop and return \\(x_{k+1}\\).  </li> <li>Increase penalty parameter  \\(\\rho_{k+1} = \\gamma\\, \\rho_k\\)   with typical \\(\\gamma \\in [5,10]\\).  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#1292-barrier-methods","title":"12.9.2 Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-barrier-method-logarithmic-barrier-interior-approximation","title":"Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)","text":"<p>Goal: Solve the constrained problem </p> <p>Logarithmic barrier:  defined only for strictly feasible points \\(g_i(x)&lt;0\\).</p> <p>Barrier subproblem:  where \\(t&gt;0\\) is the barrier parameter.</p> <p>As \\(t \\to \\infty\\), minimizers of \\(F_t\\) approach the constrained optimum.</p> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>barrier function \\(b(x)\\) </li> <li>strictly feasible starting point \\(x_0\\) (\\(g_i(x_0) &lt; 0\\))  </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>barrier growth factor \\(\\mu &gt; 1\\) (often \\(\\mu = 10\\))  </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose strictly feasible \\(x_0\\), and pick \\(t_0 &gt; 0\\).  </li> <li>For \\(k = 0,1,2,\\dots\\):  <ol> <li>Centering step (inner loop):  Solve the barrier subproblem    Typically use Newton\u2019s method (damped) on \\(F_{t_k}\\).  Stop when the Newton decrement satisfies  \\(\\lambda(x_{k+1})^2/2 \\le \\varepsilon\\)</li> <li>Optimality / stopping test:    If  \\(\\frac{m}{t_k} \\le \\varepsilon,\\)   then \\(x_{k+1}\\) is an \\(\\varepsilon\\)-approximate solution of the original constrained problem; stop and return \\(x_{k+1}\\).  </li> <li>Increase barrier parameter:  \\(t_{k+1} = \\mu\\, t_k,\\)   which tightens the approximation and moves closer to the boundary.  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#1293-interior-point-methods","title":"12.9.3 Interior-point methods","text":"<p>Interior-point methods combine barrier functions with Newton\u2019s method to solve general convex programs:</p> <ul> <li>They maintain strict feasibility throughout.</li> <li>Each iteration solves a Newton system for the barrier-augmented objective.</li> <li>They naturally generate primal\u2013dual pairs and duality gap estimates.</li> <li>Under standard assumptions (e.g., Slater\u2019s condition), they converge in a predictable number of iterations.</li> </ul> <p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>"},{"location":"convex/19_optimizationalgo/#algorithm-primaldual-interior-point-method-for-convex-inequality-constraints","title":"Algorithm: Primal\u2013Dual Interior-Point Method (for convex inequality constraints)","text":"<p>We consider the problem  </p> <p>Introduce Lagrange multipliers \\(\\lambda \\ge 0\\). The KKT conditions are  </p> <p>Interior-point methods enforce the relaxed condition  which keeps iterates strictly feasible.</p>"},{"location":"convex/19_optimizationalgo/#inputs","title":"Inputs","text":"<ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>initial primal point \\(x_0\\) with \\(g_i(x_0)&lt;0\\) </li> <li>initial dual variable \\(\\lambda_0 &gt; 0\\) </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>growth factor \\(\\mu &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul>"},{"location":"convex/19_optimizationalgo/#procedure","title":"Procedure","text":"<ol> <li> <p>Choose strictly feasible \\(x_0\\), positive \\(\\lambda_0\\), and \\(t_0\\).</p> </li> <li> <p>For \\(k = 0,1,2,\\dots\\):</p> <p>(a) Form the perturbed KKT system.  Solve for the Newton direction \\((\\Delta x, \\Delta \\lambda)\\):</p> <p> </p> <p>(b) Line search to keep strict feasibility. Choose the maximum \\(\\alpha\\in(0,1]\\) such that:</p> <ul> <li>\\(g_i(x + \\alpha \\Delta x) &lt; 0\\),</li> <li>\\(\\lambda + \\alpha \\Delta \\lambda &gt; 0\\).</li> </ul> <p>(c) Update: \\(x \\leftarrow x + \\alpha \\Delta x,    \\qquad  \\lambda \\leftarrow \\lambda + \\alpha \\Delta \\lambda.\\)</p> <p>(d) Check duality gap: \\(\\text{gap} = - g(x)^\\top \\lambda\\) If \\(\\text{gap} \\le \\varepsilon\\), stop.</p> <p>(e) Increase barrier parameter \\(t \\leftarrow \\mu t.\\)</p> </li> <li> <p>Return \\(x\\).</p> </li> </ol>"},{"location":"convex/19_optimizationalgo/#1210-choosing-the-right-method-in-practice","title":"12.10 Choosing the right method in practice","text":"<p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever the variables must satisfy one or more exact relations \u2014 such as conservation laws, normalization, or fairness criteria. We study algorithms for minimizing a function subject to linear or nonlinear equality constraints:</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>Such problems are fundamental in convex optimization, quadratic programming, and many ML formulations involving exact invariants.</p>"},{"location":"convex/19a_optimization_constraints/#131-geometric-view-optimization-on-an-affine-manifold","title":"13.1 Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set, a lower-dimensional plane within \\(\\mathbb{R}^n\\). The feasible region is:</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank (\\(\\operatorname{rank}(A)=p\\)), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine manifold.</p> <p>Geometrically, optimization proceeds not over all \\(\\mathbb{R}^n\\), but along this manifold. At the optimum, the gradient \\(\\nabla f(x^\\star)\\) cannot point in a direction that stays feasible\u2014hence it must be orthogonal to the feasible surface. This gives the first key optimality relation:</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star, \\] <p>where \\(\\nu^\\star\\) is a vector of Lagrange multipliers capturing how sensitive the objective is to constraint perturbations.</p> <p>Intuition: The gradient of the objective at the optimum lies in the span of the constraint normals (rows of \\(A\\)). Any feasible direction must lie in the null space of \\(A\\), orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#132-lagrange-function-and-kkt-system","title":"13.2 Lagrange Function and KKT System","text":"<p>Define the Lagrangian:</p> \\[ \\mathcal{L}(x, \\nu) = f(x) + \\nu^\\top (A x - b). \\] <p>The first-order (KKT) conditions for a feasible point \\((x^\\star, \\nu^\\star)\\) to be optimal are:</p> \\[ \\begin{aligned} \\nabla f(x^\\star) + A^\\top \\nu^\\star &amp;= 0, \\\\ A x^\\star &amp;= b. \\end{aligned} \\] <p>These equations express stationarity and feasibility simultaneously. They can be combined into the KKT linear system:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>At the optimum, the right-hand side is zero.</p> <p>ML Connection: Lagrange multipliers \\(\\nu\\) quantify trade-offs between objectives and hard constraints \u2014 for instance, enforcing weight normalization in a neural layer, balance constraints in fair classification, or conservation laws in physics-informed networks.</p>"},{"location":"convex/19a_optimization_constraints/#133-the-quadratic-case","title":"13.3 The Quadratic Case","text":"<p>For a quadratic objective  with \\(P \\succeq 0\\), the KKT conditions reduce to a linear system:</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>This is a saddle-point system, solvable by factorization or elimination. If \\(P \\succ 0\\) and \\(A\\) has full row rank, the solution \\((x^\\star, \\nu^\\star)\\) is unique.</p> <p>In ML, such systems appear in constrained least squares, e.g. enforcing \\(\\sum_i w_i = 1\\) in portfolio optimization or convex combination weights in mixture models.</p>"},{"location":"convex/19a_optimization_constraints/#134-the-null-space-reduced-variable-method","title":"13.4 The Null-Space (Reduced Variable) Method","text":"<p>If \\(A\\) has full row rank, we can find a particular feasible point \\(x_0\\) such that \\(A x_0 = b\\), and a basis \\(Z\\) for the null space of \\(A\\) satisfying \\(A Z = 0\\). Then any feasible \\(x\\) can be written as:</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective gives a reduced problem:</p> \\[ \\min_y \\; f(x_0 + Z y). \\] <p>This is an unconstrained problem in \\(y\\), solvable by gradient or Newton methods. The reduced gradient and Hessian are:</p> \\[ \\nabla_y f = Z^\\top \\nabla_x f, \\qquad \\nabla_y^2 f = Z^\\top \\nabla_x^2 f \\, Z. \\] <p>Interpretation: Optimization proceeds only along feasible directions \u2014 those that do not violate the constraints (i.e., within \\(\\operatorname{Null}(A)\\)). This is equivalent to projecting all gradient steps onto the tangent space of the constraint manifold.</p>"},{"location":"convex/19a_optimization_constraints/#135-newtons-method-for-equality-constrained-problems","title":"13.5 Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice differentiable \\(f\\), the equality-constrained Newton step solves the quadratic subproblem:</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d, \\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>This produces the step \\((d, \\lambda)\\) from the linearized KKT system:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>The update is \\(x_{k+1} = x_k + \\alpha d\\), ensuring \\(A x_{k+1} = b\\) if \\(A x_k = b\\).</p> <p>Geometric insight: The Newton direction is the projection of the unconstrained Newton step onto the tangent space of the feasible set (directions satisfying \\(A d = 0\\)). Thus, each step stays within the affine constraint manifold.</p> <p>In practice: The KKT system is typically solved by Schur complement factorization:  which then yields \\(d = -(\\nabla^2 f)^{-1} (\\nabla f + A^\\top \\lambda)\\).</p>"},{"location":"convex/19a_optimization_constraints/#136-infeasible-start-newton-method","title":"13.6 Infeasible Start Newton Method","text":"<p>When starting from an infeasible point (\\(A x_0 \\ne b\\)), we relax the constraint and drive feasibility progressively. At iteration \\(k\\), compute \\((\\Delta x, \\Delta \\nu)\\) by solving:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x_k) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x_k) + A^\\top \\nu_k \\\\ A x_k - b \\end{bmatrix}. \\] <p>Then update:</p> \\[ x_{k+1} = x_k + \\alpha \\Delta x, \\quad \\nu_{k+1} = \\nu_k + \\alpha \\Delta \\nu. \\] <p>This method enforces feasibility gradually, converging to \\((x^\\star, \\nu^\\star)\\) under mild conditions.</p> <p>In ML contexts, infeasible starts are typical \u2014 we rarely have feasible initialization (e.g., in constrained autoencoders or regularized fairness models). The infeasible Newton method ensures consistent progress in both primal feasibility (\\(A x = b\\)) and dual stationarity (\\(\\nabla f + A^\\top \\nu = 0\\)).</p>"},{"location":"convex/19a_optimization_constraints/#137-computational-considerations","title":"13.7 Computational Considerations","text":"<ul> <li>Factorization: KKT systems can be large but structured. Exploiting sparsity in \\(\\nabla^2 f\\) and \\(A\\) is essential in high-dimensional problems.</li> <li>Stability: Adding small regularization to the (0,0) block of the KKT matrix improves conditioning:    </li> <li>Schur Complement: Eliminating \\(\\Delta x\\) yields a smaller linear system in \\(\\Delta \\nu\\), which can be more efficient when \\(p \\ll n\\).</li> </ul>"},{"location":"convex/19a_optimization_constraints/#138-connections-to-machine-learning","title":"13.8 Connections to Machine Learning","text":"<p>Equality-constrained optimization appears in several ML and signal processing settings:</p> Example Equality Constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights must sum to 1 Fair classification \\(A w = 0\\) Enforces equal outcomes across groups Orthogonal embeddings \\(W^\\top W = I\\) Preserves independence / energy Normalization layers \\(\\|w\\|_2^2 = 1\\) Scale invariance constraint Physics-informed models \\(\\text{div}(F)=0\\) Conservation of mass / charge"},{"location":"convex/19a_optimization_constraints/#summary-approaches-to-equality-constrained-optimization","title":"Summary: Approaches to Equality-Constrained Optimization","text":"Approach Constraint Type Feasibility (Local/Global) Core Idea Advantages Limitations / Drawbacks Typical ML / Optimization Use Null-Space (Variable Elimination) Linear, full-rank \\(A\\) Global Parameterize feasible \\(x = x_0 + Z y\\) with \\(A Z = 0\\) Converts to unconstrained problem; dimension reduction; exact Requires null-space basis \\(Z\\); destroys sparsity; expensive for large \\(A\\) Constrained least squares, small-scale convex programs Local Parameterization (Manifold Method) Nonlinear \\(g(x) = 0\\) Local (around feasible point) Use implicit function theorem: locally express \\(x = x(y)\\) Captures nonlinear manifold structure; geometric insight Valid only locally; requires Jacobians; expensive Manifold learning, orthogonal embeddings, equality-regularized networks KKT / Lagrange System Linear or nonlinear Global (if convex) Solve coupled system \\(\\nabla f + A^\\top \\nu = 0\\), \\(A x = b\\) Keeps structure; allows dual interpretation; works for large sparse systems Larger system; more variables Quadratic programming, convex solvers, equality-constrained ML models Primal\u2013Dual Newton Method Linear or nonlinear Global (convex) Newton\u2019s method on full KKT system Quadratic convergence near optimum; stable numerically Requires Hessians and factorizations Interior-point solvers, primal\u2013dual optimization, barrier methods Penalty / Augmented Lagrangian General (convex or nonconvex) Approximate (drives feasibility) Add penalty term \\(\\tfrac{\\rho}{2}\\|A x - b\\|^2\\) or dual updates Simple to implement; smooth transition from unconstrained Needs tuning of \\(\\rho\\); slow convergence to exact feasibility Regularized fairness, soft constraints, physics-informed networks Projection / Normalization Step Linear or nonlinear (simple form) Iterative (after each step) Project back to feasible set: \\(x_{k+1} = \\Pi_{\\{A x = b\\}}(x_{k+1})\\) Keeps updates feasible; easy for simple constraints Costly for complex \\(A\\); may distort gradient direction Normalization layers, unit-norm or balance constraints"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In practice, optimization problems often include inequalities that restrict feasible solutions to a convex region. Examples include nonnegativity of variables, margin constraints in support vector machines, fairness or safety limits, and physical conservation laws. This chapter introduces algorithms for solving such problems efficiently, focusing on the logarithmic barrier and interior-point methods that underpin modern convex solvers.</p>"},{"location":"convex/19b_optimization_constraints/#141-problem-setup","title":"14.1 Problem Setup","text":"<p>We consider the general convex optimization problem with both equality and inequality constraints:</p> \\[ \\begin{aligned} \\text{minimize}   &amp;\\quad f_0(x) \\\\ \\text{subject to} &amp;\\quad f_i(x) \\le 0, \\quad i=1,\\dots,m,\\\\ &amp;\\quad A x = b. \\end{aligned} \\] <p>Assumptions:</p> <ul> <li>Each \\(f_i\\) is convex and twice differentiable.  </li> <li>\\(A \\in \\mathbb{R}^{p\\times n}\\) has full row rank (\\(\\mathrm{rank}(A)=p\\)).  </li> <li>There exists a strictly feasible point \\(\\bar{x}\\) such that \\(f_i(\\bar{x})&lt;0\\) and \\(A\\bar{x}=b\\) (Slater\u2019s condition).  </li> </ul> <p>Under these assumptions, strong duality holds and the KKT conditions are necessary and sufficient for optimality.</p>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem \\(f_0(x)\\) \\(f_i(x)\\) Notes / ML context Linear Program (LP) \\(c^T x\\) \\(a_i^T x - b_i\\) Feature selection, resource allocation Quadratic Program (QP) \\(\\tfrac{1}{2}x^T P x + q^T x\\) Linear \\(a_i^T x - b_i\\) SVM training, ridge regression QCQP Quadratic Quadratic Portfolio optimization, control Geometric Program (log domain) Convex in \\(\\log x\\) Linear in \\(\\log x\\) Network flow, resource allocation Entropy minimization \\(\\sum_i x_i \\log x_i\\) \\(F x \\le g\\) Probability calibration, information bottleneck"},{"location":"convex/19b_optimization_constraints/#142-indicator-function-reformulation","title":"14.2 Indicator-Function Reformulation","text":"<p>Define the indicator of the nonpositive orthant:</p> \\[ I_-(u)= \\begin{cases} 0, &amp; u \\le 0,\\\\ +\\infty, &amp; u &gt; 0. \\end{cases} \\] <p>Then the constrained problem is equivalent to</p> \\[ \\min_x \\; f_0(x) + \\sum_{i=1}^m I_-(f_i(x)) \\quad \\text{s.t. } A x = b. \\] <p>This form is conceptually clear but nondifferentiable since \\(I_-\\) is discontinuous. To apply Newton-type algorithms, we replace \\(I_-\\) with a smooth approximation: the logarithmic barrier.</p>"},{"location":"convex/19b_optimization_constraints/#143-logarithmic-barrier-approximation","title":"14.3 Logarithmic-Barrier Approximation","text":"<p>We approximate each \\(I_-(f_i(x))\\) by a differentiable barrier function \\(\\Phi(u) = -\\tfrac{1}{t} \\log(-u)\\) for \\(u &lt; 0\\). The smoothed subproblem becomes</p> \\[ \\min_x \\; f_0(x) - \\frac{1}{t} \\sum_{i=1}^m \\log(-f_i(x)) \\quad \\text{s.t. } A x = b. \\] <ul> <li>For small \\(t\\): the barrier is strong and keeps points deep inside the feasible region.  </li> <li>As \\(t \\to \\infty\\): the barrier weakens and the solution approaches the true optimum.</li> </ul> <p>Hence, the original inequality-constrained problem is replaced by a sequence of smooth equality-constrained subproblems.</p>"},{"location":"convex/19b_optimization_constraints/#144-properties-of-the-barrier-function","title":"14.4 Properties of the Barrier Function","text":"<p>Define</p> \\[ \\phi(x) = -\\sum_{i=1}^m \\log(-f_i(x)), \\qquad \\mathrm{dom}\\,\\phi = \\{x : f_i(x) &lt; 0\\}. \\] <p>Then \\(\\phi\\) is convex and twice differentiable:</p> \\[ \\nabla \\phi(x) = \\sum_i \\frac{1}{-f_i(x)} \\nabla f_i(x), \\] \\[ \\nabla^2 \\phi(x) = \\sum_i \\frac{1}{f_i(x)^2} \\nabla f_i(x)\\nabla f_i(x)^T + \\sum_i \\frac{1}{-f_i(x)} \\nabla^2 f_i(x). \\] <p>Near the boundary \\(f_i(x)=0\\), the gradient norm grows without bound \u2014 producing a repulsive force that prevents violation of constraints.</p>"},{"location":"convex/19b_optimization_constraints/#145-central-path-and-approximate-kkt-conditions","title":"14.5 Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^*(t)\\) minimize the barrier problem</p> \\[ \\min_x\\; t f_0(x) + \\phi(x) \\quad \\text{s.t. } A x = b. \\] <p>The curve \\(\\{x^*(t) : t &gt; 0\\}\\) is the central path. As \\(t \\to \\infty\\), \\(x^*(t)\\) approaches the true optimal solution \\(x^*\\). Along this path there exist dual variables \\((\\lambda^*(t), v^*(t))\\) satisfying</p> \\[ \\begin{aligned} \\nabla f_0(x^*(t)) + \\sum_i \\lambda_i^*(t) \\nabla f_i(x^*(t)) + A^T v^*(t) &amp;= 0,\\\\ A x^*(t) &amp;= b,\\\\ -\\lambda_i^*(t) f_i(x^*(t)) &amp;= \\tfrac{1}{t}, \\quad \\lambda_i^*(t) \\ge 0. \\end{aligned} \\] <p>The complementarity condition is relaxed: \\(\\lambda_i f_i(x) = -1/t\\) instead of \\(0\\). As \\(t \\to \\infty\\), these approximate KKT conditions converge to the exact ones.</p>"},{"location":"convex/19b_optimization_constraints/#146-geometric-and-physical-intuition","title":"14.6 Geometric and Physical Intuition","text":"<p>The centering subproblem</p> \\[ \\min_x\\; t f_0(x) - \\sum_i \\log(-f_i(x)) \\] <p>can be viewed as a particle system in a potential field:</p> <ul> <li>The objective \\(f_0(x)\\) pulls toward lower cost (external force).  </li> <li>Each constraint \\(f_i(x)\\le0\\) creates a repulsive potential that diverges near the boundary.  </li> </ul> <p>At equilibrium, these forces balance:</p> \\[ \\nabla f_0(x^*(t)) + \\sum_i \\frac{1}{t(-f_i(x^*(t)))} \\nabla f_i(x^*(t)) = 0. \\] <p>Thus, the solution remains strictly feasible \u2014 this is the essence of the interior-point philosophy.</p>"},{"location":"convex/19b_optimization_constraints/#147-barrier-method-algorithm","title":"14.7 Barrier-Method Algorithm","text":"<p>The barrier method converts the original inequality-constrained problem into a sequence of smooth equality-constrained subproblems. Each subproblem is solved exactly (to high precision) while a barrier parameter \\(t\\) is gradually increased, allowing the iterates to approach the boundary and the true constrained optimum.</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-outline","title":"Algorithm Outline","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (so \\(f_i(x) &lt; 0\\) for all \\(i\\)),</li> <li>an initial barrier parameter \\(t &gt; 0\\),</li> <li>a barrier scaling factor \\(\\mu &gt; 1\\) (usually between 10 and 20),</li> <li>and a desired accuracy \\(\\varepsilon &gt; 0\\) (for stopping),</li> </ul> <p>the algorithm proceeds as follows:</p> <ol> <li> <p>Centering step: Solve     using Newton\u2019s method for equality-constrained optimization. The result \\(x^*(t)\\) is the centering point for the current \\(t\\).</p> </li> <li> <p>Update iterate:  Set \\(x := x^*(t)\\).</p> </li> <li> <p>Stopping criterion:  Stop if         Here \\(m\\) is the number of inequality constraints, and \\(\\varepsilon\\) is the desired tolerance on suboptimality. This rule is derived from the duality gap bound:        meaning that if \\(m/t\\) is smaller than \\(\\varepsilon\\), the current solution is guaranteed to be within \\(\\varepsilon\\) of the true optimum.</p> </li> <li> <p>Increase barrier parameter: Set \\(t := \\mu t\\) and return to Step 1.    Each centering subproblem maintains strict feasibility, and increasing \\(t\\) gradually weakens the barrier, allowing the iterates to approach the true constraint boundary. A typical choice is \\(\\mu \\in [10, 20]\\).</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#understanding-varepsilon-the-accuracy-parameter","title":"Understanding \\(\\varepsilon\\) \u2014 the Accuracy Parameter","text":"<p>The parameter \\(\\varepsilon\\) controls how close to the optimal solution we wish to stop.</p> <ul> <li> <p>Mathematically, \\(\\varepsilon\\) specifies an upper bound on the duality gap:    </p> </li> <li> <p>Conceptually, \\(\\varepsilon\\) represents the trade-off between accuracy and computational cost:</p> </li> <li>Smaller \\(\\varepsilon\\) \u2192 more iterations (larger \\(t\\) required).</li> <li>Larger \\(\\varepsilon\\) \u2192 faster termination, but lower accuracy.</li> </ul> <p>In practice:</p> <ul> <li>For numerical optimization or ML training, \\(\\varepsilon\\) is often set between \\(10^{-3}\\) and \\(10^{-8}\\) depending on problem size and desired precision.  </li> <li>Convex solvers (like CVX, MOSEK, or ECOS) typically use \\(\\varepsilon \\approx 10^{-6}\\) as a default high-accuracy target.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#intuitive-interpretation","title":"Intuitive Interpretation","text":"<ul> <li>Think of \\(\\varepsilon\\) as the \u201cdistance\u201d between the current point and the true optimum in terms of objective value.  </li> <li>The ratio \\(m/t\\) acts like a thermometer for this distance \u2014 as \\(t\\) grows, the temperature (error) cools down.</li> <li>Once \\(m/t &lt; \\varepsilon\\), we know the algorithm has cooled sufficiently: the point lies extremely close to the optimal constrained solution.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#summary-of-key-parameters","title":"Summary of Key Parameters","text":"Symbol Meaning Typical Value / Range Intuitive Role \\(m\\) Number of inequality constraints problem dependent Total number of barrier terms \\(t\\) Barrier parameter starts small (1\u201310), grows by \\(\\mu\\) Controls strength of barrier \\(\\mu\\) Barrier growth factor 10\u201320 Controls how fast we approach constraint boundary \\(\\varepsilon\\) Desired accuracy (tolerance) \\(10^{-3}\\) to \\(10^{-8}\\) Stopping threshold based on duality gap"},{"location":"convex/19b_optimization_constraints/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Each centering step finds the best interior point for a given barrier strength \\(1/t\\).  </li> <li>Increasing \\(t\\) reduces the barrier effect, letting \\(x\\) approach the boundary.  </li> <li>The stopping rule \\(m/t &lt; \\varepsilon\\) ensures that the objective value of \\(x\\) differs from the true optimum by less than \\(\\varepsilon\\).  </li> <li>Smaller \\(\\varepsilon\\) means tighter optimality, but more work (larger \\(t\\) and more iterations).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#148-computational-and-practical-notes","title":"14.8 Computational and Practical Notes","text":"<ul> <li>Each centering problem is solved by equality-constrained Newton steps (KKT system).  </li> <li>Barrier methods inherit superlinear convergence near the optimum.  </li> <li>Initialization must be strictly feasible; feasibility restoration can be costly.  </li> <li>Large \\(t\\) makes the barrier steep, so line search and step damping are essential.</li> </ul> <p>In machine learning: - SVM and logistic regression margin constraints fit naturally in this form. - Interior-point solvers for QPs are used in sparse regression and convex relaxations. - Barrier penalties act as smooth approximations to hard constraints in physics-informed and fairness-aware models.</p>"},{"location":"convex/19b_optimization_constraints/#149-comparison-equality-vs-inequality-constrained-methods","title":"14.9 Comparison: Equality vs Inequality-Constrained Methods","text":"Aspect Equality Constraints Inequality Constraints Feasible set Affine manifold Convex region with boundary Algorithms Newton, projected Newton, KKT Barrier, interior-point, primal\u2013dual Feasibility handling Exact Maintained via barrier term Complementarity \\(A x = b\\) \\(\\lambda_i f_i(x) = 0\\) (or \\(= -1/t\\)) Feasible start Optional Required (strict) ML relevance Normalization, fairness, balance Nonnegativity, margins, sparsity, safety constraints"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often operates at massive scales \u2014 millions of variables, billions of data points, or constraints distributed across devices and networks. Classical Newton or interior-point algorithms, while theoretically elegant, become computationally impractical in these regimes.  </p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to solve large-scale convex problems efficiently. These ideas underpin the optimization engines behind most machine learning systems.</p>"},{"location":"convex/20_advanced/#151-motivation-structure-and-scale","title":"15.1 Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the difficulty lies not in theory but in computation.</p> <ul> <li>Memory limits: Storing the full Hessian or even the gradient can be infeasible.  </li> <li>Data size: Evaluating the objective over the full dataset is expensive.  </li> <li>Distributed data: Information may be spread across machines or devices.  </li> <li>Sparsity and separability: Many objectives decompose nicely into smaller components.</li> </ul> <p>Thus, the goal is to design algorithms that make incremental or local progress while exploiting the structure of the problem.</p> <p>Typical forms include:  where:</p> <ul> <li>each \\(f_i(x)\\) represents a data-sample loss term, and  </li> <li>\\(R(x)\\) is a regularizer (possibly nonsmooth, such as \\(\\lambda\\|x\\|_1\\)).</li> </ul>"},{"location":"convex/20_advanced/#152-coordinate-descent","title":"15.2 Coordinate Descent","text":"<p>Coordinate descent updates a single variable (or a small block) at a time while holding others fixed.  </p>"},{"location":"convex/20_advanced/#algorithm","title":"Algorithm","text":"<p>Given \\(x^{(k)}\\), choose coordinate \\(i\\) and update:  </p> <p>This can be seen as projecting the gradient onto the coordinate directions. For separable problems, it is computationally much cheaper than full gradient updates.</p> <ul> <li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li> <li>For problems with separable structure \u2014 e.g. sums over features, or regularisers like \\(\\|x\\|_1 = \\sum_i |x_i|\\) \u2014 the coordinate update is extremely cheap.</li> <li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li> </ul> <p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>"},{"location":"convex/20_advanced/#convergence","title":"Convergence","text":"<p>If \\(f\\) is convex with Lipschitz-continuous partial derivatives, cyclic or randomized coordinate descent converges to the global optimum.</p>"},{"location":"convex/20_advanced/#ml-context","title":"ML Context","text":"<p>Coordinate descent is widely used in:</p> <ul> <li>LASSO and Elastic Net regression (where updates are closed-form soft-thresholding),</li> <li>logistic regression with \\(\\ell_1\\) penalty,</li> <li>matrix factorization and dictionary learning.</li> </ul>"},{"location":"convex/20_advanced/#153-stochastic-gradient-and-variance-reduced-methods","title":"15.3 Stochastic Gradient and Variance-Reduced Methods","text":"<p>When the dataset is large, computing the full gradient</p> \\[ \\nabla f(x) = \\frac{1}{N} \\sum_{i=1}^N \\nabla f_i(x) \\] <p>can be prohibitively expensive, since it requires evaluating all \\(N\\) samples at every iteration. Stochastic methods overcome this by using unbiased gradient estimates based on small random subsets (mini-batches) of the data.</p>"},{"location":"convex/20_advanced/#1531-stochastic-gradient-descent-sgd","title":"15.3.1 Stochastic Gradient Descent (SGD)","text":"<p>At each iteration, choose a random sample (or mini-batch) \\(\\mathcal{B}_k\\) and perform the update:</p> \\[ x_{k+1} = x_k - \\eta_k \\, \\widehat{\\nabla f}(x_k), \\] <p>where</p> \\[ \\widehat{\\nabla f}(x_k) = \\frac{1}{|\\mathcal{B}_k|} \\sum_{i \\in \\mathcal{B}_k} \\nabla f_i(x_k) \\] <p>is a stochastic estimate of the true gradient, and \\(\\eta_k &gt; 0\\) is the step size (learning rate).</p>"},{"location":"convex/20_advanced/#interpretation","title":"Interpretation","text":"<ul> <li>SGD performs a noisy gradient step: it moves in approximately the right direction on average.</li> <li>The noise introduced by sampling allows exploration of the parameter space and helps escape shallow local minima in nonconvex problems.</li> <li>In convex settings, it trades accuracy for computational efficiency \u2014 each iteration is much cheaper, so we can afford many more of them.</li> </ul>"},{"location":"convex/20_advanced/#1532-step-size-and-averaging","title":"15.3.2 Step Size and Averaging","text":"<p>The step size \\(\\eta_k\\) controls the bias\u2013variance tradeoff:</p> <ul> <li>If \\(\\eta_k\\) is too large \u2192 iterates oscillate due to stochastic noise.</li> <li>If \\(\\eta_k\\) is too small \u2192 progress slows down.</li> </ul> <p>Common choices:</p> \\[ \\eta_k = \\frac{c}{\\sqrt{k}} \\quad \\text{(for convex $f$)},  \\qquad \\eta_k = \\frac{c}{k} \\quad \\text{(for strongly convex $f$)}. \\] <p>Two popular stabilization strategies:</p> <ol> <li> <p>Decay learning rate.</p> </li> <li> <p>Polyak\u2013Ruppert averaging:    Instead of returning the last iterate, return the running average        Averaging cancels gradient noise and ensures convergence to the optimal solution in expectation.</p> </li> <li> <p>Increasing mini-batch size:    As optimization proceeds, increasing \\(|\\mathcal{B}_k|\\) gradually reduces gradient variance while keeping updates efficient.</p> </li> </ol>"},{"location":"convex/20_advanced/#1533-convergence-properties","title":"15.3.3 Convergence Properties","text":"<p>For convex objectives: - \\(\\mathbb{E}[f(x_k)] - f^\\star = O(1/\\sqrt{k})\\) with diminishing \\(\\eta_k\\).</p> <p>For strongly convex \\(f\\), with \\(\\eta_k = O(1/k)\\): - \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</p> <p>These are optimal rates for stochastic first-order methods:  no unbiased stochastic optimizer using the same amount of data can asymptotically converge faster than SGD with Polyak averaging.</p>"},{"location":"convex/20_advanced/#1534-variance-reduction","title":"15.3.4 Variance Reduction","text":"<p>Although SGD is simple, the stochastic noise prevents it from reaching very high accuracy.  Variance-reduced methods (SVRG, SAGA, SARAH) correct this by mixing stochastic and full-gradient information.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <p>At outer iteration \\(s\\), compute a full gradient snapshot \\(\\nabla f(\\tilde{x})\\). Then, for inner iterations:  </p> <ul> <li>\\(v_k\\) is an unbiased estimate of \\(\\nabla f(x_k)\\) but with reduced variance.</li> <li>For strongly convex \\(f\\), SVRG and SAGA achieve linear convergence, bridging the gap between SGD and full gradient descent.</li> </ul> <p>Intuitively, these methods \u201canchor\u201d stochastic gradients around a periodically refreshed reference point, preventing the gradient noise from accumulating.</p>"},{"location":"convex/20_advanced/#1535-stochastic-second-order-and-momentum-methods","title":"15.3.5 Stochastic Second-Order and Momentum Methods","text":"<p>SGD can be further improved by incorporating curvature or momentum information.</p> <ol> <li> <p>Momentum / Nesterov acceleration:    Maintains an exponential moving average of past gradients:        Momentum accelerates convergence in smooth regions and damps oscillations in narrow valleys.</p> </li> <li> <p>Adaptive methods (Adam, RMSProp, Adagrad):    Use coordinate-wise scaling based on running averages of squared gradients to handle ill-conditioned curvature.</p> </li> <li> <p>Stochastic second-order methods:    Approximate curvature matrices (e.g., Fisher or Hessian) via stochastic estimates and maintain them with exponential decay:        Though theoretically limited by SGD\u2019s asymptotic rate, they often yield better pre-asymptotic performance \u2014 crucial in practical deep learning where only a few passes over the data are feasible.</p> </li> </ol>"},{"location":"convex/20_advanced/#1536-machine-learning-context-and-insights","title":"15.3.6 Machine Learning Context and Insights","text":"<ul> <li>Deep neural networks rely almost exclusively on SGD and its adaptive or momentum-based variants. The stochasticity helps generalization by acting as implicit regularization.</li> <li>Large-scale convex ML problems \u2014 logistic regression, SVMs, ridge regression \u2014 use SGD or variance-reduced methods (SVRG/SAGA) for scalability.</li> <li>The balance between variance reduction and computational cost defines practical performance.</li> </ul>"},{"location":"convex/20_advanced/#1537-summary","title":"15.3.7 Summary","text":"Method Key Idea Convergence Practical Use SGD Uses mini-batch gradients \\(O(1/\\sqrt{k})\\) Deep learning, online learning SGD + Polyak averaging Averaged iterates \\(O(1/k)\\) Theoretically optimal stochastic convergence SVRG / SAGA Variance-reduced updates Linear for strongly convex Convex ML, GLMs Momentum / Adam Smoothed gradient estimates Empirical acceleration Deep nets Stochastic 2nd-order Curvature tracking Better pre-asymptotic Large-batch training"},{"location":"convex/20_advanced/#154-proximal-and-composite-optimization","title":"15.4 Proximal and Composite Optimization","text":"<p>Many modern objectives combine a smooth loss and a nonsmooth regularizer:  where \\(g\\) is differentiable with Lipschitz gradient and \\(R\\) is convex but possibly nonsmooth.</p> <p>The proximal gradient method updates as:  where the proximal operator is:  </p>"},{"location":"convex/20_advanced/#intuition","title":"Intuition","text":"<ul> <li>The gradient step moves in a descent direction for \\(g\\).  </li> <li>The proximal step performs a local \u201cdenoising\u201d or shrinkage under \\(R\\) (e.g., soft-thresholding for \\(\\ell_1\\) norms).</li> </ul>"},{"location":"convex/20_advanced/#ml-context_1","title":"ML Context","text":"<p>Proximal methods underpin:</p> <ul> <li>Sparse regression (LASSO, Elastic Net),</li> <li>matrix completion and compressed sensing,</li> <li>total-variation image denoising,</li> <li>low-rank and structured regularization.</li> </ul>"},{"location":"convex/20_advanced/#155-alternating-direction-method-of-multipliers-admm","title":"15.5 Alternating Direction Method of Multipliers (ADMM)","text":"<p>When an objective separates into parts that depend on different variables, ADMM enables efficient distributed optimization.</p> <p>Consider:  </p>"},{"location":"convex/20_advanced/#augmented-lagrangian","title":"Augmented Lagrangian","text":"\\[ L_\\rho(x,z,y) = f(x) + g(z) + y^T(Ax + Bz - c) + \\frac{\\rho}{2}\\|A x + B z - c\\|^2. \\]"},{"location":"convex/20_advanced/#iterations","title":"Iterations","text":"<p>ADMM performs alternating updates:  </p>"},{"location":"convex/20_advanced/#interpretation_1","title":"Interpretation","text":"<p>Each step solves an easier subproblem involving only part of the variables, followed by a dual update to enforce consistency. ADMM thus merges ideas from dual ascent and penalty methods.</p>"},{"location":"convex/20_advanced/#convergence_1","title":"Convergence","text":"<p>For convex \\(f\\) and \\(g\\), ADMM converges to the global optimum. It is particularly effective when the subproblems are simple (e.g., proximal operators).</p>"},{"location":"convex/20_advanced/#ml-context_2","title":"ML Context","text":"<p>ADMM is a key tool for:</p> <ul> <li>distributed LASSO and logistic regression,</li> <li>matrix decomposition and factorization,</li> <li>consensus optimization in federated learning,</li> <li>distributed deep learning regularization.</li> </ul>"},{"location":"convex/20_advanced/#156-majorizationminimization-mm-and-em-algorithms","title":"15.6 Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The MM principle iteratively minimizes a surrogate function that upper-bounds the objective.</p> <p>Given a current point \\(x_k\\), construct a surrogate \\(g(x|x_k)\\) such that:  </p> <p>Then update:  </p> <p>Each iteration ensures \\(f(x_{k+1}) \\le f(x_k)\\).</p>"},{"location":"convex/20_advanced/#ml-context_3","title":"ML Context","text":"<ul> <li>The Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models.  </li> <li>IRLS (Iteratively Reweighted Least Squares) for logistic regression and \\(\\ell_p\\) regression follows the same idea.  </li> <li>MM methods guarantee descent even for complex, nonconvex objectives.</li> </ul>"},{"location":"convex/20_advanced/#157-distributed-and-parallel-optimization","title":"15.7 Distributed and Parallel Optimization","text":"<p>For large-scale convex problems distributed across multiple nodes, parallel methods are essential.</p>"},{"location":"convex/20_advanced/#synchronous-and-asynchronous-updates","title":"Synchronous and Asynchronous Updates","text":"<ul> <li>Synchronous: all workers compute updates and synchronize (used in federated averaging).  </li> <li>Asynchronous: updates proceed without waiting, improving throughput but increasing variance.</li> </ul>"},{"location":"convex/20_advanced/#consensus-optimization","title":"Consensus Optimization","text":"<p>In distributed convex optimization, one solves  which can be handled by ADMM or primal\u2013dual methods. Each machine optimizes its local copy \\(x_i\\), and the shared variable \\(z\\) enforces consensus.</p>"},{"location":"convex/20_advanced/#ml-context_4","title":"ML Context","text":"<ul> <li>Federated learning and parameter-server training frameworks (e.g., TensorFlow Distributed, PyTorch DDP) follow this model.  </li> <li>Decentralized convex optimization appears in sensor networks and multi-agent control.</li> </ul>"},{"location":"convex/20_advanced/#158-handling-structure-sparsity-and-low-rank","title":"15.8 Handling Structure: Sparsity and Low Rank","text":"<p>Many convex problems exhibit special structures that algorithms can exploit:</p> Structure Typical Regularizer Algorithmic Advantage Sparsity \\(\\ell_1\\) or group lasso Coordinate updates, proximal shrinkage Low rank nuclear norm \\(\\|X\\|_*\\) SVD-based proximal step Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed updates Graph structure total variation norm Local neighborhood computations Simplex or probability constraints entropy or KL penalty Mirror descent, projected methods <p>Exploiting such structure yields orders-of-magnitude speedups in both memory and computation.</p>"},{"location":"convex/20_advanced/#159-summary-and-practical-guidance","title":"15.9 Summary and Practical Guidance","text":"Method Gradient Access Scalability Parallelization Convexity Required Typical ML Uses Coordinate Descent Partial / coordinate High Easy Convex LASSO, sparse models SGD / SVRG / SAGA Stochastic Excellent Natural Convex / nonconvex Deep learning, logistic regression Proximal Gradient Full gradient + prox Moderate\u2013High Easy Convex Composite objectives ADMM Separable subproblems High Distributed Convex Consensus, large convex solvers MM / EM Surrogate-based Moderate Model-specific Convex / nonconvex Probabilistic models, IRLS Distributed / Federated Local gradients Very high Essential Convex / smooth Federated learning, large-scale convex optimization"},{"location":"convex/20_advanced/#1510-key-takeaways","title":"15.10 Key Takeaways","text":"<ul> <li>Large-scale convex optimization relies on exploiting structure, stochasticity, and separability.  </li> <li>Coordinate and proximal methods handle sparse and composite problems efficiently.  </li> <li>Stochastic and variance-reduced methods scale to massive data.  </li> <li>ADMM and distributed optimization enable multi-machine or federated settings.  </li> <li>MM and EM extend convex ideas to broader nonconvex inference tasks.</li> </ul>"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#161-regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"16.1 Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithmic pointers for 11.1:  </p> <ul> <li>Smooth+\u2113\u2082 (strongly convex) \u2192 Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li> <li>Mixed penalties (\u2113\u2081+\u2113\u2082) \u2192 Still convex; often handle like \u2113\u2081 case since smooth part dominates curvature.  </li> <li>Large-scale data \u2192 Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li> </ul> <p>Remarks:  Choose \\(\\lambda\\) via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (\\(n\\) large), coordinate or stochastic methods often outperform direct second-order methods.</p>"},{"location":"convex/21_models/#162-robust-regression-and-outlier-resistance","title":"16.2 Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#1621-least-absolute-deviations-l1-loss","title":"16.2.1 Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#1622-huber-loss","title":"16.2.2 Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints.     That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.     This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#1623-worst-case-robust-regression","title":"16.2.3 Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#163-maximum-likelihood-and-loss-design","title":"16.3 Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#164-structured-constraints-in-engineering-and-design","title":"16.4 Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers for 11.4:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#165-linear-and-conic-programming-the-canonical-models","title":"16.5 Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#166-risk-safety-margins-and-robust-design","title":"16.6 Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#167-cheat-sheet-if-your-problem-looks-like-this-use-that","title":"16.7 Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#167-matching-model-structure-to-algorithm-type","title":"16.7 Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#171-hierarchy-of-canonical-problems","title":"17.1 Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#172-linear-programming-lp","title":"17.2 Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#173-quadratic-programming-qp","title":"17.3 Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#174-quadratically-constrained-qp-qcqp","title":"17.4 Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#175-second-order-cone-programming-socp","title":"17.5 Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#176-semidefinite-programming-sdp","title":"17.6 Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#177-geometric-programming-gp","title":"17.7 Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#178-likelihood-based-convex-models-mle-and-glms","title":"17.8 Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#179-solver-selection-summary","title":"17.9 Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"convex/35_modern/","title":"18. Modern Optimizers in Machine Learning Frameworks","text":""},{"location":"convex/35_modern/#chapter-18-modern-optimizers-in-machine-learning","title":"Chapter 18: Modern Optimizers in Machine Learning","text":"<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization\u2014balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p> <p>This chapter connects the principles of convex optimization to the modern optimizers that power today\u2019s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>"},{"location":"convex/35_modern/#181-stochastic-optimization-overview","title":"18.1 Stochastic Optimization Overview","text":"<p>In machine learning, we often minimize an empirical risk:  where \\(\\ell(x; z_i)\\) is the loss on data sample \\(z_i\\).</p> <p>Computing the full gradient \\(\\nabla f(x)\\) is infeasible when \\(N\\) is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p> \\[ g_k = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla \\ell(x_k; z_i). $$ This yields the Stochastic Gradient Descent (SGD) update: $$ x_{k+1} = x_k - \\alpha_k g_k. \\] <p>SGD is the foundation for nearly all deep learning optimizers.</p>"},{"location":"convex/35_modern/#182-momentum-and-acceleration","title":"18.2 Momentum and Acceleration","text":"<p>SGD\u2019s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p> <p>  where \\(\\beta \\in [0,1)\\) controls inertia.</p> <p>Nesterov momentum adds a correction term anticipating the future position:</p> \\[ v_{k+1} = \\beta v_k + g(x_k - \\alpha \\beta v_k), \\quad x_{k+1} = x_k - \\alpha v_{k+1}. \\] <p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>"},{"location":"convex/35_modern/#183-adaptive-learning-rate-methods","title":"18.3 Adaptive Learning Rate Methods","text":"<p>Different parameters often require different step sizes. Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>"},{"location":"convex/35_modern/#1831-adagrad","title":"18.3.1 AdaGrad","text":"<p>Keeps a cumulative sum of squared gradients:</p> <p>  and updates parameters as:</p> <p>  Good for sparse data, but the learning rate can shrink too quickly.</p>"},{"location":"convex/35_modern/#1832-rmsprop","title":"18.3.2 RMSProp","text":"<p>A refinement of AdaGrad using exponential averaging:</p> \\[ E[g^2]_k = \\beta E[g^2]_{k-1} + (1-\\beta) g_k^2, \\] \\[ x_{k+1} = x_k - \\frac{\\alpha}{\\sqrt{E[g^2]_k + \\epsilon}} g_k. \\] <p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>"},{"location":"convex/35_modern/#1833-adam-adaptive-moment-estimation","title":"18.3.3 Adam: Adaptive Moment Estimation","text":"<p>Adam combines momentum and adaptive scaling:</p> \\[ m_k = \\beta_1 m_{k-1} + (1-\\beta_1) g_k, \\quad v_k = \\beta_2 v_{k-1} + (1-\\beta_2) g_k^2, \\] \\[ \\hat{m}_k = \\frac{m_k}{1-\\beta_1^k}, \\quad \\hat{v}_k = \\frac{v_k}{1-\\beta_2^k}, \\] \\[ x_{k+1} = x_k - \\alpha \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\epsilon}. \\] <p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>"},{"location":"convex/35_modern/#184-variants-and-modern-extensions","title":"18.4 Variants and Modern Extensions","text":"Optimizer Key Idea Notes AdamW Decoupled weight decay from gradient update Better regularization RAdam Rectified Adam\u2014adaptive variance correction Improves stability early in training Lookahead Combines fast and slow weights Enhances robustness and convergence AdaBelief Uses prediction error instead of raw gradient variance More adaptive learning rates Lion Uses sign-based updates and momentum Efficient for large-scale training <p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>"},{"location":"convex/35_modern/#185-implicit-regularization-and-generalization","title":"18.5 Implicit Regularization and Generalization","text":"<p>Modern optimizers not only minimize loss\u2014they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p> <p>Empirical findings suggest:</p> <ul> <li>Large-batch training finds sharper minima (risk of overfitting).  </li> <li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li> <li>Adaptive optimizers may converge faster but generalize slightly worse.</li> </ul> <p>This trade-off drives ongoing research into optimizer design.</p>"},{"location":"convex/35_modern/#186-practical-considerations","title":"18.6 Practical Considerations","text":"Aspect Guideline Learning Rate Most critical hyperparameter; use warm-up and decay schedules Batch Size Balances gradient noise and hardware efficiency Initialization Affects early dynamics, especially for Adam variants Gradient Clipping Prevents instability in exploding gradients Mixed Precision Use with adaptive optimizers for speed and memory savings"},{"location":"convex/35_modern/#187-comparative-behavior","title":"18.7 Comparative Behavior","text":"Method Adaptivity Speed Memory Typical Use SGD + Momentum Moderate Slow-medium Low General-purpose, good generalization RMSProp Adaptive per-parameter Medium-fast Medium Recurrent networks, nonstationary data Adam / AdamW Fully adaptive Fast High Deep networks, large-scale training RAdam / AdaBelief / Lion Advanced adaptivity Fast Medium Cutting-edge training tasks"},{"location":"convex/35_modern/#188-optimization-in-modern-deep-networks","title":"18.8 Optimization in Modern Deep Networks","text":"<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p> <ul> <li>Batch normalization modifies effective learning rates.  </li> <li>Skip connections ease gradient flow.  </li> <li>Large-scale distributed training relies on adaptive optimizers for stability.  </li> </ul> <p>Optimization is no longer an isolated procedure but part of the model\u2019s design philosophy.</p> <p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning\u2014adapting dynamically, balancing efficiency and generalization.</p>"},{"location":"convex/40_nonconvex/","title":"19. Beyond Convexity \u2013 Nonconvex and Global Optimization","text":""},{"location":"convex/40_nonconvex/#chapter-19-beyond-convexity-nonconvex-and-global-optimization","title":"Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization","text":"<p>Optimization extends far beyond the comfortable world of convexity.  In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p> <p>Convex optimization gives us strong guarantees \u2014 every local minimum is global, and algorithms converge predictably. But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>"},{"location":"convex/40_nonconvex/#191-the-landscape-of-nonconvex-optimization","title":"19.1 The Landscape of Nonconvex Optimization","text":"<p>A nonconvex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) violates convexity; i.e., for some \\(x, y\\) and \\(\\theta \\in (0,1)\\),  Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p> <p>A typical nonconvex landscape looks like a mountainous terrain \u2014 smooth in some regions, rugged in others. An optimization algorithm\u2019s path depends strongly on initialization and stochastic effects.</p>"},{"location":"convex/40_nonconvex/#example-a-simple-nonconvex-function","title":"Example: A Simple Nonconvex Function","text":"<p>  This function has multiple stationary points: - \\((0,0)\\) (a saddle), - \\((1,1)\\) and \\((-1,-1)\\) (local minima), - \\((1,-1)\\) and \\((-1,1)\\) (local maxima).</p> <p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>"},{"location":"convex/40_nonconvex/#192-local-vs-global-minima","title":"19.2 Local vs. Global Minima","text":"<p>A point \\(x^*\\) is a local minimum if:  </p> <p>A global minimum satisfies the stronger condition:  </p> <p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad \u2014 and there may be exponentially many of them.</p>"},{"location":"convex/40_nonconvex/#193-classes-of-nonconvex-problems","title":"19.3 Classes of Nonconvex Problems","text":"<p>Nonconvex problems appear in several distinct forms:</p> Type Example Challenge Smooth nonconvex Neural network training Multiple minima, saddle points Nonsmooth nonconvex Sparse regularization, ReLU activations Undefined gradients Discrete / combinatorial Scheduling, routing, integer programs Exponential search space Black-box Simulation-based optimization No derivatives or analytical form <p>Each category requires different algorithmic strategies \u2014 from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>"},{"location":"convex/40_nonconvex/#194-local-optimization-strategies","title":"19.4 Local Optimization Strategies","text":"<p>Even in nonconvex settings, local optimization remains useful when: - The problem is nearly convex (e.g., locally convex around good minima), - The initialization is close to a desired basin of attraction, - Or the goal is approximate, not exact, optimality.</p>"},{"location":"convex/40_nonconvex/#gradient-descent-and-its-variants","title":"Gradient Descent and Its Variants","text":"<p>Gradient descent behaves well if \\(f\\) is smooth and Lipschitz-continuous:  However, convergence is only to a stationary point \u2014 not necessarily a minimum.</p> <p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>"},{"location":"convex/40_nonconvex/#195-global-optimization-strategies","title":"19.5 Global Optimization Strategies","text":"<p>To seek the global minimum, algorithms must explore the search space more broadly. Common strategies include:</p> <ol> <li> <p>Multiple Starts:    Run local optimization from diverse random initial points and keep the best solution.</p> </li> <li> <p>Continuation and Homotopy Methods:    Start from a smooth, convex approximation \\(f_\\lambda\\) of \\(f\\) and gradually transform it into the true objective as \\(\\lambda \\to 0\\).</p> </li> <li> <p>Stochastic Search and Simulated Annealing:    Introduce randomness in updates to jump between basins.</p> </li> <li> <p>Population-Based Methods:    Maintain a swarm or population of candidate solutions evolving by selection and variation \u2014 leading to metaheuristic algorithms like GA and PSO.</p> </li> </ol>"},{"location":"convex/40_nonconvex/#196-theoretical-challenges","title":"19.6 Theoretical Challenges","text":"<p>Without convexity, most strong results vanish:</p> <ul> <li>Global optimality cannot be guaranteed.</li> <li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li> <li>Complexity often grows exponentially with problem size.</li> </ul> <p>However, theory is not hopeless:</p> <ul> <li>Many nonconvex problems are \u201cbenign\u201d \u2014 e.g., matrix factorization, phase retrieval, or deep linear networks \u2014 having no bad local minima.  </li> <li>Random initialization and overparameterization often aid convergence to global minima in practice.</li> </ul>"},{"location":"convex/40_nonconvex/#197-geometry-of-saddle-points","title":"19.7 Geometry of Saddle Points","text":"<p>A saddle point satisfies \\(\\nabla f(x)=0\\) but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p> <p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>"},{"location":"convex/40_nonconvex/#198-deterministic-vs-stochastic-global-methods","title":"19.8 Deterministic vs. Stochastic Global Methods","text":"Deterministic Methods Stochastic Methods Systematic exploration of space (branch &amp; bound, interval analysis) Randomized search (simulated annealing, evolutionary algorithms) Can provide certificates of global optimality Typically approximate but scalable High computational cost Naturally parallelizable <p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>"},{"location":"convex/40_nonconvex/#199-a-taxonomy-of-optimization-beyond-convexity","title":"19.9 A Taxonomy of Optimization Beyond Convexity","text":"Family Typical Algorithms When to Use Derivative-Free (Black-Box) Nelder\u2013Mead, CMA-ES, Bayesian Opt. When gradients unavailable Metaheuristic (Evolutionary) GA, PSO, DE, ACO Complex landscapes, combinatorial problems Modern Stochastic Gradient Adam, RMSProp, Lion Deep learning, large-scale models Combinatorial / Discrete Branch &amp; Bound, Tabu, SA Integer or graph-based problems Learning-Based Optimizers Meta-learning, Reinforcement methods Adaptive, data-driven optimization"},{"location":"convex/42_derivativefree/","title":"20. Derivative-Free and Black-Box Optimization","text":""},{"location":"convex/42_derivativefree/#chapter-20-derivative-free-and-black-box-optimization","title":"Chapter 20: Derivative-Free and Black-Box Optimization","text":"<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p> <p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>"},{"location":"convex/42_derivativefree/#201-motivation-and-challenges","title":"20.1 Motivation and Challenges","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be an objective function.  </p> <p>A derivative-free algorithm seeks to minimize \\(f(x)\\) using only evaluations of \\(f(x)\\), without access to \\(\\nabla f(x)\\) or \\(\\nabla^2 f(x)\\).</p> <p>Key challenges:</p> <ul> <li>No gradient information \u2192 difficult to infer descent directions.  </li> <li>Expensive evaluations \u2192 every call to \\(f(x)\\) might require a simulation or experiment.  </li> <li>Noise and stochasticity \u2192 evaluations may be corrupted by measurement or sampling error.  </li> <li>High-dimensionality \u2192 sampling-based methods scale poorly with \\(n\\).</li> </ul> <p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>"},{"location":"convex/42_derivativefree/#202-classification-of-derivative-free-methods","title":"20.2 Classification of Derivative-Free Methods","text":"Category Representative Algorithms Main Idea Direct Search Nelder\u2013Mead, Pattern Search, MADS Explore the space via geometric moves or meshes Model-Based BOBYQA, Trust-Region DFO Build local quadratic or surrogate models of \\(f\\) Evolutionary / Population-Based CMA-ES, Differential Evolution Evolve a population using stochastic operators Probabilistic / Bayesian Bayesian Optimization Use probabilistic surrogate models to guide exploration"},{"location":"convex/42_derivativefree/#203-direct-search-methods","title":"20.3 Direct Search Methods","text":"<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>"},{"location":"convex/42_derivativefree/#2031-neldermead-simplex-method","title":"20.3.1 Nelder\u2013Mead Simplex Method","text":"<p>Perhaps the most famous derivative-free algorithm, Nelder\u2013Mead maintains a simplex \u2014 a polytope of \\(n+1\\) vertices in \\(\\mathbb{R}^n\\).</p> <p>At each iteration:</p> <ol> <li>Evaluate \\(f\\) at all simplex vertices.</li> <li>Reflect, expand, contract, or shrink the simplex depending on performance.</li> <li>Continue until simplex collapses near a minimum.</li> </ol> <p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>"},{"location":"convex/42_derivativefree/#2032-pattern-search-methods","title":"20.3.2 Pattern Search Methods","text":"<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p> <p>Typical update rule:  </p> <p>where \\(d_i\\) is a direction from a finite set (e.g., coordinate axes). If a direction yields improvement, move there; otherwise, shrink \\(\\Delta_k\\).</p>"},{"location":"convex/42_derivativefree/#2033-mesh-adaptive-direct-search-mads","title":"20.3.3 Mesh Adaptive Direct Search (MADS)","text":"<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>"},{"location":"convex/42_derivativefree/#204-model-based-methods","title":"20.4 Model-Based Methods","text":"<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>"},{"location":"convex/42_derivativefree/#2041-trust-region-dfo","title":"20.4.1 Trust-Region DFO","text":"<p>A local model \\(m_k(x)\\) (often quadratic) is built to approximate \\(f\\) near the current iterate \\(x_k\\):  The next iterate solves a trust-region subproblem:  The trust region size \\(\\Delta_k\\) adapts based on how well \\(m_k\\) predicts true function values.</p>"},{"location":"convex/42_derivativefree/#2042-bobyqa-bound-optimization-by-quadratic-approximation","title":"20.4.2 BOBYQA (Bound Optimization BY Quadratic Approximation)","text":"<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>"},{"location":"convex/42_derivativefree/#205-evolution-strategies-and-population-methods","title":"20.5 Evolution Strategies and Population Methods","text":"<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>"},{"location":"convex/42_derivativefree/#2051-covariance-matrix-adaptation-evolution-strategy-cma-es","title":"20.5.1 Covariance Matrix Adaptation Evolution Strategy (CMA-ES)","text":"<p>CMA-ES is a powerful stochastic search algorithm. It iteratively samples new points from a multivariate Gaussian distribution:  where \\(m_k\\) is the current mean, \\(\\sigma_k\\) the global step-size, and \\(C_k\\) the covariance matrix.</p> <p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p> <p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>"},{"location":"convex/42_derivativefree/#2052-differential-evolution-de","title":"20.5.2 Differential Evolution (DE)","text":"<p>DE evolves a population \\(\\{x_i\\}\\) via vector differences:   where \\(r1, r2, r3\\) are random distinct indices and \\(F\\) controls mutation strength.</p> <p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>"},{"location":"convex/42_derivativefree/#206-bayesian-optimization","title":"20.6 Bayesian Optimization","text":"<p>When function evaluations are expensive (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>"},{"location":"convex/42_derivativefree/#2061-core-idea","title":"20.6.1 Core Idea","text":"<p>Model the objective as a random function \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))\\) (Gaussian Process prior). After each evaluation, update the posterior mean and variance to quantify uncertainty.</p> <p>Use an acquisition function \\(a(x)\\) to select the next evaluation point:  balancing exploration (high uncertainty) and exploitation (low expected value).</p> <p>Common acquisition functions:</p> <ul> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Upper Confidence Bound (UCB)</li> </ul>"},{"location":"convex/42_derivativefree/#2062-surrogate-models-beyond-gaussian-processes","title":"20.6.2 Surrogate Models Beyond Gaussian Processes","text":"<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs: - Tree-structured Parzen Estimators (TPE) - Random forests (SMAC) - Neural network surrogates (Bayesian neural networks)</p> <p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>"},{"location":"convex/42_derivativefree/#207-hybrid-and-adaptive-approaches","title":"20.7 Hybrid and Adaptive Approaches","text":"<p>Modern applications often combine derivative-free and gradient-based techniques:</p> <ul> <li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li> <li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li> <li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li> </ul> <p>Such hybridization reflects a pragmatic view: no single optimizer is best \u2014 adaptability matters most.</p>"},{"location":"convex/42_derivativefree/#208-practical-considerations","title":"20.8 Practical Considerations","text":"Aspect Guideline Function evaluations expensive Use Bayesian or model-based methods Noisy evaluations Use averaging, smoothing, or robust estimators High dimension (\\(n &gt; 50\\)) Prefer CMA-ES or evolutionary strategies Box constraints Methods like BOBYQA, DE, or PSO Parallel computation available Population-based methods excel <p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize anything we can evaluate. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p> <p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors \u2014 turning randomness into a powerful search strategy.</p>"},{"location":"convex/44_metaheuristic/","title":"21. Metaheuristic and Evolutionary Optimization","text":""},{"location":"convex/44_metaheuristic/#chapter-21-metaheuristic-and-evolutionary-algorithms","title":"Chapter 21: Metaheuristic and Evolutionary Algorithms","text":"<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions. In these settings, metaheuristic algorithms\u2014inspired by nature, biology, and collective behavior\u2014provide robust and flexible alternatives.</p> <p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas. They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>"},{"location":"convex/44_metaheuristic/#211-principles-of-metaheuristic-optimization","title":"21.1 Principles of Metaheuristic Optimization","text":"<p>All metaheuristics share three key principles:</p> <ol> <li> <p>Population-Based Search:    Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p> </li> <li> <p>Variation Operators:    Create new solutions via mutation, recombination, or stochastic perturbations.</p> </li> <li> <p>Selection and Adaptation:    Favor candidates with better objective values, guiding the search toward promising regions.</p> </li> </ol> <p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>"},{"location":"convex/44_metaheuristic/#212-genetic-algorithms-ga","title":"21.2 Genetic Algorithms (GA)","text":""},{"location":"convex/44_metaheuristic/#2121-biological-inspiration","title":"21.2.1 Biological Inspiration","text":"<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>"},{"location":"convex/44_metaheuristic/#2122-representation","title":"21.2.2 Representation","text":"<p>A solution (individual) is represented as a chromosome\u2014often a binary string, vector of reals, or permutation. Each position (gene) encodes part of the decision variable.</p>"},{"location":"convex/44_metaheuristic/#2123-algorithm-outline","title":"21.2.3 Algorithm Outline","text":"<ol> <li>Initialize a population \\(\\{x_i\\}_{i=1}^N\\) randomly.  </li> <li>Evaluate fitness \\(f(x_i)\\) for all individuals.  </li> <li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li> <li> <p>Apply:</p> <ul> <li>Crossover: combine genetic material of two parents.  </li> <li>Mutation: randomly alter some genes to maintain diversity.  </li> </ul> </li> <li> <p>Form a new population and repeat until convergence.</p> </li> </ol>"},{"location":"convex/44_metaheuristic/#2124-crossover-and-mutation-examples","title":"21.2.4 Crossover and Mutation Examples","text":"<ul> <li>Single-point crossover: exchange genes after a random index.  </li> <li>Gaussian mutation: add small noise to continuous parameters.  </li> </ul>"},{"location":"convex/44_metaheuristic/#2125-strengths-and-weaknesses","title":"21.2.5 Strengths and Weaknesses","text":"Strengths Weaknesses Highly parallel, robust, domain-independent Requires many function evaluations Effective for combinatorial and discrete optimization Parameter tuning (mutation, crossover rates) is nontrivial"},{"location":"convex/44_metaheuristic/#213-differential-evolution-de","title":"21.3 Differential Evolution (DE)","text":"<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>"},{"location":"convex/44_metaheuristic/#2131-core-idea","title":"21.3.1 Core Idea","text":"<p>Mutation is performed using differences of population members:  where \\(r1, r2, r3\\) are random distinct indices and \\(F \\in [0,2]\\) controls mutation amplitude.</p> <p>Then crossover forms trial vectors:  and selection chooses between \\(x_i\\) and \\(u_i\\) based on objective value.</p>"},{"location":"convex/44_metaheuristic/#2132-features","title":"21.3.2 Features","text":"<ul> <li>Self-adaptive exploration of the search space.</li> <li>Suitable for continuous, multimodal functions.</li> <li>Simple to implement, with few control parameters.</li> </ul>"},{"location":"convex/44_metaheuristic/#214-particle-swarm-optimization-pso","title":"21.4 Particle Swarm Optimization (PSO)","text":"<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p> <p>Each particle \\(i\\) has position \\(x_i\\) and velocity \\(v_i\\), updated as:   where:</p> <ul> <li>\\(p_i\\) = personal best position of particle \\(i\\),</li> <li>\\(g\\) = best global position found by the swarm,</li> <li>\\(w\\), \\(c_1\\), \\(c_2\\) are weight and learning coefficients,</li> <li>\\(r_1\\), \\(r_2\\) are random numbers in \\([0,1]\\).</li> </ul> <p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>"},{"location":"convex/44_metaheuristic/#2141-convergence-behavior","title":"21.4.1 Convergence Behavior","text":"<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>"},{"location":"convex/44_metaheuristic/#2142-strengths","title":"21.4.2 Strengths","text":"<ul> <li>Few parameters, easy to implement.</li> <li>Works well for noisy or discontinuous problems.</li> <li>Naturally parallelizable.</li> </ul>"},{"location":"convex/44_metaheuristic/#215-simulated-annealing-sa","title":"21.5 Simulated Annealing (SA)","text":"<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy \u2014 a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual \u201ccooling\u201d in the search for a global minimum.</p>"},{"location":"convex/44_metaheuristic/#2151-physical-analogy","title":"21.5.1 Physical Analogy","text":"<p>In thermodynamics, a system at temperature \\(T\\) has probability of occupying a state with energy \\(E\\) given by the Boltzmann distribution:</p> \\[ P(E) \\propto e^{-E / (kT)}. \\] <p>At high temperature, the system freely explores many states. As \\(T\\) decreases, it becomes increasingly likely to remain near states of minimal energy.</p> <p>Simulated Annealing maps this principle to optimization by treating:</p> <ul> <li>The objective function \\(f(x)\\) as the system\u2019s energy.</li> <li>The solution vector \\(x\\) as a configuration.</li> <li>The temperature \\(T\\) as a control parameter determining randomness.</li> </ul>"},{"location":"convex/44_metaheuristic/#2152-algorithm-outline","title":"21.5.2 Algorithm Outline","text":"<ol> <li> <p>Initialization</p> <ul> <li>Choose an initial solution \\(x_0\\) and initial temperature \\(T_0\\).</li> <li>Set a cooling schedule \\(T_{k+1} = \\alpha T_k\\), with \\(\\alpha \\in (0,1)\\).</li> </ul> </li> <li> <p>Iteration</p> <ul> <li>Generate a candidate \\(x'\\) from \\(x_k\\) via a small random perturbation.</li> <li>Compute \\(\\Delta f = f(x') - f(x_k)\\).</li> <li>Accept or reject based on the Metropolis criterion:</li> </ul> <p> </p> </li> <li> <p>Cooling</p> <ul> <li> <p>Reduce the temperature gradually according to the schedule.</p> </li> <li> <p>Repeat until \\(T\\) becomes sufficiently small or the system stabilizes.</p> </li> </ul> </li> </ol>"},{"location":"convex/44_metaheuristic/#2153-interpretation","title":"21.5.3 Interpretation","text":"<ul> <li> <p>At high temperatures, SA accepts both better and worse moves \u2192 exploration.  </p> </li> <li> <p>At low temperatures, it becomes increasingly selective \u2192 exploitation.</p> </li> </ul> <p>This balance allows SA to escape local minima and approach the global optimum over time.</p>"},{"location":"convex/44_metaheuristic/#2154-cooling-schedules","title":"21.5.4 Cooling Schedules","text":"<p>The temperature schedule determines convergence quality:</p> Type Formula Behavior Exponential \\(T_{k+1} = \\alpha T_k\\) Simple, widely used Linear \\(T_{k+1} = T_0 - \\beta k\\) Faster cooling, less exploration Logarithmic \\(T_k = \\frac{T_0}{\\log(k + c)}\\) Theoretically convergent (slow) Adaptive Adjust based on recent acceptance rates Practical and self-tuning <p>A slower cooling schedule improves accuracy but increases computational cost.</p>"},{"location":"convex/44_metaheuristic/#216-ant-colony-optimization-aco","title":"21.6 Ant Colony Optimization (ACO)","text":""},{"location":"convex/44_metaheuristic/#2161-biological-basis","title":"21.6.1 Biological Basis","text":"<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p> <p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity \\(\\tau_{ij}\\) and heuristic visibility \\(\\eta_{ij}\\):  </p>"},{"location":"convex/44_metaheuristic/#2162-pheromone-update","title":"21.6.2 Pheromone Update","text":"<p>After all ants construct their tours:  where \\(\\rho\\) controls evaporation and \\(\\Delta\\tau_{ij}\\) reinforces paths used by good solutions.</p> <p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>"},{"location":"convex/44_metaheuristic/#217-exploration-vs-exploitation","title":"21.7 Exploration vs. Exploitation","text":"<p>Every metaheuristic must balance: - Exploration: sampling diverse regions to escape local minima. - Exploitation: refining known good solutions to reach local optima.</p> High Exploration High Exploitation GA with strong mutation PSO with low inertia DE with high \\(F\\) ACO with low evaporation rate Random restarts Local refinement <p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>"},{"location":"convex/44_metaheuristic/#218-hybrid-and-memetic-algorithms","title":"21.8 Hybrid and Memetic Algorithms","text":"<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p> <p>Example:</p> <ol> <li>Use PSO or GA to explore broadly.  </li> <li>Apply gradient descent or Nelder\u2013Mead locally near promising candidates.</li> </ol> <p>This hybridization often yields faster convergence and improved accuracy.</p>"},{"location":"convex/44_metaheuristic/#219-performance-and-practical-tips","title":"21.9 Performance and Practical Tips","text":"Aspect Guideline Initialization Use wide, random distributions to promote diversity Parameter Tuning Use adaptive schedules (e.g., cooling, inertia decay) Population Size Larger for global search, smaller for fine-tuning Parallelism Evaluate populations concurrently for efficiency Stopping Criteria Use both iteration limits and stagnation detection <p>Metaheuristics are heuristic by design \u2014 they do not guarantee global optimality, but offer practical success across many fields.</p> <p>Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p> <p>In the next chapter, we turn to modern stochastic optimizers that bridge theoretical foundations and practical success in machine learning\u2014methods like Adam, RMSProp, and Lion that dominate large-scale nonconvex optimization.</p>"},{"location":"convex/48_advanced_combinatorial/","title":"22. Advanced Topics in Combinatorial Optimization","text":""},{"location":"convex/48_advanced_combinatorial/#chapter-22-advanced-topics-in-combinatorial-optimization","title":"Chapter 22: Advanced Topics in Combinatorial Optimization","text":"<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p> <p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>"},{"location":"convex/48_advanced_combinatorial/#221-nature-of-combinatorial-problems","title":"22.1 Nature of Combinatorial Problems","text":"<p>A combinatorial optimization problem can be expressed as:</p> \\[ \\min_{x \\in \\mathcal{F}} f(x), \\] <p>where \\(\\mathcal{F}\\) is a finite or countable set of feasible solutions, often exponentially large in size.</p> <p>Example forms include:</p> <ul> <li>Binary decisions: \\(x_i \\in \\{0,1\\}\\)</li> <li>Integer constraints: \\(x_i \\in \\mathbb{Z}\\)</li> <li>Permutations: ordering or ranking elements</li> </ul> <p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>"},{"location":"convex/48_advanced_combinatorial/#222-graph-theoretic-foundations","title":"22.2 Graph-Theoretic Foundations","text":"<p>Many combinatorial problems are naturally represented as graphs \\(G = (V, E)\\).</p>"},{"location":"convex/48_advanced_combinatorial/#2221-shortest-path-problem","title":"22.2.1 Shortest Path Problem","text":"<p>Given edge weights \\(w_{ij}\\), find a path from \\(s\\) to \\(t\\) minimizing total weight:  Efficiently solvable by Dijkstra\u2019s or Bellman\u2013Ford algorithms.</p>"},{"location":"convex/48_advanced_combinatorial/#2222-minimum-spanning-tree-mst","title":"22.2.2 Minimum Spanning Tree (MST)","text":"<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal\u2019s or Prim\u2019s algorithm in \\(O(E\\log V)\\) time.</p>"},{"location":"convex/48_advanced_combinatorial/#2223-maximum-flow-minimum-cut","title":"22.2.3 Maximum Flow / Minimum Cut","text":"<p>Determine how much \u201cflow\u201d can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>"},{"location":"convex/48_advanced_combinatorial/#223-integer-linear-programming-ilp","title":"22.3 Integer Linear Programming (ILP)","text":"<p>An integer program seeks:  </p> <p>It generalizes many classical problems:</p> <ul> <li>Knapsack  </li> <li>Assignment  </li> <li>Scheduling  </li> <li>Facility location</li> </ul> <p>Relaxing \\(x \\in \\mathbb{Z}^n\\) to \\(x \\in \\mathbb{R}^n\\) yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>"},{"location":"convex/48_advanced_combinatorial/#224-relaxation-and-rounding","title":"22.4 Relaxation and Rounding","text":"<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>"},{"location":"convex/48_advanced_combinatorial/#2241-lp-relaxation","title":"22.4.1 LP Relaxation","text":"<p>For binary variables \\(x_i \\in \\{0,1\\}\\), relax to \\(0 \\le x_i \\le 1\\) and solve via simplex or interior-point methods.</p>"},{"location":"convex/48_advanced_combinatorial/#2242-semidefinite-relaxation","title":"22.4.2 Semidefinite Relaxation","text":"<p>For quadratic binary problems, lift to a positive semidefinite matrix \\(X = xx^\\top\\):  Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>"},{"location":"convex/48_advanced_combinatorial/#2243-randomized-rounding","title":"22.4.3 Randomized Rounding","text":"<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>"},{"location":"convex/48_advanced_combinatorial/#225-branch-and-bound-and-search-trees","title":"22.5 Branch-and-Bound and Search Trees","text":"<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>"},{"location":"convex/48_advanced_combinatorial/#2251-basic-principle","title":"22.5.1 Basic Principle","text":"<ol> <li>Partition the feasible set into subsets (branching).  </li> <li>Compute upper/lower bounds for each subset.  </li> <li>Prune branches that cannot contain the optimum.  </li> </ol> <p>The algorithm systematically explores a search tree, guided by bounds.</p>"},{"location":"convex/48_advanced_combinatorial/#2252-bounding-via-relaxations","title":"22.5.2 Bounding via Relaxations","text":"<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>"},{"location":"convex/48_advanced_combinatorial/#226-dynamic-programming","title":"22.6 Dynamic Programming","text":"<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p> \\[ \\text{OPT}(S) = \\min_{x \\in S} \\{ c(x) + \\text{OPT}(S') \\}. \\] <p>It is exact but can suffer from exponential growth (\u201ccurse of dimensionality\u201d).</p> <p>Applications:</p> <ul> <li>Shortest paths</li> <li>Sequence alignment</li> <li>Knapsack</li> <li>Resource allocation</li> </ul> <p>DP offers exact solutions when structure allows sequential decomposition.</p>"},{"location":"convex/48_advanced_combinatorial/#227-heuristics-and-metaheuristics-for-combinatorial-problems","title":"22.7 Heuristics and Metaheuristics for Combinatorial Problems","text":"<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>"},{"location":"convex/48_advanced_combinatorial/#2271-greedy-heuristics","title":"22.7.1 Greedy Heuristics","text":"<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal\u2019s MST). Fast but not always globally optimal.</p>"},{"location":"convex/48_advanced_combinatorial/#2272-local-search-and-hill-climbing","title":"22.7.2 Local Search and Hill Climbing","text":"<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>"},{"location":"convex/48_advanced_combinatorial/#2273-metaheuristic-extensions","title":"22.7.3 Metaheuristic Extensions","text":"<ul> <li>Simulated Annealing: controlled random acceptance of worse moves.  </li> <li>Tabu Search: memory-based diversification.  </li> <li>Ant Colony Optimization: probabilistic path construction.  </li> <li>Genetic Algorithms and PSO: population-based evolution.  </li> </ul> <p>These approaches generalize to discrete structures with minimal problem-specific design.</p>"},{"location":"convex/48_advanced_combinatorial/#228-approximation-algorithms","title":"22.8 Approximation Algorithms","text":"<p>Some combinatorial problems are provably intractable but allow approximation guarantees:  where \\(\\alpha \\ge 1\\) is the approximation ratio.</p> <p>Examples:</p> <ul> <li>Greedy Set Cover: \\(\\alpha = \\ln n + 1\\) </li> <li>Christofides\u2019 Algorithm for TSP: \\(\\alpha = 1.5\\) </li> <li>MAX-CUT SDP Relaxation: \\(\\alpha \\approx 0.878\\)</li> </ul> <p>Approximation theory blends combinatorics with convex relaxation insights.</p>"},{"location":"convex/48_advanced_combinatorial/#229-advanced-topics-constraint-programming-and-decomposition","title":"22.9 Advanced Topics: Constraint Programming and Decomposition","text":""},{"location":"convex/48_advanced_combinatorial/#2291-constraint-programming-cp","title":"22.9.1 Constraint Programming (CP)","text":"<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>"},{"location":"convex/48_advanced_combinatorial/#2292-benders-and-dantzigwolfe-decomposition","title":"22.9.2 Benders and Dantzig\u2013Wolfe Decomposition","text":"<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>"},{"location":"convex/48_advanced_combinatorial/#2293-cutting-plane-methods","title":"22.9.3 Cutting Plane Methods","text":"<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>"},{"location":"convex/48_advanced_combinatorial/#2210-applications-across-domains","title":"22.10 Applications Across Domains","text":"Field Combinatorial Problem Examples Logistics Vehicle routing, warehouse layout Telecommunications Network design, channel allocation Machine Learning Feature selection, clustering, model compression Finance Portfolio optimization with integer positions Bioinformatics Genome assembly, protein structure inference <p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p> <p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p>"},{"location":"convex/50_future/","title":"23. The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":""},{"location":"convex/50_future/#chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence","title":"Chapter 23: The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":"<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead \u2014 toward optimization methods that learn, adapt, and reason \u2014 merging human insight, data-driven modeling, and algorithmic intelligence.</p>"},{"location":"convex/50_future/#231-from-fixed-algorithms-to-adaptive-systems","title":"23.1 From Fixed Algorithms to Adaptive Systems","text":"<p>Traditional optimization algorithms are designed by experts and fixed in form:</p> \\[ x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\] <p>or</p> \\[ x_{k+1} = \\text{Update}(x_k, \\nabla f(x_k); \\theta_{\\text{fixed}}). \\] <p>But real-world problems change over time \u2014 data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>"},{"location":"convex/50_future/#232-optimization-as-learning","title":"23.2 Optimization as Learning","text":"<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p> <p>A meta-optimizer \\(\\text{Opt}_\\theta\\) is parameterized by \\(\\theta\\), and trained to minimize:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{f \\sim \\mathcal{D}}[f(\\text{Opt}_\\theta(f))], \\] <p>where \\(\\mathcal{D}\\) is a distribution over problem instances.</p> <p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>"},{"location":"convex/50_future/#233-reinforcement-learned-optimization","title":"23.3 Reinforcement-Learned Optimization","text":"<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p> <p>At each iteration:</p> <ul> <li>State: current iterate \\(x_t\\), gradient \\(\\nabla f(x_t)\\), and loss \\(f(x_t)\\) </li> <li>Action: choose an update \\(\\Delta x_t\\) </li> <li>Reward: improvement in objective, \\(r_t = -[f(x_{t+1}) - f(x_t)]\\)</li> </ul> <p>A policy \\(\\pi_\\theta\\) learns to output update steps that maximize expected reward. This creates an optimizer that discovers efficient update strategies through experience.</p> <p>RL-based optimizers have been successfully applied in:</p> <ul> <li>Hyperparameter tuning  </li> <li>Neural architecture search  </li> <li>Online control systems  </li> <li>Adaptive sampling and scheduling</li> </ul>"},{"location":"convex/50_future/#234-neuroevolution-and-population-learning","title":"23.4 Neuroevolution and Population Learning","text":"<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly. Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p> <p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p> \\[ \\nabla_\\theta \\mathbb{E}[f(\\theta)] \\approx \\frac{1}{\\sigma} \\mathbb{E}[f(\\theta + \\sigma \\epsilon)\\epsilon]. \\] <p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration\u2013exploitation.</p>"},{"location":"convex/50_future/#235-optimization-and-generative-models","title":"23.5 Optimization and Generative Models","text":"<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective: Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p> <p>For example:</p> <ul> <li>Optimize a latent vector \\(z\\) to generate a design with desired properties.  </li> <li>Use differentiable surrogates to backpropagate through generative pipelines.  </li> <li>Apply gradient-based search within learned manifolds.</li> </ul> <p>This blending of optimization and generation enables creativity \u2014 from molecule design to engineering shape synthesis.</p>"},{"location":"convex/50_future/#236-federated-and-decentralized-optimization","title":"23.6 Federated and Decentralized Optimization","text":"<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization. Each client \\(i\\) holds local data \\(D_i\\) and solves:</p> \\[ \\min_x \\; F(x) = \\frac{1}{N}\\sum_i f_i(x), \\] <p>without sharing raw data.</p> <p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p> <p>Challenges include:</p> <ul> <li>Communication efficiency  </li> <li>Heterogeneity of data and computation  </li> <li>Privacy and fairness constraints</li> </ul>"},{"location":"convex/50_future/#237-optimization-under-uncertainty","title":"23.7 Optimization Under Uncertainty","text":"<p>Modern systems often face uncertain environments: - Random perturbations in data - Dynamic constraints - Unpredictable feedback</p> <p>Approaches to manage uncertainty include:</p> <ol> <li> <p>Robust Optimization:    Minimize worst-case loss under bounded perturbations:     </p> </li> <li> <p>Stochastic Programming:    Optimize expected value or risk measure:     </p> </li> <li> <p>Distributionally Robust Optimization (DRO):    Hedge against model misspecification by optimizing over nearby probability distributions.</p> </li> </ol> <p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>"},{"location":"convex/50_future/#238-quantum-and-analog-optimization","title":"23.8 Quantum and Analog Optimization","text":"<p>As hardware advances, new paradigms emerge: - Quantum Annealing: uses quantum tunneling to escape local minima. - Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem. - Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p> <p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>"},{"location":"convex/50_future/#239-optimization-and-intelligence","title":"23.9 Optimization and Intelligence","text":"<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes \u2014 in essence, optimization loops.</p> <p>Emerging research seeks to unify:</p> <ul> <li>Learning to optimize \u2014 algorithms that adapt through data.  </li> <li>Optimizing to learn \u2014 systems that adjust representations via optimization.  </li> <li>Self-improving optimizers \u2014 algorithms that recursively tune their own parameters.</li> </ul> <p>This convergence blurs the line between optimizer and learner.</p> <p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve \u2014 autonomously, efficiently, and creatively.</p> <p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems \u2014 and thinkers \u2014 to improve themselves.</p>"},{"location":"deeplearning/1_mlp/","title":"An Introduction to Neural Networks","text":""},{"location":"deeplearning/1_mlp/#an-introduction-to-neural-networks","title":"An Introduction to Neural Networks","text":""},{"location":"deeplearning/1_mlp/#1-neural-networks-as-computation-graphs","title":"1. Neural Networks as Computation Graphs","text":"<p>Modern neural networks are best understood as differentiable computation graphs.  They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p> <p>Each node in this graph corresponds to a function:</p> \\[z_i = f_i(x_1, \\dots, x_k)\\] <p>and the entire network defines a composite function:</p> \\[f_\\theta(x) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(x)\\] <p>where \\(\\theta = \\{W_i, b_i\\}\\) denotes all learnable parameters.</p>"},{"location":"deeplearning/1_mlp/#formal-structure","title":"Formal Structure","text":"<p>For a Multilayer Perceptron (MLP):</p> \\[h_0 = x, \\quad h_i = \\sigma(W_i h_{i-1} + b_i), \\quad i=1,\\dots,L-1, \\quad \\hat{y} = W_L h_{L-1} + b_L\\] <p>with: \\(W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}, \\quad b_i \\in \\mathbb{R}^{d_i}\\)</p> <p>Each layer is a small differentiable function. When we connect them, we form a composite map \u2014 the fundamental abstraction underlying autodiff, backprop, and learning.</p> <p>Key property: Because every node in the graph is differentiable, the entire function \\(f_\\theta(x)\\) is differentiable with respect to both input \\(x\\) and parameters \\(\\theta\\).</p> <p>Graphically, the network is a directed acyclic graph (DAG):</p> <ul> <li>Edges: carry tensor values.</li> <li>Nodes: represent differentiable functions.</li> <li>Forward pass: evaluates node outputs.</li> <li>Backward pass: propagates sensitivities (gradients) backward.</li> </ul> <p>This graph abstraction unifies all architectures \u2014 CNNs, RNNs, Transformers, Diffusion Models \u2014 as differentiable computation graphs.</p>"},{"location":"deeplearning/1_mlp/#2-gradients-jacobians-and-differentiation","title":"2. Gradients, Jacobians, and Differentiation","text":"<p>For any function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian matrix \\(J_f(x)\\) encodes local derivatives:</p> \\[[J_f(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\] <p>In neural networks, we often deal with a scalar loss function:</p> \\[L(\\theta) = \\ell(f_\\theta(x), y)\\] <p>and want: </p> \\[\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\] <p>However, computing full Jacobians is computationally infeasible \u2014 for a network with millions of parameters, explicit Jacobians would have trillions of entries. Instead, automatic differentiation (autodiff) computes vector\u2013Jacobian products efficiently.</p> <p>For scalar loss \\(L\\): \\(\\nabla_\\theta L = J_{f_\\theta}(x)^T \\nabla_{f_\\theta} L\\)</p> <p>where \\(J_{f_\\theta}(x)\\) is the Jacobian of the output w.r.t. parameters.</p> <p>This operation can be done efficiently in reverse-mode autodiff \u2014 the heart of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#3-forward-and-backward-passes","title":"3. Forward and Backward Passes","text":""},{"location":"deeplearning/1_mlp/#forward-pass","title":"Forward Pass","text":"<p>Given input \\(x\\) and parameters \\(\\theta\\):</p> <ol> <li>Compute layer outputs sequentially: \\(h_i = \\sigma(W_i h_{i-1} + b_i)\\)</li> <li>Compute loss \\(L = \\ell(f_\\theta(x), y)\\)</li> <li>Store intermediate activations \\(h_i\\) for reuse during backpropagation.</li> </ol> <p>This pass evaluates the function \\(L(\\theta)\\).</p>"},{"location":"deeplearning/1_mlp/#backward-pass","title":"Backward Pass","text":"<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p> <p>\\(\\frac{\\partial L}{\\partial \\theta_i} =  \\frac{\\partial L}{\\partial h_L} \\frac{\\partial h_L}{\\partial h_{L-1}} \\dots \\frac{\\partial h_{i+1}}{\\partial \\theta_i}\\)</p> <p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p> <p>Reverse-mode autodiff (backprop) algorithm:</p> <ol> <li>Initialize \\(\\bar{h}_L = \\frac{\\partial L}{\\partial h_L} = 1\\).</li> <li>For each layer \\(l = L, L-1, \\dots, 1\\):</li> <li>Compute local derivative \\(\\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Accumulate gradient: \\(\\bar{h}_{l-1} = \\bar{h}_l \\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Compute parameter gradients: \\(\\frac{\\partial L}{\\partial W_l} = \\bar{h}_l (h_{l-1})^T\\)</li> <li>Return all \\(\\nabla_\\theta L\\).</li> </ol> <p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#4-chain-rule-backpropagation-and-automatic-differentiation","title":"4. Chain Rule, Backpropagation, and Automatic Differentiation","text":"<p>The chain rule underpins all gradient computation. For scalar functions:</p> <p>\\(\\frac{dL}{dx} = \\frac{dL}{dz} \\frac{dz}{dx}\\)</p> <p>and recursively for multivariate functions:</p> <p>\\(\\nabla_x L = J_{z}(x)^T \\nabla_z L\\)</p> <p>Autodiff implements this automatically, performing either:</p> <ul> <li>Forward-mode AD: propagates derivatives forward, efficient when #inputs \u226a #outputs.</li> <li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs \u226a #inputs (our case).</li> </ul> <p>Reverse-mode AD \u2261 backpropagation.</p> <p>Computational Complexity: - Cost \u2248 2\u00d7 forward pass (one forward, one backward). - Memory \u2248 size of stored activations.</p> <p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p> <p>\\(\\min_\\theta L(\\theta)\\)</p> <p>where \\(L\\) is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms. s</p>"},{"location":"deeplearning/1_mlp/#5-from-gradients-to-optimization","title":"5. From Gradients to Optimization","text":"<p>The Learning Problem - Training a neural network means solving:</p> <p>\\(\\min_\\theta \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} [\\,\\ell(f_\\theta(x), y)\\,]\\)</p> <p>Since the true data distribution \\(\\mathcal{D}\\) is unknown, we use empirical risk minimization (ERM):</p> <p>\\(\\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\ell(f_\\theta(x_i), y_i)\\)</p> <p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods \u2014 powered by backpropagation \u2014 reliably find good solutions.</p>"},{"location":"deeplearning/1_mlp/#first-order-optimization-algorithms","title":"First-Order Optimization Algorithms","text":"<p>All modern deep learning optimization relies on gradients:</p> <p>\\(\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\)</p> <p>The basic rule: update parameters in the direction of negative gradient:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_t\\)</p> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"deeplearning/1_mlp/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>We use mini-batches instead of full data:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\frac{1}{|B_t|}\\sum_{i \\in B_t} \\ell(f_\\theta(x_i), y_i)\\)</p> <ul> <li>Cheap per-step computation.</li> <li>Introduces gradient noise, which helps escape shallow minima and saddle points.</li> </ul>"},{"location":"deeplearning/1_mlp/#momentum","title":"Momentum","text":"<p>Accelerates learning by accumulating a velocity vector:</p> <p>\\(v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L_t, \\quad \\theta_{t+1} = \\theta_t + v_{t+1}\\)</p> <p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>"},{"location":"deeplearning/1_mlp/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Maintains exponentially weighted averages of gradients and squared gradients:</p> <p>\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla_\\theta L_t\\)</p> <p>\\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla_\\theta L_t)^2\\)</p> <p>Bias-corrected updates:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\\)</p> <p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>"},{"location":"deeplearning/1_mlp/#second-order-and-curvature-aware-methods","title":"Second-Order and Curvature-Aware Methods","text":"<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p> <p>\\(H = \\frac{\\partial^2 L}{\\partial \\theta^2}\\)</p> <p>Newton\u2019s update:</p> <p>\\(\\theta_{t+1} = \\theta_t - H^{-1}\\nabla_\\theta L\\)</p> <p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets. Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>"},{"location":"deeplearning/1_mlp/#optimization-landscape-and-gradient-flow","title":"Optimization Landscape and Gradient Flow","text":"<p>Although neural network loss surfaces are highly nonconvex, they possess favorable geometry:</p> <ul> <li>Most critical points are saddle points, not local minima.</li> <li>Wide, flat minima generalize better (implicit regularization of SGD).</li> <li>Gradient noise helps explore valleys in high-dimensional space.</li> </ul> <p>Gradient flow (continuous limit of SGD):</p> <p>\\(\\frac{d\\theta(t)}{dt} = - \\nabla_\\theta L(\\theta(t))\\)</p> <p>describes a trajectory in parameter space governed by the vector field of gradients.</p> <p>The optimization algorithm defines the dynamics of this flow (e.g., momentum adds inertia).</p>"},{"location":"deeplearning/1_mlp/#6-what-mlps-cant-do","title":"6. What MLPs Can\u2019t Do?","text":""},{"location":"deeplearning/1_mlp/#a-multiplicative-interactions","title":"(a) Multiplicative Interactions","text":"<p>MLPs compute sums of weighted activations \u2014 inherently additive operations:</p> <p>\\(h = \\sigma(Wx + b)\\)</p> <p>They cannot naturally represent multiplicative relationships (like \\(x_1 x_2\\)) unless approximated via nonlinear stacking, which is inefficient.</p> <p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>"},{"location":"deeplearning/1_mlp/#b-attention-and-dynamic-routing","title":"(b) Attention and Dynamic Routing","text":"<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V\\)</p> <p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>"},{"location":"deeplearning/1_mlp/#c-metric-learning-and-inductive-bias","title":"(c) Metric Learning and Inductive Bias","text":"<p>MLPs lack structural priors about similarity or geometry. Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p> <p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient \u2014 the landscape becomes smoother and gradients more informative.</p>"},{"location":"deeplearning/1_mlp/#7-beyond-backprop-curvature-generalization-and-geometry","title":"7. Beyond Backprop: Curvature, Generalization, and Geometry","text":"<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>"},{"location":"deeplearning/1_mlp/#natural-gradient","title":"Natural Gradient","text":"<p>Instead of minimizing loss directly in parameter space, we minimize it in function space:</p> <p>\\(\\Delta \\theta = - \\eta F^{-1} \\nabla_\\theta L\\)</p> <p>where \\(F\\) is the Fisher information matrix:</p> <p>\\(F = \\mathbb{E}\\left[\\nabla_\\theta \\log p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x)^T\\right]\\)</p> <p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>"},{"location":"deeplearning/1_mlp/#implicit-bias-of-gradient-descent","title":"Implicit Bias of Gradient Descent","text":"<p>Even in overparameterized models, gradient descent tends to find low-norm or flat minima that generalize better \u2014 a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>"},{"location":"deeplearning/1_mlp/#optimization-as-inference","title":"Optimization as Inference","text":"<p>Many modern perspectives view training as approximate inference:</p> <p>\\(p(\\theta | D) \\propto e^{-L(\\theta)/T}\\)</p> <p>Gradient descent samples from this energy landscape as \\(T \\to 0\\); stochastic variants like SGD approximate Bayesian inference under certain limits.</p>"},{"location":"deeplearning/2_convnets/","title":"Chapter 2: Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#chapter-2-convolutional-neural-networks-cnns","title":"Chapter 2: Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#1-core-principles-locality-and-translation-invariance","title":"1. Core Principles: Locality and Translation Invariance","text":"<p>Before understanding convolutional networks, it\u2019s crucial to grasp why they exist \u2014 the structural priors they impose on data.</p>"},{"location":"deeplearning/2_convnets/#11-locality","title":"1.1 Locality","text":"<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p> <p>For example:</p> <ul> <li>Adjacent pixels in an image often belong to the same object or texture.</li> <li>Neighboring audio samples belong to the same phoneme.</li> <li>Nearby words in a sentence influence each other\u2019s meaning.</li> </ul> <p>MLPs treat every input dimension as independent, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, local region of the input, called its receptive field.</p> <p>Formally, for an input \\(x \\in \\mathbb{R}^{H \\times W}\\), a neuron at position \\((i,j)\\) in a CNN depends only on values in a small window \\(\\Omega(i,j)\\):  This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>"},{"location":"deeplearning/2_convnets/#12-translation-invariance","title":"1.2 Translation Invariance","text":"<p>Natural patterns are repeatable across locations \u2014 the same feature (e.g., an edge, a cat\u2019s ear) can appear anywhere in the image.</p> <p>An MLP would need to learn a separate detector for each position. CNNs overcome this through weight sharing: the same filter \\(W\\) is applied across all spatial positions.</p> <p>Mathematically:  </p> <p>This operation \u2014 convolution \u2014 ensures translation equivariance:  meaning if the input shifts by \\(\\Delta\\), the output shifts by the same amount. After pooling, this becomes translation invariance, i.e. the output doesn\u2019t change under small shifts.</p> <p>These two properties \u2014 locality and translation invariance \u2014 are the foundation of convolutional architectures.</p>"},{"location":"deeplearning/2_convnets/#2-motivation-why-convolutions","title":"2. Motivation: Why Convolutions?","text":"<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos. An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p> <p>Example: For a 256\u00d7256 RGB image (\u2248200K input features), even one hidden layer with 1,000 neurons requires: \\(\\((256 \\times 256 \\times 3) \\times 1000 = 196\\,\\text{million weights}.\\)\\)</p> <p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p> <p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>"},{"location":"deeplearning/2_convnets/#3-the-convolution-operation","title":"3. The Convolution Operation","text":""},{"location":"deeplearning/2_convnets/#31-discrete-convolution","title":"3.1 Discrete Convolution","text":"<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p> <p>For 2D inputs (e.g. images):</p> \\[ S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n) \\] <ul> <li>\\(I\\) \u2014 input (image)</li> <li>\\(K\\) \u2014 kernel (filter)</li> <li>\\(S\\) \u2014 output feature map</li> </ul> <p>Each filter detects a specific local pattern (edges, corners, textures).</p>"},{"location":"deeplearning/2_convnets/#32-convolution-in-neural-networks","title":"3.2 Convolution in Neural Networks","text":"<p>In CNNs, the convolution becomes a learnable operation:</p> \\[ h_{i,j,k} = \\sigma\\left( \\sum_{c=1}^{C_\\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \\right) \\] <ul> <li>\\(x_c\\): input channel \\(c\\) (e.g. R, G, B)</li> <li>\\(W_{k,c}\\): kernel for output channel \\(k\\) and input channel \\(c\\)</li> <li>\\(b_k\\): bias for output channel \\(k\\)</li> <li>\\(\\sigma\\): nonlinearity (ReLU, etc.)</li> </ul> <p>This produces \\(C_\\text{out}\\) feature maps, each representing a learned spatial pattern.</p> <p>Weight sharing drastically reduces parameters: Each kernel might be \\(3 \\times 3\\) or \\(5 \\times 5\\) \u2014 independent of image size.</p>"},{"location":"deeplearning/2_convnets/#4-building-blocks-of-cnns","title":"4. Building Blocks of CNNs","text":""},{"location":"deeplearning/2_convnets/#41-convolutional-layer","title":"4.1 Convolutional Layer","text":"<p>Performs learnable filtering and produces feature maps.</p> <p>If input has shape \\((H, W, C_\\text{in})\\): - Kernel: \\((k_H, k_W, C_\\text{in}, C_\\text{out})\\) - Output: \\((H', W', C_\\text{out})\\)</p>"},{"location":"deeplearning/2_convnets/#42-nonlinear-activation","title":"4.2 Nonlinear Activation","text":"<p>After convolution, apply nonlinearity (commonly ReLU):  </p>"},{"location":"deeplearning/2_convnets/#43-pooling-layer","title":"4.3 Pooling Layer","text":"<p>Reduces spatial dimensions and increases invariance.</p> <ul> <li>Max pooling: selects the largest value in a patch.</li> <li>Average pooling: takes mean value.</li> </ul> <p>Formally:  </p> <p>Pooling introduces translation invariance \u2014 small shifts in input don\u2019t drastically change outputs.</p>"},{"location":"deeplearning/2_convnets/#44-flatten-fully-connected-layers","title":"4.4 Flatten + Fully Connected Layers","text":"<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>"},{"location":"deeplearning/2_convnets/#5-cnn-architecture-as-a-computation-graph","title":"5. CNN Architecture as a Computation Graph","text":"<p>A typical CNN defines a differentiable map:</p> \\[ f_\\theta(x) = W_L (\\text{Flatten}(h_{L-1})) + b_L \\] <p>where each layer \\(h_l\\) is defined recursively as:</p> \\[ h_l = \\sigma(\\text{Conv}(h_{l-1}; W_l) + b_l), \\quad l = 1, \\dots, L-1 \\] <p>Here, <code>Conv</code> represents the convolution operation.</p> <p>Each layer is spatially local, translation-equivariant, and differentiable \u2014 meaning backpropagation works seamlessly, just as in MLPs.</p>"},{"location":"deeplearning/2_convnets/#6-backpropagation-through-convolutions","title":"6. Backpropagation Through Convolutions","text":"<p>The gradient computation is a direct extension of the chain rule.</p>"},{"location":"deeplearning/2_convnets/#61-forward-pass","title":"6.1 Forward Pass","text":"<p>Compute:  </p>"},{"location":"deeplearning/2_convnets/#62-backward-pass","title":"6.2 Backward Pass","text":"<p>We need: - Gradient w.r.t. weights: \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) - Gradient w.r.t. input: \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\)</p> <p>The flipping arises from the mathematical property of convolution. Modern frameworks handle this efficiently via convolution transpose operations.</p> <p>Optimization viewpoint: Convolution layers remain linear in their weights \u2014 the nonlinearity and local parameter sharing define their expressive power.</p>"},{"location":"deeplearning/2_convnets/#7-inductive-biases-in-cnns","title":"7. Inductive Biases in CNNs","text":"<p>Convolutional architectures embed strong inductive biases:</p> Property Mathematical Mechanism Effect Local connectivity Small kernels (3\u00d73, 5\u00d75) Exploits spatial locality Weight sharing Same filter across space Reduces parameters drastically Translation equivariance Convolution operation Same pattern detection anywhere Pooling invariance Spatial downsampling Robust to small shifts/noise <p>These biases make CNNs data-efficient and easy to train \u2014 especially compared to fully connected networks on images.</p>"},{"location":"deeplearning/2_convnets/#8-optimization-and-training-dynamics","title":"8. Optimization and Training Dynamics","text":"<p>Training CNNs is similar to MLPs \u2014 we use gradient-based optimizers (SGD, Adam, etc.) \u2014 but with different landscape geometry:</p> <ul> <li>Parameter sharing makes the loss smoother (less overfitting).</li> <li>Batch normalization stabilizes gradient flow:    </li> <li>Regularization via dropout or weight decay improves generalization.</li> <li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li> </ul> <p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>"},{"location":"deeplearning/2_convnets/#9-cnn-architectures-through-history","title":"9. CNN Architectures Through History","text":"Model Year Key Innovation Depth Inductive Bias LeNet-5 1998 First practical CNN for handwritten digits 7 layers Local receptive fields AlexNet 2012 GPU training, ReLU, dropout 8 layers Data augmentation VGG 2014 Deep stacks of small 3\u00d73 filters 19 layers Uniform architecture ResNet 2015 Skip connections for gradient flow 152 layers Identity mapping DenseNet 2016 Feature reuse via dense connectivity 201 layers Multi-scale learning EfficientNet 2019 Compound scaling variable Optimized parameter scaling"},{"location":"deeplearning/2_convnets/#10-cnns-and-the-optimization-landscape","title":"10. CNNs and the Optimization Landscape","text":"<p>CNNs reshape the optimization problem compared to MLPs:</p> <ul> <li>Reduced parameter redundancy \u2192 fewer degenerate directions in gradient space.</li> <li>Structured weight sharing \u2192 smoother loss surface, fewer sharp minima.</li> <li>Skip connections (ResNets) introduce identity mappings, improving conditioning of the Jacobian and preventing vanishing gradients.</li> </ul> <p>In optimization terms, CNNs are better-conditioned models of the input\u2013output mapping.</p>"},{"location":"deeplearning/2_convnets/#11-beyond-classical-cnns","title":"11. Beyond Classical CNNs","text":"<p>Modern vision architectures have evolved: - Residual Networks (ResNets): skip connections allow training very deep models. - Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count. - Dilated Convolutions: expand receptive field without extra parameters. - Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>"},{"location":"deeplearning/2_convnets/#12-mathematical-summary","title":"12. Mathematical Summary","text":"Concept Formula Description Convolution \\((I * K)(i,j) = \\sum_m \\sum_n I(i+m,j+n) K(m,n)\\) Weighted local sum CNN Layer \\(h = \\sigma(W * x + b)\\) Convolution + nonlinearity Pooling \\(y_{i,j} = \\max_{(m,n)\\in \\Omega(i,j)} h_{m,n}\\) Downsampling Gradient wrt weights \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) Backprop step Gradient wrt input \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\) Sensitivity propagation"},{"location":"deeplearning/2_convnets/#13-intuitive-summary","title":"13. Intuitive Summary","text":"<p>Convolutional networks are: - Local \u2192 they process neighborhoods of data. - Hierarchical \u2192 deeper layers build on lower-level features. - Translation-equivariant \u2192 same pattern anywhere is treated the same. - Efficient \u2192 far fewer parameters than MLPs.</p> <p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p>"},{"location":"deeplearning/3_sequence_data/","title":"Chapter 3: Modeling Sequence Data in Deep Learning","text":""},{"location":"deeplearning/3_sequence_data/#chapter-3-modeling-sequence-data-in-deep-learning","title":"Chapter 3: Modeling Sequence Data in Deep Learning","text":"<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length \\(T\\) can be written as \\((x_1,x_2,\\dots,x_T)\\), where each element \\(x_t\\) is indexed by its position in the sequence. Elements can repeat (e.g. the word \u201cthe\u201d may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p> <p>Sequences are collection of elements where:</p> <ul> <li>Elements can be repeated.</li> <li>Order matters.</li> <li>Of variable length.</li> </ul>"},{"location":"deeplearning/3_sequence_data/#limitations-of-traditional-supervised-models","title":"Limitations of Traditional Supervised Models:","text":"<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data \u2013 by, say, collapsing the sequence into a fixed-size feature vector \u2013 ignores the important temporal or sequential structure. As one summary notes, \u201cfeedforward neural networks are severely limited when it comes to sequential data\u201d. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p> <p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>"},{"location":"deeplearning/3_sequence_data/#the-simplest-assumption-independent-words-bag-of-words","title":"The Simplest Assumption: Independent Words (Bag-of-Words)","text":"<p>A na\u00efve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word\u2019s occurrence, treating all words as \u201cindependent features.\u201d This ignores sequence structure: \u201cthe order of words in the original documents is irrelevant\u201d. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: \u201cbag-of-words assumes words are independent of one another\u201d. In reality, words co-occur in context (\u201cpeanut butter\u201d versus \u201cpeanut giraffe\u201d) \u2013 bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>"},{"location":"deeplearning/3_sequence_data/#n-gram-models-and-fixed-context-assumptions","title":"N-gram Models and Fixed-Context Assumptions","text":"<p>To go beyond complete independence, one can incorporate local context by using \\(n\\)-gram models. An \\(n\\)-gram model makes the (Markov) assumption that the probability of each element depends only on the previous \\(n-1\\) elements. For language, a bigram model (2-gram) assumes \\(P(w_t\\mid w_{t-1})\\), a trigram (3-gram) uses \\(P(w_t\\mid w_{t-2},w_{t-1})\\), etc. In general, the chain rule with an \\(N\\)-gram approximation is</p> \\[ P(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{t-N+1}, \\ldots, x_{t-1}) \\, . \\] <p>This preserves some order information: the window of the last \\(N-1\\) items is used to predict the next. However, \\(n\\)-gram models have well-known downsides:</p> <ul> <li> <p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language \u201ccannot reason about context beyond the immediate \\(n\\)-gram window\u201d, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an \\(n\\)-gram.</p> </li> <li> <p>Data sparsity and scalability: The number of possible \\(n\\)-grams grows exponentially with vocabulary size \\(V\\). For a vocabulary of size \\(V\\), there are \\(V^N\\) possible \\(N\\)-grams. Jurafsky &amp; Martin observe that even for Shakespeare\u2019s corpus (\\(V\\approx 29{,}066\\)), there are \\(V^2\\approx8.4\\times 10^8\\) possible bigrams and \\(V^4\\approx 7\\times 10^{17}\\) possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough \\(n\\)-gram counts, and storing these tables is impractical for large \\(N\\) or \\(V\\). In practice, language models become \u201cridiculously sparse\u201d and unwieldy.</p> </li> <li> <p>No parametrization (non-differentiable): Traditional \\(n\\)-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p> </li> </ul> <p>In summary, while \\(n\\)-grams preserve local order up to length \\(N\\), they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>"},{"location":"deeplearning/3_sequence_data/#learnable-context-models-vectorization-and-neural-nets","title":"Learnable Context Models: Vectorization and Neural Nets","text":"<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p> <ul> <li> <p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p> </li> <li> <p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p> </li> <li> <p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can \u201coperate in variable length of sequences\u201d.</p> </li> <li> <p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p> </li> <li> <p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p> </li> </ul> <p>For example, one could take the last few words, map each to an embedding \\(\\mathbf{x}{t-N+1},\\dots,\\mathbf{x}{t-1}\\), concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word\u2019s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (\\(N-1\\)) and uses a separate weight for each position, so it\u2019s not efficient or variable-length. </p> <p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector \\(\\mathbf{h}_t\\) that evolves as the sequence is read. Such a context-vector \u201cacts as memory\u201d summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can \u201clearn\u201d how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>"},{"location":"deeplearning/3_sequence_data/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) \u2013 models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step \\(t\\), the RNN takes the current input \\(\\mathbf{x}t\\) and the previous hidden state \\(\\mathbf{h}{t-1}\\) and computes a new hidden state \\(\\mathbf{h}_t\\). The simplest RNN update is:</p> \\[ h_t = \\phi(W_h h_{t-1} + W_x x_t + b) \\, . \\] <p>where \\(\\phi\\) is a nonlinear activation (often \\(\\tanh\\)) and \\(W_h,W_x\\) are weight matrices. The same weight matrices \\(W_h,W_x\\) are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p> <p>Intuitively, the RNN\u2019s hidden state \\(\\mathbf{h}_t\\) \u201cremembers\u201d the information from all prior inputs up to time \\(t\\). The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p> \\[ y_t = \\mathrm{softmax}(W_y h_t + b_y) \\, . \\] <p>so that \\(P(x_{t+1}=w \\mid \\mathbf{h}_t)\\) is given by the corresponding component of \\(\\mathbf{y}_t\\). In language modeling, for instance, \\(y_t\\) gives a probability for each word in the vocabulary. As described in practice, \u201cRNNs predict the output from the last hidden state along with output parameter \\(W_y\\); a softmax function to ensure the probability over all possible words\u201d. </p> <p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an \u201cMLP with shared weights across time.\u201d At time \\(t\\), the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>"},{"location":"deeplearning/3_sequence_data/#unrolling-and-backpropagation-through-time-bptt","title":"Unrolling and Backpropagation Through Time (BPTT)","text":"<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across \\(T\\) time steps, creating a deep feedforward network of depth \\(T\\) (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, \u201cthe network needs to be expanded, or unfolded, so that the parameters could be differentiated ... \u2013 hence backpropagation through time (BPTT)\u201d. In practice, each weight matrix \\(W\\) receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if \\(L = -\\sum_t \\log P(x_t\\mid \\mathbf{h}_{t-1})\\) is the loss, then for each \\(W\\) we compute</p> \\[ \\frac{\\partial L}{\\partial W} = \\sum_{t} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} \\, . \\] <p>taking into account the influence of \\(W\\) at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>"},{"location":"deeplearning/3_sequence_data/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative \\(\\partial \\mathbf{h}t/\\partial \\mathbf{h}{t-1}\\) involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius \\(&lt;1\\) causes the gradients to shrink exponentially (vanishing), while if the spectral radius is \\(&gt;1\\) they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues \\(&gt;1\\)), whereas the vanishing gradient problem occurs when long-term components of the gradient go \u201cexponentially fast to norm 0\u201d. Formally, for a linearized RNN one can show that if the largest eigenvalue \\(\\lambda_{\\max}\\) of the recurrent weight matrix satisfies \\(|\\lambda_{\\max}|&lt;1\\), long-term gradients vanish as \\(t\\to\\infty\\), and if \\(|\\lambda_{\\max}|&gt;1\\) they explode. </p> <p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: \u201cwhen long term components go to zero, the model cannot learn correlation between distant events.\u201d In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>"},{"location":"deeplearning/3_sequence_data/#gated-architectures-lstm-and-gru","title":"Gated Architectures: LSTM and GRU","text":"<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable \u201cgates\u201d that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state \\(\\mathbf{C}_t\\) and three gates: input (\\(\\mathbf{i}_t\\)), forget (\\(\\mathbf{f}_t\\)), and output (\\(\\mathbf{o}t\\)) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time \\(t\\) with input \\(\\mathbf{x}t\\) and previous hidden \\(\\mathbf{h}{t-1}\\) and cell \\(\\mathbf{C}{t-1}\\), the gates and cell update are given by (all operations are elementwise):</p> <p>\u200b </p> <p>The new cell state \\(\\mathbf{C}_t\\) is then updated by combining the old state and the candidate:</p> \\[ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\, . \\] <p>where \\(\\odot\\) denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p> \\[ h_t = o_t \\odot \\tanh(C_t) \\, \\] <p>The intuition is that the forget gate \\(\\mathbf{f_t}\\) can reset or retain the old memory \\(\\mathbf{C}_{t-1}\\), the input gate \\(\\mathbf{i}_t\\) controls how much new information \\(\\tilde{\\mathbf{C}}_t\\) to write, and the output gate \\(\\mathbf{o}_t\\) controls how much of the cell state to expose as \\(\\mathbf{h}_t\\). By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs \u201calleviate the vanishing gradient problem,\u201d making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p> <p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>"},{"location":"deeplearning/3_sequence_data/#optimization-challenges-and-solutions","title":"Optimization Challenges and Solutions","text":"<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p> <ul> <li> <p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping \u201csolves the exploding gradients problem\u201d by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p> </li> <li> <p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing \\(W_h\\) as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, \u201cOrthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,\u201d ensuring stable gradient propagation. In practice, some implementations initialize \\(W_h\\) to random orthogonal (or unitary) matrices to encourage long memory.</p> </li> <li> <p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p> </li> <li> <p>Regularization: Some works add penalties to encourage \\(W_h\\) to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p> </li> </ul> <p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models \u2013 especially gated RNNs \u2013 provide a powerful framework: mathematically, they define hidden states \\(\\mathbf{h}_t\\) updated by \\(\\mathbf{h}t = f(\\mathbf{h}{t-1},\\mathbf{x}_t)\\) with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p>"},{"location":"deeplearning/4_nlp/","title":"Deep Learning for Natural Language Processing","text":""},{"location":"deeplearning/4_nlp/#deep-learning-for-natural-language-processing","title":"Deep Learning for Natural Language Processing","text":"<ul> <li>Natural language is context-dependent, compositional, and ambiguous.</li> <li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation \u2014 ideal for modeling contextual relationships.</li> <li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li> </ul>"},{"location":"deeplearning/4_nlp/#key-challenges-of-language","title":"Key Challenges of Language","text":"<p>Human language presents a unique set of challenges for computational models. Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p> <ul> <li> <p>Words are not discrete symbols.   The same word can have several related senses depending on context \u2014 for example: <code>face\u2081</code> (human face), <code>face\u2082</code> (clock face), <code>face\u2083</code> (to confront), and <code>face\u2084</code> (a person or presence).   Treating these as independent dictionary entries loses the shared semantic structure between them.   A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p> </li> <li> <p>Need for distributed representations.   Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).   This allows similarity, analogy, and composition to emerge geometrically \u2014 for instance, <code>king - man + woman \u2248 queen</code>.</p> </li> <li> <p>Disambiguation depends on context.   The meaning of a word or phrase is determined by its linguistic surroundings.   For example, in \u201cThe man who ate the pepper sneezed,\u201d the subject of sneezed is determined by a non-adjacent clause (the man), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p> </li> <li> <p>Non-local dependencies.   Natural language contains relationships between words that may be far apart in sequence.   Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.   Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p> </li> <li> <p>Compositionality.   The meaning of larger expressions arises from the meanings of their parts and how they are combined.   However, this combination is not purely linear.   For example, <code>carnivorous plant</code> is not simply the sum of carnivore and plant \u2014 its interpretation depends on how the features interact (a plant that eats insects).   Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p> </li> </ul> <p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically. Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>"},{"location":"deeplearning/4_nlp/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Sequence models (RNNs, LSTMs) process tokens sequentially \u2014 limiting parallelism and long-range context.</li> <li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li> </ul>"},{"location":"deeplearning/4_nlp/#core-mechanism-self-attention","title":"Core Mechanism: Self-Attention","text":"<p>Given token embeddings :</p> \\[ q_i = e_i W^Q, \\quad k_i = e_i W^K, \\quad v_i = e_i W^V \\] <p>Attention weights:</p> \\[ \\alpha_{ij} = \\mathrm{softmax}_j \\left( \\frac{q_i k_j^\\top}{\\sqrt{d}} \\right) \\] <p>Output:</p> \\[ z_i = \\sum_j \\alpha_{ij} v_j \\] <p>Each token\u2019s new representation  is a contextual blend of all others. Captures semantic and syntactic relations without explicit recurrence.</p>"},{"location":"deeplearning/4_nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Use multiple projections \\((W^Q_h, W^K_h, W^V_h)\\) \u2192 multiple \u201cheads.\u201d Each head focuses on different relations (e.g. subject\u2013verb, modifier\u2013noun). Outputs are concatenated and projected back to dimension \\(d\\):</p> \\[ \\text{MHA}(E) = [Z_1; Z_2; \\dots; Z_H] W^O \\]"},{"location":"deeplearning/4_nlp/#position-encoding","title":"Position Encoding","text":"<p>Since attention is permutation-invariant, Transformers add position information:</p> \\[ \\text{PE}_{(pos,2i)} = \\sin(pos / 10000^{2i/d}), \\quad \\text{PE}_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d}) \\] <p>\u2192 These sinusoidal signals are added to embeddings to encode word order.</p>"},{"location":"deeplearning/4_nlp/#full-transformer-block","title":"Full Transformer Block","text":"<pre><code>Input\n  \u2193\nMulti-Head Self-Attention\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nFeedforward Network (ReLU)\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nOutput\n</code></pre> <p>Skip connections enable gradient flow and top-down influence. Stacking \\(N\\) blocks yields hierarchical contextualization of meaning.</p>"},{"location":"deeplearning/4_nlp/#intuition","title":"Intuition","text":"<ul> <li>Self-attention handles non-local relations.</li> <li>Multi-head captures multiple semantic dimensions simultaneously.</li> <li>Stacked layers build abstraction \u2014 from word-level to phrase- and discourse-level features.</li> </ul>"},{"location":"deeplearning/4_nlp/#unsupervised-learning-and-bert","title":"Unsupervised Learning and BERT","text":""},{"location":"deeplearning/4_nlp/#the-need-for-contextualized-representations","title":"The Need for Contextualized Representations","text":"<ul> <li>Word embeddings like Word2Vec are static: one vector per word.</li> <li>Language understanding requires contextual embeddings: \u201cbank\u201d (river vs. finance).</li> <li>Transformers enable bidirectional context \u2014 understanding a word from both sides.</li> </ul>"},{"location":"deeplearning/4_nlp/#bert-pretraining-objectives","title":"BERT Pretraining Objectives","text":"<ol> <li>Masked Language Modeling (MLM) Randomly mask 15% of tokens, predict them:</li> </ol> \\[ \\text{Loss}_{MLM} = - \\sum_{i \\in M} \\log P(w_i | \\text{context}) \\] <p>Encourages bidirectional encoding of meaning.</p> <ol> <li>Next Sentence Prediction (NSP) Model predicts if sentence B follows sentence A. Builds discourse-level coherence and world knowledge.</li> </ol>"},{"location":"deeplearning/4_nlp/#architecture","title":"Architecture","text":"<ul> <li>Deep bidirectional Transformer encoder.</li> <li>Uses special tokens:</li> <li><code>[CLS]</code> \u2013 sentence-level classification embedding</li> <li><code>[SEP]</code> \u2013 separates segments</li> <li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li> <li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li> </ul>"},{"location":"deeplearning/4_nlp/#significance","title":"Significance","text":"<p>BERT shows self-supervised pretraining \u2192 transfer learning pipeline:</p> <pre><code>Pretrain (unsupervised)\n   \u2193\nFine-tune (supervised)\n   \u2193\nTask-specific adaptation\n</code></pre> <p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data. Learns semantic similarity, coreference, and discourse relations implicitly.</p>"},{"location":"deeplearning/4_nlp/#grounded-and-embodied-language-learning","title":"Grounded and Embodied Language Learning","text":""},{"location":"deeplearning/4_nlp/#motivation","title":"Motivation","text":"<ul> <li>Language understanding ultimately involves relating words to the world.</li> <li>Humans learn language in context \u2014 perception, action, and social interaction.</li> <li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li> </ul>"},{"location":"deeplearning/4_nlp/#grounded-agents","title":"Grounded Agents","text":"<ul> <li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li> <li>Train via predictive modeling \u2014 anticipate sensory outcomes from language-conditioned actions.</li> <li>Enables semantic grounding: linking word \u201cred\u201d to visual color, \u201cpick up\u201d to motor command.</li> </ul>"},{"location":"deeplearning/4_nlp/#predictive-and-self-supervised-paradigms","title":"Predictive and Self-Supervised Paradigms","text":"<p>Agents learn representations by predicting future sensory or linguistic states:</p> \\[ \\min_\\theta \\mathbb{E} [ \\| f_\\theta(s_t, a_t) - s_{t+1} \\|^2 ] \\] <p>\u2192 Connects to world models and predictive coding principles in neuroscience. The agent\u2019s internal model encodes both linguistic meaning and causal structure of the environment.</p>"},{"location":"deeplearning/4_nlp/#insights-from-deepmind-work","title":"Insights from DeepMind Work","text":"<ul> <li>Embodied agents trained in simulated environments exhibit:</li> <li>Systematic generalization (e.g., learning \u201cpick up red object\u201d \u2192 generalize to unseen colors).</li> <li>Question answering and instruction following grounded in perception.</li> <li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li> </ul>"},{"location":"deeplearning/4_nlp/#conceptual-shift","title":"Conceptual Shift","text":"<p>From pipeline \u2192 integrated model:</p> Classic Pipeline Embodied / Interactive Model Letters \u2192 Words \u2192 Syntax \u2192 Meaning \u2192 Action Multimodal loops: Perception \u2194 Action \u2194 Language \u2194 Prediction"},{"location":"deeplearning/4_nlp/#conceptual-map-from-representation-to-understanding","title":"Conceptual Map: From Representation to Understanding","text":"<pre><code>Word Input\n   \u2193\nDistributed Representations (embedding)\n   \u2193\nSelf-Attention Mechanism\n   \u2193\nMulti-Head Parallel Processing\n   \u2193\nHierarchical Transformer Layers\n   \u2193\nContextualized Embeddings (BERT)\n   \u2193\nTransfer Learning to Tasks\n   \u2193\nEmbodied Agents (Grounded Semantics)\n   \u2193\nLanguage Understanding as Prediction + Interaction\n</code></pre>"},{"location":"deeplearning/4_nlp/#key-transitions","title":"Key Transitions","text":"<p>Symbol \u2192 Vector: Continuous representations enable learning of semantic gradients.</p> <p>Sequence \u2192 Attention: Parallel context integration replaces recurrence.</p> <p>Text \u2192 Context: Pretraining captures knowledge without explicit supervision.</p> <p>Language \u2192 World: Grounding links linguistic representations to sensory and causal models.</p>"},{"location":"deeplearning/4_nlp/#unifying-principle","title":"Unifying Principle","text":"<p>Deep language understanding = predictive modeling of structured context across both linguistic and environmental domains.</p> Concept Core Idea Model / Mechanism Distributed representations Meanings as patterns, not symbols Embeddings Context dependence Sense resolution via interaction Self-attention Parallelism All words attend to all others Transformer Bidirectionality Context from both sides BERT encoder Transfer learning Self-supervised \u2192 supervised Fine-tuning Grounding Language tied to perception/action Embodied agents Predictive learning Understanding as anticipation World models"},{"location":"deeplearning/5_attention/","title":"5 attention","text":""},{"location":"deeplearning/5_attention/#1-attention-memory-and-cognition","title":"1. Attention, Memory, and Cognition","text":"<ul> <li>Attention = ability to focus on relevant signals and ignore distractions.  </li> <li>Enables selective processing (e.g. cocktail party effect).  </li> <li> <p>Allows focusing on one thought or event at a time.</p> </li> <li> <p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p> </li> <li> <p>Together, they form the basis of cognition \u2014 controlling what to process, store, and recall.</p> </li> <li> <p>Neural networks can model aspects of this by learning what to attend to and what to remember.</p> </li> <li> <p>Goal of attention in DL:   Reduce complexity by focusing computation on the most informative parts of data or internal state.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#2-implicit-attention-in-neural-networks","title":"2. Implicit Attention in Neural Networks","text":"<ul> <li> <p>Neural networks are parametric nonlinear functions \\(y = f_\\theta(x)\\) mapping inputs to outputs.   They naturally exhibit implicit attention: certain input dimensions influence outputs more.</p> </li> <li> <p>The Jacobian \\(J = \\frac{\\partial y}{\\partial x}\\) quantifies this sensitivity \u2014 shows which input parts the model \u201cpays attention\u201d to.</p> </li> <li> <p>Example:   In deep RL, sensitivity maps reveal focus on state-value vs action-advantage components.</p> </li> <li> <p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p> </li> <li>Hidden state \\(h_t\\) stores past info.  </li> <li>The sequential Jacobian \\(\\frac{\\partial y_t}{\\partial x_{t-k}}\\) shows which past inputs are remembered.  </li> <li> <p>Implicitly attends to relevant time steps (memory through recurrence).</p> </li> <li> <p>In tasks like machine translation, implicit attention lets models reorder tokens:</p> <p>\u201cto reach\u201d \u2192 \u201czu erreichen\u201d</p> </li> </ul>"},{"location":"deeplearning/5_attention/#3-explicit-hard-attention","title":"3. Explicit (Hard) Attention","text":"<ul> <li>Explicit attention introduces a separate attention mechanism that decides where to look or what to read.   It restricts the data fed to the main network.</li> </ul>"},{"location":"deeplearning/5_attention/#why-explicit-attention","title":"Why explicit attention?","text":"<ul> <li>Efficiency: processes only selected parts of input.  </li> <li>Scalability: works on large or variable-size data.  </li> <li>Sequential processing: e.g. moving \u201cgaze\u201d across static images.  </li> <li>Interpretability: easier to visualize focus regions.</li> </ul>"},{"location":"deeplearning/5_attention/#model-structure","title":"Model structure","text":"<ul> <li>Network outputs attention parameters \\(a\\) that define a glimpse distribution \\(p(g|a)\\) over possible data regions.  </li> <li>A glimpse \\(g\\) (subset or window of data) is sampled and passed back as input.  </li> <li>System becomes recurrent, even if the base network is not.</li> </ul>"},{"location":"deeplearning/5_attention/#training-non-differentiable","title":"Training (non-differentiable)","text":"<ul> <li> <p>When glimpse selection is discrete or stochastic, use REINFORCE:      where \\(R\\) is the reward (e.g. task loss) and \\(b\\) a baseline for variance reduction.</p> </li> <li> <p>Thus, attention acts as a policy \\(\\pi_\\theta(g)\\) over glimpses.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#examples","title":"Examples","text":"<ul> <li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li> <li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li> </ul>"},{"location":"deeplearning/5_attention/#4-soft-attention","title":"4. Soft Attention","text":"<ul> <li>Hard attention samples discrete glimpses \u2192 non-differentiable \u2192 needs RL.  </li> <li>Soft attention computes a weighted average over all glimpses \u2192 differentiable \u2192 trainable by backprop.</li> </ul>"},{"location":"deeplearning/5_attention/#basic-idea","title":"Basic idea","text":"<ul> <li> <p>Attention parameters \\(a\\) define weights \\(w_i\\) over input features \\(v_i\\):      The readout \\(v\\) is a smooth combination of inputs.</p> </li> <li> <p>Replaces sampling by expectation \u2192 continuous, differentiable.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#benefits","title":"Benefits","text":"<ul> <li>Trained end-to-end with gradients.  </li> <li>Easier and more stable than hard attention.  </li> <li>Allows focus distribution rather than a single point.</li> </ul>"},{"location":"deeplearning/5_attention/#variants","title":"Variants","text":"<ul> <li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li> <li>Content-based attention: focuses by similarity of key \\(k\\) to data vectors \\(x_i\\) via score \\(S(k, x_i)\\), usually normalized by softmax:    </li> </ul>"},{"location":"deeplearning/5_attention/#applications","title":"Applications","text":"<ul> <li>Handwriting synthesis: RNN learns soft \u201cwindow\u201d over text sequence.  </li> <li>Neural Machine Translation: associative attention aligns words between languages.  </li> <li> <p>DRAW model: uses Gaussian filters to read/write parts of an image.</p> </li> <li> <p>Soft attention = data-dependent dynamic weighting (similar to convolution with adaptive filters).</p> </li> </ul>"},{"location":"deeplearning/5_attention/#5-introspective-attention-and-memory","title":"5. Introspective Attention and Memory","text":"<ul> <li>So far: attention over external data.  </li> <li>Now: attention over internal state or memory \u2192 \u201cintrospective attention.\u201d  </li> <li>Lets the network read or write selectively to memory locations.  </li> <li>Enables reasoning, recall, and algorithmic behavior.</li> </ul>"},{"location":"deeplearning/5_attention/#neural-turing-machine-ntm","title":"Neural Turing Machine (NTM)","text":"<ul> <li>Adds a differentiable memory matrix \\(M \\in \\mathbb{R}^{N \\times W}\\).  </li> <li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li> </ul> <p>Operations - Write: modify selected rows in \\(M\\) using attention weights \\(w_t\\). - Read: output weighted sum of memory slots:    - Addressing modes:   - Content-based: match key vector \\(k_t\\) to memory contents (via cosine similarity).   - Location-based: shift attention by relative position.</p> <p>Training: fully differentiable \u2014 end-to-end via backprop.</p> <p>Example task: copying sequences of variable length \u2014 learns algorithmic generalization.</p>"},{"location":"deeplearning/5_attention/#differentiable-neural-computer-dnc","title":"Differentiable Neural Computer (DNC)","text":"<ul> <li>Successor to NTM with richer memory access:</li> <li>Tracks temporal links between writes.  </li> <li>Supports dynamic memory allocation.  </li> <li>Improves stability and scalability.</li> </ul> <p>Application: synthetic QA tasks (bAbI dataset) \u2014 answers questions requiring multiple supporting facts and temporal reasoning.</p> <p>Key insight: Attention provides selective access to memory, acting like \u201caddressing\u201d in a differentiable data structure.</p>"},{"location":"deeplearning/5_attention/#6-transformers-and-self-attention","title":"6. Transformers and Self-Attention","text":"<ul> <li>Transformers: remove recurrence and convolution entirely \u2014 rely only on attention.</li> </ul>"},{"location":"deeplearning/5_attention/#self-attention","title":"Self-Attention","text":"<ul> <li>Each token attends to all others in the sequence:      where:</li> <li>\\(Q, K, V\\) are query, key, and value matrices (learned linear projections of input embeddings).</li> <li>Produces context-aware representations for all tokens in parallel.</li> </ul>"},{"location":"deeplearning/5_attention/#multi-head-attention","title":"Multi-Head Attention","text":"<ul> <li>Multiple attention \u201cheads\u201d (\\(H\\)) learn different relationships:      Each head captures a distinct pattern (syntax, semantics, position, etc.).</li> </ul>"},{"location":"deeplearning/5_attention/#transformer-block","title":"Transformer Block","text":"<ul> <li>Structure:</li> <li>Multi-head self-attention  </li> <li>Add &amp; LayerNorm  </li> <li>Feedforward (ReLU + linear)  </li> <li>Add &amp; LayerNorm  </li> <li>Skip connections improve gradient flow and allow top-down signal mixing.</li> </ul>"},{"location":"deeplearning/5_attention/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Since model is permutation-invariant, inject position information:       Added to input embeddings.</li> </ul>"},{"location":"deeplearning/5_attention/#intuition","title":"Intuition","text":"<ul> <li>Self-attention generalizes RNN memory:</li> <li>Recurrent \u2192 sequential access  </li> <li>Transformer \u2192 direct pairwise access between all tokens.</li> <li>Enables long-range dependencies and parallelization.</li> </ul>"},{"location":"deeplearning/5_attention/#key-result","title":"Key result","text":"<ul> <li>Attention-only models achieve SOTA in translation and NLP tasks.  </li> <li>Forms basis for BERT, GPT, and modern large language models.</li> </ul>"},{"location":"deeplearning/5_attention/#7-adaptive-computation-time-act-and-summary","title":"7. Adaptive Computation Time (ACT) and Summary","text":""},{"location":"deeplearning/5_attention/#adaptive-computation-time-act","title":"Adaptive Computation Time (ACT)","text":"<ul> <li>Proposed by Graves (2016): allows networks to \u201cponder\u201d variable amounts of time per input.  </li> <li>Each step computes a halting probability \\(p_t\\); total halt when \\(\\sum_t p_t = 1\\).</li> <li>Output is a weighted sum of intermediate states:    </li> <li>Encourages efficient use of computation \u2014 more steps for harder inputs, fewer for easy ones.</li> <li>Regularized by a time penalty to avoid overthinking.</li> </ul>"},{"location":"deeplearning/5_attention/#universal-transformers","title":"Universal Transformers","text":"<ul> <li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li> <li>Shares parameters across layers \u2014 like an RNN unrolled over depth.</li> <li>Combine parallel self-attention + iterative refinement + ACT.</li> <li>Achieves better generalization and adaptive reasoning on sequence tasks.</li> </ul>"},{"location":"deeplearning/5_attention/#summary","title":"Summary","text":"<ul> <li>Attention = selective processing of relevant information.  </li> <li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li> <li>Explicit attention can be hard (sampled) or soft (differentiable).  </li> <li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li> <li>Transformers unify attention as the core mechanism \u2014 fully parallel, context-rich.  </li> <li>Adaptive computation gives flexibility in processing time and complexity.</li> </ul> <p>Takeaway: Selective attention and memory \u2014 biological inspirations \u2014 are now core architectural principles driving modern deep learning.</p>"},{"location":"deeplearning/6_gans/","title":"6 gans","text":""},{"location":"deeplearning/6_gans/#1-overview-generative-models","title":"1. Overview: Generative Models","text":"<ul> <li>Goal: learn a model of the true data distribution \\(p^*(x)\\) from samples.</li> </ul>"},{"location":"deeplearning/6_gans/#types-of-generative-models","title":"Types of Generative Models","text":"<ol> <li>Explicit likelihood models \u2013 define tractable \\(p_\\theta(x)\\)</li> <li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li> <li> <p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p> </li> <li> <p>Implicit models \u2013 define sampling procedure, not explicit \\(p_\\theta(x)\\) </p> </li> <li>Examples: GANs, Moment Matching Networks.</li> </ol>"},{"location":"deeplearning/6_gans/#11-the-gan-idea","title":"1.1 The GAN Idea","text":"<ul> <li>Two-player minimax game:</li> <li>Generator (G): maps noise \\(z \\sim p(z)\\) to data space \\(G(z)\\).</li> <li> <p>Discriminator (D): classifies samples as real (from \\(p^*(x)\\)) or fake (\\(G(z)\\)).</p> </li> <li> <p>Objectives:    </p> </li> <li> <p>Interpretation:</p> </li> <li>\\(D\\) learns to distinguish real from fake.</li> <li>\\(G\\) learns to fool \\(D\\).</li> <li>Training reaches equilibrium when \\(p_G(x) = p^*(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#12-alternative-view-teacherstudent-analogy","title":"1.2 Alternative View \u2014 Teacher\u2013Student Analogy","text":"<ul> <li>Teacher (D): distinguishes real vs fake, providing feedback.</li> <li>Student (G): improves by making fake data look real.</li> <li>Cooperative interpretation of the adversarial process.</li> </ul>"},{"location":"deeplearning/6_gans/#13-gans-as-a-game","title":"1.3 GANs as a Game","text":"<ul> <li>Zero-sum, bi-level optimization \u2192 strong connection to game theory.</li> <li>GAN equilibrium = Nash equilibrium between \\(G\\) and \\(D\\).</li> <li>Training alternates between optimizing \\(D\\) and \\(G\\).</li> </ul> <p>Key Intuition: GANs learn by competition between a generator and discriminator rather than direct likelihood maximization.</p>"},{"location":"deeplearning/6_gans/#2-gan-objective-as-divergence-minimization","title":"2. GAN Objective as Divergence Minimization","text":"<ul> <li>Generative modeling often aims to minimize a distance or divergence between   the true data distribution \\(p^*(x)\\) and model distribution \\(p_G(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#21-kl-and-related-divergences","title":"2.1 KL and Related Divergences","text":"<ul> <li> <p>Maximum Likelihood Estimation (MLE):      \u2192 drives \\(p_\\theta\\) to assign high probability to observed data.</p> </li> <li> <p>But: implicit models (like GANs) don\u2019t have explicit likelihoods, so MLE can\u2019t be used directly.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#22-gan-as-jensenshannon-js-divergence-minimization","title":"2.2 GAN as Jensen\u2013Shannon (JS) Divergence Minimization","text":"<ul> <li> <p>If discriminator \\(D\\) is optimal:      Plugging into the GAN loss shows that the generator minimizes:      \u2192 GAN \u2248 JS divergence minimization.</p> </li> <li> <p>However, this relies on an optimal discriminator \u2014 not true in practice.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#23-limitations-of-kl-js-divergences","title":"2.3 Limitations of KL / JS Divergences","text":"<ul> <li>If \\(p_G\\) and \\(p^*\\) have non-overlapping support,   \u2192 no useful gradient signal (zero gradient problem).</li> <li>The density ratio \\(\\frac{p^*(x)}{p_G(x)}\\) becomes infinite where \\(p_G=0\\).</li> <li>Thus, GANs can fail to learn when supports are disjoint.</li> </ul>"},{"location":"deeplearning/6_gans/#24-alternative-distances-divergences","title":"2.4 Alternative Distances &amp; Divergences","text":""},{"location":"deeplearning/6_gans/#a-wasserstein-distance-earth-movers","title":"(a) Wasserstein Distance (Earth Mover\u2019s)","text":"<ul> <li>Measures minimal \u201ccost\u201d of moving probability mass:    </li> <li>Provides smooth, non-vanishing gradients even when supports don\u2019t overlap.</li> <li>WGAN: enforce 1-Lipschitz \\(D\\) via:</li> <li>weight clipping,</li> <li>gradient penalty (WGAN-GP),</li> <li>spectral normalization.</li> </ul>"},{"location":"deeplearning/6_gans/#b-mmd-maximum-mean-discrepancy","title":"(b) MMD (Maximum Mean Discrepancy)","text":"<ul> <li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):    </li> <li>MMD-GAN: learns kernel features \\(\\phi\\) jointly with \\(D\\).</li> </ul>"},{"location":"deeplearning/6_gans/#c-f-divergences","title":"(c) f-divergences","text":"<ul> <li>General framework using convex functions \\(f\\):    </li> <li>GAN training derived via variational lower bound on \\(D_f\\).</li> </ul>"},{"location":"deeplearning/6_gans/#25-practical-view","title":"2.5 Practical View","text":"<ul> <li>GANs are not pure divergence minimizers in practice:</li> <li>\\(D\\) not optimal \u2192 approximate divergence.</li> <li>Neural discriminator learns a smooth approximation to density ratio.</li> <li>Provides useful gradients even when the true divergence would fail.</li> </ul>"},{"location":"deeplearning/6_gans/#26-summary-table","title":"2.6 Summary Table","text":"Perspective Example Key Idea KL Divergence MLE, VAEs Explicit likelihoods JS Divergence Original GAN Adversarial training Wasserstein WGAN Smooth gradients MMD MMD-GAN Kernel mean embedding f-divergence f-GAN Variational bound family <p>Insight: GANs can be viewed as learning a neural divergence measure that provides a stable, informative training signal.</p>"},{"location":"deeplearning/6_gans/#3-evaluating-gans","title":"3. Evaluating GANs","text":"<ul> <li>Evaluating generative models is difficult \u2014 no single metric captures all aspects.</li> <li>Must assess:</li> <li>Sample quality (fidelity, realism)</li> <li>Diversity / generalization</li> <li>Representation learning (usefulness of learned features)</li> </ul>"},{"location":"deeplearning/6_gans/#31-why-not-log-likelihood","title":"3.1 Why Not Log-Likelihood?","text":"<ul> <li>GANs are implicit models \u2014 no tractable \\(p(x)\\).</li> <li>Estimating log-likelihood is expensive and unreliable.</li> <li>Hence: use feature-based or classifier-based proxies.</li> </ul>"},{"location":"deeplearning/6_gans/#32-inception-score-is","title":"3.2 Inception Score (IS)","text":"<ul> <li>Uses a pretrained Inception v3 classifier.</li> <li>Compares predicted label distributions of generated samples.</li> </ul> <p>Formula:  </p> <p>Intuition: - High-quality images \u2192 confident predictions (\\(p(y|x)\\) low entropy). - Diverse images \u2192 marginal label distribution \\(p(y)\\) high entropy.</p> <p>Properties: - Measures sample quality and diversity. - Correlates with human judgment. - Fails to capture intra-class variation or features beyond ImageNet classes.</p> <p>Higher is better.</p>"},{"location":"deeplearning/6_gans/#33-frechet-inception-distance-fid","title":"3.3 Fr\u00e9chet Inception Distance (FID)","text":"<ul> <li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li> </ul> <p>Formula:  </p> <p>where \\((\\mu_r, \\Sigma_r)\\) and \\((\\mu_g, \\Sigma_g)\\) are mean and covariance of real and generated data features.</p> <p>Properties: - Sensitive to mode dropping and artifacts. - Correlates strongly with human evaluation. - Lower is better. - Biased for small sample sizes \u2192 use KID (Kernel Inception Distance) for correction.</p>"},{"location":"deeplearning/6_gans/#34-overfitting-check-nearest-neighbours","title":"3.4 Overfitting Check \u2014 Nearest Neighbours","text":"<ul> <li>Compute nearest real images to generated samples in pretrained feature space.</li> <li>Helps detect memorization (copying training images).</li> </ul>"},{"location":"deeplearning/6_gans/#35-evaluation-depends-on-goal","title":"3.5 Evaluation Depends on Goal","text":"Goal Metric Example Measures Image quality FID, IS Fidelity &amp; diversity Representation learning Linear probe accuracy Feature usefulness Data generation Human evaluation Perceptual quality RL / control Policy reward Functional realism <p>Key Takeaway: Use multiple complementary metrics \u2014 quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>"},{"location":"deeplearning/6_gans/#4-the-gan-zoo","title":"4. The GAN Zoo","text":"<p>GANs have evolved rapidly \u2014 from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>"},{"location":"deeplearning/6_gans/#41-the-original-gan","title":"4.1 The Original GAN","text":"<ul> <li>First formulation of adversarial training.</li> <li>Architecture: simple multilayer perceptrons (MLPs).</li> <li>Trained on small images (e.g. 32\u00d732).  </li> <li>Ignored spatial structure (flattened pixels).  </li> <li>Introduced the minimax objective still used today.</li> </ul>"},{"location":"deeplearning/6_gans/#42-conditional-gan","title":"4.2 Conditional GAN","text":"<ul> <li> <p>Adds conditioning information \\(y\\) (e.g. class label or input image). </p> </li> <li> <p>Enables controlled generation \u2014 specify category or domain.   Examples:</p> </li> <li>Class-conditional image synthesis (e.g., \"generate a dog\").  </li> <li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li> </ul>"},{"location":"deeplearning/6_gans/#43-laplacian-gan","title":"4.3 Laplacian GAN","text":"<ul> <li>Generates images progressively, starting from low resolution.  </li> <li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li> <li>Fully convolutional \u2014 can produce arbitrarily large outputs.</li> <li>Improves high-res synthesis through multi-scale structure.</li> </ul>"},{"location":"deeplearning/6_gans/#44-deep-convolutional-gan","title":"4.4 Deep Convolutional GAN","text":"<ul> <li>Replaces MLPs with deep convnets for both \\(G\\) and \\(D\\).</li> <li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li> <li>Enables smooth interpolation in latent space:</li> <li>\\(G(z_1)\\) \u2192 \\(G(\\frac{1}{2}(z_1 + z_2))\\) \u2192 \\(G(z_2)\\) produces semantically meaningful transitions.</li> <li>Latent space exhibits semantic arithmetic (e.g. \u201cman + glasses \u2013 woman\u201d).</li> </ul>"},{"location":"deeplearning/6_gans/#45-spectrally-normalized-gan","title":"4.5 Spectrally Normalized GAN","text":"<ul> <li> <p>Enforces 1-Lipschitz constraint on \\(D\\) via spectral normalization:      where \\(\\sigma_{\\max}(W)\\) is the largest singular value.</p> </li> <li> <p>Stabilizes training and improves generalization.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#46-projection-discriminator","title":"4.6 Projection Discriminator","text":"<ul> <li> <p>Adds class embedding projection inside \\(D\\):    where \\(v_y\\) is the embedding for class \\(y\\).</p> </li> <li> <p>Theoretically consistent probabilistic discriminator formulation.  </p> </li> <li>Strong empirical results on class-conditional image synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#47-self-attention-gan","title":"4.7 Self-Attention GAN","text":"<ul> <li>Introduces self-attention layers to capture long-range dependencies.  </li> <li>Improves global structure and coherence in generated images.</li> <li>Inspired by Transformer attention.</li> </ul>"},{"location":"deeplearning/6_gans/#48-biggan","title":"4.8 BigGAN","text":"<ul> <li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li> <li>Key ingredients:</li> <li>Hinge loss for \\(D\\) </li> <li>Spectral normalization  </li> <li>Self-attention  </li> <li>Projection discriminator  </li> <li>Orthogonal regularization  </li> <li>Skip connections from noise  </li> <li>Shared class embeddings  </li> <li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li> </ul>"},{"location":"deeplearning/6_gans/#49-logan","title":"4.9 LOGAN","text":"<ul> <li>Introduces latent optimization \u2014 optimize \\(z\\) via gradient updates to improve adversarial dynamics.  </li> <li>Uses natural gradient descent in latent space.  </li> <li>Yields higher FID/IS improvements over BigGAN.</li> </ul>"},{"location":"deeplearning/6_gans/#410-progressive-gan","title":"4.10 Progressive GAN","text":"<ul> <li>Trains from low to high resolution (4\u00d74 \u2192 8\u00d78 \u2192 16\u00d716 \u2026).  </li> <li>Each stage adds new layers to \\(G\\) and \\(D\\).  </li> <li>Dramatically improves stability and image quality (especially faces).</li> </ul>"},{"location":"deeplearning/6_gans/#411-stylegan","title":"4.11 StyleGAN","text":"<ul> <li>Adds style-based generator architecture:</li> <li>Latent vector \\(z\\) transformed by MLP to intermediate \\(w\\).</li> <li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li> <li> <p>Injects per-pixel noise for local details.</p> </li> <li> <p>Learns disentangled representations \u2014 global attributes (style) vs local (texture).</p> </li> </ul>"},{"location":"deeplearning/6_gans/#412-takeaways","title":"4.12 Takeaways","text":"<ul> <li>GAN progress driven by:</li> <li>Better architectures (Conv, Attention, Progressive, Style-based)</li> <li>Normalization &amp; regularization</li> <li>Stability techniques</li> <li>Large-scale training</li> </ul> <p>Trend: From small MLPs \u2192 Conv architectures \u2192 Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>"},{"location":"deeplearning/6_gans/#5-representation-learning-with-gans","title":"5. Representation Learning with GANs","text":"<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>"},{"location":"deeplearning/6_gans/#51-motivation","title":"5.1 Motivation","text":"<ul> <li>GANs implicitly learn latent spaces that capture high-level semantics.</li> <li>Exploring or constraining this latent space enables unsupervised representation learning.</li> </ul>"},{"location":"deeplearning/6_gans/#52-evidence-from-dcgan","title":"5.2 Evidence from DCGAN","text":"<ul> <li>DCGAN latent vectors encode meaningful directions:</li> <li>Smooth interpolation between points \u2192 semantic transformations.</li> <li>Linear arithmetic in latent space (e.g., smiling woman \u2013 woman + man \u2192 smiling man).</li> <li>Suggests disentangled feature representations emerge naturally.</li> </ul>"},{"location":"deeplearning/6_gans/#53-infogan","title":"5.3 InfoGAN","text":"<ul> <li>Extends GAN with information maximization objective:</li> <li>Encourages some latent codes \\(c\\) to be interpretable and disentangled.</li> </ul> <p>Objective:  where \\(I(c; G(z, c))\\) is mutual information between latent code and generated output.</p> <ul> <li>Adds an auxiliary network to infer \\(c\\) from \\(G(z, c)\\).</li> <li>Learns to associate:</li> <li>Discrete codes \u2192 categories (digits, shapes)</li> <li>Continuous codes \u2192 attributes (rotation, scale)</li> </ul>"},{"location":"deeplearning/6_gans/#54-ali-bigan","title":"5.4 ALI / BiGAN","text":"<ul> <li>Adds an encoder \\(E(x)\\) mapping real data to latent space.</li> <li> <p>Joint discriminator distinguishes pairs:    </p> </li> <li> <p>At equilibrium:</p> </li> <li> <p>\\(E\\) and \\(G\\) become approximate inverses:</p> <ul> <li>\\(x \\approx G(E(x))\\)</li> <li>\\(z \\approx E(G(z))\\)</li> </ul> </li> <li> <p>Enables inference and representation learning simultaneously.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#55-bigbigan","title":"5.5 BigBiGAN","text":"<ul> <li>Scales BiGAN to BigGAN architecture.</li> <li>Uses large-scale encoders (\\(E\\)) with ResNet blocks.</li> <li>Learns strong unsupervised representations competitive with self-supervised models.</li> </ul> <p>Observations: - Reconstructions \\(G(E(x))\\) preserve semantic content, not exact pixels. - Encoder features yield high ImageNet classification accuracy after linear probing.</p>"},{"location":"deeplearning/6_gans/#56-summary","title":"5.6 Summary","text":"Model Key Idea Outcome DCGAN Implicitly semantic latent space Interpolations meaningful InfoGAN Maximize info between codes and outputs Disentangled features BiGAN / ALI Add encoder, joint training Bidirectional mapping BigBiGAN Large-scale BiGAN Competitive unsupervised features <p>Key Insight: GANs not only generate, but also encode \u2014 their latent structure can act as a rich, learned representation space.</p>"},{"location":"deeplearning/6_gans/#6-gans-for-other-modalities-and-problems","title":"6. GANs for Other Modalities and Problems","text":"<p>GANs extend far beyond images \u2014 used for translation, audio, video, RL, and even art.</p>"},{"location":"deeplearning/6_gans/#61-image-to-image-translation","title":"6.1 Image-to-Image Translation","text":""},{"location":"deeplearning/6_gans/#a-pix2pix","title":"(a) Pix2Pix","text":"<ul> <li>Conditional GAN trained on paired datasets \\((x, y)\\).</li> <li>Learns deterministic mapping between domains (e.g., edges \u2192 photos).</li> <li>Loss combines adversarial term + L1 reconstruction:    </li> </ul>"},{"location":"deeplearning/6_gans/#b-cyclegan","title":"(b) CycleGAN","text":"<ul> <li>Unpaired domain translation \u2014 no 1:1 correspondence.</li> <li>Uses cycle consistency:</li> <li>\\(x \\in A \\to G_B(x) \\to F_A(G_B(x)) \\approx x\\)</li> <li>Enforces invertibility between domains.</li> <li>Enables tasks like horse \u2194 zebra, summer \u2194 winter.</li> </ul>"},{"location":"deeplearning/6_gans/#62-audio-synthesis","title":"6.2 Audio Synthesis","text":""},{"location":"deeplearning/6_gans/#a-wavegan","title":"(a) WaveGAN","text":"<ul> <li>Adapts convolutional GANs to 1D waveforms.</li> <li>Fully unsupervised raw-audio synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#b-melgan","title":"(b) MelGAN","text":"<ul> <li>Conditional GAN trained to generate mel-spectrogram waveforms.</li> <li>Used in text-to-speech (GAN-TTS).</li> </ul>"},{"location":"deeplearning/6_gans/#c-gan-tts","title":"(c) GAN-TTS","text":"<ul> <li>High-fidelity speech synthesis model.</li> <li>Achieves human-like audio quality via adversarial losses.</li> </ul>"},{"location":"deeplearning/6_gans/#63-video-synthesis-prediction","title":"6.3 Video Synthesis &amp; Prediction","text":"<ul> <li>GANs extended to spatiotemporal data:</li> <li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li> <li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li> <li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li> </ul>"},{"location":"deeplearning/6_gans/#64-gans-in-reinforcement-learning-imitation-control","title":"6.4 GANs in Reinforcement Learning (Imitation &amp; Control)","text":"<ul> <li>GAIL (Ho &amp; Ermon, 2016): Generative Adversarial Imitation Learning </li> <li>Discriminator distinguishes expert vs policy trajectories.</li> <li>Generator = policy network optimizing to mimic experts.</li> </ul>"},{"location":"deeplearning/6_gans/#65-creative-applied-uses","title":"6.5 Creative &amp; Applied Uses","text":"<ul> <li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li> <li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li> <li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li> <li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li> <li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li> </ul>"},{"location":"deeplearning/6_gans/#66-summary","title":"6.6 Summary","text":"Domain Example Key Idea Paired image translation Pix2Pix Conditional GAN + L1 loss Unpaired translation CycleGAN Cycle consistency Audio MelGAN, WaveGAN Conditional waveform generation Video DVD-GAN, TGAN-v2 Temporal adversarial modeling RL / Imitation GAIL Adversarial trajectory matching Art / Creativity GauGAN, SPIRAL Adversarial synthesis and style transfer <p>Insight: Adversarial learning generalizes across domains \u2014 GANs serve as a universal generator\u2013critic framework for structured data.</p>"},{"location":"deeplearning/7_unsuper/","title":"7 unsuper","text":""},{"location":"deeplearning/7_unsuper/#1-what-is-unsupervised-learning","title":"1. What is Unsupervised Learning?","text":""},{"location":"deeplearning/7_unsuper/#definition","title":"Definition","text":"<ul> <li>Goal: discover structure in data without explicit labels or rewards.  </li> <li>Learns a compact, informative representation of input data.</li> </ul> Learning Type Goal Supervision Supervised Map inputs \u2192 labels Requires labeled data Reinforcement Learn actions maximizing future reward Requires reward signal Unsupervised Find hidden structure No labels or rewards"},{"location":"deeplearning/7_unsuper/#core-ideas","title":"Core Ideas","text":"<ul> <li>Model latent structure or relationships between observations.  </li> <li>Examples:</li> <li>Clustering: group similar data points.  </li> <li>Dimensionality reduction: project data to low-dimensional latent space.  </li> <li>Manifold learning / disentangling: uncover independent factors of variation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#evaluation-challenges","title":"Evaluation Challenges","text":"<p>How do we know if unsupervised learning worked?</p> <ul> <li>Ambiguity of structure: multiple valid clusterings possible.   e.g., cluster by leg count, arm number, or height in robot dataset.</li> <li>Metrics depend on downstream use:   useful representations should improve data efficiency, generalization, or transfer.</li> </ul>"},{"location":"deeplearning/7_unsuper/#classic-methods","title":"Classic Methods","text":"<ul> <li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li> <li>ICA (Independent Component Analysis): separates statistically independent components.  </li> <li>Modern goal: move beyond orthogonality \u2192 learn disentangled factors.</li> </ul>"},{"location":"deeplearning/7_unsuper/#summary","title":"Summary","text":"<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself \u2014 forming the foundation for representation learning.</p>"},{"location":"deeplearning/7_unsuper/#2-why-is-unsupervised-learning-important","title":"2. Why is Unsupervised Learning Important?","text":""},{"location":"deeplearning/7_unsuper/#21-historical-context-of-representation-learning","title":"2.1 Historical Context of Representation Learning","text":"Era Key Milestone Approach 1950s\u20132000s Arthur Samuel (1959): Machine Learning coined Feature engineering, clustering 2000s Kernel methods (Hofmann et al., 2008) Hand-crafted similarity functions 2006 Hinton &amp; Salakhutdinov: RBMs &amp; Autoencoders Layer-wise unsupervised pretraining 2012 Krizhevsky et al.: AlexNet End-to-end supervised learning dominates <ul> <li>Progress came from more data, deeper models, and better hardware \u2014 but not necessarily more efficient learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#22-limitations-of-purely-supervised-learning","title":"2.2 Limitations of Purely Supervised Learning","text":"<p>Supervised models are: - Data inefficient \u2014 need millions of labeled samples. - Brittle \u2014 vulnerable to adversarial perturbations. - Poor at transfer \u2014 struggle with new domains or tasks. - Lack common sense \u2014 limited abstraction and reasoning.</p>"},{"location":"deeplearning/7_unsuper/#23-evidence-of-current-gaps","title":"2.3 Evidence of Current Gaps","text":"Challenge Example Reference Data efficiency Learning from few examples Lake et al. (2017) Robustness Adversarial examples, brittle decisions Goodfellow et al. (2015) Generalization CoinRun, DMLab-30 Cobbe (2018), DeepMind Transfer Schema Networks Kansky et al. (2017) Common sense Conceptual reasoning Lake et al. (2015)"},{"location":"deeplearning/7_unsuper/#24-why-unsupervised-learning-matters","title":"2.4 Why Unsupervised Learning Matters","text":"<ul> <li>Enables data-efficient adaptation to new tasks.</li> <li>Provides robust, generalizable features.</li> <li>Promotes transfer learning by separating invariant factors.</li> <li>Encourages abstract reasoning and causal understanding.</li> </ul>"},{"location":"deeplearning/7_unsuper/#25-towards-general-ai","title":"2.5 Towards General AI","text":"<p>Unsupervised learning provides shared representations enabling: - Rapid multi-task adaptation. - Reuse across vision, language, and control. - Reduced supervision in real-world learning.</p> <p>Summary: Unsupervised representation learning addresses the core limits of current AI \u2014 aiming for data efficiency, robustness, generalization, transfer, and common sense.</p>"},{"location":"deeplearning/7_unsuper/#3-what-makes-a-good-representation","title":"3. What Makes a Good Representation?","text":"<p>A representation is an internal model of the world \u2014 an abstraction that makes reasoning and prediction efficient.</p>"},{"location":"deeplearning/7_unsuper/#31-what-is-a-representation","title":"3.1 What is a Representation?","text":"<p>\u201cA formal system for making explicit certain entities or types of information, together with a specification of how the system does this.\u201d</p> <ul> <li>Represents information about the world in a way useful for computation.  </li> <li>Not about a single feature, but the geometry or manifold shape in representational space.</li> </ul>"},{"location":"deeplearning/7_unsuper/#32-why-representation-form-matters","title":"3.2 Why Representation Form Matters","text":"<ul> <li>Determines which computations are easy.  </li> <li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li> </ul>"},{"location":"deeplearning/7_unsuper/#33-desirable-properties","title":"3.3 Desirable Properties","text":"Property Description Intuition Untangling Simplifies complex input manifolds Enables linear decoding Attention Allows selective focus on relevant factors Supports task-specific filtering Clustering Groups similar experiences together Facilitates generalization Latent Information Encodes hidden or inferred causes Predicts unobserved aspects Compositionality Builds complex concepts from simple parts Enables open-ended reasoning"},{"location":"deeplearning/7_unsuper/#34-information-bottleneck-principle","title":"3.4 Information Bottleneck Principle","text":"<ul> <li>Good representations compress inputs while preserving information about outputs.    </li> <li>Encourages minimal sufficient representations \u2014 compact yet predictive.</li> </ul>"},{"location":"deeplearning/7_unsuper/#4-evaluating-the-merit-of-a-representation","title":"4. Evaluating the Merit of a Representation","text":"<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>"},{"location":"deeplearning/7_unsuper/#41-the-evaluation-challenge","title":"4.1 The Evaluation Challenge","text":"<ul> <li>No single metric defines a \u201cgood\u201d representation.</li> <li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li> </ul> <p>Representations should enable: - Data efficiency \u2014 learn new tasks from few examples. - Robustness \u2014 resist noise or perturbations. - Generalization \u2014 perform well on new data. - Transfer \u2014 reuse knowledge in new settings. - Common sense \u2014 support reasoning and abstraction.</p>"},{"location":"deeplearning/7_unsuper/#42-example-evaluating-representations-via-symmetries","title":"4.2 Example: Evaluating Representations via Symmetries","text":"<p>Let: - \\(W\\) = world space - \\(Z\\) = representational space - \\(G = G_x \\times G_y \\times G_c\\) = group of transformations (e.g., position, color)</p> <p>A good representation \\(f: W \\rightarrow Z\\) should satisfy:  </p> <p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space \u2192 equivariance.</p>"},{"location":"deeplearning/7_unsuper/#43-desirable-evaluation-criteria","title":"4.3 Desirable Evaluation Criteria","text":"Criterion Desired Property Example / Metric Equivariance Transformations map consistently Translation \u2192 shift in latent Compositionality Combine factors to form new concepts Modular latent factors Metric structure Smooth distances reflect similarity \\(L_2\\), cosine Attention Selectively focus on task-relevant parts Masking or gating mechanisms Symmetries Invariance to irrelevant transformations Rotation, scale invariance"},{"location":"deeplearning/7_unsuper/#44-downstream-evaluation-tasks","title":"4.4 Downstream Evaluation Tasks","text":"Evaluation Setting Example Task Reference Perception / Control Predict object color or position Gens &amp; Domingos, Deep Symmetry Networks (2014) Robustness Classify images under adversarial noise Gowal et al., 2019 Sequential Attention Learn task-focused vision Zoran et al., 2020 Transfer / RL Zero-shot navigation (DARLA) Higgins et al., ICML 2017 Lifelong Learning Maintain latent structure over domains Achille et al., NeurIPS 2018 Reasoning / Imagination Compositional concept inference Lake et al., Science 2015; Higgins et al., ICLR 2018"},{"location":"deeplearning/7_unsuper/#45-why-evaluation-matters","title":"4.5 Why Evaluation Matters","text":"<p>A good representation supports simple mappings to downstream tasks: - Linear classifiers for vision tasks (e.g., color or position recognition). - Efficient policy learning in RL with fewer samples. - Abstract reasoning and imagination \u2014 \u201cIf rainbow elephants live in big cities, can we expect one in London?\u201d</p>"},{"location":"deeplearning/7_unsuper/#5-representation-learning-techniques","title":"5. Representation Learning Techniques","text":"<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches \u2014 all aiming to extract structure from data without labels.</p>"},{"location":"deeplearning/7_unsuper/#51-categories-of-methods","title":"5.1 Categories of Methods","text":"Category Core Idea Typical Example Generative Modeling Learn \\(p(x)\\) or a model that can reconstruct data VAE, \u03b2-VAE, MONet, GQN, GANs Contrastive Learning Learn by discriminating similar vs dissimilar samples CPC, SimCLR, word2vec Self-Supervised Learning Design pretext tasks that predict missing or reordered parts BERT, Colorization, Context Prediction"},{"location":"deeplearning/7_unsuper/#52-generative-modeling","title":"5.2 Generative Modeling","text":""},{"location":"deeplearning/7_unsuper/#521-motivation","title":"5.2.1 Motivation","text":"<ul> <li>Goal: learn the underlying data distribution \\(p(x)\\) to reveal hidden structure and causal factors.  </li> <li>Unsupervised generative modeling captures common regularities in data \u2014 enabling representation learning, synthesis, and reasoning.  </li> <li>Instead of directly memorizing examples, the model learns a probabilistic process that could have generated them.</li> </ul> <p>Generative models explain the data by learning how it might have arisen.</p>"},{"location":"deeplearning/7_unsuper/#522-from-maximum-likelihood-to-latent-variable-models","title":"5.2.2 From Maximum Likelihood to Latent Variable Models","text":""},{"location":"deeplearning/7_unsuper/#maximum-likelihood-principle","title":"Maximum Likelihood Principle","text":"<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:  where \\(p^*(x)\\) is the true data distribution and \\(p_\\theta(x)\\) is the model.</p>"},{"location":"deeplearning/7_unsuper/#latent-variable-formulation","title":"Latent Variable Formulation","text":"<ul> <li>Assume data arises from hidden (latent) variables \\(z\\):    </li> <li>Here:</li> <li>\\(p(z)\\) \u2014 prior over latent variables (e.g., \\(\\mathcal{N}(0, I)\\))  </li> <li>\\(p_\\theta(x|z)\\) \u2014 likelihood or decoder mapping latent codes to data</li> </ul> <p>This defines a latent variable model: the data-generating process maps from a low-dimensional latent space to the observed space.</p>"},{"location":"deeplearning/7_unsuper/#523-inference-in-latent-variable-models","title":"5.2.3 Inference in Latent Variable Models","text":"<p>Goal: infer the posterior  to identify which latent factors \\(z\\) most likely generated observation \\(x\\).</p> <ul> <li>Intuition:   Recover the underlying causes that explain the data \u2014 along with uncertainty estimates.</li> <li>Problem:   Computing \\(p(z|x)\\) is often intractable, since \\(p_\\theta(x)\\) involves integrating over all \\(z\\).   \u2192 We must approximate inference using neural networks.</li> </ul> <p>Thus, generative models combine:</p> <ul> <li>Generation: \\(z \\rightarrow x\\) (decode latent causes into data)</li> <li>Inference: \\(x \\rightarrow z\\) (encode data into latent causes)</li> </ul>"},{"location":"deeplearning/7_unsuper/#524-variational-autoencoders-vaes","title":"5.2.4 Variational Autoencoders (VAEs)","text":"<p>To make inference tractable, VAEs introduce an approximate posterior \\(q_\\phi(z|x)\\) and optimize a variational bound on the likelihood:</p>"},{"location":"deeplearning/7_unsuper/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"\\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}[q_\\phi(z|x)\\,||\\,p(z)] \\]"},{"location":"deeplearning/7_unsuper/#terms","title":"Terms","text":"<ol> <li> <p>Reconstruction term    Encourages the model to faithfully reproduce the input from its latent code.</p> </li> <li> <p>KL divergence term    Regularizes the latent posterior to match the prior \u2014 ensuring smoothness and preventing overfitting.</p> </li> </ol>"},{"location":"deeplearning/7_unsuper/#neural-implementation","title":"Neural Implementation","text":"<ul> <li>Encoder \\(q_\\phi(z|x)\\): approximates inference (maps data \u2192 latent code).  </li> <li>Decoder \\(p_\\theta(x|z)\\): generates data from the latent space (latent \u2192 data).  </li> <li>Both are parameterized by deep neural networks.</li> </ul> <p>Reparameterization trick (Kingma &amp; Welling, 2014):  enables backpropagation through stochastic latent sampling.</p>"},{"location":"deeplearning/7_unsuper/#why-vaes-matter","title":"Why VAEs Matter","text":"<ul> <li>Provide continuous, structured latent spaces capturing generative factors.  </li> <li>Support smooth interpolation and semantic manipulation.  </li> <li>Foundation for disentangled and interpretable representation learning (e.g., \u03b2-VAE).  </li> <li>Bridge probabilistic modeling with deep learning.</li> </ul> <p>VAEs turn probabilistic inference into a scalable neural optimization problem \u2014 the cornerstone of modern generative representation learning.</p>"},{"location":"deeplearning/7_unsuper/#524-vae","title":"5.2.4 \u03b2-VAE","text":"<ul> <li>Adds weight \u03b2 to KL term:    </li> <li>Encourages disentangled latent factors (position, shape, rotation, color).</li> <li> <p>Provides interpretable, semantically meaningful representations.</p> </li> <li> <p>DARLA (Higgins et al., 2017): \u03b2-VAE for reinforcement learning \u2192 improved transfer and sim2real generalization.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#525-sequential-and-layered-models","title":"5.2.5 Sequential and Layered Models","text":"<p>ConvDRAW (Gregor et al., 2016) - Sequential VAE with recurrent refinement. - Models temporal and spatial dependencies.</p> <p>MONet (Burgess et al., 2019) - Attention-based scene decomposition. - Each latent corresponds to one object \u2192 compositional representations. - Enables object-centric reasoning and RL transfer.</p> <p>GQN (Eslami et al., 2018) - Generative Query Networks: learn neural scene representations. - Given partial observations, predict unseen viewpoints (3D reasoning).</p> <p>VQ-VAE (van den Oord et al., 2017) - Learns discrete latent variables via vector quantization. - Enables hierarchical or symbolic structure. - Useful for speech, images, and video. -</p>"},{"location":"deeplearning/7_unsuper/#526-gans-goodfellow-et-al-2014","title":"5.2.6 GANs (Goodfellow et al., 2014)","text":"<ul> <li>Implicit generative models \u2014 learn by adversarial game:</li> <li>Generator creates samples.</li> <li>Discriminator provides learning signal (no reconstruction loss).</li> <li>BigBiGAN (Donahue et al., 2019):</li> <li>Adds encoder for inference.</li> <li>Learns rich, high-level representations \u2192 SOTA semi-supervised performance on ImageNet.</li> </ul>"},{"location":"deeplearning/7_unsuper/#527-large-scale-generative-models","title":"5.2.7 Large-Scale Generative Models","text":"<ul> <li>GPT (Radford et al., 2019):  </li> <li>Large transformer trained via language modeling.</li> <li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li> </ul>"},{"location":"deeplearning/7_unsuper/#53-contrastive-learning","title":"5.3 Contrastive Learning","text":""},{"location":"deeplearning/7_unsuper/#core-idea","title":"Core Idea","text":"<ul> <li>No need to model \\(p(x)\\) explicitly.</li> <li>Learn representations that maximize mutual information between related samples.</li> </ul>"},{"location":"deeplearning/7_unsuper/#531-word2vec-mikolov-et-al-2013","title":"5.3.1 word2vec (Mikolov et al., 2013)","text":"<ul> <li>Predict context words given a target word.  </li> <li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li> <li>Learns semantic embeddings; supports few-shot translation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018","title":"5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)","text":"<ul> <li>Maximize mutual information between current representation and future observations.  </li> <li>Trains a classifier to distinguish real future samples from negatives.  </li> <li> <p>Learns features useful across modalities (vision, speech).</p> </li> <li> <p>Data-efficient Image Recognition (H\u00e9naff et al., 2019):   contrastive features outperform pixel-level training in low-data regimes.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#533-simclr-chen-et-al-2020","title":"5.3.3 SimCLR (Chen et al., 2020)","text":"<ul> <li>Simple, scalable contrastive framework:</li> <li>Generate two augmented views of the same image.</li> <li>Maximize agreement via contrastive loss (NT-Xent).</li> <li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li> <li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#54-self-supervised-learning","title":"5.4 Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#idea","title":"Idea","text":"<ul> <li>Design pretext tasks that use natural structure in data as supervision.  </li> <li>Representations are deterministic and transferable to new tasks.</li> </ul>"},{"location":"deeplearning/7_unsuper/#541-examples","title":"5.4.1 Examples","text":"Task Description Reference Colorization Predict color from grayscale image Zhang et al., 2016 Context Prediction Predict position of image patches Doersch et al., 2015 Sequence Sorting Predict correct frame order in videos Lee et al., 2017 BERT (Devlin et al., 2019) Masked language modeling + next sentence prediction Revolutionized NLP"},{"location":"deeplearning/7_unsuper/#542-key-benefits","title":"5.4.2 Key Benefits","text":"<ul> <li>Requires no labels \u2014 just structure in data.  </li> <li>Produces general features useful for:</li> <li>Semi-supervised classification  </li> <li>Transfer learning  </li> <li>Downstream reasoning tasks</li> </ul>"},{"location":"deeplearning/7_unsuper/#55-design-principles","title":"5.5 Design Principles","text":"Consideration Desired Property Modality Align architecture with data type (image, text, audio) Task Design Choose pretext that aligns with useful features Consistency Maintain temporal/spatial coherence Discrete + Continuous Latents Enable symbolic and continuous reasoning Adaptivity Representations should evolve with experience <p>Summary: Unsupervised representation learning uses three complementary lenses: - Generative \u2192 model what the world looks like. - Contrastive \u2192 learn what is similar or different. - Self-supervised \u2192 create pseudo-tasks that reveal structure. Together, they aim for data-efficient, transferable, and interpretable representations.</p>"},{"location":"deeplearning/8_latentvariables/","title":"8 latentvariables","text":""},{"location":"deeplearning/8_latentvariables/#1-generative-modelling","title":"1. Generative Modelling","text":""},{"location":"deeplearning/8_latentvariables/#11-what-are-generative-models","title":"1.1 What Are Generative Models?","text":"<ul> <li>Probabilistic models of high-dimensional data.</li> <li>Describe how observations are generated from underlying processes.</li> <li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#12-why-they-matter","title":"1.2 Why They Matter","text":"<p>Generative models can: - Estimate data density (detect outliers, anomalies). - Enable compression (encode \u2192 decode). - Map between domains (e.g., translation, text-to-speech). - Support model-based RL (predict future states). - Learn representations from raw data. - Improve understanding of data structure.</p>"},{"location":"deeplearning/8_latentvariables/#13-types-of-generative-models-in-deep-learning","title":"1.3 Types of Generative Models in Deep Learning","text":""},{"location":"deeplearning/8_latentvariables/#a-autoregressive-models","title":"(a) Autoregressive Models","text":"<p>Model joint distribution via chain rule:  </p> <p>Trained with maximum likelihood</p> <p>Examples:</p> <ul> <li>RNN/Transformer LMs  </li> <li>NADE  </li> <li>PixelCNN / WaveNet</li> </ul> <p>Pros:</p> <ul> <li>Easy training (max. likelihood).</li> <li>No sampling during training.</li> </ul> <p>Cons:</p> <ul> <li>Slow generation (sequential).</li> <li>Often capture local structure better than global structure.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#b-latent-variable-models","title":"(b) Latent Variable Models","text":"<p>Introduce an unobserved latent variable \\(z\\):</p> <ul> <li>Prior: \\(p(z)\\) </li> <li>Likelihood: \\(p_\\theta(x\\mid z)\\) </li> </ul> <p>Joint:  </p> <p>Pros</p> <ul> <li>Flexible &amp; interpretable  </li> <li>Natural for representation learning  </li> <li>Fast generation  </li> </ul> <p>Cons - Require approximate inference unless specially designed (e.g., invertible models).</p>"},{"location":"deeplearning/8_latentvariables/#c-implicit-models-gans","title":"(c) Implicit Models (GANs)","text":"<ul> <li>Define a generator \\(G(z)\\) with no explicit likelihood.</li> <li>Trained adversarially using a discriminator.</li> </ul> <p>Pros</p> <ul> <li>Extremely realistic samples  </li> <li>Fast sampling  </li> </ul> <p>Cons</p> <ul> <li>Cannot evaluate \\(p(x)\\) </li> <li>Mode collapse  </li> <li>Training instability  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-latent-variable-models-inference","title":"2. Latent Variable Models &amp; Inference","text":""},{"location":"deeplearning/8_latentvariables/#21-what-is-a-latent-variable-model-lvm","title":"2.1 What is a Latent Variable Model (LVM)?","text":"<p>A latent variable model introduces an unobserved variable \\(z\\) that explains the observed data \\(x\\).</p> <p>Model components:</p> <ul> <li>Prior over latent variables:  </li> </ul> <p> </p> <ul> <li>Likelihood / decoder mapping latent \u2192 observation:  </li> </ul> <p> </p> <p>Joint distribution:</p> \\[ p_\\theta(x, z) = p_\\theta(x \\mid z)\\,p(z) \\] <p>Marginal likelihood (what we want to maximize when training):</p> \\[ p_\\theta(x) = \\int p_\\theta(x \\mid z)\\,p(z)\\,dz \\]"},{"location":"deeplearning/8_latentvariables/#22-intuition-latents-as-explanations","title":"2.2 Intuition: Latents as \u201cExplanations\u201d","text":"<ul> <li> <p>A particular value of \\(z\\) is a hypothesis about hidden causes that produced \\(x\\).</p> </li> <li> <p>Generation = sample latent \u2192 map it to data:    </p> </li> </ul> <p>Most of the article focuses on the inverse of this: recovering \\(z\\) from \\(x\\).</p>"},{"location":"deeplearning/8_latentvariables/#23-what-is-inference","title":"2.3 What Is Inference?","text":"<p>Inference means computing the posterior:  </p> <p>Why it matters:</p> <ul> <li>Explains the observation (which latents likely produced it?)</li> <li>Needed inside maximum-likelihood training   (the gradient depends on the posterior!)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#24-inference-requires-the-marginal-likelihood","title":"2.4 Inference Requires the Marginal Likelihood","text":"<p>To compute the posterior, we need:  This integral is often intractable.</p> <p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>"},{"location":"deeplearning/8_latentvariables/#25-example-mixture-of-gaussians","title":"2.5 Example: Mixture of Gaussians","text":"<p>Model:</p> <ul> <li>Choose cluster \\(k\\) </li> <li>Sample \\(x\\) from Gaussian for that cluster</li> </ul> <p>Posterior:  </p> <p>This model is tractable because:</p> <ul> <li>Finite number of discrete states  </li> <li>Closed-form posterior</li> </ul>"},{"location":"deeplearning/8_latentvariables/#26-the-need-for-inference-in-learning","title":"2.6 The Need for Inference in Learning","text":""},{"location":"deeplearning/8_latentvariables/#maximum-likelihood-as-the-core-training-principle","title":"Maximum Likelihood as the Core Training Principle","text":"<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters \\(\\theta\\) that make the observed training data as probable as possible:</p> \\[ \\theta^* = \\arg\\max_\\theta \\sum_{i} \\log p_\\theta(x^{(i)}) \\] <p>For latent variable models, the marginal likelihood is:  This integral is rarely tractable, which makes direct maximization difficult.</p>"},{"location":"deeplearning/8_latentvariables/#why-optimization-is-hard-in-latent-variable-models","title":"Why Optimization Is Hard in Latent Variable Models","text":"<ul> <li>The log-likelihood involves an integral (or sum) over the latent variables \\(z\\).  </li> <li>Because this integral usually has no closed form, we must use iterative optimization methods.</li> </ul> <p>Common approaches:</p> <ol> <li>Gradient-based optimization (e.g., gradient descent)</li> <li>Expectation-Maximization (EM)</li> </ol> <p>Below we explain why inference (computing the posterior \\(p_\\theta(z \\mid x)\\)) is essential for both.</p>"},{"location":"deeplearning/8_latentvariables/#261-gradient-based-learning-requires-the-posterior","title":"2.6.1 Gradient-Based Learning Requires the Posterior","text":"<p>Using the identity:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)}[\\nabla_\\theta \\log p_\\theta(x, z)] \\] <p>Differentiate the log-marginal</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{\\nabla_\\theta p_\\theta(x)}{p_\\theta(x)}\\] <p>Using: \\(p_\\theta(x)=\\int p_\\theta(x,z)\\,dz,\\)</p> <p>differentiate under the integral:</p> \\[\\nabla_\\theta p_\\theta(x) = \\nabla_\\theta \\int p_\\theta(x,z)\\,dz = \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Combine:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Apply the log-derivative identity</p> <p>The identity:</p> \\[\\nabla_\\theta p_\\theta(x,z) = p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\] <p>Substitute:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Recognize the posterior</p> <p>Bayes\u2019 rule:</p> \\[p_\\theta(z\\mid x) = \\frac{p_\\theta(x,z)}{p_\\theta(x)}\\] <p>Substitute into the integral:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int \\frac{p_\\theta(x,z)}{p_\\theta(x)} \\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>This becomes:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int p_\\theta(z\\mid x)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Write as an expectation</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)} \\left[ \\nabla_\\theta \\log p_\\theta(x,z) \\right]\\] <p>To compute the gradient of the marginal likelihood, we must take an expectation under the posterior \\(p_\\theta(z\\mid x)\\).</p> <p>So:</p> <ul> <li>We cannot compute \\(\\nabla_\\theta \\log p_\\theta(x)\\) without knowing the posterior.</li> <li>Inference becomes part of every gradient step.</li> <li>If inference is intractable \u2192 gradient is intractable.</li> </ul> <p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>"},{"location":"deeplearning/8_latentvariables/#262-expectation-maximization-em-also-requires-inference","title":"2.6.2 Expectation-Maximization (EM) Also Requires Inference","text":"<p>EM is an alternative to gradient descent for maximizing likelihood.</p>"},{"location":"deeplearning/8_latentvariables/#e-step","title":"E-step:","text":"<p>Compute (or approximate) the posterior:  This assigns responsibilities to each latent configuration.</p>"},{"location":"deeplearning/8_latentvariables/#m-step","title":"M-step:","text":"<p>Update parameters by maximizing the expected complete-data log-likelihood:  </p> <p>Thus, the E-step directly requires inference.</p>"},{"location":"deeplearning/8_latentvariables/#27-why-exact-inference-is-hard","title":"2.7 Why Exact Inference Is Hard","text":""},{"location":"deeplearning/8_latentvariables/#continuous-latents","title":"Continuous latents:","text":"<ul> <li>Require multidimensional integration over nonlinear likelihoods.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#discrete-latents","title":"Discrete latents:","text":"<ul> <li>Require summing over exponentially many configurations.</li> </ul> <p>Only a few cases allow closed-form inference:</p> <ul> <li>Mixture models  </li> <li>Linear Gaussian systems  </li> <li>Invertible / flow-based models (covered next)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#28-two-strategies-to-handle-intractability","title":"2.8 Two Strategies to Handle Intractability","text":""},{"location":"deeplearning/8_latentvariables/#1-design-tractable-models","title":"1. Design tractable models","text":"<ul> <li>Invertible models (normalizing flows)</li> <li>Autoregressive latent structures Pros: exact inference Cons: restricted model class</li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-approximate-inference","title":"2. Approximate inference","text":"<ul> <li>Use approximations to posterior \\(p(z \\mid x)\\) </li> <li>Variational Inference or MCMC Pros: flexible, expressive models Cons: introduces approximation error</li> </ul>"},{"location":"deeplearning/8_latentvariables/#3-invertible-models-exact-inference","title":"3. Invertible Models &amp; Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#31-what-are-invertible-models","title":"3.1 What Are Invertible Models?","text":"<p>Invertible models (also called normalizing flows) are latent variable models where:</p> <ul> <li>The latent variable \\(z\\) and data \\(x\\) have the same dimensionality</li> <li>There exists an invertible, differentiable mapping </li> <li>Because \\(f_\\theta\\) is invertible:    </li> </ul> <p>Key property: Inference is exact and trivial \u2014 simply apply the inverse function.</p>"},{"location":"deeplearning/8_latentvariables/#32-generative-process","title":"3.2 Generative Process","text":"<p>To generate a sample:</p> <ol> <li>Sample \\(z \\sim p(z)\\) (usually a simple prior like \\(\\mathcal{N}(0, I)\\))</li> <li>Transform via </li> </ol> <p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>"},{"location":"deeplearning/8_latentvariables/#33-why-are-invertible-models-attractive","title":"3.3 Why Are Invertible Models Attractive?","text":"<ul> <li> <p>Exact inference:    is computed by a single function evaluation (no approximation needed).</p> </li> <li> <p>Exact likelihood:   Can compute \\(\\log p_\\theta(x)\\) exactly using the change-of-variables formula.</p> </li> </ul>"},{"location":"deeplearning/8_latentvariables/#34-change-of-variables-for-likelihood","title":"3.4 Change of Variables for Likelihood","text":"<p>Given an invertible mapping \\(x = f_\\theta(z)\\):</p> \\[ p_\\theta(x) = p(z) \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(x)}{\\partial x} \\right) \\right| \\] <p>Equivalently, using \\(z = f_\\theta^{-1}(x)\\):</p> \\[ \\log p_\\theta(x) = \\log p(z) + \\log \\left| \\det J_{f_\\theta^{-1}}(x) \\right| \\] <p>Where:</p> <ul> <li>\\(J_{f_\\theta^{-1}}\\) is the Jacobian matrix of the inverse map  </li> <li>The determinant accounts for volume change introduced by transformation</li> </ul>"},{"location":"deeplearning/8_latentvariables/#35-example-independent-component-analysis-ica","title":"3.5 Example: Independent Component Analysis (ICA)","text":"<p>ICA is the simplest invertible model:</p> <ul> <li>Latent prior:  factorial prior      with non-Gaussian heavy-tailed components</li> <li>Linear invertible mixing: </li> </ul> <p>Inference:  </p> <p>ICA recovers independent sources that explain the observed signal.</p>"},{"location":"deeplearning/8_latentvariables/#36-building-complex-invertible-models","title":"3.6 Building Complex Invertible Models","text":"<p>Modern flows build \\(f_\\theta\\) by composing many simple invertible layers:</p> \\[ f_\\theta = f_K \\circ f_{K-1} \\circ \\dots \\circ f_1 \\] <p>Composition of invertible functions is invertible.</p> <p>Building blocks:</p> <ul> <li>Linear transforms</li> <li>Autoregressive flows (IAF, MAF)</li> <li>Coupling layers (RealNVP, Glow)</li> <li>Residual flows</li> <li>Sylvester flows</li> </ul> <p>Design goal:</p> <p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>"},{"location":"deeplearning/8_latentvariables/#37-advantages-limitations","title":"3.7 Advantages &amp; Limitations","text":""},{"location":"deeplearning/8_latentvariables/#advantages","title":"Advantages","text":"<ul> <li>Exact inference  </li> <li>Exact log-likelihood  </li> <li>Fast, parallel sampling  </li> <li>Useful as components in larger probabilistic models</li> </ul>"},{"location":"deeplearning/8_latentvariables/#limitations","title":"Limitations","text":"<ul> <li>Latent and data dimensions must match  </li> <li>Latents must be continuous  </li> <li>Observations must be continuous or quantized  </li> <li>Very deep flows require large memory  </li> <li>Hard to encode strong structure or sparsity  </li> </ul> <p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>"},{"location":"deeplearning/8_latentvariables/#mar","title":"Mar","text":""},{"location":"deeplearning/8_latentvariables/#4-variational-inference-vi","title":"4. Variational Inference (VI)","text":""},{"location":"deeplearning/8_latentvariables/#41-why-variational-inference","title":"4.1 Why Variational Inference?","text":"<p>In many latent variable models, the true posterior  is intractable because computing  is impossible in closed form.</p> <p>We still need the posterior for:</p> <ul> <li>Inference (explaining the observation)</li> <li>Learning (MLE gradient depends on it)</li> <li>EM algorithm E-step</li> </ul>"},{"location":"deeplearning/8_latentvariables/#approximate-inference","title":"Approximate Inference","text":"<p>There are two major classes of approaches to approximate inference:</p>"},{"location":"deeplearning/8_latentvariables/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>Generate samples from the exact posterior using a Markov chain.</p> <ul> <li>Very general; exact in the limit of infinite time / computation  </li> <li>Computationally expensive  </li> <li>Convergence is hard to diagnose  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-variational-inference-vi","title":"2. Variational Inference (VI)","text":"<p>Approximate the posterior with a tractable distribution (e.g., fully factorized, mixture, or autoregressive).</p> <ul> <li>Fairly efficient \u2014 inference reduces to optimization of distribution parameters  </li> <li>Fast at test time (single forward pass of the inference network)  </li> <li>Cannot easily trade computation for accuracy (unlike MCMC)  </li> </ul> <p>MCMC = flexible, asymptotically exact, but slow. VI = fast and scalable, but biased due to restricted approximating family.</p>"},{"location":"deeplearning/8_latentvariables/#42-core-idea-of-variational-inference","title":"4.2 Core Idea of Variational Inference","text":"<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing. Approximate the posterior with a simpler distribution:</p> \\[ q_\\phi(z \\mid x) \\approx p_\\theta(z \\mid x) \\] <p>Where:</p> <ul> <li>\\(q_\\phi\\) is the variational posterior</li> <li>\\(\\phi\\) are variational parameters (learned)</li> </ul> <p>Requirements:</p> <ol> <li>We can sample from \\(q_\\phi(z \\mid x)\\) </li> <li>We can compute \\(\\log q_\\phi(z \\mid x)\\) and its gradient wrt \\(\\phi\\) </li> </ol> <p>Common choice: mean-field approximation</p> \\[ q_\\phi(z \\mid x) = \\prod_i q_\\phi(z_i \\mid x) \\]"},{"location":"deeplearning/8_latentvariables/#43-training-with-variational-inference","title":"4.3 Training with Variational Inference","text":"<p>Goal: maximize the marginal likelihood</p> \\[ \\log p_\\theta(x) \\] <p>Since it's intractable, VI uses a lower bound on this quantity.</p>"},{"location":"deeplearning/8_latentvariables/#variational-lower-bound-elbo","title":"Variational Lower Bound (ELBO)","text":"<p>Using Jensen\u2019s inequality:</p> \\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x, z)] - \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log q_\\phi(z \\mid x)] \\] <p>This is the Evidence Lower Bound (ELBO):</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi}\\!\\left[\\log p_\\theta(x, z)\\right] - \\mathbb{E}_{q_\\phi}\\!\\left[\\log q_\\phi(z \\mid x)\\right] \\] <p>We maximize ELBO w.r.t both \\(\\theta\\) and \\(\\phi\\).</p>"},{"location":"deeplearning/8_latentvariables/#44-kl-interpretation-variational-gap","title":"4.4 KL Interpretation (Variational Gap)","text":"<p>Rewrite ELBO:</p> \\[ \\log p_\\theta(x) = \\text{ELBO}(\\theta, \\phi) + D_{\\text{KL}}(q_\\phi(z \\mid x) \\,\\|\\, p_\\theta(z \\mid x)) \\] <p>Thus:</p> <ul> <li> <p>Maximizing ELBO wrt \\(\\phi\\)   \u2192 minimizes the KL divergence between \\(q_\\phi\\) and the true posterior.</p> </li> <li> <p>The variational gap is </p> </li> </ul> <p>If \\(q_\\phi\\) is expressive enough:  </p>"},{"location":"deeplearning/8_latentvariables/#45-what-happens-when-updating-each-parameter-set","title":"4.5 What Happens When Updating Each Parameter Set?","text":""},{"location":"deeplearning/8_latentvariables/#updating-variational-parameters-phi","title":"Updating variational parameters \\(\\phi\\):","text":"<ul> <li>Minimizes the variational gap  </li> <li>Makes \\(q_\\phi(z \\mid x)\\) closer to the true posterior  </li> <li>Does not affect the model directly</li> </ul>"},{"location":"deeplearning/8_latentvariables/#updating-model-parameters-theta","title":"Updating model parameters \\(\\theta\\):","text":"<ul> <li>Increases \\(\\log p_\\theta(x)\\) (good)</li> <li>BUT often also reduces the gap by making the posterior simpler   \u2192 Risk: posterior collapse / variational pruning</li> </ul> <p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>"},{"location":"deeplearning/8_latentvariables/#46-variational-pruning-posterior-collapse","title":"4.6 Variational Pruning (Posterior Collapse)","text":"<p>Because VI pushes \\(p_\\theta(z \\mid x)\\) towards \\(q_\\phi(z\\mid x)\\), the model may choose to ignore some latent dimensions:</p> \\[ p_\\theta(z_i \\mid x) = p(z_i) \\] <p>Meaning the latent variable carries no information about \\(x\\).</p> <p>Pros:</p> <ul> <li>Automatically learns effective latent dimensionality</li> </ul> <p>Cons:</p> <ul> <li>Prevents fully utilizing the latent capacity  </li> <li>Common issue in VAEs (particularly with strong decoders)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#47-choosing-the-variational-posterior-family","title":"4.7 Choosing the Variational Posterior Family","text":""},{"location":"deeplearning/8_latentvariables/#simple-mean-field-gaussian","title":"Simple: Mean-field Gaussian","text":"<ul> <li>Fast</li> <li>Easy to optimize</li> <li>But limited expressivity</li> </ul>"},{"location":"deeplearning/8_latentvariables/#more-expressive-options","title":"More expressive options:","text":"<ul> <li>Mixture posteriors</li> <li>Gaussians with full covariance</li> <li>Autoregressive posteriors</li> <li>Normalizing-flow posteriors</li> </ul> <p>Trade-off: accuracy vs speed.</p>"},{"location":"deeplearning/8_latentvariables/#48-amortized-variational-inference","title":"4.8 Amortized Variational Inference","text":"<p>Classic VI:</p> <ul> <li>Each datapoint \\(x\\) has its own variational parameters  </li> <li>Requires iterative optimization per datapoint  </li> <li>Too slow for deep learning</li> </ul> <p>Amortized VI:</p> <ul> <li>Use an inference network (encoder)    </li> <li>Fast inference  </li> <li>Works with SGD  </li> <li>Introduced in Helmholtz Machines  </li> <li>Popularized by Variational Autoencoders</li> </ul>"},{"location":"deeplearning/8_latentvariables/#49-variational-vs-exact-inference","title":"4.9 Variational vs Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#advantages-of-vi","title":"Advantages of VI","text":"<ul> <li>Scalable to modern deep models  </li> <li>Fast inference  </li> <li>Enables flexible model design  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#disadvantages","title":"Disadvantages","text":"<ul> <li>Approximation bias  </li> <li>Posterior may be oversimplified  </li> <li>Can limit expressiveness of the full model  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#410-summary-of-section-4","title":"4.10 Summary of Section 4","text":"<ul> <li>Variational inference approximates the true posterior with a tractable distribution.  </li> <li>ELBO gives a trainable lower bound on the marginal likelihood.  </li> <li>VI converts inference into optimization.  </li> <li>Amortized VI enables neural inference (encoders).  </li> <li>Variational pruning can arise naturally and must be managed.  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#5-gradient-estimation-in-variational-inference","title":"5. Gradient Estimation in Variational Inference","text":""},{"location":"deeplearning/8_latentvariables/#51-why-do-we-need-gradient-estimators","title":"5.1 Why Do We Need Gradient Estimators?","text":"<p>To train a latent variable model with variational inference, we maximize the ELBO:</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z\\mid x)}\\Big[ \\log p_\\theta(x, z) - \\log q_\\phi(z\\mid x) \\Big] \\] <p>We need gradients with respect to:</p> <ol> <li>Model parameters \\(\\theta\\)</li> <li>Variational parameters \\(\\phi\\)</li> </ol> <p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>"},{"location":"deeplearning/8_latentvariables/#52-gradients-wrt-model-parameters-theta","title":"5.2 Gradients w.r.t. Model Parameters (\\(\\theta\\))","text":"<p>This part is easy.</p> <p>Because \\(q_\\phi(z\\mid x)\\) does not depend on \\(\\theta\\):</p> \\[ \\nabla_\\theta \\text{ELBO} = \\mathbb{E}_{q_\\phi(z\\mid x)} \\big[ \\nabla_\\theta \\log p_\\theta(x, z) \\big] \\] <p>We estimate this using samples:</p> <ol> <li>Draw \\(z \\sim q_\\phi(z\\mid x)\\) </li> <li>Compute \\(\\nabla_\\theta \\log p_\\theta(x,z)\\) </li> <li>Average across samples</li> </ol> <p>No special techniques required.</p>"},{"location":"deeplearning/8_latentvariables/#53-gradients-wrt-variational-parameters-phi","title":"5.3 Gradients w.r.t. Variational Parameters (\\(\\phi\\))","text":"<p>This is more difficult.</p> <p>We want:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] \\] <p>But \\(q_\\phi(z\\mid x)\\) depends on \\(\\phi\\). Two main strategies exist to handle this dependence:</p>"},{"location":"deeplearning/8_latentvariables/#54-two-families-of-gradient-estimators","title":"5.4 Two Families of Gradient Estimators","text":""},{"location":"deeplearning/8_latentvariables/#1-likelihood-ratio-reinforce-estimator","title":"\ud83d\udd37 1. Likelihood-Ratio / REINFORCE Estimator","text":"<p>Uses the identity:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}[f(z)] = \\mathbb{E}_{q_\\phi(z)}[f(z)\\,\\nabla_\\phi \\log q_\\phi(z)] \\] <p>This allows gradients for: - Discrete latent variables - Non-differentiable \\(f(z)\\) - Any distribution where we can compute \\(\\log q_\\phi(z)\\)</p> <p>Pros - Very general - Works for discrete and continuous latents  </p> <p>Cons - High variance - Requires variance reduction (baselines, control variates)</p> <p>This is the same gradient estimator used in policy gradients in RL.</p>"},{"location":"deeplearning/8_latentvariables/#2-reparameterization-pathwise-estimator","title":"\ud83d\udd37 2. Reparameterization / Pathwise Estimator","text":"<p>Instead of sampling \\(z \\sim q_\\phi(z\\mid x)\\) directly, write it as a differentiable transformation of noise:</p> \\[ z = g_\\phi(\\epsilon, x), \\quad \\epsilon \\sim p(\\epsilon) \\] <p>Then:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)} \\big[ \\nabla_\\phi f(g_\\phi(\\epsilon, x)) \\big] \\] <p>This pushes the dependence on \\(\\phi\\) inside a differentiable function.</p>"},{"location":"deeplearning/8_latentvariables/#example-gaussian-posterior","title":"Example: Gaussian posterior","text":"<p>If  then:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1) \\] <p>Pros - Low variance - Enables stable VAE training  </p> <p>Cons - Only works for continuous latent variables - Requires differentiable sampling procedure</p>"},{"location":"deeplearning/8_latentvariables/#55-comparison-table","title":"5.5 Comparison Table","text":"Property REINFORCE Reparameterization Works for discrete latent variables \u2705 \u274c Works for continuous latent variables \u2705 \u2705 Low-variance gradients \u274c \u2705 Requires differentiable sampling \u274c \u2705 Used in VAEs sometimes always"},{"location":"deeplearning/8_latentvariables/#56-practical-notes","title":"5.6 Practical Notes","text":"<ul> <li>Modern VAEs always use the reparameterization trick.  </li> <li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li> <li>Discrete VAEs use:</li> <li>Gumbel-Softmax  </li> <li>NVIL / REINFORCE with baselines  </li> <li>VIMCO  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#57-summary-of-section-5","title":"5.7 Summary of Section 5","text":"<ul> <li>Gradient estimation is essential for training VI models.  </li> <li>\\(\\nabla_\\theta\\) is easy: just sample from the variational posterior.  </li> <li>\\(\\nabla_\\phi\\) is hard because sampling depends on parameters.  </li> <li>Two estimators solve this:</li> <li>Likelihood-ratio (REINFORCE)  </li> <li>Reparameterization trick  </li> <li>Reparameterization yields low-variance gradients and powers modern VAEs.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":""},{"location":"deeplearning/8_latentvariables/#61-what-is-a-vae","title":"6.1 What Is a VAE?","text":"<p>A VAE is a latent variable generative model with:</p> <ul> <li>Continuous latent variables \\(z\\)</li> <li>Neural networks for both:</li> <li>Encoder (variational posterior) \\(q_\\phi(z \\mid x)\\) </li> <li>Decoder (likelihood) \\(p_\\theta(x \\mid z)\\)</li> <li>Training through amortized variational inference  </li> <li>Gradients computed using the reparameterization trick</li> </ul> <p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>"},{"location":"deeplearning/8_latentvariables/#62-vae-model-components","title":"6.2 VAE Model Components","text":""},{"location":"deeplearning/8_latentvariables/#prior","title":"Prior","text":"<p>Usually a factorized standard Gaussian:  </p>"},{"location":"deeplearning/8_latentvariables/#likelihood-decoder","title":"Likelihood / Decoder","text":"<p>Maps latents to a distribution over observations.</p> <p>For binary data:  </p> <p>For real-valued data:  </p>"},{"location":"deeplearning/8_latentvariables/#variational-posterior-encoder","title":"Variational Posterior / Encoder","text":"\\[ q_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu_\\phi(x), \\sigma_\\phi^2(x)) \\] <p>All of these functions (encoder &amp; decoder) can be implemented with: - MLPs - ConvNets - ResNets - Transformers depending on the domain.</p>"},{"location":"deeplearning/8_latentvariables/#63-training-objective-the-elbo","title":"6.3 Training Objective: The ELBO","text":"<p>VAEs maximize the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - D_{\\text{KL}}\\!\\Big(q_\\phi(z\\mid x)\\,\\|\\, p(z)\\Big) \\] <p>Interpretation:</p> <ol> <li> <p>Reconstruction Term    Measures how well the model predicts \\(x\\) from \\(z\\).    Encourages informative latents.</p> </li> <li> <p>KL Regularization Term    Encourages \\(q_\\phi(z\\mid x)\\) to stay close to the prior \\(p(z)\\).    Prevents overfitting and encourages smooth latent spaces.</p> </li> </ol> <p>The KL term often has closed-form for Gaussian distributions.</p>"},{"location":"deeplearning/8_latentvariables/#64-reparameterization-trick-key-to-vaes","title":"6.4 Reparameterization Trick (Key to VAEs)","text":"<p>Direct backprop through a sample \\(z \\sim q_\\phi(z\\mid x)\\) is impossible.</p> <p>Solution: rewrite sampling as a differentiable transformation of noise:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>This allows gradient flow through \\(z\\) and makes VAE training practical.</p>"},{"location":"deeplearning/8_latentvariables/#65-vae-as-a-framework","title":"6.5 VAE as a Framework","text":"<p>The term \u201cVAE\u201d now refers to a broad family of models: - Continuous latent variables - Amortized inference - Reparameterization-based gradients - Trained by maximizing ELBO (or its variants)</p> <p>Modern VAEs extend the basic version in many ways: - Multiple latent layers - More expressive posteriors (flows, mixtures) - More expressive priors (hierarchical, autoregressive) - More expressive decoders (ResNets, autoregressive PixelCNN decoders) - Iterative inference networks - Variance reduction techniques</p> <p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>"},{"location":"deeplearning/8_latentvariables/#66-summary-of-section-6","title":"6.6 Summary of Section 6","text":"<ul> <li>VAEs are tractable generative models with continuous latent variables.</li> <li>They pair:</li> <li>a decoder \\(p_\\theta(x\\mid z)\\) and  </li> <li>an encoder \\(q_\\phi(z\\mid x)\\)   using amortized VI.</li> <li>Training uses ELBO + reparameterization trick.</li> <li>VAEs balance reconstruction quality with regularized latent structure.</li> <li>The VAE framework is highly extensible and central to modern deep generative modeling.</li> </ul>"},{"location":"deeplearning/alogirthmic_detials/","title":"Alogirthmic detials","text":"<ol> <li>Batch normalization</li> <li>contrastive loss</li> <li>Sematic segmentation</li> <li>class, bounding box</li> <li>poly nn</li> <li>Representation learning</li> </ol>"},{"location":"deeplearning/plan/","title":"Plan","text":"<p>diffusion  GAN</p> <p>articles</p>"},{"location":"distributedsystems/0_intro/","title":"Introduction","text":""},{"location":"distributedsystems/0_intro/#introduction","title":"Introduction","text":"<p>What is \"distributed system\":</p> <p>A group of computers cooperating to provide a service</p>"},{"location":"distributedsystems/0_intro/#why","title":"Why?","text":"<ol> <li>to increase capacity via parallel processing</li> <li>to tolerate faults via replication</li> <li>to match distribution of physical devices e.g. sensors</li> <li>to increase security via isolation</li> </ol>"},{"location":"distributedsystems/0_intro/#challanges","title":"Challanges:","text":"<ul> <li>concurrency</li> <li>complex interactions</li> <li>performance bottlenecks</li> <li>partial failure</li> </ul>"},{"location":"distributedsystems/0_intro/#key-topics","title":"Key Topics","text":""},{"location":"distributedsystems/0_intro/#fault-tolerance","title":"Fault tolerance:","text":"<ul> <li>1000s of servers, big network -&gt; always something broken</li> <li>We'd like to hide these failures from the application.</li> <li>\"High availability\": service continues despite failures</li> <li>Big idea: replicated servers. If one server crashes, can proceed using the other(s).</li> </ul>"},{"location":"distributedsystems/0_intro/#consistency","title":"Consistency:","text":"<ul> <li>General-purpose infrastructure needs well-defined behavior. E.g. \"read(x) yields the value from the most recent write(x).\"</li> <li>Achieving good behavior is hard! e.g. \"replica\" servers are hard to keep identical.</li> </ul>"},{"location":"distributedsystems/0_intro/#performance","title":"Performance:","text":"<ul> <li>The goal: scalable throughput. Nx servers -&gt; Nx total throughput via parallel CPU, RAM, disk, net.</li> <li>Scaling gets harder as N grows:<ul> <li>Load imbalance.</li> <li>Slowest-of-N latency.</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#tradeoffs","title":"Tradeoffs:","text":"<ul> <li>Fault-tolerance, consistency, and performance are enemies.</li> <li>Fault tolerance and consistency require communication<ul> <li>e.g., send data to backup server</li> <li>e.g., check if cached data is up-to-date</li> <li>communication is often slow and non-scalable</li> </ul> </li> <li>Many designs sacrifice consistency to gain speed.<ul> <li>e.g. read(x) might not yield the latest write(x)!</li> <li>Painful for application programmers (or users).</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#implementation","title":"Implementation:","text":"<ul> <li>RPC, threads, concurrency control, configuration.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#mapreduce-a-complete-guide","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#introduction","title":"Introduction","text":"<p>Modern data analysis often involves multi-hour computations on multi-terabyte datasets \u2014  for example, building search indexes, sorting massive logs, or analyzing web graphs.  Such tasks are only practical using thousands of computers working in parallel.</p> <p>MapReduce (MR) is a programming model designed to make large-scale data processing  easy for non-specialist programmers. It lets you write simple sequential code, while the framework handles parallel execution, fault tolerance, and data distribution.</p>"},{"location":"distributedsystems/1_mapreduce/#core-concept","title":"Core Concept","text":"<p>The programmer defines just two functions:</p> <ul> <li><code>Map()</code> \u2013 processes input data and emits key-value pairs.</li> <li><code>Reduce()</code> \u2013 aggregates or summarizes all values associated with a given key.</li> </ul> <p>Everything else \u2014 input splitting, task scheduling, network communication, and fault recovery \u2014  is handled automatically by the MapReduce framework.</p>"},{"location":"distributedsystems/1_mapreduce/#how-mapreduce-works-word-count-example","title":"How MapReduce Works (Word Count Example)","text":""},{"location":"distributedsystems/1_mapreduce/#abstract-view","title":"Abstract View","text":"<pre><code>Input1 -&gt; Map -&gt; a,1 b,1\nInput2 -&gt; Map -&gt;     b,1\nInput3 -&gt; Map -&gt; a,1     c,1\n                    |   |   |\n                    |   |   -&gt; Reduce -&gt; c,1\n                    |   -----&gt; Reduce -&gt; b,2\n                    ---------&gt; Reduce -&gt; a,2\n</code></pre>"},{"location":"distributedsystems/1_mapreduce/#steps","title":"Steps","text":"<ol> <li>Input Splitting \u2014 Data is divided into <code>M</code> splits (files or blocks).</li> <li>Map Phase \u2014 Each split is processed by a Map task, generating <code>(key, value)</code> pairs.</li> <li>Shuffle Phase \u2014 Intermediate pairs are grouped by key and distributed to Reduce tasks.</li> <li>Reduce Phase \u2014 Each Reduce task processes one group and outputs final results.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#word-count-example","title":"Word Count Example","text":"<pre><code># Map function\ndef Map(document):\n    words = document.split()\n    for word in words:\n        emit(word, 1)\n\n# Reduce function\ndef Reduce(word, values):\n    emit(word, sum(values))\n</code></pre> <p>Final Output: </p><pre><code>a: 2\nb: 2\nc: 1\n</code></pre><p></p>"},{"location":"distributedsystems/1_mapreduce/#why-mapreduce-scales-so-well","title":"Why MapReduce Scales So Well","text":"<ul> <li>Parallelism: Map and Reduce tasks run independently, enabling massive parallelism.</li> <li>Automatic Management: The framework handles failures, scheduling, and communication.</li> <li>Simplicity: Developers only implement <code>Map()</code> and <code>Reduce()</code>.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#input-output-storage-via-gfs","title":"Input &amp; Output Storage (via GFS)","text":"<p>MapReduce typically uses a distributed file system such as Google File System (GFS).</p> <ul> <li>Files split into 64 MB chunks, distributed across many servers.</li> <li>Maps read input in parallel; Reduces write output in parallel.</li> <li>Replication (2\u20133 copies) ensures fault tolerance.</li> <li>Data locality: Tasks are often scheduled on the same machine where their data resides.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#inside-the-mapreduce-framework","title":"Inside the MapReduce Framework","text":""},{"location":"distributedsystems/1_mapreduce/#coordinators-role","title":"Coordinator\u2019s Role","text":"<ol> <li>Map Phase</li> <li>Assigns Map tasks to workers.</li> <li>Each Map writes intermediate output to its local disk.</li> <li> <p>Intermediate data is partitioned by <code>hash(key) mod R</code> (R = number of Reduces).</p> </li> <li> <p>Reduce Phase</p> </li> <li>Coordinator assigns Reduce tasks.</li> <li>Each Reduce fetches its partition (bucket) from all Maps.</li> <li> <p>Sorts data by key and processes each group.</p> </li> <li> <p>Output</p> </li> <li>Each Reduce writes its final output to GFS.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#performance-and-bottlenecks","title":"Performance and Bottlenecks","text":""},{"location":"distributedsystems/1_mapreduce/#what-limits-performance","title":"What Limits Performance?","text":"<p>Often, network speed is the main bottleneck \u2014 not CPU or disk speed.</p> <p>Network Transfers Include:</p> <ul> <li>Maps reading input from GFS.</li> <li>Reduces fetching intermediate (shuffled) data from Maps.</li> <li>Reduces writing output to GFS.</li> </ul> <p>Because the shuffle phase may move data as large as the original input, network optimization is critical.</p>"},{"location":"distributedsystems/1_mapreduce/#network-optimizations","title":"Network Optimizations","text":"<ul> <li>Data Locality: Run Map tasks where their input data is stored.</li> <li>Single Network Transfer: Intermediate data stored locally, not in GFS.</li> <li>Hash Partitioning: Reduces transfer large data batches (buckets), minimizing small transfers.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#load-balancing","title":"Load Balancing","text":"<p>Why it matters:  Uneven load causes idle workers waiting for \u201cstragglers\u201d. Solution:  </p> <ul> <li>Create many more tasks than workers.</li> <li>The Coordinator dynamically assigns tasks to free workers.</li> <li>Faster machines handle more tasks; slower ones handle fewer.</li> </ul> <p>This keeps the cluster well-balanced and efficient.</p>"},{"location":"distributedsystems/1_mapreduce/#fault-tolerance","title":"Fault Tolerance","text":"<p>Failures are expected in large clusters. MapReduce handles them gracefully.</p>"},{"location":"distributedsystems/1_mapreduce/#worker-failures","title":"Worker Failures","text":"<ul> <li> <p>Map worker crash:</p> </li> <li> <p>Intermediate data (stored locally) is lost.</p> </li> <li>Coordinator reassigns those Map tasks to new workers.</li> <li> <p>No need to rerun if Reduces already fetched the data.</p> </li> <li> <p>Reduce worker crash:</p> </li> <li> <p>Completed results are safe (stored in GFS).</p> </li> <li>Unfinished Reduce tasks are rerun elsewhere.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#deterministic-functions-required","title":"Deterministic Functions Required","text":"<p>Because tasks may be re-executed:</p> <ul> <li><code>Map()</code> and <code>Reduce()</code> must be pure functions \u2014 deterministic and side-effect-free.</li> <li>No external state, random numbers, or I/O beyond the framework.</li> </ul> <p>This guarantees identical results across re-runs.</p>"},{"location":"distributedsystems/1_mapreduce/#handling-other-failures","title":"Handling Other Failures","text":"<ul> <li>Duplicate task execution:   Coordinator accepts output from only one instance.</li> <li>Simultaneous Reduce outputs:   GFS\u2019s atomic rename ensures one consistent final file.</li> <li>Stragglers:   Coordinator launches backup copies of slow tasks.</li> <li>Corrupted output or bad hardware:   Not handled \u2014 MR assumes fail-stop (crash, not corrupt) behavior.</li> <li>Coordinator crash:   Not fully addressed in the original paper.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-works-well","title":"Where MapReduce Works Well","text":"<p>Ideal Use Cases:</p> <ul> <li>Batch processing of huge datasets (TB\u2013PB scale)</li> <li>Log analysis (e.g., counting queries, clickstream analytics)</li> <li>Index building for search engines</li> <li>Data transformations (ETL pipelines)</li> <li>Large-scale machine learning preprocessing</li> <li>Sorting and aggregation across distributed data</li> </ul> <p>These workloads share common traits:</p> <ul> <li>Large, independent input records</li> <li>Deterministic, parallel-friendly computation</li> <li>No need for real-time feedback</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-falls-short","title":"Where MapReduce Falls Short","text":"<p>Not Suitable For:</p> <ul> <li> <p>Real-time or streaming data processing   MR is inherently batch-oriented; results appear only after job completion.</p> </li> <li> <p>Interactive querying   Jobs take minutes to hours; unsuitable for low-latency analytics.</p> </li> <li> <p>Iterative algorithms   Machine learning or graph algorithms (e.g., PageRank, K-means) need multiple    passes over data, causing heavy I/O.</p> </li> <li> <p>Stateful or dependent tasks   MR disallows inter-task communication or shared state.</p> </li> <li> <p>Small or medium datasets   Overhead of distributing tasks outweighs benefits.</p> </li> </ul> <p>Modern systems like Apache Spark, Flink, or Beam were designed to overcome these limitations by enabling in-memory and streaming computation.</p>"},{"location":"distributedsystems/2_threads/","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#65840-lecture-2-2025-threads-and-rpc","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#introduction-implementing-distributed-systems","title":"Introduction: Implementing Distributed Systems","text":"<p>This lecture introduces: - Go threads (goroutines) - Concurrency challenges - The web crawler example - Remote Procedure Calls (RPC)</p> <p>Go is the language used for this writeup.</p>"},{"location":"distributedsystems/2_threads/#why-go","title":"Why Go?","text":"<p>Go is well-suited for distributed systems:</p> <ul> <li>Excellent thread (goroutine) support  </li> <li>Convenient RPC library</li> <li>Type- and memory-safe</li> <li>Garbage-collected (safe with concurrency)</li> <li>Simpler than many other languages</li> <li>Commonly used in production distributed systems</li> </ul> <p>\ud83d\udc49 After the tutorial, read Effective Go: https://golang.org/doc/effective_go.html</p>"},{"location":"distributedsystems/2_threads/#threads-goroutines","title":"Threads (Goroutines)","text":""},{"location":"distributedsystems/2_threads/#what-is-a-thread","title":"What is a Thread?","text":"<p>A thread is a \u201cthread of execution\u201d:</p> <ul> <li>Allows a program to do multiple things at once</li> <li>Executes sequentially (like a program), but shares memory with other threads</li> <li>Has its own program counter, registers, and stack</li> </ul> <p>In Go, threads are called goroutines.</p>"},{"location":"distributedsystems/2_threads/#why-use-threads","title":"Why Use Threads?","text":"<p>Three type of threads:</p>"},{"location":"distributedsystems/2_threads/#1-io-concurrency","title":"1. I/O Concurrency","text":"<ul> <li>Client sends requests to many servers at once  </li> <li>Server handles many clients concurrently  </li> <li>When one thread blocks on I/O, another can run</li> </ul>"},{"location":"distributedsystems/2_threads/#2-multicore-performance","title":"2. Multicore Performance","text":"<p>Use multiple CPU cores simultaneously.</p>"},{"location":"distributedsystems/2_threads/#3-convenience","title":"3. Convenience","text":"<p>Run background tasks (e.g., periodic health checks).</p>"},{"location":"distributedsystems/2_threads/#alternative-event-driven-systems","title":"Alternative: Event-Driven Systems","text":"<p>Instead of threads:</p> <ul> <li>Use a single-threaded system with an event loop</li> <li>Explicitly interleave different activities</li> <li>Maintain state tables for each ongoing operation</li> </ul> <p>Pros:  </p> <ul> <li>Good for I/O concurrency  </li> <li>No thread overhead</li> </ul> <p>Cons:  </p> <ul> <li>No multicore usage  </li> <li>Hard to program and maintain</li> </ul>"},{"location":"distributedsystems/2_threads/#threading-challenges","title":"Threading Challenges","text":""},{"location":"distributedsystems/2_threads/#1-safe-data-sharing","title":"1. Safe Data Sharing","text":"<p>Race example: </p><pre><code>n = n + 1\n</code></pre> Two threads modifying <code>n</code> at the same time \u2192 race condition.<p></p> <p>A race is when: - Two threads access the same memory - At least one is a write - And there's no synchronization</p> <p>Fixes: - Use <code>sync.Mutex</code> - Avoid sharing mutable data</p>"},{"location":"distributedsystems/2_threads/#2-coordination-producerconsumer","title":"2. Coordination (Producer\u2013Consumer)","text":"<ul> <li>One thread produces data  </li> <li>Another consumes it  </li> <li>Need a way for consumers to wait and wake up</li> </ul> <p>Tools: - Go channels - <code>sync.Cond</code> - <code>sync.Wait</code>, <code>sync.WaitGroup</code></p>"},{"location":"distributedsystems/2_threads/#3-deadlock","title":"3. Deadlock","text":"<p>When threads wait on each other forever. Can happen via:</p> <ul> <li>Locks</li> <li>Channels</li> <li>RPC</li> </ul>"},{"location":"distributedsystems/2_threads/#web-crawler-example","title":"Web Crawler Example","text":"<p>A web crawler:</p> <ul> <li>Fetches web pages recursively starting from a URL</li> <li>Follows links</li> <li>Avoids revisiting pages</li> <li>Avoids cycles</li> <li>Exploits I/O concurrency for speed</li> </ul>"},{"location":"distributedsystems/2_threads/#1-serial-crawler","title":"1. Serial Crawler","text":"<ul> <li>Depth-first traversal  </li> <li>A shared map tracks visited URLs  </li> <li>Simple and correct  </li> <li>Very slow \u2014 only fetches one page at a time  </li> </ul> <p>Adding <code>go</code> before recursive calls breaks correctness:</p> <ul> <li>Many threads may fetch same URL  </li> <li>Finishing detection becomes difficult</li> </ul>"},{"location":"distributedsystems/2_threads/#2-concurrent-crawler-with-mutex","title":"2. Concurrent Crawler with Mutex","text":""},{"location":"distributedsystems/2_threads/#how-it-works","title":"How it Works","text":"<ul> <li>Launch a goroutine per page</li> <li>Shared <code>fetched</code> map  </li> <li>Mutex ensures only one thread fetches each URL</li> </ul>"},{"location":"distributedsystems/2_threads/#why-the-mutex","title":"Why the Mutex?","text":""},{"location":"distributedsystems/2_threads/#1-avoid-logical-races","title":"1. Avoid Logical Races","text":"<p>Two threads may check the same URL at once: - Both see <code>fetched[url] == false</code> - Both fetch \u2192 wrong</p> <p>Mutex ensures: - One thread checks + sets at a time</p>"},{"location":"distributedsystems/2_threads/#2-avoid-map-corruption","title":"2. Avoid Map Corruption","text":"<p>Go maps are not thread-safe.</p>"},{"location":"distributedsystems/2_threads/#what-if-lock-is-removed","title":"What If Lock Is Removed?","text":"<ul> <li>Program may appear to work sometimes  </li> <li>But races still occur  </li> <li>Use the race detector:</li> </ul> <pre><code>go run -race crawler.go\n</code></pre>"},{"location":"distributedsystems/2_threads/#completion-detection-using-syncwaitgroup","title":"Completion Detection Using sync.WaitGroup","text":"<ul> <li><code>Add(n)</code> increments  </li> <li><code>Done()</code> decrements  </li> <li><code>Wait()</code> blocks until count is zero  </li> </ul> <p>Ensures main thread waits for all children.</p>"},{"location":"distributedsystems/2_threads/#3-concurrent-crawler-with-channels","title":"3. Concurrent Crawler with Channels","text":"<p>Channels provide:</p> <ul> <li>Communication  </li> <li>Synchronization  </li> </ul>"},{"location":"distributedsystems/2_threads/#channel-basics","title":"Channel Basics","text":"<pre><code>ch := make(chan int)\n\nch &lt;- x   // send (blocks)\ny := &lt;-ch // receive (blocks)\n</code></pre>"},{"location":"distributedsystems/2_threads/#coordinator-workers-model","title":"Coordinator + Workers Model","text":"<ul> <li>Coordinator creates workers via goroutines  </li> <li>Workers fetch a page and send resulting URLs via channel  </li> <li>Coordinator receives URLs, checks visited set</li> </ul>"},{"location":"distributedsystems/2_threads/#why-no-mutex","title":"Why No Mutex?","text":"<ul> <li>Shared state is only in coordinator  </li> <li>Workers never mutate shared maps  </li> <li>Therefore no races</li> </ul>"},{"location":"distributedsystems/2_threads/#channel-safety","title":"Channel Safety","text":"<p>Example:</p> <ul> <li>Worker creates slice of URLs</li> <li>Sends it to channel</li> <li>Coordinator reads it</li> </ul> <p>Safe because:</p> <ul> <li>Worker writes slice before send completes</li> <li>Coordinator reads slice after receive completes</li> </ul> <p>No overlap \u2192 no race.</p>"},{"location":"distributedsystems/2_threads/#why-some-sends-need-a-goroutine","title":"Why Some Sends Need a Goroutine?","text":"<p>Without a goroutine:</p> <ul> <li>send blocks  </li> <li>coordinator may not reach the receive  </li> <li>\u2192 deadlock</li> </ul>"},{"location":"distributedsystems/2_threads/#locks-vs-channels","title":"Locks vs Channels","text":"<p>Both are powerful. Use whichever matches intuition:</p> <ul> <li>State-focused logic \u2192 locks</li> <li>Communication-focused logic \u2192 channels</li> </ul> <p>In 6.5840 labs: - Use sharing + locks for state - Use channels, <code>sync.Cond</code>, or sleep-based polling for notifications</p>"},{"location":"distributedsystems/2_threads/#remote-procedure-call-rpc","title":"Remote Procedure Call (RPC)","text":"<p>RPC enables easy client-server communication.</p>"},{"location":"distributedsystems/2_threads/#goals","title":"Goals","text":"<ul> <li>Hide network details</li> <li>Provide a procedure-call interface</li> <li>Automatically marshal/unmarshal data</li> <li>Enable portability across systems</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-architecture","title":"RPC Architecture","text":"<pre><code>Client               Server\n  request ----&gt;\n            &lt;---- response\n</code></pre> <p>Software structure: </p><pre><code>Client App        Server Handlers\nClient Stubs      Dispatcher\nRPC Library  ---- RPC Library\n Network     ---- Network\n</code></pre><p></p>"},{"location":"distributedsystems/2_threads/#go-rpc-example-keyvalue-store","title":"Go RPC Example: Key/Value Store","text":"<p>Handlers: - <code>Put(key, value)</code> - <code>Get(key) -&gt; value</code></p>"},{"location":"distributedsystems/2_threads/#client-side","title":"Client Side","text":"<ul> <li>Use <code>Dial()</code> to connect  </li> <li>Call RPC using:</li> </ul> <pre><code>Call(\"KVServer.Get\", args, &amp;reply)\n</code></pre> <p>RPC library: - Marshals args - Sends request - Waits for reply - Unmarshals reply - Returns error if something went wrong  </p>"},{"location":"distributedsystems/2_threads/#server-side","title":"Server Side","text":"<p>Server must: 1. Declare a type with exported RPC methods 2. Register the type 3. Accept TCP connections and let RPC library handle them  </p> <p>RPC library:</p> <ul> <li>Creates goroutine per request  </li> <li>Unmarshals request  </li> <li>Dispatches handler  </li> <li>Marshals reply  </li> <li>Sends reply  </li> </ul> <p>Handlers must use locks since multiple RPCs run concurrently.</p>"},{"location":"distributedsystems/2_threads/#rpc-details","title":"RPC Details","text":""},{"location":"distributedsystems/2_threads/#binding","title":"Binding","text":"<p>Client must know <code>\"server:port\"</code> to dial.</p>"},{"location":"distributedsystems/2_threads/#marshalling-rules","title":"Marshalling Rules","text":"<ul> <li>Sends strings, arrays, structs, maps  </li> <li>Cannot send channels or functions  </li> <li>Only exported fields in structs are marshaled  </li> <li>Pointers are sent by copying the pointee</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-failures","title":"RPC Failures","text":"<p>Client may never get a reply:</p> <p>Could mean:</p> <ul> <li>Server never received request  </li> <li>Server crashed after executing  </li> <li>Reply lost in network  </li> <li>Network or server slow  </li> </ul> <p>RPC \u2260 local function call.</p>"},{"location":"distributedsystems/2_threads/#best-effort-rpc","title":"Best-Effort RPC","text":"<p>Algorithm: 1. Send request 2. Wait 3. If no reply, resend 4. After several tries \u2192 give up  </p>"},{"location":"distributedsystems/2_threads/#problems","title":"Problems","text":"<p>Example: </p><pre><code>Put(\"k\", 10)\nPut(\"k\", 20)\n</code></pre><p></p> <p>Retries can reorder or duplicate operations.</p>"},{"location":"distributedsystems/2_threads/#when-is-best-effort-ok","title":"When Is Best-Effort OK?","text":"<ul> <li>Read-only operations  </li> <li>Idempotent operations (safe to repeat)</li> </ul>"},{"location":"distributedsystems/2_threads/#at-most-once-semantics","title":"At-Most-Once Semantics","text":"<p>Go RPC provides:</p> <ul> <li>One TCP connection  </li> <li>Sends each request once  </li> <li>No retries \u2192 no duplicates  </li> </ul> <p>But:</p> <ul> <li>Errors returned on timeouts  </li> <li>Hard to build replicated fault-tolerant systems without retries  </li> </ul> <p>Later labs explore stronger semantics.</p>"},{"location":"informationtheory/1_intro_to_infotheory/","title":"Chapter 1 \u2014 Introduction to Information Theory (for Machine Learning)","text":""},{"location":"informationtheory/1_intro_to_infotheory/#chapter-1-introduction-to-information-theory-for-machine-learning","title":"Chapter 1 \u2014 Introduction to Information Theory (for Machine Learning)","text":"<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p> <ul> <li>loss functions (cross-entropy, NLL)</li> <li>representation learning and contrastive learning</li> <li>variational inference and VAEs</li> <li>generative modeling (GANs, flows, diffusion)</li> <li>reinforcement learning (entropy bonuses, policy KL constraints)</li> <li>model capacity, generalization, and bottlenecks</li> </ul> <p>This chapter introduces the core motivations and conceptual tools.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#1-why-information-theory-matters-for-ml","title":"1. Why Information Theory Matters for ML","text":"<p>Information theory answers questions fundamental to ML:</p> <ul> <li>How much uncertainty does a model reduce?</li> <li>How do we quantify the difference between two probability distributions?</li> <li>How do we measure dependence between variables?</li> <li>What is the maximum information a neural network layer can transmit?</li> <li>How do we formalize compression and generalization?</li> </ul> <p>In ML, information theory is not abstract mathematics \u2014 it provides the language for describing learning itself:</p> <p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p> <p>This viewpoint unifies:</p> <ul> <li>Maximum likelihood  </li> <li>Variational inference  </li> <li>Contrastive learning  </li> <li>GAN objectives  </li> <li>Representation learning  </li> <li>Reinforcement learning signal shaping  </li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#2-the-communication-view-shannons-formulation","title":"2. The Communication View (Shannon\u2019s Formulation)","text":"<p>A classical communication system consists of:</p> <ol> <li> <p>Source:    Generates data (symbols, images, text, states).</p> </li> <li> <p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p> </li> <li> <p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p> </li> <li> <p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p> </li> <li> <p>Receiver:   Obtains the final predictions or reconstructions.</p> </li> </ol> <p>Information theory studies:</p> <ul> <li>Limits of efficient communication  </li> <li>Optimal encoding and representation  </li> <li>Tradeoffs between compression and fidelity  </li> <li>Effect of noise on learnability</li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#3-the-uncertainty-view-shannonbayesian-perspective","title":"3. The Uncertainty View (Shannon\u2013Bayesian Perspective)","text":"<p>Information theory also quantifies uncertainty:</p> <ul> <li>More uncertainty \u2192 more information needed  </li> <li>Less uncertainty \u2192 easier prediction and compression  </li> </ul> <p>Key idea: Information is the reduction of uncertainty.</p> <p>In ML:</p> <ul> <li>Entropy measures label uncertainty  </li> <li>Cross-entropy measures model fit  </li> <li>KL divergence measures mismatch  </li> <li>Mutual information measures representation quality  </li> <li>ELBO measures how well a generative model explains data  </li> </ul> <p>Thus, learning and compression are mathematically the same problem.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#4-machine-learning-as-communication","title":"4. Machine Learning as Communication","text":"<p>Modern ML pipelines resemble a communication system:</p>"},{"location":"informationtheory/1_intro_to_infotheory/#data-encoder-latent-representation-decoder-output","title":"Data \u2192 Encoder \u2192 Latent Representation \u2192 Decoder \u2192 Output","text":"<p>Examples:</p> <ul> <li>Autoencoders / VAEs: compress \\(x\\) into \\(z\\), then reconstruct</li> <li>Transformers: compress sequences into features, decode predictions</li> <li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li> <li>GANs: learn generator distributions close to data distribution</li> <li>RL agents: compress sensory input into state representations</li> </ul> <p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#5-roadmap-for-this-web-book","title":"5. Roadmap for This Web-book","text":"<p>This web-book is structured to build information theory specifically for ML:</p> <ol> <li> <p>Entropy &amp; Self-Information    Foundations of uncertainty, coding length, and compression.</p> </li> <li> <p>Cross-Entropy &amp; Negative Log-Likelihood    Core ML loss; the bridge between probability and training objectives.</p> </li> <li> <p>KL Divergence &amp; f-Divergences    Quantifying model mismatch, VI, GAN divergences.</p> </li> <li> <p>Jensen\u2013Shannon &amp; Wasserstein Distances    GAN stability, geometric learning, distribution metrics.</p> </li> <li> <p>Mutual Information &amp; Estimation Bounds    Representation learning, contrastive learning, InfoNCE.</p> </li> <li> <p>Variational Inference &amp; ELBO    VAEs, Bayesian deep learning, posterior approximations.</p> </li> <li> <p>Information Bottleneck &amp; Representation Theory    Why deep networks compress, and how representations generalize.</p> </li> <li> <p>Summary &amp; Concept Map    Unifying view of entropy \u2192 KL \u2192 MI \u2192 VI \u2192 representation learning.</p> </li> </ol>"},{"location":"informationtheory/2_entropy/","title":"Chapter 2 \u2014 Entropy, Self-Information, Cross-Entropy &amp; Information Measures","text":""},{"location":"informationtheory/2_entropy/#chapter-2-entropy-self-information-cross-entropy-information-measures","title":"Chapter 2 \u2014 Entropy, Self-Information, Cross-Entropy &amp; Information Measures","text":""},{"location":"informationtheory/2_entropy/#1-self-information-surprisal","title":"1. Self-Information (Surprisal)","text":"<p>Self-information quantifies the surprise of observing an event.</p> <p>For an event with probability \\(p(x)\\):</p> \\[ I(x) = - \\log_2 p(x) \\] <p>Why the log?</p> <ul> <li>Additivity of independent events  </li> <li>Probability \u2192 information monotonicity  </li> <li>Log base 2 \u2192 units in bits  </li> <li>Log-likelihoods become additive \u2192 ML becomes convex (in many models)</li> </ul> <p>Interpretations:</p> <ul> <li>Unlikely events carry more information  </li> <li>Certain events carry zero information  </li> <li>Foundation of cross-entropy and negative log-likelihood  </li> </ul> <p>In ML:  </p> <ul> <li>The loss used in classification is simply the surprisal of the correct class.</li> </ul>"},{"location":"informationtheory/2_entropy/#2-entropy-expected-uncertainty","title":"2. Entropy \u2014 Expected Uncertainty","text":"<p>Entropy is the expected self-information:</p> \\[ H(X) = -\\sum_x p(x) \\log p(x) \\] <p>Entropy measures:</p> <ul> <li>Uncertainty  </li> <li>Randomness  </li> <li>Compressibility  </li> <li>Difficulty of prediction  </li> </ul>"},{"location":"informationtheory/2_entropy/#key-properties","title":"Key properties:","text":"<ul> <li>\\(H(X) = 0\\) if a variable is deterministic  </li> <li>Maximum when distribution is uniform  </li> <li>Upper bound on achievable compression (Shannon)</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation","title":"ML Interpretation:","text":"<ul> <li>High entropy labels \u2192 noisy dataset \u2192 harder learning</li> <li>Activation entropy reflects network expressiveness</li> <li>Entropy of output distribution measures model confidence</li> <li>Entropy regularization improves exploration in RL</li> </ul>"},{"location":"informationtheory/2_entropy/#3-differential-entropy-continuous-entropy","title":"3. Differential Entropy (Continuous Entropy)","text":"<p>For continuous variables:</p> \\[ h(X) = -\\int p(x) \\log p(x)\\,dx \\] <p>Important differences:</p> <ul> <li>Can be negative</li> <li>Not invariant under reparameterization</li> <li>Not comparable between different coordinate systems</li> </ul>"},{"location":"informationtheory/2_entropy/#why-it-matters-in-ml","title":"Why it matters in ML:","text":"<ul> <li>VAEs use continuous latent variables \\(z\\)</li> <li>Flows and diffusion models use continuous densities</li> <li>Score-based models estimate gradients of log-densities, not densities directly</li> </ul> <p>Differential entropy is not the same thing as Shannon entropy \u2014 a common source of confusion.</p>"},{"location":"informationtheory/2_entropy/#4-joint-conditional-and-total-entropy","title":"4. Joint, Conditional, and Total Entropy","text":""},{"location":"informationtheory/2_entropy/#joint-entropy","title":"Joint entropy:","text":"\\[ H(X,Y) = -\\sum_{x,y} p(x,y)\\log p(x,y) \\]"},{"location":"informationtheory/2_entropy/#conditional-entropy","title":"Conditional entropy:","text":"\\[ H(Y|X) = -\\sum_{x,y} p(x,y)\\log p(y|x) \\] <p>Interpretation:</p> <ul> <li>Average residual uncertainty in \\(Y\\) after observing \\(X\\)</li> </ul>"},{"location":"informationtheory/2_entropy/#chain-rule-of-entropy","title":"Chain rule of entropy:","text":"\\[ H(X,Y) = H(X) + H(Y|X) \\] <p>This rule is foundational for:</p> <ul> <li>Autoregressive modeling  </li> <li>Sequence modeling  </li> <li>Transformers (predictive factorization)  </li> <li>Bayesian networks  </li> </ul>"},{"location":"informationtheory/2_entropy/#5-cross-entropy-coding-p-using-q","title":"5. Cross-Entropy \u2014 Coding \\(p\\) Using \\(q\\)","text":"<p>Cross-entropy is the expected surprise under model \\(q\\):</p> \\[ H(p, q) = -\\sum_x p(x)\\log q(x) \\]"},{"location":"informationtheory/2_entropy/#crucial-identity","title":"Crucial identity:","text":"\\[ H(p, q) = H(p) + D_{\\text{KL}}(p\\|q) \\] <p>Meaning:</p> <ul> <li>True entropy + penalty for using the wrong distribution</li> <li>Cross-entropy \u2265 entropy</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation_1","title":"ML Interpretation:","text":"<p>Cross-entropy = Negative Log Likelihood:</p> \\[ \\mathcal{L} = - \\log q(y_{\\text{true}}) \\] <p>This powers:</p> <ul> <li>Softmax classifiers  </li> <li>Logistic regression  </li> <li>Transformers (next-token prediction)  </li> <li>Language models (autoregressive LM)  </li> <li>Image segmentation (pixel-wise CE)  </li> </ul> <p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>"},{"location":"informationtheory/2_entropy/#6-perplexity-entropy-in-language-modeling","title":"6. Perplexity \u2014 Entropy in Language Modeling","text":"<p>Perplexity is:</p> \\[ \\text{PPL} = 2^{H} \\] <p>Interpretation:</p> <ul> <li>The \u201ceffective vocabulary size\u201d the model thinks it must guess from</li> <li>Lower perplexity = better language model</li> </ul> <p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>"},{"location":"informationtheory/2_entropy/#7-mutual-information-information-shared-between-variables","title":"7. Mutual Information \u2014 Information Shared Between Variables","text":"\\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)) \\] <p>MI measures:</p> <ul> <li>How much knowing \\(X\\) tells us about \\(Y\\)</li> <li>Reduction in entropy of one variable after observing the other</li> </ul>"},{"location":"informationtheory/2_entropy/#equivalent-forms","title":"Equivalent forms:","text":"\\[ I(X;Y) = H(X) - H(X|Y) \\] \\[ I(X;Y) = H(X) + H(Y) - H(X,Y) \\] <p>MI links entropy and KL divergence into a unified measure of dependence.</p>"},{"location":"informationtheory/2_entropy/#why-mi-is-critical-in-ml","title":"Why MI is critical in ML:","text":"<ul> <li>Representation learning (maximize MI with labels)</li> <li>Contrastive learning (InfoNCE is a lower bound to MI)</li> <li>InfoGAN (maximize MI between latent code and output)</li> <li>Feature selection (choose features with highest MI to labels)</li> <li>Stochastic encoders control MI with constraints</li> </ul>"},{"location":"informationtheory/2_entropy/#8-the-data-processing-inequality-dpi","title":"8. The Data Processing Inequality (DPI)","text":"<p>If:</p> \\[ X \\rightarrow Z \\rightarrow Y \\] <p>is a Markov chain, then:</p> \\[ I(X;Y) \\le I(X;Z) \\] <p>Meaning:</p> <ul> <li>Processing or compressing data cannot add information</li> <li>Neural networks cannot create information about the input   \u2014 they can only discard or transform it</li> </ul> <p>ML relevance:</p> <ul> <li>Explains why deeper layers become more task-specialized  </li> <li>Supports the Information Bottleneck theory in deep learning  </li> <li>Ensures that any learned representation is bounded by input information  </li> <li>Helps analyze generalization and compression in deep nets</li> </ul>"},{"location":"informationtheory/2_entropy/#9-entropy-in-neural-networks","title":"9. Entropy in Neural Networks","text":"<p>Entropy plays multiple roles in deep learning:</p>"},{"location":"informationtheory/2_entropy/#output-entropy","title":"Output entropy","text":"<p>Low entropy \u2192 confident predictions High entropy \u2192 uncertainty</p>"},{"location":"informationtheory/2_entropy/#entropy-of-hidden-representations","title":"Entropy of hidden representations","text":"<ul> <li>Early layers: reduce entropy (denoising)  </li> <li>Deep layers: compress irrelevant information  </li> <li>Good representations retain low entropy but high MI with labels</li> </ul>"},{"location":"informationtheory/2_entropy/#entropy-regularization-in-rl","title":"Entropy regularization in RL","text":"<p>  encourages exploration.</p>"},{"location":"informationtheory/2_entropy/#dropout-increases-entropy","title":"Dropout increases entropy","text":"<p>forcing models to encode more robust representations.</p>"},{"location":"informationtheory/3_KL/","title":"3 KL","text":"<p>Chapter 3 \u2014 KL Divergence, f-Divergences, Jensen\u2013Shannon Divergence, and Wasserstein Distance</p> <p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>"},{"location":"informationtheory/3_KL/#1-kl-divergence-measuring-distribution-mismatch","title":"1. KL Divergence: Measuring Distribution Mismatch","text":"<p>The Kullback\u2013Leibler (KL) divergence measures how different two probability distributions are. For distributions \\(p\\) and \\(q\\):</p> \\[ D_{\\text{KL}}(p\\|q) = \\sum_x p(x)\\log\\frac{p(x)}{q(x)}. \\] <p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from \\(p\\) using a code optimized for \\(q\\). If \\(q\\) assigns very low probability to events that occur frequently under \\(p\\), the KL divergence becomes large.</p>"},{"location":"informationtheory/3_KL/#key-properties-of-kl-divergence","title":"Key properties of KL divergence","text":"<ol> <li> <p>Non-negative </p> </li> <li> <p>Zero only when the two distributions are identical.</p> </li> <li> <p>Asymmetric </p> </li> <li> <p>Not a true metric, since it fails the triangle inequality.</p> </li> <li> <p>Can be infinite when \\(p(x) &gt; 0\\) but \\(q(x) = 0\\).    This is a crucial issue in generative modeling, where such mismatches occur frequently.</p> </li> </ol>"},{"location":"informationtheory/3_KL/#2-kl-divergence-in-machine-learning","title":"2. KL Divergence in Machine Learning","text":"<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>"},{"location":"informationtheory/3_KL/#21-maximum-likelihood-as-forward-kl-minimization","title":"2.1 Maximum likelihood as forward KL minimization","text":"<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p> \\[ \\theta^* = \\arg\\min_\\theta D_{\\text{KL}}(p_{\\text{data}} \\,\\|\\, q_\\theta). \\] <p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p> <p>This produces mode-covering behavior, which is characteristic of:</p> <ul> <li>normalizing flows  </li> <li>autoregressive models  </li> <li>density estimation models trained via log-likelihood  </li> </ul>"},{"location":"informationtheory/3_KL/#22-kl-divergence-in-variational-inference-vi","title":"2.2 KL divergence in variational inference (VI)","text":"<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior \\(q(z|x)\\) and the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Since the true posterior is typically intractable, VAEs approximate this with:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p> <p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative \u201csafe\u201d solution.</p>"},{"location":"informationtheory/3_KL/#23-kl-divergence-in-reinforcement-learning","title":"2.3 KL divergence in reinforcement learning","text":"<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p> \\[ D_{\\text{KL}}(\\pi_{\\text{old}} \\,\\|\\, \\pi_{\\text{new}}). \\] <p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>"},{"location":"informationtheory/3_KL/#24-kl-divergence-in-distillation-and-compression","title":"2.4 KL divergence in distillation and compression","text":"<p>KL divergence compares two probability distributions directly and is used for:</p> <ul> <li>teacher\u2013student distillation  </li> <li>compressing large models into smaller ones  </li> <li>aligning probability distributions across layers  </li> <li>calibrating output probabilities  </li> </ul> <p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>"},{"location":"informationtheory/3_KL/#3-understanding-kl-behavior-mode-covering-vs-mode-seeking","title":"3. Understanding KL Behavior: Mode Covering vs. Mode Seeking","text":"<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>"},{"location":"informationtheory/3_KL/#forward-kl-d_textklpq","title":"Forward KL: \\(D_{\\text{KL}}(p\\|q)\\)","text":"<p>(Used in maximum likelihood, flows \u2192 mode covering)</p> <p>Forward KL asks whether the model \\(q\\) assigns sufficient probability wherever the data distribution \\(p\\) has mass:</p> <p>\u201cDoes the model assign enough probability to every place where the data occurs?\u201d</p> <p>If \\(q\\) misses even a small region where \\(p\\) has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p> <p>Result: mode covering The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p> <p>Flows and MLE-based models display this behavior.</p>"},{"location":"informationtheory/3_KL/#reverse-kl-d_textklqp","title":"Reverse KL: \\(D_{\\text{KL}}(q\\|p)\\)","text":"<p>(Used in VI, VAEs, GAN-like behavior \u2192 mode seeking)</p> <p>Reverse KL asks the opposite question:</p> <p>\u201cIs the model placing probability in places where the data distribution is very small or zero?\u201d</p> <p>Reverse KL heavily penalizes placing mass in low-density regions of \\(p\\), making the model conservative.</p> <p>Result: mode seeking The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p> <p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>"},{"location":"informationtheory/3_KL/#4-f-divergences-a-unified-family-of-divergences","title":"4. f-Divergences: A Unified Family of Divergences","text":"<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function \\(f\\):</p> \\[ D_f(p\\|q) = \\sum_x q(x)\\, f\\!\\left(\\frac{p(x)}{q(x)}\\right). \\]"},{"location":"informationtheory/3_KL/#5-jensenshannon-divergence-the-original-gan-divergence","title":"5. Jensen\u2013Shannon Divergence: The Original GAN Divergence","text":"<p>The Jensen\u2013Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p> \\[ \\text{JS}(p\\|q) = \\frac12 D_{\\text{KL}}(p\\|m) + \\frac12 D_{\\text{KL}}(q\\|m) \\] <p>where the mixture is:</p> \\[ m = \\frac12(p+q). \\] <p>JS divergence is symmetric and always lies between 0 and \\(\\log 2\\).</p>"},{"location":"informationtheory/3_KL/#why-js-appears-in-gans","title":"Why JS appears in GANs","text":"<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p> \\[ \\text{JS}(p\\|q) - \\log 2. \\] <p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>"},{"location":"informationtheory/3_KL/#6-why-js-divergence-causes-gan-instability","title":"6. Why JS Divergence Causes GAN Instability","text":"<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of \\(p\\) and \\(q\\) are disjoint:</p> \\[ \\text{JS}(p\\|q) = \\log 2. \\] <p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p> <p>Consequences:</p> <ol> <li>The discriminator immediately becomes perfect.  </li> <li>The generator stops receiving meaningful gradients.  </li> <li>Training often collapses, oscillates, or diverges.  </li> </ol> <p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>"},{"location":"informationtheory/3_KL/#7-total-variation-and-hellinger-distances","title":"7. Total Variation and Hellinger Distances","text":"<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>"},{"location":"informationtheory/3_KL/#71-total-variation-tv-distance","title":"7.1 Total Variation (TV) Distance","text":"\\[ \\text{TV}(p,q) = \\frac12\\sum_x |p(x)-q(x)|. \\] <p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform \\(p\\) into \\(q\\).</p> <p>Applications in ML:</p> <ul> <li>Robustness under distribution shift  </li> <li>Generalization bounds (PAC-Bayes)  </li> <li>Fairness and safety  </li> </ul>"},{"location":"informationtheory/3_KL/#72-hellinger-distance","title":"7.2 Hellinger Distance","text":"\\[ H^2(p,q) = \\frac12 \\sum_x\\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^2. \\] <p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p> <p>Uses in ML include:</p> <ul> <li>Robust statistics  </li> <li>Domain adaptation  </li> <li>Generalization theory  </li> <li>Some GAN formulations  </li> </ul>"},{"location":"informationtheory/3_KL/#8-wasserstein-distance-geometry-of-probability-distributions","title":"8. Wasserstein Distance: Geometry of Probability Distributions","text":"<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>"},{"location":"informationtheory/3_KL/#81-primal-form-earth-mover-interpretation","title":"8.1 Primal form (Earth Mover interpretation)","text":"\\[ W(p,q) = \\inf_{\\gamma \\in \\Gamma(p,q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]. \\] <p>It seeks the transport plan \\(\\gamma\\) requiring the least expected effort to turn \\(p\\) into \\(q\\).</p>"},{"location":"informationtheory/3_KL/#82-dual-form-used-in-wgan","title":"8.2 Dual form (used in WGAN)","text":"\\[ W(p,q) = \\sup_{\\|f\\|_L\\le 1} \\left(\\mathbb{E}_p[f(x)]      - \\mathbb{E}_q[f(x)]\\right). \\] <p>GANs implement \\(f\\) as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>"},{"location":"informationtheory/3_KL/#83-why-wasserstein-solves-gan-instability","title":"8.3 Why Wasserstein solves GAN instability","text":"<p>Wasserstein distance has several advantages:</p> <ul> <li>Provides informative gradients even with no overlap  </li> <li>Reflects the actual geometry of the data space  </li> <li>Avoids the saturation and vanishing gradients of JS divergence  </li> <li>Works reliably in high-dimensional spaces  </li> </ul> <p>These properties make Wasserstein GANs far more stable than classical GANs.</p>"},{"location":"informationtheory/3_KL/#84-wgan-gp-gradient-penalty","title":"8.4 WGAN-GP: Gradient Penalty","text":"<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p> \\[ \\lambda(\\|\\nabla_x f(x)\\|_2 - 1)^2. \\] <p>This produces smoother and more stable training compared to weight clipping.</p>"},{"location":"informationtheory/3_KL/#9-divergence-versus-distance","title":"9. Divergence versus Distance","text":"<p>Divergences such as KL and JS:</p> <ul> <li>may be infinite  </li> <li>are asymmetric  </li> <li>do not behave well when distributions have disjoint support  </li> </ul> <p>Distances such as Wasserstein, TV, and Hellinger:</p> <ul> <li>are symmetric  </li> <li>obey triangle inequality  </li> <li>remain meaningful under distribution shift  </li> </ul> <p>In machine learning:</p> <ul> <li>Divergences are useful for inference and likelihood  </li> <li>Distances are useful for generative modeling and geometry  </li> </ul>"},{"location":"informationtheory/3_KL/#10-why-divergences-fail-in-high-dimensions","title":"10. Why Divergences Fail in High Dimensions","text":"<p>In high-dimensional spaces:</p> <ul> <li>Real and generated samples rarely overlap  </li> <li>KL divergence often becomes infinite  </li> <li>JS divergence becomes flat  </li> <li>Gradients vanish  </li> </ul> <p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p> <p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p> <p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen\u2013Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p> <p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p>"},{"location":"informationtheory/4_bayes/","title":"4 bayes","text":"<p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p> <p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>"},{"location":"informationtheory/4_bayes/#1-bayes-rule","title":"1. Bayes\u2019 Rule","text":"<p>Bayes\u2019 theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable \\(z\\) and an observed variable \\(x\\):</p> \\[ p(z|x) = \\frac{p(x|z)\\,p(z)}{p(x)}. \\] <p>Each term has a clear interpretation.</p> <ul> <li>\\(p(z)\\): prior belief about the unknown variable  </li> <li>\\(p(x|z)\\): likelihood of observing \\(x\\) given \\(z\\) </li> <li>\\(p(x)\\): marginal likelihood or evidence  </li> <li>\\(p(z|x)\\): posterior distribution after observing data  </li> </ul> <p>Bayesian inference is the task of computing \\(p(z|x)\\).</p>"},{"location":"informationtheory/4_bayes/#2-priors-encoding-assumptions-about-hidden-variables","title":"2. Priors: Encoding Assumptions About Hidden Variables","text":"<p>The prior \\(p(z)\\) expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>"},{"location":"informationtheory/4_bayes/#21-regularization","title":"2.1 Regularization","text":"<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields \\(L_2\\) regularization.</p>"},{"location":"informationtheory/4_bayes/#22-structural-assumptions","title":"2.2 Structural assumptions","text":"<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>"},{"location":"informationtheory/4_bayes/#23-uncertainty","title":"2.3 Uncertainty","text":"<p>The prior makes explicit that before observing data, we do not know the true value of \\(z\\).</p>"},{"location":"informationtheory/4_bayes/#24-generative-modeling","title":"2.4 Generative modeling","text":"<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>"},{"location":"informationtheory/4_bayes/#3-likelihood-connecting-latent-variables-to-observed-data","title":"3. Likelihood: Connecting Latent Variables to Observed Data","text":"<p>The likelihood \\(p(x|z)\\) describes how the data are generated from latent causes. In many generative models:</p> <ul> <li>\\(z\\) represents latent structure  </li> <li>\\(x\\) represents an image, time series, or text  </li> <li>\\(p(x|z)\\) is parameterized by a neural network decoder  </li> </ul> <p>The likelihood term encourages the latent variable \\(z\\) to explain the observed data.</p>"},{"location":"informationtheory/4_bayes/#4-the-posterior-what-we-really-want-to-compute","title":"4. The Posterior: What We Really Want to Compute","text":"<p>The goal of Bayesian inference is the posterior:</p> \\[ p(z|x) = \\frac{p(x|z)p(z)}{p(x)}. \\] <p>The posterior expresses how our belief about \\(z\\) changes after seeing \\(x\\). It incorporates both:</p> <ul> <li>prior knowledge  </li> <li>evidence from data  </li> </ul> <p>Unfortunately, computing this posterior is usually intractable.</p>"},{"location":"informationtheory/4_bayes/#5-why-exact-inference-is-hard","title":"5. Why Exact Inference Is Hard","text":"<p>The denominator in Bayes\u2019 rule is the marginal likelihood:</p> \\[ p(x) = \\int p(x,z)\\,dz. \\] <p>This integral is often impossible to evaluate directly because:</p> <ul> <li>the latent space \\(z\\) can be high-dimensional  </li> <li>the joint distribution \\(p(x,z)\\) may involve a complex neural network  </li> <li>the integral has no analytic form  </li> </ul> <p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>"},{"location":"informationtheory/4_bayes/#6-maximum-a-posteriori-map-vs-full-bayesian-inference","title":"6. Maximum a Posteriori (MAP) vs Full Bayesian Inference","text":"<p>There are two kinds of Bayesian computation.</p>"},{"location":"informationtheory/4_bayes/#61-map-estimation","title":"6.1 MAP estimation","text":"<p>MAP finds the single most likely value of \\(z\\):</p> \\[ z_{\\text{MAP}} = \\arg\\max_z\\, p(z|x). \\] <p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>"},{"location":"informationtheory/4_bayes/#62-full-posterior-inference","title":"6.2 Full posterior inference","text":"<p>The full posterior \\(p(z|x)\\) describes a distribution over possible values of \\(z\\), reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>"},{"location":"informationtheory/4_bayes/#7-bayesian-latent-variable-models","title":"7. Bayesian Latent-Variable Models","text":"<p>Many generative models are Bayesian latent-variable models with:</p> <ol> <li> <p>a prior over latent variables </p> </li> <li> <p>a conditional likelihood </p> </li> <li> <p>a posterior </p> </li> </ol> <p>Examples include:</p> <ul> <li>VAEs  </li> <li>mixture models  </li> <li>topic models  </li> <li>probabilistic PCA  </li> <li>diffusion models (in a specific sense)  </li> </ul> <p>Bayesian inference is the foundation of these models.</p>"},{"location":"informationtheory/4_bayes/#8-the-evidence-and-its-importance","title":"8. The Evidence and Its Importance","text":"<p>The marginal likelihood, also called the evidence:</p> \\[ p(x) = \\int p(x,z)\\,dz \\] <p>plays several roles:</p> <ul> <li>It normalizes the posterior.  </li> <li>It evaluates how well a model explains data.  </li> <li>It is used in Bayesian model comparison.  </li> <li>Its logarithm appears in training objectives for VAEs and diffusion models.</li> </ul> <p>Maximizing evidence corresponds to learning a model that explains the data well.</p>"},{"location":"informationtheory/4_bayes/#9-bayesian-interpretation-of-kl-divergence","title":"9. Bayesian Interpretation of KL Divergence","text":"<p>KL divergence naturally appears when comparing an approximate posterior \\(q(z|x)\\) with the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Minimizing this KL divergence means making the approximation \\(q\\) as close as possible to the exact posterior.</p> <p>This forms the basis of variational inference.</p>"},{"location":"informationtheory/4_bayes/#10-why-we-need-variational-inference","title":"10. Why We Need Variational Inference","text":"<p>Because the true posterior is intractable, we introduce a simpler distribution \\(q(z|x)\\) and optimize it to approximate \\(p(z|x)\\).</p> <p>We cannot compute:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)) \\] <p>directly, because \\(p(z|x)\\) depends on \\(p(x)\\), which is the intractable integral.</p> <p>Variational inference resolves this by rewriting \\(\\log p(x)\\) and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p> <p>This is the topic of the next chapter.</p> <p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes\u2019 rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p> <p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p>"},{"location":"informationtheory/5_mc_intro/","title":"5 mc intro","text":"<p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}, \\qquad  p(x) = \\int p(x,z)\\,dz. \\] <p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p> <p>This section introduces four major families of methods:</p> <ol> <li>complete enumeration  </li> <li>Laplace approximation  </li> <li>Monte Carlo methods  </li> <li>variational methods  </li> </ol> <p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>"},{"location":"informationtheory/5_mc_intro/#1-complete-enumeration","title":"1. Complete Enumeration","text":"<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p> \\[ p(x) = \\sum_z p(x,z) \\quad \\text{or} \\quad p(x) = \\int p(x,z)\\,dz. \\] <p>This is feasible only when:</p> <ul> <li>the latent variable is low dimensional  </li> <li>the domain is small or discrete  </li> <li>the joint distribution has a simple closed form  </li> </ul> <p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>"},{"location":"informationtheory/5_mc_intro/#2-laplace-approximation","title":"2. Laplace Approximation","text":"<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p> <p>Given a posterior</p> \\[ p(z|x) \\propto p(x,z), \\] <p>the Laplace approximation fits a Gaussian distribution</p> \\[ q(z|x) \\approx \\mathcal{N}(z_{\\text{MAP}}, H^{-1}), \\] <p>where:</p> <ul> <li>\\(z_{\\text{MAP}}\\) is the mode of \\(p(z|x)\\) </li> <li>\\(H\\) is the Hessian of \\(-\\log p(z|x)\\) at the mode  </li> </ul> <p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>"},{"location":"informationtheory/5_mc_intro/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":"<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p> \\[ \\mathbb{E}_{p(z|x)}[f(z)]  \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\qquad z_i \\sim p(z|x). \\] <p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p> <p>Sampling strategies fall into two groups:</p> <ul> <li>independent sampling  </li> <li>Markov chain\u2013based sampling (MCMC)  </li> </ul> <p>The next chapter explains Monte Carlo and sampling methods in detail.</p>"},{"location":"informationtheory/5_mc_intro/#4-variational-methods","title":"4. Variational Methods","text":"<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from \\(p(z|x)\\), we introduce a distribution \\(q(z|x)\\) and optimize it to be close to the true posterior. The objective is to minimize</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x) + D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>"},{"location":"informationtheory/5_mc_intro/#summary","title":"Summary","text":"<p>Approximate inference methods can be understood as four major strategies:</p> <ul> <li>complete enumeration: exact but rarely feasible  </li> <li>Laplace approximation: fast Gaussian approximation near the mode  </li> <li>Monte Carlo methods: sampling-based numerical estimation  </li> <li>variational methods: optimization-based posterior approximation  </li> </ul> <p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p>"},{"location":"informationtheory/6_mc/","title":"6 mc","text":"<p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p> <p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>"},{"location":"informationtheory/6_mc/#1-the-goal-of-sampling","title":"1. The Goal of Sampling","text":"<p>Many problems require computing expectations of the form</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\int f(z)\\,p(z)\\,dz, \\] <p>or evaluating posterior quantities such as</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p> <p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>"},{"location":"informationtheory/6_mc/#2-independent-sampling","title":"2. Independent Sampling","text":"<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p> <p>These methods work best when:</p> <ul> <li>sampling directly from \\(p(z)\\) is tractable  </li> <li>the distribution is low-dimensional  </li> <li>the support is simple (e.g., Gaussian, uniform)  </li> </ul>"},{"location":"informationtheory/6_mc/#21-direct-sampling","title":"2.1 Direct Sampling","text":"<p>When the distribution has an invertible CDF \\(F(z)\\):</p> <ol> <li>sample \\(u \\sim \\text{Uniform}(0,1)\\) </li> <li>compute \\(z = F^{-1}(u)\\) </li> </ol> <p>This yields exact samples. It is commonly used in:</p> <ul> <li>uniform sampling  </li> <li>exponential distributions  </li> <li>simple discrete distributions  </li> </ul>"},{"location":"informationtheory/6_mc/#22-importance-sampling","title":"2.2 Importance Sampling","text":"<p>When sampling from \\(p(z)\\) is difficult but evaluating \\(p(z)\\) is easy, importance sampling draws samples from a proposal distribution \\(q(z)\\) and reweights them:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\mathbb{E}_{q(z)}\\left[f(z)\\frac{p(z)}{q(z)}\\right]. \\] <p>The weights</p> <p>   \u200b</p> <p>correct for the fact that samples were drawn from \\(q\\) instead of \\(p\\).</p> <p>The main challenge is weight variability. If \\(q(z)\\) does not closely match \\(p(z)\\), the ratio \\(p(z)/q(z)\\) becomes extremely uneven: most samples have tiny weights, while a few samples have very large weights. This produces high-variance estimates because the estimator becomes dominated by a handful of rare but extremely influential samples. In high-dimensional spaces, designing a proposal \\(q(z)\\) that covers the important regions of \\(p(z)\\) is especially difficult, making importance sampling unreliable unless the proposal distribution is carefully chosen.</p>"},{"location":"informationtheory/6_mc/#23-rejection-sampling","title":"2.3 Rejection Sampling","text":"<p>Rejection sampling draws exact samples from a target distribution \\(p(z)\\) by using a simpler proposal distribution \\(q(z)\\) and accepting or rejecting candidate samples based on how well \\(q\\) covers \\(p\\). The method requires a constant \\(M\\) such that  </p> <p>This condition ensures that \\(Mq(z)\\) forms an envelope that completely contains \\(p(z)\\).</p> <p>Procedure:</p> <ol> <li>sample \\(z \\sim q(z)\\) </li> <li>accept with probability \\(\\frac{p(z)}{M q(z)}\\) </li> </ol> <p>If accepted, \\(z\\) is guaranteed to be an exact draw from \\(p\\).</p> <p>Rejection sampling is conceptually simple and does not distort the target distribution, but it becomes inefficient in many settings. When \\(q(z)\\) is a poor match for \\(p(z)\\), the constant \\(M\\) must be large, which means that most samples are rejected. In high-dimensional spaces, the mismatch between \\(p\\) and \\(q\\) typically worsens exponentially, making the acceptance probability extremely small. As a result, rejection sampling is rarely practical for modern high-dimensional machine-learning models, although it remains useful in low-dimensional problems or when \\(q\\) can be chosen to closely match \\(p\\).</p>"},{"location":"informationtheory/6_mc/#3-monte-carlo-estimation","title":"3. Monte Carlo Estimation","text":"<p>Monte Carlo approximates expectations by:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\quad z_i \\sim p(z). \\] <p>Key properties:</p> <ul> <li>error scales as \\(\\mathcal{O}(1/\\sqrt{N})\\) </li> <li>works in high dimensions  </li> <li>accuracy depends on sampling quality  </li> </ul> <p>Monte Carlo is the backbone of almost all probabilistic computation.</p>"},{"location":"informationtheory/6_mc/#4-markov-chain-monte-carlo-mcmc","title":"4. Markov Chain Monte Carlo (MCMC)","text":"<p>When sampling directly from \\(p(z)\\) is hard, Markov Chain Monte Carlo constructs a Markov chain</p> \\[ z_1 \\to z_2 \\to z_3 \\to \\cdots \\] <p>whose stationary distribution is \\(p(z)\\).</p> <p>After a burn-in period, samples approximate \\(p(z)\\) even if individual states are dependent.</p> <p>MCMC is widely applicable because it does not require the normalization constant of \\(p(z)\\):</p> \\[ p(z|x) \\propto p(x,z). \\]"},{"location":"informationtheory/6_mc/#41-metropolishastings-algorithm","title":"4.1 Metropolis\u2013Hastings Algorithm","text":"<p>The Metropolis\u2013Hastings (MH) algorithm constructs a Markov chain whose stationary distribution is the target distribution \\(p(z)\\), even when \\(p(z)\\) is known only up to a proportionality constant. This makes MH suitable for Bayesian inference, where the posterior is often available only in unnormalized form:</p> \\[p(z \\mid x) \\propto p(x, z)\\] <p>MH works by proposing a new point based on the current state and then accepting or rejecting it according to how well it aligns with the target distribution.</p> <p>Given a current sample \\(z\\), the algorithm proceeds as follows:</p> <ol> <li> <p>propose a new sample \\(z'\\) using a proposal distribution \\(z' \\sim q(z' \\mid z)\\).</p> </li> <li> <p>compute the acceptance probability       </p> </li> <li> <p>accept the proposal with probability \\(\\alpha(z,z')\\) otherwise remain at the current state If accepted, set \\(z_{t+1} = z'\\), otherwise keep \\(z_{t+1} = z\\).</p> </li> </ol> <p>This simple rule ensures that the Markov chain satisfies detailed balance and converges to the desired distribution \\(p(z)\\).</p> <p>Metropolis\u2013Hastings is flexible and works with virtually any distribution from which we can evaluate \\(p(z)\\) up to a constant. However, its efficiency depends strongly on the proposal distribution. If the proposal steps are too small, the chain performs a random walk and mixes slowly. If the steps are too large, most proposals are rejected. Choosing or adapting the proposal distribution is therefore crucial for performance, especially in high-dimensional settings.</p>"},{"location":"informationtheory/6_mc/#42-gibbs-sampling","title":"4.2 Gibbs Sampling","text":"<p>Gibbs sampling is a special case of MCMC designed for multivariate distributions where sampling from the full conditional distributions is easy. Instead of proposing a new state and accepting or rejecting it, Gibbs sampling updates one variable at a time by drawing directly from its exact conditional distribution.</p> <p>For a latent vector:</p> \\[z = (z_1, z_2, \\dots, z_d)\\] <p>a Gibbs update for coordinate \\(i\\) samples:</p> \\[ z_i \\sim p(z_i \\mid z_{-i}). \\] <p>where \\(z_{-i}\\) denotes all components except \\(z_i\\).</p> <p>By cycling through all coordinates repeatedly, the Markov chain eventually converges to the target joint distribution \\(p(z)\\).</p> <p>The key requirement is that each full conditional distribution</p> \\[p(z_i \\mid z_{-i})\\] <p>must be analytically tractable and easy to sample from. When this holds, Gibbs sampling is simple to implement and avoids the accept\u2013reject step of Metropolis\u2013Hastings.</p> <p>However, Gibbs sampling can mix slowly when variables are strongly correlated, since updating one coordinate at a time may explore the space inefficiently. Gibbs sampling is widely used in models where conditional distributions are naturally available, including:</p> <ul> <li>topic models such as Latent Dirichlet Allocation (LDA)</li> <li>hidden Markov models</li> <li>Gaussian graphical models</li> <li>Bayesian networks with conjugate priors</li> </ul>"},{"location":"informationtheory/6_mc/#43-slice-sampling","title":"4.3 Slice Sampling","text":"<p>Slice sampling is an MCMC method that avoids choosing a proposal distribution by sampling uniformly from the region (the slice) where the probability density is above a randomly chosen threshold. Given the current point \\(z\\), slice sampling introduces an auxiliary variable \\(u\\):</p> <ol> <li> <p>Draw a height</p> <p>\\(\\(u \\sim \\text{Uniform}(0,\\, p(z))\\)\\)</p> </li> <li> <p>Define the horizontal slice</p> <p>\\(\\(S = \\{ z' : p(z') &gt; u \\}\\)\\)</p> </li> <li> <p>Sample the next state</p> <p>\\(\\(z' \\sim \\text{Uniform}(S)\\)\\)</p> </li> </ol> <p>This procedure constructs a Markov chain whose stationary distribution is \\(p(z)\\). Intuitively, the algorithm first chooses a horizontal level \\(u\\) below the current density value and then samples uniformly from the region of the density that lies above this level.</p> <p>Slice sampling adapts naturally to the local geometry of the target distribution: narrow peaks produce narrow slices, and broad regions produce wide slices, without requiring manual tuning of proposal scales. In practice, slice sampling often mixes better than basic random-walk proposals, especially when the target density varies in amplitude across different regions.</p> <p>However, slice sampling requires an efficient way to identify or approximate the slice region, which may be challenging in high-dimensional or multimodal settings.</p>"},{"location":"informationtheory/6_mc/#5-reducing-random-walk-behaviour","title":"5. Reducing Random-Walk Behaviour","text":"<p>Basic MCMC algorithms such as Metropolis\u2013Hastings often move in small, local steps and therefore explore the state space slowly. This random-walk behaviour leads to poor mixing and long autocorrelation times, especially in high-dimensional or strongly correlated distributions.</p> <p>Several advanced MCMC techniques attempt to reduce random-walk dynamics by proposing more informed or distant moves.</p>"},{"location":"informationtheory/6_mc/#51-hamiltonian-monte-carlo-hmc","title":"5.1 Hamiltonian Monte Carlo (HMC)","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving smoothly through the probability landscape.</p> <p>Let \\(z\\) denote the position (the latent variable) and let \\(r\\) denote an auxiliary momentum variable. The joint density of \\((z, r)\\) is defined through a Hamiltonian:</p> \\[ H(z, r) = -\\log p(z) + \\frac{1}{2} r^\\top r. \\] <p>The first term acts like a potential energy, and the second acts like kinetic energy. The total Hamiltonian is approximately conserved under Hamiltonian dynamics governed by:</p> \\[ \\frac{dz}{dt} = r, \\qquad \\frac{dr}{dt} = \\nabla_z \\log p(z). \\] <p>Following these dynamics moves the system along continuous trajectories that remain mostly in high-probability regions, allowing the sampler to travel long distances without being rejected.</p>"},{"location":"informationtheory/6_mc/#leapfrog-integration-and-step-size","title":"Leapfrog Integration and Step Size","text":"<p>Exact Hamiltonian dynamics cannot be simulated analytically, so HMC uses a numerical integrator, typically the leapfrog method. Leapfrog integration updates position and momentum in small steps of size \\(\\epsilon\\):</p> <ol> <li> <p>half-step momentum update </p> </li> <li> <p>full-step position update </p> </li> <li> <p>half-step momentum update </p> </li> </ol> <p>These updates are repeated \\(L\\) times, producing a proposal \\((z', r')\\) after a simulated trajectory of length \\(L \\epsilon\\).</p> <p>The step size \\(\\epsilon\\) strongly influences performance:</p> <ul> <li>if \\(\\epsilon\\) is too large, numerical integration becomes inaccurate and proposals are rejected  </li> <li>if \\(\\epsilon\\) is too small, trajectories progress slowly and exploration becomes inefficient  </li> </ul> <p>Adaptive schemes such as dual averaging automatically tune \\(\\epsilon\\).</p>"},{"location":"informationtheory/6_mc/#acceptance-step","title":"Acceptance Step","text":"<p>Although leapfrog integration nearly preserves energy, numerical error accumulates. Therefore HMC applies a Metropolis acceptance step:</p> \\[ \\alpha = \\min\\left(1,\\, \\exp\\big(-H(z',r') + H(z,r)\\big) \\right). \\] <p>Because leapfrog integration is reversible and volume-preserving, acceptance rates remain high even for long trajectories.</p>"},{"location":"informationtheory/6_mc/#choosing-the-trajectory-length","title":"Choosing the Trajectory Length","text":"<p>The number of leapfrog steps \\(L\\) (or total integration time \\(L\\epsilon\\)) affects how far the sampler travels:</p> <ul> <li>small \\(L\\) results in short trajectories, similar to random-walk proposals  </li> <li>large \\(L\\) explores further but may waste computation or return near the starting point  </li> </ul> <p>The No-U-Turn Sampler (NUTS) automatically selects an appropriate integration length and forms the basis of modern HMC implementations such as Stan.</p>"},{"location":"informationtheory/6_mc/#summary-of-advantages","title":"Summary of Advantages","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving through the probability landscape.</p> <p>Hamiltonian Monte Carlo offers several advantages:</p> <ul> <li>efficient exploration in high-dimensional or correlated distributions  </li> <li>large, directed moves that avoid random-walk behaviour  </li> <li>low autocorrelation between samples  </li> <li>high acceptance rates due to approximate energy conservation  </li> <li>uses gradients of \\(\\log p(z)\\) to guide proposals  </li> </ul> <p>HMC is widely used in probabilistic programming frameworks such as Stan, PyMC, and NumPyro, largely because of its scalability and efficiency in challenging Bayesian inference problems.</p>"},{"location":"informationtheory/6_mc/#52-overrelaxation","title":"5.2 Overrelaxation","text":"<p>Overrelaxation modifies proposals so that successive samples are negatively correlated. Instead of randomly perturbing the current state, overrelaxation proposes a point on the opposite side of the conditional mean.</p> <p>Intuitively, if the current sample lies above the mean, the overrelaxation proposal nudges it below the mean, and vice versa. This helps the chain avoid local sticking and oscillation.</p> <p>Overrelaxation is most effective when the conditional distribution is approximately Gaussian or when the model exhibits strong linear structure.</p>"},{"location":"informationtheory/6_mc/#6-sensitivity-to-step-size","title":"6. Sensitivity to Step Size","text":"<p>The efficiency of MCMC algorithms depends critically on the choice of step size (or proposal scale):</p> <ul> <li>If the step size is too small, the chain takes tiny moves and mixes slowly.  </li> <li>If the step size is too large, most proposals are rejected.</li> </ul> <p>Finding an appropriate step size is essential for balancing exploration and acceptance.  </p> <p>For random-walk Metropolis\u2013Hastings:</p> <ul> <li>acceptance rates near 0.2\u20130.4 often work well in high dimensions  </li> <li>smaller dimensions tolerate larger acceptance rates</li> </ul> <p>For HMC, step size affects both the numerical integration quality and the acceptance probability. Too large a step size causes integration error and rejections; too small a step size results in slow exploration.</p> <p>Adaptive MCMC methods automatically tune the step size to achieve target acceptance rates.</p>"},{"location":"informationtheory/6_mc/#7-when-to-stop-convergence-and-diagnostics","title":"7. When to Stop: Convergence and Diagnostics","text":"<p>Running an MCMC chain forever is impossible, so practical inference requires diagnosing convergence.</p> <p>Several indicators are commonly used:</p>"},{"location":"informationtheory/6_mc/#71-burn-in","title":"7.1 Burn-in","text":"<p>The initial part of the chain (the burn-in period) may not represent the target distribution. These early samples are discarded until the chain reaches a stable region.</p>"},{"location":"informationtheory/6_mc/#72-autocorrelation","title":"7.2 Autocorrelation","text":"<p>High autocorrelation indicates slow mixing. Effective sample size (ESS) measures the number of independent samples equivalent to the correlated MCMC draws.</p>"},{"location":"informationtheory/6_mc/#73-multiple-chains","title":"7.3 Multiple chains","text":"<p>Running several independent chains allows comparison. If chains converge to the same region, the sampler is more likely to have reached equilibrium.</p>"},{"location":"informationtheory/6_mc/#74-gelmanrubin-statistic-r-hat","title":"7.4 Gelman\u2013Rubin statistic (R-hat)","text":"<p>R-hat compares within-chain and between-chain variance. Values close to 1 indicate convergence.</p>"},{"location":"informationtheory/6_mc/#75-visual-inspection","title":"7.5 Visual inspection","text":"<p>Trace plots, autocorrelation plots, and histograms provide qualitative insight into mixing and stability.</p> <p>There is no single perfect test, but combining multiple diagnostics provides reasonable confidence that the Markov chain has approximated the target distribution.</p> <p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p> <p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p> <p>Random-walk behaviour limits the efficiency of basic MCMC methods. Hamiltonian Monte Carlo reduces this by exploiting gradient information and simulating Hamiltonian dynamics, while overrelaxation introduces negative correlation to speed up mixing. Step size must be chosen carefully to balance exploration and acceptance. Convergence diagnostics such as burn-in, effective sample size, and R-hat help determine when to stop sampling and assess the quality of the generated samples.</p>"},{"location":"informationtheory/7a_vi_intro/","title":"7a vi intro","text":"<p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p> <p>This chapter introduces three optimization-based inference strategies:</p> <ol> <li>Maximum a posteriori (MAP) estimation  </li> <li>Expectation\u2013Maximization (EM)  </li> <li>Variational inference (VI), in its simplest introductory form  </li> </ol> <p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>"},{"location":"informationtheory/7a_vi_intro/#1-motivation-for-optimization-based-inference","title":"1. Motivation for Optimization-Based Inference","text":"<p>Bayesian inference requires the posterior</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The challenge lies in computing the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p> <ul> <li>high-dimensional latent spaces  </li> <li>multimodal posteriors  </li> <li>large datasets  </li> <li>models requiring gradient-based learning  </li> </ul> <p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>"},{"location":"informationtheory/7a_vi_intro/#2-maximum-a-posteriori-map-estimation","title":"2. Maximum A Posteriori (MAP) Estimation","text":"<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes\u2019 rule:</p> \\[ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}, \\] <p>MAP chooses the mode of the posterior:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta p(\\theta|x). \\] <p>Since \\(p(x)\\) does not depend on \\(\\theta\\), this is equivalent to:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta \\big[ \\log p(x|\\theta) + \\log p(\\theta) \\big]. \\] <p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through \\(p(\\theta)\\). However, it returns only a point estimate and does not capture uncertainty.</p> <p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>"},{"location":"informationtheory/7a_vi_intro/#3-expectationmaximization-em","title":"3. Expectation\u2013Maximization (EM)","text":"<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p> \\[ \\log p_\\theta(x)  = \\log \\sum_z p_\\theta(x,z). \\] <p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>"},{"location":"informationtheory/7a_vi_intro/#e-step","title":"E-step","text":"<p>Compute the posterior over latent variables under the current parameters:</p> \\[ q(z) = p_\\theta(z|x). \\]"},{"location":"informationtheory/7a_vi_intro/#m-step","title":"M-step","text":"<p>Maximize the expected complete-data log-likelihood:</p> \\[ \\theta \\leftarrow  \\arg\\max_\\theta  \\mathbb{E}_{q(z)}[\\log p_\\theta(x,z)]. \\] <p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p> <ul> <li>mixture of Gaussians  </li> <li>hidden Markov models  </li> <li>probabilistic PCA  </li> <li>clustering and density estimation  </li> </ul> <p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior \\(q(z) = p_\\theta(z|x)\\).</p>"},{"location":"informationtheory/7a_vi_intro/#4-em-and-map-map-em","title":"4. EM and MAP: MAP-EM","text":"<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p> \\[ \\theta_{\\text{MAP}}  =  \\arg\\max_\\theta  \\left[ \\mathbb{E}_{p(z|x,\\theta)}[\\log p(x,z|\\theta)] + \\log p(\\theta) \\right]. \\] <p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>"},{"location":"informationtheory/7a_vi_intro/#5-limitations-of-map-and-em","title":"5. Limitations of MAP and EM","text":"<p>Both MAP and EM have limitations that motivate more general methods:</p> <ol> <li>MAP returns only a point estimate and discards posterior uncertainty.  </li> <li>EM requires exact posterior computation in the E-step:        which is often intractable.  </li> <li>EM struggles with:</li> <li>multimodal posteriors  </li> <li>high-dimensional latent spaces  </li> <li>arbitrary likelihood forms  </li> </ol> <p>These limitations lead naturally to variational inference.</p>"},{"location":"informationtheory/7a_vi_intro/#6-a-brief-introduction-to-variational-inference-vi","title":"6. A Brief Introduction to Variational Inference (VI)","text":"<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p> \\[ q(z) = p_\\theta(z|x), \\] <p>VI chooses a family of distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q} \\] <p>and optimizes it to be close to the true posterior. The objective is:</p> \\[ \\phi^* =  \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p> <p>VI:</p> <ul> <li>generalizes MAP (when \\(q\\) is a delta function)  </li> <li>generalizes EM (when \\(q = p_\\theta(z|x)\\))  </li> <li>supports flexible approximations  </li> <li>scales to large datasets  </li> <li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li> </ul> <p>The next chapter explores variational inference in detail.</p> <p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p> <p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p> <p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p>"},{"location":"informationtheory/7b_vi/","title":"7b vi","text":"<p>Variational inference (VI) provides a general framework for approximating difficult probability distributions with simpler, tractable ones. Many modern machine-learning models rely on VI, including variational autoencoders (VAEs), Bayesian neural networks, latent-variable models, and diffusion models. VI offers a scalable alternative to sampling-based inference and converts the inference problem into an optimization problem.</p>"},{"location":"informationtheory/7b_vi/#1-the-problem-of-inference","title":"1. The Problem of Inference","text":"<p>Many probabilistic models introduce hidden variables to explain observations. Examples include:</p> <ul> <li>latent variables \\(z\\) in VAEs  </li> <li>weight distributions in Bayesian neural networks  </li> <li>cluster indicators in mixture models  </li> <li>hidden states in topic models and HMMs  </li> </ul> <p>The goal is to compute the posterior distribution</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The difficulty lies in the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is often intractable in high-dimensional or complex models. Exact Bayesian inference becomes impossible, which motivates approximate methods. Variational inference addresses this challenge.</p>"},{"location":"informationtheory/7b_vi/#2-the-idea-of-variational-inference","title":"2. The Idea of Variational Inference","text":"<p>Variational inference replaces the intractable posterior with a tractable approximation. Instead of trying to compute \\(p(z|x)\\) exactly, VI introduces a family of simpler distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q}, \\] <p>and chooses the member that is closest to the true posterior. Closeness is measured using the KL divergence:</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>The goal is:</p> \\[ \\phi^* = \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>However, the KL depends on \\(p(z|x)\\), which is unknown, making direct minimization impossible. The key insight is that the KL can be rewritten in terms of computable quantities, leading to the Evidence Lower Bound (ELBO).</p>"},{"location":"informationtheory/7b_vi/#3-deriving-the-elbo","title":"3. Deriving the ELBO","text":"<p>We start from the marginal likelihood:</p> \\[ \\log p(x) = \\log \\int p(x,z)\\,dz. \\] <p>We multiply and divide by \\(q_\\phi(z|x)\\):</p> \\[ \\log p(x) = \\log \\int q_\\phi(z|x)\\, \\frac{p(x,z)}{q_\\phi(z|x)}\\,dz. \\] <p>Applying Jensen\u2019s inequality yields:</p> \\[ \\log p(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p(x,z)}{q_\\phi(z|x)} \\right]. \\] <p>This expression is the ELBO:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x,z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]. \\] <p>A useful identity reveals:</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Since KL divergence is non-negative:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) \\le \\log p(x). \\] <p>Maximizing the ELBO is equivalent to minimizing the KL divergence between \\(q_\\phi(z|x)\\) and the true posterior.</p>"},{"location":"informationtheory/7b_vi/#4-interpreting-the-elbo","title":"4. Interpreting the ELBO","text":"<p>The ELBO can be decomposed into two terms that have clear interpretations. Writing</p> \\[ p(x,z) = p_\\theta(x|z)p(z), \\] <p>and substituting into the ELBO gives:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\]"},{"location":"informationtheory/7b_vi/#reconstruction-term","title":"Reconstruction term","text":"\\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]. \\] <p>This term ensures that \\(z\\) captures enough information to generate or reconstruct the observed data. It corresponds to likelihood or reconstruction accuracy.</p>"},{"location":"informationtheory/7b_vi/#regularization-term","title":"Regularization term","text":"\\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>This term ensures that the posterior approximation does not drift too far from the prior. In VAEs, the prior is usually a Gaussian, making the latent space smooth and structured.</p> <p>The ELBO therefore expresses a balance:</p> <ul> <li>the first term rewards informative latent variables  </li> <li>the second term penalizes overly complex or irregular latent distributions  </li> </ul>"},{"location":"informationtheory/7b_vi/#5-why-vi-uses-reverse-kl","title":"5. Why VI Uses Reverse KL","text":"<p>Variational inference minimizes</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)), \\] <p>which is reverse KL. Reverse KL has important behavioral properties:</p> <ul> <li>it heavily penalizes assigning probability mass where the true posterior is low  </li> <li>it allows \\(q\\) to ignore some modes of \\(p(z|x)\\) </li> <li>it prefers tight, conservative approximations  </li> </ul> <p>As a result:</p> <ul> <li>VI tends to be mode seeking  </li> <li>it focuses on a single high-density region  </li> <li>it can miss multimodal structure of the true posterior  </li> </ul> <p>This behavior explains why VAEs sometimes produce smooth or blurry samples: the latent space favors safe, central modes.</p>"},{"location":"informationtheory/7b_vi/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":"<p>A VAE applies variational inference to a deep latent-variable model. It introduces:</p> <ol> <li>a latent prior </li> <li>a decoder (generative model) </li> <li>an encoder (variational posterior) </li> </ol> <p>The encoder and decoder are neural networks, trained jointly by maximizing the ELBO over all data points.</p>"},{"location":"informationtheory/7b_vi/#61-generative-model","title":"6.1 Generative model","text":"<p>Given \\(z\\) sampled from the prior, the decoder produces a distribution over possible \\(x\\):</p> \\[ p_\\theta(x|z). \\]"},{"location":"informationtheory/7b_vi/#62-inference-model","title":"6.2 Inference model","text":"<p>The encoder produces the parameters of the approximate posterior:</p> \\[ q_\\phi(z|x) = \\mathcal{N}(z\\mid \\mu_\\phi(x), \\sigma^2_\\phi(x)). \\] <p>This is the distribution used inside the ELBO.</p>"},{"location":"informationtheory/7b_vi/#63-vae-training-objective","title":"6.3 VAE training objective","text":"<p>The objective for each data point is:</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>The first term encourages correct reconstruction; the second keeps latent codes regularized.</p>"},{"location":"informationtheory/7b_vi/#7-the-reparameterization-trick","title":"7. The Reparameterization Trick","text":"<p>The expectation in the ELBO involves sampling from \\(q_\\phi(z|x)\\). To differentiate through this sampling step, VAEs use the reparameterization:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\odot\\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0,I). \\] <p>This expresses sampling as a deterministic transformation of noise, allowing gradients to flow through the encoder.</p> <p>This trick is central to making VI scalable and efficient in deep learning.</p>"},{"location":"informationtheory/7b_vi/#8-consequences-of-reverse-kl-in-vaes","title":"8. Consequences of Reverse KL in VAEs","text":"<p>The reverse KL term shapes the behavior of the VAE:</p> <ul> <li>it encourages smooth, overlapping latent regions  </li> <li>it prefers safe latent representations  </li> <li>it explains why VAEs sometimes produce blurry or conservative samples  </li> <li>it stabilizes training  </li> <li>it produces well-structured latent spaces  </li> </ul> <p>Extensions such as the \\(\\beta\\)-VAE, hierarchical VAEs, and flows inside the encoder allow for more expressive or disentangled representations.</p>"},{"location":"informationtheory/7b_vi/#9-variational-inference-beyond-vaes","title":"9. Variational Inference Beyond VAEs","text":"<p>VI provides a general-purpose framework for approximate inference in many settings.</p>"},{"location":"informationtheory/7b_vi/#bayesian-neural-networks","title":"Bayesian neural networks","text":"<p>Posterior distributions over weights are approximated by variational distributions:</p> \\[ q(w)\\approx p(w|D). \\]"},{"location":"informationtheory/7b_vi/#diffusion-models","title":"Diffusion models","text":"<p>The training objective resembles a variational bound on the data likelihood, using KL divergences between transition kernels.</p>"},{"location":"informationtheory/7b_vi/#normalizing-flows-for-vi","title":"Normalizing flows for VI","text":"<p>Flows can produce more expressive variational posteriors than simple Gaussians.</p>"},{"location":"informationtheory/7b_vi/#reinforcement-learning","title":"Reinforcement learning","text":"<p>Entropy-regularized RL and soft Q-learning can be interpreted through variational principles.</p> <p>VI therefore offers a unifying viewpoint across deep generative models, Bayesian inference, and probabilistic deep learning.</p> <p>Variational inference replaces an intractable posterior distribution with a tractable approximation and optimizes this approximation by maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, capturing the trade-off between accuracy and complexity. VAEs are an important application of VI, using neural networks to parameterize both the generative model and the approximate posterior. Reverse KL explains the conservative behavior of VI-based models. Variational inference provides a flexible approach for approximate Bayesian inference and underlies many modern generative and representation-learning techniques.</p>"},{"location":"informationtheory/8_reperesentation/","title":"8 reperesentation","text":""},{"location":"informationtheory/8_reperesentation/#chapter-5-representation-learning-mutual-information-and-the-information-bottleneck","title":"Chapter 5 \u2014 Representation Learning, Mutual Information, and the Information Bottleneck","text":"<p>Representation learning seeks transformations of data that make tasks such as prediction, compression, and reasoning easier. A representation \\(Z\\) is typically obtained by applying an encoder to an input \\(X\\). Information theory provides a natural way to formalize what makes a representation useful by analyzing the mutual information between \\(Z\\), the input \\(X\\), and the target \\(Y\\).</p> <p>This chapter introduces mutual information as a measure of shared structure, explains the Information Bottleneck framework, and connects these ideas to deep learning methods such as contrastive learning, VAEs, and self-supervised learning.</p>"},{"location":"informationtheory/8_reperesentation/#1-what-is-a-representation","title":"1. What Is a Representation?","text":"<p>A representation is a transformed form of input data:</p> \\[ Z = f_\\theta(X), \\] <p>where \\(f_\\theta\\) is usually a neural network. A good representation should satisfy two goals:</p> <ol> <li>It should retain information that is relevant for predicting \\(Y\\).  </li> <li>It should discard noise or irrelevant aspects of \\(X\\).</li> </ol> <p>Information theory allows us to express these goals using mutual information.</p>"},{"location":"informationtheory/8_reperesentation/#2-mutual-information-connecting-two-variables","title":"2. Mutual Information: Connecting Two Variables","text":"<p>Mutual information (MI) measures how much knowledge of one variable reduces uncertainty about another:</p> \\[ I(X;Y) = H(X) - H(X|Y). \\] <p>It can also be written as a KL divergence:</p> \\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)). \\] <p>MI is zero when \\(X\\) and \\(Y\\) are independent and increases as \\(Y\\) becomes more predictable from \\(X\\).</p> <p>In representation learning, we are often interested in the two quantities:</p> \\[ I(Z;Y), \\qquad I(Z;X). \\] <p>These measure how informative the representation \\(Z\\) is regarding the target \\(Y\\) and how much irrelevant detail from \\(X\\) is still present.</p>"},{"location":"informationtheory/8_reperesentation/#3-the-role-of-mi-in-representation-learning","title":"3. The Role of MI in Representation Learning","text":"<p>A representation \\(Z\\) is desirable when:</p> <ul> <li> <p>\\(I(Z;Y)\\) is large   (the representation captures features relevant to prediction)</p> </li> <li> <p>\\(I(Z;X)\\) is small   (the representation removes noise and redundancy)</p> </li> </ul> <p>This idea appears in supervised learning, contrastive methods, and generative modeling.</p> <p>Some examples:</p> <ul> <li>In supervised learning, we want features that preserve label information.  </li> <li>In contrastive learning, we want features that preserve the information common across augmented views.  </li> <li>In generative models, latent variables should retain structure that explains the data while avoiding unnecessary detail.</li> </ul>"},{"location":"informationtheory/8_reperesentation/#4-the-information-bottleneck-principle","title":"4. The Information Bottleneck Principle","text":"<p>The Information Bottleneck (IB) formalizes the trade-off between informativeness and compression. The goal is:</p> \\[ \\max I(Z;Y) \\quad \\text{s.t.} \\quad  I(Z;X) \\text{ is small}. \\] <p>This can be written as the Lagrangian:</p> \\[ \\mathcal{L}_{\\text{IB}} = I(Z;Y) - \\beta I(Z;X). \\] <p>The parameter \\(\\beta\\) controls how aggressively the representation is compressed.</p> <ul> <li>Large \\(\\beta\\) leads to simpler, more compressed representations.  </li> <li>Small \\(\\beta\\) allows more expressive, detailed representations.</li> </ul> <p>IB provides a theoretical explanation for the behavior of learned features in deep neural networks.</p>"},{"location":"informationtheory/8_reperesentation/#5-deep-learning-and-the-information-bottleneck","title":"5. Deep Learning and the Information Bottleneck","text":"<p>IB theory suggests several statements about deep networks:</p> <ol> <li>Early layers preserve much of the information in \\(X\\).  </li> <li>Later layers tend to compress \\(X\\) while emphasizing information predictive of \\(Y\\).  </li> <li>Networks may first memorize and later compress during training.  </li> <li>Generalization is linked to discarding unnecessary information.</li> </ol> <p>Although the exact dynamics remain debated, the overall perspective helps interpret the evolution of features during training.</p>"},{"location":"informationtheory/8_reperesentation/#6-variational-information-bottleneck-vib","title":"6. Variational Information Bottleneck (VIB)","text":"<p>Mutual information terms are often difficult to compute directly. The Variational Information Bottleneck approximates them using variational distributions.</p> <p>We treat the representation as a random variable drawn from \\(q(z|x)\\) and estimate MI with tractable terms. The VIB objective is:</p> \\[ \\mathcal{L}_{\\text{VIB}} = \\mathbb{E}_{p(x,y)}\\! \\left[ \\mathbb{E}_{q(z|x)}\\![\\log p(y|z)] \\right] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>This resembles the VAE objective, but \\(p(y|z)\\) replaces the reconstruction term. The first term encourages predictive features, while the KL term compresses the representation.</p> <p>VIB therefore provides a practical implementation of the Information Bottleneck.</p>"},{"location":"informationtheory/8_reperesentation/#7-mutual-information-and-contrastive-learning","title":"7. Mutual Information and Contrastive Learning","text":"<p>Contrastive learning uses mutual information to learn representations without labels. The idea is:</p> <ul> <li>Generate two augmented views of the same input: \\((x_1, x_2)\\).  </li> <li>Encode them as \\((z_1, z_2)\\).  </li> <li>Encourage \\(z_1\\) and \\(z_2\\) to be similar.  </li> <li>Encourage representations of different inputs to be dissimilar.</li> </ul> <p>This encourages \\(Z\\) to retain the information that is preserved under augmentation, while ignoring irrelevant aspects of the input.</p> <p>Many methods follow this structure:</p> <ul> <li>SimCLR  </li> <li>MoCo  </li> <li>BYOL  </li> <li>InfoNCE  </li> <li>CPC  </li> <li>Deep InfoMax  </li> </ul> <p>The InfoNCE objective is:</p> \\[ \\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}\\left[ \\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)} {\\sum_k \\exp(\\text{sim}(z_i,z_k)/\\tau)} \\right], \\] <p>where \\((i,j)\\) is a positive pair. InfoNCE is a variational lower bound on \\(I(Z_1;Z_2)\\).</p>"},{"location":"informationtheory/8_reperesentation/#8-mi-in-generative-modeling","title":"8. MI in Generative Modeling","text":"<p>Mutual information also appears in generative models:</p>"},{"location":"informationtheory/8_reperesentation/#81-vaes","title":"8.1 VAEs","text":"<p>The KL term controls the structure and redundancy of \\(Z\\), and the decoder ensures \\(I(Z;X)\\) stays large enough for accurate reconstruction.</p>"},{"location":"informationtheory/8_reperesentation/#82-infogan","title":"8.2 InfoGAN","text":"<p>This model maximizes:</p> \\[ I(c; G(z,c)), \\] <p>encouraging the generator to learn interpretable latent factors.</p>"},{"location":"informationtheory/8_reperesentation/#83-normalizing-flows","title":"8.3 Normalizing flows","text":"<p>Flows maintain \\(I(X;Z) = H(X)\\) because they are invertible; they do not compress the input.</p>"},{"location":"informationtheory/8_reperesentation/#84-diffusion-models","title":"8.4 Diffusion models","text":"<p>Diffusion models gradually reduce noise and can be interpreted using information-theoretic ideas related to KL divergence and score matching.</p>"},{"location":"informationtheory/8_reperesentation/#9-mi-and-disentanglement","title":"9. MI and Disentanglement","text":"<p>Disentangled representations aim to separate independent generative factors such as orientation or color. The \\(\\beta\\)-VAE objective:</p> \\[ \\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)) \\] <p>encourages disentanglement by increasing compression in the latent space. A larger \\(\\beta\\) pushes different dimensions of \\(Z\\) to encode more independent aspects of the data.</p>"},{"location":"informationtheory/8_reperesentation/#10-estimating-mi-in-high-dimensions","title":"10. Estimating MI in High Dimensions","text":"<p>Mutual information is difficult to compute exactly in high dimensions. Neural estimation relies on variational bounds such as:</p> <ul> <li>InfoNCE  </li> <li>NWJ bound  </li> <li>Donsker\u2013Varadhan bound  </li> <li>MINE estimator  </li> <li>f-divergence lower bounds  </li> </ul> <p>These allow MI to be used in representation learning even when the true quantities are intractable.</p>"},{"location":"informationtheory/8_reperesentation/#11-summary-of-chapter-5","title":"11. Summary of Chapter 5","text":"<p>Mutual information provides a principled measure of what makes a useful representation: it should retain information relevant for prediction and discard irrelevant detail. The Information Bottleneck formalizes this trade-off and motivates practical methods such as the Variational Information Bottleneck.</p> <p>Contrastive learning methods maximize MI between augmented views, enabling self-supervised representation learning. Generative models such as VAEs, GANs, flows, and diffusion models each manipulate mutual information in different ways, leading to distinct behaviors and capabilities.</p> <p>Information theory therefore provides a unified lens through which to understand representation learning in modern deep networks.</p>"},{"location":"informationtheory/intro/","title":"Intro","text":"<p>https://www.inference.org.uk/itprnn_lectures/</p> <p>https://www.youtube.com/watch?v=sN_0iGWcyLI&amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&amp;index=12</p> <p>60mins Lect 12</p> <p>Application in  1. GANS</p>"},{"location":"informationtheory/intro/#game-theory","title":"Game theory","text":"<ul> <li>zero sum?</li> <li>min max V(D,G)</li> <li>nash equiblria</li> </ul> <p>Application in GAN</p>"},{"location":"llms/chahllanges/","title":"Chahllanges","text":"<p>Data Challenges: This pertains to the data used for training and how the model addresses gaps or missing data. Ethical Challenges: This involves addressing issues such as mitigating biases, ensuring privacy, and preventing the generation of harmful content in the deployment of LLMs. Technical Challenges: These challenges focus on the practical implementation of LLMs. Deployment Challenges: Concerned with the specific processes involved in transitioning fully-functional LLMs into real-world use-cases (productionization) Data Challenges:</p> <p>Data Bias: The presence of prejudices and imbalances in the training data leading to biased model outputs. Limited World Knowledge and Hallucination: LLMs may lack comprehensive understanding of real-world events and information and tend to hallucinate information. Note that training them on new data is a long and expensive process. Dependency on Training Data Quality: LLM performance is heavily influenced by the quality and representativeness of the training data. Ethical and Social Challenges:</p> <p>Ethical Concerns: Concerns regarding the responsible and ethical use of language models, especially in sensitive contexts. Bias Amplification: Biases present in the training data may be exacerbated, resulting in unfair or discriminatory outputs. Legal and Copyright Issues: Potential legal complications arising from generated content that infringes copyrights or violates laws. User Privacy Concerns: Risks associated with generating text based on user inputs, especially when dealing with private or sensitive information. Technical Challenges:</p> <p>Computational Resources: Significant computing power required for training and deploying large language models. Interpretability: Challenges in understanding and explaining the decision-making process of complex models. Evaluation: Evaluation presents a notable challenge as assessing models across diverse tasks and domains is inadequately designed, particularly due to the challenges posed by freely generated content. Fine-tuning Challenges: Difficulties in adapting pre-trained models to specific tasks or domains. Contextual Understanding: LLMs may face challenges in maintaining coherent context over longer passages or conversations. Robustness to Adversarial Attacks: Vulnerability to intentional manipulations of input data leading to incorrect outputs. Long-Term Context: Struggles in maintaining context and coherence over extended pieces of text or discussions. Deployment Challenges:</p> <p>Scalability: Ensuring that the model can scale efficiently to handle increased workloads and demand in production environments. Latency: Minimizing the response time or latency of the model to provide quick and efficient interactions, especially in real-time applications. Monitoring and Maintenance: Implementing robust monitoring systems to track model performance, detect issues, and perform regular maintenance to avoid downtime. Integration with Existing Systems: Ensuring smooth integration of LLMs with existing software, databases, and infrastructure within an organization. Cost Management: Optimizing the cost of deploying and maintaining large language models, as they can be resource-intensive in terms of both computation and storage. Security Concerns: Addressing potential security vulnerabilities and risks associated with deploying language models in production, including safeguarding against malicious attacks. Interoperability: Ensuring compatibility with other tools, frameworks, or systems that may be part of the overall production pipeline. User Feedback Incorporation: Developing mechanisms to incorporate user feedback to continuously improve and update the model in a production environment. Regulatory Compliance: Adhering to regulatory requirements and compliance standards, especially in industries with strict data protection and privacy regulations. Dynamic Content Handling: Managing the generation of text in dynamic environments where content and user interactions change frequently.</p> <p>Types of Domain Adaptation Methods There are several methods to incorporate domain-specific knowledge into LLMs, each with its own advantages and limitations. Here are three classes of approaches:</p> <p>Domain-Specific Pre-Training:</p> <p>Training Duration: Days to weeks to months Summary: Requires a large amount of domain training data; can customize model architecture, size, tokenizer, etc. In this method, LLMs are pre-trained on extensive datasets representing various natural language use cases. For instance, models like PaLM 540B, GPT-3, and LLaMA 2 have been pre-trained on datasets with sizes ranging from 499 billion to 2 trillion tokens. Examples of domain-specific pre-training include models like ESMFold, ProGen2 for protein sequences, Galactica for science, BloombergGPT for finance, and StarCoder for code. These models outperform generalist models within their domains but still face limitations in terms of accuracy and potential hallucinations.</p> <p>Domain-Specific Fine-Tuning:</p> <p>Training Duration: Minutes to hours Summary: Adds domain-specific data; tunes for specific tasks; updates LLM model Fine-tuning involves training a pre-trained LLM on a specific task or domain, adapting its knowledge to a narrower context. Examples include Alpaca (fine-tuned LLaMA-7B model for general tasks), xFinance (fine-tuned LLaMA-13B model for financial-specific tasks), and ChatDoctor (fine-tuned LLaMA-7B model for medical chat). The costs for fine-tuning are significantly smaller compared to pre-training.</p> <p>Retrieval Augmented Generation (RAG):</p> <p>Training Duration: Not required Summary: No model weights; external information retrieval system can be tuned RAG involves grounding the LLM's parametric knowledge with external or non-parametric knowledge from an information retrieval system. This external knowledge is provided as additional context in the prompt to the LLM. The advantages of RAG include no training costs, low expertise requirement, and the ability to cite sources for human verification. This approach addresses limitations such as hallucinations and allows for precise manipulation of knowledge. The knowledge base is easily updatable without changing the LLM. Strategies to combine non-parametric knowledge with an LLM's parametric knowledge are actively researched.</p> <p>Use Domain-Specific Pre-Training When: Exclusive Domain Focus: Pre-training is suitable when you require a model exclusively trained on data from a specific domain, creating a specialized language model for that domain. Customizing Model Architecture: It allows you to customize various aspects of the model architecture, size, tokenizer, etc., based on the specific requirements of the domain. Extensive Training Data Available: Effective pre-training often requires a large amount of domain-specific training data to ensure the model captures the intricacies of the chosen domain. Use Domain-Specific Fine-Tuning When: Specialization Needed: Fine-tuning is suitable when you already have a pre-trained LLM, and you want to adapt it for specific tasks or within a particular domain. Task Optimization: It allows you to adjust the model's parameters related to the task, such as architecture, size, or tokenizer, for optimal performance in the chosen domain. Time and Resource Efficiency: Fine-tuning saves time and computational resources compared to training a model from scratch since it leverages the knowledge gained during the pre-training phase. Use RAG When: Information Freshness Matters: RAG provides up-to-date, context-specific data from external sources. Reducing Hallucination is Crucial: Ground LLMs with verifiable facts and citations from an external knowledge base. Cost-Efficiency is a Priority: Avoid extensive model training or fine-tuning; implement without the need for training.</p>"},{"location":"llms/foundations/","title":"Foundations","text":""},{"location":"monetcarlo_simulations/plan/","title":"Plan","text":"<ol> <li>Markov Chain</li> </ol>"},{"location":"nn_training/1_intro/","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#understanding-autograd-the-engine-behind-deep-learning-with-a-micrograd-style-walkthrough","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#what-is-autograd","title":"What is Autograd?","text":"<p>Autograd \u2014 short for automatic differentiation \u2014 is a computational technique that automatically computes derivatives of functions expressed as computer programs. It is the mathematical and computational backbone of deep learning frameworks like PyTorch, TensorFlow, and JAX.</p> <p>At its core, autograd implements reverse-mode automatic differentiation, an algorithm that efficiently computes gradients of a scalar output (such as a loss) with respect to many input parameters (model weights).</p>"},{"location":"nn_training/1_intro/#how-it-works","title":"How It Works","text":"<p>When a function is executed, autograd records all elementary operations (addition, multiplication, non-linearities, etc.) in a computational graph. Each node represents a tensor or scalar value, and each edge represents an operation with a known local derivative.</p> <p>During the forward pass, the graph is constructed dynamically. During the backward pass, the engine traverses the graph in reverse order, applying the chain rule to compute gradients:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy}\\cdot\\frac{dy}{dx}. \\] <p>This process is often referred to as back-propagation. In practice, the framework automatically handles these derivative computations.</p> <p>For example, in Andrej Karpathy\u2019s micrograd, a minimal autograd engine, each scalar <code>Value</code> object keeps track of both its data and gradient, as well as the operation that produced it. The <code>.backward()</code> method propagates gradients backward through the graph, applying local chain rules for each operation.</p>"},{"location":"nn_training/1_intro/#differentiation-methods-overview","title":"Differentiation Methods Overview","text":"Method Description Pros Cons Numerical Finite difference approximation Simple Inaccurate, slow Symbolic Algebraic manipulation (e.g., SymPy) Exact Symbol explosion, not scalable Automatic (AD) Local derivatives + chain rule Exact, efficient Requires graph bookkeeping <p>Unlike numerical differentiation (which is approximate) or symbolic differentiation (which manipulates expressions), autograd computes exact derivatives efficiently by chaining local gradients.</p>"},{"location":"nn_training/1_intro/#why-use-autograd","title":"Why Use Autograd?","text":"<ol> <li> <p>Eliminates Manual Derivative Computation    Without autograd, practitioners would need to derive and code gradients manually for each model parameter. This is not only tedious but error-prone, especially for complex architectures.</p> </li> <li> <p>Ensures Correctness and Reliability    By systematically applying the chain rule, autograd frameworks guarantee correct gradient flow through even the most intricate models, reducing human error.</p> </li> <li> <p>Supports Dynamic and Flexible Graphs    Modern frameworks like PyTorch and micrograd construct computation graphs dynamically \u2014 rebuilding them on each forward pass. This allows for loops, conditionals, and recursion within model definitions.</p> </li> <li> <p>Caches Intermediate Results    Autograd stores intermediate activations during the forward pass so they can be reused efficiently during the backward pass. This improves computational speed but increases memory usage.</p> </li> <li> <p>Higher-Order Derivatives    Since the backward pass itself is differentiable, autograd can compute higher-order derivatives \u2014 useful in meta-learning, optimization research, and differentiable physics.</p> </li> <li> <p>Performance and Hardware Optimization    Frameworks optimize backward passes using techniques like operation fusion and kernel caching, ensuring gradient computations remain efficient on GPUs and TPUs.</p> </li> </ol> <p>A minimal implementation like micrograd reveals these mechanics transparently, allowing students and researchers to understand what happens under the hood of massive frameworks.</p>"},{"location":"nn_training/1_intro/#importance-in-deep-learning","title":"Importance in Deep Learning","text":""},{"location":"nn_training/1_intro/#1-the-foundation-of-backpropagation","title":"1. The Foundation of Backpropagation","text":"<p>Training neural networks relies on minimizing a loss function \\(L(\\theta)\\) with respect to parameters \\(\\theta\\). The update rule for parameters (via gradient descent) is:</p> \\[ \\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}. \\] <p>Here, autograd automates the computation of \\(\\frac{\\partial L}{\\partial \\theta}\\) \u2014 the essential ingredient of learning.</p>"},{"location":"nn_training/1_intro/#2-enabling-complex-architectures","title":"2. Enabling Complex Architectures","text":"<p>Modern networks (e.g., Transformers, ResNets, GNNs) have deep stacks, skip connections, and nonlinear branches. Autograd ensures that gradients flow correctly through these complex graphs \u2014 enabling architectural innovation without requiring users to manually derive derivatives.</p>"},{"location":"nn_training/1_intro/#3-scalability-and-efficiency","title":"3. Scalability and Efficiency","text":"<p>Reverse-mode AD (autograd) is ideal for functions mapping many inputs to a single scalar output \u2014 exactly the case for deep learning. Its computational cost is roughly proportional to the cost of the forward pass, but with a higher memory footprint.</p> <p>Compute\u2013Memory Trade-off:</p> <ul> <li>Compute: The backward pass roughly doubles compute time.  </li> <li>Memory: Storing intermediate activations increases RAM/GPU usage.</li> </ul> <p>Frameworks mitigate this using gradient checkpointing, where certain intermediate activations are recomputed on-demand to save memory.</p>"},{"location":"nn_training/1_intro/#a-micrograd-style-value-class-with-line-by-line-commentary","title":"A micrograd-style <code>Value</code> class \u2014 with line-by-line commentary","text":"<p>Below is a faithful, lightly extended micrograd-style engine. Every key line is annotated to explain what it references and why it matters for autograd.</p> <pre><code>import math\n\nclass Value:\n    # ---------------------- Initialization ----------------------\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data                # (float) the scalar value of this node\n        self.grad = 0.0                 # (float) d(output)/d(this node), filled during backprop\n        self._backward = lambda: None   # a closure set by each op to push grad to parents\n        self._prev = set(_children)     # (set[Value]) parents (inputs) that produced this node\n        self._op = _op                  # (str) op name for graph/debug ('+','*','tanh','exp','k',...)\n        self.label = label              # (str) optional name for visualization\n\n    def __repr__(self):\n        # nice debug print to see the forward value\n        return f\"Value(data={self.data})\"\n\n    # ---------------------- Binary Ops: + ----------------------\n    def __add__(self, other):\n        # allow mixing with Python scalars: 2 + Value(3)\n        other = other if isinstance(other, Value) else Value(other)\n\n        # forward pass: create the child node 'out' from parents (self, other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # local partials for z = x + y are \u2202z/\u2202x = 1, \u2202z/\u2202y = 1\n            # chain rule: x.grad += 1 * out.grad; y.grad += 1 * out.grad\n            self.grad  += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward       # attach the gradient propagation rule to 'out'\n        return out\n\n    def __radd__(self, other):\n        # support Python's other + self\n        return self + other\n\n    # ---------------------- Binary Ops: * ----------------------\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # for z = x * y: \u2202z/\u2202x = y, \u2202z/\u2202y = x\n            self.grad  += other.data * out.grad\n            other.grad += self.data  * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        # support Python's other * self\n        return self * other\n\n    # ---------------------- Power, Neg, Sub, Div ----------------------\n    def __pow__(self, other):\n        # only scalar exponents for simplicity\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.dataother, (self,), f'{other}')\n\n        def _backward():\n            # for z = x^k: \u2202z/\u2202x = k * x^(k-1)\n            self.grad += other * (self.data  (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def __truediv__(self, other):  # self / other\n        # use x / y = x * y^{-1}\n        return self * (other  -1)\n\n    def __neg__(self):             # -self\n        # use -x = (-1) * x\n        return self * -1\n\n    def __sub__(self, other):      # self - other\n        # x - y = x + (-y)\n        return self + (-other)\n\n    # ---------------------- Nonlinearities ----------------------\n    def tanh(self):\n        # forward: compute t = tanh(x) (closed form used here; math.tanh is fine too)\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n\n        def _backward():\n            # derivative: d/dx tanh(x) = 1 - tanh(x)^2 = 1 - t^2\n            self.grad += (1 - t2) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        # forward: e^x\n        x = self.data\n        out = Value(math.exp(x), (self,), 'exp')\n\n        def _backward():\n            # derivative: d/dx e^x = e^x; note e^x is out.data\n            self.grad += out.data * out.grad\n        out._backward = _backward\n        return out\n\n    # ---------------------- Backprop Driver ----------------------\n    def backward(self):\n        # Build a topological ordering of the graph so every node's\n        # _backward() runs after all of its children have pushed grads.\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for parent in v._prev:   # traverse to parents (inputs)\n                    build_topo(parent)\n                topo.append(v)            # append after traversing parents\n\n        build_topo(self)\n\n        # seed the gradient at the output node: d(self)/d(self) = 1\n        self.grad = 1.0\n\n        # go in reverse topological order and apply each node's local chain rule\n        for node in reversed(topo):\n            node._backward()\n</code></pre>"},{"location":"nn_training/1_intro/#what-each-attributemethod-references","title":"What each attribute/method references","text":"<ul> <li><code>self.data</code>: the scalar numeric value stored at this node (forward pass result).  </li> <li><code>self.grad</code>: the accumulated derivative \\(\\frac{\\partial \\text{(final output)}}{\\partial \\text{this node}}\\) after <code>.backward()</code>.  </li> <li><code>self._prev</code>: a set of parent nodes (inputs) that produced <code>self</code>; used to traverse the graph.  </li> <li><code>self._op</code>: operation label for debugging/visualization.  </li> <li><code>self._backward</code>: a closure that knows how to push gradient from this node back to its parents using local partial derivatives.  </li> <li>Binary ops (<code>__add__</code>, <code>__mul__</code>, etc.): create a new child node <code>out</code> from parent nodes <code>(self, other)</code> and attach a <code>_backward</code> rule encoding the local Jacobian.  </li> <li><code>backward()</code>: performs a reverse topological traversal starting from the target scalar node, seeding its gradient with <code>1.0</code>, then calling every node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#worked-example-build-a-small-graph-and-differentiate","title":"Worked example: build a small graph and differentiate","text":"<p>We\u2019ll compute  and obtain gradients \\(\\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\frac{\\partial f}{\\partial c}\\).</p> <pre><code># create leaf nodes (parameters / inputs)\na = Value(2.0, label='a')\nb = Value(3.0, label='b')\nc = Value(0.5, label='c')\n\n# forward build\nd = a * b            # d = a*b\ne = c.tanh()         # e = tanh(c)\nf = d + e - 0.5*(a2)\n\n# backpropagate from scalar output 'f'\nf.backward()\n\nprint(\"f:\", f.data)\nprint(\"df/da:\", a.grad)\nprint(\"df/db:\", b.grad)\nprint(\"df/dc:\", c.grad)\n</code></pre>"},{"location":"nn_training/1_intro/#hand-derivative-sanity-check","title":"Hand-derivative sanity check","text":"<ul> <li>\\(d = a b \\Rightarrow \\frac{\\partial d}{\\partial a} = b,\\; \\frac{\\partial d}{\\partial b} = a\\) </li> <li>\\(e = \\tanh(c) \\Rightarrow \\frac{\\partial e}{\\partial c} = 1 - \\tanh^2(c)\\) </li> <li>\\(f = d + e - \\tfrac{1}{2} a^2\\)</li> </ul> <p>Therefore:  </p> <p>Your printed grads should match these values numerically (up to floating point).</p>"},{"location":"nn_training/1_intro/#how-methods-reference-values-and-variables-naming-clarity","title":"How methods reference values and variables (naming clarity)","text":"<ul> <li>In methods like <code>__add__</code> and <code>__mul__</code>, <code>self</code> is the left operand, <code>other</code> is the right operand (which we coerce to <code>Value</code> when it\u2019s a Python scalar).  </li> <li>The new node created by an operation is named <code>out</code>. It references:</li> <li><code>out.data</code>: the forward result of the op.  </li> <li><code>out._prev</code>: the set <code>{self, other}</code> \u2014 i.e., the parents that produced <code>out</code>.  </li> <li><code>out._backward</code>: a closure capturing <code>self</code>, <code>other</code>, and <code>out</code>, used to push gradient contributions back to <code>self.grad</code> and <code>other.grad</code> via local partials.</li> <li>During <code>.backward()</code>, we compute a topological order over the graph using <code>_prev</code> links (parents). We seed the target node\u2019s gradient with <code>1.0</code>, then walk in reverse order, calling each node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#practical-notes-tips","title":"Practical notes &amp; tips","text":"<ul> <li>Zeroing grads: Before a new backward pass, set <code>.grad = 0.0</code> for all leaves to avoid mixing gradients across iterations, just like <code>optimizer.zero_grad()</code> in PyTorch.  </li> <li>Numerical stability: Prefer <code>math.tanh(x)</code> to the closed form for large \\(|x|\\).  </li> <li>Extensibility: New ops just need (1) a forward value, (2) parent tracking in <code>_prev</code>, and (3) a <code>_backward</code> closure with correct local derivatives.  </li> <li>Scalars vs. tensors: This toy engine is scalar-valued. Full frameworks generalize this to tensors, broadcasting rules, and highly optimized kernels.</li> </ul>"},{"location":"nn_training/1_intro/#references","title":"References","text":"<ul> <li>Karpathy, A. micrograd (minimal autograd engine).  </li> <li>PyTorch documentation: Autograd mechanics.  </li> <li>D2L.ai: Automatic differentiation.</li> </ul>"},{"location":"nn_training/2_initial/","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#practical-guide-to-weight-initialization-activation-distributions-dead-neurons-and-gradient-flow-in-deep-neural-networks","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#1-why-weight-initialization-matters","title":"1. Why Weight Initialization Matters","text":"<p>Initialization determines:</p> <ul> <li>how activations propagate forward  </li> <li>how gradients propagate backward  </li> <li>whether neurons remain active  </li> <li>whether the optimizer can begin learning  </li> <li>whether training is stable or diverges  </li> </ul> <p>Poor initialization leads to:</p> <ul> <li>vanishing gradients  </li> <li>exploding gradients  </li> <li>saturated activations (tanh = \u00b11)  </li> <li>dead neurons (ReLU stuck at 0)  </li> <li>slow \u201chockey-stick\u201d learning curves  </li> <li>unstable early training  </li> </ul> <p>Modern deep learning succeeds because initializations are designed to maintain statistical stability across depth.</p>"},{"location":"nn_training/2_initial/#2-the-role-of-n-why-we-divide-by-sqrtn","title":"2. The Role of \\(N\\): Why We Divide by \\(\\sqrt{N}\\)","text":"<p>Consider a neuron:</p> \\[ z = \\sum_{i=1}^{N} w_i x_i \\] <p>If weights and inputs are independent and zero-mean:</p> \\[ \\mathrm{Var}(z) = N \\cdot \\mathrm{Var}(w) \\cdot \\mathrm{Var}(x) \\] <p>Thus:</p> <ul> <li>large \\(N\\) \u2192 exploded activations  </li> <li>small \\(N\\) \u2192 collapsed activations  </li> </ul> <p>To keep the variance stable:</p> \\[ \\mathrm{Var}(w) = \\frac{1}{N} \\quad \\Rightarrow \\quad \\text{std}(w) = \\frac{1}{\\sqrt{N}} \\] <p>This is the core idea behind all modern weight initializers.</p>"},{"location":"nn_training/2_initial/#what-exactly-is-n","title":"What exactly is \\(N\\)?","text":"<p>It depends on the layer:</p> <ul> <li>Dense layer: \\(N = \\text{fan\\_in}\\) </li> <li>Conv layer: \\(N = \\text{in\\_channels} \\times k_h \\times k_w\\) </li> <li>Transformer linear layer: \\(N = d_{\\text{model}}\\) or \\(d_{\\text{ff}}\\) </li> <li>RNN input/recurrent matrix: input or hidden size  </li> </ul> <p>The goal is always to stabilize forward and backward signal flow.</p>"},{"location":"nn_training/2_initial/#3-activation-functions-and-their-stability-regions","title":"3. Activation Functions and Their Stability Regions","text":""},{"location":"nn_training/2_initial/#tanh","title":"Tanh","text":"<ul> <li>useful only near 0  </li> <li>saturates at \u00b11 outside a small input range  </li> <li>derivative approaches 0 \u2192 vanishing gradients  </li> </ul>"},{"location":"nn_training/2_initial/#relu","title":"ReLU","text":"<p>  - linear for \\(z &gt; 0\\) - zero output + zero gradient for \\(z \\le 0\\) </p>"},{"location":"nn_training/2_initial/#gelu-silu-swish-softplus","title":"GELU / SiLU (Swish) / SoftPlus","text":"<ul> <li>smoother transitions  </li> <li>reduce probability of dead neurons  </li> <li>default in Transformers (GELU)</li> </ul> <p>Initialization must place activations in the high-gradient region of whichever activation is used.</p>"},{"location":"nn_training/2_initial/#4-how-tanh-saturation-happens","title":"4. How Tanh Saturation Happens","text":"<p>If pre-activations \\(z\\) have high variance, tanh outputs cluster near \u00b11:</p> <p>This is saturation.</p>"},{"location":"nn_training/2_initial/#why-its-bad","title":"Why it\u2019s bad:","text":"\\[ \\tanh'(z) = 1 - \\tanh^2(z) \\approx 0 \\] <p>Thus almost no gradient flows backward.</p> <p>Tanh neurons become functionally dead when always saturated.</p>"},{"location":"nn_training/2_initial/#5-dead-neurons-in-practice","title":"5. Dead Neurons in Practice","text":""},{"location":"nn_training/2_initial/#51-dead-tanh-neurons","title":"5.1 Dead Tanh Neurons","text":"<p>A tanh neuron is effectively dead if:</p> <ul> <li>\\(z\\) is always large positive or negative  </li> <li>output always \u00b11  </li> <li>derivative \u2248 0  </li> </ul> <p>Causes include:</p> <ul> <li>too-large initial weight variance  </li> <li>unnormalized inputs  </li> <li>deep networks without residuals or normalization  </li> </ul>"},{"location":"nn_training/2_initial/#52-dead-relu-neurons","title":"5.2 Dead ReLU Neurons","text":"<p>A ReLU neuron is dead if:</p> <ul> <li>\\(z \\le 0\\) for all inputs  </li> <li>output always 0  </li> <li>gradient always 0  </li> </ul> <p>Common causes:</p> <ul> <li>weights initialized too negative  </li> <li>bias drift  </li> <li>large learning rates  </li> <li>skewed input distributions  </li> <li>poor initial variance  </li> </ul> <p>Dead ReLUs can sometimes recover (with normalization), but often remain inactive.</p>"},{"location":"nn_training/2_initial/#6-classical-initialization-methods","title":"6. Classical Initialization Methods","text":""},{"location":"nn_training/2_initial/#61-xavier-glorot-tanh-sigmoid","title":"6.1 Xavier / Glorot (Tanh, Sigmoid)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}} \\] <p>Balances forward and backward variance.</p>"},{"location":"nn_training/2_initial/#62-he-kaiming-relu-gelu","title":"6.2 He / Kaiming (ReLU, GELU)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}}} \\] <p>Based on the fact that ReLU zeroes about half of normally distributed inputs.</p>"},{"location":"nn_training/2_initial/#63-orthogonal-initialization-rnns","title":"6.3 Orthogonal Initialization (RNNs)","text":"<p>Stabilizes recurrent dynamics by preserving vector norms.</p>"},{"location":"nn_training/2_initial/#64-lsuv-initialization","title":"6.4 LSUV Initialization","text":"<p>Adjusts initial weights empirically to achieve unit variance layer-by-layer.</p>"},{"location":"nn_training/2_initial/#7-modern-initialization-for-deep-architectures","title":"7. Modern Initialization for Deep Architectures","text":"<p>Deep networks (especially Transformers and ResNets) require additional considerations.</p>"},{"location":"nn_training/2_initial/#71-layernorm-rmsnorm","title":"7.1 LayerNorm / RMSNorm","text":"<p>LayerNorm normalizes activations:</p> <ul> <li>keeps means near zero  </li> <li>standard deviation controlled  </li> <li>prevents drift or saturation  </li> <li>stabilizes attention layers and deep MLP stacks  </li> </ul> <p>Transformers rely heavily on LayerNorm to make training depth-independent.</p>"},{"location":"nn_training/2_initial/#72-residual-connections","title":"7.2 Residual Connections","text":"<p>Residual blocks:</p> \\[ x_{l+1} = x_l + f(x_l) \\] <p>This provides:</p> <ul> <li>an identity path for forward activations  </li> <li>a direct gradient path backward  </li> <li>stability for very deep networks  </li> <li>reduced sensitivity to initialization  </li> </ul> <p>Residuals make it possible to train 50\u20131000+ layer networks.</p>"},{"location":"nn_training/2_initial/#73-transformer-specific-initialization","title":"7.3 Transformer-Specific Initialization","text":"<p>Transformers often use:</p> <ul> <li>weight variance similar to He init  </li> <li>embedding scaling by \\(1/\\sqrt{d_{\\text{model}}}\\) </li> <li>residual scaling by \\(1/\\sqrt{L}\\) or similar  </li> <li>pre-LayerNorm to stabilize depth  </li> <li>\u03bc-parameterization for width-scaling consistency  </li> <li>DeepNorm or FixUp for extremely deep models  </li> </ul> <p>Modern LLMs do not use plain Xavier or He initialization alone.</p>"},{"location":"nn_training/2_initial/#8-expected-loss-at-initialization","title":"8. Expected Loss at Initialization","text":"<p>For a softmax classifier with \\(C\\) classes:</p> \\[ \\mathbb{E}[L_{\\text{init}}] = \\log C \\] <p>This baseline helps diagnose early training issues:</p> <ul> <li>higher than expected \u2192 variance too large  </li> <li>lower \u2192 bias in logits or incorrect initialization  </li> </ul>"},{"location":"nn_training/2_initial/#9-diagnosing-initialization-problems-practical","title":"9. Diagnosing Initialization Problems (Practical)","text":""},{"location":"nn_training/2_initial/#symptom-cause-fix","title":"Symptom \u2192 Cause \u2192 Fix","text":""},{"location":"nn_training/2_initial/#1-loss-flatlines-early","title":"1. Loss flatlines early","text":"<ul> <li>Cause: saturation or dead neurons  </li> <li>Fix: use Xavier/He, reduce LR, add normalization  </li> </ul>"},{"location":"nn_training/2_initial/#2-relu-units-all-zero","title":"2. ReLU units all zero","text":"<ul> <li>Cause: dead ReLUs  </li> <li>Fix: He init, LeakyReLU, LayerNorm, smaller LR  </li> </ul>"},{"location":"nn_training/2_initial/#3-tanh-outputs-at-1","title":"3. Tanh outputs at \u00b11","text":"<ul> <li>Cause: activation variance too large  </li> <li>Fix: Xavier init, normalize inputs, reduce bias scale  </li> </ul>"},{"location":"nn_training/2_initial/#4-exploding-loss","title":"4. Exploding loss","text":"<ul> <li>Cause: weight scale too large  </li> <li>Fix: reduce std, residual scaling, gradient clipping  </li> </ul>"},{"location":"nn_training/2_initial/#5-hockey-stick-learning-curve","title":"5. Hockey-stick learning curve","text":"<ul> <li>Cause: poor initialization or poorly scheduled LR warmup  </li> <li>Fix: check activations, add normalization, adjust LR schedule  </li> </ul>"},{"location":"nn_training/2_initial/#10-architecture-specific-recommendations","title":"10. Architecture-Specific Recommendations","text":""},{"location":"nn_training/2_initial/#cnns","title":"CNNs","text":"<ul> <li>Init: He  </li> <li>Norm: BatchNorm  </li> <li>Activation: ReLU or GELU  </li> <li>Notes: BN stabilizes variance, makes init forgiving  </li> </ul>"},{"location":"nn_training/2_initial/#transformers-llms","title":"Transformers / LLMs","text":"<ul> <li>Init: He-like + residual scaling  </li> <li>Norm: LayerNorm or RMSNorm  </li> <li>Activation: GELU  </li> <li>Notes: initialization must consider depth and residual structure  </li> </ul>"},{"location":"nn_training/2_initial/#rnns","title":"RNNs","text":"<ul> <li>Init: orthogonal for recurrent matrices  </li> <li>Activation: tanh or ReLU  </li> <li>Notes: highly sensitive to saturation; normalization helps  </li> </ul>"},{"location":"nn_training/2_initial/#11-summary","title":"11. Summary","text":"<p>Initialization controls:</p> <ul> <li>activation scale  </li> <li>gradient scale  </li> <li>neuron activity  </li> <li>numerical stability  </li> <li>early learning speed  </li> </ul> <p>Modern deep learning relies on three pillars:</p> <ol> <li>Variance-preserving initialization (Xavier, He, scaled residuals)  </li> <li>Normalization layers (BN, LN, RMSNorm)  </li> <li>Residual connections ensuring robust gradient flow  </li> </ol> <p>Together, they make deep networks trainable, stable, and efficient.</p> <p>If your network isn't learning, the first suspects should be:</p> <ul> <li>initialization  </li> <li>activation distributions  </li> <li>normalization  </li> <li>residual pathways  </li> <li>learning rate  </li> </ul> <p>Understanding these fundamentals is essential for building stable, scalable modern neural networks.</p>"},{"location":"statistics/introd/","title":"Introd","text":"<p>Variational Inference Exact Inference Monte Carlo  Markov Chain MCMC Sampling vs Optimization</p>"},{"location":"tools/1_pytorch/","title":"PyTorch","text":""},{"location":"tools/1_pytorch/#pytorch","title":"PyTorch","text":"<pre><code>import torch\nimport numpy as np\n</code></pre>"},{"location":"tools/1_pytorch/#1-creating-tensors","title":"1. Creating Tensors","text":""},{"location":"tools/1_pytorch/#torchzeros","title":"<code>torch.zeros</code>","text":"<pre><code>x_zeros = torch.zeros(2, 3)\n</code></pre>"},{"location":"tools/1_pytorch/#torchones","title":"<code>torch.ones</code>","text":"<pre><code>x_ones = torch.ones(2, 3)\n</code></pre>"},{"location":"tools/1_pytorch/#torchrand","title":"<code>torch.rand</code>","text":"<pre><code>x_rand = torch.rand(2, 3)\n</code></pre>"},{"location":"tools/1_pytorch/#2-tensor-from-a-python-list","title":"2. Tensor from a Python List","text":"<pre><code>data = [[1,2,3],[4,5,6]]\nx_from_list = torch.tensor(data)\n</code></pre>"},{"location":"tools/1_pytorch/#3-tensor-from-numpy-array","title":"3. Tensor from NumPy Array","text":"<pre><code>np_array = np.array([[1,2,3],[4,5,6]])\nx_from_np = torch.from_numpy(np_array)\n</code></pre>"},{"location":"tools/1_pytorch/#4-shape-batch-dimension","title":"4. Shape &amp; Batch Dimension","text":"<pre><code>images = torch.rand(32,3,64,64)\nbatch = images.shape[0]\nsample = images.shape[1:]\n</code></pre>"},{"location":"tools/1_pytorch/#5-dtype","title":"5. Dtype","text":"<pre><code>torch.zeros(2,2,dtype=torch.float32)\n</code></pre>"},{"location":"tools/1_pytorch/#6-reshaping","title":"6. Reshaping","text":""},{"location":"tools/1_pytorch/#unsqueeze","title":"Unsqueeze","text":"<pre><code>x = torch.tensor([1,2,3,4])\nx.unsqueeze(0)\n</code></pre>"},{"location":"tools/1_pytorch/#squeeze","title":"Squeeze","text":"<pre><code>y = torch.rand(1,3,1,4)\ny.squeeze()\n</code></pre>"},{"location":"tools/1_pytorch/#7-slicing","title":"7. Slicing","text":"<pre><code>x = torch.arange(12).reshape(3,4)\nx[0]\nx[:,1]\nx[0:2,1:3]\n</code></pre>"},{"location":"tools/1_pytorch/#8-item","title":"8. .item()","text":"<pre><code>loss = torch.tensor(3.14)\nloss.item()\n</code></pre>"},{"location":"tools/2_broadcasting/","title":"Broadcasting in NumPy","text":""},{"location":"tools/2_broadcasting/#broadcasting-in-numpy","title":"Broadcasting in NumPy","text":"<p>Broadcasting is a set of rules that NumPy uses to let arrays with different shapes work together in arithmetic operations. When shapes don't match, NumPy aligns dimensions from the right (the trailing dimensions) and tries to stretch dimensions of size <code>1</code> so the arrays become compatible.</p> <ul> <li>A dimension can broadcast if it matches or is <code>1</code>.</li> <li>If two dimensions differ and neither is <code>1</code>, broadcasting fails.</li> </ul>"},{"location":"tools/2_broadcasting/#why-align-from-the-right","title":"Why \"align from the right\"?","text":"<p>NumPy compares array shapes starting from the rightmost dimension, since those describe the element-level structure.</p> <p>Example of right alignment:</p> <pre><code>Array A shape:      (5, 1)\nArray B shape:          (5,)\n                     --------\nAligned shapes:    (5, 1)\n                    (1, 5)\n</code></pre>"},{"location":"tools/2_broadcasting/#simple-example","title":"Simple Example","text":"<pre><code>import numpy as np\n\nA = np.array([[10],\n              [20],\n              [30]])   # shape (3,1)\n\nB = np.array([1, 2, 3])  # shape (3,)\n\n# Broadcasting:\n# A becomes (3,3) by repeating its single column\n# B becomes (1,3) by repeating its single row\nprint(A + B)\n</code></pre> <p>Output:</p> <pre><code>[[11 12 13]\n [21 22 23]\n [31 32 33]]\n</code></pre>"},{"location":"tools/intro/","title":"Intro","text":"<p>Snowflake</p> <p>Agentic frameworks (using CruxAI, Google ADK, LangGraph).</p> <p>Multi-agent systems using frameworks such as LangChain, LangGraph, AutoGen, or CrewAI, with practical understanding of LLM orchestration, retrieval augmentation (RAG), tool calling, and dynamic reasoning.</p> <p>Docker, Kubernetes, and microservices integration.</p> <p>Object-oriented programming skills and experience working with Python, PyTorch and NumPy are desirable Advanced optimisation methods, modern ML techniques, HPC, profiling, model inference; you don\u2019t need to have all of the above Big-data technologies such as Spark, KDB</p>"}]}