{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository is a collection of concepts, algorithms, and case studies in convex optimization \u2014 a unifying framework that sits at the intersection of applied mathematics, computer science, and engineering.</p>"},{"location":"#why-convex-optimization","title":"Why Convex Optimization?","text":"<p>Convex optimisation is one of the central pillars of modern applied mathematics, machine learning, and artificial intelligence. It provides a rich framework in which we can model problems, design algorithms, and guarantee performance. Unlike general non-convex optimisation, convex optimisation problems enjoy the key property that any local solution is also global. This makes them especially important in practice, where reliability, interpretability, and theoretical guarantees are valued.</p>"},{"location":"0a%20LA/","title":"LA Prerequisites","text":""},{"location":"0a%20LA/#linear-algebra-prerequisites","title":"Linear Algebra Prerequisites","text":""},{"location":"0a%20LA/#vector-spaces-and-norms","title":"Vector spaces and norms:","text":"<ul> <li> <p>We work primarily in \\(\\mathbb{R}^n\\), the \\(n\\)-dimensional Euclidean space. The Euclidean norm is \\(\\|x\\|_2 = \\sqrt{x^T x}\\), but other norms, such as \\(\\|x\\|_1\\) or \\(\\|x\\|_\\infty\\), are also important.</p> </li> <li> <p>Affine sets: A set of the form  \\(\\{x \\in \\mathbb{R}^n : Ax = b\\}\\), where \\(A\\) is a matrix and \\(b\\) is a vector. Affine sets are the natural generalisation of lines and planes.</p> </li> <li> <p>Linear independence: A set of vectors \\(\\{v_1, v_2, \\dots, v_k\\}\\) is linearly independent if    \\(\\alpha_1 v_1 + \\alpha_2 v_2 + \\cdots + \\alpha_k v_k = 0\\)   implies \\(\\alpha_1 = \\alpha_2 = \\cdots = \\alpha_k = 0\\).</p> </li> </ul>"},{"location":"0a%20LA/#inner-outer-products-projection","title":"Inner &amp; Outer products &amp; Projection","text":"<ul> <li> <p>An inner product in \\(\\mathbb{R}^n\\) is \\(\\langle x, y \\rangle = x^T y\\).</p> </li> <li> <p>The angle \\(\\theta\\) between two nonzero vectors \\(x, y \\in \\mathbb{R}^n\\) is defined as@  </p> <p>\\(\\cos \\theta = \\dfrac{\\langle x, y \\rangle}{\\|x\\|_2 \\, \\|y\\|_2}\\)</p> </li> <li> <p>Given vectors \\(u, v \\in \\mathbb{R}^n\\), their outer product is   \\(u v^T\\), which is an \\(n \\times n\\) matrix.  (Contrast with the inner product \\(u^T v\\), which is a scalar.)</p> </li> <li> <p>Cauchy\u2013Schwarz inequality: For any \\(x, y \\in \\mathbb{R}^n\\), \\(|\\langle x, y \\rangle| \\leq \\|x\\|_2 \\, \\|y\\|_2\\). Equality holds iff \\(x\\) and \\(y\\) are linearly dependent.</p> </li> <li> <p>The projection of \\(x\\) onto \\(y\\) (with \\(y \\neq 0\\)) is  \\(\\text{proj}_y(x) = \\dfrac{\\langle x, y \\rangle}{\\langle y, y \\rangle} y\\). This gives the component of \\(x\\) in the direction of \\(y\\).</p> </li> <li> <p>Subspace spanned by \\(k\\) perpendicular (orthogonal) vectors: If \\(\\{u_1, u_2, \\dots, u_k\\}\\) are mutually perpendicular (orthogonal) and nonzero, then they form an orthogonal basis for their span. The subspace is \\(U = \\text{span}\\{u_1, u_2, \\dots, u_k\\}\\),  and any \\(x \\in U\\) can be uniquely written as \\(x = \\alpha_1 u_1 + \\alpha_2 u_2 + \\cdots + \\alpha_k u_k\\) with coefficients \\(\\alpha_i = \\dfrac{\\langle x, u_i \\rangle}{\\langle u_i, u_i \\rangle}\\).</p> </li> <li> <p>Projection onto a subspace:  Let \\(U = \\text{span}\\{u_1, u_2, \\dots, u_k\\}\\), where the vectors are linearly independent. If \\(U\\) has orthonormal basis \\(\\{q_1, q_2, \\dots, q_k\\}\\), then the projection of \\(x\\) onto \\(U\\) is   \\(\\text{proj}_U(x) = \\sum_{i=1}^k \\langle x, q_i \\rangle q_i\\). In matrix form, if \\(Q = [q_1 \\; q_2 \\; \\dots \\; q_k]\\), then \\(\\text{proj}_U(x) = QQ^T x\\).</p> </li> <li> <p>Projection onto a convex set:  Projecting two points on convex set is not expansive i.e. the distance between projection of points is always less than equal to the original distance between the points.</p> </li> </ul>"},{"location":"0a%20LA/#matrix-concepts","title":"Matrix Concepts","text":"<ul> <li> <p>Rank:  The rank of a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), denoted \\(\\text{rank}(A)\\), is the dimension of its column space (or equivalently, row space). It is the number of linearly independent columns (or rows). \\(\\text{rank}(A) \\leq \\min(m,n)\\).  </p> </li> <li> <p>Null space (kernel): The null space of \\(A \\in \\mathbb{R}^{m \\times n}\\) is /  It contains all solutions to the homogeneous system \\(Ax=0\\). Its dimension is called the nullity of \\(A\\). By the rank\u2013nullity theorem \\(\\text{rank}(A) + \\text{nullity}(A) = n.\\)</p> </li> <li> <p>Determinant:  For a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), the determinant \\(\\det(A)\\) is a scalar with these properties:  </p> <ul> <li>\\(\\det(A) = 0 \\iff A\\) is singular (non-invertible).  </li> <li>\\(\\det(AB) = \\det(A)\\det(B)\\).  </li> <li>\\(\\det(A^T) = \\det(A)\\).  </li> <li>Geometric meaning: \\(|\\det(A)|\\) gives the volume scaling factor of the linear map \\(x \\mapsto Ax\\).</li> </ul> </li> <li> <p>Range (column space): The range of \\(A \\in \\mathbb{R}^{m \\times n}\\) is \\(\\mathcal{R}(A) = \\{Ax : x \\in \\mathbb{R}^n\\} \\subseteq \\mathbb{R}^m.\\) It is the span of the columns of \\(A\\). \\(\\dim(\\mathcal{R}(A)) = \\text{rank}(A)\\).  </p> </li> </ul>"},{"location":"0a%20LA/#subspaces-and-related-concepts","title":"Subspaces and Related Concepts","text":"<ul> <li> <p>Subspace: A subset \\(U \\subseteq \\mathbb{R}^n\\) is a subspace if it satisfies:  </p> <ol> <li>\\(0 \\in U\\) (contains the zero vector)  </li> <li>Closed under addition: \\(u, v \\in U \\implies u+v \\in U\\) </li> <li>Closed under scalar multiplication: \\(u \\in U, \\alpha \\in \\mathbb{R} \\implies \\alpha u \\in U\\) </li> </ol> <p>Examples: column space of a matrix, null space of a matrix, \\(\\mathbb{R}^n\\) itself.  </p> </li> <li> <p>Orthogonal complement: For a subspace \\(U \\subseteq \\mathbb{R}^n\\), the orthogonal complement \\(U^\\perp\\) is </p> <ul> <li>\\(U^\\perp\\) is a subspace.  </li> <li>\\(\\dim(U) + \\dim(U^\\perp) = n\\).  </li> <li>\\((U^\\perp)^\\perp = U\\).  </li> </ul> </li> <li> <p>Affine subspace:  An affine subspace is a translation of a subspace. Formally,    where \\(U\\) is a subspace and \\(x_0 \\in \\mathbb{R}^n\\) is a fixed point.  </p> <ul> <li>Examples: lines or planes that do not pass through the origin.  </li> <li>If \\(x_0 = 0\\), the affine subspace is just a subspace.</li> </ul> </li> </ul>"},{"location":"0a%20LA/#eigenvalues-eigenvectors-and-symmetric-matrices","title":"Eigenvalues, Eigenvectors, and Symmetric Matrices","text":"<ul> <li> <p>Eigenvalue and Eigenvector: For a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), a scalar \\(\\lambda \\in \\mathbb{R}\\) (or \\(\\mathbb{C}\\)) is an eigenvalue if there exists a nonzero vector \\(v \\in \\mathbb{R}^n\\) such that    The vector \\(v\\) is called an eigenvector corresponding to \\(\\lambda\\).  </p> <ul> <li>\\(v \\neq 0\\) </li> <li>\\(\\det(A - \\lambda I) = 0\\) gives the characteristic equation to find eigenvalues.  </li> </ul> </li> <li> <p>Symmetric matrix:  A matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric if </p> <ol> <li>All eigenvalues of a symmetric matrix are real.  Eigenvectors of symmetric matrices are orthogonal. </li> <li>There exists an orthonormal set of eigenvectors that spans \\(\\mathbb{R}^n\\).  </li> <li>\\(A\\) can be diagonalized as    where \\(Q\\) is an orthogonal matrix of eigenvectors (\\(Q^T Q = I\\)) and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.  </li> </ol> </li> <li> <p>Positive Semidefinite Matrices   A symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is positive semidefinite (PSD) if </p> <ol> <li>\\(A\\) is symmetric: \\(A = A^T\\).  </li> <li>All eigenvalues of \\(A\\) are non-negative: \\(\\lambda_i \\ge 0\\).  </li> <li>If \\(x^T A x &gt; 0\\) for all \\(x \\neq 0\\), then \\(A\\) is positive definite (PD).  </li> <li>\\(A\\) can be diagonalized as    where \\(Q\\) is orthogonal and \\(\\Lambda\\) is a diagonal matrix with \\(\\lambda_i \\ge 0\\).  </li> <li>\\(x^T A x \\geq 0\\) if A is PSD. </li> <li>for any matrix A, \\(A^TA\\) and \\(AA^T\\) are PSD.</li> </ol> </li> </ul>"},{"location":"0a%20LA/#continuity-and-lipschitz-continuity","title":"Continuity and Lipschitz Continuity","text":"<ul> <li> <p>Continuity: A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}^m\\) is continuous at \\(x_0\\) if    Equivalently, for every \\(\\varepsilon &gt; 0\\) there exists \\(\\delta &gt; 0\\) such that    If this holds for all \\(x_0\\), then \\(f\\) is continuous on its domain.  </p> </li> <li> <p>Lipschitz continuity:  A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}^m\\) is Lipschitz continuous if there exists a constant \\(L \\geq 0\\) such that </p> <ul> <li>The smallest such \\(L\\) is called the Lipschitz constant.  </li> <li>Lipschitz continuity \\(\\implies\\) continuity (but not vice versa).  </li> <li>If \\(f\\) is differentiable and \\(\\|\\nabla f(x)\\| \\leq L\\) for all \\(x\\), then \\(f\\) is Lipschitz continuous with constant \\(L\\).  </li> </ul> </li> </ul>"},{"location":"0a%20LA/#gradient-and-hessian","title":"Gradient and Hessian","text":"<ul> <li> <p>Gradient:  Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be differentiable.   The gradient of \\(f\\) at \\(x\\) is the vector of partial derivatives: </p> <ul> <li>It points in the direction of steepest ascent of \\(f\\).  </li> <li>The magnitude \\(\\|\\nabla f(x)\\|\\) gives the rate of increase in that direction.  </li> </ul> </li> <li> <p>Hessian:  If \\(f\\) is twice differentiable, the Hessian matrix at \\(x\\) is the matrix of second-order partial derivatives: </p> <ul> <li>The Hessian is symmetric if \\(f\\) is twice continuously differentiable.  </li> <li>It describes the curvature of \\(f\\):  <ul> <li>\\(\\nabla^2 f(x) \\succeq 0\\) (PSD) \\(\\implies f\\) is locally convex.  </li> <li>\\(\\nabla^2 f(x) \\succ 0\\) (PD) \\(\\implies f\\) is locally strictly convex.  </li> </ul> </li> </ul> </li> </ul>"},{"location":"0b%20Convex%20Sets/","title":"Convex Sets","text":""},{"location":"0b%20Convex%20Sets/#convex-set","title":"Convex Set","text":"<ul> <li>Convexity of sets: A set \\(C\\) is convex if for any \\(x_1, x_2 \\in C\\) and \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta) x_2 \\in C\\).</li> <li>Closed sets: A set is closed if it contains all its limit points. The closure of a set is the smallest closed set containing it.</li> <li>Extreme points: A point in a convex set is extreme if it cannot be expressed as a convex combination of two other distinct points in the set. For polyhedra, extreme points correspond to vertices.</li> </ul>"},{"location":"0b%20Convex%20Sets/#convex-combination","title":"Convex Combination","text":"<p>A convex combination of \\(x_1, \\dots, x_k\\) is  </p> <p>This is simply a weighted average where weights are nonnegative and sum to 1.</p>"},{"location":"0b%20Convex%20Sets/#convex-hull","title":"Convex Hull","text":"<p>The convex hull of a set \\(S\\) is the collection of all convex combinations of points in \\(S\\). It is the smallest convex set containing \\(S\\).</p> <p><code>Geometric intuition: Imagine stretching a rubber band around the points; the enclosed region is the convex hull.</code></p>"},{"location":"0b%20Convex%20Sets/#cones","title":"Cones","text":"<ul> <li> <p>A  cone is a set \\(K\\) such that if \\(x \\in K\\) and \\(\\alpha \\geq 0\\), then \\(\\alpha x \\in K\\).   In words: a cone is closed under nonnegative scalar multiplication.  </p> </li> <li> <p>The conic hull (or convex cone) of a set \\(S\\) is the collection of all conic combinations of points in \\(S\\): </p> </li> <li> <p>A cone is not necessarily a subspace (since a subspace allows all linear combinations, including negative multiples). However, every subspace is a cone (because it is closed under nonnegative scaling).  </p> </li> <li> <p>A cone is not necessarily convex. To be convex, a set must be closed under addition and convex combinations, which is not guaranteed for a general cone. A convex cone is a cone that is also convex, i.e., closed under nonnegative linear combinations.</p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#polar-cones","title":"Polar Cones","text":"<ul> <li> <p>Given a cone \\(K \\subseteq \\mathbb{R}^n\\), the polar cone of \\(K\\) is defined as </p> </li> <li> <p>Intuitively, the polar cone consists of all vectors that form a non-acute angle (inner product \\(\\leq 0\\)) with every vector in \\(K\\).  </p> </li> <li> <p>The polar of any cone is always a closed convex cone, even if the original cone \\(K\\) is not convex or not closed.  </p> </li> <li> <p>If \\(K\\) is a subspace, then its polar cone \\(K^\\circ\\) is the orthogonal complement of \\(K\\).  </p> </li> <li> <p>A useful duality property: for a closed convex cone \\(K\\), </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#tangent-cone","title":"Tangent Cone","text":"<ul> <li>Given a set \\(C \\subseteq \\mathbb{R}^n\\) and a point \\(x \\in C\\), the tangent cone (also called the contingent cone) at \\(x\\) is defined as </li> <li>Intuitively: \\(T_C(x)\\) is the set of all directions \\(d\\) in which you can \u201cmove infinitesimally\u201d inside \\(C\\) starting from \\(x\\).</li> <li>If \\(x\\) is in the interior of \\(C\\), then \\(T_C(x) = \\mathbb{R}^n\\).  </li> <li>If \\(x\\) is on the boundary, \\(T_C(x)\\) consists of directions that keep you inside \\(C\\) locally.</li> </ul>"},{"location":"0b%20Convex%20Sets/#normal-cone","title":"Normal Cone","text":"<ul> <li> <p>For a convex set \\(C \\subseteq \\mathbb{R}^n\\) and a point \\(x \\in C\\), the normal cone at \\(x\\) is defined as </p> </li> <li> <p>Each \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\):  </p> </li> </ul> <p> </p> <ul> <li> <p>Relationship: the normal cone is the polar cone of the tangent cone: </p> </li> <li> <p>\\(N_C(x)\\) is always a closed convex cone.</p> </li> <li>Intuitively: \\(N_C(x)\\) is the set of vectors pointing \u201coutward\u201d and supporting the set \\(C\\) at \\(x\\).  </li> <li>Interior point of \\(C\\): \\(N_C(x) = \\{0\\}\\) (no outward direction, since \\(x\\) is surrounded).  </li> <li>Boundary point: \\(N_C(x)\\) contains outward-pointing directions normal to the boundary.  </li> <li>Corner/vertex: \\(N_C(x)\\) is a cone of outward normals, capturing multiple \u201cfaces\u201d meeting at \\(x\\).  </li> </ul>"},{"location":"0b%20Convex%20Sets/#why-is-the-normal-cone-important","title":"Why is the normal cone important?","text":""},{"location":"0b%20Convex%20Sets/#first-order-optimality-condition","title":"First-order optimality condition","text":"<ul> <li> <p>For the problem \\(\\min f(x)\\) subject to \\(x \\in C\\), a point \\(x^*\\) is optimal if:  </p> <p> </p> <p>This says the slope (subgradient) of \\(f\\) is exactly balanced by the \u201cpush back\u201d of the constraint set.  </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#connection-to-tangent-cone","title":"Connection to tangent cone","text":"<ul> <li> <p>The normal cone is the polar cone of the tangent cone:  </p> <p> </p> <p>This duality links feasible directions (tangent cone) with blocking directions (normal cone).  </p> </li> </ul>"},{"location":"0b%20Convex%20Sets/#geometric-interpretation","title":"Geometric interpretation","text":"<ul> <li>Normals describe which directions leave the set if you try to move outward.  </li> <li>They capture the \u201cboundary geometry\u201d of \\(C\\), generalizing the idea of perpendicular vectors to surfaces. </li> </ul>"},{"location":"0b%20Convex%20Sets/#comparison-of-tangent-normal-and-polar-cones","title":"Comparison of Tangent, Normal, and Polar Cones","text":"Cone Applies To Meaning Interior Point Boundary Point Key Facts Tangent \\(T_C(x)\\) Any set (convex nicer) Feasible move directions \\(T_C(x)=\\mathbb{R}^n\\) Restricted to stay in \\(C\\) Local geometry of \\(C\\) Normal \\(N_C(x)\\) Convex sets (gen. exist) Outward blocking dirs \\(N_C(x)=\\{0\\}\\) Outward rays/cones Closed, convex; \\(N_C(x)=(T_C(x))^\\circ\\) Polar \\(K^\\circ\\) Any cone \\(K\\) Non-acute dirs wrt \\(K\\) Not point-specific Not point-specific Closed, convex; \\((K^\\circ)^\\circ=K\\) if closed convex"},{"location":"0b%20Convex%20Sets/#hyperplanes-and-half-spaces","title":"Hyperplanes and Half-spaces","text":"<ul> <li>A hyperplane is the solution set of \\(a^T x = b\\).</li> <li>A half-space is one side of a hyperplane, defined as \\(a^T x \\leq b\\) or \\(a^T x \\geq b\\).</li> <li>These objects are convex and serve as building blocks in constraints.</li> </ul> <p>Separation and Supporting Hyperplanes: One of the most powerful results in convex geometry is the separating hyperplane theorem: two disjoint convex sets can be separated by a hyperplane. For a convex set \\(C\\) and a point \\(x \\notin C\\), there exists a hyperplane that separates \\(x\\) from \\(C\\). This underpins duality theory in optimisation. A supporting hyperplane touches a convex set at one or more points but does not cut through it.</p>"},{"location":"0c%20Convex%20Functions/","title":"Convex Functions","text":""},{"location":"0c%20Convex%20Functions/#convex-function","title":"Convex Function","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if its domain \\(\\mathrm{dom}(f) \\subseteq \\mathbb{R}^n\\) is convex and, for all \\(x_1, x_2 \\in \\mathrm{dom}(f)\\) and all \\(\\theta \\in [0,1]\\):</p> \\[ f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta) f(x_2) \\] <ul> <li>Convex domain: For any \\(x_1, x_2 \\in \\mathrm{dom}(f)\\), the line segment connecting them lies entirely in \\(\\mathrm{dom}(f)\\).  </li> <li>The graph of \\(f\\) lies below or on the straight line connecting any two points on it (\u201cbowl-shaped\u201d).</li> </ul>"},{"location":"0c%20Convex%20Functions/#first-order-condition","title":"First-order condition:","text":"<p>For a convex function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) defined on a convex domain \\(\\mathrm{dom}(f)\\):</p>"},{"location":"0c%20Convex%20Functions/#1-differentiable-case","title":"1. Differentiable case","text":"<p>If \\(f\\) is differentiable at \\(x\\), the gradient \\(\\nabla f(x)\\) satisfies, for all \\(y \\in \\mathrm{dom}(f)\\):</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^T (y - x) \\] <ul> <li>Geometric meaning: The tangent hyperplane at \\(x\\) lies below the graph of \\(f\\) at all points.  </li> <li>Domain of gradient: \\(\\nabla f(x)\\) exists for all \\(x \\in \\mathrm{dom}(f)\\).</li> </ul>"},{"location":"0c%20Convex%20Functions/#2-non-differentiable-case-subgradients","title":"2. Non-differentiable case (subgradients)","text":"<p>If \\(f\\) is convex but not differentiable at \\(x\\), a subgradient \\(g \\in \\mathbb{R}^n\\) satisfies:</p> \\[ f(y) \\ge f(x) + g^T (y - x), \\quad \\forall y \\in \\mathrm{dom}(f) \\] <ul> <li>The set of all such \\(g\\) is called the subdifferential at \\(x\\):  </li> </ul> \\[ \\partial f(x) = \\{ g \\in \\mathbb{R}^n \\mid f(y) \\ge f(x) + g^T (y - x), \\ \\forall y \\in \\mathrm{dom}(f) \\} \\] <ul> <li>Geometric meaning: Even at a \"kink,\" there exists a hyperplane (with slope \\(g\\)) that lies below the graph at all points.  </li> <li>If \\(f\\) is differentiable at \\(x\\), \\(\\partial f(x) = \\{\\nabla f(x)\\}\\).</li> </ul>"},{"location":"0c%20Convex%20Functions/#second-order-hessian-condition","title":"Second-order (Hessian) condition:","text":"<p>If \\(f\\) is twice differentiable, \\(f\\) is convex if and only if its Hessian matrix \\(\\nabla^2 f(x)\\) is positive semidefinite for all \\(x \\in \\mathrm{dom}(f)\\):</p> \\[ \\nabla^2 f(x) \\succeq 0 \\] <p>Positive semidefinite Hessian means the function curves upward or is flat in all directions, never downward.  </p> <p><code>\u201cCurvature is nonnegative in all directions.\u201d</code></p>"},{"location":"0c%20Convex%20Functions/#examples-of-convex-functions","title":"Examples of Convex Functions","text":"<ol> <li>Quadratic functions: \\(f(x) = \\frac{1}{2} x^T Q x + b^T x + c\\), where \\(Q \\succeq 0\\) (positive semidefinite).  </li> <li>Norms: \\(\\|x\\|_p\\) for \\(p \\ge 1\\).  </li> <li>Exponential function: \\(f(x) = e^x\\).  </li> <li>Negative logarithm: \\(f(x) = -\\log(x)\\) on \\(x &gt; 0\\).  </li> <li>Linear functions: \\(f(x) = a^T x + b\\).  </li> </ol>"},{"location":"0c%20Convex%20Functions/#subgradients-proximal-operators","title":"Subgradients &amp; Proximal Operators","text":"<p>Modern optimization in machine learning often deals with nonsmooth functions e.g., \\(L_1\\) regularization, hinge loss in SVMs, indicator constraints. Gradients are not always defined at these nonsmooth points, so we need subgradients and proximal operators. For a differentiable convex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\), the gradient \\(\\nabla f(x)\\) provides the slope for descent. But many convex functions are not differentiable everywhere:</p> <ul> <li>Absolute value: \\(f(x) = |x|\\)    (non-differentiable at \\(x=0\\))  </li> <li>Hinge loss: \\(f(x) = \\max(0, 1-x)\\) </li> <li>\\(L_1\\) norm: \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\) </li> </ul> <p>At kinks/corners, derivatives don\u2019t exist.  </p> <p>A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of a convex function \\(f\\) at point \\(x\\) if:</p> \\[f(y) \\;\\ge\\; f(x) + g^\\top (y-x), \\quad \\forall y \\in \\mathbb{R}^n\\] <ul> <li>Geometric meaning: \\(g\\) defines a supporting hyperplane at \\((x, f(x))\\) that lies below the function everywhere.  </li> <li>The set of all subgradients at \\(x\\) is called the subdifferential, written:    </li> </ul>"},{"location":"0c%20Convex%20Functions/#example-1-absolute-value","title":"Example 1: Absolute value","text":"<p>Take \\(f(x) = |x|\\).  </p> <ul> <li>If \\(x &gt; 0\\): \\(\\nabla f(x) = 1\\).  </li> <li>If \\(x &lt; 0\\): \\(\\nabla f(x) = -1\\).  </li> <li>If \\(x = 0\\): derivative doesn\u2019t exist. But  Any slope between \\(-1\\) and \\(1\\) is a valid subgradient at the kink.  Intuition: At \\(x=0\\), instead of one tangent line, there\u2019s a whole fan of supporting lines.</li> </ul>"},{"location":"0c%20Convex%20Functions/#example-2-hinge-loss","title":"Example 2: Hinge loss","text":"<p>\\(f(x) = \\max(0, 1-x)\\).  </p> <ul> <li>If \\(x &lt; 1\\): \\(\\nabla f(x) = -1\\).  </li> <li>If \\(x &gt; 1\\): \\(\\nabla f(x) = 0\\).  </li> <li>If \\(x = 1\\): </li> </ul>"},{"location":"0c%20Convex%20Functions/#why-subgradients-matter","title":"Why subgradients matter","text":"<ul> <li>They generalize gradients to nonsmooth convex functions.  </li> <li>Subgradient descent update:    </li> <li>Convergence is guaranteed, though slower than gradient descent:</li> <li>Smooth case: \\(O(1/k)\\) rate  </li> <li>Nonsmooth case: \\(O(1/\\sqrt{k})\\) rate  </li> </ul>"},{"location":"0c%20Convex%20Functions/#proximal-operators","title":"Proximal Operators","text":"<p>Nonsmooth penalties (like \\(L_1\\) norm, indicator functions) appear frequently: - Lasso: \\(\\min_x \\tfrac{1}{2}\\|Ax-b\\|^2 + \\lambda \\|x\\|_1\\)  (L1 norm is nonsmooth) - SVM: hinge loss \\(\\max(0, 1-y\\langle w,x\\rangle)\\) - Constraints: e.g., \\(x \\in C\\) for some convex set \\(C\\)</p> <p>Plain gradient descent cannot directly handle the nonsmooth part.</p> <p>The proximal operator of a function \\(g\\) with step size \\(\\alpha &gt; 0\\) is:</p> \\[ \\text{prox}_{\\alpha g}(v)  = \\arg\\min_x \\Big( g(x) + \\frac{1}{2\\alpha}\\|x-v\\|^2 \\Big). \\] <ul> <li>Interpretation:  </li> <li>Stay close to \\(v\\) (the quadratic term)  </li> <li> <p>While reducing the penalty \\(g(x)\\) </p> </li> <li> <p>Geometric meaning: A regularized projection of \\(v\\) onto a region encouraged by \\(g\\).  </p> </li> </ul>"},{"location":"0c%20Convex%20Functions/#example-1-l_1-norm-soft-thresholding","title":"Example 1: \\(L_1\\) norm (soft-thresholding)","text":"<p>Let \\(g(x) = \\lambda \\|x\\|_1 = \\lambda \\sum_i |x_i|\\). Then:</p> \\[ \\text{prox}_{\\alpha g}(v)_i =  \\begin{cases} v_i - \\alpha\\lambda, &amp; v_i &gt; \\alpha \\lambda \\\\ 0, &amp; |v_i| \\le \\alpha \\lambda \\\\ v_i + \\alpha\\lambda, &amp; v_i &lt; -\\alpha \\lambda \\end{cases} \\] <p>This is the soft-thresholding operator:</p> <ul> <li>Shrinks small entries of \\(v\\) to zero \u2192 sparsity.  </li> <li>Reduces magnitude of large entries but keeps their sign.  </li> </ul> <p>\ud83d\udc49 This is the key step in Lasso regression and compressed sensing.</p>"},{"location":"0c%20Convex%20Functions/#example-2-indicator-function","title":"Example 2: Indicator function","text":"<p>Let \\(g(x) = I_C(x)\\), where \\(I_C(x)=0\\) if \\(x \\in C\\), and \\(\\infty\\) otherwise. Then:</p> \\[ \\text{prox}_{\\alpha g}(v) = \\Pi_C(v), \\] <p>the Euclidean projection of \\(v\\) onto \\(C\\).  </p> <p>Example: if \\(C\\) is the unit ball \\(\\{x: \\|x\\|\\le 1\\}\\), prox just normalizes \\(v\\) if it\u2019s outside.</p>"},{"location":"0c%20Convex%20Functions/#example-3-squared-ell_2-norm","title":"Example 3: Squared \\(\\ell_2\\) norm","text":"<p>If \\(g(x) = \\frac{\\lambda}{2}\\|x\\|^2\\), then</p> \\[ \\text{prox}_{\\alpha g}(v) = \\frac{1}{1+\\alpha\\lambda} v. \\] <p>This is just a shrinkage toward the origin.</p>"},{"location":"0c%20Convex%20Functions/#why-proximal-operators-matter","title":"Why proximal operators matter","text":"<p>They allow efficient algorithms for composite objectives:</p> \\[ \\min_x f(x) + g(x), \\] <p>where: - \\(f\\) is smooth (differentiable with Lipschitz gradient) - \\(g\\) is convex but possibly nonsmooth  </p> <p>Proximal gradient method (ISTA):  </p> <p>This generalizes gradient descent by replacing the plain update with a proximal step that handles \\(g\\).</p> <ul> <li>If \\(g=0\\): reduces to gradient descent  </li> <li>If \\(f=0\\): reduces to proximal operator (e.g. projection, shrinkage)  </li> </ul>"},{"location":"0c%20Convex%20Functions/#3-intuition-summary","title":"3. Intuition Summary","text":"<ul> <li>Subgradients:  </li> <li>Generalized \u201cslopes\u201d for nonsmooth convex functions.  </li> <li>At corners, we have a set of possible slopes (subdifferential).  </li> <li> <p>Enable subgradient descent with convergence guarantees.  </p> </li> <li> <p>Proximal operators:  </p> </li> <li>Generalized update steps for nonsmooth regularizers.  </li> <li>Combine a gradient-like move with a \u201ccorrection\u201d that enforces structure (sparsity, constraints).  </li> <li>Core of algorithms like ISTA, FISTA, ADMM.  </li> </ul>"},{"location":"0c%20Convex%20Functions/#4-big-picture-in-ml","title":"4. Big Picture in ML","text":"<ul> <li>Subgradients: Let us train models with nonsmooth losses (SVM hinge loss, \\(L_1\\)).  </li> <li>Proximal operators: Let us efficiently solve regularized problems (Lasso, group sparsity, constrained optimization).  </li> <li>Intuition:  </li> <li>Subgradient = \"any slope that supports the function\"  </li> <li>Proximal = \"soft move toward minimizing the nonsmooth part\"  </li> </ul>"},{"location":"0c%20Convex%20Functions/#subgradients-proximal-operators_1","title":"Subgradients &amp; Proximal Operators","text":"<ul> <li>Subgradient: \\(g\\) is a subgradient if </li> <li>Proximal operator: </li> <li>Context: Needed for nonsmooth functions (e.g., L1-regularization, hinge loss).  </li> <li>ML relevance: SVM hinge loss, Lasso, sparse dictionary learning. Proximal methods handle shrinkage or projection efficiently.  </li> <li>Intuition:  </li> <li>Subgradient: Like a tangent for a function that isn\u2019t smooth\u2014provides a direction to descend.  </li> <li>Proximal operator: Think of it as a \u201csoft step\u201d toward minimizing a nonsmooth function, like gently nudging a point toward a feasible or sparse region.</li> </ul>"},{"location":"0c%20Convex%20Functions/#convex-optimisation-problems","title":"Convex Optimisation Problems","text":"<p>A convex optimisation problem has the form:</p> <p>  where \\(f_0\\) and \\(f_i\\) are convex functions, and \\(h_j\\) are affine. The feasible set is convex, and any local minimum is a global minimum.</p>"},{"location":"0d%20Algos/","title":"0d Algos","text":""},{"location":"0d%20Algos/#algorithms-for-convex-optimisation","title":"Algorithms for Convex Optimisation","text":"<p>Convex optimization algorithms exploit the geometry of convex sets and functions. Because every local minimum is global, algorithms can converge reliably without worrying about bad local minima.</p> <p>We divide algorithms into three main families:</p> <ol> <li>First-order methods \u2013 use gradients (scalable, but slower convergence).  </li> <li>Second-order methods \u2013 use Hessians (faster convergence, more expensive).  </li> <li>Interior-point methods \u2013 general-purpose, highly accurate solvers.</li> </ol>"},{"location":"0d%20Algos/#gradient-descent-first-order-method","title":"Gradient Descent (First-Order Method)","text":""},{"location":"0d%20Algos/#algorithm","title":"Algorithm","text":"<p>For step size \\(\\alpha &gt; 0\\):  </p>"},{"location":"0d%20Algos/#convergence","title":"Convergence","text":"<ul> <li>If \\(f\\) is convex and \\(\\nabla f\\) is Lipschitz continuous with constant \\(L\\):</li> <li>With fixed step \\(\\alpha \\le \\tfrac{1}{L}\\), we have:      </li> <li>If \\(f\\) is \\(\\mu\\)-strongly convex:          (linear convergence).</li> </ul>"},{"location":"0d%20Algos/#pros-cons","title":"Pros &amp; Cons","text":"<ul> <li>\u2705 Simple, scalable to very high dimensions.  </li> <li>\u274c Slow convergence compared to higher-order methods.</li> </ul>"},{"location":"0d%20Algos/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<ul> <li>Nesterov\u2019s Accelerated Gradient (NAG): Improves convergence rate from \\(\\mathcal{O}(1/k)\\) to \\(\\mathcal{O}(1/k^2)\\).  </li> <li>Widely used in machine learning (e.g., training deep neural networks).  </li> </ul>"},{"location":"0d%20Algos/#newtons-method-second-order-method","title":"Newton\u2019s Method (Second-Order Method)","text":""},{"location":"0d%20Algos/#algorithm_1","title":"Algorithm","text":"<p>Update rule:  </p>"},{"location":"0d%20Algos/#convergence_1","title":"Convergence","text":"<ul> <li>Quadratic near optimum: </li> <li>Very fast, but requires Hessian and solving linear systems.</li> </ul>"},{"location":"0d%20Algos/#damped-newton","title":"Damped Newton","text":"<p>To maintain global convergence:  </p>"},{"location":"0d%20Algos/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Approximate the Hessian to reduce cost.</p> <ul> <li>BFGS and L-BFGS (limited memory version).  </li> <li>Used in large-scale optimization (e.g., machine learning, statistics).  </li> <li>Convergence: superlinear.  </li> </ul>"},{"location":"0d%20Algos/#subgradient-methods","title":"Subgradient Methods","text":"<p>For nondifferentiable convex functions (e.g., \\(f(x) = \\|x\\|_1\\)).</p> <p>Update rule:  </p> <ul> <li>\\(\\partial f(x)\\): subdifferential (set of all subgradients).  </li> <li>Convergence: \\(\\mathcal{O}(1/\\sqrt{k})\\) with diminishing step sizes.  </li> <li>Useful in large-scale, nonsmooth optimization.</li> </ul>"},{"location":"0d%20Algos/#proximal-methods","title":"Proximal Methods","text":"<p>For composite problems:  where \\(f\\) is smooth convex, \\(g\\) convex but possibly nonsmooth.</p> <p>Proximal operator: </p> <ul> <li>Proximal gradient descent: </li> <li>Widely used in sparse optimization (e.g., Lasso).</li> </ul>"},{"location":"0d%20Algos/#interior-point-methods","title":"Interior-Point Methods","text":"<p>Transform constrained problem into a sequence of unconstrained problems using barrier functions.</p> <p>For constraint \\(g_i(x) \\le 0\\), replace with barrier:  </p> <p>Solve:  for increasing \\(t\\).</p>"},{"location":"0d%20Algos/#properties","title":"Properties","text":"<ul> <li>Polynomial-time complexity for convex problems.  </li> <li>Extremely accurate solutions.  </li> <li>Basis of general-purpose solvers (e.g., CVX, MOSEK, Gurobi).  </li> </ul>"},{"location":"0d%20Algos/#coordinate-descent","title":"Coordinate Descent","text":"<p>At each iteration, optimize w.r.t. one coordinate (or block of coordinates):</p> \\[ x_i^{k+1} = \\arg\\min_{z} f(x_1^k, \\dots, x_{i-1}^k, z, x_{i+1}^k, \\dots, x_n^k) \\] <ul> <li>Works well for high-dimensional problems.  </li> <li>Used in Lasso, logistic regression, and large-scale ML problems.</li> </ul>"},{"location":"0d%20Algos/#primal-dual-and-splitting-methods","title":"Primal-Dual and Splitting Methods","text":"<ul> <li> <p>ADMM (Alternating Direction Method of Multipliers):   Splits problem into subproblems, solves in parallel.   Popular in distributed optimization and ML.  </p> </li> <li> <p>Primal-dual interior-point methods:   Solve both primal and dual simultaneously.  </p> </li> </ul>"},{"location":"0d%20Algos/#summary-of-convergence-rates","title":"Summary of Convergence Rates","text":"Method Smooth Convex Strongly Convex Gradient Descent \\(\\mathcal{O}(1/k)\\) Linear Accelerated Gradient \\(\\mathcal{O}(1/k^2)\\) Linear Subgradient \\(\\mathcal{O}(1/\\sqrt{k})\\) \u2013 Newton\u2019s Method Quadratic (local) Quadratic Interior-Point Polynomial-time Polynomial-time"},{"location":"0d%20Algos/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<ul> <li>Small problems, high accuracy: Newton, Interior-point.  </li> <li>Large-scale smooth problems: Gradient descent, Nesterov acceleration, L-BFGS.  </li> <li>Large-scale nonsmooth problems: Subgradient, Proximal, ADMM.  </li> <li>Sparse / structured constraints: Coordinate descent, Proximal methods.  </li> </ul>"},{"location":"0d%20Algos/#log-concavity-and-log-convexity","title":"Log-concavity and Log-convexity","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}_{++}\\) is:</p> <ul> <li>Log-concave if \\(\\log f(x)\\) is concave.</li> <li>Log-convex if \\(\\log f(x)\\) is convex.</li> </ul>"},{"location":"0d%20Algos/#relevance","title":"Relevance","text":"<ul> <li>Log-concave functions appear in probability (many common distributions have log-concave densities, such as Gaussian, exponential, and uniform). This ensures tractability of maximum likelihood estimation.</li> <li>Log-convexity is useful in geometric programming, where monomials and posynomials are log-convex.</li> </ul>"},{"location":"0d%20Algos/#examples","title":"Examples","text":"<ul> <li>Gaussian density is log-concave.</li> <li>Exponential function is log-convex.</li> </ul>"},{"location":"0d%20Algos/#geometric-programming","title":"Geometric Programming","text":"<p>Geometric programming (GP) is a class of problems of the form:  where each \\(f_i\\) is a posynomial.</p> <ul> <li>Monomial: \\(f(x) = c x_1^{a_1} x_2^{a_2} \\dots x_n^{a_n}\\), with \\(c &gt; 0\\), exponents real.</li> <li>Posynomial: Sum of monomials.</li> </ul> <p>By applying the log transformation \\(y_i = \\log x_i\\), the problem becomes convex.</p>"},{"location":"0d%20Optimality%20Conditions/","title":"Optimality Conditions","text":""},{"location":"0d%20Optimality%20Conditions/#optimality-conditions-for-convex-optimization-problems","title":"Optimality Conditions for Convex Optimization Problems","text":"<p>Convex optimization problems have the important property that any local minimum is also a global minimum. Depending on whether the problem is unconstrained or constrained, and whether the function is differentiable or not, the optimality conditions differ.</p>"},{"location":"0d%20Optimality%20Conditions/#1-unconstrained-problems","title":"1. Unconstrained Problems","text":""},{"location":"0d%20Optimality%20Conditions/#11-differentiable-convex-functions","title":"1.1 Differentiable Convex Functions","text":"<p>For a differentiable convex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\):</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\] <p>Optimality condition:</p> \\[ \\nabla f(\\hat{x}) = 0 \\] <p>Intuition: The gradient points in the direction of steepest increase, so a zero gradient indicates a flat spot (minimum).</p> <p>Examples:</p> <p>a) Quadratic function: \\(f(x) = x^2 - 4x + 7\\) \\(\\nabla f(x) = 2x - 4\\) \u2192 set to 0 \u2192 \\(\\hat{x} = 2\\)</p> <p>b) Sum of Squared Errors \\(\\(f(x) = \\sum_{i=1}^{n} (x - y_i)^2\\)\\) \\(\\nabla f(x) = 2\\sum_{i=1}^{n} (x - y_i)\\) \u2192 set to 0 \u2192 \\(\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\) (the mean)</p>"},{"location":"0d%20Optimality%20Conditions/#12-non-differentiable-convex-functions","title":"1.2 Non-Differentiable Convex Functions","text":"<p>For convex but non-differentiable functions:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\] <p>Optimality condition:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>Intuition: The subgradient generalizes the derivative for functions with \"kinks.\"</p> <p>Examples:</p> <p>a) Sum of Absolute Errors (SAE): \\(\\(f(x) = \\sum_{i=1}^{n} |x - y_i|\\)\\) \\(\\hat{x}\\) = median of the data points.</p> <p>b) Maximum function: \\(\\(f(x) = \\max(x-1, 2-x)\\)\\) Subgradient: \\(\\partial f(x) = \\begin{cases} 1, &amp; x &gt; 1.5 \\\\ [-1,1], &amp; x = 1.5 \\\\ -1, &amp; x &lt; 1.5 \\end{cases}\\) \u2192 minimum at \\(x = 1.5\\)</p>"},{"location":"0d%20Optimality%20Conditions/#2-constrained-problems","title":"2. Constrained Problems","text":"<p>Consider a convex optimization problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{s.t.} \\quad x \\in \\mathcal{X} \\] <p>where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a convex feasible set.</p>"},{"location":"0d%20Optimality%20Conditions/#interior-point","title":"Interior Point:","text":"<p>If \\(\\hat{x}\\) lies strictly inside the feasible set, then the unconstrained condition applies:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>Intuition: There are no boundary restrictions, so the gradient (or subgradient) must vanish.</p>"},{"location":"0d%20Optimality%20Conditions/#boundary-point","title":"Boundary Point:","text":"<p>If \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\), then for \\(\\hat{x}\\) to be optimal:</p> <ul> <li>The negative gradient must lie in the normal cone of \\(\\mathcal{X}\\) at \\(\\hat{x}\\):</li> </ul> \\[ - \\nabla f(\\hat{x}) \\in N_{\\mathcal{X}}(\\hat{x}) \\] <ul> <li>Equivalently, the gradient must form an angle of at least \\(90^\\circ\\) with any feasible direction \\(d\\) inside \\(\\mathcal{X}\\):</li> </ul> \\[ \\nabla f(\\hat{x})^\\top d \\ge 0 \\quad \\forall d \\in T_{\\mathcal{X}}(\\hat{x}) \\] <p>where \\(T_{\\mathcal{X}}(\\hat{x})\\) is the tangent (feasible) cone at \\(\\hat{x}\\), and \\(N_{\\mathcal{X}}(\\hat{x})\\) is the normal cone at \\(\\hat{x}\\).</p> <p>Intuition: At the boundary, the optimal direction cannot point into the feasible set because any movement along a feasible direction increases the objective.</p>"},{"location":"0d%20Optimality%20Conditions/#compact-form","title":"Compact Form","text":"<p>Combining interior and boundary cases:</p> \\[ 0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x}) \\] <p>where:</p> <ul> <li>\\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\) for interior points  </li> <li>\\(N_{\\mathcal{X}}(\\hat{x})\\) is the normal cone for boundary points  </li> </ul> <p>This is a general convex optimality condition for constrained problems, valid for both differentiable and non-differentiable \\(f\\).</p>"},{"location":"0d%20Optimality%20Conditions/#intution","title":"Intution","text":"<p>Imagine a region of allowed points, called the feasible set  \\(\\mathcal{X}\\). Points strictly inside the region form the interior, where movement in any direction is possible without leaving the set. The edges and corners of the region form the boundary, where movement is restricted because you can only move along directions that remain feasible. Consider standing at a point \ud835\udc65 on this boundary. From here, you cannot move freely in all directions; you can only move along directions that stay inside the feasible set. These allowable directions form what is called the tangent cone at x, encompassing movements along the boundary or slightly into the interior.</p> <p>Opposing these feasible directions is the normal cone, which consists of vectors that point outward from the feasible region, effectively \u201cblocking\u201d any movement that would stay inside. At an optimal boundary point, the gradient of the objective function points outward, lying within the normal cone. This means that moving along any feasible direction \u2014 whether along the boundary or slightly into the interior \u2014 cannot decrease the objective function. The gradient \u201cpushes against\u201d all allowable moves, so any small displacement that respects the constraints either increases the objective or leaves it unchanged.</p> <p>This behavior contrasts with an interior optimum, where the gradient is zero and movement in any direction does not change the objective. At a boundary optimum, the gradient is non-zero but oriented such that all feasible directions are blocked from reducing the objective. Even though the gradient is not zero, the point is still optimal because the boundary restricts movement: every allowed step either raises the objective or keeps it the same. In this way, a boundary point can be a true optimum, and the outward-pointing gradient is the formal expression of the intuitive idea that you cannot \u201cgo downhill\u201d without leaving the feasible region.</p>"},{"location":"0d%20Optimality%20Conditions/#23-example-quadratic-with-constraint","title":"2.3 Example: Quadratic with Constraint","text":"\\[ \\min f(x) = x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Feasible set: \\(\\mathcal{X} = [1, \\infty)\\) </li> <li>Gradient: \\(\\nabla f(x) = 2x\\) </li> </ul> <p>Check optimality:</p> <ul> <li>Interior check (\\(x&gt;1\\)): \\(2x = 0 \\implies x = 0\\) \u2192 infeasible  </li> <li>Boundary check (\\(x=1\\)): \\(-\\nabla f(1) = -2 \\in N_{\\mathcal{X}}(1) = \\mathbb{R}_+\\) \u2192 satisfied  </li> </ul> <p>Solution: \\(\\hat{x} = 1\\)</p>"},{"location":"0e%20Optimization%20Algos/","title":"Optimization Algorithms","text":""},{"location":"0e%20Optimization%20Algos/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>Optimization is at the heart of machine learning: training a model is equivalent to finding the parameters \\(\\theta\\) that minimize a loss function \\(L(\\theta)\\) or maximize a likelihood. In other words, we solve problems of the form:</p> \\[ \\min_{\\theta \\in \\mathcal{X}} L(\\theta), \\] <p>where \\(\\mathcal{X}\\) is the feasible set of parameters.</p> <p>The landscape of optimization problems varies widely:</p> <ul> <li>Some functions are smooth and differentiable, allowing gradient-based methods.  </li> <li>Some functions are nonsmooth or piecewise, requiring subgradient or proximal methods.  </li> <li>Constraints or bounds on parameters require projection or constrained optimization.  </li> <li>The scale of the problem (large datasets or high-dimensional parameters) affects algorithm choice.  </li> <li>Stochasticity (e.g., noisy gradients from mini-batches) motivates stochastic optimization methods.</li> </ul> <p>Choosing the right algorithm involves understanding:</p> <ol> <li>Function properties: convexity, smoothness, Lipschitz constants.  </li> <li>Step size and momentum requirements: trade-off between speed and stability.  </li> <li>Convergence guarantees: linear vs. sublinear, deterministic vs. stochastic.  </li> </ol>"},{"location":"0e1%20Gradient%20Descent/","title":"Gradient Descent","text":""},{"location":"0e1%20Gradient%20Descent/#gradient-descent-derivation-and-convergence","title":"Gradient Descent: Derivation and Convergence","text":"<p>We aim to minimize a differentiable function \\(f\\) over a feasible set \\(\\mathcal{X}\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\]"},{"location":"0e1%20Gradient%20Descent/#1-local-approximation","title":"1. Local Approximation","text":"<p>At iteration \\(t\\), we have the current point \\(x_t\\). To make optimization tractable, we build a first-order Taylor approximation of \\(f\\) around \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <ul> <li>This is a linear approximation of \\(f\\) at \\(x_t\\).  </li> <li>Directly minimizing this linear model is unbounded below; it would push \\(x\\) infinitely in the negative gradient direction.</li> </ul>"},{"location":"0e1%20Gradient%20Descent/#2-adding-a-quadratic-regularization-term","title":"2. Adding a Quadratic Regularization Term","text":"<p>To prevent taking arbitrarily large steps, we add a quadratic penalty that discourages moving far from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the step size (learning rate).</p> <p>Intuition: - Limits trust in the linear approximation (local approximation only). - Creates a trade-off between decreasing the linear term and staying near \\(x_t\\). - \\(\\frac{1}{2\\eta}\\) controls the strength of the penalty:   - Small \\(\\eta\\): conservative, small steps.   - Large \\(\\eta\\): aggressive, larger steps.  </p> <p>This is conceptually related to proximal methods or trust-region approaches in optimization.</p>"},{"location":"0e1%20Gradient%20Descent/#3-deriving-the-gradient-descent-update","title":"3. Deriving the Gradient Descent Update","text":"<p>We update \\(x_{t+1}\\) by minimizing the quadratic model:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <ul> <li>Ignore \\(f(x_t)\\) since it's constant with respect to \\(x\\).  </li> <li>Take the gradient of the remaining terms and set it to zero:</li> </ul> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solve for \\(x\\):</p> \\[ x - x_t = -\\eta \\nabla f(x_t) \\] <p>Hence the gradient descent update:</p> \\[ \\boxed{x_{t+1} = x_t - \\eta \\nabla f(x_t)} \\]"},{"location":"0e1%20Gradient%20Descent/#4-convergence-analysis","title":"4. Convergence Analysis","text":""},{"location":"0e1%20Gradient%20Descent/#assumptions","title":"Assumptions","text":"<p>We assume:</p> <ol> <li>\\(L\\)-smoothness (Lipschitz-continuous gradient):</li> </ol> \\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|, \\quad \\forall x, y \\] <ol> <li>\\(\\mu\\)-strong convexity:</li> </ol> \\[ f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2} \\|y-x\\|^2, \\quad \\forall x, y \\]"},{"location":"0e1%20Gradient%20Descent/#41-descent-lemma","title":"4.1 Descent Lemma","text":"<p>For \\(L\\)-smooth functions, we have:</p> \\[ f(x_{t+1}) \\le f(x_t) + \\langle \\nabla f(x_t), x_{t+1}-x_t \\rangle + \\frac{L}{2} \\|x_{t+1}-x_t\\|^2 \\] <p>Substitute \\(x_{t+1} = x_t - \\eta \\nabla f(x_t)\\):</p> \\[ \\begin{aligned} f(x_{t+1}) &amp;\\le f(x_t) + \\langle \\nabla f(x_t), -\\eta \\nabla f(x_t) \\rangle + \\frac{L}{2} \\|\\eta \\nabla f(x_t)\\|^2 \\\\ &amp;= f(x_t) - \\eta \\|\\nabla f(x_t)\\|^2 + \\frac{L \\eta^2}{2} \\|\\nabla f(x_t)\\|^2 \\\\ &amp;= f(x_t) - \\left( \\eta - \\frac{L\\eta^2}{2} \\right) \\|\\nabla f(x_t)\\|^2 \\end{aligned} \\] <p>Implication: If \\(\\eta \\le \\frac{1}{L}\\), the term \\(\\eta - \\frac{L\\eta^2}{2} &gt; 0\\), so \\(f(x_{t+1}) \\le f(x_t)\\). Each step decreases the function value.</p>"},{"location":"0e1%20Gradient%20Descent/#42-convergence-for-strongly-convex-functions","title":"4.2 Convergence for Strongly Convex Functions","text":"<p>If \\(f\\) is \\(\\mu\\)-strongly convex, gradient descent converges linearly. Specifically:</p> \\[ \\begin{aligned} \\|x_{t+1} - x^*\\|^2 &amp;= \\|x_t - \\eta \\nabla f(x_t) - x^*\\|^2 \\\\ &amp;= \\|x_t - x^*\\|^2 - 2\\eta \\langle \\nabla f(x_t), x_t - x^* \\rangle + \\eta^2 \\|\\nabla f(x_t)\\|^2 \\end{aligned} \\] <p>From strong convexity:</p> \\[ \\langle \\nabla f(x_t), x_t - x^* \\rangle \\ge \\frac{\\mu L}{\\mu + L} \\|x_t - x^*\\|^2 + \\frac{1}{\\mu + L} \\|\\nabla f(x_t)\\|^2 \\] <p>Choosing step size \\(\\eta = \\frac{2}{\\mu + L}\\) yields:</p> \\[ \\|x_{t+1} - x^*\\|^2 \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right)^2 \\|x_t - x^*\\|^2 \\] <p>Linear convergence: Distance to optimum shrinks by a constant factor each step.</p>"},{"location":"0e1%20Gradient%20Descent/#43-convergence-rate-summary","title":"4.3 Convergence Rate Summary","text":"<ul> <li>For \\(L\\)-smooth convex (not strongly convex):</li> </ul> \\[ f(x_T) - f(x^*) \\le \\frac{L \\|x_0 - x^*\\|^2}{2T} \\] <p>Sublinear rate \\(O(1/T)\\).</p> <ul> <li>For \\(L\\)-smooth \\(\\mu\\)-strongly convex:</li> </ul> \\[ \\|x_T - x^*\\| \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right)^T \\|x_0 - x^*\\| \\] <p>Linear rate \\(O(\\rho^T)\\), \\(\\rho &lt; 1\\).</p>"},{"location":"0e2%20subgradient%20method/","title":"Subgradient Method","text":""},{"location":"0e2%20subgradient%20method/#subgradient-method-derivation-and-convergence","title":"Subgradient Method: Derivation and Convergence","text":"<p>We aim to minimize a convex function \\(f\\), which may be nonsmooth:</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but may not be differentiable everywhere.</p>"},{"location":"0e2%20subgradient%20method/#1-subgradients","title":"1. Subgradients","text":"<p>For nonsmooth convex functions, we use a subgradient \\(g_t \\in \\partial f(x_t)\\) at iteration \\(t\\), where \\(\\partial f(x_t)\\) is the subdifferential at \\(x_t\\).</p> <ul> <li>For differentiable \\(f\\), \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).</li> <li>For nonsmooth \\(f\\), \\(\\partial f(x_t)\\) is a set of vectors satisfying:</li> </ul> \\[ f(y) \\ge f(x_t) + \\langle g_t, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <p>Intuition: A subgradient generalizes the concept of gradient for nonsmooth convex functions. It points in a direction that does not decrease the function.</p>"},{"location":"0e2%20subgradient%20method/#2-subgradient-update-rule","title":"2. Subgradient Update Rule","text":"<p>The projected subgradient method updates:</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}} \\big( x_t - \\eta_t g_t \\big), \\] <p>where:</p> <ul> <li>\\(g_t \\in \\partial f(x_t)\\) is a subgradient,  </li> <li>\\(\\eta_t &gt; 0\\) is the step size (may vary with \\(t\\)),  </li> <li>\\(\\Pi_{\\mathcal{X}}\\) denotes the projection onto the feasible set \\(\\mathcal{X}\\).</li> </ul> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\) (unconstrained), this reduces to:</p> \\[ x_{t+1} = x_t - \\eta_t g_t. \\]"},{"location":"0e2%20subgradient%20method/#3-distance-recurrence","title":"3. Distance Recurrence","text":"<p>Let \\(x^\\star\\) be an optimal solution. Consider the squared distance to the optimum:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - \\eta_t g_t - x^\\star\\|^2 \\] <p>Expanding:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t \\langle g_t, x_t - x^\\star \\rangle + \\eta_t^2 \\|g_t\\|^2 \\] <p>Since \\(g_t\\) is a subgradient of convex \\(f\\):</p> \\[ f(x_t) - f(x^\\star) \\le \\langle g_t, x_t - x^\\star \\rangle \\] <p>Substitute into the distance expansion:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t \\big(f(x_t) - f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2 \\]"},{"location":"0e2%20subgradient%20method/#4-rearranging-for-function-suboptimality","title":"4. Rearranging for Function Suboptimality","text":"<p>Rewriting:</p> \\[ f(x_t) - f(x^\\star) \\le \\frac{\\|x_t - x^\\star\\|^2 - \\|x_{t+1} - x^\\star\\|^2}{2 \\eta_t} + \\frac{\\eta_t}{2} \\|g_t\\|^2 \\] <p>Intuition: The suboptimality is bounded by:</p> <ol> <li>The decrease in squared distance to the optimum.  </li> <li>A term depending on step size and the subgradient norm.</li> </ol>"},{"location":"0e2%20subgradient%20method/#5-summing-over-iterations","title":"5. Summing Over Iterations","text":"<p>Sum over \\(t = 0, \\dots, T-1\\):</p> \\[ \\sum_{t=0}^{T-1} \\big(f(x_t) - f(x^\\star)\\big) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2\\eta} + \\frac{\\eta}{2} \\sum_{t=0}^{T-1} \\|g_t\\|^2 \\] <p>Assume \\(\\|g_t\\| \\le G\\) and constant step size \\(\\eta\\):</p> \\[ \\sum_{t=0}^{T-1} \\big(f(x_t) - f(x^\\star)\\big) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2} \\] <p>Divide by \\(T\\) to bound the average iterate \\(\\bar{x}_T = \\frac{1}{T} \\sum_{t=0}^{T-1} x_t\\):</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2 \\eta T} + \\frac{\\eta G^2}{2} \\]"},{"location":"0e2%20subgradient%20method/#6-step-size-choice-and-convergence-rate","title":"6. Step Size Choice and Convergence Rate","text":"<p>Choosing a diminishing step size:</p> \\[ \\eta_t = \\frac{R}{G \\sqrt{T}}, \\quad R = \\|x_0 - x^\\star\\| \\] <p>gives the classic subgradient sublinear convergence rate:</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{R G}{\\sqrt{T}} \\] <ul> <li>\\(x^\\star\\) is an optimal solution.  </li> <li>\\(\\bar{x}_T = \\frac{1}{T}\\sum_{t=0}^{T-1} x_t\\) is the average iterate.  </li> <li>\\(R = \\|x_0 - x^\\star\\|\\), distance to optimum.  </li> <li>\\(G\\) bounds the subgradients: \\(\\|g_t\\| \\le G\\).</li> </ul> <p>Implication: The convergence rate is sublinear, \\(O(1/\\sqrt{T})\\). Unlike gradient descent, subgradient method cannot achieve linear convergence without additional assumptions (like strong convexity and smoothness).</p>"},{"location":"0e2%20subgradient%20method/#7-practical-remarks","title":"7. Practical Remarks","text":"<ol> <li>Step size selection is crucial:</li> <li>Diminishing step sizes ensure convergence.  </li> <li> <p>Constant step size may lead to oscillations near optimum.</p> </li> <li> <p>Averaging iterates (\\(\\bar{x}_T\\)) improves convergence guarantees.  </p> </li> <li> <p>Robustness: Works for nonsmooth convex functions where gradient does not exist.</p> </li> <li> <p>Slower than gradient descent: \\(O(1/\\sqrt{T})\\) vs \\(O(1/T)\\) or linear for smooth strongly convex functions.</p> </li> </ol>"},{"location":"0e3%20accelerated%20gs/","title":"Accelerated GD","text":""},{"location":"0e3%20accelerated%20gs/#accelerated-gradient-descent-momentum","title":"Accelerated Gradient Descent: Momentum","text":"<p>The standard gradient descent (GD) update is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>where: - \\(x_t\\) is the current iterate, - \\(\\eta &gt; 0\\) is the step size (learning rate), - \\(\\nabla f(x_t)\\) is the gradient at \\(x_t\\).</p> <p>Intuition: Imagine rolling a ball on a hill. The ball moves in the steepest downhill direction at each step. In long, narrow valleys, standard GD can zig-zag, taking many small steps to reach the bottom.</p>"},{"location":"0e3%20accelerated%20gs/#1-momentum-adding-inertia","title":"1. Momentum: Adding Inertia","text":"<p>Momentum adds the idea of velocity, allowing the optimization to \"remember\" previous directions:</p> \\[ v_t = x_t - x_{t-1}, \\] <p>where \\(v_t\\) represents the velocity of the iterate.  </p> <ul> <li>The update now combines the current gradient and the previous motion.  </li> <li>Momentum helps move faster along flat or consistent slopes and reduces zig-zagging in steep valleys.  </li> </ul> <p>Analogy: - No momentum \u2192 ball stops after each step, carefully following the slope. - With momentum \u2192 ball keeps rolling, building speed along the valley, only slowing when gradients push against it.</p>"},{"location":"0e3%20accelerated%20gs/#2-gradient-descent-with-momentum","title":"2. Gradient Descent with Momentum","text":"<p>The update rule with momentum is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) + \\beta (x_t - x_{t-1}), \\] <p>where: - \\(\\beta \\in [0,1)\\) is the momentum parameter, controlling how much past velocity is retained.</p>"},{"location":"0e3%20accelerated%20gs/#breakdown","title":"Breakdown","text":"<ol> <li>Gradient step: \\(-\\eta \\nabla f(x_t)\\) \u2192 moves downhill.  </li> <li>Momentum step: \\(\\beta (x_t - x_{t-1})\\) \u2192 continues moving along previous direction.</li> </ol> <p>Intuition: - If gradients consistently point in the same direction, momentum accelerates the steps. - If gradients oscillate, momentum smooths the path, reducing overshooting.</p>"},{"location":"0e3%20accelerated%20gs/#3-alternative-form-velocity-update","title":"3. Alternative Form: Velocity Update","text":"<p>Another common formulation introduces an explicit velocity variable \\(v_t\\):</p> \\[ \\begin{aligned} v_{t+1} &amp;= \\beta v_t - \\eta \\nabla f(x_t) \\\\ x_{t+1} &amp;= x_t + v_{t+1} \\end{aligned} \\] <ul> <li>Here, \\(v_t\\) accumulates the past updates weighted by \\(\\beta\\).  </li> <li>This makes the analogy to a rolling ball more explicit.</li> </ul>"},{"location":"0e3%20accelerated%20gs/#4-convergence-intuition","title":"4. Convergence Intuition","text":"<ul> <li>For convex and smooth functions, momentum accelerates convergence:  </li> <li>Standard GD: \\(O(1/t)\\) </li> <li> <p>GD + Momentum / Nesterov: \\(O(1/t^2)\\)</p> </li> <li> <p>Momentum combines current slope and accumulated speed from past steps.  </p> </li> <li>Acts like a frictionless ball in a valley: keeps moving in the right direction, accelerating convergence.</li> </ul> <p>Key idea: Momentum builds up speed along consistent gradient directions and smooths oscillations along steep valleys.</p>"},{"location":"0e3%20accelerated%20gs/#5-practical-remarks","title":"5. Practical Remarks","text":"<ol> <li>Momentum is memory: it remembers the direction of previous steps.  </li> <li>Reduces oscillations in narrow valleys.  </li> <li>Accelerates convergence along consistent gradient directions.  </li> <li>Hyperparameter \\(\\beta\\) controls inertia:  </li> <li>Higher \\(\\beta\\) \u2192 longer memory, faster but potentially riskier steps.  </li> <li>Typical values: \\(\\beta = 0.9\\) or \\(0.99\\).  </li> <li>Can be combined with Nesterov acceleration for theoretically optimal rates.</li> </ol>"},{"location":"0e3%20accelerated%20gs/#6-summary","title":"6. Summary","text":"<p>Momentum modifies gradient descent by combining:</p> <ul> <li>Immediate gradient information (\\(-\\eta \\nabla f(x_t)\\))  </li> <li>Past velocity (\\(\\beta (x_t - x_{t-1})\\))  </li> </ul> <p>Effectively, it allows the optimizer to roll through valleys faster, reduce zig-zagging, and achieve accelerated convergence, especially for convex and smooth functions.</p>"},{"location":"0f%20Convergence/","title":"Convergence Properties","text":""},{"location":"0f%20Convergence/#function-properties-for-optimization-strong-convexity-smoothness-and-conditioning","title":"Function Properties for Optimization: Strong Convexity, Smoothness, and Conditioning","text":""},{"location":"0f%20Convergence/#strong-convexity","title":"Strong Convexity","text":"<p>A function \\(f\\) is \\(\\mu\\)-strongly convex if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) + \\frac{\\mu}{2}\\|y-x\\|^2. \\] <p>If \\(f\\) is twice differentiable, this is equivalent to</p> \\[ \\nabla^2 f(x) \\succeq \\mu I \\quad \\text{for all } x. \\] <ul> <li>Guarantees a unique minimizer.  </li> <li>Gradient-based methods achieve linear convergence.  </li> <li>Prevents flat regions where optimization would stall.  </li> </ul>"},{"location":"0f%20Convergence/#why-convergence-may-be-slow-without-strong-convexity","title":"Why Convergence May Be Slow Without Strong Convexity","text":"<ul> <li>If \\(f\\) is convex but not strongly convex, it can have flat regions (zero curvature).  </li> <li>Gradients may be very small in these directions \u2192 gradient steps shrink, and convergence becomes sublinear:  </li> </ul> \\[ f(x_t) - f(x^\\star) = O\\left(\\frac{1}{t}\\right). \\] <ul> <li> <p>Example: \\(f(x) = x^4\\) is convex but not strongly convex near \\(x=0\\). Gradient descent steps become tiny near the minimum \u2192 slow convergence.  </p> </li> <li> <p>Contrast: \\(f(x) = x^2\\) is strongly convex (\\(\\mu=2\\)) \u2192 linear convergence.  </p> </li> </ul>"},{"location":"0f%20Convergence/#examples","title":"Examples","text":"<ol> <li> <p>Quadratic function: \\(f(x) = x^2\\) \u2192 \\(\\mu=2\\), strongly convex \u2192 fast convergence.  </p> </li> <li> <p>Quartic function: \\(f(x) = x^4\\) \u2192 convex but not strongly convex near \\(0\\) \u2192 slow convergence.  </p> </li> <li> <p>Ridge Regression (L2 Regularization): </p> <ul> <li>The first term \\(\\|Xw - y\\|^2\\) is convex.  </li> <li>The L2 term \\(\\lambda \\|w\\|^2\\) is strongly convex (\\(\\nabla^2 (\\lambda\\|w\\|^2) = 2\\lambda I \\succeq \\lambda I\\)).  </li> <li>Adding the L2 penalty makes the entire objective strongly convex with \\(\\mu = \\lambda\\).  </li> <li>Implications: <ul> <li>Unique solution:  even if \\(X^\\top X\\) is singular or ill-conditioned.  </li> <li>Stable optimization: gradient-based methods converge linearly.  </li> <li>Prevents overfitting by controlling the size of weights.  </li> </ul> </li> </ul> </li> </ol>"},{"location":"0f%20Convergence/#smoothness-l-smoothness","title":"Smoothness (L-smoothness)","text":"<p>A function \\(f\\) is \\(L\\)-smooth if</p> \\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|. \\] <p>If twice differentiable:</p> \\[ \\nabla^2 f(x) \\preceq L I \\quad \\text{for all } x. \\] <ul> <li>Limits how steep \\(f\\) can be.  </li> <li>Ensures gradients change gradually \u2192 stable gradient steps.  </li> <li>Guarantees safe step sizes: \\(\\alpha &lt; 1/L\\) for gradient descent.  </li> </ul>"},{"location":"0f%20Convergence/#why-smoothness-matters-for-convergence","title":"Why Smoothness Matters for Convergence","text":"<ul> <li>Without smoothness, the gradient can change abruptly.  </li> <li>A large gradient could lead to overshooting, oscillation, or divergence.  </li> <li>Smoothness ensures predictable, stable progress along the gradient.  </li> </ul> <p>Examples: - Quadratic \\(f(x) = \\frac{1}{2}x^\\top Qx\\): \\(L = \\lambda_{\\max}(Q)\\). - Logistic regression loss: smooth with \\(L\\) depending on \\(\\|X\\|^2\\). - Non-smooth case: \\(f(x) = |x|\\) \u2192 gradient jumps at \\(x=0\\), cannot guarantee smooth progress \u2192 need subgradient methods.  </p>"},{"location":"0f%20Convergence/#condition-number","title":"Condition Number","text":"<p>The condition number is defined as</p> \\[ \\kappa = \\frac{L}{\\mu}. \\] <ul> <li>Measures how \u201cstretched\u201d the optimization landscape is.  </li> <li>High \\(\\kappa\\) \u2192 narrow, elongated valleys \u2192 gradient descent zig-zags, converges slowly.  </li> <li>Low \\(\\kappa\\) \u2192 round bowl \u2192 fast convergence.  </li> </ul> <p>Examples: - \\(Q=I\\): \\(\\mu=L=1\\), \\(\\kappa=1\\) \u2192 fastest convergence. - \\(Q=\\text{diag}(1,1000)\\): \\(\\mu=1\\), \\(L=1000\\), \\(\\kappa=1000\\) \u2192 ill-conditioned, very slow. - In ML, normalization (batch norm, feature scaling, whitening) reduces \\(\\kappa\\), improving training speed.  </p>"},{"location":"0f%20Convergence/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"0f%20Convergence/#strong-convexity_1","title":"Strong Convexity","text":"<ul> <li>Unique solution (ridge regression).  </li> <li>Linear convergence of gradient-based methods.  </li> <li>Stabilizes optimization by avoiding flatness.  </li> </ul>"},{"location":"0f%20Convergence/#smoothness","title":"Smoothness","text":"<ul> <li>Ensures safe and predictable step sizes.  </li> <li>Avoids overshooting or divergence.  </li> <li>Justifies constant learning rates for many ML losses.  </li> </ul>"},{"location":"0f%20Convergence/#condition-number_1","title":"Condition Number","text":"<ul> <li>Predicts convergence speed.  </li> <li>Guides preprocessing: scaling, normalization, whitening.  </li> <li>Central in designing adaptive optimizers and preconditioning methods.  </li> </ul>"},{"location":"0f%20Convergence/#convergence-rates-of-first-order-methods","title":"Convergence Rates of First-Order Methods","text":"Function Property Gradient Descent Rate Accelerated Gradient (Nesterov) Subgradient Method Rate Convex (not strongly convex) \\(O(1/t)\\) \\(O(1/t^2)\\) \\(O(1/\\sqrt{t})\\) \\(\\mu\\)-Strongly Convex Linear: \\(O\\big((1-\\eta\\mu)^t\\big)\\) Linear: faster than GD \\(O(\\log t / t)\\) Condition Number \\(\\kappa\\) Iterations \\(\\sim O(\\kappa \\log(1/\\epsilon))\\) Iterations \\(\\sim O(\\sqrt{\\kappa}\\log(1/\\epsilon))\\) \u2013"},{"location":"0f%20Convergence/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Strong convexity: bowl is always curved enough \u2192 unique and fast convergence.  </li> <li>Smoothness: bowl is not too steep \u2192 safe steps, avoids overshooting.  </li> <li>Condition number: how round vs stretched the bowl is \u2192 dictates optimization difficulty.  </li> <li>Without strong convexity \u2192 flat regions \u2192 slow sublinear convergence.  </li> <li>Without smoothness \u2192 steep gradient changes \u2192 possible divergence or oscillations.</li> </ul>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/","title":"Projections & Proximal Operators","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projections-and-proximal-operators-in-constrained-convex-optimization","title":"Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>In many convex optimization problems, we want to minimize a convex, differentiable function \\(f(x)\\) subject to some constraint that limits \\(x\\) to a feasible region \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x) \\] <p>A standard gradient descent step is</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>but this update might move \\(x_{t+1}\\) outside the feasible region \\(\\mathcal{X}\\). To fix that, we add a projection step that brings the point back into the allowed set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projection-operator","title":"Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is the closest point in the set to \\(y\\):</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 \\] <p>So the projected gradient descent update becomes</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#geometric-intuition","title":"Geometric intuition","text":"<p>Think of taking a gradient step in the direction of steepest descent, possibly leaving the feasible region. The projection then \u201csnaps\u201d that point back to the nearest feasible location. This ensures all iterates \\(x_t\\) stay within \\(\\mathcal{X}\\) while still moving downhill with respect to \\(f\\).</p> <p>Example: If \\(\\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\}\\) (the unit ball), the projection is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)} \\] <p>That means: - If \\(y\\) is inside the ball, it stays there (\\(\\|y\\|_2 \\le 1\\)). - If \\(y\\) is outside, scale it down to lie exactly on the boundary.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#from-projections-to-proximal-operators","title":"From Projections to Proximal Operators","text":"<p>Projection helps when constraints are explicitly defined by a set (e.g., nonnegativity or norm bounds). But many optimization problems include non-smooth regularization terms instead \u2014 for example, \\(g(x) = \\lambda \\|x\\|_1\\) to promote sparsity.</p> <p>The proximal operator generalizes projection to handle such non-smooth functions directly.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#definition","title":"Definition","text":"<p>For a convex (possibly non-differentiable) function \\(g(x)\\), its proximal operator is defined as:</p> \\[ \\text{prox}_{\\lambda g}(y) = \\arg\\min_x \\left( g(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#interpretation","title":"Interpretation","text":"<p>The proximal operator finds a point \\(x\\) that balances two objectives:</p> <ol> <li>Stay close to \\(y\\) \u2014 enforced by the squared term \\(\\frac{1}{2\\lambda}\\|x - y\\|^2\\).</li> <li>Reduce \\(g(x)\\) \u2014 the regularization or penalty term.</li> </ol> <p>The parameter \\(\\lambda &gt; 0\\) controls this trade-off:</p> <ul> <li>A small \\(\\lambda\\) \u2192 stronger pull toward \\(y\\) (less movement).  </li> <li>A large \\(\\lambda\\) \u2192 more freedom to reduce \\(g(x)\\).</li> </ul> <p>The squared distance term acts as a soft tether, keeping \\(x\\) near \\(y\\) while allowing it to move toward regions where \\(g(x)\\) is smaller or structured.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#indicator-function-and-connection-to-projection","title":"Indicator Function and Connection to Projection","text":"<p>Let\u2019s see how projection appears as a special case of the proximal operator.</p> <p>Define the indicator function of a convex set \\(\\mathcal{X}\\) as:</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X} \\\\ +\\infty, &amp; x \\notin \\mathcal{X} \\end{cases} \\] <p>Now, substitute \\(g(x) = I_{\\mathcal{X}}(x)\\) into the definition of the proximal operator:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\Big) \\] <p>Because \\(I_{\\mathcal{X}}(x)\\) is infinite outside \\(\\mathcal{X}\\), the minimization is effectively restricted to \\(x \\in \\mathcal{X}\\). Thus we get:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y) \\] <p>\u2705 Therefore, projection is just a proximal operator for the indicator of a set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#understanding-the-proximal-step","title":"Understanding the Proximal Step","text":"<p>The proximal operator can be viewed as a correction step:</p> <ul> <li>The gradient step moves toward minimizing the smooth part \\(f(x)\\).</li> <li>The proximal step adjusts that move to respect the structure imposed by \\(g(x)\\) \u2014 e.g., sparsity, nonnegativity, or feasibility.</li> </ul> <p>When combining both, we get the proximal gradient method:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>This algorithm generalizes projected gradient descent \u2014 it works for both constraint sets (through indicator functions) and regularizers (like \\(\\ell_1\\)-norms).</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#example-proximal-of-the-ell_1-norm","title":"Example: Proximal of the \\(\\ell_1\\)-Norm","text":"<p>We want to compute the proximal operator of the \\(\\ell_1\\)-norm:</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda \\|x\\|_1 + \\frac{1}{2}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-1-coordinate-wise-separation","title":"Step 1. Coordinate-wise separation","text":"<p>Because both \\(\\|x\\|_1\\) and \\(\\|x - y\\|^2\\) are separable across coordinates, we can solve for each component independently:</p> \\[ \\min_x \\left( \\lambda |x| + \\frac{1}{2}(x - y)^2 \\right) \\] <p>Thus, we only need to handle the scalar problem for one coordinate \\(y \\in \\mathbb{R}\\):</p> \\[ \\phi(x) = \\lambda |x| + \\frac{1}{2}(x - y)^2 \\] <p>and find</p> \\[ x^\\star = \\arg\\min_x \\phi(x) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-2-subgradient-optimality-condition","title":"Step 2. Subgradient optimality condition","text":"<p>Since \\(\\phi\\) is convex (but not differentiable at \\(x = 0\\)), the optimality condition is</p> \\[ 0 \\in \\partial \\phi(x^\\star) \\] <p>Compute the subgradient:</p> \\[ \\partial \\phi(x) = \\lambda \\, \\partial |x| + (x - y) \\] <p>where</p> \\[ \\partial |x| = \\begin{cases} \\{1\\}, &amp; x &gt; 0 \\\\[4pt] [-1, 1], &amp; x = 0 \\\\[4pt] \\{-1\\}, &amp; x &lt; 0 \\end{cases} \\] <p>Hence, the optimality condition becomes</p> \\[ 0 \\in \\lambda s + (x^\\star - y), \\quad s \\in \\partial |x^\\star| \\] <p>Rewriting:</p> \\[ x^\\star = y - \\lambda s, \\quad s \\in \\partial |x^\\star| \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-3-case-analysis","title":"Step 3. Case Analysis","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-1-xstar-0","title":"Case 1: \\(x^\\star &gt; 0\\)","text":"<p>Then \\(s = 1\\), so</p> \\[ x^\\star = y - \\lambda \\] <p>This is valid only if \\(x^\\star &gt; 0 \\implies y &gt; \\lambda\\).</p> <p>Hence, when \\(y &gt; \\lambda\\), the minimizer is:</p> \\[ x^\\star = y - \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-2-xstar-0","title":"Case 2: \\(x^\\star &lt; 0\\)","text":"<p>Then \\(s = -1\\), so</p> \\[ x^\\star = y + \\lambda \\] <p>This is valid only if \\(x^\\star &lt; 0 \\implies y &lt; -\\lambda\\).</p> <p>Hence, when \\(y &lt; -\\lambda\\), the minimizer is:</p> \\[ x^\\star = y + \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-3-xstar-0","title":"Case 3: \\(x^\\star = 0\\)","text":"<p>Then \\(s \\in [-1, 1]\\), and the condition</p> \\[ 0 \\in \\lambda s + (0 - y) \\] <p>means there exists \\(s \\in [-1, 1]\\) such that \\(y = \\lambda s\\). This happens exactly when \\(y \\in [-\\lambda, \\lambda]\\).</p> <p>Hence, when \\(|y| \\le \\lambda\\), the minimizer is:</p> \\[ x^\\star = 0 \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-4-combine-the-cases","title":"Step 4. Combine the cases","text":"<p>Putting the three cases together:</p> \\[ \\text{prox}_{\\lambda |\\cdot|}(y) = \\begin{cases} y - \\lambda, &amp; y &gt; \\lambda \\\\[6pt] 0, &amp; |y| \\le \\lambda \\\\[6pt] y + \\lambda, &amp; y &lt; -\\lambda \\end{cases} \\] <p>Or equivalently, in compact form:</p> \\[ \\boxed{ \\text{prox}_{\\lambda |\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda, 0) } \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-5-extend-to-vector-case","title":"Step 5. Extend to vector case","text":"<p>For a vector \\(y \\in \\mathbb{R}^n\\), the proximal operator applies coordinate-wise:</p> \\[ \\big(\\text{prox}_{\\lambda \\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i) \\cdot \\max(|y_i| - \\lambda, 0) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-6-intuition","title":"Step 6. Intuition","text":"<ul> <li>When \\(|y_i| \\le \\lambda\\), the quadratic term cannot compensate for the \\(\\ell_1\\) penalty, so the coordinate shrinks to zero (sparsity).</li> <li>When \\(|y_i| &gt; \\lambda\\), the coordinate is shrunk by \\(\\lambda\\) toward zero but remains nonzero.</li> <li>This behavior is called soft-thresholding, and it is the key to algorithms like LASSO and ISTA for sparse recovery.</li> </ul>"},{"location":"0g1%20proximal%20ga/","title":"Proximal Gradient Algorithm","text":""},{"location":"0g1%20proximal%20ga/#proximal-gradient-algorithm","title":"Proximal Gradient Algorithm","text":"<p>Many optimization problems involve composite objectives of the form:</p> \\[ \\min_{x \\in \\mathbb{R}^n} F(x) := f(x) + g(x) \\] <p>where:  </p> <ul> <li>\\(f(x)\\) is convex and differentiable with a Lipschitz continuous gradient (\\(\\nabla f\\) exists and is \\(L\\)-Lipschitz).  </li> <li>\\(g(x)\\) is convex but possibly non-differentiable (e.g., \\(\\ell_1\\)-norm, indicator of a constraint set).  </li> </ul> <p>This structure appears in many applications: LASSO (\\(f = \\text{least squares}, g = \\lambda \\|x\\|_1\\)), elastic net, constrained optimization, etc.</p>"},{"location":"0g1%20proximal%20ga/#1-motivation","title":"1. Motivation","text":"<ul> <li>Standard gradient descent cannot handle \\(g(x)\\) if it is non-differentiable.  </li> <li>Projected gradient descent works only if \\(g\\) is an indicator function of a set.  </li> <li>The proximal gradient method generalizes both approaches and allows efficient updates even when \\(g\\) is non-smooth.</li> </ul>"},{"location":"0g1%20proximal%20ga/#2-proximal-gradient-update","title":"2. Proximal Gradient Update","text":"<p>For step size \\(\\eta &gt; 0\\), the proximal gradient update is:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>Interpretation:</p> <ol> <li>Take a gradient step on the smooth part \\(f\\):</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply the proximal operator of \\(g\\) to handle the non-smooth part:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <p>This ensures:</p> <ul> <li>\\(f(x)\\) decreases via the gradient step.  </li> <li>\\(g(x)\\) is accounted for via the proximal step.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#3-step-size-selection","title":"3. Step Size Selection","text":"<p>For convergence, the step size \\(\\eta\\) is typically chosen as:</p> \\[ 0 &lt; \\eta \\le \\frac{1}{L} \\] <p>where \\(L\\) is the Lipschitz constant of \\(\\nabla f\\).  </p> <ul> <li>Smaller \\(\\eta\\) \u2192 conservative steps.  </li> <li>Larger \\(\\eta\\) may overshoot and break convergence guarantees.  </li> <li>Adaptive strategies (like backtracking line search) can also be used.</li> </ul>"},{"location":"0g1%20proximal%20ga/#4-algorithm-proximal-gradient-method-ista","title":"4. Algorithm (Proximal Gradient Method / ISTA)","text":"<p>Input: \\(x_0\\), step size \\(\\eta &gt; 0\\) </p> <p>Repeat for \\(t = 0, 1, 2, \\dots\\):  </p> <ol> <li>Compute gradient step:</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply proximal operator:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <ol> <li>Check convergence (e.g., \\(\\|x_{t+1} - x_t\\| &lt; \\epsilon\\)).</li> </ol>"},{"location":"0g1%20proximal%20ga/#5-special-cases","title":"5. Special Cases","text":"Non-smooth term \\(g(x)\\) Proximal operator \\(\\text{prox}_{\\eta g}(y)\\) Interpretation \\(\\lambda \\|x\\|_1\\) Soft-thresholding: $\\text{sign}(y_i)\\max( y_i Indicator \\(I_{\\mathcal{X}}(x)\\) Projection: \\(\\text{Proj}_{\\mathcal{X}}(y)\\) Constrained optimization \\(\\lambda \\|x\\|_2^2\\) Shrinkage: \\(y / (1 + 2\\eta\\lambda)\\) Smooth regularization"},{"location":"0g1%20proximal%20ga/#6-properties-of-proximal-operators","title":"6. Properties of Proximal Operators","text":"<p>Proximal operators have several useful mathematical properties:</p>"},{"location":"0g1%20proximal%20ga/#non-expansiveness-lipschitz-continuity","title":"Non-expansiveness (Lipschitz continuity):","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2 \\le \\|x - y\\|_2, \\quad \\forall x, y \\]"},{"location":"0g1%20proximal%20ga/#firmly-non-expansive","title":"Firmly non-expansive:","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2^2 \\le \\langle \\text{prox}_{g}(x) - \\text{prox}_{g}(y), x - y \\rangle \\]"},{"location":"0g1%20proximal%20ga/#fixed-point-characterization","title":"Fixed point characterization:","text":"\\[ x^\\star = \\text{prox}_{g}(x^\\star - \\eta \\nabla f(x^\\star)) \\quad \\Longleftrightarrow \\quad 0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\] <p>This shows that proximal gradient fixed points correspond to optimality conditions for composite convex functions.</p>"},{"location":"0g1%20proximal%20ga/#translation-property","title":"Translation property:","text":"\\[ \\text{prox}_{g}(x + c) = \\text{prox}_{g(\\cdot - c)}(x) + c \\]"},{"location":"0g1%20proximal%20ga/#separable-for-sums-over-coordinates","title":"Separable for sums over coordinates:","text":"<p>If \\(g(x) = \\sum_i g_i(x_i)\\), then</p> \\[ \\text{prox}_{g}(x) = \\big( \\text{prox}_{g_1}(x_1), \\dots, \\text{prox}_{g_n}(x_n) \\big) \\] <p>This is why soft-thresholding works coordinate-wise.</p>"},{"location":"0g1%20proximal%20ga/#7-why-proximal-gradient-works","title":"7. Why Proximal Gradient Works","text":"<ul> <li>The proximal gradient method splits the objective into smooth and non-smooth parts.  </li> <li>The gradient step moves toward minimizing \\(f(x)\\) (smooth).  </li> <li>The proximal step moves toward minimizing \\(g(x)\\) (structure or constraints).  </li> <li> <p>Geometrically, the proximal operator finds a point close to the gradient update but also reduces the non-smooth term, ensuring convergence under convexity and Lipschitz continuity.  </p> </li> <li> <p>If \\(g = 0\\), it reduces to gradient descent.  </p> </li> <li>If \\(g\\) is an indicator function, it reduces to projected gradient descent.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#8-convergence","title":"8. Convergence","text":"<p>For convex \\(f\\) and \\(g\\):</p> \\[ F(x_t) - F(x^\\star) = \\mathcal{O}\\Big(\\frac{1}{t}\\Big) \\] <ul> <li>Accelerated variants (like FISTA) improve the rate to \\(\\mathcal{O}(1/t^2)\\).  </li> <li>Requires convexity and Lipschitz continuity of \\(\\nabla f\\).</li> </ul>"},{"location":"0h%20AdvancedAlgos/","title":"Advanced Algorithms","text":""},{"location":"0h%20AdvancedAlgos/#first-order-gradient-based-methods","title":"First-Order Gradient-Based Methods","text":"<p>Used when: Only gradient information is available; scalable to high-dimensional problems.</p>"},{"location":"0h%20AdvancedAlgos/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<ul> <li>Problem: Minimize smooth or convex functions.  </li> <li>Update: </li> <li>Convergence: Convex \u2192 \\(O(1/k)\\); Strongly convex \u2192 linear.  </li> <li>Use case: Small convex problems, theoretical baseline.  </li> <li>Pitfalls: Step size too small \u2192 slow; too large \u2192 divergence.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<ul> <li>Problem: Minimize empirical risk over large datasets.  </li> <li>Update:   (mini-batch gradient)  </li> <li>Pros: Scales to huge datasets; cheap per iteration.  </li> <li>Cons: Noisy updates \u2192 requires learning rate schedules.  </li> <li>ML use: Deep learning, large-scale logistic regression.  </li> </ul> <p>Best Practices: Learning rate warmup, linear scaling with batch size, momentum to stabilize updates, cyclic learning rates for exploration.</p>"},{"location":"0h%20AdvancedAlgos/#momentum-nesterov-accelerated-gradient","title":"Momentum &amp; Nesterov Accelerated Gradient","text":"<ul> <li>Problem: Reduce oscillations and accelerate convergence in ill-conditioned problems.  </li> <li>Momentum: </li> <li>Nesterov: Gradient computed at lookahead point \u2192 theoretically optimal for convex problems.  </li> <li>ML use: CNNs, ResNets, EfficientNet.  </li> <li>Pitfalls: High momentum \u2192 oscillations; careful learning rate tuning required.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#adaptive-methods-adagrad-rmsprop-adam-adamw","title":"Adaptive Methods (AdaGrad, RMSProp, Adam, AdamW)","text":"<ul> <li>Problem: Adjust learning rate per parameter for fast/stable convergence.  </li> <li>Behavior: </li> <li>AdaGrad \u2192 aggressive decay, good for sparse features.  </li> <li>RMSProp \u2192 fixes AdaGrad\u2019s rapid decay.  </li> <li>Adam \u2192 RMSProp + momentum.  </li> <li>AdamW \u2192 decouples weight decay for better generalization.  </li> <li>ML use: Transformers, NLP, sparse models.  </li> <li>Pitfalls: Adam may converge to sharp minima \u2192 worse generalization than SGD in CNNs.  </li> <li>Best Practices: Warmup, cosine LR decay, weight decay with AdamW.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#second-order-curvature-aware-methods","title":"Second-Order &amp; Curvature-Aware Methods","text":"<p>Used when: Hessian or curvature information improves convergence; mostly for small/medium models.</p>"},{"location":"0h%20AdvancedAlgos/#newtons-method","title":"Newton\u2019s Method","text":"<ul> <li>Problem: Solve  with smooth Hessian.  </li> <li>Update: </li> <li>Pros: Quadratic convergence.  </li> <li>Cons: Hessian expensive in high dimensions.  </li> <li>ML use: GLMs, small convex models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#quasi-newton-bfgs-l-bfgs","title":"Quasi-Newton (BFGS, L-BFGS)","text":"<ul> <li>Problem: Approximate Hessian using low-rank updates.  </li> <li>Pros: Efficient for medium-scale problems.  </li> <li>Cons: BFGS memory-heavy; L-BFGS preferred.  </li> <li>ML use: Logistic regression, Cox models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#conjugate-gradient","title":"Conjugate Gradient","text":"<ul> <li>Problem: Solve large linear/quadratic problems efficiently.  </li> <li>ML use: Hessian-free optimization; combined with Pearlmutter trick for Hessian-vector products.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#natural-gradient-k-fac","title":"Natural Gradient &amp; K-FAC","text":"<ul> <li>Problem: Precondition gradients using Fisher Information \u2192 invariant to parameterization.  </li> <li>ML use: Large CNNs, transformers; improves convergence in distributed training.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#constrained-specialized-optimization","title":"Constrained &amp; Specialized Optimization","text":""},{"location":"0h%20AdvancedAlgos/#interior-point","title":"Interior-Point","text":"<ul> <li>Problem: Constrained optimization via barrier functions.  </li> <li>ML use: Structured convex problems, LP/QP.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#admm-augmented-lagrangian","title":"ADMM / Augmented Lagrangian","text":"<ul> <li>Problem: Split constraints into easier subproblems with dual updates.  </li> <li>ML use: Distributed optimization, structured sparsity.  </li> </ul>"},{"location":"0h%20AdvancedAlgos/#frankwolfe","title":"Frank\u2013Wolfe","text":"<ul> <li>Problem: Projection-free constrained optimization; linear subproblem instead of projection.  </li> <li>ML use: Simplex, nuclear norm problems.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#coordinate-descent","title":"Coordinate Descent","text":"<ul> <li>Problem: Update one variable at a time.  </li> <li>ML use: Lasso, GLMs, sparse regression.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#proximal-methods","title":"Proximal Methods","text":"<ul> <li>Problem: Efficiently handle nonsmooth penalties.  </li> <li>Algorithms: ISTA (\\(O(1/k)\\)), FISTA (\\(O(1/k^2)\\))  </li> <li>ML use: Sparse coding, Lasso, elastic net.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#derivative-free-black-box","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Optimize when gradients unavailable or unreliable.  </li> <li>Algorithms: Nelder\u2013Mead, CMA-ES, Bayesian Optimization  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#optimization-problem-styles","title":"Optimization Problem Styles","text":""},{"location":"0h%20AdvancedAlgos/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<ul> <li>Problem: Maximize likelihood or minimize negative log-likelihood: </li> <li>Algorithms: Newton/Fisher scoring, L-BFGS, SGD, EM, Proximal/Coordinate.  </li> <li>ML use: Logistic regression, GLMs, Gaussian mixture models, HMMs.  </li> <li>Notes: EM guarantees monotonic likelihood increase; Fisher scoring uses expected curvature \u2192 stable.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#empirical-risk-minimization-erm","title":"Empirical Risk Minimization (ERM)","text":"<ul> <li>Problem: Minimize average loss with optional regularization: </li> <li>Algorithms: GD, SGD, Momentum, Adam, L-BFGS, Proximal.  </li> <li>ML use: Regression, classification, deep learning.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#regularized-penalized-optimization","title":"Regularized / Penalized Optimization","text":"<ul> <li>Problem: Add penalties to encourage sparsity or smoothness: </li> <li>Algorithms: Proximal gradient, ADMM, Coordinate Descent, ISTA/FISTA.  </li> <li>ML use: Lasso, Elastic Net, sparse dictionary learning.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#constrained-optimization","title":"Constrained Optimization","text":"<ul> <li>Problem: Minimize with equality/inequality constraints.  </li> <li>Algorithms: Interior-point, ADMM, Frank\u2013Wolfe, penalty/barrier methods.  </li> <li>ML use: Fairness constraints, structured prediction.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#bayesian-map-optimization","title":"Bayesian / MAP Optimization","text":"<ul> <li>Problem: Maximize posterior: </li> <li>Algorithms: Gradient-based, Laplace approximation, Variational Inference, MCMC.  </li> <li>ML use: Bayesian neural networks, probabilistic models.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#minimax-adversarial-optimization","title":"Minimax / Adversarial Optimization","text":"<ul> <li>Problem: </li> <li>Algorithms: Gradient descent/ascent, extragradient, mirror descent.  </li> <li>ML use: GANs, adversarial training, robust optimization.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#reinforcement-learning-policy-optimization","title":"Reinforcement Learning / Policy Optimization","text":"<ul> <li>Problem: Maximize expected cumulative reward: </li> <li>Algorithms: Policy gradient, Actor-Critic, Natural Gradient.  </li> <li>ML use: RL agents, sequential decision-making.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<ul> <li>Problem: Optimize multiple competing objectives \u2192 Pareto front.  </li> <li>Algorithms: Scalarization, weighted sum, evolutionary algorithms.  </li> <li>ML use: Multi-task learning, accuracy vs fairness trade-offs.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#metric-embedding-learning","title":"Metric / Embedding Learning","text":"<ul> <li>Problem: Learn embeddings preserving similarity/distance: </li> <li>Algorithms: SGD/Adam with careful sampling.  </li> <li>ML use: Contrastive learning, triplet loss, Siamese networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#combinatorial-discrete-optimization","title":"Combinatorial / Discrete Optimization","text":"<ul> <li>Problem: Optimize discrete/integer variables.  </li> <li>Algorithms: Branch-and-bound, integer programming, RL-based relaxation, Gumbel-softmax.  </li> <li>ML use: Feature selection, neural architecture search, graph matching.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#derivative-free-black-box_1","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Gradients unavailable or noisy.  </li> <li>Algorithms: Bayesian Optimization, CMA-ES, Nelder\u2013Mead.  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0h%20AdvancedAlgos/#learning-rate-practical-tips","title":"Learning Rate &amp; Practical Tips","text":"<ul> <li>Step decay, cosine annealing, OneCycle, warmup.  </li> <li>Gradient clipping (global norm 1\u20135), batch/layer normalization, FP16 mixed precision.  </li> <li>Decouple weight decay from Adam (AdamW).</li> </ul>"},{"location":"0h%20AdvancedAlgos/#summary","title":"Summary","text":"Algorithm Problem Type ML / AI Use Case GD Smooth / convex Small convex models, baseline SGD Large-scale ERM Deep learning, logistic regression SGD + Momentum Ill-conditioned / deep nets CNNs (ResNet, EfficientNet) Nesterov Accelerated GD Convex / ill-conditioned CNNs, small convex models AdaGrad Sparse features NLP, sparse embeddings RMSProp Stabilized adaptive LR RNNs, sequence models Adam Adaptive large-scale Transformers, small nets AdamW Adaptive + weight decay Transformers, NLP Newton / Fisher Scoring Smooth convex GLMs, small MLE BFGS / L-BFGS Medium convex Logistic regression, Cox models Conjugate Gradient Linear / quadratic Hessian-free optimization, linear regression Natural Gradient / K-FAC Deep nets CNNs, transformers Proximal / ISTA / FISTA Nonsmooth / sparse Lasso, sparse coding, elastic net Coordinate Descent Separable / sparse Lasso, GLMs Interior-Point Constrained convex LP/QP problems ADMM Distributed convex Sparse or structured optimization Frank\u2013Wolfe Projection-free constraints Simplex, nuclear norm problems EM Algorithm Latent variable MLE GMM, HMM, LDA Policy Gradient / Actor-Critic Sequential / RL RL agents Bayesian Optimization Black-box / derivative-free Hyperparameter tuning, NAS CMA-ES / Nelder-Mead Black-box Small networks, continuous black-box Minimax / Gradient Ascent-Descent Adversarial GANs, robust optimization Multi-Objective / Evolutionary Multiple objectives Multi-task learning, fairness Metric Learning / Triplet Loss Similarity embedding Contrastive learning, Siamese nets"},{"location":"0ssc%20Epigraphs/","title":"Epigraphs","text":""},{"location":"0ssc%20Epigraphs/#epigraphs-and-convex-optimization","title":"Epigraphs and Convex Optimization","text":""},{"location":"0ssc%20Epigraphs/#1-definition-of-an-epigraph","title":"1. Definition of an Epigraph","text":"<p>For a function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the epigraph is the set of points lying on or above its graph:</p> \\[ \\operatorname{epi}(f) = \\{ (x, t) \\in \\mathbb{R}^n \\times \\mathbb{R} \\;\\mid\\; f(x) \\le t \\}. \\] <ul> <li>\\((x, t)\\) is a point in \\((n+1)\\)-dimensional space.  </li> <li>For each \\(x\\), the condition \\(f(x) \\le t\\) means \\(t\\) is at or above the function value.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#2-intuition","title":"2. Intuition","text":"<ul> <li>If you draw a 2D function \\(f(x)\\), the epigraph is the region above the curve.  </li> <li>For example, if \\(f(x) = x^2\\), then the epigraph is everything above the parabola.  </li> </ul> <p>So:  </p> <ul> <li>Graph = the curve itself.  </li> <li>Epigraph = the curve + everything above it.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#3-convexity-via-epigraphs","title":"3. Convexity via Epigraphs","text":"<p>A function \\(f\\) is convex if and only if its epigraph is a convex set.  </p> <ul> <li>A set is convex if the line segment between any two points in the set lies entirely within the set.  </li> <li>Geometrically: the \"roof\" (epigraph) above the function must form a bowl-shaped region, not a cave.  </li> </ul>"},{"location":"0ssc%20Epigraphs/#4-examples","title":"4. Examples","text":"<ol> <li>Convex function: \\(f(x) = x^2\\) </li> <li>Epigraph = everything above the parabola.  </li> <li>This region is convex: if you connect any two points above the parabola, the line stays above the parabola.  </li> <li> <p>\u21d2 \\(f(x)\\) is convex.</p> </li> <li> <p>Non-convex function: \\(f(x) = -x^2\\) </p> </li> <li>Epigraph = everything above an upside-down parabola.  </li> <li>This region is not convex: connecting two points above the parabola can dip below it.  </li> <li>\u21d2 \\(f(x)\\) is not convex.</li> </ol>"},{"location":"0ssc%20Epigraphs/#5-why-epigraphs-matter-in-optimization","title":"5. Why Epigraphs Matter in Optimization","text":"<p>Many optimization problems can be written in epigraph form:</p> \\[ \\min_x f(x) \\quad \\equiv \\quad  \\min_{x,t} \\; t \\quad \\text{s.t. } f(x) \\le t. \\] <ul> <li>We \"lift\" the problem into one extra dimension.  </li> <li>The feasible region is the epigraph of \\(f\\).  </li> <li>Optimization over convex sets (epigraphs) is much more tractable.</li> </ul>"},{"location":"0ssc%20Epigraphs/#summary","title":"\u2705 Summary","text":"<ul> <li>The epigraph of a function is the region above its graph.  </li> <li>A function is convex iff its epigraph is convex.  </li> <li>Epigraphs let us reformulate optimization problems in a way that makes convexity clear and usable.  </li> </ul>"},{"location":"1a%20LP/","title":"LP","text":""},{"location":"1a%20LP/#linear-programming-lp-problem","title":"Linear Programming (LP) Problem","text":"<p>Linear Programming (LP) is a cornerstone of convex optimization. It is used to find a decision vector \\(x\\) that minimizes a linear objective function subject to linear constraints:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where: - \\(x \\in \\mathbb{R}^n\\) \u2014 decision variables, - \\(c \\in \\mathbb{R}^n\\) \u2014 cost vector, - \\(d \\in \\mathbb{R}\\) \u2014 constant offset (shifts objective but does not affect optimizer), - \\(G \\in \\mathbb{R}^{m \\times n}, \\; h \\in \\mathbb{R}^m\\) \u2014 inequality constraints, - \\(A \\in \\mathbb{R}^{p \\times n}, \\; b \\in \\mathbb{R}^p\\) \u2014 equality constraints.  </p>"},{"location":"1a%20LP/#why-lps-are-convex-optimization-problems","title":"Why LPs Are Convex Optimization Problems","text":"<p>A problem is convex if: 1. The objective is convex, 2. The feasible region is convex.  </p>"},{"location":"1a%20LP/#convexity-of-the-objective","title":"Convexity of the Objective","text":"<p>The LP objective is affine:</p> \\[ f(x) = c^T x + d \\] <ul> <li>Affine functions are both convex and concave (zero curvature).  </li> <li>Thus, no spurious local minima: every local optimum is global.  </li> </ul>"},{"location":"1a%20LP/#convexity-of-the-feasible-set","title":"Convexity of the Feasible Set","text":"<ul> <li>Each inequality \\(a^T x \\leq b\\) defines a half-space \u2014 convex.  </li> <li>Each equality \\(a^T x = b\\) defines a hyperplane \u2014 convex.  </li> <li>The intersection of convex sets is convex.  </li> </ul> <p>Hence, the feasible region is a convex polyhedron.</p>"},{"location":"1a%20LP/#geometric-intuition","title":"\ud83d\udcd0 Geometric Intuition","text":"<ul> <li>Inequalities act like flat walls, keeping feasible points on one side.  </li> <li>Equalities act like flat sheets, slicing through space.  </li> <li>The feasible region is a polyhedron (possibly unbounded).  </li> <li>LP solutions always occur at a vertex (extreme point) of this polyhedron \u2014 this fact powers the simplex algorithm.  </li> </ul>"},{"location":"1a%20LP/#canonical-and-standard-forms","title":"Canonical and Standard Forms","text":"<p>LPs are often reformulated for theory and solvers:</p> <ul> <li>Canonical form (minimization):</li> </ul> \\[ \\min \\; c^T x \\quad \\text{s.t. } A x = b, \\; x \\geq 0 \\] <ul> <li>Standard form (maximization):</li> </ul> \\[ \\max \\; c^T x \\quad \\text{s.t. } A x \\leq b, \\; x \\geq 0 \\] <p>Any LP can be transformed into one of these forms via slack variables and variable splitting.</p>"},{"location":"1a%20LP/#duality-in-linear-programming","title":"\u2696\ufe0f Duality in Linear Programming","text":"<p>Every LP has a dual problem:</p> <ul> <li>Primal (minimization):</li> </ul> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } Gx \\leq h, \\; A x = b \\] <ul> <li>Dual:</li> </ul> \\[ \\max_{\\lambda, \\nu} \\; -h^T \\lambda + b^T \\nu \\quad \\text{s.t. } G^T \\lambda + A^T \\nu = c, \\; \\lambda \\geq 0 \\]"},{"location":"1a%20LP/#properties","title":"Properties:","text":"<ul> <li>Weak duality: Dual objective \\(\\leq\\) Primal objective.  </li> <li>Strong duality: Holds under mild conditions (Slater\u2019s condition).  </li> <li>Complementary slackness provides optimality certificates.  </li> </ul> <p>Duality underpins modern algorithms like interior-point methods.</p>"},{"location":"1a%20LP/#robust-linear-programming-rlp","title":"Robust Linear Programming (RLP)","text":"<p>In many applications, the LP data (\\(c, A, G, b, h\\)) are uncertain due to noise, estimation errors, or worst-case planning requirements. Robust Optimization handles this by requiring constraints to hold for all possible realizations of the uncertain parameters within a given uncertainty set.</p>"},{"location":"1a%20LP/#general-robust-lp-formulation","title":"General Robust LP Formulation","text":"<p>Consider an uncertain LP:</p> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } G(u) x \\leq h(u), \\quad \\forall u \\in \\mathcal{U} \\] <ul> <li>\\(\\mathcal{U}\\): uncertainty set (polyhedron, ellipsoid, box, etc.)  </li> <li>\\(u\\): uncertain parameters affecting \\(G, h\\).  </li> </ul> <p>The robust counterpart requires feasibility for all \\(u \\in \\mathcal{U}\\).</p>"},{"location":"1a%20LP/#box-uncertainty-interval-uncertainty","title":"Box Uncertainty (Interval Uncertainty)","text":"<p>Suppose \\(G = G_0 + \\Delta G\\), with each row uncertain in a box set:</p> \\[ \\{ g_i : g_i = g_i^0 + \\delta, \\; \\|\\delta\\|_\\infty \\leq \\rho \\} \\] <p>Robust constraint:</p> \\[ g_i^T x \\leq h_i, \\quad \\forall g_i \\in \\mathcal{U} \\] <p>This is equivalent to:</p> \\[ g_i^{0T} x + \\rho \\|x\\|_1 \\leq h_i \\] <p>Thus, a robust LP under box uncertainty is still a deterministic convex program (LP with additional \\(\\ell_1\\) terms).</p>"},{"location":"1a%20LP/#ellipsoidal-uncertainty","title":"Ellipsoidal Uncertainty","text":"<p>If uncertainty lies in an ellipsoid:</p> \\[ \\mathcal{U} = \\{ g : g = g^0 + Q^{1/2} u, \\; \\|u\\|_2 \\leq 1 \\} \\] <p>then the robust counterpart becomes:</p> \\[ g^{0T} x + \\| Q^{1/2} x \\|_2 \\leq h \\] <p>This is a Second-Order Cone Program (SOCP), still convex but more general than LP.</p>"},{"location":"1a%20LP/#robust-objective","title":"Robust Objective","text":"<p>When cost vector \\(c\\) is uncertain in \\(\\mathcal{U}_c\\):</p> \\[ \\min_{x} \\max_{c \\in \\mathcal{U}_c} c^T x \\] <ul> <li>If \\(\\mathcal{U}_c\\) is a box: inner max = \\(c^T x + \\rho \\|x\\|_1\\) </li> <li>If \\(\\mathcal{U}_c\\) is ellipsoidal: inner max = \\(c^T x + \\|Q^{1/2} x\\|_2\\) </li> </ul> <p>Thus, robust objectives often introduce regularization-like terms.  </p>"},{"location":"1a%20LP/#applications-of-robust-lp","title":"Applications of Robust LP","text":"<ul> <li>Supply chain optimization: demand uncertainty \u2192 robust inventory policies.  </li> <li>Finance: portfolio selection under uncertain returns.  </li> <li>Energy systems: robust scheduling under uncertain loads.  </li> <li>AI/ML: adversarial optimization, distributionally robust ML training.  </li> </ul>"},{"location":"1a%20LP/#how-lp-scales-in-practice","title":"How LP Scales in Practice","text":""},{"location":"1a%20LP/#polynomial-time-solvability","title":"Polynomial-Time Solvability","text":"<ul> <li>LPs can be solved in polynomial time using Interior-Point Methods (IPMs).  </li> <li>For an LP with \\(n\\) variables and \\(m\\) constraints, classical IPM complexity is roughly:</li> </ul> \\[ O((n+m)^3) \\] <ul> <li>But real-world performance depends on sparsity and problem structure. Sparse LPs are often solved in nearly linear time with specialized solvers.</li> </ul>"},{"location":"1a%20LP/#solver-ecosystem","title":"Solver Ecosystem","text":"<ul> <li>Commercial solvers: Gurobi, CPLEX, Mosek \u2014 highly optimized, exploit sparsity and parallelism, support warm-starts. These dominate large-scale industrial and financial problems.  </li> <li>Open-source solvers: HiGHS, GLPK, SCIP \u2014 robust for moderate problems, widely integrated into Python/Julia (via PuLP, Pyomo, CVXPY).  </li> <li>ML integration: CVXPY and PyTorch integrations make LP-based optimization easily callable inside ML pipelines.  </li> </ul>"},{"location":"1a%20LP/#algorithmic-tradeoffs","title":"Algorithmic Tradeoffs","text":"<ul> <li>Simplex method: moves along vertices of the feasible polyhedron.  </li> <li>Often very fast in practice, though exponential in theory.  </li> <li>Warm-starts make it excellent for iterative ML problems.  </li> <li>Interior-Point Methods (IPMs): follow a central path through the feasible region.  </li> <li>Polynomial worst-case guarantees.  </li> <li>Very robust to degeneracy, well-suited to dense problems.  </li> <li>First-order and decomposition methods:  </li> <li>ADMM, primal-dual splitting, stochastic coordinate descent.  </li> <li>Scale to massive LPs with billions of variables.  </li> <li>Sacrifice exactness for approximate but usable solutions.  </li> </ul>"},{"location":"1a%20LP/#comparison-of-lp-solvers","title":"Comparison of LP Solvers","text":"Method Complexity (theory) Scaling in practice Strengths Weaknesses ML/Engineering Use Cases Simplex Worst-case exponential Very fast in practice (near-linear for sparse LPs) Supports warm-starts, excellent for re-solving May stall on degenerate problems Iterative ML models, resource allocation, network flow Interior-Point (IPM) \\(O((n+m)^3)\\) Handles millions of variables if sparse Polynomial guarantees, robust, finds central solutions Memory-heavy (factorization of large matrices) Large dense LPs, convex relaxations in ML, finance First-order methods Sublinear (per iteration) Scales to billions of variables Memory-efficient, parallelizable Only approximate solutions MAP inference in CRFs, structured SVMs, massive embeddings Decomposition methods Problem-dependent Linear or near-linear scaling when structure exploited Breaks huge problems into smaller ones Requires separable structure Supply chain optimization, distributed training, scheduling"},{"location":"1a%20LP/#solving-large-scale-lps-in-ml-and-engineering","title":"Solving Large-Scale LPs in ML and Engineering","text":"<p>When problem sizes explode (e.g., \\(10^8\\) variables in embeddings or large-scale resource scheduling), standard solvers may fail due to memory or time.</p>"},{"location":"1a%20LP/#strategies","title":"Strategies","text":"<ul> <li>Decomposition methods:  </li> <li>Dantzig\u2013Wolfe, Benders, Lagrangian relaxation break problems into subproblems solved iteratively.  </li> <li>Column generation:  </li> <li>Introduces only a subset of variables initially, generating new ones as needed.  </li> <li>Stochastic and online optimization:  </li> <li>Replaces full LP solves with SGD-like updates, used in ML training pipelines.  </li> <li>Approximate relaxations:  </li> <li>In structured ML, approximate LP solutions often suffice (e.g., in structured prediction tasks).  </li> </ul>"},{"location":"1a%20LP/#ml-perspective","title":"ML Perspective","text":"<ul> <li>Structured prediction: LP relaxations approximate inference in CRFs, structured SVMs.  </li> <li>Adversarial robustness: Worst-case perturbation problems often reduce to LP relaxations, especially under \\(\\ell_\\infty\\) constraints.  </li> <li>Fairness: Linear constraints encode fairness requirements inside risk minimization objectives.  </li> <li>Large-scale systems: Recommender systems, resource allocation, energy scheduling \u2192 decomposition + approximate LP solvers.  </li> </ul>"},{"location":"1a%20LP/#where-lp-struggles-failure-modes","title":"Where LP Struggles (Failure Modes)","text":"<p>Despite its power, LPs face limitations:</p> <ol> <li>Nonlinearities </li> <li>Many ML objectives (e.g., log-likelihood, quadratic loss) are nonlinear.  </li> <li> <p>LP relaxations may be loose, requiring QP, SOCP, or nonlinear solvers.  </p> </li> <li> <p>Integrality </p> </li> <li>LP cannot enforce discrete decisions.  </li> <li> <p>Mixed-Integer Linear Programs (MILPs) are NP-hard, limiting scalability.  </p> </li> <li> <p>Uncertainty </p> </li> <li>Classical LP assumes perfect knowledge of data.  </li> <li> <p>Real problems require Robust LP or Stochastic LP.  </p> </li> <li> <p>Numerical conditioning </p> </li> <li>Poorly scaled coefficients lead to solver instability.  </li> <li> <p>Always normalize inputs for ML-scale LPs.  </p> </li> <li> <p>Memory bottlenecks </p> </li> <li>IPMs require factorizing large dense matrices \u2014 infeasible for extremely large-scale ML problems.  </li> </ol>"},{"location":"1b%20QP/","title":"QP","text":""},{"location":"1b%20QP/#quadratic-programming-qp-problem","title":"Quadratic Programming (QP) Problem","text":"<p>Quadratic Programming (QP) is an optimization framework that generalizes Linear Programming by allowing a quadratic objective function, while keeping the constraints linear. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q x + c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector to be optimized.  </li> <li>\\(Q \\in \\mathbb{R}^{n \\times n}\\) \u2014 the Hessian matrix defining the curvature of the objective.  </li> <li>\\(c \\in \\mathbb{R}^n\\) \u2014 the linear cost term.  </li> <li>\\(d \\in \\mathbb{R}\\) \u2014 a constant offset (does not affect the minimizer\u2019s location).  </li> <li>\\(G \\in \\mathbb{R}^{m \\times n}\\), \\(h \\in \\mathbb{R}^m\\) \u2014 inequality constraints \\(Gx \\leq h\\).  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints \\(Ax = b\\).  </li> </ul>"},{"location":"1b%20QP/#why-qps-can-be-convex-optimization-problems","title":"Why QPs Can Be Convex Optimization Problems","text":"<p>Whether a QP is convex depends on one key condition:</p> <ul> <li>Convex QP: The Hessian \\(Q\\) is positive semidefinite (\\(Q \\succeq 0\\)).  </li> <li>Nonconvex QP: The Hessian has negative eigenvalues (some directions curve downward).</li> </ul>"},{"location":"1b%20QP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QP objective is:</p> \\[ f(x) = \\frac{1}{2} x^T Q x + c^T x + d \\] <p>This is a quadratic function, which is:</p> <ul> <li>Convex if \\(Q \\succeq 0\\) (all curvature is flat or bowl-shaped).</li> <li>Strictly convex if \\(Q \\succ 0\\) (curvature is strictly bowl-shaped, ensuring a unique minimizer).</li> <li>Nonconvex if \\(Q\\) has negative eigenvalues (some directions slope downward).</li> </ul> <p>The gradient and Hessian are:</p> \\[ \\nabla f(x) = Qx + c, \\quad \\nabla^2 f(x) = Q \\] <p>Since the Hessian is constant in QPs, checking convexity is straightforward:  </p> <p>Positive semidefinite Hessian \u2192 Convex objective \u2192 No spurious local minima.</p>"},{"location":"1b%20QP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>Exactly as in LPs:</p> <ul> <li>Each inequality \\(a^T x \\leq b\\) is a half-space (convex).  </li> <li>Each equality \\(a^T x = b\\) is a hyperplane (convex).  </li> </ul> <p>The feasible set:</p> \\[ \\mathcal{F} = \\{ x \\mid Gx \\leq h, \\quad Ax = b \\} \\] <p>is the intersection of convex sets, hence convex.</p>"},{"location":"1b%20QP/#the-feasible-set-is-a-convex-polyhedron","title":"The Feasible Set is a Convex Polyhedron","text":"<p>For convex QPs:</p> <ul> <li>The feasible region \\(\\mathcal{F}\\) is still a convex polyhedron (because constraints are the same as in LPs).  </li> <li>The objective is a convex quadratic \"bowl\" sitting over that polyhedron.  </li> <li>The optimal solution is where the bowl\u2019s lowest point touches the feasible polyhedron.</li> </ul>"},{"location":"1b%20QP/#geometric-intuition-visualizing-qp","title":"Geometric Intuition: Visualizing QP","text":"<ul> <li>In LP, the objective is a flat plane sliding over a polyhedron.  </li> <li>In convex QP, the objective is a curved bowl sliding over the same polyhedron.  </li> <li>If the bowl\u2019s center lies inside the feasible region, the optimum is at that center.  </li> <li>If not, the bowl \u201cleans\u201d against the polyhedron\u2019s faces, edges, or vertices \u2014 which is where the optimal solution lies.</li> </ul> <p>\u2705 Summary: A QP is a convex optimization problem if and only if \\(Q \\succeq 0\\). In that case:</p> <ul> <li>Objective: Convex quadratic.  </li> <li>Constraints: Linear, hence convex.  </li> <li>Feasible set: Convex polyhedron.  </li> <li>Solution: Found at the point in the feasible set where the quadratic surface reaches its lowest value.</li> </ul>"},{"location":"1c%20Least%20Square/","title":"Least Squares","text":""},{"location":"1c%20Least%20Square/#least-squares-ls-problem","title":"\ud83d\udd39 Least Squares (LS) Problem","text":"<p>Least Squares (LS) is one of the canonical convex optimization problems in statistics, machine learning, and signal processing. It seeks the vector \\(x \\in \\mathbb{R}^n\\) that minimizes the sum of squared errors:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\; \\|A x - b\\|_2^2 \\] <p>Where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) \u2014 data or measurement matrix, - \\(b \\in \\mathbb{R}^m\\) \u2014 observation or target vector, - \\(x \\in \\mathbb{R}^n\\) \u2014 decision vector (unknowns to estimate).  </p>"},{"location":"1c%20Least%20Square/#objective-expansion","title":"Objective Expansion","text":"<p>Expanding the squared norm:</p> \\[ \\|A x - b\\|_2^2 = (A x - b)^T (A x - b) = x^T A^T A x - 2 b^T A x + b^T b \\] <p>This is a quadratic convex function. In standard quadratic form:</p> \\[ f(x) = \\tfrac{1}{2} x^T Q x + c^T x + d \\] <p>with</p> \\[ Q = 2 A^T A, \\quad c = -2 A^T b, \\quad d = b^T b \\]"},{"location":"1c%20Least%20Square/#convexity","title":"\u2705 Convexity","text":"<ul> <li>\\(Q = 2 A^T A \\succeq 0\\) since for any \\(z\\):  </li> </ul> \\[ z^T (A^T A) z = \\|A z\\|_2^2 \\geq 0 \\] <ul> <li>Hence LS is convex.  </li> <li>If \\(A\\) has full column rank, then \\(A^T A \\succ 0\\), making the problem strictly convex with a unique minimizer.  </li> </ul>"},{"location":"1c%20Least%20Square/#geometric-intuition","title":"\ud83d\udcd0 Geometric Intuition","text":"<ul> <li>When \\(m &gt; n\\) (overdetermined system), the equations \\(A x = b\\) may not be solvable.  </li> <li>LS finds \\(x^\\star\\) such that \\(A x^\\star\\) is the orthogonal projection of \\(b\\) onto the column space of \\(A\\).  </li> <li>The residual \\(r = b - A x^\\star\\) is orthogonal to \\(\\text{col}(A)\\):</li> </ul> \\[ A^T (b - A x^\\star) = 0 \\]"},{"location":"1c%20Least%20Square/#solutions","title":"\ud83e\uddee Solutions","text":"<ul> <li>Normal Equations (full-rank case):</li> </ul> \\[ x^\\star = (A^T A)^{-1} A^T b \\] <ul> <li>General Case (possibly rank-deficient): The solution set is affine. The minimum-norm solution is given by the Moore\u2013Penrose pseudoinverse:</li> </ul> \\[ x^\\star = A^+ b \\] <ul> <li>Numerical Considerations: Normal equations can be ill-conditioned. In practice:</li> <li>Use QR decomposition or  </li> <li>SVD (stable, gives pseudoinverse).  </li> </ul>"},{"location":"1c%20Least%20Square/#constrained-least-squares-cls","title":"\ud83d\udd12 Constrained Least Squares (CLS)","text":"<p>Many practical problems require constraints on the solution. A general CLS formulation is:</p> \\[ \\begin{aligned} \\min_{x} \\quad &amp; \\|A x - b\\|_2^2 \\\\ \\text{s.t.} \\quad &amp; G x \\leq h \\\\ &amp; A_{\\text{eq}} x = b_{\\text{eq}} \\end{aligned} \\] <ul> <li>Objective: convex quadratic.  </li> <li>Constraints: linear.  </li> <li>Therefore: CLS is always a Quadratic Program (QP).</li> </ul>"},{"location":"1c%20Least%20Square/#example-1-wear-and-tear-allocation-cls-with-inequalities","title":"Example 1: Wear-and-Tear Allocation (CLS with Inequalities)","text":"<p>Suppose a landlord models annual apartment wear-and-tear costs as:</p> \\[ c_t \\approx a t + b, \\quad t = 1,\\dots,n \\] <p>with parameters \\(x = [a, b]^T\\).  </p> <p>CLS formulation:</p> \\[ \\min_{a,b} \\sum_{t=1}^n (a t + b - c_t)^2 \\] <p>Constraints (practical feasibility):</p> <ul> <li>Costs cannot be negative: </li> </ul> <p>This yields a CLS problem with linear inequality constraints, hence a QP.  </p>"},{"location":"1c%20Least%20Square/#example-2-energy-consumption-fitting-cls-with-box-constraints","title":"\ud83d\udca1 Example 2: Energy Consumption Fitting (CLS with Box Constraints)","text":"<p>Suppose we fit energy usage from appliance data:  </p> <ul> <li>\\(A \\in \\mathbb{R}^{m \\times n}\\) usage matrix,  </li> <li>\\(b \\in \\mathbb{R}^m\\) observed energy bills.  </li> </ul> <p>CLS formulation:</p> \\[ \\min_{x} \\|A x - b\\|^2 \\] <p>Constraints: each appliance has a usage cap:  </p> \\[ 0 \\leq x_i \\leq u_i, \\quad i = 1,\\dots,n \\] <p>This is a QP with box constraints, often solved efficiently by projected gradient or interior-point methods.  </p>"},{"location":"1c%20Least%20Square/#regularized-least-squares-ridge-regression","title":"\ud83d\udd27 Regularized Least Squares (Ridge Regression)","text":"<p>A common extension in ML is regularized LS, e.g., ridge regression:</p> \\[ \\min_x \\|A x - b\\|^2 + \\lambda \\|x\\|_2^2 \\] <ul> <li>Equivalent to CLS with a quadratic penalty on \\(x\\).  </li> <li>Ensures uniqueness even if \\(A\\) is rank-deficient.  </li> <li>Solution:</li> </ul> \\[ x^\\star = (A^T A + \\lambda I)^{-1} A^T b \\]"},{"location":"1c%20Least%20Square/#summary","title":"\ud83d\udcca Summary","text":"<ul> <li>Unconstrained LS: convex quadratic, closed form via pseudoinverse.  </li> <li>CLS: convex quadratic + linear constraints \u2192 QP.  </li> <li>Regularized LS: stabilizes solution, improves generalization.  </li> <li>Geometry: LS = orthogonal projection; CLS = projection with constraints.  </li> <li>Solvers: </li> <li>Small problems: QR/SVD (LS) or active-set (CLS).  </li> <li>Large problems: iterative methods (CG, projected gradient).  </li> </ul>"},{"location":"1d%20QCQP/","title":"QCQP","text":""},{"location":"1d%20QCQP/#quadratically-constrained-quadratic-programming-qcqp-problem","title":"Quadratically Constrained Quadratic Programming (QCQP) Problem","text":"<p>A Quadratically Constrained Quadratic Program (QCQP) is an optimization problem in which both the objective and the constraints can be quadratic functions. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\\\ \\text{subject to} \\quad &amp; \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0, \\quad i = 1, \\dots, m \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(Q_0, Q_i \\in \\mathbb{R}^{n \\times n}\\) \u2014 symmetric matrices defining curvature of the objective and constraints.  </li> <li>\\(c_0, c_i \\in \\mathbb{R}^n\\) \u2014 linear terms in the objective and constraints.  </li> <li>\\(d_0, d_i \\in \\mathbb{R}\\) \u2014 constant offsets.  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints (linear).  </li> </ul>"},{"location":"1d%20QCQP/#why-qcqps-can-be-convex-optimization-problems","title":"Why QCQPs Can Be Convex Optimization Problems","text":"<p>QCQPs are not automatically convex \u2014 convexity requires specific conditions:</p> <ol> <li> <p>Objective convexity: \\(Q_0 \\succeq 0\\) (positive semidefinite Hessian for the objective).</p> </li> <li> <p>Constraint convexity:    For each inequality constraint \\(i\\), \\(Q_i \\succeq 0\\) so that  defines a convex set.</p> </li> <li> <p>Equality constraints:    Must be affine (linear), e.g., \\(A x = b\\).</p> </li> </ol>"},{"location":"1d%20QCQP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QCQP objective:</p> \\[ f_0(x) = \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\] <p>is convex iff \\(Q_0 \\succeq 0\\).</p>"},{"location":"1d%20QCQP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>A single quadratic constraint:</p> \\[ f_i(x) = \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0 \\] <p>defines a convex feasible set iff \\(Q_i \\succeq 0\\).  </p> <p>If any \\(Q_i\\) is not positive semidefinite, the constraint set becomes nonconvex, and the overall problem is nonconvex.</p>"},{"location":"1d%20QCQP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>If all \\(Q_i \\succeq 0\\), inequality constraints define convex quadratic regions (ellipsoids, elliptic cylinders, or half-spaces).</li> <li>Equality constraints \\(A x = b\\) cut flat slices through these regions.</li> <li>The feasible set is the intersection of convex quadratic sets and affine sets \u2014 hence convex.</li> </ul>"},{"location":"1d%20QCQP/#geometric-intuition-visualizing-qcqp","title":"Geometric Intuition: Visualizing QCQP","text":"<ul> <li>In QP, only the objective is curved; constraints are flat.  </li> <li>In QCQP, constraints can also be curved \u2014 forming shapes like ellipsoids or paraboloids.  </li> <li>Convex QCQPs look like a \u201cbowl\u201d objective contained within (or pressed against) curved convex walls.  </li> <li>Nonconvex QCQPs can have holes or disconnected regions, making them much harder to solve.</li> </ul> <p>\u2705 Summary: A QCQP is a convex optimization problem if and only if:</p> <ul> <li>\\(Q_0 \\succeq 0\\) (objective convexity), and  </li> <li>\\(Q_i \\succeq 0\\) for all \\(i\\) (each quadratic inequality constraint convex), and  </li> <li>All equality constraints are affine.  </li> </ul> <p>When these hold: - Objective: Convex quadratic. - Constraints: Convex quadratic or affine. - Feasible set: Intersection of convex sets (can be curved). - Solution: Found where the objective\u2019s minimum touches the convex feasible region.</p>"},{"location":"1e%20SOCP/","title":"SOCP","text":""},{"location":"1e%20SOCP/#second-order-cone-programming-socp-problem","title":"Second-Order Cone Programming (SOCP) Problem","text":"<p>Second-Order Cone Programming (SOCP) is a class of convex optimization problems that generalizes Linear and (certain) Quadratic Programs by allowing constraints involving second-order (quadratic) cones.  </p> <p>Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f^T x \\\\ \\text{subject to} \\quad &amp; \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i, \\quad i = 1, \\dots, m \\\\ &amp; F x = g \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(f \\in \\mathbb{R}^n\\) \u2014 the linear objective coefficients.  </li> <li>\\(A_i \\in \\mathbb{R}^{k_i \\times n}\\), \\(b_i \\in \\mathbb{R}^{k_i}\\) \u2014 define the affine transformation inside the norm for cone \\(i\\).  </li> <li>\\(c_i \\in \\mathbb{R}^n\\), \\(d_i \\in \\mathbb{R}\\) \u2014 define the affine term on the right-hand side.  </li> <li>\\(F \\in \\mathbb{R}^{p \\times n}\\), \\(g \\in \\mathbb{R}^p\\) \u2014 define linear equality constraints.</li> </ul>"},{"location":"1e%20SOCP/#the-second-order-quadratic-cone","title":"The Second-Order (Quadratic) Cone","text":"<p>A second-order cone in \\(\\mathbb{R}^k\\) is:</p> \\[ \\mathcal{Q}^k = \\left\\{ (u,t) \\in \\mathbb{R}^{k-1} \\times \\mathbb{R} \\ \\middle|\\ \\|u\\|_2 \\leq t \\right\\} \\] <p>Key properties: - Convex set. - Rotationally symmetric around the \\(t\\)-axis. - Contains all rays pointing \u201cupward\u201d inside the cone.</p>"},{"location":"1e%20SOCP/#why-socp-is-a-convex-optimization-problem","title":"Why SOCP is a Convex Optimization Problem","text":""},{"location":"1e%20SOCP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<ul> <li>The SOCP objective \\(f^T x\\) is affine.</li> <li>Affine functions are both convex and concave \u2014 no curvature.</li> </ul>"},{"location":"1e%20SOCP/#2-convexity-of-the-constraints","title":"2. Convexity of the Constraints","text":"<p>Each second-order cone constraint:</p> \\[ \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i \\] <p>is convex because: - The left-hand side \\(\\|A_i x + b_i\\|_2\\) is a convex function of \\(x\\). - The right-hand side \\(c_i^T x + d_i\\) is affine. - The set \\(\\{x \\mid \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i\\}\\) is a convex set.</p> <p>Equality constraints \\(F x = g\\) define a hyperplane, which is convex.</p> <p>Since the feasible region is the intersection of convex sets, it is convex.</p>"},{"location":"1e%20SOCP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>Each SOCP constraint defines a rotated or shifted cone in \\(x\\)-space.</li> <li>Equality constraints slice the space with flat hyperplanes.</li> <li>The feasible set is the intersection of these cones and hyperplanes.</li> </ul>"},{"location":"1e%20SOCP/#special-cases-of-socp","title":"Special Cases of SOCP","text":"<ul> <li>Linear Programs (LP): If all \\(A_i = 0\\), cone constraints reduce to linear inequalities.</li> <li>Certain Quadratic Programs (QP): Quadratic inequalities of the form \\(\\|Q^{1/2}x\\|_2 \\leq a^T x + b\\) can be rewritten as SOCP constraints.</li> <li>Norm Constraints: Bounds on \\(\\ell_2\\)-norms (e.g., \\(\\|x\\|_2 \\leq t\\)) are directly SOCP constraints.</li> </ul>"},{"location":"1e%20SOCP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>In LP, constraints are flat walls.</li> <li>In QP, objective is curved but constraints are flat.</li> <li>In SOCP, constraints themselves are curved (cone-shaped), allowing more modeling flexibility.</li> <li>The optimal solution is where the objective plane just \u201ctouches\u201d the feasible cone-shaped region.</li> </ul> <p>\u2705 Summary: - Objective: Affine (linear) \u2192 convex. - Constraints: Intersection of affine equalities and convex second-order cone inequalities. - Feasible set: Convex \u2014 shaped by cones and hyperplanes. - Power: Captures LPs, norm minimization, robust optimization, and some QCQPs. - Solution: Found efficiently by interior-point methods specialized for conic programming.</p>"},{"location":"1f%20GeometricInterpretation/","title":"Geometric Interpretation","text":"Feature LP QP QCQP SOCP Objective Linear: \\(c^T x\\) Quadratic: \\(\\frac{1}{2} x^T Q x + c^T x\\), convex if \\(Q \\succeq 0\\), indefinite \\(Q\\) \u2192 nonconvex Quadratic: \\(\\frac{1}{2} x^T Q_0 x + c_0^T x\\), convex if \\(Q_0 \\succeq 0\\), indefinite \u2192 nonconvex Linear: \\(c^T x\\), always convex Objective level sets Hyperplanes (flat, parallel) Quadrics: ellipsoids/paraboloids if \\(Q \\succeq 0\\), hyperboloids if indefinite Quadrics: ellipsoids/paraboloids; shape depends on \\(Q_0\\) definiteness Hyperplanes (flat, parallel) Constraints Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Quadratic: \\(\\frac{1}{2} x^T Q_i x + c_i^T x \\le b_i\\) \u2192 curved surfaces; convex if \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Second-order cone: \\(\\|A_i x + b_i\\|_2 \\le c_i^T x + d_i\\) \u2192 convex, curved conic surfaces Feasible region Polyhedron (flat faces, convex) Polyhedron (flat faces, convex) Curved region; convex if all \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Intersection of convex cones; curved, convex region Optimum location Vertex (extreme point of polyhedron) Face, edge, vertex, or interior if unconstrained minimizer feasible Boundary or interior; multiple local minima possible if nonconvex Boundary or interior; linear objective touches cone tangentially 2D Example Max \\(x_1 + 2x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 4\\) \u2192 polygon Min \\(x_1^2 + x_2^2 + x_1 + x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 3\\) \u2192 polygon feasible, elliptical contours Min \\(x_1^2 + x_2^2\\), s.t. \\(x_1^2 + x_2^2 \\le 4\\), \\(x_1 + x_2 \\le 3\\) \u2192 circular + linear \u2192 curved feasible Min \\(x_1 + x_2\\), s.t. \\(\\sqrt{x_1^2 + x_2^2} \\le 2 - 0.5 x_1\\) \u2192 tilted cone 3D Example Max \\(x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 polyhedron Min \\(x_1^2 + x_2^2 + x_3^2 + x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 4\\) \u2192 polyhedron + ellipsoid Min \\(x_1^2 + x_2^2 + x_3^2\\), s.t. \\(x_1^2 + x_2^2 + x_3^2 \\le 9\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 spherical + plane \u2192 curved feasible Min \\(x_1 + x_2 + x_3\\), s.t. \\(\\sqrt{x_1^2 + x_2^2 + x_3^2} \\le 4 - 0.5 x_3\\) \u2192 3D cone 4D Intuition Polytope + 3D hyperplane Polytope + 4D ellipsoid Curved 4D region + 4D ellipsoid; convex if \\(Q_0,Q_i \\succeq 0\\) 4D cone + 3D hyperplane Curvature Hint Flat objective / flat constraints Curved objective / flat constraints Curved objective / curved constraints Flat objective / curved constraints"},{"location":"1g%20GP/","title":"GP","text":""},{"location":"1g%20GP/#geometric-programming-gp","title":"\ud83d\udcd8 Geometric Programming (GP)","text":"<p>Geometric Programming (GP) is a flexible, widely used optimization class (communications, circuit design, resource allocation, control, ML model fitting). In its natural variable form it looks nonconvex, but \u2014 crucially \u2014 there is a canonical change of variables and a monotone transformation that converts a GP into a convex optimization problem.</p>"},{"location":"1g%20GP/#definitions-monomials-posynomials-and-the-standard-gp","title":"Definitions: monomials, posynomials, and the standard GP","text":"<p>Let \\(x=(x_1,\\dots,x_n)\\) with \\(x_i&gt;0\\).</p> <ul> <li> <p>A monomial (in GP terminology) is a function of the form  where \\(c&gt;0\\) and the exponents \\(a_i\\in\\mathbb{R}\\) (real exponents allowed).  </p> <p>Note: in GP literature \"monomial\" means positive coefficient times a power product (not to be confused with polynomial monomial which has nonnegative integer powers).</p> </li> <li> <p>A posynomial is a sum of monomials:  </p> </li> <li> <p>Standard (inequality) form of a geometric program:  where each \\(p_i\\) is a posynomial and each \\(m_j\\) is a monomial. (Any GP with other RHS values can be normalized to this form by dividing.)</p> </li> </ul>"},{"location":"1g%20GP/#why-gp-in-the-original-x-variables-is-not-convex","title":"\u2757 Why GP (in the original \\(x\\) variables) is not convex","text":"<ul> <li> <p>A monomial \\(m(x)=c x^a\\) (single variable) is convex on \\(x&gt;0\\) only for certain exponent ranges (e.g. \\(a\\le 0\\) or \\(a\\ge 1\\)). For \\(0&lt;a&lt;1\\) it is concave; for general real \\(a\\) it can be neither globally convex nor concave over \\(x&gt;0\\).   Example: \\(f(x)=x^{1/2}\\) (\\(0&lt;a&lt;1\\)) is concave on \\((0,\\infty)\\) (second derivative \\(=\\tfrac{1}{4}x^{-3/2}&gt;0\\)? \u2014 check sign; in fact \\(f''(x)= -\\tfrac{1}{4} x^{-3/2}&lt;0\\) showing concavity).</p> </li> <li> <p>A posynomial is a sum of monomials. Sums of nonconvex (or concave) terms are generally nonconvex. There is no general convexity guarantee for posynomials in the original variables \\(x\\).</p> </li> <li> <p>Therefore the objective \\(p_0(x)\\) and constraints \\(p_i(x)\\le 1\\) are not convex functions/constraints in \\(x\\), so the GP is not a convex program in the \\(x\\)-space.</p> </li> </ul> <p>Concrete counterexample (1D): take \\(p(x)=x^{1/2}+x^{-1}\\). The term \\(x^{1/2}\\) is concave on \\((0,\\infty)\\), \\(x^{-1}\\) is convex, and the sum is neither convex nor concave. One can find points \\(x_1,x_2\\) and \\(\\theta\\in(0,1)\\) that violate the convexity inequality.</p>"},{"location":"1g%20GP/#how-to-make-gp-convex-the-log-change-of-variables-and-log-transformation","title":"\u2705 How to make GP convex: the log-change of variables and log transformation","text":"<p>Key facts that enable convexification:</p> <ul> <li> <p>Monomials become exponentials of affine functions in log-variables.   Define \\(y_i = \\log x_i\\) (so \\(x_i = e^{y_i}\\)) and write \\(y=(y_1,\\dots,y_n)\\). For a monomial      we have      which is affine in \\(y\\).</p> </li> <li> <p>Posynomials become sums of exponentials of affine functions. For a posynomial      where \\(a_k\\) is the exponent-vector for the \\(k\\)th monomial and \\(y=\\log x\\).</p> </li> <li> <p>Taking the log of a posynomial yields a log-sum-exp function, i.e.      where \\(\\operatorname{LSE}(z_1,\\dots,z_K)=\\log\\!\\sum_{k} e^{z_k}\\).</p> </li> <li> <p>The log-sum-exp function is convex. Hence constraints of the form \\(p_i(x)\\le 1\\) become      i.e. a convex constraint in \\(y\\) because \\(\\log p_i(e^y)\\) is convex.</p> </li> <li> <p>Since \\(\\log(\\cdot)\\) is monotone, minimizing \\(p_0(x)\\) is equivalent to minimizing \\(\\log p_0(x)\\). Therefore one may transform the GP to the equivalent convex program in \\(y\\):</p> </li> </ul> <p>\\(\\(\\boxed{%   \\begin{aligned}   \\min_{y\\in\\mathbb{R}^n}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_0} c_{0k} e^{a_{0k}^T y}\\Big) \\\\   \\text{s.t.}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}\\Big) \\le 0,\\quad i=1,\\dots,m,\\\\   &amp; a_{j}^T y + \\log c_j = 0,\\quad \\text{(for each monomial equality } m_j(x)=1).   \\end{aligned}}\\)\\)</p> <p>This \\(y\\)-problem is convex: log-sum-exp objective/constraints are convex; monomial equalities are affine in \\(y\\).</p>"},{"location":"1g%20GP/#why-log-sum-exp-is-convex-brief-proof-via-hessian","title":"\ud83d\udd0d Why log-sum-exp is convex (brief proof via Hessian)","text":"<p>Let \\(g(y)=\\log\\!\\sum_{k=1}^K e^{u_k(y)}\\) with \\(u_k(y)=a_k^T y + b_k\\) (affine). Define  - Gradient:  - Hessian:  where \\(\\bar a=\\sum_k p_k a_k\\). The Hessian is a weighted covariance matrix of the vectors \\(a_k\\) (weights \\(p_k\\ge0\\)), hence PSD. Thus \\(g\\) is convex.</p>"},{"location":"1g%20GP/#monomials-as-affine-constraints-in-y","title":"\u2733\ufe0f Monomials as affine constraints in \\(y\\)","text":"<p>A monomial equality \\(c x^{a} = 1\\) becomes  an affine equality in \\(y\\). So monomial equality constraints become linear equalities after the log change.</p>"},{"location":"1g%20GP/#equivalence-and-solving-workflow","title":"\ud83d\udd01 Equivalence and solving workflow","text":"<ol> <li>Start with GP in \\(x&gt;0\\): minimize \\(p_0(x)\\) subject to posynomial constraints and monomial equalities.  </li> <li>Change variables: \\(y=\\log x\\) (domain becomes all \\(\\mathbb{R}^n\\)).  </li> <li>Apply log to posynomials (objective + inequality LHS). Because \\(\\log\\) is monotone increasing, inequalities maintain direction.  </li> <li>Solve the convex problem in \\(y\\) (log-sum-exp objective, convex constraints). Use interior-point or other convex solvers.  </li> <li>Recover \\(x^\\star = e^{y^\\star}\\).</li> </ol> <p>Because \\(x\\mapsto \\log x\\) is a bijection for \\(x&gt;0\\), solutions correspond exactly.</p>"},{"location":"1g%20GP/#worked-out-simple-example-2-variables","title":"\ud83d\udd27 Worked-out simple example (2 variables)","text":"<p>Original GP (standard form): </p> <p>Change variables: \\(y_1=\\log x_1,\\; y_2=\\log x_2\\).</p> <ul> <li>Transform terms:</li> <li>\\(3 x_1^{-1} = 3 e^{-y_1}\\) with \\(\\log\\) term \\(\\log 3 - y_1\\).</li> <li>\\(2 x_1 x_2 = 2 e^{y_1+y_2}\\) with \\(\\log\\) term \\(\\log 2 + y_1 + y_2\\).</li> <li>Constraint posynomial: \\(0.5 e^{-y_1} + e^{y_2}\\).</li> </ul> <p>Convex form (in \\(y\\)): </p> <p>Both objective and constraint are log-sum-exp functions (convex). Solve for \\(y^\\star\\) with a convex solver; then \\(x^\\star = e^{y^\\star}\\).</p>"},{"location":"1g%20GP/#numerical-implementation-remarks","title":"\u2699\ufe0f Numerical &amp; implementation remarks","text":"<ul> <li> <p>Domain requirement: GP requires \\(x_i&gt;0\\). The log transform only works on the positive orthant. If some variables can be zero, model reformulation (introducing small positive lower bounds) may be necessary.</p> </li> <li> <p>Normalization: Standard GPs use constraints \\(p_i(x)\\le 1\\). If you have \\(p_i(x) \\le t\\), divide by \\(t\\) to normalize.</p> </li> <li> <p>Numerical stability: Use the stable log-sum-exp implementation:      to avoid overflow/underflow.</p> </li> <li> <p>Solvers: After convexification the problem can be passed to generic convex solvers (CVX, CVXOPT, MOSEK, SCS, ECOS). Many solvers accept the log-sum-exp cone directly. Interior-point methods are effective on moderate-size GPs.</p> </li> <li> <p>Interpretation: The convexified problem is not an LP; it is a convex program with log-sum-exp terms (equivalently representable using exponential/relative entropy cones or by second-order cone approximations in some cases).</p> </li> </ul>"},{"location":"1g%20GP/#limitations-and-extensions","title":"\ud83d\udeab Limitations and extensions","text":"<ul> <li> <p>Signomials: If the problem contains negative coefficients (e.g. sums of monomials with arbitrary signs), it is a signomial program and the log transform does not yield a convex problem. Signomial programs are generally nonconvex and require local optimization or sequential convex approximations (e.g., successive convex approximation / condensation, branch-and-bound heuristics).</p> </li> <li> <p>Robust GP: Uncertainty in coefficients \\(c_k\\) or exponents \\(a_k\\) can sometimes be handled by robustification. If uncertainty is modeled multiplicatively (log-convex uncertainty sets), the robust counterpart often remains convex in \\(y\\); specifics depend on the uncertainty set shape (box \u2192 additional terms, ellipsoidal \u2192 conic forms). Robust GP is a rich topic \u2014 it frequently yields convex reformulations after log-transform if the uncertainty preserves log-convexity.</p> </li> <li> <p>Sparsity/exponents: Exponents \\(a_{ik}\\) need not be integers \u2014 positive real exponents are allowed. Sparsity in exponent matrix (\\(a_{ik}\\)) often leads to efficient implementations.</p> </li> </ul>"},{"location":"1g%20GP/#takeaway-summary-concise","title":"\u2705 Takeaway / Summary (concise)","text":"<ul> <li>GP in \\(x\\) is typically nonconvex because posynomials are not convex in \\(x\\).  </li> <li>Key transformation: \\(y=\\log x\\), and take \\(\\log\\) of posynomials \u2192 posynomial \\(\\mapsto\\) log-sum-exp of affine functions in \\(y\\).  </li> <li>Result: The GP becomes a convex optimization problem in \\(y\\): log-sum-exp objective/constraints (convex), and monomial equalities become linear equalities.  </li> <li>Solve in \\(y\\), then exponentiate to get \\(x\\).  </li> <li>Caveats: requires \\(x&gt;0\\); signomials (negative coefficients) remain nonconvex and need other techniques.</li> </ul> <p>If you want, I can: - provide a numerical, step-by-step example solving the worked GP above (with numbers) and show the solver output and \\(x^\\star\\); - show how robust multiplicative uncertainty in coefficients is handled in log domain; or - give a short code snippet (CVX/MOSEK or CVXPY) that constructs and solves the convexified GP.</p> <p>Which of those would you like next?</p>"},{"location":"2a%20Duality/","title":"Duality","text":""},{"location":"2a%20Duality/#duality-theory-kkt-conditions-and-duality-gap","title":"Duality Theory, KKT Conditions, and Duality Gap","text":"<p>Duality theory is a central tool in optimization and machine learning. It provides alternative perspectives on problems, certificates of optimality, and insights into algorithm design. Applications include Support Vector Machines (SVMs), Lasso, and ridge regression.</p>"},{"location":"2a%20Duality/#1-convex-optimization-problem","title":"1. Convex Optimization Problem","text":"<p>Consider the standard convex optimization problem:</p> \\[ \\begin{aligned} &amp; \\min_{x \\in \\mathbb{R}^n} &amp; f_0(x) \\\\ &amp; \\text{s.t.} &amp; f_i(x) \\le 0, \\quad i = 1, \\dots, m \\\\ &amp; &amp; h_j(x) = 0, \\quad j = 1, \\dots, p \\end{aligned} \\] <ul> <li>\\(f_0\\): objective function.  </li> <li>\\(f_i\\): convex inequality constraints.  </li> <li>\\(h_j\\): affine equality constraints.  </li> </ul> <p>Feasible set:</p> \\[ \\mathcal{D} = \\{ x \\in \\mathbb{R}^n \\mid f_i(x) \\le 0, \\ h_j(x) = 0 \\}. \\]"},{"location":"2a%20Duality/#2-lagrangian-function","title":"2. Lagrangian Function","text":"<p>The Lagrangian incorporates constraints into the objective:</p> \\[ \\mathcal{L}(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{j=1}^p \\nu_j h_j(x) \\] <ul> <li>\\(\\lambda_i \\ge 0\\): dual variables for inequalities.  </li> <li>\\(\\nu_j\\): dual variables for equalities.  </li> </ul> <p>Intuition: \\(\\lambda_i\\) represents the \u201cprice\u201d of violating constraint \\(f_i(x) \\le 0\\). Larger \\(\\lambda_i\\) penalizes violations more.</p>"},{"location":"2a%20Duality/#3-dual-function-and-infimum","title":"3. Dual Function and Infimum","text":"<p>The dual function is:</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu), \\quad \\lambda \\ge 0 \\]"},{"location":"2a%20Duality/#31-infimum","title":"3.1 Infimum","text":"<ul> <li>The infimum (inf) of a function is its greatest lower bound: the largest number that is less than or equal to all function values.  </li> <li>Formally, for \\(f(x)\\):</li> </ul> \\[ \\inf_x f(x) = \\sup \\{ y \\in \\mathbb{R} \\mid f(x) \\ge y, \\ \\forall x \\}. \\] <ul> <li>Intuition: </li> <li>If \\(f(x)\\) has a minimum, the infimum equals the minimum.  </li> <li>If no minimum exists, the infimum is the value approached but never reached.</li> </ul> <p>Examples:</p> <ol> <li>\\(f(x) = x^2\\), \\(x \\in \\mathbb{R}\\) \u2192 \\(\\inf f(x) = 0\\) at \\(x = 0\\).  </li> <li>\\(f(x) = 1/x\\), \\(x &gt; 0\\) \u2192 \\(\\inf f(x) = 0\\), never attained.  </li> </ol>"},{"location":"2a%20Duality/#32-supremum","title":"3.2 Supremum","text":"<ul> <li>The supremum (sup) of a set \\(S \\subset \\mathbb{R}\\) is the least upper bound: the smallest number greater than or equal to all elements of \\(S\\).</li> </ul> \\[ \\sup S = \\inf \\{ y \\in \\mathbb{R} \\mid y \\ge s, \\ \\forall s \\in S \\} \\] <p>Example: \\(S = \\{ x \\in \\mathbb{R} \\mid x &lt; 1 \\}\\) \u2192 \\(\\sup S = 1\\), although no maximum exists.</p>"},{"location":"2a%20Duality/#33-why-the-dual-function-provides-a-lower-bound","title":"3.3 Why the Dual Function Provides a Lower Bound","text":"<p>For any feasible \\(x \\in \\mathcal{D}\\) and \\(\\lambda \\ge 0\\), \\(\\nu\\):</p> \\[ \\lambda_i f_i(x) \\le 0, \\quad \\nu_j h_j(x) = 0 \\implies \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x) \\] <p>Taking the infimum over all \\(x\\):</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x), \\quad \\forall x \\in \\mathcal{D} \\] <p>Thus, for any dual variables:</p> \\[ g(\\lambda, \\nu) \\le p^\\star \\] <ul> <li>Interpretation: The dual function is always a lower bound on the primal optimum.  </li> <li>Geometric intuition: Think of the dual as the highest \u201cfloor\u201d under the primal objective that supports the feasible region.  </li> </ul>"},{"location":"2a%20Duality/#4-the-dual-problem","title":"4. The Dual Problem","text":"<p>The dual problem seeks the tightest lower bound:</p> \\[ \\begin{aligned} \\max_{\\lambda, \\nu} \\quad &amp; g(\\lambda, \\nu) \\\\ \\text{s.t.} \\quad &amp; \\lambda \\ge 0 \\end{aligned} \\] <ul> <li>Dual optimal value: \\(d^\\star = \\max_{\\lambda \\ge 0, \\nu} g(\\lambda, \\nu)\\).  </li> <li>Always satisfies weak duality: \\(d^\\star \\le p^\\star\\).  </li> <li>If convexity + Slater's condition hold, strong duality: \\(d^\\star = p^\\star\\).</li> </ul>"},{"location":"2a%20Duality/#5-duality-gap","title":"5. Duality Gap","text":"<p>The duality gap measures the difference between primal and dual optima:</p> \\[ \\text{Gap} = p^\\star - d^\\star \\ge 0 \\] <ul> <li>Zero gap: strong duality (common in convex ML problems).  </li> <li>Positive gap: weak duality only; dual provides only a lower bound.  </li> </ul>"},{"location":"2a%20Duality/#51-causes-of-positive-gap","title":"5.1 Causes of Positive Gap","text":"<ol> <li>Nonconvex objective.  </li> <li>Constraint qualification fails (e.g., Slater\u2019s condition not satisfied).  </li> <li>Dual problem is infeasible or unbounded.</li> </ol>"},{"location":"2a%20Duality/#52-example","title":"5.2 Example","text":"<p>Primal problem:</p> \\[ \\min_x -x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Primal optimum: \\(p^\\star = -1\\) at \\(x^\\star = 1\\) </li> <li>Dual problem: \\(d^\\star = -\\infty\\) (unbounded below)  </li> <li>Gap: \\(p^\\star - d^\\star = \\infty\\) \u2192 positive duality gap.</li> </ul> <p>Interpretation: dual gives a guaranteed lower bound but may not achieve the primal optimum.</p>"},{"location":"2a%20Duality/#6-karushkuhntucker-kkt-conditions","title":"6. Karush\u2013Kuhn\u2013Tucker (KKT) Conditions","text":"<p>For convex problems with strong duality, KKT conditions fully characterize optimality.</p> <p>Let \\(x^\\star\\) be primal optimal and \\((\\lambda^\\star, \\nu^\\star)\\) dual optimal:</p> <ol> <li> <p>Primal feasibility: </p> </li> <li> <p>Dual feasibility: </p> </li> <li> <p>Stationarity: </p> </li> <li> <p>Use subgradients for nonsmooth problems (e.g., Lasso).  </p> </li> <li> <p>Complementary slackness: </p> </li> </ol> <p>Intuition: Only active constraints contribute; the \u201cforces\u201d of objective and constraints balance.</p>"},{"location":"2a%20Duality/#7-applications-in-machine-learning","title":"7. Applications in Machine Learning","text":""},{"location":"2a%20Duality/#71-ridge-regression","title":"7.1 Ridge Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 \\] <ul> <li>Smooth shrinkage, unique solution.  </li> <li>Dual view useful in kernelized ridge regression.</li> </ul>"},{"location":"2a%20Duality/#72-lasso-regression","title":"7.2 Lasso Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1 \\] <ul> <li>KKT conditions explain sparsity:</li> </ul> \\[ X_j^\\top (y - Xw^\\star) = \\lambda s_j, \\quad s_j \\in  \\begin{cases} \\{\\text{sign}(w_j^\\star)\\}, &amp; w_j^\\star \\neq 0 \\\\ [-1,1], &amp; w_j^\\star = 0 \\end{cases} \\] <ul> <li>Basis for coordinate descent and soft-thresholding algorithms.</li> </ul>"},{"location":"2a%20Duality/#73-support-vector-machines-svms","title":"7.3 Support Vector Machines (SVMs)","text":"<ul> <li>Dual depends only on inner products \\(x_i^\\top x_j\\), enabling kernel methods.  </li> <li>Often more efficient if number of features \\(d\\) exceeds number of data points \\(n\\).</li> </ul>"},{"location":"2a%20Duality/#8-constrained-vs-penalized-optimization","title":"8. Constrained vs. Penalized Optimization","text":"<ul> <li>Constrained form:</li> </ul> \\[ \\min_w \\text{Loss}(w) \\quad \\text{s.t. } R(w) \\le t \\] <ul> <li>Penalized form:</li> </ul> \\[ \\min_w \\text{Loss}(w) + \\lambda R(w) \\] <ul> <li>Lagrange multiplier \\(\\lambda\\) acts as a \u201cprice\u201d on the constraint.  </li> <li>Equivalence holds for convex problems, but mapping \\(t \\leftrightarrow \\lambda\\) may be non-unique.</li> </ul>"},{"location":"2a%20Duality/#9-summary","title":"9. Summary","text":"<ol> <li>Infimum and supremum: greatest lower bound and least upper bound.  </li> <li>Dual function: \\(g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu)\\) always provides a lower bound.  </li> <li>Duality gap: \\(p^\\star - d^\\star\\), zero under strong duality, positive when dual does not attain primal optimum.  </li> <li>KKT conditions: necessary and sufficient for convex problems with strong duality.  </li> <li>ML connections: Ridge, Lasso, and SVM exploit duality for computation, sparsity, and kernelization.</li> </ol> <p>Key intuition: The dual function can be visualized as the highest supporting \u201cfloor\u201d under the primal objective. Maximizing it gives the tightest lower bound, and when strong duality holds, it meets the primal optimum exactly.</p>"},{"location":"3a%20Huber/","title":"Huber","text":""},{"location":"3a%20Huber/#huber-penalty-loss","title":"Huber Penalty Loss","text":"<p>The Huber loss is a robust loss function that combines the advantages of squared loss and absolute loss, making it less sensitive to outliers while remaining convex. It is defined as:</p> \\[ L_\\delta(r) =  \\begin{cases}  \\frac{1}{2} r^2 &amp; \\text{if } |r| \\le \\delta, \\\\ \\delta (|r| - \\frac{1}{2}\\delta) &amp; \\text{if } |r| &gt; \\delta, \\end{cases} \\] <p>where \\(r = y - \\hat{y}\\) is the residual, and \\(\\delta &gt; 0\\) is a threshold parameter.  </p>"},{"location":"3a%20Huber/#key-properties","title":"Key Properties","text":"<ul> <li>Quadratic for small residuals (\\(|r| \\le \\delta\\)) \u2192 behaves like least squares.  </li> <li>Linear for large residuals (\\(|r| &gt; \\delta\\)) \u2192 reduces the influence of outliers.  </li> <li>Convex, so standard convex optimization techniques apply.  </li> </ul>"},{"location":"3a%20Huber/#use","title":"Use","text":"<ul> <li>Commonly used in robust regression to estimate parameters in the presence of outliers.</li> <li>Balances efficiency (like least squares) and robustness (like absolute loss).</li> </ul>"},{"location":"3b%20Penalty%20Functions/","title":"Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#convex-optimization-notes-penalty-function-approximation","title":"Convex Optimization Notes: Penalty Function Approximation","text":""},{"location":"3b%20Penalty%20Functions/#penalty-function-approximation","title":"Penalty Function Approximation","text":"<p>We solve:</p> <p>\\(\\min \\; \\phi(r_1) + \\cdots + \\phi(r_m) \\quad \\text{subject to} \\quad r = Ax - b\\)</p> <p>where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) - \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) is a convex penalty function</p> <p>The choice of \\(\\phi\\) determines how residuals are penalized.</p>"},{"location":"3b%20Penalty%20Functions/#common-penalty-functions","title":"Common Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#1-quadratic-least-squares","title":"1. Quadratic (Least Squares)","text":"<p>\\(\\phi(u) = u^2\\)</p> <ul> <li>Strongly convex, smooth.  </li> <li>Penalizes large residuals heavily.  </li> <li>Equivalent to Gaussian noise model in statistics.  </li> <li>Leads to unique minimizer.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#2-absolute-value-least-absolute-deviations","title":"2. Absolute Value (Least Absolute Deviations)","text":"<p>\\(\\phi(u) = |u|\\)</p> <ul> <li>Convex but nonsmooth at \\(u=0\\) (subgradient methods needed).  </li> <li>Robust to outliers compared to quadratic.  </li> <li>Equivalent to Laplace noise model in statistics.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#why-does-it-lead-to-sparsity","title":"Why does it lead to sparsity?","text":"<ul> <li>The sharp corner at \\(u=0\\) makes it favorable for optimization to set many residuals (or coefficients) exactly to zero.  </li> <li>In contrast, quadratic penalties (\\(u^2\\)) only shrink values toward zero but rarely make them exactly zero.  </li> <li>Geometric intuition: the \\(\\ell_1\\) ball has corners aligned with coordinate axes \u2192 solutions land on axes \u2192 sparse.  </li> <li>Statistical interpretation: corresponds to a Laplace prior, which induces sparsity, whereas \\(\\ell_2\\) corresponds to a Gaussian prior (no sparsity).  </li> </ul> <p>\ud83d\udc49 This property is the foundation of Lasso regression and many compressed sensing methods.  </p>"},{"location":"3b%20Penalty%20Functions/#3-deadzone-linear","title":"3. Deadzone-Linear","text":"<p>\\(\\phi(u) = \\max \\{ 0, |u| - \\alpha \\}\\), where \\(\\alpha &gt; 0\\)</p> <ul> <li>Ignores small deviations (\\(|u| &lt; \\alpha\\)).  </li> <li>Linear growth outside the \u201cdeadzone.\u201d  </li> <li>Used in support vector regression (SVR) with \\(\\epsilon\\)-insensitive loss.  </li> <li>Convex, but not strictly convex \u2192 possibly multiple minimizers.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#4-log-barrier","title":"4. Log-Barrier","text":"<p>\\(\\phi(u) = \\begin{cases} -\\alpha^2 \\log \\left(1 - (u/\\alpha)^2 \\right), &amp; |u| &lt; \\alpha \\\\ \\infty, &amp; \\text{otherwise} \\end{cases}\\)</p> <ul> <li>Smooth, convex inside domain \\(|u| &lt; \\alpha\\).  </li> <li>Grows steeply as \\(|u| \\to \\alpha\\).  </li> <li>Effectively enforces constraint \\(|u| &lt; \\alpha\\).  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#histograms-of-residuals-effect-of-penalty-choice","title":"Histograms of Residuals (Effect of Penalty Choice)","text":"<p>For \\(A \\in \\mathbb{R}^{100 \\times 30}\\), the residual distribution \\(r\\) depends on \\(\\phi\\):</p> <ul> <li>Quadratic (\\(u^2\\)): residuals spread out (Gaussian-like).  </li> <li>Absolute value (\\(|u|\\)): sharper peak at 0, heavier tails (Laplace-like).  </li> <li>Deadzone: many residuals exactly at 0 (ignored region).  </li> <li>Log-barrier: residuals concentrate away from the boundary \\(|u| = 1\\).  </li> </ul> <p>\ud83d\udc49 Takeaway: Choice of \\(\\phi\\) directly shapes residual distribution.</p>"},{"location":"3b%20Penalty%20Functions/#huber-penalty-function","title":"Huber Penalty Function","text":"<p>The Huber penalty combines quadratic and linear growth:</p> <p>\\(\\phi_{\\text{huber}}(u) = \\begin{cases} u^2, &amp; |u| \\leq M \\\\ 2M|u| - M^2, &amp; |u| &gt; M \\end{cases}\\)</p>"},{"location":"3b%20Penalty%20Functions/#properties","title":"Properties","text":"<ul> <li>Quadratic near 0 (\\(|u| \\leq M\\)) \u2192 efficient for small noise.  </li> <li>Linear for large \\(|u|\\) \u2192 robust to outliers.  </li> <li>Smooth, convex.  </li> <li>Interpolates between least squares and least absolute deviations.  </li> </ul> <p>\ud83d\udc49 Called a robust penalty, widely used in robust regression.</p>"},{"location":"3b%20Penalty%20Functions/#summary-choosing-a-penalty-function","title":"Summary: Choosing a Penalty Function","text":"<ul> <li>Quadratic: efficient, but sensitive to outliers.  </li> <li>Absolute value: robust, but nonsmooth.  </li> <li>Deadzone: ignores small errors, good for sparse modeling (e.g., SVR).  </li> <li>Log-barrier: enforces domain constraints smoothly.  </li> <li>Huber: best of both worlds \u2192 quadratic for small residuals, linear for large ones.  </li> </ul>"},{"location":"3c%20Regularized/","title":"Regularized Problems","text":""},{"location":"3c%20Regularized/#regularized-approximation","title":"Regularized Approximation","text":""},{"location":"3c%20Regularized/#1-motivation-fit-vs-complexity","title":"1. Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we often want to balance two competing goals:</p> <ol> <li>Data fidelity: minimize how poorly the model fits the observed data (\\(f(x)\\)).  </li> <li>Model simplicity: discourage overly complex solutions (\\(R(x)\\)).</li> </ol> <p>This is naturally a bicriterion optimization problem:</p> <ul> <li>Criterion 1: \\(f(x)\\) = data-fitting term (e.g., least squares loss \\(\\|Ax-b\\|_2^2\\)).  </li> <li>Criterion 2: \\(R(x)\\) = regularization term (e.g., \\(\\|x\\|_1\\), \\(\\|x\\|_2^2\\), TV).  </li> </ul> <p>Since minimizing both simultaneously is usually impossible, we form the scalarized problem:</p> \\[ \\min_x \\; f(x) + \\lambda R(x), \\quad \\lambda &gt; 0 \\] <p>Here, \\(\\lambda\\) controls the trade-off: small \\(\\lambda\\) emphasizes fit, large \\(\\lambda\\) emphasizes simplicity.</p>"},{"location":"3c%20Regularized/#2-bicriterion-and-pareto-frontier","title":"2. Bicriterion and Pareto Frontier","text":"<ul> <li>Pareto optimality: a solution \\(x^\\star\\) is Pareto optimal if no other \\(x\\) improves one criterion without worsening the other.  </li> <li>Weighted sum method:  </li> <li>For convex \\(f\\) and \\(R\\), every Pareto optimal solution can be obtained from some \\(\\lambda \\ge 0\\).  </li> <li>For nonconvex problems, weighted sums may miss parts of the frontier.  </li> </ul> <p>Thus, regularization is a way of choosing a point on the Pareto frontier between fit and complexity.</p>"},{"location":"3c%20Regularized/#3-why-keep-x-small","title":"3. Why Keep \\(x\\) Small?","text":"<p>Ill-posed or noisy problems (e.g., \\(Ax \\approx b\\) with ill-conditioned \\(A\\)) often admit solutions with very large \\(\\|x\\|\\). - These large values overfit noise and are unstable. - Regularization (especially \\(\\ell_2\\)) controls the size of \\(x\\), yielding stable and robust solutions.  </p> <p>Example (Ridge regression):</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\] <p>Leads to the normal equations:</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b \\] <ul> <li>\\(A^\\top A + \\lambda I\\) is always positive definite.  </li> <li>Even if \\(A\\) is rank-deficient, the solution is unique and stable.</li> </ul>"},{"location":"3c%20Regularized/#4-lagrangian-interpretation","title":"4. Lagrangian Interpretation","text":"<p>Regularized approximation is equivalent to a constrained optimization formulation:</p> \\[ \\min_x f(x) \\quad \\text{s.t.} \\quad R(x) \\le t \\] <p>for some bound \\(t &gt; 0\\).  </p>"},{"location":"3c%20Regularized/#kkt-and-duality","title":"KKT and Duality","text":"<ul> <li>The Lagrangian is:</li> </ul> \\[ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda(R(x)-t) \\] <ul> <li>Under convexity and Slater\u2019s condition, strong duality holds.  </li> <li>KKT conditions:</li> </ul> \\[ 0 \\in \\partial f(x^\\star) + \\lambda^\\star \\partial R(x^\\star), \\quad  \\lambda^\\star \\ge 0, \\quad R(x^\\star) \\le t, \\quad  \\lambda^\\star (R(x^\\star)-t) = 0 \\] <ul> <li>The penalized form:</li> </ul> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>has the same optimality condition:</p> \\[ 0 \\in \\partial f(x^\\star) + \\lambda \\partial R(x^\\star) \\] <p>Hence, solving the penalized problem corresponds to solving the constrained one for some \\(t\\), though the \\(\\lambda \\leftrightarrow t\\) mapping is monotone but not one-to-one.</p>"},{"location":"3c%20Regularized/#5-common-regularizers","title":"5. Common Regularizers","text":""},{"location":"3c%20Regularized/#l2-ridge","title":"L2 (Ridge)","text":"<p>\\(R(x) = \\|x\\|_2^2\\) - Strongly convex \u2192 unique solution. - Encourages small coefficients, smooth solutions. - Bayesian view: Gaussian prior on \\(x\\). - Improves conditioning of \\(A^\\top A\\).  </p>"},{"location":"3c%20Regularized/#l1-lasso","title":"L1 (Lasso)","text":"<p>\\(R(x) = \\|x\\|_1\\) - Convex, but not strongly convex \u2192 solutions may be non-unique. - Promotes sparsity: many coefficients exactly zero. - Geometric view: \\(\\ell_1\\) ball has corners aligned with coordinate axes; intersections often occur at corners \u2192 sparse solutions. - Bayesian view: Laplace prior on \\(x\\). - Proximal operator: soft-thresholding </p>"},{"location":"3c%20Regularized/#elastic-net","title":"Elastic Net","text":"<p>\\(R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2\\) - Combines L1 sparsity with L2 stability. - Ensures uniqueness even when features are correlated.  </p>"},{"location":"3c%20Regularized/#beyond-l1l2","title":"Beyond L1/L2","text":"<ul> <li>General Tikhonov: \\(R(x) = \\|Lx\\|_2^2\\), where \\(L\\) encodes smoothness (e.g., derivative operator).  </li> <li>Total Variation (TV): \\(R(x) = \\|\\nabla x\\|_1\\), promotes piecewise-constant signals.  </li> <li>Group Lasso: \\(R(x) = \\sum_g \\|x_g\\|_2\\), induces structured sparsity.  </li> <li>Nuclear Norm: \\(R(X) = \\|X\\|_\\ast\\) (sum of singular values), promotes low-rank matrices.  </li> </ul>"},{"location":"3c%20Regularized/#6-choosing-the-regularization-parameter-lambda","title":"6. Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"3c%20Regularized/#trade-off","title":"Trade-off","text":"<ul> <li>Too small \\(\\lambda\\): weak regularization \u2192 overfitting, unstable solutions.  </li> <li>Too large \\(\\lambda\\): strong regularization \u2192 underfitting, biased solutions.  </li> </ul> <p>\\(\\lambda\\) determines where on the Pareto frontier the solution lies.</p>"},{"location":"3c%20Regularized/#practical-selection","title":"Practical Selection","text":"<ul> <li>Cross-validation (CV): </li> <li>Split data into \\(k\\) folds.  </li> <li>Train on \\(k-1\\) folds, validate on the held-out fold.  </li> <li>Average validation error across folds.  </li> <li> <p>Choose \\(\\lambda\\) minimizing average error.  </p> </li> <li> <p>Best practices: </p> </li> <li>Standardize features before using L1/Elastic Net.  </li> <li>For time series, use blocked or rolling CV (avoid leakage).  </li> <li>Use nested CV for model comparison.  </li> <li>One-standard-error rule: prefer larger \\(\\lambda\\) within one SE of min error \u2192 simpler model.  </li> </ul>"},{"location":"3c%20Regularized/#alternatives","title":"Alternatives","text":"<ul> <li>Analytical rules (ridge regression has closed-form shrinkage).  </li> <li>Information criteria (AIC/BIC; heuristic for Lasso).  </li> <li>Regularization path (trace solutions as \\(\\lambda\\) varies, pick best by validation error).  </li> <li>Inverse problems: discrepancy principle, L-curve, generalized CV.  </li> </ul>"},{"location":"3c%20Regularized/#7-algorithmic-perspective","title":"7. Algorithmic Perspective","text":"<p>Regularized problems often have the form:</p> \\[ \\min_x f(x) + R(x) \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex but possibly nonsmooth.</p> <ul> <li>Proximal Gradient (ISTA, FISTA):   Iterative updates using gradient of \\(f\\) and prox of \\(R\\).  </li> <li>Coordinate Descent: very effective for Lasso/Elastic Net.  </li> <li>ADMM: handles separable structures and constraints well.  </li> </ul> <p>Proximal operators are key: - L2: shrinkage (scaling). - L1: soft-thresholding. - TV/nuclear norm: more advanced proximal maps.  </p>"},{"location":"3c%20Regularized/#8-bayesian-interpretation","title":"8. Bayesian Interpretation","text":"<ul> <li>Regularization corresponds to MAP estimation.  </li> <li>Example: Gaussian noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)\\) and Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\) yields:</li> </ul> \\[ \\min_x \\frac{1}{2\\sigma^2}\\|Ax-b\\|_2^2 + \\frac{1}{2\\tau^2}\\|x\\|_2^2 \\] <p>So \\(\\lambda = \\frac{\\sigma^2}{2\\tau^2}\\) (up to scaling). - L1 corresponds to a Laplace prior, inducing sparsity.</p>"},{"location":"3c%20Regularized/#9-key-takeaways","title":"9. Key Takeaways","text":"<ul> <li>Regularized approximation = bicriterion optimization (fit vs. complexity).  </li> <li>Penalized and constrained forms are connected via duality and KKT.  </li> <li>Regularization stabilizes ill-posed problems and improves generalization.  </li> <li>Choice of regularizer shapes the solution (small \\(\\ell_2\\), sparse \\(\\ell_1\\), structured TV/group/nuclear).  </li> <li>\\(\\lambda\\) is critical \u2014 usually chosen by cross-validation or problem-specific heuristics.  </li> <li>Proximal algorithms make regularized optimization scalable.  </li> <li>Bayesian view ties \\(\\lambda\\) to prior assumptions and noise models.</li> </ul>"},{"location":"3d%20Robust%20Approximation/","title":"Robust Approximation","text":""},{"location":"3d%20Robust%20Approximation/#robust-regression-stochastic-vs-worst-case-formulations","title":"Robust Regression: Stochastic vs. Worst-Case Formulations","text":""},{"location":"3d%20Robust%20Approximation/#setup","title":"Setup","text":"<p>We study linear regression with uncertain design matrix:</p> \\[ y = A x + \\varepsilon, \\quad A = \\bar{A} + U, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the decision variable,  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the observed response,  </li> <li>\\(\\bar{A}\\) is the nominal design matrix,  </li> <li>\\(U\\) is an uncertainty term.  </li> </ul> <p>The treatment of \\(U\\) gives rise to two main formulations: stochastic (probabilistic uncertainty) and worst-case (deterministic uncertainty).</p>"},{"location":"3d%20Robust%20Approximation/#1-stochastic-formulation","title":"1. Stochastic Formulation","text":"<p>Assume \\(U\\) is random with  </p> <ul> <li>\\(\\mathbb{E}[U] = 0\\),  </li> <li>\\(\\mathbb{E}[U^\\top U] = P \\succeq 0\\),  </li> <li>finite second moment,  </li> <li>independent of \\(y\\).  </li> </ul> <p>We minimize the expected squared residual:</p> \\[ \\min_x \\; \\mathbb{E}\\!\\left[\\|(\\bar{A} + U)x - y\\|_2^2\\right]. \\]"},{"location":"3d%20Robust%20Approximation/#expansion","title":"Expansion","text":"\\[ \\|(\\bar{A}+U)x - y\\|_2^2 = \\|\\bar{A}x - y\\|_2^2 + 2(\\bar{A}x - y)^\\top Ux + \\|Ux\\|_2^2. \\] <ul> <li>Cross-term vanishes since \\(\\mathbb{E}[U]=0\\) and \\(U\\) is independent of \\(y\\):  </li> </ul> \\[ \\mathbb{E}[(\\bar{A}x - y)^\\top Ux] = 0. \\] <ul> <li>Variance term simplifies:  </li> </ul> \\[ \\mathbb{E}[\\|Ux\\|_2^2] = x^\\top P x. \\]"},{"location":"3d%20Robust%20Approximation/#resulting-problem","title":"Resulting Problem","text":"\\[ \\min_x \\; \\|\\bar{A}x - y\\|_2^2 + x^\\top P x. \\] <ul> <li>If \\(P = \\rho I\\): ridge regression (L2 regularization).  </li> <li>If \\(P \\succeq 0\\) general: generalized Tikhonov regularization, with anisotropic penalty \\(\\|P^{1/2}x\\|_2^2\\).  </li> </ul>"},{"location":"3d%20Robust%20Approximation/#convexity","title":"Convexity","text":"<p>The Hessian is  </p> \\[ \\nabla^2 f(x) = 2(\\bar{A}^\\top \\bar{A} + P) \\succeq 0. \\] <p>Thus the problem is convex. If \\(P \\succ 0\\), it is strongly convex and the minimizer is unique.  </p>"},{"location":"3d%20Robust%20Approximation/#2-worst-case-formulation","title":"2. Worst-Case Formulation","text":"<p>Suppose \\(U\\) is unknown but bounded:</p> \\[ \\|U\\|_2 \\leq \\rho, \\] <p>where \\(\\|\\cdot\\|_2\\) is the spectral norm (largest singular value). We minimize the worst-case squared residual:</p> \\[ \\min_x \\; \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A} + U)x - y\\|_2^2. \\]"},{"location":"3d%20Robust%20Approximation/#expansion-via-spectral-norm-bound","title":"Expansion via Spectral Norm Bound","text":"<p>For spectral norm uncertainty:</p> \\[ \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A}+U)x - y\\|_2 = \\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2. \\] <p>This identity uses the fact that \\(Ux\\) can align with the residual direction when \\(\\|U\\|_2 \\leq \\rho\\). Note: If a different norm bound is used (Frobenius, \\(\\ell_\\infty\\), etc.), the expression changes.</p>"},{"location":"3d%20Robust%20Approximation/#resulting-problem_1","title":"Resulting Problem","text":"\\[ \\min_x \\; \\left(\\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2\\right)^2. \\] <p>This is convex but not quadratic. Unlike ridge regression, the regularization is coupled inside the residual norm, making the solution more conservative.</p>"},{"location":"3d%20Robust%20Approximation/#3-comparison","title":"3. Comparison","text":"Aspect Stochastic Formulation Worst-Case Formulation Model of \\(U\\) Random, mean zero, finite variance Deterministic, bounded \\(\\|U\\|_2 \\leq \\rho\\) Objective \\(\\|\\bar{A}x - y\\|_2^2 + x^\\top P x\\) \\((\\|\\bar{A}x - y\\|_2 + \\rho\\|x\\|_2)^2\\) Regularization Quadratic penalty (ellipsoidal shrinkage) Norm inflation coupled with residual Geometry Ellipsoidal shrinkage of \\(x\\) (Mahalanobis norm) Inflated residual tube, more conservative Convexity Convex quadratic; strongly convex if \\(P \\succ 0\\) Convex but non-quadratic"},{"location":"3d%20Robust%20Approximation/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Stochastic robust regression \u2192 ridge/Tikhonov regression (quadratic L2 penalty).  </li> <li>Worst-case robust regression \u2192 inflated residual norm with L2 penalty inside the loss, more conservative than ridge.  </li> <li>Both are convex, but their geometry differs:  </li> <li>Stochastic: smooth ellipsoidal shrinkage of coefficients.  </li> <li>Worst-case: enlarged residual \u201ctube\u201d that hedges against adversarial perturbations.  </li> </ul>"},{"location":"3e%20MLE/","title":"MLE","text":""},{"location":"3e%20MLE/#statistical-estimation-and-maximum-likelihood","title":"Statistical Estimation and Maximum Likelihood","text":""},{"location":"3e%20MLE/#1-maximum-likelihood-estimation-mle","title":"1. Maximum Likelihood Estimation (MLE)","text":"<p>Suppose we have a family of probability densities</p> \\[ p_x(y), \\quad x \\in \\mathcal{X}, \\] <p>where \\(x\\) (often written as \\(\\theta\\) in statistics) is the parameter to be estimated.  </p> <ul> <li>\\(p_x(y) = 0\\) for invalid parameter values \\(x\\).  </li> <li>The function \\(p_x(y)\\), viewed as a function of \\(x\\) with \\(y\\) fixed, is called the likelihood function.  </li> <li>The log-likelihood is defined as  </li> </ul> \\[ \\ell(x) = \\log p_x(y). \\] <ul> <li>The maximum likelihood estimate (MLE) is  </li> </ul> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathcal{X}} \\; p_x(y)  = \\arg\\max_{x \\in \\mathcal{X}} \\; \\ell(x). \\]"},{"location":"3e%20MLE/#convexity-perspective","title":"Convexity Perspective","text":"<ul> <li>If \\(\\ell(x)\\) is concave in \\(x\\) for each fixed \\(y\\), then the MLE problem is a convex optimization problem.  </li> <li>Important distinction: this requires concavity in \\(x\\), not in \\(y\\).  </li> <li>Example: \\(p_x(y)\\) may be a log-concave density in \\(y\\) (common in statistics),  </li> <li>but this does not imply that \\(\\ell(x)\\) is concave in \\(x\\).  </li> </ul> <p>Thus, convexity of the MLE depends on the parameterization of the distribution family.  </p>"},{"location":"3e%20MLE/#2-linear-measurements-with-iid-noise","title":"2. Linear Measurements with IID Noise","text":"<p>Consider the linear measurement model:</p> \\[ y_i = a_i^\\top x + v_i, \\quad i = 1, \\ldots, m, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the unknown parameter vector,  </li> <li>\\(a_i \\in \\mathbb{R}^n\\) are known measurement vectors,  </li> <li>\\(v_i\\) are i.i.d. noise variables with density \\(p(z)\\),  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the vector of observed measurements.  </li> </ul>"},{"location":"3e%20MLE/#likelihood-function","title":"Likelihood Function","text":"<p>Since the noise terms are independent:</p> \\[ p_x(y) = \\prod_{i=1}^m p\\!\\left(y_i - a_i^\\top x\\right). \\] <p>Taking logs:</p> \\[ \\ell(x) = \\log p_x(y)  = \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#mle-problem","title":"MLE Problem","text":"<p>The MLE is any solution to:</p> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathbb{R}^n} \\; \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#convexity-note","title":"Convexity Note","text":"<ul> <li>If \\(p(z)\\) is log-concave in \\(z\\), then \\(\\log p(y_i - a_i^\\top x)\\) is concave in \\(x\\).  </li> <li>Therefore, under log-concave noise distributions (e.g. Gaussian, Laplace, logistic), the MLE problem is a concave maximization problem, hence equivalent to a convex optimization problem after sign change:</li> </ul> \\[ \\min_x \\; -\\ell(x). \\]"},{"location":"4a%20Linear%20Discrimination/","title":"Linear Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#1-linear-discrimination-lp-feasibility","title":"1. Linear Discrimination (LP Feasibility)","text":""},{"location":"4a%20Linear%20Discrimination/#problem-setup","title":"Problem Setup","text":"<ul> <li>Variables: \\((a,b) \\in \\mathbb{R}^{n+1}\\)</li> <li>Constraints:</li> <li>\\(a^T x_i - b \\geq 1\\)</li> <li>\\(a^T y_j - b \\leq -1\\)</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity","title":"Convexity","text":"<ul> <li>Each constraint is affine in \\((a,b)\\).</li> <li>Affine inequalities define convex half-spaces.</li> <li>Intersection of half-spaces = convex polyhedron.</li> <li>No objective \u2192 pure LP feasibility.</li> </ul> <p>Type: Convex LP feasibility problem.</p>"},{"location":"4a%20Linear%20Discrimination/#2-robust-linear-discrimination-hard-margin-svm","title":"2. Robust Linear Discrimination (Hard-Margin SVM)","text":""},{"location":"4a%20Linear%20Discrimination/#problem","title":"Problem","text":"\\[ \\min \\tfrac{1}{2}\\|a\\|_2^2 \\quad  \\text{s.t. } a^T x_i - b \\geq 1, \\; a^T y_j - b \\leq -1. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_1","title":"Convexity","text":"<ul> <li>Objective: \\(\\tfrac{1}{2}\\|a\\|_2^2\\) is convex quadratic (strictly convex in \\(a\\)).</li> <li>Constraints: Affine \\(\\Rightarrow\\) convex feasible set.</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#3-soft-margin-svm","title":"3. Soft-Margin SVM","text":""},{"location":"4a%20Linear%20Discrimination/#problem_1","title":"Problem","text":"\\[ \\min_{a,b,\\xi} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\xi_i $$ subject to $$ y_i(a^T z_i - b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_2","title":"Convexity","text":"<ul> <li>Objective: Sum of convex quadratic (\\(\\|a\\|^2\\)) and linear (\\(\\sum_i \\xi_i\\)).</li> <li>Constraints: Affine in \\((a,b,\\xi)\\).</li> <li>Feasible set = intersection of half-spaces (convex).</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#4-hinge-loss-formulation","title":"4. Hinge Loss Formulation","text":""},{"location":"4a%20Linear%20Discrimination/#problem_2","title":"Problem","text":"\\[ \\min_{a,b} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\max(0, 1 - y_i(a^T z_i - b)). \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_3","title":"Convexity","text":"<ul> <li>\\(\\tfrac{1}{2}\\|a\\|_2^2\\): convex quadratic.</li> <li>Inside hinge: \\(1 - y_i(a^T z_i - b)\\) is affine.</li> <li>\\(\\max(0, \\text{affine})\\) = convex function.</li> <li>Sum of convex functions = convex.</li> </ul> <p>Type: Unconstrained convex optimization problem.</p>"},{"location":"4a%20Linear%20Discrimination/#5-dual-svm-problem","title":"5. Dual SVM Problem","text":""},{"location":"4a%20Linear%20Discrimination/#problem_3","title":"Problem","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j z_i^T z_j $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_4","title":"Convexity","text":"<ul> <li>Quadratic form has negative semidefinite Hessian (concave).</li> <li>Maximization of concave function over convex set \u2192 convex optimization.</li> </ul> <p>Type: Convex quadratic program in dual variables.</p>"},{"location":"4a%20Linear%20Discrimination/#6-nonlinear-discrimination-with-kernels","title":"6. Nonlinear Discrimination with Kernels","text":""},{"location":"4a%20Linear%20Discrimination/#problem-dual-with-kernel","title":"Problem (Dual with Kernel)","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(z_i,z_j) $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_5","title":"Convexity","text":"<ul> <li>If \\(K\\) is positive semidefinite (Mercer kernel), quadratic form is convex.</li> <li>Maximization remains convex program.</li> </ul> <p>Type: Convex QP with kernel matrix.</p>"},{"location":"4a%20Linear%20Discrimination/#7-quadratic-discrimination","title":"7. Quadratic Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#problem_4","title":"Problem","text":"<p>Classifier: \\(f(z) = z^T P z + q^T z + r\\) with variables \\((P,q,r)\\).</p> <p>Constraints: - \\(x_i^T P x_i + q^T x_i + r \\geq 1\\) - \\(y_j^T P y_j + q^T y_j + r \\leq -1\\)</p>"},{"location":"4a%20Linear%20Discrimination/#convexity_6","title":"Convexity","text":"<ul> <li>\\(x_i^T P x_i = \\mathrm{Tr}(P x_i x_i^T)\\), affine in \\(P\\).</li> <li>Constraints affine in \\((P,q,r)\\).</li> <li>If additional constraint \\(P \\succeq 0\\), this is convex (semidefinite cone).</li> </ul> <p>Type: LP feasibility or SDP (semidefinite program).</p>"},{"location":"4a%20Linear%20Discrimination/#8-polynomial-feature-maps","title":"8. Polynomial Feature Maps","text":""},{"location":"4a%20Linear%20Discrimination/#setup","title":"Setup","text":"<ul> <li>Map \\(z \\mapsto F(z)\\) with monomials up to degree \\(d\\).</li> <li>Classifier: \\(f(z) = \\theta^T F(z)\\).</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity_7","title":"Convexity","text":"<ul> <li>Constraints: \\(\\theta^T F(x_i) \\geq 1\\), affine in \\(\\theta\\).</li> <li>Margin maximization objective: \\(\\|\\theta\\|^2\\), convex quadratic.</li> </ul> <p>Type: LP feasibility or convex QP.</p>"},{"location":"4a%20Linear%20Discrimination/#9-summary-of-convex-structures","title":"9. Summary of Convex Structures","text":"<ul> <li>LP feasibility: Linear separation.  </li> <li>QP: Hard-margin and soft-margin SVM.  </li> <li>Unconstrained convex problem: Hinge loss.  </li> <li>Dual SVM: Convex QP in dual variables.  </li> <li>Kernel SVM: Convex QP with PSD kernel.  </li> <li>Quadratic/Polynomial discrimination: LP or SDP, depending on constraints.  </li> </ul>"},{"location":"6a%20First%20Order%20Optimization/","title":"First Order Optimization","text":""},{"location":"6a%20First%20Order%20Optimization/#first-order-optimization-methods","title":"First-Order Optimization Methods","text":"<p>In machine learning, especially at large scale, we often cannot afford to solve convex problems using heavy, exact solvers (like simplex or interior-point methods). Instead, we rely on first-order methods \u2014 algorithms that use only function values and gradients to iteratively approach the solution.</p>"},{"location":"6a%20First%20Order%20Optimization/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<p>Gradient descent is the most fundamental algorithm for minimizing differentiable convex functions.</p> <p>Basic Idea: At each iteration, move in the direction opposite to the gradient (the steepest descent direction), because it points toward lower values of the function.</p> <p>Update Rule:  where: - \\(\\nabla f(x^{(k)})\\) \u2014 gradient at the current point. - \\(\\alpha_k\\) \u2014 step size (learning rate).</p> <p>Convergence (Convex Case): - If \\(f\\) is convex and has Lipschitz-continuous gradients, gradient descent converges at rate \\(O(1/k)\\) for constant step size. - If \\(f\\) is also strongly convex, the rate improves to linear convergence.</p> <p>Step Size Selection: - Constant: simple but requires tuning. - Diminishing: \\(\\alpha_k \\to 0\\) ensures convergence but may be slow. - Backtracking Line Search: adaptively chooses \\(\\alpha_k\\) for efficiency.</p>"},{"location":"6a%20First%20Order%20Optimization/#subgradient-methods","title":"Subgradient Methods","text":"<p>Many important convex functions in ML are non-differentiable (e.g., \\(\\|x\\|_1\\)). The subgradient generalizes the gradient for such functions.</p> <p>Subgradient Definition: A vector \\(g\\) is a subgradient of \\(f\\) at \\(x\\) if:  </p> <p>Update Rule: </p> <p>Key Trade-Off: Subgradient methods are robust but converge more slowly (\\(O(1/\\sqrt{k})\\) in general).</p>"},{"location":"6a%20First%20Order%20Optimization/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>SGD is the workhorse of large-scale machine learning.</p> <p>When to Use: When the objective is a sum over many data points:  </p> <p>Update Rule: - Pick a random index \\(i_k\\) - Use \\(\\nabla f_{i_k}(x^{(k)})\\) as an unbiased estimate of the full gradient:  </p> <p>Advantages: - Much faster per iteration for large \\(N\\). - Enables online learning.</p> <p>Disadvantages: - Introduces variance; iterates \u201cbounce\u201d around the optimum. - Requires careful learning rate schedules.</p>"},{"location":"6a%20First%20Order%20Optimization/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<p>Nesterov\u2019s Accelerated Gradient (NAG) achieves the optimal convergence rate for smooth convex functions: \\(O(1/k^2)\\).</p> <p>Key Idea: Introduce a momentum term that anticipates the next position, correcting the gradient direction.</p> <p>Update: </p> <p>When Useful: - Smooth convex problems where plain gradient descent is too slow. - Large-scale ML tasks with batch updates.</p>"},{"location":"6a%20First%20Order%20Optimization/#why-first-order-methods-matter-for-ml","title":"Why First-Order Methods Matter for ML","text":"<ul> <li>Handle huge datasets efficiently.</li> <li>Require only gradient information, which is cheap for many models.</li> <li>Naturally fit into streaming and online learning setups.</li> <li>Form the backbone of deep learning optimizers (SGD, Adam, RMSProp \u2014 though deep nets are non-convex).</li> </ul>"},{"location":"7a%20pareto%20optimal/","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#classical-optimality","title":"Classical Optimality","text":"<p>In standard convex optimisation, we consider a single objective function \\(f(x)\\) and aim to find a globally optimal solution:  where \\(\\mathcal{X}\\) is the feasible set.  </p> <p>Here, optimality is absolute: there exists a single best point (or set of equivalent best points) with respect to one measure of performance.</p>"},{"location":"7a%20pareto%20optimal/#multi-objective-optimisation","title":"Multi-objective Optimisation","text":"<p>Many practical problems in machine learning and optimisation involve multiple competing objectives. For instance:</p> <ul> <li>In supervised learning, one wishes to minimise prediction error while also controlling model complexity.  </li> <li>In fairness-aware learning, we want high accuracy while limiting demographic disparity.  </li> <li>In finance, an investor balances expected return against risk.  </li> </ul> <p>Formally, a multi-objective optimisation problem is written as:  where \\(f_1, f_2, \\dots, f_k\\) are the competing objectives.</p>"},{"location":"7a%20pareto%20optimal/#pareto-optimality_1","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A solution \\(x^* \\in \\mathcal{X}\\) is Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  with strict inequality for at least one objective \\(j\\).  </p> <p>Intuitively, no feasible point strictly improves one objective without worsening another.</p>"},{"location":"7a%20pareto%20optimal/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A solution \\(x^*\\) is weakly Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  </p> <p>In other words, no solution improves every objective simultaneously.</p>"},{"location":"7a%20pareto%20optimal/#geometric-intuition","title":"Geometric Intuition","text":"<p>If we plot feasible solutions in the objective space \\((f_1(x), f_2(x))\\), the Pareto frontier is the lower-left boundary for minimisation problems. - Points on the frontier are non-dominated (Pareto optimal). - Points inside the feasible region but above the boundary are dominated.  </p>"},{"location":"7a%20pareto%20optimal/#scalarisation","title":"Scalarisation","text":"<p>Since multi-objective optimisation problems usually admit a set of Pareto optimal solutions rather than a single best point, practitioners use scalarisation. This reduces multiple objectives to a single scalar objective that can be optimised with standard methods.</p>"},{"location":"7a%20pareto%20optimal/#weighted-sum-scalarisation","title":"Weighted Sum Scalarisation","text":"<p>The most common approach is the weighted sum:  </p> <ul> <li>Each choice of weights \\(w\\) corresponds to a different point on the Pareto frontier.  </li> <li>Larger \\(w_i\\) prioritises objective \\(f_i\\) relative to others.  </li> </ul> <p>Convexity caveat: If the feasible set and objectives are convex, weighted sum scalarisation can recover the convex part of the Pareto frontier. Non-convex regions of the frontier may not be attainable using weighted sums alone.</p>"},{"location":"7a%20pareto%20optimal/#varepsilon-constraint-method","title":"\\(\\varepsilon\\)-Constraint Method","text":"<p>Another approach is to optimise one objective while converting others into constraints:  Here \\(\\varepsilon_i\\) are tolerance levels. By adjusting them, we can explore different trade-offs.  </p> <p>This connects directly to regularisation in machine learning: - In ridge regression, we minimise data fit subject to a complexity budget \\(\\|x\\|_2^2 \\leq \\tau\\). - The equivalent penalised form \\(\\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2\\) is obtained via Lagrangian duality, where \\(\\lambda\\) is the multiplier associated with \\(\\tau\\).  </p>"},{"location":"7a%20pareto%20optimal/#duality-and-scalarisation","title":"Duality and Scalarisation","text":"<p>Scalarisation is deeply connected to duality in convex optimisation: - The weights \\(w_i\\) or multipliers \\(\\lambda\\) can be interpreted as Lagrange multipliers balancing objectives. - Adjusting these parameters changes the point on the Pareto frontier that is selected. - This explains why hyperparameters like \\(\\lambda\\) in regularisation are so influential: they represent trade-offs in a hidden multi-objective problem.</p>"},{"location":"7a%20pareto%20optimal/#example-1-regularised-least-squares","title":"Example 1: Regularised Least Squares","text":"<p>Consider the regression problem with data matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) and target \\(b \\in \\mathbb{R}^m\\).  </p> <p>We want to minimise: 1. Prediction error: \\(f_1(x) = \\|Ax - b\\|_2^2\\) 2. Model complexity: \\(f_2(x) = \\|x\\|_2^2\\) </p> <p>This is a two-objective optimisation problem.  </p> <ul> <li> <p>Using the weighted sum:  where \\(\\lambda \\geq 0\\) determines the trade-off.  </p> </li> <li> <p>Alternatively, using the \\(\\varepsilon\\)-constraint:  </p> </li> </ul> <p>Both formulations yield Pareto optimal solutions, with \\(\\lambda\\) and \\(\\tau\\) providing different parametrisations of the frontier.</p>"},{"location":"7a%20pareto%20optimal/#example-2-portfolio-optimisation-riskreturn","title":"Example 2: Portfolio Optimisation (Risk\u2013Return)","text":"<p>In finance, suppose an investor chooses portfolio weights \\(w \\in \\mathbb{R}^n\\).</p> <ul> <li>Expected return: \\(f_1(w) = -\\mu^\\top w\\) (we minimise negative return).  </li> <li>Risk: \\(f_2(w) = w^\\top \\Sigma w\\) (variance of returns, a convex function).  </li> </ul> <p>The problem is:  </p> <ul> <li>Using weighted sum scalarisation:  </li> <li>Different \\(\\alpha\\) values give different points on the efficient frontier.  </li> </ul> <p>This convex formulation underpins modern portfolio theory.</p>"},{"location":"7a%20pareto%20optimal/#example-3-probabilistic-modelling-elbo","title":"Example 3: Probabilistic Modelling (ELBO)","text":"<p>In variational inference, the Evidence Lower Bound (ELBO) is:  </p> <p>This can be seen as a scalarisation of two competing objectives: 1. Data fit (reconstruction term). 2. Simplicity or prior adherence (KL divergence).  </p> <p>By weighting the KL divergence with a parameter \\(\\beta\\), we obtain the \\(\\beta\\)-VAE:  </p> <p>Here, \\(\\beta\\) plays the role of a scalarisation weight, selecting different Pareto optimal trade-offs between reconstruction accuracy and disentanglement.</p>"},{"location":"7a%20pareto%20optimal/#broader-connections-in-ai-and-ml","title":"Broader Connections in AI and ML","text":"<ul> <li>Fairness vs accuracy: Balancing accuracy with fairness metrics is a multi-objective problem often approached via scalarisation.  </li> <li>Generalisation vs training error: Regularisation is a scalarisation of fit versus complexity.  </li> <li>Compression vs performance: The information bottleneck principle is a Pareto trade-off between accuracy and representation complexity.  </li> <li>Inference vs divergence: Variational inference (ELBO) is naturally a scalarised multi-objective problem.  </li> </ul>"},{"location":"7a%20pareto%20optimal/#summary","title":"Summary","text":"<ul> <li>Classical optimisation yields a single best solution.  </li> <li>Multi-objective optimisation gives a set of non-dominated (Pareto optimal) solutions.  </li> <li>Scalarisation provides practical methods to compute Pareto optimal solutions.  </li> <li>Weighted sums recover convex parts of the frontier, while \\(\\varepsilon\\)-constraints provide flexibility.  </li> <li>Scalarisation connects directly to duality, where weights act as Lagrange multipliers.  </li> <li>Examples in ML (ridge regression, ELBO) and finance (portfolio optimisation) demonstrate its wide relevance.  </li> </ul> <p>Scalarisation is not only a mathematical device but the foundation for understanding regularisation, fairness, generalisation, and many practical trade-offs in machine learning.</p>"},{"location":"Example/","title":"Example","text":""},{"location":"Example/#galactic-cargo-delivery-optimization-lp-formulation","title":"Galactic Cargo Delivery Optimization \u2014 LP Formulation","text":"<p>You are the logistics commander of an interstellar fleet tasked with delivering vital supplies across the galaxy. Your fleet consists of \\(N\\) starship pilots, and you have \\(K\\) distinct types of cargo crates to deliver. Each cargo type \\(j\\) has a known volume \\(v_j\\), representing the number of crates that must reach their destinations.</p> <p>To maintain fleet balance and operational efficiency, each pilot must carry the same total number of crates. Your mission is to assign crates to pilots to minimize the total expected delivery time, accounting for each pilot\u2019s unique speed and proficiency with different cargo types.</p>"},{"location":"Example/#notation","title":"Notation","text":"<ul> <li>\\(i = 1, \\ldots, N\\): indices for starship pilots  </li> <li>\\(j = 1, \\ldots, K\\): indices for cargo types  </li> <li>\\(v_j\\): volume (number of crates) of cargo type \\(j\\) </li> <li>\\(d_{ij}\\): estimated delivery time for pilot \\(i\\) to deliver one crate of type \\(j\\)</li> </ul>"},{"location":"Example/#decision-variables","title":"Decision Variables","text":"<p>\\(x_{ij} \\geq 0\\)</p> <p>Number of crates of cargo type \\(j\\) assigned to pilot \\(i\\).</p>"},{"location":"Example/#objective-function","title":"Objective Function","text":"<p>Minimize the total delivery time across all pilots:</p> \\[\\min \\sum_{i=1}^N \\sum_{j=1}^K d_{ij} x_{ij}\\] <p>Or equivalently, in vector form:</p> \\[\\min c^T x\\] <p>where</p> \\[ c = \\begin{bmatrix} d_{11}, d_{12}, \\ldots, d_{1K}, d_{21}, \\ldots, d_{NK} \\end{bmatrix}^T, \\quad x = \\begin{bmatrix} x_{11}, x_{12}, \\ldots, x_{1K}, x_{21}, \\ldots, x_{NK} \\end{bmatrix}^T \\]"},{"location":"Example/#constraints","title":"Constraints","text":"<ol> <li>All crates must be delivered:</li> </ol> \\[\\sum_{i=1}^N x_{ij} = v_j, \\quad \\forall j = 1, \\ldots, K\\] <ol> <li>Each pilot carries the same total number of crates:</li> </ol> \\[\\sum_{j=1}^K x_{ij} = \\frac{V}{N}, \\quad \\forall i = 1, \\ldots, N \\quad \\text{where} \\quad V = \\sum_{j=1}^K v_j\\] <ol> <li>Non-negativity: \\(\\(x_{ij} \\geq 0, \\quad \\forall i,j\\)\\)</li> </ol>"},{"location":"Example/#lp-formulation","title":"LP Formulation","text":"\\[ \\begin{aligned} \\min_{x \\in \\mathbb{R}^{N \\times K}} \\quad &amp; c^T x \\\\ \\text{subject to} \\quad &amp; \\begin{cases} A_{eq} x = b_{eq} \\\\ x \\geq 0 \\end{cases} \\end{aligned}\\] <p>Where:</p> <ul> <li>\\(A_{eq} \\in \\mathbb{R}^{(N + K) \\times (N \\cdot K)}\\) encodes the equality constraints for cargo delivery and load balancing.</li> <li>\\(b_{eq} \\in \\mathbb{R}^{N + K}\\) combines the crate volumes \\(v_j\\) and equal load targets \\(\\frac{V}{N}\\).</li> </ul>"}]}