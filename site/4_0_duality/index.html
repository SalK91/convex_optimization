<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/4_0_duality/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>4 0 duality - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              4 0 duality
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1">
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_0_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_1_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_8_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_9_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7a_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_12_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_13_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_0_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_7_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_8_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_10_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_11_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/4_0_duality.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/4_0_duality.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


  <h1>4 0 duality</h1>

<p>Convex optimization problems exhibit powerful duality principles that provide deeper theoretical insight and practical tools for machine learning. In this section, we develop the fundamentals of Lagrange duality (primal and dual problems), explore conditions for strong duality (such as Slater’s condition), and derive the Karush–Kuhn–Tucker (KKT) optimality conditions for convex problems. We will emphasize geometric intuition and link the theory to machine learning applications like support vector machines (SVMs). (Recall from earlier sections that convexity ensures any local optimum is global and enables efficient algorithms; we will now see that duality offers certificates of optimality and alternative solution approaches.)</p>
<h3 id="lagrange-duality-fundamentals">Lagrange Duality Fundamentals<a class="headerlink" href="#lagrange-duality-fundamentals" title="Permanent link">¶</a></h3>
<p>Primal and Lagrangian: Consider a convex optimization problem in standard form:</p>
<div class="arithmatex">\[
\begin{aligned}
\min_x \quad &amp; f_0(x) &amp;&amp; \text{(convex objective)} \\
\text{s.t.} \quad 
&amp; f_i(x) \le 0, \quad i = 1, \dots, m &amp;&amp; \text{(convex inequality constraints)} \\
&amp; h_j(x) = 0, \quad j = 1, \dots, p &amp;&amp; \text{(affine equality constraints)}
\end{aligned}
\]</div>
<p>where <span class="arithmatex">\(f_0, f_i\)</span> are convex and <span class="arithmatex">\(h_j\)</span> are affine. We call this the primal problem. To construct the Lagrangian, we introduce multipliers <span class="arithmatex">\(\lambda_i \ge 0\)</span> for each inequality and <span class="arithmatex">\(\nu_j\)</span> (which can be positive or negative) for each equality. The Lagrangian is:</p>
<div class="arithmatex">\[
L(x, \lambda, \nu)
= f_0(x)
+ \sum_{i=1}^m \lambda_i f_i(x)
+ \sum_{j=1}^p \nu_j h_j(x)
\]</div>
<p>where <span class="arithmatex">\(\lambda_i \ge 0\)</span>. Intuitively, <span class="arithmatex">\(L(x,\lambda,\nu)\)</span> augments the objective with penalties for constraint violations. If any <span class="arithmatex">\(f_i(x)\)</span> is positive (violating <span class="arithmatex">\(f_i(x)\le0\)</span>), a sufficiently large <span class="arithmatex">\(\lambda_i\)</span> will heavily penalize <span class="arithmatex">\(x\)</span>
kuleshov-group.github.io
. Thus, adding these weighted terms “relaxes” the constraints by pushing <span class="arithmatex">\(x\)</span> to satisfy them for large multipliers.</p>
<p><strong>Dual function:</strong> For any fixed multipliers <span class="arithmatex">\((\lambda,\nu)\)</span>, we define the dual function as the infimum of the Lagrangian over <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
g(\lambda, \nu) = \inf_x \, L(x, \lambda, \nu)
\]</div>
<p>This infimum (which could be <span class="arithmatex">\(-\infty\)</span> for some multipliers) gives us a lower bound on the optimal primal value. Specifically, for any <span class="arithmatex">\(\lambda \ge 0,\nu\)</span>, and any feasible <span class="arithmatex">\(x\)</span> (satisfying all constraints), we have <span class="arithmatex">\(L(x,\lambda,\nu) \ge f_0(x)\)</span> (because <span class="arithmatex">\(f_i(x)\le0\)</span> makes <span class="arithmatex">\(\lambda_i f_i(x) \le 0\)</span> and <span class="arithmatex">\(h_j(x)=0\)</span> makes <span class="arithmatex">\(\nu_j h_j(x)=0\)</span>). In particular, at the optimum <span class="arithmatex">\(x^*\)</span> of the primal, <span class="arithmatex">\(L(x^,\lambda,\nu) \ge f_0(x^)\)</span>. Taking the infimum in <span class="arithmatex">\(x\)</span> yields <span class="arithmatex">\(g(\lambda,\nu) \le f_0(x^)\)</span>. Thus:</p>
<blockquote>
<p>Weak Duality: For any multipliers <span class="arithmatex">\((\lambda,\nu)\)</span> with <span class="arithmatex">\(\lambda \ge 0\)</span>, the dual function <span class="arithmatex">\(g(\lambda,\nu)\)</span> is less than or equal to the optimal primal value <span class="arithmatex">\(p^*\)</span>. In other words, any choice of multipliers provides a lower bound: <span class="arithmatex">\(g(\lambda,\nu) \le p^*\)</span>.</p>
</blockquote>
<p>We now pose the dual problem: maximize this lower bound subject to dual feasibility (i.e. <span class="arithmatex">\(\lambda \ge 0\)</span>). The Lagrange dual problem is:</p>
<div class="arithmatex">\[
d^* = \max_{\lambda \ge 0, \, \nu} \; g(\lambda, \nu)
\]</div>
<p>Because <span class="arithmatex">\(g(\lambda,\nu)\)</span> is concave (as an infimum of affine functions of <span class="arithmatex">\((\lambda,\nu)\)</span>) even if the primal is not, the dual is a convex maximization problem. We always have <span class="arithmatex">\(d^* \le p^*\)</span> (weak duality). The difference <span class="arithmatex">\(p^* - d^*\)</span> is called the duality gap. In general, there may be a gap (<span class="arithmatex">\(d^* &lt; p^*\)</span>), but under certain conditions (to be discussed), strong duality holds, meaning <span class="arithmatex">\(d^* = p^*\)</span>. Solving the dual can then be as good as solving the primal, and often easier. In fact, earlier we noted that leveraging a dual formulation can be a practical strategy for convex problems.</p>
<p><strong>Geometric intuition:</strong> Dual variables <span class="arithmatex">\(\lambda_i\)</span> can be viewed as “force” or “price” for satisfying constraint <span class="arithmatex">\(i\)</span>. If a constraint is violated, the dual tries to increase the objective (penalty) unless <span class="arithmatex">\(x\)</span> moves back into the feasible region. At optimum, the dual variables balance the objective’s gradient against the constraint boundaries. Geometrically, each <span class="arithmatex">\(\lambda_i \ge 0\)</span> defines a supporting hyperplane to the primal feasible region – the dual problem is essentially finding the tightest such supporting hyperplanes that still lie below the objective’s graph.</p>
<p><strong>Example – Dual of an SVM:</strong> To illustrate duality, consider the hard-margin SVM problem:</p>
<div class="arithmatex">\[
\begin{aligned}
\min_{w, b} \quad &amp; \tfrac{1}{2}\|w\|^2 \\
\text{s.t.} \quad &amp; y_i(w^\top x_i + b) \ge 1, \quad i = 1, \dots, N.
\end{aligned}
\]</div>
<p>which is convex (a QP). Introduce multipliers <span class="arithmatex">\(\lambda_i \ge 0\)</span> for each constraint. The Lagrangian is</p>
<div class="arithmatex">\[
L(w, b, \lambda)
= \tfrac{1}{2}\|w\|^2
+ \sum_{i=1}^N \lambda_i \big( 1 - y_i (w^\top x_i + b) \big)
\]</div>
<p>We minimize <span class="arithmatex">\(L\)</span> over <span class="arithmatex">\(w,b\)</span> to get the dual function <span class="arithmatex">\(g(\lambda)\)</span>. Setting gradients to zero: <span class="arithmatex">\(\partial L/\partial w = 0\)</span> yields <span class="arithmatex">\(w = \sum_{i=1}^N \lambda_i y_i x_i\)</span>, and <span class="arithmatex">\(\partial L/\partial b = 0\)</span> yields <span class="arithmatex">\(\sum_{i=1}^N \lambda_i y_i = 0\)</span>. Substituting back, the dual becomes:</p>
<div class="arithmatex">\[
\begin{aligned}
\max_{\lambda \ge 0} \quad &amp;
\sum_{i=1}^N \lambda_i
- \frac{1}{2} \sum_{i,j=1}^N
\lambda_i \lambda_j y_i y_j (x_i^\top x_j) \\
\text{s.t.} \quad &amp;
\sum_{i=1}^N \lambda_i y_i = 0.
\end{aligned}
\]</div>
<p>which is a concave quadratic maximization (or QP) in variables <span class="arithmatex">\(\lambda_i\)</span>. Here the dual has <span class="arithmatex">\(N\)</span> variables (one per training point) and one equality constraint, regardless of the feature dimension. Importantly, strong duality holds for this convex QP, so the primal and dual optima coincide. Solving the dual directly yields the optimal <span class="arithmatex">\(\lambda^*\)</span>, from which we recover <span class="arithmatex">\(w^ = \sum_i \lambda^*_i y_i x_i\)</span>. Only training points with <span class="arithmatex">\(\lambda^*_i &gt; 0\)</span> (those that tighten the margin constraint) appear in this sum – these are the support vectors. This dual formulation is the key to the kernelized SVM, since <span class="arithmatex">\(x_i^*\top x_j\)</span> appears inside the objective (one can use kernel functions in place of the dot product).</p>
<p><strong>Why duality helps:</strong> In the SVM above, the dual turned out to be more convenient: it gave insight (support vectors) and is computationally efficient when <span class="arithmatex">\(N\)</span> is smaller than feature dimension or when using kernels. More generally, the dual problem can sometimes be easier to solve (e.g. fewer constraints or simpler domain), or it provides a certificate of optimality. If we solve the dual and obtain <span class="arithmatex">\(d^*\)</span>, we instantly have a lower bound on <span class="arithmatex">\(p^*\)</span>; if we also have a primal feasible solution with objective <span class="arithmatex">\(p\)</span>, the gap <span class="arithmatex">\(p - d^*\)</span> tells us how close that solution is to optimal. In convex problems, often <span class="arithmatex">\(p^* = d^*\)</span> (strong duality), in which case finding dual-optimal <span class="arithmatex">\((\lambda^*,\nu^*)\)</span> and a primal feasible <span class="arithmatex">\(x^*\)</span> such that <span class="arithmatex">\(L(x^*,\lambda^*,\nu^*) = p^*\)</span> certifies optimality of <span class="arithmatex">\(x^*\)</span>. Duality thus not only offers alternative algorithms but also optimality conditions that are crucial in theory and practice.</p>
<h3 id="strong-duality-and-slaters-condition">Strong Duality and Slater’s Condition<a class="headerlink" href="#strong-duality-and-slaters-condition" title="Permanent link">¶</a></h3>
<p>While weak duality <span class="arithmatex">\(d^* \le p^*\)</span> always holds, strong duality (<span class="arithmatex">\(d^* = p^*\)</span>) does not hold for every problem. In general convex optimization, we require certain regularity conditions (constraint qualifications) to ensure no gap. The most common condition is Slater’s condition.</p>
<p><strong>Slater’s condition:</strong> If the primal problem is convex and there exists a strictly feasible point (a point <span class="arithmatex">\(x\)</span> satisfying all inequalities strictly and all equalities exactly), then the optimal duality gap is zero. In other words, if you can find a point that lies in the interior of the feasible region (not just on the boundary), then strong duality holds for convex problems. Formally, if <span class="arithmatex">\(\exists \tilde{x}\)</span> such that <span class="arithmatex">\(f_i(\tilde{x}) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(h_j(\tilde{x})=0\)</span> for all <span class="arithmatex">\(j\)</span>, then <span class="arithmatex">\(p^* = d^*\)</span>. This is a very mild condition “satisfied in most cases”  – intuitively, it rules out degenerate cases where the optimum is on the boundary with no interior, which can cause a duality gap.</p>
<p>An example of a convex problem failing Slater’s condition is one with contradictory constraints or an optimal solution at a corner with no interior feasible region (e.g., minimizing <span class="arithmatex">\(f(x)=x\)</span> subject to <span class="arithmatex">\(x \ge 0\)</span> and <span class="arithmatex">\(x \le 0\)</span> – the only feasible <span class="arithmatex">\(x\)</span> is 0 which lies on the boundary of both constraints; here strong duality can fail). But for standard ML problems (SVMs, logistic regression with constraints, LASSO in constrained form, etc.), strict feasibility usually holds (we can often find an interior solution by relaxing inequalities a bit), so we can assume strong duality.</p>
<p><strong>Consequences of strong duality:</strong> If strong duality holds and <span class="arithmatex">\(x^*\)</span> is primal-optimal and <span class="arithmatex">\((\lambda^*,\nu^*)\)</span> dual-optimal, then <span class="arithmatex">\(f_0(x^*) = g(\lambda^*,\nu^*)\)</span>. Combined with weak duality (<span class="arithmatex">\(g(\lambda^*,\nu^*) \le f_0(x^*)\)</span>), this implies <span class="arithmatex">\(f_0(x^*) = L(x^*,\lambda^*,\nu^*)\)</span> (since <span class="arithmatex">\(g\)</span> is the infimum of <span class="arithmatex">\(L\)</span>) and also that <span class="arithmatex">\(x^*\)</span> attains the infimum for those optimal multipliers. Thus <span class="arithmatex">\(x^*\)</span> and <span class="arithmatex">\((\lambda^*,\nu^*)\)</span> together satisfy certain optimality conditions – specifically the KKT conditions we derive next. Moreover, the zero duality gap means the dual solution provides a certificate of optimality for the primal solution. In practice, one can solve the dual (which is often easier) and get the primal solution from it (as we did with SVM). Also, verifying optimality is straightforward: if one finds any feasible <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\((\lambda,\nu)\)</span> with <span class="arithmatex">\(\lambda\ge0\)</span> such that <span class="arithmatex">\(f_0(x) = L(x,\lambda,\nu)\)</span>, then <span class="arithmatex">\(x\)</span> must be optimal.</p>
<p><strong>Primal-dual interpretation:</strong> Strong duality implies existence of a saddle point: <span class="arithmatex">\(L(x^*,\lambda^*,\nu^*) = \min_x \max_{\lambda\ge0,\nu} L(x,\lambda,\nu) = \max_{\lambda\ge0,\nu} \min_x L(x,\lambda,\nu)\)</span>. At optimum, the primal and dual objectives coincide. We can picture the primal objective’s graph and the dual constraints as supporting hyperplanes – at optimality, the lowest supporting hyperplane (dual) just touches the graph of <span class="arithmatex">\(f_0\)</span> at <span class="arithmatex">\(x^*\)</span>, with no gap in between. This touching point corresponds to equal primal and dual values.</p>
<p><strong>Slater in ML example:</strong> In the hard-margin SVM, Slater’s condition holds if the classes are linearly separable – we can find a strictly feasible separating hyperplane that classifies all points correctly with margin &gt; 1. If data is strictly separable, strong duality holds (indeed we saw <span class="arithmatex">\(p^*=d^*\)</span>). For soft-margin SVM (with slack variables), Slater’s condition also holds (take a sufficiently large margin violation allowance to get an interior point). Thus, we can be confident in solving the dual. In constrained regression problems (like LASSO formulated with a constraint <span class="arithmatex">\(|w|_1 \le t\)</span>), Slater’s condition holds as long as the constraint is not tight initially (e.g., one can usually find <span class="arithmatex">\(w=0\)</span> which strictly satisfies <span class="arithmatex">\(|w|_1 &lt; t\)</span> if <span class="arithmatex">\(t\)</span> is chosen larger than 0), guaranteeing no duality gap.</p>
<p>(Historical note: There are other constraint qualifications beyond Slater, to handle cases like affine constraints or non-strict feasibility, but Slater’s is easiest and usually applicable in convex ML problems.)</p>
<h3 id="karushkuhntucker-kkt-conditions">Karush–Kuhn–Tucker (KKT) Conditions<a class="headerlink" href="#karushkuhntucker-kkt-conditions" title="Permanent link">¶</a></h3>
<p>The Karush–Kuhn–Tucker conditions are the first-order optimality conditions for constrained problems, generalizing the method of Lagrange multipliers to include inequality constraints. For convex problems that satisfy Slater’s condition, the KKT conditions are not only necessary but also sufficient for optimality. This means solving the KKT equations is essentially equivalent to solving the original problem.</p>
<p>Consider the convex primal problem above. The KKT conditions for a tuple <span class="arithmatex">\((x^*,\lambda^*,\nu^*)\)</span> are:</p>
<ol>
<li>
<p>Primal feasibility: <span class="arithmatex">\(f_i(x^*) \le 0\)</span> for all <span class="arithmatex">\(i\)</span>, and <span class="arithmatex">\(h_j(x^*) = 0\)</span> for all <span class="arithmatex">\(j\)</span>. (The solution must satisfy the original constraints.)</p>
</li>
<li>
<p>Dual feasibility: <span class="arithmatex">\(\lambda^*_i \ge 0\)</span> for all <span class="arithmatex">\(i\)</span>. (Dual variables associated with inequalities must be nonnegative.)</p>
</li>
<li>
<p>Stationarity (gradient condition): <span class="arithmatex">\(\nabla f_0(x^*) + \sum_{i=1}^m \lambda^i,\nabla f_i(x^*) + \sum{j=1}^p \nu^*_j,\nabla h_j(x^*) = 0\)</span>. This means the gradient of the Lagrangian vanishes at <span class="arithmatex">\(x^*\)</span>. Intuitively, at optimum there is no feasible direction that can decrease the objective — the objective’s gradient is exactly balanced by a combination of the constraint gradients.</p>
</li>
<li>
<p>Complementary slackness: <span class="arithmatex">\(\lambda^*_i,f_i(x^*) = 0\)</span> for each inequality constraint <span class="arithmatex">\(i\)</span>. This crucial condition means that for each constraint, either the constraint is active (<span class="arithmatex">\(f_i(x^)=0\)</span> on the boundary) and then its multiplier <span class="arithmatex">\(\lambda^*_i\)</span> may be positive, or the constraint is inactive (<span class="arithmatex">\(f_i(x^*) &lt; 0\)</span> strictly inside) and then the multiplier must be zero. In short, <span class="arithmatex">\(\lambda_i^*\)</span> can only put “pressure” on a constraint if that constraint is tight at the solution.</p>
</li>
</ol>
<p>These four conditions together characterize optimal solutions for convex problems (assuming a constraint qualification like Slater’s holds to ensure no duality gap). If one can find <span class="arithmatex">\((x,\lambda,\nu)\)</span> satisfying all KKT conditions, then <span class="arithmatex">\(x\)</span> is optimal and <span class="arithmatex">\((\lambda,\nu)\)</span> are the corresponding optimal dual variables. Conversely, if <span class="arithmatex">\(x^*\)</span> is optimal (and Slater holds), then there exist multipliers making KKT true. Thus, KKT conditions give an equivalent system of equations/inequalities to solve for the optimum.</p>
<p><strong>Geometric interpretation:</strong> At an optimum <span class="arithmatex">\(x^*\)</span>, consider any active inequality constraints and all equality constraints – these define some boundary “face” of the feasible region that <span class="arithmatex">\(x^*\)</span> lies on. The stationarity condition says the negative objective gradient <span class="arithmatex">\(-\nabla f_0(x^*)\)</span> lies in the span of the gradients of active constraints. In other words, the descent direction is blocked by the constraints: you cannot move into any direction that decreases <span class="arithmatex">\(f_0\)</span> without leaving the feasible set. This is consistent with the earlier geometric intuition from Section C: at a boundary optimum, the gradient of <span class="arithmatex">\(f\)</span> points outward, perpendicular to the feasible region. The multipliers <span class="arithmatex">\(\lambda_i\)</span> are essentially the coefficients of this combination of constraint normals that balances the objective’s gradient. Complementary slackness further tells us that any constraint whose normal is not needed to support the optimum (i.e. not active) must have zero multiplier – it’s like saying non-binding constraints have no “force” (λ) at optimum, while binding constraints exert a force to hold the optimum in place.</p>
<p>For example, consider a simple 2D problem: minimize some convex <span class="arithmatex">\(f(x)\)</span> subject to one constraint <span class="arithmatex">\(g(x)\le0\)</span>. Two scenarios: (i) The unconstrained minimizer of <span class="arithmatex">\(f\)</span> lies inside the feasible region. Then at optimum, <span class="arithmatex">\(g(x^) &lt; 0\)</span> is inactive, so <span class="arithmatex">\(\lambda^=0\)</span> and <span class="arithmatex">\(\nabla f(x^*)=0\)</span> as usual (interior optimum). (ii) The unconstrained minimizer lies outside, so the optimum occurs on the boundary <span class="arithmatex">\(g(x)=0\)</span>. At that boundary point <span class="arithmatex">\(x^*\)</span>, the gradient <span class="arithmatex">\(\nabla f(x^*)\)</span> must point outward, proportional to <span class="arithmatex">\(\nabla g(x^*)\)</span> to prevent any feasible descent. KKT reflects this: <span class="arithmatex">\(g(x^*)=0\)</span> active, <span class="arithmatex">\(\lambda^*&gt;0\)</span>, and <span class="arithmatex">\(\nabla f(x^*) + \lambda^* \nabla g(x^*)=0\)</span>. Graphically, a level set contour of <span class="arithmatex">\(f\)</span> is tangent to the constraint boundary at <span class="arithmatex">\(x^*\)</span> – their normals align.</p>
<p><strong>KKT and Lagrange multipliers:</strong> If we had only equality constraints, KKT reduces to the classic method of Lagrange multipliers (gradient of <span class="arithmatex">\(f\)</span> equals a linear combination of equality constraint gradients). Inequalities add the twist of <span class="arithmatex">\(\lambda \ge 0\)</span> and slackness. In fact, KKT can be seen as splitting the normal cone condition: <span class="arithmatex">\(0 \in \nabla f(x^) + N_{\mathcal{X}}(x^)\)</span> (a general optimality condition) into explicit pieces: the normal cone <span class="arithmatex">\(N_{\mathcal{X}}(x^*)\)</span> generated by active constraints’ normals, and each such generator weighted by <span class="arithmatex">\(\lambda_i\)</span>.</p>
<p><strong>SVM example (revisited)</strong> – KKT reveals support vectors: For the hard-margin SVM, the KKT conditions are insightful. The primal constraints are <span class="arithmatex">\(g_i(w,b) = 1 - y_i(w^\top x_i + b) \le 0\)</span>. Let <span class="arithmatex">\(\alpha_i\)</span> denote the multiplier for constraint <span class="arithmatex">\(i\)</span> (often SVM literature uses <span class="arithmatex">\(\alpha\)</span> instead of <span class="arithmatex">\(\lambda\)</span>). KKT conditions:</p>
<ul>
<li>
<p><strong>Primal feasibility:</strong> <span class="arithmatex">\(1 - y_i(w^{* \top} x_i + b^*) \le 0\)</span> for all <span class="arithmatex">\(i\)</span> (all training points are on or outside the margin).</p>
</li>
<li>
<p>Dual feasibility: <span class="arithmatex">\(\alpha_i^* \ge 0\)</span> for all <span class="arithmatex">\(i\)</span>.</p>
</li>
<li>
<p>Stationarity: <span class="arithmatex">\(\nabla_w \Big(\frac{1}{2}|w|^2 + \sum_i \alpha_i (1 - y_i(w^\top x_i + b))\Big) = 0\)</span> and <span class="arithmatex">\(\partial L/\partial b = 0\)</span>. These give <span class="arithmatex">\(w^* = \sum_i \alpha_i^* y_i x_i\)</span> and <span class="arithmatex">\(\sum_i \alpha_i^* y_i = 0\)</span> (same as earlier from the dual).</p>
</li>
<li>
<p><strong>Complementary slackness:</strong> <span class="arithmatex">\(\alpha_i^* \big(1 - y_i(w^{*\top} x_i + b^*)\big) = 0\)</span> for each <span class="arithmatex">\(i\)</span>.</p>
</li>
</ul>
<p>This last condition means: for any training point <span class="arithmatex">\(i\)</span>, either it lies strictly outside the margin (violating nothing, so <span class="arithmatex">\(y_i(w^{*\top} x_i + b^*) &gt; 1\)</span>), in which case the constraint is inactive and <span class="arithmatex">\(\alpha_i^* = 0\)</span>; or it lies exactly on the margin (<span class="arithmatex">\(y_i(w^{*\top} x_i + b^*) = 1\)</span>), in which case <span class="arithmatex">\(\alpha_i^*\)</span> can be positive. The points on the margin are precisely the <strong>support vectors</strong>, and they end up with <span class="arithmatex">\(\alpha_i^* &gt; 0\)</span>. Points safely away from the margin have <span class="arithmatex">\(\alpha_i^* = 0\)</span> and do not appear in the weight vector <span class="arithmatex">\(w^*\)</span>. Thus, the KKT conditions formally explain the <strong>sparseness of the SVM solution</strong>: the decision boundary is supported only by a subset of the training points.</p>
<p>Geometric view of SVM KKT: The diagram shows a linearly separable classification with the optimal hyperplane (solid line) and margin boundaries (dashed lines). Support vectors (circled points) lie exactly on the margin, meaning the SVM’s constraints <span class="arithmatex">\(y_i(w^\top x_i+b)\ge1\)</span> hold with equality for these points. By complementary slackness, these points have nonzero dual weights <span class="arithmatex">\(\alpha_i^*\)</span>, thus actively determine the hyperplane. Other points (not circled) lie strictly outside the margin (constraint inactive), so <span class="arithmatex">\(\alpha_i^*=0\)</span>; perturbing any non-support vector (within the margin bounds) does not move the decision boundary. Only support vectors “push back” on the hyperplane, illustrating KKT: the objective’s optimum is achieved when its gradient is balanced by constraints from support vectors.</p>
<p>In summary, the KKT conditions provide a checklist for optimality in convex problems: feasibility (primal and dual), zero gradient of the Lagrangian, and complementary slackness. For convex optimization, these conditions are both necessary and sufficient (under Slater’s condition). They are extremely useful in practice for analyzing solutions. In ML, many algorithms can be understood in terms of KKT: for instance, the optimality conditions of LASSO (ℓ1-regularized regression) dictate which weights are zero vs non-zero (the subgradient of the ℓ1 norm must balance the gradient of least squares – if a weight is zero at optimum, the gradient of the loss at that feature must lie in the ±λ range, etc.). KKT conditions also form the basis of dual coordinate descent methods, which solve optimization by ensuring KKT is gradually satisfied for all constraints.</p>
<p>Takeaway: Duality and KKT conditions are powerful tools for convex optimization. Duality gives us alternative ways to solve problems and certify optimality (often leveraged in distributed optimization or derivations of ML algorithms), while KKT conditions distill the optimality criteria into a set of equations/inequalities that often yield insight (support vectors in SVM, thresholding in LASSO, etc.) beyond what the primal solution alone provides.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>