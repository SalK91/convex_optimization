<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/3_4_proximalprojected/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>3 4 proximalprojected - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3 4 proximalprojected
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1">
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_0_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_1_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_8_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_9_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7a_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_12_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_13_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_0_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_7_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_8_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_10_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_11_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/3_4_proximalprojected.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/3_4_proximalprojected.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


  <h1>3 4 proximalprojected</h1>

<p>Real-world convex problems often involve constraints and nonsmooth terms. Projected gradient methods and proximal gradient methods extend our first-order algorithms to handle these situations by incorporating projection or proximal steps into the update.</p>
<h3 id="projected-gradient-descent-constraints">Projected Gradient Descent (Constraints)<a class="headerlink" href="#projected-gradient-descent-constraints" title="Permanent link">¶</a></h3>
<p>Suppose we want to minimize <span class="arithmatex">\(f(x)\)</span> subject to <span class="arithmatex">\(x \in \mathcal{X}\)</span>, where <span class="arithmatex">\(\mathcal{X}\)</span> is a convex feasible set (e.g. a polytope or ball). A gradient descent step <span class="arithmatex">\(y_{k} = x_k - \alpha \nabla f(x_k)\)</span> might produce a point <span class="arithmatex">\(y_k\)</span> outside <span class="arithmatex">\(\mathcal{X}\)</span>. The idea of projected gradient descent is simple: after the gradient step, project <span class="arithmatex">\(y_k\)</span> back onto <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<p>
<script type="math/tex; mode=display">
x_{k+1} = \Pi_X \big( x_k - \alpha \nabla f(x_k) \big)
</script>
</p>
<p>Here <span class="arithmatex">\(\Pi_{\mathcal{X}}(y) = \arg\min_{x\in \mathcal{X}}|x - y|^2\)</span> is the orthogonal projection onto <span class="arithmatex">\(\mathcal{X}\)</span>. This two-step iteration ensures that <span class="arithmatex">\(x_{k+1}\in\mathcal{X}\)</span> for all <span class="arithmatex">\(k\)</span>. Geometrically, we take the usual gradient descent step into <span class="arithmatex">\(y_k\)</span> which may lie off the feasible set, then find the closest feasible point. The correction is orthogonal to the boundary of <span class="arithmatex">\(\mathcal{X}\)</span> at the projection point (no component of the step along the boundary is wasted, since <span class="arithmatex">\(y_k - x_{k+1}\)</span> is perpendicular to <span class="arithmatex">\(\mathcal{X}\)</span>). Thus, projected gradient still decreases <span class="arithmatex">\(f(x)\)</span> to first order while respecting the constraints.</p>
<p>For example, if <span class="arithmatex">\(\mathcal{X}={x:|x|_2 \le 1}\)</span> (the unit Euclidean ball), the projection is <span class="arithmatex">\(\Pi{\mathcal{X}}(y) = \frac{y}{\max{1,|y|_2}}\)</span>: any <span class="arithmatex">\(y\)</span> outside the ball is radially scaled back to the boundary, and if <span class="arithmatex">\(y\)</span> is inside the ball it stays unchanged. Projected gradient descent is widely used in constrained convex optimization (like projection onto probability simplex, box constraints <span class="arithmatex">\(x\in [l,u]\)</span>, etc.) because of its simplicity: one just needs a routine to project onto <span class="arithmatex">\(\mathcal{X}\)</span>, which is often straightforward.</p>
<p>Convergence: If <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(L\)</span>-smooth, projected gradient descent inherits similar convergence guarantees as unconstrained gradient descent (albeit the analysis uses more geometry from <span class="arithmatex">\(\mathcal{X}\)</span>). With a suitable step size, <span class="arithmatex">\(f(x_k)\to f(x^*)\)</span> and for strongly convex problems <span class="arithmatex">\(x_k\to x^*\)</span> linearly. The projection step does not spoil convergence; it only ensures feasibility. Intuitively, the error analysis includes an extra term for distance from <span class="arithmatex">\(\mathcal{X}\)</span>, but the projection minimizes that, keeping the iterates as close as possible to the unconstrained path.</p>
<p>Projection is actually a special case of a more general operator important in convex optimization: the proximal operator.</p>
<h3 id="proximal-operators-and-proximal-gradient">Proximal Operators and Proximal Gradient<a class="headerlink" href="#proximal-operators-and-proximal-gradient" title="Permanent link">¶</a></h3>
<p>Consider an optimization problem with a convex but nonsmooth term, for example:</p>
<div class="arithmatex">\[
\min_{x \in \mathbb{R}^n} \; f(x) + g(x)
\]</div>
<p>where <span class="arithmatex">\(f(x)\)</span> is convex and differentiable (smooth loss or fit term) and <span class="arithmatex">\(g(x)\)</span> is convex but possibly nondifferentiable (regularizer or indicator of constraints). Here <span class="arithmatex">\(g(x)\)</span> could be things like <span class="arithmatex">\(L_1\)</span> norm (<span class="arithmatex">\(\ell_1\)</span> penalty), <span class="arithmatex">\(\ell_\infty\)</span> norm, indicator functions enforcing <span class="arithmatex">\(x\)</span> in some set, etc. The proximal gradient method addresses such problems by splitting the handling of <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span>.</p>
<p>We know how to deal with <span class="arithmatex">\(f\)</span> using a gradient step. To handle <span class="arithmatex">\(g\)</span>, we use its proximal operator. The proximal operator of <span class="arithmatex">\(g\)</span> (with parameter <span class="arithmatex">\(\lambda&gt;0\)</span>) is defined as:</p>
<div class="arithmatex">\[
\operatorname{prox}_{\lambda g}(y)
= \arg\min_x \left\{
g(x) + \frac{1}{2\lambda} \|x - y\|^2
\right\}
\]</div>
<p>This is the solution of a regularized minimization of <span class="arithmatex">\(g(x)\)</span> where we stay as close as possible to a given point <span class="arithmatex">\(y\)</span>. In words, <span class="arithmatex">\(\operatorname{prox}{\lambda g}(y)\)</span> is a point that compromises between minimizing <span class="arithmatex">\(g(x)\)</span> and staying near <span class="arithmatex">\(y\)</span> (the quadratic term <span class="arithmatex">\(\frac{1}{2\lambda}|x-y|^2\)</span> keeps <span class="arithmatex">\(x\)</span> from straying too far from <span class="arithmatex">\(y\)</span>). The parameter <span class="arithmatex">\(\lambda\)</span> scales how strongly we insist on proximity to <span class="arithmatex">\(y\)</span>: as <span class="arithmatex">\(\lambda \to 0\)</span>, <span class="arithmatex">\(\operatorname{prox}{\lambda g}(y)\approx y\)</span> (we barely move); as <span class="arithmatex">\(\lambda\)</span> grows, we’re willing to move farther to reduce <span class="arithmatex">\(g(x)\)</span>. The proximal operator is well-defined for any closed convex <span class="arithmatex">\(g\)</span> (it has a unique minimizer due to strong convexity of the quadratic term), and it generalizes the notion of projection:</p>
<p>If <span class="arithmatex">\(g(x)\)</span> is the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span> (meaning <span class="arithmatex">\(g(x)=0\)</span> if <span class="arithmatex">\(x\in\mathcal{X}\)</span> and <span class="arithmatex">\(+\infty\)</span> otherwise), then <span class="arithmatex">\(\operatorname{prox}{\lambda g}(y)\)</span> is exactly <span class="arithmatex">\(\Pi{\mathcal{X}}(y)\)</span>. That’s because the minimization <span class="arithmatex">\(\min_x {I_{\mathcal{X}}(x) + \frac{1}{2\lambda}|x-y|^2}\)</span> forces <span class="arithmatex">\(x\)</span> to lie in <span class="arithmatex">\(\mathcal{X}\)</span> (outside <span class="arithmatex">\(\mathcal{X}\)</span> the objective is infinite) and then reduces to <span class="arithmatex">\(\min_{x\in \mathcal{X}}|x-y|^2\)</span>, the definition of projection. Thus, projection is a special case of a proximal operator. Prox operators can be thought of as “softened” projections that not only enforce constraints but can also induce certain structures (like sparsity).</p>
<p>Given <span class="arithmatex">\(\operatorname{prox}_{\lambda g}\)</span>, the proximal gradient method for <span class="arithmatex">\(f+g\)</span> works as follows:</p>
<ol>
<li>
<p>Take a usual gradient descent step on the smooth part <span class="arithmatex">\(f\)</span>: <span class="arithmatex">\(y_{k} = x_k - \alpha \nabla f(x_k)\)</span>.</p>
</li>
<li>
<p>Apply the proximal operator of <span class="arithmatex">\(g\)</span>: <span class="arithmatex">\(x_{k+1} = \operatorname{prox}_{\alpha g}(y_k)\)</span>.</p>
</li>
</ol>
<p>In formula form,</p>
<div class="arithmatex">\[
x_{k+1} = \operatorname{prox}_{\alpha g}\!\left( x_k - \alpha \nabla f(x_k) \right)
\]</div>
<p>This update handles two things: the gradient step tries to reduce the objective <span class="arithmatex">\(f\)</span>, and the prox step pulls the solution toward satisfying the “desirable structure” encoded by <span class="arithmatex">\(g\)</span>. If <span class="arithmatex">\(g\)</span> is an indicator of <span class="arithmatex">\(\mathcal{X}\)</span>, this becomes projected gradient descent (since <span class="arithmatex">\(\operatorname{prox}{\alpha I{\mathcal{X}}}(y)=\Pi_{\mathcal{X}}(y)\)</span>). If <span class="arithmatex">\(g\)</span> is something like <span class="arithmatex">\(\lambda |x|_1\)</span>, the prox step becomes a soft-thresholding of <span class="arithmatex">\(y\)</span> (encouraging sparsity). If <span class="arithmatex">\(g\)</span> is the absolute value function (total variation, etc.), prox becomes a shrinkage operator, and so on. The proximal gradient method is also known as Forward-Backward Splitting: a forward (gradient) step on <span class="arithmatex">\(f\)</span>, followed by a backward (proximal) step on <span class="arithmatex">\(g\)</span>.</p>
<p>Convergence: If <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(L\)</span>-smooth and <span class="arithmatex">\(g\)</span> is convex (proper, closed), then proximal gradient descent with a fixed step <span class="arithmatex">\(\alpha \le 1/L\)</span> converges to the optimum at essentially the same rate as plain gradient descent: <span class="arithmatex">\(O(1/k)\)</span> in the general convex case, and linear convergence if <span class="arithmatex">\(f+g\)</span> is strongly convex (under a suitable condition like <span class="arithmatex">\(g\)</span> being simple or strongly convex). The intuition is that the nonsmooth part <span class="arithmatex">\(g\)</span> doesn’t hurt the rate as long as its proximal operator is efficiently computable; the “heavy lifting” of ensuring a decrease is done by the smooth part’s Lipschitz condition and the prox step never increases the objective. However, each iteration’s cost includes computing <span class="arithmatex">\(\operatorname{prox}_{\alpha g}\)</span>, which in some cases might be as hard as solving a subproblem. Fortunately, for many common <span class="arithmatex">\(g\)</span>, the prox step is easy: e.g., <span class="arithmatex">\(\ell_1\)</span> norm (soft-thresholding), <span class="arithmatex">\(\ell_2\)</span> norm (shrink towards zero), indicator of linear constraints (simple clipping or normalization), etc.</p>
<p>Proximal point algorithm (implicit gradient): A conceptual algorithm worth mentioning is the proximal point method, which is like taking only implicit steps on the entire objective. It iterates <span class="arithmatex">\(x_{k+1} = \operatorname{prox}{\lambda f}(x_k)\)</span>, i.e. solves <span class="arithmatex">\(x{k+1} = \arg\min_x {f(x) + \frac{1}{2\lambda}|x-x_k|^2}\)</span> exactly at each step. This is in general a difficult subproblem (you basically solve the original problem in each step!), so it’s not an algorithm you’d implement for a generic <span class="arithmatex">\(f\)</span>. But theoretically, the proximal point method has strong convergence guarantees under very mild assumptions (it converges for any <span class="arithmatex">\(\lambda&gt;0\)</span> as long as a minimizer exists). It provides an implicit stabilization of the iteration (always moving to a point that is an actual minimizer of a nearby objective), which is why it converges even for some nonconvex problems and monotone operators. In convex optimization, the proximal point algorithm is more of a theoretical tool—many modern algorithms (like ADMM and SVRG) can be interpreted as approximate or accelerated versions of it.</p>
<p>Proximal vs gradient: one can view the standard gradient descent as the limit of the proximal point method when <span class="arithmatex">\(\lambda\)</span> is small. Gradient descent does <span class="arithmatex">\(x_{k+1}\approx x_k - \lambda \nabla f(x_k)\)</span> which is the first-order condition of the prox subproblem if we only take an infinitesimal step. Proximal steps, in contrast, solve the subproblem exactly, which is why they can be more stable. Proximal gradient hits a sweet spot: we solve the easy part (<span class="arithmatex">\(g\)</span>) exactly via prox, and handle the hard part (<span class="arithmatex">\(f\)</span>) via gradient.</p>
<p><strong>Example – Lasso (L1 regularization):</strong> Take <span class="arithmatex">\(f(x) = \frac{1}{2}|Ax - b|^2\)</span> (a least-squares loss) and <span class="arithmatex">\(g(x) = \lambda |x|1\)</span> (an <span class="arithmatex">\(\ell_1\)</span> penalty encouraging sparsity). The prox operator <span class="arithmatex">\(\operatorname{prox}{\alpha \lambda |\cdot|1}(y)\)</span> is the soft-thresholding: <span class="arithmatex">\([\operatorname{prox}{\alpha \lambda |\cdot|_1}(y)]_i = \text{sign}(y_i)\max{|y_i| - \alpha\lambda,,0}\)</span>. So proximal gradient descent (a.k.a. Iterative Shrinkage-Thresholding Algorithm (ISTA)) does:</p>
<div class="arithmatex">\[
\begin{aligned}
y_k &amp;= x_k - \alpha A^\top (A x_k - b), \\
x_{k+1,i} &amp;= \operatorname{sign}(y_{k,i}) \, \max\{ |y_{k,i}| - \alpha \lambda, \, 0 \}.
\end{aligned}
\]</div>
<p>for each component <span class="arithmatex">\(i\)</span>. This method will converge to the Lasso solution. Furthermore, one can add Nesterov acceleration to this (getting the FISTA algorithm), achieving an <span class="arithmatex">\(O(1/k^2)\)</span> rate for the objective similar to accelerated gradient.</p>
<p>In summary, proximal gradient methods allow us to tackle optimization problems with constraints or nonsmooth terms by splitting the problem into a smooth part (handled by a gradient step) and a simple nonsmooth part (handled by a prox step). The geometry underlying this is the idea of projection as a prox operator: we extend the notion of moving “back to the feasible region” (projection) to moving toward a region that yields lower <span class="arithmatex">\(g(x)\)</span> (proximal step). This framework vastly expands the scope of problems solvable by first-order methods, including Lasso, logistic regression with regularization, matrix norm minimizations, etc., all while maintaining convergence guarantees comparable to gradient descent.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>