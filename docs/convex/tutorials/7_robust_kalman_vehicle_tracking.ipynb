{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc15cfd",
   "metadata": {},
   "source": [
    "# VII — Vehicle Tracking — Robust Kalman Filtering Case Study\n",
    "\n",
    "We track a moving vehicle in 2D from noisy position measurements.\n",
    "\n",
    "- State: position + velocity in x/y  \n",
    "- Dynamics: linear (discrete-time) with unknown input (drive force) and damping  \n",
    "- Noise: mostly mild Gaussian, but with occasional large outliers  \n",
    "\n",
    "A classical Kalman smoother corresponds to a least-squares problem.  \n",
    "To reduce sensitivity to outliers, we replace the quadratic measurement penalty with a Huber loss, yielding a convex problem solved with CVXPY.\n",
    "\n",
    "> Adapted from the official CVXPY example “Robust Kalman filtering for vehicle tracking”.\n",
    "\n",
    "[Kalman Filter Explained](https://www.youtube.com/watch?v=IFeCIbljreY)\n",
    "\n",
    "https://www.youtube.com/watch?v=R63dU5w_djQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a01d94",
   "metadata": {},
   "source": [
    "## Theory and Pre-requisite Knowledge\n",
    "\n",
    "### Recursive Least Squares\n",
    "\n",
    "Suppose we observe a sequence of data pairs $(\\phi_t, y_t)$, where\n",
    "$y_t \\in \\mathbb{R}$ is a scalar measurement and\n",
    "$\\phi_t \\in \\mathbb{R}^d$ is a known feature (regressor) vector.\n",
    "We assume a linear measurement model\n",
    "\n",
    "$$\n",
    "y_t = \\phi_t^\\top \\theta + \\epsilon_t,\n",
    "\\qquad\n",
    "\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2),\n",
    "$$\n",
    "\n",
    "where $\\theta \\in \\mathbb{R}^d$ is an unknown parameter vector to be estimated.\n",
    "\n",
    "A standard approach is least squares, which estimates $\\theta$ by minimizing the sum of squared residuals over all observed data. However, recomputing a batch least-squares solution every time a new data point arrives is computationally expensive and impractical for online or real-time applications. Recursive least squares addresses this issue by updating the parameter estimate incrementally as new measurements become available, without revisiting the full data history. A common formulation is exponentially weighted least squares:\n",
    "\n",
    "$$\n",
    "\\hat\\theta_t\n",
    "=\n",
    "\\arg\\min_\\theta\n",
    "\\sum_{k=1}^t \\lambda^{\\,t-k}\\,\n",
    "\\big(y_k - \\phi_k^\\top \\theta\\big)^2,\n",
    "\\qquad\n",
    "0 < \\lambda \\le 1,\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a forgetting factor that discounts older data. Values of $\\lambda$ close to 1 emphasize long-term consistency, while smaller values allow the estimator to adapt more quickly to changing parameters. The RLS algorithm produces a recursive update of the parameter estimate:\n",
    "\n",
    "$$\n",
    "\\hat\\theta_t\n",
    "=\n",
    "\\hat\\theta_{t-1}\n",
    "+\n",
    "K_t\n",
    "\\big(y_t - \\phi_t^\\top \\hat\\theta_{t-1}\\big),\n",
    "$$\n",
    "\n",
    "where the term\n",
    "\n",
    "$$\n",
    "y_t - \\phi_t^\\top \\hat\\theta_{t-1}\n",
    "$$\n",
    "\n",
    "is the prediction error (or residual), measuring how well the current estimate explains the new observation. The gain vector $K_t$ determines how strongly the estimate should be corrected in response to this residual:\n",
    "\n",
    "$$\n",
    "K_t\n",
    "=\n",
    "\\frac{P_{t-1}\\phi_t}\n",
    "{\\lambda + \\phi_t^\\top P_{t-1}\\phi_t}.\n",
    "$$\n",
    "\n",
    "\n",
    "Here, $P_t \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive-definite matrix that quantifies the uncertainty in the parameter estimate. When uncertainty is large or the new data is highly informative, the correction is larger; when uncertainty is small, updates are more conservative.\n",
    "\n",
    "RLS is particularly useful in online parameter estimation problems, such as system identification, adaptive control, and signal processing, where data arrives sequentially and model parameters may change slowly over time. The central idea behind RLS is to maintain both an estimate of the unknown parameters and a measure of confidence in that estimate, updating both efficiently as new information becomes available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4deff70",
   "metadata": {},
   "source": [
    "## Kalman filter: from recursive least squares to dynamic state estimation\n",
    "\n",
    "Recursive least squares is designed to estimate unknown but essentially static parameters from sequential data. While it is effective for online regression and system identification, it does not explicitly model how the quantity being estimated evolves over time. Any temporal variation must be handled indirectly through mechanisms such as forgetting factors. In many real-world problems, however, the unknown quantity is not static. The state of a physical system—such as the position and velocity of a car—changes continuously according to known physical laws, while also being subject to random disturbances. In these settings, we need an estimator that explicitly accounts for system dynamics rather than treating each observation as an independent regression problem.\n",
    "\n",
    "The Kalman filter addresses this limitation by introducing a motion model that describes how the state evolves from one time step to the next, together with a measurement model that links the hidden state to noisy sensor observations. Both models are assumed to be linear, and uncertainty is modeled using Gaussian noise.\n",
    "\n",
    "The objective of the Kalman filter is to estimate the hidden state of a dynamical system by optimally combining:\n",
    "\n",
    "1) predictions generated by the motion model, and  \n",
    "2) information provided by noisy measurements.\n",
    "\n",
    "Like recursive least squares, the Kalman filter operates recursively and maintains both an estimate and an uncertainty matrix. However, unlike RLS, it explicitly propagates uncertainty through time using the system dynamics and corrects predictions using new measurements. This allows the filter to track time-varying states in a principled and computationally efficient manner.\n",
    "\n",
    "Under linear dynamics and Gaussian noise assumptions, the Kalman filter is optimal in the sense that it minimizes the mean squared estimation error and produces the posterior mean and covariance of the system state at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3fa97",
   "metadata": {},
   "source": [
    "### State-space model\n",
    "We model a discrete-time linear dynamical system as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Motion model (State Equation):}\\quad\n",
    "x_{t+1} &= A x_t + B u_t + w_t, \\\\\n",
    "\\text{Measurement model (Observation Equation):}\\quad\n",
    "y_t &= C x_t + v_t,\n",
    "\\end{aligned}\n",
    "\\qquad t = 0,\\dots,N-1.\n",
    "$$\n",
    "\n",
    "The motion model describes how the system state evolves from one time step to the next, while the measurement model specifies how the hidden state gives rise to observable sensor readings.\n",
    "\n",
    "State and observations: In a typical vehicle tracking example, the state vector is $x_t = (p_x, p_y, v_x, u_y) \\in \\mathbb{R}^4$ where $(p_x, p_y)$ denotes position and $(u_x, u_y)$ denotes velocity. The measurement vector is $y_t \\in \\mathbb{R}^2$  representing observed position data, for example from a GPS sensor.\n",
    "\n",
    "Inputs and noise: The model includes both control inputs and stochastic disturbances:\n",
    "\n",
    "- $u_t$ is a known control input, such as commanded acceleration.  \n",
    "- $w_t$ is process (motion) noise, capturing unmodeled dynamics and random disturbances. It is typically modeled as $w_t \\sim \\mathcal{N}(0, Q)$ where $Q$ is the process noise covariance.\n",
    "- $v_t$ is measurement noise, capturing sensor inaccuracies, modeled as $v_t \\sim \\mathcal{N}(0, R)$ where $R$ is the measurement noise covariance.\n",
    "\n",
    "Interpretation of system matrices:\n",
    "- $A$ is the state transition matrix; it describes the deterministic evolution of the state.\n",
    "- $B$ maps the control input $u_t$ into the state.\n",
    "- $C$ maps the state to the measurement space, indicating which components of the state are observable.\n",
    "- $Q$ and $R$ quantify uncertainty in the motion and measurement models, respectively.\n",
    "\n",
    "Importantly, $A$, $B$, and $C$ describe system structure, not uncertainty; only $Q$ and $R$ are covariance matrices.\n",
    "\n",
    "### What is being estimated?\n",
    "\n",
    "At each time step $t$, the Kalman filter maintains two coupled quantities:\n",
    "\n",
    "- $\\hat x_{t+1|t}$: the estimate of the state after incorporating measurements up to time $t$,\n",
    "- $P_{t|t}$: the covariance of the estimation error,\n",
    "  $$\n",
    "  P_{t|t}\n",
    "  =\n",
    "  \\mathbb{E}\\big[(x_t - \\hat x_{t|t})(x_t - \\hat x_{t|t})^\\top\\big].\n",
    "  $$\n",
    "\n",
    "These quantities summarize all past information relevant for future estimation.\n",
    "\n",
    "The recursion is initialized with a prior belief\n",
    "\n",
    "$$\n",
    "x_0 \\sim \\mathcal{N}(\\hat x_{0|0}, P_{0|0}),\n",
    "$$\n",
    "\n",
    "which encodes initial knowledge and uncertainty about the system state.\n",
    "\n",
    "## The two-step recursion\n",
    "\n",
    "As in recursive least squares, estimation proceeds by alternating between a prediction based on the current model and a correction driven by new data. However, here both the estimate and its uncertainty are explicitly propagated through the system dynamics.\n",
    "\n",
    "### Step 1: Prediction (time update)\n",
    "\n",
    "Using the motion model, the filter predicts the next state before observing $y_t$:\n",
    "\n",
    "$$\n",
    "\\hat x_{t|t-1} = A \\hat x_{t-1|t-1} + B u_{t-1}.\n",
    "$$\n",
    "\n",
    "Uncertainty is propagated forward as\n",
    "\n",
    "$$\n",
    "P_{t|t-1} = A P_{t-1|t-1} A^\\top + Q.\n",
    "$$\n",
    "\n",
    "This step extrapolates the current belief through the dynamics and increases uncertainty to account for process noise.\n",
    "\n",
    "### Step 2: Update (measurement correction)\n",
    "\n",
    "Once a new measurement arrives, the filter computes the innovation (or surprise term):\n",
    "\n",
    "$$\n",
    "r_t = y_t - C \\hat x_{t|t-1},\n",
    "$$\n",
    "\n",
    "which measures the discrepancy between the observed measurement and its predicted value.\n",
    "\n",
    "The uncertainty associated with this discrepancy is\n",
    "\n",
    "$$\n",
    "S_t = C P_{t|t-1} C^\\top + R.\n",
    "$$\n",
    "\n",
    "The Kalman gain is then defined as\n",
    "\n",
    "$$\n",
    "K_t = P_{t|t-1} C^\\top S_t^{-1}.\n",
    "$$\n",
    "\n",
    "This gain determines how much the prediction should be corrected using the new measurement.\n",
    "\n",
    "The state estimate is updated according to\n",
    "\n",
    "$$\n",
    "\\hat x_{t|t} = \\hat x_{t|t-1} + K_t r_t,\n",
    "$$\n",
    "\n",
    "and the corresponding uncertainty is reduced as\n",
    "\n",
    "$$\n",
    "P_{t|t} = (I - K_t C) P_{t|t-1}.\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- Large measurement noise (large $R$) leads to a smaller Kalman gain, causing the filter to rely more heavily on the model prediction.\n",
    "- Large prediction uncertainty (large $P_{t|t-1}$) leads to a larger Kalman gain, causing the filter to rely more heavily on the measurement.\n",
    "\n",
    "Thus, the Kalman filter automatically balances model-based prediction and data-driven correction in an uncertainty-aware manner, extending the core ideas of recursive least squares to time-evolving dynamical systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c05d23",
   "metadata": {},
   "source": [
    "## 1. Problem setup  \n",
    "\n",
    "Give breief introd to Recursive leastime qaure estimator?\n",
    "\n",
    "Kalman alos include a motion model\n",
    "\n",
    "We model a discrete-time linear dynamical system:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{Motion Model}:\\\\\n",
    "x_{t+1} &= A x_t + B u_t + w_{t} \\\\\n",
    "{Measurement Model}:\\\\\n",
    "y_t &= C x_t + v_t,\n",
    "\\end{aligned}\n",
    "\\qquad t=0,\\dots,N-1\n",
    "$$\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^4$: state $=(p_x,p_y,u_x,u_y)$ (position + velocity)  \n",
    "- $u_t \\in \\mathbb{R}^2$: unknown input (drive force in x/y)  \n",
    "- $w_t \\in \\mathbb{R}^2$: Process or Mitoion noise $2_t ~Normal (0, Q_t)$\n",
    "- $y_t \\in \\mathbb{R}^2$: observed position (Measuring insturement such as GPS)\n",
    "- $v_t \\in \\mathbb{R}^2$: measurement noise  $v ~Norma (0, R_t$)$\n",
    "- A is covraiance matric?\n",
    "- B?\n",
    "- C?\n",
    "\n",
    "\n",
    "Question: how to best estimate the posiiton of car using initial estiamte and measurment both of whcih are noise?\n",
    "Kalman filter is an algorithm to estimate the state of the system using past and possibley observations and current and possibly noisy measurments of that systement - more specidfically?\n",
    "\n",
    "Its a two step process:\n",
    "1. Pedictition step\n",
    "   $X^hat_t+1 = A x_t + B_u_t$\n",
    "   $\n",
    "2. Update step\n",
    "\n",
    "Suprise term?\n",
    "Kalman Ratio?\n",
    "\n",
    "Oprimal estimation algorithm\n",
    "### Standard (quadratic) Kalman smoothing\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\{x_t,w_t,v_t\\}} &\\sum_{t=0}^{N-1} \\left(\\|w_t\\|_2^2 + \\tau\\,\\|v_t\\|_2^2\\right) \\\\\n",
    "\\text{s.t. } &x_{t+1}=Ax_t + Bw_t,\\quad y_t=Cx_t + v_t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Robust Kalman smoothing (Huber measurements)\n",
    "\n",
    "Define the Huber penalty\n",
    "$$\n",
    "\\phi_\\rho(a)=\n",
    "\\begin{cases}\n",
    "\\|a\\|_2^2, & \\|a\\|_2\\le \\rho \\\\\n",
    "2\\rho\\|a\\|_2 - \\rho^2, & \\|a\\|_2 > \\rho\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and solve\n",
    "$$\n",
    "\\min \\sum_{t=0}^{N-1}\\left(\\|w_t\\|_2^2 + \\tau\\,\\phi_\\rho(v_t)\\right)\n",
    "\\quad \\text{s.t. dynamics + measurement constraints.}\n",
    "$$\n",
    "\n",
    "Interpretation: small residuals are penalized quadratically, but large residuals only *linearly*, reducing outlier influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & reproducibility ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp\n",
    "\n",
    "SEED_MAIN = 6\n",
    "SEED_OUTLIERS = 0\n",
    "np.random.seed(SEED_MAIN)\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"CVXPY:\", cp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66223f82",
   "metadata": {},
   "source": [
    "## 2. Helper plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_state(ts, actual, estimated=None, suptitle=None):\n",
    "    \"\"\"Plot position, velocity, and input over time in x and y.\"\"\"\n",
    "    trajectories = [actual]\n",
    "    labels = [\"True\"]\n",
    "    if estimated is not None:\n",
    "        trajectories.append(estimated)\n",
    "        labels.append(\"Estimated\")\n",
    "\n",
    "    fig, ax = plt.subplots(3, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "    for (x, w), lab in zip(trajectories, labels):\n",
    "        ax[0,0].plot(ts, x[0,:-1], label=lab)\n",
    "        ax[0,1].plot(ts, x[1,:-1], label=lab)\n",
    "        ax[1,0].plot(ts, x[2,:-1], label=lab)\n",
    "        ax[1,1].plot(ts, x[3,:-1], label=lab)\n",
    "        ax[2,0].plot(ts, w[0,:], label=lab)\n",
    "        ax[2,1].plot(ts, w[1,:], label=lab)\n",
    "\n",
    "    ax[0,0].set_ylabel('x position')\n",
    "    ax[1,0].set_ylabel('x velocity')\n",
    "    ax[2,0].set_ylabel('x input')\n",
    "    ax[0,1].set_ylabel('y position')\n",
    "    ax[1,1].set_ylabel('y velocity')\n",
    "    ax[2,1].set_ylabel('y input')\n",
    "\n",
    "    for r in range(3):\n",
    "        ax[r,1].yaxis.tick_right()\n",
    "        ax[r,1].yaxis.set_label_position(\"right\")\n",
    "\n",
    "    ax[2,0].set_xlabel('time')\n",
    "    ax[2,1].set_xlabel('time')\n",
    "\n",
    "    ax[0,0].legend(loc=\"best\")\n",
    "    if suptitle:\n",
    "        fig.suptitle(suptitle, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_positions(traj, labels, axis=None):\n",
    "    \"\"\"Point clouds for (x,y) positions.\"\"\"\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "    n = len(traj)\n",
    "    fig, ax = plt.subplots(1, n, sharex=True, sharey=True, figsize=(4*n, 4))\n",
    "    if n == 1:\n",
    "        ax = [ax]\n",
    "    for i, x in enumerate(traj):\n",
    "        ax[i].plot(x[0,:], x[1,:], 'o', alpha=.15, markersize=4)\n",
    "        ax[i].set_title(labels[i])\n",
    "        ax[i].set_xlabel(\"x\")\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(\"y\")\n",
    "        if axis is not None:\n",
    "            ax[i].axis(axis)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2074000",
   "metadata": {},
   "source": [
    "## 3. Generate synthetic tracking data (with outliers)\n",
    "\n",
    "We simulate $N$ timesteps. The vehicle starts at the origin with zero velocity.\n",
    "\n",
    "- Inputs $w_t$ are standard Gaussian.\n",
    "- Measurement noise $v_t$ is standard Gaussian except that a fraction $p$ of timesteps are outliers with a much larger standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Problem size (change N if you want faster/slower solves) ---\n",
    "N = 600              # timesteps\n",
    "T = 50               # time horizon (seconds)\n",
    "ts, delt = np.linspace(0, T, N, endpoint=True, retstep=True)\n",
    "\n",
    "gamma = 0.05         # damping (0 = no damping)\n",
    "\n",
    "# Dynamics matrices (state: [px, py, vx, vy])\n",
    "A = np.zeros((4, 4))\n",
    "B = np.zeros((4, 2))\n",
    "C = np.zeros((2, 4))\n",
    "\n",
    "A[0,0] = 1\n",
    "A[1,1] = 1\n",
    "A[0,2] = (1 - gamma*delt/2) * delt\n",
    "A[1,3] = (1 - gamma*delt/2) * delt\n",
    "A[2,2] = 1 - gamma*delt\n",
    "A[3,3] = 1 - gamma*delt\n",
    "\n",
    "B[0,0] = delt2 / 2\n",
    "B[1,1] = delt2 / 2\n",
    "B[2,0] = delt\n",
    "B[3,1] = delt\n",
    "\n",
    "C[0,0] = 1\n",
    "C[1,1] = 1\n",
    "\n",
    "# Noise / outlier settings\n",
    "sigma_outlier = 20.0\n",
    "p_outlier = 0.20\n",
    "\n",
    "# Simulate\n",
    "np.random.seed(SEED_MAIN)\n",
    "x_true = np.zeros((4, N+1))\n",
    "x_true[:, 0] = [0, 0, 0, 0]\n",
    "y = np.zeros((2, N))\n",
    "\n",
    "w_true = np.random.randn(2, N)          # drive force\n",
    "v = np.random.randn(2, N)               # nominal measurement noise\n",
    "\n",
    "# Inject outliers in the measurement noise\n",
    "np.random.seed(SEED_OUTLIERS)\n",
    "outlier_mask = (np.random.rand(N) <= p_outlier)\n",
    "v[:, outlier_mask] = sigma_outlier * np.random.randn(2, N)[:, outlier_mask]\n",
    "\n",
    "# Roll forward\n",
    "for t in range(N):\n",
    "    y[:, t] = C @ x_true[:, t] + v[:, t]\n",
    "    x_true[:, t+1] = A @ x_true[:, t] + B @ w_true[:, t]\n",
    "\n",
    "print(f\"N={N}, outliers={outlier_mask.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d23e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positions(\n",
    "    traj=[x_true[:2, :-1], y],\n",
    "    labels=[\"True positions\", \"Noisy measurements\"],\n",
    "    axis=[-4, 14, -5, 20]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a07cde",
   "metadata": {},
   "source": [
    "## 4. Standard Kalman smoothing as least squares (CVXPY)\n",
    "\n",
    "We solve the quadratic objective (least squares). This tends to “chase” outliers because large residuals are penalized quadratically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False):\n",
    "    \"\"\"Quadratic Kalman smoothing as a QP.\"\"\"\n",
    "    n = y.shape[1]\n",
    "    x = cp.Variable((4, n+1))\n",
    "    w = cp.Variable((2, n))\n",
    "    v = cp.Variable((2, n))\n",
    "\n",
    "    obj = cp.Minimize(cp.sum_squares(w) + tau * cp.sum_squares(v))\n",
    "\n",
    "    constr = []\n",
    "    for t in range(n):\n",
    "        constr += [\n",
    "            x[:, t+1] == A @ x[:, t] + B @ w[:, t],\n",
    "            y[:, t]   == C @ x[:, t] + v[:, t],\n",
    "        ]\n",
    "\n",
    "    prob = cp.Problem(obj, constr)\n",
    "    try:\n",
    "        prob.solve(solver=solver, verbose=verbose)\n",
    "    except Exception:\n",
    "        prob.solve(verbose=verbose)\n",
    "\n",
    "    return np.array(x.value), np.array(w.value), prob.value, prob.status\n",
    "\n",
    "x_kf, w_kf, obj_kf, status_kf = solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False)\n",
    "print(\"status:\", status_kf)\n",
    "print(\"objective:\", obj_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(ts, (x_true, w_true), (x_kf, w_kf), suptitle=\"Standard (quadratic) Kalman smoothing\")\n",
    "plot_positions([x_true[:2, :-1], x_kf[:2, :-1]], [\"True positions\", \"KF recovery\"], axis=[-4, 14, -5, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11415e",
   "metadata": {},
   "source": [
    "## 5. Robust Kalman smoothing (Huber measurements)\n",
    "\n",
    "We replace the quadratic measurement penalty with a Huber penalty on the residual norm $\\|v_t\\|_2$.\n",
    "\n",
    "In CVXPY:\n",
    "`huber(norm(v[:,t]), rho)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa782f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False):\n",
    "    \"\"\"Robust Kalman smoothing with Huber measurement loss (conic program).\"\"\"\n",
    "    n = y.shape[1]\n",
    "    x = cp.Variable((4, n+1))\n",
    "    w = cp.Variable((2, n))\n",
    "    v = cp.Variable((2, n))\n",
    "\n",
    "    huber_terms = [tau * cp.huber(cp.norm(v[:, t], 2), rho) for t in range(n)]\n",
    "    obj = cp.Minimize(cp.sum_squares(w) + cp.sum(huber_terms))\n",
    "\n",
    "    constr = []\n",
    "    for t in range(n):\n",
    "        constr += [\n",
    "            x[:, t+1] == A @ x[:, t] + B @ w[:, t],\n",
    "            y[:, t]   == C @ x[:, t] + v[:, t],\n",
    "        ]\n",
    "\n",
    "    prob = cp.Problem(obj, constr)\n",
    "    try:\n",
    "        prob.solve(solver=solver, verbose=verbose)\n",
    "    except Exception:\n",
    "        prob.solve(verbose=verbose)\n",
    "\n",
    "    return np.array(x.value), np.array(w.value), prob.value, prob.status\n",
    "\n",
    "x_rkf, w_rkf, obj_rkf, status_rkf = solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False)\n",
    "print(\"status:\", status_rkf)\n",
    "print(\"objective:\", obj_rkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(ts, (x_true, w_true), (x_rkf, w_rkf), suptitle=\"Robust Kalman smoothing (Huber measurements)\")\n",
    "plot_positions([x_true[:2, :-1], x_rkf[:2, :-1]], [\"True positions\", \"Robust KF recovery\"], axis=[-4, 14, -5, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8f817",
   "metadata": {},
   "source": [
    "## 6. Quantitative comparison\n",
    "\n",
    "We compare RMSE of position and velocity:\n",
    "\n",
    "- Position RMSE uses $(p_x,p_y)$\n",
    "- Velocity RMSE uses $(u_x,u_y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5905abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a - b)2))\n",
    "\n",
    "pos_true = x_true[:2, :-1]\n",
    "vel_true = x_true[2:, :-1]\n",
    "\n",
    "pos_kf = x_kf[:2, :-1]\n",
    "vel_kf = x_kf[2:, :-1]\n",
    "\n",
    "pos_rkf = x_rkf[:2, :-1]\n",
    "vel_rkf = x_rkf[2:, :-1]\n",
    "\n",
    "metrics = {\n",
    "    \"KF_pos_RMSE\": rmse(pos_kf, pos_true),\n",
    "    \"RKF_pos_RMSE\": rmse(pos_rkf, pos_true),\n",
    "    \"KF_vel_RMSE\": rmse(vel_kf, vel_true),\n",
    "    \"RKF_vel_RMSE\": rmse(vel_rkf, vel_true),\n",
    "}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df34a2",
   "metadata": {},
   "source": [
    "## 7. Residual diagnostics (who is “chasing” outliers?)\n",
    "\n",
    "The estimated measurement residual is $\\hat v_t = y_t - C\\hat x_t$.  \n",
    "We compare residual norms for quadratic vs robust smoothing and mark outlier timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59eb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_norms(C, x_hat, y):\n",
    "    v_hat = y - (C @ x_hat[:, :-1])\n",
    "    return np.linalg.norm(v_hat, axis=0)\n",
    "\n",
    "r_kf = residual_norms(C, x_kf, y)\n",
    "r_rkf = residual_norms(C, x_rkf, y)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(ts, r_kf, label=\"KF residual norm\", alpha=0.9)\n",
    "plt.plot(ts, r_rkf, label=\"Robust KF residual norm\", alpha=0.9)\n",
    "plt.scatter(ts[outlier_mask], r_kf[outlier_mask], s=12, alpha=0.6, label=\"Outlier timesteps (KF)\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(r\"$\\|y_t - C\\hat x_t\\|_2$\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b1eb3",
   "metadata": {},
   "source": [
    "## 8. Hyperparameters ($\\tau$, $\\rho$): quick sweep\n",
    "\n",
    "- Larger $\tau$ forces the model to fit measurements more (less smoothing).\n",
    "- Smaller $\n",
    "ho$ makes the Huber loss switch to linear earlier (more robust).\n",
    "\n",
    "We do a tiny sweep (keep it small to avoid long runtimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [0.5, 1.0, 2.0, 4.0]\n",
    "rhos = [0.5, 1.0, 2.0]\n",
    "\n",
    "results = []\n",
    "for tau in taus:\n",
    "    for rho in rhos:\n",
    "        x_hat, w_hat, obj, status = solve_kalman_robust_huber(A, B, C, y, tau=tau, rho=rho, solver=\"SCS\", verbose=False)\n",
    "        pos_rmse = rmse(x_hat[:2, :-1], pos_true)\n",
    "        vel_rmse = rmse(x_hat[2:, :-1], vel_true)\n",
    "        results.append((tau, rho, pos_rmse, vel_rmse, status))\n",
    "\n",
    "results_sorted = sorted(results, key=lambda t: t[2])\n",
    "for tau, rho, prmse, vrmse, st in results_sorted[:8]:\n",
    "    print(f\"tau={tau:>4}, rho={rho:>3} | pos_RMSE={prmse:.3f}, vel_RMSE={vrmse:.3f} | status={st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcbaba",
   "metadata": {},
   "source": [
    "## 9. Takeaways\n",
    "\n",
    "1. Standard quadratic smoothing is optimal under Gaussian noise, but outliers can warp the estimate.\n",
    "2. A Huber measurement penalty is a clean convex way to robustify the estimator.\n",
    "3. CVXPY makes it easy to switch between models by changing only the objective.\n",
    "\n",
    "### Exercises (good for reports / assignments)\n",
    "- Set `p_outlier = 0` and verify quadratic and robust behave similarly.\n",
    "- Increase `sigma_outlier` and see when the quadratic model collapses.\n",
    "- Replace the Huber penalty with an $\\ell_1$ penalty on $v_t$ and compare.\n",
    "- Try different solvers (e.g., ECOS if available) and compare runtime/accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
