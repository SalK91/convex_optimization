# Lectures on Optimization

Welcome to *Lectures on Optimization*, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and implement modern optimization algorithms, with a strong focus on convex analysis and scalable algorithmic methods central to machine learning.

These notes are inspired by and draw heavily on material from:
- Stanford EE364A: Convex OptimizationI â€” *Stephen Boyd (2023)*  

The goal is not to reproduce the course, but to distill, reorganize, and extend its core ideas into a machine-learning-aligned, optimization-first framework, prioritizing:

- Convex problem structure, geometry, and theoretical principles
- Algorithmic design, convergence behavior, and computational efficiency
- Scalable and practical methods used in ML, deep learning, and NLP optimization

These lectures develop intuition, mathematical rigor, and practical algorithmic insight across major areas including:
- Convex sets, convex functions, and problem modeling
- Gradient, subgradient, Newton, and proximal methods
- Constrained optimization, duality, and KKT theory
- Decomposition, interior-point methods, and path-following algorithms
- Large-scale optimization strategies that power real-world machine learning systems


