# Lectures on Optimization

Welcome to *Lectures on Optimization*, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and implement modern optimization algorithms, with a strong focus on convex analysis and scalable algorithmic methods central to machine learning.

These notes are inspired by and draw heavily on material from:

- Convex Optimization— Stephen Boyd & Lieven Vandenberghe Cambridge University Press, 2004 [link](https://web.stanford.edu/~boyd/cvxbook/)
- Stanford EE364A: Convex Optimization I - Stephen Boyd (2023) [link](https://www.youtube.com/watch?v=kV1ru-Inzl4&list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h)
- Optimization: The University of Texas at Austin — Spring 2020 [link](https://www.youtube.com/watch?v=ee-HYD6kKqM&list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc)


The goal is not to reproduce the content from these courses, but to distill, reorganize, and extend its core ideas into a machine-learning-aligned, optimization-first framework, prioritizing:

- Convex problem structure, geometry, and theoretical principles
- Algorithmic design, convergence behavior, and computational efficiency
- Scalable and practical methods used in ML, deep learning, and NLP optimization

These lectures develop intuition, mathematical rigor, and practical algorithmic insight across major areas including:
- Convex sets, convex functions, and problem modeling
- Gradient, subgradient, Newton, and proximal methods
- Constrained optimization, duality, and KKT theory
- Decomposition, interior-point methods, and path-following algorithms
- Large-scale optimization strategies that power real-world machine learning systems


