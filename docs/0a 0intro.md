Modern optimization is geometric at heart. When we optimize a loss function, we are navigating through a high-dimensional landscape defined by vectors, matrices, subspaces, projections, and curvature. Without a strong grasp of these structures, optimization algorithms feel like “black boxes.” With right intuition you can start to see gradient descent not as a formula, but as a geometric motion toward feasibility or optimality.

This sectional presents the mathematical foundations of convex optimization with emphasis on these geometric structures. Vectors and matrices are treated as operators that shape feasible sets and descent directions. Norms and inner products define the geometry in which distances and angles are measured, influencing step sizes and stopping criteria. Projections and orthogonality appear naturally in constrained optimization. Smoothness and strong convexity provide curvature bounds that determine convergence rates. Spectral quantities such as eigenvalues and singular values quantify conditioning and guide algorithmic design.

By linking each mathematical concept directly to its role in optimization, the presentation aims to make algorithms understandable not as formal routines, but as geometric processes acting on structured spaces.
