GPT is trained through a multi-stage pipeline designed to gradually improve its knowledge, reasoning, alignment, and usefulness. This process consists of four key stages, each with its own purpose and techniques:

Pretraining: The model is trained on vast amounts of raw internet text to learn general language patterns, grammar, facts, and reasoning. At this stage, it becomes good at predicting the next word but is not yet fine-tuned for helpful or safe responses.


Supervised Fine-Tuning (SFT): Human experts provide high-quality examples of ideal question-answer pairs. The model learns to imitate helpful, context-aware, and instruction-following responses based on these demonstrations.

Reward Modeling (RM): Human trainers compare multiple model-generated responses and select the better one. Using these preferences, a reward model is trained to judge response quality. This model guides future optimization.
Reinforcement Learning (RLHF): The model generates responses, and the reward model scores them. Using reinforcement learning, the model improves itself to produce more helpful, safe, and aligned answers, maximizing quality and user satisfaction.

| Stage                      | Dataset                                                                                                                             | Algorithm                                                               | Model                          | Notes                                                                                |
| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------ | ------------------------------------------------------------------------------------ |
| **Pretraining**            | Raw internet<br>text trillions of words<br>low-quality, large quantity                                                              | Language modeling<br>predict the next token                             | Base model                     | 1000s of GPUs<br>months of training<br>ex: GPT, LLaMA, PaLM<br>can deploy this model |
| **Supervised Finetuning**  | Demonstrations<br>Ideal Assistant responses,<br>~10-100K (prompt, response)<br>written by contractors<br>low quantity, high quality | Language modeling<br>predict the next token                             | SFT model<br>init from base    | 1-100 GPUs<br>days of training<br>ex: Vicuna-13B<br>can deploy this model            |
| **Reward Modeling**        | Comparisons<br>100Kâ€“1M comparisons<br>written by contractors<br>low quantity, high quality                                          | Binary classification<br>predict rewards consistent<br>with preferences | RM model<br>init from SFT      | 1-100 GPUs<br>days of training                                                       |
| **Reinforcement Learning** | Prompts<br>~10K-100K prompts<br>written by contractors<br>low quantity, high quality                                                | Reinforcement Learning<br>generate tokens that maximize<br>the reward   | RL model<br>init from SFT & RM | 1-100 GPUs<br>days of training<br>ex: ChatGPT, Claude<br>can deploy this model       |
