# Chapter 6: Advanced Policy Gradient Methods

Policy Gradient methods are powerful but face several practical challenges:

- High variance in gradient estimates  
- Poor sample efficiency (data discarded after one update)  
- Sensitivity to step-size — large updates may collapse the policy  
- Policy updates happen in parameter space, not in “policy space” directly  
- Reusing old data (off-policy) is unstable and high variance  

This chapter introduces techniques to address these issues:

1. Baselines for variance reduction  
2. Advantage estimation (TD, n-step, GAE)  
3. Actor–Critic methods  
4. Performance guarantees and KL-divergence  
5. Proximal Policy Optimization (PPO)


## Policy Gradient Recap

In the previous chapter, we derived an expression for the gradient of the policy objective:

$$
\nabla_\theta V(\theta) \approx
\frac{1}{m} \sum_{i=1}^{m}
R(\tau^{(i)}) 
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)}),
$$

where each trajectory $\tau^{(i)}$ is generated by the current policy $\pi_\theta$.

This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:

- The return $R(\tau)$ depends on the entire trajectory.
- Different trajectories can have very different returns.
- Updates become noisy, unstable, and slow to converge.

In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.

Goal:  
Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.


## Variance Reduction with Baseline

To reduce the variance of the policy gradient estimator, we introduce a baseline function $b(s_t)$:

$$
\nabla_\theta \mathbb{E}_\tau [R] \;=\;
\mathbb{E}_\tau \left[
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\;
\left(\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\right)
\right].
$$

For any choice of baseline $b(s)$ (as long as it depends only on the state and not on the action), this gradient estimator remains unbiased — the expectation does not change, only the variance.


### What is a good baseline?

A near-optimal choice for $b(s_t)$ is the expected return from state $s_t$ under the current policy, i.e., the value function:

$$
b(s_t) \approx \mathbb{E}[r_t + r_{t+1} + \dots + r_{T-1}]
= V_\pi(s_t).
$$

Intuition: The baseline represents how well the policy *typically* does from that state. The update then measures how much better or worse the actual outcome was relative to this expectation.

> Increase $\log \pi_\theta(a_t \mid s_t)$ if the action did better than expected (positive advantage).  
> Decrease it if the action did worse than expected (negative advantage).

This keeps the estimator unbiased but significantly reduces variance, leading to more stable updates.


### The Advantage Function

This naturally leads to the advantage function:

$$
A(s_t, a_t) = Q(s_t,a_t) - V(s_t).
$$

- $Q(s_t, a_t)$: expected return if we take action $a_t$ in state $s_t$ and follow the policy thereafter.  
- $V(s_t)$: expected return from $s_t$ *on average* under the current policy.

So:

- If $A(s_t,a_t) > 0$, the action performed *better than expected*.  
- If $A(s_t,a_t) < 0$, the action performed *worse than expected*.  
- If $A(s_t,a_t) = 0$, the action was exactly as good as the policy’s average behavior.

The advantage isolates the *incremental contribution* of the chosen action and tells the policy gradient update how much credit or blame each action deserves.

Benefits:

1. Variance reduction: subtracting $V(s)$ removes large parts of the return that do not depend on the specific action.  
2. Focus on decisions that matter: only deviations from expected performance influence learning.  
3. Unbiased: since $V(s)$ does not depend on $a_t$, the expectation of the gradient remains unchanged.


### 2.3 Algorithm: Policy Gradient with Baseline (Advantage Estimation)

1: Initialize policy parameter $\theta$, baseline $b(s)$  
2: for iteration $= 1, 2, \dots$ do  
3: $\quad$ Collect a set of trajectories by executing the current policy $\pi_\theta$  
4: $\quad$ for each trajectory $\tau^{(i)}$ and each timestep $t$ do  
5: $\quad\quad$ Compute return:  
$\quad\quad\quad G_t^{(i)} = \sum_{t'=t}^{T-1} r_{t'}^{(i)}$  
6: $\quad\quad$ Compute advantage estimate:  
$\quad\quad\quad \hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})$  
7: $\quad$ end for  
8: $\quad$ Re-fit baseline by minimizing:  
$\quad\quad \sum_i \sum_t \big(b(s_t^{(i)}) - G_t^{(i)}\big)^2$  
9: $\quad$ Update policy parameters using gradient estimate:  
$\quad\quad \theta \leftarrow \theta + \alpha \sum_{i,t} \nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\, \hat{A}_t^{(i)}$  
10: $\quad$ (Plug into SGD or Adam optimizer)  
11: end for  


## 3. Actor–Critic: Combining Policy and Value Function

The key idea behind Actor–Critic methods is to further reduce the high variance of Monte Carlo policy gradients by using bootstrapping and function approximation, just like the improvement from Monte Carlo to TD learning.

Instead of estimating returns purely from full episode rollouts, a critic uses a learned value function to provide lower-variance, more sample-efficient advantage estimates.


### 3.1 Components

- Actor: the policy parameterization $\pi_\theta(a \mid s)$, which selects actions and updates policy parameters $\theta$.  
- Critic: a learned value function (either $V(s)$ or $Q(s,a)$) with parameters $w$, used to approximate returns and advantages.

Actor update:  

$$
\theta \leftarrow 
\theta + \alpha \, \nabla_\theta \log \pi_\theta(a_t \mid s_t) \, A_t.
$$

Here, the policy becomes more likely to choose actions that the critic believes have positive advantage.

Critic update (value-based, TD learning):

$$
w \leftarrow 
w + \beta \, \delta_t \, \nabla_w V(s_t; w),
$$

with the TD error:

$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t).
$$

The TD error $\delta_t$ measures how much better or worse the outcome was compared to the critic’s current prediction. It serves as a learning signal for the value function.


### 3.2 Intuition

- The actor collects data by running the current policy and choosing actions.  
- The critic evaluates those actions by estimating how good the resulting states and returns were.

We call it a *critic* because it provides an explicit evaluation of the policy’s performance. The actor proposes actions, and the critic judges whether they were better or worse than expected, guiding the policy update.

This mirrors the TD vs. MC trade-off:

- Monte Carlo (REINFORCE): unbiased but high variance.  
- Actor–Critic (TD-based advantage): slightly biased but lower variance and much more sample-efficient.

Thus, Actor–Critic methods offer the best of both worlds: faster learning, lower variance, and the ability to scale to large or continuous environments.


##  Target Estimation: TD, n-Step, and Monte Carlo

So far, we have seen two extremes for estimating returns:

- Monte Carlo return: roll out the episode until termination and use the full return  

  $$
  G_t = \sum_{k=t}^{T-1} r_{k+1}.
  $$

  This estimator is unbiased but typically has very high variance.

- One-step TD target: use a bootstrapped estimate based on the critic  

  $$
  \hat{R}_t^{(1)} = r_t + \gamma V(s_{t+1}).
  $$

  This estimator is low variance but biased, because it relies on an approximate value function.

These approaches represent opposite ends of a spectrum. The critic does not need to choose only one extreme. It can blend Monte Carlo and bootstrapping, leading to n-step return estimators, which interpolate smoothly between the TD target and the full Monte Carlo return.

In general:

- smaller $n$: more bias, lower variance (TD-like)  
- larger $n$: less bias, higher variance (MC-like)  


### One-Step TD Target

The 1-step return:

$$
\hat{R}_t^{(1)} = r_t + \gamma V(s_{t+1}),
$$

with advantage:

$$
\hat{A}_t^{(1)} = \hat{R}_t^{(1)} - V(s_t)
= r_t + \gamma V(s_{t+1}) - V(s_t).
$$

This is the lowest-variance estimator, but also the most biased.


### n-Step Return

The n-step return:

$$
\hat{R}_t^{(n)} =
r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1} r_{t+n-1}
+ \gamma^n V(s_{t+n}),
$$

with corresponding advantage:

$$
\hat{A}_t^{(n)} = \hat{R}_t^{(n)} - V(s_t).
$$

By choosing $n$, we control the bias–variance trade-off:

- Small $n$ → low variance, higher bias.  
- Large $n$ → high variance, lower bias.  


### Monte Carlo Return (∞-Step)

In the limit as $n \to \infty$, the n-step return becomes the Monte Carlo return:

$$
\hat{R}_t^{(\infty)} = G_t,
$$

and the advantage:

$$
\hat{A}_t^{(\infty)} = G_t - V(s_t).
$$

This is unbiased but has very high variance, especially in long or stochastic episodes.


## 5. Problems with Vanilla Policy Gradient Methods

Although policy gradient methods provide a clean and direct way to optimize policies, they suffer from several fundamental issues. These limitations motivate more advanced algorithms such as Actor–Critic, TRPO, and PPO.


### 5.1 Sample Inefficiency

Vanilla policy gradient methods are highly sample-inefficient:

- Each batch of collected trajectories is used for only one gradient step.  
- After the update, the entire batch is discarded.  
- To obtain another unbiased gradient estimate, the agent must collect fresh trajectories using the *new* policy.

This happens because the policy gradient is an on-policy expectation:

$$
\nabla_\theta J(\theta) = 
\mathbb{E}_{\tau \sim \pi_\theta}
\left[
\sum_t \nabla_\theta \log \pi_\theta(a_t \mid s_t) A_t
\right].
$$

Using data from old policies introduces bias if we naively treat it as if it came from the current policy. Thus, vanilla PG wastes data and requires many environment interactions — a major issue in real-world RL.


###  Using Data for Multiple Gradient Steps

A natural question arises: Can we reuse old trajectories to take multiple gradient steps?

This is highly desirable for sample efficiency. However:

- The gradient expectation is explicitly over trajectories sampled from the current policy.  
- Reusing trajectories from an old policy creates a distribution mismatch.

#### Why Reusing Old Data Causes Bias (Importance Sampling)

The policy gradient objective is an expectation under the current policy $\pi_\theta$:

$$
\nabla_\theta J(\theta)
= 
\mathbb{E}_{a \sim \pi_\theta}
\left[\nabla_\theta \log \pi_\theta(a \mid s) A_t\right].
$$

If we reuse data collected under an *old* policy $\pi$ to estimate the gradient of a *new* policy $\pi'$, we get:

- samples from $\pi$  
- objective defined under $\pi'$

This mismatch makes the gradient estimator biased unless we correct for it.

To fix this, we can rewrite expectations under $\pi'$ using samples from $\pi$ via importance sampling:

$$
\mathbb{E}_{a \sim \pi'}[f(a)]
=
\mathbb{E}_{a \sim \pi}
\left[
\frac{\pi'(a \mid s)}{\pi(a \mid s)} f(a)
\right].
$$

The term

$$
r_t = \frac{\pi'(a_t \mid s_t)}{\pi(a_t \mid s_t)}
$$

is the importance sampling ratio that reweights old samples so they behave as if they were drawn from the new policy $\pi'$.


#### What Importance Sampling Fixes (and What It Does Not)

Importance sampling corrects the mismatch in the action distribution between $\pi$ and $\pi'$ *for the same states*.  

However, it assumes that the state distribution is the same:

$$
d_\pi(s) \approx d_{\pi'}(s),
$$

which is generally false in RL, because changing the policy changes which states are visited.

So importance sampling fixes:

- action distribution mismatch (at a given state),

but not:

- state visitation distribution mismatch,  
- long-horizon compounding effects,  
- exploration differences.

Worse, when $\pi'$ and $\pi$ differ a lot, the ratio $r_t$ can be very large, leading to huge variance and unstable updates.

This is why naive importance sampling is rarely used directly for large policy updates.


### 5.3 Choosing a Step Size

Policy gradients perform stochastic gradient ascent:

$$
\theta_{k+1} = \theta_k + \alpha_k \hat{g}_k.
$$

The step size $\alpha_k$ is critical:

- If the step is too large, the policy can change drastically → performance collapse.  
- If the step is too small, learning becomes extremely slow.  
- The “correct” step size depends heavily on the current parameters and landscape.

Even with adaptive optimizers (Adam, RMSProp) or gradient normalization, a single bad update can cause the agent to fall off a performance cliff. Once a policy collapses, it may be very hard or impossible for vanilla policy gradient to recover.


### 5.4 Small Parameter Changes ≠ Small Policy Changes

A deeper issue: a small step in parameter space may produce a large change in the policy itself.

Example: Consider a two-action policy parameterized by a single scalar $\theta$:

$$
\pi_\theta(a=1) = \sigma(\theta), \quad 
\pi_\theta(a=2) = 1 - \sigma(\theta),
$$

where $\sigma$ is the logistic sigmoid.

As $\theta$ changes slightly, the action probabilities may change dramatically:

- At $\theta = 4$, the policy is nearly deterministic.  
- At $\theta = 2$, probabilities already shift significantly.  
- At $\theta = 0$, both actions become equally likely.

So:

- distance in parameter space does not correspond to distance in policy space.  
- a small update to $\theta$ can:
  - make the policy nearly deterministic,  
  - flip preferences between actions,  
  - cause large, unintended behavioral changes,  
  - lead to massive performance drops.

This is not just a step-size tuning problem. It is a fundamental mismatch between how we update parameters and how policies behave.

Because small parameter updates can produce large changes in the policy, the key question becomes:

> How do we design a policy update rule that limits how much the policy itself changes, not just the parameters?

This leads to trust-region ideas:

- Constrain the policy to not deviate too far from the previous version.  
- Measure distance between policies (e.g., KL divergence), not between parameters.  
- Update safely using a surrogate objective.  

These ideas form the foundation for TRPO, PPO, and other modern policy-gradient RL algorithms.


## 6. Safe Policy Improvement: Foundations for TRPO and PPO

To design *safer* and *more stable* policy gradient algorithms, we need tools to reason about:

1. How changing the policy affects performance.  
2. How to measure the “distance” between policies.  
3. How to constrain policy updates so we never change the policy too much.

These concepts are at the heart of Trust Region methods such as TRPO and PPO.


### 6.1 Policy Performance Difference Lemma

The Policy Performance Difference Lemma tells us how much performance changes when we switch from an old policy $\pi$ to a new policy $\pi'$:

$$
J(\pi') - J(\pi)
=
\frac{1}{1-\gamma}
\;\mathbb{E}_{s \sim d_{\pi'},\, a \sim \pi'}
\big[A_\pi(s,a)\big],
$$

where $d_{\pi'}(s)$ is the discounted state visitation distribution under $\pi'$:

$$
d_{\pi}(s) = (1-\gamma)
\sum_{t=0}^{\infty} \gamma^t P(s_t = s \mid \pi).
$$

Interpretation:

- If $\pi'$ tends to choose actions with positive advantage under $\pi$, it will outperform $\pi$.  
- This lemma provides an exact formula for comparing two policies.

However, computing $d_{\pi'}$ is usually impractical.  
So we build approximations using samples from the current policy $\pi$, leading to a surrogate objective.


### 6.2 KL Divergence — A Measure of Policy Change

We want to ensure that the new policy $\pi'$ does not deviate too much from $\pi$.  
A natural way to measure this is via the Kullback–Leibler (KL) divergence:

$$
D_{KL}(\pi' \,\|\, \pi)[s] 
=
\sum_{a} \pi'(a \mid s)
\log \frac{\pi'(a \mid s)}{\pi(a \mid s)}.
$$

Why KL?

- It is zero if the two policies are identical.  
- It grows as their action distributions diverge.  
- It heavily penalizes moving probability mass to actions that were previously unlikely.

If:

$$
D_{KL}(\pi' \,\|\, \pi)[s] \text{ is small for all } s,
$$

then the policy has changed only slightly — a safe trust-region update.

This directly addresses earlier issues:

- Prevents large behavioral changes.  
- Reduces risk of performance collapse.  
- Helps ensure that old trajectories remain relevant (state distributions remain similar).  


### 6.3 Importance Sampling in the Surrogate Objective

We want to evaluate or optimize a new policy $\pi'$ using data collected from an old policy $\pi$.

Using importance sampling, we derive the surrogate objective:

$$
L_\pi(\pi')
= 
\frac{1}{1-\gamma}
\mathbb{E}_{s,a \sim \pi}
\left[
\frac{\pi'(a \mid s)}{\pi(a \mid s)} A_\pi(s,a)
\right].
$$

Here, the ratio

$$
\frac{\pi'(a \mid s)}{\pi(a \mid s)}
$$

is exactly the importance sampling correction. It reweights samples from $\pi$ so they behave statistically as if they had been drawn from $\pi'$.

However, if $\pi'$ and $\pi$ are very different, this ratio can become very large, leading to:

- high-variance gradient estimates,  
- unstable learning,  
- potentially destructive policy updates.

This is why TRPO and PPO do not rely on naive importance sampling: they stabilize it using KL constraints or clipping.

Think of TRPO/PPO as:

> importance sampling  
> + mechanisms to keep the new policy close to the old one,  
> + guarantees or heuristics to prevent large, harmful changes.


### 6.4 Monotonic Policy Improvement Bound

Using the performance difference lemma and KL divergence, we can derive a monotonic improvement bound:

There exists a constant $C > 0$ such that:

$$
J(\pi') - J(\pi)
\ge
L_\pi(\pi')
-
C \cdot 
\mathbb{E}_{s \sim d_\pi}
\big[
D_{KL}(\pi' \,\|\, \pi)[s]
\big],
$$

where the surrogate objective is:

$$
L_\pi(\pi')
=
\frac{1}{1-\gamma}
\mathbb{E}_{s,a \sim \pi}
\left[
\frac{\pi'(a \mid s)}{\pi(a \mid s)} A_\pi(s,a)
\right].
$$

Interpretation:

- $L_\pi(\pi')$ tells us how good $\pi'$ *appears* based on samples from $\pi$ (using importance sampling).  
- The KL penalty term ensures $\pi'$ is not too far from $\pi$.  
- The inequality shows that as long as the KL divergence is controlled, improvements in the surrogate objective are linked to improvements in the true return.

This bound leads directly to algorithms that:

- maximize the surrogate objective $L_\pi(\pi')$,  
- while constraining the KL divergence $D_{KL}(\pi' \,\|\, \pi)$.

This is the core idea behind Trust Region Policy Optimization (TRPO) and later Proximal Policy Optimization (PPO).


## 7. Proximal Policy Optimization (PPO)

Proximal Policy Optimization (PPO) is a family of methods that approximate trust-region updates using simpler optimization objectives.

The main idea: improve vanilla policy gradient by restricting updates so that the new policy does not move too far from the old policy, thereby avoiding policy collapse, while still allowing multiple gradient steps on the same batch of data.


### 7.1 PPO Variant 1: KL Penalty Objective

One variant of PPO adds an explicit KL penalty to the surrogate objective:

$$
\theta_{k+1} =
\arg\max_\theta
\Big[
L_{\theta_k}(\theta) - \beta_k \, \bar{D}_{KL}(\theta \,\|\, \theta_k)
\Big],
$$

where:

- $L_{\theta_k}(\theta)$ is the surrogate objective computed using data from $\pi_{\theta_k}$,  
- $\bar{D}_{KL}$ is the average KL divergence between $\pi_\theta$ and $\pi_{\theta_k}$ over states,  
- $\beta_k$ is a penalty coefficient adjusted over iterations to enforce a target KL.

Formally:

$$
\bar{D}_{KL}(\theta \,\|\, \theta_k)
=
\mathbb{E}_{s \sim d_{\pi_k}} 
\big[ D_{KL}(\pi_\theta(\cdot \mid s) \,\|\, \pi_{\theta_k}(\cdot \mid s)) \big].
$$


#### Algorithm (PPO with KL Penalty)

1: Input: initial policy parameters $\theta_0$, initial KL penalty $\beta_0$, target KL-divergence $\delta$  
2: for $k = 0, 1, 2, \ldots$ do  
3: $\quad$ Collect set of partial trajectories $\mathcal{D}_k$ using policy $\pi_k = \pi(\theta_k)$  
4: $\quad$ Estimate advantages $\hat{A}^t_k$ using any advantage estimation algorithm  
5: $\quad$ Compute policy update by approximately solving  
$\quad\quad$ $\theta_{k+1} = \arg\max_\theta \; L_{\theta_k}(\theta) - \beta_k \hat{D}_{KL}(\theta \,\|\, \theta_k)$  
6: $\quad$ Implement this optimization with $K$ steps of minibatch SGD (e.g., Adam)  
7: $\quad$ Measure actual KL: $\hat{D}_{KL}(\theta_{k+1}\|\theta_k)$  
8: $\quad$ if $\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \ge 1.5\delta$ then  
9: $\quad\quad$ Increase penalty: $\beta_{k+1} = 2\beta_k$  
10: $\quad$ else if $\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \le \delta/1.5$ then  
11: $\quad\quad$ Decrease penalty: $\beta_{k+1} = \beta_k/2$  
12: $\quad$ end if  
13: end for  


### 7.2 PPO Variant 2: Clipped Surrogate Objective

The more widely used PPO variant uses a clipped objective that directly limits the importance sampling ratio.

Define the probability ratio:

$$
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_k}(a_t \mid s_t)}.
$$

The clipped objective is:

$$
\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)
= 
\mathbb{E}_{\tau \sim \pi_{\theta_k}}
\left[
\sum_{t=0}^{T}
\min\!\left(
r_t(\theta)\,\hat{A}^t_k,\;
\operatorname{clip}\!\left(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\right)\hat{A}^t_k
\right)
\right].
$$

The clipping function:

- prevents $r_t(\theta)$ from moving too far away from $1$,  
- stops updates that would excessively increase or decrease the probability of actions,  
- acts as a soft trust-region constraint.

Policy Update:

$$
\theta_{k+1} = \arg\max_{\theta} \, \mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta).
$$

Note: The ratio  

$$
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_k}(a_t \mid s_t)}
$$

is the importance sampling correction that allows trajectories collected under $\pi_{\theta_k}$ to be reused when updating $\pi_\theta$.  
Clipping this ratio keeps it close to 1, which:

- controls variance,  
- ensures stability,  
- preserves the validity of reusing old data.


#### PPO (Clipped) Algorithm

1: Input: initial policy parameters $\theta_0$, clipping threshold $\epsilon$  
2: for $k = 0, 1, 2, \ldots$ do  
3: $\quad$ Collect a set of partial trajectories $\mathcal{D}_k$ using policy $\pi_k = \pi(\theta_k)$  
4: $\quad$ Estimate advantages $\hat{A}^{\,t}_k$ using any advantage estimation algorithm (e.g., GAE)  
5: $\quad$ Define the clipped surrogate objective  
$\quad\quad$  
$$
\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)
= 
\mathbb{E}_{\tau \sim \pi_{\theta_k}}
\left[
\sum_{t=0}^{T}
\min\!\left(
r_t(\theta)\,\hat{A}^t_k,\;
\operatorname{clip}\!\left(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\right)\hat{A}^t_k
\right)
\right]
$$
6: $\quad$ Update policy parameters with several epochs of minibatch SGD to approximately maximize $\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)$  
7: $\quad$ Set $\theta_{k+1}$ to the resulting parameters  
8: end for  


## 8. Final Summary

| Method                | Key Idea                                 | Pros                     | Cons                     |
|-----------------------|-------------------------------------------|--------------------------|--------------------------|
| REINFORCE             | MC return-based policy gradient          | Simple, unbiased         | Very high variance       |
| Actor–Critic          | TD baseline value function               | More sample-efficient    | Requires critic          |
| Advantage Actor–Critic| Uses $A(s,a)$ for updates                | Best bias–variance trade | Needs accurate value est.|
| TRPO                  | Trust-region with KL constraint          | Strong theory, stable    | Complex, second-order    |
| PPO                   | Clipped/penalized surrogate objective    | Simple, stable, popular  | Heuristic, tuning needed |

Advanced policy gradient methods address:

- variance reduction (baselines, advantages),  
- sample efficiency (Actor–Critic, reuse of data),  
- stability and safety (KL constraints, clipping, trust regions),  

and form the foundation of modern deep RL algorithms used in practice (PPO, TRPO, A3C, IMPALA, SAC, etc.).


## Advanced Policy Gradient Methods — Mental Map

```text
                  Advanced Policy Gradient Methods
     Goal: Fix limitations of vanilla PG (variance, stability, KL control)
                               │
                               ▼
             Core Challenges in Policy Gradient Methods
       ┌────────────────────────────────────────────────────────┐
       │ High variance (MC returns)                             │
       │ Poor sample efficiency (on-policy only)                │
       │ Sensitive to step size → catastrophic policy collapse  │
       │ Small θ change ≠ small policy change                   │
       │ Reusing old data is unstable                           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Variance Reduction (Baselines)
       ┌────────────────────────────────────────────────────────┐
       │ Introduce baseline b(s) → subtract expectation         │
       │ Keeps estimator unbiased                               │
       │ Good choice: b(s)= V(s) → yields Advantage A(s,a)      │
       │ Update based on: how much action outperformed expected │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                       Advantage Function A(s,a)
       ┌────────────────────────────────────────────────────────┐
       │ A(s,a) = Q(s,a) – V(s)                                 │
       │ Measures how much BETTER the action was vs average     │
       │ Positive → increase πθ(a|s); Negative → decrease it    │
       │ Major variance reduction – foundation of Actor–Critic  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                         Actor–Critic Framework
       ┌────────────────────────────────────────────────────────┐
       │ Actor: policy πθ(a|s)                                  │
       │ Critic: value function V(s;w) estimates baseline       │
       │ TD error δt reduces variance (bootstrapping)           │
       │ Faster, more sample-efficient than REINFORCE           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Target Estimation for the Critic
       ┌────────────────────────────┬────────────────────────────┐
       │ Monte Carlo (∞-step)       │  TD (1-step)               │
       │ + Unbiased                 │  + Low variance            │
       │ – High variance            │  – Biased                  │
       ├────────────────────────────┴────────────────────────────┤
       │ n-Step Returns: Blend of TD and MC                     │
       │ Control bias–variance by choosing n                    │
       │ Larger n → MC-like; smaller n → TD-like                │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
             Fundamental Problems with Vanilla Policy Gradient
       ┌────────────────────────────────────────────────────────┐
       │ Uses each batch for ONE gradient step (on-policy)      │
       │ Step size is unstable → huge performance collapse      │
       │ Small changes in θ → large unintended policy changes   │
       │ Need mechanism to limit POLICY CHANGE, not θ change    │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
            Safe Policy Improvement Theory → TRPO & PPO
       ┌────────────────────────────────────────────────────────┐
       │ Policy Performance Difference Lemma                    │
       │   J(π') − J(π) = Eπ' [Aπ(s,a)]                        │
       │ KL Divergence as policy distance metric                │
       │   D_KL(π'||π) small → safe update                      │
       │ Monotonic Improvement Bound                            │
       │   Lower bound on J(π') using surrogate loss Lπ(π')     │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                   Surrogate Objective for Safe Updates
       ┌────────────────────────────────────────────────────────┐
       │ Lπ(π') = E[ (π'(a|s)/π(a|s)) * Aπ(s,a) ]               │
       │ Importance sampling + KL regularization                │
       │ Foundation of Trust-Region Policy Optimization (TRPO)  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
              Proximal Policy Optimization (PPO) – Key Ideas
       ┌────────────────────────────┬────────────────────────────┐
       │ PPO-KL Penalty             │ PPO-Clipped Objective      │
       │ Adds β·KL to loss          │ Clips ratio r_t(θ) to      │
       │ Adjust β adaptively        │ [1−ε, 1+ε] to prevent      │
       │ Prevents large updates     │ destructive policy jumps   │
       └────────────────────────────┴────────────────────────────┘
                               │
                               ▼
                         PPO Algorithm Summary
       ┌────────────────────────────────────────────────────────┐
       │ 1. Collect trajectories from old policy                │
       │ 2. Estimate advantages Â_t (GAE, TD, etc.)            │
       │ 3. Optimize clipped surrogate for many epochs          │
       │ 4. Update parameters safely                            │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                          Final Outcome (Chapter 6)
       ┌────────────────────────────────────────────────────────┐
       │ Stable and efficient policy optimization               │
       │ Reuse data safely across multiple updates              │
       │ Avoid catastrophic policy collapse                     │
       │ Foundation of modern deep RL algorithms                │
       │ (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    │
       └────────────────────────────────────────────────────────┘
```