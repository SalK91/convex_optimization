{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning-optimisation-and-algorithms","title":"Mathematics for Machine Learning - Optimisation and Algorithms","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing.</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1:  Introduction and Overview","text":"<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p> <ol> <li>No bad local minima: any local minimum is also the global minimum.  </li> <li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li> <li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li> </ol> <p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p>"},{"location":"convex/11_intro/#motivation-optimization-in-machine-learning","title":"Motivation: Optimization in Machine Learning","text":"<p>Many supervised learning problems can be written in a common form:</p> \\[ \\min_{x \\in \\mathcal{X}}  \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i)  + \\lambda R(x), \\] <p>where</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function that measures how well the model predicts \\(b_i\\) from \\(a_i\\),  </li> <li>\\(R(x)\\) is a regularizer that encourages certain structure (such as sparsity or small weights),  </li> <li>\\(\\mathcal{X}\\) is a set of allowed parameter values, often simple and convex.</li> </ul> <p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the \\(\\ell_1\\) norm, and the \\(\\ell_2\\) norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>"},{"location":"convex/11_intro/#why-convex-optimization-remains-central-in-ml","title":"Why Convex Optimization Remains Central in ML","text":"<p>Although many modern models are nonconvex, convex optimization continues to play a major role:</p> <ol> <li> <p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the \\(0\\text{\u2013}1\\) loss. They make optimization practical while still leading to models that generalize well.</p> </li> <li> <p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p> </li> </ol> <p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>"},{"location":"convex/11_intro/#web-book-roadmap-and-how-to-use-it","title":"Web-Book Roadmap and How to Use It","text":"<ul> <li> <p>Chapter 2: Linear Algebra Foundations. Basic vector/matrix operations and linear algebra needed for optimization.</p> </li> <li> <p>Chapter 3: Multivariable Calculus. Differentiation and derivatives of functions of many variables (gradients, Hessians).</p> </li> <li> <p>Chapter 4: Convex Sets and Geometry. Definitions and examples of convex sets, cones, affine spaces, and geometric properties.</p> </li> <li> <p>Chapter 5: Convex Functions. Convexity for functions, epigraphs, and key examples (norms, quadratic functions, log-sum-exp, etc.).</p> </li> <li> <p>Chapter 6: Nonsmooth Optimization \u2013 Subgradients. Generalized derivatives for convex functions that are not differentiable, and subgradient methods.</p> </li> <li> <p>Chapter 7: First-Order Optimality Conditions. Gradient-based optimality for smooth problems, supporting theory for necessary and sufficient conditions.</p> </li> <li> <p>Chapter 8: Optimization Principles \u2013 From Gradient Descent to KKT. Unconstrained and constrained gradient methods, culminating in the Karush\u2013Kuhn\u2013Tucker (KKT) conditions.</p> </li> <li> <p>Chapter 9: Lagrange Duality Theory. Duality principles, weak/strong duality, and interpretations of Lagrange multipliers.</p> </li> <li> <p>Chapter 10: Pareto Optimality and Multi-Objective Optimization. Trade-offs in optimizing multiple goals and Pareto efficiency.</p> </li> <li> <p>Chapter 11: Regularized Approximation. Balancing fit vs. complexity with regularization (\u2113\u2081, \u2113\u2082, elastic net, etc.).</p> </li> <li> <p>Chapter 12: Algorithms for Convex Optimization. General convex optimization solvers and algorithmic frameworks (interior-point, gradient methods, etc.).</p> </li> <li> <p>Chapter 13: Equality-Constrained Problems. Specialized methods (e.g. KKT with only equalities, reduced-space methods).</p> </li> <li> <p>Chapter 14: Inequality-Constrained Problems. Algorithms handling general inequality constraints, barrier methods.</p> </li> <li> <p>Chapter 15: Advanced Large-Scale and Structured Methods. Techniques for very large or structured problems (decomposition, coordinate descent, etc.).</p> </li> <li> <p>Chapter 16: Modeling Patterns and Algorithm Selection. Practical guidance on modeling choices and selecting the right algorithm in practice.</p> </li> <li> <p>Chapter 17: Canonical Problems in Convex Optimization. Well-known problem templates (linear, quadratic, SOCP, SDP) and how to recognize them.</p> </li> <li> <p>Chapter 18: Modern Optimizers in Machine Learning Frameworks. How convex optimization appears in ML libraries and frameworks.</p> </li> <li> <p>Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization. Overview of nonconvex problems and global methods (to contrast with convex theory).</p> </li> <li> <p>Chapter 20: Derivative-Free and Black-Box Optimization. Techniques when gradients are not available.</p> </li> <li> <p>Chapter 21: Metaheuristic and Evolutionary Optimization. Heuristic algorithms (genetic algorithms, simulated annealing) for hard problems.</p> </li> <li> <p>Chapter 22: Advanced Topics in Combinatorial Optimization. Combinatorial optimization problems and convex relaxations.</p> </li> <li> <p>Chapter 23: The Future of Optimization \u2013 Learning, Adaptation, Intelligence. Emerging trends (e.g. data-driven optimization, connections to machine learning).</p> </li> </ul> <p>This roadmap helps the reader see how the material progresses from foundations (Ch.2\u20135) to theory (Ch.6\u201311) to algorithms (Ch.12\u201315) and on to specialized and modern topics (Ch.16\u201323).</p>"},{"location":"convex/11_intro/#acknowledgments","title":"Acknowledgments","text":"<p>The content and structure of this web book are strongly informed by the Stanford University course EE364A: Convex Optimization I, taught by Stephen Boyd. In particular, the presentation draws inspiration from the 2023 lecture notes and course materials, which are widely regarded as a foundational reference in modern convex optimization.</p>"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model \\(x\\) to data \\((A, b)\\) takes the form:</p> \\[ \\min_x \\ \\|A x - b\\|_2^2. \\] <p>Later in this chapter, we will see that this objective finds the point in the column space of \\(A\\) that is closest to \\(b\\). Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p> <p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas \u2014 projections, subspaces, orthogonality, eigenvalues, singular values, and norms \u2014 because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>"},{"location":"convex/12_vector/#vector-spaces-subspaces-and-affine-sets","title":"Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set of vectors that can be added and scaled without leaving the set. The familiar example is \\(\\mathbb{R}^n\\), where operations like \\(\\alpha x + \\beta y\\) keep us within the same space.</p> <p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are \u201cflat\u201d objects that always pass through the origin, such as lines or planes in \\(\\mathbb{R}^3\\). </p> <p>Affine sets extend this idea by allowing a shift away from the origin. A set \\(A\\) is affine if it contains the entire line passing through any two of its points. Equivalently, for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  \\(\\theta x + (1 - \\theta) y \\in A.\\) That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\). By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set. </p> <p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: \\(A = x_0 + S = \\{\\, x_0 + s : s \\in S \\,\\},\\) where \\(S\\) is a subspace and \\(x_0\\) is any point in the set. This representation is extremely useful in optimization. If \\(Ax = b\\) is a linear constraint, then its solution set is an affine set. A single particular solution \\(x_0\\) gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of \\(A\\). Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p> <p>Affine Transformations: An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as \\(f(x) = A x + b,\\) where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex. is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p> <p>In summary, vector spaces describe the space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the webbook.</p>"},{"location":"convex/12_vector/#linear-combinations-span-basis-dimension","title":"Linear combinations, span, basis, dimension","text":"<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p> <p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form is a linear combination. The set of all linear combinations is called the span:  The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p> <p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p> <ul> <li>A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). </li> <li>The number of basis vectors is the dimension \\(\\dim(V)\\).</li> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states: \\(\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n,\\) where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>Column Space: The column space of a matrix , denoted , is the set of all possible output vectors  that can be written as  for some . In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system  have a solution?\u201d is equivalent to asking whether . If  lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of , denoted , is the set of all input vectors  that are mapped to zero:  . It answers a different question: If a solution to  exists, is it unique? If the null space contains only the zero vector (), the solution is unique. But if  contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix  is a linear combination of others for example, \u2014the columns of  become linearly dependent. This creates a nonzero vector in the null space of , meaning multiple weight vectors  can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and  becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p> <p>Feasible Directions: In a constrained optimization problem of the form , the null space of  characterizes the directions along which one can move without violating the constraints. If , then moving from a feasible point  to  preserves feasibility, since  . Thus, the null space defines the space of free movement directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of , denoted , is the span of the rows of  (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to . The row space is orthogonal to the null space of :  .  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in  affect the constraints.</p> <p>Left Null Space: The left null space, denoted , is the set of all vectors  such that . These vectors are orthogonal to the columns of , and therefore orthogonal to the column space itself. In least squares problems,  represents residual directions\u2014components of  that cannot be explained by the model .</p> <p>Projection Interpretation (Least Squares):  When  has no exact solution (as in overdetermined systems), the least squares solution finds  such that  is the projection of  onto the column space of :  and the residual  lies in the left null space . This provides a geometric view: the solution projects  onto the closest point in the subspace that  can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of  is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where  is the number of columns of . This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in .</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by .  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map .</p>"},{"location":"convex/12_vector/#inner-products-and-orthogonality","title":"Inner products and orthogonality","text":"<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality. An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\): \\(\\(|\\langle x,y\\rangle| \\le \\|x\\|\\|y\\|~,\\)\\) with equality iff \\(x\\) and \\(y\\) are linearly dependent. Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#norms-and-distances","title":"Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms. Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of the \\(\\ell_1\\) norm is the \\(\\ell_\\infty\\) norm, and the dual of the \\(\\ell_2\\) norm is itself.</p> <p>The dual norm captures how large a linear functional can be when applied to vectors of bounded size. Geometrically, consider the unit ball \\(\\{x : \\|x\\|\\le 1\\}\\). The dot product \\(x^\\top y\\) measures how well the vector \\(x\\) aligns with \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible alignment between \\(y\\) and any vector \\(x\\) inside this unit ball.</p> <p>If \\(\\|y\\|_*\\) is large, then there exists a direction \\(x\\) that is small according to the original norm yet strongly aligned with \\(y\\), resulting in a large dot product. If \\(\\|y\\|_*\\) is small, then \\(y\\) is poorly aligned with all unit-norm vectors, and \\(x^\\top y\\) remains small for every feasible \\(x\\).</p> <p>This perspective explains common dual norm pairs. The dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), reflecting sensitivity to the largest coordinate, while the \\(\\ell_2\\) norm is self-dual due to rotational symmetry. Dual norms are fundamental in convex optimization, appearing in optimality conditions, error bounds, and regularization analysis.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. </p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>"},{"location":"convex/12_vector/#orthogonal-projections-and-least-squares","title":"Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\). If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#operator-norms-singular-values-and-spectral-structure","title":"Operator norms, singular values, and spectral structure","text":"<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>"},{"location":"convex/12_vector/#operator-norms","title":"Operator norms","text":"<p>Given a matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and norms \\(\\|\\cdot\\|_p\\) on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_q\\) on \\(\\mathbb{R}^m\\), the induced operator norm is defined as  This quantity measures the largest amount by which \\(A\\) can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p> <ul> <li>\\(\\|A\\|_{2 \\to 2}\\), the spectral norm, equals the largest singular value of \\(A\\).</li> <li>\\(\\|A\\|_{1 \\to 1}\\) is the maximum absolute column sum.</li> <li>\\(\\|A\\|_{\\infty \\to \\infty}\\) is the maximum absolute row sum.</li> </ul> <p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function  converges for step sizes \\(\\alpha &lt; 2 / \\|Q\\|_2\\). This shows that controlling the operator norm of the Hessian\u2014or a Lipschitz constant of the gradient\u2014directly governs how aggressively an algorithm can move.</p>"},{"location":"convex/12_vector/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a factorization  where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is diagonal with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots\\). The \\(\\sigma_i\\) are the singular values of \\(A\\).</p> <p>Geometrically, the SVD shows how \\(A\\) transforms the unit ball into an ellipsoid. The columns of \\(V\\) give the principal input directions, the singular values are the lengths of the ellipsoid\u2019s axes, and the columns of \\(U\\) give the output directions. The largest singular value \\(\\sigma_{\\max}\\) equals the spectral norm \\(\\|A\\|_2\\), while the smallest \\(\\sigma_{\\min}\\) describes the least expansion (or exact flattening if \\(\\sigma_{\\min} = 0\\)).</p> <p>SVD is a powerful diagnostic tool in optimization. The ratio  is the condition number of \\(A\\). A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means \\(A\\) behaves more like a uniform scaling, which is ideal for optimization.</p>"},{"location":"convex/12_vector/#low-rank-structure","title":"Low-rank structure","text":"<p>The rank of \\(A\\) is the number of nonzero singular values. When \\(A\\) has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p> <p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>"},{"location":"convex/12_vector/#mental-map","title":"Mental Map","text":"<pre><code>                 Linear Algebra Foundations for Convex Optimization\n               Geometry + Computation for Understanding Algorithms\n                                   \u2502\n                                   \u25bc\n                        Objects: Vectors, Matrices, Maps\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Vector Spaces / Subspaces / Affine Sets                      \u2502\n      \u2502  - Feasible sets of Ax=b are affine: x = x0 + N(A)            \u2502\n      \u2502  - Feasible directions live in the null space                 \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  The Four Fundamental Subspaces of A                            \u2502\n      \u2502  - Column space C(A): reachable outputs (Ax)                    \u2502\n      \u2502  - Null space N(A): indistinguishable inputs (Ax=0)             \u2502\n      \u2502  - Row space R(A): constraint directions in x-space             \u2502\n      \u2502  - Left null space N(A\u1d40): residual directions (orthogonal to C) \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Inner Products \u2192 Orthogonality \u2192 Projections                  \u2502\n      \u2502  - Defines angles/lengths                                      \u2502\n      \u2502  - Least squares = projection of b onto C(A)                   \u2502\n      \u2502  - QR / Gram\u2013Schmidt give stable numerical tools               \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Norms &amp; Dual Norms: \"How we measure size\"                     \u2502\n      \u2502  - Unit balls define geometry of constraints/regularizers      \u2502\n      \u2502  - Dual norms bound dot products and appear in optimality      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Spectral Structure: Eigenvalues, PSD, SVD                    \u2502\n      \u2502  - PSD \u21d4 convex quadratics (Hessians of quadratic objectives)\u2502\n      \u2502  - SVD shows stretching directions and conditioning           \u2502\n      \u2502  - Condition number \u2194 convergence speed / numerical stability \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p> <p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton\u2019s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p> <p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>"},{"location":"convex/13_calculus/#gradients-and-directional-derivatives","title":"Gradients and Directional Derivatives","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The function is differentiable at a point \\(x\\) if there exists a vector \\(\\nabla f(x)\\) such that  meaning that the linear function \\(h \\mapsto \\nabla f(x)^\\top h\\) provides the best local approximation to \\(f\\) near \\(x\\). The gradient is the unique vector with this property.</p> <p>A closely related concept is the directional derivative. For any direction \\(v \\in \\mathbb{R}^n\\), the directional derivative of \\(f\\) at \\(x\\) in the direction \\(v\\) is  If \\(f\\) is differentiable, then  Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction \\(v\\) tells us how rapidly \\(f\\) increases when we move infinitesimally along \\(v\\).</p> <p>This immediately yields an important geometric fact. Among all unit directions \\(u\\),  is maximized when \\(u\\) points in the direction of \\(\\nabla f(x)\\), the direction of steepest ascent. The steepest descent direction is therefore \\(-\\nabla f(x)\\), which motivates gradient-descent algorithms for minimizing functions.</p> <p>For any real number \\(c\\), the level set of \\(f\\) is   </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#jacobians","title":"Jacobians","text":"<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>"},{"location":"convex/13_calculus/#from-derivative-to-gradient","title":"From derivative to gradient","text":"<p>For a scalar function , differentiability means that near any point ,  The gradient vector  collects all partial derivatives. Each component measures how sensitive \\(f\\) is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>"},{"location":"convex/13_calculus/#from-gradient-to-jacobian","title":"From gradient to Jacobian","text":"<p>Now consider a vector-valued function \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\),  Each output \\(F_i\\) has its own gradient. Stacking these row vectors yields the Jacobian matrix:  </p> <p>The Jacobian provides the best linear approximation of \\(F\\) near \\(x\\):  Thus, locally, the nonlinear map \\(F\\) behaves like the linear map \\(h \\mapsto J_F(x)h\\). A small displacement \\(h\\) in input space is transformed into an output change governed by the Jacobian.</p>"},{"location":"convex/13_calculus/#interpreting-the-jacobian","title":"Interpreting the Jacobian","text":"Component of \\(J_F(x)\\) Meaning Row \\(i\\) Gradient of output \\(F_i(x)\\): how the \\(i\\)-th output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to \\(x_j\\): how varying input \\(x_j\\) affects the entire output vector. Determinant (when \\(m=n\\)) Local volume scaling: how \\(F\\) expands or compresses space near \\(x\\). Rank Local dimension of the image: whether any input directions are lost or collapsed. <p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>"},{"location":"convex/13_calculus/#the-hessian-and-curvature","title":"The Hessian and Curvature","text":"<p>For a twice\u2013differentiable function , the Hessian matrix collects all second-order partial derivatives:  </p> <p>The Hessian describes the local curvature of the function. While the gradient indicates the direction of steepest change, the Hessian tells us how that directional change itself varies\u2014whether the surface curves upward, curves downward, or remains nearly flat.</p>"},{"location":"convex/13_calculus/#curvature-and-positive-definiteness","title":"Curvature and positive definiteness","text":"<p>The eigenvalues of the Hessian determine its geometric behavior:</p> <ul> <li>If  (all eigenvalues nonnegative), the function is locally convex near \\(x\\).  </li> <li>If , the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li> <li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li> </ul> <p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>"},{"location":"convex/13_calculus/#example-quadratic-functions","title":"Example: Quadratic functions","text":"<p>Consider the quadratic function  where \\(Q\\) is symmetric. The gradient and Hessian are  Setting the gradient to zero gives the stationary point  If \\(Q \\succ 0\\), the solution  is the unique minimizer. The Hessian \\(Q\\) being positive definite confirms strict convexity.</p> <p>The eigenvalues of \\(Q\\) also explain the difficulty of minimizing \\(f\\):</p> <ul> <li>Large eigenvalues produce very steep, narrow directions\u2014optimization methods must take small steps.  </li> <li>Small eigenvalues produce flat directions\u2014progress is slow, especially for gradient descent.  </li> </ul> <p>The ratio of largest to smallest eigenvalue, the condition number, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>"},{"location":"convex/13_calculus/#why-the-hessian-matters-in-optimization","title":"Why the Hessian matters in optimization","text":"<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p> <ul> <li>Newton\u2019s method uses the Hessian to rescale directions, effectively \u201cwhitening\u2019\u2019 curvature and often converging rapidly.  </li> <li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li> <li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li> </ul> <p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>"},{"location":"convex/13_calculus/#taylor-approximation","title":"Taylor approximation","text":"<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>"},{"location":"convex/13_calculus/#first-order-approximation","title":"First-order approximation","text":"<p>If \\(f\\) is differentiable at \\(x\\), then for small steps \\(d\\),  The gradient gives the best linear model of the function near \\(x\\). This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>"},{"location":"convex/13_calculus/#second-order-approximation","title":"Second-order approximation","text":"<p>If \\(f\\) is twice differentiable, we can include curvature information:  The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p> <ul> <li>If , the quadratic term is nonnegative and the function curves upward\u2014locally bowl-shaped.</li> <li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others\u2014characteristic of saddle points.</li> </ul>"},{"location":"convex/13_calculus/#role-in-optimization-algorithms","title":"Role in optimization algorithms","text":"<p>Second-order Taylor models are the basis of Newton-type methods. Newton\u2019s method chooses \\(d\\) by approximately minimizing the quadratic model,  which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p> <p>Thus, Taylor expansions connect a function\u2019s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>"},{"location":"convex/13_calculus/#smoothness-and-strong-convexity","title":"Smoothness and Strong Convexity","text":"<p>In optimization, the behavior of a function\u2019s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>"},{"location":"convex/13_calculus/#lipschitz-continuous-gradients-l-smoothness","title":"Lipschitz continuous gradients (L-smoothness)","text":"<p>A differentiable function  has an \\(L\\)-Lipschitz continuous gradient if  This condition limits how quickly the gradient can change. Intuitively, an \\(L\\)-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:  This inequality states that every \\(L\\)-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p> <p>In gradient descent, smoothness directly determines a safe step size: choosing  ensures that each update decreases the function value for convex objectives. In machine learning, the constant \\(L\\) effectively controls how large the learning rate can be before training becomes unstable.</p>"},{"location":"convex/13_calculus/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function  is -strongly convex if, for some ,  This condition guarantees that \\(f\\) has at least \\(\\mu\\) amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p> <p>Strong convexity has major optimization implications:</p> <ul> <li>The minimizer is unique.  </li> <li>Gradient descent converges linearly with step size \\(\\eta \\le 1/L\\).  </li> <li>The ratio \\(L / \\mu\\) (the condition number) dictates convergence speed.</li> </ul>"},{"location":"convex/13_calculus/#curvature-in-both-directions","title":"Curvature in both directions","text":"<p>Together, smoothness and strong convexity bound the curvature of \\(f\\):  Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p> <p>These concepts, imiting curvature from above via \\(L\\) and from below via \\(\\mu\\), form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>"},{"location":"convex/13_calculus/#mental-map","title":"Mental map","text":"<pre><code>                Multivariable Calculus for Optimization\n        How objectives change, how curvature shapes algorithms\n                              \u2502\n                              \u25bc\n                 Local change of a scalar function f(x)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Differentiability &amp; First-Order Model                     \u2502\n     \u2502 f(x+h) = f(x) + \u2207f(x)\u1d40h + o(\u2016h\u2016)                          \u2502\n     \u2502 - \u2207f(x): best linear approximation                        \u2502\n     \u2502 - Directional derivative: D_v f(x) = \u2207f(x)\u1d40v              \u2502\n     \u2502 - Steepest descent: move along -\u2207f(x)                     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Geometry of Level Sets                                      \u2502\n     \u2502 L_c = {x : f(x)=c}                                          \u2502\n     \u2502 - If \u2207f(x) \u2260 0, then \u2207f(x) \u27c2 level set at x                 \u2502\n     \u2502 - Connects to constrained optimality (later: KKT)           \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Vector-Valued Maps &amp; Jacobians                              \u2502\n     \u2502 F: \u211d\u207f \u2192 \u211d\u1d50                                                  \u2502\n     \u2502 - Jacobian J_F(x) stacks gradients of outputs               \u2502\n     \u2502 - Linearization: F(x+h) \u2248 F(x) + J_F(x) h                   \u2502\n     \u2502 - Chain rule foundation for backprop / sensitivity analysis \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Structure: Hessian &amp; Curvature               \u2502\n     \u2502 \u2207\u00b2f(x): matrix of second partials                         \u2502\n     \u2502 - Curvature along v: v\u1d40\u2207\u00b2f(x)v                            \u2502\n     \u2502 - Eigenvalues quantify steep/flat directions              \u2502\n     \u2502 - PSD/PD Hessian ties directly to convexity (Ch.5)        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Taylor Models \u2192 Algorithm Design                          \u2502\n     \u2502 First-order:  f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d                     \u2502\n     \u2502 Second-order: f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d + \u00bd d\u1d40\u2207\u00b2f(x)d       \u2502\n     \u2502 - Gradient descent uses the linear model                  \u2502\n     \u2502 - Newton uses the quadratic model: d \u2248 -(\u2207\u00b2f)^{-1}\u2207f      \u2502\n     \u2502 - Trust-region / quasi-Newton approximate curvature       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Global Control of Local Behavior: Smoothness &amp; Strong Convexity\u2502\n     \u2502 L-smooth: \u2016\u2207f(x)-\u2207f(y)\u2016 \u2264 L\u2016x-y\u2016                               \u2502\n     \u2502 - Descent Lemma gives a quadratic upper bound                  \u2502\n     \u2502 - Sets safe step size: \u03b7 \u2264 1/L (for convex objectives)         \u2502\n     \u2502 \u03bc-strongly convex: f lies above tangents by (\u03bc/2)\u2016y-x\u2016\u00b2        \u2502\n     \u2502 - Unique minimizer, linear convergence of gradient descent     \u2502\n     \u2502 Combined curvature bounds: \u03bcI \u2aaf \u2207\u00b2f(x) \u2aaf LI                   \u2502\n     \u2502 - Condition number \u03ba = L/\u03bc governs difficulty                  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p> <p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>"},{"location":"convex/14_convexsets/#convex-sets","title":"Convex sets","text":"<p>A set  is convex if for any two points  and any ,  That is, the entire line segment between \\(x\\) and \\(y\\) lies inside the set. Convex sets have no \u201choles\u201d or \u201cindentations,\u201d and this geometric regularity is what makes optimization over them tractable.</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>Affine subspaces: .  </li> <li>Halfspaces: .  </li> <li>Euclidean balls: .  </li> <li>  balls (axis-aligned boxes): .  </li> <li>Probability simplex: .  </li> </ul> <p>A set fails to be convex whenever some segment between two feasible points leaves the set\u2014for example, a crescent or an annulus.</p>"},{"location":"convex/14_convexsets/#affine-sets-hyperplanes-and-halfspaces","title":"Affine sets, hyperplanes, and halfspaces","text":"<p>Affine sets generalize linear subspaces by allowing a shift. A set \\(A\\) is affine if for some point \\(x_0\\) and subspace \\(S\\),  Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p> <p>A hyperplane is an affine set defined by a single linear equation:  Hyperplanes act as the \u201cflat boundaries\u201d of higher-dimensional space and are the fundamental building blocks of polyhedra.</p> <p>A halfspace is one side of a hyperplane:  Halfspaces are convex and serve as basic local approximations to general convex sets.</p>"},{"location":"convex/14_convexsets/#convex-combinations-and-convex-hulls","title":"Convex combinations and convex hulls","text":"<p>A convex combination of points  is a weighted average  Convex sets are precisely those that contain all convex combinations of their points.</p> <p>The convex hull of a set \\(S\\), denoted \\(\\operatorname{conv}(S)\\), is the set of all convex combinations of finitely many points in \\(S\\). It is the smallest convex set containing \\(S\\). Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p> <p>Convex hulls are important because:</p> <ul> <li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li> <li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li> </ul>"},{"location":"convex/14_convexsets/#polyhedra-and-polytopes","title":"Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are always convex; they may be bounded or unbounded.</p> <p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>"},{"location":"convex/14_convexsets/#extreme-points","title":"Extreme points","text":"<p>Let  be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if  implies .</p> <p>Geometrically, extreme points are the \u201ccorners\u201d of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems\u2014such as linear programs\u2014achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>"},{"location":"convex/14_convexsets/#cones","title":"Cones","text":"<p>Cones generalize the idea of \u201cdirections\u201d in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>"},{"location":"convex/14_convexsets/#basic-definition","title":"Basic definition","text":"<p>A set \\(K \\subseteq \\mathbb{R}^n\\) is a cone if  A cone is convex if it is also closed under addition:  </p> <p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant , each coordinate axis direction is an extreme ray.</p>"},{"location":"convex/14_convexsets/#conic-hull","title":"Conic hull","text":"<p>Given any set \\(S\\), its conic hull is the set of all conic combinations:  This is the smallest convex cone containing \\(S\\). Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>"},{"location":"convex/14_convexsets/#polar-cones","title":"Polar cones","text":"<p>For a cone \\(K\\), the polar cone is defined as  </p> <p>Intuition:</p> <ul> <li>Polar vectors make a nonacute angle with every vector in \\(K\\).  </li> </ul> <p>Key properties:</p> <ul> <li>\\(K^\\circ\\) is always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, then \\(K^\\circ\\) is the orthogonal complement.  </li> <li>For any closed convex cone, </li> </ul> <p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>"},{"location":"convex/14_convexsets/#tangent-cones","title":"Tangent cones","text":"<p>For a set \\(C\\) and a point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) consists of all feasible \u201cinfinitesimal directions\u201d from \\(x\\):  </p> <p>Intuition:</p> <ul> <li>At an interior point, \\(T_C(x) = \\mathbb{R}^n\\): all small moves are allowed.  </li> <li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li> </ul> <p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>"},{"location":"convex/14_convexsets/#normal-cones","title":"Normal cones","text":"<p>For a convex set \\(C\\), the normal cone at a point \\(x \\in C\\) is  </p> <p>Interpretation:</p> <ul> <li>Every \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\).  </li> <li>At interior points, the normal cone is \\(\\{0\\}\\).  </li> <li>At boundary or corner points, it becomes a pointed cone of outward normals.</li> </ul> <p>A fundamental relationship ties tangent and normal cones together:  </p> <p>Normal cones appear directly in first-order optimality conditions. For a constrained problem  a point \\(x^*\\) is optimal only if  This expresses a balance between the objective\u2019s slope and the \u201cpushback\u2019\u2019 from the constraint set.</p> <p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplanes-and-separation","title":"Supporting Hyperplanes and Separation","text":"<p>One of the most important geometric facts about convex sets is that they can be supported or separated by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting Hyperplane Theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be nonempty, closed, and convex, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero vector \\(a\\) such that</p> \\[ a^\\top x \\le a^\\top x_0 \\qquad \\forall x \\in C. \\] <p>This means that the hyperplane</p> \\[ a^\\top x = a^\\top x_0 \\] <p>touches \\(C\\) at \\(x_0\\) but does not cut through it. The vector \\(a\\) is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating Hyperplane Theorem","text":"<p>If \\(C\\) and \\(D\\) are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector \\(a\\) and scalar \\(b\\) such that</p> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\qquad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <p>The hyperplane \\(a^\\top x = b\\) places all points of \\(C\\) on one side and all points of \\(D\\) on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p> <p>## Mental Map</p> <p>```text                Convex Sets &amp; Geometric Fundamentals      Feasible regions, geometry of constraints, and separation                               \u2502                               \u25bc                  Core idea: convexity removes \"bad geometry\"         (segments stay inside \u2192 no holes/indentations \u2192 tractable)                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Definition of Convex Set                                  \u2502      \u2502 C convex \u21d4  \u03b8x + (1-\u03b8)y \u2208 C  for all x,y\u2208C, \u03b8\u2208[0,1]      \u2502      \u2502 - Geometry: every chord lies inside                       \u2502      \u2502 - Optimization: feasible region supports global reasoning \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Affine Geometry: the \"flat\" building blocks               \u2502      \u2502 - Affine set: x0 + S                                      \u2502      \u2502 - Hyperplane: {x : a\u1d40x = b}                               \u2502      \u2502 - Halfspace:  {x : a\u1d40x \u2264 b}                               \u2502      \u2502 Role: linear constraints and local linear boundaries      \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Convex Combinations &amp; Convex Hull                         \u2502      \u2502 - Convex combination: \u03a3 \u03b8_i x_i, \u03b8_i\u22650, \u03a3\u03b8_i=1            \u2502      \u2502 - conv(S): all convex combos of points in S               \u2502      \u2502 Why it matters: convexification / relaxations / geometry  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Polyhedra &amp; Polytopes                                     \u2502      \u2502 - Polyhedron: intersection of finitely many halfspaces    \u2502      \u2502   P = {x : Ax \u2264 b}                                        \u2502      \u2502 - Polytope: bounded polyhedron                            \u2502      \u2502 Why it matters: LP feasible sets; two views (H- vs V-form)\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Extreme Points (Corners)                                     \u2502      \u2502 - x extreme \u21d4 cannot be written as nontrivial convex combo  \u2502      \u2502 - For polytopes: extremes = vertices                         \u2502      \u2502 Optimization link: linear objectives attain optima at corners\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Cones: scaling geometry for constraints &amp; duality         \u2502      \u2502 - Cone: x\u2208K, \u03b1\u22650 \u21d2 \u03b1x\u2208K                                  \u2502      \u2502 - Convex cone: also closed under addition                 \u2502      \u2502 - Conic hull cone(S): smallest convex cone containing S   \u2502      \u2502 - Extreme rays replace extreme points                     \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Local Directional Geometry at a Point x                     \u2502      \u2502 Tangent cone T_C(x): feasible infinitesimal directions      \u2502      \u2502 - Interior point: T_C(x)=\u211d\u207f                                 \u2502      \u2502 - Boundary: directions restricted                           \u2502      \u2502 Normal cone N_C(x): outward normals / supporting directions \u2502      \u2502 - Interior point: N_C(x)={0}                                \u2502      \u2502 - Boundary/corner: pointed cone of normals                  \u2502      \u2502 Duality relation: N_C(x) = (T_C(x))\u00b0                        \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Supporting Hyperplanes &amp; Separation                       \u2502      \u2502 Supporting hyperplane at boundary point x0:               \u2502      \u2502   \u2203a\u22600 s.t. a\u1d40x \u2264 a\u1d40x0  for all x\u2208C                       \u2502      \u2502 Separating hyperplane for disjoint convex sets C,D:       \u2502      \u2502   \u2203a,b s.t. a\u1d40x \u2264 b \u2264 a\u1d40y  for x\u2208C, y\u2208D                   \u2502      \u2502 Why it matters: geometry behind subgradients and duality  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>```</p>"},{"location":"convex/14_convexsets/#why-this-matters-for-optimisation","title":"Why This Matters for Optimisation","text":"<p>These geometric results are central to convex optimisation: - Subgradients correspond to supporting hyperplanes of the epigraph of a convex function. - Dual variables arise from separating infeasible points from the feasible region. - KKT conditions express the balance between the gradient of the objective and the normals of active constraints. - Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.  Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>"},{"location":"convex/15_convexfunctions/#definitions-of-convexity","title":"Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),  </p> <p>The graph of \\(f\\) never dips below the straight line between \\((x,f(x))\\) and \\((y,f(y))\\). If the inequality is strict whenever \\(x \\neq y\\), the function is strictly convex.</p> <p>A powerful geometric viewpoint comes from the epigraph:  The function \\(f\\) is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>"},{"location":"convex/15_convexfunctions/#first-order-characterisation","title":"First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if  </p> <p>Interpretation:</p> <ul> <li>The tangent plane at any point \\(x\\) lies below the function everywhere.</li> <li>\\(\\nabla f(x)\\) defines a supporting hyperplane to the epigraph.</li> <li>The gradient provides a global linear underestimator of \\(f\\).</li> </ul> <p>This geometric picture is crucial in optimisation: at a minimiser \\(x^\\star\\), convexity implies </p> <p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>"},{"location":"convex/15_convexfunctions/#second-order-characterisation","title":"Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then convexity can be checked via curvature:</p> \\[ f \\text{ is convex } \\iff \\nabla^2 f(x) \\succeq 0 \\text{ for all } x. \\] <ul> <li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li> <li>If \\(\\nabla^2 f(x) \\succ 0\\) everywhere, the function is strictly convex.  </li> <li>Negative eigenvalues indicate directions of negative curvature \u2014 impossible for convex functions.</li> </ul> <p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>"},{"location":"convex/15_convexfunctions/#examples-of-convex-functions","title":"Examples of convex functions","text":"<ol> <li> <p>Affine functions:     Always convex (and concave). They define supporting hyperplanes.</p> </li> <li> <p>Quadratic functions with PSD Hessian:     Convex because the curvature matrix \\(Q\\) is PSD.</p> </li> <li> <p>Norms:     All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p> </li> <li> <p>Maximum of affine functions:     Convex because the maximum of convex functions is convex.    (Important in SVM hinge loss.)</p> </li> <li> <p>Log-sum-exp:     A smooth approximation to the max; convex by Jensen\u2019s inequality. Appears in softmax, logistic regression, partition functions.</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#jensens-inequality","title":"Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex and \\(X\\) a random variable in its domain. Then:  </p> <p>This generalises the definition of convexity from finite averages to expectations. Practically:</p> <ul> <li>convex functions \u201cpull upward\u201d under averaging,</li> <li>log-sum-exp is convex because exponential is convex,</li> <li>EM and variational methods rely on Jensen to construct lower bounds.</li> </ul> <p>As a finite form, for \\(\\theta_i \\ge 0\\) with \\(\\sum \\theta_i = 1\\),  </p>"},{"location":"convex/15_convexfunctions/#operations-that-preserve-convexity","title":"Operations that preserve convexity","text":"<p>Convexity is preserved under many natural constructions:</p> <ul> <li> <p>Nonnegative scaling:   If \\(f\\) is convex and \\(\\alpha \\ge 0\\), then \\(\\alpha f\\) is convex.</p> </li> <li> <p>Addition:   If \\(f\\) and \\(g\\) are convex, then \\(f+g\\) is convex.</p> </li> <li> <p>Maximum: \\(\\max\\{f,g\\}\\) is convex.</p> </li> <li> <p>Affine pre-composition:   If \\(A\\) is a matrix,      is convex.</p> </li> <li> <p>Monotone composition rule:   If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex,   then \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex.</p> </li> </ul>"},{"location":"convex/15_convexfunctions/#level-sets-of-convex-functions","title":"Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), the sublevel set is  </p> <p>If \\(f\\) is convex, every sublevel set is convex. This property is crucial because inequalities \\(f(x) \\le \\alpha\\) are ubiquitous in constraints.</p> <p>Examples:</p> <ul> <li>Norm balls: \\(\\{ x : \\|x\\|_2 \\le r \\}\\) </li> <li>Linear regression confidence ellipsoids: \\(\\{ x : \\|Ax - b\\|_2 \\le \\epsilon \\}\\)</li> </ul> <p>These sets enable convex constrained optimisation formulations.</p>"},{"location":"convex/15_convexfunctions/#strict-and-strong-convexity","title":"Strict and strong convexity","text":""},{"location":"convex/15_convexfunctions/#strict-convexity","title":"Strict convexity","text":"<p>A function is strictly convex if  for all \\(x \\neq y\\) and \\(\\theta \\in (0,1)\\).</p> <p>Strict convexity implies unique minimisers.</p>"},{"location":"convex/15_convexfunctions/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function is \\(\\mu\\)-strongly convex if  </p> <p>Strong convexity adds quantitative curvature: the function grows at least quadratically away from its minimiser.</p> <p>Consequences:</p> <ul> <li>unique minimiser,</li> <li>gradient descent achieves linear convergence rate,   error shrinks as </li> <li>conditioning (\\(\\kappa = L/\\mu\\)) governs algorithmic difficulty.</li> </ul> <p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds \\(\\tfrac{\\lambda}{2}\\|x\\|_2^2\\)).</p>"},{"location":"convex/15_convexfunctions/#mental-map","title":"Mental Map","text":"<pre><code>                      Convex Functions\n      Objective landscapes with predictable geometry and guarantees\n                              \u2502\n                              \u25bc\n                Core idea: no bad local minima\n        (every local minimum is global; geometry is well-behaved)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Definition of Convexity                                   \u2502\n     \u2502 f(\u03b8x+(1\u2212\u03b8)y) \u2264 \u03b8f(x)+(1\u2212\u03b8)f(y)                            \u2502\n     \u2502 - Graph lies below all chords                             \u2502\n     \u2502 - Strict convexity: inequality is strict                  \u2502\n     \u2502 - Epigraph view: f convex \u21d4 epi(f) is a convex set       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Geometry (Supporting Hyperplanes)              \u2502\n     \u2502 f(y) \u2265 f(x)+\u2207f(x)\u1d40(y\u2212x)                                    \u2502\n     \u2502 - Tangent plane globally underestimates f                  \u2502\n     \u2502 - \u2207f(x) defines a supporting hyperplane to epi(f)          \u2502\n     \u2502 - Optimality: \u2207f(x*)=0 \u21d4 x* global minimizer (smooth case)\u2502\n     \u2502 - Nonsmooth extension: subgradients (next chapter)         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Characterisation                              \u2502\n     \u2502 \u2207\u00b2f(x) \u2ab0 0  for all x                                      \u2502\n     \u2502 - PSD Hessian \u21d4 upward curvature everywhere               \u2502\n     \u2502 - PD Hessian \u21d4 strict convexity                           \u2502\n     \u2502 - Links convexity to eigenvalues and curvature (Ch.3)      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Canonical Examples                                          \u2502\n     \u2502 - Affine functions: supporting hyperplanes                  \u2502\n     \u2502 - Quadratics (Q\u2ab00): curvature from Hessian                  \u2502\n     \u2502 - Norms: regularization geometry                            \u2502\n     \u2502 - Max of affine functions: hinge loss, LPs                  \u2502\n     \u2502 - Log-sum-exp: smooth max, softmax, logistic regression     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Jensen\u2019s Inequality                                         \u2502\n     \u2502 f(E[X]) \u2264 E[f(X)]                                           \u2502\n     \u2502 - Convex functions penalize variability                     \u2502\n     \u2502 - Basis for EM, variational bounds, log-sum-exp convexity   \u2502\n     \u2502 - Extends convexity from points to expectations             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Convexity-Preserving Operations                             \u2502\n     \u2502 - Scaling (\u03b1\u22650), addition                                   \u2502\n     \u2502 - Max of convex functions                                   \u2502\n     \u2502 - Affine pre-composition f(Ax+b)                            \u2502\n     \u2502 - Monotone composition rules                                \u2502\n     \u2502 Role: modular construction of convex models                 \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Sublevel Sets                                               \u2502\n     \u2502 {x : f(x) \u2264 \u03b1}                                              \u2502\n     \u2502 - Always convex for convex f                                \u2502\n     \u2502 - Enables convex inequality constraints                     \u2502\n     \u2502 - Norm balls, confidence ellipsoids, feasibility regions    \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Strict vs Strong Convexity                                  \u2502\n     \u2502 Strict convexity: unique minimizer                          \u2502\n     \u2502 Strong convexity: f \u2265 tangent + (\u03bc/2)\u2016x\u2212y\u2016\u00b2                 \u2502\n     \u2502 - Quantitative curvature                                    \u2502\n     \u2502 - Linear convergence of gradient descent                    \u2502\n     \u2502 - Conditioning \u03ba=L/\u03bc governs difficulty                     \u2502\n     \u2502 - Often induced via \u2113\u2082 regularization                       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p> <ul> <li>the  norm  (nondifferentiable at zero),</li> <li>pointwise-max functions such as ,</li> <li>the hinge loss  used in SVMs,</li> <li>regularisers like total variation or indicator functions of convex sets.</li> </ul> <p>Although these functions have \u201ckinks\u201d, they remain convex\u2014and convexity guarantees the existence of supporting hyperplanes at every point.</p>"},{"location":"convex/16_subgradients/#subgradients-and-the-subdifferential","title":"Subgradients and the Subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex.  A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if</p> \\[ f(y) \\ge f(x) + g^\\top (y - x) \\quad \\text{for all } y. \\] <p>Geometric interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top(y-x)\\) is a global underestimator of \\(f\\).</li> <li>Each subgradient defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).</li> <li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li> <li>At kinks, there may be infinitely many supporting hyperplanes.</li> </ul> <p>The subdifferential of \\(f\\) at \\(x\\) is the set  </p> <p>Properties:</p> <ul> <li>  is always a nonempty convex set (if \\(x\\) is in the interior of the domain).</li> <li>If \\(f\\) is differentiable at \\(x\\), then </li> <li>If \\(f\\) is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li> </ul> <p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>"},{"location":"convex/16_subgradients/#examples","title":"Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\). Then:</p> <ul> <li>If \\(t &gt; 0\\),  \\(\\partial f(t) = \\{1\\}\\).</li> <li>If \\(t &lt; 0\\),  \\(\\partial f(t) = \\{-1\\}\\).</li> <li>If \\(t = 0\\), </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) supports the graph from below.</p>"},{"location":"convex/16_subgradients/#the-ell_1-norm","title":"The  norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\):</p> \\[ g \\in \\partial \\|x\\|_1 \\quad\\Longleftrightarrow\\quad g_i \\in \\partial |x_i|. \\] <p>Thus:</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>"},{"location":"convex/16_subgradients/#pointwise-maximum-of-affine-functions","title":"Pointwise maximum of affine functions","text":"<p>Let </p> <ul> <li> <p>If only one index \\(i^\\star\\) achieves the maximum at \\(x\\), then </p> </li> <li> <p>If multiple indices are tied, then    the convex hull of the active slopes.</p> </li> </ul> <p>This structure underlies SVM hinge loss and ReLU-type functions.</p>"},{"location":"convex/16_subgradients/#subgradient-optimality-condition","title":"Subgradient Optimality Condition","text":"<p>For the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>a point \\(x^\\star\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^\\star). \\] <p>Interpretation:</p> <ul> <li>At optimality, no subgradient points to a direction that would decrease \\(f\\).</li> <li>Geometrically, the supporting hyperplane at \\(x^\\star\\) is horizontal, forming the flat bottom of the convex bowl.</li> <li>This generalises the smooth condition .</li> </ul>"},{"location":"convex/16_subgradients/#subgradient-calculus","title":"Subgradient Calculus","text":"<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let \\(f\\) and \\(g\\) be convex.</p>"},{"location":"convex/16_subgradients/#sum-rule","title":"Sum rule","text":"\\[ \\partial(f+g)(x) \\subseteq \\partial f(x) + \\partial g(x) = \\{ u+v : u \\in \\partial f(x),\\ v \\in \\partial g(x) \\}. \\] <p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>"},{"location":"convex/16_subgradients/#affine-composition","title":"Affine composition","text":"<p>If \\(h(x) = f(Ax + b)\\), then  </p> <p>This rule is heavily used in machine learning models, where losses depend on linear predictions \\(Ax\\).</p>"},{"location":"convex/16_subgradients/#maximum-of-convex-functions","title":"Maximum of convex functions","text":"<p>If \\(f(x) = \\max_i f_i(x)\\), then  </p> <p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar \u201czero gradient\u201d condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) framework developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#orders-of-optimality-why-first-order-is-enough-in-convex-optimization","title":"Orders of Optimality: Why First Order is Enough in Convex Optimization","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer \\(x^\\star\\):</p> Order Object inspected Role First-order \\(\\nabla f(x^\\star)\\) or subgradients Detects existence of a local descent direction Second-order Hessian \\(\\nabla^2 f(x^\\star)\\) Examines curvature (minimum vs saddle vs maximum) Higher-order Third derivative and beyond Rarely used; only for degenerate cases with vanishing curvature <p>In general nonconvex optimization, these conditions are used together: a point may have \\(\\nabla f(x^\\star) = 0\\) but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p> <p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{whenever the Hessian exists}. \\] <p>Therefore:</p> <ul> <li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li> <li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li> </ul> <p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>"},{"location":"convex/16a_optimality_conditions/#motivation","title":"Motivation","text":"<p>Consider the basic convex problem  where \\(f\\) is convex and \\(\\mathcal{X}\\) is a convex set.</p> <p>Intuitively, a point \\(\\hat{x}\\) is optimal if there is no feasible direction in which we can move and strictly decrease \\(f\\). In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside \\(\\mathcal{X}\\) are allowed.</p> <p>Thus, optimality can be seen as an equilibrium:</p> <ul> <li>the objective\u2019s tendency to decrease (captured by its gradient or subgradient)  </li> <li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li> </ul> <p>In machine learning, this appears as:</p> <ul> <li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li> <li>training until the force from regularization/constraints balances the data fit term (e.g., in \\(\\ell_1\\)-regularized models).</li> </ul> <p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>"},{"location":"convex/16a_optimality_conditions/#unconstrained-convex-problems","title":"Unconstrained Convex Problems","text":"<p>For the unconstrained problem  with \\(f\\) convex, the optimality conditions are especially simple.</p>"},{"location":"convex/16a_optimality_conditions/#smooth-case","title":"Smooth case","text":"<p>If \\(f\\) is differentiable, then a point \\(\\hat{x}\\) is optimal if and only if  </p> <p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>"},{"location":"convex/16a_optimality_conditions/#nonsmooth-case","title":"Nonsmooth case","text":"<p>If \\(f\\) is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes  </p> <p>Interpretation:</p> <ul> <li>The origin lies in the set of all subgradients at \\(\\hat{x}\\).  </li> <li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\).  </li> <li>No direction in \\(\\mathbb{R}^n\\) gives a first-order improvement in the objective.</li> </ul> <p>For smooth \\(f\\), this reduces to the usual condition \\(\\nabla f(\\hat{x}) = 0\\).</p>"},{"location":"convex/16a_optimality_conditions/#constrained-convex-problems","title":"Constrained Convex Problems","text":"<p>Now consider the constrained problem  where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a nonempty closed convex set.</p> <p>If \\(\\hat{x}\\) lies strictly inside \\(\\mathcal{X}\\), then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,  remains the necessary and sufficient condition for optimality.</p> <p>The interesting case is when \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\).</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-order condition with constraints","text":"<p>The general first-order optimality condition for the constrained convex problem is:  </p> <p>That is, there exist</p> <ul> <li>a subgradient \\(g \\in \\partial f(\\hat{x})\\), and  </li> <li>a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\)</li> </ul> <p>such that  </p> <p>Interpretation:</p> <ul> <li>The objective\u2019s slope \\(g\\) is exactly balanced by a normal vector \\(v\\) coming from the constraint set.  </li> <li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which \\(f\\) can decrease.  </li> <li>Geometrically, the epigraph of \\(f\\) and the feasible set meet with aligned supporting hyperplanes at \\(\\hat{x}\\).</li> </ul> <p>Special cases:</p> <ul> <li>If \\(\\hat{x}\\) is an interior point, then \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so we recover the unconstrained condition \\(0 \\in \\partial f(\\hat{x})\\).  </li> <li>If \\(\\mathcal{X}\\) is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li> </ul>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-the-kkt-framework","title":"Chapter 8: Lagrange Multipliers and the KKT Framework","text":"<p>We now have the ingredients for understanding optimality in convex optimization:</p> <ul> <li>convex functions define well-behaved objectives,</li> <li>convex sets describe feasible regions,</li> <li>gradients and subgradients encode descent directions.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions\u2014the Karush\u2013Kuhn\u2013Tucker (KKT) conditions\u2014is the central optimality framework for constrained convex optimization.</p> <p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by \u201cforces\u2019\u2019 coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>"},{"location":"convex/17_kkt/#unconstrained-convex-minimization","title":"Unconstrained Convex Minimization","text":"<p>Consider the problem  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent iteratively updates  with step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Moving opposite the gradient decreases \\(f\\).</li> <li>If the gradient is Lipschitz continuous and the step size is small enough (\\(\\alpha_k \\le 1/L\\)), then gradient descent converges to a global minimizer.</li> <li>If \\(f\\) is strongly convex, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li> </ul> <p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>"},{"location":"convex/17_kkt/#equality-constrained-problems-and-lagrange-multipliers","title":"Equality-Constrained Problems and Lagrange Multipliers","text":"<p>Now consider minimizing \\(f\\) subject to equality constraints:  </p> <p>Define the Lagrangian  where \\(\\lambda = (\\lambda_1,\\dots,\\lambda_p)\\) are the Lagrange multipliers.</p> <p>Under differentiability and regularity assumptions, a point \\(x^*\\) is optimal only if:</p> <ol> <li> <p>Primal feasibility     </p> </li> <li> <p>Stationarity     </p> </li> </ol> <p>Geometric meaning:</p> <ul> <li>The feasible set  is typically a smooth manifold.</li> <li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li> <li>The multipliers \\(\\lambda_j^*\\) weight the constraint normals to exactly cancel the objective\u2019s gradient.</li> </ul> <p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>"},{"location":"convex/17_kkt/#inequality-constraints-and-the-kkt-conditions","title":"Inequality Constraints and the KKT Conditions","text":"<p>Now consider the general convex problem:  </p> <p>Form the Lagrangian  with:</p> <ul> <li>  (equality multipliers),</li> <li>  (inequality multipliers).</li> </ul> <p>A point \\(x^*\\) with multipliers \\((\\lambda^*,\\mu^*)\\) satisfies the KKT conditions:</p>"},{"location":"convex/17_kkt/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^*) \\le 0,\\quad \\forall i, \\qquad h_j(x^*) = 0,\\quad \\forall j. \\]"},{"location":"convex/17_kkt/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^* \\ge 0,\\quad \\forall i. \\]"},{"location":"convex/17_kkt/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^*)  + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*) = 0. \\]"},{"location":"convex/17_kkt/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^*\\, g_i(x^*) = 0, \\quad i=1,\\dots,m. \\] <p>Complementary slackness expresses a clear dichotomy:</p> <ul> <li>If constraint \\(g_i(x) \\le 0\\) is inactive (strictly \\(&lt;0\\)), then it applies no force: \\(\\mu_i^* = 0\\).</li> <li>If a constraint is active at the boundary, it may exert a force: \\(\\mu_i^* &gt; 0\\), and then \\(g_i(x^*) = 0\\).</li> </ul> <p>Only active constraints can push back against the objective.</p>"},{"location":"convex/17_kkt/#slaters-condition-guaranteeing-strong-duality","title":"Slater\u2019s Condition \u2014 Guaranteeing Strong Duality","text":"<p>The KKT conditions always provide necessary conditions for optimality. For them to also be sufficient (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p> <p>For convex problems with convex \\(g_i\\) and affine \\(h_j\\), Slater\u2019s condition holds if there exists a strictly feasible point:  </p> <p>Interpretation:</p> <ul> <li>The feasible region contains an interior point.</li> <li>The constraints are not \u201ctight\u201d everywhere.</li> <li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li> </ul> <p>When Slater\u2019s condition holds:</p> <ol> <li> <p>Strong duality holds: </p> </li> <li> <p>The dual optimum is attained.</p> </li> <li> <p>The KKT conditions are both necessary and sufficient for optimality.</p> </li> </ol>"},{"location":"convex/17_kkt/#duality-gap","title":"Duality gap","text":"<p>For a primal problem with optimum \\(p^*\\) and its dual with optimum \\(d^*\\), the duality gap is  </p> <ul> <li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li> <li>Slater\u2019s condition ensures the gap is zero.</li> </ul> <p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>"},{"location":"convex/17_kkt/#geometric-and-physical-interpretation","title":"Geometric and Physical Interpretation","text":"<p>The KKT conditions describe an equilibrium of forces:</p> <ul> <li>The objective gradient pushes the point in the direction of steepest decrease.</li> <li>Active constraints push back through normal vectors scaled by multipliers.</li> <li>At optimality, these forces exactly cancel.</li> </ul> <p>Physically:</p> <ul> <li>Lagrange multipliers are \u201creaction forces\u2019\u2019 keeping a system on the constraint surface.</li> <li>In economics, they are \u201cshadow prices\u2019\u2019 indicating how much the objective would improve if a constraint were relaxed.</li> <li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li> </ul> <p>KKT theory unifies all earlier ideas\u2014convexity, gradients/subgradients, feasible regions, tangent and normal cones\u2014into one clean, general optimality framework.</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p> <ul> <li>lower bounds on the primal optimal value,</li> <li>certificates of optimality,</li> <li>interpretations of constraint \u201cprices,\u201d</li> <li>and alternative algorithmic routes to solutions.</li> </ul> <p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality \u2014 strong duality \u2014 lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>"},{"location":"convex/18_duality/#the-primal-problem","title":"The Primal Problem","text":"<p>Consider the general convex problem</p> \\[ \\begin{array}{ll} \\text{minimize} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i=1,\\dots,m, \\\\  &amp; h_j(x) = 0,\\quad j=1,\\dots,p, \\end{array} \\] <p>where:</p> <ul> <li>\\(f\\) and each \\(g_i\\) are convex,</li> <li>each equality constraint \\(h_j\\) is affine.</li> </ul> <p>The optimal value is</p> \\[ f^\\star = \\inf\\{ f(x) : g_i(x) \\le 0,\\ h_j(x)=0 \\}. \\] <p>The infimum allows for the possibility that the best value is approached but not attained.</p>"},{"location":"convex/18_duality/#why-duality","title":"Why Duality?","text":"<p>A constrained problem can be viewed as:</p> <p>minimize \\(f(x)\\) but pay a penalty whenever constraints are violated.</p> <p>If the penalties are chosen \u201ccorrectly,\u201d one can recover the original constrained problem from an unconstrained penalized problem. Dual variables \u2014 \\(\\mu_i\\) for inequalities and \\(\\lambda_j\\) for equalities \u2014 precisely encode these penalties:</p> <ul> <li>\\(\\mu_i\\) measures how costly it is to violate \\(g_i(x)\\le 0\\),</li> <li>\\(\\lambda_j\\) measures the sensitivity of the objective to relaxing \\(h_j(x)=0\\).</li> </ul> <p>Duality converts constraints into prices, and transforms geometry into algebra.</p>"},{"location":"convex/18_duality/#the-lagrangian","title":"The Lagrangian","text":"<p>The Lagrangian function is</p> \\[ L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^p \\lambda_j h_j(x), \\] <p>with:</p> <ul> <li>\\(\\mu_i \\ge 0\\) for inequality constraints,</li> <li>\\(\\lambda_j \\in \\mathbb{R}\\) unrestricted for equalities.</li> </ul> <p>Interpretation:</p> <ul> <li>If \\(\\mu_i &gt; 0\\), violating \\(g_i(x)\\le 0\\) incurs a penalty proportional to \\(\\mu_i\\).</li> <li>If \\(\\mu_i = 0\\), that constraint does not influence the Lagrangian at that point.</li> </ul>"},{"location":"convex/18_duality/#the-dual-function-lower-bounds-from-penalties","title":"The Dual Function: Lower Bounds from Penalties","text":"<p>Fix \\((\\lambda,\\mu)\\) and minimize the Lagrangian with respect to \\(x\\):</p> \\[ \\theta(\\lambda, \\mu) = \\inf_x L(x,\\lambda,\\mu). \\] <p>Because \\(g_i(x) \\le 0\\) for feasible \\(x\\) and \\(\\mu_i \\ge 0\\),</p> \\[ L(x,\\lambda,\\mu) \\le f(x), \\] <p>so taking the infimum over all \\(x\\) yields</p> \\[ \\theta(\\lambda,\\mu) \\le f^\\star. \\] <p>Thus \\(\\theta\\) always produces lower bounds on the true optimal value (weak duality).</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\) (infimum of affine functions).</li> <li>It may be \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul>"},{"location":"convex/18_duality/#the-dual-problem","title":"The Dual Problem","text":"<p>The dual problem maximizes these lower bounds:</p> \\[ \\begin{array}{ll} \\text{maximize}_{\\lambda,\\mu} &amp; \\theta(\\lambda,\\mu) \\\\ \\text{subject to} &amp; \\mu \\ge 0. \\end{array} \\] <p>Let \\(d^\\star\\) be the optimal dual value. Weak duality guarantees:</p> \\[ d^\\star \\le f^\\star. \\] <p>The dual problem is always a concave maximization, i.e., a convex optimization problem in \\((\\lambda,\\mu)\\).</p>"},{"location":"convex/18_duality/#strong-duality-and-the-duality-gap","title":"Strong Duality and the Duality Gap","text":"<p>If</p> \\[ d^\\star = f^\\star, \\] <p>we say strong duality holds. The duality gap is zero.</p>"},{"location":"convex/18_duality/#slaters-condition","title":"Slater\u2019s Condition","text":"<p>If:</p> <ul> <li>\\(g_i\\) are convex,</li> <li>\\(h_j\\) are affine,</li> <li>and there exists a \\(\\tilde{x}\\) such that </li> </ul> <p>then:</p> <ul> <li>strong duality holds (\\(f^\\star = d^\\star\\)),</li> <li>dual maximizers exist,</li> <li>KKT conditions fully characterize primal\u2013dual optimality.</li> </ul> <p>Slater\u2019s condition ensures the feasible region has interior \u2014 the constraints are not tight everywhere.</p>"},{"location":"convex/18_duality/#duality-and-the-kkt-conditions","title":"Duality and the KKT Conditions","text":"<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>"},{"location":"convex/18_duality/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^\\star) \\le 0,\\qquad h_j(x^\\star)=0. \\]"},{"location":"convex/18_duality/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^\\star \\ge 0. \\]"},{"location":"convex/18_duality/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^\\star) + \\sum_{i=1}^m \\mu_i^\\star \\nabla g_i(x^\\star) + \\sum_{j=1}^p \\lambda_j^\\star \\nabla h_j(x^\\star) = 0. \\]"},{"location":"convex/18_duality/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^\\star g_i(x^\\star) = 0,\\qquad \\forall i. \\] <p>Together these conditions ensure:</p> \\[ f(x^\\star) = \\theta(\\lambda^\\star,\\mu^\\star) = f^\\star = d^\\star. \\] <p>Geometrically, the gradients of the active constraints form a supporting hyperplane that \u201ctouches\u2019\u2019 the objective exactly at the optimum.</p>"},{"location":"convex/18_duality/#interpretation-of-dual-variables","title":"Interpretation of Dual Variables","text":"<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>"},{"location":"convex/18_duality/#shadow-prices-constraint-forces","title":"Shadow Prices / Constraint Forces","text":"<ul> <li> <p>\\(\\mu_i^\\star\\): the shadow price for relaxing \\(g_i(x)\\le 0\\).   Large \\(\\mu_i^\\star\\) means the constraint is tight and costly to relax.</p> </li> <li> <p>\\(\\lambda_j^\\star\\): sensitivity of the optimal value to perturbations of \\(h_j(x)=0\\).</p> </li> </ul>"},{"location":"convex/18_duality/#ml-interpretations","title":"ML Interpretations","text":"<ul> <li>Support Vector Machines: dual variables select support vectors (only points with \\(\\mu_i^\\star &gt; 0\\) matter).</li> <li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li> <li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li> </ul> <p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-multi-objective-convex-optimization","title":"Chapter 10: Multi-Objective Convex Optimization","text":"<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p> <ul> <li>accuracy vs. regularity,</li> <li>loss vs. fairness,</li> <li>return vs. risk,</li> <li>reconstruction vs. compression,</li> <li>energy use vs. performance.</li> </ul> <p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p> <p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>"},{"location":"convex/18a_pareto/#classical-optimality-one-objective","title":"Classical Optimality (One Objective)","text":"<p>In standard convex optimization, we solve:</p> \\[ x^* \\in \\arg\\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex and \\(\\mathcal{X}\\) is convex. In this setting, it is natural to speak of the minimizer \u2014 or set of minimizers \u2014 because the task is governed by a single quantitative measure.</p> <p>However, when multiple objectives \\((f_1,\\dots,f_k)\\) must be minimized simultaneously, a single \u201cbest\u201d point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#multi-objective-convex-optimization","title":"Multi-Objective Convex Optimization","text":"<p>A multi-objective optimization problem takes the form</p> \\[ \\min_{x \\in \\mathcal{X}} F(x) = (f_1(x), \\dots, f_k(x)), \\] <p>where each \\(f_i\\) is convex. This framework appears in many ML and statistical tasks:</p> Domain Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs. complexity Fair ML Loss Fairness metric Utility vs. fairness Portfolio Return Risk Profit vs. stability Autoencoders Reconstruction KL divergence Fidelity vs. disentanglement <p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes Pareto efficiency.</p>"},{"location":"convex/18a_pareto/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"convex/18a_pareto/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A point \\(x^*\\) is Pareto optimal if there is no other \\(x\\) such that</p> \\[ f_i(x) \\le f_i(x^*)\\quad \\forall i, \\] <p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>"},{"location":"convex/18a_pareto/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no feasible point satisfies</p> \\[ f_i(x) &lt; f_i(x^*)\\quad \\forall i. \\] <p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>"},{"location":"convex/18a_pareto/#geometric-view","title":"Geometric View","text":"<p>For two objectives \\((f_1, f_2)\\), the feasible set in objective space is a region in \\(\\mathbb{R}^2\\). Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p> <ul> <li>Points on the frontier are the best achievable trade-offs.</li> <li>Points above or inside the region are dominated and thus suboptimal.</li> </ul> <p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>"},{"location":"convex/18a_pareto/#scalarization-turning-many-objectives-into-one","title":"Scalarization: Turning Many Objectives into One","text":"<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>"},{"location":"convex/18a_pareto/#weighted-sum-scalarization","title":"Weighted-Sum Scalarization","text":"\\[ \\min_{x \\in \\mathcal{X}} \\sum_{i=1}^k w_i f_i(x), \\qquad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights encode relative importance.  </li> <li>Varying \\(w\\) traces (part of) the Pareto frontier.  </li> <li>When \\(f_i\\) and \\(\\mathcal{X}\\) are convex, this method recovers the convex portion of the frontier.</li> </ul>"},{"location":"convex/18a_pareto/#-constraint-method","title":"\u03b5-Constraint Method","text":"\\[ \\min_{x} \\ f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\ \\ i = 2,\\dots,k. \\] <ul> <li>Here the tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Each choice of \\(\\varepsilon\\) yields a different Pareto-efficient point.</li> </ul> <p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>"},{"location":"convex/18a_pareto/#duality-connection","title":"Duality Connection","text":"<p>Scalarization has a tight relationship with duality (Chapter 9):</p> <ul> <li>Weights \\(w_i\\) in a weighted sum act like dual variables.</li> <li>Regularization parameters (e.g., the \\(\\lambda\\) in L2 or L1 regularization) correspond to dual multipliers.</li> <li>Moving along \\(\\lambda\\) traces the Pareto frontier between data fit and model complexity.</li> </ul> <p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>"},{"location":"convex/18a_pareto/#examples-and-applications","title":"Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1: Regularized Least Squares","text":"<p>Consider</p> \\[ f_1(x) = \\|Ax - b\\|_2^2,\\qquad  f_2(x) = \\|x\\|_2^2. \\] <p>Two scalarizations:</p> <ol> <li> <p>Weighted:     </p> </li> <li> <p>\u03b5-constraint:     </p> </li> </ol> <p>\\(\\lambda\\) and \\(\\tau\\) trace the same Pareto curve \u2014 the classical bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2: Portfolio Optimization (Risk\u2013Return)","text":"<p>Let \\(w\\) be portfolio weights, \\(\\mu\\) expected returns, and \\(\\Sigma\\) the covariance matrix. Objectives:</p> \\[ f_1(w) = -\\mu^\\top w, \\qquad f_2(w) = w^\\top \\Sigma w. \\] <p>Weighted scalarization:</p> \\[ \\min_w \\ -\\alpha \\mu^\\top w + (1-\\alpha) w^\\top \\Sigma w, \\quad 0 \\le \\alpha \\le 1. \\] <p>Varying \\(\\alpha\\) recovers the efficient frontier of Modern Portfolio Theory.</p>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-in-ml","title":"Example 3: Fairness\u2013Accuracy in ML","text":"\\[ \\min_\\theta \\ \\mathbb{E}[\\ell(y, f_\\theta(x))] \\quad \\text{s.t. } D(f_\\theta(x),y) \\le \\varepsilon, \\] <p>where \\(D\\) is a fairness metric. Scalarized form:</p> \\[ \\min_\\theta\\  \\mathbb{E}[\\ell(y, f_\\theta(x))] + \\lambda D(f_\\theta(x), y). \\] <p>Tuning \\(\\lambda\\) walks across the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-vae","title":"Example 4: Variational Autoencoders and \u03b2-VAE","text":"<p>The ELBO is:</p> \\[ \\mathbb{E}_{q(z)}[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z)). \\] <p>Objectives:</p> <ul> <li>Reconstruction fidelity,</li> <li>Latent simplicity.</li> </ul> <p>\u03b2-VAE scalarization:</p> \\[ \\max_q \\ \\mathbb{E}[\\log p(x|z)] - \\beta \\,\\mathrm{KL}(q(z)\\|p(z)). \\] <p>\\(\\beta\\) controls the trade-off between reconstruction and disentanglement \u2014 a Pareto frontier in latent space.</p> <p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-balancing-fit-and-complexity","title":"Chapter 11:  Balancing Fit and Complexity","text":"<p>Most real-world learning and estimation problems must balance two competing goals:</p> <ol> <li>Fit the observed data well, and  </li> <li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li> </ol> <p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>"},{"location":"convex/18b_regularization/#motivation-fit-vs-complexity","title":"Motivation: Fit vs. Complexity","text":"<p>Suppose we wish to estimate parameters \\(x\\) from data via a loss function \\(f(x)\\). If the data are noisy or the model is high-dimensional, solutions minimizing \\(f\\) alone may be unstable or overly complex. We introduce a regularizer \\(R(x)\\), typically convex, to encourage desirable structure:</p> \\[ \\min_{x} \\; f(x) + \\lambda R(x), \\qquad \\lambda &gt; 0. \\] <ul> <li>\\(f(x)\\): measures data misfit (e.g., squared loss, logistic loss).  </li> <li>\\(R(x)\\): penalizes complexity (e.g., \\(\\ell_1\\) norm for sparsity, \\(\\ell_2\\) norm for smoothness).  </li> <li>\\(\\lambda\\): controls the trade-off.<ul> <li>Small \\(\\lambda\\): excellent data fit, potentially overfitting.  </li> <li>Large \\(\\lambda\\): simpler model, potentially underfitting.</li> </ul> </li> </ul> <p>This is a scalarized multi-objective optimization problem of \\((f, R)\\).</p>"},{"location":"convex/18b_regularization/#bicriterion-optimization-and-the-pareto-frontier","title":"Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization corresponds to the bicriterion objective:</p> \\[ \\min_{x} \\; (f(x), R(x)). \\] <p>A point \\(x^*\\) is Pareto optimal if there is no feasible \\(x\\) such that:  with strict inequality in at least one component.</p> <p>For convex \\(f\\) and \\(R\\):</p> <ul> <li>Every \\(\\lambda \\ge 0\\) yields a Pareto-optimal point,</li> <li>The mapping from \\(\\lambda\\) to constraint level \\(R(x^*)\\) is monotone,</li> <li>The Pareto frontier is convex and can be traced continuously by varying \\(\\lambda\\).</li> </ul> <p>Thus, tuning \\(\\lambda\\) moves the solution along the fit\u2013complexity frontier.</p>"},{"location":"convex/18b_regularization/#why-control-the-size-of-the-solution","title":"Why Control the Size of the Solution?","text":"<p>Inverse problems such as \\(Ax \\approx b\\) are often ill-posed or ill-conditioned:</p> <ul> <li>Small noise in \\(b\\) may cause large variability in the solution \\(x\\).  </li> <li>If \\(A\\) is rank-deficient or nearly singular, infinitely many solutions exist.</li> </ul> <p>Example: ridge regression</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2. \\] <p>The optimality condition is</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b. \\] <p>Benefits of L2 regularization:</p> <ul> <li>\\(A^\\top A + \\lambda I\\) becomes positive definite for any \\(\\lambda &gt; 0\\),  </li> <li>the solution becomes unique and stable,  </li> <li>small singular directions of \\(A\\) are suppressed.</li> </ul> <p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>"},{"location":"convex/18b_regularization/#constrained-vs-penalized-formulations","title":"Constrained vs. Penalized Formulations","text":"<p>Regularized problems can be expressed equivalently as constrained problems:</p> \\[ \\min_x f(x)  \\quad \\text{s.t. } R(x) \\le t. \\] <p>The Lagrangian is</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda (R(x) - t), \\qquad \\lambda \\ge 0. \\] <p>The penalized form</p> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>is the dual of the constrained form. Under convexity and Slater\u2019s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*),  \\] \\[ R(x^*) \\le t,\\qquad \\lambda^* \\ge 0,\\qquad \\lambda^*(R(x^*) - t) = 0. \\] <p>Here:</p> <ul> <li>If \\(R(x^*) &lt; t\\), then \\(\\lambda^* = 0\\).  </li> <li>If \\(\\lambda^* &gt; 0\\), then \\(R(x^*) = t\\) (constraint active).</li> </ul> <p>Thus \\(\\lambda\\) is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>"},{"location":"convex/18b_regularization/#common-regularizers-and-their-effects","title":"Common Regularizers and Their Effects","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"\\[ R(x) = \\|x\\|_2^2. \\] <ul> <li>Smooth and strongly convex.  </li> <li>Shrinks coefficients uniformly.  </li> <li>Improves conditioning.  </li> <li>MAP interpretation: Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"\\[ R(x) = \\|x\\|_1 = \\sum_i |x_i|. \\] <ul> <li>Convex but not differentiable \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball has corners aligned with coordinate axes, encouraging zeros in \\(x\\).  </li> <li>Proximal operator (soft-thresholding):</li> </ul> \\[ \\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(v) = \\operatorname{sign}(v)\\,\\max(|v|-\\tau, 0). \\] <ul> <li>MAP interpretation: Laplace prior.</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"\\[ R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2. \\] <ul> <li>Combines sparsity with numerical stability.  </li> <li>Useful with correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2-structured-regularizers","title":"(d) Beyond L1/L2: Structured Regularizers","text":"Regularizer Formula Effect Tikhonov \\(\\|Lx\\|_2^2\\) smoothness via operator \\(L\\) Total Variation \\(\\|\\nabla x\\|_1\\) piecewise-constant signals/images Group Lasso \\(\\sum_g \\|x_g\\|_2\\) structured sparsity across groups Nuclear Norm \\(\\|X\\|_* = \\sum_i \\sigma_i\\) low-rank matrices <p>Each regularizer defines a geometry for the solution \u2014 ellipsoids, diamonds, polytopes, or spectral shapes.</p>"},{"location":"convex/18b_regularization/#choosing-the-regularization-parameter-lambda","title":"Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-Off Behavior","text":"<ul> <li>\\(\\lambda \\downarrow\\): favors small training error, high variance.  </li> <li>\\(\\lambda \\uparrow\\): favors simplicity, higher bias.  </li> </ul> <p>\\(\\lambda\\) selects a point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#b-cross-validation","title":"(b) Cross-Validation","text":"<p>The most common practice:</p> <ol> <li>Split data into folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining fold.  </li> <li>Choose \\(\\lambda\\) minimizing average validation error.</li> </ol> <p>Guidelines:</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>Use time-aware CV for dependent data.  </li> <li>Use the \u201cone-standard-error rule\u201d for simpler models.</li> </ul>"},{"location":"convex/18b_regularization/#c-other-selection-methods","title":"(c) Other Selection Methods","text":"<ul> <li>Information criteria (AIC, BIC) for sparsity.  </li> <li>L-curve or discrepancy principle in inverse problems.  </li> <li>Regularization paths: computing \\(x^*(\\lambda)\\) for many \\(\\lambda\\).</li> </ul>"},{"location":"convex/18b_regularization/#algorithmic-view","title":"Algorithmic View","text":"<p>Most regularized problems have the form:</p> \\[ \\min_x \\ f(x) + R(x), \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex (possibly nonsmooth).</p> <p>Common algorithms:</p> Method Idea When Useful Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), proximal step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update coordinates cyclically Lasso, Elastic Net ADMM Split problem to exploit structure Large-scale or distributed settings <p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal \\(O(1/k^2)\\) rate for smooth+convex problems.</p>"},{"location":"convex/18b_regularization/#bayesian-interpretation","title":"Bayesian Interpretation","text":"<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p> <p>Linear model:</p> \\[ b = Ax + \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I). \\] <p>With prior \\(x \\sim p(x)\\), MAP estimation solves:</p> \\[ \\min_x \\ \\frac{1}{2\\sigma^2}\\|Ax - b\\|_2^2 - \\log p(x). \\] <p>Examples:</p> <ul> <li>Gaussian prior \\(p(x) \\propto e^{-\\|x\\|_2^2 / (2\\tau^2)}\\)   \u2192 L2 penalty with \\(\\lambda = \\sigma^2/(2\\tau^2)\\).  </li> <li>Laplace prior   \u2192 L1 penalty and sparse MAP estimate.</li> </ul> <p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p> <p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#problem-classes-vs-method-classes","title":"Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#first-order-methods-gradient-descent","title":"First-order methods: Gradient descent","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  </p> <p>Smoothness lets us control step sizes.</p> <p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). Typical choices:</p> <ul> <li>constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known,</li> <li>backtracking line search when \\(L\\) is unknown,</li> <li>diminishing step sizes in some settings.</li> </ul> <p>Derivation: </p> <p>Around \\(x_t\\), we can approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Convergence: For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p> <p>When to use gradient descent:</p> <ul> <li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li> <li>You can compute gradients cheaply.</li> <li>You only need moderate accuracy.</li> <li>Memory constraints rule out storing or factoring Hessians.</li> </ul>"},{"location":"convex/19_optimizationalgo/#accelerated-first-order-methods","title":"Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#nesterov-acceleration","title":"Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences \\(x_k\\) and \\(y_k\\):</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra momentum term \\(\\beta_k (x_{k+1}-x_k)\\) uses past iterates to \u201clook ahead\u201d and can significantly accelerate convergence.</p> <p>Convergece: For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives.</p> <ul> <li>Acceleration is effective for well-behaved smooth convex problems.</li> <li>It can be more sensitive to step size and noise than plain gradient descent.</li> <li>Variants such as FISTA apply acceleration in the composite setting \\(f + R\\).</li> </ul> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#subgradient-methods","title":"Subgradient Methods","text":"<p>Even when \\(f\\) is not differentiable, we can minimise it using subgradient descent:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k, \\qquad g_k \\in \\partial f(x_k). \\] <p>Key features:</p> <ul> <li>Requires only a subgradient (no differentiability needed).</li> <li>Works for any convex function.</li> <li>Stepsizes must typically decrease (e.g. , ).</li> <li>Guaranteed convergence for convex \\(f\\), but generally slow.</li> </ul>"},{"location":"convex/19_optimizationalgo/#convergence-rates-worst-case","title":"Convergence rates (worst case)","text":"<ul> <li>Smooth convex gradient descent: \\(O(1/k)\\) or \\(O(1/k^2)\\).  </li> <li>Nonsmooth subgradient descent: </li> </ul> <p>This slower rate reflects the lack of curvature information at kinks.</p>"},{"location":"convex/19_optimizationalgo/#proximal-and-smoothed-alternatives","title":"Proximal and Smoothed Alternatives","text":"<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>"},{"location":"convex/19_optimizationalgo/#1-proximal-methods","title":"(1) Proximal methods","text":"<p>For a convex function \\(f\\), the proximal operator is  </p> <p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p> <ul> <li>  regularisation,</li> <li>indicator functions of convex sets,</li> <li>total variation penalties.</li> </ul> <p>They achieve faster and more stable convergence than basic subgradient descent.</p>"},{"location":"convex/19_optimizationalgo/#2-smoothing-techniques","title":"(2) Smoothing techniques","text":"<p>Many nonsmooth convex functions have smooth approximations:</p> <ul> <li>Replace  with the Huber loss.</li> <li>Replace  with softplus.</li> <li>Replace  with log-sum-exp, a smooth convex approximation.</li> </ul> <p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>"},{"location":"convex/19_optimizationalgo/#steepest-descent-method","title":"Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization. Gradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems. The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p> <p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#conjugate-gradient-method-fast-optimization-for-quadratic-objectives","title":"Conjugate Gradient Method \u2014 Fast Optimization for Quadratic Objectives","text":"<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p> <p>CG is a first-order method that behaves like a second-order method for quadratics.</p> <p>For a quadratic objective function:</p> \\[ f(x) = \\tfrac12 x^\\top A x - b^\\top x  \\] <p>with \\(A \\succ 0\\), the level sets are ellipses shaped by the eigenvalues of \\(A\\). If \\(A\\) is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p> <p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors \\(p_i, p_j\\) are said to be A-conjugate if</p> \\[ p_i^\\top A p_j = 0. \\] <p>This is orthogonality measured in the geometry induced by the Hessian \\(A\\). Why is this useful?</p> <ul> <li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of \\(A\\).</li> <li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li> <li>After \\(n\\) mutually A-conjugate directions, all curvature directions are resolved \u2192 exact solution.</li> </ul> <p>In contrast, gradient descent repeatedly re-corrects previous progress.</p> <p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system \\(Ax = b\\). Let</p> \\[ r_0 = b - A x_0, \\qquad p_0 = r_0. \\] <p>For \\(k = 0,1,2,\\dots\\):</p> <ol> <li> <p>Step size     </p> </li> <li> <p>Update iterate     </p> </li> <li> <p>Update residual (negative gradient)     </p> </li> <li> <p>Direction scaling     </p> </li> <li> <p>New conjugate direction     </p> </li> </ol> <p>Stop when \\(\\|r_k\\|\\) is below tolerance.</p> <p>Every new direction \\(p_{k+1}\\) is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p> <p>Why CG Is Fast: For an \\(n\\)-dimensional quadratic, CG solves the problem in at most \\(n\\) iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in \\(O(\\sqrt{\\kappa})\\) iterations, where \\(\\kappa = \\lambda_{\\max}/\\lambda_{\\min}\\) is the condition number. The convergence bound in the A-norm is:</p> \\[ \\|x_k - x^\\star\\|_A \\le  2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k  \\|x_0 - x^\\star\\|_A. \\] <p>This is dramatically better than the \\(O(1/k)\\) rate of gradient descent.</p> <p>CG is ideal when:</p> <ul> <li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix \\(A\\).</li> <li>\\(A\\) is large and sparse or available as a matrix\u2013vector product.</li> <li>You cannot form or store \\(A^{-1}\\) or even the full matrix \\(A\\).</li> <li>You want a Hessian-aware method but cannot afford Newton\u2019s method.</li> </ul> <p>Typical scenarios:</p> Application Why CG fits Large linear systems \\(A x = b\\) Only requires \\(A p\\), not factorization. Ridge regression Normal equations form an SPD matrix. Kernel ridge regression Solves \\((K+\\lambda I)\\alpha = y\\) efficiently. Newton steps in ML Inner solver for Hessian systems without forming Hessian. PDEs and scientific computing Sparse SPD matrices, ideal for CG. <p>Assumptions Required for CG: To guarantee correctness of linear CG, we require:</p> <ul> <li>\\(A\\) is symmetric</li> <li>\\(A\\) is positive definite</li> <li>Objective is strictly convex quadratic</li> <li>Arithmetic is exact (for the finite-step guarantee)</li> </ul> <p>If the function is not quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p> <p>Practical Notes:</p> <ul> <li>You only need matrix\u2013vector products \\(Ap\\).  </li> <li>Storage cost is \\(O(n)\\).  </li> <li>Preconditioning (replacing the system with \\(M^{-1} A\\)) improves conditioning and accelerates convergence dramatically.  </li> <li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li> </ul> <p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>"},{"location":"convex/19_optimizationalgo/#newtons-method-and-second-order-methods","title":"Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p> <p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex \\(f\\), Newton\u2019s method achieves quadratic convergence\u2014dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order algorithms.</p> <p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton\u2019s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>"},{"location":"convex/19_optimizationalgo/#solving-the-newton-system","title":"Solving the Newton System","text":"<p>Each iteration requires solving</p> \\[ H \\,\\Delta x = -g, \\qquad H = \\nabla^2 f(x), \\;\\; g = \\nabla f(x). \\] <p>If \\(H\\) is symmetric positive definite, a Cholesky factorization</p> \\[ H = L L^\\top \\] <p>allows efficient and numerically stable solution via two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^\\top \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids forming \\(H^{-1}\\) explicitly.</p> <p>The Newton decrement:</p> \\[ \\lambda(x) = \\|L^{-1} g\\|_2 \\] <p>gauges proximity to the optimum and provides a natural stopping criterion: \\(\\lambda(x)^2/2 &lt; \\varepsilon\\).</p> <p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs \\(\\approx (1/3)n^3\\) operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton\u2019s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix\u2013free iterative methods.</p>"},{"location":"convex/19_optimizationalgo/#gaussnewton-method","title":"Gauss\u2013Newton Method","text":"<p>The Gauss\u2013Newton method is a specialization of Newton\u2019s method for nonlinear least squares problems</p> \\[ f(x) = \\tfrac12 \\| r(x) \\|^2, \\] <p>where \\(r(x)\\) is a vector of residual functions and a nonlinear function of \\(x\\) and \\(J\\) is its Jacobian. Newton\u2019s Hessian decomposes as</p> \\[ \\nabla^2 f(x) = J^\\top J \\;+\\; \\sum_i r_i(x)\\, \\nabla^2 r_i(x). \\] <p>The second term involves the curvature of the residuals. When \\(r(x)\\) is approximately linear near the optimum, this term is small. Gauss\u2013Newton drops it, giving the approximation</p> \\[ \\nabla^2 f(x) \\approx J^\\top J, \\] <p>leading to the Gauss\u2013Newton step:</p> \\[ (J^\\top J)\\, \\Delta = -J^\\top r. \\] <p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg\u2013Marquardt method adds a damping term,</p> \\[ (J^\\top J + \\lambda I)\\, \\Delta = -J^\\top r, \\] <p>which interpolates smoothly between  </p> <ul> <li>gradient descent (large \\(\\lambda\\)), and  </li> <li>Gauss\u2013Newton (small \\(\\lambda\\)).</li> </ul> <p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss\u2013Newton and Levenberg\u2013Marquardt are highly effective when the residuals are nearly linear\u2014common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning\u2014yielding fast convergence without the expense of full second derivatives.</p>"},{"location":"convex/19_optimizationalgo/#quasi-newton-methods","title":"Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p> <p>When to use Newton or quasi-Newton methods:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#projected-gradient-descent","title":"Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#proximal-gradient-forwardbackward-splitting","title":"Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#penalties-barriers-and-interior-point-methods","title":"Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\): Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#penalty-methods","title":"Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-basic-penalty-method-quadratic-or-general-penalization","title":"Algorithm: Basic Penalty Method (Quadratic or General Penalization)","text":"<p>Goal:  Solve </p> <p>Penalty formulation:  where  </p> <ul> <li>\\(\\phi(r) = 0\\) if \\(r \\le 0\\),  </li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (e.g., \\(\\phi(r)=\\max\\{0,r\\}^2\\)),  </li> <li>\\(\\rho &gt; 0\\) is the penalty weight.</li> </ul> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>constraints \\(g_i(x)\\) </li> <li>penalty function \\(\\phi\\) </li> <li>initial point \\(x_0\\) </li> <li>initial penalty parameter \\(\\rho_0 &gt; 0\\) </li> <li>penalty update factor \\(\\gamma &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose \\(x_0\\), \\(\\rho_0 &gt; 0\\).  </li> <li>For \\(k = 0, 1, 2, \\dots\\):  <ol> <li>Solve the penalized subproblem  \\(x_{k+1} = \\arg\\min_x F_{\\rho_k}(x)\\) using Newton\u2019s method, gradient descent, quasi-Newton, etc.  </li> <li>Check feasibility / stopping:  If \\(\\max_i g_i(x_{k+1}) \\le \\varepsilon, \\quad   \\|x_{k+1} - x_k\\| \\le \\varepsilon\\)  stop and return \\(x_{k+1}\\).  </li> <li>Increase penalty parameter  \\(\\rho_{k+1} = \\gamma\\, \\rho_k\\)   with typical \\(\\gamma \\in [5,10]\\).  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#barrier-methods","title":"Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-barrier-method-logarithmic-barrier-interior-approximation","title":"Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)","text":"<p>Goal: Solve the constrained problem </p> <p>Logarithmic barrier:  defined only for strictly feasible points \\(g_i(x)&lt;0\\).</p> <p>Barrier subproblem:  where \\(t&gt;0\\) is the barrier parameter.</p> <p>As \\(t \\to \\infty\\), minimizers of \\(F_t\\) approach the constrained optimum.</p> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>barrier function \\(b(x)\\) </li> <li>strictly feasible starting point \\(x_0\\) (\\(g_i(x_0) &lt; 0\\))  </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>barrier growth factor \\(\\mu &gt; 1\\) (often \\(\\mu = 10\\))  </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose strictly feasible \\(x_0\\), and pick \\(t_0 &gt; 0\\).  </li> <li>For \\(k = 0,1,2,\\dots\\):  <ol> <li>Centering step (inner loop):  Solve the barrier subproblem    Typically use Newton\u2019s method (damped) on \\(F_{t_k}\\).  Stop when the Newton decrement satisfies  \\(\\lambda(x_{k+1})^2/2 \\le \\varepsilon\\)</li> <li>Optimality / stopping test:    If  \\(\\frac{m}{t_k} \\le \\varepsilon,\\)   then \\(x_{k+1}\\) is an \\(\\varepsilon\\)-approximate solution of the original constrained problem; stop and return \\(x_{k+1}\\).  </li> <li>Increase barrier parameter:  \\(t_{k+1} = \\mu\\, t_k,\\)   which tightens the approximation and moves closer to the boundary.  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#interior-point-methods","title":"Interior-point methods","text":"<p>Interior-point methods combine barrier functions with Newton\u2019s method to solve general convex programs:</p> <ul> <li>They maintain strict feasibility throughout.</li> <li>Each iteration solves a Newton system for the barrier-augmented objective.</li> <li>They naturally generate primal\u2013dual pairs and duality gap estimates.</li> <li>Under standard assumptions (e.g., Slater\u2019s condition), they converge in a predictable number of iterations.</li> </ul> <p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>"},{"location":"convex/19_optimizationalgo/#algorithm-primaldual-interior-point-method-for-convex-inequality-constraints","title":"Algorithm: Primal\u2013Dual Interior-Point Method (for convex inequality constraints)","text":"<p>We consider the problem  </p> <p>Introduce Lagrange multipliers \\(\\lambda \\ge 0\\). The KKT conditions are  </p> <p>Interior-point methods enforce the relaxed condition  which keeps iterates strictly feasible.</p>"},{"location":"convex/19_optimizationalgo/#inputs","title":"Inputs","text":"<ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>initial primal point \\(x_0\\) with \\(g_i(x_0)&lt;0\\) </li> <li>initial dual variable \\(\\lambda_0 &gt; 0\\) </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>growth factor \\(\\mu &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul>"},{"location":"convex/19_optimizationalgo/#procedure","title":"Procedure","text":"<ol> <li> <p>Choose strictly feasible \\(x_0\\), positive \\(\\lambda_0\\), and \\(t_0\\).</p> </li> <li> <p>For \\(k = 0,1,2,\\dots\\):</p> <p>(a) Form the perturbed KKT system.  Solve for the Newton direction \\((\\Delta x, \\Delta \\lambda)\\):</p> <p> </p> <p>(b) Line search to keep strict feasibility. Choose the maximum \\(\\alpha\\in(0,1]\\) such that:</p> <ul> <li>\\(g_i(x + \\alpha \\Delta x) &lt; 0\\),</li> <li>\\(\\lambda + \\alpha \\Delta \\lambda &gt; 0\\).</li> </ul> <p>(c) Update: \\(x \\leftarrow x + \\alpha \\Delta x,    \\qquad  \\lambda \\leftarrow \\lambda + \\alpha \\Delta \\lambda.\\)</p> <p>(d) Check duality gap: \\(\\text{gap} = - g(x)^\\top \\lambda\\) If \\(\\text{gap} \\le \\varepsilon\\), stop.</p> <p>(e) Increase barrier parameter \\(t \\leftarrow \\mu t.\\)</p> </li> <li> <p>Return \\(x\\).</p> </li> </ol>"},{"location":"convex/19_optimizationalgo/#choosing-the-right-method-in-practice","title":"Choosing the right method in practice","text":"<p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19_optimizationalgo/#mental-map","title":"Mental Map","text":"<pre><code>                Algorithms for Convex Optimization\n     Turning convex geometry + optimality conditions into solvers\n                              \u2502\n                              \u25bc\n            Core question: how do we actually solve problems?\n   Choose an algorithm class that matches problem structure + scale\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Problem Classes  \u2192  Method Classes                         \u2502\n     \u2502 Smooth unconstrained:        GD, Acceleration, Newton      \u2502\n     \u2502 Smooth + simple constraints: Projected gradient            \u2502\n     \u2502 Composite (smooth+nonsmooth): Proximal/coordinate/splitting\u2502\n     \u2502 General constrained convex:  Interior-point / primal\u2013dual  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Core: Gradient Descent                        \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k \u2207f(x_k)                               \u2502\n     \u2502 Requirements: convex + L-smooth                           \u2502\n     \u2502 - Step size from L or line search                         \u2502\n     \u2502 - Rate (smooth convex): O(1/k)                            \u2502\n     \u2502 Geometry: move opposite supporting hyperplane slope       \n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Acceleration (Nesterov / Momentum)                        \u2502\n     \u2502 Two sequences: gradient at y_k + extrapolation to y_{k+1} \u2502\n     \u2502 - Rate (smooth convex): O(1/k^2)                          \u2502\n     \u2502 - Best possible for gradient-only methods                 \u2502\n     \u2502 Tradeoff: faster but more sensitive to tuning/noise       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Nonsmooth First-Order: Subgradient Descent                   \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k g_k,   g_k \u2208 \u2202f(x_k)                     \u2502\n     \u2502 - Works for convex nonsmooth objectives                      \u2502\n     \u2502 - Needs diminishing step sizes                               \u2502\n     \u2502 - Worst-case rate: O(1/\u221ak)                                   \u2502\n     \u2502 Use when: only subgradients available / simple implementation\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Fixing Nonsmoothness: Proximal &amp; Smoothing                \u2502\n     \u2502 Prox operator: prox_{\u03b1R}(y)=argmin_x R(x)+(1/2\u03b1)\u2016x\u2212y\u2016\u00b2    \u2502\n     \u2502 - Handles \u2113\u2081, indicators, TV, etc.                        \u2502\n     \u2502 Smoothing: Huber / softplus / log-sum-exp                 \u2502\n     \u2502 - Enables fast smooth methods                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Steepest Descent = Gradient Descent under a chosen norm     \u2502\n     \u2502 \u0394x_nsd = argmin_{\u2016v\u2016=1} \u2207f(x)\u1d40v                             \u2502\n     \u2502 - Dual norm controls gradient magnitude                     \u2502\n     \u2502 - \u2113\u2082 \u2192 standard GD                                          \u2502\n     \u2502 - Quadratic norm \u2192 preconditioned GD / Newton-like          \u2502\n     \u2502 - \u2113\u2081 \u2192 coordinate-like steps                                \u2502\n     \u2502 Purpose: change geometry to reduce anisotropy / improve rate\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Quadratic Structure: Conjugate Gradient (CG)                \u2502\n     \u2502 Solve: \u00bdx\u1d40Ax \u2212 b\u1d40x, A\u227b0  (equivalently Ax=b)                \u2502\n     \u2502 - Builds A-conjugate directions (Hessian-orthogonal)        \u2502\n     \u2502 - Uses only matrix\u2013vector products Ap                       \u2502\n     \u2502 - Converges in \u2264 n steps (exact arithmetic)                 \u2502\n     \u2502 - Practical iterations ~ O(\u221a\u03ba) with \u03ba=\u03bb_max/\u03bb_min           \u2502\n     \u2502 - Preconditioning is key for speed                          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Methods: Newton &amp; Variants                     \u2502\n     \u2502 Newton step:  \u2207\u00b2f(x_k) d = \u2212\u2207f(x_k)                         \u2502\n     \u2502 - Quadratic local model \u2192 fast local convergence            \u2502\n     \u2502 - Needs line search / trust region for robustness           \u2502\n     \u2502 Gauss\u2013Newton / Levenberg\u2013Marquardt: least-squares structure \u2502\n     \u2502 Quasi-Newton (BFGS/L-BFGS): Hessian inverse approximations  \u2502\n     \u2502 Use when: moderate dimension or efficient linear solves     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Handling Constraints: Projection &amp; Proximal Splitting        \u2502\n     \u2502 Projected GD:  y=x\u2212\u03b1\u2207f(x);  x\u207a=\u03a0_X(y)                        \u2502\n     \u2502 Proximal gradient: y=x\u2212\u03b1\u2207f(x); x\u207a=prox_{\u03b1R}(y)               \u2502\n     \u2502 Unification: indicator_R(X) \u21d2 prox = projection              \u2502\n     \u2502 Use when: constraints/regularizers have cheap prox/project   \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 General Inequalities: Penalties \u2192 Barriers \u2192 Interior-Point \u2502\n     \u2502 Penalty: f(x)+\u03c1 \u03a3 \u03c6(g_i(x))                                 \u2502\n     \u2502 Barrier: tf(x) \u2212 \u03a3 log(\u2212g_i(x))  (strict feasibility)       \u2502\n     \u2502 Interior-point: Newton steps on (primal\u2013dual) perturbed KKT \u2502\n     \u2502 - Few iterations, high accuracy, solver backbone for LP/QP  \u2502\n     \u2502 - Uses duality gap certificates                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                 Practical selection (the decision logic)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Huge-scale smooth \u2192 GD / accelerated / L-BFGS               \u2502\n     \u2502 Moderate smooth, high accuracy \u2192 Newton / quasi-Newton      \u2502\n     \u2502 Simple constraints \u2192 projected gradient                     \u2502\n     \u2502 Smooth + nonsmooth regularizer \u2192 proximal gradient / FISTA  \u2502\n     \u2502 General constraints \u2192 interior-point / primal\u2013dual          \u2502\n     \u2502 Quadratic / SPD linear systems \u2192 CG (+ preconditioning)     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever variables must satisfy exact relationships, such as conservation laws, normalization, or linear invariants. In this chapter we focus on problems of the form</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>where \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is (typically convex and differentiable) and \\(A \\in \\mathbb{R}^{p \\times n}\\) has rank \\(p\\). This linear equality structure appears in constrained least squares, portfolio optimization, and many ML formulations that impose exact balance or normalization constraints.</p>"},{"location":"convex/19a_optimization_constraints/#geometric-view-optimization-on-an-affine-manifold","title":"Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(\\operatorname{rank}(A) = p\\), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine subspace of \\(\\mathbb{R}^n\\): a \u201cflat\u201d lower-dimensional plane embedded in the ambient space. Optimization now happens along this plane, not in all of \\(\\mathbb{R}^n\\). Any feasible direction \\(d\\) must keep us in \\(\\mathcal{X}\\), so it must satisfy</p> \\[ A (x + d) = b \\quad \\Rightarrow \\quad A d = 0. \\] <p>Thus, feasible directions lie in the null space of \\(A\\):</p> \\[ \\mathcal{D}_{\\text{feas}} = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\} = \\operatorname{Null}(A). \\] <p>At an optimal point \\(x^\\star \\in \\mathcal{X}\\), moving in any feasible direction \\(d\\) cannot decrease \\(f\\). For differentiable \\(f\\), this means</p> \\[ \\nabla f(x^\\star)^\\top d \\ge 0 \\quad \\text{for all } d \\text{ with } A d = 0. \\] <p>Equivalently, \\(\\nabla f(x^\\star)\\) must be orthogonal to all feasible directions, i.e. it lies in the row space of \\(A\\). Therefore there exists a vector of Lagrange multipliers \\(\\nu^\\star\\) such that</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star. \\] <p>This is the basic geometric optimality condition: at the optimum, the gradient of \\(f\\) is a linear combination of the constraint normals (rows of \\(A\\)), and every feasible direction is orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#lagrange-function-and-kkt-system","title":"Lagrange Function and KKT System","text":"<p>The Lagrangian for the equality-constrained problem is</p> \\[ \\mathcal{L}(x,\\nu) = f(x) + \\nu^\\top (A x - b), \\] <p>where \\(\\nu \\in \\mathbb{R}^p\\) are Lagrange multipliers. The first-order (KKT) conditions for a point \\((x^\\star,\\nu^\\star)\\) to be optimal are</p> \\[ \\begin{aligned} \\nabla_x \\mathcal{L}(x^\\star,\\nu^\\star) &amp;= \\nabla f(x^\\star) + A^\\top \\nu^\\star = 0  \\quad &amp;\\text{(stationarity)},\\\\ A x^\\star &amp;= b  \\quad &amp;\\text{(primal feasibility)}. \\end{aligned} \\] <p>When \\(f\\) is convex and \\(A\\) has full row rank, these conditions are necessary and sufficient for global optimality. For Newton-type methods we linearize these conditions around a current iterate \\((x,\\nu)\\) and solve for corrections \\((\\Delta x,\\Delta \\nu)\\) from</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>This linear system is called the (equality-constrained) KKT system. At the optimum the right-hand side is zero.</p>"},{"location":"convex/19a_optimization_constraints/#quadratic-objectives","title":"Quadratic Objectives","text":"<p>A particularly important case is a convex quadratic objective</p> \\[ f(x) = \\tfrac{1}{2} x^\\top P x + q^\\top x + r, \\] <p>with \\(P \\succeq 0\\). The equality-constrained problem</p> \\[ \\min_x \\tfrac{1}{2} x^\\top P x + q^\\top x + r  \\quad \\text{s.t.} \\quad A x = b \\] <p>has KKT conditions</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>If \\(P \\succ 0\\) and \\(A\\) has full row rank, this system has a unique solution \\((x^\\star,\\nu^\\star)\\). This is the standard linear system solved in equality-constrained least squares and quadratic programming.</p> <p>Examples in ML and statistics:</p> <ul> <li>constrained least squares with sum-to-one constraints on coefficients;  </li> <li>portfolio optimization with ;  </li> <li>quadratic surrogate subproblems inside second-order methods.</li> </ul> <p>The structure of the KKT matrix (symmetric, indefinite, with blocks \\(P\\), \\(A\\)) can be exploited by specialized linear solvers and factorizations.</p>"},{"location":"convex/19a_optimization_constraints/#null-space-reduced-variable-method","title":"Null-Space (Reduced Variable) Method","text":"<p>When the constraints are linear and of full row rank, a natural approach is to eliminate them explicitly.</p> <p>Choose:</p> <ul> <li>a particular feasible point \\(x_0\\) satisfying \\(A x_0 = b\\),  </li> <li>a matrix \\(Z \\in \\mathbb{R}^{n \\times (n-p)}\\) whose columns form a basis of the null space of \\(A\\):    </li> </ul> <p>Then every feasible \\(x\\) can be written as</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective yields an unconstrained reduced problem in the smaller variable \\(y\\):</p> \\[ \\min_{y} \\; \\phi(y) := f(x_0 + Z y). \\] <p>Gradients and Hessians transform as</p> \\[ \\nabla_y \\phi(y) = Z^\\top \\nabla_x f(x_0 + Z y), \\qquad \\nabla_y^2 \\phi(y) = Z^\\top \\nabla_x^2 f(x_0 + Z y) \\, Z. \\] <p>We can now apply any unconstrained method (gradient descent, CG, Newton) to \\(\\phi(y)\\). The corresponding updates in the original space are mapped back via \\(x = x_0 + Z y\\).</p> <p>Key points:</p> <ul> <li>Optimization is restricted to feasible directions \\(\\operatorname{Null}(A)\\) by construction.  </li> <li>The dimension drops from \\(n\\) to \\(n-p\\), which can be advantageous if \\(p\\) is large.  </li> <li>The cost is computing and storing a suitable null-space basis \\(Z\\), which may destroy sparsity and be expensive for large-scale problems.</li> </ul> <p>Null-space methods are attractive when:</p> <ul> <li>the number of constraints is moderate,  </li> <li>a good factorization of \\(A\\) is available,  </li> <li>and we want an unconstrained algorithm in reduced coordinates.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#newtons-method-for-equality-constrained-problems","title":"Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice-differentiable convex \\(f\\), we can derive an equality-constrained Newton step by solving a local quadratic approximation subject to linearized constraints.</p> <p>At a point \\(x\\), approximate \\(f(x+d)\\) by its second-order Taylor expansion:</p> \\[ f(x+d) \\approx f(x) + \\nabla f(x)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d. \\] <p>We seek a step \\(d\\) that approximately minimizes this quadratic model while remaining feasible to first order, i.e.</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d\\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>The KKT conditions for this quadratic subproblem are</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>Solving this system gives the Newton step \\(d_{\\text{nt}}\\) and a multiplier update \\(\\lambda\\). The primal update is</p> \\[ x_{k+1} = x_k + \\alpha_k d_{\\text{nt}}, \\] <p>with a step size \\(\\alpha_k \\in (0,1]\\) chosen by line search to ensure sufficient decrease and preservation of feasibility (for equality constraints, \\(A d_{\\text{nt}} = 0\\) guarantees \\(A x_{k+1} = b\\) whenever \\(A x_k = b\\)).</p> <p>Geometrically:</p> <ul> <li>unconstrained Newton would move by \\(-\\nabla^2 f(x)^{-1} \\nabla f(x)\\);  </li> <li>equality-constrained Newton projects this step onto the tangent space \\(\\{ d : A d = 0 \\}\\) of the affine constraint set.</li> </ul> <p>For strictly convex \\(f\\) with positive definite Hessian on the feasible directions, this method enjoys quadratic convergence near the solution, much like the unconstrained Newton method.</p>"},{"location":"convex/19a_optimization_constraints/#connections-to-machine-learning-and-signal-processing","title":"Connections to Machine Learning and Signal Processing","text":"<p>Linear equality constraints appear naturally in ML and related areas:</p> Setting Equality constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights sum to one (full investment) Constrained regression \\(C x = d\\) Enforce domain-specific linear relations between coefficients Mixture models / convex combinations \\(\\mathbf{1}^\\top \\alpha = 1, \\; \\alpha \\ge 0\\) Mixture weights form a probability simplex Fairness constraints (linearized) \\(A w = 0\\) Enforce equal averages across groups or balance conditions Physics-informed models (discretized) \\(A x = b\\) Discrete conservation laws (mass, charge, energy) <p>More generally, nonlinear equality constraints (e.g. \\(W^\\top W = I\\) for orthonormal embeddings, or \\(\\|w\\|_2^2 = 1\\) for normalized weights) lead to optimization on curved manifolds. Techniques from this chapter extend to those settings when combined with Riemannian optimization or local parameterizations, but here we focus on the linear case as the fundamental building block.</p>"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In many applications, we must optimize an objective while respecting inequality constraints: nonnegativity of variables, margin constraints in SVMs, capacity or safety limits, physical bounds, fairness budgets, and more. Mathematically, the feasible region is now a convex set with a boundary, and the optimizer often lies on that boundary.</p> <p>This chapter introduces algorithms for solving such problems, focusing on logarithmic barrier and interior-point methods. These are the workhorses behind modern general-purpose convex solvers (for LP, QP, SOCP, SDP) and provide a smooth way to enforce inequalities while still using Newton-type methods.</p>"},{"location":"convex/19b_optimization_constraints/#problem-setup","title":"Problem Setup","text":"<p>We consider the general convex problem with inequality and equality constraints  where</p> <ul> <li>\\(f_0, f_1,\\dots,f_m\\) are convex, typically twice differentiable,</li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank,</li> <li>there exists a strictly feasible point \\(\\bar{x}\\) such that   \\(f_i(\\bar{x}) &lt; 0\\) for all \\(i\\) and \\(A\\bar{x} = b\\) (Slater\u2019s condition).</li> </ul> <p>Under these assumptions:</p> <ul> <li>the problem is convex,</li> <li>strong duality holds (zero duality gap),</li> <li>and the KKT conditions characterize optimality.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem type \\(f_0(x)\\) Constraints \\(f_i(x)\\le0\\) ML / applications Linear program (LP) \\(c^\\top x\\) \\(a_i^\\top x - b_i \\le 0\\) resource allocation, feature selection Quadratic program \\(\\tfrac12 x^\\top P x + q^\\top x\\) linear SVMs, ridge with box constraints QCQP quadratic quadratic portfolio optimization, control Entropy models \\(\\sum_i x_i \\log x_i\\) \\(F x - g \\le 0\\) probability calibration, max-entropy Nonnegativity arbitrary convex \\(-x_i \\le 0\\) sparse coding, nonnegative factorization <p>Many machine-learning training problems can be written in this template by expressing regularization, margins, fairness, or safety conditions as convex inequalities.</p>"},{"location":"convex/19b_optimization_constraints/#indicator-function-view-of-constraints","title":"Indicator-Function View of Constraints","text":"<p>Conceptually, we can write inequality constraints using an indicator function. Define  </p> <p>Then the inequality-constrained problem is equivalent to  </p> <ul> <li>If \\(x\\) is feasible (\\(f_i(x) \\le 0\\) for all \\(i\\)), the indicators contribute \\(0\\).</li> <li>If any constraint is violated (\\(f_i(x) &gt; 0\\)), the objective becomes \\(+\\infty\\).</li> </ul> <p>This formulation is clean but not numerically friendly:</p> <ul> <li>\\(I_{-}\\) is discontinuous and nonsmooth.</li> <li>We cannot directly apply Newton-type methods.</li> </ul> <p>The key idea of barrier methods is to replace the hard indicator with a smooth approximation that grows to \\(+\\infty\\) as we approach the boundary.</p>"},{"location":"convex/19b_optimization_constraints/#logarithmic-barrier-approximation","title":"Logarithmic Barrier Approximation","text":"<p>We approximate the indicator \\(I_{-}\\) with a smooth barrier function  </p> <p>For each inequality \\(f_i(x) \\le 0\\), we introduce a barrier term \\(-\\log(-f_i(x))\\). For a given parameter \\(t &gt; 0\\), we solve the barrier subproblem  where  </p> <p>Equivalently,  </p> <p>Interpretation:</p> <ul> <li>The barrier term \\(\\phi(x)\\) is finite only for strictly feasible points (\\(f_i(x) &lt; 0\\)).</li> <li>As \\(x\\) approaches the boundary \\(f_i(x) \\to 0^-\\), the term \\(-\\log(-f_i(x)) \\to +\\infty\\).</li> <li>The parameter \\(t\\) controls the trade-off:</li> <li>small \\(t\\) (large \\(1/t\\)) \u2192 strong barrier, solution stays deep inside the feasible set;</li> <li>large \\(t\\) \u2192 barrier is weaker, solutions can move closer to the boundary.</li> </ul> <p>As \\(t \\to \\infty\\), solutions of the barrier subproblem approach the solution of the original constrained problem.</p>"},{"location":"convex/19b_optimization_constraints/#derivatives-of-the-barrier","title":"Derivatives of the Barrier","text":"<p>Let  Then \\(\\phi\\) is convex and twice differentiable on its domain. Its gradient and Hessian are  </p> <p>Key features:</p> <ul> <li>As \\(f_i(x) \\uparrow 0\\) (approaching the boundary from inside), the factor \\(1/(-f_i(x))\\) blows up, so \\(\\|\\nabla \\phi(x)\\|\\) becomes very large.</li> <li>This creates a strong repulsive force that prevents iterates from crossing the boundary.</li> <li>The barrier \u201cpushes\u201d the solution away from constraint violation, while the original objective \\(f_0(x)\\) pulls toward lower cost.</li> </ul> <p>The barrier subproblem  is a smooth equality-constrained problem. We can therefore apply equality-constrained Newton methods (Chapter 13) at each fixed \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#central-path-and-approximate-kkt-conditions","title":"Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^\\star(t)\\) be a minimizer of the barrier problem  </p> <p>The set \\(\\{x^\\star(t) : t &gt; 0\\}\\) is called the central path. As \\(t \\to \\infty\\), \\(x^\\star(t)\\) converges to a solution \\(x^\\star\\) of the original inequality-constrained problem.</p> <p>We can associate approximate dual variables to \\(x^\\star(t)\\):  </p> <p>Then the KKT-like relations hold:  </p> <p>Compare with the exact KKT conditions (for optimal \\((x^\\star,\\lambda^\\star,v^\\star)\\)):  </p> <p>Along the central path we have the relaxed complementarity condition  which tends to \\(0\\) as \\(t \\to \\infty\\). Hence the barrier formulation naturally yields approximate primal\u2013dual solutions whose KKT residuals shrink as we increase \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#geometric-and-physical-intuition","title":"Geometric and Physical Intuition","text":"<p>Consider the barrier-augmented objective  </p> <p>We can interpret this as:</p> <ul> <li>\\(t f_0(x)\\): an \u201cexternal potential\u201d pulling us toward low objective values.</li> <li>\\(-\\log(-f_i(x))\\): repulsive potentials that become infinite near the boundary \\(f_i(x)=0\\).</li> </ul> <p>At a minimizer \\(x^\\star(t)\\), we have  </p> <p>The gradient of the objective is exactly balanced by a weighted sum of constraint gradients. This is a force-balance condition:</p> <ul> <li>constraints \u201cpush back\u201d more strongly when \\(x\\) is close to their boundary,</li> <li>the interior-point iterates follow a smooth path that stays strictly feasible   and moves gradually toward the optimal boundary point.</li> </ul> <p>This picture explains both:</p> <ul> <li>why iterates never leave the feasible region, and  </li> <li>why the method naturally generates dual variables (the weights on constraint gradients).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#the-barrier-method","title":"The Barrier Method","text":"<p>The barrier method solves the original inequality-constrained problem by solving a sequence of barrier subproblems with increasing \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-barrier-method-conceptual-form","title":"Algorithm: Barrier Method (Conceptual Form)","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (\\(f_i(x) &lt; 0\\), \\(A x = b\\)),</li> <li>initial barrier parameter \\(t &gt; 0\\),</li> <li>barrier growth factor \\(\\mu &gt; 1\\) (e.g. \\(\\mu \\in [10,20]\\)),</li> <li>accuracy tolerance \\(\\varepsilon &gt; 0\\),</li> </ul> <p>repeat:</p> <ol> <li> <p>Centering step    Solve the equality-constrained problem        using an equality-constrained Newton method.    (In practice, we start from the previous solution and take a small number of Newton steps rather than \u201csolve exactly\u201d.)</p> </li> <li> <p>Update iterate    Let \\(x\\) be the resulting point (the approximate minimizer for current \\(t\\)).</p> </li> <li> <p>Check stopping criterion    For the barrier problem, one can show        where \\(p^\\star\\) is the optimal value of the original problem.    If        then stop: \\(x\\) is guaranteed to be within \\(\\varepsilon\\) (in objective value) of optimal.</p> </li> <li> <p>Increase \\(t\\)    Set \\(t := \\mu t\\) to weaken the barrier and move closer to the true boundary, then go back to Step 1.</p> </li> </ol> <p>Key parameters:</p> Symbol Role \\(t\\) barrier strength (larger \\(t\\) = weaker barrier, closer to solution) \\(\\mu\\) growth factor for \\(t\\) \\(\\varepsilon\\) desired accuracy (duality-gap based) \\(m\\) number of inequality constraints <p>In practice:</p> <ul> <li>\\(\\varepsilon\\) is often in the range \\(10^{-3}\\)\u2013\\(10^{-8}\\),</li> <li>\\(\\mu\\) is chosen to balance outer iterations vs inner Newton steps,</li> <li>the centering step is usually solved to modest accuracy, not exactness.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#from-barrier-methods-to-interior-point-methods","title":"From Barrier Methods to Interior-Point Methods","text":"<p>Pure barrier methods conceptually \u201csolve a sequence of problems for increasing \\(t\\)\u201d. Modern interior-point methods refine this idea:</p> <ul> <li>they update both primal variables \\(x\\) and dual variables \\((\\lambda, v)\\),</li> <li>they use Newton\u2019s method on the (perturbed) KKT system,</li> <li>they follow the central path by simultaneously enforcing:</li> <li>primal feasibility (\\(f_i(x) \\le 0\\), \\(A x = b\\)),</li> <li>dual feasibility (\\(\\lambda_i \\ge 0\\)),</li> <li>relaxed complementarity (\\(-\\lambda_i f_i(x) \\approx 1/t\\)).</li> </ul> <p>A typical primal\u2013dual step solves a linearized KKT system of the form  </p> <p>Newton\u2019s method applied to these equations yields search directions for \\((x,\\lambda,v)\\) that move toward the central path and reduce primal and dual residuals simultaneously. This is what modern LP/QP/SOCP/SDP solvers implement.</p> <p>You do not need to implement these methods from scratch to use them: in practice, you describe your problem in a modeling language (e.g. CVX, CVXPY, JuMP) and rely on an interior-point solver under the hood.</p>"},{"location":"convex/19b_optimization_constraints/#computational-and-practical-notes","title":"Computational and Practical Notes","text":"<p>Some important practical aspects:</p> <ol> <li> <p>Equality-constrained Newton inside    Each barrier subproblem is solved by equality-constrained Newton (Chapter 13). The main cost is solving the KKT linear system at each Newton step.</p> </li> <li> <p>Strict feasibility    Barrier and interior-point methods require a strictly feasible starting point \\(x\\) with \\(f_i(x) &lt; 0\\).  </p> </li> <li>Sometimes this is easy (e.g. nonnegativity constraints with a positive initial vector).  </li> <li> <p>Otherwise, a separate phase I problem is solved to find such a point or to certify infeasibility.</p> </li> <li> <p>Step size control    Because the barrier blows up near the boundary, too aggressive Newton steps may try to leave the feasible region. A backtracking line search is used to ensure:</p> </li> <li>sufficient decrease in the barrier objective,</li> <li> <p>and preservation of strict feasibility (\\(f_i(x) &lt; 0\\) remains true).</p> </li> <li> <p>Accuracy vs cost    The duality-gap bound \\(m/t\\) provides a clear trade-off:</p> </li> <li>small \\(m/t\\) (large \\(t\\)) \u2192 high accuracy, more iterations,</li> <li> <p>larger \\(m/t\\) \u2192 faster but less precise.</p> </li> <li> <p>Sparsity and structure    For large problems, exploiting sparsity in \\(A\\) and in the Hessians \\(\\nabla^2 f_i(x)\\) is crucial. Interior-point methods scale well when linear algebra is carefully optimized.</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#equality-vs-inequality-constrained-algorithms","title":"Equality vs Inequality-Constrained Algorithms","text":"<p>Finally, it is helpful to contrast the equality-only case (Chapter 13) with the inequality case.</p> Aspect Equality constraints \\(A x = b\\) Inequality constraints \\(f_i(x) \\le 0\\) Feasible set Affine subspace General convex region with boundary Typical algorithms Lagrange/KKT, equality-constrained Newton, null-space Barrier methods, primal\u2013dual interior-point methods Feasibility during iteration Can start infeasible and converge to \\(A x = b\\) Iterates kept strictly feasible (\\(f_i(x) &lt; 0\\)) Complementarity Not present (only equalities) \\(\\lambda_i f_i(x) = 0\\) at optimum, or \\(\\approx -1/t\\) along central path Geometric picture Optimization on a flat manifold Optimization in a convex region, repelled from boundary ML relevance Normalization, linear invariants, balance constraints Nonnegativity, margin constraints, safety/fairness limits <p>In summary:</p> <ul> <li>Equality-constrained methods operate directly on an affine manifold using KKT and Newton.  </li> <li>Inequality-constrained methods use smooth barriers (or primal\u2013dual perturbed KKT systems) to stay in the interior and gradually approach the boundary and the optimal point.</li> </ul> <p>Interior-point methods unify these perspectives and are the backbone of modern convex optimization software.</p>"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often runs at massive scale: millions (or billions) of variables, datasets too large to fit in memory, and constraints spread across machines or devices. Classical Newton or interior-point methods are beautiful mathematically, but their per-iteration cost and memory usage often make them impractical for these regimes.</p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to make convex optimization scalable. These ideas underpin the optimization engines behind most modern machine learning systems.</p>"},{"location":"convex/20_advanced/#motivation-structure-and-scale","title":"Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the challenge is not \u201cdoes a solution exist?\u201d but rather \u201ccan we compute it in time and memory?\u201d.</p> <p>Bottlenecks include:</p> <ul> <li>Memory: storing Hessians (or even full gradients) may be impossible.</li> <li>Data size: one full pass over all samples can already be expensive.</li> <li>Distributed data: samples are spread across devices / workers.</li> <li>Sparsity and separability: the problem often decomposes into many small pieces.</li> </ul> <p>A common template is the empirical risk + regularizer form  where</p> <ul> <li>each \\(f_i(x)\\) is a loss term for sample \\(i\\),</li> <li>\\(R(x)\\) is a regularizer (possibly nonsmooth, e.g. \\(\\lambda\\|x\\|_1\\)).</li> </ul> <p>The methods in this chapter are designed to exploit this structure:</p> <ul> <li>update only parts of \\(x\\) at a time (coordinate/block methods),</li> <li>use only some data per step (stochastic methods),</li> <li>split the problem into simpler subproblems (proximal / ADMM),</li> <li>or distribute computation across multiple machines (consensus methods).</li> </ul>"},{"location":"convex/20_advanced/#coordinate-descent","title":"Coordinate Descent","text":"<p>Coordinate descent updates one coordinate (or a small block of coordinates) at a time, holding all others fixed. It is especially effective when updates along a single coordinate are cheap to compute. Given \\(x^{(k)}\\), choose coordinate \\(i_k\\) and define</p> \\[ x^{(k+1)}_i = \\begin{cases} \\displaystyle \\arg\\min_{z \\in \\mathbb{R}} \\; f\\big(x_1^{(k+1)},\\dots,x_{i_k-1}^{(k+1)},z,x_{i_k+1}^{(k)},\\dots,x_n^{(k)}\\big), &amp; i = i_k,\\\\[4pt] x_i^{(k)}, &amp; i \\ne i_k. \\end{cases} \\] <p>In practice:</p> <ul> <li>\\(i_k\\) is chosen either cyclically (\\(1,2,\\dots,n,1,2,\\dots\\)) or randomly.</li> <li>Each coordinate update often has a closed form (e.g. soft-thresholding for LASSO).</li> <li>You never form or store the full gradient; you only need partial derivatives.</li> </ul> <p>Why it scales:</p> <ul> <li>Each step is very cheap \u2014 often \\(O(1)\\) or proportional to the number of nonzeros in the column corresponding to coordinate \\(i_k\\).</li> <li>In high dimensions (e.g., millions of features), this can be far more efficient than updating all coordinates at once.</li> </ul> <p>Convergence: If \\(f\\) is convex and has Lipschitz-continuous partial derivatives, coordinate descent (cyclic or randomized) converges to the global minimizer. Randomized coordinate descent often has clean expected convergence rates.</p> <p>ML context:</p> <ul> <li>LASSO / Elastic Net regression (coordinate updates are soft-thresholding),</li> <li>\\(\\ell_1\\)-penalized logistic regression,</li> <li>matrix factorization and dictionary learning (updating one factor vector at a time),</li> <li>problems where \\(R(x)\\) is separable: \\(R(x) = \\sum_i R_i(x_i)\\).</li> </ul>"},{"location":"convex/20_advanced/#stochastic-gradient-and-variance-reduced-methods","title":"Stochastic Gradient and Variance-Reduced Methods","text":"<p>When \\(N\\) (number of samples) is huge, computing the full gradient  every iteration is too expensive. Stochastic methods replace this full gradient with cheap, unbiased estimates based on small random subsets (mini-batches) of data.</p>"},{"location":"convex/20_advanced/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>At iteration \\(k\\):</p> <ol> <li>Sample a mini-batch \\(\\mathcal{B}_k \\subset \\{1,\\dots,N\\}\\).</li> <li>Form the stochastic gradient     </li> <li>Update        where \\(\\eta_k &gt; 0\\) is the learning rate.</li> </ol> <p>Properties:</p> <ul> <li>\\(\\mathbb{E}[\\widehat{\\nabla f}(x_k) \\mid x_k] = \\nabla f(x_k)\\) (unbiased),</li> <li>Each iteration is cheap (depends only on \\(|\\mathcal{B}_k|\\), not \\(N\\)),</li> <li>The noise can help escape shallow nonconvex traps (in deep learning).</li> </ul> <p>In convex settings, SGD trades off per-iteration cost against convergence speed: many cheap noisy steps instead of fewer expensive precise ones.</p>"},{"location":"convex/20_advanced/#step-sizes-and-averaging","title":"Step Sizes and Averaging","text":"<p>The step size \\(\\eta_k\\) is crucial:</p> <ul> <li>Too large \u2192 iterates diverge or oscillate.</li> <li>Too small \u2192 extremely slow progress.</li> </ul> <p>Typical schedules for convex problems:</p> <ul> <li>General convex: \\(\\eta_k = \\frac{c}{\\sqrt{k}}\\),</li> <li>Strongly convex: \\(\\eta_k = \\frac{c}{k}\\).</li> </ul> <p>Two important stabilization techniques:</p> <ol> <li>Decay the learning rate over time.</li> <li>Polyak\u2013Ruppert averaging: return the average        instead of the last iterate. Averaging cancels noise and leads to optimal \\(O(1/k)\\) rates in strongly convex settings.</li> </ol> <p>Mini-batch size can also grow with \\(k\\), gradually reducing variance while keeping early iterations cheap.</p>"},{"location":"convex/20_advanced/#convergence-rates","title":"Convergence Rates","text":"<p>For convex \\(f\\):</p> <ul> <li>With appropriate diminishing \\(\\eta_k\\), \\(\\mathbb{E}[f(x_k)] - f^\\star = O(k^{-1/2})\\).</li> </ul> <p>For strongly convex \\(f\\):</p> <ul> <li>With \\(\\eta_k = O(1/k)\\) and averaging, \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</li> </ul> <p>These rates are optimal for unbiased first-order stochastic methods.</p>"},{"location":"convex/20_advanced/#variance-reduced-methods-svrg-saga-sarah","title":"Variance-Reduced Methods (SVRG, SAGA, SARAH)","text":"<p>Plain SGD cannot easily reach very high accuracy because the gradient noise never fully disappears. Variance-reduced methods reduce this noise, especially near the solution, by periodically using the full gradient.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <ul> <li>Pick a reference point \\(\\tilde{x}\\) and compute \\(\\nabla f(\\tilde{x})\\).</li> <li>For inner iterations:      where \\(i_k\\) is a random sample index.</li> </ul> <p>Here \\(v_k\\) is still an unbiased estimator of \\(\\nabla f(x_k)\\), but its variance decays as \\(x_k\\) approaches \\(\\tilde{x}\\). For strongly convex \\(f\\), methods like SVRG and SAGA achieve linear convergence, comparable to full gradient descent but at near-SGD cost.</p>"},{"location":"convex/20_advanced/#momentum-and-adaptive-methods","title":"Momentum and Adaptive Methods","text":"<p>In practice, large-scale learning often uses SGD with various modifications:</p> <ul> <li> <p>Momentum / Nesterov: keep a moving average of gradients      which accelerates progress along consistent directions and damps oscillations.</p> </li> <li> <p>Adaptive methods (Adagrad, RMSProp, Adam): maintain coordinate-wise scales based on past squared gradients, effectively using a diagonal preconditioner that adapts to curvature and feature scales.</p> </li> </ul> <p>These methods are especially popular in deep learning. For convex problems, their theoretical behavior is subtle, but empirically they often converge faster in wall-clock time.</p>"},{"location":"convex/20_advanced/#proximal-and-composite-optimization","title":"Proximal and Composite Optimization","text":"<p>Many large-scale convex problems are composite:  where</p> <ul> <li>\\(g\\) is smooth convex with Lipschitz gradient (e.g. data-fitting term),</li> <li>\\(R\\) is convex but possibly nonsmooth (e.g. \\(\\lambda\\|x\\|_1\\), indicator of a constraint set, nuclear norm).</li> </ul> <p>The proximal gradient method (a.k.a. ISTA) updates as  where the proximal operator is  </p> <p>Intuition:</p> <ul> <li>The gradient step moves \\(x\\) in a direction that lowers the smooth term \\(g\\).</li> <li>The prox step solves a small \u201cregularized\u201d problem, pulling \\(x\\) toward a structure favored by \\(R\\) (sparsity, low rank, feasibility, etc.).</li> </ul> <p>Examples of prox operators:</p> <ul> <li>\\(R(x) = \\lambda\\|x\\|_1\\) \u2192 soft-thresholding (coordinate-wise shrinkage).</li> <li>\\(R\\) = indicator of a convex set \\(\\mathcal{X}\\) \u2192 projection onto \\(\\mathcal{X}\\) (so projected gradient is a special case).</li> <li>\\(R(X) = \\|X\\|_*\\) (nuclear norm) \u2192 singular value soft-thresholding.</li> </ul> <p>For large-scale problems:</p> <ul> <li>Proximal gradient scales like gradient descent: each iteration uses only \\(\\nabla g\\) and a prox (often cheap and parallelizable).</li> <li>Accelerated variants (FISTA) achieve \\(O(1/k^2)\\) rates for smooth \\(g\\).</li> </ul>"},{"location":"convex/20_advanced/#alternating-direction-method-of-multipliers-admm","title":"Alternating Direction Method of Multipliers (ADMM)","text":"<p>When objectives naturally split into simpler pieces depending on different variables, ADMM is a powerful tool. It is especially useful when:</p> <ul> <li>\\(f\\) and \\(g\\) have simple prox operators,</li> <li>the problem is distributed or separable across machines.</li> </ul> <p>Consider  </p> <p>The augmented Lagrangian is  with dual variable \\(y\\) and penalty parameter \\(\\rho &gt; 0\\).</p> <p>ADMM performs the iterations:  </p> <p>Interpretation:</p> <ul> <li>The \\(x\\)-update solves a subproblem involving \\(f\\) only.</li> <li>The \\(z\\)-update solves a subproblem involving \\(g\\) only.</li> <li>The \\(y\\)-update nudges the constraint \\(A x + B z = c\\) toward satisfaction.</li> </ul> <p>For convex \\(f,g\\), ADMM converges to a primal\u2013dual optimal point. It is particularly effective when the \\(x\\)- and \\(z\\)-subproblems have closed-form prox solutions or can be solved cheaply in parallel.</p> <p>ML use cases:</p> <ul> <li>Distributed LASSO / logistic regression,</li> <li>matrix completion and robust PCA,</li> <li>consensus optimization (each worker has local data but shares a global model),</li> <li>some federated learning formulations.</li> </ul>"},{"location":"convex/20_advanced/#majorizationminimization-mm-and-em-algorithms","title":"Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The Majorization\u2013Minimization (MM) principle constructs at each iterate \\(x_k\\) a surrogate function \\(g(\\cdot \\mid x_k)\\) that upper-bounds \\(f\\) and is easier to minimize.</p> <p>Requirements:  </p> <p>Then define  </p> <p>This guarantees monotone decrease:  </p> <p>The famous Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models, where the surrogate arises from Jensen\u2019s inequality and missing-data structure.</p> <p>Other examples:</p> <ul> <li>Iteratively Reweighted Least Squares (IRLS) for logistic regression and robust regression,</li> <li>MM surrogates for nonconvex penalties (e.g. smoothly approximating \\(\\ell_0\\)),</li> <li>mixture models and variational inference.</li> </ul>"},{"location":"convex/20_advanced/#distributed-and-parallel-optimization","title":"Distributed and Parallel Optimization","text":"<p>When data or variables are split across machines, we need distributed or parallel optimization schemes.</p>"},{"location":"convex/20_advanced/#synchronous-vs-asynchronous","title":"Synchronous vs Asynchronous","text":"<ul> <li>Synchronous methods: all workers compute local gradients or updates, then synchronize (e.g. parameter server, federated averaging).</li> <li>Asynchronous methods: workers update parameters without global synchronization, improving hardware utilization but introducing staleness and variance.</li> </ul>"},{"location":"convex/20_advanced/#consensus-optimization","title":"Consensus Optimization","text":"<p>A standard pattern is consensus form:  where \\(f_i\\) is the local objective on worker \\(i\\) and \\(z\\) is the global consensus variable.</p> <p>ADMM applied to this problem:</p> <ul> <li>Each worker updates its local \\(x_i\\) using only local data,</li> <li>The global variable \\(z\\) is updated by averaging or aggregation,</li> <li>Dual variables enforce agreement \\(x_i \\approx z\\).</li> </ul> <p>This template underlies many federated learning and parameter-server architectures.</p>"},{"location":"convex/20_advanced/#ml-context","title":"ML Context","text":"<ul> <li>Federated learning (phone/edge devices update local models and send summaries to a server),</li> <li>Large-scale convex optimization over sharded datasets,</li> <li>Distributed sparse regression, matrix factorization, and graphical model learning.</li> </ul>"},{"location":"convex/20_advanced/#handling-structure-sparsity-and-low-rank","title":"Handling Structure: Sparsity and Low Rank","text":"<p>Large-scale convex problems often have additional structure that we can exploit algorithmically:</p> Structure Typical Regularizer / Constraint Algorithmic Benefit Sparsity \\(\\ell_1\\), group lasso Cheap coordinate updates, soft-thresholding Low rank Nuclear norm \\(\\|X\\|_*\\) SVD-based prox; rank truncation Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed block updates Graph structure Total variation on graphs Local neighborhood computations Probability simplex simplex constraint or entropy term Mirror descent, simplex projections <p>Examples:</p> <ul> <li>In compressed sensing, \\(\\ell_1\\) regularization + sparse sensing matrices \u2192 very cheap mat\u2013vecs + prox operations.</li> <li>In matrix completion, nuclear norm structure + low-rank iterates \u2192 approximate SVD instead of full SVD.</li> <li>In TV denoising, local difference structure \u2192 each prox step involves only neighboring pixels/vertices.</li> </ul> <p>Exploiting structure can yield orders-of-magnitude speedups compared to generic solvers.</p>"},{"location":"convex/20_advanced/#summary-and-practical-guidance","title":"Summary and Practical Guidance","text":"<p>Different large-scale methods are appropriate in different regimes:</p> Method Gradient Access Scalability Parallelization Convexity Needed Typical Uses Coordinate Descent Partial gradients High Easy (blockwise) Convex LASSO, sparse GLMs, matrix factorization SGD / Mini-batch SGD Stochastic gradients Excellent Natural (data parallel) Convex / nonconvex Deep learning, logistic regression SVRG / SAGA (VR methods) Stochastic + periodic full gradient High Data parallel Convex, often strongly convex Large-scale convex ML, GLMs Proximal Gradient (ISTA/FISTA) Full gradient + prox Moderate\u2013High Easy Convex Composite objectives with structure ADMM Local subproblems High Designed for distributed Convex Consensus, distributed convex solvers MM / EM Surrogates Moderate Model-specific Convex / nonconvex Latent-variable models, IRLS Distributed / Federated Local gradients Very high Essential Often convex / smooth Federated learning, multi-agent systems"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithms Summary:  </p> <ul> <li>Smooth + \u2113\u2082 (strongly convex):   Newton / quasi-Newton, conjugate gradient, or accelerated gradient.</li> <li>Smooth + \u2113\u2081 (and variants):   proximal gradient or coordinate descent; for huge data, stochastic/proximal variants.</li> <li>Mixed penalties (\u2113\u2081 + \u2113\u2082):   treat as composite smooth + nonsmooth; prox and coordinate methods still apply.</li> <li>Large \\(N\\) or \\(n\\):   mini-batch / stochastic gradients (SGD, SVRG, etc.) on the smooth part + prox for the regulariser.</li> </ul> <p>Regularisation strength \\(\\lambda\\) is usually chosen via cross-validation or a validation set, exploring the accuracy\u2013simplicity trade-off.</p>"},{"location":"convex/21_models/#robust-regression-and-outlier-resistance","title":"Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#least-absolute-deviations-l1-loss","title":"Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#huber-loss","title":"Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints. That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly. This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#worst-case-robust-regression","title":"Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#maximum-likelihood-and-loss-design","title":"Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#structured-constraints-in-engineering-and-design","title":"Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#linear-and-conic-programming-the-canonical-models","title":"Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#risk-safety-margins-and-robust-design","title":"Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#cheat-sheet-if-your-problem-looks-like-this-use-that","title":"Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#matching-model-structure-to-algorithm-type","title":"Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#hierarchy-of-canonical-problems","title":"Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#linear-programming-lp","title":"Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#quadratic-programming-qp","title":"Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#quadratically-constrained-qp-qcqp","title":"Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#second-order-cone-programming-socp","title":"Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#semidefinite-programming-sdp","title":"Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#geometric-programming-gp","title":"Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#likelihood-based-convex-models-mle-and-glms","title":"Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#solver-selection-summary","title":"Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"convex/35_modern/","title":"18. Modern Optimizers in Machine Learning Frameworks","text":""},{"location":"convex/35_modern/#chapter-18-modern-optimizers-in-machine-learning","title":"Chapter 18: Modern Optimizers in Machine Learning","text":"<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization\u2014balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p> <p>This chapter connects the principles of convex optimization to the modern optimizers that power today\u2019s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>"},{"location":"convex/35_modern/#stochastic-optimization-overview","title":"Stochastic Optimization Overview","text":"<p>In machine learning, we often minimize an empirical risk:  where \\(\\ell(x; z_i)\\) is the loss on data sample \\(z_i\\).</p> <p>Computing the full gradient \\(\\nabla f(x)\\) is infeasible when \\(N\\) is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p> \\[ g_k = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla \\ell(x_k; z_i). \\] <p>This yields the Stochastic Gradient Descent (SGD) update:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k. \\] <p>SGD is the foundation for nearly all deep learning optimizers.</p>"},{"location":"convex/35_modern/#momentum-and-acceleration","title":"Momentum and Acceleration","text":"<p>SGD\u2019s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p> <p>  where \\(\\beta \\in [0,1)\\) controls inertia.</p> <p>Nesterov momentum adds a correction term anticipating the future position:</p> \\[ v_{k+1} = \\beta v_k + g(x_k - \\alpha \\beta v_k), \\quad x_{k+1} = x_k - \\alpha v_{k+1}. \\] <p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>"},{"location":"convex/35_modern/#adaptive-learning-rate-methods","title":"Adaptive Learning Rate Methods","text":"<p>Different parameters often require different step sizes. Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>"},{"location":"convex/35_modern/#adagrad","title":"AdaGrad","text":"<p>Keeps a cumulative sum of squared gradients:</p> <p>  and updates parameters as:</p> <p>  Good for sparse data, but the learning rate can shrink too quickly.</p>"},{"location":"convex/35_modern/#rmsprop","title":"RMSProp","text":"<p>A refinement of AdaGrad using exponential averaging:</p> \\[ E[g^2]_k = \\beta E[g^2]_{k-1} + (1-\\beta) g_k^2, \\] \\[ x_{k+1} = x_k - \\frac{\\alpha}{\\sqrt{E[g^2]_k + \\epsilon}} g_k. \\] <p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>"},{"location":"convex/35_modern/#adam-adaptive-moment-estimation","title":"Adam: Adaptive Moment Estimation","text":"<p>Adam combines momentum and adaptive scaling:</p> \\[ m_k = \\beta_1 m_{k-1} + (1-\\beta_1) g_k, \\quad v_k = \\beta_2 v_{k-1} + (1-\\beta_2) g_k^2, \\] \\[ \\hat{m}_k = \\frac{m_k}{1-\\beta_1^k}, \\quad \\hat{v}_k = \\frac{v_k}{1-\\beta_2^k}, \\] \\[ x_{k+1} = x_k - \\alpha \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\epsilon}. \\] <p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>"},{"location":"convex/35_modern/#variants-and-modern-extensions","title":"Variants and Modern Extensions","text":"Optimizer Key Idea Notes AdamW Decoupled weight decay from gradient update Better regularization RAdam Rectified Adam\u2014adaptive variance correction Improves stability early in training Lookahead Combines fast and slow weights Enhances robustness and convergence AdaBelief Uses prediction error instead of raw gradient variance More adaptive learning rates Lion Uses sign-based updates and momentum Efficient for large-scale training <p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>"},{"location":"convex/35_modern/#implicit-regularization-and-generalization","title":"Implicit Regularization and Generalization","text":"<p>Modern optimizers not only minimize loss\u2014they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p> <p>Empirical findings suggest:</p> <ul> <li>Large-batch training finds sharper minima (risk of overfitting).  </li> <li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li> <li>Adaptive optimizers may converge faster but generalize slightly worse.</li> </ul> <p>This trade-off drives ongoing research into optimizer design.</p>"},{"location":"convex/35_modern/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Learning Rate Most critical hyperparameter; use warm-up and decay schedules Batch Size Balances gradient noise and hardware efficiency Initialization Affects early dynamics, especially for Adam variants Gradient Clipping Prevents instability in exploding gradients Mixed Precision Use with adaptive optimizers for speed and memory savings"},{"location":"convex/35_modern/#comparative-behavior","title":"Comparative Behavior","text":"Method Adaptivity Speed Memory Typical Use SGD + Momentum Moderate Slow-medium Low General-purpose, good generalization RMSProp Adaptive per-parameter Medium-fast Medium Recurrent networks, nonstationary data Adam / AdamW Fully adaptive Fast High Deep networks, large-scale training RAdam / AdaBelief / Lion Advanced adaptivity Fast Medium Cutting-edge training tasks"},{"location":"convex/35_modern/#optimization-in-modern-deep-networks","title":"Optimization in Modern Deep Networks","text":"<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p> <ul> <li>Batch normalization modifies effective learning rates.  </li> <li>Skip connections ease gradient flow.  </li> <li>Large-scale distributed training relies on adaptive optimizers for stability.  </li> </ul> <p>Optimization is no longer an isolated procedure but part of the model\u2019s design philosophy.</p> <p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning\u2014adapting dynamically, balancing efficiency and generalization.</p>"},{"location":"convex/40_nonconvex/","title":"19. Beyond Convexity \u2013 Nonconvex and Global Optimization","text":""},{"location":"convex/40_nonconvex/#chapter-19-beyond-convexity-nonconvex-and-global-optimization","title":"Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization","text":"<p>Optimization extends far beyond the comfortable world of convexity. In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p> <p>Convex optimization gives us strong guarantees \u2014 every local minimum is global, and algorithms converge predictably. But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>"},{"location":"convex/40_nonconvex/#the-landscape-of-nonconvex-optimization","title":"The Landscape of Nonconvex Optimization","text":"<p>A nonconvex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) violates convexity; i.e., for some \\(x, y\\) and \\(\\theta \\in (0,1)\\),  Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p> <p>A typical nonconvex landscape looks like a mountainous terrain \u2014 smooth in some regions, rugged in others. An optimization algorithm\u2019s path depends strongly on initialization and stochastic effects.</p>"},{"location":"convex/40_nonconvex/#example-a-simple-nonconvex-function","title":"Example: A Simple Nonconvex Function","text":"<p>  This function has multiple stationary points: - \\((0,0)\\) (a saddle), - \\((1,1)\\) and \\((-1,-1)\\) (local minima), - \\((1,-1)\\) and \\((-1,1)\\) (local maxima).</p> <p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>"},{"location":"convex/40_nonconvex/#local-vs-global-minima","title":"Local vs. Global Minima","text":"<p>A point \\(x^*\\) is a local minimum if:  </p> <p>A global minimum satisfies the stronger condition:  </p> <p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad \u2014 and there may be exponentially many of them.</p>"},{"location":"convex/40_nonconvex/#classes-of-nonconvex-problems","title":"Classes of Nonconvex Problems","text":"<p>Nonconvex problems appear in several distinct forms:</p> Type Example Challenge Smooth nonconvex Neural network training Multiple minima, saddle points Nonsmooth nonconvex Sparse regularization, ReLU activations Undefined gradients Discrete / combinatorial Scheduling, routing, integer programs Exponential search space Black-box Simulation-based optimization No derivatives or analytical form <p>Each category requires different algorithmic strategies \u2014 from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>"},{"location":"convex/40_nonconvex/#local-optimization-strategies","title":"Local Optimization Strategies","text":"<p>Even in nonconvex settings, local optimization remains useful when: - The problem is nearly convex (e.g., locally convex around good minima), - The initialization is close to a desired basin of attraction, - Or the goal is approximate, not exact, optimality.</p>"},{"location":"convex/40_nonconvex/#gradient-descent-and-its-variants","title":"Gradient Descent and Its Variants","text":"<p>Gradient descent behaves well if \\(f\\) is smooth and Lipschitz-continuous:  However, convergence is only to a stationary point \u2014 not necessarily a minimum.</p> <p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>"},{"location":"convex/40_nonconvex/#global-optimization-strategies","title":"Global Optimization Strategies","text":"<p>To seek the global minimum, algorithms must explore the search space more broadly. Common strategies include:</p> <ol> <li> <p>Multiple Starts:    Run local optimization from diverse random initial points and keep the best solution.</p> </li> <li> <p>Continuation and Homotopy Methods:    Start from a smooth, convex approximation \\(f_\\lambda\\) of \\(f\\) and gradually transform it into the true objective as \\(\\lambda \\to 0\\).</p> </li> <li> <p>Stochastic Search and Simulated Annealing:    Introduce randomness in updates to jump between basins.</p> </li> <li> <p>Population-Based Methods:    Maintain a swarm or population of candidate solutions evolving by selection and variation \u2014 leading to metaheuristic algorithms like GA and PSO.</p> </li> </ol>"},{"location":"convex/40_nonconvex/#theoretical-challenges","title":"Theoretical Challenges","text":"<p>Without convexity, most strong results vanish:</p> <ul> <li>Global optimality cannot be guaranteed.</li> <li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li> <li>Complexity often grows exponentially with problem size.</li> </ul> <p>However, theory is not hopeless:</p> <ul> <li>Many nonconvex problems are \u201cbenign\u201d \u2014 e.g., matrix factorization, phase retrieval, or deep linear networks \u2014 having no bad local minima.  </li> <li>Random initialization and overparameterization often aid convergence to global minima in practice.</li> </ul>"},{"location":"convex/40_nonconvex/#geometry-of-saddle-points","title":"Geometry of Saddle Points","text":"<p>A saddle point satisfies \\(\\nabla f(x)=0\\) but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p> <p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>"},{"location":"convex/40_nonconvex/#deterministic-vs-stochastic-global-methods","title":"Deterministic vs. Stochastic Global Methods","text":"Deterministic Methods Stochastic Methods Systematic exploration of space (branch &amp; bound, interval analysis) Randomized search (simulated annealing, evolutionary algorithms) Can provide certificates of global optimality Typically approximate but scalable High computational cost Naturally parallelizable <p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>"},{"location":"convex/40_nonconvex/#a-taxonomy-of-optimization-beyond-convexity","title":"A Taxonomy of Optimization Beyond Convexity","text":"Family Typical Algorithms When to Use Derivative-Free (Black-Box) Nelder\u2013Mead, CMA-ES, Bayesian Opt. When gradients unavailable Metaheuristic (Evolutionary) GA, PSO, DE, ACO Complex landscapes, combinatorial problems Modern Stochastic Gradient Adam, RMSProp, Lion Deep learning, large-scale models Combinatorial / Discrete Branch &amp; Bound, Tabu, SA Integer or graph-based problems Learning-Based Optimizers Meta-learning, Reinforcement methods Adaptive, data-driven optimization"},{"location":"convex/42_derivativefree/","title":"20. Derivative-Free and Black-Box Optimization","text":""},{"location":"convex/42_derivativefree/#chapter-20-derivative-free-and-black-box-optimization","title":"Chapter 20: Derivative-Free and Black-Box Optimization","text":"<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p> <p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>"},{"location":"convex/42_derivativefree/#motivation-and-challenges","title":"Motivation and Challenges","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be an objective function.  </p> <p>A derivative-free algorithm seeks to minimize \\(f(x)\\) using only evaluations of \\(f(x)\\), without access to \\(\\nabla f(x)\\) or \\(\\nabla^2 f(x)\\).</p> <p>Key challenges:</p> <ul> <li>No gradient information \u2192 difficult to infer descent directions.  </li> <li>Expensive evaluations \u2192 every call to \\(f(x)\\) might require a simulation or experiment.  </li> <li>Noise and stochasticity \u2192 evaluations may be corrupted by measurement or sampling error.  </li> <li>High-dimensionality \u2192 sampling-based methods scale poorly with \\(n\\).</li> </ul> <p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>"},{"location":"convex/42_derivativefree/#classification-of-derivative-free-methods","title":"Classification of Derivative-Free Methods","text":"Category Representative Algorithms Main Idea Direct Search Nelder\u2013Mead, Pattern Search, MADS Explore the space via geometric moves or meshes Model-Based BOBYQA, Trust-Region DFO Build local quadratic or surrogate models of \\(f\\) Evolutionary / Population-Based CMA-ES, Differential Evolution Evolve a population using stochastic operators Probabilistic / Bayesian Bayesian Optimization Use probabilistic surrogate models to guide exploration"},{"location":"convex/42_derivativefree/#direct-search-methods","title":"Direct Search Methods","text":"<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>"},{"location":"convex/42_derivativefree/#neldermead-simplex-method","title":"Nelder\u2013Mead Simplex Method","text":"<p>Perhaps the most famous derivative-free algorithm, Nelder\u2013Mead maintains a simplex \u2014 a polytope of \\(n+1\\) vertices in \\(\\mathbb{R}^n\\).</p> <p>At each iteration:</p> <ol> <li>Evaluate \\(f\\) at all simplex vertices.</li> <li>Reflect, expand, contract, or shrink the simplex depending on performance.</li> <li>Continue until simplex collapses near a minimum.</li> </ol> <p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>"},{"location":"convex/42_derivativefree/#pattern-search-methods","title":"Pattern Search Methods","text":"<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p> <p>Typical update rule:  </p> <p>where \\(d_i\\) is a direction from a finite set (e.g., coordinate axes). If a direction yields improvement, move there; otherwise, shrink \\(\\Delta_k\\).</p>"},{"location":"convex/42_derivativefree/#mesh-adaptive-direct-search-mads","title":"Mesh Adaptive Direct Search (MADS)","text":"<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>"},{"location":"convex/42_derivativefree/#model-based-methods","title":"Model-Based Methods","text":"<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>"},{"location":"convex/42_derivativefree/#trust-region-dfo","title":"Trust-Region DFO","text":"<p>A local model \\(m_k(x)\\) (often quadratic) is built to approximate \\(f\\) near the current iterate \\(x_k\\):  The next iterate solves a trust-region subproblem:  The trust region size \\(\\Delta_k\\) adapts based on how well \\(m_k\\) predicts true function values.</p>"},{"location":"convex/42_derivativefree/#bobyqa-bound-optimization-by-quadratic-approximation","title":"BOBYQA (Bound Optimization BY Quadratic Approximation)","text":"<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>"},{"location":"convex/42_derivativefree/#evolution-strategies-and-population-methods","title":"Evolution Strategies and Population Methods","text":"<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>"},{"location":"convex/42_derivativefree/#covariance-matrix-adaptation-evolution-strategy-cma-es","title":"Covariance Matrix Adaptation Evolution Strategy (CMA-ES)","text":"<p>CMA-ES is a powerful stochastic search algorithm. It iteratively samples new points from a multivariate Gaussian distribution:  where \\(m_k\\) is the current mean, \\(\\sigma_k\\) the global step-size, and \\(C_k\\) the covariance matrix.</p> <p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p> <p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>"},{"location":"convex/42_derivativefree/#differential-evolution-de","title":"Differential Evolution (DE)","text":"<p>DE evolves a population \\(\\{x_i\\}\\) via vector differences:   where \\(r1, r2, r3\\) are random distinct indices and \\(F\\) controls mutation strength.</p> <p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>"},{"location":"convex/42_derivativefree/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>When function evaluations are expensive (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>"},{"location":"convex/42_derivativefree/#core-idea","title":"Core Idea","text":"<p>Model the objective as a random function \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))\\) (Gaussian Process prior). After each evaluation, update the posterior mean and variance to quantify uncertainty.</p> <p>Use an acquisition function \\(a(x)\\) to select the next evaluation point:  balancing exploration (high uncertainty) and exploitation (low expected value).</p> <p>Common acquisition functions:</p> <ul> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Upper Confidence Bound (UCB)</li> </ul>"},{"location":"convex/42_derivativefree/#surrogate-models-beyond-gaussian-processes","title":"Surrogate Models Beyond Gaussian Processes","text":"<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs: - Tree-structured Parzen Estimators (TPE) - Random forests (SMAC) - Neural network surrogates (Bayesian neural networks)</p> <p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>"},{"location":"convex/42_derivativefree/#hybrid-and-adaptive-approaches","title":"Hybrid and Adaptive Approaches","text":"<p>Modern applications often combine derivative-free and gradient-based techniques:</p> <ul> <li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li> <li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li> <li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li> </ul> <p>Such hybridization reflects a pragmatic view: no single optimizer is best \u2014 adaptability matters most.</p>"},{"location":"convex/42_derivativefree/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Function evaluations expensive Use Bayesian or model-based methods Noisy evaluations Use averaging, smoothing, or robust estimators High dimension (\\(n &gt; 50\\)) Prefer CMA-ES or evolutionary strategies Box constraints Methods like BOBYQA, DE, or PSO Parallel computation available Population-based methods excel <p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize anything we can evaluate. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p> <p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors \u2014 turning randomness into a powerful search strategy.</p>"},{"location":"convex/44_metaheuristic/","title":"21. Metaheuristic and Evolutionary Optimization","text":""},{"location":"convex/44_metaheuristic/#chapter-21-metaheuristic-and-evolutionary-algorithms","title":"Chapter 21: Metaheuristic and Evolutionary Algorithms","text":"<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions.  In these settings, metaheuristic algorithms\u2014inspired by nature, biology, and collective behavior\u2014provide robust and flexible alternatives.</p> <p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas. They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>"},{"location":"convex/44_metaheuristic/#principles-of-metaheuristic-optimization","title":"Principles of Metaheuristic Optimization","text":"<p>All metaheuristics share three key principles:</p> <ol> <li> <p>Population-Based Search:    Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p> </li> <li> <p>Variation Operators:    Create new solutions via mutation, recombination, or stochastic perturbations.</p> </li> <li> <p>Selection and Adaptation:    Favor candidates with better objective values, guiding the search toward promising regions.</p> </li> </ol> <p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>"},{"location":"convex/44_metaheuristic/#genetic-algorithms-ga","title":"Genetic Algorithms (GA)","text":""},{"location":"convex/44_metaheuristic/#biological-inspiration","title":"Biological Inspiration","text":"<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>"},{"location":"convex/44_metaheuristic/#representation","title":"Representation","text":"<p>A solution (individual) is represented as a chromosome\u2014often a binary string, vector of reals, or permutation. Each position (gene) encodes part of the decision variable.</p>"},{"location":"convex/44_metaheuristic/#algorithm-outline","title":"Algorithm Outline","text":"<ol> <li>Initialize a population \\(\\{x_i\\}_{i=1}^N\\) randomly.  </li> <li>Evaluate fitness \\(f(x_i)\\) for all individuals.  </li> <li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li> <li> <p>Apply:</p> <ul> <li>Crossover: combine genetic material of two parents.  </li> <li>Mutation: randomly alter some genes to maintain diversity.  </li> </ul> </li> <li> <p>Form a new population and repeat until convergence.</p> </li> </ol>"},{"location":"convex/44_metaheuristic/#crossover-and-mutation-examples","title":"Crossover and Mutation Examples","text":"<ul> <li>Single-point crossover: exchange genes after a random index.  </li> <li>Gaussian mutation: add small noise to continuous parameters.  </li> </ul>"},{"location":"convex/44_metaheuristic/#strengths-and-weaknesses","title":"Strengths and Weaknesses","text":"Strengths Weaknesses Highly parallel, robust, domain-independent Requires many function evaluations Effective for combinatorial and discrete optimization Parameter tuning (mutation, crossover rates) is nontrivial"},{"location":"convex/44_metaheuristic/#differential-evolution-de","title":"Differential Evolution (DE)","text":"<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>"},{"location":"convex/44_metaheuristic/#core-idea","title":"Core Idea","text":"<p>Mutation is performed using differences of population members:  where \\(r1, r2, r3\\) are random distinct indices and \\(F \\in [0,2]\\) controls mutation amplitude.</p> <p>Then crossover forms trial vectors:  and selection chooses between \\(x_i\\) and \\(u_i\\) based on objective value.</p>"},{"location":"convex/44_metaheuristic/#features","title":"Features","text":"<ul> <li>Self-adaptive exploration of the search space.</li> <li>Suitable for continuous, multimodal functions.</li> <li>Simple to implement, with few control parameters.</li> </ul>"},{"location":"convex/44_metaheuristic/#particle-swarm-optimization-pso","title":"Particle Swarm Optimization (PSO)","text":"<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p> <p>Each particle \\(i\\) has position \\(x_i\\) and velocity \\(v_i\\), updated as:   where:</p> <ul> <li>\\(p_i\\) = personal best position of particle \\(i\\),</li> <li>\\(g\\) = best global position found by the swarm,</li> <li>\\(w\\), \\(c_1\\), \\(c_2\\) are weight and learning coefficients,</li> <li>\\(r_1\\), \\(r_2\\) are random numbers in \\([0,1]\\).</li> </ul> <p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>"},{"location":"convex/44_metaheuristic/#convergence-behavior","title":"Convergence Behavior","text":"<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>"},{"location":"convex/44_metaheuristic/#strengths","title":"Strengths","text":"<ul> <li>Few parameters, easy to implement.</li> <li>Works well for noisy or discontinuous problems.</li> <li>Naturally parallelizable.</li> </ul>"},{"location":"convex/44_metaheuristic/#simulated-annealing-sa","title":"Simulated Annealing (SA)","text":"<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy \u2014 a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual \u201ccooling\u201d in the search for a global minimum.</p>"},{"location":"convex/44_metaheuristic/#physical-analogy","title":"Physical Analogy","text":"<p>In thermodynamics, a system at temperature \\(T\\) has probability of occupying a state with energy \\(E\\) given by the Boltzmann distribution:</p> \\[ P(E) \\propto e^{-E / (kT)}. \\] <p>At high temperature, the system freely explores many states. As \\(T\\) decreases, it becomes increasingly likely to remain near states of minimal energy.</p> <p>Simulated Annealing maps this principle to optimization by treating:</p> <ul> <li>The objective function \\(f(x)\\) as the system\u2019s energy.</li> <li>The solution vector \\(x\\) as a configuration.</li> <li>The temperature \\(T\\) as a control parameter determining randomness.</li> </ul>"},{"location":"convex/44_metaheuristic/#algorithm-outline_1","title":"Algorithm Outline","text":"<ol> <li> <p>Initialization</p> <ul> <li>Choose an initial solution \\(x_0\\) and initial temperature \\(T_0\\).</li> <li>Set a cooling schedule \\(T_{k+1} = \\alpha T_k\\), with \\(\\alpha \\in (0,1)\\).</li> </ul> </li> <li> <p>Iteration</p> <ul> <li>Generate a candidate \\(x'\\) from \\(x_k\\) via a small random perturbation.</li> <li>Compute \\(\\Delta f = f(x') - f(x_k)\\).</li> <li>Accept or reject based on the Metropolis criterion:</li> </ul> <p> </p> </li> <li> <p>Cooling</p> <ul> <li> <p>Reduce the temperature gradually according to the schedule.</p> </li> <li> <p>Repeat until \\(T\\) becomes sufficiently small or the system stabilizes.</p> </li> </ul> </li> </ol>"},{"location":"convex/44_metaheuristic/#interpretation","title":"Interpretation","text":"<ul> <li> <p>At high temperatures, SA accepts both better and worse moves \u2192 exploration.  </p> </li> <li> <p>At low temperatures, it becomes increasingly selective \u2192 exploitation.</p> </li> </ul> <p>This balance allows SA to escape local minima and approach the global optimum over time.</p>"},{"location":"convex/44_metaheuristic/#cooling-schedules","title":"Cooling Schedules","text":"<p>The temperature schedule determines convergence quality:</p> Type Formula Behavior Exponential \\(T_{k+1} = \\alpha T_k\\) Simple, widely used Linear \\(T_{k+1} = T_0 - \\beta k\\) Faster cooling, less exploration Logarithmic \\(T_k = \\frac{T_0}{\\log(k + c)}\\) Theoretically convergent (slow) Adaptive Adjust based on recent acceptance rates Practical and self-tuning <p>A slower cooling schedule improves accuracy but increases computational cost.</p>"},{"location":"convex/44_metaheuristic/#ant-colony-optimization-aco","title":"Ant Colony Optimization (ACO)","text":""},{"location":"convex/44_metaheuristic/#biological-basis","title":"Biological Basis","text":"<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p> <p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity \\(\\tau_{ij}\\) and heuristic visibility \\(\\eta_{ij}\\):  </p>"},{"location":"convex/44_metaheuristic/#pheromone-update","title":"Pheromone Update","text":"<p>After all ants construct their tours:  where \\(\\rho\\) controls evaporation and \\(\\Delta\\tau_{ij}\\) reinforces paths used by good solutions.</p> <p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>"},{"location":"convex/44_metaheuristic/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>Every metaheuristic must balance:</p> <ul> <li>Exploration: sampling diverse regions to escape local minima.  </li> <li>Exploitation: refining known good solutions to reach local optima.</li> </ul> High Exploration High Exploitation GA with strong mutation PSO with low inertia DE with high \\(F\\) ACO with low evaporation rate Random restarts Local refinement <p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>"},{"location":"convex/44_metaheuristic/#hybrid-and-memetic-algorithms","title":"Hybrid and Memetic Algorithms","text":"<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p> <p>Example:</p> <ol> <li>Use PSO or GA to explore broadly.  </li> <li>Apply gradient descent or Nelder\u2013Mead locally near promising candidates.</li> </ol> <p>This hybridization often yields faster convergence and improved accuracy.</p>"},{"location":"convex/44_metaheuristic/#performance-and-practical-tips","title":"Performance and Practical Tips","text":"Aspect Guideline Initialization Use wide, random distributions to promote diversity Parameter Tuning Use adaptive schedules (e.g., cooling, inertia decay) Population Size Larger for global search, smaller for fine-tuning Parallelism Evaluate populations concurrently for efficiency Stopping Criteria Use both iteration limits and stagnation detection <p>Metaheuristics are heuristic by design \u2014 they do not guarantee global optimality, but offer practical success across many fields. Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p>"},{"location":"convex/48_advanced_combinatorial/","title":"22. Advanced Topics in Combinatorial Optimization","text":""},{"location":"convex/48_advanced_combinatorial/#chapter-22-advanced-topics-in-combinatorial-optimization","title":"Chapter 22: Advanced Topics in Combinatorial Optimization","text":"<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p> <p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>"},{"location":"convex/48_advanced_combinatorial/#nature-of-combinatorial-problems","title":"Nature of Combinatorial Problems","text":"<p>A combinatorial optimization problem can be expressed as:</p> \\[ \\min_{x \\in \\mathcal{F}} f(x), \\] <p>where \\(\\mathcal{F}\\) is a finite or countable set of feasible solutions, often exponentially large in size.</p> <p>Example forms include:</p> <ul> <li>Binary decisions: \\(x_i \\in \\{0,1\\}\\)</li> <li>Integer constraints: \\(x_i \\in \\mathbb{Z}\\)</li> <li>Permutations: ordering or ranking elements</li> </ul> <p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>"},{"location":"convex/48_advanced_combinatorial/#graph-theoretic-foundations","title":"Graph-Theoretic Foundations","text":"<p>Many combinatorial problems are naturally represented as graphs \\(G = (V, E)\\).</p>"},{"location":"convex/48_advanced_combinatorial/#shortest-path-problem","title":"Shortest Path Problem","text":"<p>Given edge weights \\(w_{ij}\\), find a path from \\(s\\) to \\(t\\) minimizing total weight:  Efficiently solvable by Dijkstra\u2019s or Bellman\u2013Ford algorithms.</p>"},{"location":"convex/48_advanced_combinatorial/#minimum-spanning-tree-mst","title":"Minimum Spanning Tree (MST)","text":"<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal\u2019s or Prim\u2019s algorithm in \\(O(E\\log V)\\) time.</p>"},{"location":"convex/48_advanced_combinatorial/#maximum-flow-minimum-cut","title":"Maximum Flow / Minimum Cut","text":"<p>Determine how much \u201cflow\u201d can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>"},{"location":"convex/48_advanced_combinatorial/#integer-linear-programming-ilp","title":"Integer Linear Programming (ILP)","text":"<p>An integer program seeks:  </p> <p>It generalizes many classical problems:</p> <ul> <li>Knapsack  </li> <li>Assignment  </li> <li>Scheduling  </li> <li>Facility location</li> </ul> <p>Relaxing \\(x \\in \\mathbb{Z}^n\\) to \\(x \\in \\mathbb{R}^n\\) yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>"},{"location":"convex/48_advanced_combinatorial/#relaxation-and-rounding","title":"Relaxation and Rounding","text":"<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>"},{"location":"convex/48_advanced_combinatorial/#lp-relaxation","title":"LP Relaxation","text":"<p>For binary variables \\(x_i \\in \\{0,1\\}\\), relax to \\(0 \\le x_i \\le 1\\) and solve via simplex or interior-point methods.</p>"},{"location":"convex/48_advanced_combinatorial/#semidefinite-relaxation","title":"Semidefinite Relaxation","text":"<p>For quadratic binary problems, lift to a positive semidefinite matrix \\(X = xx^\\top\\):  Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>"},{"location":"convex/48_advanced_combinatorial/#randomized-rounding","title":"Randomized Rounding","text":"<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>"},{"location":"convex/48_advanced_combinatorial/#branch-and-bound-and-search-trees","title":"Branch-and-Bound and Search Trees","text":"<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>"},{"location":"convex/48_advanced_combinatorial/#basic-principle","title":"Basic Principle","text":"<ol> <li>Partition the feasible set into subsets (branching).  </li> <li>Compute upper/lower bounds for each subset.  </li> <li>Prune branches that cannot contain the optimum.  </li> </ol> <p>The algorithm systematically explores a search tree, guided by bounds.</p>"},{"location":"convex/48_advanced_combinatorial/#bounding-via-relaxations","title":"Bounding via Relaxations","text":"<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>"},{"location":"convex/48_advanced_combinatorial/#dynamic-programming","title":"Dynamic Programming","text":"<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p> \\[ \\text{OPT}(S) = \\min_{x \\in S} \\{ c(x) + \\text{OPT}(S') \\}. \\] <p>It is exact but can suffer from exponential growth (\u201ccurse of dimensionality\u201d).</p> <p>Applications:</p> <ul> <li>Shortest paths</li> <li>Sequence alignment</li> <li>Knapsack</li> <li>Resource allocation</li> </ul> <p>DP offers exact solutions when structure allows sequential decomposition.</p>"},{"location":"convex/48_advanced_combinatorial/#heuristics-and-metaheuristics-for-combinatorial-problems","title":"Heuristics and Metaheuristics for Combinatorial Problems","text":"<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>"},{"location":"convex/48_advanced_combinatorial/#greedy-heuristics","title":"Greedy Heuristics","text":"<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal\u2019s MST). Fast but not always globally optimal.</p>"},{"location":"convex/48_advanced_combinatorial/#local-search-and-hill-climbing","title":"Local Search and Hill Climbing","text":"<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>"},{"location":"convex/48_advanced_combinatorial/#metaheuristic-extensions","title":"Metaheuristic Extensions","text":"<ul> <li>Simulated Annealing: controlled random acceptance of worse moves.  </li> <li>Tabu Search: memory-based diversification.  </li> <li>Ant Colony Optimization: probabilistic path construction.  </li> <li>Genetic Algorithms and PSO: population-based evolution.  </li> </ul> <p>These approaches generalize to discrete structures with minimal problem-specific design.</p>"},{"location":"convex/48_advanced_combinatorial/#approximation-algorithms","title":"Approximation Algorithms","text":"<p>Some combinatorial problems are provably intractable but allow approximation guarantees:  where \\(\\alpha \\ge 1\\) is the approximation ratio.</p> <p>Examples:</p> <ul> <li>Greedy Set Cover: \\(\\alpha = \\ln n + 1\\) </li> <li>Christofides\u2019 Algorithm for TSP: \\(\\alpha = 1.5\\) </li> <li>MAX-CUT SDP Relaxation: \\(\\alpha \\approx 0.878\\)</li> </ul> <p>Approximation theory blends combinatorics with convex relaxation insights.</p>"},{"location":"convex/48_advanced_combinatorial/#advanced-topics-constraint-programming-and-decomposition","title":"Advanced Topics: Constraint Programming and Decomposition","text":""},{"location":"convex/48_advanced_combinatorial/#constraint-programming-cp","title":"Constraint Programming (CP)","text":"<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>"},{"location":"convex/48_advanced_combinatorial/#benders-and-dantzigwolfe-decomposition","title":"Benders and Dantzig\u2013Wolfe Decomposition","text":"<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>"},{"location":"convex/48_advanced_combinatorial/#cutting-plane-methods","title":"Cutting Plane Methods","text":"<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>"},{"location":"convex/48_advanced_combinatorial/#applications-across-domains","title":"Applications Across Domains","text":"Field Combinatorial Problem Examples Logistics Vehicle routing, warehouse layout Telecommunications Network design, channel allocation Machine Learning Feature selection, clustering, model compression Finance Portfolio optimization with integer positions Bioinformatics Genome assembly, protein structure inference <p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p> <p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p>"},{"location":"convex/50_future/","title":"23. The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":""},{"location":"convex/50_future/#chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence","title":"Chapter 23: The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":"<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead \u2014 toward optimization methods that learn, adapt, and reason \u2014 merging human insight, data-driven modeling, and algorithmic intelligence.</p>"},{"location":"convex/50_future/#from-fixed-algorithms-to-adaptive-systems","title":"From Fixed Algorithms to Adaptive Systems","text":"<p>Traditional optimization algorithms are designed by experts and fixed in form:</p> \\[ x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\] <p>or</p> \\[ x_{k+1} = \\text{Update}(x_k, \\nabla f(x_k); \\theta_{\\text{fixed}}). \\] <p>But real-world problems change over time \u2014 data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>"},{"location":"convex/50_future/#optimization-as-learning","title":"Optimization as Learning","text":"<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p> <p>A meta-optimizer \\(\\text{Opt}_\\theta\\) is parameterized by \\(\\theta\\), and trained to minimize:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{f \\sim \\mathcal{D}}[f(\\text{Opt}_\\theta(f))], \\] <p>where \\(\\mathcal{D}\\) is a distribution over problem instances.</p> <p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>"},{"location":"convex/50_future/#reinforcement-learned-optimization","title":"Reinforcement-Learned Optimization","text":"<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p> <p>At each iteration:</p> <ul> <li>State: current iterate \\(x_t\\), gradient \\(\\nabla f(x_t)\\), and loss \\(f(x_t)\\) </li> <li>Action: choose an update \\(\\Delta x_t\\) </li> <li>Reward: improvement in objective, \\(r_t = -[f(x_{t+1}) - f(x_t)]\\)</li> </ul> <p>A policy \\(\\pi_\\theta\\) learns to output update steps that maximize expected reward. This creates an optimizer that discovers efficient update strategies through experience.</p> <p>RL-based optimizers have been successfully applied in:</p> <ul> <li>Hyperparameter tuning  </li> <li>Neural architecture search  </li> <li>Online control systems  </li> <li>Adaptive sampling and scheduling</li> </ul>"},{"location":"convex/50_future/#neuroevolution-and-population-learning","title":"Neuroevolution and Population Learning","text":"<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly. Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p> <p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p> \\[ \\nabla_\\theta \\mathbb{E}[f(\\theta)] \\approx \\frac{1}{\\sigma} \\mathbb{E}[f(\\theta + \\sigma \\epsilon)\\epsilon]. \\] <p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration\u2013exploitation.</p>"},{"location":"convex/50_future/#optimization-and-generative-models","title":"Optimization and Generative Models","text":"<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective: Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p> <p>For example:</p> <ul> <li>Optimize a latent vector \\(z\\) to generate a design with desired properties.  </li> <li>Use differentiable surrogates to backpropagate through generative pipelines.  </li> <li>Apply gradient-based search within learned manifolds.</li> </ul> <p>This blending of optimization and generation enables creativity \u2014 from molecule design to engineering shape synthesis.</p>"},{"location":"convex/50_future/#federated-and-decentralized-optimization","title":"Federated and Decentralized Optimization","text":"<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization. Each client \\(i\\) holds local data \\(D_i\\) and solves:</p> \\[ \\min_x \\; F(x) = \\frac{1}{N}\\sum_i f_i(x), \\] <p>without sharing raw data.</p> <p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p> <p>Challenges include:</p> <ul> <li>Communication efficiency  </li> <li>Heterogeneity of data and computation  </li> <li>Privacy and fairness constraints</li> </ul>"},{"location":"convex/50_future/#optimization-under-uncertainty","title":"Optimization Under Uncertainty","text":"<p>Modern systems often face uncertain environments: - Random perturbations in data - Dynamic constraints - Unpredictable feedback</p> <p>Approaches to manage uncertainty include:</p> <ol> <li> <p>Robust Optimization:    Minimize worst-case loss under bounded perturbations:     </p> </li> <li> <p>Stochastic Programming:    Optimize expected value or risk measure:     </p> </li> <li> <p>Distributionally Robust Optimization (DRO):    Hedge against model misspecification by optimizing over nearby probability distributions.</p> </li> </ol> <p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>"},{"location":"convex/50_future/#quantum-and-analog-optimization","title":"Quantum and Analog Optimization","text":"<p>As hardware advances, new paradigms emerge: - Quantum Annealing: uses quantum tunneling to escape local minima. - Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem. - Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p> <p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>"},{"location":"convex/50_future/#optimization-and-intelligence","title":"Optimization and Intelligence","text":"<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes \u2014 in essence, optimization loops.</p> <p>Emerging research seeks to unify:</p> <ul> <li>Learning to optimize \u2014 algorithms that adapt through data.  </li> <li>Optimizing to learn \u2014 systems that adjust representations via optimization.  </li> <li>Self-improving optimizers \u2014 algorithms that recursively tune their own parameters.</li> </ul> <p>This convergence blurs the line between optimizer and learner.</p> <p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve \u2014 autonomously, efficiently, and creatively.</p> <p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems \u2014 and thinkers \u2014 to improve themselves.</p>"},{"location":"convex/tutorials/1_lp_transport/","title":"I - Transportation Optimization: A Linear Programming Case Study.","text":"In\u00a0[2]: Copied! <pre>#!pip uninstall -y pandas numpy matplotlib scipy\n#!pip install --no-cache-dir numpy pandas  matplotlib scipy\n</pre> #!pip uninstall -y pandas numpy matplotlib scipy #!pip install --no-cache-dir numpy pandas  matplotlib scipy  In\u00a0[3]: Copied! <pre>import re\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional, List\nimport numpy as np\nimport pandas as pd\nimport cvxpy as cp\nfrom scipy import sparse\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import PowerNorm\n</pre> import re from pathlib import Path from typing import Dict, Tuple, Optional, List import numpy as np import pandas as pd import cvxpy as cp from scipy import sparse import matplotlib.pyplot as plt from matplotlib.colors import PowerNorm  In\u00a0[4]: Copied! <pre>rng = np.random.default_rng(42)\nnp.random.seed(42)\n</pre> rng = np.random.default_rng(42) np.random.seed(42) In\u00a0[5]: Copied! <pre>#!pip install cvxpy[glpk]\n#!pip install ecos\n</pre> #!pip install cvxpy[glpk] #!pip install ecos In\u00a0[6]: Copied! <pre># -----------------------------\n# Data Cleaning Utilities\n# -----------------------------\ndef canon(s: str) -&gt; str:\n    \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"\n    s = s.strip().lower()\n    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s\n\n\ndef clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = df.copy()\n    df.columns = [canon(c) for c in df.columns]\n    # Normalize common missing value tokens\n    df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})\n    return df\n\n\ndef load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:\n    print(f\"Loading workbook: {xlsx_path.resolve()}\")\n    xls = pd.ExcelFile(xlsx_path)\n    sheets = {}\n    for sheet_name in xls.sheet_names:\n        df = pd.read_excel(xls, sheet_name=sheet_name)\n        df = clean_df(df)\n        sheets[canon(sheet_name)] = df\n    print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")\n    return sheets\n</pre> # ----------------------------- # Data Cleaning Utilities # ----------------------------- def canon(s: str) -&gt; str:     \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"     s = s.strip().lower()     s = re.sub(r\"[^a-z0-9]+\", \"_\", s)     s = re.sub(r\"_+\", \"_\", s).strip(\"_\")     return s   def clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:     df = df.copy()     df.columns = [canon(c) for c in df.columns]     # Normalize common missing value tokens     df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})     return df   def load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:     print(f\"Loading workbook: {xlsx_path.resolve()}\")     xls = pd.ExcelFile(xlsx_path)     sheets = {}     for sheet_name in xls.sheet_names:         df = pd.read_excel(xls, sheet_name=sheet_name)         df = clean_df(df)         sheets[canon(sheet_name)] = df     print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")     return sheets In\u00a0[7]: Copied! <pre>xlsx_path = Path(\"data/SupplyChainLogisticsProblems.xlsx\")\nsheets = load_excel_sheets(xlsx_path)\n</pre> xlsx_path = Path(\"data/SupplyChainLogisticsProblems.xlsx\") sheets = load_excel_sheets(xlsx_path) <pre>Loading workbook: C:\\Users\\salmank\\Documents\\convex_optimization\\docs\\convex\\tutorials\\data\\SupplyChainLogisticsProblems.xlsx\nLoaded 7 sheets: ['orderlist', 'freightrates', 'whcosts', 'whcapacities', 'productsperplant', 'vmicustomers', 'plantports']\n</pre> In\u00a0[8]: Copied! <pre># ----------------------------\n# Demand measure\n# ----------------------------\norders = sheets['orderlist'][['order_id', 'weight']]\norders.columns = [\"order_id\", \"demand\"]\norders = orders[orders[\"demand\"] &gt; 0].copy()\norders[\"order_id\"] = orders[\"order_id\"].astype(str)\n\n# Retain top 1000 orders by demand\nk = 1000\norders = (\n    orders\n    .sort_values(\"demand\", ascending=False)\n    .head(k)\n    .reset_index(drop=True)\n)\n\nprint(f\"Orders retained: {len(orders)}\")\norders.head()\n\n\n# Total Demand capacity\ntotal_demand = orders['demand'].sum()\nprint(f\"Total orders: {len(orders)}\")\nprint(f\"Total demand capacity: {total_demand}\")\n\n#orders.head()\n</pre> # ---------------------------- # Demand measure # ---------------------------- orders = sheets['orderlist'][['order_id', 'weight']] orders.columns = [\"order_id\", \"demand\"] orders = orders[orders[\"demand\"] &gt; 0].copy() orders[\"order_id\"] = orders[\"order_id\"].astype(str)  # Retain top 1000 orders by demand k = 1000 orders = (     orders     .sort_values(\"demand\", ascending=False)     .head(k)     .reset_index(drop=True) )  print(f\"Orders retained: {len(orders)}\") orders.head()   # Total Demand capacity total_demand = orders['demand'].sum() print(f\"Total orders: {len(orders)}\") print(f\"Total demand capacity: {total_demand}\")  #orders.head()  <pre>Orders retained: 1000\nTotal orders: 1000\nTotal demand capacity: 129698.9734391597\n</pre> In\u00a0[9]: Copied! <pre># ----------------------------\n# Plant capacities\n# ----------------------------\nplants = sheets['whcapacities']\nplants.columns = [\"plant_id\", \"supply_cap\"]\nplants[\"plant_id\"] = plants[\"plant_id\"].astype(str)\nplants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\")\nplants = plants.dropna(subset=[\"supply_cap\"])\nplants[\"supply_cap\"] = plants[\"supply_cap\"].astype(float) * 30\n\n# Retain top 10 plants by supply capacity\nplants = (\n    plants\n    .sort_values(\"supply_cap\", ascending=False)\n    .head(10)\n    .reset_index(drop=True)\n)\n\n# Total Supply capacity\ntotal_supply = plants[\"supply_cap\"].sum()\nprint(f\"Total plants: {len(plants)}\")\nprint(f\"Total supply capacity: {total_supply}\")\n\n#plants.head()\n</pre> # ---------------------------- # Plant capacities # ---------------------------- plants = sheets['whcapacities'] plants.columns = [\"plant_id\", \"supply_cap\"] plants[\"plant_id\"] = plants[\"plant_id\"].astype(str) plants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\") plants = plants.dropna(subset=[\"supply_cap\"]) plants[\"supply_cap\"] = plants[\"supply_cap\"].astype(float) * 30  # Retain top 10 plants by supply capacity plants = (     plants     .sort_values(\"supply_cap\", ascending=False)     .head(10)     .reset_index(drop=True) )  # Total Supply capacity total_supply = plants[\"supply_cap\"].sum() print(f\"Total plants: {len(plants)}\") print(f\"Total supply capacity: {total_supply}\")  #plants.head()  <pre>Total plants: 10\nTotal supply capacity: 159720.0\n</pre> In\u00a0[10]: Copied! <pre># Check Supply is enough to meet Demand\nif total_supply &lt; total_demand:\n    raise ValueError(f\"Total supply ({total_supply}) is less than total demand ({total_demand}). Problem is infeasible.\")\n</pre> # Check Supply is enough to meet Demand if total_supply &lt; total_demand:     raise ValueError(f\"Total supply ({total_supply}) is less than total demand ({total_demand}). Problem is infeasible.\") In\u00a0[11]: Copied! <pre># ----------------------------\n# Build lane table (arc list) and unit costs\n# ----------------------------\n# NOTE: In this notebook we generate a mostly-dense plant\u00d7order lane set\n#       with dummy unit costs, but randomly skip ~x% of connections\n#       to mimic missing/infeasible shipping lanes in real data.\n\nrng = np.random.default_rng(7)\n\nplant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]}\nplant_ids = plants[\"plant_id\"].tolist()\n\nskip_prob = 0.5  # probability of skipping a plant-order connection\n\nlanes = []\nfor _, o in orders.iterrows():\n    oid = o[\"order_id\"]\n    dem = float(o[\"demand\"])\n    order_jitter = 0.75 + 0.25 * rng.random()\n\n    for pid in plant_ids:\n        # randomly skip some connections\n        if rng.random() &lt; skip_prob:\n            continue\n\n        unit_cost = 1 * plant_factor[pid] * order_jitter\n        lanes.append((pid, oid, unit_cost))\n\nlanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"])\n\nprint(f\"Total lanes (plant-order pairs): {len(lanes)}\")\nprint(f\"Total lanes (plant-order pairs) if we had full connectivity: {len(plants) * len(orders)}\")\n\n#lanes.head()\n</pre> # ---------------------------- # Build lane table (arc list) and unit costs # ---------------------------- # NOTE: In this notebook we generate a mostly-dense plant\u00d7order lane set #       with dummy unit costs, but randomly skip ~x% of connections #       to mimic missing/infeasible shipping lanes in real data.  rng = np.random.default_rng(7)  plant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]} plant_ids = plants[\"plant_id\"].tolist()  skip_prob = 0.5  # probability of skipping a plant-order connection  lanes = [] for _, o in orders.iterrows():     oid = o[\"order_id\"]     dem = float(o[\"demand\"])     order_jitter = 0.75 + 0.25 * rng.random()      for pid in plant_ids:         # randomly skip some connections         if rng.random() &lt; skip_prob:             continue          unit_cost = 1 * plant_factor[pid] * order_jitter         lanes.append((pid, oid, unit_cost))  lanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"])  print(f\"Total lanes (plant-order pairs): {len(lanes)}\") print(f\"Total lanes (plant-order pairs) if we had full connectivity: {len(plants) * len(orders)}\")  #lanes.head()  <pre>Total lanes (plant-order pairs): 5048\nTotal lanes (plant-order pairs) if we had full connectivity: 10000\n</pre> In\u00a0[12]: Copied! <pre># ----------------------------\n# Drop plants / orders with no lanes\n# ----------------------------\n\nprint(\n    f\"Before cleanup: {len(plants)} plants, \"\n    f\"{len(orders)} orders, \"\n    f\"{len(lanes)} lanes\"\n)\n\nplants_with_lane = set(lanes[\"plant_id\"].unique())\norders_with_lane = set(lanes[\"order_id\"].unique())\n\n# Identify disconnected nodes\ndrop_plants = set(plants[\"plant_id\"]) - plants_with_lane\ndrop_orders = set(orders[\"order_id\"]) - orders_with_lane\n\nif drop_plants:\n    print(f\"Dropping {len(drop_plants)} plant(s) with no lanes: {sorted(drop_plants)}\")\n    plants_with_lanes = plants[~plants[\"plant_id\"].isin(drop_plants)].reset_index(drop=True)\nelse:\n    plants_with_lanes = plants.copy()\n\nif drop_orders:\n    print(f\"Dropping {len(drop_orders)} order(s) with no lanes: {sorted(drop_orders)}\")\n    orders_with_lanes = orders[~orders[\"order_id\"].isin(drop_orders)].reset_index(drop=True)\nelse:\n    orders_with_lanes = orders.copy()\n\n# Keep lanes consistent with remaining plants and orders\nlanes = lanes[\n    lanes[\"plant_id\"].isin(plants_with_lanes[\"plant_id\"]) &amp;\n    lanes[\"order_id\"].isin(orders_with_lanes[\"order_id\"])\n].reset_index(drop=True)\n\nprint(\n    f\"After cleanup: {len(plants_with_lanes)} plants, \"\n    f\"{len(orders_with_lanes)} orders, \"\n    f\"{len(lanes)} lanes\"\n)\n</pre> # ---------------------------- # Drop plants / orders with no lanes # ----------------------------  print(     f\"Before cleanup: {len(plants)} plants, \"     f\"{len(orders)} orders, \"     f\"{len(lanes)} lanes\" )  plants_with_lane = set(lanes[\"plant_id\"].unique()) orders_with_lane = set(lanes[\"order_id\"].unique())  # Identify disconnected nodes drop_plants = set(plants[\"plant_id\"]) - plants_with_lane drop_orders = set(orders[\"order_id\"]) - orders_with_lane  if drop_plants:     print(f\"Dropping {len(drop_plants)} plant(s) with no lanes: {sorted(drop_plants)}\")     plants_with_lanes = plants[~plants[\"plant_id\"].isin(drop_plants)].reset_index(drop=True) else:     plants_with_lanes = plants.copy()  if drop_orders:     print(f\"Dropping {len(drop_orders)} order(s) with no lanes: {sorted(drop_orders)}\")     orders_with_lanes = orders[~orders[\"order_id\"].isin(drop_orders)].reset_index(drop=True) else:     orders_with_lanes = orders.copy()  # Keep lanes consistent with remaining plants and orders lanes = lanes[     lanes[\"plant_id\"].isin(plants_with_lanes[\"plant_id\"]) &amp;     lanes[\"order_id\"].isin(orders_with_lanes[\"order_id\"]) ].reset_index(drop=True)  print(     f\"After cleanup: {len(plants_with_lanes)} plants, \"     f\"{len(orders_with_lanes)} orders, \"     f\"{len(lanes)} lanes\" )  <pre>Before cleanup: 10 plants, 1000 orders, 5048 lanes\nDropping 1 order(s) with no lanes: ['1447183486.7']\nAfter cleanup: 10 plants, 999 orders, 5048 lanes\n</pre> In\u00a0[13]: Copied! <pre># ----------------------------\n# Data integrity checks\n# ----------------------------\n# Ensure IDs are unique before we rely on index-based alignment later.\n\nif not plants_with_lanes[\"plant_id\"].is_unique:\n    raise ValueError(\"plants.plant_id is not unique. Deduplicate or aggregate supplies first.\")\nif not orders_with_lanes[\"order_id\"].is_unique:\n    raise ValueError(\"orders.order_id is not unique. Deduplicate or aggregate demands first.\")\n</pre> # ---------------------------- # Data integrity checks # ---------------------------- # Ensure IDs are unique before we rely on index-based alignment later.  if not plants_with_lanes[\"plant_id\"].is_unique:     raise ValueError(\"plants.plant_id is not unique. Deduplicate or aggregate supplies first.\") if not orders_with_lanes[\"order_id\"].is_unique:     raise ValueError(\"orders.order_id is not unique. Deduplicate or aggregate demands first.\") In\u00a0[14]: Copied! <pre># ----------------------------\n# Clean / validate lanes\n# ----------------------------\n# 1) Drop lanes whose plant_id/order_id do not exist in the node tables.\n# 2) If duplicate (plant_id, order_id) lanes exist, aggregate them (here: keep the minimum unit cost).\n\n# Keep only lanes that connect to known plants/orders (avoid silent mismatches)\nlanes = lanes.merge(plants[[\"plant_id\"]], on=\"plant_id\", how=\"inner\")\nlanes = lanes.merge(orders[[\"order_id\"]], on=\"order_id\", how=\"inner\")\n\nif lanes.empty:\n    raise ValueError(\"No valid lanes after matching plant_id/order_id against plants/orders.\")\n\nlanes = (lanes.groupby([\"plant_id\", \"order_id\"], as_index=False).agg({'unit_cost': \"min\"}))\n\n#lanes.head()\n</pre> # ---------------------------- # Clean / validate lanes # ---------------------------- # 1) Drop lanes whose plant_id/order_id do not exist in the node tables. # 2) If duplicate (plant_id, order_id) lanes exist, aggregate them (here: keep the minimum unit cost).  # Keep only lanes that connect to known plants/orders (avoid silent mismatches) lanes = lanes.merge(plants[[\"plant_id\"]], on=\"plant_id\", how=\"inner\") lanes = lanes.merge(orders[[\"order_id\"]], on=\"order_id\", how=\"inner\")  if lanes.empty:     raise ValueError(\"No valid lanes after matching plant_id/order_id against plants/orders.\")  lanes = (lanes.groupby([\"plant_id\", \"order_id\"], as_index=False).agg({'unit_cost': \"min\"}))  #lanes.head() In\u00a0[15]: Copied! <pre># ----------------------------\n# Define a *single source of truth* for indices\n# ----------------------------\nplant_ids = plants_with_lanes[\"plant_id\"].to_numpy()\norder_ids = orders_with_lanes[\"order_id\"].to_numpy()\n\nplant_to_i = {pid: i for i, pid in enumerate(plant_ids)}\norder_to_j = {oid: j for j, oid in enumerate(order_ids)}\n\n# Arc index = row index of lanes after reset\nlanes = lanes.reset_index(drop=True)\nnA = len(lanes)\nnP = len(plant_ids)\nnO = len(order_ids)\n\n # Build arc endpoint index arrays\narc_i = lanes[\"plant_id\"].map(plant_to_i).to_numpy()\narc_j = lanes[\"order_id\"].map(order_to_j).to_numpy()\n\n\narc_i = arc_i.astype(int)\narc_j = arc_j.astype(int)\n</pre> # ---------------------------- # Define a *single source of truth* for indices # ---------------------------- plant_ids = plants_with_lanes[\"plant_id\"].to_numpy() order_ids = orders_with_lanes[\"order_id\"].to_numpy()  plant_to_i = {pid: i for i, pid in enumerate(plant_ids)} order_to_j = {oid: j for j, oid in enumerate(order_ids)}  # Arc index = row index of lanes after reset lanes = lanes.reset_index(drop=True) nA = len(lanes) nP = len(plant_ids) nO = len(order_ids)   # Build arc endpoint index arrays arc_i = lanes[\"plant_id\"].map(plant_to_i).to_numpy() arc_j = lanes[\"order_id\"].map(order_to_j).to_numpy()   arc_i = arc_i.astype(int) arc_j = arc_j.astype(int)  In\u00a0[16]: Copied! <pre># ----------------------------\n# Build aligned vectors: cost c, supply, demand\n# ----------------------------\n# IMPORTANT: `c[a]` must correspond to the same arc as decision variable `x[a]`.\n# Here, arc index a = row index of `lanes` after reset_index.\n\n# Cost vector corresponds to arc order (lanes row order).\nc = lanes['unit_cost'].to_numpy(dtype=float)\n\nsupply = plants.set_index(\"plant_id\").loc[plant_ids, 'supply_cap'].to_numpy(dtype=float)\ndemand = orders.set_index(\"order_id\").loc[order_ids, 'demand'].to_numpy(dtype=float)\n\n# Ensure every order has at least one incoming lane\nincoming_counts = np.bincount(arc_j, minlength=nO)\nmissing_orders = order_ids[incoming_counts == 0]\nif len(missing_orders) &gt; 0:\n    raise ValueError(f\"Orders with no incoming lanes: {missing_orders[:10]}{'...' if len(missing_orders) &gt; 10 else ''}\")\n</pre> # ---------------------------- # Build aligned vectors: cost c, supply, demand # ---------------------------- # IMPORTANT: `c[a]` must correspond to the same arc as decision variable `x[a]`. # Here, arc index a = row index of `lanes` after reset_index.  # Cost vector corresponds to arc order (lanes row order). c = lanes['unit_cost'].to_numpy(dtype=float)  supply = plants.set_index(\"plant_id\").loc[plant_ids, 'supply_cap'].to_numpy(dtype=float) demand = orders.set_index(\"order_id\").loc[order_ids, 'demand'].to_numpy(dtype=float)  # Ensure every order has at least one incoming lane incoming_counts = np.bincount(arc_j, minlength=nO) missing_orders = order_ids[incoming_counts == 0] if len(missing_orders) &gt; 0:     raise ValueError(f\"Orders with no incoming lanes: {missing_orders[:10]}{'...' if len(missing_orders) &gt; 10 else ''}\") In\u00a0[17]: Copied! <pre># ----------------------------\n# Visuals: supply capacity by plant &amp; demand by order\n# ----------------------------\n \nfig, ax = plt.subplots(figsize=(10, 4))\nax.bar(np.arange(len(supply)), supply)\nax.set_title(\"Supply capacity by plant (index order)\")\nax.set_xlabel(\"Plant index i\")\nax.set_ylabel(\"Supply capacity\")\nplt.show()\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.bar(np.arange(len(demand)), demand)\nax.set_title(\"Demand by order (index order)\")\nax.set_xlabel(\"Order index j\")\nax.set_ylabel(\"Demand\")\nplt.show()\n</pre> # ---------------------------- # Visuals: supply capacity by plant &amp; demand by order # ----------------------------   fig, ax = plt.subplots(figsize=(10, 4)) ax.bar(np.arange(len(supply)), supply) ax.set_title(\"Supply capacity by plant (index order)\") ax.set_xlabel(\"Plant index i\") ax.set_ylabel(\"Supply capacity\") plt.show()  fig, ax = plt.subplots(figsize=(10, 4)) ax.bar(np.arange(len(demand)), demand) ax.set_title(\"Demand by order (index order)\") ax.set_xlabel(\"Order index j\") ax.set_ylabel(\"Demand\") plt.show()  In\u00a0[18]: Copied! <pre># ----------------------------\n# Build sparse incidence matrices\n# ----------------------------\n# A_order[j, a] = 1 if arc a goes into order j\nA_order = sparse.coo_matrix(\n    (np.ones(nA), (arc_j, np.arange(nA))),\n    shape=(nO, nA)\n).tocsr()\n\n# A_plant[i, a] = 1 if arc a goes out of plant i\nA_plant = sparse.coo_matrix(\n    (np.ones(nA), (arc_i, np.arange(nA))),\n    shape=(nP, nA)\n).tocsr()\n\nA_order\n</pre> # ---------------------------- # Build sparse incidence matrices # ---------------------------- # A_order[j, a] = 1 if arc a goes into order j A_order = sparse.coo_matrix(     (np.ones(nA), (arc_j, np.arange(nA))),     shape=(nO, nA) ).tocsr()  # A_plant[i, a] = 1 if arc a goes out of plant i A_plant = sparse.coo_matrix(     (np.ones(nA), (arc_i, np.arange(nA))),     shape=(nP, nA) ).tocsr()  A_order Out[18]: <pre>&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 5048 stored elements and shape (999, 5048)&gt;</pre> In\u00a0[19]: Copied! <pre># ----------------------------\n# Solve the LP in CVXPY\n# ----------------------------\n# Decision variable:\n#   x[a] = shipment quantity on lane (arc) a\nx = cp.Variable(nA, nonneg=True)\n\n# Constraints:\n#   - every order demand must be met exactly\n#   - plant shipments cannot exceed supply capacity\nconstraints = [\n    A_order @ x == demand,\n    A_plant @ x &lt;= supply\n]\n\n# Objective: min total shipping cost\nobjective = cp.Minimize(c @ x)\nprob = cp.Problem(objective, constraints)\n\n# Choose a good LP solver explicitly (fallback if unavailable)\npreferred = \"HIGHS\"\ninstalled = set(cp.installed_solvers())\nsolver = preferred if preferred in installed else (\"GLPK\" if \"GLPK\" in installed else None)\nif solver is None:\n    raise RuntimeError(f\"No suitable LP solver found. Installed solvers: {sorted(installed)}\")\n\n_ = prob.solve(solver=solver, verbose=False)\n\nprint(\"Status:\", prob.status)\nprint(\"Objective value:\", prob.value)\n\nif prob.status not in (\"optimal\", \"optimal_inaccurate\"):\n    raise RuntimeError(f\"Solve failed: status={prob.status}\")\n\n# Attach the solution back to the lane table (arc ordering!)\nlanes_sol = lanes.copy()\nlanes_sol[\"x\"] = np.asarray(x.value).reshape(-1)\n\n# ----------------------------\n# Sanity checks (residuals)\n# ----------------------------\norder_resid = (A_order @ lanes_sol[\"x\"].to_numpy()) - demand\nplant_resid = (A_plant @ lanes_sol[\"x\"].to_numpy()) - supply  # should be &lt;= 0\n\ndiagnostics = {\n    \"status\": prob.status,\n    \"objective\": float(prob.value),\n    \"max_abs_order_residual\": float(np.max(np.abs(order_resid))) if len(order_resid) else 0.0,\n    \"max_plant_violation\": float(np.max(plant_resid)) if len(plant_resid) else 0.0,\n    \"solver_used\": solver,\n    \"n_plants\": int(nP),\n    \"n_orders\": int(nO),\n    \"n_lanes\": int(nA),\n}\n\ndiagnostics\n</pre> # ---------------------------- # Solve the LP in CVXPY # ---------------------------- # Decision variable: #   x[a] = shipment quantity on lane (arc) a x = cp.Variable(nA, nonneg=True)  # Constraints: #   - every order demand must be met exactly #   - plant shipments cannot exceed supply capacity constraints = [     A_order @ x == demand,     A_plant @ x &lt;= supply ]  # Objective: min total shipping cost objective = cp.Minimize(c @ x) prob = cp.Problem(objective, constraints)  # Choose a good LP solver explicitly (fallback if unavailable) preferred = \"HIGHS\" installed = set(cp.installed_solvers()) solver = preferred if preferred in installed else (\"GLPK\" if \"GLPK\" in installed else None) if solver is None:     raise RuntimeError(f\"No suitable LP solver found. Installed solvers: {sorted(installed)}\")  _ = prob.solve(solver=solver, verbose=False)  print(\"Status:\", prob.status) print(\"Objective value:\", prob.value)  if prob.status not in (\"optimal\", \"optimal_inaccurate\"):     raise RuntimeError(f\"Solve failed: status={prob.status}\")  # Attach the solution back to the lane table (arc ordering!) lanes_sol = lanes.copy() lanes_sol[\"x\"] = np.asarray(x.value).reshape(-1)  # ---------------------------- # Sanity checks (residuals) # ---------------------------- order_resid = (A_order @ lanes_sol[\"x\"].to_numpy()) - demand plant_resid = (A_plant @ lanes_sol[\"x\"].to_numpy()) - supply  # should be &lt;= 0  diagnostics = {     \"status\": prob.status,     \"objective\": float(prob.value),     \"max_abs_order_residual\": float(np.max(np.abs(order_resid))) if len(order_resid) else 0.0,     \"max_plant_violation\": float(np.max(plant_resid)) if len(plant_resid) else 0.0,     \"solver_used\": solver,     \"n_plants\": int(nP),     \"n_orders\": int(nO),     \"n_lanes\": int(nA), }  diagnostics  <pre>Status: optimal\nObjective value: 113265.87945592123\n</pre> Out[19]: <pre>{'status': 'optimal',\n 'objective': 113265.87945592123,\n 'max_abs_order_residual': 2.808064891723916e-11,\n 'max_plant_violation': 2.546585164964199e-11,\n 'solver_used': 'GLPK',\n 'n_plants': 10,\n 'n_orders': 999,\n 'n_lanes': 5048}</pre> In\u00a0[20]: Copied! <pre># ----------------------------\n# Heatmap of shipped amounts (plants x orders)\n# ----------------------------\n# lanes_sol must contain: plant_id, order_id, x\n# plant_ids, order_ids are the index orders used elsewhere (strings)\n\n# Build shipment matrix: rows=plants, cols=orders\nship = (lanes_sol\n        .pivot_table(index=\"plant_id\", columns=\"order_id\", values=\"x\", aggfunc=\"sum\", fill_value=0.0)\n       )\n\n# Ensure full row/col order (so it matches your indexing)\nship = ship.reindex(index=plant_ids, columns=order_ids, fill_value=0.0)\norder_totals = ship.sum(axis=0).sort_values(ascending=False)\nship = ship[order_totals.index]\nplant_totals = ship.sum(axis=1).sort_values(ascending=False)\n#ship = ship.loc[plant_totals.index]\n\nX = ship.to_numpy()\n\nplt.figure(figsize=(18, 5))\nim = plt.imshow(\n    X,\n    aspect=\"auto\",\n    cmap=\"viridis\",\n    norm=PowerNorm(gamma=0.3)\n)\n\nplt.title(\"Shipment heatmap\")\nplt.xlabel(\"Orders (Order ID)\")\nplt.ylabel(\"Plants (Plant ID)\")\n\nplt.yticks(np.arange(len(ship.index)), ship.index)\n\n#step = max(1, len(ship.columns)//50)\nstep = 20\nxt = np.arange(0, len(ship.columns), step)\nplt.xticks(xt, ship.columns[xt], rotation=90)\n\nplt.colorbar(im, label=\"Shipped quantity (power-law scaled)\")\nplt.tight_layout()\nplt.show()\n</pre> # ---------------------------- # Heatmap of shipped amounts (plants x orders) # ---------------------------- # lanes_sol must contain: plant_id, order_id, x # plant_ids, order_ids are the index orders used elsewhere (strings)  # Build shipment matrix: rows=plants, cols=orders ship = (lanes_sol         .pivot_table(index=\"plant_id\", columns=\"order_id\", values=\"x\", aggfunc=\"sum\", fill_value=0.0)        )  # Ensure full row/col order (so it matches your indexing) ship = ship.reindex(index=plant_ids, columns=order_ids, fill_value=0.0) order_totals = ship.sum(axis=0).sort_values(ascending=False) ship = ship[order_totals.index] plant_totals = ship.sum(axis=1).sort_values(ascending=False) #ship = ship.loc[plant_totals.index]  X = ship.to_numpy()  plt.figure(figsize=(18, 5)) im = plt.imshow(     X,     aspect=\"auto\",     cmap=\"viridis\",     norm=PowerNorm(gamma=0.3) )  plt.title(\"Shipment heatmap\") plt.xlabel(\"Orders (Order ID)\") plt.ylabel(\"Plants (Plant ID)\")  plt.yticks(np.arange(len(ship.index)), ship.index)  #step = max(1, len(ship.columns)//50) step = 20 xt = np.arange(0, len(ship.columns), step) plt.xticks(xt, ship.columns[xt], rotation=90)  plt.colorbar(im, label=\"Shipped quantity (power-law scaled)\") plt.tight_layout() plt.show()"},{"location":"convex/tutorials/1_lp_transport/#i-transportation-optimization-a-linear-programming-case-study","title":"I - Transportation Optimization: A Linear Programming Case Study.\u00b6","text":""},{"location":"convex/tutorials/1_lp_transport/#problem-definition","title":"Problem Definition\u00b6","text":"<p>We consider a classical transportation optimization problem, also known as the minimum-cost flow problem, arising in supply chain and logistics planning. A set of production facilities (plants) must ship goods to a set of customer orders at minimum total transportation cost, subject to supply, demand, and routing constraints.</p> <p>This problem is foundational in operations research and appears in manufacturing, retail distribution, energy logistics, and humanitarian supply chains</p>"},{"location":"convex/tutorials/1_lp_transport/#formulation","title":"Formulation\u00b6","text":"<p>We assume the following information is known:</p> <ul> <li>Supply capacity at each plant</li> <li>Demand requirement for each order</li> <li>Transportation cost between feasible plant\u2013order pairs</li> <li>Feasible shipping lanes (not all plants can serve all orders)</li> </ul> <p>We must determine how much product to ship from each plant to each order in order to minimize total transportation cost while fully satisfying demand and respecting supply limits.</p>"},{"location":"convex/tutorials/1_lp_transport/#objective-function","title":"Objective Function\u00b6","text":"<ul> <li>Minimize total transportation cost:  $\\min \\sum_{i \\in P} \\sum_{j \\in O} c_{ij} x_{ij}$</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$x_{ij} \\ge 0$: quantity shipped from plant $i$ to order $j$</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$c_{ij}$: unit transportation cost from plant $i$ to order $j$</li> <li>$s_i$: supply capacity of plant $i$</li> <li>$d_j$: demand requirement of order $j$</li> <li>$A \\subseteq P \\times O$: set of feasible shipping lanes</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Supply constraints (plant capacity):   $\\sum_{j \\in O} x_{ij} \\le s_i \\quad \\forall i \\in P$</li> <li>Demand constraints (order fulfillmen): $\\sum_{i \\in P} x_{ij} = d_j \\quad \\forall j \\in O$</li> <li>Route feasibility constraints:   $x_{ij} = 0 \\quad \\forall (i,j) \\notin A$</li> <li>Non-negativity:  $x_{ij} \\ge 0 \\quad \\forall i,j$</li> </ul> <p>This problem belongs to the class of linear programming problems, characterized by continuous decision variables, convex feasible regions, and polynomial-time solvability.</p>"},{"location":"convex/tutorials/1_lp_transport/#dataset-source","title":"Dataset Source\u00b6","text":"<p>The data used in this notebook comes from the Supply Chain Logistics Problem Dataset, originally published by researchers at Brunel University London on Figshare.</p> <p>Source details:</p> <ul> <li>Title: Supply Chain Logistics Problem Dataset</li> <li>Publisher: Brunel University London</li> <li>Link</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#dataset-structure","title":"Dataset Structure\u00b6","text":"<p>The dataset provides a complete snapshot of a multi-node supply chain, organized into several tables, including:</p> <ul> <li>OrderList \u2013 customer orders that must be fulfilled</li> <li>Plants / Facilities \u2013 supply locations with capacity limits</li> <li>Shipping Lanes / Routes \u2013 feasible connections between plants and orders</li> <li>Cost &amp; Attributes \u2013 transportation cost and lane-specific properties</li> </ul> <p>Together, these tables allow us to construct a realistic transportation network with real-world constraints.</p>"},{"location":"convex/tutorials/1_lp_transport/#data-description","title":"Data description\u00b6","text":"<ul> <li><p>Plants (<code>plants</code>)</p> <ul> <li><code>plant_id</code>: unique identifier of a plant</li> <li><code>supply_cap</code>: maximum supply capacity available at the plant</li> </ul> </li> <li><p>Orders (<code>orders</code>)</p> <ul> <li><code>order_id</code>: unique identifier of an order</li> <li><code>demand</code>: required demand that must be satisfied</li> </ul> </li> <li><p>Lanes (<code>lanes</code>)</p> <ul> <li>Each row represents a feasible shipping lane from a plant to an order</li> <li><code>plant_id</code>, <code>order_id</code>: endpoints of the lane</li> <li><code>unit_cost</code>: per-unit shipping cost on that lane</li> </ul> </li> </ul> <p>After preprocessing:</p> <ul> <li>all plants and orders included in the model have at least one feasible lane</li> <li>total supply is sufficient to meet total demand</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#cvxpy-solver","title":"CVXPY Solver\u00b6","text":""},{"location":"convex/tutorials/2_portfolio/","title":"II - Mean\u2013Variance Portfolio Optimization: A Pareto-Optimal Case Study.","text":"In\u00a0[15]: Copied! <pre>#!pip install  pandas_datareader\n</pre> #!pip install  pandas_datareader In\u00a0[16]: Copied! <pre># Install and import necessary libraries\n#!pip install yfinance cvxpy pandas numpy matplotlib seaborn\nimport pandas as pd\nimport numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_datareader import data as pdr\n</pre> # Install and import necessary libraries #!pip install yfinance cvxpy pandas numpy matplotlib seaborn import pandas as pd import numpy as np import cvxpy as cp import matplotlib.pyplot as plt import seaborn as sns from pandas_datareader import data as pdr  In\u00a0[17]: Copied! <pre>tickers_dict = {\n    # Broad Equity\n    \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",\n    \"VTI.US\": \"Total U.S. stock market\",\n\n    # Sector-Specific Equities\n    \"XLK.US\": \"Technology sector\",\n    \"XLF.US\": \"Financials sector\",\n    \"XLE.US\": \"Energy sector\",\n    \"XLV.US\": \"Healthcare sector\",\n    \"XLY.US\": \"Consumer Discretionary sector\",\n    \"XLI.US\": \"Industrials sector\",\n    \"XLRE.US\": \"Real Estate sector\",\n\n    # Size / Style\n    \"IWM.US\": \"Russell 2000 ETF\",\n    \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",\n\n    # International Equity\n    \"EFA.US\": \"Developed markets (ex-US)\",\n    \"EEM.US\": \"Emerging markets\",\n    \"VXUS.US\": \"Total International equity\",\n\n    # Government Bonds\n    \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",\n    \"TLT.US\": \"U.S. Treasuries 20+Y\",\n    \"SHY.US\": \"Short-term Treasuries\",\n\n    # Corporate / High Yield\n    \"LQD.US\": \"Investment-grade corporate bonds\",\n    \"HYG.US\": \"High-yield corporate bonds\",\n\n    # Commodities\n    \"GLD.US\": \"Gold\",\n    \"SLV.US\": \"Silver\",\n    \"DBC.US\": \"Broad commodities\",\n    \"USO.US\": \"Crude Oil\",\n    \"UNG.US\": \"Natural Gas\",\n\n    # Alternative / Real Assets\n    \"VNQ.US\": \"U.S. Real Estate (REITs)\",\n    \"ICLN.US\": \"Global Clean Energy\"\n\n    # Cryptocurrency\n    #\"BTC-USD\": \"Bitcoin (USD-denominated)\"\n}\ntickers = list(tickers_dict.keys())\n\nprint(len(tickers))\n</pre> tickers_dict = {     # Broad Equity     \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",     \"VTI.US\": \"Total U.S. stock market\",      # Sector-Specific Equities     \"XLK.US\": \"Technology sector\",     \"XLF.US\": \"Financials sector\",     \"XLE.US\": \"Energy sector\",     \"XLV.US\": \"Healthcare sector\",     \"XLY.US\": \"Consumer Discretionary sector\",     \"XLI.US\": \"Industrials sector\",     \"XLRE.US\": \"Real Estate sector\",      # Size / Style     \"IWM.US\": \"Russell 2000 ETF\",     \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",      # International Equity     \"EFA.US\": \"Developed markets (ex-US)\",     \"EEM.US\": \"Emerging markets\",     \"VXUS.US\": \"Total International equity\",      # Government Bonds     \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",     \"TLT.US\": \"U.S. Treasuries 20+Y\",     \"SHY.US\": \"Short-term Treasuries\",      # Corporate / High Yield     \"LQD.US\": \"Investment-grade corporate bonds\",     \"HYG.US\": \"High-yield corporate bonds\",      # Commodities     \"GLD.US\": \"Gold\",     \"SLV.US\": \"Silver\",     \"DBC.US\": \"Broad commodities\",     \"USO.US\": \"Crude Oil\",     \"UNG.US\": \"Natural Gas\",      # Alternative / Real Assets     \"VNQ.US\": \"U.S. Real Estate (REITs)\",     \"ICLN.US\": \"Global Clean Energy\"      # Cryptocurrency     #\"BTC-USD\": \"Bitcoin (USD-denominated)\" } tickers = list(tickers_dict.keys())  print(len(tickers)) <pre>26\n</pre> In\u00a0[18]: Copied! <pre># Define asset tickers and download data (3 years of daily data)\nstart_date = \"2023-01-01\"\nend_date = \"2025-12-20\"\n\nprices = pd.DataFrame()\n\nfor ticker in tickers:\n    print(f\"Downloading data for {ticker}...\")\n    data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)\n    prices[ticker] = data[\"Close\"]\n\nprices = prices.sort_index()\n</pre> # Define asset tickers and download data (3 years of daily data) start_date = \"2023-01-01\" end_date = \"2025-12-20\"  prices = pd.DataFrame()  for ticker in tickers:     print(f\"Downloading data for {ticker}...\")     data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)     prices[ticker] = data[\"Close\"]  prices = prices.sort_index() <pre>Downloading data for SPY.US...\nDownloading data for VTI.US...\nDownloading data for XLK.US...\nDownloading data for XLF.US...\nDownloading data for XLE.US...\nDownloading data for XLV.US...\nDownloading data for XLY.US...\nDownloading data for XLI.US...\nDownloading data for XLRE.US...\nDownloading data for IWM.US...\nDownloading data for QQQ.US...\nDownloading data for EFA.US...\nDownloading data for EEM.US...\nDownloading data for VXUS.US...\nDownloading data for IEF.US...\nDownloading data for TLT.US...\nDownloading data for SHY.US...\nDownloading data for LQD.US...\nDownloading data for HYG.US...\nDownloading data for GLD.US...\nDownloading data for SLV.US...\nDownloading data for DBC.US...\nDownloading data for USO.US...\nDownloading data for UNG.US...\nDownloading data for VNQ.US...\nDownloading data for ICLN.US...\n</pre> In\u00a0[19]: Copied! <pre>print(\"Missing values per ticker:\")\n#print(prices.isna().sum())\n\n# Drop rows with any missing values\nprices = prices.dropna()\n</pre> print(\"Missing values per ticker:\") #print(prices.isna().sum())  # Drop rows with any missing values prices = prices.dropna() <pre>Missing values per ticker:\n</pre> In\u00a0[20]: Copied! <pre>plt.figure(figsize=(28, 14))\n\nfor ticker in prices.columns:\n    plt.plot(prices.index, prices[ticker], label=tickers_dict[ticker])\n\nplt.title(\"Asset Prices (Daily Close)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.legend(loc='upper left', bbox_to_anchor=(1,1))\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(28, 14))  for ticker in prices.columns:     plt.plot(prices.index, prices[ticker], label=tickers_dict[ticker])  plt.title(\"Asset Prices (Daily Close)\") plt.xlabel(\"Date\") plt.ylabel(\"Price\") plt.legend(loc='upper left', bbox_to_anchor=(1,1)) plt.grid(True) plt.show()  In\u00a0[21]: Copied! <pre>## Returns &amp; Risk Estimation\n\n# Daily returns\nreturns = prices.pct_change().dropna()\n\ntrading_days_per_year = len(returns) / ((returns.index[-1] - returns.index[0]).days / 365)\n\n# Annualized statistics\nmu = returns.mean() * trading_days_per_year\nSigma = returns.cov() * trading_days_per_year\n\n\n\n# --- Weekly returns ---\nweekly_returns = prices.resample('W').last().pct_change().dropna()\ntrading_weeks_per_year = 52  # standard\nmu_weekly = weekly_returns.mean() * trading_weeks_per_year\n\n# --- Monthly returns ---\nmonthly_returns = prices.resample('ME').last().pct_change().dropna()\ntrading_months_per_year = 12\nmu_monthly = monthly_returns.mean() * trading_months_per_year\n\n# --- Combine into DataFrame ---\nmu_df = pd.DataFrame({\n    'Asset': [tickers_dict[t] for t in prices.columns],\n    'Annualized Daily Return': mu.values,\n    'Annualized Weekly Return': mu_weekly.values,\n    'Annualized Monthly Return': mu_monthly.values\n})\n\n# Sort by Annualized Daily Return descending\nmu_df = mu_df.sort_values(by='Annualized Monthly Return', ascending=False).reset_index(drop=True)\nmu_df\n</pre> ## Returns &amp; Risk Estimation  # Daily returns returns = prices.pct_change().dropna()  trading_days_per_year = len(returns) / ((returns.index[-1] - returns.index[0]).days / 365)  # Annualized statistics mu = returns.mean() * trading_days_per_year Sigma = returns.cov() * trading_days_per_year    # --- Weekly returns --- weekly_returns = prices.resample('W').last().pct_change().dropna() trading_weeks_per_year = 52  # standard mu_weekly = weekly_returns.mean() * trading_weeks_per_year  # --- Monthly returns --- monthly_returns = prices.resample('ME').last().pct_change().dropna() trading_months_per_year = 12 mu_monthly = monthly_returns.mean() * trading_months_per_year  # --- Combine into DataFrame --- mu_df = pd.DataFrame({     'Asset': [tickers_dict[t] for t in prices.columns],     'Annualized Daily Return': mu.values,     'Annualized Weekly Return': mu_weekly.values,     'Annualized Monthly Return': mu_monthly.values })  # Sort by Annualized Daily Return descending mu_df = mu_df.sort_values(by='Annualized Monthly Return', ascending=False).reset_index(drop=True) mu_df  Out[21]: Asset Annualized Daily Return Annualized Weekly Return Annualized Monthly Return 0 Silver 0.384067 0.387958 0.393563 1 Gold 0.299218 0.292404 0.286397 2 NASDAQ 100 ETF (tech-heavy) 0.311032 0.304409 0.271663 3 U.S. broad-market equity (S&amp;P 500) 0.218446 0.210848 0.195445 4 Total U.S. stock market 0.204311 0.196916 0.179124 5 Industrials sector 0.168020 0.159802 0.157793 6 Financials sector 0.172198 0.162923 0.152073 7 Emerging markets 0.145270 0.128816 0.115537 8 Russell 2000 ETF 0.147370 0.137934 0.111278 9 Developed markets (ex-US) 0.134415 0.125603 0.105827 10 Total International equity 0.131797 0.121534 0.103827 11 Technology sector 0.144699 0.137155 0.103551 12 Healthcare sector 0.054585 0.054160 0.060555 13 High-yield corporate bonds 0.030829 0.022513 0.018823 14 Crude Oil 0.045700 0.060544 0.018543 15 Real Estate sector 0.044486 0.036616 0.011133 16 Consumer Discretionary sector 0.068886 0.055095 0.009095 17 Short-term Treasuries 0.006527 0.005279 0.004174 18 U.S. Real Estate (REITs) 0.039841 0.031769 0.003941 19 Investment-grade corporate bonds 0.015634 0.007656 0.001178 20 U.S. Treasuries 7\u201310Y 0.001794 -0.004978 -0.007952 21 Broad commodities -0.005557 0.000307 -0.022092 22 U.S. Treasuries 20+Y -0.008399 -0.021982 -0.029124 23 Global Clean Energy -0.030941 -0.036411 -0.050184 24 Energy sector -0.129084 -0.142963 -0.158374 25 Natural Gas -0.312402 -0.285856 -0.263702 In\u00a0[22]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute correlation matrix\ncorr = returns.corr()\n\n# Rename rows and columns using full asset names\ncorr_named = corr.rename(index=tickers_dict, columns=tickers_dict)\n\ng= sns.clustermap(\n    corr_named,\n    cmap=\"magma\",\n    figsize=(16, 14),\n    linewidths=0.5,\n    cbar_kws={'label': 'Correlation'},\n    method='average',\n    metric='correlation')\ng.fig.text(\n    0.5,                # x-position (center)\n    0.875,               # y-position (near bottom)\n    \"Correlation Matrix with Clustered Ordering (Dendrogram Hidden)\",\n    ha='center',\n    fontsize=16\n)\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt  # Compute correlation matrix corr = returns.corr()  # Rename rows and columns using full asset names corr_named = corr.rename(index=tickers_dict, columns=tickers_dict)  g= sns.clustermap(     corr_named,     cmap=\"magma\",     figsize=(16, 14),     linewidths=0.5,     cbar_kws={'label': 'Correlation'},     method='average',     metric='correlation') g.fig.text(     0.5,                # x-position (center)     0.875,               # y-position (near bottom)     \"Correlation Matrix with Clustered Ordering (Dendrogram Hidden)\",     ha='center',     fontsize=16 ) plt.show()  In\u00a0[23]: Copied! <pre>## CVXPY Implementation\n\nn = len(tickers)\nw = cp.Variable(n)\n\n# Weighted scalarization function\ndef solve_portfolio(alpha):\n    objective = cp.Minimize(-alpha * mu.values @ w + (1 - alpha) * cp.quad_form(w, Sigma.values))\n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 0.25]\n    prob = cp.Problem(objective, constraints)\n    prob.solve()\n    return w.value\n</pre> ## CVXPY Implementation  n = len(tickers) w = cp.Variable(n)  # Weighted scalarization function def solve_portfolio(alpha):     objective = cp.Minimize(-alpha * mu.values @ w + (1 - alpha) * cp.quad_form(w, Sigma.values))     constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 0.25]     prob = cp.Problem(objective, constraints)     prob.solve()     return w.value  In\u00a0[24]: Copied! <pre>alphas = np.linspace(0, 1, 100)\nfrontier_returns = []\nfrontier_risks = []\nweights_list = []\n\nfor alpha in alphas:\n    weights = solve_portfolio(alpha)\n    weights_list.append(weights.round(4))\n    r_p = mu.values @ weights\n    sigma_p = np.sqrt(weights.T @ Sigma.values @ weights)\n    frontier_returns.append(r_p)\n    frontier_risks.append(sigma_p)\n</pre> alphas = np.linspace(0, 1, 100) frontier_returns = [] frontier_risks = [] weights_list = []  for alpha in alphas:     weights = solve_portfolio(alpha)     weights_list.append(weights.round(4))     r_p = mu.values @ weights     sigma_p = np.sqrt(weights.T @ Sigma.values @ weights)     frontier_returns.append(r_p)     frontier_risks.append(sigma_p) In\u00a0[\u00a0]: Copied! <pre># Convert weights to DataFrame\nweights_df = pd.DataFrame(weights_list, columns=list(tickers_dict.keys()))\nportfolio_index = np.arange(len(frontier_risks)) + 1\nnum_assets = len(tickers_dict)\n\n# Colors: first 20 solid, next 10 lighter\nbase_colors = plt.get_cmap('tab20').colors\nextra_colors = plt.get_cmap('tab20b').colors\ncolors = list(base_colors) + list(extra_colors)\ncolors = colors[:num_assets]\n\n# Hatches: first 20 none, rest diagonal '/'\n\nif num_assets &lt;= 10:\n    hatches = [\"\"]*num_assets\nelif num_assets &lt;= 20:\n    hatches = [\"\"]*10 + [\"/\"]*(num_assets-10)\nelse:\n    hatches = [\"\"]*10 + [\"/\"]*(10) + [\"\\\\\"]*(num_assets-20)\n\n# Create figure with 2 subplots\nfig, axes = plt.subplots(2, 1, figsize=(22, 16))\n\n# Top plot: Efficient Frontier\naxes[0].plot(frontier_risks, frontier_returns, marker='o', color='b', linewidth=2)\naxes[0].set_xlabel(\"Portfolio Risk (Std Dev)\", fontsize=12)\naxes[0].set_ylabel(\"Portfolio Return\", fontsize=12)\naxes[0].set_title(\"Efficient Frontier: Risk vs Return\", fontsize=14)\naxes[0].grid(True)\n\n# Bottom plot: Asset Weights along Frontier with hatches\nbottom = np.zeros(len(portfolio_index))\nfor i, col in enumerate(weights_df.columns):\n    axes[1].fill_between(portfolio_index,\n                         bottom,\n                         bottom + weights_df[col].values,\n                         color=colors[i],\n                         hatch=hatches[i],\n                         edgecolor='k',\n                         linewidth=0.5,\n                         label=tickers_dict[col])\n    bottom += weights_df[col].values\n\n# Overlay scaled risk and return\naxes[1].plot(portfolio_index, np.array(frontier_risks)/max(frontier_risks), color='k', linestyle='--', linewidth=2, label='Risk (scaled)')\naxes[1].plot(portfolio_index, np.array(frontier_returns)/max(frontier_returns), color='r', linestyle='-', linewidth=2, label='Return (scaled)')\n\naxes[1].set_xlabel(\"Portfolio along Efficient Frontier\", fontsize=12)\naxes[1].set_ylabel(\"Asset Weights / Scaled Risk &amp; Return\", fontsize=12)\naxes[1].set_title(\"Asset Allocation Along Efficient Frontier\", fontsize=14)\naxes[1].legend(loc='upper left', bbox_to_anchor=(1,1), fontsize=10)\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n</pre> # Convert weights to DataFrame weights_df = pd.DataFrame(weights_list, columns=list(tickers_dict.keys())) portfolio_index = np.arange(len(frontier_risks)) + 1 num_assets = len(tickers_dict)  # Colors: first 20 solid, next 10 lighter base_colors = plt.get_cmap('tab20').colors extra_colors = plt.get_cmap('tab20b').colors colors = list(base_colors) + list(extra_colors) colors = colors[:num_assets]  # Hatches: first 20 none, rest diagonal '/'  if num_assets &lt;= 10:     hatches = [\"\"]*num_assets elif num_assets &lt;= 20:     hatches = [\"\"]*10 + [\"/\"]*(num_assets-10) else:     hatches = [\"\"]*10 + [\"/\"]*(10) + [\"\\\\\"]*(num_assets-20)  # Create figure with 2 subplots fig, axes = plt.subplots(2, 1, figsize=(22, 16))  # Top plot: Efficient Frontier axes[0].plot(frontier_risks, frontier_returns, marker='o', color='b', linewidth=2) axes[0].set_xlabel(\"Portfolio Risk (Std Dev)\", fontsize=12) axes[0].set_ylabel(\"Portfolio Return\", fontsize=12) axes[0].set_title(\"Efficient Frontier: Risk vs Return\", fontsize=14) axes[0].grid(True)  # Bottom plot: Asset Weights along Frontier with hatches bottom = np.zeros(len(portfolio_index)) for i, col in enumerate(weights_df.columns):     axes[1].fill_between(portfolio_index,                          bottom,                          bottom + weights_df[col].values,                          color=colors[i],                          hatch=hatches[i],                          edgecolor='k',                          linewidth=0.5,                          label=tickers_dict[col])     bottom += weights_df[col].values  # Overlay scaled risk and return axes[1].plot(portfolio_index, np.array(frontier_risks)/max(frontier_risks), color='k', linestyle='--', linewidth=2, label='Risk (scaled)') axes[1].plot(portfolio_index, np.array(frontier_returns)/max(frontier_returns), color='r', linestyle='-', linewidth=2, label='Return (scaled)')  axes[1].set_xlabel(\"Portfolio along Efficient Frontier\", fontsize=12) axes[1].set_ylabel(\"Asset Weights / Scaled Risk &amp; Return\", fontsize=12) axes[1].set_title(\"Asset Allocation Along Efficient Frontier\", fontsize=14) axes[1].legend(loc='upper left', bbox_to_anchor=(1,1), fontsize=10) axes[1].grid(True)  plt.tight_layout() plt.show()"},{"location":"convex/tutorials/2_portfolio/#ii-meanvariance-portfolio-optimization-a-pareto-optimal-case-study","title":"II - Mean\u2013Variance Portfolio Optimization: A Pareto-Optimal Case Study.\u00b6","text":"<p>Modern Portfolio Theory provides a mathematical framework for constructing investment portfolios that balance expected return against risk, where risk is measured by the variance (or standard deviation) of portfolio returns. This framework\u2014commonly referred to as mean\u2013variance analysis\u2014was introduced by Harry Markowitz and forms the foundation of quantitative portfolio construction.</p> <ul> <li>Mean\u2013Variance Optimization: Select portfolio weights to trade off expected return against return variance.</li> <li>Efficient Frontier: The set of portfolios that achieve the maximum expected return for a given level of risk, or equivalently, the minimum risk for a given expected return.</li> <li>Pareto Optimality: A portfolio is Pareto-optimal if no other feasible portfolio offers higher return at the same level of risk.</li> </ul> <p>In this case study, we analyze a diversified universe of liquid financial assets using publicly available historical market data.</p> <ul> <li>Assets consist of widely traded exchange-traded funds (ETFs) spanning multiple asset classes</li> <li>Historical price data covers several years</li> <li>Expected returns and the covariance matrix are estimated from historical returns</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#formulation","title":"Formulation\u00b6","text":"<p>We seek to determine portfolio weights that optimally balance expected return and risk, subject to realistic investment constraints commonly encountered in practice.</p>"},{"location":"convex/tutorials/2_portfolio/#objective-function","title":"Objective Function\u00b6","text":"<p>Two equivalent formulations are commonly used:</p> <ul> <li><p>Minimize portfolio variance for a target return:  $\\min \\; w^\\top \\Sigma w$</p> </li> <li><p>Maximize risk-adjusted return:  $\\max \\; \\mu^\\top w - \\lambda \\, w^\\top \\Sigma w$</p> </li> </ul> <p>where:</p> <ul> <li>$w$ is the vector of portfolio weights</li> <li>$\\mu$ is the vector of expected asset returns</li> <li>$\\Sigma$ is the covariance matrix of asset returns</li> <li>$\\lambda &gt; 0$ is a risk-aversion parameter</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$w_i$: portfolio weight allocated to asset $i$</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$\\mu_i$: expected return of asset $i$</li> <li>$\\Sigma_{ij}$: covariance between returns of assets $i$ and $j$</li> <li>$\\lambda$: risk-aversion coefficient</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Long-only: portfolio weights must be non-negative ($w_i \\ge 0$).</li> <li>Fully invested: portfolio weights sum to one ($\\sum_i w_i = 1$).</li> <li>Maximum per-asset allocation: $w_i \\le 0.25$ to prevent excessive concentration.</li> </ul> <p>This optimization problem is quadratic programming (QP) problem</p> <ul> <li>The objective function contains a quadratic term $w^\\top \\Sigma w$</li> <li>The covariance matrix $\\Sigma$ is positive semidefinite</li> <li>All constraints are linear</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#data-acquisition-and-cleaning","title":"Data Acquisition and Cleaning\u00b6","text":"<p>We begin by selecting a representative universe of financial assets and obtaining their historical price data from publicly available sources. To ensure reproducibility and eliminate reliance on proprietary services or API keys, we use historical market data from Stooq, a freely accessible financial data repository .</p> <p>The asset universe consists of liquid ETFs spanning multiple asset classes, such as U.S. equities, sector-specific equities, government bonds, and commodities. Historical daily closing prices are retrieved over a multi-year horizon to capture different market conditions and provide sufficient data for robust estimation of expected returns and risk.</p>"},{"location":"convex/tutorials/2_portfolio/#mathematical-formulation","title":"Mathematical Formulation\u00b6","text":"<p>Let:</p> <ul> <li>$\\mathbf{w} = [w_1, w_2, \\dots, w_n]^\\top$ be the portfolio weights,</li> <li>$\\mathbf{r} = [r_1, r_2, \\dots, r_n]^\\top$ be the expected returns,</li> <li>$\\mathbf{C}$ be the covariance matrix of asset returns.</li> </ul> <p>The portfolio expected return and variance are:</p> <p>$$ r_p = \\mathbf{w}^\\top \\mathbf{r}, \\qquad \\sigma_p^2 = \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w}. $$</p>"},{"location":"convex/tutorials/2_portfolio/#objectives","title":"Objectives\u00b6","text":"<p>We aim to maximize return and minimize risk simultaneously. Define:</p> <p>$$ f_1(\\mathbf{w}) = -\\mathbf{r}^\\top \\mathbf{w} \\quad \\text{(maximize return)},  \\qquad f_2(\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w} \\quad \\text{(minimize risk)}. $$</p> <p>A weighted scalarization of the two objectives allows us to trace the efficient frontier:</p> <p>$$ \\min_{\\mathbf{w}} \\ -\\alpha \\, \\mathbf{r}^\\top \\mathbf{w} + (1-\\alpha) \\, \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w},  \\quad 0 \\le \\alpha \\le 1. $$</p> <ul> <li>$\\alpha = 0$ \u2192 minimize risk only.</li> <li>$\\alpha = 1$ \u2192 maximize return only.</li> <li>Varying $\\alpha$ from 0 to 1 produces the Pareto-optimal frontier.</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#constraints","title":"Constraints\u00b6","text":"<p>$$ \\sum_{i=1}^n w_i = 1, \\quad 0 \\le w_i \\le 0.25, \\quad \\forall i. $$</p> <p>By sweeping the trade-off parameter $\\alpha$ in the weighted scalarization problem, we can compute a set of optimal portfolios that balance return and risk under realistic investment constraints. This approach provides both practical portfolio recommendations and a visualization of the efficient frontier, aiding investment decision-making in multi-asset portfolios.</p>"},{"location":"convex/tutorials/3_ga/","title":"III - Vehicle Routing with Time Windows: A Metaheuristic Case Study.","text":"In\u00a0[31]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#import cvxpy as cp\nfrom itertools import product\nimport time\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt #import cvxpy as cp from itertools import product import time   In\u00a0[32]: Copied! <pre>#!pip install ortools\n#!pip install python-tsp\n</pre> #!pip install ortools #!pip install python-tsp  In\u00a0[33]: Copied! <pre>from python_tsp.exact import solve_tsp_dynamic_programming\n</pre> from python_tsp.exact import solve_tsp_dynamic_programming In\u00a0[34]: Copied! <pre>from ortools.constraint_solver import routing_enums_pb2\nfrom ortools.constraint_solver import pywrapcp\n</pre> from ortools.constraint_solver import routing_enums_pb2 from ortools.constraint_solver import pywrapcp  <p>For our case-study we use a benchmark VRPTW dataset based on the classical Solomon instances. Specifically, we utilize the extended Multi-Time-Window VRP dataset recently published on Zenodo, which augments Solomon\u2019s problems with multiple time windows for each customer. This provides realistic distance coordinates, demands, and time-window constraints for a multi-vehicle delivery scenario.</p> In\u00a0[35]: Copied! <pre>df_raw = pd.read_csv('data/multiple time window VRP dataset/multiple time window VRP dataset/data/solomon/C101_MTW.csv')\n\n# randomly select 20 customers along with the depot (CUST_NO 0)\n\ndf = df_raw.sample(n=20, random_state=1).reset_index(drop=True)\ndf = pd.concat([pd.DataFrame([df_raw.iloc[0]]), df]).reset_index(drop=True)\n\n\nprint(len(df))\n#df.head()\n#print(df.columns)\n</pre> df_raw = pd.read_csv('data/multiple time window VRP dataset/multiple time window VRP dataset/data/solomon/C101_MTW.csv')  # randomly select 20 customers along with the depot (CUST_NO 0)  df = df_raw.sample(n=20, random_state=1).reset_index(drop=True) df = pd.concat([pd.DataFrame([df_raw.iloc[0]]), df]).reset_index(drop=True)   print(len(df)) #df.head() #print(df.columns) <pre>21\n</pre> In\u00a0[36]: Copied! <pre>depot = df[df['CUST_NO'] == 0]\ncustomers = df[df['CUST_NO'] != 0]\n\n# Create the plot\nplt.figure(figsize=(8, 6))\n\n# Plot depot with red square\nplt.scatter(depot['XCOORD'], depot['YCOORD'], color='red', marker='s', s=100, label='Depot (Customer 0)')\n\n# Plot other customers with blue circles\nplt.scatter(customers['XCOORD'], customers['YCOORD'], color='blue', marker='o', s=60, label='Customers')\n\n# Annotate all points with their customer number\nfor _, row in df.iterrows():\n    plt.text(row['XCOORD'] + 0.5, row['YCOORD'] + 0.5, str(row['CUST_NO']), fontsize=9)\n\n# Set plot labels and title\nplt.xlabel('X Coordinate')\nplt.ylabel('Y Coordinate')\nplt.title('Customer Locations with Depot')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</pre> depot = df[df['CUST_NO'] == 0] customers = df[df['CUST_NO'] != 0]  # Create the plot plt.figure(figsize=(8, 6))  # Plot depot with red square plt.scatter(depot['XCOORD'], depot['YCOORD'], color='red', marker='s', s=100, label='Depot (Customer 0)')  # Plot other customers with blue circles plt.scatter(customers['XCOORD'], customers['YCOORD'], color='blue', marker='o', s=60, label='Customers')  # Annotate all points with their customer number for _, row in df.iterrows():     plt.text(row['XCOORD'] + 0.5, row['YCOORD'] + 0.5, str(row['CUST_NO']), fontsize=9)  # Set plot labels and title plt.xlabel('X Coordinate') plt.ylabel('Y Coordinate') plt.title('Customer Locations with Depot') plt.legend() plt.grid(True) plt.tight_layout() plt.show()  In\u00a0[37]: Copied! <pre># sort customers\ndf_plot = df.sort_values(\"CUST_NO\")\n# remove 0 customer number\ndf_plot = df_plot[df_plot[\"CUST_NO\"] != 0]\n# y positions\ny_pos = range(len(df_plot))\n\n# time window lengths\nwindow_length = df_plot[\"DUE_TIME_1\"] - df_plot[\"READY_TIME_1\"]\n\nplt.figure(figsize=(10, 6))\n\n# horizontal bars\nplt.barh(\n    y_pos,\n    window_length,\n    left=df_plot[\"READY_TIME_1\"],\n    alpha=0.8\n)\n\n# y-axis labels\nplt.yticks(y_pos, df_plot[\"CUST_NO\"])\n\n# labels and title\nplt.xlabel(\"Time (minutes)\")\nplt.ylabel(\"Customer\")\nplt.title(\"Customer Time Windows\")\n\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n</pre> # sort customers df_plot = df.sort_values(\"CUST_NO\") # remove 0 customer number df_plot = df_plot[df_plot[\"CUST_NO\"] != 0] # y positions y_pos = range(len(df_plot))  # time window lengths window_length = df_plot[\"DUE_TIME_1\"] - df_plot[\"READY_TIME_1\"]  plt.figure(figsize=(10, 6))  # horizontal bars plt.barh(     y_pos,     window_length,     left=df_plot[\"READY_TIME_1\"],     alpha=0.8 )  # y-axis labels plt.yticks(y_pos, df_plot[\"CUST_NO\"])  # labels and title plt.xlabel(\"Time (minutes)\") plt.ylabel(\"Customer\") plt.title(\"Customer Time Windows\")  plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5) plt.tight_layout() plt.show()  In\u00a0[38]: Copied! <pre>depot_index = int(df.index[df['CUST_NO'] == 0][0])\n\n# Compute Euclidean distance matrix\ncoords_array = df[['XCOORD','YCOORD']].to_numpy()\ndist_matrix = np.sqrt(((coords_array[:,None,:] - coords_array[None,:,:])**2).sum(axis=2))\n</pre> depot_index = int(df.index[df['CUST_NO'] == 0][0])  # Compute Euclidean distance matrix coords_array = df[['XCOORD','YCOORD']].to_numpy() dist_matrix = np.sqrt(((coords_array[:,None,:] - coords_array[None,:,:])**2).sum(axis=2)) In\u00a0[39]: Copied! <pre>scale = 1\nscaled_dist_matrix = np.rint(dist_matrix * scale).astype(int)\n</pre> scale = 1 scaled_dist_matrix = np.rint(dist_matrix * scale).astype(int) In\u00a0[40]: Copied! <pre>## Data Model\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = 1\ndata['depot'] = depot_index\n</pre> ## Data Model data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = 1 data['depot'] = depot_index In\u00a0[\u00a0]: Copied! <pre>D = np.array(scaled_dist_matrix)\n# Reorder so the depot becomes node 0 (python-tsp assumes start at 0)\nn = D.shape[0]\nperm = [depot_index] + [i for i in range(n) if i != depot_index]\nD2 = D[np.ix_(perm, perm)]\n# Exact TSP (cycle). Returns permutation starting at 0 and the optimal tour length.\n\nt_start = time.perf_counter()\n# Exact TSP (Held\u2013Karp)\n_, total_distance = solve_tsp_dynamic_programming(D2)\nt_end = time.perf_counter()\nprint(f\"Exact TSP (Held\u2013Karp) took {t_end - t_start:.4f} seconds\")\nprint(\"Optimal total distance:\", total_distance)\n</pre> D = np.array(scaled_dist_matrix) # Reorder so the depot becomes node 0 (python-tsp assumes start at 0) n = D.shape[0] perm = [depot_index] + [i for i in range(n) if i != depot_index] D2 = D[np.ix_(perm, perm)] # Exact TSP (cycle). Returns permutation starting at 0 and the optimal tour length.  t_start = time.perf_counter() # Exact TSP (Held\u2013Karp) _, total_distance = solve_tsp_dynamic_programming(D2) t_end = time.perf_counter() print(f\"Exact TSP (Held\u2013Karp) took {t_end - t_start:.4f} seconds\") print(\"Optimal total distance:\", total_distance) In\u00a0[\u00a0]: Copied! <pre># Create the routing model\n# The pywrapcp.RoutingIndexManager is responsible for translating between external \n# node indices (which you use in your problem definition) and internal solver indices.\n# The inputs to RoutingIndexManager are:\n# - The number of rows of the distance matrix, which is the number of locations (including the depot).\n# - The number of vehicles in the problem.\n# - The node corresponding to the depot.\n\nmanager = pywrapcp.RoutingIndexManager(\n    len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"]\n)\nrouting = pywrapcp.RoutingModel(manager)\n\n# Create the distance callback\n# To use the routing solver, you need to create a distance (or transit) callback: \n# a function that takes any pair of locations and returns the distance between them.\n# The easiest way to do this is using the distance matrix.\n\ndef distance_callback(from_index, to_index):\n    \"\"\"Returns the distance between the two nodes.\"\"\"\n    # Convert from routing variable Index to distance matrix NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    return data[\"distance_matrix_scaled\"][from_node][to_node]\n\ntransit_callback_index = routing.RegisterTransitCallback(distance_callback)\n\n# Set the cost of travel\n# The arc cost evaluator tells the solver how to calculate the cost of travel \n# between any two locations \u2014 in other words, the cost of the edge (or arc) \n# joining them in the graph for the problem. The following code sets the arc cost \n# evaluator.\n\nrouting.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n\n# Set search parameters\n# The code sets the first solution strategy to PATH_CHEAPEST_ARC, \n# which creates an initial route for the solver by repeatedly adding edges \n# with the least weight that don't lead to a previously visited node \n# (other than the depot).\n\nsearch_parameters = pywrapcp.DefaultRoutingSearchParameters()\nsearch_parameters.first_solution_strategy = (\n    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n)\n</pre> # Create the routing model # The pywrapcp.RoutingIndexManager is responsible for translating between external  # node indices (which you use in your problem definition) and internal solver indices. # The inputs to RoutingIndexManager are: # - The number of rows of the distance matrix, which is the number of locations (including the depot). # - The number of vehicles in the problem. # - The node corresponding to the depot.  manager = pywrapcp.RoutingIndexManager(     len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"] ) routing = pywrapcp.RoutingModel(manager)  # Create the distance callback # To use the routing solver, you need to create a distance (or transit) callback:  # a function that takes any pair of locations and returns the distance between them. # The easiest way to do this is using the distance matrix.  def distance_callback(from_index, to_index):     \"\"\"Returns the distance between the two nodes.\"\"\"     # Convert from routing variable Index to distance matrix NodeIndex.     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     return data[\"distance_matrix_scaled\"][from_node][to_node]  transit_callback_index = routing.RegisterTransitCallback(distance_callback)  # Set the cost of travel # The arc cost evaluator tells the solver how to calculate the cost of travel  # between any two locations \u2014 in other words, the cost of the edge (or arc)  # joining them in the graph for the problem. The following code sets the arc cost  # evaluator.  routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)  # Set search parameters # The code sets the first solution strategy to PATH_CHEAPEST_ARC,  # which creates an initial route for the solver by repeatedly adding edges  # with the least weight that don't lead to a previously visited node  # (other than the depot).  search_parameters = pywrapcp.DefaultRoutingSearchParameters() search_parameters.first_solution_strategy = (     routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC )  In\u00a0[\u00a0]: Copied! <pre># Solution Printer\ndef print_solution(manager, routing, solution):\n    \"\"\"Prints solution on console.\"\"\"\n    print(f\"Objective: {solution.ObjectiveValue()} miles\")\n    index = routing.Start(0)\n    plan_output = \"Route for vehicle 0:\\n\"\n    route_distance = 0\n    while not routing.IsEnd(index):\n        plan_output += f\" {manager.IndexToNode(index)} -&gt;\"\n        previous_index = index\n        index = solution.Value(routing.NextVar(index))\n        route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)\n    plan_output += f\" {manager.IndexToNode(index)}\\n\"\n    plan_output += f\"Route distance: {route_distance}miles\\n\"\n    print(plan_output)\n</pre> # Solution Printer def print_solution(manager, routing, solution):     \"\"\"Prints solution on console.\"\"\"     print(f\"Objective: {solution.ObjectiveValue()} miles\")     index = routing.Start(0)     plan_output = \"Route for vehicle 0:\\n\"     route_distance = 0     while not routing.IsEnd(index):         plan_output += f\" {manager.IndexToNode(index)} -&gt;\"         previous_index = index         index = solution.Value(routing.NextVar(index))         route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)     plan_output += f\" {manager.IndexToNode(index)}\\n\"     plan_output += f\"Route distance: {route_distance}miles\\n\"     print(plan_output)  In\u00a0[\u00a0]: Copied! <pre>t_start = time.perf_counter()\nsolution = routing.SolveWithParameters(search_parameters)\nt_end = time.perf_counter()\nprint(f\"OR Tools took {t_end - t_start:.4f} seconds\")\n\nif solution:\n    print_solution(manager, routing, solution)\n</pre> t_start = time.perf_counter() solution = routing.SolveWithParameters(search_parameters) t_end = time.perf_counter() print(f\"OR Tools took {t_end - t_start:.4f} seconds\")  if solution:     print_solution(manager, routing, solution) <pre>OR Tools took 0.0511 seconds\nObjective: 307 miles\nRoute for vehicle 0:\n 0 -&gt; 15 -&gt; 17 -&gt; 7 -&gt; 20 -&gt; 18 -&gt; 2 -&gt; 19 -&gt; 9 -&gt; 8 -&gt; 6 -&gt; 11 -&gt; 1 -&gt; 14 -&gt; 4 -&gt; 3 -&gt; 5 -&gt; 16 -&gt; 13 -&gt; 10 -&gt; 12 -&gt; 0\nRoute distance: 307miles\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_tsp_route_ortools(df, manager, routing, solution,\n                          vehicle_id=0,\n                          distance_matrix=None,   # e.g., data['distance_matrix_scaled'] or data['distance_matrix']\n                          scale=1,                # e.g., 1000 if using scaled costs, else 1\n                          x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",\n                          show_labels=True):\n    \"\"\"\n    One-stop function:\n      1) Extracts the OR-Tools route for `vehicle_id`\n      2) Plots points + arrows\n      3) Computes total route distance using your EXISTING distance matrix\n         (so it matches what OR-Tools optimized)\n\n    Notes:\n      - Assumes IndexToNode outputs indices aligned with df row order (df reset_index recommended).\n      - distance_matrix should be the same one you used for costs, typically scaled.\n    \"\"\"\n    if solution is None:\n        raise ValueError(\"No solution found (solution is None).\")\n\n    if distance_matrix is None:\n        raise ValueError(\"Please pass distance_matrix (e.g., data['distance_matrix_scaled']).\")\n\n    xs = df[x_col].to_numpy()\n    ys = df[y_col].to_numpy()\n\n    # --- Extract route nodes (including end depot) ---\n    index = routing.Start(vehicle_id)\n    route = [manager.IndexToNode(index)]\n    while not routing.IsEnd(index):\n        index = solution.Value(routing.NextVar(index))\n        route.append(manager.IndexToNode(index))\n\n    # --- Compute total distance from existing matrix ---\n    total_cost = 0\n    for a, b in zip(route[:-1], route[1:]):\n        total_cost += int(distance_matrix[a][b])\n    total_distance = total_cost / scale\n\n    # --- Plot ---\n    plt.figure(figsize=(8, 6))\n    plt.scatter(xs, ys)\n\n    # depot highlight (assumes depot is route[0])\n    depot_node = route[0]\n    plt.scatter([xs[depot_node]], [ys[depot_node]], s=140, marker=\"s\", label=\"Depot\")\n\n    # labels\n    if show_labels:\n        labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))\n        for i in range(len(df)):\n            plt.text(xs[i], ys[i], str(labels[i]), fontsize=9, ha=\"left\", va=\"bottom\")\n\n    # arrows\n    for a, b in zip(route[:-1], route[1:]):\n        plt.annotate(\n            \"\",\n            xy=(xs[b], ys[b]),\n            xytext=(xs[a], ys[a]),\n            arrowprops=dict(arrowstyle=\"-&gt;\", lw=1),\n        )\n\n    plt.title(f\"TSP (vehicle {vehicle_id}) | Total distance = {total_distance:.2f}\")\n    plt.xlabel(x_col)\n    plt.ylabel(y_col)\n    plt.axis(\"equal\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    return route, total_distance\n</pre> def plot_tsp_route_ortools(df, manager, routing, solution,                           vehicle_id=0,                           distance_matrix=None,   # e.g., data['distance_matrix_scaled'] or data['distance_matrix']                           scale=1,                # e.g., 1000 if using scaled costs, else 1                           x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",                           show_labels=True):     \"\"\"     One-stop function:       1) Extracts the OR-Tools route for `vehicle_id`       2) Plots points + arrows       3) Computes total route distance using your EXISTING distance matrix          (so it matches what OR-Tools optimized)      Notes:       - Assumes IndexToNode outputs indices aligned with df row order (df reset_index recommended).       - distance_matrix should be the same one you used for costs, typically scaled.     \"\"\"     if solution is None:         raise ValueError(\"No solution found (solution is None).\")      if distance_matrix is None:         raise ValueError(\"Please pass distance_matrix (e.g., data['distance_matrix_scaled']).\")      xs = df[x_col].to_numpy()     ys = df[y_col].to_numpy()      # --- Extract route nodes (including end depot) ---     index = routing.Start(vehicle_id)     route = [manager.IndexToNode(index)]     while not routing.IsEnd(index):         index = solution.Value(routing.NextVar(index))         route.append(manager.IndexToNode(index))      # --- Compute total distance from existing matrix ---     total_cost = 0     for a, b in zip(route[:-1], route[1:]):         total_cost += int(distance_matrix[a][b])     total_distance = total_cost / scale      # --- Plot ---     plt.figure(figsize=(8, 6))     plt.scatter(xs, ys)      # depot highlight (assumes depot is route[0])     depot_node = route[0]     plt.scatter([xs[depot_node]], [ys[depot_node]], s=140, marker=\"s\", label=\"Depot\")      # labels     if show_labels:         labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))         for i in range(len(df)):             plt.text(xs[i], ys[i], str(labels[i]), fontsize=9, ha=\"left\", va=\"bottom\")      # arrows     for a, b in zip(route[:-1], route[1:]):         plt.annotate(             \"\",             xy=(xs[b], ys[b]),             xytext=(xs[a], ys[a]),             arrowprops=dict(arrowstyle=\"-&gt;\", lw=1),         )      plt.title(f\"TSP (vehicle {vehicle_id}) | Total distance = {total_distance:.2f}\")     plt.xlabel(x_col)     plt.ylabel(y_col)     plt.axis(\"equal\")     plt.grid(True)     plt.legend()     plt.show()      return route, total_distance  In\u00a0[\u00a0]: Copied! <pre>route, dist = plot_tsp_route_ortools(df, manager, \n                                     routing, solution,\n                                     vehicle_id=0,\n                                     distance_matrix=data['distance_matrix_scaled'],\n                                     scale=1)\n</pre> route, dist = plot_tsp_route_ortools(df, manager,                                       routing, solution,                                      vehicle_id=0,                                      distance_matrix=data['distance_matrix_scaled'],                                      scale=1) In\u00a0[\u00a0]: Copied! <pre>## Data Model\n\nK = 4\n\nvehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K))\nprint(vehicle_capacity)\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = K\ndata['demands'] = df['DEMAND'].tolist()\ndata['vehicle_capacities'] = [vehicle_capacity]*K\ndata['depot'] = depot_index\n</pre> ## Data Model  K = 4  vehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K)) print(vehicle_capacity) data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = K data['demands'] = df['DEMAND'].tolist() data['vehicle_capacities'] = [vehicle_capacity]*K data['depot'] = depot_index <pre>108\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create the routing model\nmanager = pywrapcp.RoutingIndexManager(\n    len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"]\n)\n\nrouting = pywrapcp.RoutingModel(manager)\n\n# Create the distance callback\ndef distance_callback(from_index, to_index):\n    \"\"\"Returns the distance between the two nodes.\"\"\"\n    # Convert from routing variable Index to distance matrix NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    return int(data[\"distance_matrix_scaled\"][from_node][to_node])\n\ntransit_callback_index = routing.RegisterTransitCallback(distance_callback)\n\n# Define cost of each arc.\nrouting.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n\n# Add Capacity constraint.\ndef demand_callback(from_index):\n    \"\"\"Returns the demand of the node.\"\"\"\n    # Convert from routing variable Index to demands NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    return int(data[\"demands\"][from_node])\n\ndemand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)\nrouting.AddDimensionWithVehicleCapacity(\n        demand_callback_index,\n        0,  # null capacity slack\n        data[\"vehicle_capacities\"],  # vehicle maximum capacities\n        True,  # start cumul to zero\n        \"Capacity\",\n)\n\n# Set search parameters\n# Setting first solution heuristic.\nsearch_parameters = pywrapcp.DefaultRoutingSearchParameters()\nsearch_parameters.first_solution_strategy = (\n    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n)\nsearch_parameters.local_search_metaheuristic = (\n    routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH\n)\nsearch_parameters.time_limit.FromSeconds(1)\n</pre> # Create the routing model manager = pywrapcp.RoutingIndexManager(     len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"] )  routing = pywrapcp.RoutingModel(manager)  # Create the distance callback def distance_callback(from_index, to_index):     \"\"\"Returns the distance between the two nodes.\"\"\"     # Convert from routing variable Index to distance matrix NodeIndex.     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     return int(data[\"distance_matrix_scaled\"][from_node][to_node])  transit_callback_index = routing.RegisterTransitCallback(distance_callback)  # Define cost of each arc. routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)  # Add Capacity constraint. def demand_callback(from_index):     \"\"\"Returns the demand of the node.\"\"\"     # Convert from routing variable Index to demands NodeIndex.     from_node = manager.IndexToNode(from_index)     return int(data[\"demands\"][from_node])  demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback) routing.AddDimensionWithVehicleCapacity(         demand_callback_index,         0,  # null capacity slack         data[\"vehicle_capacities\"],  # vehicle maximum capacities         True,  # start cumul to zero         \"Capacity\", )  # Set search parameters # Setting first solution heuristic. search_parameters = pywrapcp.DefaultRoutingSearchParameters() search_parameters.first_solution_strategy = (     routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC ) search_parameters.local_search_metaheuristic = (     routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH ) search_parameters.time_limit.FromSeconds(1) In\u00a0[\u00a0]: Copied! <pre>solution = routing.SolveWithParameters(search_parameters)\n</pre> solution = routing.SolveWithParameters(search_parameters) In\u00a0[\u00a0]: Copied! <pre>def plot_cvrp_routes_ortools(df, manager, routing, solution,\n                            distance_matrix=None,\n                            scale=1,\n                            x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",\n                            show_labels=True,\n                            show_arrows=True):\n    \"\"\"\n    CVRP visualizer with:\n      - per-vehicle colored routes\n      - per-vehicle distance + load in legend\n      - total distance + total load in title\n    \"\"\"\n    if solution is None:\n        raise ValueError(\"No solution found.\")\n    if distance_matrix is None:\n        raise ValueError(\"Please pass distance_matrix.\")\n\n    xs = df[x_col].to_numpy()\n    ys = df[y_col].to_numpy()\n\n    labels = None\n    if show_labels:\n        labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))\n\n    depot_node = manager.IndexToNode(routing.Start(0))\n\n    # Capacity dimension (if present)\n    try:\n        cap_dim = routing.GetDimensionOrDie(\"Capacity\")\n    except Exception:\n        cap_dim = None\n\n    routes = {}\n    per_vehicle_distance = {}\n    per_vehicle_load = {}\n\n    total_cost = 0\n    total_load = 0\n\n    # --- Extract routes, distance, load ---\n    for v in range(routing.vehicles()):\n        start_index = routing.Start(v)\n        if solution.Value(routing.NextVar(start_index)) == routing.End(v):\n            continue\n\n        index = start_index\n        route = [manager.IndexToNode(index)]\n        route_cost = 0\n\n        while not routing.IsEnd(index):\n            node = manager.IndexToNode(index)\n            next_index = solution.Value(routing.NextVar(index))\n            next_node = manager.IndexToNode(next_index)\n\n            route.append(next_node)\n            route_cost += int(distance_matrix[node][next_node])\n            index = next_index\n\n        routes[v] = route\n        per_vehicle_distance[v] = route_cost / scale\n        total_cost += route_cost\n\n        if cap_dim is not None:\n            load = solution.Value(cap_dim.CumulVar(routing.End(v)))\n        else:\n            load = 0\n\n        per_vehicle_load[v] = load\n        total_load += load\n\n    total_distance = total_cost / scale\n\n    # --- Plot base points ---\n    plt.figure(figsize=(9, 7))\n    plt.scatter(xs, ys, label=\"Customers\")\n    plt.scatter([xs[depot_node]], [ys[depot_node]],\n                s=180, marker=\"s\", label=\"Depot\")\n\n    if show_labels:\n        for i in range(len(df)):\n            plt.text(xs[i], ys[i], str(labels[i]),\n                     fontsize=9, ha=\"left\", va=\"bottom\")\n\n    # --- Colors ---\n    used_vehicles = list(routes.keys())\n    cmap = plt.get_cmap(\"tab10\") if len(used_vehicles) &lt;= 10 else plt.get_cmap(\"tab20\")\n\n    # --- Draw routes ---\n    for idx, v in enumerate(used_vehicles):\n        route = routes[v]\n        color = cmap(idx % cmap.N)\n\n        for a, b in zip(route[:-1], route[1:]):\n            x1, y1 = xs[a], ys[a]\n            x2, y2 = xs[b], ys[b]\n\n            plt.plot([x1, x2], [y1, y2], color=color, lw=1.8)\n\n            if show_arrows:\n                plt.annotate(\n                    \"\",\n                    xy=(x2, y2),\n                    xytext=(x1, y1),\n                    arrowprops=dict(arrowstyle=\"-&gt;\", lw=1.8, color=color),\n                )\n\n        # legend entry\n        plt.plot(\n            [],\n            [],\n            color=color,\n            lw=3,\n            label=f\"Vehicle {v} | dist={per_vehicle_distance[v]:.2f}, load={per_vehicle_load[v]}\"\n        )\n\n    plt.title(\n        f\"CVRP Routes | Vehicles used = {len(routes)} | \"\n        f\"Total distance = {total_distance:.2f} | Total load = {total_load}\"\n    )\n    plt.xlabel(x_col)\n    plt.ylabel(y_col)\n    plt.axis(\"equal\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    return routes, total_distance, per_vehicle_distance, per_vehicle_load\n</pre> def plot_cvrp_routes_ortools(df, manager, routing, solution,                             distance_matrix=None,                             scale=1,                             x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",                             show_labels=True,                             show_arrows=True):     \"\"\"     CVRP visualizer with:       - per-vehicle colored routes       - per-vehicle distance + load in legend       - total distance + total load in title     \"\"\"     if solution is None:         raise ValueError(\"No solution found.\")     if distance_matrix is None:         raise ValueError(\"Please pass distance_matrix.\")      xs = df[x_col].to_numpy()     ys = df[y_col].to_numpy()      labels = None     if show_labels:         labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))      depot_node = manager.IndexToNode(routing.Start(0))      # Capacity dimension (if present)     try:         cap_dim = routing.GetDimensionOrDie(\"Capacity\")     except Exception:         cap_dim = None      routes = {}     per_vehicle_distance = {}     per_vehicle_load = {}      total_cost = 0     total_load = 0      # --- Extract routes, distance, load ---     for v in range(routing.vehicles()):         start_index = routing.Start(v)         if solution.Value(routing.NextVar(start_index)) == routing.End(v):             continue          index = start_index         route = [manager.IndexToNode(index)]         route_cost = 0          while not routing.IsEnd(index):             node = manager.IndexToNode(index)             next_index = solution.Value(routing.NextVar(index))             next_node = manager.IndexToNode(next_index)              route.append(next_node)             route_cost += int(distance_matrix[node][next_node])             index = next_index          routes[v] = route         per_vehicle_distance[v] = route_cost / scale         total_cost += route_cost          if cap_dim is not None:             load = solution.Value(cap_dim.CumulVar(routing.End(v)))         else:             load = 0          per_vehicle_load[v] = load         total_load += load      total_distance = total_cost / scale      # --- Plot base points ---     plt.figure(figsize=(9, 7))     plt.scatter(xs, ys, label=\"Customers\")     plt.scatter([xs[depot_node]], [ys[depot_node]],                 s=180, marker=\"s\", label=\"Depot\")      if show_labels:         for i in range(len(df)):             plt.text(xs[i], ys[i], str(labels[i]),                      fontsize=9, ha=\"left\", va=\"bottom\")      # --- Colors ---     used_vehicles = list(routes.keys())     cmap = plt.get_cmap(\"tab10\") if len(used_vehicles) &lt;= 10 else plt.get_cmap(\"tab20\")      # --- Draw routes ---     for idx, v in enumerate(used_vehicles):         route = routes[v]         color = cmap(idx % cmap.N)          for a, b in zip(route[:-1], route[1:]):             x1, y1 = xs[a], ys[a]             x2, y2 = xs[b], ys[b]              plt.plot([x1, x2], [y1, y2], color=color, lw=1.8)              if show_arrows:                 plt.annotate(                     \"\",                     xy=(x2, y2),                     xytext=(x1, y1),                     arrowprops=dict(arrowstyle=\"-&gt;\", lw=1.8, color=color),                 )          # legend entry         plt.plot(             [],             [],             color=color,             lw=3,             label=f\"Vehicle {v} | dist={per_vehicle_distance[v]:.2f}, load={per_vehicle_load[v]}\"         )      plt.title(         f\"CVRP Routes | Vehicles used = {len(routes)} | \"         f\"Total distance = {total_distance:.2f} | Total load = {total_load}\"     )     plt.xlabel(x_col)     plt.ylabel(y_col)     plt.axis(\"equal\")     plt.grid(True)     plt.legend()     plt.show()      return routes, total_distance, per_vehicle_distance, per_vehicle_load  In\u00a0[\u00a0]: Copied! <pre>routes, total_dist, per_v_dist, per_v_load  = plot_cvrp_routes_ortools(\n    df, manager, routing, solution,\n    distance_matrix=data['distance_matrix_scaled'],\n    scale=1,           \n    show_labels=True,\n    show_arrows=True\n)\nprint(\"Total distance:\", total_dist)\nprint(\"Per-vehicle:\", per_v_dist)\n</pre> routes, total_dist, per_v_dist, per_v_load  = plot_cvrp_routes_ortools(     df, manager, routing, solution,     distance_matrix=data['distance_matrix_scaled'],     scale=1,                show_labels=True,     show_arrows=True ) print(\"Total distance:\", total_dist) print(\"Per-vehicle:\", per_v_dist) <pre>Total distance: 450.0\nPer-vehicle: {0: 92.0, 1: 123.0, 2: 74.0, 3: 161.0}\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Data Model\n\nK = 4\n\nvehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K))\nprint(vehicle_capacity)\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = K\ndata['demands'] = df['DEMAND'].tolist()\ndata['vehicle_capacities'] = [vehicle_capacity]*K\ndata['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1']))  \ndata['depot'] = depot_index\n</pre> ## Data Model  K = 4  vehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K)) print(vehicle_capacity) data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = K data['demands'] = df['DEMAND'].tolist() data['vehicle_capacities'] = [vehicle_capacity]*K data['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1']))   data['depot'] = depot_index <pre>108\n</pre> In\u00a0[\u00a0]: Copied! <pre>def print_solution(data, manager, routing, solution):\n    \"\"\"Prints solution on console.\"\"\"\n    print(f\"Objective: {solution.ObjectiveValue()}\")\n    total_distance = 0\n    total_load = 0\n    for vehicle_id in range(data[\"num_vehicles\"]):\n        if not routing.IsVehicleUsed(solution, vehicle_id):\n            continue\n        index = routing.Start(vehicle_id)\n        plan_output = f\"Route for vehicle {vehicle_id}:\\n\"\n        route_distance = 0\n        route_load = 0\n        while not routing.IsEnd(index):\n            node_index = manager.IndexToNode(index)\n            route_load += data[\"demands\"][node_index]\n            plan_output += f\" {node_index} Load({route_load}) -&gt; \"\n            previous_index = index\n            index = solution.Value(routing.NextVar(index))\n            route_distance += routing.GetArcCostForVehicle(\n                previous_index, index, vehicle_id\n            )\n        plan_output += f\" {manager.IndexToNode(index)} Load({route_load})\\n\"\n        plan_output += f\"Distance of the route: {route_distance}m\\n\"\n        plan_output += f\"Load of the route: {route_load}\\n\"\n        print(plan_output)\n        total_distance += route_distance\n        total_load += route_load\n    print(f\"Total distance of all routes: {total_distance}m\")\n    print(f\"Total load of all routes: {total_load}\")\n</pre> def print_solution(data, manager, routing, solution):     \"\"\"Prints solution on console.\"\"\"     print(f\"Objective: {solution.ObjectiveValue()}\")     total_distance = 0     total_load = 0     for vehicle_id in range(data[\"num_vehicles\"]):         if not routing.IsVehicleUsed(solution, vehicle_id):             continue         index = routing.Start(vehicle_id)         plan_output = f\"Route for vehicle {vehicle_id}:\\n\"         route_distance = 0         route_load = 0         while not routing.IsEnd(index):             node_index = manager.IndexToNode(index)             route_load += data[\"demands\"][node_index]             plan_output += f\" {node_index} Load({route_load}) -&gt; \"             previous_index = index             index = solution.Value(routing.NextVar(index))             route_distance += routing.GetArcCostForVehicle(                 previous_index, index, vehicle_id             )         plan_output += f\" {manager.IndexToNode(index)} Load({route_load})\\n\"         plan_output += f\"Distance of the route: {route_distance}m\\n\"         plan_output += f\"Load of the route: {route_load}\\n\"         print(plan_output)         total_distance += route_distance         total_load += route_load     print(f\"Total distance of all routes: {total_distance}m\")     print(f\"Total load of all routes: {total_load}\")  In\u00a0[\u00a0]: Copied! <pre>total_capacity = K * vehicle_capacity\ntotal_demand = sum(data['demands'])\nif total_demand &gt; total_capacity:\n    raise ValueError(\"Total demand exceeds total vehicle capacity.\")\n</pre> total_capacity = K * vehicle_capacity total_demand = sum(data['demands']) if total_demand &gt; total_capacity:     raise ValueError(\"Total demand exceeds total vehicle capacity.\") In\u00a0[\u00a0]: Copied! <pre>N = len(df)\nK = 20\nvehicle_capacity = 100\nvehicle_fix_cost = 1 #dist[dist!=0].mean()\n</pre> N = len(df) K = 20 vehicle_capacity = 100 vehicle_fix_cost = 1 #dist[dist!=0].mean() In\u00a0[\u00a0]: Copied! <pre># ------------------------\n# OR-Tools Data Model\n# ------------------------\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = K\ndata['demands'] = df['DEMAND'].tolist()\ndata['vehicle_capacities'] = [vehicle_capacity]*K\ndata['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1']))\ndata['depot'] = depot_index\n</pre> # ------------------------ # OR-Tools Data Model # ------------------------ data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = K data['demands'] = df['DEMAND'].tolist() data['vehicle_capacities'] = [vehicle_capacity]*K data['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1'])) data['depot'] = depot_index  In\u00a0[\u00a0]: Copied! <pre>total_capacity = K * vehicle_capacity\ntotal_demand = int(df['DEMAND'].sum())\nif total_demand &gt; total_capacity:\n    raise ValueError(\"Total demand exceeds total vehicle capacity.\")\n</pre> total_capacity = K * vehicle_capacity total_demand = int(df['DEMAND'].sum()) if total_demand &gt; total_capacity:     raise ValueError(\"Total demand exceeds total vehicle capacity.\") In\u00a0[\u00a0]: Copied! <pre># ------------------------\n# Routing Model\n# ------------------------\n# OR-Tools uses a RoutingIndexManager and RoutingModel to handle VRP problems.\n# RoutingIndexManager maps the problem nodes to internal solver indices, which allows\n# multiple vehicles, depots, and flexible routing.\nmanager = pywrapcp.RoutingIndexManager(\n    len(data['distance_matrix_scaled']),  # Number of nodes (customers + depot)\n    data['num_vehicles'],          # Number of vehicles available\n    data['depot']                  # Index of the depot node\n)\nrouting = pywrapcp.RoutingModel(manager)  # Core routing model object\n\n# ------------------------\n# Distance callback\n# ------------------------\n# OR-Tools requires a callback to compute the \"cost\" of traveling from node i to j.\n# Here, we define a function that returns the distance between two nodes.\n\n# Scale distances up so that OR-Tools can see non-zero costs\n\ndef distance_callback(from_index, to_index):\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    return int(data['distance_matrix_scaled'][from_node][to_node] + 1)\n\n\n# Register the callback with the routing model\ntransit_callback_index = routing.RegisterTransitCallback(distance_callback)\n\n# Set the arc cost evaluator for all vehicles\n# This tells OR-Tools to minimize total distance when choosing routes\nrouting.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n\n# ------------------------\n# Capacity constraint\n# ------------------------\n# Each vehicle has a maximum capacity. We register a callback to return the demand\n# at each node.\ndef demand_callback(from_index):\n    from_node = manager.IndexToNode(from_index)\n    return data['demands'][from_node]  # Customer demand\n\ndemand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)\n\n# Add capacity dimension to the routing model\n# This ensures that the sum of demands along a vehicle's route does not exceed capacity\nrouting.AddDimensionWithVehicleCapacity(\n    demand_callback_index,       # Callback for demands\n    0,                           # Null slack (no extra capacity allowed)\n    data['vehicle_capacities'],  # Vehicle capacities list\n    True,                        # Start cumul (load) at zero for each vehicle\n    'Capacity'                   # Name of the dimension\n)\n\n# ------------------------\n# Time window constraint\n# ------------------------\n# Each customer has a time window (ready_time to due_time)\n# First, define a transit callback for time between nodes\ndef time_callback(from_index, to_index):\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    # Here, travel time = distance (assuming 1 unit distance = 1 unit time)\n    return int(data['distance_matrix'][from_node][to_node])\n\n# Register the time callback\ntime_callback_index = routing.RegisterTransitCallback(time_callback)\n\n# Add a time dimension to enforce time windows\n# This dimension tracks the cumulative \"arrival time\" at each node\nmax_due = int(df['DUE_TIME_1'].max())\n\nrouting.AddDimension(\n    time_callback_index,  # callback to compute transit times\n    1000,                 # allow waiting time (slack) up to 1000 units\n    max_due + 1000,       # maximum cumulative time per vehicle\n    False,                # don't force start cumul to zero\n    'Time'                # name of the dimension\n)\ntime_dimension = routing.GetDimensionOrDie('Time')  # Retrieve the time dimension\n\n# Set the allowed time window for each customer node\ndepot_tw = data['time_windows'][data['depot']]\nfor v in range(K):\n    # Set lower and upper bounds for arrival time at node i\n    # Must use Python int because OR-Tools does not accept numpy.int64\n    time_dimension.CumulVar(routing.Start(v)).SetRange(int(depot_tw[0]), int(depot_tw[1]))\n    time_dimension.CumulVar(routing.End(v)).SetRange(int(depot_tw[0]), int(depot_tw[1]))\n\n# ------------------------\n# Vehicle fixed cost (optional)\n# ------------------------\n# This adds a small fixed cost to each vehicle used.\n# By default, the solver will try to minimize the number of vehicles because\n# each active vehicle adds to the total cost.\nrouting.SetFixedCostOfAllVehicles(vehicle_fix_cost)\n\n# ------------------------\n# Search parameters\n# ------------------------\n# This defines how OR-Tools searches for an initial solution.\nsearch_parameters = pywrapcp.DefaultRoutingSearchParameters()\n# First solution strategy: choose the cheapest arc to build a route\n# This is a heuristic to quickly find a feasible solution\nsearch_parameters.first_solution_strategy = (\n    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n)\n</pre> # ------------------------ # Routing Model # ------------------------ # OR-Tools uses a RoutingIndexManager and RoutingModel to handle VRP problems. # RoutingIndexManager maps the problem nodes to internal solver indices, which allows # multiple vehicles, depots, and flexible routing. manager = pywrapcp.RoutingIndexManager(     len(data['distance_matrix_scaled']),  # Number of nodes (customers + depot)     data['num_vehicles'],          # Number of vehicles available     data['depot']                  # Index of the depot node ) routing = pywrapcp.RoutingModel(manager)  # Core routing model object  # ------------------------ # Distance callback # ------------------------ # OR-Tools requires a callback to compute the \"cost\" of traveling from node i to j. # Here, we define a function that returns the distance between two nodes.  # Scale distances up so that OR-Tools can see non-zero costs  def distance_callback(from_index, to_index):     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     return int(data['distance_matrix_scaled'][from_node][to_node] + 1)   # Register the callback with the routing model transit_callback_index = routing.RegisterTransitCallback(distance_callback)  # Set the arc cost evaluator for all vehicles # This tells OR-Tools to minimize total distance when choosing routes routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)  # ------------------------ # Capacity constraint # ------------------------ # Each vehicle has a maximum capacity. We register a callback to return the demand # at each node. def demand_callback(from_index):     from_node = manager.IndexToNode(from_index)     return data['demands'][from_node]  # Customer demand  demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)  # Add capacity dimension to the routing model # This ensures that the sum of demands along a vehicle's route does not exceed capacity routing.AddDimensionWithVehicleCapacity(     demand_callback_index,       # Callback for demands     0,                           # Null slack (no extra capacity allowed)     data['vehicle_capacities'],  # Vehicle capacities list     True,                        # Start cumul (load) at zero for each vehicle     'Capacity'                   # Name of the dimension )  # ------------------------ # Time window constraint # ------------------------ # Each customer has a time window (ready_time to due_time) # First, define a transit callback for time between nodes def time_callback(from_index, to_index):     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     # Here, travel time = distance (assuming 1 unit distance = 1 unit time)     return int(data['distance_matrix'][from_node][to_node])  # Register the time callback time_callback_index = routing.RegisterTransitCallback(time_callback)  # Add a time dimension to enforce time windows # This dimension tracks the cumulative \"arrival time\" at each node max_due = int(df['DUE_TIME_1'].max())  routing.AddDimension(     time_callback_index,  # callback to compute transit times     1000,                 # allow waiting time (slack) up to 1000 units     max_due + 1000,       # maximum cumulative time per vehicle     False,                # don't force start cumul to zero     'Time'                # name of the dimension ) time_dimension = routing.GetDimensionOrDie('Time')  # Retrieve the time dimension  # Set the allowed time window for each customer node depot_tw = data['time_windows'][data['depot']] for v in range(K):     # Set lower and upper bounds for arrival time at node i     # Must use Python int because OR-Tools does not accept numpy.int64     time_dimension.CumulVar(routing.Start(v)).SetRange(int(depot_tw[0]), int(depot_tw[1]))     time_dimension.CumulVar(routing.End(v)).SetRange(int(depot_tw[0]), int(depot_tw[1]))  # ------------------------ # Vehicle fixed cost (optional) # ------------------------ # This adds a small fixed cost to each vehicle used. # By default, the solver will try to minimize the number of vehicles because # each active vehicle adds to the total cost. routing.SetFixedCostOfAllVehicles(vehicle_fix_cost)  # ------------------------ # Search parameters # ------------------------ # This defines how OR-Tools searches for an initial solution. search_parameters = pywrapcp.DefaultRoutingSearchParameters() # First solution strategy: choose the cheapest arc to build a route # This is a heuristic to quickly find a feasible solution search_parameters.first_solution_strategy = (     routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC )  In\u00a0[\u00a0]: Copied! <pre># ------------------------\n# Solve\n# ------------------------\nsolution = routing.SolveWithParameters(search_parameters)\n</pre> # ------------------------ # Solve # ------------------------ solution = routing.SolveWithParameters(search_parameters)  In\u00a0[\u00a0]: Copied! <pre>if solution:\n    total_distance = 0\n    vehicles_used = 0\n    fixed_cost = vehicle_fix_cost  # whatever you set\n\n    # --- Safe dimension retrieval across OR-Tools versions ---\n    try:\n        time_dimension = routing.GetDimensionOrDie('Time')\n    except Exception:\n        time_dimension = None\n\n    try:\n        cap_dimension = routing.GetDimensionOrDie('Capacity')\n    except Exception:\n        cap_dimension = None\n\n    for v in range(data['num_vehicles']):\n        start_index = routing.Start(v)\n\n        # Vehicle unused if it goes straight Start -&gt; End\n        if solution.Value(routing.NextVar(start_index)) == routing.End(v):\n            continue\n\n        vehicles_used += 1\n        route_nodes = []\n        route_distance = 0\n\n        index = start_index\n        while not routing.IsEnd(index):\n            node = manager.IndexToNode(index)\n            route_nodes.append(node)\n\n            next_index = solution.Value(routing.NextVar(index))\n            next_node = manager.IndexToNode(next_index)\n\n            # Robust distance calc from your matrix (scaled or unscaled)\n            route_distance += int(data['distance_matrix_scaled'][node][next_node])\n\n            index = next_index\n\n        # include end node for display (usually depot)\n        route_nodes.append(manager.IndexToNode(index))\n\n        total_distance += route_distance\n\n        # Optional reporting\n        time_info = \"\"\n        if time_dimension is not None:\n            start_time = solution.Value(time_dimension.CumulVar(routing.Start(v)))\n            end_time = solution.Value(time_dimension.CumulVar(routing.End(v)))\n            time_info = f\", time: {start_time}-&gt;{end_time}\"\n\n        cap_info = \"\"\n        if cap_dimension is not None:\n            end_load = solution.Value(cap_dimension.CumulVar(routing.End(v)))\n            cap_info = f\", load: {end_load}\"\n\n        print(f\"Vehicle {v} route: {route_nodes}, distance: {route_distance}{time_info}{cap_info}\")\n\n    total_cost = total_distance + vehicles_used * fixed_cost\n    print(\"Total distance for all vehicles:\", total_distance)\n    print(\"Number of vehicles used:\", vehicles_used)\n    print(\"Total cost (distance + fixed cost):\", total_cost)\n\nelse:\n    print(\"No solution found!\")\n</pre> if solution:     total_distance = 0     vehicles_used = 0     fixed_cost = vehicle_fix_cost  # whatever you set      # --- Safe dimension retrieval across OR-Tools versions ---     try:         time_dimension = routing.GetDimensionOrDie('Time')     except Exception:         time_dimension = None      try:         cap_dimension = routing.GetDimensionOrDie('Capacity')     except Exception:         cap_dimension = None      for v in range(data['num_vehicles']):         start_index = routing.Start(v)          # Vehicle unused if it goes straight Start -&gt; End         if solution.Value(routing.NextVar(start_index)) == routing.End(v):             continue          vehicles_used += 1         route_nodes = []         route_distance = 0          index = start_index         while not routing.IsEnd(index):             node = manager.IndexToNode(index)             route_nodes.append(node)              next_index = solution.Value(routing.NextVar(index))             next_node = manager.IndexToNode(next_index)              # Robust distance calc from your matrix (scaled or unscaled)             route_distance += int(data['distance_matrix_scaled'][node][next_node])              index = next_index          # include end node for display (usually depot)         route_nodes.append(manager.IndexToNode(index))          total_distance += route_distance          # Optional reporting         time_info = \"\"         if time_dimension is not None:             start_time = solution.Value(time_dimension.CumulVar(routing.Start(v)))             end_time = solution.Value(time_dimension.CumulVar(routing.End(v)))             time_info = f\", time: {start_time}-&gt;{end_time}\"          cap_info = \"\"         if cap_dimension is not None:             end_load = solution.Value(cap_dimension.CumulVar(routing.End(v)))             cap_info = f\", load: {end_load}\"          print(f\"Vehicle {v} route: {route_nodes}, distance: {route_distance}{time_info}{cap_info}\")      total_cost = total_distance + vehicles_used * fixed_cost     print(\"Total distance for all vehicles:\", total_distance)     print(\"Number of vehicles used:\", vehicles_used)     print(\"Total cost (distance + fixed cost):\", total_cost)  else:     print(\"No solution found!\")  <pre>Vehicle 19 route: [0, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0], distance: 2031, time: 0-&gt;0, load: 0\nTotal distance for all vehicles: 2031\nNumber of vehicles used: 1\nTotal cost (distance + fixed cost): 2032\n</pre> In\u00a0[\u00a0]: Copied! <pre># ------------------------\n# Extract solution\n# ------------------------\ndef get_routes(manager, routing, solution):\n    routes = []\n    for k in range(data['num_vehicles']):\n        index = routing.Start(k)\n        route = []\n        if routing.IsEnd(solution.Value(routing.NextVar(index))):\n            continue  # vehicle not used\n        while not routing.IsEnd(index):\n            route.append(manager.IndexToNode(index))\n            index = solution.Value(routing.NextVar(index))\n        route.append(manager.IndexToNode(index))\n        routes.append(route)\n    return routes\n\nroutes = get_routes(manager, routing, solution)\nprint(\"Routes:\", routes)\n</pre> # ------------------------ # Extract solution # ------------------------ def get_routes(manager, routing, solution):     routes = []     for k in range(data['num_vehicles']):         index = routing.Start(k)         route = []         if routing.IsEnd(solution.Value(routing.NextVar(index))):             continue  # vehicle not used         while not routing.IsEnd(index):             route.append(manager.IndexToNode(index))             index = solution.Value(routing.NextVar(index))         route.append(manager.IndexToNode(index))         routes.append(route)     return routes  routes = get_routes(manager, routing, solution) print(\"Routes:\", routes)  <pre>Routes: [[0, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>capacity = 25        # vehicle capacity\nservice_time = 5.0   # service duration per customer\nvehicle_cost = 50.0  # penalty cost per used vehicle\ntime_penalty = 500.0 # penalty for each time-window violation\n\ndef eval_solution(chromosome):\n    \"\"\"\n    Compute the fitness of a solution given by a permutation of customers.\n    Returns (cost, total_distance, vehicles_used, violations).\n    \"\"\"\n    total_dist = 0.0\n    vehicles = 0\n    violations = 0\n    current_load = 0\n    current_node = 0  # start at depot\n    current_time = 0.0\n    \n    for cust in chromosome:\n        demand = df.at[cust, 'demand']\n        # Check capacity for next customer\n        if current_load + demand &gt; capacity:\n            # Return to depot and start new vehicle\n            total_dist += dist_matrix[current_node, 0]\n            vehicles += 1\n            current_node = 0\n            current_time = 0.0\n            current_load = 0\n        # Travel to next customer\n        total_dist += dist_matrix[current_node, cust]\n        current_time += dist_matrix[current_node, cust]\n        # Wait if early\n        if current_time &lt; df.at[cust, 'ready']:\n            current_time = df.at[cust, 'ready']\n        # Check for late arrival\n        if current_time &gt; df.at[cust, 'due']:\n            violations += 1\n        # Service time and update load/time/node\n        current_time += service_time\n        current_load += demand\n        current_node = cust\n    \n    # Return last vehicle to depot\n    total_dist += dist_matrix[current_node, 0]\n    vehicles += 1\n    \n    # Compute fitness with penalties\n    cost = total_dist + vehicle_cost * vehicles + time_penalty * violations\n    return cost, total_dist, vehicles, violations\n</pre> capacity = 25        # vehicle capacity service_time = 5.0   # service duration per customer vehicle_cost = 50.0  # penalty cost per used vehicle time_penalty = 500.0 # penalty for each time-window violation  def eval_solution(chromosome):     \"\"\"     Compute the fitness of a solution given by a permutation of customers.     Returns (cost, total_distance, vehicles_used, violations).     \"\"\"     total_dist = 0.0     vehicles = 0     violations = 0     current_load = 0     current_node = 0  # start at depot     current_time = 0.0          for cust in chromosome:         demand = df.at[cust, 'demand']         # Check capacity for next customer         if current_load + demand &gt; capacity:             # Return to depot and start new vehicle             total_dist += dist_matrix[current_node, 0]             vehicles += 1             current_node = 0             current_time = 0.0             current_load = 0         # Travel to next customer         total_dist += dist_matrix[current_node, cust]         current_time += dist_matrix[current_node, cust]         # Wait if early         if current_time &lt; df.at[cust, 'ready']:             current_time = df.at[cust, 'ready']         # Check for late arrival         if current_time &gt; df.at[cust, 'due']:             violations += 1         # Service time and update load/time/node         current_time += service_time         current_load += demand         current_node = cust          # Return last vehicle to depot     total_dist += dist_matrix[current_node, 0]     vehicles += 1          # Compute fitness with penalties     cost = total_dist + vehicle_cost * vehicles + time_penalty * violations     return cost, total_dist, vehicles, violations  <p>GA Operators. We implement a standard GA:</p> <ul> <li><p>Initialization: Start with a population of random permutations of customers.</p> </li> <li><p>Selection: We use tournament selection (randomly pick a few individuals and choose the best).</p> </li> <li><p>Crossover: Ordered Crossover (OX): a common GA crossover for permutations that preserves relative order.</p> </li> <li><p>Mutation: Swap Mutation: randomly swap two customer positions with some probability.</p> </li> <li><p>Evolution Loop: Over generations, we select parents, apply crossover and mutation to produce offspring, evaluate them, and form the next population (elitism may be included to keep best solutions).</p> </li> <li><p>Below is a simplified GA implementation. (Details like elitism rate or exact tournament size can be adjusted.)</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import random\n\n# GA parameters\nPOP_SIZE = 50\nN_GEN = 100\nTOURN_SIZE = 3\nCROSSOVER_PROB = 0.9\nMUTATION_PROB = 0.2\n\n# Generate initial population (list of chromosomes)\ndef init_population():\n    population = []\n    customers = list(df.index[1:])  # exclude depot 0\n    for _ in range(POP_SIZE):\n        indiv = customers.copy()\n        random.shuffle(indiv)\n        population.append(indiv)\n    return population\n\n# Tournament selection\ndef tournament_select(pop, scores, k=TOURN_SIZE):\n    selected = random.sample(list(zip(pop, scores)), k)\n    # Return the chromosome with lowest cost\n    return min(selected, key=lambda x: x[1])[0]\n\n# Ordered crossover (OX)\ndef ordered_crossover(parent1, parent2):\n    size = len(parent1)\n    a, b = sorted(random.sample(range(size), 2))\n    child = [-1]*size\n    # Copy slice from parent1\n    child[a:b] = parent1[a:b]\n    # Fill remaining from parent2 in order\n    ptr = b\n    for gene in parent2[b:]+parent2[:b]:\n        if gene not in child:\n            if ptr &gt;= size: ptr = 0\n            child[ptr] = gene\n            ptr += 1\n    return child\n\n# Swap mutation\ndef swap_mutation(chrom):\n    a, b = random.sample(range(len(chrom)), 2)\n    chrom[a], chrom[b] = chrom[b], chrom[a]\n\n# Main GA loop\npopulation = init_population()\nbest_history = []\nfor gen in range(N_GEN):\n    # Evaluate all individuals\n    scores = [eval_solution(indiv)[0] for indiv in population]\n    gen_best = min(scores)\n    best_history.append(gen_best)\n    \n    new_pop = []\n    # Elitism: carry over the best individual\n    best_idx = scores.index(gen_best)\n    new_pop.append(population[best_idx])\n    \n    # Create rest of new population\n    while len(new_pop) &lt; POP_SIZE:\n        # Selection\n        p1 = tournament_select(population, scores)\n        p2 = tournament_select(population, scores)\n        # Crossover\n        if random.random() &lt; CROSSOVER_PROB:\n            child = ordered_crossover(p1, p2)\n        else:\n            child = p1.copy()\n        # Mutation\n        if random.random() &lt; MUTATION_PROB:\n            swap_mutation(child)\n        new_pop.append(child)\n    population = new_pop\n\n# After evolution, get best solution\nfinal_scores = [eval_solution(indiv)[0] for indiv in population]\nbest_idx = final_scores.index(min(final_scores))\nbest_sol = population[best_idx]\nbest_cost, best_dist, best_veh, best_viol = eval_solution(best_sol)\nprint(f\"Best solution cost: {best_cost:.1f} (dist={best_dist:.1f}, vehicles={best_veh}, violations={best_viol})\")\n</pre> import random  # GA parameters POP_SIZE = 50 N_GEN = 100 TOURN_SIZE = 3 CROSSOVER_PROB = 0.9 MUTATION_PROB = 0.2  # Generate initial population (list of chromosomes) def init_population():     population = []     customers = list(df.index[1:])  # exclude depot 0     for _ in range(POP_SIZE):         indiv = customers.copy()         random.shuffle(indiv)         population.append(indiv)     return population  # Tournament selection def tournament_select(pop, scores, k=TOURN_SIZE):     selected = random.sample(list(zip(pop, scores)), k)     # Return the chromosome with lowest cost     return min(selected, key=lambda x: x[1])[0]  # Ordered crossover (OX) def ordered_crossover(parent1, parent2):     size = len(parent1)     a, b = sorted(random.sample(range(size), 2))     child = [-1]*size     # Copy slice from parent1     child[a:b] = parent1[a:b]     # Fill remaining from parent2 in order     ptr = b     for gene in parent2[b:]+parent2[:b]:         if gene not in child:             if ptr &gt;= size: ptr = 0             child[ptr] = gene             ptr += 1     return child  # Swap mutation def swap_mutation(chrom):     a, b = random.sample(range(len(chrom)), 2)     chrom[a], chrom[b] = chrom[b], chrom[a]  # Main GA loop population = init_population() best_history = [] for gen in range(N_GEN):     # Evaluate all individuals     scores = [eval_solution(indiv)[0] for indiv in population]     gen_best = min(scores)     best_history.append(gen_best)          new_pop = []     # Elitism: carry over the best individual     best_idx = scores.index(gen_best)     new_pop.append(population[best_idx])          # Create rest of new population     while len(new_pop) &lt; POP_SIZE:         # Selection         p1 = tournament_select(population, scores)         p2 = tournament_select(population, scores)         # Crossover         if random.random() &lt; CROSSOVER_PROB:             child = ordered_crossover(p1, p2)         else:             child = p1.copy()         # Mutation         if random.random() &lt; MUTATION_PROB:             swap_mutation(child)         new_pop.append(child)     population = new_pop  # After evolution, get best solution final_scores = [eval_solution(indiv)[0] for indiv in population] best_idx = final_scores.index(min(final_scores)) best_sol = population[best_idx] best_cost, best_dist, best_veh, best_viol = eval_solution(best_sol) print(f\"Best solution cost: {best_cost:.1f} (dist={best_dist:.1f}, vehicles={best_veh}, violations={best_viol})\")  <pre>---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas\\\\_libs\\\\hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas\\\\_libs\\\\hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'demand'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 52\n     49 best_history = []\n     50 for gen in range(N_GEN):\n     51     # Evaluate all individuals\n---&gt; 52     scores = [eval_solution(indiv)[0] for indiv in population]\n     53     gen_best = min(scores)\n     54     best_history.append(gen_best)\n\nCell In[26], line 52, in &lt;listcomp&gt;(.0)\n     49 best_history = []\n     50 for gen in range(N_GEN):\n     51     # Evaluate all individuals\n---&gt; 52     scores = [eval_solution(indiv)[0] for indiv in population]\n     53     gen_best = min(scores)\n     54     best_history.append(gen_best)\n\nCell In[25], line 19, in eval_solution(chromosome)\n     16 current_time = 0.0\n     18 for cust in chromosome:\n---&gt; 19     demand = df.at[cust, 'demand']\n     20     # Check capacity for next customer\n     21     if current_load + demand &gt; capacity:\n     22         # Return to depot and start new vehicle\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\indexing.py:2575, in _AtIndexer.__getitem__(self, key)\n   2572         raise ValueError(\"Invalid call for scalar access (getting)!\")\n   2573     return self.obj.loc[key]\n-&gt; 2575 return super().__getitem__(key)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\indexing.py:2527, in _ScalarAccessIndexer.__getitem__(self, key)\n   2524         raise ValueError(\"Invalid call for scalar access (getting)!\")\n   2526 key = self._convert_key(key)\n-&gt; 2527 return self.obj._get_value(*key, takeable=self._takeable)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\frame.py:4202, in DataFrame._get_value(self, index, col, takeable)\n   4199     series = self._ixs(col, axis=1)\n   4200     return series._values[index]\n-&gt; 4202 series = self._get_item_cache(col)\n   4203 engine = self.index._engine\n   4205 if not isinstance(self.index, MultiIndex):\n   4206     # CategoricalIndex: Trying to use the engine fastpath may give incorrect\n   4207     #  results if our categories are integers that dont match our codes\n   4208     # IntervalIndex: IntervalTree has no get_loc\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\frame.py:4626, in DataFrame._get_item_cache(self, item)\n   4621 res = cache.get(item)\n   4622 if res is None:\n   4623     # All places that call _get_item_cache have unique columns,\n   4624     #  pending resolution of GH#33047\n-&gt; 4626     loc = self.columns.get_loc(item)\n   4627     res = self._ixs(loc, axis=1)\n   4629     cache[item] = res\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'demand'</pre> <p>The GA iteratively improves solutions. The best_history list recorded the best fitness each generation (not plotted here, but can be visualized). Finally, we print the best-found solution\u2019s cost, total distance, number of vehicles, and any time-window violations.</p>"},{"location":"convex/tutorials/3_ga/#iii-vehicle-routing-with-time-windows-a-metaheuristic-case-study","title":"III - Vehicle Routing with Time Windows: A Metaheuristic Case Study.\u00b6","text":""},{"location":"convex/tutorials/3_ga/#problem-definition","title":"Problem Definition\u00b6","text":"<p>We consider a real-world logistics challenge: the Vehicle Routing Problem with Time Windows (VRPTW). In VRPTW, a fleet of vehicles must deliver goods from a depot to a set of geographically dispersed customers, each with a specified delivery time window. In addition to the classic VRP constraints (e.g. vehicle capacity), each customer must be visited within its time window.</p> <ul> <li>Travelling Salesman Problem (TSP): Given a single vehicle, find the minimum-cost tour that visits each customer exactly once and returns to the depot.</li> <li>Vehicle Routing Problem (VRP): A generalization of the TSP in which multiple vehicles are available; customers must be partitioned among vehicles, and each vehicle performs a route starting and ending at the depot, subject to constraints such as vehicle capacity.</li> <li>Vehicle Routing Problem with Time Windows (VRPTW): An extension of the VRP where each customer must be serviced within a specified time interval, and vehicle routes must respect both capacity constraints and time-window feasibility.</li> </ul>"},{"location":"convex/tutorials/3_ga/#formulation","title":"Formulation\u00b6","text":"<p>We know:</p> <ul> <li>The demand (quantity required) of each customer</li> <li>The travel cost (or distance/time) between every pair of locations (customers and depot)</li> <li>The number of available vehicles</li> <li>The capacity of each vehicle</li> <li>A time window for each customer $i$, denoted by $[a_i, b_i]$</li> </ul> <p>We must determine which customers are assigned to each vehicle and the order in which they are served, so as to minimize the total routing cost while satisfying all capacity and time-window constraints.</p>"},{"location":"convex/tutorials/3_ga/#objective-function","title":"Objective Function\u00b6","text":"<ul> <li>Minimize total travel cost: $\\min \\sum_{i,j} c_{ij} \\sum_k x_{ijk}$</li> </ul>"},{"location":"convex/tutorials/3_ga/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$x_{ijk} = 1$ if vehicle $k$ travels directly from $i$ to $j$, $0$ otherwise</li> <li>$T_{ik}$ = service start time of vehicle $k$ at customer $i$</li> </ul>"},{"location":"convex/tutorials/3_ga/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$c_{ij}$ = travel cost from $i$ to $j$</li> <li>$t_{ij}$ = travel time from $i$ to $j$</li> <li>$q_i$ = demand of customer $i$</li> <li>$Q_k$ = capacity of vehicle $k$</li> <li>$[a_i, b_i]$ = time window of customer $i$</li> <li>$s_i$ = service time at customer $i$ (assumed 0 in this case-study)</li> <li>$M$ = sufficiently large constant</li> </ul>"},{"location":"convex/tutorials/3_ga/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Exactly one vehicle enters each customer (except depot): $\\sum_i \\sum_k x_{ijk} = 1 \\;\\forall j \\neq \\text{depot}$</li> <li>Exactly one vehicle leaves each customer (except depot): $\\sum_j \\sum_k x_{ijk} = 1 \\;\\forall i \\neq \\text{depot}$</li> <li>Flow conservation (same vehicle enters and leaves): $\\sum_i x_{ihk} - \\sum_j x_{hjk} = 0 \\;\\forall h,k$</li> <li>Vehicle capacity: $\\sum_i q_i \\sum_j x_{ijk} \\le Q_k \\;\\forall k$</li> <li>Subtour elimination (Prevent disconnected cycles that do not include the depot): $\\sum_{i,j \\in S} x_{ijk} \\le |S| - 1 \\;\\forall S \\subseteq N,\\; S \\neq \\emptyset,\\; \\forall k$</li> <li>Service must start within the allowed time window: $a_i \\le T_{ik} \\le b_i \\;\\forall i \\neq \\text{depot},\\forall k$</li> <li>Time consistency along routes: $T_{jk} \\ge T_{ik} + s_i + t_{ij} - M(1 - x_{ijk}) \\;\\forall i,j,k$</li> </ul> <p>M is a large constant used in Big-M constraints to turn constraints on or off depending on a binary decision variable.</p>"},{"location":"convex/tutorials/3_ga/#discrete-combinatorial-nature-of-vrptw","title":"Discrete / Combinatorial Nature of VRPTW\u00b6","text":"<p>This problem is combinatorial and non-convex, it generalizes the Traveling Salesman Problem and is therefore NP-hard (i.e. solution time grows exponentially with problem size).</p> <ul> <li><p>Binary routing variables: The decision variables  $x_{ijk} \\in \\{0,1\\}$ make the Vehicle Routing Problem with Time Windows (VRPTW) a mixed-integer optimization problem. Each variable indicates whether vehicle ( k ) travels directly from customer ( i ) to customer ( j ). There is no concept of a fractional route or \u201chalf a vehicle,\u201d unlike continuous flow variables in linear programming.</p> </li> <li><p>Exponential combinations: Assigning ( n ) customers to ( m ) vehicles and determining the service sequence for each vehicle leads to a factorial / exponential growth in the number of possible solutions. Even for moderately sized instances, the number of feasible routing combinations is astronomically large.</p> </li> </ul> <p>This combinatorial explosion is the fundamental reason why exact solution methods scale poorly and why heuristic and metaheuristic approaches are commonly used in practice.</p>"},{"location":"convex/tutorials/3_ga/#data-source","title":"Data Source\u00b6","text":""},{"location":"convex/tutorials/3_ga/#scenario-1-traveling-salesman-problem","title":"Scenario 1 -  Traveling Salesman Problem\u00b6","text":"<p>In this scenario, a single vehicle starts from a depot, visits all customers exactly once, and returns to the depot. The objective is to minimize the total travel distance.</p> <ul> <li>One vehicle</li> <li>No capacity constraints</li> <li>No time window constraints</li> </ul>"},{"location":"convex/tutorials/3_ga/#option-1-exact-solution","title":"Option 1 - Exact Solution\u00b6","text":"<p>We will first approach this using this problem using an exact dynamic programming algorithm (Held\u2013Karp). The method is exact and guarantees optimality, but it is slow because it must enumerate all $2^\ud835\udc5b$ subsets of customers, leading to exponential time complexity</p>"},{"location":"convex/tutorials/3_ga/#option-2-metaheuristic-solution","title":"Option 2 \u2013 Metaheuristic Solution\u00b6","text":"<p>To overcome the computational limitations of exact methods, we employ a metaheuristic approach using Google OR-Tools. The Traveling Salesman Problem is modeled as a weighted graph, and a feasible tour is first constructed using a greedy heuristic (Path Cheapest Arc), which incrementally selects the lowest-cost admissible edges. This initial solution is then refined through local search techniques that explore neighboring solutions to reduce total travel cost. While this approach does not guarantee optimality, it efficiently produces high-quality, near-optimal solutions and scales well to large problem instances where exact optimization is impractical.</p>"},{"location":"convex/tutorials/3_ga/#scenario-2-capacitated-vehicle-routing-problem-cvrp","title":"Scenario 2 \u2013 Capacitated Vehicle Routing Problem (CVRP)\u00b6","text":"<p>In this scenario, there are K vehicles available at a central depot. Each vehicle has a maximum load capacity, and customer demands must be delivered without exceeding that capacity. The objective is to design a set of routes that minimizes total travel cost (or distance) while ensuring all customer demands are served.</p> <ul> <li>K vehicles</li> <li>Capacity constraints (total demand on each route must not exceed vehicle capacity)</li> <li>No time window constraints on customer visits</li> </ul>"},{"location":"convex/tutorials/3_ga/#scenario-3-vehicle-routing-with-time-windows","title":"Scenario 3 \u2013 Vehicle Routing with Time Windows\u00b6","text":"<p>In this scenario, a fleet of $K$ vehicles is available at a central depot. Each vehicle has a fixed load capacity, and customer demands must be delivered without exceeding vehicle capacity constraints. Each customer must also be visited within a specified time window.</p> <p>The objective is to design a set of vehicle routes that minimizes total travel cost (or distance) while ensuring that all customer demands are satisfied and all operational constraints are respected.</p> <ul> <li>$K$ vehicles starting and ending at a central depot</li> <li>Capacity constraints: total demand served on each route must not exceed vehicle capacity</li> <li>Time window constraints on customer service times</li> </ul>"},{"location":"convex/tutorials/3_ga/#genetic-algorithm-implementation","title":"Genetic Algorithm Implementation\u00b6","text":"<p>The VRPTW is NP-hard and has many local optima, making exact methods slow for large instances. GAs can explore diverse solutions via crossover and mutation, and can incorporate problem-specific constraints in the fitness evaluation (e.g. penalizing time-window violations or excessive vehicles). Prior research has shown GAs to be effective heuristics for routing problems with constraints. In this study, the GA\u2019s evolutionary search provides a flexible way to balance distance, time windows, and vehicle usage.</p> <p>We represent each solution (individual) as a permutation of customer visits. A chromosome is an ordering of the customer IDs [1,2,...,N]. We will interpret this sequence by splitting it into vehicle routes: iterate through the list, adding customers to the current vehicle\u2019s route until adding the next customer would exceed capacity, then start a new route. Each route implicitly starts and ends at the depot. This decoding enforces the capacity constraint. The time windows are enforced via penalties in the fitness.</p> <ul> <li><p>Fitness Evaluation. Given a candidate route-sequence, we simulate each vehicle\u2019s route and compute:</p> </li> <li><p>Travel distance: Sum of Euclidean distances along each route (including returns to depot).</p> </li> <li><p>Time tracking: As we traverse, we track the current time; if arrival is before the customer\u2019s ready time, we add waiting (to start service at ready), and if arrival is after the due time, we count a violation.</p> </li> <li><p>Load/vehicles: We increment the vehicle count each time a new route starts (each vehicle).</p> </li> <li><p>Penalties: We add a fixed cost per vehicle (encouraging fewer vehicles) and a large penalty for time-window violations (to discourage infeasible arrivals).</p> </li> </ul> <p>The objective is to minimize the sum of distance plus these penalties. Below is the eval_solution function implementing this.</p>"},{"location":"convex/tutorials/3_ga/#results-and-analysis","title":"Results and Analysis\u00b6","text":"<p>After running the GA, we analyze the solution quality:</p> <ul> <li><p>Convergence: We tracked the best cost per generation (best_history). Typically, the best cost decreases over time, indicating convergence (plateau when near-optimal). Visualizing this (e.g. a plot of cost vs generation) would show rapid improvement early on and flattening as it converges.</p> </li> <li><p>Solution Example: The best solution found by our GA had cost {best_cost:.1f}, with total travel distance {best_dist:.1f}. It used {best_veh} vehicles and incurred {best_viol} time-window violations. A small number of violations suggests the GA respected customer deadlines well (we heavily penalized violations). In a final implementation, we could increase penalty or do a repair step to eliminate any violations, ensuring feasibility.</p> </li> <li><p>Baselines: As a naive baseline, one could compare to a greedy heuristic (e.g. nearest-neighbor or simple saving algorithm). We did not implement that here, but typically the GA yields significantly lower cost than a naive solution, especially given the complex constraints.</p> </li> <li><p>Route Visualization (conceptual): Each route from the best solution could be plotted on the coordinates. For clarity, suppose the best solution produced 2 routes: Route 1 visits customers [5,2,9,7,6] and Route 2 visits [3,4,10,1,8] (example). These routes would start and end at the depot (node 0). Plotting them on a map (with lines) would show efficient coverage of the points and adherence to time windows (customers visited within their allowed intervals). (Due to the text format here we omit the actual plot, but it would validate that the routes are spatially sensible.)</p> </li> <li><p>Performance vs. Data Size: Our toy example is small, but the approach scales to larger datasets (100+ customers). GA runtimes grow with population size and fitness complexity. For large real instances, one can tune GA parameters, add parallelism, or hybridize with local search to improve speed and solution quality.</p> </li> </ul> <p>Overall, the GA found a feasible routing solution that appears reasonable given the randomly generated data. The number of vehicles is minimized given the capacity constraint, and most time windows are met (few or zero violations). The results demonstrate that a GA can effectively handle the non-convex VRPTW with complex constraints.</p>"}]}