{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p> <ul> <li>Concise derivations of essential results</li> <li>Geometric intuition and figures where helpful</li> <li>Connections to real-world algorithms</li> <li>Appendices that extend into more advanced or specialized topics</li> </ul> <p>Whether you\u2019re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p>"},{"location":"appendices/120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"appendices/120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"appendices/120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"appendices/120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"appendices/120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"appendices/120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"appendices/120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"appendices/120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"appendices/120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"appendices/120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"appendices/130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"appendices/140_support/","title":"Appendix C - Support Functions and Dual Geometry","text":""},{"location":"appendices/140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"appendices/140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"appendices/140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"appendices/140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"appendices/140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"appendices/140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"appendices/140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"appendices/140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"appendices/160_conjugates/","title":"Appendix D - Convex Conjugates and Fenchel Duality","text":""},{"location":"appendices/160_conjugates/#appendix-d-convex-conjugates-and-fenchel-duality","title":"Appendix D: Convex Conjugates and Fenchel Duality","text":"<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis. They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p> <ul> <li>Convex conjugates convert a function into its \u201cslope-space\u201d representation \u2014 capturing its tightest linear overestimates.  </li> <li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li> </ul> <p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>"},{"location":"appendices/160_conjugates/#d1-intuitive-picture","title":"D.1 Intuitive Picture","text":"<p>Imagine a convex function \\(f(x)\\) drawn as a bowl in space. Each point \\(y\\) defines a line (or hyperplane) of slope \\(y\\):  The convex conjugate \\(f^*(y)\\) is the smallest height \\(b\\) such that this line always stays above \\(f(x)\\). In other words:</p> <p>\\(f^*(y)\\) measures the tightest linear overestimate of \\(f\\) in direction \\(y\\).</p> <p>So \\(f^*\\) encodes how \u201csteep\u201d \\(f\\) can be in every direction \u2014 it transforms the geometry of \\(f\\) into a new convex function on slope-space.</p>"},{"location":"appendices/160_conjugates/#d2-definition-and-key-properties","title":"D.2 Definition and Key Properties","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}\\) be a proper convex function. Its convex (Fenchel) conjugate is  </p> <p>Interpretation - \\(y\\): a slope or linear functional. - The supremum seeks the largest gap between the linear function \\(\\langle y,x\\rangle\\) and the graph of \\(f\\). - \\(f^*(y)\\) is always convex, even if \\(f\\) isn\u2019t strictly convex.</p>"},{"location":"appendices/160_conjugates/#fundamental-identities","title":"Fundamental Identities","text":"<ol> <li> <p>Fenchel\u2013Young inequality        with equality iff \\(y \\in \\partial f(x)\\).</p> </li> <li> <p>Biconjugation        This tells us the conjugate transform loses no information for convex functions.</p> </li> <li> <p>Order reversal    \\(f \\le g \\;\\Rightarrow\\; f^* \\ge g^*\\).</p> </li> <li> <p>Scaling and shift</p> </li> <li>\\((f + a)^*(y) = f^*(y) - a\\),</li> <li>\\((\\alpha f)^*(y) = \\alpha f^*(y/\\alpha)\\) for \\(\\alpha&gt;0.\\)</li> </ol>"},{"location":"appendices/160_conjugates/#d3-canonical-examples","title":"D.3 Canonical Examples","text":"Function \\(f(x)\\) Conjugate \\(f^*(y)\\) Notes \\( \\tfrac{1}{2}\\|x\\|_2^2 \\) \\( \\tfrac{1}{2}\\|y\\|_2^2 \\) Self-conjugate quadratic \\( \\|x\\|_1 \\) \\( \\delta_{\\{\\|y\\|_\\infty \\le 1\\}}(y) \\) Dual norm indicator \\( \\delta_C(x) \\) \\( \\sigma_C(y)=\\sup_{x\\in C}\\langle y,x\\rangle \\) Support function of set \\(C\\) \\( e^x \\) \\( y\\log y - y,\\, y&gt;0 \\) Appears in entropy and KL-divergence <p>These examples illustrate how conjugation connects: - Norms \u2194 dual norms, - Sets \u2194 support functions, - Exponentials \u2194 entropy, - Quadratics \u2194 themselves.</p>"},{"location":"appendices/160_conjugates/#d4-geometric-interpretation","title":"D.4 Geometric Interpretation","text":"<ul> <li>Each point on \\(f\\) has a tangent hyperplane whose slope is a subgradient.  </li> <li>The collection of all such hyperplanes forms the epigraph of \\(f^*\\).  </li> <li>The transformation \\(f \\mapsto f^*\\) swaps the roles of \u201cposition\u201d and \u201cslope\u201d:   convex geometry \u2194 supporting hyperplanes.</li> </ul> <p>Visually: - \\(f\\) describes a bowl in \\((x,t)\\)-space. - \\(f^*\\) describes the envelope of tangent planes to that bowl.</p>"},{"location":"appendices/160_conjugates/#d5-from-conjugates-to-duality-fenchel-duality","title":"D.5 From Conjugates to Duality \u2014 Fenchel Duality","text":"<p>Many convex optimization problems can be written as  where \\(f,g\\) are convex and \\(A\\) is linear. Fenchel duality uses conjugates to build a dual problem in terms of \\(f^*\\) and \\(g^*\\).</p>"},{"location":"appendices/160_conjugates/#the-fenchel-dual-problem","title":"The Fenchel Dual Problem","text":"\\[ \\max_y \\; -f^*(A^\\top y) - g^*(-y). \\] <p>Interpretation - \\(y\\) is the dual variable (similar to Lagrange multipliers). - The dual objective collects the best linear lower bounds on the primal cost.</p>"},{"location":"appendices/160_conjugates/#d6-weak-and-strong-duality","title":"D.6 Weak and Strong Duality","text":"<ul> <li> <p>Weak duality: For any \\(x,y\\),      So the dual value always underestimates the primal value.</p> </li> <li> <p>Strong duality:   If \\(f,g\\) are closed convex and a mild constraint qualification holds (e.g. Slater\u2019s condition \u2014 existence of strictly feasible \\(x\\)), then    </p> </li> </ul> <p>At the optimum:  These are the Fenchel\u2013KKT conditions, directly linking primal and dual subgradients.</p>"},{"location":"appendices/160_conjugates/#d7-illustrative-examples","title":"D.7 Illustrative Examples","text":""},{"location":"appendices/160_conjugates/#a-linear-programming","title":"(a) Linear Programming","text":"<p>Primal:  </p> <p>Take \\(f(x) = c^\\top x + \\delta_{\\{x\\ge0\\}}(x)\\), \\(g(z)=\\delta_{\\{z=b\\}}(z)\\).</p> <p>Then  </p> <p>Dual:  which is the standard LP dual.</p>"},{"location":"appendices/160_conjugates/#b-quadratic-set-constraint","title":"(b) Quadratic + Set Constraint","text":"<p>Primal:  </p> <p>Then  so the dual is  Optimality gives \\(x^*=y^*\\), the projection condition in Euclidean geometry.</p>"},{"location":"appendices/160_conjugates/#d8-practical-significance","title":"D.8 Practical Significance","text":"Area How Fenchel Duality Appears Optimization theory Derives general dual problems beyond inequality constraints. Algorithm design Basis for primal\u2013dual and splitting methods (ADMM, Chambolle\u2013Pock, Mirror Descent). Geometry Dual problem finds the \u201cbest supporting hyperplane\u201d to the primal epigraph. Machine Learning Loss\u2013regularizer pairs (hinge \u2194 clipped loss, logistic \u2194 log-sum-exp) often form conjugate pairs. Proximal operators Linked via Moreau identity:  \\(\\mathrm{prox}_{f^*}(y) = y - \\mathrm{prox}_f(y)\\)."},{"location":"appendices/160_conjugates/#d9-conceptual-unification","title":"D.9 Conceptual Unification","text":"<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p> <ul> <li>From geometry: support functions, projections, subgradients (Appendices B\u2013C).  </li> <li>From analysis: inequalities like Fenchel\u2019s and Jensen\u2019s (Appendix A).  </li> <li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7\u20138).  </li> <li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9\u201310).</li> </ul> <p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>"},{"location":"appendices/160_conjugates/#d10-summary-and-takeaways","title":"D.10 Summary and Takeaways","text":"<ul> <li>The convex conjugate \\(f^*\\) expresses \\(f\\) through its linear support planes.  </li> <li>The Fenchel\u2013Young inequality connects primal variables and dual slopes.  </li> <li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li> <li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li> <li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li> </ul> <p>Further Reading</p> <ul> <li>Rockafellar, R. T. (1970). Convex Analysis. Princeton UP.  </li> <li>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization, Chs. 3 &amp; 5.  </li> <li>Bauschke, H. H., &amp; Combettes, P. L. (2017). Convex Analysis and Monotone Operator Theory.  </li> <li>Hiriart-Urruty, J.-B., &amp; Lemar\u00e9chal, C. (2001). Fundamentals of Convex Analysis.  </li> </ul>"},{"location":"appendices/170_probability/","title":"Appendix E - Convexity in Probability and Statistics","text":""},{"location":"appendices/170_probability/#appendix-e-convexity-in-probability-and-statistics","title":"Appendix E : Convexity in Probability and Statistics","text":"<p>Convex analysis is not just geometry and optimization \u2014 it is deeply woven into probability, statistics, and information theory. Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p> <p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>"},{"location":"appendices/170_probability/#e1-convexity-of-expectations","title":"E.1 Convexity of Expectations","text":"<p>Let \\(f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}\\) be convex and \\(X\\) a random vector. Then by Jensen\u2019s inequality (Appendix A):</p> \\[ f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]. \\]"},{"location":"appendices/170_probability/#consequences","title":"Consequences","text":"<ul> <li>Expectations preserve convexity:   if each \\(f(\\cdot,\\xi)\\) is convex, then \\(F(x)=\\mathbb{E}_\\xi[f(x,\\xi)]\\) is convex.</li> <li>Stochastic objectives in ML \u2014 e.g. expected loss \\(\\mathbb{E}_{(a,b)}[\\ell(a^\\top x,b)]\\) \u2014 are convex when the sample-wise loss is convex.</li> </ul> <p>Hence almost all empirical risk minimization problems are discrete approximations of convex expectations.</p>"},{"location":"appendices/170_probability/#e2-convexity-of-log-partition-and-moment-generating-functions","title":"E.2 Convexity of Log-Partition and Moment-Generating Functions","text":"<p>For a random variable \\(X\\), the moment-generating function (MGF) and cumulant-generating function (CGF) are</p> \\[ M_X(t)=\\mathbb{E}[e^{tX}], \\qquad K_X(t)=\\log M_X(t). \\] <p>Fact: \\(K_X(t)\\) is always convex in \\(t\\).</p> <p>Reason: \\(K_X''(t)=\\mathrm{Var}_t(X)\\ge0\\); variance is nonnegative.  </p>"},{"location":"appendices/170_probability/#implications","title":"Implications","text":"<ul> <li>\\(K_X(t)\\) acts as a convex \u201cpotential\u201d controlling exponential families.</li> <li>The log-partition function in statistics,      is convex in \\(\\theta\\) (strictly convex for full exponential families).</li> <li>Its gradient gives the mean parameter: \\(\\nabla A(\\theta)=\\mathbb{E}_\\theta[T(X)]\\).</li> </ul> <p>Thus convexity of \\(A\\) guarantees a one-to-one mapping between natural and mean parameters \u2014 a foundation of exponential-family inference.</p>"},{"location":"appendices/170_probability/#e3-exponential-families-and-dual-convexity","title":"E.3 Exponential Families and Dual Convexity","text":"<p>An exponential-family density has the form  </p> <p>Properties:</p> <ol> <li>\\(A(\\theta)\\) is convex, smooth, and serves as a potential function.</li> <li>Its convex conjugate \\(A^*(\\mu)\\) defines the entropy of the family:        where \\(H\\) is the Shannon entropy of the distribution with mean \\(\\mu\\).</li> </ol> <p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>"},{"location":"appendices/170_probability/#e4-convex-divergences-and-information-measures","title":"E.4 Convex Divergences and Information Measures","text":""},{"location":"appendices/170_probability/#a-kullbackleibler-kl-divergence","title":"(a) Kullback\u2013Leibler (KL) Divergence","text":"<p>For densities \\(p,q\\),  </p> <ul> <li>\\(D_{\\mathrm{KL}}\\) is jointly convex in \\((p,q)\\).  </li> <li>Proof: the function \\((u,v)\\mapsto u\\log(u/v)\\) is convex on \\(\\mathbb{R}_+^2\\).  </li> <li>Consequently, mixtures of distributions cannot increase KL divergence \u2014 a key fact in variational inference and EM.</li> </ul>"},{"location":"appendices/170_probability/#b-bregman-divergences","title":"(b) Bregman Divergences","text":"<p>Given a differentiable convex \\(\\phi\\), define  KL divergence is a Bregman divergence for \\(\\phi(p)=\\sum_i p_i\\log p_i\\). Thus information-theoretic distances are geometric shadows of convex functions.</p>"},{"location":"appendices/170_probability/#c-f-divergences","title":"(c) f-Divergences","text":"<p>A general convex generator \\(f\\) with \\(f(1)=0\\) yields  Convexity of \\(f\\) \u21d2 convexity of \\(D_f\\). Common choices recover KL, \u03c7\u00b2, Hellinger, and Jensen\u2013Shannon divergences.</p>"},{"location":"appendices/170_probability/#e5-convex-loss-functions-in-statistics-and-machine-learning","title":"E.5 Convex Loss Functions in Statistics and Machine Learning","text":"<p>Convexity ensures estimators are globally optimal and algorithms converge.</p> Setting Loss / Negative Log-Likelihood Convexity Gaussian noise \\(\\tfrac12\\|Ax-b\\|_2^2\\) quadratic, strongly convex Laplace noise \\(\\|Ax-b\\|_1\\) convex, nonsmooth Logistic regression \\(\\log(1+e^{-y a^\\top x})\\) convex, smooth Poisson regression \\(e^{a^\\top x}-y a^\\top x\\) convex, exponential Huber loss piecewise quadratic/linear convex, robust <p>Convexity of the negative log-likelihood follows from convexity of the log-partition function \\(A(\\theta)\\) in exponential families.</p>"},{"location":"appendices/170_probability/#e6-convexity-and-bayesian-inference","title":"E.6 Convexity and Bayesian Inference","text":"<p>In Bayesian inference, convexity appears in:</p> <ul> <li> <p>Log-concave posteriors:   If the likelihood and prior are log-concave, the posterior \\(p(x|y)\\propto \\exp(-f(x))\\) is also log-concave \u21d2 \\(\\log p(x|y)\\) concave, \\(f(x)\\) convex.</p> </li> <li> <p>MAP estimation:   Maximizing \\(\\log p(x|y)\\) \u2261 minimizing a convex function when \\(p(x|y)\\) is log-concave \u21d2 global optimum guaranteed.</p> </li> <li> <p>Variational inference:   The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).   Optimizing it is equivalent to minimizing a convex divergence.</p> </li> </ul> <p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>"},{"location":"appendices/170_probability/#e7-statistical-risk-and-convex-surrogates","title":"E.7 Statistical Risk and Convex Surrogates","text":"<p>Convex surrogate losses replace nonconvex 0\u20131 loss with convex approximations:</p> <ul> <li>Hinge loss (\\(\\max(0,1-y a^\\top x)\\)) \u2192 support-vector machines.  </li> <li>Logistic loss \u2192 probabilistic classification (cross-entropy).  </li> <li>Exponential loss \u2192 AdaBoost.</li> </ul> <p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p>"},{"location":"appendices/180_subgradient_methods/","title":"Appendix F - Subgradient Method and Variants","text":""},{"location":"appendices/180_subgradient_methods/#appendix-f-subgradient-method-derivation-geometry-and-convergence","title":"Appendix F: Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>This appendix presents the subgradient method\u2014the fundamental algorithm for minimizing nonsmooth convex functions. It generalizes gradient descent to functions such as the \\(\\ell_1\\) norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>"},{"location":"appendices/180_subgradient_methods/#f1-problem-setup","title":"F.1 Problem Setup","text":"<p>We consider</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but possibly nondifferentiable and \\(\\mathcal{X}\\) is a convex feasible set.</p>"},{"location":"appendices/180_subgradient_methods/#f2-subgradients-and-geometry","title":"F.2 Subgradients and Geometry","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) satisfies</p> \\[ f(y) \\ge f(x_t) + \\langle g_t,\\, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>If \\(f\\) is differentiable, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).  </li> <li>At a nonsmooth point (e.g. \\(|x|\\) at \\(x=0\\)), \\(\\partial f(x_t)\\) is a set of supporting slopes.  </li> <li>Each subgradient defines a supporting hyperplane below the graph of \\(f\\).</li> </ul> <p>Hence a subgradient gives a descent direction even when \\(f\\) lacks a unique gradient.</p>"},{"location":"appendices/180_subgradient_methods/#f3-update-rule-and-projection-view","title":"F.3 Update Rule and Projection View","text":"<p>The projected subgradient step is</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}}\\!\\big(x_t - \\eta_t g_t\\big), \\] <p>where - \\(g_t \\in \\partial f(x_t)\\), - \\(\\eta_t&gt;0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) projects onto \\(\\mathcal{X}\\).</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\), projection disappears:  </p> <p>Geometric view: move in a subgradient direction, then project back to feasibility. The method \u201cslides\u201d along the edges of \\(f\\)\u2019s epigraph.</p>"},{"location":"appendices/180_subgradient_methods/#f4-distance-analysis","title":"F.4 Distance Analysis","text":"<p>Let \\(x^\\star\\) be an optimal solution. Expanding the squared distance:</p> \\[ \\|x_{t+1}-x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t\\langle g_t, x_t - x^\\star\\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity,  </p> <p>Substitute to get</p> \\[ \\|x_{t+1}-x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t\\big(f(x_t)-f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"appendices/180_subgradient_methods/#f5-bounding-suboptimality","title":"F.5 Bounding Suboptimality","text":"<p>Rearranging:</p> \\[ f(x_t)-f(x^\\star) \\le \\frac{\\|x_t-x^\\star\\|^2 - \\|x_{t+1}-x^\\star\\|^2}{2\\eta_t} + \\frac{\\eta_t}{2}\\|g_t\\|^2. \\] <p>This shows a trade-off:</p> <ul> <li>Large \\(\\eta_t\\) \u2192 faster steps but higher error term.  </li> <li>Small \\(\\eta_t\\) \u2192 more precise but slower progress.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f6-convergence-rate","title":"F.6 Convergence Rate","text":"<p>Assume \\(\\|g_t\\| \\le G\\). Summing over \\(t=0,\\dots,T-1\\):</p> \\[ \\sum_{t=0}^{T-1}\\!\\big(f(x_t)-f(x^\\star)\\big) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Define \\(\\bar{x}_T = \\tfrac{1}{T}\\sum_{t=0}^{T-1} x_t\\). By convexity of \\(f\\),</p> \\[ f(\\bar{x}_T)-f(x^\\star) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing \\(\\eta_t = \\tfrac{R}{G\\sqrt{T}}\\) with \\(R=\\|x_0-x^\\star\\|\\) yields</p> <p>  i.e. a sublinear rate \\(O(1/\\sqrt{T})\\).</p>"},{"location":"appendices/180_subgradient_methods/#f7-interpretation-and-practice","title":"F.7 Interpretation and Practice","text":"<ul> <li>Works for any convex function, smooth or not.  </li> <li>Converges slower than smooth-gradient methods (\\(O(1/T)\\) or linear), but applies more generally.  </li> <li>Step size schedule is crucial: \\(\\eta_t \\!\\downarrow 0\\) for convergence, or fixed \\(\\eta\\) for steady error.  </li> <li>Averaging \\(\\bar{x}_T\\) improves stability.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#typical-ml-uses","title":"Typical ML Uses","text":"Model Objective Nonsmooth Term LASSO \\(\\tfrac12\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1\\) \\(\\ell_1\\) penalty SVM \\(\\tfrac12\\|w\\|^2 + C\\sum_i \\max(0,1-y_i w^\\top x_i)\\) hinge loss Robust regression $\\sum_i a_i^\\top x - b_i Neural nets \\(\\|w\\|_1\\) or ReLU activations piecewise linear"},{"location":"appendices/180_subgradient_methods/#f8-beyond-basic-subgradients","title":"F.8 Beyond Basic Subgradients","text":"<p>Many advanced methods refine or accelerate the basic idea:</p> <ul> <li>Stochastic subgradients: sample-based updates for large-scale ML.  </li> <li>Mirror descent: adapt geometry via Bregman divergences.  </li> <li>Proximal methods: replace step with proximal operator (see Appendix B).  </li> <li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f9-summary","title":"F.9 Summary","text":"<ul> <li>Subgradients generalize gradients to nondifferentiable convex functions.  </li> <li>The projected subgradient method provides a universal, robust minimization algorithm.  </li> <li>Achieves \\(O(1/\\sqrt{T})\\) convergence under bounded subgradients.  </li> <li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9\u201310.</li> </ul>"},{"location":"appendices/190_proximal/","title":"Appendix G - Proximal Operators","text":""},{"location":"appendices/190_proximal/#appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization","title":"Appendix G | Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>Many convex optimization problems involve constraints or nonsmooth penalties. This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>"},{"location":"appendices/190_proximal/#g1-problem-setup","title":"G.1 Problem Setup","text":"<p>We wish to minimize a convex, differentiable function \\( f(x) \\) subject to a convex feasible set \\( \\mathcal{X} \\subseteq \\mathbb{R}^n \\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>A plain gradient step,</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>may leave \\( x_{t+1} \\notin \\mathcal{X} \\). We fix this by projecting the iterate back into the feasible region.</p>"},{"location":"appendices/190_proximal/#g2-projection-operator","title":"G.2 Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2. \\] <p>Hence, the projected gradient descent update is</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\]"},{"location":"appendices/190_proximal/#geometric-insight","title":"Geometric Insight","text":"<ul> <li>Take a descent step possibly outside the feasible set.  </li> <li>Project back to the closest feasible point.  </li> <li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li> </ul> <p>Example \u2014 Euclidean ball: If \\( \\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\} \\), then</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)}. \\] <ul> <li>Inside the ball \u2192 unchanged.  </li> <li>Outside \u2192 scaled back to the boundary.</li> </ul>"},{"location":"appendices/190_proximal/#g3-from-projections-to-proximal-operators","title":"G.3 From Projections to Proximal Operators","text":"<p>Projections handle explicit constraints, but many problems use implicit penalties \u2014 e.g. sparsity (\\(\\|x\\|_1\\)), total variation, or nonnegativity penalties.</p> <p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>"},{"location":"appendices/190_proximal/#definition","title":"Definition","text":"<p>For a convex (possibly nondifferentiable) function \\( g(x) \\),</p> <p>  where \\( \\lambda &gt; 0 \\) balances regularization vs. proximity.</p>"},{"location":"appendices/190_proximal/#interpretation","title":"Interpretation","text":"<ul> <li>The quadratic term \\( \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\) keeps \\(x\\) close to \\(y\\).  </li> <li>The function \\( g(x) \\) encourages structure (sparsity, smoothness, feasibility).  </li> <li>Small \\(\\lambda\\): conservative correction; large \\(\\lambda\\): stronger regularization.</li> </ul> <p>The proximal step acts as a soft correction after a gradient step.</p>"},{"location":"appendices/190_proximal/#g4-projection-as-a-special-case","title":"G.4 Projection as a Special Case","text":"<p>Define the indicator function of a convex set \\(\\mathcal{X}\\):</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X}, \\\\[4pt] +\\infty, &amp; x \\notin \\mathcal{X}. \\end{cases} \\] <p>Substitute \\(g(x)=I_{\\mathcal{X}}(x)\\) into the proximal definition:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\Big) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y). \\] <p>\u2705 Projection is just a proximal operator for an indicator function.</p>"},{"location":"appendices/190_proximal/#g5-proximal-gradient-method","title":"G.5 Proximal Gradient Method","text":"<p>When minimizing a composite convex objective  where \\(f\\) is smooth and \\(g\\) convex (possibly nonsmooth), the proximal gradient method updates:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\] <ul> <li>The gradient step reduces the smooth part \\(f(x)\\).  </li> <li>The proximal step enforces structure via \\(g(x)\\). This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li> </ul>"},{"location":"appendices/190_proximal/#g6-example-proximal-operator-of-the-ell_1-norm","title":"G.6 Example: Proximal Operator of the \\(\\ell_1\\)-Norm","text":"<p>We seek</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda\\|x\\|_1 + \\tfrac{1}{2}\\|x - y\\|^2 \\right). \\]"},{"location":"appendices/190_proximal/#step-1-coordinate-separation","title":"Step 1. Coordinate Separation","text":"<p>The problem is separable across coordinates:  so each coordinate solves  </p>"},{"location":"appendices/190_proximal/#step-2-subgradient-optimality","title":"Step 2. Subgradient Optimality","text":"<p>Optimality condition:  Thus,  </p>"},{"location":"appendices/190_proximal/#step-3-case-analysis","title":"Step 3. Case Analysis","text":"Case Condition Solution \\(x^\\star&gt;0\\) \\(y&gt;\\lambda\\) \\(x^\\star = y - \\lambda\\) \\(x^\\star&lt;0\\) \\(y&lt;-\\lambda\\) \\(x^\\star = y + \\lambda\\) \\(x^\\star=0\\) ( y"},{"location":"appendices/190_proximal/#step-4-compact-form","title":"Step 4. Compact Form","text":"\\[ \\boxed{ \\text{prox}_{\\lambda|\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda,\\, 0) } \\] <p>This is the soft-thresholding operator.</p>"},{"location":"appendices/190_proximal/#step-5-vector-case","title":"Step 5. Vector Case","text":"<p>For \\(y \\in \\mathbb{R}^n\\),</p> \\[ \\big(\\text{prox}_{\\lambda\\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i)\\cdot\\max(|y_i| - \\lambda, 0). \\] <p>Each coordinate is independently shrunk toward zero \u2014 producing sparse solutions.</p>"},{"location":"appendices/190_proximal/#step-6-interpretation","title":"Step 6. Interpretation","text":"<ul> <li>Coordinates with \\(|y_i| \\le \\lambda\\) \u2192 set to zero (promotes sparsity).  </li> <li>Coordinates with \\(|y_i| &gt; \\lambda\\) \u2192 shrink by \\(\\lambda\\).  </li> <li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li> </ul>"},{"location":"appendices/190_proximal/#g7-geometry-and-connection-to-algorithms","title":"G.7 Geometry and Connection to Algorithms","text":"<ul> <li>Projection = nearest feasible point \u2192 handles hard constraints.  </li> <li>Proximal operator = nearest structured point \u2192 handles soft regularization.  </li> <li>Proximal gradient = combines both, yielding algorithms like:</li> <li>ISTA / FISTA (sparse recovery, LASSO),</li> <li>Projected gradient (feasibility),</li> <li>ADMM (splitting into subproblems).</li> </ul> <p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>"},{"location":"appendices/190_proximal/#g8-summary","title":"G.8 Summary","text":"<ul> <li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li> <li>Projection is a special case of the proximal operator for an indicator function.  </li> <li>Proximal mappings handle nonsmooth regularizers (e.g., \\(\\ell_1\\)-norm).  </li> <li>The proximal gradient method unifies constrained and regularized optimization.  </li> <li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li> </ul>"},{"location":"appendices/200_mirror/","title":"Appendix H - Mirror Descent and Bregman Geometry","text":""},{"location":"appendices/200_mirror/#appendix-h-mirror-descent-and-bregman-geometry","title":"Appendix H: Mirror Descent and Bregman Geometry","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry. In many structured domains\u2014such as probability simplices or sparse models\u2014Euclidean updates can destroy problem structure or cause instability.  </p> <p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence. It performs gradient-like updates in a dual space, respecting the intrinsic geometry of the domain.</p>"},{"location":"appendices/200_mirror/#h1-motivation-and-limitations-of-euclidean-gd","title":"H.1 Motivation and Limitations of Euclidean GD","text":"<p>Standard GD update:  assumes Euclidean distance  </p> <p>This works well in \\(\\mathbb{R}^n\\) without structure, but fails to respect constraints or sparsity.</p> <p>In practice:</p> <ul> <li>Many parameters are nonnegative or normalized (probabilities, weights).  </li> <li>Euclidean steps can violate constraints or zero out coordinates.  </li> <li>The \u201cflat\u201d \\(\\ell_2\\) geometry treats all directions equally.</li> </ul> <p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the metric via a mirror map.</p>"},{"location":"appendices/200_mirror/#h2-geometry-in-optimization","title":"H.2 Geometry in Optimization","text":"<p>The \u201csteepest descent\u201d direction depends on the notion of distance. GD implicitly minimizes a linearized loss plus a Euclidean proximity term.</p> Scenario Natural Constraint Appropriate Geometry Probability vectors \\(x_i\\ge0, \\sum_i x_i=1\\) KL / entropy geometry Sparse models \\(\\|x\\|_1\\)-structured \\(\\ell_1\\) geometry Online learning multiplicative updates log-space geometry <p>Using Euclidean projections in these domains can cause:</p> <ul> <li>abrupt projection onto boundaries,</li> <li>loss of positivity or sparsity,</li> <li>geometric inconsistency.</li> </ul>"},{"location":"appendices/200_mirror/#h3-mirror-descent-framework","title":"H.3 Mirror Descent Framework","text":"<p>Let \\(\\psi(x)\\) be a mirror map \u2014 a strictly convex, differentiable potential encoding the geometry.</p> <p>Define the dual coordinate:  and its inverse mapping through the convex conjugate \\(\\psi^*\\):  </p>"},{"location":"appendices/200_mirror/#bregman-divergence","title":"Bregman Divergence","text":"<p>The geometry is quantified by the Bregman divergence:  </p> <ul> <li>Measures how nonlinear \\(\\psi\\) is between \\(x\\) and \\(y\\).  </li> <li>When \\(\\psi(x)=\\tfrac12\\|x\\|_2^2\\), \\(D_\\psi\\) becomes \\(\\tfrac12\\|x-y\\|_2^2\\).  </li> <li>When \\(\\psi(x)=\\sum_i x_i\\log x_i\\), \\(D_\\psi\\) becomes KL divergence.</li> </ul>"},{"location":"appendices/200_mirror/#h4-mirror-descent-update-rule","title":"H.4 Mirror Descent Update Rule","text":"<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:  </p> <p>Equivalent dual-space form:  </p> <p>\u2705 MD is gradient descent in dual coordinates, where distances are measured by \\(D_\\psi\\) instead of \\(\\|x-y\\|_2\\).</p>"},{"location":"appendices/200_mirror/#h5-comparing-gd-projected-gd-and-md","title":"H.5 Comparing GD, Projected GD, and MD","text":"Method Update Rule Geometry Comments Gradient Descent \\(x - \\eta\\nabla f\\) Euclidean may leave feasible set Projected GD \\(\\text{Proj}(x - \\eta\\nabla f)\\) Euclidean + projection can cause discontinuous jumps Mirror Descent \\(\\arg\\min_x \\langle\\nabla f, x - x_t\\rangle + \\frac{1}{\\eta}D_\\psi(x\\|x_t)\\) Bregman smooth, structure-preserving"},{"location":"appendices/200_mirror/#h6-simplex-example-kl-geometry","title":"H.6 Simplex Example (KL Geometry)","text":"<p>Let \\(x\\in\\Delta^2=\\{x\\ge0, x_1+x_2=1\\}\\), objective \\(f(x)=x_1^2+2x_2\\), \\(\\eta=0.3\\).</p>"},{"location":"appendices/200_mirror/#euclidean-gd-projection","title":"Euclidean GD + Projection","text":"<ol> <li>\\(\\nabla f=(2x_1,2)=(1,2)\\),  </li> <li>\\(y=x-\\eta\\nabla f=(0.2,-0.1)\\),  </li> <li>Project \u2192 \\(x_{new}=(1,0)\\).</li> </ol> <p>\u2192 Projection kills one coordinate \u21d2 lost smoothness.</p>"},{"location":"appendices/200_mirror/#mirror-descent-with-negative-entropy","title":"Mirror Descent with Negative Entropy","text":"<p>Mirror map \\(\\psi(x)=\\sum_i x_i\\log x_i\\). Update:  Gives \\(x\\approx(0.57,0.43)\\) \u2014 smooth, positive, stays in simplex.</p> <p>MD follows the manifold of the simplex naturally\u2014no harsh projection.</p>"},{"location":"appendices/200_mirror/#h7-choosing-the-mirror-map","title":"H.7 Choosing the Mirror Map","text":"Mirror Map \\(\\psi(x)\\) Bregman Divergence \\(D_\\psi\\) Typical Domain / Application \\(\\tfrac12\\|x\\|_2^2\\) Euclidean distance unconstrained \\(\\mathbb{R}^n\\) \\(\\sum_i x_i\\log x_i\\) KL divergence simplex, probabilities \\(\\|x\\|_1\\) or variants \\(\\ell_1\\) geometry sparse models log-barrier \\(\\sum_i -\\log x_i\\) barrier divergence positive orthant <p>Mirror maps act as design choices defining the optimization geometry.</p>"},{"location":"appendices/200_mirror/#h8-practical-remarks","title":"H.8 Practical Remarks","text":"<p>When to prefer Mirror Descent:</p> <ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates desired</li> <li>Avoiding discontinuous projections</li> </ul> <p>Computational notes:</p> <ul> <li>Some \\(\\psi\\) yield closed-form updates (e.g. multiplicative weights).  </li> <li>Works with adaptive or momentum step-size schemes.  </li> <li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li> </ul>"},{"location":"appendices/200_mirror/#h9-convergence-at-a-glance","title":"H.9 Convergence at a Glance","text":"<p>For convex \\(f\\) with bounded gradients \\(\\|\\nabla f\\|\\le G\\) and strong convex mirror map \\(\\psi\\), Mirror Descent achieves the same sublinear rate as projected subgradient methods:  but with improved geometry-adapted constants that exploit curvature of \\(\\psi\\).</p>"},{"location":"appendices/300_matrixfactorization/","title":"Appendix I - Matrix Factorization","text":""},{"location":"appendices/300_matrixfactorization/#numerical-linear-algebra-for-convex-optimization","title":"Numerical Linear Algebra for Convex Optimization","text":"<p>Numerical linear algebra is the computational foundation of convex optimization. Every modern optimization algorithm \u2014 from Newton\u2019s method to interior-point or proximal algorithms \u2014 ultimately requires solving a structured linear system:  where \\(H\\) may represent a Hessian, a normal equations matrix, or a KKT (Karush\u2013Kuhn\u2013Tucker) system.</p> <p>In practice, we never compute \\(H^{-1}\\) directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>"},{"location":"appendices/300_matrixfactorization/#1-why-linear-algebra-matters-in-convex-optimization","title":"1. Why Linear Algebra Matters in Convex Optimization","text":"<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p> <ul> <li> <p>Newton\u2019s method:    </p> </li> <li> <p>Interior-point methods (KKT systems):</p> </li> </ul> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\[3pt] \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -r_d \\\\[3pt] -r_p \\end{bmatrix} \\] <ul> <li>Least-squares problems: \\(A^T A x = A^T b\\)</li> </ul> <p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>"},{"location":"appendices/300_matrixfactorization/#2-the-matrix-factorization-toolbox","title":"2. The Matrix Factorization Toolbox","text":"<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure. They enable efficient triangular solves instead of direct inversion.</p> Factorization Applies To Form Common Use Key Notes LU Any nonsingular matrix \\(A = L U\\) General linear systems Requires pivoting for stability QR Any (rectangular) matrix \\(A = Q R\\) Least-squares Orthogonal, stable Cholesky Symmetric positive definite \\(A = L L^T\\) SPD systems, normal equations Fastest for SPD \\(LDL^T\\) Symmetric indefinite \\(A = L D L^T\\) KKT systems Handles indefiniteness Eigen Symmetric/Hermitian \\(A = Q \\Lambda Q^T\\) Curvature, convexity checks Diagonalizes \\(A\\) SVD Any matrix \\(A = U \\Sigma V^T\\) Rank, conditioning, pseudoinverse Most stable, expensive <p>Each factorization corresponds to a numerically preferred strategy for certain classes of problems.</p>"},{"location":"appendices/300_matrixfactorization/#3-lu-factorization-the-general-purpose-workhorse","title":"3. LU Factorization \u2014 The General-Purpose Workhorse","text":"<p>Form:  where \\(P\\) is a permutation matrix ensuring stability.</p> <ul> <li>Used for: General linear systems, nonsymmetric matrices.</li> <li>Cost: \\(\\approx \\tfrac{2}{3}n^3\\) (dense).</li> <li>Stability: Requires partial pivoting (\\(PA=LU\\)) to avoid numerical blow-up.</li> </ul> <p>Example use case:</p> <ul> <li>Solving KKT systems in linear programming (LP simplex tableau).</li> <li>Small dense systems with no symmetry or SPD property.</li> </ul> <p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or \\(LDL^T\\).</p>"},{"location":"appendices/300_matrixfactorization/#4-qr-factorization-orthogonal-and-stable","title":"4. QR Factorization \u2014 Orthogonal and Stable","text":"<p>Form:  </p> <ul> <li>Used for: Least-squares problems      Instead of forming normal equations (\\(A^T A x = A^T b\\)), we solve:    </li> <li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li> </ul> <p>Example use cases:</p> <ul> <li>Linear regression via least squares.</li> <li>ADMM and proximal steps with overdetermined systems.</li> <li>Orthogonal projections in signal processing.</li> </ul> <p>Variants:</p> <ul> <li>Householder QR: numerically robust, used in LAPACK.</li> <li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#5-cholesky-factorization-fastest-for-spd-systems","title":"5. Cholesky Factorization \u2014 Fastest for SPD Systems","text":"<p>Form:  Applicable when \\(A\\) is symmetric positive definite (SPD) \u2014 common in convex problems.</p> <p>Why it\u2019s central: Convexity ensures \\(A \\succeq 0\\). For strictly convex problems, \\(A \\succ 0\\) and Cholesky is the most efficient and stable method.</p> <p>Cost: \\(\\tfrac{1}{3}n^3\\) operations \u2014 half of LU.</p> <p>Example use cases:</p> <ul> <li>Newton\u2019s method on unconstrained convex functions.</li> <li>Solving normal equations \\(A^T A x = A^T b\\).</li> <li>QP subproblems and ridge regression.</li> </ul> <p>Implementation detail: No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>"},{"location":"appendices/300_matrixfactorization/#6-ldlt-factorization-for-indefinite-symmetric-systems","title":"6. LDL\u1d40 Factorization \u2014 For Indefinite Symmetric Systems","text":"<p>Form:  where \\(D\\) is block diagonal (1\u00d71 or 2\u00d72 blocks), and \\(L\\) is unit lower triangular.</p> <p>Used when \\(A\\) is symmetric but not SPD (e.g., KKT systems).</p> <p>Example use cases:</p> <ul> <li> <p>Interior-point methods for QPs and SDPs:    </p> </li> <li> <p>Equality-constrained least-squares.</p> </li> <li>Sparse symmetric indefinite systems in primal-dual algorithms.</li> </ul> <p>Algorithmic note: Uses Bunch\u2013Kaufman pivoting to maintain numerical stability. In practice, LDL\u1d40 is used with sparse reordering and partial elimination.</p>"},{"location":"appendices/300_matrixfactorization/#7-block-systems-and-the-schur-complement","title":"7. Block Systems and the Schur Complement","text":"<p>Many KKT or structured systems naturally appear in block form:  </p> <p>Assuming \\(A_{11}\\) is invertible:</p> <ol> <li>Eliminate \\(x_1\\):     </li> <li>Substitute into the second block:     </li> </ol> <p>The matrix  is the Schur complement of \\(A_{11}\\) in \\(A\\).</p>"},{"location":"appendices/300_matrixfactorization/#schur-complement-in-optimization","title":"Schur Complement in Optimization","text":"<ul> <li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li> <li>Preserves symmetry and often positive definiteness.</li> <li>Foundation of block elimination and reduced Hessian methods.</li> </ul> <p>Example use cases:</p> <ul> <li>Interior-point Newton systems (eliminate \\(\\Delta x\\) to get a system in \\(\\Delta \\lambda\\)).</li> <li>Partial elimination in sequential quadratic programming (SQP).</li> <li>Covariance conditioning and Gaussian marginalization.</li> </ul> <p>Numerical caution: Never form \\(A_{11}^{-1}\\) explicitly \u2014 use triangular solves via Cholesky or LU.</p>"},{"location":"appendices/300_matrixfactorization/#8-block-elimination-algorithm","title":"8. Block Elimination Algorithm","text":"<p>Given a nonsingular \\(A_{11}\\):</p> <ol> <li>Compute \\(A_{11}^{-1}A_{12}\\) and \\(A_{11}^{-1}b_1\\) by solving triangular systems.</li> <li>Form \\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\\), \\(\\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\\).</li> <li>Solve \\(Sx_2 = \\tilde{b}\\).</li> <li>Recover \\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\\).</li> </ol> <p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p> <p>Example use case:</p> <ul> <li>Partitioned least-squares with fixed and variable parameters.</li> <li>Constrained optimization where some variables can be analytically eliminated.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#9-structured-plus-low-rank-matrices","title":"9. Structured Plus Low-Rank Matrices","text":"<p>Suppose we need to solve:  where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{n \\times n}\\) is structured or easily invertible (e.g., diagonal or sparse),</li> <li>\\(B \\in \\mathbb{R}^{n \\times p}\\), \\(C \\in \\mathbb{R}^{p \\times n}\\) are low rank.</li> </ul> <p>This situation arises when updating an existing system with a small modification.</p>"},{"location":"appendices/300_matrixfactorization/#block-reformulation","title":"Block Reformulation","text":"<p>Introduce \\(y = Cx\\), yielding:</p> <p>$$   =</p> <p> . $$</p> <p>Block elimination gives:  </p>"},{"location":"appendices/300_matrixfactorization/#matrix-inversion-lemma-woodbury-identity","title":"Matrix Inversion Lemma (Woodbury Identity)","text":"<p>If \\(A\\) and \\(A + BC\\) are nonsingular:  </p> <p>Example use cases:</p> <ul> <li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li> <li>Ridge regression / kernel methods: low-rank updates to \\((X^T X + \\lambda I)^{-1}\\).</li> <li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li> </ul> <p>Numerical note: Avoid explicit inversion; use solves with \\(A\\) and small dense matrices.</p>"},{"location":"appendices/300_matrixfactorization/#10-conditioning-stability-and-sparsity","title":"10. Conditioning, Stability, and Sparsity","text":""},{"location":"appendices/300_matrixfactorization/#conditioning","title":"Conditioning","text":"<ul> <li>Condition number: \\(\\kappa(A) = |A||A^{-1}|\\) measures sensitivity to perturbations.</li> <li>High \\(\\kappa(A)\\) \u21d2 round-off errors amplified \u21d2 ill-conditioning.</li> <li>Regularization (adding \\(\\lambda I\\)) improves numerical stability.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#stability","title":"Stability","text":"<ul> <li>Orthogonal transformations (QR, SVD) are backward stable.</li> <li>LU needs partial pivoting.</li> <li>LDL\u1d40 needs symmetric pivoting (Bunch\u2013Kaufman).</li> <li>Cholesky is stable for SPD matrices.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#sparsity-and-fill-in","title":"Sparsity and Fill-In","text":"<ul> <li>Large convex solvers exploit sparse Cholesky / LDL\u1d40.</li> <li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li> <li>Symbolic factorization determines the pattern before numeric factorization.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#11-iterative-solvers-and-preconditioning","title":"11. Iterative Solvers and Preconditioning","text":"<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>"},{"location":"appendices/300_matrixfactorization/#common-iterative-methods","title":"Common Iterative Methods","text":"Method For Description CG SPD systems Uses matrix\u2013vector products; converges in \u2264 n steps MINRES / SYMMLQ Symmetric indefinite Handles KKT and saddle-point systems GMRES / BiCGSTAB Nonsymmetric General-purpose Krylov solvers"},{"location":"appendices/300_matrixfactorization/#preconditioning","title":"Preconditioning","text":"<p>Preconditioners \\(M \\approx A^{-1}\\) improve convergence:</p> <ul> <li>Jacobi (diagonal): \\(M = \\text{diag}(A)^{-1}\\)</li> <li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li> <li>Block preconditioners: use Schur complement approximations for KKT systems</li> </ul> <p>Example use case:</p> <ul> <li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li> <li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#12-eigenvalue-and-svd-decompositions","title":"12. Eigenvalue and SVD Decompositions","text":""},{"location":"appendices/300_matrixfactorization/#eigenvalue-decomposition","title":"Eigenvalue Decomposition","text":"<p>  Reveals curvature, stability, and definiteness:</p> <ul> <li>Convexity \u21d4 \\(\\Lambda \\ge 0\\).</li> <li>Used in semidefinite programming (SDP) and spectral analysis.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>  with \\(\\Sigma = \\text{diag}(\\sigma_i) \\ge 0\\).</p> <p>Applications:</p> <ul> <li>Rank and condition number estimation (\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\)).</li> <li>Low-rank approximation (\\(A_k = U_k \\Sigma_k V_k^T\\)).</li> <li>Pseudoinverse: \\(A^+ = V \\Sigma^{-1} U^T\\).</li> <li>Convex relaxations: nuclear-norm minimization (matrix completion).</li> </ul>"},{"location":"appendices/300_matrixfactorization/#13-computational-complexity-summary","title":"13. Computational Complexity Summary","text":"Factorization Dense Cost Notes LU \\(\\frac{2}{3}n^3\\) Needs pivoting Cholesky \\(\\frac{1}{3}n^3\\) Fastest for SPD QR \\(\\approx \\frac{2}{3}n^3\\) Stable, more memory LDL\u1d40 \\(\\approx \\frac{2}{3}n^3\\) For indefinite SVD \\(\\approx \\frac{4}{3}n^3\\) Most accurate CG / MINRES Variable Depends on condition number and preconditioning <p>Sparse systems reduce cost to roughly \\(O(n^{1.5})\\)\u2013\\(O(n^2)\\) depending on fill-in.</p>"},{"location":"appendices/300_matrixfactorization/#14-example-applications-overview","title":"14. Example Applications Overview","text":"Problem Type Typical Matrix Solver / Factorization Example Unconstrained Newton step SPD Hessian Cholesky Convex quadratic, ridge regression Equality-constrained QP Symmetric indefinite KKT LDL\u1d40 Interior-point QP solver Overdetermined LS Rectangular \\(A\\) QR Linear regression, ADMM subproblem KKT block system Block-symmetric Schur complement Primal-dual method Low-rank correction \\(A + U U^T\\) Woodbury Kalman filter, online update Rank-deficient system Any SVD Matrix completion, regularization Large-scale Hessian SPD CG + preconditioner Logistic regression, large ML models"},{"location":"cheatsheets/20a_cheatsheet/","title":"Optimization Algos - Cheat Sheet","text":""},{"location":"cheatsheets/20a_cheatsheet/#comprehensive-optimization-algorithm-cheat-sheet","title":"Comprehensive Optimization Algorithm Cheat Sheet","text":"<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search. It balances theoretical precision with practical intuition\u2014from gradient-based solvers to black-box evolutionary methods.</p>"},{"location":"cheatsheets/20a_cheatsheet/#how-to-read-this-table","title":"\ud83e\udded How to Read This Table","text":"<p>Each method lists: - Problem Type \u2014 the class of objectives it applies to. - Assumptions \u2014 smoothness, convexity, or structural conditions. - Core Update Rule \u2014 canonical iteration. - Scalability \u2014 computational feasibility. - Per-Iteration Cost \u2014 approximate computational complexity. - Applications \u2014 typical ML or engineering use cases.</p>"},{"location":"cheatsheets/20a_cheatsheet/#first-order-methods","title":"\ud83d\ude80 First-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Gradient Descent (GD) Unconstrained smooth (convex/nonconvex) Differentiable; \\(L\\)-smooth \\(x_{k+1} = x_k - \\eta \\nabla f(x_k)\\) Medium \\(O(nd)\\) Logistic regression, least squares Nesterov\u2019s Accelerated GD Smooth convex (fast rate) Convex, \\(L\\)-smooth \\(y_k = x_k + \\frac{k-1}{k+2}(x_k - x_{k-1})\\); \\(x_{k+1} = y_k - \\eta \\nabla f(y_k)\\) Medium \\(O(nd)\\) Accelerated convex models (Polyak) Heavy-Ball Momentum Unconstrained smooth Differentiable, \\(\\beta \\in (0,1)\\) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k) + \\beta(x_k - x_{k-1})\\) Large \\(O(nd)\\) Deep networks, convex smooth losses Conjugate Gradient (CG) Quadratic or linear systems \\(Ax=b\\) \\(A\\) symmetric positive definite \\(p_{k+1}=r_{k+1}+\\beta_k p_k\\), \\(x_{k+1}=x_k+\\alpha_k p_k\\) Large \\(O(nd)\\) Large-scale least squares, implicit Newton steps Mirror Descent Non-Euclidean geometry Convex; mirror map \\(\\psi\\) strongly convex \\(x_{k+1} = \\nabla \\psi^*(\\nabla \\psi(x_k) - \\eta \\nabla f(x_k))\\) Medium \\(O(nd)\\) Probability simplex, online learning <p>Conjugate Gradient (CG) bridges first- and second-order methods: it achieves exact convergence in at most \\(d\\) steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>"},{"location":"cheatsheets/20a_cheatsheet/#second-order-methods","title":"\u2699\ufe0f Second-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Newton\u2019s Method Smooth convex Twice differentiable; \\(\\nabla^2 f(x)\\) PD \\(x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\\) Small\u2013Medium \\(O(d^3)\\) Logistic regression (IRLS), convex solvers BFGS / L-BFGS Smooth convex Differentiable, approximate Hessian Solve \\(B_k p_k=-\\nabla f(x_k)\\); update \\(B_k\\) via secant rule Medium \\(O(d^2)\\) GLMs, medium ML models Trust-Region Smooth convex/nonconvex Twice differentiable \\(\\min_p \\tfrac{1}{2}p^\\top \\nabla^2 f(x_k)p + \\nabla f(x_k)^\\top p\\) s.t. \\(\\|p\\|\\le\\Delta_k\\) Medium \\(O(d^2)\\) TRPO, physics-based ML"},{"location":"cheatsheets/20a_cheatsheet/#proximal-projected-splitting-methods","title":"\ud83e\uddee Proximal, Projected &amp; Splitting Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Proximal Gradient (ISTA) Composite \\(f=g+h\\) \\(g\\) smooth, \\(h\\) convex \\(x_{k+1}=\\operatorname{prox}_{\\alpha h}(x_k-\\alpha\\nabla g(x_k))\\) Medium \\(O(nd)\\) LASSO, sparse recovery FISTA Same as ISTA Convex, \\(L\\)-smooth \\(g\\) Like ISTA with momentum Medium \\(O(nd)\\) Compressed sensing Projected Gradient (PG) Convex constrained \\(f\\) smooth; easy projection \\(x_{k+1}=\\Pi_C(x_k-\\eta\\nabla f(x_k))\\) Medium \\(O(nd)\\) + projection Box/simplex constraints ADMM Separable convex + linear constraints \\(f,g\\) convex Alternating minimization + dual update Medium \\(O(nd)\\) per block Distributed ML, consensus Majorization\u2013Minimization (MM) Convex/nonconvex $g(x x_k)\\ge f(x)$ $x_{k+1}=\\arg\\min g(x x_k)$ Medium"},{"location":"cheatsheets/20a_cheatsheet/#coordinate-block-methods","title":"\ud83e\udde9 Coordinate &amp; Block Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Coordinate Descent (CD) Separable convex Convex, differentiable Update one coordinate: \\(x_{i}^{k+1}=x_i^k-\\eta\\partial_i f(x^k)\\) Large \\(O(d)\\) LASSO, SVM duals Block Coordinate Descent (BCD) Block separable Convex per block Minimize over \\(x^{(j)}\\) while fixing others Large \\(O(nd_j)\\) Matrix factorization, alternating minimization <p>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</p>"},{"location":"cheatsheets/20a_cheatsheet/#stochastic-mini-batch-methods","title":"\ud83c\udfb2 Stochastic &amp; Mini-Batch Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Stochastic Gradient Descent (SGD) Large-scale / streaming Unbiased stochastic gradients \\(x_{k+1}=x_k-\\eta_t\\nabla f_{i_k}(x_k)\\) Very Large \\(O(bd)\\) Deep learning, online learning Variance-Reduced (SVRG/SAGA/SARAH) Finite-sum convex Smooth, strongly convex \\(v_k=\\nabla f_{i_k}(x_k)-\\nabla f_{i_k}(\\tilde{x})+\\nabla f(\\tilde{x})\\) Large \\(O(bd)\\) Logistic regression, GLMs Adaptive SGD (Adam/RMSProp/Adagrad) Nonconvex stochastic Bounded variance \\(m_k=\\beta_1m_{k-1}+(1-\\beta_1)g_k\\), \\(v_k=\\beta_2v_{k-1}+(1-\\beta_2)g_k^2\\) Very Large \\(O(bd)\\) Neural networks Proximal Stochastic (Prox-SGD / Prox-SAGA) Nonsmooth stochastic \\(f=g+h\\) with prox of \\(h\\) known \\(x_{k+1}=\\operatorname{prox}_{\\eta h}(x_k-\\eta\\widehat{\\nabla g}(x_k))\\) Large \\(O(bd)\\) Sparse online learning"},{"location":"cheatsheets/20a_cheatsheet/#interior-point-augmented-methods","title":"\ud83e\uddf1 Interior-Point &amp; Augmented Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Interior-Point Convex with inequalities Slater\u2019s condition, self-concordant barrier Solve \\(\\min f_0(x)-\\tfrac{1}{t}\\sum_i\\log(-g_i(x))\\) Small\u2013Medium \\(O(d^3)\\) LP, QP, SDP Augmented Lagrangian (ALM) Constrained convex \\(f,g\\) convex; equality constraints \\(L_\\rho(x,\\lambda)=f(x)+\\lambda^T g(x)+\\tfrac{\\rho}{2}\\|g(x)\\|^2\\) Medium \\(O(nd)\\) Penalty methods, PDEs"},{"location":"cheatsheets/20a_cheatsheet/#derivative-free-black-box-optimization","title":"\ud83c\udf10 Derivative-Free &amp; Black-Box Optimization","text":"Method Problem Type Assumptions Core Idea Scalability Cost Applications Nelder\u2013Mead Simplex Low-dimensional, smooth or noisy No gradients; continuous \\(f\\) Maintain simplex of \\(d+1\\) points; reflect\u2013expand\u2013contract\u2013shrink operations Small \\(O(d^2)\\) Parameter tuning, physics models Simulated Annealing Nonconvex, global Stochastic exploration via temperature Random perturbations accepted w.p. \\(\\exp(-\\Delta f/T)\\); \\(T\\downarrow\\) Medium High (many samples) Hyperparameter tuning, design optimization Multi-start Local Search Nonconvex None; relies on restart diversity Run local solver from multiple random inits, pick best result Medium \\(k\\times\\) local solver Avoids local minima; cheap global heuristic Evolutionary Algorithms (EA) Black-box, global Population-based; fitness function only Mutate, select, recombine candidates Large \\(O(Pd)\\) per gen Global optimization, control, AutoML Genetic Algorithms (GA) Combinatorial / continuous Chromosomal encoding of solutions Apply selection, crossover, mutation; evolve over generations Medium\u2013Large \\(O(Pd)\\) Feature selection, neural architecture search Evolution Strategies (ES) Continuous, black-box Gaussian mutation around mean \\(\\theta_{k+1} = \\theta_k + \\eta \\sum_i w_i \\epsilon_i f(\\theta_k+\\sigma \\epsilon_i)\\) Large \\(O(Pd)\\) Reinforcement learning, black-box control Derivative-Free Optimization (DFO) Black-box, noisy \\(f\\) Only function values available Gradient estimated via random perturbations: \\(g\\approx\\frac{f(x+hu)-f(x)}{h}u\\) Medium \\(O(d)\\)\u2013\\(O(d^2)\\) Robotics, policy search, design Black-Box Optimization Framework General No analytical gradients; often stochastic Unified term covering EA, GA, ES, and DFO Medium\u2013Large varies Hyperparameter search, AutoML, reinforcement learning Numerical Encodings Used in GA/EA Represents variables in binary, integer, or floating-point form Choice of encoding impacts mutation/crossover behavior N/A negligible Optimization of mixed or discrete variables <p>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</p>"},{"location":"cheatsheets/20a_cheatsheet/#convergence-complexity-snapshot","title":"\ud83d\udcc8 Convergence &amp; Complexity Snapshot","text":"Method Type Convergence (Convex) Notes Subgradient \\(O(1/\\sqrt{k})\\) Nonsmooth convex Gradient Descent \\(O(1/k)\\) Smooth convex Accelerated Gradient \\(O(1/k^2)\\) Optimal first-order Newton / Quasi-Newton Quadratic / Superlinear Local only Strongly Convex \\((1-\\mu/L)^k\\) Linear rate Variance-Reduced Linear (strongly convex) Finite-sum optimization ADMM / Proximal \\(O(1/k)\\) Composite convex Interior-Point Polynomial time High-accuracy convex Derivative-Free / Heuristics No formal bound Empirical convergence only"},{"location":"cheatsheets/20a_cheatsheet/#practitioner-summary","title":"\ud83e\udde0 Practitioner Summary","text":"Situation Recommended Methods Gradients available, smooth convex Gradient Descent, Nesterov Curvature matters, moderate scale Newton, BFGS, Conjugate Gradient Nonsmooth regularizer Proximal Gradient, ADMM Simple constraints Projected Gradient Large-scale / streaming SGD, Adam, RMSProp Finite-sum convex SVRG, SAGA Online / adaptive Mirror Descent, FTRL No gradients (black-box) DFO, Nelder\u2013Mead, ES, GA Global nonconvex search Simulated Annealing, Multi-starts, Evolutionary Algorithms Distributed / separable ADMM, ALM High-precision convex programs Interior-Point, Trust-Region"},{"location":"cheatsheets/20a_cheatsheet/#notes-on-global-black-box-optimization","title":"\ud83e\udde9 Notes on Global &amp; Black-Box Optimization","text":"<ul> <li>Conjugate Gradient: memory-efficient quasi-second-order method for large convex quadratics.  </li> <li>Nelder\u2013Mead: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li> <li>Simulated Annealing: probabilistic global search inspired by thermodynamics.  </li> <li>Multi-Starts: pragmatic global exploration by repeated local optimization.  </li> <li>Evolutionary / Genetic / ES: population-based global heuristics; robust to noise and discontinuity.  </li> <li>Derivative-Free Optimization (DFO): umbrella for random, surrogate-based, or adaptive black-box methods.  </li> <li>Numerical Encoding: crucial in discrete search\u2014how real or binary variables are represented determines performance.</li> </ul> <p>Summary Insight: - Convex + differentiable \u2192 use gradient-based or Newton-type methods. - Convex + nonsmooth \u2192 use proximal, ADMM, or coordinate descent. - Large-scale or stochastic \u2192 use SGD or adaptive variants. - No gradients or nonconvex \u2192 use derivative-free or evolutionary methods. - The structure of the objective, not its size alone, determines the optimal solver family.</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1:  Introduction and Overview","text":"<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p> <ol> <li>No bad local minima: any local minimum is also the global minimum.  </li> <li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li> <li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li> </ol> <p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p>"},{"location":"convex/11_intro/#motivation-optimization-in-machine-learning","title":"Motivation: Optimization in Machine Learning","text":"<p>Many supervised learning problems can be written in a common form:</p> \\[ \\min_{x \\in \\mathcal{X}}  \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i)  + \\lambda R(x), \\] <p>where</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function that measures how well the model predicts \\(b_i\\) from \\(a_i\\),  </li> <li>\\(R(x)\\) is a regularizer that encourages certain structure (such as sparsity or small weights),  </li> <li>\\(\\mathcal{X}\\) is a set of allowed parameter values, often simple and convex.</li> </ul> <p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the \\(\\ell_1\\) norm, and the \\(\\ell_2\\) norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>"},{"location":"convex/11_intro/#why-convex-optimization-remains-central-in-ml","title":"Why Convex Optimization Remains Central in ML","text":"<p>Although many modern models are nonconvex, convex optimization continues to play a major role:</p> <ol> <li> <p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the \\(0\\text{\u2013}1\\) loss. They make optimization practical while still leading to models that generalize well.</p> </li> <li> <p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p> </li> </ol> <p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>"},{"location":"convex/11_intro/#web-book-roadmap-and-how-to-use-it","title":"Web-Book Roadmap and How to Use It","text":"<ul> <li> <p>Chapter 2: Linear Algebra Foundations. Basic vector/matrix operations and linear algebra needed for optimization.</p> </li> <li> <p>Chapter 3: Multivariable Calculus. Differentiation and derivatives of functions of many variables (gradients, Hessians).</p> </li> <li> <p>Chapter 4: Convex Sets and Geometry. Definitions and examples of convex sets, cones, affine spaces, and geometric properties.</p> </li> <li> <p>Chapter 5: Convex Functions. Convexity for functions, epigraphs, and key examples (norms, quadratic functions, log-sum-exp, etc.).</p> </li> <li> <p>Chapter 6: Nonsmooth Optimization \u2013 Subgradients. Generalized derivatives for convex functions that are not differentiable, and subgradient methods.</p> </li> <li> <p>Chapter 7: First-Order Optimality Conditions. Gradient-based optimality for smooth problems, supporting theory for necessary and sufficient conditions.</p> </li> <li> <p>Chapter 8: Optimization Principles \u2013 From Gradient Descent to KKT. Unconstrained and constrained gradient methods, culminating in the Karush\u2013Kuhn\u2013Tucker (KKT) conditions.</p> </li> <li> <p>Chapter 9: Lagrange Duality Theory. Duality principles, weak/strong duality, and interpretations of Lagrange multipliers.</p> </li> <li> <p>Chapter 10: Pareto Optimality and Multi-Objective Optimization. Trade-offs in optimizing multiple goals and Pareto efficiency.</p> </li> <li> <p>Chapter 11: Regularized Approximation. Balancing fit vs. complexity with regularization (\u2113\u2081, \u2113\u2082, elastic net, etc.).</p> </li> <li> <p>Chapter 12: Algorithms for Convex Optimization. General convex optimization solvers and algorithmic frameworks (interior-point, gradient methods, etc.).</p> </li> <li> <p>Chapter 13: Equality-Constrained Problems. Specialized methods (e.g. KKT with only equalities, reduced-space methods).</p> </li> <li> <p>Chapter 14: Inequality-Constrained Problems. Algorithms handling general inequality constraints, barrier methods.</p> </li> <li> <p>Chapter 15: Advanced Large-Scale and Structured Methods. Techniques for very large or structured problems (decomposition, coordinate descent, etc.).</p> </li> <li> <p>Chapter 16: Modeling Patterns and Algorithm Selection. Practical guidance on modeling choices and selecting the right algorithm in practice.</p> </li> <li> <p>Chapter 17: Canonical Problems in Convex Optimization. Well-known problem templates (linear, quadratic, SOCP, SDP) and how to recognize them.</p> </li> <li> <p>Chapter 18: Modern Optimizers in Machine Learning Frameworks. How convex optimization appears in ML libraries and frameworks.</p> </li> <li> <p>Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization. Overview of nonconvex problems and global methods (to contrast with convex theory).</p> </li> <li> <p>Chapter 20: Derivative-Free and Black-Box Optimization. Techniques when gradients are not available.</p> </li> <li> <p>Chapter 21: Metaheuristic and Evolutionary Optimization. Heuristic algorithms (genetic algorithms, simulated annealing) for hard problems.</p> </li> <li> <p>Chapter 22: Advanced Topics in Combinatorial Optimization. Combinatorial optimization problems and convex relaxations.</p> </li> <li> <p>Chapter 23: The Future of Optimization \u2013 Learning, Adaptation, Intelligence. Emerging trends (e.g. data-driven optimization, connections to machine learning).</p> </li> </ul> <p>This roadmap helps the reader see how the material progresses from foundations (Ch.2\u20135) to theory (Ch.6\u201311) to algorithms (Ch.12\u201315) and on to specialized and modern topics (Ch.16\u201323).</p>"},{"location":"convex/11_intro/#acknowledgments","title":"Acknowledgments","text":"<p>The content and structure of this web book are strongly informed by the Stanford University course EE364A: Convex Optimization I, taught by Stephen Boyd. In particular, the presentation draws inspiration from the 2023 lecture notes and course materials, which are widely regarded as a foundational reference in modern convex optimization.</p>"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model \\(x\\) to data \\((A, b)\\) takes the form:</p> \\[ \\min_x \\ \\|A x - b\\|_2^2. \\] <p>Later in this chapter, we will see that this objective finds the point in the column space of \\(A\\) that is closest to \\(b\\). Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p> <p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas \u2014 projections, subspaces, orthogonality, eigenvalues, singular values, and norms \u2014 because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>"},{"location":"convex/12_vector/#vector-spaces-subspaces-and-affine-sets","title":"Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set of vectors that can be added and scaled without leaving the set. The familiar example is \\(\\mathbb{R}^n\\), where operations like \\(\\alpha x + \\beta y\\) keep us within the same space.</p> <p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are \u201cflat\u201d objects that always pass through the origin, such as lines or planes in \\(\\mathbb{R}^3\\). </p> <p>Affine sets extend this idea by allowing a shift away from the origin. A set \\(A\\) is affine if it contains the entire line passing through any two of its points. Equivalently, for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  \\(\\theta x + (1 - \\theta) y \\in A.\\) That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\). By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set. </p> <p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: \\(A = x_0 + S = \\{\\, x_0 + s : s \\in S \\,\\},\\) where \\(S\\) is a subspace and \\(x_0\\) is any point in the set. This representation is extremely useful in optimization. If \\(Ax = b\\) is a linear constraint, then its solution set is an affine set. A single particular solution \\(x_0\\) gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of \\(A\\). Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p> <p>Affine Transformations: An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as \\(f(x) = A x + b,\\) where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex. is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p> <p>In summary, vector spaces describe the space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the webbook.</p>"},{"location":"convex/12_vector/#linear-combinations-span-basis-dimension","title":"Linear combinations, span, basis, dimension","text":"<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p> <p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form is a linear combination. The set of all linear combinations is called the span:  The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p> <p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p> <ul> <li>A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). </li> <li>The number of basis vectors is the dimension \\(\\dim(V)\\).</li> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states: \\(\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n,\\) where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>Column Space: The column space of a matrix , denoted , is the set of all possible output vectors  that can be written as  for some . In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system  have a solution?\u201d is equivalent to asking whether . If  lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of , denoted , is the set of all input vectors  that are mapped to zero:  . It answers a different question: If a solution to  exists, is it unique? If the null space contains only the zero vector (), the solution is unique. But if  contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix  is a linear combination of others for example, \u2014the columns of  become linearly dependent. This creates a nonzero vector in the null space of , meaning multiple weight vectors  can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and  becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p> <p>Feasible Directions: In a constrained optimization problem of the form , the null space of  characterizes the directions along which one can move without violating the constraints. If , then moving from a feasible point  to  preserves feasibility, since  . Thus, the null space defines the space of free movement directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of , denoted , is the span of the rows of  (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to . The row space is orthogonal to the null space of :  .  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in  affect the constraints.</p> <p>Left Null Space: The left null space, denoted , is the set of all vectors  such that . These vectors are orthogonal to the columns of , and therefore orthogonal to the column space itself. In least squares problems,  represents residual directions\u2014components of  that cannot be explained by the model .</p> <p>Projection Interpretation (Least Squares):  When  has no exact solution (as in overdetermined systems), the least squares solution finds  such that  is the projection of  onto the column space of :  and the residual  lies in the left null space . This provides a geometric view: the solution projects  onto the closest point in the subspace that  can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of  is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where  is the number of columns of . This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in .</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by .  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map .</p>"},{"location":"convex/12_vector/#inner-products-and-orthogonality","title":"Inner products and orthogonality","text":"<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality. An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\): \\(\\(|\\langle x,y\\rangle| \\le \\|x\\|\\|y\\|~,\\)\\) with equality iff \\(x\\) and \\(y\\) are linearly dependent. Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#norms-and-distances","title":"Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms. Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of the \\(\\ell_1\\) norm is the \\(\\ell_\\infty\\) norm, and the dual of the \\(\\ell_2\\) norm is itself.</p> <p>The dual norm captures how large a linear functional can be when applied to vectors of bounded size. Geometrically, consider the unit ball \\(\\{x : \\|x\\|\\le 1\\}\\). The dot product \\(x^\\top y\\) measures how well the vector \\(x\\) aligns with \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible alignment between \\(y\\) and any vector \\(x\\) inside this unit ball.</p> <p>If \\(\\|y\\|_*\\) is large, then there exists a direction \\(x\\) that is small according to the original norm yet strongly aligned with \\(y\\), resulting in a large dot product. If \\(\\|y\\|_*\\) is small, then \\(y\\) is poorly aligned with all unit-norm vectors, and \\(x^\\top y\\) remains small for every feasible \\(x\\).</p> <p>This perspective explains common dual norm pairs. The dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), reflecting sensitivity to the largest coordinate, while the \\(\\ell_2\\) norm is self-dual due to rotational symmetry. Dual norms are fundamental in convex optimization, appearing in optimality conditions, error bounds, and regularization analysis.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. </p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>"},{"location":"convex/12_vector/#orthogonal-projections-and-least-squares","title":"Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\). If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#operator-norms-singular-values-and-spectral-structure","title":"Operator norms, singular values, and spectral structure","text":"<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>"},{"location":"convex/12_vector/#operator-norms","title":"Operator norms","text":"<p>Given a matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and norms \\(\\|\\cdot\\|_p\\) on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_q\\) on \\(\\mathbb{R}^m\\), the induced operator norm is defined as  This quantity measures the largest amount by which \\(A\\) can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p> <ul> <li>\\(\\|A\\|_{2 \\to 2}\\), the spectral norm, equals the largest singular value of \\(A\\).</li> <li>\\(\\|A\\|_{1 \\to 1}\\) is the maximum absolute column sum.</li> <li>\\(\\|A\\|_{\\infty \\to \\infty}\\) is the maximum absolute row sum.</li> </ul> <p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function  converges for step sizes \\(\\alpha &lt; 2 / \\|Q\\|_2\\). This shows that controlling the operator norm of the Hessian\u2014or a Lipschitz constant of the gradient\u2014directly governs how aggressively an algorithm can move.</p>"},{"location":"convex/12_vector/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a factorization  where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is diagonal with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots\\). The \\(\\sigma_i\\) are the singular values of \\(A\\).</p> <p>Geometrically, the SVD shows how \\(A\\) transforms the unit ball into an ellipsoid. The columns of \\(V\\) give the principal input directions, the singular values are the lengths of the ellipsoid\u2019s axes, and the columns of \\(U\\) give the output directions. The largest singular value \\(\\sigma_{\\max}\\) equals the spectral norm \\(\\|A\\|_2\\), while the smallest \\(\\sigma_{\\min}\\) describes the least expansion (or exact flattening if \\(\\sigma_{\\min} = 0\\)).</p> <p>SVD is a powerful diagnostic tool in optimization. The ratio  is the condition number of \\(A\\). A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means \\(A\\) behaves more like a uniform scaling, which is ideal for optimization.</p>"},{"location":"convex/12_vector/#low-rank-structure","title":"Low-rank structure","text":"<p>The rank of \\(A\\) is the number of nonzero singular values. When \\(A\\) has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p> <p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>"},{"location":"convex/12_vector/#mental-map","title":"Mental Map","text":"<pre><code>                 Linear Algebra Foundations for Convex Optimization\n               Geometry + Computation for Understanding Algorithms\n                                   \u2502\n                                   \u25bc\n                        Objects: Vectors, Matrices, Maps\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Vector Spaces / Subspaces / Affine Sets                      \u2502\n      \u2502  - Feasible sets of Ax=b are affine: x = x0 + N(A)            \u2502\n      \u2502  - Feasible directions live in the null space                 \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  The Four Fundamental Subspaces of A                            \u2502\n      \u2502  - Column space C(A): reachable outputs (Ax)                    \u2502\n      \u2502  - Null space N(A): indistinguishable inputs (Ax=0)             \u2502\n      \u2502  - Row space R(A): constraint directions in x-space             \u2502\n      \u2502  - Left null space N(A\u1d40): residual directions (orthogonal to C) \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Inner Products \u2192 Orthogonality \u2192 Projections                  \u2502\n      \u2502  - Defines angles/lengths                                      \u2502\n      \u2502  - Least squares = projection of b onto C(A)                   \u2502\n      \u2502  - QR / Gram\u2013Schmidt give stable numerical tools               \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Norms &amp; Dual Norms: \"How we measure size\"                     \u2502\n      \u2502  - Unit balls define geometry of constraints/regularizers      \u2502\n      \u2502  - Dual norms bound dot products and appear in optimality      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Spectral Structure: Eigenvalues, PSD, SVD                    \u2502\n      \u2502  - PSD \u21d4 convex quadratics (Hessians of quadratic objectives)\u2502\n      \u2502  - SVD shows stretching directions and conditioning           \u2502\n      \u2502  - Condition number \u2194 convergence speed / numerical stability \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p> <p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton\u2019s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p> <p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>"},{"location":"convex/13_calculus/#gradients-and-directional-derivatives","title":"Gradients and Directional Derivatives","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The function is differentiable at a point \\(x\\) if there exists a vector \\(\\nabla f(x)\\) such that  meaning that the linear function \\(h \\mapsto \\nabla f(x)^\\top h\\) provides the best local approximation to \\(f\\) near \\(x\\). The gradient is the unique vector with this property.</p> <p>A closely related concept is the directional derivative. For any direction \\(v \\in \\mathbb{R}^n\\), the directional derivative of \\(f\\) at \\(x\\) in the direction \\(v\\) is  If \\(f\\) is differentiable, then  Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction \\(v\\) tells us how rapidly \\(f\\) increases when we move infinitesimally along \\(v\\).</p> <p>This immediately yields an important geometric fact. Among all unit directions \\(u\\),  is maximized when \\(u\\) points in the direction of \\(\\nabla f(x)\\), the direction of steepest ascent. The steepest descent direction is therefore \\(-\\nabla f(x)\\), which motivates gradient-descent algorithms for minimizing functions.</p> <p>For any real number \\(c\\), the level set of \\(f\\) is   </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#jacobians","title":"Jacobians","text":"<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>"},{"location":"convex/13_calculus/#from-derivative-to-gradient","title":"From derivative to gradient","text":"<p>For a scalar function , differentiability means that near any point ,  The gradient vector  collects all partial derivatives. Each component measures how sensitive \\(f\\) is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>"},{"location":"convex/13_calculus/#from-gradient-to-jacobian","title":"From gradient to Jacobian","text":"<p>Now consider a vector-valued function \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\),  Each output \\(F_i\\) has its own gradient. Stacking these row vectors yields the Jacobian matrix:  </p> <p>The Jacobian provides the best linear approximation of \\(F\\) near \\(x\\):  Thus, locally, the nonlinear map \\(F\\) behaves like the linear map \\(h \\mapsto J_F(x)h\\). A small displacement \\(h\\) in input space is transformed into an output change governed by the Jacobian.</p>"},{"location":"convex/13_calculus/#interpreting-the-jacobian","title":"Interpreting the Jacobian","text":"Component of \\(J_F(x)\\) Meaning Row \\(i\\) Gradient of output \\(F_i(x)\\): how the \\(i\\)-th output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to \\(x_j\\): how varying input \\(x_j\\) affects the entire output vector. Determinant (when \\(m=n\\)) Local volume scaling: how \\(F\\) expands or compresses space near \\(x\\). Rank Local dimension of the image: whether any input directions are lost or collapsed. <p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>"},{"location":"convex/13_calculus/#the-hessian-and-curvature","title":"The Hessian and Curvature","text":"<p>For a twice\u2013differentiable function , the Hessian matrix collects all second-order partial derivatives:  </p> <p>The Hessian describes the local curvature of the function. While the gradient indicates the direction of steepest change, the Hessian tells us how that directional change itself varies\u2014whether the surface curves upward, curves downward, or remains nearly flat.</p>"},{"location":"convex/13_calculus/#curvature-and-positive-definiteness","title":"Curvature and positive definiteness","text":"<p>The eigenvalues of the Hessian determine its geometric behavior:</p> <ul> <li>If  (all eigenvalues nonnegative), the function is locally convex near \\(x\\).  </li> <li>If , the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li> <li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li> </ul> <p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>"},{"location":"convex/13_calculus/#example-quadratic-functions","title":"Example: Quadratic functions","text":"<p>Consider the quadratic function  where \\(Q\\) is symmetric. The gradient and Hessian are  Setting the gradient to zero gives the stationary point  If \\(Q \\succ 0\\), the solution  is the unique minimizer. The Hessian \\(Q\\) being positive definite confirms strict convexity.</p> <p>The eigenvalues of \\(Q\\) also explain the difficulty of minimizing \\(f\\):</p> <ul> <li>Large eigenvalues produce very steep, narrow directions\u2014optimization methods must take small steps.  </li> <li>Small eigenvalues produce flat directions\u2014progress is slow, especially for gradient descent.  </li> </ul> <p>The ratio of largest to smallest eigenvalue, the condition number, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>"},{"location":"convex/13_calculus/#why-the-hessian-matters-in-optimization","title":"Why the Hessian matters in optimization","text":"<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p> <ul> <li>Newton\u2019s method uses the Hessian to rescale directions, effectively \u201cwhitening\u2019\u2019 curvature and often converging rapidly.  </li> <li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li> <li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li> </ul> <p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>"},{"location":"convex/13_calculus/#taylor-approximation","title":"Taylor approximation","text":"<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>"},{"location":"convex/13_calculus/#first-order-approximation","title":"First-order approximation","text":"<p>If \\(f\\) is differentiable at \\(x\\), then for small steps \\(d\\),  The gradient gives the best linear model of the function near \\(x\\). This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>"},{"location":"convex/13_calculus/#second-order-approximation","title":"Second-order approximation","text":"<p>If \\(f\\) is twice differentiable, we can include curvature information:  The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p> <ul> <li>If , the quadratic term is nonnegative and the function curves upward\u2014locally bowl-shaped.</li> <li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others\u2014characteristic of saddle points.</li> </ul>"},{"location":"convex/13_calculus/#role-in-optimization-algorithms","title":"Role in optimization algorithms","text":"<p>Second-order Taylor models are the basis of Newton-type methods. Newton\u2019s method chooses \\(d\\) by approximately minimizing the quadratic model,  which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p> <p>Thus, Taylor expansions connect a function\u2019s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>"},{"location":"convex/13_calculus/#smoothness-and-strong-convexity","title":"Smoothness and Strong Convexity","text":"<p>In optimization, the behavior of a function\u2019s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>"},{"location":"convex/13_calculus/#lipschitz-continuous-gradients-l-smoothness","title":"Lipschitz continuous gradients (L-smoothness)","text":"<p>A differentiable function  has an \\(L\\)-Lipschitz continuous gradient if  This condition limits how quickly the gradient can change. Intuitively, an \\(L\\)-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:  This inequality states that every \\(L\\)-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p> <p>In gradient descent, smoothness directly determines a safe step size: choosing  ensures that each update decreases the function value for convex objectives. In machine learning, the constant \\(L\\) effectively controls how large the learning rate can be before training becomes unstable.</p>"},{"location":"convex/13_calculus/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function  is -strongly convex if, for some ,  This condition guarantees that \\(f\\) has at least \\(\\mu\\) amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p> <p>Strong convexity has major optimization implications:</p> <ul> <li>The minimizer is unique.  </li> <li>Gradient descent converges linearly with step size \\(\\eta \\le 1/L\\).  </li> <li>The ratio \\(L / \\mu\\) (the condition number) dictates convergence speed.</li> </ul>"},{"location":"convex/13_calculus/#curvature-in-both-directions","title":"Curvature in both directions","text":"<p>Together, smoothness and strong convexity bound the curvature of \\(f\\):  Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p> <p>These concepts, imiting curvature from above via \\(L\\) and from below via \\(\\mu\\), form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>"},{"location":"convex/13_calculus/#mental-map","title":"Mental map","text":"<pre><code>                Multivariable Calculus for Optimization\n        How objectives change, how curvature shapes algorithms\n                              \u2502\n                              \u25bc\n                 Local change of a scalar function f(x)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Differentiability &amp; First-Order Model                     \u2502\n     \u2502 f(x+h) = f(x) + \u2207f(x)\u1d40h + o(\u2016h\u2016)                          \u2502\n     \u2502 - \u2207f(x): best linear approximation                        \u2502\n     \u2502 - Directional derivative: D_v f(x) = \u2207f(x)\u1d40v              \u2502\n     \u2502 - Steepest descent: move along -\u2207f(x)                     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Geometry of Level Sets                                      \u2502\n     \u2502 L_c = {x : f(x)=c}                                          \u2502\n     \u2502 - If \u2207f(x) \u2260 0, then \u2207f(x) \u27c2 level set at x                 \u2502\n     \u2502 - Connects to constrained optimality (later: KKT)           \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Vector-Valued Maps &amp; Jacobians                              \u2502\n     \u2502 F: \u211d\u207f \u2192 \u211d\u1d50                                                  \u2502\n     \u2502 - Jacobian J_F(x) stacks gradients of outputs               \u2502\n     \u2502 - Linearization: F(x+h) \u2248 F(x) + J_F(x) h                   \u2502\n     \u2502 - Chain rule foundation for backprop / sensitivity analysis \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Structure: Hessian &amp; Curvature               \u2502\n     \u2502 \u2207\u00b2f(x): matrix of second partials                         \u2502\n     \u2502 - Curvature along v: v\u1d40\u2207\u00b2f(x)v                            \u2502\n     \u2502 - Eigenvalues quantify steep/flat directions              \u2502\n     \u2502 - PSD/PD Hessian ties directly to convexity (Ch.5)        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Taylor Models \u2192 Algorithm Design                          \u2502\n     \u2502 First-order:  f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d                     \u2502\n     \u2502 Second-order: f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d + \u00bd d\u1d40\u2207\u00b2f(x)d       \u2502\n     \u2502 - Gradient descent uses the linear model                  \u2502\n     \u2502 - Newton uses the quadratic model: d \u2248 -(\u2207\u00b2f)^{-1}\u2207f      \u2502\n     \u2502 - Trust-region / quasi-Newton approximate curvature       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Global Control of Local Behavior: Smoothness &amp; Strong Convexity\u2502\n     \u2502 L-smooth: \u2016\u2207f(x)-\u2207f(y)\u2016 \u2264 L\u2016x-y\u2016                               \u2502\n     \u2502 - Descent Lemma gives a quadratic upper bound                  \u2502\n     \u2502 - Sets safe step size: \u03b7 \u2264 1/L (for convex objectives)         \u2502\n     \u2502 \u03bc-strongly convex: f lies above tangents by (\u03bc/2)\u2016y-x\u2016\u00b2        \u2502\n     \u2502 - Unique minimizer, linear convergence of gradient descent     \u2502\n     \u2502 Combined curvature bounds: \u03bcI \u2aaf \u2207\u00b2f(x) \u2aaf LI                   \u2502\n     \u2502 - Condition number \u03ba = L/\u03bc governs difficulty                  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p> <p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>"},{"location":"convex/14_convexsets/#convex-sets","title":"Convex sets","text":"<p>A set  is convex if for any two points  and any ,  That is, the entire line segment between \\(x\\) and \\(y\\) lies inside the set. Convex sets have no \u201choles\u201d or \u201cindentations,\u201d and this geometric regularity is what makes optimization over them tractable.</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>Affine subspaces: .  </li> <li>Halfspaces: .  </li> <li>Euclidean balls: .  </li> <li>  balls (axis-aligned boxes): .  </li> <li>Probability simplex: .  </li> </ul> <p>A set fails to be convex whenever some segment between two feasible points leaves the set\u2014for example, a crescent or an annulus.</p>"},{"location":"convex/14_convexsets/#affine-sets-hyperplanes-and-halfspaces","title":"Affine sets, hyperplanes, and halfspaces","text":"<p>Affine sets generalize linear subspaces by allowing a shift. A set \\(A\\) is affine if for some point \\(x_0\\) and subspace \\(S\\),  Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p> <p>A hyperplane is an affine set defined by a single linear equation:  Hyperplanes act as the \u201cflat boundaries\u201d of higher-dimensional space and are the fundamental building blocks of polyhedra.</p> <p>A halfspace is one side of a hyperplane:  Halfspaces are convex and serve as basic local approximations to general convex sets.</p>"},{"location":"convex/14_convexsets/#convex-combinations-and-convex-hulls","title":"Convex combinations and convex hulls","text":"<p>A convex combination of points  is a weighted average  Convex sets are precisely those that contain all convex combinations of their points.</p> <p>The convex hull of a set \\(S\\), denoted \\(\\operatorname{conv}(S)\\), is the set of all convex combinations of finitely many points in \\(S\\). It is the smallest convex set containing \\(S\\). Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p> <p>Convex hulls are important because:</p> <ul> <li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li> <li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li> </ul>"},{"location":"convex/14_convexsets/#polyhedra-and-polytopes","title":"Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are always convex; they may be bounded or unbounded.</p> <p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>"},{"location":"convex/14_convexsets/#extreme-points","title":"Extreme points","text":"<p>Let  be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if  implies .</p> <p>Geometrically, extreme points are the \u201ccorners\u201d of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems\u2014such as linear programs\u2014achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>"},{"location":"convex/14_convexsets/#cones","title":"Cones","text":"<p>Cones generalize the idea of \u201cdirections\u201d in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>"},{"location":"convex/14_convexsets/#basic-definition","title":"Basic definition","text":"<p>A set \\(K \\subseteq \\mathbb{R}^n\\) is a cone if  A cone is convex if it is also closed under addition:  </p> <p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant , each coordinate axis direction is an extreme ray.</p>"},{"location":"convex/14_convexsets/#conic-hull","title":"Conic hull","text":"<p>Given any set \\(S\\), its conic hull is the set of all conic combinations:  This is the smallest convex cone containing \\(S\\). Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>"},{"location":"convex/14_convexsets/#polar-cones","title":"Polar cones","text":"<p>For a cone \\(K\\), the polar cone is defined as  </p> <p>Intuition:</p> <ul> <li>Polar vectors make a nonacute angle with every vector in \\(K\\).  </li> </ul> <p>Key properties:</p> <ul> <li>\\(K^\\circ\\) is always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, then \\(K^\\circ\\) is the orthogonal complement.  </li> <li>For any closed convex cone, </li> </ul> <p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>"},{"location":"convex/14_convexsets/#tangent-cones","title":"Tangent cones","text":"<p>For a set \\(C\\) and a point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) consists of all feasible \u201cinfinitesimal directions\u201d from \\(x\\):  </p> <p>Intuition:</p> <ul> <li>At an interior point, \\(T_C(x) = \\mathbb{R}^n\\): all small moves are allowed.  </li> <li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li> </ul> <p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>"},{"location":"convex/14_convexsets/#normal-cones","title":"Normal cones","text":"<p>For a convex set \\(C\\), the normal cone at a point \\(x \\in C\\) is  </p> <p>Interpretation:</p> <ul> <li>Every \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\).  </li> <li>At interior points, the normal cone is \\(\\{0\\}\\).  </li> <li>At boundary or corner points, it becomes a pointed cone of outward normals.</li> </ul> <p>A fundamental relationship ties tangent and normal cones together:  </p> <p>Normal cones appear directly in first-order optimality conditions. For a constrained problem  a point \\(x^*\\) is optimal only if  This expresses a balance between the objective\u2019s slope and the \u201cpushback\u2019\u2019 from the constraint set.</p> <p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplanes-and-separation","title":"Supporting Hyperplanes and Separation","text":"<p>One of the most important geometric facts about convex sets is that they can be supported or separated by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting Hyperplane Theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be nonempty, closed, and convex, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero vector \\(a\\) such that</p> \\[ a^\\top x \\le a^\\top x_0 \\qquad \\forall x \\in C. \\] <p>This means that the hyperplane</p> \\[ a^\\top x = a^\\top x_0 \\] <p>touches \\(C\\) at \\(x_0\\) but does not cut through it. The vector \\(a\\) is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating Hyperplane Theorem","text":"<p>If \\(C\\) and \\(D\\) are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector \\(a\\) and scalar \\(b\\) such that</p> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\qquad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <p>The hyperplane \\(a^\\top x = b\\) places all points of \\(C\\) on one side and all points of \\(D\\) on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p> <p>## Mental Map</p> <p>```text                Convex Sets &amp; Geometric Fundamentals      Feasible regions, geometry of constraints, and separation                               \u2502                               \u25bc                  Core idea: convexity removes \"bad geometry\"         (segments stay inside \u2192 no holes/indentations \u2192 tractable)                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Definition of Convex Set                                  \u2502      \u2502 C convex \u21d4  \u03b8x + (1-\u03b8)y \u2208 C  for all x,y\u2208C, \u03b8\u2208[0,1]      \u2502      \u2502 - Geometry: every chord lies inside                       \u2502      \u2502 - Optimization: feasible region supports global reasoning \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Affine Geometry: the \"flat\" building blocks               \u2502      \u2502 - Affine set: x0 + S                                      \u2502      \u2502 - Hyperplane: {x : a\u1d40x = b}                               \u2502      \u2502 - Halfspace:  {x : a\u1d40x \u2264 b}                               \u2502      \u2502 Role: linear constraints and local linear boundaries      \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Convex Combinations &amp; Convex Hull                         \u2502      \u2502 - Convex combination: \u03a3 \u03b8_i x_i, \u03b8_i\u22650, \u03a3\u03b8_i=1            \u2502      \u2502 - conv(S): all convex combos of points in S               \u2502      \u2502 Why it matters: convexification / relaxations / geometry  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Polyhedra &amp; Polytopes                                     \u2502      \u2502 - Polyhedron: intersection of finitely many halfspaces    \u2502      \u2502   P = {x : Ax \u2264 b}                                        \u2502      \u2502 - Polytope: bounded polyhedron                            \u2502      \u2502 Why it matters: LP feasible sets; two views (H- vs V-form)\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Extreme Points (Corners)                                     \u2502      \u2502 - x extreme \u21d4 cannot be written as nontrivial convex combo  \u2502      \u2502 - For polytopes: extremes = vertices                         \u2502      \u2502 Optimization link: linear objectives attain optima at corners\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Cones: scaling geometry for constraints &amp; duality         \u2502      \u2502 - Cone: x\u2208K, \u03b1\u22650 \u21d2 \u03b1x\u2208K                                  \u2502      \u2502 - Convex cone: also closed under addition                 \u2502      \u2502 - Conic hull cone(S): smallest convex cone containing S   \u2502      \u2502 - Extreme rays replace extreme points                     \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Local Directional Geometry at a Point x                     \u2502      \u2502 Tangent cone T_C(x): feasible infinitesimal directions      \u2502      \u2502 - Interior point: T_C(x)=\u211d\u207f                                 \u2502      \u2502 - Boundary: directions restricted                           \u2502      \u2502 Normal cone N_C(x): outward normals / supporting directions \u2502      \u2502 - Interior point: N_C(x)={0}                                \u2502      \u2502 - Boundary/corner: pointed cone of normals                  \u2502      \u2502 Duality relation: N_C(x) = (T_C(x))\u00b0                        \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502                               \u25bc      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 Supporting Hyperplanes &amp; Separation                       \u2502      \u2502 Supporting hyperplane at boundary point x0:               \u2502      \u2502   \u2203a\u22600 s.t. a\u1d40x \u2264 a\u1d40x0  for all x\u2208C                       \u2502      \u2502 Separating hyperplane for disjoint convex sets C,D:       \u2502      \u2502   \u2203a,b s.t. a\u1d40x \u2264 b \u2264 a\u1d40y  for x\u2208C, y\u2208D                   \u2502      \u2502 Why it matters: geometry behind subgradients and duality  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>```</p>"},{"location":"convex/14_convexsets/#why-this-matters-for-optimisation","title":"Why This Matters for Optimisation","text":"<p>These geometric results are central to convex optimisation: - Subgradients correspond to supporting hyperplanes of the epigraph of a convex function. - Dual variables arise from separating infeasible points from the feasible region. - KKT conditions express the balance between the gradient of the objective and the normals of active constraints. - Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.  Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>"},{"location":"convex/15_convexfunctions/#definitions-of-convexity","title":"Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),  </p> <p>The graph of \\(f\\) never dips below the straight line between \\((x,f(x))\\) and \\((y,f(y))\\). If the inequality is strict whenever \\(x \\neq y\\), the function is strictly convex.</p> <p>A powerful geometric viewpoint comes from the epigraph:  The function \\(f\\) is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>"},{"location":"convex/15_convexfunctions/#first-order-characterisation","title":"First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if  </p> <p>Interpretation:</p> <ul> <li>The tangent plane at any point \\(x\\) lies below the function everywhere.</li> <li>\\(\\nabla f(x)\\) defines a supporting hyperplane to the epigraph.</li> <li>The gradient provides a global linear underestimator of \\(f\\).</li> </ul> <p>This geometric picture is crucial in optimisation: at a minimiser \\(x^\\star\\), convexity implies </p> <p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>"},{"location":"convex/15_convexfunctions/#second-order-characterisation","title":"Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then convexity can be checked via curvature:</p> \\[ f \\text{ is convex } \\iff \\nabla^2 f(x) \\succeq 0 \\text{ for all } x. \\] <ul> <li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li> <li>If \\(\\nabla^2 f(x) \\succ 0\\) everywhere, the function is strictly convex.  </li> <li>Negative eigenvalues indicate directions of negative curvature \u2014 impossible for convex functions.</li> </ul> <p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>"},{"location":"convex/15_convexfunctions/#examples-of-convex-functions","title":"Examples of convex functions","text":"<ol> <li> <p>Affine functions:     Always convex (and concave). They define supporting hyperplanes.</p> </li> <li> <p>Quadratic functions with PSD Hessian:     Convex because the curvature matrix \\(Q\\) is PSD.</p> </li> <li> <p>Norms:     All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p> </li> <li> <p>Maximum of affine functions:     Convex because the maximum of convex functions is convex.    (Important in SVM hinge loss.)</p> </li> <li> <p>Log-sum-exp:     A smooth approximation to the max; convex by Jensen\u2019s inequality. Appears in softmax, logistic regression, partition functions.</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#jensens-inequality","title":"Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex and \\(X\\) a random variable in its domain. Then:  </p> <p>This generalises the definition of convexity from finite averages to expectations. Practically:</p> <ul> <li>convex functions \u201cpull upward\u201d under averaging,</li> <li>log-sum-exp is convex because exponential is convex,</li> <li>EM and variational methods rely on Jensen to construct lower bounds.</li> </ul> <p>As a finite form, for \\(\\theta_i \\ge 0\\) with \\(\\sum \\theta_i = 1\\),  </p>"},{"location":"convex/15_convexfunctions/#operations-that-preserve-convexity","title":"Operations that preserve convexity","text":"<p>Convexity is preserved under many natural constructions:</p> <ul> <li> <p>Nonnegative scaling:   If \\(f\\) is convex and \\(\\alpha \\ge 0\\), then \\(\\alpha f\\) is convex.</p> </li> <li> <p>Addition:   If \\(f\\) and \\(g\\) are convex, then \\(f+g\\) is convex.</p> </li> <li> <p>Maximum: \\(\\max\\{f,g\\}\\) is convex.</p> </li> <li> <p>Affine pre-composition:   If \\(A\\) is a matrix,      is convex.</p> </li> <li> <p>Monotone composition rule:   If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex,   then \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex.</p> </li> </ul>"},{"location":"convex/15_convexfunctions/#level-sets-of-convex-functions","title":"Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), the sublevel set is  </p> <p>If \\(f\\) is convex, every sublevel set is convex. This property is crucial because inequalities \\(f(x) \\le \\alpha\\) are ubiquitous in constraints.</p> <p>Examples:</p> <ul> <li>Norm balls: \\(\\{ x : \\|x\\|_2 \\le r \\}\\) </li> <li>Linear regression confidence ellipsoids: \\(\\{ x : \\|Ax - b\\|_2 \\le \\epsilon \\}\\)</li> </ul> <p>These sets enable convex constrained optimisation formulations.</p>"},{"location":"convex/15_convexfunctions/#strict-and-strong-convexity","title":"Strict and strong convexity","text":""},{"location":"convex/15_convexfunctions/#strict-convexity","title":"Strict convexity","text":"<p>A function is strictly convex if  for all \\(x \\neq y\\) and \\(\\theta \\in (0,1)\\).</p> <p>Strict convexity implies unique minimisers.</p>"},{"location":"convex/15_convexfunctions/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function is \\(\\mu\\)-strongly convex if  </p> <p>Strong convexity adds quantitative curvature: the function grows at least quadratically away from its minimiser.</p> <p>Consequences:</p> <ul> <li>unique minimiser,</li> <li>gradient descent achieves linear convergence rate,   error shrinks as </li> <li>conditioning (\\(\\kappa = L/\\mu\\)) governs algorithmic difficulty.</li> </ul> <p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds \\(\\tfrac{\\lambda}{2}\\|x\\|_2^2\\)).</p>"},{"location":"convex/15_convexfunctions/#mental-map","title":"Mental Map","text":"<pre><code>                      Convex Functions\n      Objective landscapes with predictable geometry and guarantees\n                              \u2502\n                              \u25bc\n                Core idea: no bad local minima\n        (every local minimum is global; geometry is well-behaved)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Definition of Convexity                                   \u2502\n     \u2502 f(\u03b8x+(1\u2212\u03b8)y) \u2264 \u03b8f(x)+(1\u2212\u03b8)f(y)                            \u2502\n     \u2502 - Graph lies below all chords                             \u2502\n     \u2502 - Strict convexity: inequality is strict                  \u2502\n     \u2502 - Epigraph view: f convex \u21d4 epi(f) is a convex set       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Geometry (Supporting Hyperplanes)              \u2502\n     \u2502 f(y) \u2265 f(x)+\u2207f(x)\u1d40(y\u2212x)                                    \u2502\n     \u2502 - Tangent plane globally underestimates f                  \u2502\n     \u2502 - \u2207f(x) defines a supporting hyperplane to epi(f)          \u2502\n     \u2502 - Optimality: \u2207f(x*)=0 \u21d4 x* global minimizer (smooth case)\u2502\n     \u2502 - Nonsmooth extension: subgradients (next chapter)         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Characterisation                              \u2502\n     \u2502 \u2207\u00b2f(x) \u2ab0 0  for all x                                      \u2502\n     \u2502 - PSD Hessian \u21d4 upward curvature everywhere               \u2502\n     \u2502 - PD Hessian \u21d4 strict convexity                           \u2502\n     \u2502 - Links convexity to eigenvalues and curvature (Ch.3)      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Canonical Examples                                          \u2502\n     \u2502 - Affine functions: supporting hyperplanes                  \u2502\n     \u2502 - Quadratics (Q\u2ab00): curvature from Hessian                  \u2502\n     \u2502 - Norms: regularization geometry                            \u2502\n     \u2502 - Max of affine functions: hinge loss, LPs                  \u2502\n     \u2502 - Log-sum-exp: smooth max, softmax, logistic regression     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Jensen\u2019s Inequality                                         \u2502\n     \u2502 f(E[X]) \u2264 E[f(X)]                                           \u2502\n     \u2502 - Convex functions penalize variability                     \u2502\n     \u2502 - Basis for EM, variational bounds, log-sum-exp convexity   \u2502\n     \u2502 - Extends convexity from points to expectations             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Convexity-Preserving Operations                             \u2502\n     \u2502 - Scaling (\u03b1\u22650), addition                                   \u2502\n     \u2502 - Max of convex functions                                   \u2502\n     \u2502 - Affine pre-composition f(Ax+b)                            \u2502\n     \u2502 - Monotone composition rules                                \u2502\n     \u2502 Role: modular construction of convex models                 \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Sublevel Sets                                               \u2502\n     \u2502 {x : f(x) \u2264 \u03b1}                                              \u2502\n     \u2502 - Always convex for convex f                                \u2502\n     \u2502 - Enables convex inequality constraints                     \u2502\n     \u2502 - Norm balls, confidence ellipsoids, feasibility regions    \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Strict vs Strong Convexity                                  \u2502\n     \u2502 Strict convexity: unique minimizer                          \u2502\n     \u2502 Strong convexity: f \u2265 tangent + (\u03bc/2)\u2016x\u2212y\u2016\u00b2                 \u2502\n     \u2502 - Quantitative curvature                                    \u2502\n     \u2502 - Linear convergence of gradient descent                    \u2502\n     \u2502 - Conditioning \u03ba=L/\u03bc governs difficulty                     \u2502\n     \u2502 - Often induced via \u2113\u2082 regularization                       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p> <ul> <li>the  norm  (nondifferentiable at zero),</li> <li>pointwise-max functions such as ,</li> <li>the hinge loss  used in SVMs,</li> <li>regularisers like total variation or indicator functions of convex sets.</li> </ul> <p>Although these functions have \u201ckinks\u201d, they remain convex\u2014and convexity guarantees the existence of supporting hyperplanes at every point.</p>"},{"location":"convex/16_subgradients/#subgradients-and-the-subdifferential","title":"Subgradients and the Subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex.  A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if</p> \\[ f(y) \\ge f(x) + g^\\top (y - x) \\quad \\text{for all } y. \\] <p>Geometric interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top(y-x)\\) is a global underestimator of \\(f\\).</li> <li>Each subgradient defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).</li> <li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li> <li>At kinks, there may be infinitely many supporting hyperplanes.</li> </ul> <p>The subdifferential of \\(f\\) at \\(x\\) is the set  </p> <p>Properties:</p> <ul> <li>  is always a nonempty convex set (if \\(x\\) is in the interior of the domain).</li> <li>If \\(f\\) is differentiable at \\(x\\), then </li> <li>If \\(f\\) is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li> </ul> <p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>"},{"location":"convex/16_subgradients/#examples","title":"Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\). Then:</p> <ul> <li>If \\(t &gt; 0\\),  \\(\\partial f(t) = \\{1\\}\\).</li> <li>If \\(t &lt; 0\\),  \\(\\partial f(t) = \\{-1\\}\\).</li> <li>If \\(t = 0\\), </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) supports the graph from below.</p>"},{"location":"convex/16_subgradients/#the-ell_1-norm","title":"The  norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\):</p> \\[ g \\in \\partial \\|x\\|_1 \\quad\\Longleftrightarrow\\quad g_i \\in \\partial |x_i|. \\] <p>Thus:</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>"},{"location":"convex/16_subgradients/#pointwise-maximum-of-affine-functions","title":"Pointwise maximum of affine functions","text":"<p>Let </p> <ul> <li> <p>If only one index \\(i^\\star\\) achieves the maximum at \\(x\\), then </p> </li> <li> <p>If multiple indices are tied, then    the convex hull of the active slopes.</p> </li> </ul> <p>This structure underlies SVM hinge loss and ReLU-type functions.</p>"},{"location":"convex/16_subgradients/#subgradient-optimality-condition","title":"Subgradient Optimality Condition","text":"<p>For the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>a point \\(x^\\star\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^\\star). \\] <p>Interpretation:</p> <ul> <li>At optimality, no subgradient points to a direction that would decrease \\(f\\).</li> <li>Geometrically, the supporting hyperplane at \\(x^\\star\\) is horizontal, forming the flat bottom of the convex bowl.</li> <li>This generalises the smooth condition .</li> </ul>"},{"location":"convex/16_subgradients/#subgradient-calculus","title":"Subgradient Calculus","text":"<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let \\(f\\) and \\(g\\) be convex.</p>"},{"location":"convex/16_subgradients/#sum-rule","title":"Sum rule","text":"\\[ \\partial(f+g)(x) \\subseteq \\partial f(x) + \\partial g(x) = \\{ u+v : u \\in \\partial f(x),\\ v \\in \\partial g(x) \\}. \\] <p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>"},{"location":"convex/16_subgradients/#affine-composition","title":"Affine composition","text":"<p>If \\(h(x) = f(Ax + b)\\), then  </p> <p>This rule is heavily used in machine learning models, where losses depend on linear predictions \\(Ax\\).</p>"},{"location":"convex/16_subgradients/#maximum-of-convex-functions","title":"Maximum of convex functions","text":"<p>If \\(f(x) = \\max_i f_i(x)\\), then  </p> <p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar \u201czero gradient\u201d condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) framework developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#orders-of-optimality-why-first-order-is-enough-in-convex-optimization","title":"Orders of Optimality: Why First Order is Enough in Convex Optimization","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer \\(x^\\star\\):</p> Order Object inspected Role First-order \\(\\nabla f(x^\\star)\\) or subgradients Detects existence of a local descent direction Second-order Hessian \\(\\nabla^2 f(x^\\star)\\) Examines curvature (minimum vs saddle vs maximum) Higher-order Third derivative and beyond Rarely used; only for degenerate cases with vanishing curvature <p>In general nonconvex optimization, these conditions are used together: a point may have \\(\\nabla f(x^\\star) = 0\\) but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p> <p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{whenever the Hessian exists}. \\] <p>Therefore:</p> <ul> <li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li> <li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li> </ul> <p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>"},{"location":"convex/16a_optimality_conditions/#motivation","title":"Motivation","text":"<p>Consider the basic convex problem  where \\(f\\) is convex and \\(\\mathcal{X}\\) is a convex set.</p> <p>Intuitively, a point \\(\\hat{x}\\) is optimal if there is no feasible direction in which we can move and strictly decrease \\(f\\). In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside \\(\\mathcal{X}\\) are allowed.</p> <p>Thus, optimality can be seen as an equilibrium:</p> <ul> <li>the objective\u2019s tendency to decrease (captured by its gradient or subgradient)  </li> <li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li> </ul> <p>In machine learning, this appears as:</p> <ul> <li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li> <li>training until the force from regularization/constraints balances the data fit term (e.g., in \\(\\ell_1\\)-regularized models).</li> </ul> <p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>"},{"location":"convex/16a_optimality_conditions/#unconstrained-convex-problems","title":"Unconstrained Convex Problems","text":"<p>For the unconstrained problem  with \\(f\\) convex, the optimality conditions are especially simple.</p>"},{"location":"convex/16a_optimality_conditions/#smooth-case","title":"Smooth case","text":"<p>If \\(f\\) is differentiable, then a point \\(\\hat{x}\\) is optimal if and only if  </p> <p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>"},{"location":"convex/16a_optimality_conditions/#nonsmooth-case","title":"Nonsmooth case","text":"<p>If \\(f\\) is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes  </p> <p>Interpretation:</p> <ul> <li>The origin lies in the set of all subgradients at \\(\\hat{x}\\).  </li> <li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\).  </li> <li>No direction in \\(\\mathbb{R}^n\\) gives a first-order improvement in the objective.</li> </ul> <p>For smooth \\(f\\), this reduces to the usual condition \\(\\nabla f(\\hat{x}) = 0\\).</p>"},{"location":"convex/16a_optimality_conditions/#constrained-convex-problems","title":"Constrained Convex Problems","text":"<p>Now consider the constrained problem  where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a nonempty closed convex set.</p> <p>If \\(\\hat{x}\\) lies strictly inside \\(\\mathcal{X}\\), then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,  remains the necessary and sufficient condition for optimality.</p> <p>The interesting case is when \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\).</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-order condition with constraints","text":"<p>The general first-order optimality condition for the constrained convex problem is:  </p> <p>That is, there exist</p> <ul> <li>a subgradient \\(g \\in \\partial f(\\hat{x})\\), and  </li> <li>a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\)</li> </ul> <p>such that  </p> <p>Interpretation:</p> <ul> <li>The objective\u2019s slope \\(g\\) is exactly balanced by a normal vector \\(v\\) coming from the constraint set.  </li> <li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which \\(f\\) can decrease.  </li> <li>Geometrically, the epigraph of \\(f\\) and the feasible set meet with aligned supporting hyperplanes at \\(\\hat{x}\\).</li> </ul> <p>Special cases:</p> <ul> <li>If \\(\\hat{x}\\) is an interior point, then \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so we recover the unconstrained condition \\(0 \\in \\partial f(\\hat{x})\\).  </li> <li>If \\(\\mathcal{X}\\) is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li> </ul>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-the-kkt-framework","title":"Chapter 8: Lagrange Multipliers and the KKT Framework","text":"<p>We now have the ingredients for understanding optimality in convex optimization:</p> <ul> <li>convex functions define well-behaved objectives,</li> <li>convex sets describe feasible regions,</li> <li>gradients and subgradients encode descent directions.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions\u2014the Karush\u2013Kuhn\u2013Tucker (KKT) conditions\u2014is the central optimality framework for constrained convex optimization.</p> <p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by \u201cforces\u2019\u2019 coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>"},{"location":"convex/17_kkt/#unconstrained-convex-minimization","title":"Unconstrained Convex Minimization","text":"<p>Consider the problem  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent iteratively updates  with step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Moving opposite the gradient decreases \\(f\\).</li> <li>If the gradient is Lipschitz continuous and the step size is small enough (\\(\\alpha_k \\le 1/L\\)), then gradient descent converges to a global minimizer.</li> <li>If \\(f\\) is strongly convex, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li> </ul> <p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>"},{"location":"convex/17_kkt/#equality-constrained-problems-and-lagrange-multipliers","title":"Equality-Constrained Problems and Lagrange Multipliers","text":"<p>Now consider minimizing \\(f\\) subject to equality constraints:  </p> <p>Define the Lagrangian  where \\(\\lambda = (\\lambda_1,\\dots,\\lambda_p)\\) are the Lagrange multipliers.</p> <p>Under differentiability and regularity assumptions, a point \\(x^*\\) is optimal only if:</p> <ol> <li> <p>Primal feasibility     </p> </li> <li> <p>Stationarity     </p> </li> </ol> <p>Geometric meaning:</p> <ul> <li>The feasible set  is typically a smooth manifold.</li> <li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li> <li>The multipliers \\(\\lambda_j^*\\) weight the constraint normals to exactly cancel the objective\u2019s gradient.</li> </ul> <p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>"},{"location":"convex/17_kkt/#inequality-constraints-and-the-kkt-conditions","title":"Inequality Constraints and the KKT Conditions","text":"<p>Now consider the general convex problem:  </p> <p>Form the Lagrangian  with:</p> <ul> <li>  (equality multipliers),</li> <li>  (inequality multipliers).</li> </ul> <p>A point \\(x^*\\) with multipliers \\((\\lambda^*,\\mu^*)\\) satisfies the KKT conditions:</p>"},{"location":"convex/17_kkt/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^*) \\le 0,\\quad \\forall i, \\qquad h_j(x^*) = 0,\\quad \\forall j. \\]"},{"location":"convex/17_kkt/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^* \\ge 0,\\quad \\forall i. \\]"},{"location":"convex/17_kkt/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^*)  + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*) = 0. \\]"},{"location":"convex/17_kkt/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^*\\, g_i(x^*) = 0, \\quad i=1,\\dots,m. \\] <p>Complementary slackness expresses a clear dichotomy:</p> <ul> <li>If constraint \\(g_i(x) \\le 0\\) is inactive (strictly \\(&lt;0\\)), then it applies no force: \\(\\mu_i^* = 0\\).</li> <li>If a constraint is active at the boundary, it may exert a force: \\(\\mu_i^* &gt; 0\\), and then \\(g_i(x^*) = 0\\).</li> </ul> <p>Only active constraints can push back against the objective.</p>"},{"location":"convex/17_kkt/#slaters-condition-guaranteeing-strong-duality","title":"Slater\u2019s Condition \u2014 Guaranteeing Strong Duality","text":"<p>The KKT conditions always provide necessary conditions for optimality. For them to also be sufficient (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p> <p>For convex problems with convex \\(g_i\\) and affine \\(h_j\\), Slater\u2019s condition holds if there exists a strictly feasible point:  </p> <p>Interpretation:</p> <ul> <li>The feasible region contains an interior point.</li> <li>The constraints are not \u201ctight\u201d everywhere.</li> <li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li> </ul> <p>When Slater\u2019s condition holds:</p> <ol> <li> <p>Strong duality holds: </p> </li> <li> <p>The dual optimum is attained.</p> </li> <li> <p>The KKT conditions are both necessary and sufficient for optimality.</p> </li> </ol>"},{"location":"convex/17_kkt/#duality-gap","title":"Duality gap","text":"<p>For a primal problem with optimum \\(p^*\\) and its dual with optimum \\(d^*\\), the duality gap is  </p> <ul> <li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li> <li>Slater\u2019s condition ensures the gap is zero.</li> </ul> <p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>"},{"location":"convex/17_kkt/#geometric-and-physical-interpretation","title":"Geometric and Physical Interpretation","text":"<p>The KKT conditions describe an equilibrium of forces:</p> <ul> <li>The objective gradient pushes the point in the direction of steepest decrease.</li> <li>Active constraints push back through normal vectors scaled by multipliers.</li> <li>At optimality, these forces exactly cancel.</li> </ul> <p>Physically:</p> <ul> <li>Lagrange multipliers are \u201creaction forces\u2019\u2019 keeping a system on the constraint surface.</li> <li>In economics, they are \u201cshadow prices\u2019\u2019 indicating how much the objective would improve if a constraint were relaxed.</li> <li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li> </ul> <p>KKT theory unifies all earlier ideas\u2014convexity, gradients/subgradients, feasible regions, tangent and normal cones\u2014into one clean, general optimality framework.</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p> <ul> <li>lower bounds on the primal optimal value,</li> <li>certificates of optimality,</li> <li>interpretations of constraint \u201cprices,\u201d</li> <li>and alternative algorithmic routes to solutions.</li> </ul> <p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality \u2014 strong duality \u2014 lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>"},{"location":"convex/18_duality/#the-primal-problem","title":"The Primal Problem","text":"<p>Consider the general convex problem</p> \\[ \\begin{array}{ll} \\text{minimize} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i=1,\\dots,m, \\\\  &amp; h_j(x) = 0,\\quad j=1,\\dots,p, \\end{array} \\] <p>where:</p> <ul> <li>\\(f\\) and each \\(g_i\\) are convex,</li> <li>each equality constraint \\(h_j\\) is affine.</li> </ul> <p>The optimal value is</p> \\[ f^\\star = \\inf\\{ f(x) : g_i(x) \\le 0,\\ h_j(x)=0 \\}. \\] <p>The infimum allows for the possibility that the best value is approached but not attained.</p>"},{"location":"convex/18_duality/#why-duality","title":"Why Duality?","text":"<p>A constrained problem can be viewed as:</p> <p>minimize \\(f(x)\\) but pay a penalty whenever constraints are violated.</p> <p>If the penalties are chosen \u201ccorrectly,\u201d one can recover the original constrained problem from an unconstrained penalized problem. Dual variables \u2014 \\(\\mu_i\\) for inequalities and \\(\\lambda_j\\) for equalities \u2014 precisely encode these penalties:</p> <ul> <li>\\(\\mu_i\\) measures how costly it is to violate \\(g_i(x)\\le 0\\),</li> <li>\\(\\lambda_j\\) measures the sensitivity of the objective to relaxing \\(h_j(x)=0\\).</li> </ul> <p>Duality converts constraints into prices, and transforms geometry into algebra.</p>"},{"location":"convex/18_duality/#the-lagrangian","title":"The Lagrangian","text":"<p>The Lagrangian function is</p> \\[ L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^p \\lambda_j h_j(x), \\] <p>with:</p> <ul> <li>\\(\\mu_i \\ge 0\\) for inequality constraints,</li> <li>\\(\\lambda_j \\in \\mathbb{R}\\) unrestricted for equalities.</li> </ul> <p>Interpretation:</p> <ul> <li>If \\(\\mu_i &gt; 0\\), violating \\(g_i(x)\\le 0\\) incurs a penalty proportional to \\(\\mu_i\\).</li> <li>If \\(\\mu_i = 0\\), that constraint does not influence the Lagrangian at that point.</li> </ul>"},{"location":"convex/18_duality/#the-dual-function-lower-bounds-from-penalties","title":"The Dual Function: Lower Bounds from Penalties","text":"<p>Fix \\((\\lambda,\\mu)\\) and minimize the Lagrangian with respect to \\(x\\):</p> \\[ \\theta(\\lambda, \\mu) = \\inf_x L(x,\\lambda,\\mu). \\] <p>Because \\(g_i(x) \\le 0\\) for feasible \\(x\\) and \\(\\mu_i \\ge 0\\),</p> \\[ L(x,\\lambda,\\mu) \\le f(x), \\] <p>so taking the infimum over all \\(x\\) yields</p> \\[ \\theta(\\lambda,\\mu) \\le f^\\star. \\] <p>Thus \\(\\theta\\) always produces lower bounds on the true optimal value (weak duality).</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\) (infimum of affine functions).</li> <li>It may be \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul>"},{"location":"convex/18_duality/#the-dual-problem","title":"The Dual Problem","text":"<p>The dual problem maximizes these lower bounds:</p> \\[ \\begin{array}{ll} \\text{maximize}_{\\lambda,\\mu} &amp; \\theta(\\lambda,\\mu) \\\\ \\text{subject to} &amp; \\mu \\ge 0. \\end{array} \\] <p>Let \\(d^\\star\\) be the optimal dual value. Weak duality guarantees:</p> \\[ d^\\star \\le f^\\star. \\] <p>The dual problem is always a concave maximization, i.e., a convex optimization problem in \\((\\lambda,\\mu)\\).</p>"},{"location":"convex/18_duality/#strong-duality-and-the-duality-gap","title":"Strong Duality and the Duality Gap","text":"<p>If</p> \\[ d^\\star = f^\\star, \\] <p>we say strong duality holds. The duality gap is zero.</p>"},{"location":"convex/18_duality/#slaters-condition","title":"Slater\u2019s Condition","text":"<p>If:</p> <ul> <li>\\(g_i\\) are convex,</li> <li>\\(h_j\\) are affine,</li> <li>and there exists a \\(\\tilde{x}\\) such that </li> </ul> <p>then:</p> <ul> <li>strong duality holds (\\(f^\\star = d^\\star\\)),</li> <li>dual maximizers exist,</li> <li>KKT conditions fully characterize primal\u2013dual optimality.</li> </ul> <p>Slater\u2019s condition ensures the feasible region has interior \u2014 the constraints are not tight everywhere.</p>"},{"location":"convex/18_duality/#duality-and-the-kkt-conditions","title":"Duality and the KKT Conditions","text":"<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>"},{"location":"convex/18_duality/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^\\star) \\le 0,\\qquad h_j(x^\\star)=0. \\]"},{"location":"convex/18_duality/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^\\star \\ge 0. \\]"},{"location":"convex/18_duality/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^\\star) + \\sum_{i=1}^m \\mu_i^\\star \\nabla g_i(x^\\star) + \\sum_{j=1}^p \\lambda_j^\\star \\nabla h_j(x^\\star) = 0. \\]"},{"location":"convex/18_duality/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^\\star g_i(x^\\star) = 0,\\qquad \\forall i. \\] <p>Together these conditions ensure:</p> \\[ f(x^\\star) = \\theta(\\lambda^\\star,\\mu^\\star) = f^\\star = d^\\star. \\] <p>Geometrically, the gradients of the active constraints form a supporting hyperplane that \u201ctouches\u2019\u2019 the objective exactly at the optimum.</p>"},{"location":"convex/18_duality/#interpretation-of-dual-variables","title":"Interpretation of Dual Variables","text":"<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>"},{"location":"convex/18_duality/#shadow-prices-constraint-forces","title":"Shadow Prices / Constraint Forces","text":"<ul> <li> <p>\\(\\mu_i^\\star\\): the shadow price for relaxing \\(g_i(x)\\le 0\\).   Large \\(\\mu_i^\\star\\) means the constraint is tight and costly to relax.</p> </li> <li> <p>\\(\\lambda_j^\\star\\): sensitivity of the optimal value to perturbations of \\(h_j(x)=0\\).</p> </li> </ul>"},{"location":"convex/18_duality/#ml-interpretations","title":"ML Interpretations","text":"<ul> <li>Support Vector Machines: dual variables select support vectors (only points with \\(\\mu_i^\\star &gt; 0\\) matter).</li> <li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li> <li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li> </ul> <p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-multi-objective-convex-optimization","title":"Chapter 10: Multi-Objective Convex Optimization","text":"<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p> <ul> <li>accuracy vs. regularity,</li> <li>loss vs. fairness,</li> <li>return vs. risk,</li> <li>reconstruction vs. compression,</li> <li>energy use vs. performance.</li> </ul> <p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p> <p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>"},{"location":"convex/18a_pareto/#classical-optimality-one-objective","title":"Classical Optimality (One Objective)","text":"<p>In standard convex optimization, we solve:</p> \\[ x^* \\in \\arg\\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex and \\(\\mathcal{X}\\) is convex. In this setting, it is natural to speak of the minimizer \u2014 or set of minimizers \u2014 because the task is governed by a single quantitative measure.</p> <p>However, when multiple objectives \\((f_1,\\dots,f_k)\\) must be minimized simultaneously, a single \u201cbest\u201d point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#multi-objective-convex-optimization","title":"Multi-Objective Convex Optimization","text":"<p>A multi-objective optimization problem takes the form</p> \\[ \\min_{x \\in \\mathcal{X}} F(x) = (f_1(x), \\dots, f_k(x)), \\] <p>where each \\(f_i\\) is convex. This framework appears in many ML and statistical tasks:</p> Domain Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs. complexity Fair ML Loss Fairness metric Utility vs. fairness Portfolio Return Risk Profit vs. stability Autoencoders Reconstruction KL divergence Fidelity vs. disentanglement <p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes Pareto efficiency.</p>"},{"location":"convex/18a_pareto/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"convex/18a_pareto/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A point \\(x^*\\) is Pareto optimal if there is no other \\(x\\) such that</p> \\[ f_i(x) \\le f_i(x^*)\\quad \\forall i, \\] <p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>"},{"location":"convex/18a_pareto/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no feasible point satisfies</p> \\[ f_i(x) &lt; f_i(x^*)\\quad \\forall i. \\] <p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>"},{"location":"convex/18a_pareto/#geometric-view","title":"Geometric View","text":"<p>For two objectives \\((f_1, f_2)\\), the feasible set in objective space is a region in \\(\\mathbb{R}^2\\). Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p> <ul> <li>Points on the frontier are the best achievable trade-offs.</li> <li>Points above or inside the region are dominated and thus suboptimal.</li> </ul> <p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>"},{"location":"convex/18a_pareto/#scalarization-turning-many-objectives-into-one","title":"Scalarization: Turning Many Objectives into One","text":"<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>"},{"location":"convex/18a_pareto/#weighted-sum-scalarization","title":"Weighted-Sum Scalarization","text":"\\[ \\min_{x \\in \\mathcal{X}} \\sum_{i=1}^k w_i f_i(x), \\qquad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights encode relative importance.  </li> <li>Varying \\(w\\) traces (part of) the Pareto frontier.  </li> <li>When \\(f_i\\) and \\(\\mathcal{X}\\) are convex, this method recovers the convex portion of the frontier.</li> </ul>"},{"location":"convex/18a_pareto/#-constraint-method","title":"\u03b5-Constraint Method","text":"\\[ \\min_{x} \\ f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\ \\ i = 2,\\dots,k. \\] <ul> <li>Here the tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Each choice of \\(\\varepsilon\\) yields a different Pareto-efficient point.</li> </ul> <p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>"},{"location":"convex/18a_pareto/#duality-connection","title":"Duality Connection","text":"<p>Scalarization has a tight relationship with duality (Chapter 9):</p> <ul> <li>Weights \\(w_i\\) in a weighted sum act like dual variables.</li> <li>Regularization parameters (e.g., the \\(\\lambda\\) in L2 or L1 regularization) correspond to dual multipliers.</li> <li>Moving along \\(\\lambda\\) traces the Pareto frontier between data fit and model complexity.</li> </ul> <p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>"},{"location":"convex/18a_pareto/#examples-and-applications","title":"Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1: Regularized Least Squares","text":"<p>Consider</p> \\[ f_1(x) = \\|Ax - b\\|_2^2,\\qquad  f_2(x) = \\|x\\|_2^2. \\] <p>Two scalarizations:</p> <ol> <li> <p>Weighted:     </p> </li> <li> <p>\u03b5-constraint:     </p> </li> </ol> <p>\\(\\lambda\\) and \\(\\tau\\) trace the same Pareto curve \u2014 the classical bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2: Portfolio Optimization (Risk\u2013Return)","text":"<p>Let \\(w\\) be portfolio weights, \\(\\mu\\) expected returns, and \\(\\Sigma\\) the covariance matrix. Objectives:</p> \\[ f_1(w) = -\\mu^\\top w, \\qquad f_2(w) = w^\\top \\Sigma w. \\] <p>Weighted scalarization:</p> \\[ \\min_w \\ -\\alpha \\mu^\\top w + (1-\\alpha) w^\\top \\Sigma w, \\quad 0 \\le \\alpha \\le 1. \\] <p>Varying \\(\\alpha\\) recovers the efficient frontier of Modern Portfolio Theory.</p>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-in-ml","title":"Example 3: Fairness\u2013Accuracy in ML","text":"\\[ \\min_\\theta \\ \\mathbb{E}[\\ell(y, f_\\theta(x))] \\quad \\text{s.t. } D(f_\\theta(x),y) \\le \\varepsilon, \\] <p>where \\(D\\) is a fairness metric. Scalarized form:</p> \\[ \\min_\\theta\\  \\mathbb{E}[\\ell(y, f_\\theta(x))] + \\lambda D(f_\\theta(x), y). \\] <p>Tuning \\(\\lambda\\) walks across the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-vae","title":"Example 4: Variational Autoencoders and \u03b2-VAE","text":"<p>The ELBO is:</p> \\[ \\mathbb{E}_{q(z)}[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z)). \\] <p>Objectives:</p> <ul> <li>Reconstruction fidelity,</li> <li>Latent simplicity.</li> </ul> <p>\u03b2-VAE scalarization:</p> \\[ \\max_q \\ \\mathbb{E}[\\log p(x|z)] - \\beta \\,\\mathrm{KL}(q(z)\\|p(z)). \\] <p>\\(\\beta\\) controls the trade-off between reconstruction and disentanglement \u2014 a Pareto frontier in latent space.</p> <p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-balancing-fit-and-complexity","title":"Chapter 11:  Balancing Fit and Complexity","text":"<p>Most real-world learning and estimation problems must balance two competing goals:</p> <ol> <li>Fit the observed data well, and  </li> <li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li> </ol> <p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>"},{"location":"convex/18b_regularization/#motivation-fit-vs-complexity","title":"Motivation: Fit vs. Complexity","text":"<p>Suppose we wish to estimate parameters \\(x\\) from data via a loss function \\(f(x)\\). If the data are noisy or the model is high-dimensional, solutions minimizing \\(f\\) alone may be unstable or overly complex. We introduce a regularizer \\(R(x)\\), typically convex, to encourage desirable structure:</p> \\[ \\min_{x} \\; f(x) + \\lambda R(x), \\qquad \\lambda &gt; 0. \\] <ul> <li>\\(f(x)\\): measures data misfit (e.g., squared loss, logistic loss).  </li> <li>\\(R(x)\\): penalizes complexity (e.g., \\(\\ell_1\\) norm for sparsity, \\(\\ell_2\\) norm for smoothness).  </li> <li>\\(\\lambda\\): controls the trade-off.<ul> <li>Small \\(\\lambda\\): excellent data fit, potentially overfitting.  </li> <li>Large \\(\\lambda\\): simpler model, potentially underfitting.</li> </ul> </li> </ul> <p>This is a scalarized multi-objective optimization problem of \\((f, R)\\).</p>"},{"location":"convex/18b_regularization/#bicriterion-optimization-and-the-pareto-frontier","title":"Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization corresponds to the bicriterion objective:</p> \\[ \\min_{x} \\; (f(x), R(x)). \\] <p>A point \\(x^*\\) is Pareto optimal if there is no feasible \\(x\\) such that:  with strict inequality in at least one component.</p> <p>For convex \\(f\\) and \\(R\\):</p> <ul> <li>Every \\(\\lambda \\ge 0\\) yields a Pareto-optimal point,</li> <li>The mapping from \\(\\lambda\\) to constraint level \\(R(x^*)\\) is monotone,</li> <li>The Pareto frontier is convex and can be traced continuously by varying \\(\\lambda\\).</li> </ul> <p>Thus, tuning \\(\\lambda\\) moves the solution along the fit\u2013complexity frontier.</p>"},{"location":"convex/18b_regularization/#why-control-the-size-of-the-solution","title":"Why Control the Size of the Solution?","text":"<p>Inverse problems such as \\(Ax \\approx b\\) are often ill-posed or ill-conditioned:</p> <ul> <li>Small noise in \\(b\\) may cause large variability in the solution \\(x\\).  </li> <li>If \\(A\\) is rank-deficient or nearly singular, infinitely many solutions exist.</li> </ul> <p>Example: ridge regression</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2. \\] <p>The optimality condition is</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b. \\] <p>Benefits of L2 regularization:</p> <ul> <li>\\(A^\\top A + \\lambda I\\) becomes positive definite for any \\(\\lambda &gt; 0\\),  </li> <li>the solution becomes unique and stable,  </li> <li>small singular directions of \\(A\\) are suppressed.</li> </ul> <p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>"},{"location":"convex/18b_regularization/#constrained-vs-penalized-formulations","title":"Constrained vs. Penalized Formulations","text":"<p>Regularized problems can be expressed equivalently as constrained problems:</p> \\[ \\min_x f(x)  \\quad \\text{s.t. } R(x) \\le t. \\] <p>The Lagrangian is</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda (R(x) - t), \\qquad \\lambda \\ge 0. \\] <p>The penalized form</p> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>is the dual of the constrained form. Under convexity and Slater\u2019s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*),  \\] \\[ R(x^*) \\le t,\\qquad \\lambda^* \\ge 0,\\qquad \\lambda^*(R(x^*) - t) = 0. \\] <p>Here:</p> <ul> <li>If \\(R(x^*) &lt; t\\), then \\(\\lambda^* = 0\\).  </li> <li>If \\(\\lambda^* &gt; 0\\), then \\(R(x^*) = t\\) (constraint active).</li> </ul> <p>Thus \\(\\lambda\\) is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>"},{"location":"convex/18b_regularization/#common-regularizers-and-their-effects","title":"Common Regularizers and Their Effects","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"\\[ R(x) = \\|x\\|_2^2. \\] <ul> <li>Smooth and strongly convex.  </li> <li>Shrinks coefficients uniformly.  </li> <li>Improves conditioning.  </li> <li>MAP interpretation: Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"\\[ R(x) = \\|x\\|_1 = \\sum_i |x_i|. \\] <ul> <li>Convex but not differentiable \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball has corners aligned with coordinate axes, encouraging zeros in \\(x\\).  </li> <li>Proximal operator (soft-thresholding):</li> </ul> \\[ \\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(v) = \\operatorname{sign}(v)\\,\\max(|v|-\\tau, 0). \\] <ul> <li>MAP interpretation: Laplace prior.</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"\\[ R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2. \\] <ul> <li>Combines sparsity with numerical stability.  </li> <li>Useful with correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2-structured-regularizers","title":"(d) Beyond L1/L2: Structured Regularizers","text":"Regularizer Formula Effect Tikhonov \\(\\|Lx\\|_2^2\\) smoothness via operator \\(L\\) Total Variation \\(\\|\\nabla x\\|_1\\) piecewise-constant signals/images Group Lasso \\(\\sum_g \\|x_g\\|_2\\) structured sparsity across groups Nuclear Norm \\(\\|X\\|_* = \\sum_i \\sigma_i\\) low-rank matrices <p>Each regularizer defines a geometry for the solution \u2014 ellipsoids, diamonds, polytopes, or spectral shapes.</p>"},{"location":"convex/18b_regularization/#choosing-the-regularization-parameter-lambda","title":"Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-Off Behavior","text":"<ul> <li>\\(\\lambda \\downarrow\\): favors small training error, high variance.  </li> <li>\\(\\lambda \\uparrow\\): favors simplicity, higher bias.  </li> </ul> <p>\\(\\lambda\\) selects a point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#b-cross-validation","title":"(b) Cross-Validation","text":"<p>The most common practice:</p> <ol> <li>Split data into folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining fold.  </li> <li>Choose \\(\\lambda\\) minimizing average validation error.</li> </ol> <p>Guidelines:</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>Use time-aware CV for dependent data.  </li> <li>Use the \u201cone-standard-error rule\u201d for simpler models.</li> </ul>"},{"location":"convex/18b_regularization/#c-other-selection-methods","title":"(c) Other Selection Methods","text":"<ul> <li>Information criteria (AIC, BIC) for sparsity.  </li> <li>L-curve or discrepancy principle in inverse problems.  </li> <li>Regularization paths: computing \\(x^*(\\lambda)\\) for many \\(\\lambda\\).</li> </ul>"},{"location":"convex/18b_regularization/#algorithmic-view","title":"Algorithmic View","text":"<p>Most regularized problems have the form:</p> \\[ \\min_x \\ f(x) + R(x), \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex (possibly nonsmooth).</p> <p>Common algorithms:</p> Method Idea When Useful Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), proximal step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update coordinates cyclically Lasso, Elastic Net ADMM Split problem to exploit structure Large-scale or distributed settings <p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal \\(O(1/k^2)\\) rate for smooth+convex problems.</p>"},{"location":"convex/18b_regularization/#bayesian-interpretation","title":"Bayesian Interpretation","text":"<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p> <p>Linear model:</p> \\[ b = Ax + \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I). \\] <p>With prior \\(x \\sim p(x)\\), MAP estimation solves:</p> \\[ \\min_x \\ \\frac{1}{2\\sigma^2}\\|Ax - b\\|_2^2 - \\log p(x). \\] <p>Examples:</p> <ul> <li>Gaussian prior \\(p(x) \\propto e^{-\\|x\\|_2^2 / (2\\tau^2)}\\)   \u2192 L2 penalty with \\(\\lambda = \\sigma^2/(2\\tau^2)\\).  </li> <li>Laplace prior   \u2192 L1 penalty and sparse MAP estimate.</li> </ul> <p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p> <p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#problem-classes-vs-method-classes","title":"Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#first-order-methods-gradient-descent","title":"First-order methods: Gradient descent","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  </p> <p>Smoothness lets us control step sizes.</p> <p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). Typical choices:</p> <ul> <li>constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known,</li> <li>backtracking line search when \\(L\\) is unknown,</li> <li>diminishing step sizes in some settings.</li> </ul> <p>Derivation: </p> <p>Around \\(x_t\\), we can approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Convergence: For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p> <p>When to use gradient descent:</p> <ul> <li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li> <li>You can compute gradients cheaply.</li> <li>You only need moderate accuracy.</li> <li>Memory constraints rule out storing or factoring Hessians.</li> </ul>"},{"location":"convex/19_optimizationalgo/#accelerated-first-order-methods","title":"Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#nesterov-acceleration","title":"Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences \\(x_k\\) and \\(y_k\\):</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra momentum term \\(\\beta_k (x_{k+1}-x_k)\\) uses past iterates to \u201clook ahead\u201d and can significantly accelerate convergence.</p> <p>Convergece: For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives.</p> <ul> <li>Acceleration is effective for well-behaved smooth convex problems.</li> <li>It can be more sensitive to step size and noise than plain gradient descent.</li> <li>Variants such as FISTA apply acceleration in the composite setting \\(f + R\\).</li> </ul> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#subgradient-methods","title":"Subgradient Methods","text":"<p>Even when \\(f\\) is not differentiable, we can minimise it using subgradient descent:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k, \\qquad g_k \\in \\partial f(x_k). \\] <p>Key features:</p> <ul> <li>Requires only a subgradient (no differentiability needed).</li> <li>Works for any convex function.</li> <li>Stepsizes must typically decrease (e.g. , ).</li> <li>Guaranteed convergence for convex \\(f\\), but generally slow.</li> </ul>"},{"location":"convex/19_optimizationalgo/#convergence-rates-worst-case","title":"Convergence rates (worst case)","text":"<ul> <li>Smooth convex gradient descent: \\(O(1/k)\\) or \\(O(1/k^2)\\).  </li> <li>Nonsmooth subgradient descent: </li> </ul> <p>This slower rate reflects the lack of curvature information at kinks.</p>"},{"location":"convex/19_optimizationalgo/#proximal-and-smoothed-alternatives","title":"Proximal and Smoothed Alternatives","text":"<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>"},{"location":"convex/19_optimizationalgo/#1-proximal-methods","title":"(1) Proximal methods","text":"<p>For a convex function \\(f\\), the proximal operator is  </p> <p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p> <ul> <li>  regularisation,</li> <li>indicator functions of convex sets,</li> <li>total variation penalties.</li> </ul> <p>They achieve faster and more stable convergence than basic subgradient descent.</p>"},{"location":"convex/19_optimizationalgo/#2-smoothing-techniques","title":"(2) Smoothing techniques","text":"<p>Many nonsmooth convex functions have smooth approximations:</p> <ul> <li>Replace  with the Huber loss.</li> <li>Replace  with softplus.</li> <li>Replace  with log-sum-exp, a smooth convex approximation.</li> </ul> <p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>"},{"location":"convex/19_optimizationalgo/#steepest-descent-method","title":"Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization. Gradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems. The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p> <p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#conjugate-gradient-method-fast-optimization-for-quadratic-objectives","title":"Conjugate Gradient Method \u2014 Fast Optimization for Quadratic Objectives","text":"<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p> <p>CG is a first-order method that behaves like a second-order method for quadratics.</p> <p>For a quadratic objective function:</p> \\[ f(x) = \\tfrac12 x^\\top A x - b^\\top x  \\] <p>with \\(A \\succ 0\\), the level sets are ellipses shaped by the eigenvalues of \\(A\\). If \\(A\\) is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p> <p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors \\(p_i, p_j\\) are said to be A-conjugate if</p> \\[ p_i^\\top A p_j = 0. \\] <p>This is orthogonality measured in the geometry induced by the Hessian \\(A\\). Why is this useful?</p> <ul> <li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of \\(A\\).</li> <li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li> <li>After \\(n\\) mutually A-conjugate directions, all curvature directions are resolved \u2192 exact solution.</li> </ul> <p>In contrast, gradient descent repeatedly re-corrects previous progress.</p> <p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system \\(Ax = b\\). Let</p> \\[ r_0 = b - A x_0, \\qquad p_0 = r_0. \\] <p>For \\(k = 0,1,2,\\dots\\):</p> <ol> <li> <p>Step size     </p> </li> <li> <p>Update iterate     </p> </li> <li> <p>Update residual (negative gradient)     </p> </li> <li> <p>Direction scaling     </p> </li> <li> <p>New conjugate direction     </p> </li> </ol> <p>Stop when \\(\\|r_k\\|\\) is below tolerance.</p> <p>Every new direction \\(p_{k+1}\\) is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p> <p>Why CG Is Fast: For an \\(n\\)-dimensional quadratic, CG solves the problem in at most \\(n\\) iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in \\(O(\\sqrt{\\kappa})\\) iterations, where \\(\\kappa = \\lambda_{\\max}/\\lambda_{\\min}\\) is the condition number. The convergence bound in the A-norm is:</p> \\[ \\|x_k - x^\\star\\|_A \\le  2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k  \\|x_0 - x^\\star\\|_A. \\] <p>This is dramatically better than the \\(O(1/k)\\) rate of gradient descent.</p> <p>CG is ideal when:</p> <ul> <li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix \\(A\\).</li> <li>\\(A\\) is large and sparse or available as a matrix\u2013vector product.</li> <li>You cannot form or store \\(A^{-1}\\) or even the full matrix \\(A\\).</li> <li>You want a Hessian-aware method but cannot afford Newton\u2019s method.</li> </ul> <p>Typical scenarios:</p> Application Why CG fits Large linear systems \\(A x = b\\) Only requires \\(A p\\), not factorization. Ridge regression Normal equations form an SPD matrix. Kernel ridge regression Solves \\((K+\\lambda I)\\alpha = y\\) efficiently. Newton steps in ML Inner solver for Hessian systems without forming Hessian. PDEs and scientific computing Sparse SPD matrices, ideal for CG. <p>Assumptions Required for CG: To guarantee correctness of linear CG, we require:</p> <ul> <li>\\(A\\) is symmetric</li> <li>\\(A\\) is positive definite</li> <li>Objective is strictly convex quadratic</li> <li>Arithmetic is exact (for the finite-step guarantee)</li> </ul> <p>If the function is not quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p> <p>Practical Notes:</p> <ul> <li>You only need matrix\u2013vector products \\(Ap\\).  </li> <li>Storage cost is \\(O(n)\\).  </li> <li>Preconditioning (replacing the system with \\(M^{-1} A\\)) improves conditioning and accelerates convergence dramatically.  </li> <li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li> </ul> <p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>"},{"location":"convex/19_optimizationalgo/#newtons-method-and-second-order-methods","title":"Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p> <p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex \\(f\\), Newton\u2019s method achieves quadratic convergence\u2014dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order algorithms.</p> <p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton\u2019s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>"},{"location":"convex/19_optimizationalgo/#solving-the-newton-system","title":"Solving the Newton System","text":"<p>Each iteration requires solving</p> \\[ H \\,\\Delta x = -g, \\qquad H = \\nabla^2 f(x), \\;\\; g = \\nabla f(x). \\] <p>If \\(H\\) is symmetric positive definite, a Cholesky factorization</p> \\[ H = L L^\\top \\] <p>allows efficient and numerically stable solution via two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^\\top \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids forming \\(H^{-1}\\) explicitly.</p> <p>The Newton decrement:</p> \\[ \\lambda(x) = \\|L^{-1} g\\|_2 \\] <p>gauges proximity to the optimum and provides a natural stopping criterion: \\(\\lambda(x)^2/2 &lt; \\varepsilon\\).</p> <p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs \\(\\approx (1/3)n^3\\) operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton\u2019s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix\u2013free iterative methods.</p>"},{"location":"convex/19_optimizationalgo/#gaussnewton-method","title":"Gauss\u2013Newton Method","text":"<p>The Gauss\u2013Newton method is a specialization of Newton\u2019s method for nonlinear least squares problems</p> \\[ f(x) = \\tfrac12 \\| r(x) \\|^2, \\] <p>where \\(r(x)\\) is a vector of residual functions and a nonlinear function of \\(x\\) and \\(J\\) is its Jacobian. Newton\u2019s Hessian decomposes as</p> \\[ \\nabla^2 f(x) = J^\\top J \\;+\\; \\sum_i r_i(x)\\, \\nabla^2 r_i(x). \\] <p>The second term involves the curvature of the residuals. When \\(r(x)\\) is approximately linear near the optimum, this term is small. Gauss\u2013Newton drops it, giving the approximation</p> \\[ \\nabla^2 f(x) \\approx J^\\top J, \\] <p>leading to the Gauss\u2013Newton step:</p> \\[ (J^\\top J)\\, \\Delta = -J^\\top r. \\] <p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg\u2013Marquardt method adds a damping term,</p> \\[ (J^\\top J + \\lambda I)\\, \\Delta = -J^\\top r, \\] <p>which interpolates smoothly between  </p> <ul> <li>gradient descent (large \\(\\lambda\\)), and  </li> <li>Gauss\u2013Newton (small \\(\\lambda\\)).</li> </ul> <p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss\u2013Newton and Levenberg\u2013Marquardt are highly effective when the residuals are nearly linear\u2014common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning\u2014yielding fast convergence without the expense of full second derivatives.</p>"},{"location":"convex/19_optimizationalgo/#quasi-newton-methods","title":"Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p> <p>When to use Newton or quasi-Newton methods:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#projected-gradient-descent","title":"Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#proximal-gradient-forwardbackward-splitting","title":"Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#penalties-barriers-and-interior-point-methods","title":"Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\): Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#penalty-methods","title":"Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-basic-penalty-method-quadratic-or-general-penalization","title":"Algorithm: Basic Penalty Method (Quadratic or General Penalization)","text":"<p>Goal:  Solve </p> <p>Penalty formulation:  where  </p> <ul> <li>\\(\\phi(r) = 0\\) if \\(r \\le 0\\),  </li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (e.g., \\(\\phi(r)=\\max\\{0,r\\}^2\\)),  </li> <li>\\(\\rho &gt; 0\\) is the penalty weight.</li> </ul> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>constraints \\(g_i(x)\\) </li> <li>penalty function \\(\\phi\\) </li> <li>initial point \\(x_0\\) </li> <li>initial penalty parameter \\(\\rho_0 &gt; 0\\) </li> <li>penalty update factor \\(\\gamma &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose \\(x_0\\), \\(\\rho_0 &gt; 0\\).  </li> <li>For \\(k = 0, 1, 2, \\dots\\):  <ol> <li>Solve the penalized subproblem  \\(x_{k+1} = \\arg\\min_x F_{\\rho_k}(x)\\) using Newton\u2019s method, gradient descent, quasi-Newton, etc.  </li> <li>Check feasibility / stopping:  If \\(\\max_i g_i(x_{k+1}) \\le \\varepsilon, \\quad   \\|x_{k+1} - x_k\\| \\le \\varepsilon\\)  stop and return \\(x_{k+1}\\).  </li> <li>Increase penalty parameter  \\(\\rho_{k+1} = \\gamma\\, \\rho_k\\)   with typical \\(\\gamma \\in [5,10]\\).  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#barrier-methods","title":"Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-barrier-method-logarithmic-barrier-interior-approximation","title":"Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)","text":"<p>Goal: Solve the constrained problem </p> <p>Logarithmic barrier:  defined only for strictly feasible points \\(g_i(x)&lt;0\\).</p> <p>Barrier subproblem:  where \\(t&gt;0\\) is the barrier parameter.</p> <p>As \\(t \\to \\infty\\), minimizers of \\(F_t\\) approach the constrained optimum.</p> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>barrier function \\(b(x)\\) </li> <li>strictly feasible starting point \\(x_0\\) (\\(g_i(x_0) &lt; 0\\))  </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>barrier growth factor \\(\\mu &gt; 1\\) (often \\(\\mu = 10\\))  </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose strictly feasible \\(x_0\\), and pick \\(t_0 &gt; 0\\).  </li> <li>For \\(k = 0,1,2,\\dots\\):  <ol> <li>Centering step (inner loop):  Solve the barrier subproblem    Typically use Newton\u2019s method (damped) on \\(F_{t_k}\\).  Stop when the Newton decrement satisfies  \\(\\lambda(x_{k+1})^2/2 \\le \\varepsilon\\)</li> <li>Optimality / stopping test:    If  \\(\\frac{m}{t_k} \\le \\varepsilon,\\)   then \\(x_{k+1}\\) is an \\(\\varepsilon\\)-approximate solution of the original constrained problem; stop and return \\(x_{k+1}\\).  </li> <li>Increase barrier parameter:  \\(t_{k+1} = \\mu\\, t_k,\\)   which tightens the approximation and moves closer to the boundary.  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#interior-point-methods","title":"Interior-point methods","text":"<p>Interior-point methods combine barrier functions with Newton\u2019s method to solve general convex programs:</p> <ul> <li>They maintain strict feasibility throughout.</li> <li>Each iteration solves a Newton system for the barrier-augmented objective.</li> <li>They naturally generate primal\u2013dual pairs and duality gap estimates.</li> <li>Under standard assumptions (e.g., Slater\u2019s condition), they converge in a predictable number of iterations.</li> </ul> <p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>"},{"location":"convex/19_optimizationalgo/#algorithm-primaldual-interior-point-method-for-convex-inequality-constraints","title":"Algorithm: Primal\u2013Dual Interior-Point Method (for convex inequality constraints)","text":"<p>We consider the problem  </p> <p>Introduce Lagrange multipliers \\(\\lambda \\ge 0\\). The KKT conditions are  </p> <p>Interior-point methods enforce the relaxed condition  which keeps iterates strictly feasible.</p>"},{"location":"convex/19_optimizationalgo/#inputs","title":"Inputs","text":"<ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>initial primal point \\(x_0\\) with \\(g_i(x_0)&lt;0\\) </li> <li>initial dual variable \\(\\lambda_0 &gt; 0\\) </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>growth factor \\(\\mu &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul>"},{"location":"convex/19_optimizationalgo/#procedure","title":"Procedure","text":"<ol> <li> <p>Choose strictly feasible \\(x_0\\), positive \\(\\lambda_0\\), and \\(t_0\\).</p> </li> <li> <p>For \\(k = 0,1,2,\\dots\\):</p> <p>(a) Form the perturbed KKT system.  Solve for the Newton direction \\((\\Delta x, \\Delta \\lambda)\\):</p> <p> </p> <p>(b) Line search to keep strict feasibility. Choose the maximum \\(\\alpha\\in(0,1]\\) such that:</p> <ul> <li>\\(g_i(x + \\alpha \\Delta x) &lt; 0\\),</li> <li>\\(\\lambda + \\alpha \\Delta \\lambda &gt; 0\\).</li> </ul> <p>(c) Update: \\(x \\leftarrow x + \\alpha \\Delta x,    \\qquad  \\lambda \\leftarrow \\lambda + \\alpha \\Delta \\lambda.\\)</p> <p>(d) Check duality gap: \\(\\text{gap} = - g(x)^\\top \\lambda\\) If \\(\\text{gap} \\le \\varepsilon\\), stop.</p> <p>(e) Increase barrier parameter \\(t \\leftarrow \\mu t.\\)</p> </li> <li> <p>Return \\(x\\).</p> </li> </ol>"},{"location":"convex/19_optimizationalgo/#choosing-the-right-method-in-practice","title":"Choosing the right method in practice","text":"<p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19_optimizationalgo/#mental-map","title":"Mental Map","text":"<pre><code>                Algorithms for Convex Optimization\n     Turning convex geometry + optimality conditions into solvers\n                              \u2502\n                              \u25bc\n            Core question: how do we actually solve problems?\n   Choose an algorithm class that matches problem structure + scale\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Problem Classes  \u2192  Method Classes                         \u2502\n     \u2502 Smooth unconstrained:        GD, Acceleration, Newton      \u2502\n     \u2502 Smooth + simple constraints: Projected gradient            \u2502\n     \u2502 Composite (smooth+nonsmooth): Proximal/coordinate/splitting\u2502\n     \u2502 General constrained convex:  Interior-point / primal\u2013dual  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Core: Gradient Descent                        \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k \u2207f(x_k)                               \u2502\n     \u2502 Requirements: convex + L-smooth                           \u2502\n     \u2502 - Step size from L or line search                         \u2502\n     \u2502 - Rate (smooth convex): O(1/k)                            \u2502\n     \u2502 Geometry: move opposite supporting hyperplane slope       \n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Acceleration (Nesterov / Momentum)                        \u2502\n     \u2502 Two sequences: gradient at y_k + extrapolation to y_{k+1} \u2502\n     \u2502 - Rate (smooth convex): O(1/k^2)                          \u2502\n     \u2502 - Best possible for gradient-only methods                 \u2502\n     \u2502 Tradeoff: faster but more sensitive to tuning/noise       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Nonsmooth First-Order: Subgradient Descent                   \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k g_k,   g_k \u2208 \u2202f(x_k)                     \u2502\n     \u2502 - Works for convex nonsmooth objectives                      \u2502\n     \u2502 - Needs diminishing step sizes                               \u2502\n     \u2502 - Worst-case rate: O(1/\u221ak)                                   \u2502\n     \u2502 Use when: only subgradients available / simple implementation\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Fixing Nonsmoothness: Proximal &amp; Smoothing                \u2502\n     \u2502 Prox operator: prox_{\u03b1R}(y)=argmin_x R(x)+(1/2\u03b1)\u2016x\u2212y\u2016\u00b2    \u2502\n     \u2502 - Handles \u2113\u2081, indicators, TV, etc.                        \u2502\n     \u2502 Smoothing: Huber / softplus / log-sum-exp                 \u2502\n     \u2502 - Enables fast smooth methods                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Steepest Descent = Gradient Descent under a chosen norm     \u2502\n     \u2502 \u0394x_nsd = argmin_{\u2016v\u2016=1} \u2207f(x)\u1d40v                             \u2502\n     \u2502 - Dual norm controls gradient magnitude                     \u2502\n     \u2502 - \u2113\u2082 \u2192 standard GD                                          \u2502\n     \u2502 - Quadratic norm \u2192 preconditioned GD / Newton-like          \u2502\n     \u2502 - \u2113\u2081 \u2192 coordinate-like steps                                \u2502\n     \u2502 Purpose: change geometry to reduce anisotropy / improve rate\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Quadratic Structure: Conjugate Gradient (CG)                \u2502\n     \u2502 Solve: \u00bdx\u1d40Ax \u2212 b\u1d40x, A\u227b0  (equivalently Ax=b)                \u2502\n     \u2502 - Builds A-conjugate directions (Hessian-orthogonal)        \u2502\n     \u2502 - Uses only matrix\u2013vector products Ap                       \u2502\n     \u2502 - Converges in \u2264 n steps (exact arithmetic)                 \u2502\n     \u2502 - Practical iterations ~ O(\u221a\u03ba) with \u03ba=\u03bb_max/\u03bb_min           \u2502\n     \u2502 - Preconditioning is key for speed                          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Methods: Newton &amp; Variants                     \u2502\n     \u2502 Newton step:  \u2207\u00b2f(x_k) d = \u2212\u2207f(x_k)                         \u2502\n     \u2502 - Quadratic local model \u2192 fast local convergence            \u2502\n     \u2502 - Needs line search / trust region for robustness           \u2502\n     \u2502 Gauss\u2013Newton / Levenberg\u2013Marquardt: least-squares structure \u2502\n     \u2502 Quasi-Newton (BFGS/L-BFGS): Hessian inverse approximations  \u2502\n     \u2502 Use when: moderate dimension or efficient linear solves     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Handling Constraints: Projection &amp; Proximal Splitting        \u2502\n     \u2502 Projected GD:  y=x\u2212\u03b1\u2207f(x);  x\u207a=\u03a0_X(y)                        \u2502\n     \u2502 Proximal gradient: y=x\u2212\u03b1\u2207f(x); x\u207a=prox_{\u03b1R}(y)               \u2502\n     \u2502 Unification: indicator_R(X) \u21d2 prox = projection              \u2502\n     \u2502 Use when: constraints/regularizers have cheap prox/project   \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 General Inequalities: Penalties \u2192 Barriers \u2192 Interior-Point \u2502\n     \u2502 Penalty: f(x)+\u03c1 \u03a3 \u03c6(g_i(x))                                 \u2502\n     \u2502 Barrier: tf(x) \u2212 \u03a3 log(\u2212g_i(x))  (strict feasibility)       \u2502\n     \u2502 Interior-point: Newton steps on (primal\u2013dual) perturbed KKT \u2502\n     \u2502 - Few iterations, high accuracy, solver backbone for LP/QP  \u2502\n     \u2502 - Uses duality gap certificates                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                 Practical selection (the decision logic)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Huge-scale smooth \u2192 GD / accelerated / L-BFGS               \u2502\n     \u2502 Moderate smooth, high accuracy \u2192 Newton / quasi-Newton      \u2502\n     \u2502 Simple constraints \u2192 projected gradient                     \u2502\n     \u2502 Smooth + nonsmooth regularizer \u2192 proximal gradient / FISTA  \u2502\n     \u2502 General constraints \u2192 interior-point / primal\u2013dual          \u2502\n     \u2502 Quadratic / SPD linear systems \u2192 CG (+ preconditioning)     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever variables must satisfy exact relationships, such as conservation laws, normalization, or linear invariants. In this chapter we focus on problems of the form</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>where \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is (typically convex and differentiable) and \\(A \\in \\mathbb{R}^{p \\times n}\\) has rank \\(p\\). This linear equality structure appears in constrained least squares, portfolio optimization, and many ML formulations that impose exact balance or normalization constraints.</p>"},{"location":"convex/19a_optimization_constraints/#geometric-view-optimization-on-an-affine-manifold","title":"Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(\\operatorname{rank}(A) = p\\), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine subspace of \\(\\mathbb{R}^n\\): a \u201cflat\u201d lower-dimensional plane embedded in the ambient space. Optimization now happens along this plane, not in all of \\(\\mathbb{R}^n\\). Any feasible direction \\(d\\) must keep us in \\(\\mathcal{X}\\), so it must satisfy</p> \\[ A (x + d) = b \\quad \\Rightarrow \\quad A d = 0. \\] <p>Thus, feasible directions lie in the null space of \\(A\\):</p> \\[ \\mathcal{D}_{\\text{feas}} = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\} = \\operatorname{Null}(A). \\] <p>At an optimal point \\(x^\\star \\in \\mathcal{X}\\), moving in any feasible direction \\(d\\) cannot decrease \\(f\\). For differentiable \\(f\\), this means</p> \\[ \\nabla f(x^\\star)^\\top d \\ge 0 \\quad \\text{for all } d \\text{ with } A d = 0. \\] <p>Equivalently, \\(\\nabla f(x^\\star)\\) must be orthogonal to all feasible directions, i.e. it lies in the row space of \\(A\\). Therefore there exists a vector of Lagrange multipliers \\(\\nu^\\star\\) such that</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star. \\] <p>This is the basic geometric optimality condition: at the optimum, the gradient of \\(f\\) is a linear combination of the constraint normals (rows of \\(A\\)), and every feasible direction is orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#lagrange-function-and-kkt-system","title":"Lagrange Function and KKT System","text":"<p>The Lagrangian for the equality-constrained problem is</p> \\[ \\mathcal{L}(x,\\nu) = f(x) + \\nu^\\top (A x - b), \\] <p>where \\(\\nu \\in \\mathbb{R}^p\\) are Lagrange multipliers. The first-order (KKT) conditions for a point \\((x^\\star,\\nu^\\star)\\) to be optimal are</p> \\[ \\begin{aligned} \\nabla_x \\mathcal{L}(x^\\star,\\nu^\\star) &amp;= \\nabla f(x^\\star) + A^\\top \\nu^\\star = 0  \\quad &amp;\\text{(stationarity)},\\\\ A x^\\star &amp;= b  \\quad &amp;\\text{(primal feasibility)}. \\end{aligned} \\] <p>When \\(f\\) is convex and \\(A\\) has full row rank, these conditions are necessary and sufficient for global optimality. For Newton-type methods we linearize these conditions around a current iterate \\((x,\\nu)\\) and solve for corrections \\((\\Delta x,\\Delta \\nu)\\) from</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>This linear system is called the (equality-constrained) KKT system. At the optimum the right-hand side is zero.</p>"},{"location":"convex/19a_optimization_constraints/#quadratic-objectives","title":"Quadratic Objectives","text":"<p>A particularly important case is a convex quadratic objective</p> \\[ f(x) = \\tfrac{1}{2} x^\\top P x + q^\\top x + r, \\] <p>with \\(P \\succeq 0\\). The equality-constrained problem</p> \\[ \\min_x \\tfrac{1}{2} x^\\top P x + q^\\top x + r  \\quad \\text{s.t.} \\quad A x = b \\] <p>has KKT conditions</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>If \\(P \\succ 0\\) and \\(A\\) has full row rank, this system has a unique solution \\((x^\\star,\\nu^\\star)\\). This is the standard linear system solved in equality-constrained least squares and quadratic programming.</p> <p>Examples in ML and statistics:</p> <ul> <li>constrained least squares with sum-to-one constraints on coefficients;  </li> <li>portfolio optimization with ;  </li> <li>quadratic surrogate subproblems inside second-order methods.</li> </ul> <p>The structure of the KKT matrix (symmetric, indefinite, with blocks \\(P\\), \\(A\\)) can be exploited by specialized linear solvers and factorizations.</p>"},{"location":"convex/19a_optimization_constraints/#null-space-reduced-variable-method","title":"Null-Space (Reduced Variable) Method","text":"<p>When the constraints are linear and of full row rank, a natural approach is to eliminate them explicitly.</p> <p>Choose:</p> <ul> <li>a particular feasible point \\(x_0\\) satisfying \\(A x_0 = b\\),  </li> <li>a matrix \\(Z \\in \\mathbb{R}^{n \\times (n-p)}\\) whose columns form a basis of the null space of \\(A\\):    </li> </ul> <p>Then every feasible \\(x\\) can be written as</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective yields an unconstrained reduced problem in the smaller variable \\(y\\):</p> \\[ \\min_{y} \\; \\phi(y) := f(x_0 + Z y). \\] <p>Gradients and Hessians transform as</p> \\[ \\nabla_y \\phi(y) = Z^\\top \\nabla_x f(x_0 + Z y), \\qquad \\nabla_y^2 \\phi(y) = Z^\\top \\nabla_x^2 f(x_0 + Z y) \\, Z. \\] <p>We can now apply any unconstrained method (gradient descent, CG, Newton) to \\(\\phi(y)\\). The corresponding updates in the original space are mapped back via \\(x = x_0 + Z y\\).</p> <p>Key points:</p> <ul> <li>Optimization is restricted to feasible directions \\(\\operatorname{Null}(A)\\) by construction.  </li> <li>The dimension drops from \\(n\\) to \\(n-p\\), which can be advantageous if \\(p\\) is large.  </li> <li>The cost is computing and storing a suitable null-space basis \\(Z\\), which may destroy sparsity and be expensive for large-scale problems.</li> </ul> <p>Null-space methods are attractive when:</p> <ul> <li>the number of constraints is moderate,  </li> <li>a good factorization of \\(A\\) is available,  </li> <li>and we want an unconstrained algorithm in reduced coordinates.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#newtons-method-for-equality-constrained-problems","title":"Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice-differentiable convex \\(f\\), we can derive an equality-constrained Newton step by solving a local quadratic approximation subject to linearized constraints.</p> <p>At a point \\(x\\), approximate \\(f(x+d)\\) by its second-order Taylor expansion:</p> \\[ f(x+d) \\approx f(x) + \\nabla f(x)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d. \\] <p>We seek a step \\(d\\) that approximately minimizes this quadratic model while remaining feasible to first order, i.e.</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d\\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>The KKT conditions for this quadratic subproblem are</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>Solving this system gives the Newton step \\(d_{\\text{nt}}\\) and a multiplier update \\(\\lambda\\). The primal update is</p> \\[ x_{k+1} = x_k + \\alpha_k d_{\\text{nt}}, \\] <p>with a step size \\(\\alpha_k \\in (0,1]\\) chosen by line search to ensure sufficient decrease and preservation of feasibility (for equality constraints, \\(A d_{\\text{nt}} = 0\\) guarantees \\(A x_{k+1} = b\\) whenever \\(A x_k = b\\)).</p> <p>Geometrically:</p> <ul> <li>unconstrained Newton would move by \\(-\\nabla^2 f(x)^{-1} \\nabla f(x)\\);  </li> <li>equality-constrained Newton projects this step onto the tangent space \\(\\{ d : A d = 0 \\}\\) of the affine constraint set.</li> </ul> <p>For strictly convex \\(f\\) with positive definite Hessian on the feasible directions, this method enjoys quadratic convergence near the solution, much like the unconstrained Newton method.</p>"},{"location":"convex/19a_optimization_constraints/#connections-to-machine-learning-and-signal-processing","title":"Connections to Machine Learning and Signal Processing","text":"<p>Linear equality constraints appear naturally in ML and related areas:</p> Setting Equality constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights sum to one (full investment) Constrained regression \\(C x = d\\) Enforce domain-specific linear relations between coefficients Mixture models / convex combinations \\(\\mathbf{1}^\\top \\alpha = 1, \\; \\alpha \\ge 0\\) Mixture weights form a probability simplex Fairness constraints (linearized) \\(A w = 0\\) Enforce equal averages across groups or balance conditions Physics-informed models (discretized) \\(A x = b\\) Discrete conservation laws (mass, charge, energy) <p>More generally, nonlinear equality constraints (e.g. \\(W^\\top W = I\\) for orthonormal embeddings, or \\(\\|w\\|_2^2 = 1\\) for normalized weights) lead to optimization on curved manifolds. Techniques from this chapter extend to those settings when combined with Riemannian optimization or local parameterizations, but here we focus on the linear case as the fundamental building block.</p>"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In many applications, we must optimize an objective while respecting inequality constraints: nonnegativity of variables, margin constraints in SVMs, capacity or safety limits, physical bounds, fairness budgets, and more. Mathematically, the feasible region is now a convex set with a boundary, and the optimizer often lies on that boundary.</p> <p>This chapter introduces algorithms for solving such problems, focusing on logarithmic barrier and interior-point methods. These are the workhorses behind modern general-purpose convex solvers (for LP, QP, SOCP, SDP) and provide a smooth way to enforce inequalities while still using Newton-type methods.</p>"},{"location":"convex/19b_optimization_constraints/#problem-setup","title":"Problem Setup","text":"<p>We consider the general convex problem with inequality and equality constraints  where</p> <ul> <li>\\(f_0, f_1,\\dots,f_m\\) are convex, typically twice differentiable,</li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank,</li> <li>there exists a strictly feasible point \\(\\bar{x}\\) such that   \\(f_i(\\bar{x}) &lt; 0\\) for all \\(i\\) and \\(A\\bar{x} = b\\) (Slater\u2019s condition).</li> </ul> <p>Under these assumptions:</p> <ul> <li>the problem is convex,</li> <li>strong duality holds (zero duality gap),</li> <li>and the KKT conditions characterize optimality.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem type \\(f_0(x)\\) Constraints \\(f_i(x)\\le0\\) ML / applications Linear program (LP) \\(c^\\top x\\) \\(a_i^\\top x - b_i \\le 0\\) resource allocation, feature selection Quadratic program \\(\\tfrac12 x^\\top P x + q^\\top x\\) linear SVMs, ridge with box constraints QCQP quadratic quadratic portfolio optimization, control Entropy models \\(\\sum_i x_i \\log x_i\\) \\(F x - g \\le 0\\) probability calibration, max-entropy Nonnegativity arbitrary convex \\(-x_i \\le 0\\) sparse coding, nonnegative factorization <p>Many machine-learning training problems can be written in this template by expressing regularization, margins, fairness, or safety conditions as convex inequalities.</p>"},{"location":"convex/19b_optimization_constraints/#indicator-function-view-of-constraints","title":"Indicator-Function View of Constraints","text":"<p>Conceptually, we can write inequality constraints using an indicator function. Define  </p> <p>Then the inequality-constrained problem is equivalent to  </p> <ul> <li>If \\(x\\) is feasible (\\(f_i(x) \\le 0\\) for all \\(i\\)), the indicators contribute \\(0\\).</li> <li>If any constraint is violated (\\(f_i(x) &gt; 0\\)), the objective becomes \\(+\\infty\\).</li> </ul> <p>This formulation is clean but not numerically friendly:</p> <ul> <li>\\(I_{-}\\) is discontinuous and nonsmooth.</li> <li>We cannot directly apply Newton-type methods.</li> </ul> <p>The key idea of barrier methods is to replace the hard indicator with a smooth approximation that grows to \\(+\\infty\\) as we approach the boundary.</p>"},{"location":"convex/19b_optimization_constraints/#logarithmic-barrier-approximation","title":"Logarithmic Barrier Approximation","text":"<p>We approximate the indicator \\(I_{-}\\) with a smooth barrier function  </p> <p>For each inequality \\(f_i(x) \\le 0\\), we introduce a barrier term \\(-\\log(-f_i(x))\\). For a given parameter \\(t &gt; 0\\), we solve the barrier subproblem  where  </p> <p>Equivalently,  </p> <p>Interpretation:</p> <ul> <li>The barrier term \\(\\phi(x)\\) is finite only for strictly feasible points (\\(f_i(x) &lt; 0\\)).</li> <li>As \\(x\\) approaches the boundary \\(f_i(x) \\to 0^-\\), the term \\(-\\log(-f_i(x)) \\to +\\infty\\).</li> <li>The parameter \\(t\\) controls the trade-off:</li> <li>small \\(t\\) (large \\(1/t\\)) \u2192 strong barrier, solution stays deep inside the feasible set;</li> <li>large \\(t\\) \u2192 barrier is weaker, solutions can move closer to the boundary.</li> </ul> <p>As \\(t \\to \\infty\\), solutions of the barrier subproblem approach the solution of the original constrained problem.</p>"},{"location":"convex/19b_optimization_constraints/#derivatives-of-the-barrier","title":"Derivatives of the Barrier","text":"<p>Let  Then \\(\\phi\\) is convex and twice differentiable on its domain. Its gradient and Hessian are  </p> <p>Key features:</p> <ul> <li>As \\(f_i(x) \\uparrow 0\\) (approaching the boundary from inside), the factor \\(1/(-f_i(x))\\) blows up, so \\(\\|\\nabla \\phi(x)\\|\\) becomes very large.</li> <li>This creates a strong repulsive force that prevents iterates from crossing the boundary.</li> <li>The barrier \u201cpushes\u201d the solution away from constraint violation, while the original objective \\(f_0(x)\\) pulls toward lower cost.</li> </ul> <p>The barrier subproblem  is a smooth equality-constrained problem. We can therefore apply equality-constrained Newton methods (Chapter 13) at each fixed \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#central-path-and-approximate-kkt-conditions","title":"Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^\\star(t)\\) be a minimizer of the barrier problem  </p> <p>The set \\(\\{x^\\star(t) : t &gt; 0\\}\\) is called the central path. As \\(t \\to \\infty\\), \\(x^\\star(t)\\) converges to a solution \\(x^\\star\\) of the original inequality-constrained problem.</p> <p>We can associate approximate dual variables to \\(x^\\star(t)\\):  </p> <p>Then the KKT-like relations hold:  </p> <p>Compare with the exact KKT conditions (for optimal \\((x^\\star,\\lambda^\\star,v^\\star)\\)):  </p> <p>Along the central path we have the relaxed complementarity condition  which tends to \\(0\\) as \\(t \\to \\infty\\). Hence the barrier formulation naturally yields approximate primal\u2013dual solutions whose KKT residuals shrink as we increase \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#geometric-and-physical-intuition","title":"Geometric and Physical Intuition","text":"<p>Consider the barrier-augmented objective  </p> <p>We can interpret this as:</p> <ul> <li>\\(t f_0(x)\\): an \u201cexternal potential\u201d pulling us toward low objective values.</li> <li>\\(-\\log(-f_i(x))\\): repulsive potentials that become infinite near the boundary \\(f_i(x)=0\\).</li> </ul> <p>At a minimizer \\(x^\\star(t)\\), we have  </p> <p>The gradient of the objective is exactly balanced by a weighted sum of constraint gradients. This is a force-balance condition:</p> <ul> <li>constraints \u201cpush back\u201d more strongly when \\(x\\) is close to their boundary,</li> <li>the interior-point iterates follow a smooth path that stays strictly feasible   and moves gradually toward the optimal boundary point.</li> </ul> <p>This picture explains both:</p> <ul> <li>why iterates never leave the feasible region, and  </li> <li>why the method naturally generates dual variables (the weights on constraint gradients).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#the-barrier-method","title":"The Barrier Method","text":"<p>The barrier method solves the original inequality-constrained problem by solving a sequence of barrier subproblems with increasing \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-barrier-method-conceptual-form","title":"Algorithm: Barrier Method (Conceptual Form)","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (\\(f_i(x) &lt; 0\\), \\(A x = b\\)),</li> <li>initial barrier parameter \\(t &gt; 0\\),</li> <li>barrier growth factor \\(\\mu &gt; 1\\) (e.g. \\(\\mu \\in [10,20]\\)),</li> <li>accuracy tolerance \\(\\varepsilon &gt; 0\\),</li> </ul> <p>repeat:</p> <ol> <li> <p>Centering step    Solve the equality-constrained problem        using an equality-constrained Newton method.    (In practice, we start from the previous solution and take a small number of Newton steps rather than \u201csolve exactly\u201d.)</p> </li> <li> <p>Update iterate    Let \\(x\\) be the resulting point (the approximate minimizer for current \\(t\\)).</p> </li> <li> <p>Check stopping criterion    For the barrier problem, one can show        where \\(p^\\star\\) is the optimal value of the original problem.    If        then stop: \\(x\\) is guaranteed to be within \\(\\varepsilon\\) (in objective value) of optimal.</p> </li> <li> <p>Increase \\(t\\)    Set \\(t := \\mu t\\) to weaken the barrier and move closer to the true boundary, then go back to Step 1.</p> </li> </ol> <p>Key parameters:</p> Symbol Role \\(t\\) barrier strength (larger \\(t\\) = weaker barrier, closer to solution) \\(\\mu\\) growth factor for \\(t\\) \\(\\varepsilon\\) desired accuracy (duality-gap based) \\(m\\) number of inequality constraints <p>In practice:</p> <ul> <li>\\(\\varepsilon\\) is often in the range \\(10^{-3}\\)\u2013\\(10^{-8}\\),</li> <li>\\(\\mu\\) is chosen to balance outer iterations vs inner Newton steps,</li> <li>the centering step is usually solved to modest accuracy, not exactness.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#from-barrier-methods-to-interior-point-methods","title":"From Barrier Methods to Interior-Point Methods","text":"<p>Pure barrier methods conceptually \u201csolve a sequence of problems for increasing \\(t\\)\u201d. Modern interior-point methods refine this idea:</p> <ul> <li>they update both primal variables \\(x\\) and dual variables \\((\\lambda, v)\\),</li> <li>they use Newton\u2019s method on the (perturbed) KKT system,</li> <li>they follow the central path by simultaneously enforcing:</li> <li>primal feasibility (\\(f_i(x) \\le 0\\), \\(A x = b\\)),</li> <li>dual feasibility (\\(\\lambda_i \\ge 0\\)),</li> <li>relaxed complementarity (\\(-\\lambda_i f_i(x) \\approx 1/t\\)).</li> </ul> <p>A typical primal\u2013dual step solves a linearized KKT system of the form  </p> <p>Newton\u2019s method applied to these equations yields search directions for \\((x,\\lambda,v)\\) that move toward the central path and reduce primal and dual residuals simultaneously. This is what modern LP/QP/SOCP/SDP solvers implement.</p> <p>You do not need to implement these methods from scratch to use them: in practice, you describe your problem in a modeling language (e.g. CVX, CVXPY, JuMP) and rely on an interior-point solver under the hood.</p>"},{"location":"convex/19b_optimization_constraints/#computational-and-practical-notes","title":"Computational and Practical Notes","text":"<p>Some important practical aspects:</p> <ol> <li> <p>Equality-constrained Newton inside    Each barrier subproblem is solved by equality-constrained Newton (Chapter 13). The main cost is solving the KKT linear system at each Newton step.</p> </li> <li> <p>Strict feasibility    Barrier and interior-point methods require a strictly feasible starting point \\(x\\) with \\(f_i(x) &lt; 0\\).  </p> </li> <li>Sometimes this is easy (e.g. nonnegativity constraints with a positive initial vector).  </li> <li> <p>Otherwise, a separate phase I problem is solved to find such a point or to certify infeasibility.</p> </li> <li> <p>Step size control    Because the barrier blows up near the boundary, too aggressive Newton steps may try to leave the feasible region. A backtracking line search is used to ensure:</p> </li> <li>sufficient decrease in the barrier objective,</li> <li> <p>and preservation of strict feasibility (\\(f_i(x) &lt; 0\\) remains true).</p> </li> <li> <p>Accuracy vs cost    The duality-gap bound \\(m/t\\) provides a clear trade-off:</p> </li> <li>small \\(m/t\\) (large \\(t\\)) \u2192 high accuracy, more iterations,</li> <li> <p>larger \\(m/t\\) \u2192 faster but less precise.</p> </li> <li> <p>Sparsity and structure    For large problems, exploiting sparsity in \\(A\\) and in the Hessians \\(\\nabla^2 f_i(x)\\) is crucial. Interior-point methods scale well when linear algebra is carefully optimized.</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#equality-vs-inequality-constrained-algorithms","title":"Equality vs Inequality-Constrained Algorithms","text":"<p>Finally, it is helpful to contrast the equality-only case (Chapter 13) with the inequality case.</p> Aspect Equality constraints \\(A x = b\\) Inequality constraints \\(f_i(x) \\le 0\\) Feasible set Affine subspace General convex region with boundary Typical algorithms Lagrange/KKT, equality-constrained Newton, null-space Barrier methods, primal\u2013dual interior-point methods Feasibility during iteration Can start infeasible and converge to \\(A x = b\\) Iterates kept strictly feasible (\\(f_i(x) &lt; 0\\)) Complementarity Not present (only equalities) \\(\\lambda_i f_i(x) = 0\\) at optimum, or \\(\\approx -1/t\\) along central path Geometric picture Optimization on a flat manifold Optimization in a convex region, repelled from boundary ML relevance Normalization, linear invariants, balance constraints Nonnegativity, margin constraints, safety/fairness limits <p>In summary:</p> <ul> <li>Equality-constrained methods operate directly on an affine manifold using KKT and Newton.  </li> <li>Inequality-constrained methods use smooth barriers (or primal\u2013dual perturbed KKT systems) to stay in the interior and gradually approach the boundary and the optimal point.</li> </ul> <p>Interior-point methods unify these perspectives and are the backbone of modern convex optimization software.</p>"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often runs at massive scale: millions (or billions) of variables, datasets too large to fit in memory, and constraints spread across machines or devices. Classical Newton or interior-point methods are beautiful mathematically, but their per-iteration cost and memory usage often make them impractical for these regimes.</p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to make convex optimization scalable. These ideas underpin the optimization engines behind most modern machine learning systems.</p>"},{"location":"convex/20_advanced/#motivation-structure-and-scale","title":"Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the challenge is not \u201cdoes a solution exist?\u201d but rather \u201ccan we compute it in time and memory?\u201d.</p> <p>Bottlenecks include:</p> <ul> <li>Memory: storing Hessians (or even full gradients) may be impossible.</li> <li>Data size: one full pass over all samples can already be expensive.</li> <li>Distributed data: samples are spread across devices / workers.</li> <li>Sparsity and separability: the problem often decomposes into many small pieces.</li> </ul> <p>A common template is the empirical risk + regularizer form  where</p> <ul> <li>each \\(f_i(x)\\) is a loss term for sample \\(i\\),</li> <li>\\(R(x)\\) is a regularizer (possibly nonsmooth, e.g. \\(\\lambda\\|x\\|_1\\)).</li> </ul> <p>The methods in this chapter are designed to exploit this structure:</p> <ul> <li>update only parts of \\(x\\) at a time (coordinate/block methods),</li> <li>use only some data per step (stochastic methods),</li> <li>split the problem into simpler subproblems (proximal / ADMM),</li> <li>or distribute computation across multiple machines (consensus methods).</li> </ul>"},{"location":"convex/20_advanced/#coordinate-descent","title":"Coordinate Descent","text":"<p>Coordinate descent updates one coordinate (or a small block of coordinates) at a time, holding all others fixed. It is especially effective when updates along a single coordinate are cheap to compute. Given \\(x^{(k)}\\), choose coordinate \\(i_k\\) and define</p> \\[ x^{(k+1)}_i = \\begin{cases} \\displaystyle \\arg\\min_{z \\in \\mathbb{R}} \\; f\\big(x_1^{(k+1)},\\dots,x_{i_k-1}^{(k+1)},z,x_{i_k+1}^{(k)},\\dots,x_n^{(k)}\\big), &amp; i = i_k,\\\\[4pt] x_i^{(k)}, &amp; i \\ne i_k. \\end{cases} \\] <p>In practice:</p> <ul> <li>\\(i_k\\) is chosen either cyclically (\\(1,2,\\dots,n,1,2,\\dots\\)) or randomly.</li> <li>Each coordinate update often has a closed form (e.g. soft-thresholding for LASSO).</li> <li>You never form or store the full gradient; you only need partial derivatives.</li> </ul> <p>Why it scales:</p> <ul> <li>Each step is very cheap \u2014 often \\(O(1)\\) or proportional to the number of nonzeros in the column corresponding to coordinate \\(i_k\\).</li> <li>In high dimensions (e.g., millions of features), this can be far more efficient than updating all coordinates at once.</li> </ul> <p>Convergence: If \\(f\\) is convex and has Lipschitz-continuous partial derivatives, coordinate descent (cyclic or randomized) converges to the global minimizer. Randomized coordinate descent often has clean expected convergence rates.</p> <p>ML context:</p> <ul> <li>LASSO / Elastic Net regression (coordinate updates are soft-thresholding),</li> <li>\\(\\ell_1\\)-penalized logistic regression,</li> <li>matrix factorization and dictionary learning (updating one factor vector at a time),</li> <li>problems where \\(R(x)\\) is separable: \\(R(x) = \\sum_i R_i(x_i)\\).</li> </ul>"},{"location":"convex/20_advanced/#stochastic-gradient-and-variance-reduced-methods","title":"Stochastic Gradient and Variance-Reduced Methods","text":"<p>When \\(N\\) (number of samples) is huge, computing the full gradient  every iteration is too expensive. Stochastic methods replace this full gradient with cheap, unbiased estimates based on small random subsets (mini-batches) of data.</p>"},{"location":"convex/20_advanced/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>At iteration \\(k\\):</p> <ol> <li>Sample a mini-batch \\(\\mathcal{B}_k \\subset \\{1,\\dots,N\\}\\).</li> <li>Form the stochastic gradient     </li> <li>Update        where \\(\\eta_k &gt; 0\\) is the learning rate.</li> </ol> <p>Properties:</p> <ul> <li>\\(\\mathbb{E}[\\widehat{\\nabla f}(x_k) \\mid x_k] = \\nabla f(x_k)\\) (unbiased),</li> <li>Each iteration is cheap (depends only on \\(|\\mathcal{B}_k|\\), not \\(N\\)),</li> <li>The noise can help escape shallow nonconvex traps (in deep learning).</li> </ul> <p>In convex settings, SGD trades off per-iteration cost against convergence speed: many cheap noisy steps instead of fewer expensive precise ones.</p>"},{"location":"convex/20_advanced/#step-sizes-and-averaging","title":"Step Sizes and Averaging","text":"<p>The step size \\(\\eta_k\\) is crucial:</p> <ul> <li>Too large \u2192 iterates diverge or oscillate.</li> <li>Too small \u2192 extremely slow progress.</li> </ul> <p>Typical schedules for convex problems:</p> <ul> <li>General convex: \\(\\eta_k = \\frac{c}{\\sqrt{k}}\\),</li> <li>Strongly convex: \\(\\eta_k = \\frac{c}{k}\\).</li> </ul> <p>Two important stabilization techniques:</p> <ol> <li>Decay the learning rate over time.</li> <li>Polyak\u2013Ruppert averaging: return the average        instead of the last iterate. Averaging cancels noise and leads to optimal \\(O(1/k)\\) rates in strongly convex settings.</li> </ol> <p>Mini-batch size can also grow with \\(k\\), gradually reducing variance while keeping early iterations cheap.</p>"},{"location":"convex/20_advanced/#convergence-rates","title":"Convergence Rates","text":"<p>For convex \\(f\\):</p> <ul> <li>With appropriate diminishing \\(\\eta_k\\), \\(\\mathbb{E}[f(x_k)] - f^\\star = O(k^{-1/2})\\).</li> </ul> <p>For strongly convex \\(f\\):</p> <ul> <li>With \\(\\eta_k = O(1/k)\\) and averaging, \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</li> </ul> <p>These rates are optimal for unbiased first-order stochastic methods.</p>"},{"location":"convex/20_advanced/#variance-reduced-methods-svrg-saga-sarah","title":"Variance-Reduced Methods (SVRG, SAGA, SARAH)","text":"<p>Plain SGD cannot easily reach very high accuracy because the gradient noise never fully disappears. Variance-reduced methods reduce this noise, especially near the solution, by periodically using the full gradient.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <ul> <li>Pick a reference point \\(\\tilde{x}\\) and compute \\(\\nabla f(\\tilde{x})\\).</li> <li>For inner iterations:      where \\(i_k\\) is a random sample index.</li> </ul> <p>Here \\(v_k\\) is still an unbiased estimator of \\(\\nabla f(x_k)\\), but its variance decays as \\(x_k\\) approaches \\(\\tilde{x}\\). For strongly convex \\(f\\), methods like SVRG and SAGA achieve linear convergence, comparable to full gradient descent but at near-SGD cost.</p>"},{"location":"convex/20_advanced/#momentum-and-adaptive-methods","title":"Momentum and Adaptive Methods","text":"<p>In practice, large-scale learning often uses SGD with various modifications:</p> <ul> <li> <p>Momentum / Nesterov: keep a moving average of gradients      which accelerates progress along consistent directions and damps oscillations.</p> </li> <li> <p>Adaptive methods (Adagrad, RMSProp, Adam): maintain coordinate-wise scales based on past squared gradients, effectively using a diagonal preconditioner that adapts to curvature and feature scales.</p> </li> </ul> <p>These methods are especially popular in deep learning. For convex problems, their theoretical behavior is subtle, but empirically they often converge faster in wall-clock time.</p>"},{"location":"convex/20_advanced/#proximal-and-composite-optimization","title":"Proximal and Composite Optimization","text":"<p>Many large-scale convex problems are composite:  where</p> <ul> <li>\\(g\\) is smooth convex with Lipschitz gradient (e.g. data-fitting term),</li> <li>\\(R\\) is convex but possibly nonsmooth (e.g. \\(\\lambda\\|x\\|_1\\), indicator of a constraint set, nuclear norm).</li> </ul> <p>The proximal gradient method (a.k.a. ISTA) updates as  where the proximal operator is  </p> <p>Intuition:</p> <ul> <li>The gradient step moves \\(x\\) in a direction that lowers the smooth term \\(g\\).</li> <li>The prox step solves a small \u201cregularized\u201d problem, pulling \\(x\\) toward a structure favored by \\(R\\) (sparsity, low rank, feasibility, etc.).</li> </ul> <p>Examples of prox operators:</p> <ul> <li>\\(R(x) = \\lambda\\|x\\|_1\\) \u2192 soft-thresholding (coordinate-wise shrinkage).</li> <li>\\(R\\) = indicator of a convex set \\(\\mathcal{X}\\) \u2192 projection onto \\(\\mathcal{X}\\) (so projected gradient is a special case).</li> <li>\\(R(X) = \\|X\\|_*\\) (nuclear norm) \u2192 singular value soft-thresholding.</li> </ul> <p>For large-scale problems:</p> <ul> <li>Proximal gradient scales like gradient descent: each iteration uses only \\(\\nabla g\\) and a prox (often cheap and parallelizable).</li> <li>Accelerated variants (FISTA) achieve \\(O(1/k^2)\\) rates for smooth \\(g\\).</li> </ul>"},{"location":"convex/20_advanced/#alternating-direction-method-of-multipliers-admm","title":"Alternating Direction Method of Multipliers (ADMM)","text":"<p>When objectives naturally split into simpler pieces depending on different variables, ADMM is a powerful tool. It is especially useful when:</p> <ul> <li>\\(f\\) and \\(g\\) have simple prox operators,</li> <li>the problem is distributed or separable across machines.</li> </ul> <p>Consider  </p> <p>The augmented Lagrangian is  with dual variable \\(y\\) and penalty parameter \\(\\rho &gt; 0\\).</p> <p>ADMM performs the iterations:  </p> <p>Interpretation:</p> <ul> <li>The \\(x\\)-update solves a subproblem involving \\(f\\) only.</li> <li>The \\(z\\)-update solves a subproblem involving \\(g\\) only.</li> <li>The \\(y\\)-update nudges the constraint \\(A x + B z = c\\) toward satisfaction.</li> </ul> <p>For convex \\(f,g\\), ADMM converges to a primal\u2013dual optimal point. It is particularly effective when the \\(x\\)- and \\(z\\)-subproblems have closed-form prox solutions or can be solved cheaply in parallel.</p> <p>ML use cases:</p> <ul> <li>Distributed LASSO / logistic regression,</li> <li>matrix completion and robust PCA,</li> <li>consensus optimization (each worker has local data but shares a global model),</li> <li>some federated learning formulations.</li> </ul>"},{"location":"convex/20_advanced/#majorizationminimization-mm-and-em-algorithms","title":"Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The Majorization\u2013Minimization (MM) principle constructs at each iterate \\(x_k\\) a surrogate function \\(g(\\cdot \\mid x_k)\\) that upper-bounds \\(f\\) and is easier to minimize.</p> <p>Requirements:  </p> <p>Then define  </p> <p>This guarantees monotone decrease:  </p> <p>The famous Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models, where the surrogate arises from Jensen\u2019s inequality and missing-data structure.</p> <p>Other examples:</p> <ul> <li>Iteratively Reweighted Least Squares (IRLS) for logistic regression and robust regression,</li> <li>MM surrogates for nonconvex penalties (e.g. smoothly approximating \\(\\ell_0\\)),</li> <li>mixture models and variational inference.</li> </ul>"},{"location":"convex/20_advanced/#distributed-and-parallel-optimization","title":"Distributed and Parallel Optimization","text":"<p>When data or variables are split across machines, we need distributed or parallel optimization schemes.</p>"},{"location":"convex/20_advanced/#synchronous-vs-asynchronous","title":"Synchronous vs Asynchronous","text":"<ul> <li>Synchronous methods: all workers compute local gradients or updates, then synchronize (e.g. parameter server, federated averaging).</li> <li>Asynchronous methods: workers update parameters without global synchronization, improving hardware utilization but introducing staleness and variance.</li> </ul>"},{"location":"convex/20_advanced/#consensus-optimization","title":"Consensus Optimization","text":"<p>A standard pattern is consensus form:  where \\(f_i\\) is the local objective on worker \\(i\\) and \\(z\\) is the global consensus variable.</p> <p>ADMM applied to this problem:</p> <ul> <li>Each worker updates its local \\(x_i\\) using only local data,</li> <li>The global variable \\(z\\) is updated by averaging or aggregation,</li> <li>Dual variables enforce agreement \\(x_i \\approx z\\).</li> </ul> <p>This template underlies many federated learning and parameter-server architectures.</p>"},{"location":"convex/20_advanced/#ml-context","title":"ML Context","text":"<ul> <li>Federated learning (phone/edge devices update local models and send summaries to a server),</li> <li>Large-scale convex optimization over sharded datasets,</li> <li>Distributed sparse regression, matrix factorization, and graphical model learning.</li> </ul>"},{"location":"convex/20_advanced/#handling-structure-sparsity-and-low-rank","title":"Handling Structure: Sparsity and Low Rank","text":"<p>Large-scale convex problems often have additional structure that we can exploit algorithmically:</p> Structure Typical Regularizer / Constraint Algorithmic Benefit Sparsity \\(\\ell_1\\), group lasso Cheap coordinate updates, soft-thresholding Low rank Nuclear norm \\(\\|X\\|_*\\) SVD-based prox; rank truncation Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed block updates Graph structure Total variation on graphs Local neighborhood computations Probability simplex simplex constraint or entropy term Mirror descent, simplex projections <p>Examples:</p> <ul> <li>In compressed sensing, \\(\\ell_1\\) regularization + sparse sensing matrices \u2192 very cheap mat\u2013vecs + prox operations.</li> <li>In matrix completion, nuclear norm structure + low-rank iterates \u2192 approximate SVD instead of full SVD.</li> <li>In TV denoising, local difference structure \u2192 each prox step involves only neighboring pixels/vertices.</li> </ul> <p>Exploiting structure can yield orders-of-magnitude speedups compared to generic solvers.</p>"},{"location":"convex/20_advanced/#summary-and-practical-guidance","title":"Summary and Practical Guidance","text":"<p>Different large-scale methods are appropriate in different regimes:</p> Method Gradient Access Scalability Parallelization Convexity Needed Typical Uses Coordinate Descent Partial gradients High Easy (blockwise) Convex LASSO, sparse GLMs, matrix factorization SGD / Mini-batch SGD Stochastic gradients Excellent Natural (data parallel) Convex / nonconvex Deep learning, logistic regression SVRG / SAGA (VR methods) Stochastic + periodic full gradient High Data parallel Convex, often strongly convex Large-scale convex ML, GLMs Proximal Gradient (ISTA/FISTA) Full gradient + prox Moderate\u2013High Easy Convex Composite objectives with structure ADMM Local subproblems High Designed for distributed Convex Consensus, distributed convex solvers MM / EM Surrogates Moderate Model-specific Convex / nonconvex Latent-variable models, IRLS Distributed / Federated Local gradients Very high Essential Often convex / smooth Federated learning, multi-agent systems"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithms Summary:  </p> <ul> <li>Smooth + \u2113\u2082 (strongly convex):   Newton / quasi-Newton, conjugate gradient, or accelerated gradient.</li> <li>Smooth + \u2113\u2081 (and variants):   proximal gradient or coordinate descent; for huge data, stochastic/proximal variants.</li> <li>Mixed penalties (\u2113\u2081 + \u2113\u2082):   treat as composite smooth + nonsmooth; prox and coordinate methods still apply.</li> <li>Large \\(N\\) or \\(n\\):   mini-batch / stochastic gradients (SGD, SVRG, etc.) on the smooth part + prox for the regulariser.</li> </ul> <p>Regularisation strength \\(\\lambda\\) is usually chosen via cross-validation or a validation set, exploring the accuracy\u2013simplicity trade-off.</p>"},{"location":"convex/21_models/#robust-regression-and-outlier-resistance","title":"Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#least-absolute-deviations-l1-loss","title":"Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#huber-loss","title":"Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints. That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly. This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#worst-case-robust-regression","title":"Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#maximum-likelihood-and-loss-design","title":"Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#structured-constraints-in-engineering-and-design","title":"Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#linear-and-conic-programming-the-canonical-models","title":"Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#risk-safety-margins-and-robust-design","title":"Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#cheat-sheet-if-your-problem-looks-like-this-use-that","title":"Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#matching-model-structure-to-algorithm-type","title":"Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#hierarchy-of-canonical-problems","title":"Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#linear-programming-lp","title":"Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#quadratic-programming-qp","title":"Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#quadratically-constrained-qp-qcqp","title":"Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#second-order-cone-programming-socp","title":"Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#semidefinite-programming-sdp","title":"Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#geometric-programming-gp","title":"Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#likelihood-based-convex-models-mle-and-glms","title":"Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#solver-selection-summary","title":"Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"convex/35_modern/","title":"18. Modern Optimizers in Machine Learning Frameworks","text":""},{"location":"convex/35_modern/#chapter-18-modern-optimizers-in-machine-learning","title":"Chapter 18: Modern Optimizers in Machine Learning","text":"<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization\u2014balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p> <p>This chapter connects the principles of convex optimization to the modern optimizers that power today\u2019s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>"},{"location":"convex/35_modern/#stochastic-optimization-overview","title":"Stochastic Optimization Overview","text":"<p>In machine learning, we often minimize an empirical risk:  where \\(\\ell(x; z_i)\\) is the loss on data sample \\(z_i\\).</p> <p>Computing the full gradient \\(\\nabla f(x)\\) is infeasible when \\(N\\) is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p> \\[ g_k = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla \\ell(x_k; z_i). \\] <p>This yields the Stochastic Gradient Descent (SGD) update:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k. \\] <p>SGD is the foundation for nearly all deep learning optimizers.</p>"},{"location":"convex/35_modern/#momentum-and-acceleration","title":"Momentum and Acceleration","text":"<p>SGD\u2019s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p> <p>  where \\(\\beta \\in [0,1)\\) controls inertia.</p> <p>Nesterov momentum adds a correction term anticipating the future position:</p> \\[ v_{k+1} = \\beta v_k + g(x_k - \\alpha \\beta v_k), \\quad x_{k+1} = x_k - \\alpha v_{k+1}. \\] <p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>"},{"location":"convex/35_modern/#adaptive-learning-rate-methods","title":"Adaptive Learning Rate Methods","text":"<p>Different parameters often require different step sizes. Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>"},{"location":"convex/35_modern/#adagrad","title":"AdaGrad","text":"<p>Keeps a cumulative sum of squared gradients:</p> <p>  and updates parameters as:</p> <p>  Good for sparse data, but the learning rate can shrink too quickly.</p>"},{"location":"convex/35_modern/#rmsprop","title":"RMSProp","text":"<p>A refinement of AdaGrad using exponential averaging:</p> \\[ E[g^2]_k = \\beta E[g^2]_{k-1} + (1-\\beta) g_k^2, \\] \\[ x_{k+1} = x_k - \\frac{\\alpha}{\\sqrt{E[g^2]_k + \\epsilon}} g_k. \\] <p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>"},{"location":"convex/35_modern/#adam-adaptive-moment-estimation","title":"Adam: Adaptive Moment Estimation","text":"<p>Adam combines momentum and adaptive scaling:</p> \\[ m_k = \\beta_1 m_{k-1} + (1-\\beta_1) g_k, \\quad v_k = \\beta_2 v_{k-1} + (1-\\beta_2) g_k^2, \\] \\[ \\hat{m}_k = \\frac{m_k}{1-\\beta_1^k}, \\quad \\hat{v}_k = \\frac{v_k}{1-\\beta_2^k}, \\] \\[ x_{k+1} = x_k - \\alpha \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\epsilon}. \\] <p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>"},{"location":"convex/35_modern/#variants-and-modern-extensions","title":"Variants and Modern Extensions","text":"Optimizer Key Idea Notes AdamW Decoupled weight decay from gradient update Better regularization RAdam Rectified Adam\u2014adaptive variance correction Improves stability early in training Lookahead Combines fast and slow weights Enhances robustness and convergence AdaBelief Uses prediction error instead of raw gradient variance More adaptive learning rates Lion Uses sign-based updates and momentum Efficient for large-scale training <p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>"},{"location":"convex/35_modern/#implicit-regularization-and-generalization","title":"Implicit Regularization and Generalization","text":"<p>Modern optimizers not only minimize loss\u2014they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p> <p>Empirical findings suggest:</p> <ul> <li>Large-batch training finds sharper minima (risk of overfitting).  </li> <li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li> <li>Adaptive optimizers may converge faster but generalize slightly worse.</li> </ul> <p>This trade-off drives ongoing research into optimizer design.</p>"},{"location":"convex/35_modern/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Learning Rate Most critical hyperparameter; use warm-up and decay schedules Batch Size Balances gradient noise and hardware efficiency Initialization Affects early dynamics, especially for Adam variants Gradient Clipping Prevents instability in exploding gradients Mixed Precision Use with adaptive optimizers for speed and memory savings"},{"location":"convex/35_modern/#comparative-behavior","title":"Comparative Behavior","text":"Method Adaptivity Speed Memory Typical Use SGD + Momentum Moderate Slow-medium Low General-purpose, good generalization RMSProp Adaptive per-parameter Medium-fast Medium Recurrent networks, nonstationary data Adam / AdamW Fully adaptive Fast High Deep networks, large-scale training RAdam / AdaBelief / Lion Advanced adaptivity Fast Medium Cutting-edge training tasks"},{"location":"convex/35_modern/#optimization-in-modern-deep-networks","title":"Optimization in Modern Deep Networks","text":"<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p> <ul> <li>Batch normalization modifies effective learning rates.  </li> <li>Skip connections ease gradient flow.  </li> <li>Large-scale distributed training relies on adaptive optimizers for stability.  </li> </ul> <p>Optimization is no longer an isolated procedure but part of the model\u2019s design philosophy.</p> <p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning\u2014adapting dynamically, balancing efficiency and generalization.</p>"},{"location":"convex/40_nonconvex/","title":"19. Beyond Convexity \u2013 Nonconvex and Global Optimization","text":""},{"location":"convex/40_nonconvex/#chapter-19-beyond-convexity-nonconvex-and-global-optimization","title":"Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization","text":"<p>Optimization extends far beyond the comfortable world of convexity. In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p> <p>Convex optimization gives us strong guarantees \u2014 every local minimum is global, and algorithms converge predictably. But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>"},{"location":"convex/40_nonconvex/#the-landscape-of-nonconvex-optimization","title":"The Landscape of Nonconvex Optimization","text":"<p>A nonconvex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) violates convexity; i.e., for some \\(x, y\\) and \\(\\theta \\in (0,1)\\),  Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p> <p>A typical nonconvex landscape looks like a mountainous terrain \u2014 smooth in some regions, rugged in others. An optimization algorithm\u2019s path depends strongly on initialization and stochastic effects.</p>"},{"location":"convex/40_nonconvex/#example-a-simple-nonconvex-function","title":"Example: A Simple Nonconvex Function","text":"<p>  This function has multiple stationary points: - \\((0,0)\\) (a saddle), - \\((1,1)\\) and \\((-1,-1)\\) (local minima), - \\((1,-1)\\) and \\((-1,1)\\) (local maxima).</p> <p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>"},{"location":"convex/40_nonconvex/#local-vs-global-minima","title":"Local vs. Global Minima","text":"<p>A point \\(x^*\\) is a local minimum if:  </p> <p>A global minimum satisfies the stronger condition:  </p> <p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad \u2014 and there may be exponentially many of them.</p>"},{"location":"convex/40_nonconvex/#classes-of-nonconvex-problems","title":"Classes of Nonconvex Problems","text":"<p>Nonconvex problems appear in several distinct forms:</p> Type Example Challenge Smooth nonconvex Neural network training Multiple minima, saddle points Nonsmooth nonconvex Sparse regularization, ReLU activations Undefined gradients Discrete / combinatorial Scheduling, routing, integer programs Exponential search space Black-box Simulation-based optimization No derivatives or analytical form <p>Each category requires different algorithmic strategies \u2014 from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>"},{"location":"convex/40_nonconvex/#local-optimization-strategies","title":"Local Optimization Strategies","text":"<p>Even in nonconvex settings, local optimization remains useful when: - The problem is nearly convex (e.g., locally convex around good minima), - The initialization is close to a desired basin of attraction, - Or the goal is approximate, not exact, optimality.</p>"},{"location":"convex/40_nonconvex/#gradient-descent-and-its-variants","title":"Gradient Descent and Its Variants","text":"<p>Gradient descent behaves well if \\(f\\) is smooth and Lipschitz-continuous:  However, convergence is only to a stationary point \u2014 not necessarily a minimum.</p> <p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>"},{"location":"convex/40_nonconvex/#global-optimization-strategies","title":"Global Optimization Strategies","text":"<p>To seek the global minimum, algorithms must explore the search space more broadly. Common strategies include:</p> <ol> <li> <p>Multiple Starts:    Run local optimization from diverse random initial points and keep the best solution.</p> </li> <li> <p>Continuation and Homotopy Methods:    Start from a smooth, convex approximation \\(f_\\lambda\\) of \\(f\\) and gradually transform it into the true objective as \\(\\lambda \\to 0\\).</p> </li> <li> <p>Stochastic Search and Simulated Annealing:    Introduce randomness in updates to jump between basins.</p> </li> <li> <p>Population-Based Methods:    Maintain a swarm or population of candidate solutions evolving by selection and variation \u2014 leading to metaheuristic algorithms like GA and PSO.</p> </li> </ol>"},{"location":"convex/40_nonconvex/#theoretical-challenges","title":"Theoretical Challenges","text":"<p>Without convexity, most strong results vanish:</p> <ul> <li>Global optimality cannot be guaranteed.</li> <li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li> <li>Complexity often grows exponentially with problem size.</li> </ul> <p>However, theory is not hopeless:</p> <ul> <li>Many nonconvex problems are \u201cbenign\u201d \u2014 e.g., matrix factorization, phase retrieval, or deep linear networks \u2014 having no bad local minima.  </li> <li>Random initialization and overparameterization often aid convergence to global minima in practice.</li> </ul>"},{"location":"convex/40_nonconvex/#geometry-of-saddle-points","title":"Geometry of Saddle Points","text":"<p>A saddle point satisfies \\(\\nabla f(x)=0\\) but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p> <p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>"},{"location":"convex/40_nonconvex/#deterministic-vs-stochastic-global-methods","title":"Deterministic vs. Stochastic Global Methods","text":"Deterministic Methods Stochastic Methods Systematic exploration of space (branch &amp; bound, interval analysis) Randomized search (simulated annealing, evolutionary algorithms) Can provide certificates of global optimality Typically approximate but scalable High computational cost Naturally parallelizable <p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>"},{"location":"convex/40_nonconvex/#a-taxonomy-of-optimization-beyond-convexity","title":"A Taxonomy of Optimization Beyond Convexity","text":"Family Typical Algorithms When to Use Derivative-Free (Black-Box) Nelder\u2013Mead, CMA-ES, Bayesian Opt. When gradients unavailable Metaheuristic (Evolutionary) GA, PSO, DE, ACO Complex landscapes, combinatorial problems Modern Stochastic Gradient Adam, RMSProp, Lion Deep learning, large-scale models Combinatorial / Discrete Branch &amp; Bound, Tabu, SA Integer or graph-based problems Learning-Based Optimizers Meta-learning, Reinforcement methods Adaptive, data-driven optimization"},{"location":"convex/42_derivativefree/","title":"20. Derivative-Free and Black-Box Optimization","text":""},{"location":"convex/42_derivativefree/#chapter-20-derivative-free-and-black-box-optimization","title":"Chapter 20: Derivative-Free and Black-Box Optimization","text":"<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p> <p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>"},{"location":"convex/42_derivativefree/#motivation-and-challenges","title":"Motivation and Challenges","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be an objective function.  </p> <p>A derivative-free algorithm seeks to minimize \\(f(x)\\) using only evaluations of \\(f(x)\\), without access to \\(\\nabla f(x)\\) or \\(\\nabla^2 f(x)\\).</p> <p>Key challenges:</p> <ul> <li>No gradient information \u2192 difficult to infer descent directions.  </li> <li>Expensive evaluations \u2192 every call to \\(f(x)\\) might require a simulation or experiment.  </li> <li>Noise and stochasticity \u2192 evaluations may be corrupted by measurement or sampling error.  </li> <li>High-dimensionality \u2192 sampling-based methods scale poorly with \\(n\\).</li> </ul> <p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>"},{"location":"convex/42_derivativefree/#classification-of-derivative-free-methods","title":"Classification of Derivative-Free Methods","text":"Category Representative Algorithms Main Idea Direct Search Nelder\u2013Mead, Pattern Search, MADS Explore the space via geometric moves or meshes Model-Based BOBYQA, Trust-Region DFO Build local quadratic or surrogate models of \\(f\\) Evolutionary / Population-Based CMA-ES, Differential Evolution Evolve a population using stochastic operators Probabilistic / Bayesian Bayesian Optimization Use probabilistic surrogate models to guide exploration"},{"location":"convex/42_derivativefree/#direct-search-methods","title":"Direct Search Methods","text":"<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>"},{"location":"convex/42_derivativefree/#neldermead-simplex-method","title":"Nelder\u2013Mead Simplex Method","text":"<p>Perhaps the most famous derivative-free algorithm, Nelder\u2013Mead maintains a simplex \u2014 a polytope of \\(n+1\\) vertices in \\(\\mathbb{R}^n\\).</p> <p>At each iteration:</p> <ol> <li>Evaluate \\(f\\) at all simplex vertices.</li> <li>Reflect, expand, contract, or shrink the simplex depending on performance.</li> <li>Continue until simplex collapses near a minimum.</li> </ol> <p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>"},{"location":"convex/42_derivativefree/#pattern-search-methods","title":"Pattern Search Methods","text":"<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p> <p>Typical update rule:  </p> <p>where \\(d_i\\) is a direction from a finite set (e.g., coordinate axes). If a direction yields improvement, move there; otherwise, shrink \\(\\Delta_k\\).</p>"},{"location":"convex/42_derivativefree/#mesh-adaptive-direct-search-mads","title":"Mesh Adaptive Direct Search (MADS)","text":"<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>"},{"location":"convex/42_derivativefree/#model-based-methods","title":"Model-Based Methods","text":"<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>"},{"location":"convex/42_derivativefree/#trust-region-dfo","title":"Trust-Region DFO","text":"<p>A local model \\(m_k(x)\\) (often quadratic) is built to approximate \\(f\\) near the current iterate \\(x_k\\):  The next iterate solves a trust-region subproblem:  The trust region size \\(\\Delta_k\\) adapts based on how well \\(m_k\\) predicts true function values.</p>"},{"location":"convex/42_derivativefree/#bobyqa-bound-optimization-by-quadratic-approximation","title":"BOBYQA (Bound Optimization BY Quadratic Approximation)","text":"<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>"},{"location":"convex/42_derivativefree/#evolution-strategies-and-population-methods","title":"Evolution Strategies and Population Methods","text":"<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>"},{"location":"convex/42_derivativefree/#covariance-matrix-adaptation-evolution-strategy-cma-es","title":"Covariance Matrix Adaptation Evolution Strategy (CMA-ES)","text":"<p>CMA-ES is a powerful stochastic search algorithm. It iteratively samples new points from a multivariate Gaussian distribution:  where \\(m_k\\) is the current mean, \\(\\sigma_k\\) the global step-size, and \\(C_k\\) the covariance matrix.</p> <p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p> <p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>"},{"location":"convex/42_derivativefree/#differential-evolution-de","title":"Differential Evolution (DE)","text":"<p>DE evolves a population \\(\\{x_i\\}\\) via vector differences:   where \\(r1, r2, r3\\) are random distinct indices and \\(F\\) controls mutation strength.</p> <p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>"},{"location":"convex/42_derivativefree/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>When function evaluations are expensive (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>"},{"location":"convex/42_derivativefree/#core-idea","title":"Core Idea","text":"<p>Model the objective as a random function \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))\\) (Gaussian Process prior). After each evaluation, update the posterior mean and variance to quantify uncertainty.</p> <p>Use an acquisition function \\(a(x)\\) to select the next evaluation point:  balancing exploration (high uncertainty) and exploitation (low expected value).</p> <p>Common acquisition functions:</p> <ul> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Upper Confidence Bound (UCB)</li> </ul>"},{"location":"convex/42_derivativefree/#surrogate-models-beyond-gaussian-processes","title":"Surrogate Models Beyond Gaussian Processes","text":"<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs: - Tree-structured Parzen Estimators (TPE) - Random forests (SMAC) - Neural network surrogates (Bayesian neural networks)</p> <p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>"},{"location":"convex/42_derivativefree/#hybrid-and-adaptive-approaches","title":"Hybrid and Adaptive Approaches","text":"<p>Modern applications often combine derivative-free and gradient-based techniques:</p> <ul> <li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li> <li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li> <li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li> </ul> <p>Such hybridization reflects a pragmatic view: no single optimizer is best \u2014 adaptability matters most.</p>"},{"location":"convex/42_derivativefree/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Function evaluations expensive Use Bayesian or model-based methods Noisy evaluations Use averaging, smoothing, or robust estimators High dimension (\\(n &gt; 50\\)) Prefer CMA-ES or evolutionary strategies Box constraints Methods like BOBYQA, DE, or PSO Parallel computation available Population-based methods excel <p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize anything we can evaluate. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p> <p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors \u2014 turning randomness into a powerful search strategy.</p>"},{"location":"convex/44_metaheuristic/","title":"21. Metaheuristic and Evolutionary Optimization","text":""},{"location":"convex/44_metaheuristic/#chapter-21-metaheuristic-and-evolutionary-algorithms","title":"Chapter 21: Metaheuristic and Evolutionary Algorithms","text":"<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions.  In these settings, metaheuristic algorithms\u2014inspired by nature, biology, and collective behavior\u2014provide robust and flexible alternatives.</p> <p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas. They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>"},{"location":"convex/44_metaheuristic/#principles-of-metaheuristic-optimization","title":"Principles of Metaheuristic Optimization","text":"<p>All metaheuristics share three key principles:</p> <ol> <li> <p>Population-Based Search:    Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p> </li> <li> <p>Variation Operators:    Create new solutions via mutation, recombination, or stochastic perturbations.</p> </li> <li> <p>Selection and Adaptation:    Favor candidates with better objective values, guiding the search toward promising regions.</p> </li> </ol> <p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>"},{"location":"convex/44_metaheuristic/#genetic-algorithms-ga","title":"Genetic Algorithms (GA)","text":""},{"location":"convex/44_metaheuristic/#biological-inspiration","title":"Biological Inspiration","text":"<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>"},{"location":"convex/44_metaheuristic/#representation","title":"Representation","text":"<p>A solution (individual) is represented as a chromosome\u2014often a binary string, vector of reals, or permutation. Each position (gene) encodes part of the decision variable.</p>"},{"location":"convex/44_metaheuristic/#algorithm-outline","title":"Algorithm Outline","text":"<ol> <li>Initialize a population \\(\\{x_i\\}_{i=1}^N\\) randomly.  </li> <li>Evaluate fitness \\(f(x_i)\\) for all individuals.  </li> <li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li> <li> <p>Apply:</p> <ul> <li>Crossover: combine genetic material of two parents.  </li> <li>Mutation: randomly alter some genes to maintain diversity.  </li> </ul> </li> <li> <p>Form a new population and repeat until convergence.</p> </li> </ol>"},{"location":"convex/44_metaheuristic/#crossover-and-mutation-examples","title":"Crossover and Mutation Examples","text":"<ul> <li>Single-point crossover: exchange genes after a random index.  </li> <li>Gaussian mutation: add small noise to continuous parameters.  </li> </ul>"},{"location":"convex/44_metaheuristic/#strengths-and-weaknesses","title":"Strengths and Weaknesses","text":"Strengths Weaknesses Highly parallel, robust, domain-independent Requires many function evaluations Effective for combinatorial and discrete optimization Parameter tuning (mutation, crossover rates) is nontrivial"},{"location":"convex/44_metaheuristic/#differential-evolution-de","title":"Differential Evolution (DE)","text":"<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>"},{"location":"convex/44_metaheuristic/#core-idea","title":"Core Idea","text":"<p>Mutation is performed using differences of population members:  where \\(r1, r2, r3\\) are random distinct indices and \\(F \\in [0,2]\\) controls mutation amplitude.</p> <p>Then crossover forms trial vectors:  and selection chooses between \\(x_i\\) and \\(u_i\\) based on objective value.</p>"},{"location":"convex/44_metaheuristic/#features","title":"Features","text":"<ul> <li>Self-adaptive exploration of the search space.</li> <li>Suitable for continuous, multimodal functions.</li> <li>Simple to implement, with few control parameters.</li> </ul>"},{"location":"convex/44_metaheuristic/#particle-swarm-optimization-pso","title":"Particle Swarm Optimization (PSO)","text":"<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p> <p>Each particle \\(i\\) has position \\(x_i\\) and velocity \\(v_i\\), updated as:   where:</p> <ul> <li>\\(p_i\\) = personal best position of particle \\(i\\),</li> <li>\\(g\\) = best global position found by the swarm,</li> <li>\\(w\\), \\(c_1\\), \\(c_2\\) are weight and learning coefficients,</li> <li>\\(r_1\\), \\(r_2\\) are random numbers in \\([0,1]\\).</li> </ul> <p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>"},{"location":"convex/44_metaheuristic/#convergence-behavior","title":"Convergence Behavior","text":"<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>"},{"location":"convex/44_metaheuristic/#strengths","title":"Strengths","text":"<ul> <li>Few parameters, easy to implement.</li> <li>Works well for noisy or discontinuous problems.</li> <li>Naturally parallelizable.</li> </ul>"},{"location":"convex/44_metaheuristic/#simulated-annealing-sa","title":"Simulated Annealing (SA)","text":"<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy \u2014 a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual \u201ccooling\u201d in the search for a global minimum.</p>"},{"location":"convex/44_metaheuristic/#physical-analogy","title":"Physical Analogy","text":"<p>In thermodynamics, a system at temperature \\(T\\) has probability of occupying a state with energy \\(E\\) given by the Boltzmann distribution:</p> \\[ P(E) \\propto e^{-E / (kT)}. \\] <p>At high temperature, the system freely explores many states. As \\(T\\) decreases, it becomes increasingly likely to remain near states of minimal energy.</p> <p>Simulated Annealing maps this principle to optimization by treating:</p> <ul> <li>The objective function \\(f(x)\\) as the system\u2019s energy.</li> <li>The solution vector \\(x\\) as a configuration.</li> <li>The temperature \\(T\\) as a control parameter determining randomness.</li> </ul>"},{"location":"convex/44_metaheuristic/#algorithm-outline_1","title":"Algorithm Outline","text":"<ol> <li> <p>Initialization</p> <ul> <li>Choose an initial solution \\(x_0\\) and initial temperature \\(T_0\\).</li> <li>Set a cooling schedule \\(T_{k+1} = \\alpha T_k\\), with \\(\\alpha \\in (0,1)\\).</li> </ul> </li> <li> <p>Iteration</p> <ul> <li>Generate a candidate \\(x'\\) from \\(x_k\\) via a small random perturbation.</li> <li>Compute \\(\\Delta f = f(x') - f(x_k)\\).</li> <li>Accept or reject based on the Metropolis criterion:</li> </ul> <p> </p> </li> <li> <p>Cooling</p> <ul> <li> <p>Reduce the temperature gradually according to the schedule.</p> </li> <li> <p>Repeat until \\(T\\) becomes sufficiently small or the system stabilizes.</p> </li> </ul> </li> </ol>"},{"location":"convex/44_metaheuristic/#interpretation","title":"Interpretation","text":"<ul> <li> <p>At high temperatures, SA accepts both better and worse moves \u2192 exploration.  </p> </li> <li> <p>At low temperatures, it becomes increasingly selective \u2192 exploitation.</p> </li> </ul> <p>This balance allows SA to escape local minima and approach the global optimum over time.</p>"},{"location":"convex/44_metaheuristic/#cooling-schedules","title":"Cooling Schedules","text":"<p>The temperature schedule determines convergence quality:</p> Type Formula Behavior Exponential \\(T_{k+1} = \\alpha T_k\\) Simple, widely used Linear \\(T_{k+1} = T_0 - \\beta k\\) Faster cooling, less exploration Logarithmic \\(T_k = \\frac{T_0}{\\log(k + c)}\\) Theoretically convergent (slow) Adaptive Adjust based on recent acceptance rates Practical and self-tuning <p>A slower cooling schedule improves accuracy but increases computational cost.</p>"},{"location":"convex/44_metaheuristic/#ant-colony-optimization-aco","title":"Ant Colony Optimization (ACO)","text":""},{"location":"convex/44_metaheuristic/#biological-basis","title":"Biological Basis","text":"<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p> <p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity \\(\\tau_{ij}\\) and heuristic visibility \\(\\eta_{ij}\\):  </p>"},{"location":"convex/44_metaheuristic/#pheromone-update","title":"Pheromone Update","text":"<p>After all ants construct their tours:  where \\(\\rho\\) controls evaporation and \\(\\Delta\\tau_{ij}\\) reinforces paths used by good solutions.</p> <p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>"},{"location":"convex/44_metaheuristic/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>Every metaheuristic must balance:</p> <ul> <li>Exploration: sampling diverse regions to escape local minima.  </li> <li>Exploitation: refining known good solutions to reach local optima.</li> </ul> High Exploration High Exploitation GA with strong mutation PSO with low inertia DE with high \\(F\\) ACO with low evaporation rate Random restarts Local refinement <p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>"},{"location":"convex/44_metaheuristic/#hybrid-and-memetic-algorithms","title":"Hybrid and Memetic Algorithms","text":"<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p> <p>Example:</p> <ol> <li>Use PSO or GA to explore broadly.  </li> <li>Apply gradient descent or Nelder\u2013Mead locally near promising candidates.</li> </ol> <p>This hybridization often yields faster convergence and improved accuracy.</p>"},{"location":"convex/44_metaheuristic/#performance-and-practical-tips","title":"Performance and Practical Tips","text":"Aspect Guideline Initialization Use wide, random distributions to promote diversity Parameter Tuning Use adaptive schedules (e.g., cooling, inertia decay) Population Size Larger for global search, smaller for fine-tuning Parallelism Evaluate populations concurrently for efficiency Stopping Criteria Use both iteration limits and stagnation detection <p>Metaheuristics are heuristic by design \u2014 they do not guarantee global optimality, but offer practical success across many fields. Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p>"},{"location":"convex/48_advanced_combinatorial/","title":"22. Advanced Topics in Combinatorial Optimization","text":""},{"location":"convex/48_advanced_combinatorial/#chapter-22-advanced-topics-in-combinatorial-optimization","title":"Chapter 22: Advanced Topics in Combinatorial Optimization","text":"<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p> <p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>"},{"location":"convex/48_advanced_combinatorial/#nature-of-combinatorial-problems","title":"Nature of Combinatorial Problems","text":"<p>A combinatorial optimization problem can be expressed as:</p> \\[ \\min_{x \\in \\mathcal{F}} f(x), \\] <p>where \\(\\mathcal{F}\\) is a finite or countable set of feasible solutions, often exponentially large in size.</p> <p>Example forms include:</p> <ul> <li>Binary decisions: \\(x_i \\in \\{0,1\\}\\)</li> <li>Integer constraints: \\(x_i \\in \\mathbb{Z}\\)</li> <li>Permutations: ordering or ranking elements</li> </ul> <p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>"},{"location":"convex/48_advanced_combinatorial/#graph-theoretic-foundations","title":"Graph-Theoretic Foundations","text":"<p>Many combinatorial problems are naturally represented as graphs \\(G = (V, E)\\).</p>"},{"location":"convex/48_advanced_combinatorial/#shortest-path-problem","title":"Shortest Path Problem","text":"<p>Given edge weights \\(w_{ij}\\), find a path from \\(s\\) to \\(t\\) minimizing total weight:  Efficiently solvable by Dijkstra\u2019s or Bellman\u2013Ford algorithms.</p>"},{"location":"convex/48_advanced_combinatorial/#minimum-spanning-tree-mst","title":"Minimum Spanning Tree (MST)","text":"<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal\u2019s or Prim\u2019s algorithm in \\(O(E\\log V)\\) time.</p>"},{"location":"convex/48_advanced_combinatorial/#maximum-flow-minimum-cut","title":"Maximum Flow / Minimum Cut","text":"<p>Determine how much \u201cflow\u201d can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>"},{"location":"convex/48_advanced_combinatorial/#integer-linear-programming-ilp","title":"Integer Linear Programming (ILP)","text":"<p>An integer program seeks:  </p> <p>It generalizes many classical problems:</p> <ul> <li>Knapsack  </li> <li>Assignment  </li> <li>Scheduling  </li> <li>Facility location</li> </ul> <p>Relaxing \\(x \\in \\mathbb{Z}^n\\) to \\(x \\in \\mathbb{R}^n\\) yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>"},{"location":"convex/48_advanced_combinatorial/#relaxation-and-rounding","title":"Relaxation and Rounding","text":"<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>"},{"location":"convex/48_advanced_combinatorial/#lp-relaxation","title":"LP Relaxation","text":"<p>For binary variables \\(x_i \\in \\{0,1\\}\\), relax to \\(0 \\le x_i \\le 1\\) and solve via simplex or interior-point methods.</p>"},{"location":"convex/48_advanced_combinatorial/#semidefinite-relaxation","title":"Semidefinite Relaxation","text":"<p>For quadratic binary problems, lift to a positive semidefinite matrix \\(X = xx^\\top\\):  Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>"},{"location":"convex/48_advanced_combinatorial/#randomized-rounding","title":"Randomized Rounding","text":"<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>"},{"location":"convex/48_advanced_combinatorial/#branch-and-bound-and-search-trees","title":"Branch-and-Bound and Search Trees","text":"<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>"},{"location":"convex/48_advanced_combinatorial/#basic-principle","title":"Basic Principle","text":"<ol> <li>Partition the feasible set into subsets (branching).  </li> <li>Compute upper/lower bounds for each subset.  </li> <li>Prune branches that cannot contain the optimum.  </li> </ol> <p>The algorithm systematically explores a search tree, guided by bounds.</p>"},{"location":"convex/48_advanced_combinatorial/#bounding-via-relaxations","title":"Bounding via Relaxations","text":"<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>"},{"location":"convex/48_advanced_combinatorial/#dynamic-programming","title":"Dynamic Programming","text":"<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p> \\[ \\text{OPT}(S) = \\min_{x \\in S} \\{ c(x) + \\text{OPT}(S') \\}. \\] <p>It is exact but can suffer from exponential growth (\u201ccurse of dimensionality\u201d).</p> <p>Applications:</p> <ul> <li>Shortest paths</li> <li>Sequence alignment</li> <li>Knapsack</li> <li>Resource allocation</li> </ul> <p>DP offers exact solutions when structure allows sequential decomposition.</p>"},{"location":"convex/48_advanced_combinatorial/#heuristics-and-metaheuristics-for-combinatorial-problems","title":"Heuristics and Metaheuristics for Combinatorial Problems","text":"<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>"},{"location":"convex/48_advanced_combinatorial/#greedy-heuristics","title":"Greedy Heuristics","text":"<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal\u2019s MST). Fast but not always globally optimal.</p>"},{"location":"convex/48_advanced_combinatorial/#local-search-and-hill-climbing","title":"Local Search and Hill Climbing","text":"<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>"},{"location":"convex/48_advanced_combinatorial/#metaheuristic-extensions","title":"Metaheuristic Extensions","text":"<ul> <li>Simulated Annealing: controlled random acceptance of worse moves.  </li> <li>Tabu Search: memory-based diversification.  </li> <li>Ant Colony Optimization: probabilistic path construction.  </li> <li>Genetic Algorithms and PSO: population-based evolution.  </li> </ul> <p>These approaches generalize to discrete structures with minimal problem-specific design.</p>"},{"location":"convex/48_advanced_combinatorial/#approximation-algorithms","title":"Approximation Algorithms","text":"<p>Some combinatorial problems are provably intractable but allow approximation guarantees:  where \\(\\alpha \\ge 1\\) is the approximation ratio.</p> <p>Examples:</p> <ul> <li>Greedy Set Cover: \\(\\alpha = \\ln n + 1\\) </li> <li>Christofides\u2019 Algorithm for TSP: \\(\\alpha = 1.5\\) </li> <li>MAX-CUT SDP Relaxation: \\(\\alpha \\approx 0.878\\)</li> </ul> <p>Approximation theory blends combinatorics with convex relaxation insights.</p>"},{"location":"convex/48_advanced_combinatorial/#advanced-topics-constraint-programming-and-decomposition","title":"Advanced Topics: Constraint Programming and Decomposition","text":""},{"location":"convex/48_advanced_combinatorial/#constraint-programming-cp","title":"Constraint Programming (CP)","text":"<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>"},{"location":"convex/48_advanced_combinatorial/#benders-and-dantzigwolfe-decomposition","title":"Benders and Dantzig\u2013Wolfe Decomposition","text":"<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>"},{"location":"convex/48_advanced_combinatorial/#cutting-plane-methods","title":"Cutting Plane Methods","text":"<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>"},{"location":"convex/48_advanced_combinatorial/#applications-across-domains","title":"Applications Across Domains","text":"Field Combinatorial Problem Examples Logistics Vehicle routing, warehouse layout Telecommunications Network design, channel allocation Machine Learning Feature selection, clustering, model compression Finance Portfolio optimization with integer positions Bioinformatics Genome assembly, protein structure inference <p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p> <p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p>"},{"location":"convex/50_future/","title":"23. The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":""},{"location":"convex/50_future/#chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence","title":"Chapter 23: The Future of Optimization \u2014 Learning, Adaptation, and Intelligence","text":"<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead \u2014 toward optimization methods that learn, adapt, and reason \u2014 merging human insight, data-driven modeling, and algorithmic intelligence.</p>"},{"location":"convex/50_future/#from-fixed-algorithms-to-adaptive-systems","title":"From Fixed Algorithms to Adaptive Systems","text":"<p>Traditional optimization algorithms are designed by experts and fixed in form:</p> \\[ x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\] <p>or</p> \\[ x_{k+1} = \\text{Update}(x_k, \\nabla f(x_k); \\theta_{\\text{fixed}}). \\] <p>But real-world problems change over time \u2014 data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>"},{"location":"convex/50_future/#optimization-as-learning","title":"Optimization as Learning","text":"<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p> <p>A meta-optimizer \\(\\text{Opt}_\\theta\\) is parameterized by \\(\\theta\\), and trained to minimize:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{f \\sim \\mathcal{D}}[f(\\text{Opt}_\\theta(f))], \\] <p>where \\(\\mathcal{D}\\) is a distribution over problem instances.</p> <p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>"},{"location":"convex/50_future/#reinforcement-learned-optimization","title":"Reinforcement-Learned Optimization","text":"<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p> <p>At each iteration:</p> <ul> <li>State: current iterate \\(x_t\\), gradient \\(\\nabla f(x_t)\\), and loss \\(f(x_t)\\) </li> <li>Action: choose an update \\(\\Delta x_t\\) </li> <li>Reward: improvement in objective, \\(r_t = -[f(x_{t+1}) - f(x_t)]\\)</li> </ul> <p>A policy \\(\\pi_\\theta\\) learns to output update steps that maximize expected reward. This creates an optimizer that discovers efficient update strategies through experience.</p> <p>RL-based optimizers have been successfully applied in:</p> <ul> <li>Hyperparameter tuning  </li> <li>Neural architecture search  </li> <li>Online control systems  </li> <li>Adaptive sampling and scheduling</li> </ul>"},{"location":"convex/50_future/#neuroevolution-and-population-learning","title":"Neuroevolution and Population Learning","text":"<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly. Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p> <p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p> \\[ \\nabla_\\theta \\mathbb{E}[f(\\theta)] \\approx \\frac{1}{\\sigma} \\mathbb{E}[f(\\theta + \\sigma \\epsilon)\\epsilon]. \\] <p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration\u2013exploitation.</p>"},{"location":"convex/50_future/#optimization-and-generative-models","title":"Optimization and Generative Models","text":"<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective: Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p> <p>For example:</p> <ul> <li>Optimize a latent vector \\(z\\) to generate a design with desired properties.  </li> <li>Use differentiable surrogates to backpropagate through generative pipelines.  </li> <li>Apply gradient-based search within learned manifolds.</li> </ul> <p>This blending of optimization and generation enables creativity \u2014 from molecule design to engineering shape synthesis.</p>"},{"location":"convex/50_future/#federated-and-decentralized-optimization","title":"Federated and Decentralized Optimization","text":"<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization. Each client \\(i\\) holds local data \\(D_i\\) and solves:</p> \\[ \\min_x \\; F(x) = \\frac{1}{N}\\sum_i f_i(x), \\] <p>without sharing raw data.</p> <p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p> <p>Challenges include:</p> <ul> <li>Communication efficiency  </li> <li>Heterogeneity of data and computation  </li> <li>Privacy and fairness constraints</li> </ul>"},{"location":"convex/50_future/#optimization-under-uncertainty","title":"Optimization Under Uncertainty","text":"<p>Modern systems often face uncertain environments: - Random perturbations in data - Dynamic constraints - Unpredictable feedback</p> <p>Approaches to manage uncertainty include:</p> <ol> <li> <p>Robust Optimization:    Minimize worst-case loss under bounded perturbations:     </p> </li> <li> <p>Stochastic Programming:    Optimize expected value or risk measure:     </p> </li> <li> <p>Distributionally Robust Optimization (DRO):    Hedge against model misspecification by optimizing over nearby probability distributions.</p> </li> </ol> <p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>"},{"location":"convex/50_future/#quantum-and-analog-optimization","title":"Quantum and Analog Optimization","text":"<p>As hardware advances, new paradigms emerge: - Quantum Annealing: uses quantum tunneling to escape local minima. - Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem. - Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p> <p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>"},{"location":"convex/50_future/#optimization-and-intelligence","title":"Optimization and Intelligence","text":"<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes \u2014 in essence, optimization loops.</p> <p>Emerging research seeks to unify:</p> <ul> <li>Learning to optimize \u2014 algorithms that adapt through data.  </li> <li>Optimizing to learn \u2014 systems that adjust representations via optimization.  </li> <li>Self-improving optimizers \u2014 algorithms that recursively tune their own parameters.</li> </ul> <p>This convergence blurs the line between optimizer and learner.</p> <p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve \u2014 autonomously, efficiently, and creatively.</p> <p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems \u2014 and thinkers \u2014 to improve themselves.</p>"},{"location":"convex/tutorials/1_lp/","title":"1 lp","text":"In\u00a0[55]: Copied! <pre>from __future__ import annotations\n\nimport re\nimport zipfile\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional, List\n\nimport numpy as np\nimport pandas as pd\nimport cvxpy as cp\nimport requests\n</pre> from __future__ import annotations  import re import zipfile from pathlib import Path from typing import Dict, Tuple, Optional, List  import numpy as np import pandas as pd import cvxpy as cp import requests  In\u00a0[56]: Copied! <pre>#!pip install cvxpy[glpk]\n#!pip install ecos\n</pre> #!pip install cvxpy[glpk] #!pip install ecos In\u00a0[57]: Copied! <pre>print(cp.installed_solvers())\n</pre> print(cp.installed_solvers())  <pre>['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'OSQP', 'SCIPY', 'SCS']\n</pre> In\u00a0[58]: Copied! <pre>WORKDIR = Path(\"./supply_chain_lp_work\")\nRAW_DIR = WORKDIR / \"raw\"\nEXTRACT_DIR = RAW_DIR / \"repo_extract\"\nOUT_DIR = WORKDIR / \"out\"\n\nWORKDIR.mkdir(parents=True, exist_ok=True)\nRAW_DIR.mkdir(parents=True, exist_ok=True)\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n</pre> WORKDIR = Path(\"./supply_chain_lp_work\") RAW_DIR = WORKDIR / \"raw\" EXTRACT_DIR = RAW_DIR / \"repo_extract\" OUT_DIR = WORKDIR / \"out\"  WORKDIR.mkdir(parents=True, exist_ok=True) RAW_DIR.mkdir(parents=True, exist_ok=True) OUT_DIR.mkdir(parents=True, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># -----------------------------\n# Utilities: download &amp; extract\n# -----------------------------\ndef download_zip(url: str, dest_path: Path) -&gt; None:\n    dest_path.parent.mkdir(parents=True, exist_ok=True)\n    print(f\"Downloading: {url}\")\n    r = requests.get(url, timeout=60)\n    r.raise_for_status()\n    dest_path.write_bytes(r.content)\n    print(f\"Saved zip to: {dest_path.resolve()}\")\n\n\ndef extract_zip(zip_path: Path, dest_dir: Path) -&gt; None:\n    dest_dir.mkdir(parents=True, exist_ok=True)\n    with zipfile.ZipFile(zip_path, \"r\") as zf:\n        zf.extractall(dest_dir)\n    print(f\"Extracted zip to: {dest_dir.resolve()}\")\n\n# -----------------------------\n# Cleaning helpers\n# -----------------------------\ndef canon(s: str) -&gt; str:\n    \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"\n    s = s.strip().lower()\n    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s\n\n\ndef clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = df.copy()\n    df.columns = [canon(c) for c in df.columns]\n    # Normalize common missing value tokens\n    df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})\n    return df\n\n\ndef load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:\n    print(f\"Loading workbook: {xlsx_path.resolve()}\")\n    xls = pd.ExcelFile(xlsx_path)\n    sheets = {}\n    for sheet_name in xls.sheet_names:\n        df = pd.read_excel(xls, sheet_name=sheet_name)\n        df = clean_df(df)\n        sheets[canon(sheet_name)] = df\n    print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")\n    return sheets\n</pre> # ----------------------------- # Utilities: download &amp; extract # ----------------------------- def download_zip(url: str, dest_path: Path) -&gt; None:     dest_path.parent.mkdir(parents=True, exist_ok=True)     print(f\"Downloading: {url}\")     r = requests.get(url, timeout=60)     r.raise_for_status()     dest_path.write_bytes(r.content)     print(f\"Saved zip to: {dest_path.resolve()}\")   def extract_zip(zip_path: Path, dest_dir: Path) -&gt; None:     dest_dir.mkdir(parents=True, exist_ok=True)     with zipfile.ZipFile(zip_path, \"r\") as zf:         zf.extractall(dest_dir)     print(f\"Extracted zip to: {dest_dir.resolve()}\")  # ----------------------------- # Cleaning helpers # ----------------------------- def canon(s: str) -&gt; str:     \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"     s = s.strip().lower()     s = re.sub(r\"[^a-z0-9]+\", \"_\", s)     s = re.sub(r\"_+\", \"_\", s).strip(\"_\")     return s   def clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:     df = df.copy()     df.columns = [canon(c) for c in df.columns]     # Normalize common missing value tokens     df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})     return df   def load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:     print(f\"Loading workbook: {xlsx_path.resolve()}\")     xls = pd.ExcelFile(xlsx_path)     sheets = {}     for sheet_name in xls.sheet_names:         df = pd.read_excel(xls, sheet_name=sheet_name)         df = clean_df(df)         sheets[canon(sheet_name)] = df     print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")     return sheets In\u00a0[61]: Copied! <pre>zip_path = RAW_DIR / \"LogisticsDataset_main.zip\"\nGITHUB_ZIP_URL = \"https://github.com/jaredbach/LogisticsDataset/archive/refs/heads/main.zip\"\n</pre> zip_path = RAW_DIR / \"LogisticsDataset_main.zip\" GITHUB_ZIP_URL = \"https://github.com/jaredbach/LogisticsDataset/archive/refs/heads/main.zip\" In\u00a0[62]: Copied! <pre>if not zip_path.exists():\n    download_zip(GITHUB_ZIP_URL, zip_path)\nelse:\n    print(f\"Zip already exists: {zip_path.resolve()}\")\n\nif EXTRACT_DIR.exists():\n        # keep it simple: don't re-extract unless user deletes folder\n    print(f\"Extract dir exists: {EXTRACT_DIR.resolve()}\")\nelse:\n    extract_zip(zip_path, EXTRACT_DIR)\n</pre> if not zip_path.exists():     download_zip(GITHUB_ZIP_URL, zip_path) else:     print(f\"Zip already exists: {zip_path.resolve()}\")  if EXTRACT_DIR.exists():         # keep it simple: don't re-extract unless user deletes folder     print(f\"Extract dir exists: {EXTRACT_DIR.resolve()}\") else:     extract_zip(zip_path, EXTRACT_DIR)   <pre>Zip already exists: C:\\Users\\salmank\\Documents\\convex_optimization\\docs\\convex\\tutorials\\supply_chain_lp_work\\raw\\LogisticsDataset_main.zip\nExtract dir exists: C:\\Users\\salmank\\Documents\\convex_optimization\\docs\\convex\\tutorials\\supply_chain_lp_work\\raw\\repo_extract\n</pre> In\u00a0[83]: Copied! <pre>xlsx_path\n</pre> xlsx_path Out[83]: <pre>WindowsPath('supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/SupplyChainLogisticsProblems.xlsx')</pre> In\u00a0[87]: Copied! <pre>xlsx_path = Path(\"supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/SupplyChainLogisticsProblems.xlsx\")\n</pre> xlsx_path = Path(\"supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/SupplyChainLogisticsProblems.xlsx\") In\u00a0[\u00a0]: Copied! <pre>sheets = load_excel_sheets(xlsx_path)\n</pre> sheets = load_excel_sheets(xlsx_path) <pre>Loading workbook: C:\\Users\\salmank\\Documents\\convex_optimization\\docs\\convex\\tutorials\\supply_chain_lp_work\\raw\\repo_extract\\LogisticsDataset-main\\SupplyChainLogisticsProblems.xlsx\nLoaded 7 sheets: ['orderlist', 'freightrates', 'whcosts', 'whcapacities', 'productsperplant', 'vmicustomers', 'plantports']\n</pre> In\u00a0[65]: Copied! <pre>## Demand measure\norders  = sheets['orderlist'][['order_id','weight']]\norders.columns = [\"order_id\", \"demand\"]\norders = orders[orders[\"demand\"] &gt; 0].copy()\norders[\"order_id\"] = orders[\"order_id\"].astype(str)\norders.head()\n</pre> ## Demand measure orders  = sheets['orderlist'][['order_id','weight']] orders.columns = [\"order_id\", \"demand\"] orders = orders[orders[\"demand\"] &gt; 0].copy() orders[\"order_id\"] = orders[\"order_id\"].astype(str) orders.head() Out[65]: order_id demand 0 1447296446.7 14.30 1 1447158014.7 87.94 2 1447138898.7 61.20 3 1447363527.7 16.16 4 1447363980.7 52.34 In\u00a0[66]: Copied! <pre># Plant capacities\nplants = sheets['whcapacities']\nplants.columns = [\"plant_id\", \"supply_cap\"]\nplants[\"plant_id\"] = plants[\"plant_id\"].astype(str)\nplants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\")\nplants = plants.dropna(subset=[\"supply_cap\"])\n#plants = plants.merge(cap, on=\"plant_id\", how=\"left\")\nplants.head()\n</pre> # Plant capacities plants = sheets['whcapacities'] plants.columns = [\"plant_id\", \"supply_cap\"] plants[\"plant_id\"] = plants[\"plant_id\"].astype(str) plants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\") plants = plants.dropna(subset=[\"supply_cap\"]) #plants = plants.merge(cap, on=\"plant_id\", how=\"left\") plants.head() Out[66]: plant_id supply_cap 0 PLANT15 11 1 PLANT17 8 2 PLANT18 111 3 PLANT05 385 4 PLANT02 138 In\u00a0[67]: Copied! <pre># dummy unit cost between for each plant-order pair\nrng = np.random.default_rng(7)\nplant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]}\n\n# Optional pruning: for each order, only keep MAX_PLANTS_PER_ORDER plants with lowest base cost factor\nplant_ids = plants[\"plant_id\"].tolist()\nif MAX_PLANTS_PER_ORDER is not None and MAX_PLANTS_PER_ORDER &lt; len(plant_ids):\n    # rank plants by plant_factor\n    ranked_plants = sorted(plant_ids, key=lambda p: plant_factor[p])\n    keep_plants = ranked_plants[:MAX_PLANTS_PER_ORDER]\nelse:\n    keep_plants = plant_ids\n\nlanes = []\nfor _, o in orders.iterrows():\n    oid = o[\"order_id\"]\n    dem = float(o[\"demand\"])\n    order_jitter = 0.95 + 0.1 * rng.random()\n    for pid in keep_plants:\n        unit_cost = 1 * plant_factor[pid] * order_jitter\n        lanes.append((pid, oid, unit_cost))\n\nlanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"])\nlanes.head()\n</pre> # dummy unit cost between for each plant-order pair rng = np.random.default_rng(7) plant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]}  # Optional pruning: for each order, only keep MAX_PLANTS_PER_ORDER plants with lowest base cost factor plant_ids = plants[\"plant_id\"].tolist() if MAX_PLANTS_PER_ORDER is not None and MAX_PLANTS_PER_ORDER &lt; len(plant_ids):     # rank plants by plant_factor     ranked_plants = sorted(plant_ids, key=lambda p: plant_factor[p])     keep_plants = ranked_plants[:MAX_PLANTS_PER_ORDER] else:     keep_plants = plant_ids  lanes = [] for _, o in orders.iterrows():     oid = o[\"order_id\"]     dem = float(o[\"demand\"])     order_jitter = 0.95 + 0.1 * rng.random()     for pid in keep_plants:         unit_cost = 1 * plant_factor[pid] * order_jitter         lanes.append((pid, oid, unit_cost))  lanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"]) lanes.head() Out[67]: plant_id order_id unit_cost 0 PLANT06 1447296446.7 0.841326 1 PLANT05 1447296446.7 0.933604 2 PLANT11 1447296446.7 0.946049 3 PLANT12 1447296446.7 0.955933 4 PLANT02 1447296446.7 0.965054 In\u00a0[68]: Copied! <pre># Solve LP in CVXPY\n\n# Index maps\nplant_ids = plants[\"plant_id\"].astype(str).tolist()\norder_ids = orders[\"order_id\"].astype(str).tolist()\n\nplant_index = {p: i for i, p in enumerate(plant_ids)}\norder_index = {o: j for j, o in enumerate(order_ids)}\n\n# Keep only lanes with valid endpoints\nlanes = lanes[lanes[\"plant_id\"].isin(plant_index) &amp; lanes[\"order_id\"].isin(order_index)].copy()\nlanes = lanes.reset_index(drop=True)\n\nnA = len(lanes)\nnP = len(plant_ids)\nnO = len(order_ids)\n</pre> # Solve LP in CVXPY  # Index maps plant_ids = plants[\"plant_id\"].astype(str).tolist() order_ids = orders[\"order_id\"].astype(str).tolist()  plant_index = {p: i for i, p in enumerate(plant_ids)} order_index = {o: j for j, o in enumerate(order_ids)}  # Keep only lanes with valid endpoints lanes = lanes[lanes[\"plant_id\"].isin(plant_index) &amp; lanes[\"order_id\"].isin(order_index)].copy() lanes = lanes.reset_index(drop=True)  nA = len(lanes) nP = len(plant_ids) nO = len(order_ids)    In\u00a0[69]: Copied! <pre>order_to_arcs: List[List[int]] = [[] for _ in range(nO)]\nplant_to_arcs: List[List[int]] = [[] for _ in range(nP)]\n\nfor a, row in lanes.iterrows():\n    i = plant_index[str(row[\"plant_id\"])]\n    j = order_index[str(row[\"order_id\"])]\n    plant_to_arcs[i].append(a)\n    order_to_arcs[j].append(a)\n\n# Data vectors\ncost = lanes[\"unit_cost\"].to_numpy(dtype=float)\ndemand = orders.set_index(\"order_id\").loc[order_ids, \"demand\"].to_numpy(dtype=float)\nsupply = plants.set_index(\"plant_id\").loc[plant_ids, \"supply_cap\"].to_numpy(dtype=float)\n</pre> order_to_arcs: List[List[int]] = [[] for _ in range(nO)] plant_to_arcs: List[List[int]] = [[] for _ in range(nP)]  for a, row in lanes.iterrows():     i = plant_index[str(row[\"plant_id\"])]     j = order_index[str(row[\"order_id\"])]     plant_to_arcs[i].append(a)     order_to_arcs[j].append(a)  # Data vectors cost = lanes[\"unit_cost\"].to_numpy(dtype=float) demand = orders.set_index(\"order_id\").loc[order_ids, \"demand\"].to_numpy(dtype=float) supply = plants.set_index(\"plant_id\").loc[plant_ids, \"supply_cap\"].to_numpy(dtype=float)  In\u00a0[70]: Copied! <pre># Decision variable\nx = cp.Variable(nA, nonneg=True)\n</pre> # Decision variable x = cp.Variable(nA, nonneg=True)  In\u00a0[71]: Copied! <pre>constraints = []\n\n# Order demand equalities\norder_constraints = []\nfor j in range(nO):\n    arcs_j = order_to_arcs[j]\n    if not arcs_j:\n        raise ValueError(f\"Order {order_ids[j]} has no incoming lanes; infeasible.\")\n    con = cp.sum(x[arcs_j]) == demand[j]\n    constraints.append(con)\n    order_constraints.append(con)\n\n# Plant supply inequalities\nplant_constraints = []\nfor i in range(nP):\n    arcs_i = plant_to_arcs[i]\n    if not arcs_i:\n        # Plant unused; constraint 0 &lt;= supply holds trivially\n        continue\n    con = cp.sum(x[arcs_i]) &lt;= supply[i]\n    constraints.append(con)\n    plant_constraints.append((plant_ids[i], con))\n\nobj = cp.Minimize(cost @ x)\nprob = cp.Problem(obj, constraints)\n</pre> constraints = []  # Order demand equalities order_constraints = [] for j in range(nO):     arcs_j = order_to_arcs[j]     if not arcs_j:         raise ValueError(f\"Order {order_ids[j]} has no incoming lanes; infeasible.\")     con = cp.sum(x[arcs_j]) == demand[j]     constraints.append(con)     order_constraints.append(con)  # Plant supply inequalities plant_constraints = [] for i in range(nP):     arcs_i = plant_to_arcs[i]     if not arcs_i:         # Plant unused; constraint 0 &lt;= supply holds trivially         continue     con = cp.sum(x[arcs_i]) &lt;= supply[i]     constraints.append(con)     plant_constraints.append((plant_ids[i], con))  obj = cp.Minimize(cost @ x) prob = cp.Problem(obj, constraints)  In\u00a0[72]: Copied! <pre>print(cp.installed_solvers())\n</pre> print(cp.installed_solvers()) <pre>['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'OSQP', 'SCIPY', 'SCS']\n</pre> In\u00a0[75]: Copied! <pre>from cvxopt import glpk\n</pre> from cvxopt import glpk In\u00a0[77]: Copied! <pre>prob.solve(solver=cp.GLPK, verbose=False)\n</pre> prob.solve(solver=cp.GLPK, verbose=False) <pre>---------------------------------------------------------------------------\nSolverError                               Traceback (most recent call last)\nCell In[77], line 1\n----&gt; 1 prob.solve(solver=cp.GLPK, verbose=False)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\cvxpy\\problems\\problem.py:609, in Problem.solve(self, *args, **kwargs)\n    606         raise ValueError(\n    607             \"Cannot specify both 'solver' and 'solver_path'. Please choose one.\")\n    608     return self._solve_solver_path(solve_func,solver_path, args, kwargs)\n--&gt; 609 return solve_func(self, *args, **kwargs)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\cvxpy\\problems\\problem.py:1191, in Problem._solve(self, solver, warm_start, verbose, bibtex, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\n   1188         self.unpack(chain.retrieve(soln))\n   1189         return self.value\n-&gt; 1191 data, solving_chain, inverse_data = self.get_problem_data(\n   1192     solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, kwargs\n   1193 )\n   1195 if verbose:\n   1196     print(_NUM_SOLVER_STR)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\cvxpy\\problems\\problem.py:755, in Problem.get_problem_data(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\n    753 if key != self._cache.key:\n    754     self._cache.invalidate()\n--&gt; 755     solving_chain = self._construct_chain(\n    756         solver=solver, gp=gp,\n    757         enforce_dpp=enforce_dpp,\n    758         ignore_dpp=ignore_dpp,\n    759         canon_backend=canon_backend,\n    760         solver_opts=solver_opts)\n    761     self._cache.key = key\n    762     self._cache.solving_chain = solving_chain\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\cvxpy\\problems\\problem.py:1007, in Problem._construct_chain(self, solver, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\n    965 def _construct_chain(\n    966         self,\n    967         solver: Optional[str] = None,\n   (...)\n    972         solver_opts: Optional[dict] = None\n    973 ) -&gt; SolvingChain:\n    974     \"\"\"\n    975     Construct the chains required to reformulate and solve the problem.\n    976 \n   (...)\n   1005     A solving chain\n   1006     \"\"\"\n-&gt; 1007     candidate_solvers = self._find_candidate_solvers(solver=solver, gp=gp)\n   1008     self._sort_candidate_solvers(candidate_solvers)\n   1009     return construct_solving_chain(self, candidate_solvers, gp=gp,\n   1010                                    enforce_dpp=enforce_dpp,\n   1011                                    ignore_dpp=ignore_dpp,\n   1012                                    canon_backend=canon_backend,\n   1013                                    solver_opts=solver_opts,\n   1014                                    specified_solver=solver)\n\nFile c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\cvxpy\\problems\\problem.py:870, in Problem._find_candidate_solvers(self, solver, gp)\n    868 if solver is not None:\n    869     if solver not in slv_def.INSTALLED_SOLVERS:\n--&gt; 870         raise error.SolverError(\"The solver %s is not installed.\" % solver)\n    871     if solver in slv_def.CONIC_SOLVERS:\n    872         candidates['conic_solvers'] += [solver]\n\nSolverError: The solver GLPK is not installed.</pre> In\u00a0[\u00a0]: Copied! <pre>solved = False\nlast_err = None\nfor solver in [cp.GLPK, cp.ECOS]:\n    try:\n        prob.solve(solver=solver, verbose=False)\n        if prob.status in (\"optimal\", \"optimal_inaccurate\"):\n            solved = True\n            break\n    except Exception as e:\n        last_err = e\n\nif not solved:\n    raise RuntimeError(f\"LP did not solve. status={prob.status}. last_err={last_err}\")\n\n# Extract solution\nflow = x.value\nsol = lanes.copy()\nsol[\"flow\"] = flow\nsol = sol[sol[\"flow\"] &gt; 1e-9].copy()\nsol[\"ship_cost\"] = sol[\"unit_cost\"] * sol[\"flow\"]\n\nduals = {\n    \"order_shadow_price\": np.array([c.dual_value for c in order_constraints], dtype=float),\n    \"objective_value\": float(prob.value),\n    \"status\": prob.status,\n}\n</pre> solved = False last_err = None for solver in [cp.GLPK, cp.ECOS]:     try:         prob.solve(solver=solver, verbose=False)         if prob.status in (\"optimal\", \"optimal_inaccurate\"):             solved = True             break     except Exception as e:         last_err = e  if not solved:     raise RuntimeError(f\"LP did not solve. status={prob.status}. last_err={last_err}\")  # Extract solution flow = x.value sol = lanes.copy() sol[\"flow\"] = flow sol = sol[sol[\"flow\"] &gt; 1e-9].copy() sol[\"ship_cost\"] = sol[\"unit_cost\"] * sol[\"flow\"]  duals = {     \"order_shadow_price\": np.array([c.dual_value for c in order_constraints], dtype=float),     \"objective_value\": float(prob.value),     \"status\": prob.status, }  <pre>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[45], line 13\n     10         last_err = e\n     12 if not solved:\n---&gt; 13     raise RuntimeError(f\"LP did not solve. status={prob.status}. last_err={last_err}\")\n     15 # Extract solution\n     16 flow = x.value\n\nRuntimeError: LP did not solve. status=None. last_err=The solver ECOS is not installed.</pre>"},{"location":"convex/tutorials/1_lp/#download-data","title":"Download Data\u00b6","text":""},{"location":"convex/tutorials/1_lp/#cvxpy-solver","title":"CVXPY Solver\u00b6","text":""},{"location":"convex/tutorials/1_ls/","title":"1 ls","text":"In\u00a0[1]: Copied! <pre>import cvxpy as cp\nimport numpy\n</pre> import cvxpy as cp import numpy In\u00a0[2]: Copied! <pre>m = 30\nn = 20\nnumpy.random.seed(1)\nA = numpy.random.randn(m, n)\nb = numpy.random.randn(m)\n</pre> m = 30 n = 20 numpy.random.seed(1) A = numpy.random.randn(m, n) b = numpy.random.randn(m) In\u00a0[3]: Copied! <pre>x = cp.Variable(n)\nobjective = cp.Minimize(cp.sum_squares(A @ x - b))\nconstraints = [0 &lt;= x, x &lt;= 1]\nprob = cp.Problem(objective, constraints)\n</pre> x = cp.Variable(n) objective = cp.Minimize(cp.sum_squares(A @ x - b)) constraints = [0 &lt;= x, x &lt;= 1] prob = cp.Problem(objective, constraints) In\u00a0[4]: Copied! <pre># The optimal objective is returned by prob.solve().\nresult = prob.solve()\n# The optimal value for x is stored in x.value.\nprint(x.value)\n# The optimal Lagrange multiplier for a constraint\n# is stored in constraint.dual_value.\nprint(constraints[0].dual_value)\n</pre> # The optimal objective is returned by prob.solve(). result = prob.solve() # The optimal value for x is stored in x.value. print(x.value) # The optimal Lagrange multiplier for a constraint # is stored in constraint.dual_value. print(constraints[0].dual_value) <pre>[-1.76373908e-19  2.85112420e-02  2.77057133e-19  3.29328765e-20\n -2.72421603e-19  1.49285011e-01 -1.00029811e-19  8.38271167e-20\n  2.46718649e-01  5.78224144e-01 -4.02565988e-19  1.01242860e-03\n -9.32502155e-20  2.26767464e-01 -1.57018657e-19 -8.90554231e-20\n -1.21142691e-19 -1.53804616e-19  1.11248970e-19 -3.47407896e-19]\n[ 2.50938945  0.          2.78354615  1.79425782 13.08579183  0.\n  0.73716363  3.35344995  0.          0.          8.93825054  0.\n  7.02955161  0.          4.71068649  3.18873635  2.06090107 10.08166738\n  3.0481157   8.53268239]\n</pre> In\u00a0[5]: Copied! <pre>print(\"\\nThe optimal value is\", prob.value)\nprint(\"The optimal x is\")\nprint(x.value)\nprint(\"The norm of the residual is \", cp.norm(A @ x - b, p=2).value)\n</pre> print(\"\\nThe optimal value is\", prob.value) print(\"The optimal x is\") print(x.value) print(\"The norm of the residual is \", cp.norm(A @ x - b, p=2).value)  <pre>The optimal value is 19.831263706445025\nThe optimal x is\n[-1.76373908e-19  2.85112420e-02  2.77057133e-19  3.29328765e-20\n -2.72421603e-19  1.49285011e-01 -1.00029811e-19  8.38271167e-20\n  2.46718649e-01  5.78224144e-01 -4.02565988e-19  1.01242860e-03\n -9.32502155e-20  2.26767464e-01 -1.57018657e-19 -8.90554231e-20\n -1.21142691e-19 -1.53804616e-19  1.11248970e-19 -3.47407896e-19]\nThe norm of the residual is  4.453230704381374\n</pre>"},{"location":"convex/tutorials/1_ls/#least-squares-problem-where-the-variable-is-constrained-by-lower-and-upper-bounds","title":"Least-squares problem where the variable is constrained by lower and upper bounds\u00b6","text":""},{"location":"convex/tutorials/1_ls/#data","title":"Data\u00b6","text":""},{"location":"convex/tutorials/1_ls/#construct-the-problem","title":"Construct the problem\u00b6","text":""},{"location":"convex/tutorials/supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/","title":"LogisticsDataset","text":""},{"location":"convex/tutorials/supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/#logisticsdataset","title":"LogisticsDataset","text":"<p>Supply Chain Logistics Problem Dataset</p> <p></p> <p>I found this dataset at Brunel University London. This repository contains the original dataset in its Excel format from Brunel University London, as well as a Python script that I wrote to split-up each of the worksheets in the Excel file into seperate CSV files.</p> <p>The SQL files in this repository are what I used to create a snowflake schema.</p> <p>To learn more about this dataset and the variables, please visit the link above.</p>"},{"location":"convex/tutorials/supply_chain_lp_work/raw/repo_extract/LogisticsDataset-main/Excel%20Sheets%20to%20CSV/","title":"Excel Sheets to CSV","text":"In\u00a0[1]: Copied! <pre># Import Packages\nimport pandas as pd\nimport os\n\n# Define the Excel File Variable\nExcel_WB = 'SupplyChainLogisticsProblems.xlsx'\n\n# Create a Data Frame from the Excel File\nExcel_DF = pd.ExcelFile(Excel_WB)\n</pre> # Import Packages import pandas as pd import os  # Define the Excel File Variable Excel_WB = 'SupplyChainLogisticsProblems.xlsx'  # Create a Data Frame from the Excel File Excel_DF = pd.ExcelFile(Excel_WB) In\u00a0[2]: Copied! <pre># Set Index\nTemp_Index = 0\n\n# Export the Sheets as CSVs\nfor sheet in Excel_DF.sheet_names:\n    Temp_DF = Excel_DF.parse(Temp_Index, index=False) # The Temporary Holding Place of a Sheet as it Goes Through the Loop\n    Temp_DF = Temp_DF[Temp_DF.filter(regex='^(?!Unnamed)').columns] # Drop NoName Columns\n    CSV_File = sheet+'.csv' # Name of CSV File That We Will Export (Same as Sheet Name)\n    os.chdir('/Users/jibach/Documents/GitHub/LogisticsDataset/CSVs') # Define Path Where I Want CSVs Stored\n    Temp_DF.to_csv(CSV_File, index=False) # Export the DF to CSV\n    Temp_Index += 1 # Reset the Index, Adding a '1' Moves Us to the Next Sheet\n</pre> # Set Index Temp_Index = 0  # Export the Sheets as CSVs for sheet in Excel_DF.sheet_names:     Temp_DF = Excel_DF.parse(Temp_Index, index=False) # The Temporary Holding Place of a Sheet as it Goes Through the Loop     Temp_DF = Temp_DF[Temp_DF.filter(regex='^(?!Unnamed)').columns] # Drop NoName Columns     CSV_File = sheet+'.csv' # Name of CSV File That We Will Export (Same as Sheet Name)     os.chdir('/Users/jibach/Documents/GitHub/LogisticsDataset/CSVs') # Define Path Where I Want CSVs Stored     Temp_DF.to_csv(CSV_File, index=False) # Export the DF to CSV     Temp_Index += 1 # Reset the Index, Adding a '1' Moves Us to the Next Sheet In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deeplearning/1_mlp/","title":"1. Introduction to Deep Learning Optimization","text":""},{"location":"deeplearning/1_mlp/#an-introduction-to-neural-networks","title":"An Introduction to Neural Networks","text":""},{"location":"deeplearning/1_mlp/#1-neural-networks-as-computation-graphs","title":"1. Neural Networks as Computation Graphs","text":"<p>Modern neural networks are best understood as differentiable computation graphs.  They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p> <p>Each node in this graph corresponds to a function:</p> \\[z_i = f_i(x_1, \\dots, x_k)\\] <p>and the entire network defines a composite function:</p> \\[f_\\theta(x) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(x)\\] <p>where \\(\\theta = \\{W_i, b_i\\}\\) denotes all learnable parameters.</p>"},{"location":"deeplearning/1_mlp/#formal-structure","title":"Formal Structure","text":"<p>For a Multilayer Perceptron (MLP):</p> \\[h_0 = x, \\quad h_i = \\sigma(W_i h_{i-1} + b_i), \\quad i=1,\\dots,L-1, \\quad \\hat{y} = W_L h_{L-1} + b_L\\] <p>with: \\(W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}, \\quad b_i \\in \\mathbb{R}^{d_i}\\)</p> <p>Each layer is a small differentiable function. When we connect them, we form a composite map \u2014 the fundamental abstraction underlying autodiff, backprop, and learning.</p> <p>Key property: Because every node in the graph is differentiable, the entire function \\(f_\\theta(x)\\) is differentiable with respect to both input \\(x\\) and parameters \\(\\theta\\).</p> <p>Graphically, the network is a directed acyclic graph (DAG):</p> <ul> <li>Edges: carry tensor values.</li> <li>Nodes: represent differentiable functions.</li> <li>Forward pass: evaluates node outputs.</li> <li>Backward pass: propagates sensitivities (gradients) backward.</li> </ul> <p>This graph abstraction unifies all architectures \u2014 CNNs, RNNs, Transformers, Diffusion Models \u2014 as differentiable computation graphs.</p>"},{"location":"deeplearning/1_mlp/#2-gradients-jacobians-and-differentiation","title":"2. Gradients, Jacobians, and Differentiation","text":"<p>For any function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian matrix \\(J_f(x)\\) encodes local derivatives:</p> \\[[J_f(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\] <p>In neural networks, we often deal with a scalar loss function:</p> \\[L(\\theta) = \\ell(f_\\theta(x), y)\\] <p>and want: </p> \\[\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\] <p>However, computing full Jacobians is computationally infeasible \u2014 for a network with millions of parameters, explicit Jacobians would have trillions of entries. Instead, automatic differentiation (autodiff) computes vector\u2013Jacobian products efficiently.</p> <p>For scalar loss \\(L\\): \\(\\nabla_\\theta L = J_{f_\\theta}(x)^T \\nabla_{f_\\theta} L\\)</p> <p>where \\(J_{f_\\theta}(x)\\) is the Jacobian of the output w.r.t. parameters.</p> <p>This operation can be done efficiently in reverse-mode autodiff \u2014 the heart of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#3-forward-and-backward-passes","title":"3. Forward and Backward Passes","text":""},{"location":"deeplearning/1_mlp/#forward-pass","title":"Forward Pass","text":"<p>Given input \\(x\\) and parameters \\(\\theta\\):</p> <ol> <li>Compute layer outputs sequentially: \\(h_i = \\sigma(W_i h_{i-1} + b_i)\\)</li> <li>Compute loss \\(L = \\ell(f_\\theta(x), y)\\)</li> <li>Store intermediate activations \\(h_i\\) for reuse during backpropagation.</li> </ol> <p>This pass evaluates the function \\(L(\\theta)\\).</p>"},{"location":"deeplearning/1_mlp/#backward-pass","title":"Backward Pass","text":"<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p> <p>\\(\\frac{\\partial L}{\\partial \\theta_i} =  \\frac{\\partial L}{\\partial h_L} \\frac{\\partial h_L}{\\partial h_{L-1}} \\dots \\frac{\\partial h_{i+1}}{\\partial \\theta_i}\\)</p> <p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p> <p>Reverse-mode autodiff (backprop) algorithm:</p> <ol> <li>Initialize \\(\\bar{h}_L = \\frac{\\partial L}{\\partial h_L} = 1\\).</li> <li>For each layer \\(l = L, L-1, \\dots, 1\\):</li> <li>Compute local derivative \\(\\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Accumulate gradient: \\(\\bar{h}_{l-1} = \\bar{h}_l \\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Compute parameter gradients: \\(\\frac{\\partial L}{\\partial W_l} = \\bar{h}_l (h_{l-1})^T\\)</li> <li>Return all \\(\\nabla_\\theta L\\).</li> </ol> <p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#4-chain-rule-backpropagation-and-automatic-differentiation","title":"4. Chain Rule, Backpropagation, and Automatic Differentiation","text":"<p>The chain rule underpins all gradient computation. For scalar functions:</p> <p>\\(\\frac{dL}{dx} = \\frac{dL}{dz} \\frac{dz}{dx}\\)</p> <p>and recursively for multivariate functions:</p> <p>\\(\\nabla_x L = J_{z}(x)^T \\nabla_z L\\)</p> <p>Autodiff implements this automatically, performing either:</p> <ul> <li>Forward-mode AD: propagates derivatives forward, efficient when #inputs \u226a #outputs.</li> <li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs \u226a #inputs (our case).</li> </ul> <p>Reverse-mode AD \u2261 backpropagation.</p> <p>Computational Complexity: - Cost \u2248 2\u00d7 forward pass (one forward, one backward). - Memory \u2248 size of stored activations.</p> <p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p> <p>\\(\\min_\\theta L(\\theta)\\)</p> <p>where \\(L\\) is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms. s</p>"},{"location":"deeplearning/1_mlp/#5-from-gradients-to-optimization","title":"5. From Gradients to Optimization","text":"<p>The Learning Problem - Training a neural network means solving:</p> <p>\\(\\min_\\theta \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} [\\,\\ell(f_\\theta(x), y)\\,]\\)</p> <p>Since the true data distribution \\(\\mathcal{D}\\) is unknown, we use empirical risk minimization (ERM):</p> <p>\\(\\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\ell(f_\\theta(x_i), y_i)\\)</p> <p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods \u2014 powered by backpropagation \u2014 reliably find good solutions.</p>"},{"location":"deeplearning/1_mlp/#first-order-optimization-algorithms","title":"First-Order Optimization Algorithms","text":"<p>All modern deep learning optimization relies on gradients:</p> <p>\\(\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\)</p> <p>The basic rule: update parameters in the direction of negative gradient:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_t\\)</p> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"deeplearning/1_mlp/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>We use mini-batches instead of full data:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\frac{1}{|B_t|}\\sum_{i \\in B_t} \\ell(f_\\theta(x_i), y_i)\\)</p> <ul> <li>Cheap per-step computation.</li> <li>Introduces gradient noise, which helps escape shallow minima and saddle points.</li> </ul>"},{"location":"deeplearning/1_mlp/#momentum","title":"Momentum","text":"<p>Accelerates learning by accumulating a velocity vector:</p> <p>\\(v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L_t, \\quad \\theta_{t+1} = \\theta_t + v_{t+1}\\)</p> <p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>"},{"location":"deeplearning/1_mlp/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Maintains exponentially weighted averages of gradients and squared gradients:</p> <p>\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla_\\theta L_t\\)</p> <p>\\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla_\\theta L_t)^2\\)</p> <p>Bias-corrected updates:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\\)</p> <p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>"},{"location":"deeplearning/1_mlp/#second-order-and-curvature-aware-methods","title":"Second-Order and Curvature-Aware Methods","text":"<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p> <p>\\(H = \\frac{\\partial^2 L}{\\partial \\theta^2}\\)</p> <p>Newton\u2019s update:</p> <p>\\(\\theta_{t+1} = \\theta_t - H^{-1}\\nabla_\\theta L\\)</p> <p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets. Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>"},{"location":"deeplearning/1_mlp/#optimization-landscape-and-gradient-flow","title":"Optimization Landscape and Gradient Flow","text":"<p>Although neural network loss surfaces are highly nonconvex, they possess favorable geometry:</p> <ul> <li>Most critical points are saddle points, not local minima.</li> <li>Wide, flat minima generalize better (implicit regularization of SGD).</li> <li>Gradient noise helps explore valleys in high-dimensional space.</li> </ul> <p>Gradient flow (continuous limit of SGD):</p> <p>\\(\\frac{d\\theta(t)}{dt} = - \\nabla_\\theta L(\\theta(t))\\)</p> <p>describes a trajectory in parameter space governed by the vector field of gradients.</p> <p>The optimization algorithm defines the dynamics of this flow (e.g., momentum adds inertia).</p>"},{"location":"deeplearning/1_mlp/#6-what-mlps-cant-do","title":"6. What MLPs Can\u2019t Do?","text":""},{"location":"deeplearning/1_mlp/#a-multiplicative-interactions","title":"(a) Multiplicative Interactions","text":"<p>MLPs compute sums of weighted activations \u2014 inherently additive operations:</p> <p>\\(h = \\sigma(Wx + b)\\)</p> <p>They cannot naturally represent multiplicative relationships (like \\(x_1 x_2\\)) unless approximated via nonlinear stacking, which is inefficient.</p> <p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>"},{"location":"deeplearning/1_mlp/#b-attention-and-dynamic-routing","title":"(b) Attention and Dynamic Routing","text":"<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V\\)</p> <p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>"},{"location":"deeplearning/1_mlp/#c-metric-learning-and-inductive-bias","title":"(c) Metric Learning and Inductive Bias","text":"<p>MLPs lack structural priors about similarity or geometry. Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p> <p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient \u2014 the landscape becomes smoother and gradients more informative.</p>"},{"location":"deeplearning/1_mlp/#7-beyond-backprop-curvature-generalization-and-geometry","title":"7. Beyond Backprop: Curvature, Generalization, and Geometry","text":"<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>"},{"location":"deeplearning/1_mlp/#natural-gradient","title":"Natural Gradient","text":"<p>Instead of minimizing loss directly in parameter space, we minimize it in function space:</p> <p>\\(\\Delta \\theta = - \\eta F^{-1} \\nabla_\\theta L\\)</p> <p>where \\(F\\) is the Fisher information matrix:</p> <p>\\(F = \\mathbb{E}\\left[\\nabla_\\theta \\log p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x)^T\\right]\\)</p> <p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>"},{"location":"deeplearning/1_mlp/#implicit-bias-of-gradient-descent","title":"Implicit Bias of Gradient Descent","text":"<p>Even in overparameterized models, gradient descent tends to find low-norm or flat minima that generalize better \u2014 a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>"},{"location":"deeplearning/1_mlp/#optimization-as-inference","title":"Optimization as Inference","text":"<p>Many modern perspectives view training as approximate inference:</p> <p>\\(p(\\theta | D) \\propto e^{-L(\\theta)/T}\\)</p> <p>Gradient descent samples from this energy landscape as \\(T \\to 0\\); stochastic variants like SGD approximate Bayesian inference under certain limits.</p>"},{"location":"deeplearning/2_convnets/","title":"2. Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#chapter-2-convolutional-neural-networks-cnns","title":"Chapter 2: Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#1-core-principles-locality-and-translation-invariance","title":"1. Core Principles: Locality and Translation Invariance","text":"<p>Before understanding convolutional networks, it\u2019s crucial to grasp why they exist \u2014 the structural priors they impose on data.</p>"},{"location":"deeplearning/2_convnets/#11-locality","title":"1.1 Locality","text":"<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p> <p>For example:</p> <ul> <li>Adjacent pixels in an image often belong to the same object or texture.</li> <li>Neighboring audio samples belong to the same phoneme.</li> <li>Nearby words in a sentence influence each other\u2019s meaning.</li> </ul> <p>MLPs treat every input dimension as independent, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, local region of the input, called its receptive field.</p> <p>Formally, for an input \\(x \\in \\mathbb{R}^{H \\times W}\\), a neuron at position \\((i,j)\\) in a CNN depends only on values in a small window \\(\\Omega(i,j)\\):  This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>"},{"location":"deeplearning/2_convnets/#12-translation-invariance","title":"1.2 Translation Invariance","text":"<p>Natural patterns are repeatable across locations \u2014 the same feature (e.g., an edge, a cat\u2019s ear) can appear anywhere in the image.</p> <p>An MLP would need to learn a separate detector for each position. CNNs overcome this through weight sharing: the same filter \\(W\\) is applied across all spatial positions.</p> <p>Mathematically:  </p> <p>This operation \u2014 convolution \u2014 ensures translation equivariance:  meaning if the input shifts by \\(\\Delta\\), the output shifts by the same amount. After pooling, this becomes translation invariance, i.e. the output doesn\u2019t change under small shifts.</p> <p>These two properties \u2014 locality and translation invariance \u2014 are the foundation of convolutional architectures.</p>"},{"location":"deeplearning/2_convnets/#2-motivation-why-convolutions","title":"2. Motivation: Why Convolutions?","text":"<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos. An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p> <p>Example: For a 256\u00d7256 RGB image (\u2248200K input features), even one hidden layer with 1,000 neurons requires: \\(\\((256 \\times 256 \\times 3) \\times 1000 = 196\\,\\text{million weights}.\\)\\)</p> <p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p> <p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>"},{"location":"deeplearning/2_convnets/#3-the-convolution-operation","title":"3. The Convolution Operation","text":""},{"location":"deeplearning/2_convnets/#31-discrete-convolution","title":"3.1 Discrete Convolution","text":"<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p> <p>For 2D inputs (e.g. images):</p> \\[ S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n) \\] <ul> <li>\\(I\\) \u2014 input (image)</li> <li>\\(K\\) \u2014 kernel (filter)</li> <li>\\(S\\) \u2014 output feature map</li> </ul> <p>Each filter detects a specific local pattern (edges, corners, textures).</p>"},{"location":"deeplearning/2_convnets/#32-convolution-in-neural-networks","title":"3.2 Convolution in Neural Networks","text":"<p>In CNNs, the convolution becomes a learnable operation:</p> \\[ h_{i,j,k} = \\sigma\\left( \\sum_{c=1}^{C_\\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \\right) \\] <ul> <li>\\(x_c\\): input channel \\(c\\) (e.g. R, G, B)</li> <li>\\(W_{k,c}\\): kernel for output channel \\(k\\) and input channel \\(c\\)</li> <li>\\(b_k\\): bias for output channel \\(k\\)</li> <li>\\(\\sigma\\): nonlinearity (ReLU, etc.)</li> </ul> <p>This produces \\(C_\\text{out}\\) feature maps, each representing a learned spatial pattern.</p> <p>Weight sharing drastically reduces parameters: Each kernel might be \\(3 \\times 3\\) or \\(5 \\times 5\\) \u2014 independent of image size.</p>"},{"location":"deeplearning/2_convnets/#4-building-blocks-of-cnns","title":"4. Building Blocks of CNNs","text":""},{"location":"deeplearning/2_convnets/#41-convolutional-layer","title":"4.1 Convolutional Layer","text":"<p>Performs learnable filtering and produces feature maps.</p> <p>If input has shape \\((H, W, C_\\text{in})\\): - Kernel: \\((k_H, k_W, C_\\text{in}, C_\\text{out})\\) - Output: \\((H', W', C_\\text{out})\\)</p>"},{"location":"deeplearning/2_convnets/#42-nonlinear-activation","title":"4.2 Nonlinear Activation","text":"<p>After convolution, apply nonlinearity (commonly ReLU):  </p>"},{"location":"deeplearning/2_convnets/#43-pooling-layer","title":"4.3 Pooling Layer","text":"<p>Reduces spatial dimensions and increases invariance.</p> <ul> <li>Max pooling: selects the largest value in a patch.</li> <li>Average pooling: takes mean value.</li> </ul> <p>Formally:  </p> <p>Pooling introduces translation invariance \u2014 small shifts in input don\u2019t drastically change outputs.</p>"},{"location":"deeplearning/2_convnets/#44-flatten-fully-connected-layers","title":"4.4 Flatten + Fully Connected Layers","text":"<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>"},{"location":"deeplearning/2_convnets/#5-cnn-architecture-as-a-computation-graph","title":"5. CNN Architecture as a Computation Graph","text":"<p>A typical CNN defines a differentiable map:</p> \\[ f_\\theta(x) = W_L (\\text{Flatten}(h_{L-1})) + b_L \\] <p>where each layer \\(h_l\\) is defined recursively as:</p> \\[ h_l = \\sigma(\\text{Conv}(h_{l-1}; W_l) + b_l), \\quad l = 1, \\dots, L-1 \\] <p>Here, <code>Conv</code> represents the convolution operation.</p> <p>Each layer is spatially local, translation-equivariant, and differentiable \u2014 meaning backpropagation works seamlessly, just as in MLPs.</p>"},{"location":"deeplearning/2_convnets/#6-backpropagation-through-convolutions","title":"6. Backpropagation Through Convolutions","text":"<p>The gradient computation is a direct extension of the chain rule.</p>"},{"location":"deeplearning/2_convnets/#61-forward-pass","title":"6.1 Forward Pass","text":"<p>Compute:  </p>"},{"location":"deeplearning/2_convnets/#62-backward-pass","title":"6.2 Backward Pass","text":"<p>We need: - Gradient w.r.t. weights: \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) - Gradient w.r.t. input: \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\)</p> <p>The flipping arises from the mathematical property of convolution. Modern frameworks handle this efficiently via convolution transpose operations.</p> <p>Optimization viewpoint: Convolution layers remain linear in their weights \u2014 the nonlinearity and local parameter sharing define their expressive power.</p>"},{"location":"deeplearning/2_convnets/#7-inductive-biases-in-cnns","title":"7. Inductive Biases in CNNs","text":"<p>Convolutional architectures embed strong inductive biases:</p> Property Mathematical Mechanism Effect Local connectivity Small kernels (3\u00d73, 5\u00d75) Exploits spatial locality Weight sharing Same filter across space Reduces parameters drastically Translation equivariance Convolution operation Same pattern detection anywhere Pooling invariance Spatial downsampling Robust to small shifts/noise <p>These biases make CNNs data-efficient and easy to train \u2014 especially compared to fully connected networks on images.</p>"},{"location":"deeplearning/2_convnets/#8-optimization-and-training-dynamics","title":"8. Optimization and Training Dynamics","text":"<p>Training CNNs is similar to MLPs \u2014 we use gradient-based optimizers (SGD, Adam, etc.) \u2014 but with different landscape geometry:</p> <ul> <li>Parameter sharing makes the loss smoother (less overfitting).</li> <li>Batch normalization stabilizes gradient flow:    </li> <li>Regularization via dropout or weight decay improves generalization.</li> <li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li> </ul> <p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>"},{"location":"deeplearning/2_convnets/#9-cnn-architectures-through-history","title":"9. CNN Architectures Through History","text":"Model Year Key Innovation Depth Inductive Bias LeNet-5 1998 First practical CNN for handwritten digits 7 layers Local receptive fields AlexNet 2012 GPU training, ReLU, dropout 8 layers Data augmentation VGG 2014 Deep stacks of small 3\u00d73 filters 19 layers Uniform architecture ResNet 2015 Skip connections for gradient flow 152 layers Identity mapping DenseNet 2016 Feature reuse via dense connectivity 201 layers Multi-scale learning EfficientNet 2019 Compound scaling variable Optimized parameter scaling"},{"location":"deeplearning/2_convnets/#10-cnns-and-the-optimization-landscape","title":"10. CNNs and the Optimization Landscape","text":"<p>CNNs reshape the optimization problem compared to MLPs:</p> <ul> <li>Reduced parameter redundancy \u2192 fewer degenerate directions in gradient space.</li> <li>Structured weight sharing \u2192 smoother loss surface, fewer sharp minima.</li> <li>Skip connections (ResNets) introduce identity mappings, improving conditioning of the Jacobian and preventing vanishing gradients.</li> </ul> <p>In optimization terms, CNNs are better-conditioned models of the input\u2013output mapping.</p>"},{"location":"deeplearning/2_convnets/#11-beyond-classical-cnns","title":"11. Beyond Classical CNNs","text":"<p>Modern vision architectures have evolved: - Residual Networks (ResNets): skip connections allow training very deep models. - Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count. - Dilated Convolutions: expand receptive field without extra parameters. - Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>"},{"location":"deeplearning/2_convnets/#12-mathematical-summary","title":"12. Mathematical Summary","text":"Concept Formula Description Convolution \\((I * K)(i,j) = \\sum_m \\sum_n I(i+m,j+n) K(m,n)\\) Weighted local sum CNN Layer \\(h = \\sigma(W * x + b)\\) Convolution + nonlinearity Pooling \\(y_{i,j} = \\max_{(m,n)\\in \\Omega(i,j)} h_{m,n}\\) Downsampling Gradient wrt weights \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) Backprop step Gradient wrt input \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\) Sensitivity propagation"},{"location":"deeplearning/2_convnets/#13-intuitive-summary","title":"13. Intuitive Summary","text":"<p>Convolutional networks are: - Local \u2192 they process neighborhoods of data. - Hierarchical \u2192 deeper layers build on lower-level features. - Translation-equivariant \u2192 same pattern anywhere is treated the same. - Efficient \u2192 far fewer parameters than MLPs.</p> <p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p>"},{"location":"deeplearning/3_sequence_data/","title":"3. Sequence Data and Recurrent Neural Networks (RNNs)","text":""},{"location":"deeplearning/3_sequence_data/#chapter-3-modeling-sequence-data-in-deep-learning","title":"Chapter 3: Modeling Sequence Data in Deep Learning","text":"<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length \\(T\\) can be written as \\((x_1,x_2,\\dots,x_T)\\), where each element \\(x_t\\) is indexed by its position in the sequence. Elements can repeat (e.g. the word \u201cthe\u201d may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p> <p>Sequences are collection of elements where:</p> <ul> <li>Elements can be repeated.</li> <li>Order matters.</li> <li>Of variable length.</li> </ul>"},{"location":"deeplearning/3_sequence_data/#limitations-of-traditional-supervised-models","title":"Limitations of Traditional Supervised Models:","text":"<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data \u2013 by, say, collapsing the sequence into a fixed-size feature vector \u2013 ignores the important temporal or sequential structure. As one summary notes, \u201cfeedforward neural networks are severely limited when it comes to sequential data\u201d. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p> <p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>"},{"location":"deeplearning/3_sequence_data/#the-simplest-assumption-independent-words-bag-of-words","title":"The Simplest Assumption: Independent Words (Bag-of-Words)","text":"<p>A na\u00efve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word\u2019s occurrence, treating all words as \u201cindependent features.\u201d This ignores sequence structure: \u201cthe order of words in the original documents is irrelevant\u201d. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: \u201cbag-of-words assumes words are independent of one another\u201d. In reality, words co-occur in context (\u201cpeanut butter\u201d versus \u201cpeanut giraffe\u201d) \u2013 bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>"},{"location":"deeplearning/3_sequence_data/#n-gram-models-and-fixed-context-assumptions","title":"N-gram Models and Fixed-Context Assumptions","text":"<p>To go beyond complete independence, one can incorporate local context by using \\(n\\)-gram models. An \\(n\\)-gram model makes the (Markov) assumption that the probability of each element depends only on the previous \\(n-1\\) elements. For language, a bigram model (2-gram) assumes \\(P(w_t\\mid w_{t-1})\\), a trigram (3-gram) uses \\(P(w_t\\mid w_{t-2},w_{t-1})\\), etc. In general, the chain rule with an \\(N\\)-gram approximation is</p> \\[ P(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{t-N+1}, \\ldots, x_{t-1}) \\, . \\] <p>This preserves some order information: the window of the last \\(N-1\\) items is used to predict the next. However, \\(n\\)-gram models have well-known downsides:</p> <ul> <li> <p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language \u201ccannot reason about context beyond the immediate \\(n\\)-gram window\u201d, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an \\(n\\)-gram.</p> </li> <li> <p>Data sparsity and scalability: The number of possible \\(n\\)-grams grows exponentially with vocabulary size \\(V\\). For a vocabulary of size \\(V\\), there are \\(V^N\\) possible \\(N\\)-grams. Jurafsky &amp; Martin observe that even for Shakespeare\u2019s corpus (\\(V\\approx 29{,}066\\)), there are \\(V^2\\approx8.4\\times 10^8\\) possible bigrams and \\(V^4\\approx 7\\times 10^{17}\\) possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough \\(n\\)-gram counts, and storing these tables is impractical for large \\(N\\) or \\(V\\). In practice, language models become \u201cridiculously sparse\u201d and unwieldy.</p> </li> <li> <p>No parametrization (non-differentiable): Traditional \\(n\\)-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p> </li> </ul> <p>In summary, while \\(n\\)-grams preserve local order up to length \\(N\\), they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>"},{"location":"deeplearning/3_sequence_data/#learnable-context-models-vectorization-and-neural-nets","title":"Learnable Context Models: Vectorization and Neural Nets","text":"<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p> <ul> <li> <p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p> </li> <li> <p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p> </li> <li> <p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can \u201coperate in variable length of sequences\u201d.</p> </li> <li> <p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p> </li> <li> <p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p> </li> </ul> <p>For example, one could take the last few words, map each to an embedding \\(\\mathbf{x}{t-N+1},\\dots,\\mathbf{x}{t-1}\\), concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word\u2019s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (\\(N-1\\)) and uses a separate weight for each position, so it\u2019s not efficient or variable-length. </p> <p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector \\(\\mathbf{h}_t\\) that evolves as the sequence is read. Such a context-vector \u201cacts as memory\u201d summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can \u201clearn\u201d how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>"},{"location":"deeplearning/3_sequence_data/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) \u2013 models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step \\(t\\), the RNN takes the current input \\(\\mathbf{x}t\\) and the previous hidden state \\(\\mathbf{h}{t-1}\\) and computes a new hidden state \\(\\mathbf{h}_t\\). The simplest RNN update is:</p> \\[ h_t = \\phi(W_h h_{t-1} + W_x x_t + b) \\, . \\] <p>where \\(\\phi\\) is a nonlinear activation (often \\(\\tanh\\)) and \\(W_h,W_x\\) are weight matrices. The same weight matrices \\(W_h,W_x\\) are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p> <p>Intuitively, the RNN\u2019s hidden state \\(\\mathbf{h}_t\\) \u201cremembers\u201d the information from all prior inputs up to time \\(t\\). The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p> \\[ y_t = \\mathrm{softmax}(W_y h_t + b_y) \\, . \\] <p>so that \\(P(x_{t+1}=w \\mid \\mathbf{h}_t)\\) is given by the corresponding component of \\(\\mathbf{y}_t\\). In language modeling, for instance, \\(y_t\\) gives a probability for each word in the vocabulary. As described in practice, \u201cRNNs predict the output from the last hidden state along with output parameter \\(W_y\\); a softmax function to ensure the probability over all possible words\u201d. </p> <p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an \u201cMLP with shared weights across time.\u201d At time \\(t\\), the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>"},{"location":"deeplearning/3_sequence_data/#unrolling-and-backpropagation-through-time-bptt","title":"Unrolling and Backpropagation Through Time (BPTT)","text":"<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across \\(T\\) time steps, creating a deep feedforward network of depth \\(T\\) (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, \u201cthe network needs to be expanded, or unfolded, so that the parameters could be differentiated ... \u2013 hence backpropagation through time (BPTT)\u201d. In practice, each weight matrix \\(W\\) receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if \\(L = -\\sum_t \\log P(x_t\\mid \\mathbf{h}_{t-1})\\) is the loss, then for each \\(W\\) we compute</p> \\[ \\frac{\\partial L}{\\partial W} = \\sum_{t} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} \\, . \\] <p>taking into account the influence of \\(W\\) at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>"},{"location":"deeplearning/3_sequence_data/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative \\(\\partial \\mathbf{h}t/\\partial \\mathbf{h}{t-1}\\) involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius \\(&lt;1\\) causes the gradients to shrink exponentially (vanishing), while if the spectral radius is \\(&gt;1\\) they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues \\(&gt;1\\)), whereas the vanishing gradient problem occurs when long-term components of the gradient go \u201cexponentially fast to norm 0\u201d. Formally, for a linearized RNN one can show that if the largest eigenvalue \\(\\lambda_{\\max}\\) of the recurrent weight matrix satisfies \\(|\\lambda_{\\max}|&lt;1\\), long-term gradients vanish as \\(t\\to\\infty\\), and if \\(|\\lambda_{\\max}|&gt;1\\) they explode. </p> <p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: \u201cwhen long term components go to zero, the model cannot learn correlation between distant events.\u201d In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>"},{"location":"deeplearning/3_sequence_data/#gated-architectures-lstm-and-gru","title":"Gated Architectures: LSTM and GRU","text":"<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable \u201cgates\u201d that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state \\(\\mathbf{C}_t\\) and three gates: input (\\(\\mathbf{i}_t\\)), forget (\\(\\mathbf{f}_t\\)), and output (\\(\\mathbf{o}t\\)) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time \\(t\\) with input \\(\\mathbf{x}t\\) and previous hidden \\(\\mathbf{h}{t-1}\\) and cell \\(\\mathbf{C}{t-1}\\), the gates and cell update are given by (all operations are elementwise):</p> <p>\u200b </p> <p>The new cell state \\(\\mathbf{C}_t\\) is then updated by combining the old state and the candidate:</p> \\[ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\, . \\] <p>where \\(\\odot\\) denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p> \\[ h_t = o_t \\odot \\tanh(C_t) \\, \\] <p>The intuition is that the forget gate \\(\\mathbf{f_t}\\) can reset or retain the old memory \\(\\mathbf{C}_{t-1}\\), the input gate \\(\\mathbf{i}_t\\) controls how much new information \\(\\tilde{\\mathbf{C}}_t\\) to write, and the output gate \\(\\mathbf{o}_t\\) controls how much of the cell state to expose as \\(\\mathbf{h}_t\\). By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs \u201calleviate the vanishing gradient problem,\u201d making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p> <p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>"},{"location":"deeplearning/3_sequence_data/#optimization-challenges-and-solutions","title":"Optimization Challenges and Solutions","text":"<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p> <ul> <li> <p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping \u201csolves the exploding gradients problem\u201d by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p> </li> <li> <p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing \\(W_h\\) as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, \u201cOrthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,\u201d ensuring stable gradient propagation. In practice, some implementations initialize \\(W_h\\) to random orthogonal (or unitary) matrices to encourage long memory.</p> </li> <li> <p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p> </li> <li> <p>Regularization: Some works add penalties to encourage \\(W_h\\) to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p> </li> </ul> <p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models \u2013 especially gated RNNs \u2013 provide a powerful framework: mathematically, they define hidden states \\(\\mathbf{h}_t\\) updated by \\(\\mathbf{h}t = f(\\mathbf{h}{t-1},\\mathbf{x}_t)\\) with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p>"},{"location":"deeplearning/4_nlp/","title":"4. Natural Language Processing (NLP) with Deep Learning","text":""},{"location":"deeplearning/4_nlp/#deep-learning-for-natural-language-processing","title":"Deep Learning for Natural Language Processing","text":"<ul> <li>Natural language is context-dependent, compositional, and ambiguous.</li> <li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation \u2014 ideal for modeling contextual relationships.</li> <li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li> </ul>"},{"location":"deeplearning/4_nlp/#key-challenges-of-language","title":"Key Challenges of Language","text":"<p>Human language presents a unique set of challenges for computational models. Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p> <ul> <li> <p>Words are not discrete symbols.   The same word can have several related senses depending on context \u2014 for example: <code>face\u2081</code> (human face), <code>face\u2082</code> (clock face), <code>face\u2083</code> (to confront), and <code>face\u2084</code> (a person or presence).   Treating these as independent dictionary entries loses the shared semantic structure between them.   A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p> </li> <li> <p>Need for distributed representations.   Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).   This allows similarity, analogy, and composition to emerge geometrically \u2014 for instance, <code>king - man + woman \u2248 queen</code>.</p> </li> <li> <p>Disambiguation depends on context.   The meaning of a word or phrase is determined by its linguistic surroundings.   For example, in \u201cThe man who ate the pepper sneezed,\u201d the subject of sneezed is determined by a non-adjacent clause (the man), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p> </li> <li> <p>Non-local dependencies.   Natural language contains relationships between words that may be far apart in sequence.   Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.   Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p> </li> <li> <p>Compositionality.   The meaning of larger expressions arises from the meanings of their parts and how they are combined.   However, this combination is not purely linear.   For example, <code>carnivorous plant</code> is not simply the sum of carnivore and plant \u2014 its interpretation depends on how the features interact (a plant that eats insects).   Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p> </li> </ul> <p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically. Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>"},{"location":"deeplearning/4_nlp/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Sequence models (RNNs, LSTMs) process tokens sequentially \u2014 limiting parallelism and long-range context.</li> <li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li> </ul>"},{"location":"deeplearning/4_nlp/#core-mechanism-self-attention","title":"Core Mechanism: Self-Attention","text":"<p>Given token embeddings :</p> \\[ q_i = e_i W^Q, \\quad k_i = e_i W^K, \\quad v_i = e_i W^V \\] <p>Attention weights:</p> \\[ \\alpha_{ij} = \\mathrm{softmax}_j \\left( \\frac{q_i k_j^\\top}{\\sqrt{d}} \\right) \\] <p>Output:</p> \\[ z_i = \\sum_j \\alpha_{ij} v_j \\] <p>Each token\u2019s new representation  is a contextual blend of all others. Captures semantic and syntactic relations without explicit recurrence.</p>"},{"location":"deeplearning/4_nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Use multiple projections \\((W^Q_h, W^K_h, W^V_h)\\) \u2192 multiple \u201cheads.\u201d Each head focuses on different relations (e.g. subject\u2013verb, modifier\u2013noun). Outputs are concatenated and projected back to dimension \\(d\\):</p> \\[ \\text{MHA}(E) = [Z_1; Z_2; \\dots; Z_H] W^O \\]"},{"location":"deeplearning/4_nlp/#position-encoding","title":"Position Encoding","text":"<p>Since attention is permutation-invariant, Transformers add position information:</p> \\[ \\text{PE}_{(pos,2i)} = \\sin(pos / 10000^{2i/d}), \\quad \\text{PE}_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d}) \\] <p>\u2192 These sinusoidal signals are added to embeddings to encode word order.</p>"},{"location":"deeplearning/4_nlp/#full-transformer-block","title":"Full Transformer Block","text":"<pre><code>Input\n  \u2193\nMulti-Head Self-Attention\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nFeedforward Network (ReLU)\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nOutput\n</code></pre> <p>Skip connections enable gradient flow and top-down influence. Stacking \\(N\\) blocks yields hierarchical contextualization of meaning.</p>"},{"location":"deeplearning/4_nlp/#intuition","title":"Intuition","text":"<ul> <li>Self-attention handles non-local relations.</li> <li>Multi-head captures multiple semantic dimensions simultaneously.</li> <li>Stacked layers build abstraction \u2014 from word-level to phrase- and discourse-level features.</li> </ul>"},{"location":"deeplearning/4_nlp/#unsupervised-learning-and-bert","title":"Unsupervised Learning and BERT","text":""},{"location":"deeplearning/4_nlp/#the-need-for-contextualized-representations","title":"The Need for Contextualized Representations","text":"<ul> <li>Word embeddings like Word2Vec are static: one vector per word.</li> <li>Language understanding requires contextual embeddings: \u201cbank\u201d (river vs. finance).</li> <li>Transformers enable bidirectional context \u2014 understanding a word from both sides.</li> </ul>"},{"location":"deeplearning/4_nlp/#bert-pretraining-objectives","title":"BERT Pretraining Objectives","text":"<ol> <li>Masked Language Modeling (MLM) Randomly mask 15% of tokens, predict them:</li> </ol> \\[ \\text{Loss}_{MLM} = - \\sum_{i \\in M} \\log P(w_i | \\text{context}) \\] <p>Encourages bidirectional encoding of meaning.</p> <ol> <li>Next Sentence Prediction (NSP) Model predicts if sentence B follows sentence A. Builds discourse-level coherence and world knowledge.</li> </ol>"},{"location":"deeplearning/4_nlp/#architecture","title":"Architecture","text":"<ul> <li>Deep bidirectional Transformer encoder.</li> <li>Uses special tokens:</li> <li><code>[CLS]</code> \u2013 sentence-level classification embedding</li> <li><code>[SEP]</code> \u2013 separates segments</li> <li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li> <li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li> </ul>"},{"location":"deeplearning/4_nlp/#significance","title":"Significance","text":"<p>BERT shows self-supervised pretraining \u2192 transfer learning pipeline:</p> <pre><code>Pretrain (unsupervised)\n   \u2193\nFine-tune (supervised)\n   \u2193\nTask-specific adaptation\n</code></pre> <p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data. Learns semantic similarity, coreference, and discourse relations implicitly.</p>"},{"location":"deeplearning/4_nlp/#grounded-and-embodied-language-learning","title":"Grounded and Embodied Language Learning","text":""},{"location":"deeplearning/4_nlp/#motivation","title":"Motivation","text":"<ul> <li>Language understanding ultimately involves relating words to the world.</li> <li>Humans learn language in context \u2014 perception, action, and social interaction.</li> <li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li> </ul>"},{"location":"deeplearning/4_nlp/#grounded-agents","title":"Grounded Agents","text":"<ul> <li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li> <li>Train via predictive modeling \u2014 anticipate sensory outcomes from language-conditioned actions.</li> <li>Enables semantic grounding: linking word \u201cred\u201d to visual color, \u201cpick up\u201d to motor command.</li> </ul>"},{"location":"deeplearning/4_nlp/#predictive-and-self-supervised-paradigms","title":"Predictive and Self-Supervised Paradigms","text":"<p>Agents learn representations by predicting future sensory or linguistic states:</p> \\[ \\min_\\theta \\mathbb{E} [ \\| f_\\theta(s_t, a_t) - s_{t+1} \\|^2 ] \\] <p>\u2192 Connects to world models and predictive coding principles in neuroscience. The agent\u2019s internal model encodes both linguistic meaning and causal structure of the environment.</p>"},{"location":"deeplearning/4_nlp/#insights-from-deepmind-work","title":"Insights from DeepMind Work","text":"<ul> <li>Embodied agents trained in simulated environments exhibit:</li> <li>Systematic generalization (e.g., learning \u201cpick up red object\u201d \u2192 generalize to unseen colors).</li> <li>Question answering and instruction following grounded in perception.</li> <li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li> </ul>"},{"location":"deeplearning/4_nlp/#conceptual-shift","title":"Conceptual Shift","text":"<p>From pipeline \u2192 integrated model:</p> Classic Pipeline Embodied / Interactive Model Letters \u2192 Words \u2192 Syntax \u2192 Meaning \u2192 Action Multimodal loops: Perception \u2194 Action \u2194 Language \u2194 Prediction"},{"location":"deeplearning/4_nlp/#conceptual-map-from-representation-to-understanding","title":"Conceptual Map: From Representation to Understanding","text":"<pre><code>Word Input\n   \u2193\nDistributed Representations (embedding)\n   \u2193\nSelf-Attention Mechanism\n   \u2193\nMulti-Head Parallel Processing\n   \u2193\nHierarchical Transformer Layers\n   \u2193\nContextualized Embeddings (BERT)\n   \u2193\nTransfer Learning to Tasks\n   \u2193\nEmbodied Agents (Grounded Semantics)\n   \u2193\nLanguage Understanding as Prediction + Interaction\n</code></pre>"},{"location":"deeplearning/4_nlp/#key-transitions","title":"Key Transitions","text":"<p>Symbol \u2192 Vector: Continuous representations enable learning of semantic gradients.</p> <p>Sequence \u2192 Attention: Parallel context integration replaces recurrence.</p> <p>Text \u2192 Context: Pretraining captures knowledge without explicit supervision.</p> <p>Language \u2192 World: Grounding links linguistic representations to sensory and causal models.</p>"},{"location":"deeplearning/4_nlp/#unifying-principle","title":"Unifying Principle","text":"<p>Deep language understanding = predictive modeling of structured context across both linguistic and environmental domains.</p> Concept Core Idea Model / Mechanism Distributed representations Meanings as patterns, not symbols Embeddings Context dependence Sense resolution via interaction Self-attention Parallelism All words attend to all others Transformer Bidirectionality Context from both sides BERT encoder Transfer learning Self-supervised \u2192 supervised Fine-tuning Grounding Language tied to perception/action Embodied agents Predictive learning Understanding as anticipation World models"},{"location":"deeplearning/5_attention/","title":"5. Transformers and Attention Mechanisms","text":""},{"location":"deeplearning/5_attention/#1-attention-memory-and-cognition","title":"1. Attention, Memory, and Cognition","text":"<ul> <li>Attention = ability to focus on relevant signals and ignore distractions.  </li> <li>Enables selective processing (e.g. cocktail party effect).  </li> <li> <p>Allows focusing on one thought or event at a time.</p> </li> <li> <p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p> </li> <li> <p>Together, they form the basis of cognition \u2014 controlling what to process, store, and recall.</p> </li> <li> <p>Neural networks can model aspects of this by learning what to attend to and what to remember.</p> </li> <li> <p>Goal of attention in DL:   Reduce complexity by focusing computation on the most informative parts of data or internal state.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#2-implicit-attention-in-neural-networks","title":"2. Implicit Attention in Neural Networks","text":"<ul> <li> <p>Neural networks are parametric nonlinear functions \\(y = f_\\theta(x)\\) mapping inputs to outputs.   They naturally exhibit implicit attention: certain input dimensions influence outputs more.</p> </li> <li> <p>The Jacobian \\(J = \\frac{\\partial y}{\\partial x}\\) quantifies this sensitivity \u2014 shows which input parts the model \u201cpays attention\u201d to.</p> </li> <li> <p>Example:   In deep RL, sensitivity maps reveal focus on state-value vs action-advantage components.</p> </li> <li> <p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p> </li> <li>Hidden state \\(h_t\\) stores past info.  </li> <li>The sequential Jacobian \\(\\frac{\\partial y_t}{\\partial x_{t-k}}\\) shows which past inputs are remembered.  </li> <li> <p>Implicitly attends to relevant time steps (memory through recurrence).</p> </li> <li> <p>In tasks like machine translation, implicit attention lets models reorder tokens:</p> <p>\u201cto reach\u201d \u2192 \u201czu erreichen\u201d</p> </li> </ul>"},{"location":"deeplearning/5_attention/#3-explicit-hard-attention","title":"3. Explicit (Hard) Attention","text":"<ul> <li>Explicit attention introduces a separate attention mechanism that decides where to look or what to read.   It restricts the data fed to the main network.</li> </ul>"},{"location":"deeplearning/5_attention/#why-explicit-attention","title":"Why explicit attention?","text":"<ul> <li>Efficiency: processes only selected parts of input.  </li> <li>Scalability: works on large or variable-size data.  </li> <li>Sequential processing: e.g. moving \u201cgaze\u201d across static images.  </li> <li>Interpretability: easier to visualize focus regions.</li> </ul>"},{"location":"deeplearning/5_attention/#model-structure","title":"Model structure","text":"<ul> <li>Network outputs attention parameters \\(a\\) that define a glimpse distribution \\(p(g|a)\\) over possible data regions.  </li> <li>A glimpse \\(g\\) (subset or window of data) is sampled and passed back as input.  </li> <li>System becomes recurrent, even if the base network is not.</li> </ul>"},{"location":"deeplearning/5_attention/#training-non-differentiable","title":"Training (non-differentiable)","text":"<ul> <li> <p>When glimpse selection is discrete or stochastic, use REINFORCE:      where \\(R\\) is the reward (e.g. task loss) and \\(b\\) a baseline for variance reduction.</p> </li> <li> <p>Thus, attention acts as a policy \\(\\pi_\\theta(g)\\) over glimpses.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#examples","title":"Examples","text":"<ul> <li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li> <li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li> </ul>"},{"location":"deeplearning/5_attention/#4-soft-attention","title":"4. Soft Attention","text":"<ul> <li>Hard attention samples discrete glimpses \u2192 non-differentiable \u2192 needs RL.  </li> <li>Soft attention computes a weighted average over all glimpses \u2192 differentiable \u2192 trainable by backprop.</li> </ul>"},{"location":"deeplearning/5_attention/#basic-idea","title":"Basic idea","text":"<ul> <li> <p>Attention parameters \\(a\\) define weights \\(w_i\\) over input features \\(v_i\\):      The readout \\(v\\) is a smooth combination of inputs.</p> </li> <li> <p>Replaces sampling by expectation \u2192 continuous, differentiable.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#benefits","title":"Benefits","text":"<ul> <li>Trained end-to-end with gradients.  </li> <li>Easier and more stable than hard attention.  </li> <li>Allows focus distribution rather than a single point.</li> </ul>"},{"location":"deeplearning/5_attention/#variants","title":"Variants","text":"<ul> <li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li> <li>Content-based attention: focuses by similarity of key \\(k\\) to data vectors \\(x_i\\) via score \\(S(k, x_i)\\), usually normalized by softmax:    </li> </ul>"},{"location":"deeplearning/5_attention/#applications","title":"Applications","text":"<ul> <li>Handwriting synthesis: RNN learns soft \u201cwindow\u201d over text sequence.  </li> <li>Neural Machine Translation: associative attention aligns words between languages.  </li> <li> <p>DRAW model: uses Gaussian filters to read/write parts of an image.</p> </li> <li> <p>Soft attention = data-dependent dynamic weighting (similar to convolution with adaptive filters).</p> </li> </ul>"},{"location":"deeplearning/5_attention/#5-introspective-attention-and-memory","title":"5. Introspective Attention and Memory","text":"<ul> <li>So far: attention over external data.  </li> <li>Now: attention over internal state or memory \u2192 \u201cintrospective attention.\u201d  </li> <li>Lets the network read or write selectively to memory locations.  </li> <li>Enables reasoning, recall, and algorithmic behavior.</li> </ul>"},{"location":"deeplearning/5_attention/#neural-turing-machine-ntm","title":"Neural Turing Machine (NTM)","text":"<ul> <li>Adds a differentiable memory matrix \\(M \\in \\mathbb{R}^{N \\times W}\\).  </li> <li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li> </ul> <p>Operations - Write: modify selected rows in \\(M\\) using attention weights \\(w_t\\). - Read: output weighted sum of memory slots:    - Addressing modes:   - Content-based: match key vector \\(k_t\\) to memory contents (via cosine similarity).   - Location-based: shift attention by relative position.</p> <p>Training: fully differentiable \u2014 end-to-end via backprop.</p> <p>Example task: copying sequences of variable length \u2014 learns algorithmic generalization.</p>"},{"location":"deeplearning/5_attention/#differentiable-neural-computer-dnc","title":"Differentiable Neural Computer (DNC)","text":"<ul> <li>Successor to NTM with richer memory access:</li> <li>Tracks temporal links between writes.  </li> <li>Supports dynamic memory allocation.  </li> <li>Improves stability and scalability.</li> </ul> <p>Application: synthetic QA tasks (bAbI dataset) \u2014 answers questions requiring multiple supporting facts and temporal reasoning.</p> <p>Key insight: Attention provides selective access to memory, acting like \u201caddressing\u201d in a differentiable data structure.</p>"},{"location":"deeplearning/5_attention/#6-transformers-and-self-attention","title":"6. Transformers and Self-Attention","text":"<ul> <li>Transformers: remove recurrence and convolution entirely \u2014 rely only on attention.</li> </ul>"},{"location":"deeplearning/5_attention/#self-attention","title":"Self-Attention","text":"<ul> <li>Each token attends to all others in the sequence:      where:</li> <li>\\(Q, K, V\\) are query, key, and value matrices (learned linear projections of input embeddings).</li> <li>Produces context-aware representations for all tokens in parallel.</li> </ul>"},{"location":"deeplearning/5_attention/#multi-head-attention","title":"Multi-Head Attention","text":"<ul> <li>Multiple attention \u201cheads\u201d (\\(H\\)) learn different relationships:      Each head captures a distinct pattern (syntax, semantics, position, etc.).</li> </ul>"},{"location":"deeplearning/5_attention/#transformer-block","title":"Transformer Block","text":"<ul> <li>Structure:</li> <li>Multi-head self-attention  </li> <li>Add &amp; LayerNorm  </li> <li>Feedforward (ReLU + linear)  </li> <li>Add &amp; LayerNorm  </li> <li>Skip connections improve gradient flow and allow top-down signal mixing.</li> </ul>"},{"location":"deeplearning/5_attention/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Since model is permutation-invariant, inject position information:       Added to input embeddings.</li> </ul>"},{"location":"deeplearning/5_attention/#intuition","title":"Intuition","text":"<ul> <li>Self-attention generalizes RNN memory:</li> <li>Recurrent \u2192 sequential access  </li> <li>Transformer \u2192 direct pairwise access between all tokens.</li> <li>Enables long-range dependencies and parallelization.</li> </ul>"},{"location":"deeplearning/5_attention/#key-result","title":"Key result","text":"<ul> <li>Attention-only models achieve SOTA in translation and NLP tasks.  </li> <li>Forms basis for BERT, GPT, and modern large language models.</li> </ul>"},{"location":"deeplearning/5_attention/#7-adaptive-computation-time-act-and-summary","title":"7. Adaptive Computation Time (ACT) and Summary","text":""},{"location":"deeplearning/5_attention/#adaptive-computation-time-act","title":"Adaptive Computation Time (ACT)","text":"<ul> <li>Proposed by Graves (2016): allows networks to \u201cponder\u201d variable amounts of time per input.  </li> <li>Each step computes a halting probability \\(p_t\\); total halt when \\(\\sum_t p_t = 1\\).</li> <li>Output is a weighted sum of intermediate states:    </li> <li>Encourages efficient use of computation \u2014 more steps for harder inputs, fewer for easy ones.</li> <li>Regularized by a time penalty to avoid overthinking.</li> </ul>"},{"location":"deeplearning/5_attention/#universal-transformers","title":"Universal Transformers","text":"<ul> <li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li> <li>Shares parameters across layers \u2014 like an RNN unrolled over depth.</li> <li>Combine parallel self-attention + iterative refinement + ACT.</li> <li>Achieves better generalization and adaptive reasoning on sequence tasks.</li> </ul>"},{"location":"deeplearning/5_attention/#summary","title":"Summary","text":"<ul> <li>Attention = selective processing of relevant information.  </li> <li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li> <li>Explicit attention can be hard (sampled) or soft (differentiable).  </li> <li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li> <li>Transformers unify attention as the core mechanism \u2014 fully parallel, context-rich.  </li> <li>Adaptive computation gives flexibility in processing time and complexity.</li> </ul> <p>Takeaway: Selective attention and memory \u2014 biological inspirations \u2014 are now core architectural principles driving modern deep learning.</p>"},{"location":"deeplearning/6_gans/","title":"6. Generative Models and GANs","text":""},{"location":"deeplearning/6_gans/#1-overview-generative-models","title":"1. Overview: Generative Models","text":"<ul> <li>Goal: learn a model of the true data distribution \\(p^*(x)\\) from samples.</li> </ul>"},{"location":"deeplearning/6_gans/#types-of-generative-models","title":"Types of Generative Models","text":"<ol> <li>Explicit likelihood models \u2013 define tractable \\(p_\\theta(x)\\)</li> <li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li> <li> <p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p> </li> <li> <p>Implicit models \u2013 define sampling procedure, not explicit \\(p_\\theta(x)\\) </p> </li> <li>Examples: GANs, Moment Matching Networks.</li> </ol>"},{"location":"deeplearning/6_gans/#11-the-gan-idea","title":"1.1 The GAN Idea","text":"<ul> <li>Two-player minimax game:</li> <li>Generator (G): maps noise \\(z \\sim p(z)\\) to data space \\(G(z)\\).</li> <li> <p>Discriminator (D): classifies samples as real (from \\(p^*(x)\\)) or fake (\\(G(z)\\)).</p> </li> <li> <p>Objectives:    </p> </li> <li> <p>Interpretation:</p> </li> <li>\\(D\\) learns to distinguish real from fake.</li> <li>\\(G\\) learns to fool \\(D\\).</li> <li>Training reaches equilibrium when \\(p_G(x) = p^*(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#12-alternative-view-teacherstudent-analogy","title":"1.2 Alternative View \u2014 Teacher\u2013Student Analogy","text":"<ul> <li>Teacher (D): distinguishes real vs fake, providing feedback.</li> <li>Student (G): improves by making fake data look real.</li> <li>Cooperative interpretation of the adversarial process.</li> </ul>"},{"location":"deeplearning/6_gans/#13-gans-as-a-game","title":"1.3 GANs as a Game","text":"<ul> <li>Zero-sum, bi-level optimization \u2192 strong connection to game theory.</li> <li>GAN equilibrium = Nash equilibrium between \\(G\\) and \\(D\\).</li> <li>Training alternates between optimizing \\(D\\) and \\(G\\).</li> </ul> <p>Key Intuition: GANs learn by competition between a generator and discriminator rather than direct likelihood maximization.</p>"},{"location":"deeplearning/6_gans/#2-gan-objective-as-divergence-minimization","title":"2. GAN Objective as Divergence Minimization","text":"<ul> <li>Generative modeling often aims to minimize a distance or divergence between   the true data distribution \\(p^*(x)\\) and model distribution \\(p_G(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#21-kl-and-related-divergences","title":"2.1 KL and Related Divergences","text":"<ul> <li> <p>Maximum Likelihood Estimation (MLE):      \u2192 drives \\(p_\\theta\\) to assign high probability to observed data.</p> </li> <li> <p>But: implicit models (like GANs) don\u2019t have explicit likelihoods, so MLE can\u2019t be used directly.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#22-gan-as-jensenshannon-js-divergence-minimization","title":"2.2 GAN as Jensen\u2013Shannon (JS) Divergence Minimization","text":"<ul> <li> <p>If discriminator \\(D\\) is optimal:      Plugging into the GAN loss shows that the generator minimizes:      \u2192 GAN \u2248 JS divergence minimization.</p> </li> <li> <p>However, this relies on an optimal discriminator \u2014 not true in practice.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#23-limitations-of-kl-js-divergences","title":"2.3 Limitations of KL / JS Divergences","text":"<ul> <li>If \\(p_G\\) and \\(p^*\\) have non-overlapping support,   \u2192 no useful gradient signal (zero gradient problem).</li> <li>The density ratio \\(\\frac{p^*(x)}{p_G(x)}\\) becomes infinite where \\(p_G=0\\).</li> <li>Thus, GANs can fail to learn when supports are disjoint.</li> </ul>"},{"location":"deeplearning/6_gans/#24-alternative-distances-divergences","title":"2.4 Alternative Distances &amp; Divergences","text":""},{"location":"deeplearning/6_gans/#a-wasserstein-distance-earth-movers","title":"(a) Wasserstein Distance (Earth Mover\u2019s)","text":"<ul> <li>Measures minimal \u201ccost\u201d of moving probability mass:    </li> <li>Provides smooth, non-vanishing gradients even when supports don\u2019t overlap.</li> <li>WGAN: enforce 1-Lipschitz \\(D\\) via:</li> <li>weight clipping,</li> <li>gradient penalty (WGAN-GP),</li> <li>spectral normalization.</li> </ul>"},{"location":"deeplearning/6_gans/#b-mmd-maximum-mean-discrepancy","title":"(b) MMD (Maximum Mean Discrepancy)","text":"<ul> <li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):    </li> <li>MMD-GAN: learns kernel features \\(\\phi\\) jointly with \\(D\\).</li> </ul>"},{"location":"deeplearning/6_gans/#c-f-divergences","title":"(c) f-divergences","text":"<ul> <li>General framework using convex functions \\(f\\):    </li> <li>GAN training derived via variational lower bound on \\(D_f\\).</li> </ul>"},{"location":"deeplearning/6_gans/#25-practical-view","title":"2.5 Practical View","text":"<ul> <li>GANs are not pure divergence minimizers in practice:</li> <li>\\(D\\) not optimal \u2192 approximate divergence.</li> <li>Neural discriminator learns a smooth approximation to density ratio.</li> <li>Provides useful gradients even when the true divergence would fail.</li> </ul>"},{"location":"deeplearning/6_gans/#26-summary-table","title":"2.6 Summary Table","text":"Perspective Example Key Idea KL Divergence MLE, VAEs Explicit likelihoods JS Divergence Original GAN Adversarial training Wasserstein WGAN Smooth gradients MMD MMD-GAN Kernel mean embedding f-divergence f-GAN Variational bound family <p>Insight: GANs can be viewed as learning a neural divergence measure that provides a stable, informative training signal.</p>"},{"location":"deeplearning/6_gans/#3-evaluating-gans","title":"3. Evaluating GANs","text":"<ul> <li>Evaluating generative models is difficult \u2014 no single metric captures all aspects.</li> <li>Must assess:</li> <li>Sample quality (fidelity, realism)</li> <li>Diversity / generalization</li> <li>Representation learning (usefulness of learned features)</li> </ul>"},{"location":"deeplearning/6_gans/#31-why-not-log-likelihood","title":"3.1 Why Not Log-Likelihood?","text":"<ul> <li>GANs are implicit models \u2014 no tractable \\(p(x)\\).</li> <li>Estimating log-likelihood is expensive and unreliable.</li> <li>Hence: use feature-based or classifier-based proxies.</li> </ul>"},{"location":"deeplearning/6_gans/#32-inception-score-is","title":"3.2 Inception Score (IS)","text":"<ul> <li>Uses a pretrained Inception v3 classifier.</li> <li>Compares predicted label distributions of generated samples.</li> </ul> <p>Formula:  </p> <p>Intuition: - High-quality images \u2192 confident predictions (\\(p(y|x)\\) low entropy). - Diverse images \u2192 marginal label distribution \\(p(y)\\) high entropy.</p> <p>Properties: - Measures sample quality and diversity. - Correlates with human judgment. - Fails to capture intra-class variation or features beyond ImageNet classes.</p> <p>Higher is better.</p>"},{"location":"deeplearning/6_gans/#33-frechet-inception-distance-fid","title":"3.3 Fr\u00e9chet Inception Distance (FID)","text":"<ul> <li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li> </ul> <p>Formula:  </p> <p>where \\((\\mu_r, \\Sigma_r)\\) and \\((\\mu_g, \\Sigma_g)\\) are mean and covariance of real and generated data features.</p> <p>Properties: - Sensitive to mode dropping and artifacts. - Correlates strongly with human evaluation. - Lower is better. - Biased for small sample sizes \u2192 use KID (Kernel Inception Distance) for correction.</p>"},{"location":"deeplearning/6_gans/#34-overfitting-check-nearest-neighbours","title":"3.4 Overfitting Check \u2014 Nearest Neighbours","text":"<ul> <li>Compute nearest real images to generated samples in pretrained feature space.</li> <li>Helps detect memorization (copying training images).</li> </ul>"},{"location":"deeplearning/6_gans/#35-evaluation-depends-on-goal","title":"3.5 Evaluation Depends on Goal","text":"Goal Metric Example Measures Image quality FID, IS Fidelity &amp; diversity Representation learning Linear probe accuracy Feature usefulness Data generation Human evaluation Perceptual quality RL / control Policy reward Functional realism <p>Key Takeaway: Use multiple complementary metrics \u2014 quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>"},{"location":"deeplearning/6_gans/#4-the-gan-zoo","title":"4. The GAN Zoo","text":"<p>GANs have evolved rapidly \u2014 from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>"},{"location":"deeplearning/6_gans/#41-the-original-gan","title":"4.1 The Original GAN","text":"<ul> <li>First formulation of adversarial training.</li> <li>Architecture: simple multilayer perceptrons (MLPs).</li> <li>Trained on small images (e.g. 32\u00d732).  </li> <li>Ignored spatial structure (flattened pixels).  </li> <li>Introduced the minimax objective still used today.</li> </ul>"},{"location":"deeplearning/6_gans/#42-conditional-gan","title":"4.2 Conditional GAN","text":"<ul> <li> <p>Adds conditioning information \\(y\\) (e.g. class label or input image). </p> </li> <li> <p>Enables controlled generation \u2014 specify category or domain.   Examples:</p> </li> <li>Class-conditional image synthesis (e.g., \"generate a dog\").  </li> <li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li> </ul>"},{"location":"deeplearning/6_gans/#43-laplacian-gan","title":"4.3 Laplacian GAN","text":"<ul> <li>Generates images progressively, starting from low resolution.  </li> <li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li> <li>Fully convolutional \u2014 can produce arbitrarily large outputs.</li> <li>Improves high-res synthesis through multi-scale structure.</li> </ul>"},{"location":"deeplearning/6_gans/#44-deep-convolutional-gan","title":"4.4 Deep Convolutional GAN","text":"<ul> <li>Replaces MLPs with deep convnets for both \\(G\\) and \\(D\\).</li> <li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li> <li>Enables smooth interpolation in latent space:</li> <li>\\(G(z_1)\\) \u2192 \\(G(\\frac{1}{2}(z_1 + z_2))\\) \u2192 \\(G(z_2)\\) produces semantically meaningful transitions.</li> <li>Latent space exhibits semantic arithmetic (e.g. \u201cman + glasses \u2013 woman\u201d).</li> </ul>"},{"location":"deeplearning/6_gans/#45-spectrally-normalized-gan","title":"4.5 Spectrally Normalized GAN","text":"<ul> <li> <p>Enforces 1-Lipschitz constraint on \\(D\\) via spectral normalization:      where \\(\\sigma_{\\max}(W)\\) is the largest singular value.</p> </li> <li> <p>Stabilizes training and improves generalization.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#46-projection-discriminator","title":"4.6 Projection Discriminator","text":"<ul> <li> <p>Adds class embedding projection inside \\(D\\):    where \\(v_y\\) is the embedding for class \\(y\\).</p> </li> <li> <p>Theoretically consistent probabilistic discriminator formulation.  </p> </li> <li>Strong empirical results on class-conditional image synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#47-self-attention-gan","title":"4.7 Self-Attention GAN","text":"<ul> <li>Introduces self-attention layers to capture long-range dependencies.  </li> <li>Improves global structure and coherence in generated images.</li> <li>Inspired by Transformer attention.</li> </ul>"},{"location":"deeplearning/6_gans/#48-biggan","title":"4.8 BigGAN","text":"<ul> <li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li> <li>Key ingredients:</li> <li>Hinge loss for \\(D\\) </li> <li>Spectral normalization  </li> <li>Self-attention  </li> <li>Projection discriminator  </li> <li>Orthogonal regularization  </li> <li>Skip connections from noise  </li> <li>Shared class embeddings  </li> <li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li> </ul>"},{"location":"deeplearning/6_gans/#49-logan","title":"4.9 LOGAN","text":"<ul> <li>Introduces latent optimization \u2014 optimize \\(z\\) via gradient updates to improve adversarial dynamics.  </li> <li>Uses natural gradient descent in latent space.  </li> <li>Yields higher FID/IS improvements over BigGAN.</li> </ul>"},{"location":"deeplearning/6_gans/#410-progressive-gan","title":"4.10 Progressive GAN","text":"<ul> <li>Trains from low to high resolution (4\u00d74 \u2192 8\u00d78 \u2192 16\u00d716 \u2026).  </li> <li>Each stage adds new layers to \\(G\\) and \\(D\\).  </li> <li>Dramatically improves stability and image quality (especially faces).</li> </ul>"},{"location":"deeplearning/6_gans/#411-stylegan","title":"4.11 StyleGAN","text":"<ul> <li>Adds style-based generator architecture:</li> <li>Latent vector \\(z\\) transformed by MLP to intermediate \\(w\\).</li> <li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li> <li> <p>Injects per-pixel noise for local details.</p> </li> <li> <p>Learns disentangled representations \u2014 global attributes (style) vs local (texture).</p> </li> </ul>"},{"location":"deeplearning/6_gans/#412-takeaways","title":"4.12 Takeaways","text":"<ul> <li>GAN progress driven by:</li> <li>Better architectures (Conv, Attention, Progressive, Style-based)</li> <li>Normalization &amp; regularization</li> <li>Stability techniques</li> <li>Large-scale training</li> </ul> <p>Trend: From small MLPs \u2192 Conv architectures \u2192 Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>"},{"location":"deeplearning/6_gans/#5-representation-learning-with-gans","title":"5. Representation Learning with GANs","text":"<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>"},{"location":"deeplearning/6_gans/#51-motivation","title":"5.1 Motivation","text":"<ul> <li>GANs implicitly learn latent spaces that capture high-level semantics.</li> <li>Exploring or constraining this latent space enables unsupervised representation learning.</li> </ul>"},{"location":"deeplearning/6_gans/#52-evidence-from-dcgan","title":"5.2 Evidence from DCGAN","text":"<ul> <li>DCGAN latent vectors encode meaningful directions:</li> <li>Smooth interpolation between points \u2192 semantic transformations.</li> <li>Linear arithmetic in latent space (e.g., smiling woman \u2013 woman + man \u2192 smiling man).</li> <li>Suggests disentangled feature representations emerge naturally.</li> </ul>"},{"location":"deeplearning/6_gans/#53-infogan","title":"5.3 InfoGAN","text":"<ul> <li>Extends GAN with information maximization objective:</li> <li>Encourages some latent codes \\(c\\) to be interpretable and disentangled.</li> </ul> <p>Objective:  where \\(I(c; G(z, c))\\) is mutual information between latent code and generated output.</p> <ul> <li>Adds an auxiliary network to infer \\(c\\) from \\(G(z, c)\\).</li> <li>Learns to associate:</li> <li>Discrete codes \u2192 categories (digits, shapes)</li> <li>Continuous codes \u2192 attributes (rotation, scale)</li> </ul>"},{"location":"deeplearning/6_gans/#54-ali-bigan","title":"5.4 ALI / BiGAN","text":"<ul> <li>Adds an encoder \\(E(x)\\) mapping real data to latent space.</li> <li> <p>Joint discriminator distinguishes pairs:    </p> </li> <li> <p>At equilibrium:</p> </li> <li> <p>\\(E\\) and \\(G\\) become approximate inverses:</p> <ul> <li>\\(x \\approx G(E(x))\\)</li> <li>\\(z \\approx E(G(z))\\)</li> </ul> </li> <li> <p>Enables inference and representation learning simultaneously.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#55-bigbigan","title":"5.5 BigBiGAN","text":"<ul> <li>Scales BiGAN to BigGAN architecture.</li> <li>Uses large-scale encoders (\\(E\\)) with ResNet blocks.</li> <li>Learns strong unsupervised representations competitive with self-supervised models.</li> </ul> <p>Observations: - Reconstructions \\(G(E(x))\\) preserve semantic content, not exact pixels. - Encoder features yield high ImageNet classification accuracy after linear probing.</p>"},{"location":"deeplearning/6_gans/#56-summary","title":"5.6 Summary","text":"Model Key Idea Outcome DCGAN Implicitly semantic latent space Interpolations meaningful InfoGAN Maximize info between codes and outputs Disentangled features BiGAN / ALI Add encoder, joint training Bidirectional mapping BigBiGAN Large-scale BiGAN Competitive unsupervised features <p>Key Insight: GANs not only generate, but also encode \u2014 their latent structure can act as a rich, learned representation space.</p>"},{"location":"deeplearning/6_gans/#6-gans-for-other-modalities-and-problems","title":"6. GANs for Other Modalities and Problems","text":"<p>GANs extend far beyond images \u2014 used for translation, audio, video, RL, and even art.</p>"},{"location":"deeplearning/6_gans/#61-image-to-image-translation","title":"6.1 Image-to-Image Translation","text":""},{"location":"deeplearning/6_gans/#a-pix2pix","title":"(a) Pix2Pix","text":"<ul> <li>Conditional GAN trained on paired datasets \\((x, y)\\).</li> <li>Learns deterministic mapping between domains (e.g., edges \u2192 photos).</li> <li>Loss combines adversarial term + L1 reconstruction:    </li> </ul>"},{"location":"deeplearning/6_gans/#b-cyclegan","title":"(b) CycleGAN","text":"<ul> <li>Unpaired domain translation \u2014 no 1:1 correspondence.</li> <li>Uses cycle consistency:</li> <li>\\(x \\in A \\to G_B(x) \\to F_A(G_B(x)) \\approx x\\)</li> <li>Enforces invertibility between domains.</li> <li>Enables tasks like horse \u2194 zebra, summer \u2194 winter.</li> </ul>"},{"location":"deeplearning/6_gans/#62-audio-synthesis","title":"6.2 Audio Synthesis","text":""},{"location":"deeplearning/6_gans/#a-wavegan","title":"(a) WaveGAN","text":"<ul> <li>Adapts convolutional GANs to 1D waveforms.</li> <li>Fully unsupervised raw-audio synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#b-melgan","title":"(b) MelGAN","text":"<ul> <li>Conditional GAN trained to generate mel-spectrogram waveforms.</li> <li>Used in text-to-speech (GAN-TTS).</li> </ul>"},{"location":"deeplearning/6_gans/#c-gan-tts","title":"(c) GAN-TTS","text":"<ul> <li>High-fidelity speech synthesis model.</li> <li>Achieves human-like audio quality via adversarial losses.</li> </ul>"},{"location":"deeplearning/6_gans/#63-video-synthesis-prediction","title":"6.3 Video Synthesis &amp; Prediction","text":"<ul> <li>GANs extended to spatiotemporal data:</li> <li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li> <li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li> <li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li> </ul>"},{"location":"deeplearning/6_gans/#64-gans-in-reinforcement-learning-imitation-control","title":"6.4 GANs in Reinforcement Learning (Imitation &amp; Control)","text":"<ul> <li>GAIL (Ho &amp; Ermon, 2016): Generative Adversarial Imitation Learning </li> <li>Discriminator distinguishes expert vs policy trajectories.</li> <li>Generator = policy network optimizing to mimic experts.</li> </ul>"},{"location":"deeplearning/6_gans/#65-creative-applied-uses","title":"6.5 Creative &amp; Applied Uses","text":"<ul> <li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li> <li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li> <li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li> <li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li> <li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li> </ul>"},{"location":"deeplearning/6_gans/#66-summary","title":"6.6 Summary","text":"Domain Example Key Idea Paired image translation Pix2Pix Conditional GAN + L1 loss Unpaired translation CycleGAN Cycle consistency Audio MelGAN, WaveGAN Conditional waveform generation Video DVD-GAN, TGAN-v2 Temporal adversarial modeling RL / Imitation GAIL Adversarial trajectory matching Art / Creativity GauGAN, SPIRAL Adversarial synthesis and style transfer <p>Insight: Adversarial learning generalizes across domains \u2014 GANs serve as a universal generator\u2013critic framework for structured data.</p>"},{"location":"deeplearning/7_unsuper/","title":"7. Unsupervised and Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#1-what-is-unsupervised-learning","title":"1. What is Unsupervised Learning?","text":""},{"location":"deeplearning/7_unsuper/#definition","title":"Definition","text":"<ul> <li>Goal: discover structure in data without explicit labels or rewards.  </li> <li>Learns a compact, informative representation of input data.</li> </ul> Learning Type Goal Supervision Supervised Map inputs \u2192 labels Requires labeled data Reinforcement Learn actions maximizing future reward Requires reward signal Unsupervised Find hidden structure No labels or rewards"},{"location":"deeplearning/7_unsuper/#core-ideas","title":"Core Ideas","text":"<ul> <li>Model latent structure or relationships between observations.  </li> <li>Examples:</li> <li>Clustering: group similar data points.  </li> <li>Dimensionality reduction: project data to low-dimensional latent space.  </li> <li>Manifold learning / disentangling: uncover independent factors of variation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#evaluation-challenges","title":"Evaluation Challenges","text":"<p>How do we know if unsupervised learning worked?</p> <ul> <li>Ambiguity of structure: multiple valid clusterings possible.   e.g., cluster by leg count, arm number, or height in robot dataset.</li> <li>Metrics depend on downstream use:   useful representations should improve data efficiency, generalization, or transfer.</li> </ul>"},{"location":"deeplearning/7_unsuper/#classic-methods","title":"Classic Methods","text":"<ul> <li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li> <li>ICA (Independent Component Analysis): separates statistically independent components.  </li> <li>Modern goal: move beyond orthogonality \u2192 learn disentangled factors.</li> </ul>"},{"location":"deeplearning/7_unsuper/#summary","title":"Summary","text":"<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself \u2014 forming the foundation for representation learning.</p>"},{"location":"deeplearning/7_unsuper/#2-why-is-unsupervised-learning-important","title":"2. Why is Unsupervised Learning Important?","text":""},{"location":"deeplearning/7_unsuper/#21-historical-context-of-representation-learning","title":"2.1 Historical Context of Representation Learning","text":"Era Key Milestone Approach 1950s\u20132000s Arthur Samuel (1959): Machine Learning coined Feature engineering, clustering 2000s Kernel methods (Hofmann et al., 2008) Hand-crafted similarity functions 2006 Hinton &amp; Salakhutdinov: RBMs &amp; Autoencoders Layer-wise unsupervised pretraining 2012 Krizhevsky et al.: AlexNet End-to-end supervised learning dominates <ul> <li>Progress came from more data, deeper models, and better hardware \u2014 but not necessarily more efficient learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#22-limitations-of-purely-supervised-learning","title":"2.2 Limitations of Purely Supervised Learning","text":"<p>Supervised models are: - Data inefficient \u2014 need millions of labeled samples. - Brittle \u2014 vulnerable to adversarial perturbations. - Poor at transfer \u2014 struggle with new domains or tasks. - Lack common sense \u2014 limited abstraction and reasoning.</p>"},{"location":"deeplearning/7_unsuper/#23-evidence-of-current-gaps","title":"2.3 Evidence of Current Gaps","text":"Challenge Example Reference Data efficiency Learning from few examples Lake et al. (2017) Robustness Adversarial examples, brittle decisions Goodfellow et al. (2015) Generalization CoinRun, DMLab-30 Cobbe (2018), DeepMind Transfer Schema Networks Kansky et al. (2017) Common sense Conceptual reasoning Lake et al. (2015)"},{"location":"deeplearning/7_unsuper/#24-why-unsupervised-learning-matters","title":"2.4 Why Unsupervised Learning Matters","text":"<ul> <li>Enables data-efficient adaptation to new tasks.</li> <li>Provides robust, generalizable features.</li> <li>Promotes transfer learning by separating invariant factors.</li> <li>Encourages abstract reasoning and causal understanding.</li> </ul>"},{"location":"deeplearning/7_unsuper/#25-towards-general-ai","title":"2.5 Towards General AI","text":"<p>Unsupervised learning provides shared representations enabling: - Rapid multi-task adaptation. - Reuse across vision, language, and control. - Reduced supervision in real-world learning.</p> <p>Summary: Unsupervised representation learning addresses the core limits of current AI \u2014 aiming for data efficiency, robustness, generalization, transfer, and common sense.</p>"},{"location":"deeplearning/7_unsuper/#3-what-makes-a-good-representation","title":"3. What Makes a Good Representation?","text":"<p>A representation is an internal model of the world \u2014 an abstraction that makes reasoning and prediction efficient.</p>"},{"location":"deeplearning/7_unsuper/#31-what-is-a-representation","title":"3.1 What is a Representation?","text":"<p>\u201cA formal system for making explicit certain entities or types of information, together with a specification of how the system does this.\u201d</p> <ul> <li>Represents information about the world in a way useful for computation.  </li> <li>Not about a single feature, but the geometry or manifold shape in representational space.</li> </ul>"},{"location":"deeplearning/7_unsuper/#32-why-representation-form-matters","title":"3.2 Why Representation Form Matters","text":"<ul> <li>Determines which computations are easy.  </li> <li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li> </ul>"},{"location":"deeplearning/7_unsuper/#33-desirable-properties","title":"3.3 Desirable Properties","text":"Property Description Intuition Untangling Simplifies complex input manifolds Enables linear decoding Attention Allows selective focus on relevant factors Supports task-specific filtering Clustering Groups similar experiences together Facilitates generalization Latent Information Encodes hidden or inferred causes Predicts unobserved aspects Compositionality Builds complex concepts from simple parts Enables open-ended reasoning"},{"location":"deeplearning/7_unsuper/#34-information-bottleneck-principle","title":"3.4 Information Bottleneck Principle","text":"<ul> <li>Good representations compress inputs while preserving information about outputs.    </li> <li>Encourages minimal sufficient representations \u2014 compact yet predictive.</li> </ul>"},{"location":"deeplearning/7_unsuper/#4-evaluating-the-merit-of-a-representation","title":"4. Evaluating the Merit of a Representation","text":"<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>"},{"location":"deeplearning/7_unsuper/#41-the-evaluation-challenge","title":"4.1 The Evaluation Challenge","text":"<ul> <li>No single metric defines a \u201cgood\u201d representation.</li> <li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li> </ul> <p>Representations should enable: - Data efficiency \u2014 learn new tasks from few examples. - Robustness \u2014 resist noise or perturbations. - Generalization \u2014 perform well on new data. - Transfer \u2014 reuse knowledge in new settings. - Common sense \u2014 support reasoning and abstraction.</p>"},{"location":"deeplearning/7_unsuper/#42-example-evaluating-representations-via-symmetries","title":"4.2 Example: Evaluating Representations via Symmetries","text":"<p>Let: - \\(W\\) = world space - \\(Z\\) = representational space - \\(G = G_x \\times G_y \\times G_c\\) = group of transformations (e.g., position, color)</p> <p>A good representation \\(f: W \\rightarrow Z\\) should satisfy:  </p> <p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space \u2192 equivariance.</p>"},{"location":"deeplearning/7_unsuper/#43-desirable-evaluation-criteria","title":"4.3 Desirable Evaluation Criteria","text":"Criterion Desired Property Example / Metric Equivariance Transformations map consistently Translation \u2192 shift in latent Compositionality Combine factors to form new concepts Modular latent factors Metric structure Smooth distances reflect similarity \\(L_2\\), cosine Attention Selectively focus on task-relevant parts Masking or gating mechanisms Symmetries Invariance to irrelevant transformations Rotation, scale invariance"},{"location":"deeplearning/7_unsuper/#44-downstream-evaluation-tasks","title":"4.4 Downstream Evaluation Tasks","text":"Evaluation Setting Example Task Reference Perception / Control Predict object color or position Gens &amp; Domingos, Deep Symmetry Networks (2014) Robustness Classify images under adversarial noise Gowal et al., 2019 Sequential Attention Learn task-focused vision Zoran et al., 2020 Transfer / RL Zero-shot navigation (DARLA) Higgins et al., ICML 2017 Lifelong Learning Maintain latent structure over domains Achille et al., NeurIPS 2018 Reasoning / Imagination Compositional concept inference Lake et al., Science 2015; Higgins et al., ICLR 2018"},{"location":"deeplearning/7_unsuper/#45-why-evaluation-matters","title":"4.5 Why Evaluation Matters","text":"<p>A good representation supports simple mappings to downstream tasks: - Linear classifiers for vision tasks (e.g., color or position recognition). - Efficient policy learning in RL with fewer samples. - Abstract reasoning and imagination \u2014 \u201cIf rainbow elephants live in big cities, can we expect one in London?\u201d</p>"},{"location":"deeplearning/7_unsuper/#5-representation-learning-techniques","title":"5. Representation Learning Techniques","text":"<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches \u2014 all aiming to extract structure from data without labels.</p>"},{"location":"deeplearning/7_unsuper/#51-categories-of-methods","title":"5.1 Categories of Methods","text":"Category Core Idea Typical Example Generative Modeling Learn \\(p(x)\\) or a model that can reconstruct data VAE, \u03b2-VAE, MONet, GQN, GANs Contrastive Learning Learn by discriminating similar vs dissimilar samples CPC, SimCLR, word2vec Self-Supervised Learning Design pretext tasks that predict missing or reordered parts BERT, Colorization, Context Prediction"},{"location":"deeplearning/7_unsuper/#52-generative-modeling","title":"5.2 Generative Modeling","text":""},{"location":"deeplearning/7_unsuper/#521-motivation","title":"5.2.1 Motivation","text":"<ul> <li>Goal: learn the underlying data distribution \\(p(x)\\) to reveal hidden structure and causal factors.  </li> <li>Unsupervised generative modeling captures common regularities in data \u2014 enabling representation learning, synthesis, and reasoning.  </li> <li>Instead of directly memorizing examples, the model learns a probabilistic process that could have generated them.</li> </ul> <p>Generative models explain the data by learning how it might have arisen.</p>"},{"location":"deeplearning/7_unsuper/#522-from-maximum-likelihood-to-latent-variable-models","title":"5.2.2 From Maximum Likelihood to Latent Variable Models","text":""},{"location":"deeplearning/7_unsuper/#maximum-likelihood-principle","title":"Maximum Likelihood Principle","text":"<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:  where \\(p^*(x)\\) is the true data distribution and \\(p_\\theta(x)\\) is the model.</p>"},{"location":"deeplearning/7_unsuper/#latent-variable-formulation","title":"Latent Variable Formulation","text":"<ul> <li>Assume data arises from hidden (latent) variables \\(z\\):    </li> <li>Here:</li> <li>\\(p(z)\\) \u2014 prior over latent variables (e.g., \\(\\mathcal{N}(0, I)\\))  </li> <li>\\(p_\\theta(x|z)\\) \u2014 likelihood or decoder mapping latent codes to data</li> </ul> <p>This defines a latent variable model: the data-generating process maps from a low-dimensional latent space to the observed space.</p>"},{"location":"deeplearning/7_unsuper/#523-inference-in-latent-variable-models","title":"5.2.3 Inference in Latent Variable Models","text":"<p>Goal: infer the posterior  to identify which latent factors \\(z\\) most likely generated observation \\(x\\).</p> <ul> <li>Intuition:   Recover the underlying causes that explain the data \u2014 along with uncertainty estimates.</li> <li>Problem:   Computing \\(p(z|x)\\) is often intractable, since \\(p_\\theta(x)\\) involves integrating over all \\(z\\).   \u2192 We must approximate inference using neural networks.</li> </ul> <p>Thus, generative models combine:</p> <ul> <li>Generation: \\(z \\rightarrow x\\) (decode latent causes into data)</li> <li>Inference: \\(x \\rightarrow z\\) (encode data into latent causes)</li> </ul>"},{"location":"deeplearning/7_unsuper/#524-variational-autoencoders-vaes","title":"5.2.4 Variational Autoencoders (VAEs)","text":"<p>To make inference tractable, VAEs introduce an approximate posterior \\(q_\\phi(z|x)\\) and optimize a variational bound on the likelihood:</p>"},{"location":"deeplearning/7_unsuper/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"\\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}[q_\\phi(z|x)\\,||\\,p(z)] \\]"},{"location":"deeplearning/7_unsuper/#terms","title":"Terms","text":"<ol> <li> <p>Reconstruction term    Encourages the model to faithfully reproduce the input from its latent code.</p> </li> <li> <p>KL divergence term    Regularizes the latent posterior to match the prior \u2014 ensuring smoothness and preventing overfitting.</p> </li> </ol>"},{"location":"deeplearning/7_unsuper/#neural-implementation","title":"Neural Implementation","text":"<ul> <li>Encoder \\(q_\\phi(z|x)\\): approximates inference (maps data \u2192 latent code).  </li> <li>Decoder \\(p_\\theta(x|z)\\): generates data from the latent space (latent \u2192 data).  </li> <li>Both are parameterized by deep neural networks.</li> </ul> <p>Reparameterization trick (Kingma &amp; Welling, 2014):  enables backpropagation through stochastic latent sampling.</p>"},{"location":"deeplearning/7_unsuper/#why-vaes-matter","title":"Why VAEs Matter","text":"<ul> <li>Provide continuous, structured latent spaces capturing generative factors.  </li> <li>Support smooth interpolation and semantic manipulation.  </li> <li>Foundation for disentangled and interpretable representation learning (e.g., \u03b2-VAE).  </li> <li>Bridge probabilistic modeling with deep learning.</li> </ul> <p>VAEs turn probabilistic inference into a scalable neural optimization problem \u2014 the cornerstone of modern generative representation learning.</p>"},{"location":"deeplearning/7_unsuper/#524-vae","title":"5.2.4 \u03b2-VAE","text":"<ul> <li>Adds weight \u03b2 to KL term:    </li> <li>Encourages disentangled latent factors (position, shape, rotation, color).</li> <li> <p>Provides interpretable, semantically meaningful representations.</p> </li> <li> <p>DARLA (Higgins et al., 2017): \u03b2-VAE for reinforcement learning \u2192 improved transfer and sim2real generalization.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#525-sequential-and-layered-models","title":"5.2.5 Sequential and Layered Models","text":"<p>ConvDRAW (Gregor et al., 2016) - Sequential VAE with recurrent refinement. - Models temporal and spatial dependencies.</p> <p>MONet (Burgess et al., 2019) - Attention-based scene decomposition. - Each latent corresponds to one object \u2192 compositional representations. - Enables object-centric reasoning and RL transfer.</p> <p>GQN (Eslami et al., 2018) - Generative Query Networks: learn neural scene representations. - Given partial observations, predict unseen viewpoints (3D reasoning).</p> <p>VQ-VAE (van den Oord et al., 2017) - Learns discrete latent variables via vector quantization. - Enables hierarchical or symbolic structure. - Useful for speech, images, and video. -</p>"},{"location":"deeplearning/7_unsuper/#526-gans-goodfellow-et-al-2014","title":"5.2.6 GANs (Goodfellow et al., 2014)","text":"<ul> <li>Implicit generative models \u2014 learn by adversarial game:</li> <li>Generator creates samples.</li> <li>Discriminator provides learning signal (no reconstruction loss).</li> <li>BigBiGAN (Donahue et al., 2019):</li> <li>Adds encoder for inference.</li> <li>Learns rich, high-level representations \u2192 SOTA semi-supervised performance on ImageNet.</li> </ul>"},{"location":"deeplearning/7_unsuper/#527-large-scale-generative-models","title":"5.2.7 Large-Scale Generative Models","text":"<ul> <li>GPT (Radford et al., 2019):  </li> <li>Large transformer trained via language modeling.</li> <li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li> </ul>"},{"location":"deeplearning/7_unsuper/#53-contrastive-learning","title":"5.3 Contrastive Learning","text":""},{"location":"deeplearning/7_unsuper/#core-idea","title":"Core Idea","text":"<ul> <li>No need to model \\(p(x)\\) explicitly.</li> <li>Learn representations that maximize mutual information between related samples.</li> </ul>"},{"location":"deeplearning/7_unsuper/#531-word2vec-mikolov-et-al-2013","title":"5.3.1 word2vec (Mikolov et al., 2013)","text":"<ul> <li>Predict context words given a target word.  </li> <li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li> <li>Learns semantic embeddings; supports few-shot translation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018","title":"5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)","text":"<ul> <li>Maximize mutual information between current representation and future observations.  </li> <li>Trains a classifier to distinguish real future samples from negatives.  </li> <li> <p>Learns features useful across modalities (vision, speech).</p> </li> <li> <p>Data-efficient Image Recognition (H\u00e9naff et al., 2019):   contrastive features outperform pixel-level training in low-data regimes.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#533-simclr-chen-et-al-2020","title":"5.3.3 SimCLR (Chen et al., 2020)","text":"<ul> <li>Simple, scalable contrastive framework:</li> <li>Generate two augmented views of the same image.</li> <li>Maximize agreement via contrastive loss (NT-Xent).</li> <li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li> <li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#54-self-supervised-learning","title":"5.4 Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#idea","title":"Idea","text":"<ul> <li>Design pretext tasks that use natural structure in data as supervision.  </li> <li>Representations are deterministic and transferable to new tasks.</li> </ul>"},{"location":"deeplearning/7_unsuper/#541-examples","title":"5.4.1 Examples","text":"Task Description Reference Colorization Predict color from grayscale image Zhang et al., 2016 Context Prediction Predict position of image patches Doersch et al., 2015 Sequence Sorting Predict correct frame order in videos Lee et al., 2017 BERT (Devlin et al., 2019) Masked language modeling + next sentence prediction Revolutionized NLP"},{"location":"deeplearning/7_unsuper/#542-key-benefits","title":"5.4.2 Key Benefits","text":"<ul> <li>Requires no labels \u2014 just structure in data.  </li> <li>Produces general features useful for:</li> <li>Semi-supervised classification  </li> <li>Transfer learning  </li> <li>Downstream reasoning tasks</li> </ul>"},{"location":"deeplearning/7_unsuper/#55-design-principles","title":"5.5 Design Principles","text":"Consideration Desired Property Modality Align architecture with data type (image, text, audio) Task Design Choose pretext that aligns with useful features Consistency Maintain temporal/spatial coherence Discrete + Continuous Latents Enable symbolic and continuous reasoning Adaptivity Representations should evolve with experience <p>Summary: Unsupervised representation learning uses three complementary lenses: - Generative \u2192 model what the world looks like. - Contrastive \u2192 learn what is similar or different. - Self-supervised \u2192 create pseudo-tasks that reveal structure. Together, they aim for data-efficient, transferable, and interpretable representations.</p>"},{"location":"deeplearning/8_latentvariables/","title":"8. Latent Variable Models","text":""},{"location":"deeplearning/8_latentvariables/#1-generative-modelling","title":"1. Generative Modelling","text":""},{"location":"deeplearning/8_latentvariables/#11-what-are-generative-models","title":"1.1 What Are Generative Models?","text":"<ul> <li>Probabilistic models of high-dimensional data.</li> <li>Describe how observations are generated from underlying processes.</li> <li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#12-why-they-matter","title":"1.2 Why They Matter","text":"<p>Generative models can: - Estimate data density (detect outliers, anomalies). - Enable compression (encode \u2192 decode). - Map between domains (e.g., translation, text-to-speech). - Support model-based RL (predict future states). - Learn representations from raw data. - Improve understanding of data structure.</p>"},{"location":"deeplearning/8_latentvariables/#13-types-of-generative-models-in-deep-learning","title":"1.3 Types of Generative Models in Deep Learning","text":""},{"location":"deeplearning/8_latentvariables/#a-autoregressive-models","title":"(a) Autoregressive Models","text":"<p>Model joint distribution via chain rule:  </p> <p>Trained with maximum likelihood</p> <p>Examples:</p> <ul> <li>RNN/Transformer LMs  </li> <li>NADE  </li> <li>PixelCNN / WaveNet</li> </ul> <p>Pros:</p> <ul> <li>Easy training (max. likelihood).</li> <li>No sampling during training.</li> </ul> <p>Cons:</p> <ul> <li>Slow generation (sequential).</li> <li>Often capture local structure better than global structure.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#b-latent-variable-models","title":"(b) Latent Variable Models","text":"<p>Introduce an unobserved latent variable \\(z\\):</p> <ul> <li>Prior: \\(p(z)\\) </li> <li>Likelihood: \\(p_\\theta(x\\mid z)\\) </li> </ul> <p>Joint:  </p> <p>Pros</p> <ul> <li>Flexible &amp; interpretable  </li> <li>Natural for representation learning  </li> <li>Fast generation  </li> </ul> <p>Cons - Require approximate inference unless specially designed (e.g., invertible models).</p>"},{"location":"deeplearning/8_latentvariables/#c-implicit-models-gans","title":"(c) Implicit Models (GANs)","text":"<ul> <li>Define a generator \\(G(z)\\) with no explicit likelihood.</li> <li>Trained adversarially using a discriminator.</li> </ul> <p>Pros</p> <ul> <li>Extremely realistic samples  </li> <li>Fast sampling  </li> </ul> <p>Cons</p> <ul> <li>Cannot evaluate \\(p(x)\\) </li> <li>Mode collapse  </li> <li>Training instability  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-latent-variable-models-inference","title":"2. Latent Variable Models &amp; Inference","text":""},{"location":"deeplearning/8_latentvariables/#21-what-is-a-latent-variable-model-lvm","title":"2.1 What is a Latent Variable Model (LVM)?","text":"<p>A latent variable model introduces an unobserved variable \\(z\\) that explains the observed data \\(x\\).</p> <p>Model components:</p> <ul> <li>Prior over latent variables:  </li> </ul> <p> </p> <ul> <li>Likelihood / decoder mapping latent \u2192 observation:  </li> </ul> <p> </p> <p>Joint distribution:</p> \\[ p_\\theta(x, z) = p_\\theta(x \\mid z)\\,p(z) \\] <p>Marginal likelihood (what we want to maximize when training):</p> \\[ p_\\theta(x) = \\int p_\\theta(x \\mid z)\\,p(z)\\,dz \\]"},{"location":"deeplearning/8_latentvariables/#22-intuition-latents-as-explanations","title":"2.2 Intuition: Latents as \u201cExplanations\u201d","text":"<ul> <li> <p>A particular value of \\(z\\) is a hypothesis about hidden causes that produced \\(x\\).</p> </li> <li> <p>Generation = sample latent \u2192 map it to data:    </p> </li> </ul> <p>Most of the article focuses on the inverse of this: recovering \\(z\\) from \\(x\\).</p>"},{"location":"deeplearning/8_latentvariables/#23-what-is-inference","title":"2.3 What Is Inference?","text":"<p>Inference means computing the posterior:  </p> <p>Why it matters:</p> <ul> <li>Explains the observation (which latents likely produced it?)</li> <li>Needed inside maximum-likelihood training   (the gradient depends on the posterior!)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#24-inference-requires-the-marginal-likelihood","title":"2.4 Inference Requires the Marginal Likelihood","text":"<p>To compute the posterior, we need:  This integral is often intractable.</p> <p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>"},{"location":"deeplearning/8_latentvariables/#25-example-mixture-of-gaussians","title":"2.5 Example: Mixture of Gaussians","text":"<p>Model:</p> <ul> <li>Choose cluster \\(k\\) </li> <li>Sample \\(x\\) from Gaussian for that cluster</li> </ul> <p>Posterior:  </p> <p>This model is tractable because:</p> <ul> <li>Finite number of discrete states  </li> <li>Closed-form posterior</li> </ul>"},{"location":"deeplearning/8_latentvariables/#26-the-need-for-inference-in-learning","title":"2.6 The Need for Inference in Learning","text":""},{"location":"deeplearning/8_latentvariables/#maximum-likelihood-as-the-core-training-principle","title":"Maximum Likelihood as the Core Training Principle","text":"<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters \\(\\theta\\) that make the observed training data as probable as possible:</p> \\[ \\theta^* = \\arg\\max_\\theta \\sum_{i} \\log p_\\theta(x^{(i)}) \\] <p>For latent variable models, the marginal likelihood is:  This integral is rarely tractable, which makes direct maximization difficult.</p>"},{"location":"deeplearning/8_latentvariables/#why-optimization-is-hard-in-latent-variable-models","title":"Why Optimization Is Hard in Latent Variable Models","text":"<ul> <li>The log-likelihood involves an integral (or sum) over the latent variables \\(z\\).  </li> <li>Because this integral usually has no closed form, we must use iterative optimization methods.</li> </ul> <p>Common approaches:</p> <ol> <li>Gradient-based optimization (e.g., gradient descent)</li> <li>Expectation-Maximization (EM)</li> </ol> <p>Below we explain why inference (computing the posterior \\(p_\\theta(z \\mid x)\\)) is essential for both.</p>"},{"location":"deeplearning/8_latentvariables/#261-gradient-based-learning-requires-the-posterior","title":"2.6.1 Gradient-Based Learning Requires the Posterior","text":"<p>Using the identity:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)}[\\nabla_\\theta \\log p_\\theta(x, z)] \\] <p>Differentiate the log-marginal</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{\\nabla_\\theta p_\\theta(x)}{p_\\theta(x)}\\] <p>Using: \\(p_\\theta(x)=\\int p_\\theta(x,z)\\,dz,\\)</p> <p>differentiate under the integral:</p> \\[\\nabla_\\theta p_\\theta(x) = \\nabla_\\theta \\int p_\\theta(x,z)\\,dz = \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Combine:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Apply the log-derivative identity</p> <p>The identity:</p> \\[\\nabla_\\theta p_\\theta(x,z) = p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\] <p>Substitute:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Recognize the posterior</p> <p>Bayes\u2019 rule:</p> \\[p_\\theta(z\\mid x) = \\frac{p_\\theta(x,z)}{p_\\theta(x)}\\] <p>Substitute into the integral:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int \\frac{p_\\theta(x,z)}{p_\\theta(x)} \\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>This becomes:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int p_\\theta(z\\mid x)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Write as an expectation</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)} \\left[ \\nabla_\\theta \\log p_\\theta(x,z) \\right]\\] <p>To compute the gradient of the marginal likelihood, we must take an expectation under the posterior \\(p_\\theta(z\\mid x)\\).</p> <p>So:</p> <ul> <li>We cannot compute \\(\\nabla_\\theta \\log p_\\theta(x)\\) without knowing the posterior.</li> <li>Inference becomes part of every gradient step.</li> <li>If inference is intractable \u2192 gradient is intractable.</li> </ul> <p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>"},{"location":"deeplearning/8_latentvariables/#262-expectation-maximization-em-also-requires-inference","title":"2.6.2 Expectation-Maximization (EM) Also Requires Inference","text":"<p>EM is an alternative to gradient descent for maximizing likelihood.</p>"},{"location":"deeplearning/8_latentvariables/#e-step","title":"E-step:","text":"<p>Compute (or approximate) the posterior:  This assigns responsibilities to each latent configuration.</p>"},{"location":"deeplearning/8_latentvariables/#m-step","title":"M-step:","text":"<p>Update parameters by maximizing the expected complete-data log-likelihood:  </p> <p>Thus, the E-step directly requires inference.</p>"},{"location":"deeplearning/8_latentvariables/#27-why-exact-inference-is-hard","title":"2.7 Why Exact Inference Is Hard","text":""},{"location":"deeplearning/8_latentvariables/#continuous-latents","title":"Continuous latents:","text":"<ul> <li>Require multidimensional integration over nonlinear likelihoods.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#discrete-latents","title":"Discrete latents:","text":"<ul> <li>Require summing over exponentially many configurations.</li> </ul> <p>Only a few cases allow closed-form inference:</p> <ul> <li>Mixture models  </li> <li>Linear Gaussian systems  </li> <li>Invertible / flow-based models (covered next)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#28-two-strategies-to-handle-intractability","title":"2.8 Two Strategies to Handle Intractability","text":""},{"location":"deeplearning/8_latentvariables/#1-design-tractable-models","title":"1. Design tractable models","text":"<ul> <li>Invertible models (normalizing flows)</li> <li>Autoregressive latent structures Pros: exact inference Cons: restricted model class</li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-approximate-inference","title":"2. Approximate inference","text":"<ul> <li>Use approximations to posterior \\(p(z \\mid x)\\) </li> <li>Variational Inference or MCMC Pros: flexible, expressive models Cons: introduces approximation error</li> </ul>"},{"location":"deeplearning/8_latentvariables/#3-invertible-models-exact-inference","title":"3. Invertible Models &amp; Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#31-what-are-invertible-models","title":"3.1 What Are Invertible Models?","text":"<p>Invertible models (also called normalizing flows) are latent variable models where:</p> <ul> <li>The latent variable \\(z\\) and data \\(x\\) have the same dimensionality</li> <li>There exists an invertible, differentiable mapping </li> <li>Because \\(f_\\theta\\) is invertible:    </li> </ul> <p>Key property: Inference is exact and trivial \u2014 simply apply the inverse function.</p>"},{"location":"deeplearning/8_latentvariables/#32-generative-process","title":"3.2 Generative Process","text":"<p>To generate a sample:</p> <ol> <li>Sample \\(z \\sim p(z)\\) (usually a simple prior like \\(\\mathcal{N}(0, I)\\))</li> <li>Transform via </li> </ol> <p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>"},{"location":"deeplearning/8_latentvariables/#33-why-are-invertible-models-attractive","title":"3.3 Why Are Invertible Models Attractive?","text":"<ul> <li> <p>Exact inference:    is computed by a single function evaluation (no approximation needed).</p> </li> <li> <p>Exact likelihood:   Can compute \\(\\log p_\\theta(x)\\) exactly using the change-of-variables formula.</p> </li> </ul>"},{"location":"deeplearning/8_latentvariables/#34-change-of-variables-for-likelihood","title":"3.4 Change of Variables for Likelihood","text":"<p>Given an invertible mapping \\(x = f_\\theta(z)\\):</p> \\[ p_\\theta(x) = p(z) \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(x)}{\\partial x} \\right) \\right| \\] <p>Equivalently, using \\(z = f_\\theta^{-1}(x)\\):</p> \\[ \\log p_\\theta(x) = \\log p(z) + \\log \\left| \\det J_{f_\\theta^{-1}}(x) \\right| \\] <p>Where:</p> <ul> <li>\\(J_{f_\\theta^{-1}}\\) is the Jacobian matrix of the inverse map  </li> <li>The determinant accounts for volume change introduced by transformation</li> </ul>"},{"location":"deeplearning/8_latentvariables/#35-example-independent-component-analysis-ica","title":"3.5 Example: Independent Component Analysis (ICA)","text":"<p>ICA is the simplest invertible model:</p> <ul> <li>Latent prior:  factorial prior      with non-Gaussian heavy-tailed components</li> <li>Linear invertible mixing: </li> </ul> <p>Inference:  </p> <p>ICA recovers independent sources that explain the observed signal.</p>"},{"location":"deeplearning/8_latentvariables/#36-building-complex-invertible-models","title":"3.6 Building Complex Invertible Models","text":"<p>Modern flows build \\(f_\\theta\\) by composing many simple invertible layers:</p> \\[ f_\\theta = f_K \\circ f_{K-1} \\circ \\dots \\circ f_1 \\] <p>Composition of invertible functions is invertible.</p> <p>Building blocks:</p> <ul> <li>Linear transforms</li> <li>Autoregressive flows (IAF, MAF)</li> <li>Coupling layers (RealNVP, Glow)</li> <li>Residual flows</li> <li>Sylvester flows</li> </ul> <p>Design goal:</p> <p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>"},{"location":"deeplearning/8_latentvariables/#37-advantages-limitations","title":"3.7 Advantages &amp; Limitations","text":""},{"location":"deeplearning/8_latentvariables/#advantages","title":"Advantages","text":"<ul> <li>Exact inference  </li> <li>Exact log-likelihood  </li> <li>Fast, parallel sampling  </li> <li>Useful as components in larger probabilistic models</li> </ul>"},{"location":"deeplearning/8_latentvariables/#limitations","title":"Limitations","text":"<ul> <li>Latent and data dimensions must match  </li> <li>Latents must be continuous  </li> <li>Observations must be continuous or quantized  </li> <li>Very deep flows require large memory  </li> <li>Hard to encode strong structure or sparsity  </li> </ul> <p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>"},{"location":"deeplearning/8_latentvariables/#mar","title":"Mar","text":""},{"location":"deeplearning/8_latentvariables/#4-variational-inference-vi","title":"4. Variational Inference (VI)","text":""},{"location":"deeplearning/8_latentvariables/#41-why-variational-inference","title":"4.1 Why Variational Inference?","text":"<p>In many latent variable models, the true posterior  is intractable because computing  is impossible in closed form.</p> <p>We still need the posterior for:</p> <ul> <li>Inference (explaining the observation)</li> <li>Learning (MLE gradient depends on it)</li> <li>EM algorithm E-step</li> </ul>"},{"location":"deeplearning/8_latentvariables/#approximate-inference","title":"Approximate Inference","text":"<p>There are two major classes of approaches to approximate inference:</p>"},{"location":"deeplearning/8_latentvariables/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>Generate samples from the exact posterior using a Markov chain.</p> <ul> <li>Very general; exact in the limit of infinite time / computation  </li> <li>Computationally expensive  </li> <li>Convergence is hard to diagnose  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-variational-inference-vi","title":"2. Variational Inference (VI)","text":"<p>Approximate the posterior with a tractable distribution (e.g., fully factorized, mixture, or autoregressive).</p> <ul> <li>Fairly efficient \u2014 inference reduces to optimization of distribution parameters  </li> <li>Fast at test time (single forward pass of the inference network)  </li> <li>Cannot easily trade computation for accuracy (unlike MCMC)  </li> </ul> <p>MCMC = flexible, asymptotically exact, but slow. VI = fast and scalable, but biased due to restricted approximating family.</p>"},{"location":"deeplearning/8_latentvariables/#42-core-idea-of-variational-inference","title":"4.2 Core Idea of Variational Inference","text":"<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing. Approximate the posterior with a simpler distribution:</p> \\[ q_\\phi(z \\mid x) \\approx p_\\theta(z \\mid x) \\] <p>Where:</p> <ul> <li>\\(q_\\phi\\) is the variational posterior</li> <li>\\(\\phi\\) are variational parameters (learned)</li> </ul> <p>Requirements:</p> <ol> <li>We can sample from \\(q_\\phi(z \\mid x)\\) </li> <li>We can compute \\(\\log q_\\phi(z \\mid x)\\) and its gradient wrt \\(\\phi\\) </li> </ol> <p>Common choice: mean-field approximation</p> \\[ q_\\phi(z \\mid x) = \\prod_i q_\\phi(z_i \\mid x) \\]"},{"location":"deeplearning/8_latentvariables/#43-training-with-variational-inference","title":"4.3 Training with Variational Inference","text":"<p>Goal: maximize the marginal likelihood</p> \\[ \\log p_\\theta(x) \\] <p>Since it's intractable, VI uses a lower bound on this quantity.</p>"},{"location":"deeplearning/8_latentvariables/#variational-lower-bound-elbo","title":"Variational Lower Bound (ELBO)","text":"<p>Using Jensen\u2019s inequality:</p> \\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x, z)] - \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log q_\\phi(z \\mid x)] \\] <p>This is the Evidence Lower Bound (ELBO):</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi}\\!\\left[\\log p_\\theta(x, z)\\right] - \\mathbb{E}_{q_\\phi}\\!\\left[\\log q_\\phi(z \\mid x)\\right] \\] <p>We maximize ELBO w.r.t both \\(\\theta\\) and \\(\\phi\\).</p>"},{"location":"deeplearning/8_latentvariables/#44-kl-interpretation-variational-gap","title":"4.4 KL Interpretation (Variational Gap)","text":"<p>Rewrite ELBO:</p> \\[ \\log p_\\theta(x) = \\text{ELBO}(\\theta, \\phi) + D_{\\text{KL}}(q_\\phi(z \\mid x) \\,\\|\\, p_\\theta(z \\mid x)) \\] <p>Thus:</p> <ul> <li> <p>Maximizing ELBO wrt \\(\\phi\\)   \u2192 minimizes the KL divergence between \\(q_\\phi\\) and the true posterior.</p> </li> <li> <p>The variational gap is </p> </li> </ul> <p>If \\(q_\\phi\\) is expressive enough:  </p>"},{"location":"deeplearning/8_latentvariables/#45-what-happens-when-updating-each-parameter-set","title":"4.5 What Happens When Updating Each Parameter Set?","text":""},{"location":"deeplearning/8_latentvariables/#updating-variational-parameters-phi","title":"Updating variational parameters \\(\\phi\\):","text":"<ul> <li>Minimizes the variational gap  </li> <li>Makes \\(q_\\phi(z \\mid x)\\) closer to the true posterior  </li> <li>Does not affect the model directly</li> </ul>"},{"location":"deeplearning/8_latentvariables/#updating-model-parameters-theta","title":"Updating model parameters \\(\\theta\\):","text":"<ul> <li>Increases \\(\\log p_\\theta(x)\\) (good)</li> <li>BUT often also reduces the gap by making the posterior simpler   \u2192 Risk: posterior collapse / variational pruning</li> </ul> <p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>"},{"location":"deeplearning/8_latentvariables/#46-variational-pruning-posterior-collapse","title":"4.6 Variational Pruning (Posterior Collapse)","text":"<p>Because VI pushes \\(p_\\theta(z \\mid x)\\) towards \\(q_\\phi(z\\mid x)\\), the model may choose to ignore some latent dimensions:</p> \\[ p_\\theta(z_i \\mid x) = p(z_i) \\] <p>Meaning the latent variable carries no information about \\(x\\).</p> <p>Pros:</p> <ul> <li>Automatically learns effective latent dimensionality</li> </ul> <p>Cons:</p> <ul> <li>Prevents fully utilizing the latent capacity  </li> <li>Common issue in VAEs (particularly with strong decoders)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#47-choosing-the-variational-posterior-family","title":"4.7 Choosing the Variational Posterior Family","text":""},{"location":"deeplearning/8_latentvariables/#simple-mean-field-gaussian","title":"Simple: Mean-field Gaussian","text":"<ul> <li>Fast</li> <li>Easy to optimize</li> <li>But limited expressivity</li> </ul>"},{"location":"deeplearning/8_latentvariables/#more-expressive-options","title":"More expressive options:","text":"<ul> <li>Mixture posteriors</li> <li>Gaussians with full covariance</li> <li>Autoregressive posteriors</li> <li>Normalizing-flow posteriors</li> </ul> <p>Trade-off: accuracy vs speed.</p>"},{"location":"deeplearning/8_latentvariables/#48-amortized-variational-inference","title":"4.8 Amortized Variational Inference","text":"<p>Classic VI:</p> <ul> <li>Each datapoint \\(x\\) has its own variational parameters  </li> <li>Requires iterative optimization per datapoint  </li> <li>Too slow for deep learning</li> </ul> <p>Amortized VI:</p> <ul> <li>Use an inference network (encoder)    </li> <li>Fast inference  </li> <li>Works with SGD  </li> <li>Introduced in Helmholtz Machines  </li> <li>Popularized by Variational Autoencoders</li> </ul>"},{"location":"deeplearning/8_latentvariables/#49-variational-vs-exact-inference","title":"4.9 Variational vs Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#advantages-of-vi","title":"Advantages of VI","text":"<ul> <li>Scalable to modern deep models  </li> <li>Fast inference  </li> <li>Enables flexible model design  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#disadvantages","title":"Disadvantages","text":"<ul> <li>Approximation bias  </li> <li>Posterior may be oversimplified  </li> <li>Can limit expressiveness of the full model  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#410-summary-of-section-4","title":"4.10 Summary of Section 4","text":"<ul> <li>Variational inference approximates the true posterior with a tractable distribution.  </li> <li>ELBO gives a trainable lower bound on the marginal likelihood.  </li> <li>VI converts inference into optimization.  </li> <li>Amortized VI enables neural inference (encoders).  </li> <li>Variational pruning can arise naturally and must be managed.  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#5-gradient-estimation-in-variational-inference","title":"5. Gradient Estimation in Variational Inference","text":""},{"location":"deeplearning/8_latentvariables/#51-why-do-we-need-gradient-estimators","title":"5.1 Why Do We Need Gradient Estimators?","text":"<p>To train a latent variable model with variational inference, we maximize the ELBO:</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z\\mid x)}\\Big[ \\log p_\\theta(x, z) - \\log q_\\phi(z\\mid x) \\Big] \\] <p>We need gradients with respect to:</p> <ol> <li>Model parameters \\(\\theta\\)</li> <li>Variational parameters \\(\\phi\\)</li> </ol> <p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>"},{"location":"deeplearning/8_latentvariables/#52-gradients-wrt-model-parameters-theta","title":"5.2 Gradients w.r.t. Model Parameters (\\(\\theta\\))","text":"<p>This part is easy.</p> <p>Because \\(q_\\phi(z\\mid x)\\) does not depend on \\(\\theta\\):</p> \\[ \\nabla_\\theta \\text{ELBO} = \\mathbb{E}_{q_\\phi(z\\mid x)} \\big[ \\nabla_\\theta \\log p_\\theta(x, z) \\big] \\] <p>We estimate this using samples:</p> <ol> <li>Draw \\(z \\sim q_\\phi(z\\mid x)\\) </li> <li>Compute \\(\\nabla_\\theta \\log p_\\theta(x,z)\\) </li> <li>Average across samples</li> </ol> <p>No special techniques required.</p>"},{"location":"deeplearning/8_latentvariables/#53-gradients-wrt-variational-parameters-phi","title":"5.3 Gradients w.r.t. Variational Parameters (\\(\\phi\\))","text":"<p>This is more difficult.</p> <p>We want:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] \\] <p>But \\(q_\\phi(z\\mid x)\\) depends on \\(\\phi\\). Two main strategies exist to handle this dependence:</p>"},{"location":"deeplearning/8_latentvariables/#54-two-families-of-gradient-estimators","title":"5.4 Two Families of Gradient Estimators","text":""},{"location":"deeplearning/8_latentvariables/#1-likelihood-ratio-reinforce-estimator","title":"\ud83d\udd37 1. Likelihood-Ratio / REINFORCE Estimator","text":"<p>Uses the identity:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}[f(z)] = \\mathbb{E}_{q_\\phi(z)}[f(z)\\,\\nabla_\\phi \\log q_\\phi(z)] \\] <p>This allows gradients for: - Discrete latent variables - Non-differentiable \\(f(z)\\) - Any distribution where we can compute \\(\\log q_\\phi(z)\\)</p> <p>Pros - Very general - Works for discrete and continuous latents  </p> <p>Cons - High variance - Requires variance reduction (baselines, control variates)</p> <p>This is the same gradient estimator used in policy gradients in RL.</p>"},{"location":"deeplearning/8_latentvariables/#2-reparameterization-pathwise-estimator","title":"\ud83d\udd37 2. Reparameterization / Pathwise Estimator","text":"<p>Instead of sampling \\(z \\sim q_\\phi(z\\mid x)\\) directly, write it as a differentiable transformation of noise:</p> \\[ z = g_\\phi(\\epsilon, x), \\quad \\epsilon \\sim p(\\epsilon) \\] <p>Then:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)} \\big[ \\nabla_\\phi f(g_\\phi(\\epsilon, x)) \\big] \\] <p>This pushes the dependence on \\(\\phi\\) inside a differentiable function.</p>"},{"location":"deeplearning/8_latentvariables/#example-gaussian-posterior","title":"Example: Gaussian posterior","text":"<p>If  then:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1) \\] <p>Pros - Low variance - Enables stable VAE training  </p> <p>Cons - Only works for continuous latent variables - Requires differentiable sampling procedure</p>"},{"location":"deeplearning/8_latentvariables/#55-comparison-table","title":"5.5 Comparison Table","text":"Property REINFORCE Reparameterization Works for discrete latent variables \u2705 \u274c Works for continuous latent variables \u2705 \u2705 Low-variance gradients \u274c \u2705 Requires differentiable sampling \u274c \u2705 Used in VAEs sometimes always"},{"location":"deeplearning/8_latentvariables/#56-practical-notes","title":"5.6 Practical Notes","text":"<ul> <li>Modern VAEs always use the reparameterization trick.  </li> <li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li> <li>Discrete VAEs use:</li> <li>Gumbel-Softmax  </li> <li>NVIL / REINFORCE with baselines  </li> <li>VIMCO  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#57-summary-of-section-5","title":"5.7 Summary of Section 5","text":"<ul> <li>Gradient estimation is essential for training VI models.  </li> <li>\\(\\nabla_\\theta\\) is easy: just sample from the variational posterior.  </li> <li>\\(\\nabla_\\phi\\) is hard because sampling depends on parameters.  </li> <li>Two estimators solve this:</li> <li>Likelihood-ratio (REINFORCE)  </li> <li>Reparameterization trick  </li> <li>Reparameterization yields low-variance gradients and powers modern VAEs.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":""},{"location":"deeplearning/8_latentvariables/#61-what-is-a-vae","title":"6.1 What Is a VAE?","text":"<p>A VAE is a latent variable generative model with:</p> <ul> <li>Continuous latent variables \\(z\\)</li> <li>Neural networks for both:</li> <li>Encoder (variational posterior) \\(q_\\phi(z \\mid x)\\) </li> <li>Decoder (likelihood) \\(p_\\theta(x \\mid z)\\)</li> <li>Training through amortized variational inference  </li> <li>Gradients computed using the reparameterization trick</li> </ul> <p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>"},{"location":"deeplearning/8_latentvariables/#62-vae-model-components","title":"6.2 VAE Model Components","text":""},{"location":"deeplearning/8_latentvariables/#prior","title":"Prior","text":"<p>Usually a factorized standard Gaussian:  </p>"},{"location":"deeplearning/8_latentvariables/#likelihood-decoder","title":"Likelihood / Decoder","text":"<p>Maps latents to a distribution over observations.</p> <p>For binary data:  </p> <p>For real-valued data:  </p>"},{"location":"deeplearning/8_latentvariables/#variational-posterior-encoder","title":"Variational Posterior / Encoder","text":"\\[ q_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu_\\phi(x), \\sigma_\\phi^2(x)) \\] <p>All of these functions (encoder &amp; decoder) can be implemented with: - MLPs - ConvNets - ResNets - Transformers depending on the domain.</p>"},{"location":"deeplearning/8_latentvariables/#63-training-objective-the-elbo","title":"6.3 Training Objective: The ELBO","text":"<p>VAEs maximize the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - D_{\\text{KL}}\\!\\Big(q_\\phi(z\\mid x)\\,\\|\\, p(z)\\Big) \\] <p>Interpretation:</p> <ol> <li> <p>Reconstruction Term    Measures how well the model predicts \\(x\\) from \\(z\\).    Encourages informative latents.</p> </li> <li> <p>KL Regularization Term    Encourages \\(q_\\phi(z\\mid x)\\) to stay close to the prior \\(p(z)\\).    Prevents overfitting and encourages smooth latent spaces.</p> </li> </ol> <p>The KL term often has closed-form for Gaussian distributions.</p>"},{"location":"deeplearning/8_latentvariables/#64-reparameterization-trick-key-to-vaes","title":"6.4 Reparameterization Trick (Key to VAEs)","text":"<p>Direct backprop through a sample \\(z \\sim q_\\phi(z\\mid x)\\) is impossible.</p> <p>Solution: rewrite sampling as a differentiable transformation of noise:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>This allows gradient flow through \\(z\\) and makes VAE training practical.</p>"},{"location":"deeplearning/8_latentvariables/#65-vae-as-a-framework","title":"6.5 VAE as a Framework","text":"<p>The term \u201cVAE\u201d now refers to a broad family of models: - Continuous latent variables - Amortized inference - Reparameterization-based gradients - Trained by maximizing ELBO (or its variants)</p> <p>Modern VAEs extend the basic version in many ways: - Multiple latent layers - More expressive posteriors (flows, mixtures) - More expressive priors (hierarchical, autoregressive) - More expressive decoders (ResNets, autoregressive PixelCNN decoders) - Iterative inference networks - Variance reduction techniques</p> <p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>"},{"location":"deeplearning/8_latentvariables/#66-summary-of-section-6","title":"6.6 Summary of Section 6","text":"<ul> <li>VAEs are tractable generative models with continuous latent variables.</li> <li>They pair:</li> <li>a decoder \\(p_\\theta(x\\mid z)\\) and  </li> <li>an encoder \\(q_\\phi(z\\mid x)\\)   using amortized VI.</li> <li>Training uses ELBO + reparameterization trick.</li> <li>VAEs balance reconstruction quality with regularized latent structure.</li> <li>The VAE framework is highly extensible and central to modern deep generative modeling.</li> </ul>"},{"location":"deeplearning/alogirthmic_detials/","title":"Alogirthmic detials","text":"<ol> <li>Batch normalization</li> </ol>"},{"location":"deeplearning/alogirthmic_detials/#211-batchnorm2d-layer","title":"2.1.1 - <code>BatchNorm2d Layer</code>","text":"<p>As part of this new, improved block, you will also introduce a powerful new layer: <code>BatchNorm2d</code>. This layer is a pivotal technique for building modern, high performing deep neural networks.</p> <p>Think of Batch Normalization as a traffic controller for the data flowing between your network's layers. After a convolutional layer processes a batch of images, the outputs (or activations) can have widely varying distributions from one batch to the next. <code>BatchNorm2d</code> steps in and normalizes these activations within each mini batch, adjusting them to have a consistent mean and standard deviation. It then uses two learnable parameters to scale and shift this normalized output, allowing the network itself to learn the optimal distribution for the data at that point.</p> <p>This seemingly simple step provides three profound benefits:</p> <ul> <li> <p>It Stabilizes and Accelerates Training: By keeping the distribution of data consistent between layers, it prevents later layers from having to constantly adapt to a shifting input from the layers before them. This stability allows you to use higher learning rates, which can dramatically speed up how quickly your model learns.</p> </li> <li> <p>It Acts as a Regularizer: Because the normalization statistics are calculated for each unique mini batch, it introduces a slight amount of noise into the training process. This noise makes it harder for the model to perfectly memorize the training data, encouraging it to learn more general features and thus reducing overfitting.</p> </li> <li> <p>It Reduces Sensitivity to Initialization: The layer makes your model less dependent on the specific random weights it starts with, leading to more reliable and repeatable training results.</p> </li> </ul> <p>By adding <code>BatchNorm2d</code> to your <code>CNNBlock</code>, you are not just adding another layer; you are fundamentally making your model's training process more stable, efficient, and robust.</p> <ol> <li>contrastive loss</li> <li>Sematic segmentation</li> <li>class, bounding box</li> <li>poly nn</li> <li>Representation learning</li> </ol>"},{"location":"distributedsystems/0_intro/","title":"Introduction","text":""},{"location":"distributedsystems/0_intro/#introduction","title":"Introduction","text":"<p>What is \"distributed system\":</p> <p>A group of computers cooperating to provide a service</p>"},{"location":"distributedsystems/0_intro/#why","title":"Why?","text":"<ol> <li>to increase capacity via parallel processing</li> <li>to tolerate faults via replication</li> <li>to match distribution of physical devices e.g. sensors</li> <li>to increase security via isolation</li> </ol>"},{"location":"distributedsystems/0_intro/#challanges","title":"Challanges:","text":"<ul> <li>concurrency</li> <li>complex interactions</li> <li>performance bottlenecks</li> <li>partial failure</li> </ul>"},{"location":"distributedsystems/0_intro/#key-topics","title":"Key Topics","text":""},{"location":"distributedsystems/0_intro/#fault-tolerance","title":"Fault tolerance:","text":"<ul> <li>1000s of servers, big network -&gt; always something broken</li> <li>We'd like to hide these failures from the application.</li> <li>\"High availability\": service continues despite failures</li> <li>Big idea: replicated servers. If one server crashes, can proceed using the other(s).</li> </ul>"},{"location":"distributedsystems/0_intro/#consistency","title":"Consistency:","text":"<ul> <li>General-purpose infrastructure needs well-defined behavior. E.g. \"read(x) yields the value from the most recent write(x).\"</li> <li>Achieving good behavior is hard! e.g. \"replica\" servers are hard to keep identical.</li> </ul>"},{"location":"distributedsystems/0_intro/#performance","title":"Performance:","text":"<ul> <li>The goal: scalable throughput. Nx servers -&gt; Nx total throughput via parallel CPU, RAM, disk, net.</li> <li>Scaling gets harder as N grows:<ul> <li>Load imbalance.</li> <li>Slowest-of-N latency.</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#tradeoffs","title":"Tradeoffs:","text":"<ul> <li>Fault-tolerance, consistency, and performance are enemies.</li> <li>Fault tolerance and consistency require communication<ul> <li>e.g., send data to backup server</li> <li>e.g., check if cached data is up-to-date</li> <li>communication is often slow and non-scalable</li> </ul> </li> <li>Many designs sacrifice consistency to gain speed.<ul> <li>e.g. read(x) might not yield the latest write(x)!</li> <li>Painful for application programmers (or users).</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#implementation","title":"Implementation:","text":"<ul> <li>RPC, threads, concurrency control, configuration.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#mapreduce-a-complete-guide","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#introduction","title":"Introduction","text":"<p>Modern data analysis often involves multi-hour computations on multi-terabyte datasets \u2014  for example, building search indexes, sorting massive logs, or analyzing web graphs.  Such tasks are only practical using thousands of computers working in parallel.</p> <p>MapReduce (MR) is a programming model designed to make large-scale data processing  easy for non-specialist programmers. It lets you write simple sequential code, while the framework handles parallel execution, fault tolerance, and data distribution.</p>"},{"location":"distributedsystems/1_mapreduce/#core-concept","title":"Core Concept","text":"<p>The programmer defines just two functions:</p> <ul> <li><code>Map()</code> \u2013 processes input data and emits key-value pairs.</li> <li><code>Reduce()</code> \u2013 aggregates or summarizes all values associated with a given key.</li> </ul> <p>Everything else \u2014 input splitting, task scheduling, network communication, and fault recovery \u2014  is handled automatically by the MapReduce framework.</p>"},{"location":"distributedsystems/1_mapreduce/#how-mapreduce-works-word-count-example","title":"How MapReduce Works (Word Count Example)","text":""},{"location":"distributedsystems/1_mapreduce/#abstract-view","title":"Abstract View","text":"<pre><code>Input1 -&gt; Map -&gt; a,1 b,1\nInput2 -&gt; Map -&gt;     b,1\nInput3 -&gt; Map -&gt; a,1     c,1\n                    |   |   |\n                    |   |   -&gt; Reduce -&gt; c,1\n                    |   -----&gt; Reduce -&gt; b,2\n                    ---------&gt; Reduce -&gt; a,2\n</code></pre>"},{"location":"distributedsystems/1_mapreduce/#steps","title":"Steps","text":"<ol> <li>Input Splitting \u2014 Data is divided into <code>M</code> splits (files or blocks).</li> <li>Map Phase \u2014 Each split is processed by a Map task, generating <code>(key, value)</code> pairs.</li> <li>Shuffle Phase \u2014 Intermediate pairs are grouped by key and distributed to Reduce tasks.</li> <li>Reduce Phase \u2014 Each Reduce task processes one group and outputs final results.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#word-count-example","title":"Word Count Example","text":"<pre><code># Map function\ndef Map(document):\n    words = document.split()\n    for word in words:\n        emit(word, 1)\n\n# Reduce function\ndef Reduce(word, values):\n    emit(word, sum(values))\n</code></pre> <p>Final Output: </p><pre><code>a: 2\nb: 2\nc: 1\n</code></pre><p></p>"},{"location":"distributedsystems/1_mapreduce/#why-mapreduce-scales-so-well","title":"Why MapReduce Scales So Well","text":"<ul> <li>Parallelism: Map and Reduce tasks run independently, enabling massive parallelism.</li> <li>Automatic Management: The framework handles failures, scheduling, and communication.</li> <li>Simplicity: Developers only implement <code>Map()</code> and <code>Reduce()</code>.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#input-output-storage-via-gfs","title":"Input &amp; Output Storage (via GFS)","text":"<p>MapReduce typically uses a distributed file system such as Google File System (GFS).</p> <ul> <li>Files split into 64 MB chunks, distributed across many servers.</li> <li>Maps read input in parallel; Reduces write output in parallel.</li> <li>Replication (2\u20133 copies) ensures fault tolerance.</li> <li>Data locality: Tasks are often scheduled on the same machine where their data resides.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#inside-the-mapreduce-framework","title":"Inside the MapReduce Framework","text":""},{"location":"distributedsystems/1_mapreduce/#coordinators-role","title":"Coordinator\u2019s Role","text":"<ol> <li>Map Phase</li> <li>Assigns Map tasks to workers.</li> <li>Each Map writes intermediate output to its local disk.</li> <li> <p>Intermediate data is partitioned by <code>hash(key) mod R</code> (R = number of Reduces).</p> </li> <li> <p>Reduce Phase</p> </li> <li>Coordinator assigns Reduce tasks.</li> <li>Each Reduce fetches its partition (bucket) from all Maps.</li> <li> <p>Sorts data by key and processes each group.</p> </li> <li> <p>Output</p> </li> <li>Each Reduce writes its final output to GFS.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#performance-and-bottlenecks","title":"Performance and Bottlenecks","text":""},{"location":"distributedsystems/1_mapreduce/#what-limits-performance","title":"What Limits Performance?","text":"<p>Often, network speed is the main bottleneck \u2014 not CPU or disk speed.</p> <p>Network Transfers Include:</p> <ul> <li>Maps reading input from GFS.</li> <li>Reduces fetching intermediate (shuffled) data from Maps.</li> <li>Reduces writing output to GFS.</li> </ul> <p>Because the shuffle phase may move data as large as the original input, network optimization is critical.</p>"},{"location":"distributedsystems/1_mapreduce/#network-optimizations","title":"Network Optimizations","text":"<ul> <li>Data Locality: Run Map tasks where their input data is stored.</li> <li>Single Network Transfer: Intermediate data stored locally, not in GFS.</li> <li>Hash Partitioning: Reduces transfer large data batches (buckets), minimizing small transfers.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#load-balancing","title":"Load Balancing","text":"<p>Why it matters:  Uneven load causes idle workers waiting for \u201cstragglers\u201d. Solution:  </p> <ul> <li>Create many more tasks than workers.</li> <li>The Coordinator dynamically assigns tasks to free workers.</li> <li>Faster machines handle more tasks; slower ones handle fewer.</li> </ul> <p>This keeps the cluster well-balanced and efficient.</p>"},{"location":"distributedsystems/1_mapreduce/#fault-tolerance","title":"Fault Tolerance","text":"<p>Failures are expected in large clusters. MapReduce handles them gracefully.</p>"},{"location":"distributedsystems/1_mapreduce/#worker-failures","title":"Worker Failures","text":"<ul> <li> <p>Map worker crash:</p> </li> <li> <p>Intermediate data (stored locally) is lost.</p> </li> <li>Coordinator reassigns those Map tasks to new workers.</li> <li> <p>No need to rerun if Reduces already fetched the data.</p> </li> <li> <p>Reduce worker crash:</p> </li> <li> <p>Completed results are safe (stored in GFS).</p> </li> <li>Unfinished Reduce tasks are rerun elsewhere.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#deterministic-functions-required","title":"Deterministic Functions Required","text":"<p>Because tasks may be re-executed:</p> <ul> <li><code>Map()</code> and <code>Reduce()</code> must be pure functions \u2014 deterministic and side-effect-free.</li> <li>No external state, random numbers, or I/O beyond the framework.</li> </ul> <p>This guarantees identical results across re-runs.</p>"},{"location":"distributedsystems/1_mapreduce/#handling-other-failures","title":"Handling Other Failures","text":"<ul> <li>Duplicate task execution:   Coordinator accepts output from only one instance.</li> <li>Simultaneous Reduce outputs:   GFS\u2019s atomic rename ensures one consistent final file.</li> <li>Stragglers:   Coordinator launches backup copies of slow tasks.</li> <li>Corrupted output or bad hardware:   Not handled \u2014 MR assumes fail-stop (crash, not corrupt) behavior.</li> <li>Coordinator crash:   Not fully addressed in the original paper.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-works-well","title":"Where MapReduce Works Well","text":"<p>Ideal Use Cases:</p> <ul> <li>Batch processing of huge datasets (TB\u2013PB scale)</li> <li>Log analysis (e.g., counting queries, clickstream analytics)</li> <li>Index building for search engines</li> <li>Data transformations (ETL pipelines)</li> <li>Large-scale machine learning preprocessing</li> <li>Sorting and aggregation across distributed data</li> </ul> <p>These workloads share common traits:</p> <ul> <li>Large, independent input records</li> <li>Deterministic, parallel-friendly computation</li> <li>No need for real-time feedback</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-falls-short","title":"Where MapReduce Falls Short","text":"<p>Not Suitable For:</p> <ul> <li> <p>Real-time or streaming data processing   MR is inherently batch-oriented; results appear only after job completion.</p> </li> <li> <p>Interactive querying   Jobs take minutes to hours; unsuitable for low-latency analytics.</p> </li> <li> <p>Iterative algorithms   Machine learning or graph algorithms (e.g., PageRank, K-means) need multiple    passes over data, causing heavy I/O.</p> </li> <li> <p>Stateful or dependent tasks   MR disallows inter-task communication or shared state.</p> </li> <li> <p>Small or medium datasets   Overhead of distributing tasks outweighs benefits.</p> </li> </ul> <p>Modern systems like Apache Spark, Flink, or Beam were designed to overcome these limitations by enabling in-memory and streaming computation.</p>"},{"location":"distributedsystems/2_threads/","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#65840-lecture-2-2025-threads-and-rpc","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#introduction-implementing-distributed-systems","title":"Introduction: Implementing Distributed Systems","text":"<p>This lecture introduces: - Go threads (goroutines) - Concurrency challenges - The web crawler example - Remote Procedure Calls (RPC)</p> <p>Go is the language used for this writeup.</p>"},{"location":"distributedsystems/2_threads/#why-go","title":"Why Go?","text":"<p>Go is well-suited for distributed systems:</p> <ul> <li>Excellent thread (goroutine) support  </li> <li>Convenient RPC library</li> <li>Type- and memory-safe</li> <li>Garbage-collected (safe with concurrency)</li> <li>Simpler than many other languages</li> <li>Commonly used in production distributed systems</li> </ul> <p>\ud83d\udc49 After the tutorial, read Effective Go: https://golang.org/doc/effective_go.html</p>"},{"location":"distributedsystems/2_threads/#threads-goroutines","title":"Threads (Goroutines)","text":""},{"location":"distributedsystems/2_threads/#what-is-a-thread","title":"What is a Thread?","text":"<p>A thread is a \u201cthread of execution\u201d:</p> <ul> <li>Allows a program to do multiple things at once</li> <li>Executes sequentially (like a program), but shares memory with other threads</li> <li>Has its own program counter, registers, and stack</li> </ul> <p>In Go, threads are called goroutines.</p>"},{"location":"distributedsystems/2_threads/#why-use-threads","title":"Why Use Threads?","text":"<p>Three type of threads:</p>"},{"location":"distributedsystems/2_threads/#1-io-concurrency","title":"1. I/O Concurrency","text":"<ul> <li>Client sends requests to many servers at once  </li> <li>Server handles many clients concurrently  </li> <li>When one thread blocks on I/O, another can run</li> </ul>"},{"location":"distributedsystems/2_threads/#2-multicore-performance","title":"2. Multicore Performance","text":"<p>Use multiple CPU cores simultaneously.</p>"},{"location":"distributedsystems/2_threads/#3-convenience","title":"3. Convenience","text":"<p>Run background tasks (e.g., periodic health checks).</p>"},{"location":"distributedsystems/2_threads/#alternative-event-driven-systems","title":"Alternative: Event-Driven Systems","text":"<p>Instead of threads:</p> <ul> <li>Use a single-threaded system with an event loop</li> <li>Explicitly interleave different activities</li> <li>Maintain state tables for each ongoing operation</li> </ul> <p>Pros:  </p> <ul> <li>Good for I/O concurrency  </li> <li>No thread overhead</li> </ul> <p>Cons:  </p> <ul> <li>No multicore usage  </li> <li>Hard to program and maintain</li> </ul>"},{"location":"distributedsystems/2_threads/#threading-challenges","title":"Threading Challenges","text":""},{"location":"distributedsystems/2_threads/#1-safe-data-sharing","title":"1. Safe Data Sharing","text":"<p>Race example: </p><pre><code>n = n + 1\n</code></pre> Two threads modifying <code>n</code> at the same time \u2192 race condition.<p></p> <p>A race is when: - Two threads access the same memory - At least one is a write - And there's no synchronization</p> <p>Fixes: - Use <code>sync.Mutex</code> - Avoid sharing mutable data</p>"},{"location":"distributedsystems/2_threads/#2-coordination-producerconsumer","title":"2. Coordination (Producer\u2013Consumer)","text":"<ul> <li>One thread produces data  </li> <li>Another consumes it  </li> <li>Need a way for consumers to wait and wake up</li> </ul> <p>Tools: - Go channels - <code>sync.Cond</code> - <code>sync.Wait</code>, <code>sync.WaitGroup</code></p>"},{"location":"distributedsystems/2_threads/#3-deadlock","title":"3. Deadlock","text":"<p>When threads wait on each other forever. Can happen via:</p> <ul> <li>Locks</li> <li>Channels</li> <li>RPC</li> </ul>"},{"location":"distributedsystems/2_threads/#web-crawler-example","title":"Web Crawler Example","text":"<p>A web crawler:</p> <ul> <li>Fetches web pages recursively starting from a URL</li> <li>Follows links</li> <li>Avoids revisiting pages</li> <li>Avoids cycles</li> <li>Exploits I/O concurrency for speed</li> </ul>"},{"location":"distributedsystems/2_threads/#1-serial-crawler","title":"1. Serial Crawler","text":"<ul> <li>Depth-first traversal  </li> <li>A shared map tracks visited URLs  </li> <li>Simple and correct  </li> <li>Very slow \u2014 only fetches one page at a time  </li> </ul> <p>Adding <code>go</code> before recursive calls breaks correctness:</p> <ul> <li>Many threads may fetch same URL  </li> <li>Finishing detection becomes difficult</li> </ul>"},{"location":"distributedsystems/2_threads/#2-concurrent-crawler-with-mutex","title":"2. Concurrent Crawler with Mutex","text":""},{"location":"distributedsystems/2_threads/#how-it-works","title":"How it Works","text":"<ul> <li>Launch a goroutine per page</li> <li>Shared <code>fetched</code> map  </li> <li>Mutex ensures only one thread fetches each URL</li> </ul>"},{"location":"distributedsystems/2_threads/#why-the-mutex","title":"Why the Mutex?","text":""},{"location":"distributedsystems/2_threads/#1-avoid-logical-races","title":"1. Avoid Logical Races","text":"<p>Two threads may check the same URL at once: - Both see <code>fetched[url] == false</code> - Both fetch \u2192 wrong</p> <p>Mutex ensures: - One thread checks + sets at a time</p>"},{"location":"distributedsystems/2_threads/#2-avoid-map-corruption","title":"2. Avoid Map Corruption","text":"<p>Go maps are not thread-safe.</p>"},{"location":"distributedsystems/2_threads/#what-if-lock-is-removed","title":"What If Lock Is Removed?","text":"<ul> <li>Program may appear to work sometimes  </li> <li>But races still occur  </li> <li>Use the race detector:</li> </ul> <pre><code>go run -race crawler.go\n</code></pre>"},{"location":"distributedsystems/2_threads/#completion-detection-using-syncwaitgroup","title":"Completion Detection Using sync.WaitGroup","text":"<ul> <li><code>Add(n)</code> increments  </li> <li><code>Done()</code> decrements  </li> <li><code>Wait()</code> blocks until count is zero  </li> </ul> <p>Ensures main thread waits for all children.</p>"},{"location":"distributedsystems/2_threads/#3-concurrent-crawler-with-channels","title":"3. Concurrent Crawler with Channels","text":"<p>Channels provide:</p> <ul> <li>Communication  </li> <li>Synchronization  </li> </ul>"},{"location":"distributedsystems/2_threads/#channel-basics","title":"Channel Basics","text":"<pre><code>ch := make(chan int)\n\nch &lt;- x   // send (blocks)\ny := &lt;-ch // receive (blocks)\n</code></pre>"},{"location":"distributedsystems/2_threads/#coordinator-workers-model","title":"Coordinator + Workers Model","text":"<ul> <li>Coordinator creates workers via goroutines  </li> <li>Workers fetch a page and send resulting URLs via channel  </li> <li>Coordinator receives URLs, checks visited set</li> </ul>"},{"location":"distributedsystems/2_threads/#why-no-mutex","title":"Why No Mutex?","text":"<ul> <li>Shared state is only in coordinator  </li> <li>Workers never mutate shared maps  </li> <li>Therefore no races</li> </ul>"},{"location":"distributedsystems/2_threads/#channel-safety","title":"Channel Safety","text":"<p>Example:</p> <ul> <li>Worker creates slice of URLs</li> <li>Sends it to channel</li> <li>Coordinator reads it</li> </ul> <p>Safe because:</p> <ul> <li>Worker writes slice before send completes</li> <li>Coordinator reads slice after receive completes</li> </ul> <p>No overlap \u2192 no race.</p>"},{"location":"distributedsystems/2_threads/#why-some-sends-need-a-goroutine","title":"Why Some Sends Need a Goroutine?","text":"<p>Without a goroutine:</p> <ul> <li>send blocks  </li> <li>coordinator may not reach the receive  </li> <li>\u2192 deadlock</li> </ul>"},{"location":"distributedsystems/2_threads/#locks-vs-channels","title":"Locks vs Channels","text":"<p>Both are powerful. Use whichever matches intuition:</p> <ul> <li>State-focused logic \u2192 locks</li> <li>Communication-focused logic \u2192 channels</li> </ul> <p>In 6.5840 labs: - Use sharing + locks for state - Use channels, <code>sync.Cond</code>, or sleep-based polling for notifications</p>"},{"location":"distributedsystems/2_threads/#remote-procedure-call-rpc","title":"Remote Procedure Call (RPC)","text":"<p>RPC enables easy client-server communication.</p>"},{"location":"distributedsystems/2_threads/#goals","title":"Goals","text":"<ul> <li>Hide network details</li> <li>Provide a procedure-call interface</li> <li>Automatically marshal/unmarshal data</li> <li>Enable portability across systems</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-architecture","title":"RPC Architecture","text":"<pre><code>Client               Server\n  request ----&gt;\n            &lt;---- response\n</code></pre> <p>Software structure: </p><pre><code>Client App        Server Handlers\nClient Stubs      Dispatcher\nRPC Library  ---- RPC Library\n Network     ---- Network\n</code></pre><p></p>"},{"location":"distributedsystems/2_threads/#go-rpc-example-keyvalue-store","title":"Go RPC Example: Key/Value Store","text":"<p>Handlers: - <code>Put(key, value)</code> - <code>Get(key) -&gt; value</code></p>"},{"location":"distributedsystems/2_threads/#client-side","title":"Client Side","text":"<ul> <li>Use <code>Dial()</code> to connect  </li> <li>Call RPC using:</li> </ul> <pre><code>Call(\"KVServer.Get\", args, &amp;reply)\n</code></pre> <p>RPC library: - Marshals args - Sends request - Waits for reply - Unmarshals reply - Returns error if something went wrong  </p>"},{"location":"distributedsystems/2_threads/#server-side","title":"Server Side","text":"<p>Server must: 1. Declare a type with exported RPC methods 2. Register the type 3. Accept TCP connections and let RPC library handle them  </p> <p>RPC library:</p> <ul> <li>Creates goroutine per request  </li> <li>Unmarshals request  </li> <li>Dispatches handler  </li> <li>Marshals reply  </li> <li>Sends reply  </li> </ul> <p>Handlers must use locks since multiple RPCs run concurrently.</p>"},{"location":"distributedsystems/2_threads/#rpc-details","title":"RPC Details","text":""},{"location":"distributedsystems/2_threads/#binding","title":"Binding","text":"<p>Client must know <code>\"server:port\"</code> to dial.</p>"},{"location":"distributedsystems/2_threads/#marshalling-rules","title":"Marshalling Rules","text":"<ul> <li>Sends strings, arrays, structs, maps  </li> <li>Cannot send channels or functions  </li> <li>Only exported fields in structs are marshaled  </li> <li>Pointers are sent by copying the pointee</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-failures","title":"RPC Failures","text":"<p>Client may never get a reply:</p> <p>Could mean:</p> <ul> <li>Server never received request  </li> <li>Server crashed after executing  </li> <li>Reply lost in network  </li> <li>Network or server slow  </li> </ul> <p>RPC \u2260 local function call.</p>"},{"location":"distributedsystems/2_threads/#best-effort-rpc","title":"Best-Effort RPC","text":"<p>Algorithm: 1. Send request 2. Wait 3. If no reply, resend 4. After several tries \u2192 give up  </p>"},{"location":"distributedsystems/2_threads/#problems","title":"Problems","text":"<p>Example: </p><pre><code>Put(\"k\", 10)\nPut(\"k\", 20)\n</code></pre><p></p> <p>Retries can reorder or duplicate operations.</p>"},{"location":"distributedsystems/2_threads/#when-is-best-effort-ok","title":"When Is Best-Effort OK?","text":"<ul> <li>Read-only operations  </li> <li>Idempotent operations (safe to repeat)</li> </ul>"},{"location":"distributedsystems/2_threads/#at-most-once-semantics","title":"At-Most-Once Semantics","text":"<p>Go RPC provides:</p> <ul> <li>One TCP connection  </li> <li>Sends each request once  </li> <li>No retries \u2192 no duplicates  </li> </ul> <p>But:</p> <ul> <li>Errors returned on timeouts  </li> <li>Hard to build replicated fault-tolerant systems without retries  </li> </ul> <p>Later labs explore stronger semantics.</p>"},{"location":"informationtheory/1_intro_to_infotheory/","title":"1. Introduction","text":""},{"location":"informationtheory/1_intro_to_infotheory/#chapter-1-introduction-to-information-theory-for-machine-learning","title":"Chapter 1 \u2014 Introduction to Information Theory (for Machine Learning)","text":"<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p> <ul> <li>loss functions (cross-entropy, NLL)</li> <li>representation learning and contrastive learning</li> <li>variational inference and VAEs</li> <li>generative modeling (GANs, flows, diffusion)</li> <li>reinforcement learning (entropy bonuses, policy KL constraints)</li> <li>model capacity, generalization, and bottlenecks</li> </ul> <p>This chapter introduces the core motivations and conceptual tools.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#1-why-information-theory-matters-for-ml","title":"1. Why Information Theory Matters for ML","text":"<p>Information theory answers questions fundamental to ML:</p> <ul> <li>How much uncertainty does a model reduce?</li> <li>How do we quantify the difference between two probability distributions?</li> <li>How do we measure dependence between variables?</li> <li>What is the maximum information a neural network layer can transmit?</li> <li>How do we formalize compression and generalization?</li> </ul> <p>In ML, information theory is not abstract mathematics \u2014 it provides the language for describing learning itself:</p> <p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p> <p>This viewpoint unifies:</p> <ul> <li>Maximum likelihood  </li> <li>Variational inference  </li> <li>Contrastive learning  </li> <li>GAN objectives  </li> <li>Representation learning  </li> <li>Reinforcement learning signal shaping  </li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#2-the-communication-view-shannons-formulation","title":"2. The Communication View (Shannon\u2019s Formulation)","text":"<p>A classical communication system consists of:</p> <ol> <li> <p>Source:    Generates data (symbols, images, text, states).</p> </li> <li> <p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p> </li> <li> <p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p> </li> <li> <p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p> </li> <li> <p>Receiver:   Obtains the final predictions or reconstructions.</p> </li> </ol> <p>Information theory studies:</p> <ul> <li>Limits of efficient communication  </li> <li>Optimal encoding and representation  </li> <li>Tradeoffs between compression and fidelity  </li> <li>Effect of noise on learnability</li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#3-the-uncertainty-view-shannonbayesian-perspective","title":"3. The Uncertainty View (Shannon\u2013Bayesian Perspective)","text":"<p>Information theory also quantifies uncertainty:</p> <ul> <li>More uncertainty \u2192 more information needed  </li> <li>Less uncertainty \u2192 easier prediction and compression  </li> </ul> <p>Key idea: Information is the reduction of uncertainty.</p> <p>In ML:</p> <ul> <li>Entropy measures label uncertainty  </li> <li>Cross-entropy measures model fit  </li> <li>KL divergence measures mismatch  </li> <li>Mutual information measures representation quality  </li> <li>ELBO measures how well a generative model explains data  </li> </ul> <p>Thus, learning and compression are mathematically the same problem.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#4-machine-learning-as-communication","title":"4. Machine Learning as Communication","text":"<p>Modern ML pipelines resemble a communication system:</p>"},{"location":"informationtheory/1_intro_to_infotheory/#data-encoder-latent-representation-decoder-output","title":"Data \u2192 Encoder \u2192 Latent Representation \u2192 Decoder \u2192 Output","text":"<p>Examples:</p> <ul> <li>Autoencoders / VAEs: compress \\(x\\) into \\(z\\), then reconstruct</li> <li>Transformers: compress sequences into features, decode predictions</li> <li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li> <li>GANs: learn generator distributions close to data distribution</li> <li>RL agents: compress sensory input into state representations</li> </ul> <p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#5-roadmap-for-this-web-book","title":"5. Roadmap for This Web-book","text":"<p>This web-book is structured to build information theory specifically for ML:</p> <ol> <li> <p>Entropy &amp; Self-Information    Foundations of uncertainty, coding length, and compression.</p> </li> <li> <p>Cross-Entropy &amp; Negative Log-Likelihood    Core ML loss; the bridge between probability and training objectives.</p> </li> <li> <p>KL Divergence &amp; f-Divergences    Quantifying model mismatch, VI, GAN divergences.</p> </li> <li> <p>Jensen\u2013Shannon &amp; Wasserstein Distances    GAN stability, geometric learning, distribution metrics.</p> </li> <li> <p>Mutual Information &amp; Estimation Bounds    Representation learning, contrastive learning, InfoNCE.</p> </li> <li> <p>Variational Inference &amp; ELBO    VAEs, Bayesian deep learning, posterior approximations.</p> </li> <li> <p>Information Bottleneck &amp; Representation Theory    Why deep networks compress, and how representations generalize.</p> </li> <li> <p>Summary &amp; Concept Map    Unifying view of entropy \u2192 KL \u2192 MI \u2192 VI \u2192 representation learning.</p> </li> </ol>"},{"location":"informationtheory/2_entropy/","title":"2. Entropy, Self-Information & Cross-Entropy","text":""},{"location":"informationtheory/2_entropy/#chapter-2-entropy-self-information-cross-entropy-information-measures","title":"Chapter 2 \u2014 Entropy, Self-Information, Cross-Entropy &amp; Information Measures","text":""},{"location":"informationtheory/2_entropy/#1-self-information-surprisal","title":"1. Self-Information (Surprisal)","text":"<p>Self-information quantifies the surprise of observing an event.</p> <p>For an event with probability \\(p(x)\\):</p> \\[ I(x) = - \\log_2 p(x) \\] <p>Why the log?</p> <ul> <li>Additivity of independent events  </li> <li>Probability \u2192 information monotonicity  </li> <li>Log base 2 \u2192 units in bits  </li> <li>Log-likelihoods become additive \u2192 ML becomes convex (in many models)</li> </ul> <p>Interpretations:</p> <ul> <li>Unlikely events carry more information  </li> <li>Certain events carry zero information  </li> <li>Foundation of cross-entropy and negative log-likelihood  </li> </ul> <p>In ML:  </p> <ul> <li>The loss used in classification is simply the surprisal of the correct class.</li> </ul>"},{"location":"informationtheory/2_entropy/#2-entropy-expected-uncertainty","title":"2. Entropy \u2014 Expected Uncertainty","text":"<p>Entropy is the expected self-information:</p> \\[ H(X) = -\\sum_x p(x) \\log p(x) \\] <p>Entropy measures:</p> <ul> <li>Uncertainty  </li> <li>Randomness  </li> <li>Compressibility  </li> <li>Difficulty of prediction  </li> </ul>"},{"location":"informationtheory/2_entropy/#key-properties","title":"Key properties:","text":"<ul> <li>\\(H(X) = 0\\) if a variable is deterministic  </li> <li>Maximum when distribution is uniform  </li> <li>Upper bound on achievable compression (Shannon)</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation","title":"ML Interpretation:","text":"<ul> <li>High entropy labels \u2192 noisy dataset \u2192 harder learning</li> <li>Activation entropy reflects network expressiveness</li> <li>Entropy of output distribution measures model confidence</li> <li>Entropy regularization improves exploration in RL</li> </ul>"},{"location":"informationtheory/2_entropy/#3-differential-entropy-continuous-entropy","title":"3. Differential Entropy (Continuous Entropy)","text":"<p>For continuous variables:</p> \\[ h(X) = -\\int p(x) \\log p(x)\\,dx \\] <p>Important differences:</p> <ul> <li>Can be negative</li> <li>Not invariant under reparameterization</li> <li>Not comparable between different coordinate systems</li> </ul>"},{"location":"informationtheory/2_entropy/#why-it-matters-in-ml","title":"Why it matters in ML:","text":"<ul> <li>VAEs use continuous latent variables \\(z\\)</li> <li>Flows and diffusion models use continuous densities</li> <li>Score-based models estimate gradients of log-densities, not densities directly</li> </ul> <p>Differential entropy is not the same thing as Shannon entropy \u2014 a common source of confusion.</p>"},{"location":"informationtheory/2_entropy/#4-joint-conditional-and-total-entropy","title":"4. Joint, Conditional, and Total Entropy","text":""},{"location":"informationtheory/2_entropy/#joint-entropy","title":"Joint entropy:","text":"\\[ H(X,Y) = -\\sum_{x,y} p(x,y)\\log p(x,y) \\]"},{"location":"informationtheory/2_entropy/#conditional-entropy","title":"Conditional entropy:","text":"\\[ H(Y|X) = -\\sum_{x,y} p(x,y)\\log p(y|x) \\] <p>Interpretation:</p> <ul> <li>Average residual uncertainty in \\(Y\\) after observing \\(X\\)</li> </ul>"},{"location":"informationtheory/2_entropy/#chain-rule-of-entropy","title":"Chain rule of entropy:","text":"\\[ H(X,Y) = H(X) + H(Y|X) \\] <p>This rule is foundational for:</p> <ul> <li>Autoregressive modeling  </li> <li>Sequence modeling  </li> <li>Transformers (predictive factorization)  </li> <li>Bayesian networks  </li> </ul>"},{"location":"informationtheory/2_entropy/#5-cross-entropy-coding-p-using-q","title":"5. Cross-Entropy \u2014 Coding \\(p\\) Using \\(q\\)","text":"<p>Cross-entropy is the expected surprise under model \\(q\\):</p> \\[ H(p, q) = -\\sum_x p(x)\\log q(x) \\]"},{"location":"informationtheory/2_entropy/#crucial-identity","title":"Crucial identity:","text":"\\[ H(p, q) = H(p) + D_{\\text{KL}}(p\\|q) \\] <p>Meaning:</p> <ul> <li>True entropy + penalty for using the wrong distribution</li> <li>Cross-entropy \u2265 entropy</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation_1","title":"ML Interpretation:","text":"<p>Cross-entropy = Negative Log Likelihood:</p> \\[ \\mathcal{L} = - \\log q(y_{\\text{true}}) \\] <p>This powers:</p> <ul> <li>Softmax classifiers  </li> <li>Logistic regression  </li> <li>Transformers (next-token prediction)  </li> <li>Language models (autoregressive LM)  </li> <li>Image segmentation (pixel-wise CE)  </li> </ul> <p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>"},{"location":"informationtheory/2_entropy/#6-perplexity-entropy-in-language-modeling","title":"6. Perplexity \u2014 Entropy in Language Modeling","text":"<p>Perplexity is:</p> \\[ \\text{PPL} = 2^{H} \\] <p>Interpretation:</p> <ul> <li>The \u201ceffective vocabulary size\u201d the model thinks it must guess from</li> <li>Lower perplexity = better language model</li> </ul> <p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>"},{"location":"informationtheory/2_entropy/#7-mutual-information-information-shared-between-variables","title":"7. Mutual Information \u2014 Information Shared Between Variables","text":"\\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)) \\] <p>MI measures:</p> <ul> <li>How much knowing \\(X\\) tells us about \\(Y\\)</li> <li>Reduction in entropy of one variable after observing the other</li> </ul>"},{"location":"informationtheory/2_entropy/#equivalent-forms","title":"Equivalent forms:","text":"\\[ I(X;Y) = H(X) - H(X|Y) \\] \\[ I(X;Y) = H(X) + H(Y) - H(X,Y) \\] <p>MI links entropy and KL divergence into a unified measure of dependence.</p>"},{"location":"informationtheory/2_entropy/#why-mi-is-critical-in-ml","title":"Why MI is critical in ML:","text":"<ul> <li>Representation learning (maximize MI with labels)</li> <li>Contrastive learning (InfoNCE is a lower bound to MI)</li> <li>InfoGAN (maximize MI between latent code and output)</li> <li>Feature selection (choose features with highest MI to labels)</li> <li>Stochastic encoders control MI with constraints</li> </ul>"},{"location":"informationtheory/2_entropy/#8-the-data-processing-inequality-dpi","title":"8. The Data Processing Inequality (DPI)","text":"<p>If:</p> \\[ X \\rightarrow Z \\rightarrow Y \\] <p>is a Markov chain, then:</p> \\[ I(X;Y) \\le I(X;Z) \\] <p>Meaning:</p> <ul> <li>Processing or compressing data cannot add information</li> <li>Neural networks cannot create information about the input   \u2014 they can only discard or transform it</li> </ul> <p>ML relevance:</p> <ul> <li>Explains why deeper layers become more task-specialized  </li> <li>Supports the Information Bottleneck theory in deep learning  </li> <li>Ensures that any learned representation is bounded by input information  </li> <li>Helps analyze generalization and compression in deep nets</li> </ul>"},{"location":"informationtheory/2_entropy/#9-entropy-in-neural-networks","title":"9. Entropy in Neural Networks","text":"<p>Entropy plays multiple roles in deep learning:</p>"},{"location":"informationtheory/2_entropy/#output-entropy","title":"Output entropy","text":"<p>Low entropy \u2192 confident predictions High entropy \u2192 uncertainty</p>"},{"location":"informationtheory/2_entropy/#entropy-of-hidden-representations","title":"Entropy of hidden representations","text":"<ul> <li>Early layers: reduce entropy (denoising)  </li> <li>Deep layers: compress irrelevant information  </li> <li>Good representations retain low entropy but high MI with labels</li> </ul>"},{"location":"informationtheory/2_entropy/#entropy-regularization-in-rl","title":"Entropy regularization in RL","text":"<p>  encourages exploration.</p>"},{"location":"informationtheory/2_entropy/#dropout-increases-entropy","title":"Dropout increases entropy","text":"<p>forcing models to encode more robust representations.</p>"},{"location":"informationtheory/3_KL/","title":"3. Kullback-Leibler Divergence","text":"<p>Chapter 3 \u2014 KL Divergence, f-Divergences, Jensen\u2013Shannon Divergence, and Wasserstein Distance</p> <p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>"},{"location":"informationtheory/3_KL/#1-kl-divergence-measuring-distribution-mismatch","title":"1. KL Divergence: Measuring Distribution Mismatch","text":"<p>The Kullback\u2013Leibler (KL) divergence measures how different two probability distributions are. For distributions \\(p\\) and \\(q\\):</p> \\[ D_{\\text{KL}}(p\\|q) = \\sum_x p(x)\\log\\frac{p(x)}{q(x)}. \\] <p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from \\(p\\) using a code optimized for \\(q\\). If \\(q\\) assigns very low probability to events that occur frequently under \\(p\\), the KL divergence becomes large.</p>"},{"location":"informationtheory/3_KL/#key-properties-of-kl-divergence","title":"Key properties of KL divergence","text":"<ol> <li> <p>Non-negative </p> </li> <li> <p>Zero only when the two distributions are identical.</p> </li> <li> <p>Asymmetric </p> </li> <li> <p>Not a true metric, since it fails the triangle inequality.</p> </li> <li> <p>Can be infinite when \\(p(x) &gt; 0\\) but \\(q(x) = 0\\).    This is a crucial issue in generative modeling, where such mismatches occur frequently.</p> </li> </ol>"},{"location":"informationtheory/3_KL/#2-kl-divergence-in-machine-learning","title":"2. KL Divergence in Machine Learning","text":"<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>"},{"location":"informationtheory/3_KL/#21-maximum-likelihood-as-forward-kl-minimization","title":"2.1 Maximum likelihood as forward KL minimization","text":"<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p> \\[ \\theta^* = \\arg\\min_\\theta D_{\\text{KL}}(p_{\\text{data}} \\,\\|\\, q_\\theta). \\] <p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p> <p>This produces mode-covering behavior, which is characteristic of:</p> <ul> <li>normalizing flows  </li> <li>autoregressive models  </li> <li>density estimation models trained via log-likelihood  </li> </ul>"},{"location":"informationtheory/3_KL/#22-kl-divergence-in-variational-inference-vi","title":"2.2 KL divergence in variational inference (VI)","text":"<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior \\(q(z|x)\\) and the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Since the true posterior is typically intractable, VAEs approximate this with:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p> <p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative \u201csafe\u201d solution.</p>"},{"location":"informationtheory/3_KL/#23-kl-divergence-in-reinforcement-learning","title":"2.3 KL divergence in reinforcement learning","text":"<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p> \\[ D_{\\text{KL}}(\\pi_{\\text{old}} \\,\\|\\, \\pi_{\\text{new}}). \\] <p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>"},{"location":"informationtheory/3_KL/#24-kl-divergence-in-distillation-and-compression","title":"2.4 KL divergence in distillation and compression","text":"<p>KL divergence compares two probability distributions directly and is used for:</p> <ul> <li>teacher\u2013student distillation  </li> <li>compressing large models into smaller ones  </li> <li>aligning probability distributions across layers  </li> <li>calibrating output probabilities  </li> </ul> <p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>"},{"location":"informationtheory/3_KL/#3-understanding-kl-behavior-mode-covering-vs-mode-seeking","title":"3. Understanding KL Behavior: Mode Covering vs. Mode Seeking","text":"<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>"},{"location":"informationtheory/3_KL/#forward-kl-d_textklpq","title":"Forward KL: \\(D_{\\text{KL}}(p\\|q)\\)","text":"<p>(Used in maximum likelihood, flows \u2192 mode covering)</p> <p>Forward KL asks whether the model \\(q\\) assigns sufficient probability wherever the data distribution \\(p\\) has mass:</p> <p>\u201cDoes the model assign enough probability to every place where the data occurs?\u201d</p> <p>If \\(q\\) misses even a small region where \\(p\\) has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p> <p>Result: mode covering The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p> <p>Flows and MLE-based models display this behavior.</p>"},{"location":"informationtheory/3_KL/#reverse-kl-d_textklqp","title":"Reverse KL: \\(D_{\\text{KL}}(q\\|p)\\)","text":"<p>(Used in VI, VAEs, GAN-like behavior \u2192 mode seeking)</p> <p>Reverse KL asks the opposite question:</p> <p>\u201cIs the model placing probability in places where the data distribution is very small or zero?\u201d</p> <p>Reverse KL heavily penalizes placing mass in low-density regions of \\(p\\), making the model conservative.</p> <p>Result: mode seeking The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p> <p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>"},{"location":"informationtheory/3_KL/#4-f-divergences-a-unified-family-of-divergences","title":"4. f-Divergences: A Unified Family of Divergences","text":"<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function \\(f\\):</p> \\[ D_f(p\\|q) = \\sum_x q(x)\\, f\\!\\left(\\frac{p(x)}{q(x)}\\right). \\]"},{"location":"informationtheory/3_KL/#5-jensenshannon-divergence-the-original-gan-divergence","title":"5. Jensen\u2013Shannon Divergence: The Original GAN Divergence","text":"<p>The Jensen\u2013Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p> \\[ \\text{JS}(p\\|q) = \\frac12 D_{\\text{KL}}(p\\|m) + \\frac12 D_{\\text{KL}}(q\\|m) \\] <p>where the mixture is:</p> \\[ m = \\frac12(p+q). \\] <p>JS divergence is symmetric and always lies between 0 and \\(\\log 2\\).</p>"},{"location":"informationtheory/3_KL/#why-js-appears-in-gans","title":"Why JS appears in GANs","text":"<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p> \\[ \\text{JS}(p\\|q) - \\log 2. \\] <p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>"},{"location":"informationtheory/3_KL/#6-why-js-divergence-causes-gan-instability","title":"6. Why JS Divergence Causes GAN Instability","text":"<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of \\(p\\) and \\(q\\) are disjoint:</p> \\[ \\text{JS}(p\\|q) = \\log 2. \\] <p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p> <p>Consequences:</p> <ol> <li>The discriminator immediately becomes perfect.  </li> <li>The generator stops receiving meaningful gradients.  </li> <li>Training often collapses, oscillates, or diverges.  </li> </ol> <p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>"},{"location":"informationtheory/3_KL/#7-total-variation-and-hellinger-distances","title":"7. Total Variation and Hellinger Distances","text":"<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>"},{"location":"informationtheory/3_KL/#71-total-variation-tv-distance","title":"7.1 Total Variation (TV) Distance","text":"\\[ \\text{TV}(p,q) = \\frac12\\sum_x |p(x)-q(x)|. \\] <p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform \\(p\\) into \\(q\\).</p> <p>Applications in ML:</p> <ul> <li>Robustness under distribution shift  </li> <li>Generalization bounds (PAC-Bayes)  </li> <li>Fairness and safety  </li> </ul>"},{"location":"informationtheory/3_KL/#72-hellinger-distance","title":"7.2 Hellinger Distance","text":"\\[ H^2(p,q) = \\frac12 \\sum_x\\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^2. \\] <p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p> <p>Uses in ML include:</p> <ul> <li>Robust statistics  </li> <li>Domain adaptation  </li> <li>Generalization theory  </li> <li>Some GAN formulations  </li> </ul>"},{"location":"informationtheory/3_KL/#8-wasserstein-distance-geometry-of-probability-distributions","title":"8. Wasserstein Distance: Geometry of Probability Distributions","text":"<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>"},{"location":"informationtheory/3_KL/#81-primal-form-earth-mover-interpretation","title":"8.1 Primal form (Earth Mover interpretation)","text":"\\[ W(p,q) = \\inf_{\\gamma \\in \\Gamma(p,q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]. \\] <p>It seeks the transport plan \\(\\gamma\\) requiring the least expected effort to turn \\(p\\) into \\(q\\).</p>"},{"location":"informationtheory/3_KL/#82-dual-form-used-in-wgan","title":"8.2 Dual form (used in WGAN)","text":"\\[ W(p,q) = \\sup_{\\|f\\|_L\\le 1} \\left(\\mathbb{E}_p[f(x)]      - \\mathbb{E}_q[f(x)]\\right). \\] <p>GANs implement \\(f\\) as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>"},{"location":"informationtheory/3_KL/#83-why-wasserstein-solves-gan-instability","title":"8.3 Why Wasserstein solves GAN instability","text":"<p>Wasserstein distance has several advantages:</p> <ul> <li>Provides informative gradients even with no overlap  </li> <li>Reflects the actual geometry of the data space  </li> <li>Avoids the saturation and vanishing gradients of JS divergence  </li> <li>Works reliably in high-dimensional spaces  </li> </ul> <p>These properties make Wasserstein GANs far more stable than classical GANs.</p>"},{"location":"informationtheory/3_KL/#84-wgan-gp-gradient-penalty","title":"8.4 WGAN-GP: Gradient Penalty","text":"<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p> \\[ \\lambda(\\|\\nabla_x f(x)\\|_2 - 1)^2. \\] <p>This produces smoother and more stable training compared to weight clipping.</p>"},{"location":"informationtheory/3_KL/#9-divergence-versus-distance","title":"9. Divergence versus Distance","text":"<p>Divergences such as KL and JS:</p> <ul> <li>may be infinite  </li> <li>are asymmetric  </li> <li>do not behave well when distributions have disjoint support  </li> </ul> <p>Distances such as Wasserstein, TV, and Hellinger:</p> <ul> <li>are symmetric  </li> <li>obey triangle inequality  </li> <li>remain meaningful under distribution shift  </li> </ul> <p>In machine learning:</p> <ul> <li>Divergences are useful for inference and likelihood  </li> <li>Distances are useful for generative modeling and geometry  </li> </ul>"},{"location":"informationtheory/3_KL/#10-why-divergences-fail-in-high-dimensions","title":"10. Why Divergences Fail in High Dimensions","text":"<p>In high-dimensional spaces:</p> <ul> <li>Real and generated samples rarely overlap  </li> <li>KL divergence often becomes infinite  </li> <li>JS divergence becomes flat  </li> <li>Gradients vanish  </li> </ul> <p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p> <p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p> <p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen\u2013Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p> <p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p>"},{"location":"informationtheory/4_bayes/","title":"4. Bayesian Inference","text":"<p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p> <p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>"},{"location":"informationtheory/4_bayes/#1-bayes-rule","title":"1. Bayes\u2019 Rule","text":"<p>Bayes\u2019 theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable \\(z\\) and an observed variable \\(x\\):</p> \\[ p(z|x) = \\frac{p(x|z)\\,p(z)}{p(x)}. \\] <p>Each term has a clear interpretation.</p> <ul> <li>\\(p(z)\\): prior belief about the unknown variable  </li> <li>\\(p(x|z)\\): likelihood of observing \\(x\\) given \\(z\\) </li> <li>\\(p(x)\\): marginal likelihood or evidence  </li> <li>\\(p(z|x)\\): posterior distribution after observing data  </li> </ul> <p>Bayesian inference is the task of computing \\(p(z|x)\\).</p>"},{"location":"informationtheory/4_bayes/#2-priors-encoding-assumptions-about-hidden-variables","title":"2. Priors: Encoding Assumptions About Hidden Variables","text":"<p>The prior \\(p(z)\\) expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>"},{"location":"informationtheory/4_bayes/#21-regularization","title":"2.1 Regularization","text":"<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields \\(L_2\\) regularization.</p>"},{"location":"informationtheory/4_bayes/#22-structural-assumptions","title":"2.2 Structural assumptions","text":"<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>"},{"location":"informationtheory/4_bayes/#23-uncertainty","title":"2.3 Uncertainty","text":"<p>The prior makes explicit that before observing data, we do not know the true value of \\(z\\).</p>"},{"location":"informationtheory/4_bayes/#24-generative-modeling","title":"2.4 Generative modeling","text":"<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>"},{"location":"informationtheory/4_bayes/#3-likelihood-connecting-latent-variables-to-observed-data","title":"3. Likelihood: Connecting Latent Variables to Observed Data","text":"<p>The likelihood \\(p(x|z)\\) describes how the data are generated from latent causes. In many generative models:</p> <ul> <li>\\(z\\) represents latent structure  </li> <li>\\(x\\) represents an image, time series, or text  </li> <li>\\(p(x|z)\\) is parameterized by a neural network decoder  </li> </ul> <p>The likelihood term encourages the latent variable \\(z\\) to explain the observed data.</p>"},{"location":"informationtheory/4_bayes/#4-the-posterior-what-we-really-want-to-compute","title":"4. The Posterior: What We Really Want to Compute","text":"<p>The goal of Bayesian inference is the posterior:</p> \\[ p(z|x) = \\frac{p(x|z)p(z)}{p(x)}. \\] <p>The posterior expresses how our belief about \\(z\\) changes after seeing \\(x\\). It incorporates both:</p> <ul> <li>prior knowledge  </li> <li>evidence from data  </li> </ul> <p>Unfortunately, computing this posterior is usually intractable.</p>"},{"location":"informationtheory/4_bayes/#5-why-exact-inference-is-hard","title":"5. Why Exact Inference Is Hard","text":"<p>The denominator in Bayes\u2019 rule is the marginal likelihood:</p> \\[ p(x) = \\int p(x,z)\\,dz. \\] <p>This integral is often impossible to evaluate directly because:</p> <ul> <li>the latent space \\(z\\) can be high-dimensional  </li> <li>the joint distribution \\(p(x,z)\\) may involve a complex neural network  </li> <li>the integral has no analytic form  </li> </ul> <p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>"},{"location":"informationtheory/4_bayes/#6-maximum-a-posteriori-map-vs-full-bayesian-inference","title":"6. Maximum a Posteriori (MAP) vs Full Bayesian Inference","text":"<p>There are two kinds of Bayesian computation.</p>"},{"location":"informationtheory/4_bayes/#61-map-estimation","title":"6.1 MAP estimation","text":"<p>MAP finds the single most likely value of \\(z\\):</p> \\[ z_{\\text{MAP}} = \\arg\\max_z\\, p(z|x). \\] <p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>"},{"location":"informationtheory/4_bayes/#62-full-posterior-inference","title":"6.2 Full posterior inference","text":"<p>The full posterior \\(p(z|x)\\) describes a distribution over possible values of \\(z\\), reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>"},{"location":"informationtheory/4_bayes/#7-bayesian-latent-variable-models","title":"7. Bayesian Latent-Variable Models","text":"<p>Many generative models are Bayesian latent-variable models with:</p> <ol> <li> <p>a prior over latent variables </p> </li> <li> <p>a conditional likelihood </p> </li> <li> <p>a posterior </p> </li> </ol> <p>Examples include:</p> <ul> <li>VAEs  </li> <li>mixture models  </li> <li>topic models  </li> <li>probabilistic PCA  </li> <li>diffusion models (in a specific sense)  </li> </ul> <p>Bayesian inference is the foundation of these models.</p>"},{"location":"informationtheory/4_bayes/#8-the-evidence-and-its-importance","title":"8. The Evidence and Its Importance","text":"<p>The marginal likelihood, also called the evidence:</p> \\[ p(x) = \\int p(x,z)\\,dz \\] <p>plays several roles:</p> <ul> <li>It normalizes the posterior.  </li> <li>It evaluates how well a model explains data.  </li> <li>It is used in Bayesian model comparison.  </li> <li>Its logarithm appears in training objectives for VAEs and diffusion models.</li> </ul> <p>Maximizing evidence corresponds to learning a model that explains the data well.</p>"},{"location":"informationtheory/4_bayes/#9-bayesian-interpretation-of-kl-divergence","title":"9. Bayesian Interpretation of KL Divergence","text":"<p>KL divergence naturally appears when comparing an approximate posterior \\(q(z|x)\\) with the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Minimizing this KL divergence means making the approximation \\(q\\) as close as possible to the exact posterior.</p> <p>This forms the basis of variational inference.</p>"},{"location":"informationtheory/4_bayes/#10-why-we-need-variational-inference","title":"10. Why We Need Variational Inference","text":"<p>Because the true posterior is intractable, we introduce a simpler distribution \\(q(z|x)\\) and optimize it to approximate \\(p(z|x)\\).</p> <p>We cannot compute:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)) \\] <p>directly, because \\(p(z|x)\\) depends on \\(p(x)\\), which is the intractable integral.</p> <p>Variational inference resolves this by rewriting \\(\\log p(x)\\) and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p> <p>This is the topic of the next chapter.</p> <p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes\u2019 rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p> <p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p>"},{"location":"informationtheory/5_mc_intro/","title":"5. Probability toolbox","text":"<p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}, \\qquad  p(x) = \\int p(x,z)\\,dz. \\] <p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p> <p>This section introduces four major families of methods:</p> <ol> <li>complete enumeration  </li> <li>Laplace approximation  </li> <li>Monte Carlo methods  </li> <li>variational methods  </li> </ol> <p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>"},{"location":"informationtheory/5_mc_intro/#1-complete-enumeration","title":"1. Complete Enumeration","text":"<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p> \\[ p(x) = \\sum_z p(x,z) \\quad \\text{or} \\quad p(x) = \\int p(x,z)\\,dz. \\] <p>This is feasible only when:</p> <ul> <li>the latent variable is low dimensional  </li> <li>the domain is small or discrete  </li> <li>the joint distribution has a simple closed form  </li> </ul> <p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>"},{"location":"informationtheory/5_mc_intro/#2-laplace-approximation","title":"2. Laplace Approximation","text":"<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p> <p>Given a posterior</p> \\[ p(z|x) \\propto p(x,z), \\] <p>the Laplace approximation fits a Gaussian distribution</p> \\[ q(z|x) \\approx \\mathcal{N}(z_{\\text{MAP}}, H^{-1}), \\] <p>where:</p> <ul> <li>\\(z_{\\text{MAP}}\\) is the mode of \\(p(z|x)\\) </li> <li>\\(H\\) is the Hessian of \\(-\\log p(z|x)\\) at the mode  </li> </ul> <p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>"},{"location":"informationtheory/5_mc_intro/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":"<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p> \\[ \\mathbb{E}_{p(z|x)}[f(z)]  \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\qquad z_i \\sim p(z|x). \\] <p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p> <p>Sampling strategies fall into two groups:</p> <ul> <li>independent sampling  </li> <li>Markov chain\u2013based sampling (MCMC)  </li> </ul> <p>The next chapter explains Monte Carlo and sampling methods in detail.</p>"},{"location":"informationtheory/5_mc_intro/#4-variational-methods","title":"4. Variational Methods","text":"<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from \\(p(z|x)\\), we introduce a distribution \\(q(z|x)\\) and optimize it to be close to the true posterior. The objective is to minimize</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x) + D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>"},{"location":"informationtheory/5_mc_intro/#summary","title":"Summary","text":"<p>Approximate inference methods can be understood as four major strategies:</p> <ul> <li>complete enumeration: exact but rarely feasible  </li> <li>Laplace approximation: fast Gaussian approximation near the mode  </li> <li>Monte Carlo methods: sampling-based numerical estimation  </li> <li>variational methods: optimization-based posterior approximation  </li> </ul> <p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p>"},{"location":"informationtheory/6_mc/","title":"6. Monte Carlo Methods","text":"<p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p> <p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>"},{"location":"informationtheory/6_mc/#1-the-goal-of-sampling","title":"1. The Goal of Sampling","text":"<p>Many problems require computing expectations of the form</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\int f(z)\\,p(z)\\,dz, \\] <p>or evaluating posterior quantities such as</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p> <p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>"},{"location":"informationtheory/6_mc/#2-independent-sampling","title":"2. Independent Sampling","text":"<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p> <p>These methods work best when:</p> <ul> <li>sampling directly from \\(p(z)\\) is tractable  </li> <li>the distribution is low-dimensional  </li> <li>the support is simple (e.g., Gaussian, uniform)  </li> </ul>"},{"location":"informationtheory/6_mc/#21-direct-sampling","title":"2.1 Direct Sampling","text":"<p>When the distribution has an invertible CDF \\(F(z)\\):</p> <ol> <li>sample \\(u \\sim \\text{Uniform}(0,1)\\) </li> <li>compute \\(z = F^{-1}(u)\\) </li> </ol> <p>This yields exact samples. It is commonly used in:</p> <ul> <li>uniform sampling  </li> <li>exponential distributions  </li> <li>simple discrete distributions  </li> </ul>"},{"location":"informationtheory/6_mc/#22-importance-sampling","title":"2.2 Importance Sampling","text":"<p>When sampling from \\(p(z)\\) is difficult but evaluating \\(p(z)\\) is easy, importance sampling draws samples from a proposal distribution \\(q(z)\\) and reweights them:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\mathbb{E}_{q(z)}\\left[f(z)\\frac{p(z)}{q(z)}\\right]. \\] <p>The weights</p> <p>   \u200b</p> <p>correct for the fact that samples were drawn from \\(q\\) instead of \\(p\\).</p> <p>The main challenge is weight variability. If \\(q(z)\\) does not closely match \\(p(z)\\), the ratio \\(p(z)/q(z)\\) becomes extremely uneven: most samples have tiny weights, while a few samples have very large weights. This produces high-variance estimates because the estimator becomes dominated by a handful of rare but extremely influential samples. In high-dimensional spaces, designing a proposal \\(q(z)\\) that covers the important regions of \\(p(z)\\) is especially difficult, making importance sampling unreliable unless the proposal distribution is carefully chosen.</p>"},{"location":"informationtheory/6_mc/#23-rejection-sampling","title":"2.3 Rejection Sampling","text":"<p>Rejection sampling draws exact samples from a target distribution \\(p(z)\\) by using a simpler proposal distribution \\(q(z)\\) and accepting or rejecting candidate samples based on how well \\(q\\) covers \\(p\\). The method requires a constant \\(M\\) such that  </p> <p>This condition ensures that \\(Mq(z)\\) forms an envelope that completely contains \\(p(z)\\).</p> <p>Procedure:</p> <ol> <li>sample \\(z \\sim q(z)\\) </li> <li>accept with probability \\(\\frac{p(z)}{M q(z)}\\) </li> </ol> <p>If accepted, \\(z\\) is guaranteed to be an exact draw from \\(p\\).</p> <p>Rejection sampling is conceptually simple and does not distort the target distribution, but it becomes inefficient in many settings. When \\(q(z)\\) is a poor match for \\(p(z)\\), the constant \\(M\\) must be large, which means that most samples are rejected. In high-dimensional spaces, the mismatch between \\(p\\) and \\(q\\) typically worsens exponentially, making the acceptance probability extremely small. As a result, rejection sampling is rarely practical for modern high-dimensional machine-learning models, although it remains useful in low-dimensional problems or when \\(q\\) can be chosen to closely match \\(p\\).</p>"},{"location":"informationtheory/6_mc/#3-monte-carlo-estimation","title":"3. Monte Carlo Estimation","text":"<p>Monte Carlo approximates expectations by:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\quad z_i \\sim p(z). \\] <p>Key properties:</p> <ul> <li>error scales as \\(\\mathcal{O}(1/\\sqrt{N})\\) </li> <li>works in high dimensions  </li> <li>accuracy depends on sampling quality  </li> </ul> <p>Monte Carlo is the backbone of almost all probabilistic computation.</p>"},{"location":"informationtheory/6_mc/#4-markov-chain-monte-carlo-mcmc","title":"4. Markov Chain Monte Carlo (MCMC)","text":"<p>When sampling directly from \\(p(z)\\) is hard, Markov Chain Monte Carlo constructs a Markov chain</p> \\[ z_1 \\to z_2 \\to z_3 \\to \\cdots \\] <p>whose stationary distribution is \\(p(z)\\).</p> <p>After a burn-in period, samples approximate \\(p(z)\\) even if individual states are dependent.</p> <p>MCMC is widely applicable because it does not require the normalization constant of \\(p(z)\\):</p> \\[ p(z|x) \\propto p(x,z). \\]"},{"location":"informationtheory/6_mc/#41-metropolishastings-algorithm","title":"4.1 Metropolis\u2013Hastings Algorithm","text":"<p>The Metropolis\u2013Hastings (MH) algorithm constructs a Markov chain whose stationary distribution is the target distribution \\(p(z)\\), even when \\(p(z)\\) is known only up to a proportionality constant. This makes MH suitable for Bayesian inference, where the posterior is often available only in unnormalized form:</p> \\[p(z \\mid x) \\propto p(x, z)\\] <p>MH works by proposing a new point based on the current state and then accepting or rejecting it according to how well it aligns with the target distribution.</p> <p>Given a current sample \\(z\\), the algorithm proceeds as follows:</p> <ol> <li> <p>propose a new sample \\(z'\\) using a proposal distribution \\(z' \\sim q(z' \\mid z)\\).</p> </li> <li> <p>compute the acceptance probability       </p> </li> <li> <p>accept the proposal with probability \\(\\alpha(z,z')\\) otherwise remain at the current state If accepted, set \\(z_{t+1} = z'\\), otherwise keep \\(z_{t+1} = z\\).</p> </li> </ol> <p>This simple rule ensures that the Markov chain satisfies detailed balance and converges to the desired distribution \\(p(z)\\).</p> <p>Metropolis\u2013Hastings is flexible and works with virtually any distribution from which we can evaluate \\(p(z)\\) up to a constant. However, its efficiency depends strongly on the proposal distribution. If the proposal steps are too small, the chain performs a random walk and mixes slowly. If the steps are too large, most proposals are rejected. Choosing or adapting the proposal distribution is therefore crucial for performance, especially in high-dimensional settings.</p>"},{"location":"informationtheory/6_mc/#42-gibbs-sampling","title":"4.2 Gibbs Sampling","text":"<p>Gibbs sampling is a special case of MCMC designed for multivariate distributions where sampling from the full conditional distributions is easy. Instead of proposing a new state and accepting or rejecting it, Gibbs sampling updates one variable at a time by drawing directly from its exact conditional distribution.</p> <p>For a latent vector:</p> \\[z = (z_1, z_2, \\dots, z_d)\\] <p>a Gibbs update for coordinate \\(i\\) samples:</p> \\[ z_i \\sim p(z_i \\mid z_{-i}). \\] <p>where \\(z_{-i}\\) denotes all components except \\(z_i\\).</p> <p>By cycling through all coordinates repeatedly, the Markov chain eventually converges to the target joint distribution \\(p(z)\\).</p> <p>The key requirement is that each full conditional distribution</p> \\[p(z_i \\mid z_{-i})\\] <p>must be analytically tractable and easy to sample from. When this holds, Gibbs sampling is simple to implement and avoids the accept\u2013reject step of Metropolis\u2013Hastings.</p> <p>However, Gibbs sampling can mix slowly when variables are strongly correlated, since updating one coordinate at a time may explore the space inefficiently. Gibbs sampling is widely used in models where conditional distributions are naturally available, including:</p> <ul> <li>topic models such as Latent Dirichlet Allocation (LDA)</li> <li>hidden Markov models</li> <li>Gaussian graphical models</li> <li>Bayesian networks with conjugate priors</li> </ul>"},{"location":"informationtheory/6_mc/#43-slice-sampling","title":"4.3 Slice Sampling","text":"<p>Slice sampling is an MCMC method that avoids choosing a proposal distribution by sampling uniformly from the region (the slice) where the probability density is above a randomly chosen threshold. Given the current point \\(z\\), slice sampling introduces an auxiliary variable \\(u\\):</p> <ol> <li> <p>Draw a height</p> <p>\\(\\(u \\sim \\text{Uniform}(0,\\, p(z))\\)\\)</p> </li> <li> <p>Define the horizontal slice</p> <p>\\(\\(S = \\{ z' : p(z') &gt; u \\}\\)\\)</p> </li> <li> <p>Sample the next state</p> <p>\\(\\(z' \\sim \\text{Uniform}(S)\\)\\)</p> </li> </ol> <p>This procedure constructs a Markov chain whose stationary distribution is \\(p(z)\\). Intuitively, the algorithm first chooses a horizontal level \\(u\\) below the current density value and then samples uniformly from the region of the density that lies above this level.</p> <p>Slice sampling adapts naturally to the local geometry of the target distribution: narrow peaks produce narrow slices, and broad regions produce wide slices, without requiring manual tuning of proposal scales. In practice, slice sampling often mixes better than basic random-walk proposals, especially when the target density varies in amplitude across different regions.</p> <p>However, slice sampling requires an efficient way to identify or approximate the slice region, which may be challenging in high-dimensional or multimodal settings.</p>"},{"location":"informationtheory/6_mc/#5-reducing-random-walk-behaviour","title":"5. Reducing Random-Walk Behaviour","text":"<p>Basic MCMC algorithms such as Metropolis\u2013Hastings often move in small, local steps and therefore explore the state space slowly. This random-walk behaviour leads to poor mixing and long autocorrelation times, especially in high-dimensional or strongly correlated distributions.</p> <p>Several advanced MCMC techniques attempt to reduce random-walk dynamics by proposing more informed or distant moves.</p>"},{"location":"informationtheory/6_mc/#51-hamiltonian-monte-carlo-hmc","title":"5.1 Hamiltonian Monte Carlo (HMC)","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving smoothly through the probability landscape.</p> <p>Let \\(z\\) denote the position (the latent variable) and let \\(r\\) denote an auxiliary momentum variable. The joint density of \\((z, r)\\) is defined through a Hamiltonian:</p> \\[ H(z, r) = -\\log p(z) + \\frac{1}{2} r^\\top r. \\] <p>The first term acts like a potential energy, and the second acts like kinetic energy. The total Hamiltonian is approximately conserved under Hamiltonian dynamics governed by:</p> \\[ \\frac{dz}{dt} = r, \\qquad \\frac{dr}{dt} = \\nabla_z \\log p(z). \\] <p>Following these dynamics moves the system along continuous trajectories that remain mostly in high-probability regions, allowing the sampler to travel long distances without being rejected.</p>"},{"location":"informationtheory/6_mc/#leapfrog-integration-and-step-size","title":"Leapfrog Integration and Step Size","text":"<p>Exact Hamiltonian dynamics cannot be simulated analytically, so HMC uses a numerical integrator, typically the leapfrog method. Leapfrog integration updates position and momentum in small steps of size \\(\\epsilon\\):</p> <ol> <li> <p>half-step momentum update </p> </li> <li> <p>full-step position update </p> </li> <li> <p>half-step momentum update </p> </li> </ol> <p>These updates are repeated \\(L\\) times, producing a proposal \\((z', r')\\) after a simulated trajectory of length \\(L \\epsilon\\).</p> <p>The step size \\(\\epsilon\\) strongly influences performance:</p> <ul> <li>if \\(\\epsilon\\) is too large, numerical integration becomes inaccurate and proposals are rejected  </li> <li>if \\(\\epsilon\\) is too small, trajectories progress slowly and exploration becomes inefficient  </li> </ul> <p>Adaptive schemes such as dual averaging automatically tune \\(\\epsilon\\).</p>"},{"location":"informationtheory/6_mc/#acceptance-step","title":"Acceptance Step","text":"<p>Although leapfrog integration nearly preserves energy, numerical error accumulates. Therefore HMC applies a Metropolis acceptance step:</p> \\[ \\alpha = \\min\\left(1,\\, \\exp\\big(-H(z',r') + H(z,r)\\big) \\right). \\] <p>Because leapfrog integration is reversible and volume-preserving, acceptance rates remain high even for long trajectories.</p>"},{"location":"informationtheory/6_mc/#choosing-the-trajectory-length","title":"Choosing the Trajectory Length","text":"<p>The number of leapfrog steps \\(L\\) (or total integration time \\(L\\epsilon\\)) affects how far the sampler travels:</p> <ul> <li>small \\(L\\) results in short trajectories, similar to random-walk proposals  </li> <li>large \\(L\\) explores further but may waste computation or return near the starting point  </li> </ul> <p>The No-U-Turn Sampler (NUTS) automatically selects an appropriate integration length and forms the basis of modern HMC implementations such as Stan.</p>"},{"location":"informationtheory/6_mc/#summary-of-advantages","title":"Summary of Advantages","text":"<p>Hamiltonian Monte Carlo reduces random-walk behaviour by introducing an auxiliary momentum variable and using Hamiltonian dynamics to propose transitions. Instead of taking small, diffusive steps, HMC simulates the motion of a particle moving through the probability landscape.</p> <p>Hamiltonian Monte Carlo offers several advantages:</p> <ul> <li>efficient exploration in high-dimensional or correlated distributions  </li> <li>large, directed moves that avoid random-walk behaviour  </li> <li>low autocorrelation between samples  </li> <li>high acceptance rates due to approximate energy conservation  </li> <li>uses gradients of \\(\\log p(z)\\) to guide proposals  </li> </ul> <p>HMC is widely used in probabilistic programming frameworks such as Stan, PyMC, and NumPyro, largely because of its scalability and efficiency in challenging Bayesian inference problems.</p>"},{"location":"informationtheory/6_mc/#52-overrelaxation","title":"5.2 Overrelaxation","text":"<p>Overrelaxation modifies proposals so that successive samples are negatively correlated. Instead of randomly perturbing the current state, overrelaxation proposes a point on the opposite side of the conditional mean.</p> <p>Intuitively, if the current sample lies above the mean, the overrelaxation proposal nudges it below the mean, and vice versa. This helps the chain avoid local sticking and oscillation.</p> <p>Overrelaxation is most effective when the conditional distribution is approximately Gaussian or when the model exhibits strong linear structure.</p>"},{"location":"informationtheory/6_mc/#6-sensitivity-to-step-size","title":"6. Sensitivity to Step Size","text":"<p>The efficiency of MCMC algorithms depends critically on the choice of step size (or proposal scale):</p> <ul> <li>If the step size is too small, the chain takes tiny moves and mixes slowly.  </li> <li>If the step size is too large, most proposals are rejected.</li> </ul> <p>Finding an appropriate step size is essential for balancing exploration and acceptance.  </p> <p>For random-walk Metropolis\u2013Hastings:</p> <ul> <li>acceptance rates near 0.2\u20130.4 often work well in high dimensions  </li> <li>smaller dimensions tolerate larger acceptance rates</li> </ul> <p>For HMC, step size affects both the numerical integration quality and the acceptance probability. Too large a step size causes integration error and rejections; too small a step size results in slow exploration.</p> <p>Adaptive MCMC methods automatically tune the step size to achieve target acceptance rates.</p>"},{"location":"informationtheory/6_mc/#7-when-to-stop-convergence-and-diagnostics","title":"7. When to Stop: Convergence and Diagnostics","text":"<p>Running an MCMC chain forever is impossible, so practical inference requires diagnosing convergence.</p> <p>Several indicators are commonly used:</p>"},{"location":"informationtheory/6_mc/#71-burn-in","title":"7.1 Burn-in","text":"<p>The initial part of the chain (the burn-in period) may not represent the target distribution. These early samples are discarded until the chain reaches a stable region.</p>"},{"location":"informationtheory/6_mc/#72-autocorrelation","title":"7.2 Autocorrelation","text":"<p>High autocorrelation indicates slow mixing. Effective sample size (ESS) measures the number of independent samples equivalent to the correlated MCMC draws.</p>"},{"location":"informationtheory/6_mc/#73-multiple-chains","title":"7.3 Multiple chains","text":"<p>Running several independent chains allows comparison. If chains converge to the same region, the sampler is more likely to have reached equilibrium.</p>"},{"location":"informationtheory/6_mc/#74-gelmanrubin-statistic-r-hat","title":"7.4 Gelman\u2013Rubin statistic (R-hat)","text":"<p>R-hat compares within-chain and between-chain variance. Values close to 1 indicate convergence.</p>"},{"location":"informationtheory/6_mc/#75-visual-inspection","title":"7.5 Visual inspection","text":"<p>Trace plots, autocorrelation plots, and histograms provide qualitative insight into mixing and stability.</p> <p>There is no single perfect test, but combining multiple diagnostics provides reasonable confidence that the Markov chain has approximated the target distribution.</p> <p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p> <p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p> <p>Random-walk behaviour limits the efficiency of basic MCMC methods. Hamiltonian Monte Carlo reduces this by exploiting gradient information and simulating Hamiltonian dynamics, while overrelaxation introduces negative correlation to speed up mixing. Step size must be chosen carefully to balance exploration and acceptance. Convergence diagnostics such as burn-in, effective sample size, and R-hat help determine when to stop sampling and assess the quality of the generated samples.</p>"},{"location":"informationtheory/7a_vi_intro/","title":"8. Optimization-Based Inference","text":"<p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p> <p>This chapter introduces three optimization-based inference strategies:</p> <ol> <li>Maximum a posteriori (MAP) estimation  </li> <li>Expectation\u2013Maximization (EM)  </li> <li>Variational inference (VI), in its simplest introductory form  </li> </ol> <p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>"},{"location":"informationtheory/7a_vi_intro/#1-motivation-for-optimization-based-inference","title":"1. Motivation for Optimization-Based Inference","text":"<p>Bayesian inference requires the posterior</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The challenge lies in computing the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p> <ul> <li>high-dimensional latent spaces  </li> <li>multimodal posteriors  </li> <li>large datasets  </li> <li>models requiring gradient-based learning  </li> </ul> <p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>"},{"location":"informationtheory/7a_vi_intro/#2-maximum-a-posteriori-map-estimation","title":"2. Maximum A Posteriori (MAP) Estimation","text":"<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes\u2019 rule:</p> \\[ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}, \\] <p>MAP chooses the mode of the posterior:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta p(\\theta|x). \\] <p>Since \\(p(x)\\) does not depend on \\(\\theta\\), this is equivalent to:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta \\big[ \\log p(x|\\theta) + \\log p(\\theta) \\big]. \\] <p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through \\(p(\\theta)\\). However, it returns only a point estimate and does not capture uncertainty.</p> <p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>"},{"location":"informationtheory/7a_vi_intro/#3-expectationmaximization-em","title":"3. Expectation\u2013Maximization (EM)","text":"<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p> \\[ \\log p_\\theta(x)  = \\log \\sum_z p_\\theta(x,z). \\] <p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>"},{"location":"informationtheory/7a_vi_intro/#e-step","title":"E-step","text":"<p>Compute the posterior over latent variables under the current parameters:</p> \\[ q(z) = p_\\theta(z|x). \\]"},{"location":"informationtheory/7a_vi_intro/#m-step","title":"M-step","text":"<p>Maximize the expected complete-data log-likelihood:</p> \\[ \\theta \\leftarrow  \\arg\\max_\\theta  \\mathbb{E}_{q(z)}[\\log p_\\theta(x,z)]. \\] <p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p> <ul> <li>mixture of Gaussians  </li> <li>hidden Markov models  </li> <li>probabilistic PCA  </li> <li>clustering and density estimation  </li> </ul> <p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior \\(q(z) = p_\\theta(z|x)\\).</p>"},{"location":"informationtheory/7a_vi_intro/#4-em-and-map-map-em","title":"4. EM and MAP: MAP-EM","text":"<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p> \\[ \\theta_{\\text{MAP}}  =  \\arg\\max_\\theta  \\left[ \\mathbb{E}_{p(z|x,\\theta)}[\\log p(x,z|\\theta)] + \\log p(\\theta) \\right]. \\] <p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>"},{"location":"informationtheory/7a_vi_intro/#5-limitations-of-map-and-em","title":"5. Limitations of MAP and EM","text":"<p>Both MAP and EM have limitations that motivate more general methods:</p> <ol> <li>MAP returns only a point estimate and discards posterior uncertainty.  </li> <li>EM requires exact posterior computation in the E-step:        which is often intractable.  </li> <li>EM struggles with:</li> <li>multimodal posteriors  </li> <li>high-dimensional latent spaces  </li> <li>arbitrary likelihood forms  </li> </ol> <p>These limitations lead naturally to variational inference.</p>"},{"location":"informationtheory/7a_vi_intro/#6-a-brief-introduction-to-variational-inference-vi","title":"6. A Brief Introduction to Variational Inference (VI)","text":"<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p> \\[ q(z) = p_\\theta(z|x), \\] <p>VI chooses a family of distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q} \\] <p>and optimizes it to be close to the true posterior. The objective is:</p> \\[ \\phi^* =  \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p> <p>VI:</p> <ul> <li>generalizes MAP (when \\(q\\) is a delta function)  </li> <li>generalizes EM (when \\(q = p_\\theta(z|x)\\))  </li> <li>supports flexible approximations  </li> <li>scales to large datasets  </li> <li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li> </ul> <p>The next chapter explores variational inference in detail.</p> <p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p> <p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p> <p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p>"},{"location":"informationtheory/7b_vi/","title":"7. Variatonal Inference","text":"<p>Variational inference (VI) provides a general framework for approximating difficult probability distributions with simpler, tractable ones. Many modern machine-learning models rely on VI, including variational autoencoders (VAEs), Bayesian neural networks, latent-variable models, and diffusion models. VI offers a scalable alternative to sampling-based inference and converts the inference problem into an optimization problem.</p>"},{"location":"informationtheory/7b_vi/#1-the-problem-of-inference","title":"1. The Problem of Inference","text":"<p>Many probabilistic models introduce hidden variables to explain observations. Examples include:</p> <ul> <li>latent variables \\(z\\) in VAEs  </li> <li>weight distributions in Bayesian neural networks  </li> <li>cluster indicators in mixture models  </li> <li>hidden states in topic models and HMMs  </li> </ul> <p>The goal is to compute the posterior distribution</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The difficulty lies in the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is often intractable in high-dimensional or complex models. Exact Bayesian inference becomes impossible, which motivates approximate methods. Variational inference addresses this challenge.</p>"},{"location":"informationtheory/7b_vi/#2-the-idea-of-variational-inference","title":"2. The Idea of Variational Inference","text":"<p>Variational inference replaces the intractable posterior with a tractable approximation. Instead of trying to compute \\(p(z|x)\\) exactly, VI introduces a family of simpler distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q}, \\] <p>and chooses the member that is closest to the true posterior. Closeness is measured using the KL divergence:</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>The goal is:</p> \\[ \\phi^* = \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>However, the KL depends on \\(p(z|x)\\), which is unknown, making direct minimization impossible. The key insight is that the KL can be rewritten in terms of computable quantities, leading to the Evidence Lower Bound (ELBO).</p>"},{"location":"informationtheory/7b_vi/#3-deriving-the-elbo","title":"3. Deriving the ELBO","text":"<p>We start from the marginal likelihood:</p> \\[ \\log p(x) = \\log \\int p(x,z)\\,dz. \\] <p>We multiply and divide by \\(q_\\phi(z|x)\\):</p> \\[ \\log p(x) = \\log \\int q_\\phi(z|x)\\, \\frac{p(x,z)}{q_\\phi(z|x)}\\,dz. \\] <p>Applying Jensen\u2019s inequality yields:</p> \\[ \\log p(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p(x,z)}{q_\\phi(z|x)} \\right]. \\] <p>This expression is the ELBO:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x,z)] - \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x)]. \\] <p>A useful identity reveals:</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Since KL divergence is non-negative:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) \\le \\log p(x). \\] <p>Maximizing the ELBO is equivalent to minimizing the KL divergence between \\(q_\\phi(z|x)\\) and the true posterior.</p>"},{"location":"informationtheory/7b_vi/#4-interpreting-the-elbo","title":"4. Interpreting the ELBO","text":"<p>The ELBO can be decomposed into two terms that have clear interpretations. Writing</p> \\[ p(x,z) = p_\\theta(x|z)p(z), \\] <p>and substituting into the ELBO gives:</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\]"},{"location":"informationtheory/7b_vi/#reconstruction-term","title":"Reconstruction term","text":"\\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]. \\] <p>This term ensures that \\(z\\) captures enough information to generate or reconstruct the observed data. It corresponds to likelihood or reconstruction accuracy.</p>"},{"location":"informationtheory/7b_vi/#regularization-term","title":"Regularization term","text":"\\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>This term ensures that the posterior approximation does not drift too far from the prior. In VAEs, the prior is usually a Gaussian, making the latent space smooth and structured.</p> <p>The ELBO therefore expresses a balance:</p> <ul> <li>the first term rewards informative latent variables  </li> <li>the second term penalizes overly complex or irregular latent distributions  </li> </ul>"},{"location":"informationtheory/7b_vi/#5-why-vi-uses-reverse-kl","title":"5. Why VI Uses Reverse KL","text":"<p>Variational inference minimizes</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)), \\] <p>which is reverse KL. Reverse KL has important behavioral properties:</p> <ul> <li>it heavily penalizes assigning probability mass where the true posterior is low  </li> <li>it allows \\(q\\) to ignore some modes of \\(p(z|x)\\) </li> <li>it prefers tight, conservative approximations  </li> </ul> <p>As a result:</p> <ul> <li>VI tends to be mode seeking  </li> <li>it focuses on a single high-density region  </li> <li>it can miss multimodal structure of the true posterior  </li> </ul> <p>This behavior explains why VAEs sometimes produce smooth or blurry samples: the latent space favors safe, central modes.</p>"},{"location":"informationtheory/7b_vi/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":"<p>A VAE applies variational inference to a deep latent-variable model. It introduces:</p> <ol> <li>a latent prior </li> <li>a decoder (generative model) </li> <li>an encoder (variational posterior) </li> </ol> <p>The encoder and decoder are neural networks, trained jointly by maximizing the ELBO over all data points.</p>"},{"location":"informationtheory/7b_vi/#61-generative-model","title":"6.1 Generative model","text":"<p>Given \\(z\\) sampled from the prior, the decoder produces a distribution over possible \\(x\\):</p> \\[ p_\\theta(x|z). \\]"},{"location":"informationtheory/7b_vi/#62-inference-model","title":"6.2 Inference model","text":"<p>The encoder produces the parameters of the approximate posterior:</p> \\[ q_\\phi(z|x) = \\mathcal{N}(z\\mid \\mu_\\phi(x), \\sigma^2_\\phi(x)). \\] <p>This is the distribution used inside the ELBO.</p>"},{"location":"informationtheory/7b_vi/#63-vae-training-objective","title":"6.3 VAE training objective","text":"<p>The objective for each data point is:</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>The first term encourages correct reconstruction; the second keeps latent codes regularized.</p>"},{"location":"informationtheory/7b_vi/#7-the-reparameterization-trick","title":"7. The Reparameterization Trick","text":"<p>The expectation in the ELBO involves sampling from \\(q_\\phi(z|x)\\). To differentiate through this sampling step, VAEs use the reparameterization:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\odot\\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0,I). \\] <p>This expresses sampling as a deterministic transformation of noise, allowing gradients to flow through the encoder.</p> <p>This trick is central to making VI scalable and efficient in deep learning.</p>"},{"location":"informationtheory/7b_vi/#8-consequences-of-reverse-kl-in-vaes","title":"8. Consequences of Reverse KL in VAEs","text":"<p>The reverse KL term shapes the behavior of the VAE:</p> <ul> <li>it encourages smooth, overlapping latent regions  </li> <li>it prefers safe latent representations  </li> <li>it explains why VAEs sometimes produce blurry or conservative samples  </li> <li>it stabilizes training  </li> <li>it produces well-structured latent spaces  </li> </ul> <p>Extensions such as the \\(\\beta\\)-VAE, hierarchical VAEs, and flows inside the encoder allow for more expressive or disentangled representations.</p>"},{"location":"informationtheory/7b_vi/#9-variational-inference-beyond-vaes","title":"9. Variational Inference Beyond VAEs","text":"<p>VI provides a general-purpose framework for approximate inference in many settings.</p>"},{"location":"informationtheory/7b_vi/#bayesian-neural-networks","title":"Bayesian neural networks","text":"<p>Posterior distributions over weights are approximated by variational distributions:</p> \\[ q(w)\\approx p(w|D). \\]"},{"location":"informationtheory/7b_vi/#diffusion-models","title":"Diffusion models","text":"<p>The training objective resembles a variational bound on the data likelihood, using KL divergences between transition kernels.</p>"},{"location":"informationtheory/7b_vi/#normalizing-flows-for-vi","title":"Normalizing flows for VI","text":"<p>Flows can produce more expressive variational posteriors than simple Gaussians.</p>"},{"location":"informationtheory/7b_vi/#reinforcement-learning","title":"Reinforcement learning","text":"<p>Entropy-regularized RL and soft Q-learning can be interpreted through variational principles.</p> <p>VI therefore offers a unifying viewpoint across deep generative models, Bayesian inference, and probabilistic deep learning.</p> <p>Variational inference replaces an intractable posterior distribution with a tractable approximation and optimizes this approximation by maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, capturing the trade-off between accuracy and complexity. VAEs are an important application of VI, using neural networks to parameterize both the generative model and the approximate posterior. Reverse KL explains the conservative behavior of VI-based models. Variational inference provides a flexible approach for approximate Bayesian inference and underlies many modern generative and representation-learning techniques.</p>"},{"location":"informationtheory/8_reperesentation/","title":"8 reperesentation","text":""},{"location":"informationtheory/8_reperesentation/#chapter-5-representation-learning-mutual-information-and-the-information-bottleneck","title":"Chapter 5 \u2014 Representation Learning, Mutual Information, and the Information Bottleneck","text":"<p>Representation learning seeks transformations of data that make tasks such as prediction, compression, and reasoning easier. A representation \\(Z\\) is typically obtained by applying an encoder to an input \\(X\\). Information theory provides a natural way to formalize what makes a representation useful by analyzing the mutual information between \\(Z\\), the input \\(X\\), and the target \\(Y\\).</p> <p>This chapter introduces mutual information as a measure of shared structure, explains the Information Bottleneck framework, and connects these ideas to deep learning methods such as contrastive learning, VAEs, and self-supervised learning.</p>"},{"location":"informationtheory/8_reperesentation/#1-what-is-a-representation","title":"1. What Is a Representation?","text":"<p>A representation is a transformed form of input data:</p> \\[ Z = f_\\theta(X), \\] <p>where \\(f_\\theta\\) is usually a neural network. A good representation should satisfy two goals:</p> <ol> <li>It should retain information that is relevant for predicting \\(Y\\).  </li> <li>It should discard noise or irrelevant aspects of \\(X\\).</li> </ol> <p>Information theory allows us to express these goals using mutual information.</p>"},{"location":"informationtheory/8_reperesentation/#2-mutual-information-connecting-two-variables","title":"2. Mutual Information: Connecting Two Variables","text":"<p>Mutual information (MI) measures how much knowledge of one variable reduces uncertainty about another:</p> \\[ I(X;Y) = H(X) - H(X|Y). \\] <p>It can also be written as a KL divergence:</p> \\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)). \\] <p>MI is zero when \\(X\\) and \\(Y\\) are independent and increases as \\(Y\\) becomes more predictable from \\(X\\).</p> <p>In representation learning, we are often interested in the two quantities:</p> \\[ I(Z;Y), \\qquad I(Z;X). \\] <p>These measure how informative the representation \\(Z\\) is regarding the target \\(Y\\) and how much irrelevant detail from \\(X\\) is still present.</p>"},{"location":"informationtheory/8_reperesentation/#3-the-role-of-mi-in-representation-learning","title":"3. The Role of MI in Representation Learning","text":"<p>A representation \\(Z\\) is desirable when:</p> <ul> <li> <p>\\(I(Z;Y)\\) is large   (the representation captures features relevant to prediction)</p> </li> <li> <p>\\(I(Z;X)\\) is small   (the representation removes noise and redundancy)</p> </li> </ul> <p>This idea appears in supervised learning, contrastive methods, and generative modeling.</p> <p>Some examples:</p> <ul> <li>In supervised learning, we want features that preserve label information.  </li> <li>In contrastive learning, we want features that preserve the information common across augmented views.  </li> <li>In generative models, latent variables should retain structure that explains the data while avoiding unnecessary detail.</li> </ul>"},{"location":"informationtheory/8_reperesentation/#4-the-information-bottleneck-principle","title":"4. The Information Bottleneck Principle","text":"<p>The Information Bottleneck (IB) formalizes the trade-off between informativeness and compression. The goal is:</p> \\[ \\max I(Z;Y) \\quad \\text{s.t.} \\quad  I(Z;X) \\text{ is small}. \\] <p>This can be written as the Lagrangian:</p> \\[ \\mathcal{L}_{\\text{IB}} = I(Z;Y) - \\beta I(Z;X). \\] <p>The parameter \\(\\beta\\) controls how aggressively the representation is compressed.</p> <ul> <li>Large \\(\\beta\\) leads to simpler, more compressed representations.  </li> <li>Small \\(\\beta\\) allows more expressive, detailed representations.</li> </ul> <p>IB provides a theoretical explanation for the behavior of learned features in deep neural networks.</p>"},{"location":"informationtheory/8_reperesentation/#5-deep-learning-and-the-information-bottleneck","title":"5. Deep Learning and the Information Bottleneck","text":"<p>IB theory suggests several statements about deep networks:</p> <ol> <li>Early layers preserve much of the information in \\(X\\).  </li> <li>Later layers tend to compress \\(X\\) while emphasizing information predictive of \\(Y\\).  </li> <li>Networks may first memorize and later compress during training.  </li> <li>Generalization is linked to discarding unnecessary information.</li> </ol> <p>Although the exact dynamics remain debated, the overall perspective helps interpret the evolution of features during training.</p>"},{"location":"informationtheory/8_reperesentation/#6-variational-information-bottleneck-vib","title":"6. Variational Information Bottleneck (VIB)","text":"<p>Mutual information terms are often difficult to compute directly. The Variational Information Bottleneck approximates them using variational distributions.</p> <p>We treat the representation as a random variable drawn from \\(q(z|x)\\) and estimate MI with tractable terms. The VIB objective is:</p> \\[ \\mathcal{L}_{\\text{VIB}} = \\mathbb{E}_{p(x,y)}\\! \\left[ \\mathbb{E}_{q(z|x)}\\![\\log p(y|z)] \\right] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>This resembles the VAE objective, but \\(p(y|z)\\) replaces the reconstruction term. The first term encourages predictive features, while the KL term compresses the representation.</p> <p>VIB therefore provides a practical implementation of the Information Bottleneck.</p>"},{"location":"informationtheory/8_reperesentation/#7-mutual-information-and-contrastive-learning","title":"7. Mutual Information and Contrastive Learning","text":"<p>Contrastive learning uses mutual information to learn representations without labels. The idea is:</p> <ul> <li>Generate two augmented views of the same input: \\((x_1, x_2)\\).  </li> <li>Encode them as \\((z_1, z_2)\\).  </li> <li>Encourage \\(z_1\\) and \\(z_2\\) to be similar.  </li> <li>Encourage representations of different inputs to be dissimilar.</li> </ul> <p>This encourages \\(Z\\) to retain the information that is preserved under augmentation, while ignoring irrelevant aspects of the input.</p> <p>Many methods follow this structure:</p> <ul> <li>SimCLR  </li> <li>MoCo  </li> <li>BYOL  </li> <li>InfoNCE  </li> <li>CPC  </li> <li>Deep InfoMax  </li> </ul> <p>The InfoNCE objective is:</p> \\[ \\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}\\left[ \\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)} {\\sum_k \\exp(\\text{sim}(z_i,z_k)/\\tau)} \\right], \\] <p>where \\((i,j)\\) is a positive pair. InfoNCE is a variational lower bound on \\(I(Z_1;Z_2)\\).</p>"},{"location":"informationtheory/8_reperesentation/#8-mi-in-generative-modeling","title":"8. MI in Generative Modeling","text":"<p>Mutual information also appears in generative models:</p>"},{"location":"informationtheory/8_reperesentation/#81-vaes","title":"8.1 VAEs","text":"<p>The KL term controls the structure and redundancy of \\(Z\\), and the decoder ensures \\(I(Z;X)\\) stays large enough for accurate reconstruction.</p>"},{"location":"informationtheory/8_reperesentation/#82-infogan","title":"8.2 InfoGAN","text":"<p>This model maximizes:</p> \\[ I(c; G(z,c)), \\] <p>encouraging the generator to learn interpretable latent factors.</p>"},{"location":"informationtheory/8_reperesentation/#83-normalizing-flows","title":"8.3 Normalizing flows","text":"<p>Flows maintain \\(I(X;Z) = H(X)\\) because they are invertible; they do not compress the input.</p>"},{"location":"informationtheory/8_reperesentation/#84-diffusion-models","title":"8.4 Diffusion models","text":"<p>Diffusion models gradually reduce noise and can be interpreted using information-theoretic ideas related to KL divergence and score matching.</p>"},{"location":"informationtheory/8_reperesentation/#9-mi-and-disentanglement","title":"9. MI and Disentanglement","text":"<p>Disentangled representations aim to separate independent generative factors such as orientation or color. The \\(\\beta\\)-VAE objective:</p> \\[ \\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)) \\] <p>encourages disentanglement by increasing compression in the latent space. A larger \\(\\beta\\) pushes different dimensions of \\(Z\\) to encode more independent aspects of the data.</p>"},{"location":"informationtheory/8_reperesentation/#10-estimating-mi-in-high-dimensions","title":"10. Estimating MI in High Dimensions","text":"<p>Mutual information is difficult to compute exactly in high dimensions. Neural estimation relies on variational bounds such as:</p> <ul> <li>InfoNCE  </li> <li>NWJ bound  </li> <li>Donsker\u2013Varadhan bound  </li> <li>MINE estimator  </li> <li>f-divergence lower bounds  </li> </ul> <p>These allow MI to be used in representation learning even when the true quantities are intractable.</p>"},{"location":"informationtheory/8_reperesentation/#11-summary-of-chapter-5","title":"11. Summary of Chapter 5","text":"<p>Mutual information provides a principled measure of what makes a useful representation: it should retain information relevant for prediction and discard irrelevant detail. The Information Bottleneck formalizes this trade-off and motivates practical methods such as the Variational Information Bottleneck.</p> <p>Contrastive learning methods maximize MI between augmented views, enabling self-supervised representation learning. Generative models such as VAEs, GANs, flows, and diffusion models each manipulate mutual information in different ways, leading to distinct behaviors and capabilities.</p> <p>Information theory therefore provides a unified lens through which to understand representation learning in modern deep networks.</p>"},{"location":"informationtheory/intro/","title":"Intro","text":"<p>https://www.inference.org.uk/itprnn_lectures/</p> <p>https://www.youtube.com/watch?v=sN_0iGWcyLI&amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&amp;index=12</p> <p>60mins Lect 12</p> <p>Application in  1. GANS</p>"},{"location":"informationtheory/intro/#game-theory","title":"Game theory","text":"<ul> <li>zero sum?</li> <li>min max V(D,G)</li> <li>nash equiblria</li> </ul> <p>Application in GAN</p>"},{"location":"llms/chahllanges/","title":"Chahllanges","text":"<p>Data Challenges: This pertains to the data used for training and how the model addresses gaps or missing data. Ethical Challenges: This involves addressing issues such as mitigating biases, ensuring privacy, and preventing the generation of harmful content in the deployment of LLMs. Technical Challenges: These challenges focus on the practical implementation of LLMs. Deployment Challenges: Concerned with the specific processes involved in transitioning fully-functional LLMs into real-world use-cases (productionization) Data Challenges:</p> <p>Data Bias: The presence of prejudices and imbalances in the training data leading to biased model outputs. Limited World Knowledge and Hallucination: LLMs may lack comprehensive understanding of real-world events and information and tend to hallucinate information. Note that training them on new data is a long and expensive process. Dependency on Training Data Quality: LLM performance is heavily influenced by the quality and representativeness of the training data. Ethical and Social Challenges:</p> <p>Ethical Concerns: Concerns regarding the responsible and ethical use of language models, especially in sensitive contexts. Bias Amplification: Biases present in the training data may be exacerbated, resulting in unfair or discriminatory outputs. Legal and Copyright Issues: Potential legal complications arising from generated content that infringes copyrights or violates laws. User Privacy Concerns: Risks associated with generating text based on user inputs, especially when dealing with private or sensitive information. Technical Challenges:</p> <p>Computational Resources: Significant computing power required for training and deploying large language models. Interpretability: Challenges in understanding and explaining the decision-making process of complex models. Evaluation: Evaluation presents a notable challenge as assessing models across diverse tasks and domains is inadequately designed, particularly due to the challenges posed by freely generated content. Fine-tuning Challenges: Difficulties in adapting pre-trained models to specific tasks or domains. Contextual Understanding: LLMs may face challenges in maintaining coherent context over longer passages or conversations. Robustness to Adversarial Attacks: Vulnerability to intentional manipulations of input data leading to incorrect outputs. Long-Term Context: Struggles in maintaining context and coherence over extended pieces of text or discussions. Deployment Challenges:</p> <p>Scalability: Ensuring that the model can scale efficiently to handle increased workloads and demand in production environments. Latency: Minimizing the response time or latency of the model to provide quick and efficient interactions, especially in real-time applications. Monitoring and Maintenance: Implementing robust monitoring systems to track model performance, detect issues, and perform regular maintenance to avoid downtime. Integration with Existing Systems: Ensuring smooth integration of LLMs with existing software, databases, and infrastructure within an organization. Cost Management: Optimizing the cost of deploying and maintaining large language models, as they can be resource-intensive in terms of both computation and storage. Security Concerns: Addressing potential security vulnerabilities and risks associated with deploying language models in production, including safeguarding against malicious attacks. Interoperability: Ensuring compatibility with other tools, frameworks, or systems that may be part of the overall production pipeline. User Feedback Incorporation: Developing mechanisms to incorporate user feedback to continuously improve and update the model in a production environment. Regulatory Compliance: Adhering to regulatory requirements and compliance standards, especially in industries with strict data protection and privacy regulations. Dynamic Content Handling: Managing the generation of text in dynamic environments where content and user interactions change frequently.</p> <p>Types of Domain Adaptation Methods There are several methods to incorporate domain-specific knowledge into LLMs, each with its own advantages and limitations. Here are three classes of approaches:</p> <p>Domain-Specific Pre-Training:</p> <p>Training Duration: Days to weeks to months Summary: Requires a large amount of domain training data; can customize model architecture, size, tokenizer, etc. In this method, LLMs are pre-trained on extensive datasets representing various natural language use cases. For instance, models like PaLM 540B, GPT-3, and LLaMA 2 have been pre-trained on datasets with sizes ranging from 499 billion to 2 trillion tokens. Examples of domain-specific pre-training include models like ESMFold, ProGen2 for protein sequences, Galactica for science, BloombergGPT for finance, and StarCoder for code. These models outperform generalist models within their domains but still face limitations in terms of accuracy and potential hallucinations.</p> <p>Domain-Specific Fine-Tuning:</p> <p>Training Duration: Minutes to hours Summary: Adds domain-specific data; tunes for specific tasks; updates LLM model Fine-tuning involves training a pre-trained LLM on a specific task or domain, adapting its knowledge to a narrower context. Examples include Alpaca (fine-tuned LLaMA-7B model for general tasks), xFinance (fine-tuned LLaMA-13B model for financial-specific tasks), and ChatDoctor (fine-tuned LLaMA-7B model for medical chat). The costs for fine-tuning are significantly smaller compared to pre-training.</p> <p>Retrieval Augmented Generation (RAG):</p> <p>Training Duration: Not required Summary: No model weights; external information retrieval system can be tuned RAG involves grounding the LLM's parametric knowledge with external or non-parametric knowledge from an information retrieval system. This external knowledge is provided as additional context in the prompt to the LLM. The advantages of RAG include no training costs, low expertise requirement, and the ability to cite sources for human verification. This approach addresses limitations such as hallucinations and allows for precise manipulation of knowledge. The knowledge base is easily updatable without changing the LLM. Strategies to combine non-parametric knowledge with an LLM's parametric knowledge are actively researched.</p> <p>Use Domain-Specific Pre-Training When: Exclusive Domain Focus: Pre-training is suitable when you require a model exclusively trained on data from a specific domain, creating a specialized language model for that domain. Customizing Model Architecture: It allows you to customize various aspects of the model architecture, size, tokenizer, etc., based on the specific requirements of the domain. Extensive Training Data Available: Effective pre-training often requires a large amount of domain-specific training data to ensure the model captures the intricacies of the chosen domain. Use Domain-Specific Fine-Tuning When: Specialization Needed: Fine-tuning is suitable when you already have a pre-trained LLM, and you want to adapt it for specific tasks or within a particular domain. Task Optimization: It allows you to adjust the model's parameters related to the task, such as architecture, size, or tokenizer, for optimal performance in the chosen domain. Time and Resource Efficiency: Fine-tuning saves time and computational resources compared to training a model from scratch since it leverages the knowledge gained during the pre-training phase. Use RAG When: Information Freshness Matters: RAG provides up-to-date, context-specific data from external sources. Reducing Hallucination is Crucial: Ground LLMs with verifiable facts and citations from an external knowledge base. Cost-Efficiency is a Priority: Avoid extensive model training or fine-tuning; implement without the need for training.</p>"},{"location":"llms/foundations/","title":"Foundations","text":""},{"location":"monetcarlo_simulations/plan/","title":"Plan","text":"<ol> <li>Markov Chain</li> </ol>"},{"location":"nn_training/1_intro/","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#understanding-autograd-the-engine-behind-deep-learning-with-a-micrograd-style-walkthrough","title":"Understanding Autograd: The Engine Behind Deep Learning (with a micrograd-style walkthrough)","text":""},{"location":"nn_training/1_intro/#what-is-autograd","title":"What is Autograd?","text":"<p>Autograd \u2014 short for automatic differentiation \u2014 is a computational technique that automatically computes derivatives of functions expressed as computer programs. It is the mathematical and computational backbone of deep learning frameworks like PyTorch, TensorFlow, and JAX.</p> <p>At its core, autograd implements reverse-mode automatic differentiation, an algorithm that efficiently computes gradients of a scalar output (such as a loss) with respect to many input parameters (model weights).</p>"},{"location":"nn_training/1_intro/#how-it-works","title":"How It Works","text":"<p>When a function is executed, autograd records all elementary operations (addition, multiplication, non-linearities, etc.) in a computational graph. Each node represents a tensor or scalar value, and each edge represents an operation with a known local derivative.</p> <p>During the forward pass, the graph is constructed dynamically. During the backward pass, the engine traverses the graph in reverse order, applying the chain rule to compute gradients:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy}\\cdot\\frac{dy}{dx}. \\] <p>This process is often referred to as back-propagation. In practice, the framework automatically handles these derivative computations.</p> <p>For example, in Andrej Karpathy\u2019s micrograd, a minimal autograd engine, each scalar <code>Value</code> object keeps track of both its data and gradient, as well as the operation that produced it. The <code>.backward()</code> method propagates gradients backward through the graph, applying local chain rules for each operation.</p>"},{"location":"nn_training/1_intro/#differentiation-methods-overview","title":"Differentiation Methods Overview","text":"Method Description Pros Cons Numerical Finite difference approximation Simple Inaccurate, slow Symbolic Algebraic manipulation (e.g., SymPy) Exact Symbol explosion, not scalable Automatic (AD) Local derivatives + chain rule Exact, efficient Requires graph bookkeeping <p>Unlike numerical differentiation (which is approximate) or symbolic differentiation (which manipulates expressions), autograd computes exact derivatives efficiently by chaining local gradients.</p>"},{"location":"nn_training/1_intro/#why-use-autograd","title":"Why Use Autograd?","text":"<ol> <li> <p>Eliminates Manual Derivative Computation    Without autograd, practitioners would need to derive and code gradients manually for each model parameter. This is not only tedious but error-prone, especially for complex architectures.</p> </li> <li> <p>Ensures Correctness and Reliability    By systematically applying the chain rule, autograd frameworks guarantee correct gradient flow through even the most intricate models, reducing human error.</p> </li> <li> <p>Supports Dynamic and Flexible Graphs    Modern frameworks like PyTorch and micrograd construct computation graphs dynamically \u2014 rebuilding them on each forward pass. This allows for loops, conditionals, and recursion within model definitions.</p> </li> <li> <p>Caches Intermediate Results    Autograd stores intermediate activations during the forward pass so they can be reused efficiently during the backward pass. This improves computational speed but increases memory usage.</p> </li> <li> <p>Higher-Order Derivatives    Since the backward pass itself is differentiable, autograd can compute higher-order derivatives \u2014 useful in meta-learning, optimization research, and differentiable physics.</p> </li> <li> <p>Performance and Hardware Optimization    Frameworks optimize backward passes using techniques like operation fusion and kernel caching, ensuring gradient computations remain efficient on GPUs and TPUs.</p> </li> </ol> <p>A minimal implementation like micrograd reveals these mechanics transparently, allowing students and researchers to understand what happens under the hood of massive frameworks.</p>"},{"location":"nn_training/1_intro/#importance-in-deep-learning","title":"Importance in Deep Learning","text":""},{"location":"nn_training/1_intro/#1-the-foundation-of-backpropagation","title":"1. The Foundation of Backpropagation","text":"<p>Training neural networks relies on minimizing a loss function \\(L(\\theta)\\) with respect to parameters \\(\\theta\\). The update rule for parameters (via gradient descent) is:</p> \\[ \\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}. \\] <p>Here, autograd automates the computation of \\(\\frac{\\partial L}{\\partial \\theta}\\) \u2014 the essential ingredient of learning.</p>"},{"location":"nn_training/1_intro/#2-enabling-complex-architectures","title":"2. Enabling Complex Architectures","text":"<p>Modern networks (e.g., Transformers, ResNets, GNNs) have deep stacks, skip connections, and nonlinear branches. Autograd ensures that gradients flow correctly through these complex graphs \u2014 enabling architectural innovation without requiring users to manually derive derivatives.</p>"},{"location":"nn_training/1_intro/#3-scalability-and-efficiency","title":"3. Scalability and Efficiency","text":"<p>Reverse-mode AD (autograd) is ideal for functions mapping many inputs to a single scalar output \u2014 exactly the case for deep learning. Its computational cost is roughly proportional to the cost of the forward pass, but with a higher memory footprint.</p> <p>Compute\u2013Memory Trade-off:</p> <ul> <li>Compute: The backward pass roughly doubles compute time.  </li> <li>Memory: Storing intermediate activations increases RAM/GPU usage.</li> </ul> <p>Frameworks mitigate this using gradient checkpointing, where certain intermediate activations are recomputed on-demand to save memory.</p>"},{"location":"nn_training/1_intro/#a-micrograd-style-value-class-with-line-by-line-commentary","title":"A micrograd-style <code>Value</code> class \u2014 with line-by-line commentary","text":"<p>Below is a faithful, lightly extended micrograd-style engine. Every key line is annotated to explain what it references and why it matters for autograd.</p> <pre><code>import math\n\nclass Value:\n    # ---------------------- Initialization ----------------------\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data                # (float) the scalar value of this node\n        self.grad = 0.0                 # (float) d(output)/d(this node), filled during backprop\n        self._backward = lambda: None   # a closure set by each op to push grad to parents\n        self._prev = set(_children)     # (set[Value]) parents (inputs) that produced this node\n        self._op = _op                  # (str) op name for graph/debug ('+','*','tanh','exp','k',...)\n        self.label = label              # (str) optional name for visualization\n\n    def __repr__(self):\n        # nice debug print to see the forward value\n        return f\"Value(data={self.data})\"\n\n    # ---------------------- Binary Ops: + ----------------------\n    def __add__(self, other):\n        # allow mixing with Python scalars: 2 + Value(3)\n        other = other if isinstance(other, Value) else Value(other)\n\n        # forward pass: create the child node 'out' from parents (self, other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # local partials for z = x + y are \u2202z/\u2202x = 1, \u2202z/\u2202y = 1\n            # chain rule: x.grad += 1 * out.grad; y.grad += 1 * out.grad\n            self.grad  += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward       # attach the gradient propagation rule to 'out'\n        return out\n\n    def __radd__(self, other):\n        # support Python's other + self\n        return self + other\n\n    # ---------------------- Binary Ops: * ----------------------\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # for z = x * y: \u2202z/\u2202x = y, \u2202z/\u2202y = x\n            self.grad  += other.data * out.grad\n            other.grad += self.data  * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        # support Python's other * self\n        return self * other\n\n    # ---------------------- Power, Neg, Sub, Div ----------------------\n    def __pow__(self, other):\n        # only scalar exponents for simplicity\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.dataother, (self,), f'{other}')\n\n        def _backward():\n            # for z = x^k: \u2202z/\u2202x = k * x^(k-1)\n            self.grad += other * (self.data  (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def __truediv__(self, other):  # self / other\n        # use x / y = x * y^{-1}\n        return self * (other  -1)\n\n    def __neg__(self):             # -self\n        # use -x = (-1) * x\n        return self * -1\n\n    def __sub__(self, other):      # self - other\n        # x - y = x + (-y)\n        return self + (-other)\n\n    # ---------------------- Nonlinearities ----------------------\n    def tanh(self):\n        # forward: compute t = tanh(x) (closed form used here; math.tanh is fine too)\n        x = self.data\n        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n\n        def _backward():\n            # derivative: d/dx tanh(x) = 1 - tanh(x)^2 = 1 - t^2\n            self.grad += (1 - t2) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        # forward: e^x\n        x = self.data\n        out = Value(math.exp(x), (self,), 'exp')\n\n        def _backward():\n            # derivative: d/dx e^x = e^x; note e^x is out.data\n            self.grad += out.data * out.grad\n        out._backward = _backward\n        return out\n\n    # ---------------------- Backprop Driver ----------------------\n    def backward(self):\n        # Build a topological ordering of the graph so every node's\n        # _backward() runs after all of its children have pushed grads.\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for parent in v._prev:   # traverse to parents (inputs)\n                    build_topo(parent)\n                topo.append(v)            # append after traversing parents\n\n        build_topo(self)\n\n        # seed the gradient at the output node: d(self)/d(self) = 1\n        self.grad = 1.0\n\n        # go in reverse topological order and apply each node's local chain rule\n        for node in reversed(topo):\n            node._backward()\n</code></pre>"},{"location":"nn_training/1_intro/#what-each-attributemethod-references","title":"What each attribute/method references","text":"<ul> <li><code>self.data</code>: the scalar numeric value stored at this node (forward pass result).  </li> <li><code>self.grad</code>: the accumulated derivative \\(\\frac{\\partial \\text{(final output)}}{\\partial \\text{this node}}\\) after <code>.backward()</code>.  </li> <li><code>self._prev</code>: a set of parent nodes (inputs) that produced <code>self</code>; used to traverse the graph.  </li> <li><code>self._op</code>: operation label for debugging/visualization.  </li> <li><code>self._backward</code>: a closure that knows how to push gradient from this node back to its parents using local partial derivatives.  </li> <li>Binary ops (<code>__add__</code>, <code>__mul__</code>, etc.): create a new child node <code>out</code> from parent nodes <code>(self, other)</code> and attach a <code>_backward</code> rule encoding the local Jacobian.  </li> <li><code>backward()</code>: performs a reverse topological traversal starting from the target scalar node, seeding its gradient with <code>1.0</code>, then calling every node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#worked-example-build-a-small-graph-and-differentiate","title":"Worked example: build a small graph and differentiate","text":"<p>We\u2019ll compute  and obtain gradients \\(\\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\frac{\\partial f}{\\partial c}\\).</p> <pre><code># create leaf nodes (parameters / inputs)\na = Value(2.0, label='a')\nb = Value(3.0, label='b')\nc = Value(0.5, label='c')\n\n# forward build\nd = a * b            # d = a*b\ne = c.tanh()         # e = tanh(c)\nf = d + e - 0.5*(a2)\n\n# backpropagate from scalar output 'f'\nf.backward()\n\nprint(\"f:\", f.data)\nprint(\"df/da:\", a.grad)\nprint(\"df/db:\", b.grad)\nprint(\"df/dc:\", c.grad)\n</code></pre>"},{"location":"nn_training/1_intro/#hand-derivative-sanity-check","title":"Hand-derivative sanity check","text":"<ul> <li>\\(d = a b \\Rightarrow \\frac{\\partial d}{\\partial a} = b,\\; \\frac{\\partial d}{\\partial b} = a\\) </li> <li>\\(e = \\tanh(c) \\Rightarrow \\frac{\\partial e}{\\partial c} = 1 - \\tanh^2(c)\\) </li> <li>\\(f = d + e - \\tfrac{1}{2} a^2\\)</li> </ul> <p>Therefore:  </p> <p>Your printed grads should match these values numerically (up to floating point).</p>"},{"location":"nn_training/1_intro/#how-methods-reference-values-and-variables-naming-clarity","title":"How methods reference values and variables (naming clarity)","text":"<ul> <li>In methods like <code>__add__</code> and <code>__mul__</code>, <code>self</code> is the left operand, <code>other</code> is the right operand (which we coerce to <code>Value</code> when it\u2019s a Python scalar).  </li> <li>The new node created by an operation is named <code>out</code>. It references:</li> <li><code>out.data</code>: the forward result of the op.  </li> <li><code>out._prev</code>: the set <code>{self, other}</code> \u2014 i.e., the parents that produced <code>out</code>.  </li> <li><code>out._backward</code>: a closure capturing <code>self</code>, <code>other</code>, and <code>out</code>, used to push gradient contributions back to <code>self.grad</code> and <code>other.grad</code> via local partials.</li> <li>During <code>.backward()</code>, we compute a topological order over the graph using <code>_prev</code> links (parents). We seed the target node\u2019s gradient with <code>1.0</code>, then walk in reverse order, calling each node\u2019s <code>_backward()</code> exactly once so that gradients accumulate correctly (<code>+=</code>, not <code>=</code>).</li> </ul>"},{"location":"nn_training/1_intro/#practical-notes-tips","title":"Practical notes &amp; tips","text":"<ul> <li>Zeroing grads: Before a new backward pass, set <code>.grad = 0.0</code> for all leaves to avoid mixing gradients across iterations, just like <code>optimizer.zero_grad()</code> in PyTorch.  </li> <li>Numerical stability: Prefer <code>math.tanh(x)</code> to the closed form for large \\(|x|\\).  </li> <li>Extensibility: New ops just need (1) a forward value, (2) parent tracking in <code>_prev</code>, and (3) a <code>_backward</code> closure with correct local derivatives.  </li> <li>Scalars vs. tensors: This toy engine is scalar-valued. Full frameworks generalize this to tensors, broadcasting rules, and highly optimized kernels.</li> </ul>"},{"location":"nn_training/1_intro/#references","title":"References","text":"<ul> <li>Karpathy, A. micrograd (minimal autograd engine).  </li> <li>PyTorch documentation: Autograd mechanics.  </li> <li>D2L.ai: Automatic differentiation.</li> </ul>"},{"location":"nn_training/2_initial/","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#practical-guide-to-weight-initialization-activation-distributions-dead-neurons-and-gradient-flow-in-deep-neural-networks","title":"Practical Guide to Weight Initialization, Activation Distributions, Dead Neurons, and Gradient Flow in Deep Neural Networks","text":""},{"location":"nn_training/2_initial/#1-why-weight-initialization-matters","title":"1. Why Weight Initialization Matters","text":"<p>Initialization determines:</p> <ul> <li>how activations propagate forward  </li> <li>how gradients propagate backward  </li> <li>whether neurons remain active  </li> <li>whether the optimizer can begin learning  </li> <li>whether training is stable or diverges  </li> </ul> <p>Poor initialization leads to:</p> <ul> <li>vanishing gradients  </li> <li>exploding gradients  </li> <li>saturated activations (tanh = \u00b11)  </li> <li>dead neurons (ReLU stuck at 0)  </li> <li>slow \u201chockey-stick\u201d learning curves  </li> <li>unstable early training  </li> </ul> <p>Modern deep learning succeeds because initializations are designed to maintain statistical stability across depth.</p>"},{"location":"nn_training/2_initial/#2-the-role-of-n-why-we-divide-by-sqrtn","title":"2. The Role of \\(N\\): Why We Divide by \\(\\sqrt{N}\\)","text":"<p>Consider a neuron:</p> \\[ z = \\sum_{i=1}^{N} w_i x_i \\] <p>If weights and inputs are independent and zero-mean:</p> \\[ \\mathrm{Var}(z) = N \\cdot \\mathrm{Var}(w) \\cdot \\mathrm{Var}(x) \\] <p>Thus:</p> <ul> <li>large \\(N\\) \u2192 exploded activations  </li> <li>small \\(N\\) \u2192 collapsed activations  </li> </ul> <p>To keep the variance stable:</p> \\[ \\mathrm{Var}(w) = \\frac{1}{N} \\quad \\Rightarrow \\quad \\text{std}(w) = \\frac{1}{\\sqrt{N}} \\] <p>This is the core idea behind all modern weight initializers.</p>"},{"location":"nn_training/2_initial/#what-exactly-is-n","title":"What exactly is \\(N\\)?","text":"<p>It depends on the layer:</p> <ul> <li>Dense layer: \\(N = \\text{fan\\_in}\\) </li> <li>Conv layer: \\(N = \\text{in\\_channels} \\times k_h \\times k_w\\) </li> <li>Transformer linear layer: \\(N = d_{\\text{model}}\\) or \\(d_{\\text{ff}}\\) </li> <li>RNN input/recurrent matrix: input or hidden size  </li> </ul> <p>The goal is always to stabilize forward and backward signal flow.</p>"},{"location":"nn_training/2_initial/#3-activation-functions-and-their-stability-regions","title":"3. Activation Functions and Their Stability Regions","text":""},{"location":"nn_training/2_initial/#tanh","title":"Tanh","text":"<ul> <li>useful only near 0  </li> <li>saturates at \u00b11 outside a small input range  </li> <li>derivative approaches 0 \u2192 vanishing gradients  </li> </ul>"},{"location":"nn_training/2_initial/#relu","title":"ReLU","text":"<p>  - linear for \\(z &gt; 0\\) - zero output + zero gradient for \\(z \\le 0\\) </p>"},{"location":"nn_training/2_initial/#gelu-silu-swish-softplus","title":"GELU / SiLU (Swish) / SoftPlus","text":"<ul> <li>smoother transitions  </li> <li>reduce probability of dead neurons  </li> <li>default in Transformers (GELU)</li> </ul> <p>Initialization must place activations in the high-gradient region of whichever activation is used.</p>"},{"location":"nn_training/2_initial/#4-how-tanh-saturation-happens","title":"4. How Tanh Saturation Happens","text":"<p>If pre-activations \\(z\\) have high variance, tanh outputs cluster near \u00b11:</p> <p>This is saturation.</p>"},{"location":"nn_training/2_initial/#why-its-bad","title":"Why it\u2019s bad:","text":"\\[ \\tanh'(z) = 1 - \\tanh^2(z) \\approx 0 \\] <p>Thus almost no gradient flows backward.</p> <p>Tanh neurons become functionally dead when always saturated.</p>"},{"location":"nn_training/2_initial/#5-dead-neurons-in-practice","title":"5. Dead Neurons in Practice","text":""},{"location":"nn_training/2_initial/#51-dead-tanh-neurons","title":"5.1 Dead Tanh Neurons","text":"<p>A tanh neuron is effectively dead if:</p> <ul> <li>\\(z\\) is always large positive or negative  </li> <li>output always \u00b11  </li> <li>derivative \u2248 0  </li> </ul> <p>Causes include:</p> <ul> <li>too-large initial weight variance  </li> <li>unnormalized inputs  </li> <li>deep networks without residuals or normalization  </li> </ul>"},{"location":"nn_training/2_initial/#52-dead-relu-neurons","title":"5.2 Dead ReLU Neurons","text":"<p>A ReLU neuron is dead if:</p> <ul> <li>\\(z \\le 0\\) for all inputs  </li> <li>output always 0  </li> <li>gradient always 0  </li> </ul> <p>Common causes:</p> <ul> <li>weights initialized too negative  </li> <li>bias drift  </li> <li>large learning rates  </li> <li>skewed input distributions  </li> <li>poor initial variance  </li> </ul> <p>Dead ReLUs can sometimes recover (with normalization), but often remain inactive.</p>"},{"location":"nn_training/2_initial/#6-classical-initialization-methods","title":"6. Classical Initialization Methods","text":""},{"location":"nn_training/2_initial/#61-xavier-glorot-tanh-sigmoid","title":"6.1 Xavier / Glorot (Tanh, Sigmoid)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}} \\] <p>Balances forward and backward variance.</p>"},{"location":"nn_training/2_initial/#62-he-kaiming-relu-gelu","title":"6.2 He / Kaiming (ReLU, GELU)","text":"\\[ \\mathrm{Var}(W) = \\frac{2}{\\text{fan}_{\\text{in}}} \\] <p>Based on the fact that ReLU zeroes about half of normally distributed inputs.</p>"},{"location":"nn_training/2_initial/#63-orthogonal-initialization-rnns","title":"6.3 Orthogonal Initialization (RNNs)","text":"<p>Stabilizes recurrent dynamics by preserving vector norms.</p>"},{"location":"nn_training/2_initial/#64-lsuv-initialization","title":"6.4 LSUV Initialization","text":"<p>Adjusts initial weights empirically to achieve unit variance layer-by-layer.</p>"},{"location":"nn_training/2_initial/#7-modern-initialization-for-deep-architectures","title":"7. Modern Initialization for Deep Architectures","text":"<p>Deep networks (especially Transformers and ResNets) require additional considerations.</p>"},{"location":"nn_training/2_initial/#71-layernorm-rmsnorm","title":"7.1 LayerNorm / RMSNorm","text":"<p>LayerNorm normalizes activations:</p> <ul> <li>keeps means near zero  </li> <li>standard deviation controlled  </li> <li>prevents drift or saturation  </li> <li>stabilizes attention layers and deep MLP stacks  </li> </ul> <p>Transformers rely heavily on LayerNorm to make training depth-independent.</p>"},{"location":"nn_training/2_initial/#72-residual-connections","title":"7.2 Residual Connections","text":"<p>Residual blocks:</p> \\[ x_{l+1} = x_l + f(x_l) \\] <p>This provides:</p> <ul> <li>an identity path for forward activations  </li> <li>a direct gradient path backward  </li> <li>stability for very deep networks  </li> <li>reduced sensitivity to initialization  </li> </ul> <p>Residuals make it possible to train 50\u20131000+ layer networks.</p>"},{"location":"nn_training/2_initial/#73-transformer-specific-initialization","title":"7.3 Transformer-Specific Initialization","text":"<p>Transformers often use:</p> <ul> <li>weight variance similar to He init  </li> <li>embedding scaling by \\(1/\\sqrt{d_{\\text{model}}}\\) </li> <li>residual scaling by \\(1/\\sqrt{L}\\) or similar  </li> <li>pre-LayerNorm to stabilize depth  </li> <li>\u03bc-parameterization for width-scaling consistency  </li> <li>DeepNorm or FixUp for extremely deep models  </li> </ul> <p>Modern LLMs do not use plain Xavier or He initialization alone.</p>"},{"location":"nn_training/2_initial/#8-expected-loss-at-initialization","title":"8. Expected Loss at Initialization","text":"<p>For a softmax classifier with \\(C\\) classes:</p> \\[ \\mathbb{E}[L_{\\text{init}}] = \\log C \\] <p>This baseline helps diagnose early training issues:</p> <ul> <li>higher than expected \u2192 variance too large  </li> <li>lower \u2192 bias in logits or incorrect initialization  </li> </ul>"},{"location":"nn_training/2_initial/#9-diagnosing-initialization-problems-practical","title":"9. Diagnosing Initialization Problems (Practical)","text":""},{"location":"nn_training/2_initial/#symptom-cause-fix","title":"Symptom \u2192 Cause \u2192 Fix","text":""},{"location":"nn_training/2_initial/#1-loss-flatlines-early","title":"1. Loss flatlines early","text":"<ul> <li>Cause: saturation or dead neurons  </li> <li>Fix: use Xavier/He, reduce LR, add normalization  </li> </ul>"},{"location":"nn_training/2_initial/#2-relu-units-all-zero","title":"2. ReLU units all zero","text":"<ul> <li>Cause: dead ReLUs  </li> <li>Fix: He init, LeakyReLU, LayerNorm, smaller LR  </li> </ul>"},{"location":"nn_training/2_initial/#3-tanh-outputs-at-1","title":"3. Tanh outputs at \u00b11","text":"<ul> <li>Cause: activation variance too large  </li> <li>Fix: Xavier init, normalize inputs, reduce bias scale  </li> </ul>"},{"location":"nn_training/2_initial/#4-exploding-loss","title":"4. Exploding loss","text":"<ul> <li>Cause: weight scale too large  </li> <li>Fix: reduce std, residual scaling, gradient clipping  </li> </ul>"},{"location":"nn_training/2_initial/#5-hockey-stick-learning-curve","title":"5. Hockey-stick learning curve","text":"<ul> <li>Cause: poor initialization or poorly scheduled LR warmup  </li> <li>Fix: check activations, add normalization, adjust LR schedule  </li> </ul>"},{"location":"nn_training/2_initial/#10-architecture-specific-recommendations","title":"10. Architecture-Specific Recommendations","text":""},{"location":"nn_training/2_initial/#cnns","title":"CNNs","text":"<ul> <li>Init: He  </li> <li>Norm: BatchNorm  </li> <li>Activation: ReLU or GELU  </li> <li>Notes: BN stabilizes variance, makes init forgiving  </li> </ul>"},{"location":"nn_training/2_initial/#transformers-llms","title":"Transformers / LLMs","text":"<ul> <li>Init: He-like + residual scaling  </li> <li>Norm: LayerNorm or RMSNorm  </li> <li>Activation: GELU  </li> <li>Notes: initialization must consider depth and residual structure  </li> </ul>"},{"location":"nn_training/2_initial/#rnns","title":"RNNs","text":"<ul> <li>Init: orthogonal for recurrent matrices  </li> <li>Activation: tanh or ReLU  </li> <li>Notes: highly sensitive to saturation; normalization helps  </li> </ul>"},{"location":"nn_training/2_initial/#11-summary","title":"11. Summary","text":"<p>Initialization controls:</p> <ul> <li>activation scale  </li> <li>gradient scale  </li> <li>neuron activity  </li> <li>numerical stability  </li> <li>early learning speed  </li> </ul> <p>Modern deep learning relies on three pillars:</p> <ol> <li>Variance-preserving initialization (Xavier, He, scaled residuals)  </li> <li>Normalization layers (BN, LN, RMSNorm)  </li> <li>Residual connections ensuring robust gradient flow  </li> </ol> <p>Together, they make deep networks trainable, stable, and efficient.</p> <p>If your network isn't learning, the first suspects should be:</p> <ul> <li>initialization  </li> <li>activation distributions  </li> <li>normalization  </li> <li>residual pathways  </li> <li>learning rate  </li> </ul> <p>Understanding these fundamentals is essential for building stable, scalable modern neural networks.</p>"},{"location":"nn_training/3_crossentropy/","title":"Understanding Cross-Entropy Loss and Softmax:","text":""},{"location":"nn_training/3_crossentropy/#understanding-cross-entropy-loss-and-softmax","title":"Understanding Cross-Entropy Loss and Softmax:","text":""},{"location":"nn_training/3_crossentropy/#why-only-the-correct-class-appears-yet-every-class-learns","title":"Why Only the Correct Class Appears \u2014 Yet Every Class Learns","text":"<p>Cross-entropy is the default loss function for classification in modern deep learning. But one part often confuses learners:</p> <p>Why does cross-entropy only include the probability of the correct class, yet the network still updates all class logits?</p> <p>This article provides a clean, rigorous, and intuitive explanation.</p>"},{"location":"nn_training/3_crossentropy/#1-what-cross-entropy-actually-measures","title":"1. What Cross-Entropy Actually Measures","text":"<p>For a single sample with true class \\(y\\) and predicted probabilities \\(p_i\\) from softmax:</p> \\[ L = -\\log(p_{\\text{correct}}) = -\\log(p_y) \\] <p>It seems cross-entropy cares only about the correct class. And that part is true.</p> <p>But that\u2019s not the full story.</p>"},{"location":"nn_training/3_crossentropy/#2-softmax-creates-competition-among-classes","title":"2. Softmax Creates Competition Among Classes","text":"<p>Softmax converts logits \\(z_i\\) into probabilities:</p> \\[ p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\] <p>Softmax ensures:</p> \\[ \\sum_i p_i = 1 \\] <p>This couples all classes together. Increasing the logit of any incorrect class automatically reduces the probability of the correct class.</p> <p>Thus, even though the loss formula only includes \\(p_y\\), changing any logit \\(z_i\\) changes \\(p_y\\).</p> <p>This is why incorrect classes still influence the loss.</p>"},{"location":"nn_training/3_crossentropy/#3-the-real-reason-every-class-learns-the-gradient","title":"3. The Real Reason Every Class Learns: The Gradient","text":"<p>The most important fact in cross-entropy + softmax is this gradient:</p> \\[ \\frac{\\partial L}{\\partial z_i} = p_i - Y_i \\] <p>Where:</p> <ul> <li>\\(Y_i = 1\\) for the correct class</li> <li>\\(Y_i = 0\\) for all incorrect classes</li> </ul> <p>This single equation explains everything:</p> <ul> <li>For the correct class \\(i=y\\):</li> </ul> \\[ \\frac{\\partial L}{\\partial z_y} = p_y - 1 \\] <ul> <li>For every incorrect class:</li> </ul> \\[ \\frac{\\partial L}{\\partial z_i} = p_i \\]"},{"location":"nn_training/3_crossentropy/#all-incorrect-classes-get-gradients-proportional-to-their-predicted-probabilities","title":"\u2714 All incorrect classes get gradients proportional to their predicted probabilities","text":""},{"location":"nn_training/3_crossentropy/#the-correct-class-gets-pushed-upward","title":"\u2714 The correct class gets pushed upward","text":""},{"location":"nn_training/3_crossentropy/#incorrect-classes-get-pushed-downward","title":"\u2714 Incorrect classes get pushed downward","text":"<p>This makes the \u201ccompetition\u201d between classes mathematically explicit.</p> <p>Cross-entropy doesn't have to include incorrect probabilities explicitly \u2014 the gradient already penalizes them. -</p>"},{"location":"nn_training/3_crossentropy/#4-why-frameworks-dont-explicitly-apply-softmax","title":"4. Why Frameworks Don\u2019t Explicitly Apply Softmax","text":"<p>In PyTorch / TensorFlow, the loss takes logits, not probabilities:</p> <p>```python CrossEntropyLoss(logits, labels)</p>"},{"location":"nn_training/4_train_llms/","title":"4 train llms","text":"<p>GPT is trained through a multi-stage pipeline designed to gradually improve its knowledge, reasoning, alignment, and usefulness. This process consists of four key stages, each with its own purpose and techniques:</p> <p>Pretraining: The model is trained on vast amounts of raw internet text to learn general language patterns, grammar, facts, and reasoning. At this stage, it becomes good at predicting the next word but is not yet fine-tuned for helpful or safe responses.</p> <p>Supervised Fine-Tuning (SFT): Human experts provide high-quality examples of ideal question-answer pairs. The model learns to imitate helpful, context-aware, and instruction-following responses based on these demonstrations.</p> <p>Reward Modeling (RM): Human trainers compare multiple model-generated responses and select the better one. Using these preferences, a reward model is trained to judge response quality. This model guides future optimization. Reinforcement Learning (RLHF): The model generates responses, and the reward model scores them. Using reinforcement learning, the model improves itself to produce more helpful, safe, and aligned answers, maximizing quality and user satisfaction.</p> Stage Dataset Algorithm Model Notes Pretraining Raw internettext trillions of wordslow-quality, large quantity Language modelingpredict the next token Base model 1000s of GPUsmonths of trainingex: GPT, LLaMA, PaLMcan deploy this model Supervised Finetuning DemonstrationsIdeal Assistant responses,~10-100K (prompt, response)written by contractorslow quantity, high quality Language modelingpredict the next token SFT modelinit from base 1-100 GPUsdays of trainingex: Vicuna-13Bcan deploy this model Reward Modeling Comparisons100K\u20131M comparisonswritten by contractorslow quantity, high quality Binary classificationpredict rewards consistentwith preferences RM modelinit from SFT 1-100 GPUsdays of training Reinforcement Learning Prompts~10K-100K promptswritten by contractorslow quantity, high quality Reinforcement Learninggenerate tokens that maximizethe reward RL modelinit from SFT &amp; RM 1-100 GPUsdays of trainingex: ChatGPT, Claudecan deploy this model"},{"location":"nn_training/notebooks/1_micrograd/","title":"1 micrograd","text":"In\u00a0[1]: Copied! <pre>import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[3]: Copied! <pre>def f(x):\n    return 3*x**2 - 4*x + 5\n</pre> def f(x):     return 3*x**2 - 4*x + 5 In\u00a0[4]: Copied! <pre>f(3.0)\n</pre> f(3.0) Out[4]: <pre>20.0</pre> In\u00a0[5]: Copied! <pre>xs = np.arange(-5, 5, 0.25)\nys = f(xs)\n\nplt.plot(xs, ys)\n</pre> xs = np.arange(-5, 5, 0.25) ys = f(xs)  plt.plot(xs, ys) Out[5]: <pre>[&lt;matplotlib.lines.Line2D at 0x142a8b70310&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>h = 0.000001\nx = 3.0\n(f(x + h) - f(x)) / (h)\n</pre> h = 0.000001 x = 3.0 (f(x + h) - f(x)) / (h) Out[\u00a0]: <pre>14.00003000000538</pre> In\u00a0[\u00a0]: Copied! <pre>class Value:\n  \n  def __init__(self, data, _children=(), _op='', label=''):\n    self.data = data\n    self.grad = 0.0\n    self._backward = lambda: None\n    self._prev = set(_children)\n    self._op = _op\n    self.label = label\n\n  def __repr__(self):\n    return f\"Value(data={self.data})\"\n  \n  def __add__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data + other.data, (self, other), '+')\n    \n    def _backward():\n      self.grad += 1.0 * out.grad\n      other.grad += 1.0 * out.grad\n    out._backward = _backward\n    \n    return out\n\n  def __mul__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data * other.data, (self, other), '*')\n    \n    def _backward():\n      self.grad += other.data * out.grad\n      other.grad += self.data * out.grad\n    out._backward = _backward\n      \n    return out\n  \n  def __pow__(self, other):\n    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n    out = Value(self.data**other, (self,), f'**{other}')\n\n    def _backward():\n        self.grad += other * (self.data ** (other - 1)) * out.grad\n    out._backward = _backward\n\n    return out\n  \n  def __rmul__(self, other): # other * self\n    return self * other\n\n  def __truediv__(self, other): # self / other\n    return self * other**-1\n\n  def __neg__(self): # -self\n    return self * -1\n\n  def __sub__(self, other): # self - other\n    return self + (-other)\n\n  def __radd__(self, other): # other + self\n    return self + other\n\n  def tanh(self):\n    x = self.data\n    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n    out = Value(t, (self, ), 'tanh')\n    \n    def _backward():\n      self.grad += (1 - t**2) * out.grad\n    out._backward = _backward\n    \n    return out\n  \n  def exp(self):\n    x = self.data\n    out = Value(math.exp(x), (self, ), 'exp')\n    \n    def _backward():\n      self.grad += out.data * out.grad \n    out._backward = _backward\n    \n    return out\n  \n  \n  def backward(self):\n    \n    topo = []\n    visited = set()\n    def build_topo(v):\n      if v not in visited:\n        visited.add(v)\n        for child in v._prev:\n          build_topo(child)\n        topo.append(v)\n    build_topo(self)\n    \n    self.grad = 1.0\n    for node in reversed(topo):\n      node._backward()\n</pre> class Value:      def __init__(self, data, _children=(), _op='', label=''):     self.data = data     self.grad = 0.0     self._backward = lambda: None     self._prev = set(_children)     self._op = _op     self.label = label    def __repr__(self):     return f\"Value(data={self.data})\"      def __add__(self, other):     other = other if isinstance(other, Value) else Value(other)     out = Value(self.data + other.data, (self, other), '+')          def _backward():       self.grad += 1.0 * out.grad       other.grad += 1.0 * out.grad     out._backward = _backward          return out    def __mul__(self, other):     other = other if isinstance(other, Value) else Value(other)     out = Value(self.data * other.data, (self, other), '*')          def _backward():       self.grad += other.data * out.grad       other.grad += self.data * out.grad     out._backward = _backward            return out      def __pow__(self, other):     assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"     out = Value(self.data**other, (self,), f'**{other}')      def _backward():         self.grad += other * (self.data ** (other - 1)) * out.grad     out._backward = _backward      return out      def __rmul__(self, other): # other * self     return self * other    def __truediv__(self, other): # self / other     return self * other**-1    def __neg__(self): # -self     return self * -1    def __sub__(self, other): # self - other     return self + (-other)    def __radd__(self, other): # other + self     return self + other    def tanh(self):     x = self.data     t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)     out = Value(t, (self, ), 'tanh')          def _backward():       self.grad += (1 - t**2) * out.grad     out._backward = _backward          return out      def exp(self):     x = self.data     out = Value(math.exp(x), (self, ), 'exp')          def _backward():       self.grad += out.data * out.grad      out._backward = _backward          return out         def backward(self):          topo = []     visited = set()     def build_topo(v):       if v not in visited:         visited.add(v)         for child in v._prev:           build_topo(child)         topo.append(v)     build_topo(self)          self.grad = 1.0     for node in reversed(topo):       node._backward()  In\u00a0[66]: Copied! <pre>from graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_graph(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n    nodes, edges = trace(root)  \n\n    for n in nodes:\n        uid = str(id(n))\n        dot.node(uid, label=\"{ %s | data % .4f | grad % .4f }\" % (n.label, n.data, n.grad), shape='record')\n\n        if n._op:\n            dot.node(name = uid + n._op, label=n._op)\n            dot.edge(uid + n._op, uid)\n    \n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n    return dot\n</pre> from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_graph(root):     dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})     nodes, edges = trace(root)        for n in nodes:         uid = str(id(n))         dot.node(uid, label=\"{ %s | data % .4f | grad % .4f }\" % (n.label, n.data, n.grad), shape='record')          if n._op:             dot.node(name = uid + n._op, label=n._op)             dot.edge(uid + n._op, uid)          for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)     return dot  In\u00a0[68]: Copied! <pre>x1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb1 = Value(6.8813735870195432, label='b1')\n\nx1w1 = x1 * w1; x1w1.label = 'x1w1'\nx2w2 = x2 * w2; x2w2.label = 'x2w2'\nxq1w1x2w2 = x1w1 + x2w2; xq1w1x2w2.label = 'xq1w1x2w2'\nn = xq1w1x2w2 + b1; n.label = 'n'\ne = (2*n).exp()\no = (e - 1) / (e +1)\no.label = 'o'\n</pre> x1 = Value(2.0, label='x1') x2 = Value(0.0, label='x2')  w1 = Value(-3.0, label='w1') w2 = Value(1.0, label='w2')  b1 = Value(6.8813735870195432, label='b1')  x1w1 = x1 * w1; x1w1.label = 'x1w1' x2w2 = x2 * w2; x2w2.label = 'x2w2' xq1w1x2w2 = x1w1 + x2w2; xq1w1x2w2.label = 'xq1w1x2w2' n = xq1w1x2w2 + b1; n.label = 'n' e = (2*n).exp() o = (e - 1) / (e +1) o.label = 'o' In\u00a0[69]: Copied! <pre>o.grad = 1.0  # the gradient of the output with respect to itself\n</pre> o.grad = 1.0  # the gradient of the output with respect to itself In\u00a0[\u00a0]: Copied! <pre># o._backward()\n# n._backward()\n# b1._backward()\n# xq1w1x2w2._backward()\n# x1w1._backward()\n# x2w2._backward()\n# w1._backward()\n# w2._backward()\n</pre> # o._backward() # n._backward() # b1._backward() # xq1w1x2w2._backward() # x1w1._backward() # x2w2._backward() # w1._backward() # w2._backward() In\u00a0[\u00a0]: Copied! <pre># # Manual backward pass\n# o.grad = 1.0 # the gradient of the o with respect to itself\n# n.grad = (1 - o.data**2) # the gradient of o with respect to n - derivative of tanh do/dn = 1 - tanh(n)^2\n# b1.grad = n.grad # the gradient of o with respect to b1\n# xq1w1x2w2.grad = n.grad # the gradient of o with respect to xq1w1x2w2\n# x2w2.grad = xq1w1x2w2.grad # the gradient of o with respect to x2w2 = chain rule dl/dx2w2 = dl/d * dn/dx2w2\n# x1w1.grad = xq1w1x2w2.grad # the gradient of o with respect to x1w1 = chain rule dl/dx1w1 = dl/d * dn/dx1w1\n# w2.grad = x2.data * x2w2.grad\n# x2.grad = w2.data * x2w2.grad # the gradient of o with respect to x2 = chain rule dl/dx2 = dl/d * dn/dx2\n# w1.grad = x1.data * x1w1.grad # the gradient of o with respect to w1 = chain rule dl/dw1 = dl/d * dn/dw1\n# x1.grad = w1.data * x1w1.grad # the gradient of o with respect to x1 = chain rule dl/dx1 = dl/d * dn/dx1\n</pre> # # Manual backward pass # o.grad = 1.0 # the gradient of the o with respect to itself # n.grad = (1 - o.data**2) # the gradient of o with respect to n - derivative of tanh do/dn = 1 - tanh(n)^2 # b1.grad = n.grad # the gradient of o with respect to b1 # xq1w1x2w2.grad = n.grad # the gradient of o with respect to xq1w1x2w2 # x2w2.grad = xq1w1x2w2.grad # the gradient of o with respect to x2w2 = chain rule dl/dx2w2 = dl/d * dn/dx2w2 # x1w1.grad = xq1w1x2w2.grad # the gradient of o with respect to x1w1 = chain rule dl/dx1w1 = dl/d * dn/dx1w1 # w2.grad = x2.data * x2w2.grad # x2.grad = w2.data * x2w2.grad # the gradient of o with respect to x2 = chain rule dl/dx2 = dl/d * dn/dx2 # w1.grad = x1.data * x1w1.grad # the gradient of o with respect to w1 = chain rule dl/dw1 = dl/d * dn/dw1 # x1.grad = w1.data * x1w1.grad # the gradient of o with respect to x1 = chain rule dl/dx1 = dl/d * dn/dx1 In\u00a0[70]: Copied! <pre>o.backward()\n</pre> o.backward() In\u00a0[71]: Copied! <pre>draw_graph(o)\n</pre> draw_graph(o) Out[71]: In\u00a0[\u00a0]: Copied! <pre>\n</pre> Out[\u00a0]: <pre>[Value(data=0.0),\n Value(data=1.0),\n Value(data=0.0),\n Value(data=-3.0),\n Value(data=2.0),\n Value(data=-6.0),\n Value(data=-6.0),\n Value(data=6.881373587019543),\n Value(data=0.8813735870195432),\n Value(data=0.7071067811865476)]</pre> In\u00a0[72]: Copied! <pre>import torch\n</pre> import torch In\u00a0[73]: Copied! <pre>x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('---')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n</pre> x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True n = x1*w1 + x2*w2 + b o = torch.tanh(n)  print(o.data.item()) o.backward()  print('---') print('x2', x2.grad.item()) print('w2', w2.grad.item()) print('x1', x1.grad.item()) print('w1', w1.grad.item()) <pre>0.7071066904050358\n---\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n</pre>"},{"location":"nn_training/notebooks/1_micrograd/#what-is-a-gradient","title":"What is a gradient?\u00b6","text":"<p>It represents the change in the output with respect to an infinitesimal change in the input x.</p>"},{"location":"nn_training/notebooks/1_micrograd/#pytorch-implementation","title":"Pytorch Implementation\u00b6","text":""},{"location":"nn_training/notebooks/2_makemore/","title":"2 makemore","text":"In\u00a0[\u00a0]: Copied! In\u00a0[2]: Copied! <pre>words = open('names.txt', 'r').read().splitlines()\n</pre> words = open('names.txt', 'r').read().splitlines() In\u00a0[3]: Copied! <pre>words[0:10]\n</pre> words[0:10] Out[3]: <pre>['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']</pre> In\u00a0[4]: Copied! <pre>len(words)\n</pre> len(words) Out[4]: <pre>32033</pre> In\u00a0[5]: Copied! <pre>min(len(w) for w in words)\n</pre> min(len(w) for w in words) Out[5]: <pre>2</pre> In\u00a0[6]: Copied! <pre>b = {}\nfor w in words:\n  chs = ['&lt;S&gt;'] + list(w) + ['&lt;E&gt;']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    bigram = (ch1, ch2)\n    b[bigram] = b.get(bigram, 0) + 1\nsorted(b.items(), key = lambda kv: -kv[1])\n</pre> b = {} for w in words:   chs = [''] + list(w) + ['']   for ch1, ch2 in zip(chs, chs[1:]):     bigram = (ch1, ch2)     b[bigram] = b.get(bigram, 0) + 1 sorted(b.items(), key = lambda kv: -kv[1]) Out[6]: <pre>[(('n', '&lt;E&gt;'), 6763),\n (('a', '&lt;E&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;S&gt;', 'a'), 4410),\n (('e', '&lt;E&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;S&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;S&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;E&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;S&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;E&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;S&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;E&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;S&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;S&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;S&gt;', 'l'), 1572),\n (('&lt;S&gt;', 'c'), 1542),\n (('&lt;S&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;E&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;E&gt;'), 1314),\n (('&lt;S&gt;', 't'), 1308),\n (('&lt;S&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;E&gt;'), 1169),\n (('&lt;S&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;S&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;S&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;E&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;S&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;S&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;S&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;E&gt;'), 516),\n (('d', '&lt;E&gt;'), 516),\n (('&lt;S&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;E&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;S&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;S&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;S&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;E&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;S&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;E&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;E&gt;'), 160),\n (('u', '&lt;E&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;S&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;E&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;E&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;E&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;S&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;E&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;E&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;S&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;E&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;E&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;E&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;E&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]</pre> In\u00a0[7]: Copied! <pre>import torch\n</pre> import torch In\u00a0[8]: Copied! <pre>N = torch.zeros((27, 27), dtype=torch.int32)\n</pre> N = torch.zeros((27, 27), dtype=torch.int32) In\u00a0[9]: Copied! <pre>chars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\n</pre> chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} In\u00a0[10]: Copied! <pre>for w in words:\n    chs = ['.'] + list(w) + ['.']\n    for ch1, ch2 in zip(chs, chs[1:]):\n        i = stoi[ch1]\n        j = stoi[ch2]\n        N[i, j] += 1\n</pre> for w in words:     chs = ['.'] + list(w) + ['.']     for ch1, ch2 in zip(chs, chs[1:]):         i = stoi[ch1]         j = stoi[ch2]         N[i, j] += 1 In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\n\nfor i in range(27):\n    for j in range(27):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off');\n</pre> import matplotlib.pyplot as plt %matplotlib inline  plt.figure(figsize=(16,16)) plt.imshow(N, cmap='Blues')  for i in range(27):     for j in range(27):         chstr = itos[i] + itos[j]         plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')         plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray') plt.axis('off');  In\u00a0[12]: Copied! <pre>N[0,:]\n</pre> N[0,:] Out[12]: <pre>tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)</pre> In\u00a0[13]: Copied! <pre>p = N[0].float()\np = p / p.sum()\np\n</pre> p = N[0].float() p = p / p.sum() p  Out[13]: <pre>tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])</pre> In\u00a0[14]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\nix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitos[ix]\n</pre> g = torch.Generator().manual_seed(2147483647) ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() itos[ix]  Out[14]: <pre>'c'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[15]: Copied! <pre>P = (N+1).float()\nP /= P.sum(1, keepdims=True)\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    p = P[ix]\n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n</pre> P = (N+1).float() P /= P.sum(1, keepdims=True)  g = torch.Generator().manual_seed(2147483647)  for i in range(5):      out = []   ix = 0   while True:     p = P[ix]     ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()     out.append(itos[ix])     if ix == 0:       break   print(''.join(out))  <pre>cexze.\nmomasurailezitynn.\nkonimittain.\nllayn.\nka.\n</pre> In\u00a0[16]: Copied! <pre>log_likelihood = 0.0\nn = 0\n\nfor w in words:\n#for w in [\"andrejq\"]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = P[ix1, ix2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    n += 1\n    #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nnll = -log_likelihood\nprint(f'{nll=}')\nprint(f'{nll/n}')\n</pre> log_likelihood = 0.0 n = 0  for w in words: #for w in [\"andrejq\"]:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     prob = P[ix1, ix2]     logprob = torch.log(prob)     log_likelihood += logprob     n += 1     #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')  print(f'{log_likelihood=}') nll = -log_likelihood print(f'{nll=}') print(f'{nll/n}')  <pre>log_likelihood=tensor(-559951.5625)\nnll=tensor(559951.5625)\n2.4543561935424805\n</pre> In\u00a0[17]: Copied! <pre>xs, ys = [], []\n\nfor w in words[:1]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    print(ch1, ch2)\n    xs.append(ix1)\n    ys.append(ix2)\n    \nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n</pre> xs, ys = [], []  for w in words[:1]:   chs = ['.'] + list(w) + ['.']   for ch1, ch2 in zip(chs, chs[1:]):     ix1 = stoi[ch1]     ix2 = stoi[ch2]     print(ch1, ch2)     xs.append(ix1)     ys.append(ix2)      xs = torch.tensor(xs) ys = torch.tensor(ys)  <pre>. e\ne m\nm m\nm a\na .\n</pre> In\u00a0[18]: Copied! <pre>import torch.nn.functional as F\nxenc = F.one_hot(xs, num_classes=27).float()\nxenc\n</pre> import torch.nn.functional as F xenc = F.one_hot(xs, num_classes=27).float() xenc Out[18]: <pre>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])</pre> In\u00a0[19]: Copied! <pre>plt.imshow(xenc)\n</pre> plt.imshow(xenc)  Out[19]: <pre>&lt;matplotlib.image.AxesImage at 0x1d3c0c72250&gt;</pre> In\u00a0[22]: Copied! <pre>W = torch.randn((27, 1))\nxenc @ W\n</pre> W = torch.randn((27, 1)) xenc @ W Out[22]: <pre>tensor([[0.8016],\n        [1.1898],\n        [0.2439],\n        [0.2439],\n        [0.0595]])</pre> In\u00a0[25]: Copied! <pre>W = torch.randn((27, 27))\nlogits = xenc @ W # log-counts\ncounts = logits.exp() # equivalent N\nprobs = counts / counts.sum(1, keepdims=True)\nprobs\n</pre> W = torch.randn((27, 27)) logits = xenc @ W # log-counts counts = logits.exp() # equivalent N probs = counts / counts.sum(1, keepdims=True) probs Out[25]: <pre>tensor([[0.1582, 0.0880, 0.0177, 0.0354, 0.0502, 0.0101, 0.0101, 0.0571, 0.0139,\n         0.0092, 0.0559, 0.0266, 0.0247, 0.0270, 0.1020, 0.0070, 0.0418, 0.0312,\n         0.0606, 0.0245, 0.0326, 0.0305, 0.0156, 0.0375, 0.0193, 0.0036, 0.0099],\n        [0.0072, 0.0397, 0.0095, 0.1407, 0.0837, 0.0505, 0.0194, 0.0147, 0.0153,\n         0.0474, 0.0243, 0.0103, 0.0158, 0.0234, 0.0095, 0.0245, 0.0149, 0.0173,\n         0.0890, 0.0109, 0.0022, 0.2042, 0.0509, 0.0094, 0.0180, 0.0019, 0.0454],\n        [0.0246, 0.0241, 0.0118, 0.0622, 0.0100, 0.1145, 0.1043, 0.0236, 0.0200,\n         0.0090, 0.0052, 0.0096, 0.0064, 0.0140, 0.0250, 0.0519, 0.2227, 0.0108,\n         0.0490, 0.0520, 0.0331, 0.0277, 0.0031, 0.0184, 0.0122, 0.0372, 0.0179],\n        [0.0246, 0.0241, 0.0118, 0.0622, 0.0100, 0.1145, 0.1043, 0.0236, 0.0200,\n         0.0090, 0.0052, 0.0096, 0.0064, 0.0140, 0.0250, 0.0519, 0.2227, 0.0108,\n         0.0490, 0.0520, 0.0331, 0.0277, 0.0031, 0.0184, 0.0122, 0.0372, 0.0179],\n        [0.0637, 0.0483, 0.0019, 0.0393, 0.0081, 0.0380, 0.0152, 0.0064, 0.1990,\n         0.0097, 0.0140, 0.0661, 0.0265, 0.0160, 0.0317, 0.0111, 0.0228, 0.0232,\n         0.0388, 0.0049, 0.0113, 0.0066, 0.0369, 0.0512, 0.1346, 0.0495, 0.0251]])</pre> In\u00a0[26]: Copied! <pre>g = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g)\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n</pre> g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g) xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding logits = xenc @ W # predict log-counts counts = logits.exp() # counts, equivalent to N probs = counts / counts.sum(1, keepdims=True) # probabilities for next character  In\u00a0[27]: Copied! <pre>probs.shape\n</pre> probs.shape Out[27]: <pre>torch.Size([5, 27])</pre> In\u00a0[28]: Copied! <pre>nlls = torch.zeros(5)\nfor i in range(5):\n  # i-th bigram:\n  x = xs[i].item() # input character index\n  y = ys[i].item() # label character index\n  print('--------')\n  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n  print('input to the neural net:', x)\n  print('output probabilities from the neural net:', probs[i])\n  print('label (actual next character):', y)\n  p = probs[i, y]\n  print('probability assigned by the net to the the correct character:', p.item())\n  logp = torch.log(p)\n  print('log likelihood:', logp.item())\n  nll = -logp\n  print('negative log likelihood:', nll.item())\n  nlls[i] = nll\n\nprint('=========')\nprint('average negative log likelihood, i.e. loss =', nlls.mean().item())\n</pre> nlls = torch.zeros(5) for i in range(5):   # i-th bigram:   x = xs[i].item() # input character index   y = ys[i].item() # label character index   print('--------')   print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')   print('input to the neural net:', x)   print('output probabilities from the neural net:', probs[i])   print('label (actual next character):', y)   p = probs[i, y]   print('probability assigned by the net to the the correct character:', p.item())   logp = torch.log(p)   print('log likelihood:', logp.item())   nll = -logp   print('negative log likelihood:', nll.item())   nlls[i] = nll  print('=========') print('average negative log likelihood, i.e. loss =', nlls.mean().item())  <pre>--------\nbigram example 1: .e (indexes 0,5)\ninput to the neural net: 0\noutput probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\nlabel (actual next character): 5\nprobability assigned by the net to the the correct character: 0.01228625513613224\nlog likelihood: -4.399273872375488\nnegative log likelihood: 4.399273872375488\n--------\nbigram example 2: em (indexes 5,13)\ninput to the neural net: 5\noutput probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.018050700426101685\nlog likelihood: -4.014570713043213\nnegative log likelihood: 4.014570713043213\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.026691533625125885\nlog likelihood: -3.623408794403076\nnegative log likelihood: 3.623408794403076\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 1\nprobability assigned by the net to the the correct character: 0.07367686182260513\nlog likelihood: -2.6080665588378906\nnegative log likelihood: 2.6080665588378906\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the neural net: 1\noutput probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\nlabel (actual next character): 0\nprobability assigned by the net to the the correct character: 0.014977526850998402\nlog likelihood: -4.201204299926758\nnegative log likelihood: 4.201204299926758\n=========\naverage negative log likelihood, i.e. loss = 3.7693049907684326\n</pre> In\u00a0[\u00a0]: Copied! <pre># ---- !!! Optimization !!!! ----- #\n</pre> # ---- !!! Optimization !!!! ----- # In\u00a0[\u00a0]: Copied! <pre># randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n</pre> # randomly initialize 27 neurons' weights. each neuron receives 27 inputs g = torch.Generator().manual_seed(2147483647) W = torch.randn((27, 27), generator=g, requires_grad=True)  In\u00a0[\u00a0]: Copied! <pre># forward pass\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\nloss = -probs[torch.arange(5), ys].log().mean()\n</pre> # forward pass xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding logits = xenc @ W # predict log-counts counts = logits.exp() # counts, equivalent to N probs = counts / counts.sum(1, keepdims=True) # probabilities for next character loss = -probs[torch.arange(5), ys].log().mean()  In\u00a0[\u00a0]: Copied! <pre>print(loss.item())\n</pre> print(loss.item())  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"nn_training/notebooks/3_makemore_part3/","title":"makemore: part 3","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[2]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() words[:8] Out[2]: <pre>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']</pre> In\u00a0[3]: Copied! <pre>len(words)\n</pre> len(words) Out[3]: <pre>32033</pre> In\u00a0[4]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[5]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10%  <pre>torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n</pre> In\u00a0[6]: Copied! <pre># MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\n# BatchNorm parameters\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\nbnmean_running = torch.zeros((1, n_hidden))\nbnstd_running = torch.ones((1, n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # MLP revisited n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2 #b1 = torch.randn(n_hidden,                        generator=g) * 0.01 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01 b2 = torch.randn(vocab_size,                      generator=g) * 0  # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden))  parameters = [C, W1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>12097\n</pre> In\u00a0[7]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n  # Linear layer\n  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n  # BatchNorm layer\n  # -------------------------------------------------------------\n  bnmeani = hpreact.mean(0, keepdim=True)\n  bnstdi = hpreact.std(0, keepdim=True)\n  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n  with torch.no_grad():\n    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n  # -------------------------------------------------------------\n  # Non-linearity\n  h = torch.tanh(hpreact) # hidden layer\n  logits = h @ W2 + b2 # output layer\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n  \n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   embcat = emb.view(emb.shape[0], -1) # concatenate the vectors   # Linear layer   hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation   # BatchNorm layer   # -------------------------------------------------------------   bnmeani = hpreact.mean(0, keepdim=True)   bnstdi = hpreact.std(0, keepdim=True)   hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias   with torch.no_grad():     bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani     bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi   # -------------------------------------------------------------   # Non-linearity   h = torch.tanh(hpreact) # hidden layer   logits = h @ W2 + b2 # output layer   loss = F.cross_entropy(logits, Yb) # loss function      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())    <pre>      0/ 200000: 3.3239\n  10000/ 200000: 2.0322\n  20000/ 200000: 2.5675\n  30000/ 200000: 2.0125\n  40000/ 200000: 2.2446\n  50000/ 200000: 1.8897\n  60000/ 200000: 2.0785\n  70000/ 200000: 2.3681\n  80000/ 200000: 2.2918\n  90000/ 200000: 2.0238\n 100000/ 200000: 2.3673\n 110000/ 200000: 2.3132\n 120000/ 200000: 1.6414\n 130000/ 200000: 1.9311\n 140000/ 200000: 2.2231\n 150000/ 200000: 2.0027\n 160000/ 200000: 2.0997\n 170000/ 200000: 2.4949\n 180000/ 200000: 2.0198\n 190000/ 200000: 2.1707\n</pre> In\u00a0[8]: Copied! <pre>plt.plot(lossi)\n</pre> plt.plot(lossi) Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x2c9600bfad0&gt;]</pre> In\u00a0[9]: Copied! <pre># calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n  # pass the training set through\n  emb = C[Xtr]\n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 # + b1\n  # measure the mean/std over the entire training set\n  bnmean = hpreact.mean(0, keepdim=True)\n  bnstd = hpreact.std(0, keepdim=True)\n</pre> # calibrate the batch norm at the end of training  with torch.no_grad():   # pass the training set through   emb = C[Xtr]   embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 # + b1   # measure the mean/std over the entire training set   bnmean = hpreact.mean(0, keepdim=True)   bnstd = hpreact.std(0, keepdim=True)  In\u00a0[10]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 # + b1\n  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   hpreact = embcat @ W1 # + b1   #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias   hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias   h = torch.tanh(hpreact) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.0674145221710205\nval 2.1056838035583496\n</pre> In\u00a0[12]: Copied! <pre># SUMMARY + PYTORCHIFYING -----------\n</pre> # SUMMARY + PYTORCHIFYING ----------- In\u00a0[13]: Copied! <pre># Let's train a deeper network\n# The classes we create here are the same API as nn.Module in PyTorch\n\nclass Linear:\n  \n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      xmean = x.mean(0, keepdim=True) # batch mean\n      xvar = x.var(0, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\nclass Tanh:\n  def __call__(self, x):\n    self.out = torch.tanh(x)\n    return self.out\n  def parameters(self):\n    return []\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 100 # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\n\nC = torch.randn((vocab_size, n_embd),            generator=g)\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n]\n# layers = [\n#   Linear(n_embd * block_size, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, vocab_size),\n# ]\n\nwith torch.no_grad():\n  # last layer: make less confident\n  layers[-1].gamma *= 0.1\n  #layers[-1].weight *= 0.1\n  # all other layers: apply gain\n  for layer in layers[:-1]:\n    if isinstance(layer, Linear):\n      layer.weight *= 1.0 #5/3\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # Let's train a deeper network # The classes we create here are the same API as nn.Module in PyTorch  class Linear:      def __init__(self, fan_in, fan_out, bias=True):     self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5     self.bias = torch.zeros(fan_out) if bias else None      def __call__(self, x):     self.out = x @ self.weight     if self.bias is not None:       self.out += self.bias     return self.out      def parameters(self):     return [self.weight] + ([] if self.bias is None else [self.bias])   class BatchNorm1d:      def __init__(self, dim, eps=1e-5, momentum=0.1):     self.eps = eps     self.momentum = momentum     self.training = True     # parameters (trained with backprop)     self.gamma = torch.ones(dim)     self.beta = torch.zeros(dim)     # buffers (trained with a running 'momentum update')     self.running_mean = torch.zeros(dim)     self.running_var = torch.ones(dim)      def __call__(self, x):     # calculate the forward pass     if self.training:       xmean = x.mean(0, keepdim=True) # batch mean       xvar = x.var(0, keepdim=True) # batch variance     else:       xmean = self.running_mean       xvar = self.running_var     xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance     self.out = self.gamma * xhat + self.beta     # update the buffers     if self.training:       with torch.no_grad():         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean         self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar     return self.out      def parameters(self):     return [self.gamma, self.beta]  class Tanh:   def __call__(self, x):     self.out = torch.tanh(x)     return self.out   def parameters(self):     return []  n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 100 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility  C = torch.randn((vocab_size, n_embd),            generator=g) layers = [   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size), ] # layers = [ #   Linear(n_embd * block_size, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, n_hidden), Tanh(), #   Linear(           n_hidden, vocab_size), # ]  with torch.no_grad():   # last layer: make less confident   layers[-1].gamma *= 0.1   #layers[-1].weight *= 0.1   # all other layers: apply gain   for layer in layers[:-1]:     if isinstance(layer, Linear):       layer.weight *= 1.0 #5/3  parameters = [C] + [p for layer in layers for p in layer.parameters()] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>47024\n</pre> In\u00a0[14]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nud = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  emb = C[Xb] # embed the characters into vectors\n  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, Yb) # loss function\n  \n  # backward pass\n  for layer in layers:\n    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n  with torch.no_grad():\n    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n\n  if i &gt;= 1000:\n    break # AFTER_DEBUG: would take out obviously to run full optimization\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] ud = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   emb = C[Xb] # embed the characters into vectors   x = emb.view(emb.shape[0], -1) # concatenate the vectors   for layer in layers:     x = layer(x)   loss = F.cross_entropy(x, Yb) # loss function      # backward pass   for layer in layers:     layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph   for p in parameters:     p.grad = None   loss.backward()      # update   lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())   with torch.no_grad():     ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])    if i &gt;= 1000:     break # AFTER_DEBUG: would take out obviously to run full optimization <pre>      0/ 200000: 3.2870\n</pre> In\u00a0[15]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out\n    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('activation distribution')\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i, layer in enumerate(layers[:-1]): # note: exclude the output layer   if isinstance(layer, Tanh):     t = layer.out     print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'layer {i} ({layer.__class__.__name__}') plt.legend(legends); plt.title('activation distribution') <pre>layer 2 (      Tanh): mean -0.00, std 0.63, saturated: 2.78%\nlayer 5 (      Tanh): mean +0.00, std 0.64, saturated: 2.56%\nlayer 8 (      Tanh): mean -0.00, std 0.65, saturated: 2.25%\nlayer 11 (      Tanh): mean +0.00, std 0.65, saturated: 1.69%\nlayer 14 (      Tanh): mean +0.00, std 0.65, saturated: 1.88%\n</pre> Out[15]: <pre>Text(0.5, 1.0, 'activation distribution')</pre> In\u00a0[16]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n  if isinstance(layer, Tanh):\n    t = layer.out.grad\n    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'layer {i} ({layer.__class__.__name__}')\nplt.legend(legends);\nplt.title('gradient distribution')\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i, layer in enumerate(layers[:-1]): # note: exclude the output layer   if isinstance(layer, Tanh):     t = layer.out.grad     print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'layer {i} ({layer.__class__.__name__}') plt.legend(legends); plt.title('gradient distribution') <pre>layer 2 (      Tanh): mean -0.000000, std 2.640702e-03\nlayer 5 (      Tanh): mean +0.000000, std 2.245584e-03\nlayer 8 (      Tanh): mean -0.000000, std 2.045742e-03\nlayer 11 (      Tanh): mean +0.000000, std 1.983134e-03\nlayer 14 (      Tanh): mean -0.000000, std 1.952382e-03\n</pre> Out[16]: <pre>Text(0.5, 1.0, 'gradient distribution')</pre> In\u00a0[17]: Copied! <pre># visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i,p in enumerate(parameters):\n  t = p.grad\n  if p.ndim == 2:\n    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n    hy, hx = torch.histogram(t, density=True)\n    plt.plot(hx[:-1].detach(), hy.detach())\n    legends.append(f'{i} {tuple(p.shape)}')\nplt.legend(legends)\nplt.title('weights gradient distribution');\n</pre> # visualize histograms plt.figure(figsize=(20, 4)) # width and height of the plot legends = [] for i,p in enumerate(parameters):   t = p.grad   if p.ndim == 2:     print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))     hy, hx = torch.histogram(t, density=True)     plt.plot(hx[:-1].detach(), hy.detach())     legends.append(f'{i} {tuple(p.shape)}') plt.legend(legends) plt.title('weights gradient distribution'); <pre>weight   (27, 10) | mean +0.000000 | std 8.020534e-03 | grad:data ratio 8.012630e-03\nweight  (30, 100) | mean +0.000246 | std 9.241077e-03 | grad:data ratio 4.881091e-02\nweight (100, 100) | mean +0.000113 | std 7.132879e-03 | grad:data ratio 6.964619e-02\nweight (100, 100) | mean -0.000086 | std 6.234305e-03 | grad:data ratio 6.073741e-02\nweight (100, 100) | mean +0.000052 | std 5.742187e-03 | grad:data ratio 5.631483e-02\nweight (100, 100) | mean +0.000032 | std 5.672205e-03 | grad:data ratio 5.570125e-02\nweight  (100, 27) | mean -0.000082 | std 1.209416e-02 | grad:data ratio 1.160106e-01\n</pre> In\u00a0[18]: Copied! <pre>plt.figure(figsize=(20, 4))\nlegends = []\nfor i,p in enumerate(parameters):\n  if p.ndim == 2:\n    plt.plot([ud[j][i] for j in range(len(ud))])\n    legends.append('param %d' % i)\nplt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\nplt.legend(legends);\n</pre> plt.figure(figsize=(20, 4)) legends = [] for i,p in enumerate(parameters):   if p.ndim == 2:     plt.plot([ud[j][i] for j in range(len(ud))])     legends.append('param %d' % i) plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot plt.legend(legends);  In\u00a0[19]: Copied! <pre>@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  for layer in layers:\n    x = layer(x)\n  loss = F.cross_entropy(x, y)\n  print(split, loss.item())\n\n# put layers into eval mode\nfor layer in layers:\n  layer.training = False\nsplit_loss('train')\nsplit_loss('val')\n</pre> @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   for layer in layers:     x = layer(x)   loss = F.cross_entropy(x, y)   print(split, loss.item())  # put layers into eval mode for layer in layers:   layer.training = False split_loss('train') split_loss('val') <pre>train 2.4002976417541504\nval 2.3982467651367188\n</pre> In\u00a0[20]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n      for layer in layers:\n        x = layer(x)\n      logits = x\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       emb = C[torch.tensor([context])] # (1,block_size,n_embd)       x = emb.view(emb.shape[0], -1) # concatenate the vectors       for layer in layers:         x = layer(x)       logits = x       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       # shift the context window and track the samples       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break          print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>carpah.\nqarlileif.\njmrix.\nthty.\nsacansa.\njazhnte.\ndpn.\narciigqeiunellaia.\nchriiv.\nkalein.\ndhlm.\njoin.\nqhinn.\nsroin.\narian.\nquiqaelogiearyxix.\nkaeklinsan.\ned.\necoia.\ngtleley.\n</pre> In\u00a0[21]: Copied! <pre># DONE; BONUS content below, not covered in video\n</pre> # DONE; BONUS content below, not covered in video In\u00a0[22]: Copied! <pre># BatchNorm forward pass as a widget\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport scipy.stats as stats\nimport numpy as np\n\ndef normshow(x0):\n  \n  g = torch.Generator().manual_seed(2147483647+1)\n  x = torch.randn(5, generator=g) * 5\n  x[0] = x0 # override the 0th example with the slider\n  mu = x.mean()\n  sig = x.std()\n  y = (x - mu)/sig\n\n  plt.figure(figsize=(10, 5))\n  # plot 0\n  plt.plot([-6,6], [0,0], 'k')\n  # plot the mean and std\n  xx = np.linspace(-6, 6, 100)\n  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n  xx = np.linspace(-6, 6, 100)\n  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n  # plot little lines connecting input and output\n  for i in range(len(x)):\n    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n  # plot the input and output values\n  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n  plt.xlim(-6, 6)\n  # title\n  plt.title('input mu %.2f std %.2f' % (mu, sig))\n\ninteract(normshow, x0=(-30,30,0.5));\n</pre> # BatchNorm forward pass as a widget  from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets import scipy.stats as stats import numpy as np  def normshow(x0):      g = torch.Generator().manual_seed(2147483647+1)   x = torch.randn(5, generator=g) * 5   x[0] = x0 # override the 0th example with the slider   mu = x.mean()   sig = x.std()   y = (x - mu)/sig    plt.figure(figsize=(10, 5))   # plot 0   plt.plot([-6,6], [0,0], 'k')   # plot the mean and std   xx = np.linspace(-6, 6, 100)   plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')   xx = np.linspace(-6, 6, 100)   plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')   # plot little lines connecting input and output   for i in range(len(x)):     plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)   # plot the input and output values   plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)   plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)   plt.xlim(-6, 6)   # title   plt.title('input mu %.2f std %.2f' % (mu, sig))  interact(normshow, x0=(-30,30,0.5));  <pre>interactive(children=(FloatSlider(value=0.0, description='x0', max=30.0, min=-30.0, step=0.5), Output()), _dom\u2026</pre> In\u00a0[23]: Copied! <pre># Linear: activation statistics of forward and backward pass\n\ng = torch.Generator().manual_seed(2147483647)\n\na = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\nb = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\nc = b @ a\nloss = torch.randn(1000, generator=g) @ c\na.retain_grad()\nb.retain_grad()\nc.retain_grad()\nloss.backward()\nprint('a std:', a.std().item())\nprint('b std:', b.std().item())\nprint('c std:', c.std().item())\nprint('-----')\nprint('c grad std:', c.grad.std().item())\nprint('a grad std:', a.grad.std().item())\nprint('b grad std:', b.grad.std().item())\n</pre> # Linear: activation statistics of forward and backward pass  g = torch.Generator().manual_seed(2147483647)  a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T c = b @ a loss = torch.randn(1000, generator=g) @ c a.retain_grad() b.retain_grad() c.retain_grad() loss.backward() print('a std:', a.std().item()) print('b std:', b.std().item()) print('c std:', c.std().item()) print('-----') print('c grad std:', c.grad.std().item()) print('a grad std:', a.grad.std().item()) print('b grad std:', b.grad.std().item()) <pre>a std: 0.9875972270965576\nb std: 1.0006722211837769\nc std: 31.01241683959961\n-----\nc grad std: 0.9782556295394897\na grad std: 30.8818302154541\nb grad std: 0.9666601419448853\n</pre> In\u00a0[24]: Copied! <pre># Linear + BatchNorm: activation statistics of forward and backward pass\n\ng = torch.Generator().manual_seed(2147483647)\n\nn = 1000\n# linear layer ---\ninp = torch.randn(n, requires_grad=True, generator=g)\nw = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\nx = w @ inp\n# bn layer ---\nxmean = x.mean()\nxvar = x.var()\nout = (x - xmean) / torch.sqrt(xvar + 1e-5)\n# ----\nloss = out @ torch.randn(n, generator=g)\ninp.retain_grad()\nx.retain_grad()\nw.retain_grad()\nout.retain_grad()\nloss.backward()\n\nprint('inp std: ', inp.std().item())\nprint('w std: ', w.std().item())\nprint('x std: ', x.std().item())\nprint('out std: ', out.std().item())\nprint('------')\nprint('out grad std: ', out.grad.std().item())\nprint('x grad std: ', x.grad.std().item())\nprint('w grad std: ', w.grad.std().item())\nprint('inp grad std: ', inp.grad.std().item())\n</pre> # Linear + BatchNorm: activation statistics of forward and backward pass  g = torch.Generator().manual_seed(2147483647)  n = 1000 # linear layer --- inp = torch.randn(n, requires_grad=True, generator=g) w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5 x = w @ inp # bn layer --- xmean = x.mean() xvar = x.var() out = (x - xmean) / torch.sqrt(xvar + 1e-5) # ---- loss = out @ torch.randn(n, generator=g) inp.retain_grad() x.retain_grad() w.retain_grad() out.retain_grad() loss.backward()  print('inp std: ', inp.std().item()) print('w std: ', w.std().item()) print('x std: ', x.std().item()) print('out std: ', out.std().item()) print('------') print('out grad std: ', out.grad.std().item()) print('x grad std: ', x.grad.std().item()) print('w grad std: ', w.grad.std().item()) print('inp grad std: ', inp.grad.std().item()) <pre>inp std:  0.9875972270965576\nw std:  1.0006722211837769\nx std:  31.01241683959961\nout std:  1.0\n------\nout grad std:  0.9782556295394897\nx grad std:  0.031543977558612823\nw grad std:  0.031169468536973\ninp grad std:  0.9953052997589111\n</pre>"},{"location":"nn_training/notebooks/3_makemore_part3/#makemore-part-3","title":"makemore: part 3\u00b6","text":""},{"location":"nn_training/notebooks/3_makemore_part3/#loss-log","title":"loss log\u00b6","text":""},{"location":"nn_training/notebooks/3_makemore_part3/#original","title":"original:\u00b6","text":"<p>train 2.1245384216308594 val   2.168196439743042</p>"},{"location":"nn_training/notebooks/3_makemore_part3/#fix-softmax-confidently-wrong","title":"fix softmax confidently wrong:\u00b6","text":"<p>train 2.07 val   2.13</p>"},{"location":"nn_training/notebooks/3_makemore_part3/#fix-tanh-layer-too-saturated-at-init","title":"fix tanh layer too saturated at init:\u00b6","text":"<p>train 2.0355966091156006 val   2.1026785373687744</p>"},{"location":"nn_training/notebooks/3_makemore_part3/#use-semi-principled-kaiming-init-instead-of-hacky-init","title":"use semi-principled \"kaiming init\" instead of hacky init:\u00b6","text":"<p>train 2.0376641750335693 val   2.106989622116089</p>"},{"location":"nn_training/notebooks/3_makemore_part3/#add-batch-norm-layer","title":"add batch norm layer\u00b6","text":"<p>train 2.0668270587921143 val 2.104844808578491</p>"},{"location":"nn_training/notebooks/4_makemore_part4_backprop/","title":"4 makemore part4 backprop","text":"In\u00a0[1]: Copied! <pre># there no change change in the first several cells from last lecture\n</pre> # there no change change in the first several cells from last lecture In\u00a0[10]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[7]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() print(len(words)) print(max(len(w) for w in words)) print(words[:8]) <pre>32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n</pre> In\u00a0[11]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[12]: Copied! <pre># build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 3 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words))  Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182580, 3]) torch.Size([182580])\ntorch.Size([22767, 3]) torch.Size([22767])\ntorch.Size([22799, 3]) torch.Size([22799])\n</pre> In\u00a0[6]: Copied! <pre># ok biolerplate done, now we get to the action:\n</pre> # ok biolerplate done, now we get to the action: In\u00a0[7]: Copied! <pre># utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n</pre> # utility function we will use later when comparing manual gradients to PyTorch gradients def cmp(s, dt, t):   ex = torch.all(dt == t.grad).item()   app = torch.allclose(dt, t.grad)   maxdiff = (dt - t.grad).abs().max().item()   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') In\u00a0[13]: Copied! <pre>n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass.  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>4137\n</pre> In\u00a0[14]: Copied! <pre>batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n</pre> batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y In\u00a0[15]: Copied! <pre># forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss\n</pre> # forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time  emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean()  # PyTorch backward pass for p in parameters:   p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way           norm_logits, logit_maxes, logits, h, hpreact, bnraw,          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,          embcat, emb]:   t.retain_grad() loss.backward() loss Out[15]: <pre>tensor(3.4837, grad_fn=&lt;NegBackward0&gt;)</pre> In\u00a0[17]: Copied! <pre>logprobs#[range(n), Yb]\n</pre> logprobs#[range(n), Yb] Out[17]: <pre>tensor([[-3.3591, -3.9382, -3.6262, -3.8188, -3.1008, -2.8536, -2.6143, -3.8992,\n         -3.1712, -3.4472, -3.4954, -3.5738, -3.1064, -2.9496, -2.7918, -2.7494,\n         -2.8780, -3.9415, -3.5940, -3.3374, -3.1420, -4.4049, -3.8037, -3.5869,\n         -3.3949, -3.5935, -3.1852],\n        [-3.7466, -2.8859, -3.3637, -3.7366, -3.5405, -2.7681, -3.8884, -3.8024,\n         -3.9610, -3.6619, -2.8001, -3.6673, -3.4396, -3.9222, -4.0430, -3.8375,\n         -3.7302, -4.1514, -2.7422, -2.9223, -4.1888, -3.5343, -3.6645, -2.3233,\n         -2.2329, -3.2728, -3.3657],\n        [-3.0698, -4.0426, -3.5094, -4.0983, -2.5895, -3.4174, -3.1383, -3.5417,\n         -3.4653, -3.1083, -3.7284, -3.4586, -3.2492, -2.9643, -3.5033, -3.4286,\n         -2.8977, -3.3686, -3.2446, -3.9228, -3.5065, -4.0304, -2.1962, -3.9773,\n         -3.2678, -3.1411, -4.0433],\n        [-4.0515, -3.3339, -3.4923, -3.5461, -4.0052, -3.3074, -2.8232, -2.8910,\n         -2.8031, -3.9382, -3.8182, -3.5069, -2.8951, -2.5044, -3.6440, -3.3422,\n         -4.2659, -3.5345, -3.7149, -2.0828, -2.8737, -4.1397, -3.6018, -3.6125,\n         -3.6477, -3.7161, -4.0102],\n        [-3.2457, -3.5433, -3.8998, -2.9732, -3.1064, -3.1137, -3.1688, -2.9649,\n         -3.7318, -4.2997, -3.0514, -4.1377, -3.8543, -3.8235, -3.8350, -2.9875,\n         -3.3888, -3.6926, -2.9487, -2.6664, -2.9821, -3.2966, -3.3742, -3.1370,\n         -2.9284, -3.9116, -3.1536],\n        [-3.8604, -3.8707, -3.2821, -3.8036, -3.6657, -3.0367, -3.6997, -3.5182,\n         -2.8374, -3.3224, -4.2865, -3.4733, -3.0304, -2.3130, -3.4792, -3.2413,\n         -3.5935, -3.5440, -3.4141, -2.8552, -2.2573, -4.4046, -3.4674, -3.1498,\n         -3.5920, -3.8310, -3.6233],\n        [-2.8703, -4.1914, -3.1734, -3.9344, -3.5592, -3.2279, -2.3789, -2.9464,\n         -3.3015, -3.5643, -3.4468, -3.1427, -2.8647, -3.2041, -3.6859, -3.9809,\n         -2.9877, -4.4950, -4.0376, -3.3751, -3.9681, -3.0795, -2.6307, -3.9103,\n         -3.5107, -2.9877, -3.8477],\n        [-3.5316, -2.7235, -2.9063, -2.1074, -4.3237, -2.9668, -3.7407, -3.0005,\n         -2.9502, -3.6228, -4.0749, -3.9329, -3.5915, -3.8133, -4.5083, -4.0912,\n         -3.8659, -2.8609, -4.3849, -2.7156, -3.0387, -4.0245, -4.2517, -2.6919,\n         -3.3561, -3.4982, -3.5759],\n        [-2.6523, -3.5602, -3.2155, -3.1185, -3.4611, -2.8365, -3.7963, -3.6360,\n         -3.9080, -3.5257, -2.7545, -3.0227, -2.9295, -3.7232, -2.9041, -3.4249,\n         -3.6385, -4.3667, -3.5316, -3.1597, -4.1117, -3.7207, -4.3742, -2.6510,\n         -3.1842, -3.0191, -3.6340],\n        [-3.4891, -3.8650, -3.8063, -3.9055, -2.6304, -4.1400, -2.3816, -3.4176,\n         -3.8115, -3.4632, -3.2915, -3.2723, -2.9495, -3.1237, -2.9792, -3.3432,\n         -3.3273, -4.5199, -3.5892, -3.5144, -3.6855, -3.6612, -2.0288, -4.5267,\n         -3.9152, -3.2224, -3.6952],\n        [-3.7448, -4.0361, -3.1988, -3.2678, -3.1962, -3.6073, -3.8444, -3.9311,\n         -2.9810, -3.5122, -3.7796, -4.3165, -3.6265, -3.5707, -3.9377, -2.1089,\n         -2.6815, -3.0667, -2.8450, -2.5039, -2.8774, -3.4687, -4.0381, -3.5981,\n         -3.3253, -4.1302, -3.9669],\n        [-2.6530, -3.9345, -3.6947, -3.0041, -4.0222, -3.2476, -3.4950, -2.5799,\n         -3.4795, -4.2476, -3.8762, -3.1696, -2.9138, -2.7837, -3.8349, -3.3447,\n         -3.9450, -3.7378, -3.4714, -2.5415, -2.4528, -3.6020, -3.4807, -3.2081,\n         -4.0858, -4.3351, -3.7134],\n        [-2.9772, -3.1313, -3.4961, -4.0841, -3.9182, -2.1245, -4.2142, -3.7909,\n         -4.0322, -3.1587, -3.5653, -3.1414, -3.0847, -3.5419, -4.5078, -4.6688,\n         -2.9358, -3.8588, -2.5685, -3.6087, -3.3173, -3.1539, -3.4828, -2.8114,\n         -2.7348, -3.5142, -4.3978],\n        [-3.0816, -4.1256, -3.2895, -4.3052, -2.7100, -4.0910, -3.0020, -3.2994,\n         -2.9457, -3.6194, -3.9995, -2.6153, -3.5212, -2.4536, -4.0554, -3.4305,\n         -2.6667, -3.3617, -3.2608, -3.1525, -3.3713, -3.8411, -2.7726, -4.1344,\n         -3.7576, -3.5789, -4.5647],\n        [-3.3960, -3.1842, -4.5353, -3.2329, -4.2248, -2.7321, -3.0539, -2.6561,\n         -2.9188, -4.1329, -3.1810, -3.3586, -3.3235, -3.4252, -4.2559, -4.7082,\n         -4.1114, -3.5097, -2.9640, -2.5387, -3.9642, -2.8280, -2.8751, -3.9580,\n         -3.1899, -2.9005, -4.3426],\n        [-3.9665, -3.5624, -2.9488, -2.9001, -2.8370, -3.3607, -3.4610, -3.2653,\n         -3.7316, -4.2213, -3.5992, -4.2541, -3.2681, -2.5477, -2.8664, -2.7404,\n         -4.1080, -4.0450, -4.2119, -2.1565, -3.2194, -4.9702, -4.3526, -3.1953,\n         -3.7369, -3.7309, -2.9689],\n        [-2.7521, -3.9797, -3.5022, -3.2396, -4.1818, -2.7140, -3.5029, -3.1999,\n         -3.5382, -3.2915, -3.3220, -3.4501, -2.8926, -3.7688, -3.3046, -2.9173,\n         -3.2594, -3.1857, -3.1691, -3.9545, -3.0810, -3.2133, -3.2984, -4.0064,\n         -3.5843, -3.3577, -3.0456],\n        [-4.0382, -4.4100, -2.9697, -4.3350, -3.2227, -4.2731, -2.5806, -3.1448,\n         -2.8352, -3.1502, -3.9818, -3.4528, -3.4083, -3.4434, -4.0024, -2.9535,\n         -2.5886, -3.3029, -3.2698, -3.4224, -3.2060, -3.3777, -2.3205, -4.7277,\n         -3.8437, -3.5984, -3.4167],\n        [-2.9943, -3.8981, -3.2237, -3.5868, -2.9189, -3.7047, -3.1987, -3.0942,\n         -3.6569, -3.9363, -3.6138, -3.2152, -2.9561, -2.7232, -3.2080, -2.8017,\n         -3.7399, -3.5967, -3.9715, -2.5938, -3.2067, -4.8478, -3.4919, -3.0891,\n         -3.3136, -3.6879, -3.3331],\n        [-2.9485, -3.6295, -3.3567, -2.8767, -4.9055, -3.2021, -3.6620, -2.6629,\n         -2.9375, -3.7332, -3.4123, -3.6372, -3.1044, -4.9988, -4.1596, -3.1126,\n         -3.9856, -2.4082, -4.1905, -3.3152, -3.3415, -2.3620, -3.5323, -4.4172,\n         -3.9115, -3.3810, -2.9280],\n        [-3.6736, -3.3808, -3.8634, -2.3040, -4.0421, -1.6520, -4.1592, -3.8021,\n         -2.8965, -3.1868, -3.6751, -4.1148, -3.9791, -4.1479, -4.1314, -4.1984,\n         -3.7440, -3.2450, -4.3570, -3.7818, -3.1523, -3.4994, -4.0881, -2.9416,\n         -3.0439, -3.1641, -3.2666],\n        [-3.0732, -2.8204, -3.7996, -3.9475, -3.6867, -2.8790, -3.9388, -3.6430,\n         -3.5436, -3.4302, -3.1675, -3.6169, -3.2514, -3.7039, -3.7367, -4.1009,\n         -3.4496, -3.2253, -2.5703, -3.9045, -3.1540, -3.0803, -2.7530, -3.2954,\n         -2.5975, -3.2430, -3.8884],\n        [-3.4095, -2.6994, -3.2474, -2.8407, -3.7986, -3.3702, -3.2380, -3.4559,\n         -3.2715, -3.1193, -2.8296, -3.8689, -3.1237, -3.6087, -3.4320, -3.3745,\n         -3.9768, -3.6748, -4.2371, -3.5968, -3.2120, -4.0450, -3.9997, -2.4151,\n         -3.2344, -3.4477, -2.9464],\n        [-2.2200, -3.9354, -3.8847, -4.5825, -4.2391, -2.6173, -3.7906, -3.1611,\n         -3.8407, -3.6407, -3.5158, -2.8123, -2.9969, -3.7894, -3.6359, -3.4594,\n         -3.5274, -4.5474, -2.7412, -3.2456, -3.7265, -2.9640, -3.0572, -2.9089,\n         -2.8748, -3.6811, -3.9882],\n        [-4.0382, -4.4100, -2.9697, -4.3350, -3.2227, -4.2731, -2.5806, -3.1448,\n         -2.8352, -3.1502, -3.9818, -3.4528, -3.4083, -3.4434, -4.0024, -2.9535,\n         -2.5886, -3.3029, -3.2698, -3.4224, -3.2060, -3.3777, -2.3205, -4.7277,\n         -3.8437, -3.5984, -3.4167],\n        [-3.1037, -3.1935, -3.4848, -2.2771, -3.7559, -2.3724, -3.7042, -3.6334,\n         -3.2273, -3.7305, -3.1448, -3.7677, -3.6540, -4.2696, -4.0513, -4.0868,\n         -3.8898, -3.7233, -3.9008, -2.6024, -4.5799, -3.5262, -4.1956, -2.5589,\n         -2.6312, -3.0932, -3.8691],\n        [-3.3707, -2.9372, -3.4767, -3.0694, -2.7880, -3.1776, -3.4515, -3.2141,\n         -3.9208, -4.2210, -2.4943, -4.3563, -3.9874, -3.5673, -2.6154, -2.6992,\n         -3.8925, -3.4642, -3.7653, -3.0077, -3.5650, -3.9284, -4.2504, -3.5688,\n         -3.3907, -3.3103, -2.8438],\n        [-2.7526, -3.0127, -3.3080, -3.2583, -3.0153, -2.6776, -4.6705, -3.9087,\n         -3.9505, -2.8857, -3.1511, -3.9301, -3.8738, -3.8223, -3.8638, -4.2410,\n         -3.3501, -3.1351, -3.4983, -4.3839, -2.8313, -3.5338, -3.0467, -2.6680,\n         -2.9706, -2.9005, -4.0927],\n        [-3.3539, -4.4002, -3.2435, -3.4392, -3.0139, -3.6441, -3.9791, -3.0477,\n         -3.4241, -3.9739, -3.0964, -4.0462, -3.0163, -3.6380, -2.7570, -2.0420,\n         -4.2517, -3.6780, -4.1888, -2.6444, -3.5419, -3.4354, -3.6140, -3.9355,\n         -4.0265, -3.2338, -2.7032],\n        [-4.0382, -4.4100, -2.9697, -4.3350, -3.2227, -4.2731, -2.5806, -3.1448,\n         -2.8352, -3.1502, -3.9818, -3.4528, -3.4083, -3.4434, -4.0024, -2.9535,\n         -2.5886, -3.3029, -3.2698, -3.4224, -3.2060, -3.3777, -2.3205, -4.7277,\n         -3.8437, -3.5984, -3.4167],\n        [-3.1826, -2.6747, -3.4535, -3.3269, -3.7452, -3.4098, -3.3264, -2.7966,\n         -3.1873, -3.7971, -2.8081, -3.6825, -3.5984, -4.2361, -3.4439, -3.3891,\n         -4.3959, -3.3951, -3.8094, -3.2413, -3.5474, -2.6670, -3.1704, -3.6431,\n         -3.0095, -3.3279, -2.9376],\n        [-3.2254, -4.1146, -3.0885, -4.5754, -2.6262, -3.6633, -3.4788, -3.5095,\n         -4.2878, -3.0810, -3.4585, -3.7761, -3.4272, -2.9961, -2.7673, -2.4933,\n         -2.8185, -3.8522, -3.4745, -4.2109, -3.0072, -4.3134, -2.8074, -3.6061,\n         -3.1184, -3.3490, -3.4767]], grad_fn=&lt;LogBackward0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[210]: Copied! <pre># Exercise 1: backprop through the whole thing manually, \n# backpropagating through exactly all of the variables \n# as they are defined in the forward pass above, one by one\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ndprobs = (1.0 / probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\ndhpreact = (1.0 - h**2) * dh\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\ndbnmeani = (-dbndiff).sum(0)\ndhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(emb.shape)\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n  for j in range(Xb.shape[1]):\n    ix = Xb[k,j]\n    dC[ix] += demb[k,j]\n    \ncmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)\n</pre> # Exercise 1: backprop through the whole thing manually,  # backpropagating through exactly all of the variables  # as they are defined in the forward pass above, one by one  dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0/n dprobs = (1.0 / probs) * dlogprobs dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) dcounts = counts_sum_inv * dprobs dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv dcounts += torch.ones_like(counts) * dcounts_sum dnorm_logits = counts * dcounts dlogits = dnorm_logits.clone() dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes dh = dlogits @ W2.T dW2 = h.T @ dlogits db2 = dlogits.sum(0) dhpreact = (1.0 - h**2) * dh dbngain = (bnraw * dhpreact).sum(0, keepdim=True) dbnraw = bngain * dhpreact dbnbias = dhpreact.sum(0, keepdim=True) dbndiff = bnvar_inv * dbnraw dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar dbndiff += (2*bndiff) * dbndiff2 dhprebn = dbndiff.clone() dbnmeani = (-dbndiff).sum(0) dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn db1 = dhprebn.sum(0) demb = dembcat.view(emb.shape) dC = torch.zeros_like(C) for k in range(Xb.shape[0]):   for j in range(Xb.shape[1]):     ix = Xb[k,j]     dC[ix] += demb[k,j]      cmp('logprobs', dlogprobs, logprobs) cmp('probs', dprobs, probs) cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv) cmp('counts_sum', dcounts_sum, counts_sum) cmp('counts', dcounts, counts) cmp('norm_logits', dnorm_logits, norm_logits) cmp('logit_maxes', dlogit_maxes, logit_maxes) cmp('logits', dlogits, logits) cmp('h', dh, h) cmp('W2', dW2, W2) cmp('b2', db2, b2) cmp('hpreact', dhpreact, hpreact) cmp('bngain', dbngain, bngain) cmp('bnbias', dbnbias, bnbias) cmp('bnraw', dbnraw, bnraw) cmp('bnvar_inv', dbnvar_inv, bnvar_inv) cmp('bnvar', dbnvar, bnvar) cmp('bndiff2', dbndiff2, bndiff2) cmp('bndiff', dbndiff, bndiff) cmp('bnmeani', dbnmeani, bnmeani) cmp('hprebn', dhprebn, hprebn) cmp('embcat', dembcat, embcat) cmp('W1', dW1, W1) cmp('b1', db1, b1) cmp('emb', demb, emb) cmp('C', dC, C) <pre>logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: True  | approximate: True  | maxdiff: 0.0\nbngain          | exact: True  | approximate: True  | maxdiff: 0.0\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar           | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff          | exact: True  | approximate: True  | maxdiff: 0.0\nbnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0\nembcat          | exact: True  | approximate: True  | maxdiff: 0.0\nW1              | exact: True  | approximate: True  | maxdiff: 0.0\nb1              | exact: True  | approximate: True  | maxdiff: 0.0\nemb             | exact: True  | approximate: True  | maxdiff: 0.0\nC               | exact: True  | approximate: True  | maxdiff: 0.0\n</pre> In\u00a0[211]: Copied! <pre># Exercise 2: backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# logit_maxes = logits.max(1, keepdim=True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdims=True)\n# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n</pre> # Exercise 2: backprop through cross_entropy but all in one go # to complete this challenge look at the mathematical expression of the loss, # take the derivative, simplify the expression, and just write it out  # forward pass  # before: # logit_maxes = logits.max(1, keepdim=True).values # norm_logits = logits - logit_maxes # subtract max for numerical stability # counts = norm_logits.exp() # counts_sum = counts.sum(1, keepdims=True) # counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact... # probs = counts * counts_sum_inv # logprobs = probs.log() # loss = -logprobs[range(n), Yb].mean()  # now: loss_fast = F.cross_entropy(logits, Yb) print(loss_fast.item(), 'diff:', (loss_fast - loss).item()) <pre>3.3377387523651123 diff: 2.384185791015625e-07\n</pre> In\u00a0[229]: Copied! <pre># backward pass\n\ndlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1\ndlogits /= n\n\ncmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n</pre> # backward pass  dlogits = F.softmax(logits, 1) dlogits[range(n), Yb] -= 1 dlogits /= n  cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9 <pre>logits          | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n</pre> In\u00a0[225]: Copied! <pre>logits.shape, Yb.shape\n</pre> logits.shape, Yb.shape Out[225]: <pre>(torch.Size([32, 27]), torch.Size([32]))</pre> In\u00a0[235]: Copied! <pre>F.softmax(logits, 1)[0]\n</pre> F.softmax(logits, 1)[0] Out[235]: <pre>tensor([0.0719, 0.0881, 0.0193, 0.0493, 0.0169, 0.0864, 0.0226, 0.0356, 0.0165,\n        0.0314, 0.0364, 0.0383, 0.0424, 0.0279, 0.0317, 0.0142, 0.0085, 0.0195,\n        0.0152, 0.0555, 0.0450, 0.0236, 0.0250, 0.0662, 0.0616, 0.0269, 0.0239],\n       grad_fn=&lt;SelectBackward0&gt;)</pre> In\u00a0[234]: Copied! <pre>dlogits[0] * n\n</pre> dlogits[0] * n Out[234]: <pre>tensor([ 0.0719,  0.0881,  0.0193,  0.0493,  0.0169,  0.0864,  0.0226,  0.0356,\n        -0.9835,  0.0314,  0.0364,  0.0383,  0.0424,  0.0279,  0.0317,  0.0142,\n         0.0085,  0.0195,  0.0152,  0.0555,  0.0450,  0.0236,  0.0250,  0.0662,\n         0.0616,  0.0269,  0.0239], grad_fn=&lt;MulBackward0&gt;)</pre> In\u00a0[238]: Copied! <pre>dlogits[0].sum()\n</pre> dlogits[0].sum() Out[238]: <pre>tensor(1.3970e-09, grad_fn=&lt;SumBackward0&gt;)</pre> In\u00a0[239]: Copied! <pre>plt.figure(figsize=(4, 4))\nplt.imshow(dlogits.detach(), cmap='gray')\n</pre> plt.figure(figsize=(4, 4)) plt.imshow(dlogits.detach(), cmap='gray') Out[239]: <pre>&lt;matplotlib.image.AxesImage at 0x7fcb28430340&gt;</pre> In\u00a0[269]: Copied! <pre># Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())\n</pre> # Exercise 3: backprop through batchnorm but all in one go # to complete this challenge look at the mathematical expression of the output of batchnorm, # take the derivative w.r.t. its input, simplify the expression, and just write it out  # forward pass  # before: # bnmeani = 1/n*hprebn.sum(0, keepdim=True) # bndiff = hprebn - bnmeani # bndiff2 = bndiff**2 # bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) # bnvar_inv = (bnvar + 1e-5)**-0.5 # bnraw = bndiff * bnvar_inv # hpreact = bngain * bnraw + bnbias  # now: hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias print('max diff:', (hpreact_fast - hpreact).abs().max()) <pre>max diff: tensor(4.7684e-07, grad_fn=&lt;MaxBackward1&gt;)\n</pre> In\u00a0[279]: Copied! <pre># backward pass\n\n# before we had:\n# dbnraw = bngain * dhpreact\n# dbndiff = bnvar_inv * dbnraw\n# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n# dbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n\n# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n# (you'll also need to use some of the variables from the forward pass up above)\n\ndhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n\ncmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10\n</pre> # backward pass  # before we had: # dbnraw = bngain * dhpreact # dbndiff = bnvar_inv * dbnraw # dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) # dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv # dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar # dbndiff += (2*bndiff) * dbndiff2 # dhprebn = dbndiff.clone() # dbnmeani = (-dbndiff).sum(0) # dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)  # calculate dhprebn given dhpreact (i.e. backprop through the batchnorm) # (you'll also need to use some of the variables from the forward pass up above)  dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))  cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10 <pre>hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n</pre> In\u00a0[278]: Copied! <pre>dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape\n</pre> dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape Out[278]: <pre>(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]),\n torch.Size([64]))</pre> In\u00a0[286]: Copied! <pre># Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\n\n# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n  # kick off optimization\n  for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n    bnvar_inv = (bnvar + 1e-5)**-0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n      p.grad = None\n    #loss.backward() # use this for correctness comparisons, delete it later!\n\n    # manual backprop! #swole_doge_meme\n    # -----------------\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(n), Yb] -= 1\n    dlogits /= n\n    # 2nd layer backprop\n    dh = dlogits @ W2.T\n    dW2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n    # 1st layer\n    dembcat = dhprebn @ W1.T\n    dW1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    for k in range(Xb.shape[0]):\n      for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j]\n    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n    # -----------------\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p, grad in zip(parameters, grads):\n      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n      p.data += -lr * grad # new way of swole doge TODO: enable\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n  #   if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net\n  #     break\n</pre> # Exercise 4: putting it all together! # Train the MLP neural net with your own backward pass  # init n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP  g = torch.Generator().manual_seed(2147483647) # for reproducibility C  = torch.randn((vocab_size, n_embd),            generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # Layer 2 W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1 b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1  parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True  # same optimization as last time max_steps = 200000 batch_size = 32 n = batch_size # convenience lossi = []  # use this context manager for efficiency once your backward pass is written (TODO) with torch.no_grad():    # kick off optimization   for i in range(max_steps):      # minibatch construct     ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)     Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass     emb = C[Xb] # embed the characters into vectors     embcat = emb.view(emb.shape[0], -1) # concatenate the vectors     # Linear layer     hprebn = embcat @ W1 + b1 # hidden layer pre-activation     # BatchNorm layer     # -------------------------------------------------------------     bnmean = hprebn.mean(0, keepdim=True)     bnvar = hprebn.var(0, keepdim=True, unbiased=True)     bnvar_inv = (bnvar + 1e-5)**-0.5     bnraw = (hprebn - bnmean) * bnvar_inv     hpreact = bngain * bnraw + bnbias     # -------------------------------------------------------------     # Non-linearity     h = torch.tanh(hpreact) # hidden layer     logits = h @ W2 + b2 # output layer     loss = F.cross_entropy(logits, Yb) # loss function      # backward pass     for p in parameters:       p.grad = None     #loss.backward() # use this for correctness comparisons, delete it later!      # manual backprop! #swole_doge_meme     # -----------------     dlogits = F.softmax(logits, 1)     dlogits[range(n), Yb] -= 1     dlogits /= n     # 2nd layer backprop     dh = dlogits @ W2.T     dW2 = h.T @ dlogits     db2 = dlogits.sum(0)     # tanh     dhpreact = (1.0 - h**2) * dh     # batchnorm backprop     dbngain = (bnraw * dhpreact).sum(0, keepdim=True)     dbnbias = dhpreact.sum(0, keepdim=True)     dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))     # 1st layer     dembcat = dhprebn @ W1.T     dW1 = embcat.T @ dhprebn     db1 = dhprebn.sum(0)     # embedding     demb = dembcat.view(emb.shape)     dC = torch.zeros_like(C)     for k in range(Xb.shape[0]):       for j in range(Xb.shape[1]):         ix = Xb[k,j]         dC[ix] += demb[k,j]     grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]     # -----------------      # update     lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay     for p, grad in zip(parameters, grads):       #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())       p.data += -lr * grad # new way of swole doge TODO: enable      # track stats     if i % 10000 == 0: # print every once in a while       print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')     lossi.append(loss.log10().item())    #   if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net   #     break <pre>12297\n      0/ 200000: 3.7805\n  10000/ 200000: 2.1775\n  20000/ 200000: 2.3957\n  30000/ 200000: 2.5032\n  40000/ 200000: 2.0065\n  50000/ 200000: 2.3873\n  60000/ 200000: 2.3378\n  70000/ 200000: 2.0640\n  80000/ 200000: 2.3497\n  90000/ 200000: 2.1093\n 100000/ 200000: 1.9132\n 110000/ 200000: 2.2229\n 120000/ 200000: 1.9912\n 130000/ 200000: 2.4441\n 140000/ 200000: 2.3198\n 150000/ 200000: 2.1857\n 160000/ 200000: 2.0296\n 170000/ 200000: 1.8391\n 180000/ 200000: 2.0436\n 190000/ 200000: 1.9200\n</pre> In\u00a0[285]: Copied! <pre># useful for checking your gradients\n# for p,g in zip(parameters, grads):\n#   cmp(str(tuple(p.shape)), g, p)\n</pre> # useful for checking your gradients # for p,g in zip(parameters, grads): #   cmp(str(tuple(p.shape)), g, p) In\u00a0[299]: Copied! <pre># calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n  # pass the training set through\n  emb = C[Xtr]\n  embcat = emb.view(emb.shape[0], -1)\n  hpreact = embcat @ W1 + b1\n  # measure the mean/std over the entire training set\n  bnmean = hpreact.mean(0, keepdim=True)\n  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n</pre> # calibrate the batch norm at the end of training  with torch.no_grad():   # pass the training set through   emb = C[Xtr]   embcat = emb.view(emb.shape[0], -1)   hpreact = embcat @ W1 + b1   # measure the mean/std over the entire training set   bnmean = hpreact.mean(0, keepdim=True)   bnvar = hpreact.var(0, keepdim=True, unbiased=True)  In\u00a0[300]: Copied! <pre># evaluate train and val loss\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  emb = C[x] # (N, block_size, n_embd)\n  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n  hpreact = embcat @ W1 + b1\n  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n  h = torch.tanh(hpreact) # (N, n_hidden)\n  logits = h @ W2 + b2 # (N, vocab_size)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> # evaluate train and val loss  @torch.no_grad() # this decorator disables gradient tracking def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   emb = C[x] # (N, block_size, n_embd)   embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)   hpreact = embcat @ W1 + b1   hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias   h = torch.tanh(hpreact) # (N, n_hidden)   logits = h @ W2 + b2 # (N, vocab_size)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 2.070523500442505\nval 2.109893560409546\n</pre> In\u00a0[294]: Copied! <pre># I achieved:\n# train 2.0718822479248047\n# val 2.1162495613098145\n</pre> # I achieved: # train 2.0718822479248047 # val 2.1162495613098145 In\u00a0[301]: Copied! <pre># sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # ------------\n      # forward pass:\n      # Embedding\n      emb = C[torch.tensor([context])] # (1,block_size,d)      \n      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n      hpreact = embcat @ W1 + b1\n      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n      h = torch.tanh(hpreact) # (N, n_hidden)\n      logits = h @ W2 + b2 # (N, vocab_size)\n      # ------------\n      # Sample\n      probs = F.softmax(logits, dim=1)\n      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n      context = context[1:] + [ix]\n      out.append(ix)\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out))\n</pre> # sample from the model g = torch.Generator().manual_seed(2147483647 + 10)  for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # ------------       # forward pass:       # Embedding       emb = C[torch.tensor([context])] # (1,block_size,d)             embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)       hpreact = embcat @ W1 + b1       hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias       h = torch.tanh(hpreact) # (N, n_hidden)       logits = h @ W2 + b2 # (N, vocab_size)       # ------------       # Sample       probs = F.softmax(logits, dim=1)       ix = torch.multinomial(probs, num_samples=1, generator=g).item()       context = context[1:] + [ix]       out.append(ix)       if ix == 0:         break          print(''.join(itos[i] for i in out)) <pre>carmahzamille.\nkhi.\nmreigeet.\nkhalaysie.\nmahnen.\ndelynn.\njareen.\nnellara.\nchaiiv.\nkaleigh.\nham.\njoce.\nquinn.\nshoison.\njadiquintero.\ndearyxi.\njace.\npinsley.\ndae.\niia.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"nn_training/notebooks/4_makemore_part4_backprop/#makemore-becoming-a-backprop-ninja","title":"makemore: becoming a backprop ninja\u00b6","text":"<p>swole doge style</p>"},{"location":"nn_training/notebooks/5_makemore_part5_cnn1/","title":"5 makemore part5 cnn1","text":"In\u00a0[4]: Copied! <pre>import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n</pre> import torch import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline In\u00a0[5]: Copied! <pre># read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n</pre> # read in all the words words = open('names.txt', 'r').read().splitlines() print(len(words)) print(max(len(w) for w in words)) print(words[:8]) <pre>32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n</pre> In\u00a0[6]: Copied! <pre># build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n</pre> # build the vocabulary of characters and mappings to/from integers chars = sorted(list(set(''.join(words)))) stoi = {s:i+1 for i,s in enumerate(chars)} stoi['.'] = 0 itos = {i:s for s,i in stoi.items()} vocab_size = len(itos) print(itos) print(vocab_size) <pre>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n</pre> In\u00a0[7]: Copied! <pre># shuffle up the words\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n</pre> # shuffle up the words import random random.seed(42) random.shuffle(words) In\u00a0[9]: Copied! <pre># build the dataset\nblock_size = 8 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n</pre> # build the dataset block_size = 8 # context length: how many characters do we take to predict the next one?  def build_dataset(words):     X, Y = [], []      for w in words:     context = [0] * block_size     for ch in w + '.':       ix = stoi[ch]       X.append(context)       Y.append(ix)       context = context[1:] + [ix] # crop and append    X = torch.tensor(X)   Y = torch.tensor(Y)   print(X.shape, Y.shape)   return X, Y  n1 = int(0.8*len(words)) n2 = int(0.9*len(words)) Xtr,  Ytr  = build_dataset(words[:n1])     # 80% Xdev, Ydev = build_dataset(words[n1:n2])   # 10% Xte,  Yte  = build_dataset(words[n2:])     # 10% <pre>torch.Size([182625, 8]) torch.Size([182625])\ntorch.Size([22655, 8]) torch.Size([22655])\ntorch.Size([22866, 8]) torch.Size([22866])\n</pre> In\u00a0[10]: Copied! <pre>for x,y in zip(Xtr[:20], Ytr[:20]):\n  print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n</pre> for x,y in zip(Xtr[:20], Ytr[:20]):   print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()]) <pre>........ --&gt; y\n.......y --&gt; u\n......yu --&gt; h\n.....yuh --&gt; e\n....yuhe --&gt; n\n...yuhen --&gt; g\n..yuheng --&gt; .\n........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n........ --&gt; x\n.......x --&gt; a\n......xa --&gt; v\n.....xav --&gt; i\n....xavi --&gt; e\n</pre> In\u00a0[7]: Copied! <pre># Near copy paste of the layers we have developed in Part 3\n\n# -----------------------------------------------------------------------------------------------\nclass Linear:\n  \n  def __init__(self, fan_in, fan_out, bias=True):\n    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n    self.bias = torch.zeros(fan_out) if bias else None\n  \n  def __call__(self, x):\n    self.out = x @ self.weight\n    if self.bias is not None:\n      self.out += self.bias\n    return self.out\n  \n  def parameters(self):\n    return [self.weight] + ([] if self.bias is None else [self.bias])\n\n# -----------------------------------------------------------------------------------------------\nclass BatchNorm1d:\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      if x.ndim == 2:\n        dim = 0\n      elif x.ndim == 3:\n        dim = (0,1)\n      xmean = x.mean(dim, keepdim=True) # batch mean\n      xvar = x.var(dim, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\n# -----------------------------------------------------------------------------------------------\nclass Tanh:\n  def __call__(self, x):\n    self.out = torch.tanh(x)\n    return self.out\n  def parameters(self):\n    return []\n\n# -----------------------------------------------------------------------------------------------\nclass Embedding:\n  \n  def __init__(self, num_embeddings, embedding_dim):\n    self.weight = torch.randn((num_embeddings, embedding_dim))\n    \n  def __call__(self, IX):\n    self.out = self.weight[IX]\n    return self.out\n  \n  def parameters(self):\n    return [self.weight]\n\n# -----------------------------------------------------------------------------------------------\nclass FlattenConsecutive:\n  \n  def __init__(self, n):\n    self.n = n\n    \n  def __call__(self, x):\n    B, T, C = x.shape\n    x = x.view(B, T//self.n, C*self.n)\n    if x.shape[1] == 1:\n      x = x.squeeze(1)\n    self.out = x\n    return self.out\n  \n  def parameters(self):\n    return []\n\n# -----------------------------------------------------------------------------------------------\nclass Sequential:\n  \n  def __init__(self, layers):\n    self.layers = layers\n  \n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    self.out = x\n    return self.out\n  \n  def parameters(self):\n    # get parameters of all layers and stretch them out into one list\n    return [p for layer in self.layers for p in layer.parameters()]\n</pre> # Near copy paste of the layers we have developed in Part 3  # ----------------------------------------------------------------------------------------------- class Linear:      def __init__(self, fan_in, fan_out, bias=True):     self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init     self.bias = torch.zeros(fan_out) if bias else None      def __call__(self, x):     self.out = x @ self.weight     if self.bias is not None:       self.out += self.bias     return self.out      def parameters(self):     return [self.weight] + ([] if self.bias is None else [self.bias])  # ----------------------------------------------------------------------------------------------- class BatchNorm1d:      def __init__(self, dim, eps=1e-5, momentum=0.1):     self.eps = eps     self.momentum = momentum     self.training = True     # parameters (trained with backprop)     self.gamma = torch.ones(dim)     self.beta = torch.zeros(dim)     # buffers (trained with a running 'momentum update')     self.running_mean = torch.zeros(dim)     self.running_var = torch.ones(dim)      def __call__(self, x):     # calculate the forward pass     if self.training:       if x.ndim == 2:         dim = 0       elif x.ndim == 3:         dim = (0,1)       xmean = x.mean(dim, keepdim=True) # batch mean       xvar = x.var(dim, keepdim=True) # batch variance     else:       xmean = self.running_mean       xvar = self.running_var     xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance     self.out = self.gamma * xhat + self.beta     # update the buffers     if self.training:       with torch.no_grad():         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean         self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar     return self.out      def parameters(self):     return [self.gamma, self.beta]  # ----------------------------------------------------------------------------------------------- class Tanh:   def __call__(self, x):     self.out = torch.tanh(x)     return self.out   def parameters(self):     return []  # ----------------------------------------------------------------------------------------------- class Embedding:      def __init__(self, num_embeddings, embedding_dim):     self.weight = torch.randn((num_embeddings, embedding_dim))        def __call__(self, IX):     self.out = self.weight[IX]     return self.out      def parameters(self):     return [self.weight]  # ----------------------------------------------------------------------------------------------- class FlattenConsecutive:      def __init__(self, n):     self.n = n        def __call__(self, x):     B, T, C = x.shape     x = x.view(B, T//self.n, C*self.n)     if x.shape[1] == 1:       x = x.squeeze(1)     self.out = x     return self.out      def parameters(self):     return []  # ----------------------------------------------------------------------------------------------- class Sequential:      def __init__(self, layers):     self.layers = layers      def __call__(self, x):     for layer in self.layers:       x = layer(x)     self.out = x     return self.out      def parameters(self):     # get parameters of all layers and stretch them out into one list     return [p for layer in self.layers for p in layer.parameters()]  In\u00a0[8]: Copied! <pre>torch.manual_seed(42); # seed rng for reproducibility\n</pre> torch.manual_seed(42); # seed rng for reproducibility In\u00a0[9]: Copied! <pre># original network\n# n_embd = 10 # the dimensionality of the character embedding vectors\n# n_hidden = 300 # the number of neurons in the hidden layer of the MLP\n# model = Sequential([\n#   Embedding(vocab_size, n_embd),\n#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n#   Linear(n_hidden, vocab_size),\n# ])\n\n# hierarchical network\nn_embd = 24 # the dimensionality of the character embedding vectors\nn_hidden = 128 # the number of neurons in the hidden layer of the MLP\nmodel = Sequential([\n  Embedding(vocab_size, n_embd),\n  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n  model.layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n</pre> # original network # n_embd = 10 # the dimensionality of the character embedding vectors # n_hidden = 300 # the number of neurons in the hidden layer of the MLP # model = Sequential([ #   Embedding(vocab_size, n_embd), #   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), #   Linear(n_hidden, vocab_size), # ])  # hierarchical network n_embd = 24 # the dimensionality of the character embedding vectors n_hidden = 128 # the number of neurons in the hidden layer of the MLP model = Sequential([   Embedding(vocab_size, n_embd),   FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),   Linear(n_hidden, vocab_size), ])  # parameter init with torch.no_grad():   model.layers[-1].weight *= 0.1 # last layer make less confident  parameters = model.parameters() print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters:   p.requires_grad = True <pre>76579\n</pre> In\u00a0[10]: Copied! <pre># same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n  # forward pass\n  logits = model(Xb)\n  loss = F.cross_entropy(logits, Yb) # loss function\n  \n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n  \n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n</pre> # same optimization as last time max_steps = 200000 batch_size = 32 lossi = []  for i in range(max_steps):      # minibatch construct   ix = torch.randint(0, Xtr.shape[0], (batch_size,))   Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y      # forward pass   logits = model(Xb)   loss = F.cross_entropy(logits, Yb) # loss function      # backward pass   for p in parameters:     p.grad = None   loss.backward()      # update: simple SGD   lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay   for p in parameters:     p.data += -lr * p.grad    # track stats   if i % 10000 == 0: # print every once in a while     print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')   lossi.append(loss.log10().item())  <pre>      0/ 200000: 3.3167\n  10000/ 200000: 2.0576\n  20000/ 200000: 2.0723\n  30000/ 200000: 2.5134\n  40000/ 200000: 2.1476\n  50000/ 200000: 1.7836\n  60000/ 200000: 2.2592\n  70000/ 200000: 1.9331\n  80000/ 200000: 1.6875\n  90000/ 200000: 2.0395\n 100000/ 200000: 1.7736\n 110000/ 200000: 1.9570\n 120000/ 200000: 1.7465\n 130000/ 200000: 1.8126\n 140000/ 200000: 1.7406\n 150000/ 200000: 1.7466\n 160000/ 200000: 1.8806\n 170000/ 200000: 1.6266\n 180000/ 200000: 1.6476\n 190000/ 200000: 1.8555\n</pre> In\u00a0[11]: Copied! <pre>plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n</pre> plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1)) Out[11]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fb5a03e3b50&gt;]</pre> In\u00a0[12]: Copied! <pre># put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n  layer.training = False\n</pre> # put layers into eval mode (needed for batchnorm especially) for layer in model.layers:   layer.training = False In\u00a0[13]: Copied! <pre># evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n  x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n  }[split]\n  logits = model(x)\n  loss = F.cross_entropy(logits, y)\n  print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n</pre> # evaluate the loss @torch.no_grad() # this decorator disables gradient tracking inside pytorch def split_loss(split):   x,y = {     'train': (Xtr, Ytr),     'val': (Xdev, Ydev),     'test': (Xte, Yte),   }[split]   logits = model(x)   loss = F.cross_entropy(logits, y)   print(split, loss.item())  split_loss('train') split_loss('val') <pre>train 1.7690284252166748\nval 1.9936515092849731\n</pre> In\u00a0[14]: Copied! <pre># sample from the model\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n      # forward pass the neural net\n      logits = model(torch.tensor([context]))\n      probs = F.softmax(logits, dim=1)\n      # sample from the distribution\n      ix = torch.multinomial(probs, num_samples=1).item()\n      # shift the context window and track the samples\n      context = context[1:] + [ix]\n      out.append(ix)\n      # if we sample the special '.' token, break\n      if ix == 0:\n        break\n    \n    print(''.join(itos[i] for i in out)) # decode and print the generated word\n</pre> # sample from the model for _ in range(20):          out = []     context = [0] * block_size # initialize with all ...     while True:       # forward pass the neural net       logits = model(torch.tensor([context]))       probs = F.softmax(logits, dim=1)       # sample from the distribution       ix = torch.multinomial(probs, num_samples=1).item()       # shift the context window and track the samples       context = context[1:] + [ix]       out.append(ix)       # if we sample the special '.' token, break       if ix == 0:         break          print(''.join(itos[i] for i in out)) # decode and print the generated word <pre>arlij.\nchetta.\nheago.\nrocklei.\nhendrix.\njamylie.\nbroxin.\ndenish.\nanslibt.\nmarianah.\nastavia.\nannayve.\naniah.\njayce.\nnodiel.\nremita.\nniyelle.\njaylene.\naiyan.\naubreana.\n</pre> In\u00a0[15]: Copied! <pre>for x,y in zip(Xtr[7:15], Ytr[7:15]):\n  print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n</pre> for x,y in zip(Xtr[7:15], Ytr[7:15]):   print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()]) <pre>........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n</pre> In\u00a0[16]: Copied! <pre># forward a single example:\nlogits = model(Xtr[[7]])\nlogits.shape\n</pre> # forward a single example: logits = model(Xtr[[7]]) logits.shape Out[16]: <pre>torch.Size([1, 27])</pre> In\u00a0[17]: Copied! <pre># forward all of them\nlogits = torch.zeros(8, 27)\nfor i in range(8):\n  logits[i] = model(Xtr[[7+i]])\nlogits.shape\n</pre> # forward all of them logits = torch.zeros(8, 27) for i in range(8):   logits[i] = model(Xtr[[7+i]]) logits.shape Out[17]: <pre>torch.Size([8, 27])</pre> In\u00a0[18]: Copied! <pre># convolution is a \"for loop\"\n# allows us to forward Linear layers efficiently over space\n</pre> # convolution is a \"for loop\" # allows us to forward Linear layers efficiently over space"},{"location":"nn_training/notebooks/5_makemore_part5_cnn1/#makemore-part-5","title":"makemore: part 5\u00b6","text":""},{"location":"nn_training/notebooks/5_makemore_part5_cnn1/#performance-log","title":"performance log\u00b6","text":"<ul> <li>original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</li> <li>context: 3 -&gt; 8 (22K params): train 1.918, val 2.027</li> <li>flat -&gt; hierarchical (22K params): train 1.941, val 2.029</li> <li>fix bug in batchnorm: train 1.912, val 2.022</li> <li>scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993</li> </ul>"},{"location":"nn_training/notebooks/5_makemore_part5_cnn1/#next-time","title":"Next time:\u00b6","text":"<p>Why convolutions? Brief preview/hint</p>"},{"location":"nn_training/notebooks/nanogpt/gpt_dev/","title":"Gpt dev","text":"In\u00a0[25]: Copied! <pre># We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n!curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n</pre> # We always start with a dataset to train on. Let's download the tiny shakespeare dataset #!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt  !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt  <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 1089k  100 1089k    0     0  4896k      0 --:--:-- --:--:-- --:--:-- 4951k\n</pre> In\u00a0[26]: Copied! <pre># read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n</pre> # read it in to inspect it with open('input.txt', 'r', encoding='utf-8') as f:     text = f.read() In\u00a0[27]: Copied! <pre>print(\"length of dataset in characters: \", len(text))\n</pre> print(\"length of dataset in characters: \", len(text)) <pre>length of dataset in characters:  1115394\n</pre> In\u00a0[28]: Copied! <pre># let's look at the first 1000 characters\nprint(text[:1000])\n</pre> # let's look at the first 1000 characters print(text[:1000]) <pre>First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n</pre> In\u00a0[29]: Copied! <pre># here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n</pre> # here are all the unique characters that occur in this text chars = sorted(list(set(text))) vocab_size = len(chars) print(''.join(chars)) print(vocab_size) <pre> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n</pre> In\u00a0[30]: Copied! <pre># create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n</pre> # create a mapping from characters to integers stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string  print(encode(\"hii there\")) print(decode(encode(\"hii there\"))) <pre>[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n</pre> In\u00a0[31]: Copied! <pre># let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n</pre> # let's now encode the entire text dataset and store it into a torch.Tensor import torch # we use PyTorch: https://pytorch.org data = torch.tensor(encode(text), dtype=torch.long) print(data.shape, data.dtype) print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this <pre>torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n</pre> In\u00a0[32]: Copied! <pre># Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n</pre> # Let's now split up the data into train and validation sets n = int(0.9*len(data)) # first 90% will be train, rest val train_data = data[:n] val_data = data[n:] In\u00a0[33]: Copied! <pre>block_size = 8\ntrain_data[:block_size+1]\n</pre> block_size = 8 train_data[:block_size+1] Out[33]: <pre>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])</pre> In\u00a0[34]: Copied! <pre>x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n</pre> x = train_data[:block_size] y = train_data[1:block_size+1] for t in range(block_size):     context = x[:t+1]     target = y[t]     print(f\"when input is {context} the target: {target}\") <pre>when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n</pre> In\u00a0[35]: Copied! <pre>torch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n</pre> torch.manual_seed(1337) batch_size = 4 # how many independent sequences will we process in parallel? block_size = 8 # what is the maximum context length for predictions?  def get_batch(split):     # generate a small batch of data of inputs x and targets y     data = train_data if split == 'train' else val_data     ix = torch.randint(len(data) - block_size, (batch_size,))     x = torch.stack([data[i:i+block_size] for i in ix])     y = torch.stack([data[i+1:i+block_size+1] for i in ix])     return x, y  xb, yb = get_batch('train') print('inputs:') print(xb.shape) print(xb) print('targets:') print(yb.shape) print(yb)  print('----')  for b in range(batch_size): # batch dimension     for t in range(block_size): # time dimension         context = xb[b, :t+1]         target = yb[b,t]         print(f\"when input is {context.tolist()} the target: {target}\") <pre>inputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n</pre> In\u00a0[38]: Copied! <pre>print(yb) # our input to the transformer\n</pre> print(yb) # our input to the transformer <pre>tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n</pre> In\u00a0[39]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n</pre> import torch import torch.nn as nn from torch.nn import functional as F torch.manual_seed(1337)  class BigramLanguageModel(nn.Module):      def __init__(self, vocab_size):         super().__init__()         # each token directly reads off the logits for the next token from a lookup table         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)      def forward(self, idx, targets=None):          # idx and targets are both (B,T) tensor of integers         logits = self.token_embedding_table(idx) # (B,T,C)          if targets is None:             loss = None         else:             B, T, C = logits.shape             logits = logits.view(B*T, C)             targets = targets.view(B*T)             loss = F.cross_entropy(logits, targets)          return logits, loss      def generate(self, idx, max_new_tokens):         # idx is (B, T) array of indices in the current context         for _ in range(max_new_tokens):             # get the predictions             logits, loss = self(idx)             # focus only on the last time step             logits = logits[:, -1, :] # becomes (B, C)             # apply softmax to get probabilities             probs = F.softmax(logits, dim=-1) # (B, C)             # sample from the distribution             idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)             # append sampled index to the running sequence             idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)         return idx  m = BigramLanguageModel(vocab_size) logits, loss = m(xb, yb) print(logits.shape) print(loss)  print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  <pre>torch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n</pre> In\u00a0[40]: Copied! <pre># create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n</pre> # create a PyTorch optimizer optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) In\u00a0[41]: Copied! <pre>batch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n</pre> batch_size = 32 for steps in range(100): # increase number of steps for good results...      # sample a batch of data     xb, yb = get_batch('train')      # evaluate the loss     logits, loss = m(xb, yb)     optimizer.zero_grad(set_to_none=True)     loss.backward()     optimizer.step()  print(loss.item())  <pre>4.65630578994751\n</pre> In\u00a0[42]: Copied! <pre>print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n</pre> print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) <pre>oTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&amp;CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&amp;Ywbc;BLCUd&amp;vZINLIzkuTGZa\nD.?\n</pre> In\u00a0[\u00a0]: Copied! <pre># toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n</pre> # toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\" torch.manual_seed(42) a = torch.tril(torch.ones(3, 3)) a = a / torch.sum(a, 1, keepdim=True) b = torch.randint(0,10,(3,2)).float() c = a @ b print('a=') print(a) print('--') print('b=') print(b) print('--') print('c=') print(c) <pre>a=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n</pre> In\u00a0[\u00a0]: Copied! <pre># consider the following toy example:\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n</pre> # consider the following toy example:  torch.manual_seed(1337) B,T,C = 4,8,2 # batch, time, channels x = torch.randn(B,T,C) x.shape Out[\u00a0]: <pre>torch.Size([4, 8, 2])</pre> In\u00a0[\u00a0]: Copied! <pre># We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n</pre> # We want x[b,t] = mean_{i&lt;=t} x[b,i] xbow = torch.zeros((B,T,C)) for b in range(B):     for t in range(T):         xprev = x[b,:t+1] # (t,C)         xbow[b,t] = torch.mean(xprev, 0)  In\u00a0[\u00a0]: Copied! <pre># version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n</pre> # version 2: using matrix multiply for a weighted aggregation wei = torch.tril(torch.ones(T, T)) wei = wei / wei.sum(1, keepdim=True) xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C) torch.allclose(xbow, xbow2) Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n</pre> # version 3: use Softmax tril = torch.tril(torch.ones(T, T)) wei = torch.zeros((T,T)) wei = wei.masked_fill(tril == 0, float('-inf')) wei = F.softmax(wei, dim=-1) xbow3 = wei @ x torch.allclose(xbow, xbow3)  Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n</pre> # version 4: self-attention! torch.manual_seed(1337) B,T,C = 4,8,32 # batch, time, channels x = torch.randn(B,T,C)  # let's see a single Head perform self-attention head_size = 16 key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) value = nn.Linear(C, head_size, bias=False) k = key(x)   # (B, T, 16) q = query(x) # (B, T, 16) wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)  tril = torch.tril(torch.ones(T, T)) #wei = torch.zeros((T,T)) wei = wei.masked_fill(tril == 0, float('-inf')) wei = F.softmax(wei, dim=-1)  v = value(x) out = wei @ v #out = wei @ x  out.shape Out[\u00a0]: <pre>torch.Size([4, 8, 16])</pre> In\u00a0[\u00a0]: Copied! <pre>wei[0]\n</pre> wei[0] Out[\u00a0]: <pre>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)</pre> <p>Notes:</p> <ul> <li>Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.</li> <li>There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.</li> <li>Each example across batch dimension is of course processed completely independently and never \"talk\" to each other</li> <li>In an \"encoder\" attention block just delete the single line that does masking with <code>tril</code>, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.</li> <li>\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)</li> <li>\"Scaled\" attention additional divides <code>wei</code> by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</li> </ul> In\u00a0[\u00a0]: Copied! <pre>k = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n</pre> k = torch.randn(B,T,head_size) q = torch.randn(B,T,head_size) wei = q @ k.transpose(-2, -1) * head_size**-0.5 In\u00a0[\u00a0]: Copied! <pre>k.var()\n</pre> k.var() Out[\u00a0]: <pre>tensor(1.0449)</pre> In\u00a0[\u00a0]: Copied! <pre>q.var()\n</pre> q.var() Out[\u00a0]: <pre>tensor(1.0700)</pre> In\u00a0[\u00a0]: Copied! <pre>wei.var()\n</pre> wei.var() Out[\u00a0]: <pre>tensor(1.0918)</pre> In\u00a0[\u00a0]: Copied! <pre>torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n</pre> torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) Out[\u00a0]: <pre>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])</pre> In\u00a0[\u00a0]: Copied! <pre>torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n</pre> torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot Out[\u00a0]: <pre>tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])</pre> In\u00a0[\u00a0]: Copied! <pre>class LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n</pre> class LayerNorm1d: # (used to be BatchNorm1d)    def __init__(self, dim, eps=1e-5, momentum=0.1):     self.eps = eps     self.gamma = torch.ones(dim)     self.beta = torch.zeros(dim)    def __call__(self, x):     # calculate the forward pass     xmean = x.mean(1, keepdim=True) # batch mean     xvar = x.var(1, keepdim=True) # batch variance     xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance     self.out = self.gamma * xhat + self.beta     return self.out    def parameters(self):     return [self.gamma, self.beta]  torch.manual_seed(1337) module = LayerNorm1d(100) x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors x = module(x) x.shape Out[\u00a0]: <pre>torch.Size([32, 100])</pre> In\u00a0[\u00a0]: Copied! <pre>x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n</pre> x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs Out[\u00a0]: <pre>(tensor(0.1469), tensor(0.8803))</pre> In\u00a0[\u00a0]: Copied! <pre>x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n</pre> x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features Out[\u00a0]: <pre>(tensor(-9.5367e-09), tensor(1.0000))</pre> In\u00a0[\u00a0]: Copied! <pre># French to English translation example:\n\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les r\u00e9seaux de neurones sont g\u00e9niaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n</pre> # French to English translation example:  # &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt; # les r\u00e9seaux de neurones sont g\u00e9niaux!  neural networks are awesome! In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n</pre> import torch import torch.nn as nn from torch.nn import functional as F  # hyperparameters batch_size = 16 # how many independent sequences will we process in parallel? block_size = 32 # what is the maximum context length for predictions? max_iters = 5000 eval_interval = 100 learning_rate = 1e-3 device = 'cuda' if torch.cuda.is_available() else 'cpu' eval_iters = 200 n_embd = 64 n_head = 4 n_layer = 4 dropout = 0.0 # ------------  torch.manual_seed(1337)  # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open('input.txt', 'r', encoding='utf-8') as f:     text = f.read()  # here are all the unique characters that occur in this text chars = sorted(list(set(text))) vocab_size = len(chars) # create a mapping from characters to integers stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for i,ch in enumerate(chars) } encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string  # Train and test splits data = torch.tensor(encode(text), dtype=torch.long) n = int(0.9*len(data)) # first 90% will be train, rest val train_data = data[:n] val_data = data[n:]  # data loading def get_batch(split):     # generate a small batch of data of inputs x and targets y     data = train_data if split == 'train' else val_data     ix = torch.randint(len(data) - block_size, (batch_size,))     x = torch.stack([data[i:i+block_size] for i in ix])     y = torch.stack([data[i+1:i+block_size+1] for i in ix])     x, y = x.to(device), y.to(device)     return x, y  @torch.no_grad() def estimate_loss():     out = {}     model.eval()     for split in ['train', 'val']:         losses = torch.zeros(eval_iters)         for k in range(eval_iters):             X, Y = get_batch(split)             logits, loss = model(X, Y)             losses[k] = loss.item()         out[split] = losses.mean()     model.train()     return out  class Head(nn.Module):     \"\"\" one head of self-attention \"\"\"      def __init__(self, head_size):         super().__init__()         self.key = nn.Linear(n_embd, head_size, bias=False)         self.query = nn.Linear(n_embd, head_size, bias=False)         self.value = nn.Linear(n_embd, head_size, bias=False)         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))          self.dropout = nn.Dropout(dropout)      def forward(self, x):         B,T,C = x.shape         k = self.key(x)   # (B,T,C)         q = self.query(x) # (B,T,C)         # compute attention scores (\"affinities\")         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)         wei = F.softmax(wei, dim=-1) # (B, T, T)         wei = self.dropout(wei)         # perform the weighted aggregation of the values         v = self.value(x) # (B,T,C)         out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)         return out  class MultiHeadAttention(nn.Module):     \"\"\" multiple heads of self-attention in parallel \"\"\"      def __init__(self, num_heads, head_size):         super().__init__()         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])         self.proj = nn.Linear(n_embd, n_embd)         self.dropout = nn.Dropout(dropout)      def forward(self, x):         out = torch.cat([h(x) for h in self.heads], dim=-1)         out = self.dropout(self.proj(out))         return out  class FeedFoward(nn.Module):     \"\"\" a simple linear layer followed by a non-linearity \"\"\"      def __init__(self, n_embd):         super().__init__()         self.net = nn.Sequential(             nn.Linear(n_embd, 4 * n_embd),             nn.ReLU(),             nn.Linear(4 * n_embd, n_embd),             nn.Dropout(dropout),         )      def forward(self, x):         return self.net(x)  class Block(nn.Module):     \"\"\" Transformer block: communication followed by computation \"\"\"      def __init__(self, n_embd, n_head):         # n_embd: embedding dimension, n_head: the number of heads we'd like         super().__init__()         head_size = n_embd // n_head         self.sa = MultiHeadAttention(n_head, head_size)         self.ffwd = FeedFoward(n_embd)         self.ln1 = nn.LayerNorm(n_embd)         self.ln2 = nn.LayerNorm(n_embd)      def forward(self, x):         x = x + self.sa(self.ln1(x))         x = x + self.ffwd(self.ln2(x))         return x  # super simple bigram model class BigramLanguageModel(nn.Module):      def __init__(self):         super().__init__()         # each token directly reads off the logits for the next token from a lookup table         self.token_embedding_table = nn.Embedding(vocab_size, n_embd)         self.position_embedding_table = nn.Embedding(block_size, n_embd)         self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])         self.ln_f = nn.LayerNorm(n_embd) # final layer norm         self.lm_head = nn.Linear(n_embd, vocab_size)      def forward(self, idx, targets=None):         B, T = idx.shape          # idx and targets are both (B,T) tensor of integers         tok_emb = self.token_embedding_table(idx) # (B,T,C)         pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)         x = tok_emb + pos_emb # (B,T,C)         x = self.blocks(x) # (B,T,C)         x = self.ln_f(x) # (B,T,C)         logits = self.lm_head(x) # (B,T,vocab_size)          if targets is None:             loss = None         else:             B, T, C = logits.shape             logits = logits.view(B*T, C)             targets = targets.view(B*T)             loss = F.cross_entropy(logits, targets)          return logits, loss      def generate(self, idx, max_new_tokens):         # idx is (B, T) array of indices in the current context         for _ in range(max_new_tokens):             # crop idx to the last block_size tokens             idx_cond = idx[:, -block_size:]             # get the predictions             logits, loss = self(idx_cond)             # focus only on the last time step             logits = logits[:, -1, :] # becomes (B, C)             # apply softmax to get probabilities             probs = F.softmax(logits, dim=-1) # (B, C)             # sample from the distribution             idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)             # append sampled index to the running sequence             idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)         return idx  model = BigramLanguageModel() m = model.to(device) # print the number of parameters in the model print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')  # create a PyTorch optimizer optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  for iter in range(max_iters):      # every once in a while evaluate the loss on train and val sets     if iter % eval_interval == 0 or iter == max_iters - 1:         losses = estimate_loss()         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")      # sample a batch of data     xb, yb = get_batch('train')      # evaluate the loss     logits, loss = model(xb, yb)     optimizer.zero_grad(set_to_none=True)     loss.backward()     optimizer.step()  # generate from the model context = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))  <pre>0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5090, val loss 2.5058\nstep 300: train loss 2.4198, val loss 2.4340\nstep 400: train loss 2.3503, val loss 2.3567\nstep 500: train loss 2.2970, val loss 2.3136\nstep 600: train loss 2.2410, val loss 2.2506\nstep 700: train loss 2.2062, val loss 2.2198\nstep 800: train loss 2.1638, val loss 2.1871\nstep 900: train loss 2.1232, val loss 2.1494\nstep 1000: train loss 2.1020, val loss 2.1293\nstep 1100: train loss 2.0704, val loss 2.1196\nstep 1200: train loss 2.0382, val loss 2.0798\nstep 1300: train loss 2.0249, val loss 2.0640\nstep 1400: train loss 1.9922, val loss 2.0354\nstep 1500: train loss 1.9707, val loss 2.0308\nstep 1600: train loss 1.9614, val loss 2.0474\nstep 1700: train loss 1.9393, val loss 2.0130\nstep 1800: train loss 1.9070, val loss 1.9943\nstep 1900: train loss 1.9057, val loss 1.9871\nstep 2000: train loss 1.8834, val loss 1.9954\nstep 2100: train loss 1.8719, val loss 1.9758\nstep 2200: train loss 1.8582, val loss 1.9623\nstep 2300: train loss 1.8546, val loss 1.9517\nstep 2400: train loss 1.8410, val loss 1.9476\nstep 2500: train loss 1.8167, val loss 1.9455\nstep 2600: train loss 1.8263, val loss 1.9401\nstep 2700: train loss 1.8108, val loss 1.9340\nstep 2800: train loss 1.8040, val loss 1.9247\nstep 2900: train loss 1.8044, val loss 1.9304\nstep 3000: train loss 1.7963, val loss 1.9242\nstep 3100: train loss 1.7687, val loss 1.9147\nstep 3200: train loss 1.7547, val loss 1.9102\nstep 3300: train loss 1.7557, val loss 1.9037\nstep 3400: train loss 1.7547, val loss 1.8946\nstep 3500: train loss 1.7385, val loss 1.8968\nstep 3600: train loss 1.7260, val loss 1.8914\nstep 3700: train loss 1.7257, val loss 1.8808\nstep 3800: train loss 1.7204, val loss 1.8919\nstep 3900: train loss 1.7215, val loss 1.8788\nstep 4000: train loss 1.7146, val loss 1.8639\nstep 4100: train loss 1.7095, val loss 1.8724\nstep 4200: train loss 1.7079, val loss 1.8707\nstep 4300: train loss 1.7035, val loss 1.8502\nstep 4400: train loss 1.7043, val loss 1.8693\nstep 4500: train loss 1.6914, val loss 1.8522\nstep 4600: train loss 1.6853, val loss 1.8357\nstep 4700: train loss 1.6862, val loss 1.8483\nstep 4800: train loss 1.6671, val loss 1.8434\nstep 4900: train loss 1.6736, val loss 1.8415\nstep 4999: train loss 1.6635, val loss 1.8226\n\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\n\nJOHN MARGARET:\nThan up I wark, what out, I ever of and love,\none these do sponce, vois I me;\nBut my pray sape to ries all to the not erralied in may.\n\nBENVOLIO:\nTo spits as stold's bewear I would and say mesby all\non sworn make he anough\nAs cousins the solle, whose be my conforeful may lie them yet\nnobe allimely untraled to be thre I say be,\nNotham a brotes theme an make come,\nAnd that his reach to the duke ento\nthe grmeants bell! and now there king-liff-or grief?\n\nGLOUCESTER:\nAll the bettle dreene, for To his like thou thron!\n\nMENENIUS:\nThen, if I knom her all.\nMy lord, but terruly friend\nRish of the ploceiness and wilt tends sure?\nIs you knows a fasir wead\nThat with him my spaut,\nI shall not tas where's not, becomity; my coulds sting,\nthen the wit be dong to tyget our hereefore,\nWho strop me, mend here, if agains, bitten, thy lack.\nThe but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\nHow the enclady: It you and how,\nI am in him, And ladderle:\nTheir hand whose wife, it my hithre,\nRoman and where sposs gives'd you.\n\nTROMIOLANUS:\nBut livants you great, I shom mistrot come, for to she to lot\nfor smy to men ventry mehus. Gazise;\nFull't were some the cause, and stouch set,\nOr promises, which a kingsasted to your gove them; and sterrer,\nAnd that wae love him.\n\nBRUTUS:\nYou shape with these sweet.\n\nCORTENGONO:\nLo, where 'twon elmes, 'morth young agres;\nSir, azavoust to striel accurded we missery sets crave.\n\nANGOLUM:\nFor is Henry to have gleise the dreason\nThat I ant shorfold wefth their servy in enscy.\n\nISABELLA:\nO, I better you eyse such formfetrews.\n\nBUCKINGHARENT:\nQead my lightle this righanneds flase them\nWam which an take was our some pleasurs,\nLovisoname to me, then fult me?--have it?\n\nHENRY BOLINGBROY:\nThat wha\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"nn_training/notebooks/nanogpt/gpt_dev/#building-a-gpt","title":"Building a GPT\u00b6","text":""},{"location":"nn_training/notebooks/nanogpt/gpt_dev/#the-mathematical-trick-in-self-attention","title":"The mathematical trick in self-attention\u00b6","text":""},{"location":"nn_training/notebooks/nanogpt/gpt_dev/#full-finished-code-for-reference","title":"Full finished code, for reference\u00b6","text":"<p>You may want to refer directly to the git repo instead though.</p>"},{"location":"reinforcement/10_offline_rl/","title":"10. Offline Reinforcement Learning","text":""},{"location":"reinforcement/10_offline_rl/#chapter-10-batch-offline-rl-policy-evaluation-optimization","title":"Chapter 10: Batch / Offline RL Policy Evaluation &amp; Optimization","text":"<p>Learning from the Past</p> <ul> <li>Learning from Past Human Demonstrations: Imitation Learning</li> <li>Learning from Past Human Preferences: RLHF and DPO</li> <li>Learning from Past Decisions and Actions: Offline RL</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-reinforcement-learning-a-different-approach","title":"Offline Reinforcement Learning: A Different Approach","text":"<p>Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.</p> <p>In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.</p> <p>Why Can\u2019t We Just Use Q-Learning?</p> <ul> <li>Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values</li> <li>But deadly triad of bootstrapping, function approximation and off policy, and can fail</li> </ul>"},{"location":"reinforcement/10_offline_rl/#batch-policy-evaluation-estimating-the-performance-of-a-policy","title":"Batch Policy Evaluation: Estimating the Performance of a Policy","text":"<p>Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.</p> <ol> <li> <p>Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.</p> <p>Specifically, it learns two main components from data: a reward function \\(\\hat{r}(s,a)\\) and transition dynamics \\(\\hat{P}(s' \\mid s,a)\\). These are learned via supervised learning on the offline dataset \\(D\\) of transitions collected by some behavior policy \\(\\pi_b\\). For example, \\(\\hat{r}(s,a)\\) can be trained by regression to predict the observed reward given state \\(s\\) and action \\(a\\), and \\(\\hat{P}(s' \\mid s,a)\\) can be fit to predict the next-state \\(s'\\) (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in \\(D\\) under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to \u201clearn the environment\u2019s rules\". Once learned, this model \\(\\hat{\\mathcal{M}} = (\\hat{P}, \\hat{r})\\) serves as a proxy for the real environment, which we can use for evaluating any policy \\(\\pi\\) without further real experience.</p> <p>It\u2019s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics \\(\\hat{P}\\) will be reliable only in regions covered by \\(D\\); if \\(\\pi\\) later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).</p> </li> <li> <p>Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.</p> </li> <li> <p>Importance Sampling:     This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy\u2019s value \u2013 assuming coverage (i.e. the target policy doesn\u2019t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithmic-outline-offline-policy-evaluation-via-model","title":"Algorithmic Outline - Offline Policy Evaluation via Model:","text":"<ol> <li> <p>Input: offline dataset \\(D\\) of transitions (from behavior \\(\\pi_b\\)), a policy \\(\\pi\\) to evaluate, discount \\(\\gamma\\).</p> </li> <li> <p>Model Learning: Fit \\(\\hat{P}(s'|s,a)\\) and \\(\\hat{r}(s,a)\\) using \\(D\\) (e.g. maximum likelihood estimation for dynamics, regression for rewards).</p> </li> <li> <p>Policy Evaluation: Initialize \\(V(s)=0\\) for all states (or some initial guess).</p> </li> <li> <p>Loop (Bellman backups using \\(\\hat{P},\\hat{r}\\)): For each state \\(s\\) in the state space (or a representative set of states):</p> </li> <li> <p>Compute \\(\\hat{R}^\\pi(s) = \\sum_a \\pi(a|s)\\hat{r}(s,a)\\).</p> </li> <li> <p>Compute \\(V_{\\text{new}}(s) = \\hat{R}^\\pi(s) + \\gamma \\sum_{s'} \\hat{P}^\\pi(s'\\mid s),V(s')\\).</p> </li> <li> <p>Update \\(V \\leftarrow V_{\\text{new}}\\) and repeat until convergence (the changes in \\(V\\) are below a threshold).</p> </li> <li> <p>Output: \\(V(s)\\) for states of interest (e.g. the estimated value of \\(\\pi\\) under the initial state distribution \\(S_0\\) can be obtained by \\(\\mathbb{E}_{s_0\\sim S_0}[V(s_0)]\\)).</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithm-3-fitted-q-evaluation-fqe-pi-c","title":"Algorithm 3 Fitted Q Evaluation: FQE \\((\\pi, c)\\)","text":"<p>Input: Dataset \\(\\mathcal{D} = \\{(x_i, a_i, x'_i, c_i)\\}_{i=1}^n \\sim \\pi_D\\). Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy \\(\\pi\\) by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state \\(s_i\\), taking action \\(a_i\\), and then following policy \\(\\pi\\) thereafter.At each iteration, we construct a Bellman target:</p> \\[\\tilde{Q}^\\pi(s_i, a_i) = c_i + \\gamma V_\\theta^\\pi(s_{i+1}) \\] <p>where</p> \\[ V_\\theta^\\pi(s_{i+1}) = Q_\\theta^\\pi(s_{i+1}, \\pi(s_{i+1})). \\] <p>The Q-function is parameterized by \\(\\theta\\) (e.g., a neural network), and is learned by solving a supervised regression problem:</p> \\[\\arg\\min_\\theta \\sum_i \\Big( Q_\\theta^\\pi(s_i, a_i) - \\tilde{Q}^\\pi(s_i, a_i) \\Big)^2\\] <p>This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy \\(\\pi\\) under the data distribution induced by \\(\\pi_D\\).</p> <p>Function class \\(F\\) (Let's assume we use a DNN for F). Policy </p> <p>\\(\\pi\\) to be evaluated. 1: Initialize \\(Q_0 \\in F\\) randomly 2: for \\(k = 1, 2, \\dots, K\\) do 3: \\(\\quad\\) Compute target \\(y_i = c_i + \\gamma Q_{k-1}(x'_i, \\pi(x'_i)) \\quad \\forall i\\) 4: \\(\\quad\\) Build training set \\(\\tilde{\\mathcal{D}}_k = \\{(x_i, a_i), y_i\\}_{i=1}^n\\) 5: \\(\\quad\\) Solve a supervised learning problem:  6: end for Output: \\(\\hat{C}^\\pi(x) = Q_K(x, \\pi(x)) \\quad \\forall x\\)</p>"},{"location":"reinforcement/10_offline_rl/#what-is-different-vs-dqn","title":"What is different vs DQN?","text":"<p>DQN learns an optimal policy by interacting with the environment, while FQE evaluates a fixed policy using a fixed offline dataset.</p> Aspect FQE (Fitted Q Evaluation) DQN (Deep Q-Network) Goal Policy evaluation Policy optimization / control Policy Fixed target policy \\(\\pi\\) Implicitly learned via \\(\\max_a Q(s,a)\\) Data Offline, fixed dataset \\(\\mathcal{D}\\) Online, collected during training Bellman target \\(c + \\gamma Q(s', \\pi(s'))\\) \\(r + \\gamma \\max_a Q(s', a)\\) Action at next state From given policy \\(\\pi\\) Greedy over Q-values Exploration None Required (e.g. \\(\\epsilon\\)-greedy) Dataset changes? \u274c No \u2705 Yes Off-policy instability Low High Convergence guarantees Yes (tabular / linear) No (with function approximation)"},{"location":"reinforcement/10_offline_rl/#1-no-maximization-bias-in-fqe","title":"1. No maximization bias in FQE","text":"<ul> <li>DQN suffers from overestimation bias</li> <li>FQE does pure regression, no bootstrapped max</li> </ul>"},{"location":"reinforcement/10_offline_rl/#2-stability","title":"2. Stability","text":"<ul> <li>FQE \u2248 supervised learning  </li> <li>DQN \u2248 bootstrapped + non-stationary targets</li> </ul>"},{"location":"reinforcement/10_offline_rl/#3-offline-vs-online","title":"3. Offline vs Online","text":"<ul> <li>FQE cannot improve the policy  </li> <li>DQN must interact with environment</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-policy-learning-optimization","title":"Offline Policy Learning / Optimization","text":"<p>Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.</p> <ol> <li> <p>Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.</p> </li> <li> <p>Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.</p> </li> <li> <p>Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#challenges-in-offline-policy-optimization","title":"Challenges in Offline Policy Optimization","text":"<ol> <li> <p>Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.</p> </li> <li> <p>Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.</p> </li> <li> <p>Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.</p> </li> </ol> <p>Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.</p>"},{"location":"reinforcement/10_offline_rl/#mental-map","title":"Mental Map","text":"<pre><code>                 Offline / Batch Reinforcement Learning\n        Goal: Learn and evaluate policies from fixed historical data\n           when exploration is unsafe, expensive, or impossible\n                                \u2502\n                                \u25bc\n              Why Online RL Is Not Always Feasible\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Exploration can be dangerous (healthcare, driving, robotics)\u2502\n \u2502 Data already exists from past decisions                     \u2502\n \u2502 Real systems cannot reset or freely experiment              \u2502\n \u2502 We must learn without interacting with the environment      \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Offline RL vs Standard RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Standard RL:                                                \u2502\n \u2502  \u2013 Collect data with current policy                         \u2502\n \u2502  \u2013 Explore \u2192 improve \u2192 repeat                               \u2502\n \u2502                                                             \u2502\n \u2502 Offline RL:                                                 \u2502\n \u2502  \u2013 Fixed dataset D from behavior policy \u03c0_b                 \u2502\n \u2502  \u2013 No new interaction allowed                               \u2502\n \u2502  \u2013 Must generalize only from observed data                  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             Why \u201cJust Use Q-Learning\u201d Fails Offline\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Q-learning is off-policy \u2014 but not offline-safe             \u2502\n \u2502 Deadly triad:                                               \u2502\n \u2502   \u2022 Bootstrapping                                           \u2502\n \u2502   \u2022 Function approximation                                  \u2502\n \u2502   \u2022 Off-policy learning                                     \u2502\n \u2502 Leads to divergence &amp; overestimation                        \u2502\n \u2502 Especially severe with distribution mismatch                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Offline RL Decomposed into Two Core Problems\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 1. Policy Evaluation (OPE)    \u2502 2. Policy Optimization      \u2502\n \u2502    \u201cHow good is this policy?\u201d \u2502    \u201cHow can we improve it?\u201d \u2502\n \u2502    Without running it         \u2502    Without new data         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Batch / Offline Policy Evaluation (OPE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Estimate V^\u03c0 or J(\u03c0) using only dataset D                   \u2502\n \u2502 Three major approaches:                                     \u2502\n \u2502  1. Model-based evaluation                                  \u2502\n \u2502  2. Model-free evaluation (FQE)                             \u2502\n \u2502  3. Importance Sampling                                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        1. Model-Based Offline Policy Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn a model from data:                                    \u2502\n \u2502   \u2022 Reward model: r\u0302(s,a)                                    \u2502\n \u2502   \u2022 Transition model: P\u0302(s'|s,a)                             \u2502\n \u2502 Treat batch data as supervised learning                     \u2502\n \u2502 Then simulate policy \u03c0 inside learned model                 \u2502\n \u2502 Use Bellman backups on (P\u0302, r\u0302)                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Model-Based OPE: Key Limitation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Model is only reliable where data exists                    \u2502\n \u2502 Policy visiting unseen states/actions \u2192 extrapolation error \u2502\n \u2502 Model hallucination \u2192 highly biased value estimates         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Evaluate a *fixed policy* \u03c0                                 \u2502\n \u2502 Learn Q^\u03c0(s,a) from offline data via regression             \u2502\n \u2502 Bellman target:                                             \u2502\n \u2502   y = c + \u03b3 Q(s', \u03c0(s'))                                    \u2502\n \u2502 Pure supervised learning loop                               \u2502\n \u2502 Stable compared to Q-learning / DQN                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             FQE vs DQN (Key Insight)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 DQN                         \u2502 FQE                         \u2502\n \u2502 Learns optimal policy       \u2502 Evaluates fixed policy      \u2502\n \u2502 Uses max over actions       \u2502 Uses given \u03c0(s')            \u2502\n \u2502 Online data collection      \u2502 Fully offline               \u2502\n \u2502 Overestimation bias         \u2502 No max \u2192 more stable        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        3. Importance Sampling (IS) Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat OPE as statistical estimation                         \u2502\n \u2502 Reweight trajectories by \u03c0 / \u03c0_b                            \u2502\n \u2502 Unbiased if coverage holds                                  \u2502\n \u2502 Severe variance for long horizons or policy mismatch        \u2502\n \u2502 Variants:                                                   \u2502\n \u2502   \u2022 Per-decision IS                                         \u2502\n \u2502   \u2022 Weighted IS                                             \u2502\n \u2502   \u2022 Doubly robust estimators                                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Offline Policy Optimization\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Goal: improve policy using only dataset D                   \u2502\n \u2502 Model-free: Fitted Q Iteration (FQI)                        \u2502\n \u2502 Model-based: planning inside learned model                  \u2502\n \u2502 Core challenge: distribution shift                          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        The Central Problem: Distribution Mismatch\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learned policy chooses actions unseen in data               \u2502\n \u2502 Q-values extrapolate \u2192 overly optimistic                    \u2502\n \u2502 Performance collapses at deployment                         \u2502\n \u2502 Offline RL \u2260 just off-policy RL                             \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Conservative / Pessimistic Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Assume unknown actions are risky                            \u2502\n \u2502 Penalize state-action pairs with low data support           \u2502\n \u2502 Prefer policies close to behavior policy                    \u2502\n \u2502 Examples (conceptually):                                    \u2502\n \u2502   \u2022 Conservative Q-Learning (CQL)                           \u2502\n \u2502   \u2022 Regularization toward \u03c0_b                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Key Challenges in Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Coverage / overlap requirement                              \u2502\n \u2502 Model misspecification                                      \u2502\n \u2502 Value overestimation                                        \u2502\n \u2502 Bias\u2013variance tradeoffs                                     \u2502\n \u2502 Safety vs optimality                                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n               Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Offline RL learns entirely from past experience             \u2502\n \u2502 Policy evaluation is foundational before optimization       \u2502\n \u2502 Model-based, FQE, and IS provide OPE tools                  \u2502\n \u2502 Main risk: distribution shift &amp; extrapolation               \u2502\n \u2502 Conservative methods trade performance for safety           \u2502\n \u2502 Offline RL is essential for real-world decision systems     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/11_fast_rl/","title":"11. Data-Efficient Reinforcement Learning","text":""},{"location":"reinforcement/11_fast_rl/#chapter-11-data-efficient-reinforcement-learning-bandit-foundations","title":"Chapter 11: Data-Efficient Reinforcement Learning \u2014 Bandit Foundations","text":"<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>"},{"location":"reinforcement/11_fast_rl/#the-multi-armed-bandit-model","title":"The Multi-Armed Bandit Model","text":"<p>A multi-armed bandit is defined as a tuple \\((\\mathcal{A}, \\mathcal{R})\\), where:</p> <ul> <li>\\(\\mathcal{A} = \\{a_1, \\dots, a_m\\}\\) is a known, finite set of actions (arms),</li> <li>\\(R_a(r) = \\mathbb{P}[r \\mid a]\\) is an unknown probability distribution over rewards for each action.</li> <li>there is no \"state\".</li> </ul> <p>At each timestep \\(t\\), the agent:</p> <ol> <li>Chooses an action \\(a_t \\in \\mathcal{A}\\),</li> <li>Receives a stochastic reward \\(r_t \\sim R_{a_t}\\).</li> </ol> <p>Goal: Maximize cumulative reward: </p> <p>This simple model embodies the core RL challenges\u2014particularly exploration vs. exploitation\u2014in an isolated setting.</p>"},{"location":"reinforcement/11_fast_rl/#evaluating-algorithms-regret-framework","title":"Evaluating Algorithms: Regret Framework","text":"<p>Regret: </p> <ul> <li>\\(Q(a) = \\mathbb{E}[r \\mid a]\\) be the expected reward for action \\(a\\),</li> <li>\\(a^* = \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\),</li> <li>Optimal Value \\(V^* = Q(a^*)\\)</li> </ul> <p>Then regret is the opportunity loss for one step:  </p> <p>Total Regret is the total opportunity loss: Total regret over \\(T\\) timesteps</p> <p>  Where:</p> <ul> <li>\\(N_T(a)\\): Number of times arm \\(a\\) is selected by time \\(T\\),</li> <li>\\(\\Delta_a = V^* - Q(a)\\): Suboptimality gap.</li> </ul> <p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>"},{"location":"reinforcement/11_fast_rl/#baseline-approaches-and-their-regret","title":"Baseline Approaches and Their Regret","text":""},{"location":"reinforcement/11_fast_rl/#greedy-algorithm","title":"Greedy Algorithm","text":"\\[ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\cdot \\mathbb{1}(a_\\tau = a) \\] \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\hat{Q}_t(a) \\]"},{"location":"reinforcement/11_fast_rl/#key-insight","title":"Key Insight:","text":"<ul> <li>Exploits current estimates.</li> <li>May lock onto suboptimal arms due to early bad luck.</li> <li>Linear regret in expectation.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#example","title":"Example:","text":"<p>If \\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\\), and the first sample of \\(a_1\\) yields 0, the greedy agent may ignore it indefinitely.</p>"},{"location":"reinforcement/11_fast_rl/#varepsilon-greedy-algorithm","title":"\\(\\varepsilon\\)-Greedy Algorithm","text":"<p>At each timestep:</p> <ul> <li>With probability \\(1 - \\varepsilon\\): exploit (\\(\\arg\\max \\hat{Q}_t(a)\\)),</li> <li>With probability \\(\\varepsilon\\): explore uniformly at random.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#performance","title":"Performance:","text":"<ul> <li>Guarantees exploration.</li> <li>Linear regret unless \\(\\varepsilon\\) decays over time.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#decaying-varepsilon-greedy","title":"Decaying \\(\\varepsilon\\)-Greedy","text":"<p>Allows \\(\\varepsilon_t \\to 0\\) as \\(t \\to \\infty\\), enabling convergence.</p>"},{"location":"reinforcement/11_fast_rl/#optimism-in-the-face-of-uncertainty","title":"Optimism in the Face of Uncertainty","text":"<p>Prefer actions with uncertain but potentially high value:</p> <p>Why? Two possible outcomes:</p> <ol> <li> <p>Getting a high reward:    If the arm really has a high mean reward.</p> </li> <li> <p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p> </li> </ol> <p>Algorithm: </p> <ul> <li> <p>Estimate an upper confidence bound \\(U_t(a)\\) for each action value, such that   \\(Q(a) \\le U_t(a)\\) with high probability.</p> </li> <li> <p>This depends on the number of times \\(N_t(a)\\) action \\(a\\) has been selected.</p> </li> <li> <p>Select the action maximizing the Upper Confidence Bound (UCB):</p> </li> </ul> \\[a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ U_t(a) \\right]\\] <p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards \\(X_i \\in [0,1]\\),  </p> <p>Setting the right-hand side equal to \\(\\delta\\) and solving for \\(u\\),  Here, \\(\\delta\\) is the failure probability, and the confidence interval holds with probability at least \\(1 - \\delta\\). This means that, with probability at least \\(1 - \\delta\\),  </p> \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ \\hat{Q}_t(a) + \\text{UCB}_t(a) \\right] \\]"},{"location":"reinforcement/11_fast_rl/#ucb1-algorithm","title":"UCB1 Algorithm","text":"\\[ \\text{UCB}_t(a) = \\hat{Q}_t(a) + \\sqrt{\\frac{2 \\log \\frac{1}{\\delta} }{N_t(a)}} \\] <ul> <li>where \\(\\hat{Q}_t(a)\\) is empirical average</li> <li>\\(N_t(a)\\) is number of samples of \\(a\\) after \\(t\\) timesteps.</li> <li>Provable sublinear regret.</li> <li>Balances estimated value and exploration bonus.</li> </ul> <p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p> <p>1: Initialize for each arm \\(a \\in \\mathcal{A}\\):  \\(\\quad N(a) \\leftarrow 0,\\;\\; \\hat{Q}(a) \\leftarrow 0\\) 2: Warm start (sample each arm once): 3: for each arm \\(a \\in \\mathcal{A}\\) do 4: \\(\\quad\\) Pull arm \\(a\\), observe reward \\(r \\in [0,1]\\) 5: \\(\\quad N(a) \\leftarrow 1\\) 6: \\(\\quad \\hat{Q}(a) \\leftarrow r\\) 7: end for 8: Set \\(t \\leftarrow |\\mathcal{A}|\\)</p> <p>9: for \\(t = |\\mathcal{A}|+1, |\\mathcal{A}|+2, \\dots\\) do 10: \\(\\quad\\) Compute UCB for each arm: \\(\\quad \\mathrm{UCB}_t(a) = \\hat{Q}(a) + \\sqrt{\\frac{2\\log t}{N(a)}}\\)</p> <p>11: \\(\\quad\\) Select action:\\(\\quad a_t \\leftarrow \\arg\\max_{a \\in \\mathcal{A}} \\mathrm{UCB}_t(a)\\)</p> <p>12: \\(\\quad\\) Pull arm \\(a_t\\), observe reward \\(r_t\\)</p> <p>13: \\(\\quad\\) Update count: \\(\\quad N(a_t) \\leftarrow N(a_t) + 1\\)</p> <p>14: \\(\\quad\\) Update empirical mean (incremental): </p> <p>15: end for</p>"},{"location":"reinforcement/11_fast_rl/#119-optimistic-initialization-in-greedy-bandit-algorithms","title":"11.9 Optimistic Initialization in Greedy Bandit Algorithms","text":"<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>"},{"location":"reinforcement/11_fast_rl/#motivation","title":"Motivation","text":"<p>Greedy algorithms, by default, select actions with the highest estimated value:</p> \\[ a_t = \\arg\\max_a \\hat{Q}_t(a) \\] <p>If these \\(\\hat{Q}_t(a)\\) estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>"},{"location":"reinforcement/11_fast_rl/#algorithmic-details","title":"Algorithmic Details","text":"<p>We initialize:</p> <ul> <li>\\(\\hat{Q}_0(a) = Q_{\\text{init}}\\) for all \\(a \\in \\mathcal{A}\\), where \\(Q_{\\text{init}}\\) is set higher than any reasonable expected reward (e.g., \\(Q_{\\text{init}} = 1\\) if rewards are bounded in \\([0, 1]\\)).</li> <li>\\(N(a) = 1\\) to ensure initial update is well-defined.</li> </ul> <p>Then we update action values using an incremental Monte Carlo estimate:</p> \\[ \\hat{Q}_{t}(a_t) = \\hat{Q}_{t-1}(a_t) + \\frac{1}{N_t(a_t)} \\left( r_t - \\hat{Q}_{t-1}(a_t) \\right) \\] <p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p> <ul> <li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li> <li>Simple to implement: No need for tuning \\(\\varepsilon\\) or computing uncertainty estimates.</li> <li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#key-design-considerations","title":"Key Design Considerations","text":"<ul> <li>How optimistic is optimistic enough?   If \\(Q_{\\text{init}}\\) is not much larger than the true values, the agent may not explore effectively.</li> <li>What if \\(Q_{\\text{init}}\\) is too high?   Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#function-approximation","title":"Function Approximation","text":"<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>"},{"location":"reinforcement/11_fast_rl/#1110-theoretical-frameworks-regret-and-pac","title":"11.10 Theoretical Frameworks: Regret and PAC","text":""},{"location":"reinforcement/11_fast_rl/#regret-based-evaluation","title":"Regret-Based Evaluation","text":"<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p> <ul> <li>Many small mistakes (frequent near-optimal actions),</li> <li>A few large mistakes (infrequent but very suboptimal actions).</li> </ul> <p>Minimizing regret growth with \\(T\\) is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>"},{"location":"reinforcement/11_fast_rl/#probably-approximately-correct-pac-framework","title":"Probably Approximately Correct (PAC) Framework","text":"<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p> <p>An algorithm is \\((\\varepsilon, \\delta)\\)-PAC if, on each time step \\(t\\), it chooses an action \\(a_t\\) such that:</p> \\[ Q(a_t) \\ge Q(a^*) - \\varepsilon \\quad \\text{with probability at least } 1 - \\delta \\] <p>on all but a polynomial number of time steps (in \\(|\\mathcal{A}|\\), \\(1/\\varepsilon\\), \\(1/\\delta\\), etc). This ensures:</p> <ul> <li>The agent almost always behaves nearly optimally,</li> <li>With high probability, after a reasonable amount of time.</li> </ul> <p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>"},{"location":"reinforcement/11_fast_rl/#comparing-exploration-strategies","title":"Comparing Exploration Strategies","text":"Strategy Regret Behavior Notes Greedy Linear No exploration mechanism Constant \\(\\varepsilon\\)-greedy Linear Fixed chance of exploring Decaying \\(\\varepsilon\\)-greedy Sublinear (if tuned) Requires prior knowledge of reward gaps Optimistic Initialization Sublinear (if optimistic enough) Simple, effective in tabular settings <p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>"},{"location":"reinforcement/11_fast_rl/#bayesian-bandits","title":"Bayesian Bandits","text":"<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in \\([0,1]\\)). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>"},{"location":"reinforcement/11_fast_rl/#key-idea-maintain-beliefs-over-arm-reward-distributions","title":"Key Idea: Maintain Beliefs Over Arm Reward Distributions","text":"<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter \\(\\\\phi_i\\) for arm \\(i\\). Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of \\(\\\\phi_i\\), representing our uncertainty.</p>"},{"location":"reinforcement/11_fast_rl/#prior-and-posterior","title":"Prior and Posterior","text":"<ul> <li>Prior: Our initial belief about \\(\\\\phi_i\\) is encoded in a probability distribution \\(p(\\\\phi_i)\\).</li> <li>Data: After pulling arm \\(i\\) and observing reward \\(r_{i1}\\), we update our belief.</li> <li>Posterior: The new belief is computed using Bayes' rule:</li> </ul> \\[p(\\phi_i \\mid r_{i1}) = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ p(r_{i1}) } = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ \\int p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i)\\, d\\phi_i }\\] <p>This posterior becomes the new prior for future updates as more data arrives.</p>"},{"location":"reinforcement/11_fast_rl/#practical-considerations","title":"Practical Considerations","text":"<p>Computing the posterior \\(p(\\phi_i \\mid D)\\) (where \\(D\\) is the observed data for arm \\(i\\)) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p> <ul> <li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li> <li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li> </ul>"},{"location":"reinforcement/11_fast_rl/#why-use-bayesian-bandits","title":"Why Use Bayesian Bandits?","text":"<ul> <li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li> <li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li> <li>Captures uncertainty in a principled and statistically coherent manner.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#summary","title":"Summary","text":"<ul> <li>Bayesian bandits treat the reward-generating parameters \\(\\phi_i\\) as random variables.</li> <li>We maintain a posterior belief \\(p(\\phi_i \\mid D)\\) using Bayes' rule.</li> <li>When conjugate priors are used, analytical updates are possible.</li> <li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#thompson-sampling","title":"Thompson Sampling:","text":"<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p> <p>This approach is also known as probability matching: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn\u2019t deterministically select the arm with the highest mean\u2014it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p> <p>Algorithm: Thompson Sampling:</p> <p>1: Initialize prior over each arm \\(a\\), \\(p(\\mathcal{R}_a)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) For each arm \\(a\\) sample a reward distribution \\(\\mathcal{R}_a\\) from posterior 4: \\(\\quad\\) Compute action-value function \\(Q(a) = \\mathbb{E}[\\mathcal{R}_a]\\) 5: \\(\\quad a_t \\equiv \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\) 6: \\(\\quad\\) Observe reward \\(r\\) 7: \\(\\quad\\) Update posterior \\(p(\\mathcal{R}_a)\\) using Bayes Rule 8: end for  </p>"},{"location":"reinforcement/11_fast_rl/#contextual-bandits","title":"Contextual Bandits","text":"<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context \\(x_t\\) drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p> <p>In this setting, the goal is to learn a context-dependent policy \\(\\pi(a \\mid x)\\) that maps the observed context \\(x_t\\) to a suitable arm \\(a_t\\), maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p> <p>Formally, the interaction at each time step \\(t\\) is:</p> <ol> <li>Observe context \\(x_t \\in \\mathcal{X}\\)</li> <li>Choose action \\(a_t \\in \\mathcal{A}\\) based on policy \\(\\pi(a \\mid x_t)\\)</li> <li>Receive reward \\(r_t(a_t, x_t)\\)</li> </ol> <p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p> \\[ \\pi^*(x) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}[r(a, x)] \\] <p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>"},{"location":"reinforcement/11_fast_rl/#mental-map","title":"Mental Map","text":"<pre><code>                Bandits: Foundations of Data-Efficient RL\n     Goal: Understand exploration-exploitation in simplest setting\n           Learn to act with minimal data through principled tradeoffs\n                                \u2502\n                                \u25bc\n               What Are Multi-Armed Bandits (MAB)?\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Single-state (stateless) decision problems                  \u2502\n \u2502 Fixed set of actions (arms)                                 \u2502\n \u2502 Unknown reward distribution per arm                         \u2502\n \u2502 Choose an action, receive reward, repeat                    \u2502\n \u2502 No transition dynamics \u2014 unlike full RL                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Objective: Maximize Reward\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Maximize total reward = minimize regret                     \u2502\n \u2502 Regret = missed opportunity vs optimal action               \u2502\n \u2502 Total regret used to evaluate algorithm efficiency          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                  Basic Bandit Algorithms\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Greedy: exploit current best estimates (linear regret)      \u2502\n \u2502 \u03b5-Greedy: random exploration with fixed \u03b5                   \u2502\n \u2502 Decaying \u03b5-Greedy: reduces \u03b5 over time                      \u2502\n \u2502 Optimistic Initialization: set high initial Q\u0302 values        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Principle: Optimism in the Face of Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat unvisited arms as potentially good                    \u2502\n \u2502 Upper Confidence Bound (UCB) algorithms                     \u2502\n \u2502 Tradeoff: mean reward + exploration bonus                   \u2502\n \u2502 Guarantees sublinear regret                                 \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Algorithmic Realization: UCB1\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 UCB_t(a) = Q\u0302_t(a) + \u221a(2 log t / N_t(a))                     \u2502\n \u2502 Encourages pulling uncertain arms early                     \u2502\n \u2502 Regret \u2248 O(\u221a(T log T))                                      \u2502\n \u2502 Theoretically grounded and simple to implement              \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Theoretical Frameworks: Regret vs PAC\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Regret: cumulative gap from always acting optimally         \u2502\n \u2502 PAC: guarantees near-optimal behavior with high probability \u2502\n \u2502 Regret cares about sum of mistakes; PAC focuses on steps    \u2502\n \u2502 Both evaluate quality and efficiency of learning            \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Bayesian Bandits and Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat arm rewards as random variables                       \u2502\n \u2502 Use prior + observed data \u2192 posterior via Bayes rule        \u2502\n \u2502 Conjugate priors simplify computation                       \u2502\n \u2502 Enable principled uncertainty reasoning                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Thompson Sampling (Bayesian)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Sample reward distribution from posterior per arm           \u2502\n \u2502 Pull arm with highest sampled reward                        \u2502\n \u2502 Probabilistic optimism: match probability of being best     \u2502\n \u2502 Natural exploration and strong empirical performance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Probability Matching Perspective\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Thompson Sampling \u2248 sample optimal arm w/ correct frequency \u2502\n \u2502 Avoids hard-coded uncertainty bonuses                       \u2502\n \u2502 Simpler and often better in practice                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       Contextual Bandits\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Input context x_t at each timestep                          \u2502\n \u2502 Reward distribution depends on (action, context)            \u2502\n \u2502 Learn policy \u03c0(a | x): context-aware decision making        \u2502\n \u2502 Real-world applications: ads, medicine, personalization     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n          Summary: Bandits as Foundation for Efficient RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Bandits isolate the exploration-exploitation tradeoff       \u2502\n \u2502 Simpler than full RL, but deeply insightful                 \u2502\n \u2502 Concepts generalize to value estimation, uncertainty        \u2502\n \u2502 Key tools: regret, PAC bounds, posterior reasoning          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/12_fast_mdps/","title":"12. Fast Reinforcement Learning in MDPs and Generalization","text":""},{"location":"reinforcement/12_fast_mdps/#chapter-12-fast-reinforcement-learning-in-mdps-and-generalization","title":"Chapter 12: Fast Reinforcement Learning in MDPs and Generalization","text":"<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p> <ul> <li>Bandits: Single-step decision-making problems.</li> <li>MDPs: Sequential decision-making with transition dynamics.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#evaluation-frameworks","title":"Evaluation Frameworks","text":"<p>To assess learning efficiency, we use:</p> <ul> <li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li> <li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li> <li>PAC (Probably Approximately Correct): Number of steps when the policy is not \\(\\epsilon\\)-optimal is bounded with high probability.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#exploration-approaches","title":"Exploration Approaches","text":"<ul> <li>Optimism under uncertainty (e.g., UCB)</li> <li>Probability matching (e.g., Thompson Sampling)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#pac-framework-for-mdps","title":"PAC Framework for MDPs","text":"<p>A reinforcement learning algorithm \\(A\\) is PAC if with probability at least \\(1 - \\delta\\), it selects an \\(\\epsilon\\)-optimal action on all but a bounded number of time steps \\(N\\), where:</p> \\[ N = \\text{poly} \\left( |S|, |A|, \\frac{1}{1 - \\gamma}, \\frac{1}{\\epsilon}, \\frac{1}{\\delta} \\right) \\]"},{"location":"reinforcement/12_fast_mdps/#mbie-eb-model-based-interval-estimation-with-exploration-bonus","title":"MBIE-EB: Model-Based Interval Estimation with Exploration Bonus","text":"<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function \\(Q(s, a)\\).</p> <p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p> <ul> <li> <p>It estimates \\(\\hat{R}(s, a)\\) and \\(\\hat{T}(s' \\mid s, a)\\) from data using empirical counts.</p> </li> <li> <p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p> </li> </ul> <p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p> <p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p> <p>Algorithm:</p> <p>1: Given \\(\\epsilon\\), \\(\\delta\\), \\(m\\) 2: \\(\\beta = \\dfrac{1}{1-\\gamma}\\sqrt{0.5 \\ln \\!\\left(\\dfrac{2|S||A|m}{\\delta}\\right)}\\) 3: \\(n_{sas}(s,a,s') = 0\\), \\(\\forall s \\in S, a \\in A, s' \\in S\\) 4: \\(rc(s,a) = 0\\), \\(n_{sa}(s,a) = 0\\), \\(\\hat{Q}(s,a) = \\dfrac{1}{1-\\gamma}\\), \\(\\forall s \\in S, a \\in A\\) 5: \\(t = 0\\), \\(s_t = s_{\\text{init}}\\) 6: loop 7: \\(\\quad a_t = \\arg\\max_{a \\in A} \\hat{Q}(s_t, a)\\) 8: \\(\\quad\\) Observe reward \\(r_t\\) and state \\(s_{t+1}\\) 9: \\(\\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\\), \\(\\quad\\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\\) 10: \\(\\quad rc(s_t,a_t) = \\dfrac{rc(s_t,a_t)\\big(n_{sa}(s_t,a_t)-1\\big) + r_t}{n_{sa}(s_t,a_t)}\\) 11: \\(\\quad \\hat{R}(s_t,a_t) = rc(s_t,a_t)\\) and \\(\\quad\\quad \\hat{T}(s' \\mid s_t,a_t) = \\dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\\), \\(\\forall s' \\in S\\) 12: \\(\\quad\\) while not converged do 13: \\(\\quad\\quad \\hat{Q}(s,a) = \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s' \\mid s,a)\\max_{a'} \\hat{Q}(s',a') + \\dfrac{\\beta}{\\sqrt{n_{sa}(s,a)}}\\), \\(\\quad\\quad\\quad \\forall s \\in S, a \\in A\\) 14: \\(\\quad\\) end while 15: end loop</p>"},{"location":"reinforcement/12_fast_mdps/#bayesian-model-based-reinforcement-learning","title":"Bayesian Model-Based Reinforcement Learning","text":"<p>Bayesian RL methods maintain a posterior over MDP models \\((P, R)\\) and sample plausible environments from the posterior to plan and act.</p> <p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p> <p>Formally, the Thompson sampling policy is:</p> \\[ \\pi(s, a \\mid h_t) = \\mathbb{P}\\left(Q(s, a) \\ge Q(s, a'),\\; \\forall a' \\ne a \\;\\middle|\\; h_t \\right) = \\mathbb{E}_{\\mathcal{P}, \\mathcal{R} \\mid h_t} \\left[ \\mathbb{1}\\left(a = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)\\right) \\right] \\] <p>Where: - \\(h_t\\) is the history up to time \\(t\\) (including all observed transitions and rewards), - \\(\\mathcal{P}, \\mathcal{R}\\) are the transition and reward functions respectively, - The expectation is taken over the posterior belief on the MDP \\((\\mathcal{P}, \\mathcal{R})\\).</p>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-algorithm-in-mdps","title":"Thompson Sampling Algorithm in MDPs","text":"<ol> <li>Maintain a posterior \\(p(\\mathcal{P}, \\mathcal{R} \\mid h_t)\\) over the transition and reward models based on all observed data.</li> <li>Sample a model \\((\\mathcal{P}, \\mathcal{R})\\) from the posterior distribution.</li> <li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function \\(Q^*(s, a)\\).</li> <li>Select the action according to the optimal action in the sampled model:     </li> </ol>"},{"location":"reinforcement/12_fast_mdps/#algorithm-thompson-sampling-for-mdps","title":"Algorithm: Thompson Sampling for MDPs","text":"<p>1: Initialize prior over dynamics and reward models for each \\((s, a)\\):  \\(\\quad p(\\mathcal{T}(s' \\mid s, a)), \\quad p(\\mathcal{R}(s, a))\\) 2: Initialize initial state \\(s_0\\) 3: for \\(k = 1\\) to \\(K\\) episodes do 4: \\(\\quad\\) Sample an MDP \\(\\mathcal{M}\\): 5: \\(\\quad\\quad\\) for each \\((s, a)\\) pair do 6: \\(\\quad\\quad\\quad\\) Sample transition model \\(\\mathcal{T}(s' \\mid s, a)\\) from posterior 7: \\(\\quad\\quad\\quad\\) Sample reward model \\(\\mathcal{R}(s, a)\\) from posterior 8: \\(\\quad\\quad\\) end for 9: \\(\\quad\\) Compute optimal value function \\(Q_{\\mathcal{M}}^*\\) for sampled MDP \\(\\mathcal{M}\\) 10: \\(\\quad\\) for \\(t = 1\\) to \\(H\\) do 11: \\(\\quad\\quad a_t = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\mathcal{M}}^*(s_t, a)\\) 12: \\(\\quad\\quad\\) Take action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\) 13: \\(\\quad\\) end for 14: \\(\\quad\\) Update posteriors: \\(\\quad\\quad p(\\mathcal{R}_{s_t, a_t} \\mid r_t), \\quad p(\\mathcal{T}(s' \\mid s_t, a_t) \\mid s_{t+1})\\) using Bayes Rule 15: end for</p>"},{"location":"reinforcement/12_fast_mdps/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li> <li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li> <li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li> </ul> <p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>"},{"location":"reinforcement/12_fast_mdps/#generalization-in-contextual-bandits","title":"Generalization in Contextual Bandits","text":"<p>Contextual bandits generalize standard bandits by associating a context or state \\(s\\) with each decision:</p> <ul> <li>Reward depends on both context and action: \\(r \\sim P[r | s,a]\\)</li> <li>Often model reward as linear: \\(r = \\theta^\\top \\phi(s,a) + \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#benefits-of-generalization","title":"Benefits of Generalization","text":"<ul> <li>Allows learning across states/actions</li> <li>Enables sample-efficient exploration in large state/action spaces</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#strategic-exploration-in-deep-rl","title":"Strategic Exploration in Deep RL","text":"<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>"},{"location":"reinforcement/12_fast_mdps/#optimistic-q-learning-with-function-approximation","title":"Optimistic Q-Learning with Function Approximation","text":"<p>Modified Q-learning update:</p> \\[ \\Delta w = \\alpha \\left( r + r_{\\text{bonus}}(s,a) + \\gamma \\max_{a'} Q(s', a'; w) - Q(s,a;w) \\right) \\nabla_w Q(s,a;w) \\] <p>Bonus \\(r_{\\text{bonus}}\\) reflects novelty or epistemic uncertainty.</p>"},{"location":"reinforcement/12_fast_mdps/#count-based-and-density-based-exploration","title":"Count-Based and Density-Based Exploration","text":"<ul> <li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li> <li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li> <li>Tang et al. (2017) use hashing-based counts.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-for-deep-rl","title":"Thompson Sampling for Deep RL","text":"<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>"},{"location":"reinforcement/12_fast_mdps/#bootstrapped-dqn","title":"Bootstrapped DQN","text":"<ul> <li>Train multiple Q-networks on bootstrapped datasets.</li> <li>Select one head randomly at each episode for exploration.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#bayesian-deep-q-networks","title":"Bayesian Deep Q-Networks","text":"<ul> <li>Bayesian linear regression on final layer</li> <li>Posterior used to sample Q-values, enabling optimism</li> <li>Outperforms naive bootstrapped DQNs in some settings</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#mental-map","title":"Mental Map","text":"<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization\n  Goal: Learn near-optimal policies in MDPs with limited data\n    Extend bandit exploration ideas to sequential decision making\n                            \u2502\n                            \u25bc\n             Why MDPs Are Harder Than Bandits\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 MDPs involve sequential decisions with transitions           \u2502  \u2502 Agent must explore over states and transitions              \u2502  \u2502 Exploration affects future knowledge &amp; rewards              \u2502  \u2502 Sample inefficiency is a major practical bottleneck         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                    Evaluation Frameworks for RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Regret: cumulative gap vs optimal policy over time          \u2502  \u2502 PAC (Probably Approximately Correct):                       \u2502  \u2502   Guarantees \u03b5-optimality with high probability             \u2502  \u2502 Bayesian Regret: expected regret under prior over MDPs      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               PAC Learning in MDPs: Formal Guarantee  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Algorithm is PAC if all but N steps are \u03b5-optimal           \u2502  \u2502 N = poly(|S|, |A|, 1/(1-\u03b3), 1/\u03b5, 1/\u03b4)                        \u2502  \u2502 Ensures high-probability performance bounds                 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                Optimism: MBIE-EB Algorithm (Model-Based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Estimate reward + transitions from data                     \u2502  \u2502 Add bonus to Q-values: encourages actions with high uncertainty \u2502  \u2502 Optimistic model induces exploration                        \u2502  \u2502 Dynamic programming over Q\u0302 + bonus \u2192 exploration policy     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Algorithmic Principle: Optimism Under Uncertainty  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Add uncertainty-driven bonus to reward or Q-value           \u2502  \u2502 Drives exploration to unknown regions                       \u2502  \u2502 Simple but effective in tabular MDPs                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian RL and Posterior Sampling  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maintain belief (posterior) over MDP model (P, R)           \u2502  \u2502 Sample MDP from posterior \u2192 plan optimally in sampled MDP   \u2502  \u2502 Leads to probability matching via Thompson Sampling         \u2502  \u2502 Posterior concentrates with data \u2192 convergence to optimal   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Algorithm: Thompson Sampling in Model-Based RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Sample dynamics + rewards from posterior                    \u2502  \u2502 Solve sampled MDP for optimal Q                            \u2502  \u2502 Act according to Q in sample MDP                           \u2502  \u2502 Update posterior using Bayes rule after each step           \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              Exploration via Posterior Variance (Bayes)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Thompson Sampling \u2248 Probability Matching                    \u2502  \u2502 Probabilistically favors optimal but uncertain policies     \u2502  \u2502 Elegant &amp; adaptive exploration                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Generalization via Contextual Bandits  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Rewards depend on both context and action                   \u2502  \u2502 Learn generalizable function: Q(s,a) or \u03c0(a|s)              \u2502  \u2502 Enables learning across states / actions                    \u2502  \u2502 Use linear models or embeddings: \u03c6(s,a)                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc          Exploration + Generalization in Deep RL Settings  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Optimistic Q-learning: add r_bonus(s,a) in TD target        \u2502  \u2502 r_bonus from novelty, density models, or uncertainty        \u2502  \u2502 Count-based, hashing, or learned density bonuses            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian Deep RL: Posterior Approximation  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Bootstrapped DQN: ensemble of Q-networks for exploration    \u2502  \u2502 Bayesian DQN: sample from approximate Q-posteriors          \u2502  \u2502 Enables implicit Thompson-like behavior                     \u2502  \u2502 Scales to high-dimensional state/action spaces              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                          Chapter Summary  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Strategic exploration = key to fast learning in MDPs        \u2502  \u2502 Optimism (MBIE-EB) and Bayesian methods (Thompson)          \u2502  \u2502 PAC and Bayesian regret are key evaluation tools            \u2502  \u2502 Generalization (via features or deep nets) enables scaling  \u2502  \u2502 Thompson Sampling and bootstrapped approximations bridge gap\u2502  \u2502 Between tabular and high-dimensional RL                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ````</p>"},{"location":"reinforcement/13_montecarlo/","title":"13. Monte Carlo Tree Search and Planning","text":""},{"location":"reinforcement/13_montecarlo/#chapter-13-monte-carlo-tree-search","title":"Chapter 13: Monte Carlo Tree Search","text":"<p>Monte Carlo Tree Search (MCTS) is a powerful planning algorithm that uses simulation-based search to select actions in complex decision-making problems. It is especially effective in large or unknown environments where exact planning is infeasible. MCTS balances exploration and exploitation through sampling and is the backbone of major AI breakthroughs like AlphaGo and AlphaZero.</p>"},{"location":"reinforcement/13_montecarlo/#131-motivation","title":"13.1 Motivation","text":"<p>In classical reinforcement learning (RL), agents often compute policies over the entire state space. MCTS takes a different approach: it performs local search from the current state, using simulated episodes to estimate action values and make near-optimal decisions on the fly.</p> <p>This method is particularly useful in:</p> <ul> <li>Large state/action spaces</li> <li>Games with high branching factor (e.g., Go, Chess)</li> <li>Black-box or simulator-only environments</li> </ul>"},{"location":"reinforcement/13_montecarlo/#132-monte-carlo-search","title":"13.2 Monte Carlo Search","text":"<p>A simple Monte Carlo search uses a model \\(\\mathcal{M}\\) (dynamics and resward model) and a rollout policy \\(\\pi\\) to simulate \\(K\\) trajectories for each action \\(a\\) from the current state \\(s_t\\):</p> <ol> <li>Simulate episodes \\(\\{s_t, a, r_{t+1}^{(k)}, \\ldots, s_T^{(k)}\\}\\) from \\(\\mathcal{M}, \\pi\\).</li> <li>Estimate \\(Q(s_t, a)\\) via sample average:</li> </ol> \\[ Q(s_t, a) = \\frac{1}{K} \\sum_{k=1}^K G_t^{(k)} \\rightarrow q^\\pi(s_t, a) \\] <ol> <li>Select the best action:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\] <p>This performs one-step policy improvement, but does not build deeper search trees.</p>"},{"location":"reinforcement/13_montecarlo/#133-expectimax-search","title":"13.3 Expectimax Search","text":"<p>To go beyond single-step rollouts, expectimax trees compute \\(Q^*(s, a)\\) recursively using the model:</p> <ul> <li>Each node expands by looking ahead using the transition model.</li> <li>Combines maximization (over actions) and expectation (over next states).</li> <li>Forward search avoids solving the entire MDP and focuses only on the subtree starting at \\(s_t\\).</li> </ul> <p>However, the number of nodes grows exponentially with horizon \\(H\\): \\(O(|S||A|)^H\\).</p>"},{"location":"reinforcement/13_montecarlo/#134-monte-carlo-tree-search-mcts","title":"13.4 Monte Carlo Tree Search (MCTS)","text":"<p>MCTS improves on expectimax by sampling rather than fully expanding the tree:</p> <ol> <li>Build a tree rooted at current state \\(s_t\\).</li> <li>Perform \\(K\\) simulations to expand and update parts of the tree.</li> <li>Estimate \\(Q(s, a)\\) using sampled returns.</li> <li>Select the best action at the root:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\]"},{"location":"reinforcement/13_montecarlo/#135-upper-confidence-tree-uct","title":"13.5 Upper Confidence Tree (UCT)","text":"<p>A key challenge in MCTS is deciding which action to simulate at each tree node. UCT addresses this by treating each decision as a multi-armed bandit problem and using an Upper Confidence Bound:</p> \\[ Q(s, a, i) = \\underbrace{\\frac{1}{N(i, a)} \\sum_{k=1}^{N(i,a)} G_k(i,a)}_{\\text{Mean Return}} + \\underbrace{c \\sqrt{\\frac{\\log N(i)}{N(i, a)}}}_{\\text{Exploration Bonus}} \\] <ul> <li>\\(N(i, a)\\): number of times action \\(a\\) taken at node \\(i\\)</li> <li>\\(N(i)\\): total visits to node \\(i\\)</li> <li>\\(c\\): exploration constant</li> <li>\\(G_k(i, a)\\): return from simulation \\(k\\) for \\((i, a)\\)</li> </ul> <p>Action selection:</p> \\[ a_k^i = \\arg\\max_a Q(s, a, i) \\] <p>This balances exploitation of known good actions and exploration of uncertain ones.</p>"},{"location":"reinforcement/13_montecarlo/#136-advantages-of-mcts","title":"13.6 Advantages of MCTS","text":"<ul> <li>Anytime: Can stop search at any time and use the best estimates so far.</li> <li>Model-based or black-box: Only needs sample access to the environment.</li> <li>Best-first: Focuses computation on promising actions.</li> <li>Scalable: Avoids full enumeration of action/state spaces.</li> <li>Parallelizable: Independent simulations can be run in parallel.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#137-alphazero-and-deep-mcts","title":"13.7 AlphaZero and Deep MCTS","text":"<p>AlphaZero revolutionized game-playing AI by combining deep learning with MCTS. Key ideas:</p>"},{"location":"reinforcement/13_montecarlo/#policy-and-value-networks","title":"Policy and Value Networks","text":"<p>A neural network \\(f_\\theta(s)\\) outputs:</p> <ul> <li>\\(P\\): action probabilities</li> <li>\\(V\\): value estimate</li> </ul> \\[ (p, v) = f_\\theta(s) \\]"},{"location":"reinforcement/13_montecarlo/#alphazero-mcts-steps","title":"AlphaZero MCTS Steps","text":"<ol> <li>Select: Traverse tree using \\(Q + U\\) to choose child nodes.</li> <li>Expand: Add a new node, initialized with \\(P\\) from \\(f_\\theta\\).</li> <li>Evaluate: Use \\(v\\) from the network as the value of the leaf.</li> <li>Backup: Propagate value estimates up the tree.</li> <li>Repeat: Perform many rollouts to refine the tree.</li> </ol>"},{"location":"reinforcement/13_montecarlo/#root-action-selection","title":"Root Action Selection","text":"<p>At the root, use visit counts \\(N(s,a)\\) to compute the improved policy:</p> \\[ \\pi(s, a) \\propto N(s, a)^{1/\\tau} \\] <p>where \\(\\tau\\) controls exploration vs exploitation.</p>"},{"location":"reinforcement/13_montecarlo/#138-self-play-and-training","title":"13.8 Self-Play and Training","text":"<p>AlphaZero uses self-play to generate training data:</p> <ol> <li>Play full games using MCTS.</li> <li>Record \\((s, \\pi, z)\\) tuples where:</li> <li>\\(s\\): game state</li> <li>\\(\\pi\\): improved policy from MCTS</li> <li>\\(z\\): final game outcome</li> <li>Train \\(f_\\theta\\) to minimize combined loss:</li> </ol> \\[ \\mathcal{L} = (z - v)^2 - \\pi^\\top \\log p + \\lambda \\|\\theta\\|^2 \\] <p>This allows continual improvement without human supervision.</p>"},{"location":"reinforcement/13_montecarlo/#139-evaluation-and-impact","title":"13.9 Evaluation and Impact","text":"<ul> <li>MCTS dramatically improves performance over raw policy/value networks.</li> <li>Essential to surpassing human performance in Go, Chess, and Shogi.</li> <li>Eliminates the need for human expert data.</li> </ul> <p>Insights:</p> <ul> <li>UCT enables principled tree search with exploration.</li> <li>Neural nets guide and accelerate MCTS.</li> <li>MCTS can be used in any environment where lookahead is possible.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#1310-summary","title":"13.10 Summary","text":"<ul> <li>MCTS uses simulation-based planning with a growing search tree.</li> <li>UCT adds upper confidence bounds to balance exploration/exploitation.</li> <li>AlphaZero combines MCTS with deep learning for superhuman performance.</li> <li>Self-play enables autonomous training without labeled data.</li> </ul> <p>MCTS represents a powerful bridge between planning and learning, enabling agents to make strong decisions under uncertainty in complex domains.</p>"},{"location":"reinforcement/14_final/","title":"14. Summary and Overview","text":""},{"location":"reinforcement/14_final/#chapter-14-c","title":"Chapter 14: c","text":"<p>In this final chapter, we recap the journey of reinforcement learning (RL) from its foundational ideas in multi-armed bandits through to the cutting-edge of deep RL. Along the way we will revisit key algorithmic concepts \u2013 including Upper Confidence Bounds (UCB), Thompson Sampling, Model-Based Interval Estimation with Exploration Bonus (MBIE-EB), and Monte Carlo Tree Search (MCTS) \u2013 and highlight how different approaches to exploration (optimism vs. probability matching) have shaped the field. We will also emphasize the theoretical foundations of RL (regret minimization, PAC guarantees, Bayesian methods) and illustrate how these principles connect to real-world successes like AlphaTensor and ChatGPT. Throughout, the aim is to provide a high-level summary and synthesis, reinforcing the insights gained across previous chapters.</p>"},{"location":"reinforcement/14_final/#recap-from-bandits-to-deep-reinforcement-learning","title":"Recap: From Bandits to Deep Reinforcement Learning","text":"<p>Reinforcement learning can be defined as learning through experience (data) to make good decisions under uncertainty. In an RL problem, an agent interacts with an environment, observes states \\(s\\), takes actions \\(a\\), and receives rewards \\(r\\), with the goal of learning a policy \\(\\pi(a|s)\\) that maximizes future expected reward. Several core features distinguish RL from other learning paradigms:</p> <ul> <li> <p>Optimization of Long-Term Reward: The agent seeks to maximize cumulative reward, accounting for delayed consequences of actions.</p> </li> <li> <p>Trial-and-Error Learning: The agent learns by exploring different actions and observing outcomes, balancing exploration vs. exploitation.</p> </li> <li> <p>Generalization: The agent must generalize from limited experience to new situations (often via function approximation in large state spaces).</p> </li> <li> <p>Data Distribution Shift: Unlike supervised learning, the agent\u2019s own actions affect the data it collects and the states it visits, creating a feedback loop in the learning process.</p> </li> </ul> <p>We began our journey with multi-armed bandits, the simplest RL setting. In a bandit problem there is a single state (no state transitions); each action (arm) yields a reward drawn from an unknown distribution, and the goal is to maximize reward over repeated plays. A bandit is essentially a stateless decision problem \u2013 the next situation does not depend on the previous action. This contrasts with the general Markov Decision Process (MDP) setting, where each action can change the state and influence future rewards and decisions. Bandits capture the essence of exploration-exploitation without the complication of state transitions, making them a perfect starting point.</p> <p>From bandits we progressed to MDPs and multi-step RL problems, which introduce state dynamics and temporal credit assignment. We studied model-free methods (like Q-learning and policy gradient) and model-based methods (like planning with known models or learned models), as well as combinations thereof. As tasks grew more complex, we incorporated function approximation (e.g. using deep neural networks) to handle large or continuous state spaces. This led us into the realm of deep reinforcement learning, where algorithms like DQN and policy optimization methods (PPO, etc.) leverage deep networks as powerful function approximators. While function approximation enables scaling to complex domains, it also introduced new challenges such as stability of learning (e.g. off-policy learning instability, need for techniques like experience replay, target networks, or trust region methods). In parallel, we discussed how off-policy learning and exploration in large domains remain critical challenges, and saw approaches to address these (from clipped policy optimization (PPO) for stability, to imitation learning like DAGGER to incorporate expert knowledge, to pessimistic value adjustments for safer offline learning).</p> <p>Throughout this journey, a unifying theme has been the exploration-exploitation dilemma and the development of algorithms to efficiently learn optimal strategies. In the following sections, we summarize some key algorithmic ideas for exploration and discuss how they exemplify different strategies to address this core challenge.</p>"},{"location":"reinforcement/14_final/#key-algorithmic-ideas-in-exploration-and-planning","title":"Key Algorithmic Ideas in Exploration and Planning","text":""},{"location":"reinforcement/14_final/#optimistic-exploration-upper-confidence-bounds-ucb","title":"Optimistic Exploration: Upper Confidence Bounds (UCB)","text":"<p>A foundational idea for efficient exploration is optimism in the face of uncertainty. The principle is simple: assume the best about untried actions so that the agent is driven to explore them. The Upper Confidence Bound (UCB) algorithm is a classic realization of this idea for multi-armed bandits. UCB maintains an estimate \\(\\hat{Q}_t(a)\\) for the mean reward of each arm \\(a\\) and an uncertainty interval (confidence bound) around that estimate. At each time \\(t\\), it selects the action maximizing an upper-confidence estimate of the reward:</p> \\[ a_t = \\arg\\max_{a \\in A} \\left[ \\hat{Q}_t(a) + c \\frac{\\ln t}{N_t(a)} \\right], \\] <p>where \\(N_t(a)\\) is the number of times action \\(a\\) has been taken up to time \\(t\\), and \\(c\\) is a constant (e.g. \\(c=\\sqrt{2}\\) for the UCB1 algorithm).</p> <p>This selection rule balances exploitation (the \\(\\hat{Q}_t(a)\\) term) with exploration (the bonus term that is large for rarely-selected actions). Intuitively, UCB explores actions with high potential payoffs or high uncertainty. This approach yields strong theoretical guarantees: for instance, UCB1 achieves sublinear regret on the order of \\(O(\\ln T)\\) for bandits, meaning the gap between the accumulated reward of UCB and that of an oracle choosing the best arm at each play grows only logarithmically with time. Optimistic algorithms like UCB are attractive because they are simple and provide worst-case performance guarantees (they will eventually try everything enough to near-certainty). Variants of UCB and optimism-driven exploration have been extended beyond bandits, for example to MDPs via exploration bonus terms.</p>"},{"location":"reinforcement/14_final/#probability-matching-thompson-sampling","title":"Probability Matching: Thompson Sampling","text":"<p>An alternative approach to exploration comes from a Bayesian perspective. Instead of confidence bounds, the agent maintains a posterior distribution over the reward parameters of each action and samples an action according to the probability it is optimal. This strategy is known as Thompson Sampling (or probability matching). In the multi-armed bandit setting, Thompson Sampling can be implemented by assuming a prior for each arm\u2019s mean reward, updating it with observed rewards, and then at each step sampling a value \\(\\tilde{\\theta}_a\\) from the posterior of each arm\u2019s mean. The agent then plays the arm with the highest sampled value. By randomly exploring according to its uncertainty, Thompson Sampling naturally balances exploration and exploitation in a Bayesian-optimal way for certain problems.</p> <p>For example, if rewards are Bernoulli and a Beta prior is used for each arm\u2019s success probability, Thompson Sampling draws a sample from each arm\u2019s Beta posterior and picks the arm with the largest sample. This probability matching tends to allocate more trials to arms that are likely to be best, yet still occasionally tries others proportional to uncertainty. Empirically, Thompson Sampling often performs exceptionally well, sometimes even outperforming UCB in practice, and it has a Bayesian regret that is optimal in certain settings. The caveat is that analyzing Thompson Sampling\u2019s worst-case performance is more complex; however, theoretical advances have shown Thompson Sampling achieves \\(O(\\ln T)\\) regret for many bandit problems as well. A key appeal of Thompson Sampling is its flexibility \u2013 it can be applied to complex problems if one can sample from a posterior (or an approximate posterior) of the model\u2019s parameters. In modern RL, variants of Thompson Sampling inspire approaches like Bootstrapped DQN (which maintains an ensemble of value networks to generate randomized Q-value estimates for exploration).</p>"},{"location":"reinforcement/14_final/#pac-mdp-algorithms-and-exploration-bonuses-mbie-eb","title":"PAC-MDP Algorithms and Exploration Bonuses (MBIE-EB)","text":"<p>In full reinforcement learning problems (MDPs), the exploration challenge becomes more intricate due to state transitions. PAC-MDP algorithms provide a framework for efficient exploration with theoretical guarantees. PAC stands for \u201cProbably Approximately Correct,\u201d meaning these algorithms guarantee that with high probability (\\(1-\\delta\\)) the agent will behave near-optimally (within \\(\\varepsilon\\) of the optimal return) after a certain number of time steps that is polynomial in relevant problem parameters. In other words, a PAC-MDP algorithm will make only a finite (polynomial) number of suboptimal decisions before it effectively converges to an \\(\\varepsilon\\)-optimal policy.</p> <p>One representative PAC-MDP approach is Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) by Strehl and Littman (2008). This algorithm uses an optimistic model-based strategy: it learns an estimated MDP (transition probabilities \\(\\hat{T}\\) and rewards \\(\\hat{R}\\)) from experience and uses dynamic programming to compute a value function \\(\\tilde{Q}(s,a)\\) for that estimated model. Critically, MBIE-EB adds an exploration bonus term to reward or value updates for state-action pairs that have been infrequently visited. For example, the update might be:</p> \\[ \\tilde{Q}(s,a) \\leftarrow \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s'|s,a) \\max_{a'} \\tilde{Q}(s',a') + \\beta \\frac{1}{\\sqrt{N(s,a)}}, \\] <p>where \\(N(s,a)\\) counts visits to \\((s,a)\\) and \\(\\beta\\) is a bonus scale derived from PAC confidence bounds. The \\(\\sqrt{1/N(s,a)}\\) bonus term is large for rarely tried state-action pairs, injecting optimism that encourages the agent to explore them. MBIE-EB selects actions according to the optimistic \\(\\tilde{Q}\\) values (i.e. optimism under uncertainty in an MDP context). Strehl and Littman proved that MBIE-EB is PAC-MDP: with probability \\(1-\\delta\\), after a number of steps polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta\\), etc., the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal. PAC algorithms like MBIE-EB (and related methods like R-MAX and UCRL) guarantee efficient exploration in theory, though they can be computationally demanding in practice for large domains. They illustrate how theoretical foundations (confidence intervals and PAC guarantees) directly inform algorithm design.</p>"},{"location":"reinforcement/14_final/#monte-carlo-tree-search-mcts-for-planning","title":"Monte Carlo Tree Search (MCTS) for Planning","text":"<p>So far we have discussed exploration in the context of learning unknown values or models. Another key idea in the RL toolkit is planning using simulation, particularly via Monte Carlo Tree Search (MCTS). MCTS is a family of simulation-based search algorithms that became famous through their use in game-playing AI (e.g. AlphaGo and AlphaZero). The idea is to build a partial search tree from the current state by simulating many random play-outs (rollouts) and using the results to gradually refine value estimates for states and actions.</p> <p>One of the most widely used MCTS algorithms is UCT (Upper Confidence Trees), which blends the UCB idea with tree search. In each simulation (from root state until a terminal state or depth limit), UCT traverses the tree by choosing actions that maximize an upper confidence bound: at a state (tree node) \\(s\\), it selects the action \\(a\\) that maximizes</p> \\[ \\frac{w_{s,a}}{n_{s,a}} + c \\sqrt{\\frac{\\ln N_s}{n_{s,a}}}, \\] <p>where \\(w_{s,a}\\) is the total reward accrued from past simulations taking action \\(a\\) in state \\(s\\), \\(n_{s,a}\\) is the number of simulations that took that action, and \\(N_s = \\sum_a n_{s,a}\\) is the total simulations from state \\(s\\). This formula is essentially the UCB1 formula extended to tree nodes: the first term is exploitation (the empirical mean reward), and the second is an exploration bonus that is higher for seldom-tried actions. By using this rule at each step of simulation (Selection phase), MCTS efficiently explores the game tree, focusing on promising moves while still trying less-visited moves once in a while. After selection, a random Simulation (rollout) is played out to the end, and the outcome is backpropagated to update \\(w\\) and \\(n\\) along the path. Repeating thousands or millions of simulations yields increasingly accurate value estimates for the root state and preferred actions.</p> <p>MCTS does not learn parameters from data in the traditional sense; rather it is a planning method that can be applied if we have a generative model of the environment (e.g. a simulator or game rules). However, it connects to our theme as another approach to balancing exploration and exploitation via UCB-like algorithms. In practice, MCTS can be combined with learning. Notably, AlphaGo and AlphaZero combined deep neural networks (for state evaluation and policy guidance) with Monte Carlo Tree Search to achieve superhuman performance in Go, chess, and shogi. In those systems, the neural network\u2019s value estimates guide the rollout, and MCTS provides a powerful lookahead search that complements the learned policy. This combination dramatically improves data efficiency \u2013 for example, AlphaZero uses MCTS to effectively explore the game space instead of needing an exorbitant amount of self-play games, and the knowledge gained from MCTS is distilled back into the network through training. MCTS exemplifies how models and planning can be leveraged in RL: if a model of the environment is available (or learned), one can simulate experience to aid decision-making without direct real-world trial-and-error for every decision. This is crucial in domains where real experiments are costly or limited.</p> <p>Computational vs Data Efficiency: It is worth noting that methods like MCTS (and exhaustive exploration algorithms) tend to be computationally intensive \u2013 they trade computation for reduced real-world data needs. We often face a trade-off: algorithms that are very data-efficient (using fewer environment interactions) are often computationally expensive, whereas simpler algorithms that learn quickly in computation might require more data. In some domains (like games or simulated environments), we can afford massive computation, effectively converting computation into simulated \u201cdata\u201d for learning. In others (like physical systems or online user interactions), data is scarce or expensive, so sample-efficient algorithms (even if computationally heavy) are preferred. This trade-off has been a recurring consideration as we moved from bandits to deep RL.</p>"},{"location":"reinforcement/14_final/#exploration-paradigms-optimism-vs-probability-matching","title":"Exploration Paradigms: Optimism vs. Probability Matching","text":"<p>We have seen two major paradigms for addressing the exploration-exploitation challenge:</p> <ul> <li> <p>Optimism in the face of uncertainty: The agent behaves as if the environment is as rewarding as plausibly possible, given the data. This leads to algorithms like UCB, optimistic initial values, exploration bonuses (e.g. MBIE-EB, optimistic Q-learning), and UCT in MCTS. Optimistic methods systematically encourage trying actions that could be best. They often come with strong theoretical guarantees (UCB\u2019s regret bound, PAC-MDP bounds, etc.) because they ensure sufficient exploration of each alternative. Optimism tends to be a more worst-case (frequentist) approach: it doesn\u2019t assume a prior, just relies on confidence intervals that hold with high probability for any reward distribution.</p> </li> <li> <p>Probability matching (Thompson Sampling and Bayesian methods): The agent maintains a belief (probability distribution) about the environment\u2019s parameters and randomizes its actions according to this belief. Effectively, it samples a hypothesis for the true model and then exploits that hypothesis (e.g., play the best action for that sampled model). Over time, the belief is updated with Bayes\u2019 rule as more data comes in, so the sampling naturally shifts toward optimal actions. This approach is more Bayesian in spirit: it assumes a prior distribution and seeks to maximize performance on average with respect to that prior (i.e., good Bayesian regret). Probability matching can be very effective in practice and can incorporate prior knowledge elegantly. The downside is that providing theoretical guarantees in the worst-case sense can be challenging \u2013 the guarantees are often Bayesian (in expectation over the prior) rather than uniform for all environments. Recent theoretical work, however, has shown that even without a perfect prior, Thompson Sampling performs near-optimally in many settings, and there are ways to bound its regret. In terms of implementation complexity, Thompson Sampling may require the ability to sample from posterior distributions, which can be non-trivial in large-scale problems (though approximate methods exist). Optimistic methods, on the other hand, require confidence bound calculations, which for simple tabular cases are straightforward, but for complex function approximation can be difficult (leading to research on exploration bonuses using predictive models or uncertainty estimates).</p> </li> </ul> <p>In summary, optimism vs. probability matching represents two different philosophies for exploration. Optimistic algorithms behave more deterministically (always picking the current optimistic-best option), ensuring systematic coverage of possibilities, while Thompson-style algorithms inject randomized exploration in proportion to uncertainty. Interestingly, human decision-making experiments suggest people may combine elements of both strategies \u2013 not purely optimistic nor purely Thompson. Both paradigms have influenced modern RL: for example, exploration bonuses (optimism) are commonly used in deep RL (e.g. with bonus rewards from prediction error or curiosity), and Bayesian RL approaches (like posterior sampling for MDPs) are gaining traction for problems where a reasonable prior is available or an ensemble can approximate uncertainty.</p>"},{"location":"reinforcement/14_final/#theoretical-foundations-regret-pac-and-bayesian-optimality","title":"Theoretical Foundations: Regret, PAC, and Bayesian Optimality","text":"<p>Understanding how well an RL algorithm performs relative to an ideal standard is a major theme in RL theory. We revisited two main frameworks for this: regret analysis and PAC (sample complexity) analysis, along with the Bayesian viewpoint.</p> <ul> <li>Regret: Regret measures the opportunity loss from not acting optimally at each time step. Formally, in a bandit with optimal expected reward \\(\\mu^*\\), the regret after \\(T\\) plays is</li> </ul> \\[ R(T) = T\\mu^* - \\sum_{t=1}^T r_t, \\] <p>i.e. the difference between the reward that would be obtained by always executing the optimal arm and the reward actually obtained. Sublinear regret (e.g. \\(R(T) = o(T)\\)) implies the algorithm eventually learns the optimal policy (average regret \\(\\to 0\\) as \\(T\\) grows). We saw that \\(\\varepsilon\\)-greedy exploration can lead to linear regret in the worst case (always pulling some suboptimal arm a constant fraction of the time yields \\(R(T) \\sim \\Omega(T)\\)). In contrast, UCB1 achieves \\(R(T) = O(\\ln T)\\), which is asymptotically optimal up to constant factors (matching the Lai &amp; Robbins lower bound for bandits that \\(R(T) \\ge \\Omega(\\ln T)\\) for any algorithm). Regret analysis can be extended to MDPs (though it becomes more complex). For example, algorithms like UCRL2 (an optimistic tabular RL algorithm) have regret bounds on the order of \\(\\tilde{O}(\\sqrt{T})\\) in an MDP (reflecting the harder challenge of states) under certain assumptions. Regret is a worst-case, online metric \u2013 it asks how well we do even against an adversarially chosen problem (or in the unknown actual environment) without assumptions of a prior, focusing on long-term performance.</p> <ul> <li> <p>PAC (Probably Approximately Correct) guarantees: PAC analysis focuses on sample complexity: how many time steps or episodes are required for the algorithm to achieve near-optimal performance with high probability. A PAC guarantee typically states: for any \\(\\varepsilon, \\delta\\), there exists \\(N(\\varepsilon,\\delta)\\) (poly in relevant parameters) such that with probability at least \\(1-\\delta\\), the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal after \\(N\\) steps (or, equivalently, all but at most \\(N\\) of the steps are \\(\\varepsilon\\)-suboptimal). This is a finite-sample guarantee, giving confidence that the learning will not take too long. We discussed that algorithms like MBIE-EB and R-MAX are PAC-MDP: for a given accuracy \\(\\varepsilon\\) and confidence \\(1-\\delta\\), their sample complexity (number of suboptimal actions) is bounded by a polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta, 1/(1-\\gamma)\\), etc. PAC analysis is particularly useful when we care about guarantees in a learning phase before near-optimal performance is reached (important in safety-critical or costly domains where we need to know learning will be efficient with high probability). While regret goes to zero only asymptotically, PAC gives an explicit bound on how long it takes to be good. Often, achieving PAC guarantees in large-scale problems requires simplifying assumptions or limited function approximation classes, as general function approximation PAC results are quite difficult.</p> </li> <li> <p>Bayesian approaches and Bayes-optimality: In a Bayesian formulation, we assume a prior distribution over environments (bandit reward distributions or MDP dynamics). We can then consider the Bayes-optimal policy, which is the policy that maximizes expected cumulative reward with respect to this prior. This leads to the concept of Bayesian regret \u2013 the expected regret under the prior. A Bayes-optimal algorithm minimizes Bayesian regret and, by definition, will outperform any other algorithm on average if the prior is correct. One famous result in this vein is the Gittins Index for multi-armed bandits, which gives an optimal solution when each arm has independent known priors (casting the problem as a Markov process and solving it via dynamic programming). However, computing Bayes-optimal solutions for general RL (especially with state) is usually intractable \u2013 it involves solving a POMDP (partially observable MDP) where the hidden state is the true environment parameters. Thompson Sampling can be interpreted as an approximation to the Bayes-optimal policy that is much easier to implement. It has low Bayesian regret and in some cases can be shown to be asymptotically Bayes-optimal. The Bayesian view is powerful because it allows incorporation of prior knowledge and gives a normative standard (what should we do if we know what we don\u2019t know, in distribution). But its limitation is the computational difficulty and the dependence on having a reasonable prior. In practice, algorithms inspired by Bayesian ideas (like ensemble sampling or posterior sampling for reinforcement learning) try to capture some of the benefit without solving the full Bayes-optimal policy.</p> </li> </ul> <p>These theoretical frameworks complement each other. Regret and PAC analyses give worst-case performance assurances (no matter what the true environment is, within assumptions) and often inspire optimistic algorithms. Bayesian analysis aims for average-case optimality given prior knowledge and often inspires probability matching or adaptive algorithms. As an RL practitioner or researcher, understanding these foundations helps in choosing and designing algorithms appropriate for the problem at hand \u2013 whether one prioritizes guaranteed efficiency, practical performance with prior info, or a mix of both.</p>"},{"location":"reinforcement/14_final/#from-theory-to-practice-real-world-applications-and-achievements","title":"From Theory to Practice: Real-World Applications and Achievements","text":"<p>One of the most exciting aspects of the recent decade in RL is seeing theoretical ideas translate into real-world (or at least real-problem) successes. In this section, we connect some of the classic algorithms and concepts to notable applications:</p> <ul> <li> <p>Game Mastery and Planning \u2013 AlphaGo, AlphaZero, AlphaTensor: Starting with games, AlphaGo famously combined deep neural networks with MCTS (using UCT) and was trained with reinforcement learning to defeat human Go champions. Its successor AlphaZero took this further by learning from scratch (self-play) for multiple games, using Monte Carlo Tree Search guided by a learned value/policy network. The blend of planning (MCTS) and learning (deep RL) that AlphaZero employs is a direct embodiment of concepts we covered: it uses optimistic simulations (MCTS uses UCB in the tree) and improves data efficiency by leveraging a model (the game simulator) for exploration. The success of AlphaZero demonstrates the power of combining model-based search with model-free function approximation. Recently, these ideas have even extended to domains beyond traditional games. AlphaTensor (DeepMind, 2022) is a system that treated the discovery of new matrix multiplication algorithms as a single-player game, and it applied a variant of AlphaZero\u2019s RL approach to find faster algorithms for matrix multiply. The AlphaTensor agent was trained via self-play reinforcement learning to manipulate tensor representations of matrix multiplication and achieved a breakthrough: it discovered matrix multiplication algorithms that surpass the decades-old human benchmarks in efficiency. This is a striking example of RL not just playing games but discovering algorithms \u2013 essentially using reward signals to guide a search through the space of mathematical formulas. It showcases how MCTS (for planning) and deep RL can work together on combinatorial optimization problems: the agent expands a search tree of partial solutions, guided by value networks and an exploration policy, very much like how it would approach a board game. AlphaTensor\u2019s success underscores the generality of RL methods and how ideas like optimism (self-play explores new moves) and guided search can yield new discoveries.</p> </li> <li> <p>Natural Language and Human Feedback \u2013 ChatGPT: A more recent and widely impactful application of reinforcement learning is in natural language processing \u2013 specifically, training large language models to better align with human intentions. ChatGPT (OpenAI, 2022) is a prime example, where RL was used to fine-tune a pretrained language model using human feedback. The technique, known as Reinforcement Learning from Human Feedback (RLHF), involves first collecting human preference data on model outputs and then training a reward model that predicts human preference. The language model (policy) is then optimized (via a policy gradient method like PPO) to maximize the reward model\u2019s score, i.e. to produce answers humans would rate highly. This is essentially an RL loop on top of the language model, treating the task of generating helpful, correct responses as an MDP (or episodic decision problem) and using the learned reward function as the reward signal. The result, ChatGPT, is notably more aligned with user expectations than its predecessor models. In our context, ChatGPT\u2019s training illustrates several RL ideas in action: offline data (pretraining on text) combined with online RL fine-tuning, and the critical role of a well-shaped reward function for alignment. It also highlights exploration in a different sense \u2013 exploring the space of possible answers to find those that yield high reward according to human feedback. The success of ChatGPT demonstrates that RL is not limited to games or robotics; it can be scaled to very high-dimensional action spaces (like generating entire paragraphs of text) when guided by human-informed rewards. From a theoretical lens, one can view RLHF as optimizing an objective that marries the model\u2019s knowledge (from supervised training) with a policy optimization under a learned reward. While classical exploration algorithms (UCB, Thompson) are not directly apparent in ChatGPT\u2019s training (since the \u201cexploration\u201d comes from the model generating varied outputs and the policy optimization process), the high-level principle remains: use feedback signals to iteratively refine behavior.</p> </li> <li> <p>Scientific and Industrial Applications: Beyond these headline examples, RL is increasingly applied in scientific and industrial domains. The course of our study touched on a few, such as:</p> </li> </ul> <p>Controlling nuclear fusion plasmas: Researchers applied deep RL to control the magnetic coils in a tokamak reactor to sustain plasma configurations. This is a complex continuous control problem with safety constraints, where function approximation and careful exploration (largely in simulations before real experiments) were key.</p> <p>Optimizing public health interventions: An RL approach was used to design efficient COVID-19 border testing policies. Framing the problem as a sequential decision task (who to test and when) and using RL to maximize some health outcome or efficiency metric allowed automating policy design that adapted to data.</p> <p>Robotics and Autonomous Systems: Many advances in robotics have come from RL algorithms that allow robots to learn locomotion, manipulation, or flight. Often these use deep RL and sometimes simulation-to-reality transfer. The exploration techniques we learned (like curiosity-driven bonuses or domain randomization) help address the challenge of learning in these complex environments.</p> <p>Recommender Systems and Online Decision Making: Multi-armed bandit algorithms (including Thompson Sampling and UCB) are widely used in industry for things like A/B testing, website optimization, and personalized recommendations. For example, serving personalized content can be seen as a bandit problem where each content choice is an arm and click-through or engagement is the reward. Companies employ bandit algorithms to balance exploration of new content with exploitation of known user preferences, often in a context of contextual bandits (where the state or context is user features). The theoretical guarantees of bandit algorithms give confidence in their performance, and their simplicity makes them practical at scale.</p> <p>In all these cases, the fundamental concepts from this course appear and validate themselves: whether it\u2019s optimism guiding AlphaZero\u2019s search, or Thompson Sampling driving an online recommendation strategy, or policy gradients tuning ChatGPT using human rewards, the same core ideas of reinforcement learning apply. Modern applications often hybridize approaches \u2013 for instance, using model-based simulations (AlphaTensor, AlphaZero), or combining learning from offline data with online exploration (ChatGPT\u2019s RLHF, or robotics). This underscores the importance of mastering the basics: understanding value functions, policy optimization, exploration mechanisms, and theoretical limits has direct relevance even as we push RL into new territory.</p>"},{"location":"reinforcement/14_final/#final-takeaways","title":"Final Takeaways","text":"<p>In closing, we synthesize a few key insights and lessons from the full RL journey:</p> <ul> <li> <p>Reinforcement Learning Unifies Many Themes: We saw that RL problems range from simple bandits to complex high-dimensional control, but they share the need for sequential decision making under uncertainty. Concepts like state, action, reward, policy, value function, model form a common language to describe problems as diverse as games, robotics, and recommendation systems. Recognizing an appropriate RL formulation (MDP, bandit, etc.) for a given real-world problem is the first step to applying these methods.</p> </li> <li> <p>Exploration vs. Exploitation is Fundamental: The trade-off between trying new actions and leveraging known good actions underpins all of RL. We examined different strategies:</p> <ul> <li> <p>Heuristics like \\(\\epsilon\\)-greedy (simple but can be suboptimal),</p> </li> <li> <p>Optimistic algorithms (UCB, optimism in value iteration,  exploration bonuses) which ensure systematic exploration using confidence bounds,</p> </li> <li> <p>Probabilistic approaches (Thompson Sampling, randomized value functions) which inject randomness based on uncertainty.</p> </li> </ul> </li> </ul> <p>Each approach has its advantages \u2013 optimism often yields strong guarantees and is conceptually straightforward, while Thompson Sampling often gives excellent practical performance and naturally incorporates prior knowledge. In large-scale problems, clever exploration bonuses (intrinsic rewards for novelty) and approximate uncertainty estimates are key to maintaining exploration. The central lesson is that successful RL requires deliberate exploration strategies; naive exploration can lead to poor sample efficiency or getting stuck in suboptimal behaviors.</p> <ul> <li> <p>Theoretical Foundations Guide Algorithm Design: Concepts like regret and PAC provide ways to formally measure learning efficiency. They not only help us compare algorithms (e.g. which has lower regret or better sample complexity) but have directly inspired algorithmic techniques (like UCB from the idea of minimizing regret, or PAC-inspired algorithms like MBIE-EB and R-MAX designed to guarantee learning within polynomial time). Meanwhile, the Bayesian perspective offers a gold-standard for optimal decision-making given prior info, even if it\u2019s often computationally intractable \u2013 it guides us toward algorithms that perform well on average and informs approaches like posterior sampling. As RL practitioners, we should remember:</p> <ul> <li> <p>Regret minimization focuses on not wasting too many opportunities \u2013 it\u2019s about learning as fast as possible in an online sense.</p> </li> <li> <p>PAC guarantees focus on bounding the learning time with high confidence \u2013 giving safety that an algorithm won\u2019t do too poorly for too long.</p> </li> <li> <p>Bayesian optimality focuses on using prior knowledge efficiently \u2013 it\u2019s about doing the best given what you (probabilistically) know.</p> </li> </ul> </li> </ul> <p>All three perspectives are important; balancing them or choosing the right one depends on the application (e.g., in a one-off A/B test you might care about regret, in a lifelong robot learning you care about sample efficiency with high probability, and in a personalized system you might incorporate Bayesian priors about users).</p> <ul> <li> <p>Function Approximation and Deep RL Open New Possibilities (and Challenges): The leap from tabular or small-scale problems to real-world complexity required using function approximation (especially deep neural networks). This enabled RL to handle images, continuous states, and enormous state spaces \u2013 as seen in Atari games, Go, and continuous control benchmarks. The success of deep RL (DQN, policy gradient methods, etc.) comes from blending RL algorithms with powerful representation learning. However, it also brought challenges like stability of training, overfitting, exploration in high dimensions, and reproducibility issues. Key techniques to mitigate these include experience replay, target networks, regularization, large-scale parallel training, and reward shaping. The takeaway is that theoretical convergence guarantees often break down with function approximation, so a lot of practical know-how and experimentation is needed. Yet, the core ideas (Bellman equations, policy improvement, etc.) still apply \u2013 just approximate. The field is actively developing better theories for RL with function approximation (e.g. understanding generalization, error propagation) and techniques for more reliable training.</p> </li> <li> <p>Real-World Impact and Ongoing Research: Reinforcement learning has graduated from textbook problems to impacting real-world systems. Its principles have powered superhuman game AIs, improved scientific research (e.g. algorithm discovery, experiment design), enhanced language models, and optimized business decisions. At the same time, truly robust and general-purpose RL is still an open challenge. Issues of stability, efficiency, and safety remain \u2013 for instance:</p> <ul> <li>Developing algorithms that work out-of-the-box with minimal tuning for any problem (robustness).</li> <li>Improving data efficiency so that RL can be applied with limited real-world interactions (e.g., via model-based methods, better exploration, or transfer learning).</li> <li>Integrating learning and planning seamlessly, and handling settings that mix offline data with online exploration.</li> <li>Expanding the RL framework to account for multiple objectives, collaboration or competition (multi-agent RL), and richer feedback modalities beyond scalar rewards.</li> </ul> </li> </ul> <p>These are active research directions. The skills and concepts acquired \u2013 from understanding theoretical bounds to implementing algorithms \u2013 equip us to tackle these frontiers.</p> <p>In summary, the journey from multi-armed bandits to deep reinforcement learning has taught us not only a catalogue of algorithms, but a way of thinking about sequential decision problems. We learned how to measure learning efficiency and why exploration is hard yet critical. We saw simple ideas like optimism and probability matching scale up to complex systems that play Go or converse in English. As you move forward from this textbook, remember the foundational principles: reward is your guide, value estimation is your tool, policy is your output, and exploration is your catalyst. With these in mind, you are well-prepared to both apply RL to challenging problems and to contribute to the advancing frontier of reinforcement learning research.</p>"},{"location":"reinforcement/1_intro/","title":"1. Introduction to Reinforcement Learning","text":""},{"location":"reinforcement/1_intro/#chapter-1-introduction-to-reinforcement-learning","title":"Chapter 1: Introduction to Reinforcement Learning","text":"<p>Reinforcement Learning is a paradigm in machine learning where an agent learns to make sequential decisions through interaction with an environment. Unlike supervised learning, where the agent learns from labeled examples, or unsupervised learning, where it learns patterns from unlabeled data, reinforcement learning is driven by the goal of maximizing cumulative reward through trial and error. The agent is not told which actions to take but must discover them by exploring the consequences of its actions.</p> <p>Sequential decision-making under uncertainty is at the heart of reinforcement learning. The agent must balance exploration and exploitation. Exploration is needed to gather information about the environment, while exploitation uses this information to select actions that appear best.</p> <pre><code> RL vs Supervised Learning (Key Differences)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Supervised Learning:                                \u2502\n       \u2502   \u2013 Learns from labeled examples (input \u2192 target)   \u2502\n       \u2502   \u2013 Feedback is immediate and correct               \u2502\n       \u2502   \u2013 IID data; no sequential dependence              \u2502\n       \u2502                                                     \u2502\n       \u2502 Reinforcement Learning:                             \u2502\n       \u2502   \u2013 Learns from interaction (trial &amp; error)         \u2502\n       \u2502   \u2013 Feedback (reward) may be delayed or sparse      \u2502\n       \u2502   \u2013 Data depends on agent's actions (non-IID)       \u2502\n       \u2502   \u2013 Must balance exploration vs exploitation        \u2502\n       \u2502   \u2013 Must solve temporal credit assignment           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>A key characteristic of reinforcement learning is that the outcome of an action may not be immediately known. Rewards can be delayed, making it hard to determine which past actions are responsible for future outcomes. This challenge is known as temporal credit assignment. Successful reinforcement learning algorithms must learn to attribute long-term consequences to earlier decisions.</p> <p>At each time step, the agent observes some representation of the world, takes an action, and receives a reward. The world then transitions to a new state. This interaction continues over time, forming an experience trajectory:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, \\dots \\] <p>The agent\u2019s goal is to learn a policy, which is a mapping from states to actions, that maximizes the total reward it collects over time.</p> <p>The total future reward is defined through the notion of return. The most common formulation is the discounted return:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>where \\(0 \\le \\gamma \\le 1\\) is called the discount factor. It determines how much the agent values immediate rewards compared to future rewards. A smaller \\(\\gamma\\) encourages short-term decisions, while a larger \\(\\gamma\\) favors long-term planning.</p> <p>Reinforcement learning involves four fundamental challenges:</p> <ol> <li>Optimization: The agent must find an optimal policy that maximizes expected return.</li> <li>Delayed consequences: Actions can affect rewards far into the future, making credit assignment difficult.</li> <li>Exploration: The agent must try actions to learn their consequences, even though some actions may seem suboptimal in the short term.</li> <li>Generalization: The agent must use limited experience to generalize to states it has never seen before.</li> </ol> <p>The main components of a reinforcement learning system are the agent, the environment, actions, states, and rewards. The agent chooses an action based on its current state. The environment responds with the next state and a reward. From this interaction, the agent must infer how to improve its decisions over time.</p> <p>The concept of state is crucial. A state is a summary of information that can influence future outcomes. In theory, a state is Markov if it satisfies:</p> \\[ p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t) \\] <p>where \\(h_t\\) is the full history of past observations, actions, and rewards. This means that the future depends only on the current state, not on the entire past. The Markov property is important because it simplifies the learning problem and allows powerful mathematical tools to be applied.</p> <p>State (environment)  \u2192  Action (agent)  \u2192  Next State (environment)</p> <p>Reinforcement learning is particularly useful in domains where optimal behavior is not easily specified, data is limited or must be collected through interaction, and long-term consequences matter. Examples include robotics, autonomous vehicles, game playing, resource allocation, recommendation systems, and online decision-making.</p>"},{"location":"reinforcement/1_intro/#key-concepts","title":"Key Concepts:","text":""},{"location":"reinforcement/1_intro/#episodic-vs-continuing","title":"Episodic vs Continuing:","text":"<p>Reinforcement learning problems can be episodic or continuing. In episodic tasks, interactions end after a finite number of steps, and the agent resets for a new episode. In continuing tasks, the interactions never formally end, and the agent must learn to behave well indefinitely. In episodic settings, the return is naturally finite. In continuing tasks, discounting or average reward formulations are used to ensure the return is well-defined.</p>"},{"location":"reinforcement/1_intro/#types-of-rl-tasks","title":"Types of RL tasks:","text":"<p>There are several types of learning tasks in RL:</p> <ol> <li>Prediction/Policy Evaluation: Estimating how good a given policy is.</li> <li>Control: Finding an optimal policy that maximizes expected return.</li> <li>Planning: Computing optimal policies using a known model of the environment.</li> </ol>"},{"location":"reinforcement/1_intro/#model-based-vs-model-free","title":"Model based vs Model-free","text":"<p>Reinforcement learning algorithms can be classified into two major categories: model-based and model-free. Model-based methods assume that the transition dynamics and reward function of the environment are known or learned. They use this information for planning. Model-free methods do not assume access to this knowledge and must learn directly from interaction.</p>"},{"location":"reinforcement/1_intro/#on-policy-vs-off-policy","title":"On-policy vs off-policy","text":"<ul> <li> <p>On-policy learning:</p> <ul> <li>Direct experience.</li> <li>Learn to estimate and evaluate a policy from experience obtained from following that policy.</li> </ul> </li> <li> <p>Off-policy Learning</p> <ul> <li>Learn to estimate and evaluate a policy using experience gathered from following a different policy.</li> </ul> </li> </ul>"},{"location":"reinforcement/1_intro/#tabular-vs-function-approximation","title":"Tabular vs Function Approximation","text":"<p>In small environments with a limited number of states and actions, value functions and policies can be represented using tables. This is known as the tabular setting. However, real-world problems often involve very large or continuous state spaces, where it is impossible to maintain a separate entry for every state or action.</p> <p>In such cases, we approximate the value function or policy using a parameterized function, such as a linear model or neural network. This approach is called function approximation. Function approximation enables generalization: knowledge gained from one state can be applied to many similar states, making learning feasible in large or continuous environments.</p>"},{"location":"reinforcement/2_mdp/","title":"2. MDPs & Dynamic Programming","text":""},{"location":"reinforcement/2_mdp/#chapter-2-markov-decision-processes-and-dynamic-programming","title":"Chapter 2: Markov Decision Processes and Dynamic Programming","text":"<p>Reinforcement Learning relies on the mathematical framework of Markov Decision Processes (MDPs) to formalize sequential decision-making under uncertainty. The key idea is that an agent interacts with an environment, making decisions that influence both immediate and future rewards.</p> <p>Reinforcement Learning is about selecting actions over time to maximize long-term reward.</p>"},{"location":"reinforcement/2_mdp/#the-markovian-hierarchy","title":"The Markovian Hierarchy","text":"<p>The RL framework is built upon three foundational models, each adding complexity and agency.</p>"},{"location":"reinforcement/2_mdp/#the-markov-process","title":"The Markov Process","text":"<p>A Markov Process, or Markov Chain, is the simplest model, concerned only with the flow of states. It is defined by the set of States (\\(S\\)) and the Transition Model (\\(P(s' \\mid s)\\)). The defining characteristic is the Markov Property: the next state is independent of the past states, given only the current state.</p> \\[ P(s_{t+1} \\mid s_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t) \\] <p>The future is conditionally independent of the past given the present. Intuition: MPs describe what happens but do not assign any value to these events.</p>"},{"location":"reinforcement/2_mdp/#the-markov-reward-process-mrp","title":"The Markov Reward Process (MRP)","text":"<p>A Markov Reward Process (MRP) extends an MP by adding rewards and discounting. An MRP is a tuple \\((S, P, R, \\gamma)\\) where \\(R(s)\\) is the expected reward for being in state \\(s\\) and \\(\\gamma\\) is the discount factor. The return is:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>The goal is to compute the value function, which is the expected return starting from a state \\(s\\):</p> \\[ V(s) = \\mathbb{E}[G_t | s_t = s] \\] <p>The value function satisfies the Bellman Expectation Equation:</p> \\[ V(s) = R(s) + \\gamma \\sum_{s'} P(s'|s)V(s') \\] <p>This recursive structure relates the value of a state to the values of its successor states.</p>"},{"location":"reinforcement/2_mdp/#the-markov-decision-process-mdp","title":"The Markov Decision Process (MDP)","text":"<p>An MDP introduces agency. Defined by the tuple \\((S, A, P, R, \\gamma)\\), it extends the MRP by giving the agent a set of Actions (\\(A\\)) to choose from.</p> <ul> <li>Action-Dependent Transition: \\(P(s' \\mid s, a)\\)</li> <li>Action-Dependent Reward: \\(R(s, a)\\)</li> </ul> <p>The agent's strategy is described by a Policy (\\(\\pi(a \\mid s)\\)), the probability of selecting action \\(a\\) in state \\(s\\). A key insight is that fixing any policy \\(\\pi\\) reduces an MDP back into an MRP, allowing all tools developed for MRPs to be applied to the MDP.</p> \\[ R_\\pi(s) = \\sum_a \\pi(a|s) R(s,a) \\] \\[ P_\\pi(s'|s) = \\sum_a \\pi(a|s) P(s'|s,a) \\] <p>Once actions are introduced in an MDP, it becomes useful to evaluate not only how good a state is, but how good a particular action is relative to the policy\u2019s expected behavior. This leads to the advantage function.</p> <p>The state-value function measures how good it is to be in a state: \\(V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\).</p> <p>The action-value function measures how good it is to take action \\(a\\) in state \\(s\\):\\(Q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s,\\; a_t = a]\\)</p> <p>The advantage function compares these two: \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s).\\)</p> <p>\\(V_\\pi(s)\\) is how well the policy performs on average from state \\(s\\).</p> <p>\\(Q_\\pi(s,a)\\) is how well it performs if it specifically takes action \\(a\\).</p> <p>Therefore, the advantage tells us: How much better or worse action \\(a\\) is compared to what the policy would normally do in state \\(s\\).</p>"},{"location":"reinforcement/2_mdp/#value-functions-and-expectation","title":"Value Functions and Expectation","text":"<p>To evaluate a fixed policy \\(\\pi\\), we define two inter-related value functions based on the Bellman Expectation Equations.</p>"},{"location":"reinforcement/2_mdp/#state-value-function-vpis","title":"State Value Function (\\(V^\\pi(s)\\))","text":"<p>\\(V^\\pi(s)\\) quantifies the long-term expected return starting from state \\(s\\) and strictly following policy \\(\\pi\\).  </p> <p>How much total reward should I expect if I start in state s and follow policy \\(\\pi\\): forever?</p>"},{"location":"reinforcement/2_mdp/#state-action-value-function-qpisa","title":"State-Action Value Function (\\(Q^\\pi(s,a)\\))","text":"<p>\\(Q^\\pi(s,a)\\) is a more granular measure, quantifying the expected return if the agent takes action \\(a\\) in state \\(s\\) first, and then follows policy \\(\\pi\\).  </p> <p>Intuition: The \\(Q\\)-function is the value of doing a specific action; the \\(V\\)-function is the value of being in a state (the weighted average of the \\(Q\\)-values offered by the policy \\(\\pi\\) in that state):  </p> <p>The Bellman Expectation Equation for \\(V^\\pi\\) links the value of a state to the values of the actions chosen by \\(\\pi\\) and the resulting future states:  </p>"},{"location":"reinforcement/2_mdp/#optimal-control-finding-pi","title":"Optimal Control: Finding \\(\\pi^*\\)","text":"<p>The ultimate goal of solving an MDP is to find the optimal policy (\\(\\pi^*\\)) that maximizes the expected return from every state \\(s\\).</p> \\[ \\pi^* = \\operatorname*{arg\\,max}_{\\pi} V^\\pi(s) \\quad \\text{for all } s \\in S \\] <p>This optimal policy is characterized by the Optimal Value Functions (\\(V^*\\) and \\(Q^*\\)).</p>"},{"location":"reinforcement/2_mdp/#the-bellman-optimality-equations","title":"The Bellman Optimality Equations","text":"<p>These equations are fundamental, describing the unique value functions that arise when acting optimally. Unlike the expectation equations, they contain a \\(\\max\\) operator, making them non-linear.</p> <ul> <li> <p>Optimal State Value (\\(V^*\\)): The optimal value of a state equals the maximum expected return achievable from any single action \\(a\\) taken from that state:</p> \\[ V^*(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right] \\] </li> <li> <p>Optimal Action-Value (\\(Q^*\\)): The optimal value of taking action \\(a\\) is the immediate reward plus the discounted value of the optimal subsequent actions (\\(\\max_{a'}\\)) in the next state \\(s'\\):</p> \\[ Q^*(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a') \\] </li> </ul> <p>Once \\(Q^*\\) is known, the optimal policy \\(\\pi^*\\) is easily extracted by simply choosing the action that maximizes \\(Q^*(s,a)\\) in every state:  </p> <p>These equations are non-linear due to the max operator and must be solved iteratively.</p>"},{"location":"reinforcement/2_mdp/#dynamic-programming-algorithms","title":"Dynamic Programming Algorithms","text":"<p>For MDPs where the model (\\(P\\) and \\(R\\)) is fully known, Dynamic Programming methods are used to solve the Bellman Optimality Equations iteratively.</p>"},{"location":"reinforcement/2_mdp/#policy-iteration","title":"Policy Iteration","text":"<p>Policy Iteration follows an alternating cycle of Evaluation and Improvement. It takes fewer, but more expensive, iterations to converge.</p> <ol> <li>Policy Evaluation: For the current policy \\(\\pi_k\\), compute \\(V^{\\pi_k}\\) by iteratively applying the Bellman Expectation Equation until full convergence. This is the computationally intensive step.      </li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}\\) by choosing an action that is greedy with respect to the fully converged \\(V^{\\pi_k}\\).      </li> </ol> <p>The process repeats until the policy stabilizes (\\(\\pi_{k+1} = \\pi_k\\)), guaranteeing convergence to \\(\\pi^*\\).</p>"},{"location":"reinforcement/2_mdp/#value-iteration","title":"Value Iteration","text":"<p>Value Iteration is a single, continuous process that combines evaluation and improvement by repeatedly applying the Bellman Optimality Equation. It takes many, but computationally cheap, iterations.</p> <ol> <li>Iterative Update: For every state \\(s\\), update the value function \\(V_k(s)\\) using the \\(\\max\\) operation. This immediately incorporates a greedy improvement step into the value update.      </li> <li>Convergence: The iterations stop when \\(V_{k+1}\\) is sufficiently close to \\(V^*\\).</li> <li>Extraction: The optimal policy \\(\\pi^*\\) is then extracted greedily from the final \\(V^*\\).</li> </ol>"},{"location":"reinforcement/2_mdp/#pi-vs-vi","title":"PI vs VI","text":"Feature Policy Iteration (PI) Value Iteration (VI) Core Idea Evaluate completely, then improve. Greedily improve values in every step. Equation Uses Bellman Expectation (inner loop) Uses Bellman Optimality (max) Convergence Few, large policy steps. Policy guaranteed to stabilize faster. Many, small value steps. Value function converges slowly to \\(V^*\\). Cost High cost per iteration (due to full evaluation). Low cost per iteration (due to one-step backup)."},{"location":"reinforcement/2_mdp/#mdps-mental-map","title":"MDPs Mental Map","text":"<pre><code>                   Markov Decision Processes (MDPs)\n        Formalizing Sequential Decision-Making under Uncertainty\n                                  \u2502\n                                  \u25bc\n                       Progression of Markov Models\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Process (MP): States &amp; Transition Probabilities \u2502\n       \u2502   [S, P(s'|s)] \u2014 No rewards, no decisions               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Reward Process (MRP): MP + Rewards + \u03b3          \u2502\n       \u2502  [S, P(s'|s), R(s), \u03b3]                                  \u2502\n       \u2502    Value Function: V(s) = E[Gt | st = s]                \u2502\n       \u2502     Bellman Expectation Eqn:                            \u2502\n       \u2502     V(s) = R(s) + \u03b3 \u2211 P(s'|s)V(s')                      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Decision Process (MDP): MRP + Actions           \u2502\n       \u2502   [S, A, P(s'|s,a), R(s,a), \u03b3]                          \u2502\n       \u2502    Adds Agency: Agent chooses actions                   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                             Policy \u03c0(a|s)\n                        Agent\u2019s decision strategy\n                                  \u2502\n                                  \u25bc\n                          Value Functions\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 State Value V\u03c0(s): Expected return following \u03c0                 \u2502\n     \u2502 Q\u03c0(s,a): Expected return from (s,a) then follow \u03c0              \u2502\n     \u2502 Relationship: V\u03c0(s) = \u2211 \u03c0(a|s) Q\u03c0(s,a)                         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                    Bellman Expectation Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V\u03c0(s) = \u2211 \u03c0(a|s)[R(s,a) + \u03b3 \u2211 P(s'|s,a)V\u03c0(s')]                 \u2502\n     \u2502 Q\u03c0(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) V\u03c0(s')                        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n               Goal: Find Optimal Policy \u03c0*\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u03c0*(s) = argmax\u2090 Q*(s,a)                                     \u2502\n     \u2502 V*(s): Max possible value from state s under the optimal    |\n     |        policy                                               \u2502\n     \u2502 Q*(s,a): Max possible return state s by taking action a     |\n     |          and thereafter following the optimal policy        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                      Bellman Optimality Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V*(s) = max\u2090 [R(s,a) + \u03b3 \u2211 P(s'|s,a)V*(s')]                 \u2502\n     \u2502 Q*(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) max\u2090' Q*(s',a')            \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                 Solution when Model (P,R) is known:\n                    Dynamic Programming (DP)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Policy Iteration                              \u2502 Value Iteration - \u2502\n     \u2502 (Alternating Evaluation &amp; Improvement)        \u2502 Single update step\u2502\n     \u2502                                               \u2502 repeatedly        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy Eval     \u2502                          \u2502 Bellman Optimality      \u2502\n  \u2502 Using V\u03c0 until  \u2502                          \u2502 Update every iteration  \u2502\n  \u2502 convergence     \u2502                          \u2502 V_(k+1) = max_a[....]   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy          \u2502                          \u2502 After convergence:      \u2502\n  \u2502 Improvement:    \u2502                          \u2502 extract \u03c0* from Q*      \u2502\n  \u2502 \u03c0_(k+1)=argmax Q\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n             Outcome: Optimal Policy and Value Functions\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03c0*(s) \u2014 Best action at each state                   \u2502\n       \u2502 V*(s) \u2014 Max return achievable                       \u2502\n       \u2502 Q*(s,a) \u2014 Max return from (s,a)                     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/3_modelfree/","title":"3. Model-Free Prediction","text":""},{"location":"reinforcement/3_modelfree/#chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy","title":"Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy","text":"<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function \\(V^\\pi(s)\\) or \\(Q^\\pi(s,a)\\) for a given policy \\(\\pi\\) using data of the form:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\dots \\] <p>The true value of a state under policy \\(\\pi\\) is still defined by the expected return:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | s_t = s] \\] <p>but the agent must approximate this expectation using sampled experience.</p> <p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p> <ol> <li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li> <li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li> </ol>"},{"location":"reinforcement/3_modelfree/#monte-carlo-policy-evaluation","title":"Monte Carlo Policy Evaluation","text":"<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function \\(V^\\pi(s)\\) is the expected return, we can approximate it by simply averaging the observed returns (\\(G_t\\)) from many episodes that start at state \\(s\\).</p> \\[ V^\\pi(s) \\approx \\text{Average of observed returns } G_t \\text{ starting from } s \\]"},{"location":"reinforcement/3_modelfree/#key-properties-of-mc","title":"Key Properties of MC","text":"<ol> <li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (\\(s_T\\)) to calculate the full return \\(G_t\\).</li> <li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li> </ol> <p>We can maintain the value estimates \\(V(s)\\) using counts and sums, or through incremental updates.</p>"},{"location":"reinforcement/3_modelfree/#a-first-visit-vs-every-visit-mc","title":"A. First-Visit vs. Every-Visit MC","text":"<p>When computing the return \\(G_t\\) for a state \\(s\\) in a single trajectory, a state might be visited multiple times.</p> <ul> <li>First-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) only the first time state \\(s\\) is visited in an episode.<ul> <li>Properties: First-Visit MC is an unbiased estimator of \\(V^\\pi(s)\\). It is also consistent (converges to the true value as data \\(\\rightarrow \\infty\\)) by the Law of Large Numbers.</li> </ul> </li> <li>Every-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) every time state \\(s\\) is visited in an episode.<ul> <li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li> </ul> </li> </ul>"},{"location":"reinforcement/3_modelfree/#b-incremental-monte-carlo","title":"B. Incremental Monte Carlo","text":"<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p> \\[ V(s) \\leftarrow V(s) + \\alpha \\left[ G_t - V(s) \\right] \\] <p>Where:</p> <ul> <li>\\(G_t\\): The actual observed return (our target).</li> <li>\\(V(s)\\): Our current estimate (our old value).</li> <li>\\(\\alpha\\): The learning rate (\\(\\alpha \\in (0, 1]\\)), which can be fixed or decayed.</li> </ul> <p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (\\(V^\\pi\\)), the learning rate \\(\\alpha_t\\) (which may be \\(1/N(s)\\) or a fixed constant) must satisfy the following conditions:</p> <ol> <li>The sum of all learning rates for state \\(s\\) must diverge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s) = \\infty\\)</li> <li>The sum of the squared learning rates must converge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s)^2 &lt; \\infty\\)</li> </ol>"},{"location":"reinforcement/3_modelfree/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>While MC uses the full return \\(G_t\\), TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>"},{"location":"reinforcement/3_modelfree/#bootstrapping-the-core-idea","title":"Bootstrapping: The Core Idea","text":"<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the next state, \\(V(s_{t+1})\\), to update the value of the current state, \\(V(s_t)\\). The standard TD algorithm is TD(0) (or one-step TD).</p>"},{"location":"reinforcement/3_modelfree/#the-td0-update-rule","title":"The TD(0) Update Rule","text":"<p>The TD(0) update replaces the full return \\(G_t\\) with the TD Target (\\(r_t + \\gamma V(s_{t+1})\\)):</p> \\[ V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ \\underbrace{r_{t+1} + \\gamma V(s_{t+1})}_{\\text{TD Target}} - V(s_t) \\right] \\] <p>The term inside the brackets is the TD Error (\\(\\delta_t\\)):  This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>"},{"location":"reinforcement/3_modelfree/#td-vs-monte-carlo","title":"TD vs. Monte Carlo","text":"<p>The distinction between TD and MC centers on what is used as the target value:</p> Feature Monte Carlo (MC) Temporal Difference (TD) Target \\(G_t\\) (Full observed return to episode end) \\(r_{t+1} + \\gamma V(s_{t+1})\\) (One-step return + estimated future value) Bootstrapping No (waits until episode end) Yes (uses \\(V(s_{t+1})\\)) Bias Unbiased (First-Visit MC) Biased (because \\(V(s_{t+1})\\) is an estimate) Variance High Variance (Return \\(G_t\\) is a sum of many random steps) Low Variance (TD target depends on only one random reward/next state) Convergence Consistent (converges to true \\(V^\\pi\\)) TD(0) converges to true \\(V^\\pi\\) in the tabular case <p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>"},{"location":"reinforcement/3_modelfree/#example-setup","title":"Example Setup","text":""},{"location":"reinforcement/3_modelfree/#parameters","title":"Parameters","text":"<ul> <li>States (\\(S\\)): \\(s_A, s_B, s_C\\)</li> <li>Discount Factor (\\(\\gamma\\)): \\(0.9\\)</li> <li>Learning Rate (\\(\\alpha\\)): \\(0.5\\) (Used for TD updates)</li> <li>Initial Value Estimates (\\(V_0\\)): \\(V(s_A)=0, V(s_B)=0, V(s_C)=0\\)</li> </ul>"},{"location":"reinforcement/3_modelfree/#episodes-and-returns","title":"Episodes and Returns","text":"<p>The full return (\\(G_t\\)) is calculated for every visit in every episode:</p> Episode (E) Trajectory (State \\(\\xrightarrow{r}\\) Next State) Visit Time (\\(t\\)) State (\\(s_t\\)) Full Return (\\(G_t\\)) E1 \\(s_A \\xrightarrow{r=1} s_B \\xrightarrow{r=0} s_C \\xrightarrow{r=5} s_B \\xrightarrow{r=2} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{6.508}\\) 1 \\(s_B\\) (1st) \\(\\mathbf{6.12}\\) 2 \\(s_C\\) \\(\\mathbf{6.8}\\) 3 \\(s_B\\) (2nd) \\(\\mathbf{2.0}\\) E2 \\(s_A \\xrightarrow{r=-2} s_C \\xrightarrow{r=8} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{5.2}\\) 1 \\(s_C\\) \\(\\mathbf{8.0}\\) E3 \\(s_B \\xrightarrow{r=10} s_C \\xrightarrow{r=-5} s_B \\xrightarrow{r=1} \\text{T}\\) 0 \\(s_B\\) (1st) \\(\\mathbf{6.31}\\) 1 \\(s_C\\) \\(\\mathbf{-4.1}\\) 2 \\(s_B\\) (2nd) \\(\\mathbf{1.0}\\)"},{"location":"reinforcement/3_modelfree/#1-first-visit-monte-carlo-mc","title":"1. First-Visit Monte Carlo (MC)","text":"<p>Rule: Only the first return for a state in any given episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508\\) (E1), \\(5.2\\) (E2) \\(11.708\\) 2 \\(s_B\\) \\(6.12\\) (E1), \\(6.31\\) (E3) \\(12.43\\) 2 \\(s_C\\) \\(6.8\\) (E1), \\(8.0\\) (E2), \\(-4.1\\) (E3) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":""},{"location":"reinforcement/3_modelfree/#2-every-visit-monte-carlo-mc","title":"2. Every-Visit Monte Carlo (MC)","text":"<p>Rule: The return from every time a state is encountered in any episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns_1","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508, 5.2\\) \\(11.708\\) 2 \\(s_B\\) \\(6.12, 2.0, 6.31, 1.0\\) \\(15.43\\) 4 \\(s_C\\) \\(6.8, 8.0, -4.1\\) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns_1","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":"\\[ V(s_A) = \\frac{11.708}{2} = \\mathbf{5.854} \\\\ V(s_B) = \\frac{15.43}{4} = \\mathbf{3.858} \\\\ V(s_C) = \\frac{10.7}{3} = \\mathbf{3.567} \\]"},{"location":"reinforcement/3_modelfree/#3-temporal-difference-td0","title":"3. Temporal Difference (TD(0))","text":"<p>Rule: The value is updated after every step using the TD Target (\\(r_{t+1} + \\gamma V(s_{t+1})\\)) and the learning rate \\(\\alpha\\). The updated \\(V(s)\\) estimates are carried over to the next step and episode.</p>"},{"location":"reinforcement/3_modelfree/#a-step-by-step-td-calculation-summary","title":"A. Step-by-Step TD Calculation Summary","text":"Step Transition Old \\(V(s_t)\\) New \\(V(s_t)\\) \\(V(s_A)\\) \\(V(s_B)\\) \\(V(s_C)\\) E1-1 \\(s_A \\xrightarrow{r=1} s_B\\) 0 0.500 0.500 0.000 0.000 E1-2 \\(s_B \\xrightarrow{r=0} s_C\\) 0 0.000 0.500 0.000 0.000 E1-3 \\(s_C \\xrightarrow{r=5} s_B\\) 0 2.500 0.500 0.000 2.500 E1-4 \\(s_B \\xrightarrow{r=2} \\text{T}\\) 0.0 1.000 0.500 1.000 2.500 E2-1 \\(s_A \\xrightarrow{r=-2} s_C\\) 0.500 0.375 0.375 1.000 2.500 E2-2 \\(s_C \\xrightarrow{r=8} \\text{T}\\) 2.500 5.250 0.375 1.000 5.250 E3-1 \\(s_B \\xrightarrow{r=10} s_C\\) 1.000 7.863 0.375 7.863 5.250 E3-2 \\(s_C \\xrightarrow{r=-5} s_B\\) 5.250 3.663 0.375 7.863 3.663 E3-3 \\(s_B \\xrightarrow{r=1} \\text{T}\\) 7.863 4.431 0.375 4.431 3.663"},{"location":"reinforcement/3_modelfree/#b-final-estimates-v_td0","title":"B. Final Estimates (\\(V_{TD(0)}\\))","text":"\\[ V(s_A) = \\mathbf{0.375} \\\\ V(s_B) = \\mathbf{4.431} \\\\ V(s_C) = \\mathbf{3.663} \\]"},{"location":"reinforcement/3_modelfree/#comparison-of-results","title":"Comparison of Results","text":"State First-Visit MC Every-Visit MC TD(0) (\\(\\alpha=0.5, \\gamma=0.9\\)) Note \\(s_A\\) 5.854 5.854 0.375 TD heavily penalized \\(s_A\\) in E2 (Target 0.25), while MC averaged the full observed high returns. \\(s_B\\) 6.215 3.858 4.431 TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping. \\(s_C\\) 3.567 3.567 3.663 All methods are close for \\(s_C\\). <p>This comparison illustrates the bias-variance trade-off: * MC uses the sample return (\\(G_t\\)), which has high variance but is an unbiased target (First-Visit). * TD uses a bootstrapped estimate (\\(r + \\gamma V(s')\\)), which has lower variance but introduces bias by relying on an estimated successor value.</p>"},{"location":"reinforcement/3_modelfree/#model-free-prediction-mental-map","title":"Model Free Prediction Mental Map","text":"<pre><code>            Model-Free Prediction\n     (Policy Evaluation without Model P or R)\n                        \u2502\n                        \u25bc\n                Goal: Estimate\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 State Value: V\u03c0(s)                \u2502\n       \u2502 Action Value: Q\u03c0(s,a)             \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n              Using Sampled Experience\n        (s\u2080,a\u2080,r\u2081,s\u2081,a\u2081,r\u2082,... from \u03c0)\n                        \u2502\n                        \u25bc\n            Two Families of Methods\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Monte Carlo (MC)              \u2502 Temporal Difference (TD)      \u2502\n    \u2502 \"Learn from full episodes\"    \u2502 \"Learn step-by-step\"          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u25bc                           \u25bc\n             Monte Carlo (MC)              Temporal Difference (TD)\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502 Needs full episodes     \u2502       \u2502Works on incomplete episodes\u2502\n      \u2502 No bootstrapping        \u2502       \u2502Uses bootstrapping          \u2502\n      \u2502 High variance           \u2502       \u2502Low variance                \u2502\n      \u2502 Unbiased (first visit)  \u2502       \u2502Biased                      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n        \u2502                               \u2502           \u2502\n        \u25bc                               \u25bc           \u25bc\n First-Visit MC                  Every-Visit MC     TD(0) Update Rule\n (One update per episode          (Multiple updates \u2502 V(s) \u2190 V(s) +\n  per state)                      per episode)      \u2502 \u03b1[ r + \u03b3V(s') \u2212 V(s) ]\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                        Comparison (Bias\u2013Variance)\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 MC: Unbiased, High variance             \u2502\n               \u2502 TD: Biased, Lower variance              \u2502\n               \u2502 MC: Not bootstrapping                   \u2502\n               \u2502 TD: Bootstraps using V(s\u2019)              \u2502\n               \u2502 MC: Episodic only                       \u2502\n               \u2502 TD: Works for continuing tasks          \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                      Outcome: Learned Value Function\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 V\u03c0(s) or Q\u03c0(s,a) from real experience \u2502\n               \u2502 (No model of environment required)    \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/4_model_free_control/","title":"4. Model-Free Control","text":""},{"location":"reinforcement/4_model_free_control/#chapter-4-model-free-control-learning-optimal-behavior-without-a-model","title":"Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model","text":"<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy \\(\\pi^*\\) without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>"},{"location":"reinforcement/4_model_free_control/#from-state-values-to-action-values","title":"From State Values to Action Values","text":"<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p> <p>  This update requires two things:</p> <ul> <li>the transition probabilities \\(P(s'|s,a)\\)</li> <li>the expected reward R(s,a)$</li> </ul> <p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p> <p>Instead of learning the state-value function \\(V^\\pi(s)\\) and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.  </p> <p>The Model-Free Policy Iteration loop:</p> <ol> <li>Policy Evaluation: Compute \\(Q^{\\pi}\\) from experience.</li> <li>Policy Improvement: Update the policy \\(\\pi\\) given the estimated \\(Q^{\\pi}\\).</li> </ol> <p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>"},{"location":"reinforcement/4_model_free_control/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>To learn optimal behavior, the agent must balance two goals:</p> <ol> <li>Exploitation: choose actions believed to yield high rewards.</li> <li>Exploration: try actions whose consequences are uncertain or poorly understood.</li> </ol> <p>A common solution is the \\(\\epsilon\\)-greedy policy:</p> <p>With probability \\(1 - \\epsilon\\), choose the action with the highest estimated value. With probability \\(\\epsilon\\), choose a random action.</p> <p>Formally:</p> \\[ \\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A|} &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\frac{\\epsilon}{|A|} &amp; \\text{otherwise} \\end{cases} \\] <p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>"},{"location":"reinforcement/4_model_free_control/#monte-carlo-control","title":"Monte Carlo Control","text":"<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating \\(V(s)\\), it estimates \\(Q(s,a)\\) using sampled returns.</p> <p>Monte Carlo Policy Evaluation, Now for Q:       1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  \\(k=1\\),  Input \\(\\epsilon=1\\), \\(\\pi\\) 2: loop over epiosdes      3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 4: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 5: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 6: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 7: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 8: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 9: \\(\\quad\\quad\\)       end if 10: \\(\\quad\\)  end for 11: \\(\\quad\\) \\(k = k + 1\\) 12: end loop</p> <p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using \\(\\epsilon\\)-greedy policies for exploration.</p> <ul> <li>Policy Evaluation: \\(Q(s, a)\\) is updated using the full return (\\(G_t\\)) observed after the state-action pair \\((s_t, a_t)\\) has occurred in an episode. The incremental update uses the formula \\(Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\\). </li> <li>Policy Improvement: The new policy \\(\\pi_{k+1}\\) is set to be \\(\\epsilon\\)-greedy with respect to the updated \\(Q\\) function.</li> </ul>"},{"location":"reinforcement/4_model_free_control/#greedy-in-the-limit-of-infinite-exploration-glie","title":"Greedy in the Limit of Infinite Exploration (GLIE)","text":"<p>For Monte Carlo Control to converge to the optimal action-value function \\(Q^*(s, a)\\), the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p> <ol> <li>Infinite Visits: All state-action pairs \\((s, a)\\) must be visited an infinite number of times (\\(\\lim_{i \\rightarrow \\infty} N_i(s, a) \\rightarrow \\infty\\)).</li> <li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li> </ol> <p>A simple strategy to satisfy GLIE is to use an \\(\\epsilon\\)-greedy policy where \\(\\epsilon\\) is decayed over time, such as \\(\\epsilon_i = 1/i\\) (where \\(i\\) is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function \\(Q^*(s, a)\\).</p> <p>Monte Carlo Online Control/On Policy Improvement:    </p> <p>1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  Set \\(k=1\\), \\(\\epsilon=1\\).    2: \\(\\pi_k = \\epsilon - greedy (Q)\\) // Create initial \\(\\epsilon\\) - greedy policy. 3: loop over epiosdes    4: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 5: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 6: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 7: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 8: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 9: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 10: \\(\\quad\\quad\\)       end if  11: \\(\\quad\\)  end for   12:  \\(\\quad\\) \\(\\pi_k = \\epsilon - greedy (Q)\\) //Policy improvement 12: \\(\\quad\\) \\(k = k + 1\\) , \\(\\epsilon = \\frac{1}{k}\\)  13: end loop    </p> <p>This process gradually adjusts the policy and the value estimates until they converge.</p>"},{"location":"reinforcement/4_model_free_control/#iv-temporal-difference-td-control","title":"IV. Temporal Difference (TD) Control","text":"<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>"},{"location":"reinforcement/4_model_free_control/#on-policy-td-control-sarsa","title":"On-Policy TD Control: SARSA","text":"<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy currently being followed (\\(\\pi\\)). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p> <p>The update for the action-value \\(Q(s_t, a_t)\\) uses the value of the next state-action pair, \\((s_{t+1}, a_{t+1})\\), selected by the current policy \\(\\pi\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>The TD Target here is \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\). SARSA learns \\(Q^{\\pi}\\) while \\(\\pi\\) is improved greedily with respect to \\(Q^{\\pi}\\), allowing it to find the optimal policy \\(\\pi^*\\).</p> <p>1: Set initial \\(\\epsilon\\)-greedy policy \\(\\pi\\) randomly, \\(t=0\\), initial state \\(s_t=s_0\\)  2: Take \\(a_t \\sim \\pi(s_t)\\)  3: Observe \\((r_t, s_{t+1})\\)  4: loop        5: \\(\\quad\\) Take action \\(a_{t+1} \\sim \\pi(s_{t+1})\\) // Sample action from policy         6: \\(\\quad\\) Observe \\((r_{t+1}, s_{t+2})\\)  7: \\(\\quad\\) Update \\(Q\\) given \\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\\):     8: \\(\\quad\\) Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p> \\[\\forall s \\in S,\\;\\; \\pi(s) = \\begin{cases} \\arg\\max\\limits_a Q(s,a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{a random action}   &amp; \\text{with probability } \\epsilon \\end{cases}\\] <p>9: \\(\\quad\\) \\(t = t + 1\\) , \\(\\epsilon = \\frac{1}{t}\\)  10: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#b-off-policy-td-control-q-learning","title":"B. Off-Policy TD Control: Q-Learning","text":"<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (\\(\\pi^*\\), the target policy) using experience gathered by a different behavior policy (\\(\\pi_b\\)).</p> <p>In Q-Learning, the agent acts using a soft, exploratory \\(\\pi_b\\) (like \\(\\epsilon\\)-greedy) but the value function update is based on the best possible action from the next state, effectively estimating \\(Q^*\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right] \\] <p>The key difference is the target: Q-Learning uses the value of the max action (\\(\\max_{a'} Q(s_{t+1}, a')\\)), regardless of what action was actually taken in the next step. This makes it a greedy update towards \\(Q^*\\).</p> <p>Q-Learning (Off-Policy TD Control):</p> <p>1: Initialize \\(Q(s,a)=0 \\quad \\forall s \\in S, a \\in A\\), set \\(t = 0\\), initial state \\(s_t = s_0\\)  2: Set \\(\\pi_b\\) to be \\(\\epsilon\\)-greedy w.r.t. \\(Q\\)  3: loop    4: \\(\\quad\\) Take \\(a_t \\sim \\pi_b(s_t)\\) // Sample action from behavior policy    5: \\(\\quad\\) Observe \\((r_t, s_{t+1})\\)  6: \\(\\quad\\) \\(Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\\)  7: \\(\\quad\\) \\(\\pi(s_t) = \\begin{cases} \\arg\\max\\limits_a Q(s_t,a) &amp; \\text{with probability } 1 - \\epsilon \\ \\text{a random action} &amp; \\text{with probability } \\epsilon \\end{cases}\\)  8: \\(\\quad\\) \\(t = t + 1\\)  9: end loop     </p>"},{"location":"reinforcement/4_model_free_control/#value-function-approximation-vfa","title":"Value Function Approximation (VFA)","text":"<p>All methods discussed so far assume a tabular representation, where a separate entry for \\(Q(s, a)\\) is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>"},{"location":"reinforcement/4_model_free_control/#motivation-for-approximation","title":"Motivation for Approximation","text":"<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p> <ol> <li>Memory: Explicitly storing every \\(V\\) or \\(Q\\) value is impossible.</li> <li>Computation: Computing or updating every value is too slow.</li> <li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li> </ol> <p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: \\(\\hat{Q}(s, a; \\mathbf{w}) \\approx Q(s, a)\\). The goal shifts from filling a table to finding the parameter vector \\(\\mathbf{w}\\) that minimizes the error between the true value and the estimate.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\left( Q^{\\pi}(s, a) - \\hat{Q}(s, a; \\mathbf{w}) \\right)^2 \\right] \\] <p>The parameter vector \\(\\mathbf{w}\\) is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function \\(J(\\mathbf{w})\\).</p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-with-vfa-policy-evaluation","title":"Model-Free Control with VFA Policy Evaluation","text":"<p>When using function approximation, we substitute the old \\(Q(s, a)\\) in the update rules (MC, SARSA, Q-Learning) with the function approximator \\(\\hat{Q}(s, a; \\mathbf{w})\\).</p> <ul> <li> <p>MC VFA for Policy Evaluation: </p> <p>The return \\(G_t\\) is used as the target in an SGD update: \\(\\Delta \\mathbf{w} \\propto \\alpha (G_t - \\hat{Q}(s_t, a_t; \\mathbf{w})) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t, a_t; \\mathbf{w})\\).</p> <p>1: Initialize \\(\\mathbf{w}\\), set \\(k = 1\\)  2: loop    3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,L_k})\\) given \\(\\pi\\)  4: \\(\\quad\\) for \\(t = 1, \\dots, L_k\\) do      5: \\(\\quad\\quad\\) if First visit to \\((s,a)\\) in episode \\(k\\) then      6: \\(\\quad\\quad\\quad\\) \\(G_t(s,a) = \\sum_{j=t}^{L_k} r_{k,j}\\)  7: \\(\\quad\\quad\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 \\left[ G_t(s,a) - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w})\\) // Compute Gradient    8: \\(\\quad\\quad\\quad\\) Update weights: \\(\\Delta \\mathbf{w}\\)  9: \\(\\quad\\quad\\) end if 10: \\(\\quad\\) end for  11: \\(\\quad\\) \\(k = k + 1\\)  12: end loop    </p> </li> <li> <p>SARSA with VFA: The TD target is \\(r + \\gamma \\hat{Q}(s', a'; \\mathbf{w})\\), leveraging the current function approximation.</p> <p>1: Initialize \\(\\mathbf{w}\\), \\(s\\)  2: loop    3: \\(\\quad\\) Given \\(s\\), sample \\(a \\sim \\pi(s)\\), observe \\(r(s,a)\\), and \\(s' \\sim p(s'|s,a)\\)  4: \\(\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 [r + \\gamma \\hat{V}(s';\\mathbf{w}) - \\hat{V}(s;\\mathbf{w})] \\nabla_{\\mathbf{w}} \\hat{V}(s;\\mathbf{w})\\)  5: \\(\\quad\\) Update weights \\(\\Delta \\mathbf{w}\\)  6: \\(\\quad\\) if \\(s'\\) is not a terminal state then    7: \\(\\quad\\quad\\) Set \\(s = s'\\)  8: \\(\\quad\\) else        9: \\(\\quad\\quad\\) Restart episode, sample initial state \\(s\\)  10: \\(\\quad\\) end if     11: end loop       * Q-Learning with VFA: The TD target is \\(r + \\gamma \\max_{a'} \\hat{Q}(s', a'; \\mathbf{w})\\).</p> </li> </ul>"},{"location":"reinforcement/4_model_free_control/#control-using-vfa","title":"Control using VFA","text":"<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular \\(Q(s,a)\\) with a function approximator \\(\\hat{Q}(s,a;\\mathbf{w})\\), we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by \\(\\hat{Q}(s,a;\\mathbf{w})\\).</p> <p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each \\((s,a)\\), the agent learns a parameter vector \\(\\mathbf{w}\\) that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function \\(Q^*(s,a)\\).</p> <p>The learning problem becomes:</p> \\[ \\min_{\\mathbf{w}} \\; J(\\mathbf{w}) = \\mathbb{E} \\left[ \\left( Q^*(s,a) - \\hat{Q}(s,a;\\mathbf{w}) \\right)^2 \\right] \\] <p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p> \\[ \\Delta \\mathbf{w} \\propto \\left( \\text{target} - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w}) \\] <p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p> Method Target for updating \\(\\mathbf{w}\\) Monte Carlo \\(G_t\\) SARSA \\(r + \\gamma \\hat{Q}(s',a';\\mathbf{w})\\) Q-Learning \\(r + \\gamma \\max_{a'} \\hat{Q}(s',a';\\mathbf{w})\\) <p>These methods now operate in the same way as before, except instead of updating a single \\(Q(s,a)\\) entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>"},{"location":"reinforcement/4_model_free_control/#challenges-the-deadly-triad","title":"Challenges: The Deadly Triad","text":"<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p> \\[ \\text{Function Approximation} \\;+\\; \\text{Bootstrapping} \\;+\\; \\text{Off-policy Learning} \\] <p>This combination is known as the Deadly Triad .</p> <ul> <li>Function Approximation : Generalizes across states but may introduce bias.</li> <li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li> <li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li> </ul> <p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p> <p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate \\(Q(s,a)\\).</p>"},{"location":"reinforcement/4_model_free_control/#deep-q-networks-dqn","title":"Deep Q-Networks (DQN)","text":"<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function \\(\\hat{Q}(s, a; \\mathbf{w})\\) is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p> <p>DQN stabilizes the non-linear learning process using two critical techniques:</p> <ol> <li> <p>Experience Replay (ER): Transitions \\((s_t, a_t, r_t, s_{t+1})\\) are stored in a replay buffer (\\(\\mathcal{D}\\)). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from \\(\\mathcal{D}\\) for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p> </li> <li> <p>Fixed Q-Targets: The Q-Learning update requires a target value \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w})\\). To prevent the estimate \\(\\hat{Q}(s, a; \\mathbf{w})\\) from chasing its own rapidly changing target, the parameters \\(\\mathbf{w}^{-}\\) used to compute the target are fixed for a period of time, then synchronized with the current parameters \\(\\mathbf{w}\\). This provides a stable target \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w}^{-})\\).</p> </li> </ol> <p>Deep Q-Network (DQN) Algorithm:</p> <p>1: Input \\(C\\), \\(\\alpha\\), \\(D = {}\\), Initialize \\(\\mathbf{w}\\), \\(\\mathbf{w}^- = \\mathbf{w}\\), \\(t = 0\\)  2: Get initial state \\(s_0\\)  3: loop        4: \\(\\quad\\) Sample action \\(a_t\\) using \\(\\epsilon\\)-greedy policy w.r.t. current \\(\\hat{Q}(s_t, a; \\mathbf{w})\\)  5: \\(\\quad\\) Observe reward \\(r_t\\) and next state \\(s_{t+1}\\)  6: \\(\\quad\\) Store transition \\((s_t, a_t, r_t, s_{t+1})\\) in replay buffer \\(D\\)  7: \\(\\quad\\) Sample a random minibatch of tuples \\((s_i, a_i, r_i, s'i)\\) from \\(D\\)  8: \\(\\quad\\) for \\(j\\) in minibatch do     9: \\(\\quad\\quad\\) if episode terminates at step \\(i+1\\) then       10: \\(\\quad\\quad\\quad\\) \\(y_i = r_i\\)  11: \\(\\quad\\quad\\) else      12: \\(\\quad\\quad\\quad\\) \\(y_i = r_i + \\gamma \\max\\limits{a'} \\hat{Q}(s'i, a'; \\mathbf{w}^-)\\)  13: \\(\\quad\\quad\\) end if    14: \\(\\quad\\quad\\) Update \\(\\mathbf{w}\\) using gradient descent:   \\(\\quad\\quad\\quad\\) \\(\\Delta \\mathbf{w} = \\alpha \\left( y_i - \\hat{Q}(s_i, a_i; \\mathbf{w}) \\right) \\nabla{\\mathbf{w}} \\hat{Q}(s_i, a_i; \\mathbf{w})\\)  15: \\(\\quad\\) end for        16: \\(\\quad\\) \\(t = t + 1\\)  17: \\(\\quad\\) if \\(t \\mod C == 0\\) then    18: \\(\\quad\\quad\\) \\(\\mathbf{w}^- \\leftarrow \\mathbf{w}\\)  19: \\(\\quad\\) end if     20: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-mental-map","title":"Model Free Control Mental Map","text":"<pre><code>                     Model-Free Control\n    Goal: Learn the Optimal Policy \u03c0* without knowing P or R\n                               \u2502\n                               \u25bc\n           Key Concept: Action-Value Function Q(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Q\u03c0(s,a) = Expected return by taking action a \u2502\n       \u2502in state s and following policy \u03c0 thereafter \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                      No model \u2192 Learn Q directly\n                               \u2502\n                               \u25bc\n                   Generalized Policy Iteration\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Policy Evaluation       \u2502     Policy Improvement    \u2502\n       \u2502   Learn Q\u03c0(s,a)           \u2502   \u03c0 \u2190 greedy w.r.t Q      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Challenge: Exploration vs. Exploitation\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Greedy policy \u2192 Exploits but stops exploring          \u2502\n       \u2502\u03b5-greedy policy \u2192 Balances exploration &amp; exploitation \u2502\n       \u2502GLIE condition: \u03b5 \u2192 0 and \u221e exploration               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Model-Free Control Families (Tabular)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Monte Carlo Control      \u2502      Temporal Difference   \u2502\n       \u2502   (Episode-based)          \u2502      (Step-based)          \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                        \u25bc\n Monte Carlo Control:                       TD Control:\n Estimates Q from full returns          Estimates Q usingbootstrapped targets\n Uses \u03b5-greedy policy                   Works online, faster, low variance\n Episodic only                          Works for episodic &amp; continuing\n          \u2502                                        \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 GLIE MC Control   \u2502             \u2502 On-Policy TD: SARSA        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 Off-Policy TD: Q-Learning  \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    |\n                                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n| On-Policy TD \u2014 SARSA                     |  Off-Policy TD \u2014 Q-Learning       |\n| Learns Q\u03c0 for the policy being followed  |  Learns Q* while following \u03c0_b    |\n| Update uses next action from \u03c0           |  Update uses max action (greedy)  |\n| Update Target:                           |  Update Target:                   |\n|  r + \u03b3 Q(s',a')                          |  r + \u03b3 max\u2090 Q(s',a)               |\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n      Value Function Approximation (Large/Continuous spaces)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Replace Q(s,a) with Q\u0302(s,a;w) using function approx   \u2502\n       \u2502 Generalization across states                         \u2502\n       \u2502 Gradient-based updates (SGD)                         \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Deep Q-Learning (DQN) \u2014 Stable VFA Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Experience Replay \u2014 decorrelate samples             \u2502\n       \u2502 Target Networks \u2014 stabilize bootstrapped targets    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Final Outcome of Model-Free Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly from experience without model       \u2502\n       \u2502 Learn Q*(s,a) through MC, SARSA, or Q-Learning        \u2502\n       \u2502 Scale to large spaces using function approximation    \u2502\n       \u2502 DQN enables deep RL in complex environments           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/5_policy_gradient/","title":"5. Policy Gradient Methods","text":""},{"location":"reinforcement/5_policy_gradient/#chapter-5-policy-gradient-methods","title":"Chapter 5: Policy Gradient Methods","text":"<p>In previous chapters, we derived policies indirectly from value functions using greedy or \u03b5-greedy strategies. However, value-based RL has several challenges:</p> <ul> <li>Does not naturally support stochastic policies  </li> <li>Struggles in continuous action spaces  </li> <li>Optimizing through value functions is often indirect and unstable</li> </ul> <p>Policy Gradient Methods directly optimize the policy itself:</p> \\[ \\pi_\\theta(a|s) = P(a \\mid s; \\theta) \\] <p>Our goal becomes:</p> \\[ \\theta^* = \\arg\\max_\\theta V(\\theta) \\] <p>That is, learn policy parameters \u03b8 that maximize expected return.</p> <p>Value-based methods struggle in these cases because they do not directly learn the policy. Instead, they estimate action values \\(Q(s,a)\\) and derive a policy using greedy or \u03b5-greedy strategies. This makes the policy indirect and unstable. Small changes in \\(Q(s,a)\\) can suddenly change the best action, making learning discontinuous and erratic \u2014 especially with function approximation like neural networks. Furthermore, value-based methods do not naturally support stochastic or continuous action spaces, since computing \\(\\arg\\max_a Q(s,a)\\) is infeasible when actions are continuous or infinite. Policy-based methods solve this problem by directly modeling and learning the policy, such as using a softmax distribution for discrete actions or Gaussian distributions for continuous actions.</p>"},{"location":"reinforcement/5_policy_gradient/#value-based-vs-policy-based-rl","title":"Value-Based vs Policy-Based RL","text":"Approach What is Learned? Policy Type Works in Continuous Actions? Value-Based \\(V(s)\\) or \\(Q(s,a)\\) Indirect (\u03b5-greedy, greedy) No Policy-Based \\(\\pi_\\theta(a/s)\\) Direct, stochastic Yes Actor-Critic Both Direct &amp; learned Yes"},{"location":"reinforcement/5_policy_gradient/#policy-optimization-objective","title":"Policy Optimization Objective","text":"<p>In policy-based reinforcement learning, the policy itself is directly parameterized as \\(\\pi_\\theta(a|s)\\), and our goal is to find the parameters \\(\\theta\\) that produce the best possible behavior. For episodic tasks starting at initial state \\(s_0\\), the quality of a policy is measured by its expected return:</p> \\[ V(\\theta) = V_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[G_0] \\] <p>Therefore, policy optimization can be formulated as an optimization problem, where the goal is to find the policy parameters \\(\\theta\\) that maximize the expected return:</p> \\[ \\theta^* = \\arg\\max_\\theta V(s_0, \\theta) \\] <p>This optimization does not necessarily require gradients.  We can also use gradient-free (derivative-free) optimization methods such as:</p> <ul> <li>Hill Climbing \u2013 Iteratively adjusts parameters in small random directions and keeps changes that improve performance.</li> <li>Simplex / Amoeba / Nelder-Mead \u2013 Uses a geometric shape (simplex) to explore the parameter space and moves it towards higher-performing regions.</li> <li>Genetic Algorithms \u2013 Evolves a population of candidate policies using selection, crossover, and mutation, inspired by natural evolution.</li> <li>Cross-Entropy Method (CEM) \u2013 Samples multiple policy candidates, selects the top performers, and updates the sampling distribution towards them.</li> <li>Covariance Matrix Adaptation (CMA) \u2013 Adapts both the mean and covariance of a Gaussian distribution to efficiently search complex, high-dimensional policy spaces.</li> </ul> <p>Gradient-free policy optimization methods are often excellent and simple baselines to try.  They are highly flexible, can work with any policy parameterization (including non-differentiable ones), and are easy to parallelize, as policies can be evaluated independently across multiple environments. However, these methods are typically less sample efficient because they treat each policy evaluation as a black box and ignore the temporal structure of trajectories. They do not make use of gradients, value functions, or bootstrapping.</p> <p>To improve efficiency, we can use gradient-based optimization techniques, which exploit  the structure of the return function and update parameters using local information. Common gradient-based optimizers include:</p> <ul> <li>Gradient Descent</li> <li>Conjugate Gradient</li> <li>Quasi-Newton Methods (e.g., BFGS, L-BFGS)</li> </ul> <p>These methods are generally more sample efficient, especially in large or continuous state-action spaces.</p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient","title":"Policy Gradient","text":"<p>The goal is to find parameters \\(\\theta\\) that maximize the expected return. Policy gradient algorithms search for a local maximum of \\(V(\\theta)\\) by performing gradient ascent:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta V(s_0, \\theta) \\] <p>where \\(\\alpha\\) is the step-size (learning rate) and \\(\\nabla_\\theta V(s_0, \\theta)\\) is the policy gradient.</p> <p>Assuming an episodic MDP with discount factor \\(\\gamma = 1\\), the value of a parameterized policy \\(\\pi_\\theta\\) starting from state \\(s_0\\) is</p> \\[ V(s_0,\\theta)  = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} r(s_t, a_t); \\; s_0, \\pi_\\theta \\right], \\] <p>where the expectation is taken over the states and actions visited when following \\(\\pi_\\theta\\). This policy value can be re-expressed in multiple ways.  </p> <ul> <li> <p>First, in terms of the action-value function:      </p> </li> <li> <p>Second, in terms of full trajectories. Let a state\u2013action trajectory be:</p> <p>\\(\\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\\dots,s_{T-1},a_{T-1},r_T),\\)</p> <p>and define</p> <p>  as the sum of rewards of trajectory \\(\\tau\\).  </p> <p>Let \\(P(\\tau;\\theta)\\) denote the probability of trajectory \\(\\tau\\) when starting in \\(s_0\\) and following policy \\(\\pi_\\theta\\). Then:</p> \\[ V(s_0,\\theta) = \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau).\\] </li> </ul> <p>In this trajectory notation, our optimization objective becomes</p> \\[ \\theta^*  = \\arg\\max_{\\theta} V(s_0,\\theta)  = \\arg\\max_{\\theta} \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau). \\] <p>Taking gradient yields:</p> \\[ \\nabla_\\theta V(\\theta) =  \\sum_{\\tau} P(\\tau|\\theta)R(\\tau)  \\nabla_\\theta \\log P(\\tau|\\theta) \\] <p>Using sampled trajectories:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m}  R(\\tau^{(i)}) \\nabla_\\theta \\log P(\\tau^{(i)}|\\theta) \\] <p>Trajectory probability:</p> \\[ P(\\tau|\\theta) = P(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t)\\cdot P(s_{t+1}|s_t,a_t) \\] <p>Since dynamics are independent of \\(\\theta\\):</p> \\[ \\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\] <p>Thus:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m} R(\\tau^{(i)}) \\sum_{t=0}^{T-1}  \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) \\] <p>The term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) is called the score function.  It is the gradient of the log of a parameterized probability distribution and measures how sensitive the policy\u2019s action probability is to changes in the parameters \\(\\theta\\).  It plays a central role in policy gradient methods because it allows us to estimate gradients without knowing the environment dynamics, using only samples from the policy.</p>"},{"location":"reinforcement/5_policy_gradient/#softmax-policy-discrete-action-spaces","title":"Softmax Policy (Discrete Action Spaces)","text":"<p>In discrete action spaces, a common parameterization of the policy is the softmax policy, which assigns probabilities based on exponentiated weighted features. Each action is represented using feature vector \\(\\phi(s,a)\\), and the policy is defined as:</p> \\[ \\pi_\\theta(a|s) =  \\frac{e^{\\phi(s,a)^T \\theta}}      {\\sum_{a'} e^{\\phi(s,a')^T \\theta}} \\] <p>The corresponding score function is:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\phi(s,a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\phi(s,a')] \\] <p>This means the gradient increases the probability of the selected action's features and decreases the probability of competing actions based on their expected feature values.</p>"},{"location":"reinforcement/5_policy_gradient/#gaussian-policy-continuous-action-spaces","title":"Gaussian Policy (Continuous Action Spaces)","text":"<p>For continuous action spaces, the Gaussian policy is a natural choice.  The policy outputs actions by sampling from a normal distribution:</p> \\[ a \\sim \\mathcal{N}(\\mu(s), \\sigma^2) \\] <p>The mean is a linear function of state features:</p> \\[ \\mu(s) = \\phi(s)^T \\theta \\] <p>If we assume a fixed variance \\(\\sigma^2\\), the score function becomes:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\frac{(a - \\mu(s))}{\\sigma^2} \\, \\phi(s) \\] <p>This tells us that the gradient increases the likelihood of actions that are close to the mean \\(\\mu(s)\\) and reduces the probability of actions that deviate from it.</p> <p>Deep neural networks (and other differentiable models) can also be used to represent \\(\\pi_\\theta(a|s)\\), allowing score functions to be computed automatically using backpropagation.</p> <p>Intution:  Think of a sample trajectory \\(\\tau\\) as something we tried \u2014 a sequence of states, actions, and rewards collected during an episode. The return \\(R(\\tau)\\) tells us how good that sample was (higher return means better behavior). The gradient term \\(\\nabla_\\theta \\log P(\\tau|\\theta)\\) tells us how to adjust the policy parameters \\(\\theta\\) to make the trajectory more or less likely. So, when we multiply them:  we are effectively saying: If a trajectory was good, update the policy to make it more likely to occur again. If it was bad, update the policy to make it less likely. This simple idea is the core of policy gradient methods.</p>"},{"location":"reinforcement/5_policy_gradient/#reinforce-algorithm-monte-carlo-policy-gradient","title":"REINFORCE Algorithm (Monte Carlo Policy Gradient)","text":"<p>Update rule:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t \\] <p>Algorithm:</p> <p>1: Initialize policy parameters \\(\\theta\\) 2: loop (for each episode) 3: \\(\\quad\\) Generate a trajectory \\(\\tau = (s_0, a_0, r_1, \\dots, s_T)\\) using \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each time step \\(t\\) in \\(\\tau\\) 5: \\(\\quad\\quad\\) Compute return: \\(\\qquad R_t = \\sum_{k=t}^{T-1} \\gamma^{\\,k-t} r_{k+1}\\) 6: \\(\\quad\\quad\\) Compute policy gradient term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) 7: \\(\\quad\\quad\\) Update policy parameters: \\(\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t\\) 8: \\(\\quad\\) end for 9: end loop  </p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient-methods-mental-map","title":"Policy Gradient Methods \u2014 Mental Map","text":"<pre><code>                     Policy Gradient Methods\n    Goal: Learn the optimal policy \u03c0* directly (no Q or V tables)\n                               \u2502\n                               \u25bc\n         Key Concept: Parameterized Policy \u03c0\u03b8(a|s)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy is a function with parameters \u03b8      \u2502\n       \u2502 \u03c0\u03b8(a|s) gives probability of taking action a\u2502\n       \u2502 Optimization targets J(\u03b8)=Expected Return   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                    Direct Policy Optimization\n                               \u2502\n                               \u25bc\n                 Optimization Objective (J(\u03b8))\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03b8* = argmax\u03b8 V(\u03b8) = argmax\u03b8 E\u03c0\u03b8[G\u2080]         \u2502\n       \u2502 Search in parameter space for best policy   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Two Families of Policy Optimization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Gradient-Free Methods     \u2502   Gradient-Based Methods   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                      \u2502\n                \u2502                                      \u25bc\n                \u2502                          Policy Gradient Methods\n                \u2502                                      \u2502\n                \u25bc                                      \u25bc\n   No gradient needed                     Uses \u2207\u03b8 log \u03c0\u03b8(a|s) * Return\n   \u2013 Hill Climbing                        \u2013 REINFORCE\n   \u2013 CEM, CMA                             \u2013 Actor-Critic\n   \u2013 Genetic Algorithms                   \u2013 Advantage Methods\n   \u2502                                      \u2502\n   \u2514\u2500\u2500\u2500\u2500 Flexible &amp; parallelizable        \u2514\u2500\u2500\u2500\u2500 Sample efficient\n                               \u2502\n                               \u25bc\n                   Policy Gradient Core Idea\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Increase probability of good actions          \u2502\n       \u2502 Decrease probability of poor actions          \u2502\n       \u2502 Gradient term: \u2207\u03b8 log \u03c0\u03b8(a|s)(score function) \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     REINFORCE Algorithm (MC)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Sample full episodes (Monte Carlo)            \u2502\n       \u2502 Compute return Gt at each time step           \u2502\n       \u2502 Update: \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8 log \u03c0\u03b8(a_t|s_t) * G_t    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Policy Parameterization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Softmax Policy (Discrete)  \u2502 Gaussian Policy(Continuous)\u2502\n       \u2502 \u03c0\u03b8(a|s) = exp(...)         \u2502 a ~ N(\u03bc(s), \u03c3\u00b2)            \u2502\n       \u2502 \u2207\u03b8 log \u03c0\u03b8 = \u03c6 - E\u03c6         \u2502 \u2207\u03b8 log \u03c0\u03b8 = (a-\u03bc)/\u03c3\u00b2 * \u03c6   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Final Outcome\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly (no need for Q or V tables)   \u2502\n       \u2502 Works naturally with stochastic &amp; continuous    \u2502\n       \u2502 Supports neural network policy parameterization \u2502\n       \u2502 Foundation of modern deep RL (PPO, A3C, DDPG)   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/6_pg2/","title":"6. Policy Gradient Variance Reduction and Actor-Critic","text":""},{"location":"reinforcement/6_pg2/#chapter-6-policy-gradient-variance-reduction-and-actor-critic","title":"Chapter 6: Policy Gradient Variance Reduction and Actor-Critic","text":"<p>In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy \\(\\pi_\\theta(a|s)\\) in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:</p> <ul> <li> <p>High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return \\(G_t\\) depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.</p> </li> <li> <p>Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.</p> </li> <li> <p>Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.</p> </li> <li> <p>Parameter Space vs Policy Space Mismatch: A small change in policy parameters \\(\\theta\\) does not always translate to a small change in the policy\u2019s behavior. For example, in a two-action policy with probability \\(\\pi_\\theta(a=1)=\\sigma(\\theta)\\) (sigmoid), a slight shift in \\(\\theta\\) can swing the action probabilities significantly if \\(\\theta\\) is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.</p> </li> </ul> <p>The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor\u2013critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.</p>"},{"location":"reinforcement/6_pg2/#policy-gradient-theorem-and-reinforce","title":"Policy Gradient Theorem and REINFORCE","text":"<p>n the previous chapter, we derived an expression for the gradient of the policy objective:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^{m} R(\\tau^{(i)})  \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)}), \\] <p>where each trajectory \\(\\tau^{(i)}\\) is generated by the current policy \\(\\pi_\\theta\\).</p> <p>This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:</p> <ul> <li>The return \\(R(\\tau)\\) depends on the entire trajectory.</li> <li>Different trajectories can have very different returns.</li> <li>Updates become noisy, unstable, and slow to converge.</li> </ul> <p>In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.</p> <p>Goal: Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.</p>"},{"location":"reinforcement/6_pg2/#reducing-variance-with-baselines","title":"Reducing Variance with Baselines","text":"<p>One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action\u2019s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return. Mathematically, we modify the gradient as follows:</p> \\[ \\nabla_\\theta \\mathbb{E}_\\tau [R] \\;=\\; \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; \\left(\\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\\right) \\right]. \\] <p>where \\(b(s_t)\\) is an arbitrary baseline that depends on the state \\(s_t\\) (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, \\(b(s_t)\\) represents a reference level for the return at state \\(s_t\\); the term \\((G_t - b(s_t))\\) is asking: did the action at \\(s_t\\) do better or worse than this baseline expectation?</p> <p>A particularly good choice for the baseline is the value function under the current policy, \\(b(s_t) \\approx V_{\\pi}(s_t)\\). This is the expected return from state \\(s_t\\) if we continue following the current policy. Using \\(V_{\\pi}(s_t)\\) as \\(b(s_t)\\) minimizes variance because it subtracts out the expected part of \\(G_t\\), leaving only the unexpected advantage of the action \\(a_t\\). Using a value function baseline leads to defining the advantage function:  where \\(Q(s_t,a_t)\\) is the expected return for taking action \\(a_t\\) in \\(s_t\\) and following the policy thereafter, and \\(V(s_t)\\) is the expected return from \\(s_t\\) on average. The advantage \\(A(s_t,a_t)\\) tells us how much better or worse the chosen action was compared to the policy\u2019s typical action at that state. If \\(A(s_t,a_t)\\) is positive, the action did better than expected; if negative, it did worse than expected. Using \\(A(s_t,a_t)\\) in the gradient update focuses learning on the deviations from usual outcomes.</p> <p>Benefits of Using a Baseline (Advantage): 1.  Variance Reduction: Subtracting \\(V(s_t)\\) removes the predictable part of the return, reducing the variability of the term \\((G_t - b(s_t))\\). The policy update then depends on advantage, which typically has lower variance than raw returns. 2.  Focused Learning: The update ignores outcomes that are \u201cas expected\u201d and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment. 3.  Unbiased Gradient: Because \\(b(s_t)\\) does not depend on the action, the expected value of \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, b(s_t)\\) is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance</p> <p>Using a baseline in practice usually means we need to estimate the value function \\(V_{\\pi}(s)\\) for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress \\(b(s)\\) toward the observed returns \\(G_t\\) to improve the baseline estimate.</p> <p>Algorithm: Policy Gradient with Baseline (Advantage Estimation)</p> <p>1: Initialize policy parameter \\(\\theta\\), baseline \\(b(s)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) Collect a set of trajectories by executing the current policy \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each trajectory \\(\\tau^{(i)}\\) and each timestep \\(t\\) do 5: \\(\\quad\\quad\\) Compute return: \\(\\quad\\quad\\quad G_t^{(i)} = \\sum_{t'=t}^{T-1} r_{t'}^{(i)}\\) 6: \\(\\quad\\quad\\) Compute advantage estimate: \\(\\quad\\quad\\quad \\hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})\\) 7: \\(\\quad\\) end for 8: \\(\\quad\\) Re-fit baseline by minimizing: \\(\\quad\\quad \\sum_i \\sum_t \\big(b(s_t^{(i)}) - G_t^{(i)}\\big)^2\\) 9: \\(\\quad\\) Update policy parameters using gradient estimate: \\(\\quad\\quad \\theta \\leftarrow \\theta + \\alpha \\sum_{i,t} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\, \\hat{A}_t^{(i)}\\) 10: \\(\\quad\\) (Plug into SGD or Adam optimizer) 11: end for  </p> <p>This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect (\\(b(s)=V_\\pi(s)\\)), then \\(\\hat{A}t = G_t - V\\pi(s_t)\\) is an estimate of the advantage \\(A(s_t,a_t)\\). Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.</p>"},{"location":"reinforcement/6_pg2/#actorcritic-methods","title":"Actor\u2013Critic Methods","text":"<p>Using a learned baseline brings us to the idea of actor\u2013critic algorithms. In the policy gradient with baseline above, the policy is the \"actor\" and the value function baseline is a \"critic\" that evaluates the actor\u2019s decisions. Actor\u2013critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.</p> <ul> <li>Actor: the policy \\(\\pi_\\theta(a|s)\\) that selects actions and is updated by gradient ascent.</li> <li>Critic: a value function \\(V_w(s)\\) (with parameters \\(w\\)) that estimates the return from state \\(s\\) under the current policy. The critic provides the baseline or advantage estimates used in the actor\u2019s update.</li> </ul> <p>Instead of waiting for full episode returns \\(G_t\\), an actor\u2013critic uses the critic\u2019s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic\u2019s current estimate of advantage \\(A(s_t,a_t)\\), and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.</p> <p>Actor update: The policy (actor) update is similar to before, but using the critic\u2019s advantage estimate \\(A_t\\) at time \\(t\\):  Here \\(A_t \\approx Q(s_t,a_t) - V(s_t)\\) is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive \\(A_t\\)) and away from actions that seem worse than expected.</p> <p>Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error \\(\\delta_t\\) to update \\(w\\):  which measures the discrepancy between the predicted value at \\(s_t\\) and the reward plus discounted value of the next state. The critic\u2019s parameters \\(w\\) are updated by a gradient step proportional to \\(\\delta_t \\nabla_w V_w(s_t)\\) (this is essentially a TD(0) update). In practice:  with \\(\\beta\\) a critic learning rate. This update pushes the critic\u2019s value estimate \\(V_w(s_t)\\) toward the observed reward plus the estimated value of \\(s_{t+1}\\). The TD error \\(\\delta_t\\) is also used as an advantage estimate for the actor: notice \\(\\delta_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\) serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.</p> <p>Why Actor\u2013Critic? Actor\u2013critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor\u2013critic algorithms tend to: - Learn faster (lower variance updates thanks to the critic\u2019s guidance) - Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends) - Naturally handle continuing (non-episodic) tasks via the critic\u2019s ongoing value estimates - Still allow stochastic policies and continuous actions (since the actor is explicit)</p> <p>However, actor\u2013critics introduce bias through the critic\u2019s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor\u2013critic; later algorithms address this).</p> <p>Advantage Estimation: In an actor\u2013critic, one often uses n-step returns or more generally \\(\\lambda\\)-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return \\(R^{(3)}t = r_t + \\gamma r{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V(s_{t+3})\\), and define \\(\\hat{A}_t = R^{(3)}_t - V(s_t)\\). Smaller \\(n\\) gives lower variance but more bias; larger \\(n\\) gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.</p> <p>In summary, the actor\u2013critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy\u2019s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.</p>"},{"location":"reinforcement/6_pg2/#limitations-of-vanilla-policy-gradient-and-trust-region-motivation","title":"Limitations of Vanilla Policy Gradient and Trust-Region Motivation","text":"<p>Despite using baselines and even actor\u2013critic methods, vanilla policy gradient algorithms (including basic actor\u2013critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:</p> <ul> <li> <p>On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy \\(\\pi_{\\text{old}}\\) to update the current policy \\(\\pi_{\\text{new}}\\) introduces bias, because the gradient formula assumes data comes from \\(\\pi_{\\text{new}}\\). In short, standard policy gradients waste a lot of data, making them sample-inefficient.</p> </li> <li> <p>Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios \\(r_t = \\frac{\\pi_{\\text{new}}(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}\\). This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if \\(\\pi_{\\text{new}}\\) differs significantly from \\(\\pi_{\\text{old}}\\). A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.</p> </li> <li> <p>Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile \u2013 even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not \u201ctoo large\u201d in terms of its impact on the policy\u2019s behavior.</p> </li> <li> <p>Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in \\(\\theta\\)-space. A small change in parameters can lead to a disproportionate change in the policy\u2019s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes \u2260 small policy changes, it\u2019s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.</p> </li> </ul> <p>These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update \u2013 in other words, to stay within a \u201ctrust region\u201d around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback\u2013Leibler divergence as a distance between \\(\\pi_{\\text{new}}\\) and \\(\\pi_{\\text{old}}\\)), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor\u2013critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.</p>"},{"location":"reinforcement/7_gae/","title":"7. Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":""},{"location":"reinforcement/7_gae/#chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo","title":"Chapter 7: Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":"<p>In the previous chapter, we improved the foundation of policy gradients by reducing variance (using baselines) and introducing actor\u2013critic methods. We also noted that unrestricted policy updates can be unstable and sample-inefficient. In this chapter, we present modern advances in policy optimization that build on those ideas to achieve much better performance in practice. We focus on two main developments: Generalized Advantage Estimation (GAE), which refines how we estimate advantages to balance bias and variance, and trust-region methods (specifically Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)) that ensure updates do not destabilize the policy. These techniques enable more sample-efficient, stable learning by reusing data safely and preventing large detrimental policy shifts.</p>"},{"location":"reinforcement/7_gae/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>Accurate and low-variance advantage estimates are crucial for effective policy gradient updates. Recall that the policy gradient update uses the term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\,A(s_t,a_t)\\) \u2013 if \\(A(s_t,a_t)\\) is noisy or biased, it can severely affect learning. Advantage can be estimated via: - Monte Carlo returns: \\(A_t = G_t - V(s_t)\\) using the full return \\(G_t\\) (summing all future rewards until episode end). This is an unbiased estimator of the true advantage, but it has very high variance because it includes all random future outcomes.</p> <ul> <li> <p>One-step TD returns: \\(A_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\), using the critic\u2019s bootstrapped estimate of the future. This one-step advantage (equivalently the TD error \\(\\delta_t\\)) has much lower variance (it relies on the learned value for the next state) but is biased by function approximation and by truncating the return after one step.</p> </li> <li> <p>n-Step returns:We can also use intermediate approaches, for example a 2-step return \\(R^{(2)}t = r_t + \\gamma r{t+1} + \\gamma^2 V(s_{t+2})\\) giving an advantage \\(\\hat{A}^{(2)}_t = R^{(2)}_t - V(s_t)\\). In general, an n-step advantage estimator can be written as:</p> \\[A^{t}(n) = \\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i+1} + \\gamma^{n} V(s_{t+n}) -V(s_{t})\\] <p>which blends \\(n\\) actual rewards with a bootstrap at time \\(t+n\\). Smaller \\(n\\) (like 1) means more bias (due to heavy reliance on \\(V\\)) but low variance; larger \\(n\\) (approaching the episode length) reduces bias but increases variance.</p> </li> </ul> <p>The pattern becomes clearer if we express these in terms of the TD error \\(\\delta_t\\) (the one-step advantage at \\(t\\)):</p> <p>  - For a 1-step return, \\(\\hat{A}^{(1)}_t = \\delta_t\\). - For a 2-step return, \\(\\hat{A}^{(2)}t = \\delta_t + \\gamma\\,\\delta\\). - For an \\(n\\)-step return, \\(\\hat{A}^{(n)}t = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + \\cdots + \\gamma^{n-1}\\delta_{t+n-1}\\)</p> <p>Each additional term \\(\\gamma^i \\delta_{t+i}\\) extends the return by one more step of real reward before bootstrapping, increasing bias a bit (since it assumes the later \\(\\delta\\) terms are based on an approximate \\(V\\)) but capturing more actual reward outcomes (reducing variance less).</p> <p>Generalized Advantage Estimation (GAE) takes this idea to its logical conclusion by forming a weighted sum of all n-step advantages, with exponentially decreasing weights. Instead of picking a fixed \\(n\\), GAE uses a parameter \\(0 \\le \\lambda \\le 1\\) to blend advantages of different lengths:</p> \\[\\hat{A}^{\\text{GAE}(\\gamma,\\lambda)}_t \\;=\\; (1-\\lambda)\\Big(\\hat{A}^{(1)}_t + \\lambda\\,\\hat{A}^{(2)}_t + \\lambda^2\\,\\hat{A}^{(3)}_t + \\cdots\\Big)\\] <p>This infinite series can be shown to simplify to a very convenient form:</p> \\[\\hat{A}_t^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i\\delta_{t+i}\\] <p>which is an exponentially-weighted sum of the future TD errors. In practice, this is implemented with a simple recursion running backward through each trajectory (since it\u2019s a sum of discounted TD errors).</p> <p>Key intuition: \\(\\lambda\\) controls the bias\u2013variance trade-off in advantage estimation:</p> <ul> <li> <p>\\(\\lambda = 0\\) uses only the one-step TD error: \\(\\hat{A}^{\\text{GAE}(0)}_t = \\delta_t\\). This is the lowest-variance, highest-bias estimator (similar to TD(0) advantage)[21].</p> </li> <li> <p>\\(\\lambda = 1\\) uses an infinitely long sum of un-discounted TD errors, which in theory equals the full Monte Carlo return advantage (since all bootstrapping is deferred to the end). This is unbiased (in the limit of exact \\(V\\)) but highest variance \u2013 essentially Monte Carlo estimation.</p> </li> <li> <p>Intermediate \\(0&lt;\\lambda&lt;1\\) gives a mixture. A typical choice is \\(\\lambda = 0.95\\) in many applications, which provides a good balance (mostly long-horizon returns with a bit of bootstrapping to damp variance).</p> </li> </ul> <p>GAE is not introducing a new kind of return; rather, it generalizes existing returns. It smoothly interpolates between TD and Monte Carlo methods. When \\(\\lambda\\) is low, GAE trusts the critic more (using more bootstrapped estimates); when \\(\\lambda\\) is high, GAE leans toward actual returns over many steps. In modern actor\u2013critic algorithms (including TRPO and PPO), GAE is used to compute the advantage for each state-action in a batch. A typical implementation for each iteration is:</p> <ol> <li>Collect trajectories using the current policy \\(\\pi_{\\theta}\\) (e.g. run \\(N\\) episodes or \\(T\\) time steps of experience).</li> <li>Compute state values \\(V(s_t)\\) for each state visited (using the current value function estimate).</li> <li>Compute TD residuals \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) for each time step.</li> <li> <p>Apply GAE formula: going from \\(t=T-1\\) down to \\(0\\), accumulate \\(\\hat{A}_t = \\delta_t + \\gamma \\lambda, \\hat{A}{t+1}\\), with \\(\\hat{A}_{T} = 0\\). This yields \\(\\hat{A}_t \\approx \\sum{i\\ge0} (\\gamma \\lambda)^i \\delta{t+i}\\).</p> </li> <li> <p>Use Advantages for Update: These \\(\\hat{A}_t\\) values serve as the advantage estimates in the policy gradient update. Simultaneously, you can compute proxy returns for the critic by adding \\(\\hat{A}_t\\) to the baseline \\(V(s_t)\\) (i.e. \\(\\hat{R}_t = \\hat{A}_t + V(s_t)\\), an estimate of the actual return) and use those to update the value function parameters.</p> </li> </ol> <p>The result of GAE is a much smoother, lower-variance advantage signal for the actor, without introducing too much bias. Empirically, this greatly stabilizes training: the policy doesn\u2019t overreact to single high-return episodes, and it doesn\u2019t ignore long-term outcomes either. GAE essentially bridges the gap between the high-variance Monte Carlo world of Chapter 5 and the low-variance TD world of Chapter 3\u20134, and it has become a standard component in virtually all modern policy optimization algorithms.</p>"},{"location":"reinforcement/7_gae/#kl-divergence-constraints-and-surrogate-objectives","title":"KL Divergence Constraints and Surrogate Objectives","text":"<p>We now turn to the question of stable policy updates. As discussed, a major issue with vanilla policy gradient is that a single update can accidentally push the policy into a disastrous region (because the gradient is computed at the current policy but we might step too far). To make updates safer, we want to constrain how much the policy changes at each step. A natural way to measure change between the old policy \\(\\pi_{\\text{old}}\\) and a new policy \\(\\pi_{\\text{new}}\\) is to use the Kullback\u2013Leibler (KL) divergence. For example, we can require:</p> \\[\\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}}(\\cdot \\mid s)\\,\\|\\,\\pi_{\\text{old}}(\\cdot \\mid s)\\bigr) \\right] \\le \\delta\\] <p>for some small \\(\\delta\\). This means that on average over states (under the old policy\u2019s state distribution \\(d_{\\pi_{\\text{old}}}\\)), the new policy\u2019s probability distribution is not too far from the old policy\u2019s distribution. A small KL divergence ensures the policies behave similarly, limiting the \u201csurprise\u201d from one update.</p> <p>But how do we optimize under such a constraint? We need an objective function that tells us whether \\(\\pi_{\\text{new}}\\) is better than \\(\\pi_{\\text{old}}\\). Fortunately, theory provides a useful tool: a surrogate objective that approximates the change in performance if the policy change is small. One version, derived from the policy performance difference lemma and monotonic improvement theorem, is:</p> \\[L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\text{old}}} \\left[ \\frac{\\pi_{\\text{new}}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A_{\\pi_{\\text{old}}}(s,a) \\right]\\] <p>This is an objective functional\u2014it evaluates the new policy using samples from the old policy, weighting rewards by the importance ratio \\(r(s,a) = \\pi_{\\text{new}}(a|s)/\\pi_{\\text{old}}(a|s)\\). Intuitively, \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) is asking: if the old policy visited state \\(s\\) and took action \\(a\\), how good would that decision be under the new policy\u2019s probabilities? Actions that the new policy wants to do more of (\\(r &gt; 1\\)) will contribute their advantage (good or bad) proportionally more.</p> <p>Critically, one can show that if \\(\\pi_{\\text{new}}\\) is very close to \\(\\pi_{\\text{old}}\\) (in KL terms), then improving this surrogate \\(L\\) guarantees an improvement in the true return \\(J(\\pi)\\). Specifically, there is a bound such that:</p> \\[J(\\pi_{\\text{new}}) \\ge J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - C \\, \\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}} \\,\\|\\, \\pi_{\\text{old}}\\bigr)[s] \\right]\\] <p>for some constant \\(C\\) related to horizon and policy support. When the KL divergence is small, the last term is second-order (negligible), so roughly we get \\(J(\\pi_{\\text{new}}) \\gtrapprox J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\). In other words, maximizing \\(L\\) while keeping KL small ensures monotonic improvement: each update should not reduce true performance.</p> <p>This insight leads directly to a constrained optimization formulation for safe policy updates:</p> <ul> <li>Objective: Maximize the surrogate \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) (i.e. maximize expected advantage-weighted probability ratios).</li> <li>Constraint: Limit the policy divergence via \\(D_{\\mathrm{KL}}(\\pi_{\\text{new}}\\Vert \\pi_{\\text{old}}) \\le \\delta\\) (for some small \\(\\delta\\)). Algorithms that implement this idea are called trust-region methods, because they optimize the policy within a trust region of the old policy. Next, we discuss two prominent algorithms: TRPO, which tackles the constrained problem directly (with some approximations), and PPO, which simplifies it into an easier unconstrained loss function.</li> </ul>"},{"location":"reinforcement/7_gae/#trust-region-policy-optimization","title":"Trust Region Policy Optimization","text":"<p>Trust Region Policy Optimization (TRPO) is a seminal algorithm that explicitly embodies the constrained update approach. TRPO chooses a new policy by approximately solving:</p> \\[\\max_{\\theta_{\\text{new}}} \\; L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}}) \\quad \\text{s.t.} \\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\theta_{\\text{new}}} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\bigr) \\right] \\le \\delta\\] <p>where \\(L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}})\\) is the surrogate objective defined above, and \\(\\delta\\) is a small trust-region threshold. In practice, solving this exactly is difficult due to the infinite-dimensional policy space. TRPO makes it tractable by using a few key ideas:</p> <ul> <li> <p>Approximating the constraint via a quadratic expansion of the KL divergence (which yields a Fisher Information Matrix). This turns the problem into something like a second-order update (a natural gradient step). In fact, TRPO\u2019s solution can be shown to correspond to a natural gradient ascent:</p> <p>\\(\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^T F^{-1} g}}\\; F^{-1} g\\)$</p> <p>where \\(g = \\nabla_\\theta L\\) and \\(F\\) is the Fisher matrix. This ensures the KL constraint is satisfied approximately, and is equivalent to scaling the gradient by \\(F^{-1}\\). In simpler terms, TRPO updates \\(\\theta\\) in a direction that accounts for the curvature of the policy space, so that the change in policy (KL) is proportional to the step size.</p> </li> <li> <p>Using a line search to ensure the new policy actually improves \\(J(\\pi)\\). TRPO will back off the step size if the updated policy violates the constraint or fails to achieve a performance improvement. This safeguard maintains the monotonic improvement guarantee in practice.</p> </li> </ul> <p>A simplified outline of TRPO is:</p> <ol> <li>Collect trajectories with the current policy \\(\\pi_{\\theta_{\\text{old}}}\\).</li> <li>Estimate advantages \\(\\hat{A}_t\\) for each time step (using GAE or another method for high-quality advantage estimates).</li> <li>Compute surrogate objective \\(L(\\theta) = \\mathbb{E}[r_t(\\theta), \\hat{A}t]\\) where \\(r_t(\\theta) = \\frac{\\pi{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\).</li> <li>Approximate KL constraint: Compute the policy gradient \\(\\nabla_\\theta L\\) and the Fisher matrix \\(F\\) (via sample-based estimation of the Hessian of the KL divergence). Solve for the update direction \\(p \\approx F^{-1} \\nabla_\\theta L\\) (e.g. using conjugate gradient).</li> <li>Line search: Scale and apply the update step \\(\\theta \\leftarrow \\theta + p\\) gradually, checking the KL and improvement. Stop when the KL constraint or improvement criterion is satisfied.</li> </ol> <p>TRPO\u2019s updates are therefore conservative by design \u2013 they will only take as large a step as can be trusted not to degrade performance. TRPO was influential because it demonstrated much more stable and reliable training on complex continuous control tasks than vanilla policy gradient.</p> <p>Strengths and Weaknesses of TRPO: TRPO offers a theoretical guarantee of non-destructive updates \u2013 under certain assumptions, each iteration is guaranteed to improve or at least not decrease performance. It uses a natural gradient approach that respects the geometry of policy space, which is more effective than an arbitrary gradient in parameter space. However, TRPO comes at a cost: it requires calculating second-order information (the Fisher matrix), and implementing the conjugate gradient solver and line search adds complexity. The algorithm can be slower per iteration and is more complex to code and tune. In practice, TRPO, while effective, proved somewhat cumbersome for large-scale problems due to these complexities.</p>"},{"location":"reinforcement/7_gae/#proximal-policy-optimization","title":"Proximal Policy Optimization","text":"<p>Proximal Policy Optimization (PPO) was introduced as a simpler, more user-friendly variant of TRPO that achieves similar results with only first-order optimization. The core idea of PPO is to keep the spirit of trust-region updates (don\u2019t move the policy too far in one go) but implement it via a relaxed objective that can be optimized with standard stochastic gradient descent. There are two main variants of PPO:</p>"},{"location":"reinforcement/7_gae/#kl-penalty-objective","title":"KL-Penalty Objective:","text":"<p>One version of PPO adds the KL-divergence as a penalty to the objective rather than a hard constraint. The objective becomes:</p> \\[J_{\\text{PPO-KL}}(\\theta) = \\mathbb{E}\\!\\left[ r_t(\\theta)\\, \\hat{A}^t \\right] - \\beta \\,\\mathbb{E}\\!\\left[ D_{\\mathrm{KL}}\\!\\left(\\pi_{\\theta} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\right) \\right]\\] <p>where \\(\\beta\\) is a coefficient determining how strongly to penalize deviation from the old policy. If the KL divergence in an update becomes too large, \\(\\beta\\) can be adjusted (increased) to enforce smaller steps in subsequent updates. This approach maintains a soft notion of a trust region.</p>"},{"location":"reinforcement/7_gae/#algorithm-ppo-with-kl-penalty","title":"Algorithm (PPO with KL Penalty)","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), initial KL penalty \\(\\beta_0\\), target KL-divergence \\(\\delta\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^t_k\\) using any advantage estimation algorithm 5: \\(\\quad\\) Compute policy update by approximately solving \\(\\quad\\quad\\) \\(\\theta_{k+1} = \\arg\\max_\\theta \\; L_{\\theta_k}(\\theta) - \\beta_k \\hat{D}_{KL}(\\theta \\,\\|\\, \\theta_k)\\) 6: \\(\\quad\\) Implement this optimization with \\(K\\) steps of minibatch SGD (e.g., Adam) 7: \\(\\quad\\) Measure actual KL: \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k)\\) 8: \\(\\quad\\) if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\ge 1.5\\delta\\) then 9: \\(\\quad\\quad\\) Increase penalty: \\(\\beta_{k+1} = 2\\beta_k\\) 10: \\(\\quad\\) else if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\le \\delta/1.5\\) then 11: \\(\\quad\\quad\\) Decrease penalty: \\(\\beta_{k+1} = \\beta_k/2\\) 12: \\(\\quad\\) end if 13: end for  </p>"},{"location":"reinforcement/7_gae/#clipped-surrogate-objective-ppo-clip","title":"Clipped Surrogate Objective (PPO-Clip):","text":"<p>The more popular variant of PPO uses a clipped surrogate objective to restrict policy updates:</p> \\[L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t}\\!\\left[ \\min\\!\\Big( r_t(\\theta)\\,\\hat{A}^t,\\; \\text{clip}\\!\\big(r_t(\\theta),\\, 1-\\epsilon,\\, 1+\\epsilon\\big)\\,\\hat{A}^t \\Big) \\right]\\] <p>where \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\) as before, and \\(\\epsilon\\) is a small hyperparameter (e.g. 0.1 or 0.2) that defines the clipping range. This objective says: if the new policy\u2019s probability ratio \\(r_t(\\theta)\\) stays within \\([1-\\epsilon,\\,1+\\epsilon]\\), we use the normal surrogate \\(r_t \\hat{A}_t\\). But if \\(r_t\\) tries to go outside this range (meaning the policy probability for an action has changed dramatically), we clip \\(r_t\\) to either \\(1+\\epsilon\\) or \\(1-\\epsilon\\) before multiplying by \\(\\hat{A}_t\\). Effectively, the advantage contribution is capped once the policy deviates too much from the old policy.</p> <p>The clipped objective is not exactly the original constrained problem, but it serves a similar purpose: it removes the incentive for the optimizer to push \\(r_t\\) outside of \\([1-\\epsilon,1+\\epsilon]\\). If increasing \\(|\\theta|\\) further doesn\u2019t increase the objective (because the min() will select the clipped term), then overly large policy changes are discouraged.</p> <p>Why Clipping Works: Clipping is a simple heuristic, but it has proven extremely effective:</p> <ul> <li> <p>It enforces a soft trust region by preventing extreme updates for any single state-action probability. The policy can still change, but not so much that any one probability ratio blows up.</p> </li> <li> <p>It avoids the complexity of solving a constrained optimization or computing second-order derivatives \u2013 we can just do standard SGD on \\(L^{CLIP}(\\theta)\\).</p> </li> <li> <p>It keeps importance sampling ratios near 1, which means the algorithm can safely perform multiple epochs of updates on the same batch of data without the estimates drifting too far. This directly improves sample efficiency (unlike vanilla policy gradient, PPO typically updates each batch for several epochs).</p> </li> </ul>"},{"location":"reinforcement/7_gae/#ppo-clipped-algorithm","title":"PPO (Clipped) Algorithm","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), clipping threshold \\(\\epsilon\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect a set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^{\\,t}_k\\) using any advantage estimation algorithm (e.g., GAE) 5: \\(\\quad\\) Define the clipped surrogate objective \\(\\quad\\quad\\)  6: \\(\\quad\\) Update policy parameters with several epochs of minibatch SGD to approximately maximize \\(\\mathcal{L}^{\\text{CLIP}}_{\\theta_k}(\\theta)\\) 7: \\(\\quad\\) Set \\(\\theta_{k+1}\\) to the resulting parameters 8: end for  </p> <p>In practice, PPO with clipping has become one of the most widely used RL algorithms because it strikes a good balance between performance and simplicity. It is relatively easy to implement (compared to TRPO) and has been found to be robust across many tasks and hyperparameters. While it doesn\u2019t guarantee monotonic improvement in theory, in practice it achieves stable training behavior very similar to TRPO.</p> <p>In modern practice, PPO is the dominant choice for policy optimization in deep RL, due to its relative simplicity and strong performance across many environments. TRPO is still important conceptually (and sometimes used in scenarios where theoretical guarantees are desired), but PPO\u2019s convenience usually wins out.</p>"},{"location":"reinforcement/7_gae/#putting-it-together-sample-efficiency-stability-and-monotonic-improvement","title":"Putting It Together: Sample Efficiency, Stability, and Monotonic Improvement","text":"<p>The advances covered in this chapter are often used together in state-of-the-art algorithms:</p> <ul> <li> <p>Generalized Advantage Estimation (GAE) provides high-quality advantage estimates that significantly reduce variance without too much bias. This means we can get away with smaller batch sizes or fewer episodes to get a good learning signal \u2013 improving sample efficiency.</p> </li> <li> <p>Trust-region update rules (TRPO/PPO) ensure that each policy update is safe and stable \u2013 the policy doesn\u2019t change erratically, preventing the kind of catastrophic drops in reward that naive policy gradients can suffer. By keeping policy changes small (via KL constraints or clipping), these methods enable multiple updates on the same batch of data (improving data efficiency) and maintain policy monotonicity, i.e. each update is expected to improve or at least not significantly degrade performance.</p> </li> <li> <p>In practice, an algorithm like PPO with GAE is an actor\u2013critic method that uses all these ideas: an actor policy updated with a clipped surrogate objective (making updates stable), a critic to approximate \\(V(s)\\) (enabling advantage estimation), GAE to compute advantages (trading off bias/variance), and typically multiple gradient epochs per batch to squeeze more learning out of each sample. This combination has proven remarkably successful in domains from simulated control tasks to games.</p> </li> </ul> <p>By building on the foundational policy gradient framework and addressing its shortcomings, GAE and trust-region approaches have made deep reinforcement learning much more practical and reliable. They illustrate how theoretical insights (performance bounds, policy geometry) and practical tricks (advantage normalization, clipping) come together to yield algorithms that can solve challenging RL problems while using reasonable amounts of training data and maintaining stability throughout learning. Each component \u2013 be it advantage estimation or constrained updates \u2013 plays a role in ensuring that learning is as efficient, stable, and monotonic as possible. Together, they represent the state-of-the-art toolkit for policy optimization in reinforcement learning.</p> Method Key Idea Pros Cons REINFORCE MC return-based policy gradient Simple, unbiased Very high variance Actor\u2013Critic TD baseline value function More sample-efficient Requires critic Advantage Actor\u2013Critic Uses \\(A(s,a)\\) for updates Best bias\u2013variance trade Needs accurate value est. TRPO Trust-region with KL constraint Strong theory, stable Complex, second-order PPO Clipped/penalized surrogate objective Simple, stable, popular Heuristic, tuning needed"},{"location":"reinforcement/7_gae/#mental-map","title":"Mental Map","text":"<pre><code>                  Advanced Policy Gradient Methods\n     Goal: Fix limitations of vanilla PG (variance, stability, KL control)\n                               \u2502\n                               \u25bc\n             Core Challenges in Policy Gradient Methods\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 High variance (MC returns)                             \u2502\n       \u2502 Poor sample efficiency (on-policy only)                \u2502\n       \u2502 Sensitive to step size \u2192 catastrophic policy collapse  \u2502\n       \u2502 Small \u03b8 change \u2260 small policy change                   \u2502\n       \u2502 Reusing old data is unstable                           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Variance Reduction (Baselines)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Introduce baseline b(s) \u2192 subtract expectation         \u2502\n       \u2502 Keeps estimator unbiased                               \u2502\n       \u2502 Good choice: b(s)= V(s) \u2192 yields Advantage A(s,a)      \u2502\n       \u2502 Update based on: how much action outperformed expected \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                       Advantage Function A(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 A(s,a) = Q(s,a) \u2013 V(s)                                 \u2502\n       \u2502 Measures how much BETTER the action was vs average     \u2502\n       \u2502 Positive \u2192 increase \u03c0\u03b8(a|s); Negative \u2192 decrease it    \u2502\n       \u2502 Major variance reduction \u2013 foundation of Actor\u2013Critic  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Actor\u2013Critic Framework\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Actor: policy \u03c0\u03b8(a|s)                                  \u2502\n       \u2502 Critic: value function V(s;w) estimates baseline       \u2502\n       \u2502 TD error \u03b4t reduces variance (bootstrapping)           \u2502\n       \u2502 Faster, more sample-efficient than REINFORCE           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Target Estimation for the Critic\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Monte Carlo (\u221e-step)       \u2502  TD (1-step)               \u2502\n       \u2502 + Unbiased                 \u2502  + Low variance            \u2502\n       \u2502 \u2013 High variance            \u2502  \u2013 Biased                  \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502 n-Step Returns: Blend of TD and MC                      \u2502\n       \u2502 Control bias\u2013variance by choosing n                     \u2502\n       \u2502 Larger n \u2192 MC-like; smaller n \u2192 TD-like                 \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n             Fundamental Problems with Vanilla Policy Gradient\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Uses each batch for ONE gradient step (on-policy)      \u2502\n       \u2502 Step size is unstable \u2192 huge performance collapse      \u2502\n       \u2502 Small changes in \u03b8 \u2192 large unintended policy changes   \u2502\n       \u2502 Need mechanism to limit POLICY CHANGE, not \u03b8 change    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Safe Policy Improvement Theory \u2192 TRPO &amp; PPO\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy Performance Difference Lemma                    \u2502\n       \u2502   J(\u03c0') \u2212 J(\u03c0) = E\u03c0' [A\u03c0(s,a)]                         \u2502\n       \u2502 KL Divergence as policy distance metric                \u2502\n       \u2502   D_KL(\u03c0'||\u03c0) small \u2192 safe update                      \u2502\n       \u2502 Monotonic Improvement Bound                            \u2502\n       \u2502   Lower bound on J(\u03c0') using surrogate loss L\u03c0(\u03c0')     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                   Surrogate Objective for Safe Updates\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 L\u03c0(\u03c0') = E[ (\u03c0'(a|s)/\u03c0(a|s)) * A\u03c0(s,a) ]               \u2502\n       \u2502 Importance sampling + KL regularization                \u2502\n       \u2502 Foundation of Trust-Region Policy Optimization (TRPO)  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Proximal Policy Optimization (PPO) \u2013 Key Ideas\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 PPO-KL Penalty             \u2502 PPO-Clipped Objective      \u2502\n       \u2502 Adds \u03b2\u00b7KL to loss          \u2502 Clips ratio r_t(\u03b8) to      \u2502\n       \u2502 Adjust \u03b2 adaptively        \u2502 [1\u2212\u03b5, 1+\u03b5] to prevent      \u2502\n       \u2502 Prevents large updates     \u2502 destructive policy jumps   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         PPO Algorithm Summary\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 1. Collect trajectories from old policy                \u2502\n       \u2502 2. Estimate advantages A\u0302_t (GAE, TD, etc.)            \u2502\n       \u2502 3. Optimize clipped surrogate for many epochs          \u2502\n       \u2502 4. Update parameters safely                            \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                          Final Outcome (Chapter 6)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Stable and efficient policy optimization               \u2502\n       \u2502 Reuse data safely across multiple updates              \u2502\n       \u2502 Avoid catastrophic policy collapse                     \u2502\n       \u2502 Foundation of modern deep RL algorithms                \u2502\n       \u2502 (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/8_imitation_learning/","title":"8. Imitation Learning","text":""},{"location":"reinforcement/8_imitation_learning/#chapter-8-imitation-learning","title":"Chapter 8: Imitation Learning","text":"<p>In previous chapters, we focused on reinforcement learning with explicit reward signals guiding the agent's behavior. We assumed that a well-defined reward function \\(R(s,a)\\) was provided as part of the MDP, and the agent\u2019s goal was to learn a policy that maximizes cumulative reward. But what if specifying the reward is difficult or the agent cannot safely explore to learn from reward? Imitation Learning (IL) addresses these scenarios by leveraging expert demonstrations instead of explicit rewards.</p> <p>Imitation Learning allows an agent to learn how to act by mimicking an expert\u2019s behavior, rather than by maximizing a hand-crafted reward.</p>"},{"location":"reinforcement/8_imitation_learning/#motivation-the-case-for-learning-from-demonstrations","title":"Motivation: The Case for Learning from Demonstrations","text":"<p>Designing a reward function that truly captures the desired behavior can be extremely challenging. A misspecified reward can lead to unintended behaviors (reward hacking) or require exhaustive tuning. Even with a good reward, some environments present sparse rewards (e.g. only a success/failure signal at the very end of an episode) \u2013 making pure trial-and-error learning inefficient. In other cases, unsafe exploration is a concern: letting an agent freely explore (as classic RL would) could be dangerous or costly (imagine a self-driving car learning by crashing to discover that crashing is bad).</p> <p>However, in many of these settings expert behavior is available: we might have logs of human drivers driving safely, or demonstrations of a robot performing the task. Imitation Learning leverages this data. Instead of specifying what to do via a reward function, we show the agent how to do it via example trajectories. The agent's objective is then to imitate the expert as closely as possible.</p> <p>This paradigm contrasts with reward-based RL in key ways:</p> <ul> <li> <p>Reward-Based RL: The agent explores and learns by trial-and-error, guided by a numeric reward signal for feedback. It requires careful reward design and often extensive exploration.</p> </li> <li> <p>Imitation Learning: The agent learns from demonstrations of the desired behavior, treating the expert\u2019s actions as ground truth. No explicit reward is needed to train; learning is driven by matching the expert's behavior.</p> </li> </ul> <p>By learning from an expert, IL can produce competent policies much faster and safer in these scenarios. It essentially sidesteps the credit assignment problem of RL (because the \"right\" action is directly provided by the expert) and avoids dangerous exploration. In domains like autonomous driving, robotics, or any task where a human can demonstrate the skill, IL offers a powerful shortcut to get an agent up to a reasonable performance.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-problem-setup","title":"Imitation Learning Problem Setup","text":"<p>Formally, we can describe the imitation learning scenario using the same environment structure as an MDP \\((S, A, P, R, \\gamma)\\) except that the reward function \\(R\\) is unknown or not used. The agent still has a state space \\(S\\), an action space \\(A\\), and the environment transition dynamics \\(P(s' \\mid s, a)\\). What we do have, instead of \\(R\\), is access to expert demonstrations. An expert (which could be a human or a pre-trained optimal agent) provides example trajectories:</p> \\[ \\tau_E = (s_0, a_0, s_1, a_1, \\dots , s_T) \\] <p>collected by following the expert\u2019s policy \\(\\pi_E\\) in the environment. We may have a dataset \\(D\\) of these expert trajectories (or simply a set of state-action pairs drawn from expert behavior). The key point is that in IL, the agent does not receive numeric rewards from the environment. Instead, success is measured by how well the agent\u2019s behavior matches the expert\u2019s behavior.</p> <p>The goal of imitation learning can be stated as: find a policy \\(\\pi\\) for the agent that reproduces the expert's behavior (and ideally, achieves similar performance on the task). If the expert is optimal or highly skilled, we hope \\(\\pi\\) will achieve near-optimal results as well. This is an alternative path to finding a good policy without ever specifying a reward function explicitly or performing unguided exploration.</p> <p>(If we imagine there was some true but unknown reward \\(R\\) the expert is optimizing, then ideally \\(\\pi\\) should perform nearly as well as \\(\\pi_E\\) on that reward. IL attempts to reach that outcome via demonstrations rather than explicit reward feedback.)</p>"},{"location":"reinforcement/8_imitation_learning/#3-behavioral-cloning-learning-by-supervised-imitation","title":"3. Behavioral Cloning: Learning by Supervised Imitation","text":"<p>The most direct approach to imitation learning is Behavioral Cloning. Behavioral cloning treats imitation as a pure supervised learning problem: we train a policy to map states to the expert\u2019s actions, using the expert demonstrations as labeled examples. In essence, the agent \"clones\" the expert's behavior by learning to predict the expert's action in any given state.</p> <p>BC: Learn state to action mappings using expert demonstrations.</p> <p>In practice, we parameterize a policy \\(\\pi_\\theta(a\\mid s)\\) (e.g. a neural network with parameters \\(\\theta\\)) and adjust \\(\\theta\\) so that \\(\\pi_\\theta(\\cdot\\mid s)\\) is as close as possible to the expert\u2019s action choice in state \\(s\\). We define a loss function on the dataset of state-action pairs. For example:</p> <ul> <li>Discrete actions: Use cross-entropy (negative log-likelihood) of the expert\u2019s action.</li> </ul> \\[L(\\theta) = - \\mathbb{E}_{(s,a)\\sim D}\\left[ \\log \\pi_{\\theta}(a \\mid s) \\right]\\] <ul> <li>Continuous actions: Use mean squared error (regression loss).</li> </ul> \\[L(\\theta) = \\mathbb{E}_{(s,a)\\sim D} \\left[ \\left( \\pi_\\theta(s) - a \\right)^2 \\right]\\] <p>Minimizing these losses drives the policy to imitate the expert decisions on the training set.</p> <p>Training a behavioral cloning agent typically involves three steps:</p> <ol> <li> <p>Collect demonstrations: Gather a dataset \\(D = {(s_i, a_i)}\\) of expert state-action examples by observing the expert \\(\\pi_E\\) in the environment.</p> </li> <li> <p>Supervised learning on \\((s, a)\\) pairs: Choose a policy representation for \\(\\pi_\\theta\\) and use the collected data to adjust \\(\\theta\\). For each example \\((s_i, a_i)\\), we update \\(\\pi_\\theta\\) to reduce the error between its prediction \\(\\pi_\\theta(s_i)\\) and the expert\u2019s action \\(a_i\\). (For instance, if actions are discrete, we increase the probability \\(\\pi_\\theta(a_i \\mid s_i)\\) for the expert\u2019s action; if continuous, we move \\(\\pi_\\theta(s_i)\\) closer to \\(a_i\\) in value.)</p> </li> <li> <p>Deployment: Once the policy is trained (approximating \\(\\pi_E\\)), we fix \\(\\theta\\). The agent then acts autonomously: at each state \\(s\\), it outputs \\(a = \\pi_\\theta(s)\\) as its action. Ideally, this learned policy will behave similarly to the expert in the environment.</p> </li> </ol> <p>If the expert demonstrations are representative of the situations the agent will face, behavioral cloning can yield a policy that mimics the expert\u2019s behavior effectively. BC has some clear advantages:</p> <ul> <li> <p>Simplicity: It reduces policy learning to standard supervised learning, for which many stable algorithms and optimizations exist.</p> </li> <li> <p>Offline training: The model can be trained entirely from pre-recorded expert data, without requiring interactive environment feedback. This makes it data-efficient in terms of environment interactions.</p> </li> <li> <p>Safety: No random exploration is needed. The agent never tries highly suboptimal actions during training, since it always learns from demonstrated good behavior (critical in safety-sensitive domains).</p> </li> </ul> <p>However, purely copying the expert also comes with important limitations.</p>"},{"location":"reinforcement/8_imitation_learning/#covariate-shift-and-compounding-errors","title":"Covariate Shift and Compounding Errors","text":"<p>The main problem with behavioral cloning is that the training distribution of states can differ from the test distribution when the agent actually runs. During training, \\(\\pi_\\theta\\) is only exposed to states that the expert visited. But once the agent is deployed, if it ever deviates even slightly from the expert\u2019s trajectory, it may enter states not seen in the training data. In those unfamiliar states, the policy\u2019s predictions may be unreliable, leading to errors that cause it to drift further from expert-like behavior.</p> <p>A small mistake can snowball: once the agent strays from what the expert would do, it encounters novel situations where its learned policy might be very poor. One error leads to another, and the agent can cascade into failure because it was never taught how to recover.</p> <p>This phenomenon is known as covariate shift or distributional shift. The learner is trained on the state distribution induced by the expert policy \\(\\pi_E\\), but it is testing on the state distribution induced by its own policy \\(\\pi_\\theta\\). Unless \\(\\pi_\\theta\\) is perfect, these distributions will diverge over time, and the divergence can grow unchecked. In other words, the agent might handle situations similar to the expert's trajectories well, but if it finds itself in a situation the expert never encountered (often a result of a prior mistake), it has no guidance on what to do and can rapidly veer off course. This is often illustrated by the example of a self-driving car learned by BC: if it slightly misjudges a turn and drifts, it may end up in a part of the road it never saw during training, leading to more errors (compounding until possibly a crash).</p> <p>Another limitation is that BC does not inherently guarantee optimality or improvement beyond the expert: the policy is only as good as the demonstration data. If the expert is suboptimal or the dataset doesn\u2019t cover certain scenarios, the cloned policy will reflect those shortcomings and cannot improve by itself (since it has no feedback signal like reward to further refine its behavior). In reinforcement learning terms, BC has no notion of feedback for success or failure; it merely apes the expert, so it cannot discover better strategies or correct mistakes outside the expert's shadow.</p> <p>Researchers have developed strategies to mitigate the covariate shift problem. One approach is Dataset Aggregation (DAgger), which is an iterative algorithm: after training an initial policy via BC, let the policy interact with the environment and observe where it makes mistakes or visits unseen states; then have the expert provide the correct actions for those states, add these state-action pairs to the training set, and retrain the policy. By repeating this process, the policy\u2019s training distribution is gradually brought closer to the distribution it will encounter when it controls the agent. DAgger can significantly reduce compounding errors, but it requires ongoing access to an expert for feedback during training.</p> <p>In summary, behavioral cloning is a powerful first step for imitation learning\u2014it's straightforward and avoids many challenges of pure RL. But one must be mindful of its limitations: a blindly cloned policy can fail catastrophically when it encounters situations outside the expert\u2019s experience. This motivates more sophisticated imitation learning methods that incorporate the dynamics of the environment and attempt to infer the intent behind expert actions, rather than just copying them. We turn to those next.</p>"},{"location":"reinforcement/8_imitation_learning/#inverse-reinforcement-learning-learning-the-why","title":"Inverse Reinforcement Learning: Learning the \"Why\"","text":"<p>Behavioral cloning directly learns what to do (mapping states to actions) but does not capture why those actions are desirable. Inverse Reinforcement Learning (IRL) instead asks: Given expert behavior, what underlying reward function \\(R\\) could explain it? In other words, IRL attempts to reverse-engineer the expert's objectives from its observed behavior.</p> <p>In IRL, we assume that the expert \\(\\pi_E\\) is (approximately) optimal for some unknown reward function \\(R^*\\). The goal is to infer a reward function \\(\\hat{R}\\) such that, if an agent were to optimize \\(\\hat{R}\\), it would reproduce the expert\u2019s behavior. Formally, we want \\(\\pi_E\\) to be the optimal policy under the learned reward:</p> \\[\\pi_E = \\arg\\max_{\\pi} \\, V_R^{\\pi}\\] <p>where \\(V^{\\pi}_{\\hat{R}}\\) is the expected return of policy \\(\\pi\\) under the reward function \\(\\hat{R}\\). In words, the expert should have higher cumulative reward (according to \\(\\hat{R}\\)) than any other policy. If we can find such an \\(\\hat{R}\\), we have explained the expert\u2019s behavior in terms of incentives.</p> <p>Intuition: IRL flips the reinforcement learning problem on its head. Rather than starting with a reward and finding a policy, we start with a policy (the expert's) and try to find a reward that this policy optimizes. It's like observing an expert driver and deducing that they must be implicitly trading off goals like \"reach the destination quickly\" and \"avoid collisions\" because their driving balances speed and safety.</p> <p>One challenge is that IRL is inherently an under-defined (ill-posed) problem: many possible reward functions might make \\(\\pi_E\\) appear optimal. To resolve this ambiguity, IRL algorithms introduce additional criteria or regularization. For example, they might prefer the simplest reward function that explains the behavior, or in the case of maximum entropy IRL, prefer a reward that leads to the most random (maximally entropic) policy among those that match the expert's behavior \u2013 this avoids overly narrow explanations and spreads probability over possible behaviors unless forced by data.</p> <p>Once a candidate reward function \\(\\hat{R}(s,a)\\) is learned through IRL, the process typically continues as follows: we plug \\(\\hat{R}\\) back into the environment and solve a forward RL problem (using any suitable algorithm from earlier chapters) to obtain a policy \\(\\pi_{\\hat{R}}\\) that maximizes this recovered reward. Ideally, \\(\\pi_{\\hat{R}}\\) will then behave similarly to the expert's policy \\(\\pi_E\\) (since \\(\\hat{R}\\) was chosen to explain \\(\\pi_E\\)). The end result is an agent that not only imitates the expert, but also has an explicit reward model of the task it is performing.</p> <p>IRL is usually more complex and computationally expensive than behavioral cloning, because it often involves a nested loop: for each candidate reward function, the algorithm may need to perform an inner optimization (solving an MDP) to evaluate how well that reward explains the expert. However, IRL provides several potential benefits:</p> <ul> <li> <p>It yields a reward function, which is a portable definition of the task. This inferred reward can then be reused: for example, to train new agents from scratch, to evaluate different policies, or to modify the task (by tweaking the reward) in a principled way.</p> </li> <li> <p>It can generalize better to new situations. If the environment changes in dynamics or constraints, having \\(\\hat{R}\\) allows us to re-optimize and find a new optimal policy for the new conditions. A policy learned by pure BC might not adapt well beyond the situations it was shown, whereas a reward captures the goal and can be re-optimized.</p> </li> <li> <p>It may allow the agent to exceed the demonstrator\u2019s performance. Since IRL ultimately produces a reward function, an agent can continue to improve with further RL optimization. If the expert was suboptimal or noisy, a sufficiently good RL algorithm might find a policy that achieves an even higher reward (i.e. fine-tunes the behavior) while still aligning with the expert\u2019s intent encoded in \\(\\hat{R}\\).</p> </li> </ul> <p>In summary, IRL shifts the imitation learning problem from policy regression to reward inference. It answers a fundamentally different question: instead of directly cloning actions, infer the hidden goals that the expert is pursuing. With \\(\\hat{R}\\) in hand, we then fall back on standard RL techniques (like those from Chapters 4\u20138) to derive a policy. IRL is especially appealing in scenarios where we suspect the expert\u2019s behavior is optimizing some elegant underlying objective, and we want to uncover that objective for reuse or interpretation. The cost of IRL is the added complexity of the learning process, but the payoff is a deeper understanding of the task and potentially greater robustness and optimality of the learned policy.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-reinforcement-learning","title":"Maximum Entropy Inverse Reinforcement Learning","text":""},{"location":"reinforcement/8_imitation_learning/#principle-of-maximum-entropy","title":"Principle of Maximum Entropy","text":"<p>The entropy of a distribution \\(p(s)\\) is defined as:</p> \\[H(p) = -\\sum_{s} p(s)\\log p(s)\\] <p>The principle of maximum entropy states: The probability distribution that best represents our state of knowledge is the one with the largest entropy, given the constraints of precisely stated prior data. Consider all probability distributions consistent with the observed data. Select the one with maximum entropy\u2014i.e., the least biased distribution that fits what we know while assuming nothing extra.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-applied-to-irl","title":"Maximum Entropy Applied to IRL","text":"<p>We seek a distribution over trajectories \\(P(\\tau)\\) that:</p> <ol> <li>Has maximum entropy, and</li> <li>Matches expert feature expectations.</li> </ol> <p>Formally, we maximize:</p> \\[\\max_{P} -\\sum_{\\tau} P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)\\mu(\\tau) = \\frac{1}{|D|}\\sum_{\\tau_i \\in D} \\mu(\\tau_i)\\] \\[\\sum_{\\tau} P(\\tau) = 1\\] <p>Here:</p> <ul> <li>\\(\\mu(\\tau)\\) represents feature counts for trajectory \\(\\tau\\)</li> <li>\\(D\\) is the expert demonstration set</li> </ul> <p>This says: among all possible distributions consistent with observed expert feature averages, choose the one with maximum uncertainty.</p>"},{"location":"reinforcement/8_imitation_learning/#matching-rewards","title":"Matching Rewards","text":"<p>In linear reward IRL, we assume rewards take the form:</p> \\[r_\\phi(\\tau) = \\phi^\\top \\mu(\\tau)\\] <p>We want a policy \\(\\pi\\) that induces a trajectory distribution \\(P(\\tau)\\) matching the expert\u2019s expected reward under \\(r_\\phi\\):</p> \\[\\max_{P(\\tau)} -\\sum_{\\tau}P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)r_\\phi(\\tau) = \\sum_{\\tau} \\hat{P}(\\tau)r_\\phi(\\tau)\\] \\[\\sum_{\\tau}P(\\tau)=1\\] <p>This aligns the learner\u2019s expected reward with the expert\u2019s reward estimate.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-exponential-family-distributions","title":"Maximum Entropy \u21d2 Exponential Family Distributions","text":"<p>Using constrained optimization (Lagrangians), we obtain:</p> \\[\\log P(\\tau) = \\lambda_1 r_\\phi(\\tau) - 1 - \\lambda_0\\] <p>Thus:</p> \\[P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\] <p>This reveals a key result: The maximum entropy distribution consistent with constraints belongs to the exponential family.</p> <p>That is,</p> \\[p(\\tau|\\phi) = \\frac{1}{Z(\\phi)}\\exp(r_\\phi(\\tau))\\] <p>where</p> \\[Z(\\phi)=\\sum_{\\tau}\\exp(r_\\phi(\\tau))\\] <p>This means we can now learn \\(\\phi\\) by maximizing likelihood of observed expert data, because the trajectory distribution becomes a normalized exponential model.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution","title":"Maximum Entropy Over \\(\\tau\\) Equals Maximum Likelihood of Observed Data Under Max Entropy (Exponential Family) Distribution","text":"<p>Jaynes (1957) showed: Maximizing entropy over trajectories = maximizing likelihood of data under the maximum-entropy distribution.</p> <p>So we:</p> <ol> <li>Assume \\(p(\\tau|\\phi)\\) has exponential form</li> <li>Learn \\(\\phi\\) by maximizing:</li> </ol> \\[\\max_{\\phi} \\prod_{\\tau \\in D} p(\\tau|\\phi)\\] <p>This allows IRL to treat expert demonstrations as data to be probabilistically explained.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-rl-algorithm","title":"Maximum Entropy Inverse RL Algorithm","text":"<p>Assuming known dynamics and linear rewards:</p> <ol> <li>Input: expert demonstrations \\(\\mathcal{D}\\)</li> <li>Initialize reward weights \\(r_\\phi\\)</li> <li>Compute optimal policy \\(\\pi(a|s)\\) given \\(r_\\phi\\) (via dynamic programming / value iteration)</li> <li>Compute state visitation frequencies \\(\\rho(s|\\phi,T)\\)</li> <li> <p>Compute gradient on reward parameters:</p> <p>\\(\\nabla J(\\phi) = \\frac{1}{N}\\sum_{\\tau_i \\in \\mathcal{D}} \\mu(\\tau_i) - \\sum_{s}\\rho(s|\\phi,T)\\mu(s)\\)</p> </li> <li> <p>Update \\(\\phi\\) via gradient step</p> </li> <li>Repeat from Step 3</li> </ol> <p>Maximum Entropy IRL assumes experts act stochastically but optimally. Instead of selecting a single best policy, it finds a distribution over trajectories consistent with expert behavior. The resulting trajectory probabilities follow: \\(\\(P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\)\\)</p> <p>Learning becomes maximum likelihood estimation: find reward parameters \\(\\phi\\) that best explain expert demonstrations.</p>"},{"location":"reinforcement/8_imitation_learning/#apprenticeship-learning","title":"Apprenticeship Learning","text":"<p>Apprenticeship Learning usually refers to the scenario where an agent learns to perform a task by iteratively improving its policy using expert demonstrations as a reference. In many contexts, this term is used when an IRL algorithm is combined with policy learning: the agent behaves as an apprentice to the expert, gradually mastering the task. The classic formulation by Abbeel and Ng (2004) introduced apprenticeship learning via IRL, which guarantees that the learner\u2019s policy will perform nearly as well as the expert\u2019s, given enough demonstration data.</p> <p>One way to think of apprenticeship learning is as follows: rather than directly cloning actions, we try to match the feature expectations of the expert. Suppose we have some features \\(\\phi(s)\\) of states (or state-action pairs) that capture what we care about in the task (for example, in driving, features might include lane deviation, speed, collision count, etc.). The expert will have some expected cumulative feature values . Apprenticeship learning methods aim for the learner to achieve similar feature expectations.</p> <p>A prototypical apprenticeship learning algorithm proceeds like this:</p> <ol> <li> <p>Initialize a candidate policy (it could even start random).</p> </li> <li> <p>Evaluate how this policy behaves in terms of features (run it in simulation to estimate \\(\\mathbb{E}_{\\pi}\\left[\\sum_t \\phi(s_t)\\right]\\)).</p> </li> <li> <p>Compare the policy\u2019s behavior to the expert\u2019s behavior. Identify the biggest discrepancy in feature expectations.</p> </li> <li> <p>Adjust the reward (implicitly defined as a weighted sum of features) to penalize the discrepancy. In other words, find reward weights \\(w\\) such that the expert\u2019s advantage over the apprentice in those feature dimensions is highlighted.</p> </li> <li> <p>Optimize a new policy for this updated reward function (solve the MDP with the new \\(w\\) to get \\(\\pi_{\\text{new}}\\) that maximizes \\(w \\cdot \\phi\\)).</p> </li> <li> <p>Set this \\(\\pi_{\\text{new}}\\) as the apprentice\u2019s policy and repeat the evaluation -&gt; comparison -&gt; reward adjustment cycle.</p> </li> </ol> <p>Each iteration pushes the apprentice to close the gap on the feature that most distinguishes it from the expert. After a few iterations, this process yields a policy that matches the expert on all key feature dimensions within some tolerance. At that point, the apprentice is essentially as good as the expert with respect to any reward expressible as a combination of those features.</p> <p>The term apprenticeship learning highlights that the agent is not just mimicking blindly but is engaged in a process of improvement guided by the expert\u2019s example. Importantly, the focus is on achieving at least the expert\u2019s level of performance. We don\u2019t necessarily care about identifying the exact reward the expert had; we care that our apprentice\u2019s policy is successful. In fact, in the algorithm above, the reward weights \\(w\\) found in each iteration are intermediate tools \u2013 at the end, one can take the final policy and deploy it, without needing to stick to a single explicit reward interpretation.</p> <p>In relation to IRL, apprenticeship learning can be seen as a practical approach to use IRL for control: IRL finds a reward that explains the expert, and then the agent learns a policy for that reward; if it\u2019s not yet good enough, adjust and repeat. Modern developments in imitation learning often follow this spirit. For example, Generative Adversarial Imitation Learning (GAIL) is a more recent technique where the agent learns a policy by trying to fool a discriminator into thinking the agent\u2019s trajectories are from the expert \u2013 conceptually, the discriminator\u2019s judgment provides a sort of reward signal telling the agent how \"expert-like\" its behavior is. This can be viewed as a form of apprenticeship learning, since the agent is iteratively tweaking its policy to become indistinguishable from the expert.</p> <p>In summary, apprenticeship learning is about learning by iteratively comparing to an expert and closing the gap. It often uses IRL under the hood, but its end goal is the policy (the apprentice\u2019s skill), not necessarily the reward. It underscores a key point: in imitation learning, sometimes we care more about performing as well as the expert (a direct goal), and sometimes we care about understanding the expert\u2019s intentions (the indirect goal via IRL). Apprenticeship learning emphasizes the former.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-in-the-rl-landscape","title":"Imitation Learning in the RL Landscape","text":"<p>Imitation learning fills an important niche in the overall reinforcement learning framework. It is especially useful when:</p> <ol> <li> <p>Rewards are difficult to specify: If it's unclear how to craft a reward that captures all aspects of the desired behavior, providing demonstrations can bypass this. IL shines in complex tasks (e.g. high-level driving maneuvers, dexterous robot manipulation) where manually writing a reward function would be cumbersome or prone to error.</p> </li> <li> <p>Rewards are sparse or delayed: When reward feedback is very rare or only given at the end of an episode, a pure RL agent might struggle to get enough signal to learn. An expert trajectory provides dense guidance at every time step (state-action pairs), effectively providing a shaped signal through imitation. This can jump-start learning in tasks that are otherwise too sparse for RL to crack (Chapter 4 discussed how sparse rewards make value estimation difficult \u2013 IL sidesteps that by using expert knowledge).</p> </li> <li> <p>Exploration is risky or expensive: In real-world environments like robotics, autonomous driving, or healthcare, exploring with random or untrained policies can be dangerous or costly. IL allows learning a policy without the agent ever taking unguided actions in the real environment; it learns from safe, successful behaviors demonstrated by the expert. This makes it an attractive approach when safety is a hard constraint.</p> </li> </ol> <p>It\u2019s important to note that IL is not necessarily a replacement for reward-based RL, but rather a complement to it. A common practical approach is to bootstrap an agent with imitation learning and then fine-tune it with reinforcement learning. For example, one might first use behavioral cloning to teach a robot arm the basics of a task from human demonstrations, getting it into a reasonable regime of behavior; then, if a reward function is available (even a sparse one for success), use RL to further improve the policy, possibly surpassing the human expert's performance or adapting to slight changes in the task. The initial IL phase provides a good policy prior (saving time and avoiding dangerous exploration), and the subsequent RL phase lets the agent optimize and explore around that policy to refine skills.</p> <p>On the flip side, imitation learning does require expert data. If obtaining demonstrations is hard (or if no expert exists for a brand-new task), IL might not be applicable. Moreover, if the expert demonstrations are of varying quality or contain noise, the agent will faithfully learn those imperfections unless additional measures (like filtering data or combining with RL optimization) are taken. In contrast, a pure RL approach, given a well-defined reward and enough exploration, can in principle discover superior strategies that no demonstrator provided. Thus, in practice, there is a trade-off: IL can dramatically speed up learning and improve safety given an expert, whereas RL remains the go-to when we only have a reward signal and the freedom to explore.</p> <p>Imitation learning has become a critical part of the toolbox for solving real-world sequential decision problems. It enables success in domains that might be intractable for pure reinforcement learning by providing an external source of guidance. By learning directly from expert behavior \u2013 through methods like behavioral cloning (learning the policy directly) or inverse reinforcement learning (learning the underlying reward and then the policy) \u2013 an agent can shortcut the trial-and-error process. Of course, IL introduces its own challenges (distribution shift, reliance on demonstration coverage, potential suboptimality of the expert), but these can often be managed with algorithmic innovations (DAgger, combining IL with RL, etc.). In summary, imitation learning serves as a powerful paradigm for training agents in cases where designing rewards or allowing extensive exploration is impractical, and it often works hand-in-hand with traditional RL to achieve the best results in complex environments.</p>"},{"location":"reinforcement/8_imitation_learning/#mental-map","title":"Mental map","text":"<pre><code>                    Imitation Learning (IL)\n      Goal: Learn behavior from expert demonstrations\n                     instead of explicit rewards\n                                \u2502\n                                \u25bc\n             Why Imitation Learning? (Motivation)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Hard to design rewards \u2192 reward hacking, tuning           \u2502\n \u2502 Sparse rewards \u2192 inefficient trial &amp; error                \u2502\n \u2502 Unsafe exploration (robots, driving, healthcare)          \u2502\n \u2502 Expert data available \u2192 demonstrations as guidance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   IL vs Reward-Based RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Reward-Based RL             \u2502 Imitation Learning           \u2502\n \u2502 + Explores actively         \u2502 + Learns from expert         \u2502\n \u2502 + Needs reward design       \u2502 + No explicit reward         \u2502\n \u2502 \u2013 Unsafe / inefficient      \u2502 \u2013 Depends on demo quality   \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                         IL Problem Setup\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 MDP without reward function                                \u2502\n \u2502 Access to expert trajectories \u03c4E (s,a pairs)               \u2502\n \u2502 Goal \u2192 Learn policy \u03c0 that mimics \u03c0E                       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Core Method 1: Behavioral Cloning (BC)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat imitation as supervised learning                    \u2502\n \u2502 Train \u03c0\u03b8(s) \u2192 aE using dataset D                          \u2502\n \u2502 Discrete: cross-entropy loss                              \u2502\n \u2502 Continuous: mean squared error                            \u2502\n \u2502 Advantages: simple, offline, safe                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Key BC Problem: Covariate / Distribution Shift\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Trained only on expert states                             \u2502\n \u2502 When deployed, policy errors lead to unseen states        \u2502\n \u2502 \u2192 Poor decisions \u2192 more drift \u2192 compounding failure       \u2502\n \u2502 BC cannot recover or improve beyond expert                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                    Fixing BC: DAgger (Idea)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Let policy act, collect mistakes                          \u2502\n \u2502 Ask expert for correct action                             \u2502\n \u2502 Add to dataset and retrain                                \u2502\n \u2502 \u2192 brings training data closer to deployment distribution  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n       Core Method 2: Inverse Reinforcement Learning (IRL)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn the \u201cwhy\u201d behind actions \u2192 infer hidden reward R*   \u2502\n \u2502 Expert assumed optimal                                     \u2502\n \u2502 Solve inverse problem: \u03c0E \u2248 optimal for R*                 \u2502\n \u2502 After reward recovered \u2192 run normal RL to learn policy     \u2502\n \u2502 Benefits: generalization, interpretability, improve expert \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Method 3: Apprenticeship Learning\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Iteratively improve policy via comparing to expert        \u2502\n \u2502 Match feature expectations \u03c6(s)                           \u2502\n \u2502 Reweights reward \u2192 optimize \u2192 evaluate \u2192 repeat           \u2502\n \u2502 Goal: perform at least as well as expert                  \u2502\n \u2502 Often implemented via IRL (e.g., GAIL conceptually)       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Role of IL within broader RL landscape\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 When IL is useful:                                        \u2502\n \u2502 - Reward hard to design                                   \u2502\n \u2502 - Unsafe or costly to explore                             \u2502\n \u2502 - Sparse reward tasks                                     \u2502\n \u2502 IL + RL hybrid: BC warm-start \u2192 RL fine-tune beyond expert\u2502\n \u2502 Limitations: need expert, demos may be suboptimal         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 IL bypasses reward engineering &amp; risky exploration         \u2502\n \u2502 BC learns \u201cwhat,\u201d IRL learns \u201cwhy,\u201d apprenticeship learns  \u2502\n \u2502 \u201chow to get as good as expert.\u201d                           \u2502\n \u2502 IL often combined with RL for best performance.           \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/9_rlhf/","title":"9. RLHF","text":""},{"location":"reinforcement/9_rlhf/#chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment","title":"Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment","text":"<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for \u201cgood\u201d behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment \u2013 ensuring an AI\u2019s behavior aligns with human preferences and values \u2013 by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute \u201cscore\u201d to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p> <p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>"},{"location":"reinforcement/9_rlhf/#bradleyterry-preference-modeling-in-rlhf","title":"Bradley\u2013Terry Preference Modeling in RLHF","text":"<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley\u2013Terry model, a probabilistic model for noisy comparisons. </p> <p>Consider a \\(K\\)-armed bandit with actions \\(b_1, b_2, \\dots, b_K\\), and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action \\(b_i\\) over \\(b_j\\) is modeled as:</p> \\[ P(b_i \\succ b_j) = \\frac{\\exp(r(b_i))}{\\exp(r(b_i)) + \\exp(r(b_j))} = p_{ij} \\] <p>where \\(r(b)\\) is an unobserved scalar reward associated with action \\(b\\). Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p> <p>Assume we collect a dataset \\(\\mathcal{D}\\) of \\(N\\) comparisons of the form \\((b_i, b_j, \\mu)\\), where:</p> <ul> <li>\\(\\mu(1) = 1\\) if the human marked \\(b_i \\succ b_j\\)</li> <li>\\(\\mu(1) = 0.5\\) if the human marked \\(b_i = b_j\\)</li> <li>\\(\\mu(1) = 0\\) if the human marked \\(b_j \\succ b_i\\)</li> </ul> <p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p> \\[ \\mathcal{L} = - \\sum_{(b_i,b_j,\\mu)\\in\\mathcal{D}} \\left[ \\mu(1)\\log P(b_i \\succ b_j) + \\mu(2)\\log P(b_j \\succ b_i) \\right] \\] <p>Optimizing this loss adjusts the reward function \\(r(\\cdot)\\) so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p> <p>Once the reward model is trained using the Bradley\u2013Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley\u2013Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>"},{"location":"reinforcement/9_rlhf/#the-rlhf-training-pipeline","title":"The RLHF Training Pipeline","text":"<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT) \u2013 Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p> </li> <li> <p>Reward Model Training from Human Preferences \u2013 Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function \\(r_\\phi(x,y)\\) (parameterized by \\(\\phi\\)) that predicts which response is more preferable for a given input x using Bradley\u2013Terry model.</p> </li> <li> <p>Reinforcement Learning Fine-Tuning \u2013 In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy \\(\\pi_\\theta(y|x)\\) (with parameters \\(\\theta\\)) is updated to maximize the expected reward \\(r_\\phi(x,y)\\) of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model\u2019s score, the policy might exploit flaws in \\(r_\\phi\\) (a form of \u201creward hacking\u201d) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback\u2013Leibler (KL) penalty that keeps the new policy \\(\\pi_\\theta\\) close to the reference policy \\(\\pi_{\\text{ref}}\\) (often the SFT model). In summary, the RL objective can be written as:</p> \\[\\max_{\\pi_\\theta} ( \\underbrace{ \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi_\\theta(y \\mid x)} }_{\\text{Sample from policy}} \\left[ \\underbrace{ r_\\phi(x,y) }_{\\text{Want high reward}} \\right] - \\underbrace{ \\beta \\, \\mathbb{D}_{\\mathrm{KL}} \\left[ \\pi_\\theta(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\right] }_{\\text{Keep KL to original model small}}) \\] <p>where \\(\\beta&gt;0\\) controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if \\(\\pi_\\theta\\) deviates too much from the original model\u2019s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p> </li> </ol> <p>Through this pipeline \u2013 SFT, reward modeling, and RL fine-tuning \u2013 we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like \\(\\beta\\) and PPO clip thresholds). The process can be unstable; for instance, if \\(\\beta\\) is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if \\(\\beta\\) is too high, the policy might hardly improve at all. Researchers have described RLHF as a \u201ccomplex and often unstable procedure\u201d that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>"},{"location":"reinforcement/9_rlhf/#direct-preference-optimization-rlhf-without-rl","title":"Direct Preference Optimization: RLHF without RL?","text":"<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p> <p>Recall that in the RLHF setting, our goal is to find a policy \\(\\pi^*(y|x)\\) that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of \\(J(\\pi)\\) occurs when \\(\\pi\\) is proportional to the reference policy times an exponential of the reward:</p> \\[\\pi^*(y \\mid x) \\propto \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{1}{\\beta}\\, r_\\phi(x, y)\\right)\\] <p>This equation gives a closed-form solution for the optimal policy in terms of the reward function \\(r_\\phi\\). It makes sense: actions \\(y\\) that have higher human-derived reward should be taken with higher probability, but we temper this by \\(\\beta\\) and weight by the reference probabilities \\(\\pi_{\\text{ref}}(y|x)\\) so that we don\u2019t stray too far. If we were to normalize the right-hand side, we\u2019d write:</p> \\[\\pi^*(y \\mid x) = \\frac{ \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y)}{\\beta}\\right) }{ \\sum_{y'} \\pi_{\\text{ref}}(y' \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y')}{\\beta}\\right) }\\] <p>Here the denominator is a partition functionsumming over all possible responses \\(y'\\) for input \\(x\\). This normalization involves a sum over the entire response space, which is astronomically large for language models \u2013 hence we cannot directly compute \\(\\pi^*(y|x)\\) in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p> <p>DPO\u2019s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses \\(y_+\\) (preferred) and \\(y_-\\) (dispreferred) for the same context \\(x\\), the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p> \\[\\frac{\\pi^*(y^+ \\mid x)}{\\pi^\\ast(y^- \\mid x)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^+)}{\\beta}\\right)} {\\pi_{\\text{ref}}(y^- \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^-)}{\\beta}\\right)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)} {\\pi_{\\text{ref}}(y^- \\mid x)} \\exp\\!\\left( \\frac{1}{\\beta} \\big[ r_\\phi(x, y^+) - r_\\phi(x, y^-) \\big] \\right)\\] <p>Taking the log of both sides, we get a neat relationship:</p> \\[\\frac{1}{\\beta} \\big( r_\\phi(x, y^{+}) - r_\\phi(x, y^{-}) \\big) = \\big[ \\log \\pi^\\ast(y^{+} \\mid x) - \\log \\pi^\\ast(y^{-} \\mid x) \\big] - \\big[ \\log \\pi_{\\text{ref}}(y^{+} \\mid x) - \\log \\pi_{\\text{ref}}(y^{-} \\mid x) \\big]\\] <p>The term in brackets on the right is the difference in log-probabilities that the optimal policy \\(\\pi^*\\) assigns to the two responses (which in turn would equal the difference in our learned policy\u2019s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if \\(y_+\\) is better than \\(y_-\\) by some amount of reward, then the optimal policy should tilt its probabilities in favor of \\(y_+\\) by a corresponding factor.</p> <p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts \\(\\pi_\\theta\\) to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context \\(x\\) and two candidate outputs \\(y_+\\) (human-preferred) and \\(y_-\\) (human-dispreferred), we want the model to assign a higher probability to \\(y_+\\) than to \\(y_-\\), with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p> <p>In practice, the DPO loss for a pair \\((x, y_+, y_-)\\) is something like:</p> \\[\\ell_{\\text{DPO}}(\\theta) = - \\log \\sigma \\!\\left( \\beta\\, \\big[ \\log \\pi_\\theta(y^{+} \\mid x) - \\log \\pi_\\theta(y^{-} \\mid x) \\big] \\right)\\] <p>where \\(\\sigma\\) is the sigmoid function. This loss is low (i.e. good) when \\(\\log \\pi_\\theta(y_+|x) \\gg \\log \\pi_\\theta(y_-|x)\\), meaning the model assigns much higher probability to the preferred outcome \u2013 which is what we want. If the model hasn\u2019t yet learned the preference, the loss will be higher, and gradient descent on this loss will push \\(\\pi_\\theta\\) to increase the probability of \\(y_+\\) and decrease that of \\(y_-\\). Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy\u2019s logits: effectively, \\(\\log \\pi_\\theta(y|x)\\) plays the role of a reward score for how good \\(y\\) is, up to the scaling factor \\(1/\\beta\\). In fact, the DPO derivation can be seen as combining the preference loss on \\(r_\\phi\\) with the \\(\\pi^*\\) solution formula to produce a preference loss on \\(\\pi_\\theta\\). The original DPO paper calls this approach \u201cyour language model is secretly a reward model\u201d \u2013 by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p> <p>## Mental map</p> <p><code>text          Reinforcement Learning from Human Feedback (RLHF)    Goal: Align model behavior with human preferences and values           when explicit reward design is impractical                                 \u2502                                 \u25bc            Why Dense Rewards Are Hard for Language Models  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Open-ended tasks (dialogue, summarization, reasoning)     \u2502  \u2502 No clear numeric notion of \u201cgood\u201d behavior                \u2502  \u2502 Hand-crafted dense rewards \u2192 miss nuance, reward hacking  \u2502  \u2502 Metrics (BLEU, ROUGE, length) poorly reflect human values \u2502  \u2502 Human values are subjective, contextual, and fuzzy        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc             From Imitation Learning to Human Preferences  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Behavioral Cloning (IL): imitate demonstrations           \u2502  \u2502 + Simple, safe, no reward needed                          \u2502  \u2502 \u2013 Cannot exceed expert, sensitive to distribution shift   \u2502  \u2502 DAgger: fixes BC but requires step-by-step human labeling \u2502  \u2502 Pairwise preferences = middle ground                      \u2502  \u2502 \u2192 no dense rewards, no per-step supervision               \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Pairwise Preference Feedback (Key Idea)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Humans compare two outputs and choose the better one      \u2502  \u2502 Easier than assigning numeric rewards                     \u2502  \u2502 More informative than raw demonstrations                  \u2502  \u2502 Scales to complex, open-ended behaviors                   \u2502  \u2502 Forms basis of reward learning in RLHF                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc         Bradley\u2013Terry Model: Preferences \u2192 Reward Signal  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Model noisy human comparisons probabilistically           \u2502  \u2502 P(b_i \u227b b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    \u2502  \u2502 r(b): latent scalar reward                                \u2502  \u2502 Fit r(\u00b7) by maximizing likelihood / cross-entropy         \u2502  \u2502 Preferred outputs get higher reward scores                \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              RLHF Training Pipeline (3 Stages)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 1. Supervised Fine-Tuning (SFT)                           \u2502  \u2502    \u2013 Behavioral cloning on human-written demos            \u2502  \u2502    \u2013 Produces reference policy \u03c0_ref                      \u2502  \u2502                                                           \u2502  \u2502 2. Reward Model Training                                  \u2502  \u2502    \u2013 Human pairwise preferences                           \u2502  \u2502    \u2013 Train r_\u03c6(x,y) via Bradley\u2013Terry loss                \u2502  \u2502                                                           \u2502  \u2502 3. RL Fine-Tuning (PPO)                                   \u2502  \u2502    \u2013 Maximize reward r_\u03c6(x,y)                             \u2502  \u2502    \u2013 KL penalty keeps \u03c0_\u03b8 close to \u03c0_ref                  \u2502  \u2502    \u2013 Prevents reward hacking &amp; language drift             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               RLHF Objective (KL-Regularized RL)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maximize:                                                 \u2502  \u2502   E[r_\u03c6(x,y)] \u2212 \u03b2 \u00b7 KL(\u03c0_\u03b8 || \u03c0_ref)                      \u2502  \u2502 \u03b2 controls tradeoff:                                      \u2502  \u2502   Low \u03b2 \u2192 reward hacking / mode collapse                  \u2502  \u2502   High \u03b2 \u2192 little improvement over SFT                    \u2502  \u2502 PPO provides stable policy updates                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Limitations of Standard RLHF (PPO-based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Requires training multiple models                         \u2502  \u2502 Many hyperparameters (\u03b2, PPO clip, value loss, etc.)      \u2502  \u2502 Sampling-based RL can be unstable                         \u2502  \u2502 Expensive and complex pipeline                            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc       Direct Preference Optimization (DPO): RLHF without RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Solve RLHF objective in closed form                       \u2502  \u2502 Optimal policy:                                           \u2502  \u2502   \u03c0*(y|x) \u221d \u03c0_ref(y|x) \u00b7 exp(r_\u03c6(x,y)/\u03b2)                  |  \u2502 Use probability ratios \u2192 normalization cancels            \u2502  \u2502 Train \u03c0_\u03b8 directly on preference pairs                    \u2502  \u2502 Loss: sigmoid on log-prob difference                      \u2502  \u2502 \u201cYour LM is secretly a reward model\u201d                      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               DPO vs PPO-based RLHF  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 RLHF (PPO)                  \u2502 DPO                         \u2502  \u2502 + Explicit RL optimization  \u2502 + Pure supervised learning  \u2502  \u2502 \u2013 Complex &amp; unstable        \u2502 \u2013 Assumes KL-optimal form   \u2502  \u2502 \u2013 Many hyperparameters      \u2502 + Simple, stable, efficient \u2502  \u2502                             \u2502 + No separate reward model  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Final Takeaway (Chapter Summary)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Dense rewards are hard for language tasks                 \u2502  \u2502 Pairwise preferences provide natural human feedback       \u2502  \u2502 RLHF learns rewards from preferences + optimizes policy   \u2502  \u2502 DPO simplifies RLHF by removing explicit RL               \u2502  \u2502 Together, they extend imitation learning toward           \u2502  \u2502 scalable value alignment for modern language models       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</code></p>"},{"location":"reinforcement/readme/","title":"Readme","text":"<p>https://www.youtube.com/watch?v=L6OVEmV3NcE&amp;list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX&amp;index=5</p> <p>https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=1</p> <p>https://www.youtube.com/watch?v=WxRDyObrm_M</p>"},{"location":"statistics/introd/","title":"Introd","text":"<p>Variational Inference Exact Inference Monte Carlo  Markov Chain MCMC Sampling vs Optimization</p>"},{"location":"tools/intro/","title":"Intro","text":"<p>Snowflake</p> <p>Agentic frameworks (using CruxAI, Google ADK, LangGraph).</p> <p>Multi-agent systems using frameworks such as LangChain, LangGraph, AutoGen, or CrewAI, with practical understanding of LLM orchestration, retrieval augmentation (RAG), tool calling, and dynamic reasoning.</p> <p>Docker, Kubernetes, and microservices integration.</p> <p>Object-oriented programming skills and experience working with Python, PyTorch and NumPy are desirable Advanced optimisation methods, modern ML techniques, HPC, profiling, model inference; you don\u2019t need to have all of the above Big-data technologies such as Spark, KDB</p>"},{"location":"tools/coding%20practices/1_env_setup/","title":"1 env setup","text":""},{"location":"tools/coding%20practices/1_env_setup/#setting-up-a-python-virtual-environment","title":"Setting Up a Python Virtual Environment","text":""},{"location":"tools/coding%20practices/1_env_setup/#1-create-a-separate-python-virtual-environment","title":"1. Create a Separate Python Virtual Environment","text":"<p>Use the following command to create a virtual environment named <code>.venv</code>:</p> <pre><code>python -m venv .venv\n</code></pre>"},{"location":"tools/coding%20practices/1_env_setup/#2-activate-the-virtual-environment","title":"2. Activate the Virtual Environment","text":"<ul> <li> <p>On Windows (Git Bash / PowerShell): </p><pre><code>source .venv/Scripts/activate\n</code></pre><p></p> </li> <li> <p>On macOS / Linux: </p><pre><code>source .venv/bin/activate\n</code></pre><p></p> </li> </ul> <p>Once activated, your terminal prompt will change toindicate that the virtual environment is active.</p>"},{"location":"tools/gRPC/gRPC/","title":"gRPC","text":""},{"location":"tools/gRPC/gRPC/#api-technologies-overview","title":"API Technologies Overview","text":"API Style Communication Style Core Technology Primary Data Format Best For REST Stateless, Request/Response HTTP/HTTPS JSON, XML, HTML General-purpose web services, public APIs, mobile apps, and simple resource management (CRUD). SOAP Protocol-agnostic, Structured HTTP, SMTP, TCP XML Enterprise-level services, financial/banking, and highly secure or reliable transactions with strict contracts. gRPC RPC, Binary Streaming HTTP/2 Protocol Buffers (Protobuf) High-performance microservices communication, internal APIs, and streaming data where speed is critical. GraphQL Request/Response (Single Endpoint) HTTP/HTTPS JSON Modern web/mobile frontends that need flexible and precise data fetching to avoid over/under-fetching. Webhook Event-Driven (Server to Client) HTTP POST JSON, XML Real-time notifications, system integrations, and subscribing to external events (e.g., payment status change). WebSocket Stateful, Full-Duplex TCP/WebSocket Protocol Binary, Text, JSON Real-time interactive applications like chat, live dashboards, gaming, and collaborative editing. WebRTC Peer-to-Peer Various Protocols (e.g., STUN, TURN) Media Streams Direct, low-latency browser-to-browser communication for video calls, voice chat, and screen sharing."},{"location":"tools/gRPC/gRPC/#detailed-breakdown-use-cases","title":"Detailed Breakdown &amp; Use Cases","text":""},{"location":"tools/gRPC/gRPC/#1-rest-representational-state-transfer","title":"1. REST (Representational State Transfer)","text":"<ul> <li>Basic Details: Architectural style, stateless. Uses standard HTTP methods (GET, POST, PUT, DELETE) to operate on resources identified by URLs.</li> <li>When to Use:<ul> <li>\u2705 Building a public-facing API where simplicity, broad browser support, and a stateless, cacheable design are priorities.</li> <li>\u2705 For simple CRUD (Create, Read, Update, Delete) operations.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#2-soap-simple-object-access-protocol","title":"2. SOAP (Simple Object Access Protocol)","text":"<ul> <li>Basic Details: Formal, XML-based messaging protocol. Protocol-independent (HTTP, SMTP, etc.) with built-in standards for security and reliability (WS-Security).</li> <li>When to Use:<ul> <li>\u2705 In enterprise environments, such as banking or healthcare, where strict contracts (WSDL), security, and transaction reliability are non-negotiable.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#3-grpc-google-remote-procedure-call","title":"3. gRPC (Google Remote Procedure Call)","text":"<ul> <li>Basic Details: High-performance RPC framework using HTTP/2 and Protocol Buffers (Protobuf) for efficient, binary data serialization. Supports multiple types of streaming.</li> <li>When to Use:<ul> <li>\u2705 For internal microservices communication where speed, efficiency, and strongly-typed contracts across multiple languages are essential.</li> <li>\u2705 When high-throughput streaming of data is a core requirement.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#4-graphql-graph-query-language","title":"4. GraphQL (Graph Query Language)","text":"<ul> <li>Basic Details: A query language where clients request exactly the data they need from a single endpoint, preventing over/under-fetching.</li> <li>When to Use:<ul> <li>\u2705 For applications with complex, interconnected data or multiple clients (web, mobile) with varying data needs.</li> <li>\u2705 To improve performance on mobile clients by minimizing payload size.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#5-webhook","title":"5. Webhook","text":"<ul> <li>Basic Details: A \"reverse API\" that acts as an event-driven push notification. The server makes an HTTP POST request to a client-defined URL (callback) when an event occurs.</li> <li>When to Use:<ul> <li>\u2705 When you need real-time alerts for events (e.g., \"a payment was processed\") without continuous polling.</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#6-websocket","title":"6. WebSocket","text":"<ul> <li>Basic Details: A communication protocol that provides a persistent, two-way (full-duplex) connection over a single TCP connection, enabling both client and server to send data at any time.</li> <li>When to Use:<ul> <li>\u2705 For real-time interactive applications where sustained, low-latency, two-way communication is necessary (e.g., chat applications, live data feeds).</li> </ul> </li> </ul>"},{"location":"tools/gRPC/gRPC/#7-webrtc-web-real-time-communication","title":"7. WebRTC (Web Real-Time Communication)","text":"<ul> <li>Basic Details: Enables real-time, peer-to-peer (P2P) communication directly between browsers for streaming audio, video, and arbitrary data.</li> <li>When to Use:<ul> <li>\u2705 For building live video/audio conferencing, voice-over-IP (VoIP), or P2P data sharing.</li> </ul> </li> </ul>"},{"location":"tools/kafka/kafka/","title":"Apache Kafka","text":""},{"location":"tools/kafka/kafka/#apache-kafka","title":"Apache Kafka","text":""},{"location":"tools/kafka/kafka/#1-what-is-kafka","title":"1. What Is Kafka?","text":"<p>Apache Kafka is a distributed event streaming platform designed for: - High-throughput data ingestion - Real-time analytics - Decoupling microservices - Reliable event storage and replay  </p> <p>Kafka acts like a conveyor belt for data: producers place events on the belt; consumers pick them up independently.</p> <p>fundamental ideas behind Kafka: messages sent and received through Kafka require a user specified distribution strategy</p>"},{"location":"tools/kafka/kafka/#2-why-kafka-problems-it-solves","title":"2. Why Kafka? (Problems It Solves)","text":""},{"location":"tools/kafka/kafka/#a-the-problem-with-tightly-coupled-microservices","title":"A. The Problem With Tightly Coupled Microservices","text":"<p>Traditional microservices call each other directly (REST, RPC).</p> <p>Issues: 1. Tight Coupling    If Service A depends on Service B, and B slows or fails \u2192 A also fails.</p> <ol> <li> <p>Single Point of Failure    One service outage disrupts the entire chain.</p> </li> <li> <p>Scalability Limitations    Upstream must handle downstream load directly.</p> </li> <li> <p>Lost Analytics Data    If analytics is down, events are lost.</p> </li> </ol> <p>Kafka solves this by decoupling communication and introducing an event broker.</p>"},{"location":"tools/kafka/kafka/#3-kafka-as-an-event-broker","title":"3. Kafka as an Event Broker","text":"<p>Microservices no longer call each other. Instead:</p> <ul> <li>Producers publish events to Kafka.</li> <li>Topics organize those events.</li> <li>Consumers independently subscribe and process them.</li> </ul> <p>This supports: - Asynchronous communication - Fault isolation - Independent scaling - Reliable buffering - Replay of past events  </p>"},{"location":"tools/kafka/kafka/#4-core-concepts","title":"4. Core Concepts","text":""},{"location":"tools/kafka/kafka/#a-events","title":"A. Events","text":"<p>An event records that something happened.</p> <p>Structure: - Key - Value - Timestamp - Optional metadata (headers)</p> <p>Example: </p><pre><code>Key: orderId=123\nValue: {\"status\": \"CREATED\", \"amount\": 200}\n</code></pre><p></p>"},{"location":"tools/kafka/kafka/#b-topics","title":"B. Topics","text":"<p>A topic is a category for events of the same type (e.g., <code>orders</code>, <code>payments</code>).</p>"},{"location":"tools/kafka/kafka/#where-are-topics-stored","title":"Where Are Topics Stored?","text":"<p>On Kafka brokers, distributed into partitions.</p>"},{"location":"tools/kafka/kafka/#c-partitions","title":"C. Partitions","text":"<p>Partition = append-only, ordered event log.</p> <p>Benefits: - Scalability (multiple partitions) - High throughput - Parallel processing - Ordering (guaranteed only within a partition)</p> <p>A topic is a logical grouping of messages. A partition is a physical grouping of messages. A topic can have multiple partitions, and each partition can be on a different broker. Topics are just a way to organize your data, while partitions are a way to scale your data.</p>"},{"location":"tools/kafka/kafka/#d-kafka-broker","title":"D. Kafka Broker","text":"<p>A broker is a Kafka server.</p> <p>Responsibilities: - Store topic partitions - Handle reads/writes from producers/consumers - Replicate data for fault tolerance  </p> <p>Replication: - Each partition has:   - Leader (handles requests)   - Followers (replicas for failover)</p>"},{"location":"tools/kafka/kafka/#e-consumers-consumer-groups","title":"E. Consumers &amp; Consumer Groups","text":""},{"location":"tools/kafka/kafka/#consumer","title":"Consumer","text":"<p>Reads records from topics.</p>"},{"location":"tools/kafka/kafka/#consumer-group","title":"Consumer Group","text":"<p>A group of consumers sharing the work of reading a topic.</p> <p>Rules: - One consumer per partition (within a group) - But multiple groups can read the same topic independently</p> <p>Examples: - One group processes orders - Another group replays events for analytics - Another group trains ML models  </p>"},{"location":"tools/kafka/kafka/#5-kafka-vs-traditional-message-queues-rabbitmq-activemq","title":"5. Kafka vs Traditional Message Queues (RabbitMQ, ActiveMQ)","text":"Feature Kafka Traditional MQ Message deletion Stored for retention period Deleted after consumption Replay messages Yes No Storage Disk-backed log Often in-memory Primary use Event streaming &amp; analytics Task queues <p>Kafka is not a database, but it reliably stores events and allows consumers to replay them.</p>"},{"location":"tools/kafka/kafka/#6-kafka-for-real-time-processing","title":"6. Kafka for Real-Time Processing","text":""},{"location":"tools/kafka/kafka/#a-kafka-streams-api","title":"A. Kafka Streams API","text":"<p>A powerful library for real-time event processing.</p> <p>Features: - Continuous streaming (no polls) - Functional transformations:   - map   - filter   - join   - aggregate - Windowing - Stateful processing  </p> <p>Use cases: - Fraud detection - Real-time dashboards - Metric aggregation - Data enrichment  </p>"},{"location":"tools/kafka/kafka/#7-kafka-architecture-components-summary-table","title":"7. Kafka Architecture Components (Summary Table)","text":"Component Role Producer Writes events to topics Topic Category of events Partition Ordered log segment for scalability Consumer Reads events Consumer Group Enables parallel processing Broker Stores partitions and handles traffic Cluster Multiple brokers working together"},{"location":"tools/kafka/kafka/#8-kafka-coordination-zookeeper-vs-kafka-raft-kraft","title":"8. Kafka Coordination: Zookeeper vs Kafka Raft (KRaft)","text":""},{"location":"tools/kafka/kafka/#originally-zookeeper","title":"Originally: Zookeeper","text":"<p>Used for: - Metadata management - Leader election - Cluster configuration  </p>"},{"location":"tools/kafka/kafka/#now-kafka-kraft-kafka-raft","title":"Now: Kafka KRaft (Kafka Raft)","text":"<ul> <li>Built-in consensus protocol  </li> <li>No Zookeeper dependency  </li> <li>Simplifies deployment  </li> <li>Better scalability and reliability  </li> </ul> <p>Zookeeper is being phased out.</p>"},{"location":"tools/kafka/kafka/#9-key-benefits-of-kafka","title":"9. Key Benefits of Kafka","text":"<ul> <li>High throughput  </li> <li>Low latency  </li> <li>Durable storage  </li> <li>Scalable horizontally  </li> <li>Fault-tolerant  </li> <li>Can replay historical events  </li> <li>Decouples microservices  </li> <li>Supports real-time analytics  </li> </ul> <p>https://www.youtube.com/watch?v=QkdkLdMBuL0</p> <p>https://www.youtube.com/watch?v=B7CwU_tNYIE</p>"},{"location":"tools/kafka/kafka_rabbitmq/","title":"Kafka rabbitmq","text":"<p>Kafka vs RabbitMQ</p> <p>Kafka - Extremely high trhouhou - Rplay - Data Retention - Fan out</p> <p>Tranditional Queues - COmplex message routing - message indended for one consumer - modeerate data volume</p>"},{"location":"tools/mcp/1_mcp_architecture/","title":"Understanding MCP Client-Server Architecture (Step by Step)","text":""},{"location":"tools/mcp/1_mcp_architecture/#understanding-mcp-client-server-architecture-step-by-step","title":"Understanding MCP Client-Server Architecture (Step by Step)","text":""},{"location":"tools/mcp/1_mcp_architecture/#1-mcp-uses-a-client-server-model","title":"1. MCP Uses a \u201cClient-Server\u201d Model","text":"<ul> <li>Think of client-server like a restaurant:</li> <li>The server is the kitchen, which prepares food.</li> <li>The client is the customer, who asks for food.</li> <li>In MCP, the server provides AI functionality (like responding to code or commands), and the client asks for it.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#2-what-is-an-mcp-host","title":"2. What is an MCP Host?","text":"<ul> <li>The MCP host is like the manager of the restaurant.  </li> <li>It\u2019s the AI application you\u2019re using, e.g., Claude Code or Claude Desktop.  </li> <li>Its job is to connect to MCP servers so it can get AI services.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#3-mcp-host-creates-mcp-clients","title":"3. MCP Host Creates MCP Clients","text":"<ul> <li>For each server the host wants to talk to, it makes a client.  </li> <li>Imagine this as the manager sending one waiter per kitchen to get the food.  </li> <li>Each client talks only to its assigned server.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#4-local-vs-remote-servers","title":"4. Local vs Remote Servers","text":"<ul> <li>Local MCP servers:</li> <li>Run on your own computer.</li> <li>Usually only serve one client (your AI app).  </li> <li>They communicate using STDIO (sending text back and forth like typing into a console).</li> <li>Remote MCP servers:</li> <li>Run somewhere else on the internet.</li> <li>Can serve many clients at the same time.  </li> <li>They use Streamable HTTP, which sends data efficiently over the internet.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#5-summary-with-a-simple-analogy","title":"5. Summary with a Simple Analogy","text":"<p>Imagine you have a chain of kitchens (servers) and a restaurant manager (host):</p> <ol> <li>The manager (host) wants dishes from multiple kitchens.</li> <li>For each kitchen, the manager sends a waiter (client) to place orders.</li> <li>Each waiter only talks to their assigned kitchen.</li> <li>Some kitchens are nearby (local) \u2192 serve one waiter at a time.</li> <li>Some kitchens are far away (remote) \u2192 serve many waiters at once.</li> </ol>"},{"location":"tools/mcp/1_mcp_architecture/#understanding-mcp-layers-data-layer-and-transport-layer-analogy-based","title":"Understanding MCP Layers: Data Layer and Transport Layer (Analogy-Based)","text":""},{"location":"tools/mcp/1_mcp_architecture/#mcp-layers","title":"MCP Layers","text":"<p>MCP Has Two Layers</p> <ol> <li>Data Layer (Inner Layer \u2013 the menu &amp; recipes)  </li> <li>Defines what can be ordered and how it\u2019s prepared.  </li> <li>Includes the structure of orders, cooking steps, and communication rules between the waiter and the kitchen.  </li> <li> <p>Key responsibilities:</p> <ul> <li>Lifecycle Management \u2192 Setting up the kitchen and waiters, making sure they can communicate, and closing the kitchen at the end of the day.</li> <li>Server Features \u2192 Kitchen provides tools (utensils), resources (ingredients), and prompts (recipe instructions) to make dishes.</li> <li>Client Features \u2192 Waiters (clients) can ask the kitchen to sample dishes, get input from customers, and log orders.</li> <li>Utility Features \u2192 Notifications (like \u201corder ready\u201d) and tracking long cooking times.</li> </ul> </li> <li> <p>Transport Layer (Outer Layer \u2013 the delivery system)  </p> </li> <li>Handles how orders are sent and delivered between waiters and kitchens.  </li> <li> <p>Key responsibilities:</p> <ul> <li>Ensures messages (orders) are framed correctly.</li> <li>Manages secure communication.</li> <li>Handles authentication (who is allowed to place orders).</li> </ul> </li> <li> <p>Transport Methods:</p> </li> <li>Stdio Transport \u2192 Waiter walks directly to a local kitchen in the same building. Fast, simple, no network needed.</li> <li>Streamable HTTP Transport \u2192 Waiter sends orders over the internet to a distant kitchen, possibly streaming updates. Supports authentication like ID badges, keys, or OAuth tokens.</li> </ol>"},{"location":"tools/mcp/1_mcp_architecture/#putting-it-together","title":"Putting It Together","text":"<ul> <li>Data Layer = the menu, recipes, and instructions (defines what can be done).  </li> <li>Transport Layer = the delivery system (defines how the orders travel).  </li> <li>This separation allows the same recipes (JSON-RPC messages) to work whether the kitchen is next door (local) or across town (remote).</li> </ul> <p>Analogy Summary: </p> <ul> <li>Kitchen = MCP Server  </li> <li>Waiter = MCP Client  </li> <li>Manager = MCP Host  </li> <li>Menu &amp; recipes = Data Layer  </li> <li>Delivery system = Transport Layer</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#mcp-data-layer-protocol-analogy-based","title":"MCP Data Layer Protocol (Analogy-Based)","text":""},{"location":"tools/mcp/1_mcp_architecture/#mcp-data-layer-overview","title":"MCP Data Layer Overview","text":"<p>Think of the data layer as the menu, recipes, and instructions in a restaurant:</p> <ul> <li>It defines what can be done between waiters (clients) and kitchens (servers).  </li> <li>This is the part developers interact with most, because it specifies the context and actions that can be shared.  </li> <li>MCP uses JSON-RPC 2.0, which is like a standard way for waiters and kitchens to send orders and responses:</li> <li>Requests = ordering a dish  </li> <li>Responses = kitchen replies with the dish  </li> <li>Notifications = announcements like \"special of the day\" (no reply needed)</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#lifecycle-management","title":"Lifecycle Management","text":"<ul> <li>MCP is stateful \u2014 meaning it remembers who\u2019s talking to whom.  </li> <li>Lifecycle management ensures the waiter and kitchen agree on capabilities before starting work:</li> <li>Example: Which tools, ingredients, or prompts each side supports.  </li> <li>This is like a waiter confirming the kitchen can handle gluten-free or vegan orders before taking the order.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#mcp-primitives-the-menu-items","title":"MCP Primitives (The \u201cMenu Items\u201d)","text":"<p>Primitives are the core items that can be shared between client and server \u2014 think of them as the dishes on the menu.</p>"},{"location":"tools/mcp/1_mcp_architecture/#1-server-primitives-kitchen-offerings","title":"1. Server Primitives (Kitchen Offerings)","text":"<ul> <li>Tools \u2192 Executable functions the kitchen can perform (e.g., make a pizza, run a database query, call an API).  </li> <li>Resources \u2192 Data or ingredients provided to the waiter (e.g., database schema, file contents, API responses).  </li> <li>Prompts \u2192 Reusable templates to help structure interactions (e.g., recipes, system prompts, few-shot examples).</li> </ul> <p>How it works: 1. Waiter lists available dishes: <code>tools/list</code>, <code>resources/list</code>, <code>prompts/list</code>. 2. Waiter gets details or executes actions: <code>tools/call</code>, <code>resources/get</code>, etc.  </p> <p>Analogy Example: - Kitchen offers a database context:   - Tool: query the database   - Resource: database schema   - Prompt: few-shot instructions for querying</p>"},{"location":"tools/mcp/1_mcp_architecture/#2-client-primitives-waiter-capabilities","title":"2. Client Primitives (Waiter Capabilities)","text":"<p>These let kitchens ask the waiter to do things:</p> <ul> <li>Sampling \u2192 Kitchen asks waiter to generate AI completions (like letting the waiter suggest a dish from another kitchen).  </li> <li>Elicitation \u2192 Kitchen asks waiter to get more info from the customer (user input or confirmation).  </li> <li>Logging \u2192 Kitchen sends logs to the waiter for monitoring or debugging.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#3-utility-primitives-special-services","title":"3. Utility Primitives (Special Services)","text":"<ul> <li>Tasks (Experimental) \u2192 Wrappers for long-running or deferred actions (like pre-orders, batch cooking, or multi-step dishes).  </li> <li>These allow kitchens to track progress and retrieve results later.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#notifications-real-time-updates","title":"Notifications (Real-Time Updates)","text":"<ul> <li>Servers can send updates to clients without expecting a response:  </li> <li>Example: A new tool becomes available, or an existing one is modified.  </li> <li>Analogy: Kitchen announces \u201cNew seasonal dish available!\u201d to all connected waiters.  </li> <li>Notifications are sent as JSON-RPC 2.0 notification messages.</li> </ul>"},{"location":"tools/mcp/1_mcp_architecture/#summary-analogy-table","title":"Summary Analogy Table","text":"MCP Concept Restaurant Analogy Server (MCP Server) Kitchen Client (MCP Client) Waiter Host (MCP Host) Restaurant Manager Data Layer Menu &amp; Recipes (defines what can be done) Tools Kitchen actions (make dishes, run queries) Resources Ingredients / data for dishes Prompts Recipes / instructions Client Primitives Waiter capabilities (ask for help, log info) Utility Primitives Special services like batch orders or long tasks Notifications Announcements like \u201cspecials of the day\u201d"},{"location":"tools/pytorch/1_pytorch/","title":"PyTorch","text":""},{"location":"tools/pytorch/1_pytorch/#pytorch","title":"PyTorch","text":"<pre><code>import torch\nimport numpy as np\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#1-creating-tensors","title":"1. Creating Tensors","text":""},{"location":"tools/pytorch/1_pytorch/#torchzeros","title":"<code>torch.zeros</code>","text":"<pre><code>x_zeros = torch.zeros(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#torchones","title":"<code>torch.ones</code>","text":"<pre><code>x_ones = torch.ones(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#torchrand","title":"<code>torch.rand</code>","text":"<pre><code>x_rand = torch.rand(2, 3)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#2-tensor-from-a-python-list","title":"2. Tensor from a Python List","text":"<pre><code>data = [[1,2,3],[4,5,6]]\nx_from_list = torch.tensor(data)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#3-tensor-from-numpy-array","title":"3. Tensor from NumPy Array","text":"<pre><code>np_array = np.array([[1,2,3],[4,5,6]])\nx_from_np = torch.from_numpy(np_array)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#4-shape-batch-dimension","title":"4. Shape &amp; Batch Dimension","text":"<pre><code>images = torch.rand(32,3,64,64)\nbatch = images.shape[0]\nsample = images.shape[1:]\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#5-dtype","title":"5. Dtype","text":"<pre><code>torch.zeros(2,2,dtype=torch.float32)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#6-reshaping","title":"6. Reshaping","text":""},{"location":"tools/pytorch/1_pytorch/#unsqueeze","title":"Unsqueeze","text":"<pre><code>x = torch.tensor([1,2,3,4])\nx.unsqueeze(0)\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#squeeze","title":"Squeeze","text":"<pre><code>y = torch.rand(1,3,1,4)\ny.squeeze()\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#7-slicing","title":"7. Slicing","text":"<pre><code>x = torch.arange(12).reshape(3,4)\nx[0]\nx[:,1]\nx[0:2,1:3]\n</code></pre>"},{"location":"tools/pytorch/1_pytorch/#8-item","title":"8. .item()","text":"<pre><code>loss = torch.tensor(3.14)\nloss.item()\n</code></pre>"},{"location":"tools/pytorch/2_broadcasting/","title":"Broadcasting in NumPy","text":""},{"location":"tools/pytorch/2_broadcasting/#broadcasting-in-numpy","title":"Broadcasting in NumPy","text":"<p>Broadcasting is a set of rules that NumPy uses to let arrays with different shapes work together in arithmetic operations. When shapes don't match, NumPy aligns dimensions from the right (the trailing dimensions) and tries to stretch dimensions of size <code>1</code> so the arrays become compatible.</p> <ul> <li>A dimension can broadcast if it matches or is <code>1</code>.</li> <li>If two dimensions differ and neither is <code>1</code>, broadcasting fails.</li> </ul>"},{"location":"tools/pytorch/2_broadcasting/#why-align-from-the-right","title":"Why \"align from the right\"?","text":"<p>NumPy compares array shapes starting from the rightmost dimension, since those describe the element-level structure.</p> <p>Example of right alignment:</p> <pre><code>Array A shape:      (5, 1)\nArray B shape:          (5,)\n                     --------\nAligned shapes:    (5, 1)\n                    (1, 5)\n</code></pre>"},{"location":"tools/pytorch/2_broadcasting/#simple-example","title":"Simple Example","text":"<pre><code>import numpy as np\n\nA = np.array([[10],\n              [20],\n              [30]])   # shape (3,1)\n\nB = np.array([1, 2, 3])  # shape (3,)\n\n# Broadcasting:\n# A becomes (3,3) by repeating its single column\n# B becomes (1,3) by repeating its single row\nprint(A + B)\n</code></pre> <p>Output:</p> <pre><code>[[11 12 13]\n [21 22 23]\n [31 32 33]]\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/","title":"PyTorch Data Utilities: Transforms, Datasets, and DataLoaders","text":""},{"location":"tools/pytorch/3_pytorch_datautilities/#pytorch-data-utilities-transforms-datasets-and-dataloaders","title":"PyTorch Data Utilities: Transforms, Datasets, and DataLoaders","text":"<p>Efficient data handling is a core part of every machine learning workflow. PyTorch provides a clean, modular data pipeline built around three utilities: Transforms, Datasets, and DataLoaders. Together, they make it easy to prepare data, apply preprocessing, and feed batches to your model during training.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#1-transforms","title":"1. Transforms","text":"<p>Transforms perform preprocessing operations on input data. They are typically used to convert raw samples (images, text, audio, etc.) into tensors and normalize them for training.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#common-uses","title":"Common Uses","text":"<ul> <li>Convert images to tensors\\</li> <li>Normalize pixel values\\</li> <li>Data augmentation (flip, crop, rotate)\\</li> <li>Compose multiple preprocessing steps</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#underlying-idea","title":"Underlying Idea","text":"<p>Transforms are callable objects. A transform takes one sample as input and returns a modified sample.</p> <pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips","title":"Practitioner Tips","text":"<ul> <li>Use <code>transforms.Compose()</code> to chain steps.</li> <li>Apply data augmentation only on the training set.</li> <li>Keep normalization consistent with the pretrained models you use.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#2-datasets","title":"2. Datasets","text":"<p>A Dataset is a wrapper around your data. It tells PyTorch how to access one sample at a time. Every custom dataset must implement two methods:</p> <ul> <li><code>__len__</code> \u2192 returns dataset size\\</li> <li><code>__getitem__</code> \u2192 returns one sample at index i</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#example-custom-image-dataset","title":"Example: Custom Image Dataset","text":"<pre><code>from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass MyImages(Dataset):\n    def __init__(self, filepaths, labels, transform=None):\n        self.filepaths = filepaths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.filepaths[idx])\n        label = self.labels[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips_1","title":"Practitioner Tips","text":"<ul> <li>Use <code>torchvision.datasets</code> for standard datasets (CIFAR, MNIST,     ImageNet).</li> <li>Load files inside <code>__getitem__</code>, not beforehand.</li> <li>Keep transforms inside the dataset to ensure consistent     preprocessing.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#3-dataloader","title":"3. DataLoader","text":"<p>A DataLoader wraps a Dataset and helps you iterate through the data efficiently by:</p> <ul> <li>batching samples\\</li> <li>shuffling\\</li> <li>loading data in parallel with worker processes</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#example","title":"Example","text":"<pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4\n)\n\nfor images, labels in train_loader:\n    pass\n</code></pre>"},{"location":"tools/pytorch/3_pytorch_datautilities/#underlying-idea_1","title":"Underlying Idea","text":"<p>The DataLoader retrieves indices from the Dataset, loads samples, applies batching, and returns them in iterable form.</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#practitioner-tips_2","title":"Practitioner Tips","text":"<ul> <li>Set <code>shuffle=True</code> for training; <code>False</code> for evaluation.</li> <li>Increase <code>num_workers</code> to speed up loading (based on CPU cores).</li> <li>Use <code>pin_memory=True</code> when training on GPU.</li> </ul>"},{"location":"tools/pytorch/3_pytorch_datautilities/#putting-it-all-together","title":"Putting It All Together","text":"<pre><code>transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()\n])\n\ndataset = MyImages(filepaths, labels, transform=transform)\n\nloader = DataLoader(dataset, batch_size=64, shuffle=True)\n</code></pre> <p>This structure cleanly separates: - how data is processed (Transforms) - how data is accessed (Dataset) - how data is delivered to the model (DataLoader)</p>"},{"location":"tools/pytorch/3_pytorch_datautilities/#conclusion","title":"Conclusion","text":"<p>PyTorch's data utilities provide a flexible and powerful way to prepare data for deep learning workflows. Understanding these components enables practitioners to build efficient, scalable training pipelines.</p>"},{"location":"tools/pytorch/4_pytorch_device_management/","title":"Understanding Device Management in PyTorch: A Practical Guide","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#understanding-device-management-in-pytorch-a-practical-guide","title":"Understanding Device Management in PyTorch: A Practical Guide","text":"<p>When working with tensors and neural networks in PyTorch, understanding device management is essential. Every tensor and model parameter lives on a device---either the CPU or an accelerator like a GPU. PyTorch does not automatically move your data or model between devices. If your tensors and model are not on the same device, your program may crash with errors such as:</p> <p>RuntimeError: Expected all tensors to be on the same device...</p>"},{"location":"tools/pytorch/4_pytorch_device_management/#in-this-article-youll-learn-how-to-properly-manage-devices-avoid-common-beginner-errors-and-build-reliable-training-loops","title":"In this article, you'll learn how to properly manage devices, avoid common beginner errors, and build reliable training loops.","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#why-devices-matter","title":"Why Devices Matter","text":""},{"location":"tools/pytorch/4_pytorch_device_management/#cpu","title":"CPU","text":"<ul> <li>Default device in PyTorch.</li> <li>Handles general-purpose computations.</li> <li>Runs operations sequentially.</li> <li>Slower for large deep learning workloads.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#gpu-cuda","title":"GPU (CUDA)","text":"<ul> <li>Parallel accelerator.</li> <li>Can train models 10--15\u00d7 faster than CPU.</li> <li>Essential for scaling deep learning.</li> <li>Must explicitly move data and models to this device.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#checking-for-gpu-availability","title":"Checking for GPU Availability","text":"<p>PyTorch provides a simple API to verify whether a GPU is available:</p> <pre><code>torch.cuda.is_available()\n</code></pre> <p>A common pattern for selecting a device is:</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <ul> <li><code>\"cuda\"</code> \u2192 NVIDIA GPU.</li> <li><code>\"cpu\"</code> \u2192 fallback if no GPU\\</li> <li>Other options exist (e.g., <code>\"mps\"</code> for Apple Silicon), but CUDA is still the standard in most PyTorch workflows.</li> </ul>"},{"location":"tools/pytorch/4_pytorch_device_management/#moving-models-and-data-to-a-device","title":"Moving Models and Data to a Device","text":"<p>Once you choose a device, you must manually move:</p> <ol> <li>Your model</li> <li>Your input tensors</li> <li>Your target labels</li> </ol>"},{"location":"tools/pytorch/4_pytorch_device_management/#move-the-model","title":"Move the model:","text":"<pre><code>model = MyModel().to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#move-the-batch-data-inside-the-training-loop","title":"Move the batch data inside the training loop:","text":"<pre><code>inputs = inputs.to(device)\ntargets = targets.to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#check-where-something-lives","title":"Check where something lives","text":"<pre><code>tensor.device\nnext(model.parameters()).device\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#important-to-does-not-modify-in-place","title":"Important: <code>.to()</code> Does NOT Modify in Place","text":"<p>A very common mistake is:</p> <pre><code>inputs.to(device)  # WRONG \u2192 result is discarded\n</code></pre> <p>You must assign it:</p> <pre><code>inputs = inputs.to(device)\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#training-loop-with-proper-device-management","title":"Training Loop With Proper Device Management","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MyModel().to(device)\noptimizer = optim.Adam(model.parameters())\nloss_function = nn.CrossEntropyLoss()\n\nfor inputs, targets in dataloader:\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tools/pytorch/4_pytorch_device_management/#avoiding-gpu-memory-errors","title":"Avoiding GPU Memory Errors","text":"<p>Even when everything is set correctly, you might encounter:</p> <p>RuntimeError: CUDA out of memory</p> <p>This occurs when your model or batch size requires more memory than the GPU has.</p>"},{"location":"tools/pytorch/4_pytorch_device_management/#how-to-fix-it","title":"How to fix it","text":"<ul> <li>Lower your batch size first (the most common fix)</li> <li>Reduce image resolution\\</li> <li>Use <code>torch.cuda.empty_cache()</code> if necessary\\</li> <li>Try gradient accumulation</li> </ul> <p>For many systems: - Batch size 32--64 is a good starting point</p>"},{"location":"tools/pytorch/5_datapipelines/","title":"Building a Custom PyTorch Dataset for Oxford Flowers","text":""},{"location":"tools/pytorch/5_datapipelines/#building-a-custom-pytorch-dataset-for-oxford-flowers","title":"Building a Custom PyTorch Dataset for Oxford Flowers","text":"<p>Real-world datasets rarely come neatly packaged like MNIST or CIFAR. The Oxford Flowers dataset is a perfect example of messy data:</p> <ul> <li>Images are stored in a flat folder: <code>image0001.jpg</code>,     <code>image0002.jpg</code>, ...</li> <li>Labels are stored separately in a MATLAB <code>.mat</code> file</li> <li>Labels start at 1, but PyTorch expects 0-based labels</li> <li>There's no built-in PyTorch dataset for it</li> </ul> <p>To make this dataset usable for training, we need to build a custom PyTorch <code>Dataset</code> class.</p> <p>A PyTorch <code>Dataset</code> only needs to answer three questions:</p> <ol> <li><code>__init__</code> --- How do I set up the dataset?</li> <li><code>__len__</code> --- How many samples are there?</li> <li><code>__getitem__</code> --- Given an index <code>i</code>, what image and label     should I return? --</li> </ol>"},{"location":"tools/pytorch/5_datapipelines/#key-concepts","title":"Key Concepts","text":""},{"location":"tools/pytorch/5_datapipelines/#light-setup-__init__","title":"\u2714 Light setup (<code>__init__</code>)","text":"<ul> <li>Store image folder path</li> <li>Load labels from <code>.mat</code></li> <li>Fix label indexing from 1--102 to 0--101</li> <li>But do not load images here</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#lazy-loading-__getitem__","title":"\u2714 Lazy loading (<code>__getitem__</code>)","text":"<ul> <li>Load only the image you need</li> <li>Convert it with PIL</li> <li>Return image + label</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#simple-length-__len__","title":"\u2714 Simple length (<code>__len__</code>)","text":"<ul> <li>Just return number of samples (labels)</li> </ul>"},{"location":"tools/pytorch/5_datapipelines/#example-implementation-oxfordflowersdataset","title":"Example Implementation: <code>OxfordFlowersDataset</code>","text":"<pre><code>import os\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport scipy.io as sio   # pip install scipy\n\n\nclass OxfordFlowersDataset(Dataset):\n    def __init__(self, root_dir, labels_mat_file, transform=None):\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n\n        mat = sio.loadmat(labels_mat_file)\n        raw_labels = mat[\"labels\"].squeeze()\n        self.labels = raw_labels.astype(\"int64\") - 1\n        self.num_samples = len(self.labels)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.item()\n\n        filename = f\"image{idx + 1:04d}.jpg\"\n        img_path = self.root_dir / filename\n\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n</code></pre>"},{"location":"tools/pytorch/5_datapipelines/#using-the-dataset-with-a-dataloader","title":"Using the Dataset with a DataLoader","text":"<pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndataset = OxfordFlowersDataset(\n    root_dir=\"path/to/jpg\",\n    labels_mat_file=\"path/to/labels.mat\",\n    transform=transform,\n)\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n)\n\nimages, labels = next(iter(dataloader))\n\nprint(\"Batch image shape:\", images.shape)\nprint(\"Batch labels:\", labels[:10])\n</code></pre>"},{"location":"tools/pytorch/6_transform/","title":"PyTorch Image Transformations: Fixing Real-World Data Quality Issues","text":""},{"location":"tools/pytorch/6_transform/#pytorch-image-transformations-fixing-real-world-data-quality-issues","title":"PyTorch Image Transformations: Fixing Real-World Data Quality Issues","text":"<p>In the previous article, you built a custom <code>Dataset</code> class for the Oxford Flowers dataset. Now it's time to deal with the second major challenge: raw images are messy.</p> <p>Real-world image datasets rarely come in the exact format a model expects. Common problems:</p> <ul> <li>Images have different sizes</li> <li>Images come as PIL objects, but PyTorch models expect     tensors</li> <li>Pixel values range from 0--255, which can destabilize training</li> <li>Color distributions vary dramatically, requiring normalization</li> </ul> <p>This article walks through PyTorch's powerful transformation pipeline, explaining how each transform works, why order matters, and how to debug issues.</p>"},{"location":"tools/pytorch/6_transform/#why-your-dataset-breaks-without-transforms","title":"Why Your Dataset Breaks Without Transforms","text":"<p>If you try to load Oxford Flowers with a DataLoader as-is, you'll likely see this error:</p> <pre><code>RuntimeError: stack expects each tensor to be equal size...\n</code></pre> <p>Why?</p>"},{"location":"tools/pytorch/6_transform/#images-have-different-sizes","title":"\u2714 Images have different sizes","text":"<p>PyTorch wants batches shaped like:</p> <pre><code>(batch_size, channels, height, width)\n</code></pre> <p>If height/width differ, tensors can't be stacked \u2192 crash.</p>"},{"location":"tools/pytorch/6_transform/#images-are-pil-objects","title":"\u2714 Images are PIL objects","text":"<p>Your model expects tensors, not PIL images.</p> <p>So you need to fix both format and shape.</p>"},{"location":"tools/pytorch/6_transform/#pytorch-transformations-to-the-rescue","title":"PyTorch Transformations to the Rescue","text":"<p>PyTorch's <code>torchvision.transforms</code> lets you chain image processing stepscusing <code>Compose</code>.</p> <p>Two major issues to solve:</p> <ol> <li>Different image sizes</li> <li>Wrong data format (PIL instead of tensor)</li> </ol>"},{"location":"tools/pytorch/6_transform/#1-fixing-size-mismatches","title":"1. Fixing Size Mismatches","text":""},{"location":"tools/pytorch/6_transform/#bad-approach","title":"Bad approach","text":"<pre><code>transforms.Resize((224, 224))\n</code></pre> <p>This forces the image to exactly 224\u00d7224 \u2192 stretches rectangular photos.</p>"},{"location":"tools/pytorch/6_transform/#better-approach","title":"Better approach","text":"<pre><code>transforms.Resize(256)       # resizes shortest edge, preserves aspect ratio\ntransforms.CenterCrop(224)   # extract a clean square\n</code></pre> <p>This avoids distortion.</p>"},{"location":"tools/pytorch/6_transform/#2-converting-images-to-tensors","title":"2. Converting Images to Tensors","text":"<p><code>ToTensor()</code> does more than people realize.</p>"},{"location":"tools/pytorch/6_transform/#what-totensor-does","title":"What <code>ToTensor()</code> does:","text":"<ul> <li>Converts PIL \u2192 Tensor\\</li> <li>Rearranges dimensions to <code>(C, H, W)</code></li> <li>Scales pixel values from 0--255 \u2192 0--1</li> </ul> <p>Example:</p> <pre><code>img = Image.open(\"flower.jpg\")\nt = transforms.ToTensor()(img)\n\n# PIL: (224, 224)\n# Tensor: (3, 224, 224)\n# Values: 0.0\u20131.0\n</code></pre> <p>This scaling stabilizes training because:</p> <ul> <li>Values share a consistent scale\\</li> <li>Prevents exploding activations</li> </ul>"},{"location":"tools/pytorch/6_transform/#3-normalization","title":"3. Normalization","text":"<p>Even after scaling to 0--1, your dataset may have:</p> <ul> <li>mostly bright flowers \u2192 values cluster near 1\\</li> <li>mostly dark backgrounds \u2192 values cluster near 0</li> </ul> <p>Normalization spreads values out and helps the model learn subtlecdetail.</p> <p>Typical ImageNet-style normalization:</p> <pre><code>transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n)\n</code></pre> <p>\u26a0 Normalize only works on tensors, never PIL images.</p>"},{"location":"tools/pytorch/6_transform/#understanding-transform-order","title":"Understanding Transform Order","text":"<p>Transformation pipelines happen in sequence.</p> <p>Think of <code>ToTensor</code> as a bridge:</p> <p>Before <code>ToTensor</code>     After <code>ToTensor</code></p> <p>Works on PIL images   Works on tensors   Resize, crop, flip    Normalize</p> <p>In modern TorchVision, many image ops work on both sides, but some don't.</p>"},{"location":"tools/pytorch/6_transform/#complete-transform-pipeline-for-oxford-flowers","title":"Complete Transform Pipeline for Oxford Flowers","text":"<pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(256),            # keep aspect ratio\n    transforms.CenterCrop(224),        # square crop\n    transforms.ToTensor(),             # PIL \u2192 Tensor, scale to 0\u20131\n    transforms.Normalize(               # improve training stability\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n</code></pre> <p>Add this when creating your dataset:</p> <pre><code>dataset = OxfordFlowersDataset(\n    root_dir=\"path/to/jpg\",\n    labels_mat_file=\"path/to/labels.mat\",\n    transform=transform\n)\n</code></pre>"},{"location":"tools/pytorch/6_transform/#debugging-transform-pipelines","title":"Debugging Transform Pipelines","text":"<p>A life-saving technique:</p>"},{"location":"tools/pytorch/6_transform/#1-pull-a-single-transformed-image","title":"1. Pull a single transformed image","text":"<pre><code>img, label = dataset[42]\nprint(img.shape)\n</code></pre>"},{"location":"tools/pytorch/6_transform/#2-pull-a-raw-image","title":"2. Pull a raw image","text":"<pre><code>raw_img = Image.open(\"path/to/image0043.jpg\")\n</code></pre>"},{"location":"tools/pytorch/6_transform/#3-apply-transforms-one-by-one","title":"3. Apply transforms one by one","text":"<pre><code>step1 = transforms.Resize(256)(raw_img)\nstep2 = transforms.CenterCrop(224)(step1)\nstep3 = transforms.ToTensor()(step2)\nstep4 = transforms.Normalize(...)(step3)\n</code></pre> <p>This lets you catch:</p> <ul> <li>Distorted images\\</li> <li>Wrong order bugs\\</li> <li>Type mismatches (e.g., Normalize on PIL)</li> </ul> <p>Debugging transforms this way saves hours.</p>"},{"location":"tools/pytorch/7_dataloader/","title":"PyTorch Data Splitting &amp; DataLoader: Building a Reliable Training Pipeline","text":""},{"location":"tools/pytorch/7_dataloader/#pytorch-data-splitting-dataloader-building-a-reliable-training-pipeline","title":"PyTorch Data Splitting &amp; DataLoader: Building a Reliable Training Pipeline","text":"<p>In the previous article, we transformed and prepared your images for model consumption. Now, it's time to tackle two critical steps in every machine learning workflow:</p> <ul> <li>Splitting  dataset into  training ,  validation , and  test  sets  </li> <li>Using  DataLoader  to efficiently serve your data in  batches </li> </ul> <p>These steps may feel like simple logistics, but they directly affect model performance, fairness, and reliability.</p>"},{"location":"tools/pytorch/7_dataloader/#why-not-train-on-the-whole-dataset","title":"Why Not Train on the Whole Dataset?","text":"<p>If more data is better, why don\u2019t we train using  all  flower images?</p> <p>Because training accuracy tells you only how well your model memorizes. It does  not  tell you how well it  generalizes  to new images.</p> <p>That\u2019s why we split the data:</p> Split Purpose Training set Used to teach the model Validation set Used during training to tune hyperparameters &amp; detect overfitting Test set Used once at the very end to report final unbiased performance <p>This split ensures your model learns  patterns , not  memorizes answers .</p>"},{"location":"tools/pytorch/7_dataloader/#splitting-the-dataset-in-pytorch","title":"Splitting the Dataset in PyTorch","text":"<p>Use <code>random_split()</code> to ensure a  random and balanced  distribution of flower types.</p> <p>Example for Oxford Flowers (8,189 images):</p> <pre><code>train_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_set, val_set, test_set = torch.utils.data.random_split(\n    dataset, [train_size, val_size, test_size]\n)\n</code></pre> <p>This ensures: - No rounding errors - All images are used exactly once  </p> <p>Your  original dataset remains unchanged  \u2014 you\u2019re only creating views.</p>"},{"location":"tools/pytorch/7_dataloader/#dataloader-efficiently-serving-data-in-batches","title":"DataLoader: Efficiently Serving Data in Batches","text":"<p>A <code>Dataset</code> gives access to  one sample at a time . A <code>DataLoader</code> groups data into  batches , making training much faster on GPU.</p> <pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_set, batch_size=32, shuffle=False)\ntest_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n</code></pre>"},{"location":"tools/pytorch/7_dataloader/#why-shuffle-only-in-training","title":"Why Shuffle Only in Training?","text":"Split shuffle? Why? Training \u2714 Yes Mixes classes to avoid bias &amp; forgetting Validation \u274c No Does not affect evaluation Test \u274c No Ensures deterministic, repeatable results <p>Shuffling prevents your model from mistakenly learning:</p> <p>\"Daisies always come first, then roses.\"</p> <p>Instead, it learns image  patterns , not  order .</p>"},{"location":"tools/pytorch/7_dataloader/#inspecting-batches","title":"Inspecting Batches","text":"<p>A single batch contains:</p> <ul> <li>A tensor of 32 images \u2192 <code>(32, 3, 224, 224)</code></li> <li>A tensor of 32 labels \u2192 <code>(32,)</code></li> </ul> <p>Verify:</p> <pre><code>images, labels = next(iter(train_loader))\nprint(images.shape)   # torch.Size([32, 3, 224, 224])\nprint(labels.shape)   # torch.Size([32])\n</code></pre>"},{"location":"tools/pytorch/7_dataloader/#batches-remainders-epochs-explained","title":"Batches, Remainders &amp; Epochs Explained","text":"<p>If you have 5,732 training images and a batch size of 32:</p> <pre><code>5732 / 32 = 179 full batches + 1 partial batch (4 images)\n</code></pre> <p>So an  epoch  = 180 batches.</p> <p>PyTorch still uses that last small batch \u2014 that\u2019s  normal and expected .</p>"},{"location":"tools/pytorch/7_dataloader/#common-mistake-reloading-data-in-__getitem__","title":"Common Mistake: Reloading Data in <code>__getitem__</code>","text":"<pre><code>def __getitem__(self, idx):\n    df = pd.read_csv(\"labels.csv\")  # \u274c Very expensive!\n</code></pre> <p>This reloads the whole file  thousands of times per epoch .</p> <p>\u2714 Instead: Load once in <code>__init__</code></p> <pre><code>def __init__(self, csv_path):\n    self.labels = pd.read_csv(csv_path)  # loaded once\n</code></pre> <p>This change alone can speed training up by  10\u00d7 or more .</p>"},{"location":"tools/pytorch/7_dataloader/#if-you-get-cuda-out-of-memory-errors","title":"If You Get CUDA Out of Memory Errors\u2026","text":"<p>Try reducing batch size before anything else:</p> <pre><code>train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n</code></pre> <p>Then gradually increase until you find the optimal value for your GPU.</p>"},{"location":"tools/pytorch/7_dataloader/#complete-data-pipeline-example","title":"Complete Data Pipeline Example","text":"<pre><code>train_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\nfor images, labels in train_loader:\n    print(images.shape, labels.shape)\n    break\n</code></pre>"},{"location":"tools/pytorch/8_augmentation/","title":"Making Your PyTorch Pipeline Robust: Augmentation, Error Handling &amp; Monitoring","text":""},{"location":"tools/pytorch/8_augmentation/#making-your-pytorch-pipeline-robust-augmentation-error-handling-monitoring","title":"Making Your PyTorch Pipeline Robust: Augmentation, Error Handling &amp; Monitoring","text":"<p>Real-world datasets are messy, unpredictable, and full of surprises. In this article, you'll learn how to make your pipeline:</p> <ul> <li>More  robust  (able to handle corrupted or problematic images)  </li> <li>More  generalizable  (able to perform well in real-world conditions)  </li> <li>More  visible  (able to monitor access patterns and detect issues early)</li> </ul>"},{"location":"tools/pytorch/8_augmentation/#1-making-your-model-more-robust-with-data-augmentation","title":"1. Making Your Model More Robust with Data Augmentation","text":"<p>So far, your model has only seen ideal flower images\u2014centered, well-lit, and clean. But real-world images vary in:</p> <ul> <li>Lighting  </li> <li>Orientation  </li> <li>Backgrounds  </li> <li>Perspective  </li> <li>Positioning  </li> </ul> <p>That's where  data augmentation  helps.</p>"},{"location":"tools/pytorch/8_augmentation/#old-approach-inefficient","title":"Old Approach (Inefficient)","text":"<p>Save rotated, flipped, or brightened copies as physical files.</p>"},{"location":"tools/pytorch/8_augmentation/#smarter-solution-on-the-fly-augmentation","title":"Smarter Solution: On-the-Fly Augmentation","text":"<p>PyTorch applies random transformations  each time  an image is loaded.</p> <pre><code>from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n</code></pre>"},{"location":"tools/pytorch/8_augmentation/#why-no-augmentation-in-validation","title":"Why no augmentation in validation?","text":"<p>Because validation should reflect  real-world performance , not random alterations.</p>"},{"location":"tools/pytorch/8_augmentation/#single-batch-test-catch-problems-before-training","title":"Single Batch Test \u2014 Catch Problems Before Training","text":"<p>Before training for hours, verify that:</p> <ul> <li>Shapes are correct  </li> <li>Transformations work  </li> <li>Labels are valid  </li> <li>No file crashes the loader  </li> </ul> <pre><code>images, labels = next(iter(train_loader))\nprint(\"Images shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)\n</code></pre> <p>Expected output:</p> <pre><code>Images shape: torch.Size([32, 3, 224, 224])\nLabels shape: torch.Size([32])\n</code></pre> <p>Tip: Test this before every major model change.</p>"},{"location":"tools/pytorch/8_augmentation/#3-error-handling-dont-let-one-bad-image-crash-training","title":"3. Error Handling \u2014 Don\u2019t Let One Bad Image Crash Training","text":"<p>Real-world datasets contain:</p> <ul> <li>Corrupted files  </li> <li>Extremely small images  </li> <li>Grayscale images  </li> <li>Unsupported formats  </li> </ul> <p>Here\u2019s how to make <code>__getitem__</code> resilient:</p> <pre><code>from PIL import Image\n\ndef __getitem__(self, idx):\n    img_path = self.image_paths[idx]\n\n    try:\n        img = Image.open(img_path)\n        img.verify()        # file is not corrupted\n        img = Image.open(img_path)  # reopen\n        img = img.convert(\"RGB\")\n\n        if img.size[0] &lt; 100 or img.size[1] &lt; 100:\n            raise ValueError(\"Image too small\")\n\n        if self.transform:\n            img = self.transform(img)\n\n    except Exception as e:\n        print(f\"\u26a0 Error loading {img_path}: {e}\")\n        return self.__getitem__((idx + 1) % len(self))\n\n    return img, self.labels[idx]\n</code></pre> <p>\ud83d\udee1 Your pipeline now  skips  invalid files instead of crashing.</p>"},{"location":"tools/pytorch/8_augmentation/#4-visualizing-augmentations-are-they-too-strong","title":"4. Visualizing Augmentations \u2014 Are They Too Strong?","text":"<p>Overly aggressive augmentations can destroy key features.</p> <pre><code>def reverse_normalize(tensor):\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    return tensor * std + mean\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 4, figsize=(12,6))\nfor i in range(8):\n    img, _ = dataset[i]\n    img = reverse_normalize(img).permute(1,2,0)\n    axes[i//4, i%4].imshow(img)\n    axes[i//4, i%4].axis('off')\nplt.show()\n</code></pre> <p>Look for: | Result | Meaning | |--------|---------| | Recognizable flowers | \ud83d\udc4d Good augmentation | | No change | \u274c Too weak | | Abstract blobs | \u26a0 Too strong | | Black/wild colors | \u26a0 Normalization mistake |</p>"},{"location":"tools/pytorch/8_augmentation/#5-monitoring-dataset-usage-see-whats-happening-during-training","title":"5. Monitoring Dataset Usage \u2014 See What's Happening During Training","text":"<p>Your data might  look fine , but hidden problems include:</p> <ul> <li>Some images never used (shuffling bug)  </li> <li>Certain images used too often (bias)  </li> <li>Extremely slow load times  </li> <li>Poor variability in samples  </li> </ul>"},{"location":"tools/pytorch/8_augmentation/#add-lightweight-tracking","title":"Add lightweight tracking:","text":"<pre><code>from time import time\n\nself.access_count = {}\nself.load_times = []\n\ndef __getitem__(self, idx):\n    start = time()\n    img, label = load_image(idx)  # existing logic\n    self.load_times.append(time() - start)\n\n    self.access_count[idx] = self.access_count.get(idx, 0) + 1\n    return img, label\n\ndef print_stats(self):\n    print(\"Average load time:\", sum(self.load_times)/len(self.load_times))\n    print(\"Most accessed:\", sorted(self.access_count.items(), key=lambda x: x[1], reverse=True)[:5])\n</code></pre> <p>Call <code>print_stats()</code> at the end of each epoch to track issues early.</p>"},{"location":"tools/pytorch/9_hyperparamtersearch/","title":"9 hyperparamtersearch","text":""},{"location":"tools/pytorch/9_hyperparamtersearch/#optuna-tree-structured-parzen-estimator","title":"Optuna - Tree Structured Parzen Estimator","text":""},{"location":"tools/pytorch/Examples/2_tensor_basics/","title":"2 tensor basics","text":"In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch In\u00a0[2]: Copied! <pre>x = torch.tensor([25.0])\nx = x.unsqueeze(0)\nx = x.squeeze()\nprint(x.shape)\n</pre> x = torch.tensor([25.0]) x = x.unsqueeze(0) x = x.squeeze() print(x.shape) <pre>torch.Size([])\n</pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/","title":"Programming Assignment - Overcoming Overfitting: Building a Robust CNN","text":"TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT: <ul> <li><p>All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.</p> </li> <li><p>In each exercise cell, look for comments <code>### START CODE HERE ###</code> and <code>### END CODE HERE ###</code>. These show you where to write the solution code. Do not add or change any code that is outside these comments.</p> </li> <li><p>You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.</p> </li> <li><p>Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.</p> </li> <li><p>To submit your notebook for grading, first save it by clicking the \ud83d\udcbe icon on the top left of the page and then click on the <code>Submit assignment</code> button on the top right of the page.</p> </li> </ul> <p></p> In\u00a0[6]: graded Copied! <pre>import copy \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n</pre> import copy   import torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as transforms from torch.utils.data import DataLoader In\u00a0[7]: Copied! <pre>import helper_utils\nimport unittests\n</pre> import helper_utils import unittests In\u00a0[8]: Copied! <pre># Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n</pre> # Device configuration device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p></p> In\u00a0[9]: Copied! <pre># Pre-calculated mean for each of the 3 channels of the CIFAR-100 dataset\ncifar100_mean = (0.5071, 0.4867, 0.4408)\n# Pre-calculated standard deviation for each of the 3 channels of the CIFAR-100 dataset\ncifar100_std = (0.2675, 0.2565, 0.2761)\n</pre> # Pre-calculated mean for each of the 3 channels of the CIFAR-100 dataset cifar100_mean = (0.5071, 0.4867, 0.4408) # Pre-calculated standard deviation for each of the 3 channels of the CIFAR-100 dataset cifar100_std = (0.2675, 0.2565, 0.2761) <p>As you learned previously, the training transformation pipeline is where you apply data augmentation. To make your model even more robust, you will add a new technique to your arsenal this time: <code>RandomVerticalFlip</code>. While horizontal flipping is common, adding vertical flips can also help the model learn that an object's orientation might not always be upright, a useful feature for classifying things like insects or flowers from various angles.</p> <p></p> In\u00a0[16]: graded Copied! <pre># GRADED FUNCTION: define_transformations\n\ndef define_transformations(mean, std):\n    \"\"\"\n    Creates image transformation pipelines for training and validation.\n\n    Args:\n        mean (list or tuple): A sequence of mean values for each channel.\n        std (list or tuple): A sequence of standard deviation values for each channel.\n\n    Returns:\n        train_transformations (torchvision.transforms.Compose): The training\n                                                                transformation pipeline.\n        val_transformations (torchvision.transforms.Compose): The validation\n                                                                transformation pipeline.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # Define the sequence of transformations for the training dataset.\n    \n    train_transformations = transforms.Compose([\n        # Randomly flip the image horizontally with a 50% probability.\n        transforms.RandomHorizontalFlip(0.5),\n        # Randomly flip the image vertically with a 50% probability.\n        transforms.RandomVerticalFlip(0.5),\n        # Rotate the image by a random angle between -15 and +15 degrees.\n        transforms.RandomRotation(degrees=[-15.0, 15.0]),\n        # Convert the image from a PIL Image or NumPy array to a PyTorch tensor.\n        transforms.ToTensor(),\n        # Normalize the tensor image with the given mean and standard deviation.\n        transforms.Normalize(mean, std)\n    ]) \n    \n    # Define the sequence of transformations for the validation dataset.\n    val_transformations = transforms.Compose([\n        # Convert the image from a PIL Image or NumPy array to a PyTorch tensor.\n        transforms.ToTensor(),\n        # Normalize the tensor image with the given mean and standard deviation.\n        transforms.Normalize(mean, std)\n    ]) \n    \n    ### END CODE HERE ###\n\n    # Return both transformation pipelines.\n    return train_transformations, val_transformations\n</pre> # GRADED FUNCTION: define_transformations  def define_transformations(mean, std):     \"\"\"     Creates image transformation pipelines for training and validation.      Args:         mean (list or tuple): A sequence of mean values for each channel.         std (list or tuple): A sequence of standard deviation values for each channel.      Returns:         train_transformations (torchvision.transforms.Compose): The training                                                                 transformation pipeline.         val_transformations (torchvision.transforms.Compose): The validation                                                                 transformation pipeline.     \"\"\"          ### START CODE HERE ###          # Define the sequence of transformations for the training dataset.          train_transformations = transforms.Compose([         # Randomly flip the image horizontally with a 50% probability.         transforms.RandomHorizontalFlip(0.5),         # Randomly flip the image vertically with a 50% probability.         transforms.RandomVerticalFlip(0.5),         # Rotate the image by a random angle between -15 and +15 degrees.         transforms.RandomRotation(degrees=[-15.0, 15.0]),         # Convert the image from a PIL Image or NumPy array to a PyTorch tensor.         transforms.ToTensor(),         # Normalize the tensor image with the given mean and standard deviation.         transforms.Normalize(mean, std)     ])           # Define the sequence of transformations for the validation dataset.     val_transformations = transforms.Compose([         # Convert the image from a PIL Image or NumPy array to a PyTorch tensor.         transforms.ToTensor(),         # Normalize the tensor image with the given mean and standard deviation.         transforms.Normalize(mean, std)     ])           ### END CODE HERE ###      # Return both transformation pipelines.     return train_transformations, val_transformations In\u00a0[17]: Copied! <pre># Verify the Transformations\nprint(\"--- Verifying define_transformations ---\\n\")\ntrain_transform_verify, val_transform_verify = define_transformations(cifar100_mean, cifar100_std)\n\n\nprint(\"Training Transformations:\")\nprint(train_transform_verify)\nprint(\"-\" * 30)\nprint(\"\\nValidation Transformations:\")\nprint(val_transform_verify)\n</pre> # Verify the Transformations print(\"--- Verifying define_transformations ---\\n\") train_transform_verify, val_transform_verify = define_transformations(cifar100_mean, cifar100_std)   print(\"Training Transformations:\") print(train_transform_verify) print(\"-\" * 30) print(\"\\nValidation Transformations:\") print(val_transform_verify) <pre>--- Verifying define_transformations ---\n\nTraining Transformations:\nCompose(\n    RandomHorizontalFlip(p=0.5)\n    RandomVerticalFlip(p=0.5)\n    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n    ToTensor()\n    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n)\n------------------------------\n\nValidation Transformations:\nCompose(\n    ToTensor()\n    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n)\n</pre> In\u00a0[18]: Copied! <pre># Test your code!\nunittests.exercise_1(define_transformations)\n</pre> # Test your code! unittests.exercise_1(define_transformations) <pre> All tests passed!\n</pre> <ul> <li>Call the <code>define_transformations</code> function, passing the <code>cifar100_mean</code> and <code>cifar100_std</code> as arguments.</li> <li>This returns two separate transformation pipelines, which are stored in the <code>train_transform</code> and <code>val_transform</code> variables for later use.</li> </ul> In\u00a0[19]: Copied! <pre># Create and store the training and validation transformation pipelines\ntrain_transform, val_transform = define_transformations(cifar100_mean, cifar100_std)\n</pre> # Create and store the training and validation transformation pipelines train_transform, val_transform = define_transformations(cifar100_mean, cifar100_std) <p></p> In\u00a0[20]: Copied! <pre># Define the full class list.\nall_target_classes = [\n    # Flowers\n    'orchid', 'poppy', 'rose', 'sunflower', 'tulip',\n    # Mammals\n    'fox', 'porcupine', 'possum', 'raccoon', 'skunk',\n    # Insects\n    'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'\n]\n</pre> # Define the full class list. all_target_classes = [     # Flowers     'orchid', 'poppy', 'rose', 'sunflower', 'tulip',     # Mammals     'fox', 'porcupine', 'possum', 'raccoon', 'skunk',     # Insects     'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach' ] <ul> <li>Next, call the <code>load_cifar100_subset</code> function, passing in your class list (<code>all_target_classes</code>) and both transformation pipelines (<code>train_transform</code> and <code>val_transform</code>).</li> <li>This function handles the entire loading process and returns two PyTorch <code>Dataset</code> objects, which are stored in the <code>train_dataset</code> and <code>val_dataset</code> variables.</li> </ul> In\u00a0[21]: Copied! <pre># Load the full datasets.\ntrain_dataset, val_dataset = helper_utils.load_cifar100_subset(all_target_classes, train_transform, val_transform)\n</pre> # Load the full datasets. train_dataset, val_dataset = helper_utils.load_cifar100_subset(all_target_classes, train_transform, val_transform) <pre>Dataset found in './cifar_100'. Loading from local files.\nDataset loaded successfully.\n\nFiltering for 15 classes...\nFiltering complete. Returning training and validation datasets.\n</pre> <p>With your datasets prepared, the final step is to wrap them in PyTorch's <code>DataLoader</code>. This utility is essential for feeding data to your model in manageable batches.</p> <ul> <li>Create the <code>train_loader</code> for your training data.</li> <li>Create the <code>val_loader</code> for your validation data.</li> </ul> In\u00a0[22]: Copied! <pre># Set the number of samples to be processed in each batch\nbatch_size = 64\n\n# Create a data loader for the training set, with shuffling enabled\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# Create a data loader for the validation set, without shuffling\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n</pre> # Set the number of samples to be processed in each batch batch_size = 64  # Create a data loader for the training set, with shuffling enabled train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Create a data loader for the validation set, without shuffling val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) <p></p> In\u00a0[23]: Copied! <pre># Visualize a grid of random training images\nhelper_utils.visualise_images(train_loader, grid=(3, 5))\n</pre> # Visualize a grid of random training images helper_utils.visualise_images(train_loader, grid=(3, 5)) <p></p> In\u00a0[27]: graded Copied! <pre># GRADED CLASS: CNNBlock\n\nclass CNNBlock(nn.Module):\n    \"\"\"\n    Defines a single convolutional block for a CNN.\n\n    This block consists of a convolutional layer, batch normalization,\n    a ReLU activation, and a max-pooling layer, bundled as a sequential module.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        \"\"\"\n        Initializes the layers of the CNNBlock.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the convolution.\n            kernel_size (int, optional): Size of the convolving kernel. Defaults to 3.\n            padding (int, optional): Zero-padding added to both sides of the input. Defaults to 1.\n        \"\"\"\n        # Initialize the parent nn.Module class.\n        super(CNNBlock, self).__init__()\n        \n        ### START CODE HERE ###\n        \n        # Define the sequential container for the block's layers.\n        self.block = nn.Sequential(\n            # 2D convolutional layer to apply learnable filters to the input.\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n            # Batch normalization to stabilize and accelerate training.\n            nn.BatchNorm2d(out_channels),\n            # ReLU activation function to introduce non-linearity.\n            nn.ReLU(),\n            # Max pooling layer to downsample the feature map and reduce spatial dimensions.\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        ) \n        \n        ### END CODE HERE ###\n\n    def forward(self, x):\n        \"\"\"\n        Defines the forward pass for the CNNBlock.\n\n        Args:\n            x: The input tensor for the block.\n\n        Returns:\n            The output tensor after passing through the block.\n        \"\"\"\n        \n        ### START CODE HERE ###\n        \n        # Pass the input tensor through the sequential block of layers.\n        x = self.block(x)\n        return x\n    \n        ### END CODE HERE ###\n</pre> # GRADED CLASS: CNNBlock  class CNNBlock(nn.Module):     \"\"\"     Defines a single convolutional block for a CNN.      This block consists of a convolutional layer, batch normalization,     a ReLU activation, and a max-pooling layer, bundled as a sequential module.     \"\"\"     def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):         \"\"\"         Initializes the layers of the CNNBlock.          Args:             in_channels (int): Number of channels in the input image.             out_channels (int): Number of channels produced by the convolution.             kernel_size (int, optional): Size of the convolving kernel. Defaults to 3.             padding (int, optional): Zero-padding added to both sides of the input. Defaults to 1.         \"\"\"         # Initialize the parent nn.Module class.         super(CNNBlock, self).__init__()                  ### START CODE HERE ###                  # Define the sequential container for the block's layers.         self.block = nn.Sequential(             # 2D convolutional layer to apply learnable filters to the input.             nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),             # Batch normalization to stabilize and accelerate training.             nn.BatchNorm2d(out_channels),             # ReLU activation function to introduce non-linearity.             nn.ReLU(),             # Max pooling layer to downsample the feature map and reduce spatial dimensions.             nn.MaxPool2d(kernel_size=2, stride=2)         )                   ### END CODE HERE ###      def forward(self, x):         \"\"\"         Defines the forward pass for the CNNBlock.          Args:             x: The input tensor for the block.          Returns:             The output tensor after passing through the block.         \"\"\"                  ### START CODE HERE ###                  # Pass the input tensor through the sequential block of layers.         x = self.block(x)         return x              ### END CODE HERE ### In\u00a0[28]: Copied! <pre># Verify the CNNBlock\nprint(\"--- Verifying CNNBlock ---\\n\")\n\n# Instantiate the block with 3 input channels and 16 output channels\nverify_cnn_block = CNNBlock(in_channels=3, out_channels=16)\nprint(\"Block Structure:\\n\")\nprint(verify_cnn_block)\n\n# Verify the output shape after a forward pass\n# Create a dummy input tensor (batch_size=1, channels=3, height=32, width=32)\ndummy_input = torch.randn(1, 3, 32, 32)\nprint(f\"\\nInput tensor shape:  {dummy_input.shape}\")\n\n# Pass the dummy tensor through the block\noutput = verify_cnn_block(dummy_input)\nprint(f\"Output tensor shape: {output.shape}\")\n</pre> # Verify the CNNBlock print(\"--- Verifying CNNBlock ---\\n\")  # Instantiate the block with 3 input channels and 16 output channels verify_cnn_block = CNNBlock(in_channels=3, out_channels=16) print(\"Block Structure:\\n\") print(verify_cnn_block)  # Verify the output shape after a forward pass # Create a dummy input tensor (batch_size=1, channels=3, height=32, width=32) dummy_input = torch.randn(1, 3, 32, 32) print(f\"\\nInput tensor shape:  {dummy_input.shape}\")  # Pass the dummy tensor through the block output = verify_cnn_block(dummy_input) print(f\"Output tensor shape: {output.shape}\") <pre>--- Verifying CNNBlock ---\n\nBlock Structure:\n\nCNNBlock(\n  (block): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n)\n\nInput tensor shape:  torch.Size([1, 3, 32, 32])\nOutput tensor shape: torch.Size([1, 16, 16, 16])\n</pre> In\u00a0[29]: Copied! <pre># Test your code!\nunittests.exercise_2(CNNBlock)\n</pre> # Test your code! unittests.exercise_2(CNNBlock) <pre> All tests passed!\n</pre> <p></p> In\u00a0[33]: graded Copied! <pre># GRADED CLASS: SimpleCNN\n\nclass SimpleCNN(nn.Module):\n    \"\"\"\n    Defines a simple CNN architecture using modular CNNBlocks.\n\n    This model stacks three reusable convolutional blocks followed by a fully\n    connected classifier to perform image classification.\n    \"\"\"\n    def __init__(self, num_classes):\n        \"\"\"\n        Initializes the layers of the SimpleCNN model.\n\n        Args:\n            num_classes (int): The number of output classes for the classifier.\n        \"\"\"\n        # Initialize the parent nn.Module class.\n        super(SimpleCNN, self).__init__()\n        \n        ### START CODE HERE ###\n\n        # Define the first convolutional block.\n        self.conv_block1 = CNNBlock(3, 32)\n        # Define the second convolutional block.\n        self.conv_block2 = CNNBlock(32, 64)\n        # Define the third convolutional block.\n        self.conv_block3 = CNNBlock(64, 128)\n\n        # Define the fully connected classifier block.\n        self.classifier = nn.Sequential(\n            # Flatten the 3D feature map (channels, height, width) into a 1D vector.\n            nn.Flatten(),\n            # First fully connected (linear) layer that maps the flattened features to a hidden layer.\n            nn.Linear(2048, 512),\n            # ReLU activation function to introduce non-linearity.\n            nn.ReLU(),\n            # Dropout layer to prevent overfitting by randomly setting a fraction of inputs to zero.\n            nn.Dropout(0.6),\n            # Final fully connected (linear) layer that maps the hidden layer to the output classes.\n            nn.Linear(512, num_classes)\n        ) \n        \n        ### END CODE HERE ###\n\n    def forward(self, x):\n        \"\"\"\n        Defines the forward pass of the SimpleCNN model.\n\n        Args:\n            x (torch.Tensor): The input tensor containing a batch of images.\n\n        Returns:\n            torch.Tensor: The output tensor with logits for each class.\n        \"\"\"\n        \n        ### START CODE HERE ###\n        \n        # Pass the input through the first convolutional block.\n        x = self.conv_block1(x)\n        # Pass the result through the second convolutional block.\n        x = self.conv_block2(x)\n        # Pass the result through the third convolutional block.\n        x = self.conv_block3(x)\n\n        # Pass the final feature map through the classifier.\n        x = self.classifier(x)\n        \n        ### END CODE HERE ###\n        \n        # Return the final output tensor.\n        return x\n</pre> # GRADED CLASS: SimpleCNN  class SimpleCNN(nn.Module):     \"\"\"     Defines a simple CNN architecture using modular CNNBlocks.      This model stacks three reusable convolutional blocks followed by a fully     connected classifier to perform image classification.     \"\"\"     def __init__(self, num_classes):         \"\"\"         Initializes the layers of the SimpleCNN model.          Args:             num_classes (int): The number of output classes for the classifier.         \"\"\"         # Initialize the parent nn.Module class.         super(SimpleCNN, self).__init__()                  ### START CODE HERE ###          # Define the first convolutional block.         self.conv_block1 = CNNBlock(3, 32)         # Define the second convolutional block.         self.conv_block2 = CNNBlock(32, 64)         # Define the third convolutional block.         self.conv_block3 = CNNBlock(64, 128)          # Define the fully connected classifier block.         self.classifier = nn.Sequential(             # Flatten the 3D feature map (channels, height, width) into a 1D vector.             nn.Flatten(),             # First fully connected (linear) layer that maps the flattened features to a hidden layer.             nn.Linear(2048, 512),             # ReLU activation function to introduce non-linearity.             nn.ReLU(),             # Dropout layer to prevent overfitting by randomly setting a fraction of inputs to zero.             nn.Dropout(0.6),             # Final fully connected (linear) layer that maps the hidden layer to the output classes.             nn.Linear(512, num_classes)         )                   ### END CODE HERE ###      def forward(self, x):         \"\"\"         Defines the forward pass of the SimpleCNN model.          Args:             x (torch.Tensor): The input tensor containing a batch of images.          Returns:             torch.Tensor: The output tensor with logits for each class.         \"\"\"                  ### START CODE HERE ###                  # Pass the input through the first convolutional block.         x = self.conv_block1(x)         # Pass the result through the second convolutional block.         x = self.conv_block2(x)         # Pass the result through the third convolutional block.         x = self.conv_block3(x)          # Pass the final feature map through the classifier.         x = self.classifier(x)                  ### END CODE HERE ###                  # Return the final output tensor.         return x In\u00a0[34]: Copied! <pre># Verify the SimpleCNN\nprint(\"--- Verifying SimpleCNN ---\\n\")\n\n# Verify the structure of the model\n# Instantiate the model with 15 output classes\nverify_simple_cnn = SimpleCNN(num_classes=15)\nprint(\"Model Structure:\\n\")\nprint(verify_simple_cnn)\n\n# Verify the output shape after a forward pass\n# Create a dummy input tensor (batch_size=64, channels=3, height=32, width=32)\ndummy_input = torch.randn(64, 3, 32, 32)\nprint(f\"\\nInput tensor shape:  {dummy_input.shape}\")\n\n# Pass the dummy tensor through the model\noutput = verify_simple_cnn(dummy_input)\nprint(f\"Output tensor shape: {output.shape}\")\n</pre> # Verify the SimpleCNN print(\"--- Verifying SimpleCNN ---\\n\")  # Verify the structure of the model # Instantiate the model with 15 output classes verify_simple_cnn = SimpleCNN(num_classes=15) print(\"Model Structure:\\n\") print(verify_simple_cnn)  # Verify the output shape after a forward pass # Create a dummy input tensor (batch_size=64, channels=3, height=32, width=32) dummy_input = torch.randn(64, 3, 32, 32) print(f\"\\nInput tensor shape:  {dummy_input.shape}\")  # Pass the dummy tensor through the model output = verify_simple_cnn(dummy_input) print(f\"Output tensor shape: {output.shape}\") <pre>--- Verifying SimpleCNN ---\n\nModel Structure:\n\nSimpleCNN(\n  (conv_block1): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (conv_block2): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (conv_block3): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2048, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.6, inplace=False)\n    (4): Linear(in_features=512, out_features=15, bias=True)\n  )\n)\n\nInput tensor shape:  torch.Size([64, 3, 32, 32])\nOutput tensor shape: torch.Size([64, 15])\n</pre> <p>NOTE: The test below evaluates your <code>SimpleCNN</code> class, which internally uses the <code>CNNBlock</code> you implemented in the previous exercise. If you did not pass the test for <code>Exercise 2 - CNNBlock</code>, running the following cell will likely return an error. Please make sure your <code>CNNBlock</code> implementation is correct first!</p> In\u00a0[35]: Copied! <pre># Test your code!\nunittests.exercise_3(SimpleCNN, CNNBlock)\n</pre> # Test your code! unittests.exercise_3(SimpleCNN, CNNBlock) <pre> All tests passed!\n</pre> <p>With your <code>SimpleCNN</code> class defined, the next step is to create an instance of the model.</p> <ul> <li>First, dynamically determine the number of classes by getting the length of the <code>.classes</code> attribute from your <code>train_dataset</code>.</li> <li>Next, create an instance of your <code>SimpleCNN</code> model, passing the <code>num_classes</code> variable to its constructor. This ensures the final layer of your model is correctly sized for your 15-class problem.</li> </ul> In\u00a0[36]: Copied! <pre># Get the number of classes\nnum_classes = len(train_dataset.classes)\n\n# Instantiate the model\nmodel = SimpleCNN(num_classes)\n</pre> # Get the number of classes num_classes = len(train_dataset.classes)  # Instantiate the model model = SimpleCNN(num_classes) <p></p> In\u00a0[37]: Copied! <pre># Loss function\nloss_function = nn.CrossEntropyLoss()\n\n# Optimizer for the model with weight_decay\noptimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n</pre> # Loss function loss_function = nn.CrossEntropyLoss()  # Optimizer for the model with weight_decay optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005) <p></p> In\u00a0[38]: graded Copied! <pre># GRADED FUNCTION: train_epoch\n\ndef train_epoch(model, train_loader, loss_function, optimizer, device):\n    \"\"\"\n    Performs a single training epoch.\n\n    Args:\n        model (torch.nn.Module): The neural network model to train.\n        train_loader (torch.utils.data.DataLoader): The DataLoader for the training data.\n        loss_function (callable): The loss function.\n        optimizer (torch.optim.Optimizer): The optimizer.\n        device (torch.device): The device (CPU or GPU) to perform training on.\n\n    Returns:\n        float: The average training loss for the epoch.\n    \"\"\"\n    # Set the model to training mode\n    model.train()\n    running_loss = 0.0\n    # Iterate over batches of data in the training loader\n    for images, labels in train_loader:\n        # Move images and labels to the specified device\n        images, labels = images.to(device), labels.to(device)\n        \n        ### START CODE HERE ###\n        \n        # Clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # Perform a forward pass to get model outputs\n        outputs = model(images)\n        # Calculate the loss\n        loss = loss_function(outputs, labels)\n        # Perform a backward pass to compute gradients\n        loss.backward()\n        # Update the model parameters\n        optimizer.step()\n        \n        ### END CODE HERE ###\n        \n        # Accumulate the training loss for the batch\n        running_loss += loss.item() * images.size(0)\n        \n    # Calculate and return the average training loss for the epoch\n    epoch_loss = running_loss / len(train_loader.dataset)\n    return epoch_loss\n</pre> # GRADED FUNCTION: train_epoch  def train_epoch(model, train_loader, loss_function, optimizer, device):     \"\"\"     Performs a single training epoch.      Args:         model (torch.nn.Module): The neural network model to train.         train_loader (torch.utils.data.DataLoader): The DataLoader for the training data.         loss_function (callable): The loss function.         optimizer (torch.optim.Optimizer): The optimizer.         device (torch.device): The device (CPU or GPU) to perform training on.      Returns:         float: The average training loss for the epoch.     \"\"\"     # Set the model to training mode     model.train()     running_loss = 0.0     # Iterate over batches of data in the training loader     for images, labels in train_loader:         # Move images and labels to the specified device         images, labels = images.to(device), labels.to(device)                  ### START CODE HERE ###                  # Clear the gradients of all optimized variables         optimizer.zero_grad()         # Perform a forward pass to get model outputs         outputs = model(images)         # Calculate the loss         loss = loss_function(outputs, labels)         # Perform a backward pass to compute gradients         loss.backward()         # Update the model parameters         optimizer.step()                  ### END CODE HERE ###                  # Accumulate the training loss for the batch         running_loss += loss.item() * images.size(0)              # Calculate and return the average training loss for the epoch     epoch_loss = running_loss / len(train_loader.dataset)     return epoch_loss In\u00a0[39]: Copied! <pre># Use a helper function to perform a sanity check on the train_epoch implementation\nhelper_utils.verify_training_process(SimpleCNN, train_loader, loss_function, train_epoch, device)\n</pre> # Use a helper function to perform a sanity check on the train_epoch implementation helper_utils.verify_training_process(SimpleCNN, train_loader, loss_function, train_epoch, device) <pre>--- Verifying train_epoch (training for 5 epochs) ---\n\nTraining on 640 images for 5 epochs:\n\nEpoch [1/5], Loss: 2.7142\nEpoch [2/5], Loss: 2.3190\nEpoch [3/5], Loss: 2.0868\nEpoch [4/5], Loss: 1.8954\nEpoch [5/5], Loss: 1.7484\n\nWeight Update Check:\tModel weights changed during training.\nLoss Trend Check:\tLoss decreased from 2.7142 to 1.7484.\n</pre> In\u00a0[40]: Copied! <pre># Test your code!\nunittests.exercise_4(train_epoch)\n</pre> # Test your code! unittests.exercise_4(train_epoch) <pre> All tests passed!\n</pre> <p></p> In\u00a0[54]: graded Copied! <pre># GRADED FUNCTION: validate_epoch\n\ndef validate_epoch(model, val_loader, loss_function, device):\n    \"\"\"\n    Performs a single validation epoch.\n\n    Args:\n        model (torch.nn.Module): The neural network model to validate.\n        val_loader (torch.utils.data.DataLoader): The DataLoader for the validation data.\n        loss_function (callable): The loss function.\n        device (torch.device): The device (CPU or GPU) to perform validation on.\n\n    Returns:\n        tuple: A tuple containing the average validation loss and validation accuracy.\n    \"\"\"\n    # Set the model to evaluation mode\n    model.eval()\n    running_val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    ### START CODE HERE ###\n    \n    # Disable gradient calculations for validation\n    with torch.no_grad():\n        \n    ### END CODE HERE ###\n    \n        # Iterate over batches of data in the validation loader\n        for images, labels in val_loader:\n            # Move images and labels to the specified device\n            images, labels = images.to(device), labels.to(device)\n            \n            ### START CODE HERE ###\n            \n            # Perform a forward pass to get model outputs\n            outputs = model(images)\n            \n            # Calculate the validation loss for the batch\n            val_loss = loss_function(outputs, labels)\n            # Accumulate the validation loss\n            running_val_loss += val_loss.item()\n            \n            # Get the predicted class labels\n            _, predicted = outputs.max(1)\n            \n            ### END CODE HERE ###\n            \n            # Update the total number of samples\n            total += labels.size(0)\n            # Update the number of correct predictions\n            correct += (predicted == labels).sum().item()\n            \n    # Calculate the average validation loss and accuracy for the epoch\n    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n    epoch_accuracy = 100.0 * correct / total\n    \n    return epoch_val_loss, epoch_accuracy\n</pre> # GRADED FUNCTION: validate_epoch  def validate_epoch(model, val_loader, loss_function, device):     \"\"\"     Performs a single validation epoch.      Args:         model (torch.nn.Module): The neural network model to validate.         val_loader (torch.utils.data.DataLoader): The DataLoader for the validation data.         loss_function (callable): The loss function.         device (torch.device): The device (CPU or GPU) to perform validation on.      Returns:         tuple: A tuple containing the average validation loss and validation accuracy.     \"\"\"     # Set the model to evaluation mode     model.eval()     running_val_loss = 0.0     correct = 0     total = 0          ### START CODE HERE ###          # Disable gradient calculations for validation     with torch.no_grad():              ### END CODE HERE ###              # Iterate over batches of data in the validation loader         for images, labels in val_loader:             # Move images and labels to the specified device             images, labels = images.to(device), labels.to(device)                          ### START CODE HERE ###                          # Perform a forward pass to get model outputs             outputs = model(images)                          # Calculate the validation loss for the batch             val_loss = loss_function(outputs, labels)             # Accumulate the validation loss             running_val_loss += val_loss.item()                          # Get the predicted class labels             _, predicted = outputs.max(1)                          ### END CODE HERE ###                          # Update the total number of samples             total += labels.size(0)             # Update the number of correct predictions             correct += (predicted == labels).sum().item()                  # Calculate the average validation loss and accuracy for the epoch     epoch_val_loss = running_val_loss / len(val_loader.dataset)     epoch_accuracy = 100.0 * correct / total          return epoch_val_loss, epoch_accuracy In\u00a0[55]: Copied! <pre># Use a helper function to perform a sanity check on the validate_epoch implementation\nhelper_utils.verify_validation_process(SimpleCNN, val_loader, loss_function, validate_epoch, device)\n</pre> # Use a helper function to perform a sanity check on the validate_epoch implementation helper_utils.verify_validation_process(SimpleCNN, val_loader, loss_function, validate_epoch, device) <pre>--- Verifying validate_epoch ---\n\nValidating on 640 images:\n\nReturned Validation Loss: 0.0423\nReturned Validation Accuracy: 4.38%\n\n\nReturn Types Check:\tFunction returned a float for loss and accuracy.\nWeight Integrity Check:\tModel weights were not changed during validation.\n</pre> In\u00a0[56]: Copied! <pre># Test your code!\nunittests.exercise_5(validate_epoch)\n</pre> # Test your code! unittests.exercise_5(validate_epoch) <pre> All tests passed!\n</pre> <p>With the individual functions for training and validation complete, you can now bring them together in the main <code>training_loop</code>. This function orchestrates the entire training process over a set number of epochs and includes a pivotal upgrade.</p> <p>A common challenge is that a model's performance can peak and then decline if training continues for too long. To address this, the <code>training_loop</code> will:</p> <ul> <li>Monitor the validation accuracy at the end of each epoch.</li> <li>Keep track of the best performing model state seen so far.</li> <li>After the final epoch, it automatically returns the model from its single best epoch.</li> </ul> <p>This guarantees that you always get back the version of your model that achieved the highest validation accuracy during the entire training run.</p> In\u00a0[\u00a0]: Copied! <pre>def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):\n    \"\"\"\n    Trains and validates a PyTorch neural network model.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n        loss_function (callable): The loss function.\n        optimizer (torch.optim.Optimizer): The optimization algorithm.\n        num_epochs (int): The total number of epochs to train for.\n        device (torch.device): The device (e.g., 'cuda' or 'cpu') to run training on.\n\n    Returns:\n        tuple: A tuple containing the best trained model and a list of metrics\n               (train_losses, val_losses, val_accuracies).\n    \"\"\"\n    # Move the model to the specified device (CPU or GPU)\n    model.to(device)\n    \n    # Initialize variables to track the best performing model\n    best_val_accuracy = 0.0\n    best_model_state = None\n    best_epoch = 0\n    \n    # Initialize lists to store training and validation metrics\n    train_losses, val_losses, val_accuracies = [], [], []\n    \n    print(\"--- Training Started ---\")\n    \n    # Loop over the specified number of epochs\n    for epoch in range(num_epochs):\n        # Perform one epoch of training\n        epoch_loss = train_epoch(model, train_loader, loss_function, optimizer, device)\n        train_losses.append(epoch_loss)\n        \n        # Perform one epoch of validation\n        epoch_val_loss, epoch_accuracy = validate_epoch(model, val_loader, loss_function, device)\n        val_losses.append(epoch_val_loss)\n        val_accuracies.append(epoch_accuracy)\n        \n        # Print the metrics for the current epoch\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")\n        \n        # Check if the current model is the best one so far\n        if epoch_accuracy &gt; best_val_accuracy:\n            best_val_accuracy = epoch_accuracy\n            best_epoch = epoch + 1\n            # Save the state of the best model in memory\n            best_model_state = copy.deepcopy(model.state_dict())\n            \n    print(\"--- Finished Training ---\")\n    \n    # Load the best model weights before returning\n    if best_model_state:\n        print(f\"\\n--- Returning best model with {best_val_accuracy:.2f}% validation accuracy, achieved at epoch {best_epoch} ---\")\n        model.load_state_dict(best_model_state)\n    \n    # Consolidate all metrics into a single list\n    metrics = [train_losses, val_losses, val_accuracies]\n    \n    # Return the trained model and the collected metrics\n    return model, metrics\n</pre> def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):     \"\"\"     Trains and validates a PyTorch neural network model.      Args:         model (torch.nn.Module): The model to be trained.         train_loader (torch.utils.data.DataLoader): DataLoader for the training set.         val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.         loss_function (callable): The loss function.         optimizer (torch.optim.Optimizer): The optimization algorithm.         num_epochs (int): The total number of epochs to train for.         device (torch.device): The device (e.g., 'cuda' or 'cpu') to run training on.      Returns:         tuple: A tuple containing the best trained model and a list of metrics                (train_losses, val_losses, val_accuracies).     \"\"\"     # Move the model to the specified device (CPU or GPU)     model.to(device)          # Initialize variables to track the best performing model     best_val_accuracy = 0.0     best_model_state = None     best_epoch = 0          # Initialize lists to store training and validation metrics     train_losses, val_losses, val_accuracies = [], [], []          print(\"--- Training Started ---\")          # Loop over the specified number of epochs     for epoch in range(num_epochs):         # Perform one epoch of training         epoch_loss = train_epoch(model, train_loader, loss_function, optimizer, device)         train_losses.append(epoch_loss)                  # Perform one epoch of validation         epoch_val_loss, epoch_accuracy = validate_epoch(model, val_loader, loss_function, device)         val_losses.append(epoch_val_loss)         val_accuracies.append(epoch_accuracy)                  # Print the metrics for the current epoch         print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")                  # Check if the current model is the best one so far         if epoch_accuracy &gt; best_val_accuracy:             best_val_accuracy = epoch_accuracy             best_epoch = epoch + 1             # Save the state of the best model in memory             best_model_state = copy.deepcopy(model.state_dict())                  print(\"--- Finished Training ---\")          # Load the best model weights before returning     if best_model_state:         print(f\"\\n--- Returning best model with {best_val_accuracy:.2f}% validation accuracy, achieved at epoch {best_epoch} ---\")         model.load_state_dict(best_model_state)          # Consolidate all metrics into a single list     metrics = [train_losses, val_losses, val_accuracies]          # Return the trained model and the collected metrics     return model, metrics <p>Everything is now in place. The following code will call your <code>training_loop</code> function to kick off the full training and validation process.</p> <p>The model will train for 50 epochs. With the powerful regularization techniques you have added (Batch Normalization, increased Dropout, and Weight Decay) and a smaller learning rate, the model is designed to learn more cautiously. This longer training run gives the model sufficient time to converge to a robust, generalized solution.</p> In\u00a0[\u00a0]: Copied! <pre># Start the training process by calling the training loop function\ntrained_model, training_metrics = training_loop(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    loss_function=loss_function, \n    optimizer=optimizer, \n    num_epochs=50, \n    device=device\n)\n\n# Visualize the training metrics (loss and accuracy)\nprint(\"\\n--- Training Plots ---\\n\")\nhelper_utils.plot_training_metrics(training_metrics)\n</pre> # Start the training process by calling the training loop function trained_model, training_metrics = training_loop(     model=model,      train_loader=train_loader,      val_loader=val_loader,      loss_function=loss_function,      optimizer=optimizer,      num_epochs=50,      device=device )  # Visualize the training metrics (loss and accuracy) print(\"\\n--- Training Plots ---\\n\") helper_utils.plot_training_metrics(training_metrics) <p>Analyzing the Results</p> <p>Take a close look at the new training plots and compare them to the ones from the previous lab. The difference is remarkable.</p> <p>The training and validation loss curves now follow each other very closely, and the wide gap that signaled overfitting is gone. The validation accuracy shows a much healthier, more consistent climb. This is clear evidence that you have successfully solved the overfitting problem! The combination of more data augmentation, Batch Normalization, and Weight Decay worked together to create a model that generalizes far better than before.</p> <p>The Performance Plateau</p> <p>Your model's validation accuracy now peaks somewhere around 70%, which is a solid result. You might wonder, however, why it did not achieve 90% or higher, especially with all these advanced techniques and longer training. The answer lies in how effectively you have used the tools available to you.</p> <p>The fundamentals you have learned in this course provide a solid foundation for building deep learning models. The techniques now at your disposal, from data augmentation to modular design and regularization, are powerful. Applying them correctly is precisely what allowed you to solve the initial overfitting problem and achieve this strong result. This demonstrates that you are pushing the limits of what can be accomplished with this foundational toolkit.</p> <p>You have achieved something significant. You started by building a simple CNN that suffered from a common and challenging problem, and you systematically upgraded your entire pipeline with professional techniques to create this final, robust model. Congratulations on a successful result!</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Import the preview function that demonstrates concepts from the next course\nfrom c2_preview.c2_preview import course_2_preview\n\n# This helper function runs a training loop using a powerful strategy that will be taught\n# in the next course. Run this cell to see the improved results in action.\ntrained_model = course_2_preview(\n    train_dataset, \n    val_dataset, \n    loss_function,\n    device,\n    num_epochs=5\n    )\n</pre> # Import the preview function that demonstrates concepts from the next course from c2_preview.c2_preview import course_2_preview  # This helper function runs a training loop using a powerful strategy that will be taught # in the next course. Run this cell to see the improved results in action. trained_model = course_2_preview(     train_dataset,      val_dataset,      loss_function,     device,     num_epochs=5     ) <p>Incredible, right? In just 5 epochs, the validation accuracy soared past 80%, a level of performance your previous model did not reach even after 50 epochs.</p> <p>How is such a rapid and dramatic improvement possible on the exact same data?</p> <p>This result was achieved by combining several powerful, next-level techniques that you will master in the upcoming course. This was just a preview, but the strategy involved three key upgrades:</p> <ul> <li><p>Using a Pre-trained Model: This is the most significant change. Instead of starting from scratch with random weights, this approach uses a sophisticated model that has already been trained on millions of images. It already possesses a deep understanding of visual patterns, which you can then fine-tune for your specific task.</p> </li> <li><p>Dynamic Learning Rate Scheduling: Rather than using a single, fixed learning rate, this strategy uses a learning rate scheduler. This tool intelligently adjusts the learning rate during training, making larger updates at the beginning and smaller, more precise adjustments as the model gets closer to the best solution.</p> </li> <li><p>More Advanced Transformations: The data augmentation pipeline used for this preview was also more advanced. It included techniques tailored specifically for these high performance models, ensuring the network learned from a richer and more challenging set of training examples.</p> </li> </ul> <p>These concepts are just a glimpse of what comes next. You have built an incredible foundation, and now you are ready to learn the strategies that professionals use to achieve state of the art results quickly and efficiently.</p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#programming-assignment-overcoming-overfitting-building-a-robust-cnn","title":"Programming Assignment - Overcoming Overfitting: Building a Robust CNN\u00b6","text":"<p>Welcome to the final assignment of this course! You have built a solid foundation in PyTorch, moving from basic tensors to a complete, working Convolutional Neural Network in a previous lab. That was an essential first step. Now, it is time to take the next step and tackle a challenge that every deep learning practitioner faces: to take a promising but flawed model and elevate it.</p> <p>Your previous model showed clear signs of overfitting, a common hurdle where a network memorizes training data instead of learning to generalize. This assignment is your mission to solve that problem, not just by tweaking a parameter, but by systematically re-engineering your entire machine learning pipeline with a suite of professional tools and techniques.</p> <p>To accomplish this, you will deploy a multi-faceted strategy, upgrading every component of your setup:</p> <ul> <li><p>Enhance the Data Pipeline with more powerful data augmentation to create a richer training set.</p> </li> <li><p>Refactor the Architecture for Modularity, creating reusable <code>CNNBlocks</code> for cleaner, more scalable code.</p> </li> <li><p>Integrate Advanced Layers like Batch Normalization to stabilize training and improve generalization.</p> </li> <li><p>Deploy a Robust Regularization Strategy using Dropout and Weight Decay to combat overfitting directly.</p> </li> </ul> <p>Let's get started and elevate your model to the next level!</p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#table-of-contents","title":"Table of Contents\u00b6","text":"<ul> <li>Imports</li> <li>1 - Upgrading Your Data Pipeline<ul> <li>1.1 - Defining More Powerful Transformations<ul> <li>Exercise 1 - define_transformations</li> </ul> </li> <li>1.2 - Assembling the Data Loaders</li> <li>1.3 - Visualizing the Training Images</li> </ul> </li> <li>2 - Building a Modular and Robust CNN<ul> <li>2.1 - The Power of Modularity: The CNNBlock<ul> <li>2.1.1 - BatchNorm2d Layer<ul> <li>Exercise 2 - CNNBlock</li> </ul> </li> </ul> </li> <li>2.2 - Assembling the Full CNN with Modular Blocks<ul> <li>Exercise 3 - SimpleCNN</li> </ul> </li> </ul> </li> <li>3 - Training the Upgraded Model<ul> <li>3.1 - Configuring the Loss and Optimizer</li> <li>3.2 - Implementing the Training and Validation Logic<ul> <li>Exercise 4 - train_epoch</li> <li>Exercise 5 - validate_epoch</li> </ul> </li> </ul> </li> <li>4 - Beyond the Foundations: A Glimpse into the Next Level</li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#imports","title":"Imports\u00b6","text":""},{"location":"tools/pytorch/Examples/C1M4_Assignment/#1-upgrading-your-data-pipeline","title":"1 - Upgrading Your Data Pipeline\u00b6","text":"<p>In the first lab of this module, you built a powerful CNN classifier from scratch. While it worked, you also encountered a classic machine learning hurdle: overfitting. Your model started to memorize the training data instead of learning to generalize, a common issue when a model's performance on validation data stalls or degrades.</p> <p>Your training results from that lab likely produced a plot similar to the one below. It perfectly illustrates this challenge, showing the telltale signs of overfitting. Look closely at the widening gap between the training loss, which continues to improve, and the validation loss, which stagnates or even worsens. This divergence, along with the validation accuracy hitting a plateau, is the classic evidence of a model that is memorizing the training data instead of truly learning how to generalize.</p> <p></p> <p>You will now tackle this challenge head on. The goal is twofold: first, to resolve the overfitting, and second, to push your model's performance to new heights. Before you can enhance the model architecture, however, you must first enhance the data it learns from.</p> <p>A fundamental strategy for building more robust models is data augmentation. By creating modified versions of your training images, flipping them, rotating them, you teach your model to recognize subjects in a variety of conditions. This technique is a pivotal first line of defense against overfitting. Your first task is to build an even more powerful set of image transformations to supercharge your dataset.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#11-defining-more-powerful-transformations","title":"1.1 - Defining More Powerful Transformations\u00b6","text":"<p>Let's begin by setting up the essential components for your data pipeline. You will start by defining the standard normalization values for the CIFAR-100 dataset and then create the transformation pipelines themselves.</p> <ul> <li>Define the <code>cifar100_mean</code> and <code>cifar100_std</code>, the mean and standard deviation values for the CIFAR-100 dataset.</li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#exercise-1-define_transformations","title":"Exercise 1 - define_transformations\u00b6","text":"<p>Your task is to define two distinct image transformation pipelines using <code>torchvision.transforms</code>.</p> <p>Your Task:</p> <ul> <li>For <code>train_transformations</code>: Create a composition of transforms for the training dataset.</li> </ul> <pre><code>* This pipeline should include random [horizontal](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html) and [vertical](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomVerticalFlip.html) flips.\n* It should also randomly [rotate](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html) the images by up to **15 degrees**.\n* Finally, it must convert images to PyTorch [tensors](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) and [normalize](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html) them using the provided `mean` and `std`.</code></pre> <ul> <li>For <code>val_transformations</code>: Create a second, simpler pipeline for the validation dataset.</li> </ul> <pre><code>* This pipeline should only perform the two essential steps: converting images to [tensors](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) and [normalizing](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html) them with the same `mean` and `std`.</code></pre> Additional Code Hints (Click to expand if you are stuck) <p>If you're stuck, here is a more detailed breakdown.</p> <p>You will use <code>transforms.Compose([...])</code> to create a list of transformations for both pipelines. All the required functions are part of the <code>transforms</code> module.</p> <p>For <code>train_transformations</code>:</p> <ul> <li><p>You need to create a list of five transformation objects inside <code>transforms.Compose</code>.</p> </li> <li><p>The first one is for horizontal flips. The call looks like this: <code>transforms.RandomHorizontalFlip()</code>.</p> </li> <li><p>The next two for vertical flips and rotations follow a similar pattern. Remember to pass <code>15</code> as the argument for the rotation.</p> </li> <li><p>The last two transformations are:</p> <ul> <li><p><code>call the ToTensor method from the transforms module</code></p> </li> <li><p><code>call the Normalize method from the transforms module, passing it the mean and std variables</code></p> </li> </ul> </li> </ul> <p>For <code>val_transformations</code>:</p> <ul> <li><p>This pipeline is much simpler and only contains the last two steps from the training pipeline.</p> </li> <li><p>Your list inside <code>transforms.Compose</code> should contain just two items:</p> <ul> <li><p><code>first, the transformation to convert an image to a tensor</code></p> </li> <li><p><code>second, the transformation to normalize the tensor using the given mean and std</code></p> </li> </ul> </li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#expected-output","title":"Expected Output:\u00b6","text":"<pre><code>Training Transformations:\nCompose(\n    RandomHorizontalFlip(p=0.5)\n    RandomVerticalFlip(p=0.5)\n    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n    ToTensor()\n    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n)\n------------------------------\n\nValidation Transformations:\nCompose(\n    ToTensor()\n    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n)\n</code></pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#12-assembling-the-data-loaders","title":"1.2 - Assembling the Data Loaders\u00b6","text":"<p>With your powerful new transformation pipelines defined, it is time to prepare the data for training. You will first specify the 15 target classes and then use your transformations to load the images and wrap them in <code>DataLoader</code> objects, which will feed the data to your model in batches.</p> <ul> <li>First, define the <code>all_target_classes</code> list.</li> <li>These are the same classes of flowers, mammals, and insects you worked with in the previous lab, ensuring you are tackling the same classification problem, but with an upgraded pipeline.</li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#13-visualizing-the-training-images","title":"1.3 - Visualizing the Training Images\u00b6","text":"<p>It is always a good practice to visualize your data. The following line calls a helper function to display a grid of random images from your <code>train_loader</code>.</p> <p>Pay close attention to the output. Since these images come from the training set, you should see the effects of your data augmentation pipeline in action. Look for images that have been randomly flipped horizontally, vertically, or rotated. This is an excellent way to confirm your transformations are working as expected.</p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#2-building-a-modular-and-robust-cnn","title":"2 - Building a Modular and Robust CNN\u00b6","text":"<p>With a more robust data pipeline in place, your next step is to enhance the model's architecture itself. You will refactor the original CNN to be more modular, efficient, and powerful. This is the next pivotal step toward resolving the overfitting problem and pushing your model's performance to new heights.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#21-the-power-of-modularity-the-cnnblock","title":"2.1 - The Power of Modularity: The CNNBlock\u00b6","text":"<p>In the previous lab, your model's architecture had a repeating pattern of convolution, activation, and pooling layers. Defining these layers individually can become repetitive and makes the model harder to modify. A much better approach is to group these patterns into a single, reusable module. Your first task is to create a <code>CNNBlock</code> that packages these layers together. This modular design makes your main model's code significantly cleaner and easier to manage.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#211-batchnorm2d-layer","title":"2.1.1 - <code>BatchNorm2d Layer</code>\u00b6","text":"<p>As part of this new, improved block, you will also introduce a powerful new layer: <code>BatchNorm2d</code>. This layer is a pivotal technique for building modern, high performing deep neural networks.</p> <p>Think of Batch Normalization as a traffic controller for the data flowing between your network's layers. After a convolutional layer processes a batch of images, the outputs (or activations) can have widely varying distributions from one batch to the next. <code>BatchNorm2d</code> steps in and normalizes these activations within each mini batch, adjusting them to have a consistent mean and standard deviation. It then uses two learnable parameters to scale and shift this normalized output, allowing the network itself to learn the optimal distribution for the data at that point.</p> <p>This seemingly simple step provides three profound benefits:</p> <ul> <li><p>It Stabilizes and Accelerates Training: By keeping the distribution of data consistent between layers, it prevents later layers from having to constantly adapt to a shifting input from the layers before them. This stability allows you to use higher learning rates, which can dramatically speed up how quickly your model learns.</p> </li> <li><p>It Acts as a Regularizer: Because the normalization statistics are calculated for each unique mini batch, it introduces a slight amount of noise into the training process. This noise makes it harder for the model to perfectly memorize the training data, encouraging it to learn more general features and thus reducing overfitting.</p> </li> <li><p>It Reduces Sensitivity to Initialization: The layer makes your model less dependent on the specific random weights it starts with, leading to more reliable and repeatable training results.</p> </li> </ul> <p>By adding <code>BatchNorm2d</code> to your <code>CNNBlock</code>, you are not just adding another layer; you are fundamentally making your model's training process more stable, efficient, and robust.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#exercise-2-cnnblock","title":"Exercise 2 - CNNBlock\u00b6","text":"<p>You will now implement the <code>CNNBlock</code> class. This class will package the four layers into a single <code>nn.Sequential</code> module.</p> <p>Your Task:</p> <p>Inside the <code>__init__</code> method:</p> <ul> <li>You need to define a sequential container named <code>self.block</code>.</li> <li>Inside this <code>nn.Sequential</code> container, you will add the following layers in order:<ol> <li>A <code>nn.Conv2d</code> layer. Use the <code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code>, and <code>padding</code> arguments that are passed to the <code>__init__</code> method.</li> <li>A <code>nn.BatchNorm2d</code> layer. This layer needs to know the number of channels of its input, which is the output of the previous convolutional layer.</li> <li>A <code>nn.ReLU</code> activation function.</li> <li>A <code>nn.MaxPool2d</code> layer. This will downsample the feature map. You should set both the <code>kernel_size</code> and <code>stride</code> to <code>2</code>.</li> </ol> </li> </ul> <p>Inside the <code>forward</code> method:</p> <ul> <li>This method performs the forward pass.</li> <li>Pass the input tensor <code>x</code> through the <code>self.block</code> you defined and return the result.</li> </ul> Additional Code Hints (Click to expand if you are stuck) <p>If you are looking for more guidance, here is a detailed breakdown.</p> <p>For the <code>__init__</code> method:</p> <ul> <li><p>You are defining a sequence of layers. The entire sequence will be assigned to <code>self.block</code>. The structure starts like this: <code>self.block = nn.Sequential(...)</code>.</p> </li> <li><p>The layers are provided as arguments to <code>nn.Sequential</code>, separated by commas.</p> </li> <li><p>1. Convolutional Layer: The first layer is <code>nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)</code>. Notice how it uses the parameters from the <code>__init__</code> method's signature.</p> </li> <li><p>2. Batch Norm Layer: The second layer is <code>nn.BatchNorm2d(...)</code>. It needs one argument: the number of channels it will normalize. This is equal to the number of output channels from the previous layer, which is <code>out_channels</code>.</p> </li> <li><p>3. ReLU Layer: The third layer is simply <code>nn.ReLU()</code>. It does not require any arguments.</p> </li> <li><p>4. Max Pooling Layer: The final layer is <code>nn.MaxPool2d(...)</code>. You need to provide the <code>kernel_size</code> and <code>stride</code>. The call will look like: <code>nn.MaxPool2d(kernel_size=2, stride=2)</code>.</p> </li> </ul> <p>For the <code>forward</code> method:</p> <ul> <li>This is a single line of code. You simply need to call the module you created in the <code>__init__</code> method on the input tensor.</li> <li>The pseudocode would be: <code>return the result of applying self.block to the input x</code>.</li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#expected-output","title":"Expected Output:\u00b6","text":"<pre><code>Block Structure:\n\nCNNBlock(\n  (block): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n)\n\nInput tensor shape:  torch.Size([1, 3, 32, 32])\nOutput tensor shape: torch.Size([1, 16, 16, 16])\n</code></pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#22-assembling-the-full-cnn-with-modular-blocks","title":"2.2 - Assembling the Full CNN with Modular Blocks\u00b6","text":"<p>Now that you have a reusable <code>CNNBlock</code>, you can assemble your full <code>SimpleCNN</code> architecture. By using your new modular block, you will see how much cleaner and more professional your model definition becomes. Instead of defining many individual layers for the convolutional part of your network, you will now define just three <code>CNNBlock</code> instances.</p> <p>Your model will consist of two main parts:</p> <ul> <li>A feature extractor: A sequence of three CNNBlocks that will learn to identify visual patterns in the images.</li> <li>A classifier: A sequence of fully connected layers that will take the features from the convolutional blocks and make the final prediction.</li> </ul> <p>In this new version, you will also increase the dropout rate to <code>0.6</code>. This is another important step in your fight against overfitting, as it makes the model less likely to rely on any single feature.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#exercise-3-simplecnn","title":"Exercise 3 - SimpleCNN\u00b6","text":"<p>You will now implement the <code>__init__</code> and <code>forward</code> methods for the <code>SimpleCNN</code> class. You will use the <code>CNNBlock</code> you just built as the primary component of the network's body.</p> <p>Your Task:</p> <p>Inside the <code>__init__</code> method:</p> <ul> <li><p>Feature Extractor:</p> <ul> <li>Instantiate three <code>CNNBlock</code> layers (<code>conv_block1</code>, <code>conv_block2</code>, <code>conv_block3</code>).</li> <li>The first block should take an input with 3 channels (for RGB images) and produce 32 output channels.</li> <li>For the subsequent blocks, the number of input channels must match the number of output channels from the previous block. You will double the number of channels at each step <code>(3 -&gt; 32 -&gt; 64 -&gt; 128)</code>.</li> </ul> </li> </ul> <ul> <li><p>Classifier:</p> <ul> <li>Define a <code>self.classifier</code> using an <code>nn.Sequential</code> container.</li> <li>This container should have the following layers in order:<ol> <li>An <code>nn.Flatten</code> layer to transform the 2D feature map into a 1D vector.</li> <li>An <code>nn.Linear</code> layer. You must calculate the correct number of input features. This depends on the output shape of the last <code>CNNBlock</code>. The output size of this layer should be 512.</li> <li>An <code>nn.ReLU</code> activation.</li> <li>An <code>nn.Dropout</code> layer with a rate of <code>0.6</code> to help prevent overfitting.</li> <li>A final <code>nn.Linear</code> layer that maps the 512 features to the number of output classes.</li> </ol> </li> </ul> </li> </ul> <p>Inside the <code>forward</code> method:</p> <ul> <li>Define the data flow through the network.</li> <li>Pass the input <code>x</code> sequentially through <code>conv_block1</code>, then <code>conv_block2</code>, and then <code>conv_block3</code>.</li> <li>Finally, pass the output of the last convolutional block through your <code>classifier</code>.</li> <li>Return the final output.</li> </ul> Additional Code Hints (Click to expand if you are stuck) <p>If you are stuck, here is a more detailed breakdown for the implementation.</p> <p>For the <code>__init__</code> method:</p> <ul> <li>Convolutional Blocks:</li> </ul> <pre><code>* The first block is a straightforward instantiation: `self.conv_block1 = CNNBlock(in_channels=3, out_channels=32)`.\n* For the second block, the `in_channels` must be `32` (the `out_channels` of the first). The `out_channels` will be `64`. Follow this pattern for the third block.</code></pre> <ul> <li><p>Classifier:</p> <ul> <li>Start by defining the sequential container: <code>self.classifier = nn.Sequential(...)</code> 1. Flatten Layer: The first layer is <code>nn.Flatten()</code>. It takes no arguments. 2. First Linear Layer: This is <code>nn.Linear(in_features=..., out_features=512)</code>.<ul> <li>To find the <code>in_features</code>, you need to calculate the size of the flattened tensor. The input images are 32x32. Each <code>CNNBlock</code> contains a <code>MaxPool2d</code> layer with a stride of 2, which halves the height and width. After three blocks, the dimensions will be <code>32 \u2192 16 \u2192 8 \u2192 4</code>.</li> <li>The last <code>CNNBlock</code> outputs 128 channels. So, the total number of features is <code>128 * 4 * 4</code>. 3. ReLU Layer: Add <code>nn.ReLU()</code>. 4. Dropout Layer: Add <code>nn.Dropout(0.6)</code>. 5. Final Linear Layer: This is <code>nn.Linear(in_features=512, out_features=num_classes)</code>.</li> </ul> </li> </ul> </li> </ul> <p>For the <code>forward</code> method:</p> <ul> <li>This method describes how data flows from input to output. You can use the same variable <code>x</code> and reassign it after each step.</li> <li>The pseudocode for the sequence is:<ul> <li><code>x = pass the input x through self.conv_block1</code></li> <li><code>x = pass the new x through self.conv_block2</code></li> <li><code>x = pass the new x through self.conv_block3</code></li> <li><code>x = pass the final feature map through self.classifier</code></li> </ul> </li> <li>Finally, return <code>x</code>.</li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#expected-output","title":"Expected Output:\u00b6","text":"<pre><code>Model Structure:\n\nSimpleCNN(\n  (conv_block1): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (conv_block2): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (conv_block3): CNNBlock(\n    (block): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2048, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.6, inplace=False)\n    (4): Linear(in_features=512, out_features=15, bias=True)\n  )\n)\n\nInput tensor shape:  torch.Size([64, 3, 32, 32])\nOutput tensor shape: torch.Size([64, 15])\n</code></pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#3-training-the-upgraded-model","title":"3 - Training the Upgraded Model\u00b6","text":"<p>With your upgraded data pipeline and modular CNN architecture complete, you are ready to begin the training process. In this section, you will configure the final pieces of your training pipeline: the loss function and the optimizer. Then, you will implement the core training and validation logic that will run your experiment and reveal how well your new model performs.</p> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#31-configuring-the-loss-and-optimizer","title":"3.1 - Configuring the Loss and Optimizer\u00b6","text":"<p>Before you can train the model, you must define two key components: a loss function to measure error and an optimizer to update the model's weights.</p> <ul> <li>For the loss function, you will continue to use <code>nn.CrossEntropyLoss</code>, the standard choice for multi-class classification.</li> </ul> <ul> <li>For the optimizer, you will use <code>Adam</code>, but with an important addition to combat overfitting: <code>weight_decay</code>.<ul> <li>Weight decay adds a penalty to the loss function based on the magnitude of the model's weights. It encourages the network to learn smaller, simpler weight values, which makes it more robust and less likely to memorize the training data. This is another vital tool for improving your model's ability to generalize.</li> </ul> </li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#32-implementing-the-training-and-validation-logic","title":"3.2 - Implementing the Training and Validation Logic\u00b6","text":"<p>You will now implement the core logic for training and evaluating your model. This will be done in two separate functions:</p> <ul> <li><code>train_epoch</code>: To perform a single pass over the training data to update the model.</li> <li><code>validate_epoch</code>: To perform a single pass over the validation data to measure performance.</li> </ul> <p></p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#exercise-4-train_epoch","title":"Exercise 4 -  train_epoch\u00b6","text":"<p>Your task is to complete the core training logic within the <code>for</code> loop of the <code>train_epoch</code> function. You will implement the five fundamental steps of a single training iteration.</p> <p>Your Task:</p> <p>Inside the <code>train_epoch</code> function, for each batch of <code>images</code> and <code>labels</code>:</p> <ul> <li>Clear Gradients:<ul> <li>Before computing the gradients for the current batch, you must clear any gradients that were stored from the previous batch.</li> </ul> </li> <li>Forward Pass:<ul> <li>Feed the <code>images</code> through the <code>model</code> to get the output predictions.</li> </ul> </li> <li>Calculate Loss:<ul> <li>Use the provided <code>loss_function</code> to measure the difference between the model's <code>outputs</code> and the true <code>labels</code>.</li> </ul> </li> <li>Backward Pass:<ul> <li>Compute the gradients of the loss with respect to all the model's parameters. This is also known as backpropagation.</li> </ul> </li> <li>Update Parameters:<ul> <li>Use the <code>optimizer</code> to adjust the model's parameters based on the gradients you just computed.</li> </ul> </li> </ul> Additional Code Hints (Click to expand if you are stuck) <p>If you need some help, here is a more direct guide for each step.</p> <ul> <li>Clear Gradients: This is done to prevent the accumulation of gradients across batches.<ul> <li>The pseudocode is: <code>call the zero_grad() method on the optimizer</code>.</li> </ul> </li> </ul> <ul> <li>Forward Pass: This is how you get the model's predictions for the current batch.<ul> <li>The pseudocode is: <code>outputs = call the model, passing the images as the argument</code>.</li> </ul> </li> </ul> <ul> <li>Calculate Loss: You compare the model's predictions with the actual ground truth labels.<ul> <li>The pseudocode is: <code>loss = call the loss_function, passing the outputs and labels as arguments</code>.</li> </ul> </li> </ul> <ul> <li>Backward Pass: This step calculates how much each model parameter contributed to the overall loss.<ul> <li>The pseudocode is: <code>call the backward() method on the loss tensor</code>.</li> </ul> </li> </ul> <ul> <li>Update Parameters: The optimizer uses the calculated gradients to take a small step in the direction that minimizes the loss.<ul> <li>The pseudocode is: <code>call the step() method on the optimizer</code>.</li> </ul> </li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#expected-output-approximately","title":"Expected Output (Approximately):\u00b6","text":"<pre><code>Training on 640 images for 5 epochs:\n\nEpoch [1/5], Loss: 2.6735\nEpoch [2/5], Loss: 2.3238\nEpoch [3/5], Loss: 2.0528\nEpoch [4/5], Loss: 1.8341\nEpoch [5/5], Loss: 1.7676\n\nWeight Update Check:\tModel weights changed during training.\nLoss Trend Check:\tLoss decreased from 2.6735 to 1.7676.\n</code></pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#exercise-5-validate_epoch","title":"Exercise 5 - validate_epoch\u00b6","text":"<p>Your task is to complete the validation logic. This involves performing a forward pass and then calculating both the loss and the number of correct predictions to determine the accuracy.</p> <p>Your Task:</p> <ul> <li>Disable Gradient Calculation:</li> </ul> <pre><code>* Wrap the entire for loop within the `torch.no_grad()` context manager. This tells PyTorch not to compute gradients, which saves memory and computation time during validation.</code></pre> <ul> <li>Inside the <code>for</code> loop:</li> </ul> <pre><code>* **Forward Pass**: \n    * Just like in training, pass the `images` through the `model` to get its `outputs`.        \n* **Calculate Loss**: \n    * Use the `loss_function` to compute the `val_loss` between the `outputs` and the true `labels`.\n* **Accumulate Loss**: \n    * Add the batch's loss to the `running_val_loss`. Remember to get the scalar value from the loss tensor and scale it by the batch size.\n* **Get Predictions**: \n    * Determine the model's predicted class for each image in the batch. The `outputs` from your model are raw scores (logits). The class with the highest score is the model's prediction. You need to find the index of this maximum score.</code></pre> Additional Code Hints (Click to expand if you are stuck) <p>If you need a bit more direction, here is a detailed guide.</p> <p>Disable Gradient Calculation:</p> <p>This is a context manager in PyTorch. The structure you need is <code>with torch.no_grad()</code>:. The for loop should be indented inside this block.</p> <p>Inside the for loop:</p> <ul> <li><p>Forward Pass: This is identical to the training loop. The pseudocode is: <code>outputs = call the model, passing the images as the argument</code>.</p> </li> <li><p>Calculate Loss: This is also the same as in the training loop. The pseudocode is: <code>val_loss = call the loss_function, passing the outputs and labels as arguments</code>.</p> </li> <li><p>Accumulate Loss: You need to update the <code>running_val_loss</code>. The pseudocode is: <code>running_val_loss += get the scalar value of val_loss using the .item() method * the number of images in the current batch</code>.</p> </li> <li><p>Get Predictions: You need to find the most likely class from the output logits.</p> <ul> <li><p>The <code>torch.max()</code> function is perfect for this. You need to call it on the <code>outputs</code> tensor along dimension 1 (the class dimension).</p> </li> <li><p>The pseudocode is: <code>_, predicted = use torch.max() on the outputs tensor, specifying dimension 1</code>.</p> </li> <li><p>Note that <code>torch.max()</code> returns a tuple of (max_values, max_indices). You only need the second element, the indices, which correspond to the predicted class labels.</p> </li> </ul> </li> </ul>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#expected-output","title":"Expected Output:\u00b6","text":"<pre><code>Return Types Check:\tFunction returned a float for loss and accuracy.\nWeight Integrity Check:\tModel weights were not changed during validation.\n</code></pre>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#submission-note","title":"Submission Note\u00b6","text":"<p>Congratulations! You've completed the final graded exercise of this assignment.</p> <p>If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to submit your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.</p> <p>\ud83d\udea8 IMPORTANT NOTE If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:</p> <p>Grader Error: Grader feedback not found</p> <p>Autograder failed to produce the feedback...</p> <p>This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once.</p> <p>If the error persists, please reach out for support in the DeepLearning.AI Community Forum.</p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#4-beyond-the-foundations-a-glimpse-into-the-next-level","title":"4 - Beyond the Foundations: A Glimpse into the Next Level\u00b6","text":"<p>You have successfully taken a simple CNN, diagnosed its flaws, and systematically upgraded it into a robust, well-generalized model. You have pushed the foundational toolkit you've learned to its limits to achieve a strong result.</p> <p>But what if this isn't the limit? What if there was another way?</p> <p>What if you could take your model's accuracy from around 70% to over 80% on this exact same dataset?</p> <p>Take a look at the results from a different, more powerful training strategy. Run the next cell to see that in action.</p>"},{"location":"tools/pytorch/Examples/C1M4_Assignment/#conclusion","title":"Conclusion\u00b6","text":"<p>Congratulations on completing this assignment!</p> <p>You have successfully navigated a complete and realistic machine learning workflow. You began with a model that suffered from overfitting, diagnosed the problem, and then systematically applied a series of powerful, professional techniques to solve it. You have not just improved a model; you have learned a repeatable process for refining and strengthening any neural network you build in the future.</p> <p>The skills you practiced here, modular design, implementing regularization, and analyzing training dynamics, are fundamental to building effective deep learning models. You have moved beyond the basics and are now equipped with the practical knowledge needed to tackle more complex, real world problems. Well done!</p>"},{"location":"tools/pytorch/Examples/C1_M1_Lab_1_simple_nn/","title":"Building a Simple Neural Network","text":"In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# This line ensures that your results are reproducible and consistent every time.\ntorch.manual_seed(42)\n</pre> import torch import torch.nn as nn import torch.optim as optim  # This line ensures that your results are reproducible and consistent every time. torch.manual_seed(42) Out[2]: <pre>&lt;torch._C.Generator at 0x1cf2d584370&gt;</pre> In\u00a0[3]: Copied! <pre># Distances in miles for recent bike deliveries\ndistances = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n\n# Corresponding delivery times in minutes\ntimes = torch.tensor([[6.96], [12.11], [16.77], [22.21]], dtype=torch.float32)\n</pre> # Distances in miles for recent bike deliveries distances = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)  # Corresponding delivery times in minutes times = torch.tensor([[6.96], [12.11], [16.77], [22.21]], dtype=torch.float32) In\u00a0[5]: Copied! <pre>model = nn.Sequential(nn.Linear(1, 1))\n</pre> model = nn.Sequential(nn.Linear(1, 1)) In\u00a0[6]: Copied! <pre># Define the loss function and optimizer\nloss_function = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n</pre> # Define the loss function and optimizer loss_function = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) In\u00a0[7]: Copied! <pre># Training loop\nfor epoch in range(500):\n    # Reset the optimizer's gradients\n    optimizer.zero_grad()\n    # Make predictions (forward pass)\n    outputs = model(distances)\n    # Calculate the loss\n    loss = loss_function(outputs, times)\n    # Calculate adjustments (backward pass)\n    loss.backward()\n    # Update the model's parameters\n    optimizer.step()\n    # Print loss every 50 epochs\n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n</pre> # Training loop for epoch in range(500):     # Reset the optimizer's gradients     optimizer.zero_grad()     # Make predictions (forward pass)     outputs = model(distances)     # Calculate the loss     loss = loss_function(outputs, times)     # Calculate adjustments (backward pass)     loss.backward()     # Update the model's parameters     optimizer.step()     # Print loss every 50 epochs     if (epoch + 1) % 50 == 0:         print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\") <pre>Epoch 50: Loss = 0.03944866359233856\nEpoch 100: Loss = 0.03581171855330467\nEpoch 150: Loss = 0.03311903774738312\nEpoch 200: Loss = 0.03112369030714035\nEpoch 250: Loss = 0.02964562550187111\nEpoch 300: Loss = 0.02855011820793152\nEpoch 350: Loss = 0.0277385413646698\nEpoch 400: Loss = 0.027137158438563347\nEpoch 450: Loss = 0.026691768318414688\nEpoch 500: Loss = 0.026361485943198204\n</pre> In\u00a0[8]: Copied! <pre>distance_to_predict = 7.0\n</pre> distance_to_predict = 7.0 In\u00a0[9]: Copied! <pre># Use the torch.no_grad() context manager for efficient predictions\nwith torch.no_grad():\n    # Convert the Python variable into a 2D PyTorch tensor that the model expects\n    new_distance = torch.tensor([[distance_to_predict]], dtype=torch.float32)\n    \n    # Pass the new data to the trained model to get a prediction\n    predicted_time = model(new_distance)\n    \n    # Use .item() to extract the scalar value from the tensor for printing\n    print(f\"Prediction for a {distance_to_predict}-mile delivery: {predicted_time.item():.1f} minutes\")\n\n    # Use the scalar value in a conditional statement to make the final decision\n    if predicted_time.item() &gt; 30:\n        print(\"\\nDecision: Do NOT take the job. You will likely be late.\")\n    else:\n        print(\"\\nDecision: Take the job. You can make it!\")\n</pre> # Use the torch.no_grad() context manager for efficient predictions with torch.no_grad():     # Convert the Python variable into a 2D PyTorch tensor that the model expects     new_distance = torch.tensor([[distance_to_predict]], dtype=torch.float32)          # Pass the new data to the trained model to get a prediction     predicted_time = model(new_distance)          # Use .item() to extract the scalar value from the tensor for printing     print(f\"Prediction for a {distance_to_predict}-mile delivery: {predicted_time.item():.1f} minutes\")      # Use the scalar value in a conditional statement to make the final decision     if predicted_time.item() &gt; 30:         print(\"\\nDecision: Do NOT take the job. You will likely be late.\")     else:         print(\"\\nDecision: Take the job. You can make it!\") <pre>Prediction for a 7.0-mile delivery: 37.1 minutes\n\nDecision: Do NOT take the job. You will likely be late.\n</pre> In\u00a0[10]: Copied! <pre># Access the first (and only) layer in the sequential model\nlayer = model[0]\n\n# Get weights and bias\nweights = layer.weight.data.numpy()\nbias = layer.bias.data.numpy()\n\nprint(f\"Weight: {weights}\")\nprint(f\"Bias: {bias}\")\n</pre> # Access the first (and only) layer in the sequential model layer = model[0]  # Get weights and bias weights = layer.weight.data.numpy() bias = layer.bias.data.numpy()  print(f\"Weight: {weights}\") print(f\"Bias: {bias}\") <pre>Weight: [[5.015503]]\nBias: [1.9849643]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tools/pytorch/Examples/C1_M1_Lab_1_simple_nn/#building-a-simple-neural-network","title":"Building a Simple Neural Network\u00b6","text":""},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/","title":"Data Management","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport tarfile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport scipy\nfrom PIL import Image\nfrom torch.utils.data import Dataset, Subset, random_split, DataLoader\nfrom torchvision import transforms\n\nimport helper_utils\n\n##\n</pre> import os import tarfile import matplotlib.pyplot as plt import numpy as np import requests import scipy from PIL import Image from torch.utils.data import Dataset, Subset, random_split, DataLoader from torchvision import transforms  import helper_utils  ## In\u00a0[2]: Copied! <pre>def download_dataset():\n    \"\"\"\n    Downloads and extracts a dataset from remote URLs if not already present locally.\n\n    This function first checks for the existence of the dataset files in a specific\n    directory. If the files are not found, it proceeds to download them from\n    pre-defined URLs, showing progress bars, and then extracts the contents.\n    \"\"\"\n    # Define the directory to store the dataset.\n    data_dir = \"flower_data\"\n    \n    # Define paths for key files and folders.\n    image_folder_path = os.path.join(data_dir, \"jpg\")\n    labels_file_path = os.path.join(data_dir, \"imagelabels.mat\")\n    tgz_path = os.path.join(data_dir, \"102flowers.tgz\")\n\n    # Check if the primary data folder and a key label file already exist.\n    if os.path.exists(image_folder_path) and os.path.exists(labels_file_path):\n        # Inform the user that the dataset is already available locally.\n        print(f\"Dataset already exists. Loading locally from '{data_dir}'.\")\n        # Exit the function since no download is needed.\n        return\n\n    # Inform the user that the dataset is not found and the download will start.\n    print(\"Dataset not found locally. Downloading...\")\n\n    # Define the URLs for the image archive and the labels file.\n    image_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n    labels_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n\n    # Create the target directory for the dataset, if it doesn't already exist.\n    os.makedirs(data_dir, exist_ok=True)\n\n    # Announce the start of the image download process.\n    print(\"Downloading images...\")\n    # Send an HTTP GET request to the image URL, enabling streaming for large files.\n    response = requests.get(image_url, stream=True)\n    # Get the total size of the file from the response headers for the progress bar.\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    # Open a local file in binary write mode to save the downloaded archive.\n    with open(tgz_path, \"wb\") as file:\n        # Iterate over the response content in chunks with a progress bar.\n        for data in tqdm(\n            # Define the chunk size for iterating over the content.\n            response.iter_content(chunk_size=1024),\n            # Set the total for the progress bar based on the file size in kilobytes.\n            total=total_size // 1024,\n        ):\n            # Write each chunk of data to the file.\n            file.write(data)\n\n    # Announce the start of the file extraction process.\n    print(\"Extracting files...\")\n    # Open the downloaded tar.gz archive in read mode.\n    with tarfile.open(tgz_path, \"r:gz\") as tar:\n        # Extract all contents of the archive into the target directory.\n        tar.extractall(data_dir)\n\n    # Announce the start of the labels download process.\n    print(\"Downloading labels...\")\n    # Send an HTTP GET request to the labels URL.\n    response = requests.get(labels_url)\n    # Open a local file in binary write mode to save the labels.\n    with open(labels_file_path, \"wb\") as file:\n        # Write the entire content of the response to the file.\n        file.write(response.content)\n\n    # Inform the user that the download and extraction are complete.\n    print(f\"Dataset downloaded and extracted to '{data_dir}'.\")\n</pre> def download_dataset():     \"\"\"     Downloads and extracts a dataset from remote URLs if not already present locally.      This function first checks for the existence of the dataset files in a specific     directory. If the files are not found, it proceeds to download them from     pre-defined URLs, showing progress bars, and then extracts the contents.     \"\"\"     # Define the directory to store the dataset.     data_dir = \"flower_data\"          # Define paths for key files and folders.     image_folder_path = os.path.join(data_dir, \"jpg\")     labels_file_path = os.path.join(data_dir, \"imagelabels.mat\")     tgz_path = os.path.join(data_dir, \"102flowers.tgz\")      # Check if the primary data folder and a key label file already exist.     if os.path.exists(image_folder_path) and os.path.exists(labels_file_path):         # Inform the user that the dataset is already available locally.         print(f\"Dataset already exists. Loading locally from '{data_dir}'.\")         # Exit the function since no download is needed.         return      # Inform the user that the dataset is not found and the download will start.     print(\"Dataset not found locally. Downloading...\")      # Define the URLs for the image archive and the labels file.     image_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"     labels_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"      # Create the target directory for the dataset, if it doesn't already exist.     os.makedirs(data_dir, exist_ok=True)      # Announce the start of the image download process.     print(\"Downloading images...\")     # Send an HTTP GET request to the image URL, enabling streaming for large files.     response = requests.get(image_url, stream=True)     # Get the total size of the file from the response headers for the progress bar.     total_size = int(response.headers.get(\"content-length\", 0))      # Open a local file in binary write mode to save the downloaded archive.     with open(tgz_path, \"wb\") as file:         # Iterate over the response content in chunks with a progress bar.         for data in tqdm(             # Define the chunk size for iterating over the content.             response.iter_content(chunk_size=1024),             # Set the total for the progress bar based on the file size in kilobytes.             total=total_size // 1024,         ):             # Write each chunk of data to the file.             file.write(data)      # Announce the start of the file extraction process.     print(\"Extracting files...\")     # Open the downloaded tar.gz archive in read mode.     with tarfile.open(tgz_path, \"r:gz\") as tar:         # Extract all contents of the archive into the target directory.         tar.extractall(data_dir)      # Announce the start of the labels download process.     print(\"Downloading labels...\")     # Send an HTTP GET request to the labels URL.     response = requests.get(labels_url)     # Open a local file in binary write mode to save the labels.     with open(labels_file_path, \"wb\") as file:         # Write the entire content of the response to the file.         file.write(response.content)      # Inform the user that the download and extraction are complete.     print(f\"Dataset downloaded and extracted to '{data_dir}'.\") In\u00a0[3]: Copied! <pre># Call the function to download and prepare the dataset.\ndownload_dataset()\n</pre> # Call the function to download and prepare the dataset. download_dataset() <pre>Dataset already exists. Loading locally from 'flower_data'.\n</pre> In\u00a0[4]: Copied! <pre># Define the path to the root directory of the dataset.\npath_dataset = './flower_data'\n\n# Display the folder structure of the dataset directory up to a depth of one.\nhelper_utils.print_data_folder_structure(path_dataset, max_depth=1)\n</pre> # Define the path to the root directory of the dataset. path_dataset = './flower_data'  # Display the folder structure of the dataset directory up to a depth of one. helper_utils.print_data_folder_structure(path_dataset, max_depth=1) <pre>flower_data/\n\u251c\u2500\u2500 imagelabels.mat\n\u251c\u2500\u2500 jpg/\n\u2514\u2500\u2500 labels_description.txt\n</pre> <p>You can see that the dataset consists of:</p> <ul> <li>A  <code>jpg</code> folder containing images in JPEG format</li> <li>An <code>imagelabels.mat</code> file that stores the labels (MATLAB format)</li> <li>A <code>labels_description.txt</code> file describing each label</li> </ul> <p>This gives you a clear sense of how the dataset is structured and what you\u2019ll be working with.</p> <p>Regarding lazy loading: The images are not loaded all at once when the dataset object is created. Instead, they are loaded on-the-fly when accessed via the <code>__getitem__</code> method. This approach is memory efficient, especially when dealing with large datasets, as it avoids loading all images into memory at once.</p> In\u00a0[\u00a0]: Copied! <pre>class FlowerDataset(Dataset):\n    \"\"\"\n    A custom dataset class for loading flower image data.\n\n    This class is designed to work with PyTorch's Dataset and DataLoader\n    abstractions. It handles loading images and their corresponding labels\n    from a specific directory structure.\n    \"\"\"\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Initializes the dataset object.\n\n        Args:\n            root_dir (str): The root directory where the dataset is stored.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        # Store the root directory path.\n        self.root_dir = root_dir\n        # Store the optional transformations.\n        self.transform = transform\n        # Construct the full path to the image directory.\n        self.image_dir = os.path.join(self.root_dir, \"jpg\")\n        # Load and process the labels from the corresponding file.\n        self.labels = self.load_and_correct_labels()\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n        \"\"\"\n        # The total number of samples is the number of labels.\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a sample from the dataset at the specified index.\n\n        Args:\n            idx (int): The index of the sample to retrieve.\n\n        Returns:\n            tuple: A tuple containing the image and its label.\n        \"\"\"\n        # Retrieve the image for the given index.\n        image = self.retrieve_image(idx)\n\n        # Check if a transform is provided.\n        if self.transform is not None:\n            # Apply the transform to the image.\n            image = self.transform(image)\n\n        # Get the label corresponding to the index.\n        label = self.labels[idx]\n\n        # Return the processed image and its label.\n        return image, label\n\n    def retrieve_image(self, idx):\n        \"\"\"\n        Loads a single image from disk based on its index.\n\n        Args:\n            idx (int): The index of the image to load.\n\n        Returns:\n            PIL.Image.Image: The loaded image, converted to RGB.\n        \"\"\"\n        # Construct the image filename based on the index (e.g., 'image_00001.jpg').\n        img_name = f\"image_{idx + 1:05d}.jpg\"\n        # Construct the full path to the image file.\n        img_path = os.path.join(self.image_dir, img_name)\n        # Open the image file.\n        with Image.open(img_path) as img:\n            # Convert the image to the RGB color space and return it.\n            image = img.convert(\"RGB\")\n        return image\n\n    def load_and_correct_labels(self):\n        \"\"\"\n        Loads labels from a .mat file and adjusts them to be zero-indexed.\n\n        Returns:\n            numpy.ndarray: An array of zero-indexed integer labels.\n        \"\"\"\n        # Load the MATLAB file containing the labels.\n        self.labels_mat = scipy.io.loadmat(\n            os.path.join(self.root_dir, \"imagelabels.mat\")\n        )\n        # Extract the labels array and correct for zero-based indexing.\n        labels = self.labels_mat[\"labels\"][0] - 1\n        # Return the processed labels.\n        return labels\n\n    def get_label_description(self, label):\n        \"\"\"\n        Retrieves the text description for a given label index.\n\n        Args:\n            label (int): The integer label.\n\n        Returns:\n            str: The corresponding text description of the label.\n        \"\"\"\n        # Construct the path to the file containing label descriptions.\n        path_labels_description = os.path.join(self.root_dir, \"labels_description.txt\")\n        # Open the label description file for reading.\n        with open(path_labels_description, \"r\") as f:\n            # Read all lines from the file.\n            lines = f.readlines()\n        # Get the description for the specified label and remove leading/trailing whitespace.\n        description = lines[label].strip()\n        # Return the clean description.\n        return description\n</pre> class FlowerDataset(Dataset):     \"\"\"     A custom dataset class for loading flower image data.      This class is designed to work with PyTorch's Dataset and DataLoader     abstractions. It handles loading images and their corresponding labels     from a specific directory structure.     \"\"\"     def __init__(self, root_dir, transform=None):         \"\"\"         Initializes the dataset object.          Args:             root_dir (str): The root directory where the dataset is stored.             transform (callable, optional): Optional transform to be applied                 on a sample.         \"\"\"         # Store the root directory path.         self.root_dir = root_dir         # Store the optional transformations.         self.transform = transform         # Construct the full path to the image directory.         self.image_dir = os.path.join(self.root_dir, \"jpg\")         # Load and process the labels from the corresponding file.         self.labels = self.load_and_correct_labels()      def __len__(self):         \"\"\"         Returns the total number of samples in the dataset.         \"\"\"         # The total number of samples is the number of labels.         return len(self.labels)      def __getitem__(self, idx):         \"\"\"         Retrieves a sample from the dataset at the specified index.          Args:             idx (int): The index of the sample to retrieve.          Returns:             tuple: A tuple containing the image and its label.         \"\"\"         # Retrieve the image for the given index.         image = self.retrieve_image(idx)          # Check if a transform is provided.         if self.transform is not None:             # Apply the transform to the image.             image = self.transform(image)          # Get the label corresponding to the index.         label = self.labels[idx]          # Return the processed image and its label.         return image, label      def retrieve_image(self, idx):         \"\"\"         Loads a single image from disk based on its index.          Args:             idx (int): The index of the image to load.          Returns:             PIL.Image.Image: The loaded image, converted to RGB.         \"\"\"         # Construct the image filename based on the index (e.g., 'image_00001.jpg').         img_name = f\"image_{idx + 1:05d}.jpg\"         # Construct the full path to the image file.         img_path = os.path.join(self.image_dir, img_name)         # Open the image file.         with Image.open(img_path) as img:             # Convert the image to the RGB color space and return it.             image = img.convert(\"RGB\")         return image      def load_and_correct_labels(self):         \"\"\"         Loads labels from a .mat file and adjusts them to be zero-indexed.          Returns:             numpy.ndarray: An array of zero-indexed integer labels.         \"\"\"         # Load the MATLAB file containing the labels.         self.labels_mat = scipy.io.loadmat(             os.path.join(self.root_dir, \"imagelabels.mat\")         )         # Extract the labels array and correct for zero-based indexing.         labels = self.labels_mat[\"labels\"][0] - 1         # Return the processed labels.         return labels      def get_label_description(self, label):         \"\"\"         Retrieves the text description for a given label index.          Args:             label (int): The integer label.          Returns:             str: The corresponding text description of the label.         \"\"\"         # Construct the path to the file containing label descriptions.         path_labels_description = os.path.join(self.root_dir, \"labels_description.txt\")         # Open the label description file for reading.         with open(path_labels_description, \"r\") as f:             # Read all lines from the file.             lines = f.readlines()         # Get the description for the specified label and remove leading/trailing whitespace.         description = lines[label].strip()         # Return the clean description.         return description In\u00a0[\u00a0]: Copied! <pre># Initialize the dataset object, providing the path to the data.\ndataset = FlowerDataset(path_dataset)\n</pre> # Initialize the dataset object, providing the path to the data. dataset = FlowerDataset(path_dataset) <p>It is good practice to verify that your dataset class works as expected before using it to train a model. After creating an instance of the <code>FlowerDataset</code> class, you will:</p> <ul> <li>Check the total number of samples in the dataset using the <code>len()</code> function.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Print the total number of samples in the dataset.\nprint(f'Number of samples in the dataset: {len(dataset)}\\n')\n</pre> # Print the total number of samples in the dataset. print(f'Number of samples in the dataset: {len(dataset)}\\n') <ul> <li>Retrieve a specific sample using an index.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Define an index for a sample to retrieve.\nsel_idx = 10\n\n# Retrieve the image and label for the selected index.\nimg, label = dataset[sel_idx]\n</pre> # Define an index for a sample to retrieve. sel_idx = 10  # Retrieve the image and label for the selected index. img, label = dataset[sel_idx] <ul> <li>Inspect the image size and label.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Create a string detailing the image's dimensions.\nimg_size_info = f\"Image size: {img.size}\"\n\n# Print the image size information along with its corresponding label.\nprint(f'{img_size_info}, Label: {label}\\n')\n</pre> # Create a string detailing the image's dimensions. img_size_info = f\"Image size: {img.size}\"  # Print the image size information along with its corresponding label. print(f'{img_size_info}, Label: {label}\\n') <ul> <li>Visualize the image using the helper function <code>plot_img</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>helper_utils.plot_img(img, label=label, info=img_size_info)\n</pre> helper_utils.plot_img(img, label=label, info=img_size_info) <p>Next, inspect the labels in the dataset. For each unique label, print its corresponding description using the <code>get_label_description</code> method.</p> <p>How are the labels?</p> In\u00a0[\u00a0]: Copied! <pre># Get all labels from the dataset object.\ndataset_labels = dataset.labels\n\n# Create a set of unique labels to remove duplicates.\nunique_labels = set(dataset_labels)\n\n# Iterate through each unique label.\nfor label in unique_labels:\n    # Print the numerical label and its corresponding text description.\n    print(f'Label: {label}, Description: {dataset.get_label_description(label)}')\n</pre> # Get all labels from the dataset object. dataset_labels = dataset.labels  # Create a set of unique labels to remove duplicates. unique_labels = set(dataset_labels)  # Iterate through each unique label. for label in unique_labels:     # Print the numerical label and its corresponding text description.     print(f'Label: {label}, Description: {dataset.get_label_description(label)}') In\u00a0[\u00a0]: Copied! <pre>def visual_exploration(dataset, num_rows=2, num_cols=4):\n    \"\"\"\n    Displays a grid of randomly selected samples from a dataset for visual inspection.\n\n    Args:\n        dataset: The dataset object from which to draw samples. It should support\n                 indexing and have a `get_label_description` method.\n        num_rows (int): The number of rows in the display grid.\n        num_cols (int): The number of columns in the display grid.\n    \"\"\"\n    # Calculate the total number of images to display in the grid.\n    total_samples = num_rows * num_cols\n\n    # Select a random set of unique indices from the dataset.\n    indices = np.random.choice(len(dataset), total_samples, replace=False)\n\n    # Create a grid of subplots to hold the images.\n    fig, axes = helper_utils.get_grid(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 4))\n\n    # Iterate over each subplot axis and the corresponding random sample index.\n    for ax, idx in zip(axes.flatten(), indices):\n        # Retrieve the image and its numerical label from the dataset.\n        image, label = dataset[idx]\n\n        # Get the human-readable text description for the label.\n        description = dataset.get_label_description(label)\n\n        # Format a new label string that includes both the number and description.\n        label = f\"{label} - {description}\"\n\n        # Create an information string with the sample's index and image dimensions.\n        info = f\"Index: {idx} Size: {image.size}\"\n\n        # Plot the image on the current subplot with its label and info.\n        helper_utils.plot_img(image, label=label, info=info, ax=ax)\n\n    # Render and display the entire grid of images.\n    plt.show()\n</pre> def visual_exploration(dataset, num_rows=2, num_cols=4):     \"\"\"     Displays a grid of randomly selected samples from a dataset for visual inspection.      Args:         dataset: The dataset object from which to draw samples. It should support                  indexing and have a `get_label_description` method.         num_rows (int): The number of rows in the display grid.         num_cols (int): The number of columns in the display grid.     \"\"\"     # Calculate the total number of images to display in the grid.     total_samples = num_rows * num_cols      # Select a random set of unique indices from the dataset.     indices = np.random.choice(len(dataset), total_samples, replace=False)      # Create a grid of subplots to hold the images.     fig, axes = helper_utils.get_grid(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 4))      # Iterate over each subplot axis and the corresponding random sample index.     for ax, idx in zip(axes.flatten(), indices):         # Retrieve the image and its numerical label from the dataset.         image, label = dataset[idx]          # Get the human-readable text description for the label.         description = dataset.get_label_description(label)          # Format a new label string that includes both the number and description.         label = f\"{label} - {description}\"          # Create an information string with the sample's index and image dimensions.         info = f\"Index: {idx} Size: {image.size}\"          # Plot the image on the current subplot with its label and info.         helper_utils.plot_img(image, label=label, info=info, ax=ax)      # Render and display the entire grid of images.     plt.show() In\u00a0[\u00a0]: Copied! <pre># Display a 2x4 grid of random samples from the dataset for visual inspection.\nvisual_exploration(dataset, num_rows=2, num_cols=4)\n</pre> # Display a 2x4 grid of random samples from the dataset for visual inspection. visual_exploration(dataset, num_rows=2, num_cols=4) In\u00a0[\u00a0]: Copied! <pre># Define the mean values for normalization.\nmean = [0.485, 0.456, 0.406]\n# Define the standard deviation values for normalization.\nstd = [0.229, 0.224, 0.225]\n</pre> # Define the mean values for normalization. mean = [0.485, 0.456, 0.406] # Define the standard deviation values for normalization. std = [0.229, 0.224, 0.225] In\u00a0[\u00a0]: Copied! <pre>transform = transforms.Compose([\n    # images transforms\n    transforms.Resize((256, 256)),  # Resize images to 256x256 pixels\n    transforms.CenterCrop(224),  # Center crop to 224x224 pixels\n    # bridge to tensor\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    # tensor transforms\n    transforms.Normalize(mean=mean, std=std),\n])\n</pre> transform = transforms.Compose([     # images transforms     transforms.Resize((256, 256)),  # Resize images to 256x256 pixels     transforms.CenterCrop(224),  # Center crop to 224x224 pixels     # bridge to tensor     transforms.ToTensor(),  # Convert images to PyTorch tensors     # tensor transforms     transforms.Normalize(mean=mean, std=std), ]) <p>You will now create a new instance of the <code>FlowerDataset</code> class, this time passing the transformation pipeline <code>transform</code> as an argument.</p> In\u00a0[\u00a0]: Copied! <pre># Create a new dataset instance with the specified image transformations.\ndataset_transformed = FlowerDataset(path_dataset, transform=transform)\n</pre> # Create a new dataset instance with the specified image transformations. dataset_transformed = FlowerDataset(path_dataset, transform=transform) <p>Inspect the same sample again to see the effect of the transformations. Using <code>quick_debug</code>, you can see that the image has been resized and cropped to 224x224 pixels, and its pixel values have been normalized.</p> In\u00a0[\u00a0]: Copied! <pre># Retrieve the transformed image and its label using the same index.\nimg_transformed, label = dataset_transformed[sel_idx]\n\n# quick check\nhelper_utils.quick_debug(img_transformed)\n\n# Plot the transformed image\nhelper_utils.plot_img(img_transformed, label=label)\n</pre> # Retrieve the transformed image and its label using the same index. img_transformed, label = dataset_transformed[sel_idx]  # quick check helper_utils.quick_debug(img_transformed)  # Plot the transformed image helper_utils.plot_img(img_transformed, label=label) <p>Observe that the image is now a tensor with shape <code>[3, 224, 224]</code> representing 3 color channels (RGB) and 224\u00d7224 pixels. Because the pixel values have been normalized, they are no longer in the original range of [0, 255].</p> <p>That\u2019s why the image looks different when visualized directly. To display it correctly, you need to denormalize it first. You can do this by applying the <code>Normalize</code> transformation again, using <code>new_mean</code> and <code>new_std</code> values to reverse the normalization process.</p> In\u00a0[\u00a0]: Copied! <pre>class Denormalize:\n    \"\"\"\n    A callable class to reverse the normalization of a tensor image.\n\n    This class calculates the inverse transformation of a standard normalization\n    and can be used as a transform step, for instance, to visualize images\n    after they have been normalized for a model.\n    \"\"\"\n    def __init__(self, mean, std):\n        \"\"\"\n        Initializes the denormalization transform.\n\n        Args:\n            mean (list or tuple): The mean values used for the original normalization.\n            std (list or tuple): The standard deviation values used for the original\n                                 normalization.\n        \"\"\"\n        # Calculate the adjusted mean for the denormalization process.\n        new_mean = [-m / s for m, s in zip(mean, std)]\n        # Calculate the adjusted standard deviation for the denormalization process.\n        new_std = [1 / s for s in std]\n        # Create a Normalize transform object with the inverse parameters.\n        self.denormalize = transforms.Normalize(mean=new_mean, std=new_std)\n\n    def __call__(self, tensor):\n        \"\"\"\n        Applies the denormalization transform to a tensor.\n\n        Args:\n            tensor: The normalized tensor to be denormalized.\n\n        Returns:\n            The denormalized tensor.\n        \"\"\"\n        # Apply the denormalization transform to the input tensor.\n        return self.denormalize(tensor)\n</pre> class Denormalize:     \"\"\"     A callable class to reverse the normalization of a tensor image.      This class calculates the inverse transformation of a standard normalization     and can be used as a transform step, for instance, to visualize images     after they have been normalized for a model.     \"\"\"     def __init__(self, mean, std):         \"\"\"         Initializes the denormalization transform.          Args:             mean (list or tuple): The mean values used for the original normalization.             std (list or tuple): The standard deviation values used for the original                                  normalization.         \"\"\"         # Calculate the adjusted mean for the denormalization process.         new_mean = [-m / s for m, s in zip(mean, std)]         # Calculate the adjusted standard deviation for the denormalization process.         new_std = [1 / s for s in std]         # Create a Normalize transform object with the inverse parameters.         self.denormalize = transforms.Normalize(mean=new_mean, std=new_std)      def __call__(self, tensor):         \"\"\"         Applies the denormalization transform to a tensor.          Args:             tensor: The normalized tensor to be denormalized.          Returns:             The denormalized tensor.         \"\"\"         # Apply the denormalization transform to the input tensor.         return self.denormalize(tensor) In\u00a0[\u00a0]: Copied! <pre># Create an instance of the Denormalize class with the original mean and std.\ndenormalize = Denormalize(mean=mean, std=std)\n# Apply the denormalization transform to the image tensor.\nimg_tensor = denormalize(img_transformed)\n\n# Create an information string with the tensor's shape.\nimg_shape_info = f\"Image Shape: {img_tensor.size()}\"\n# Plot the denormalized image to visualize the result.\nhelper_utils.plot_img(img_tensor, label=label, info=img_shape_info)\n</pre> # Create an instance of the Denormalize class with the original mean and std. denormalize = Denormalize(mean=mean, std=std) # Apply the denormalization transform to the image tensor. img_tensor = denormalize(img_transformed)  # Create an information string with the tensor's shape. img_shape_info = f\"Image Shape: {img_tensor.size()}\" # Plot the denormalized image to visualize the result. helper_utils.plot_img(img_tensor, label=label, info=img_shape_info) In\u00a0[\u00a0]: Copied! <pre>def split_dataset(dataset, val_fraction=0.15, test_fraction=0.15):\n    \"\"\"\n    Split the dataset into training, validation, and test sets.\n    \n    By default, this function splits the data into 70% for training,\n    15% for validation, and 15% for testing.\n    \"\"\"\n\n    # Calculate the sizes of each split.\n    total_size = len(dataset)\n    val_size = int(total_size * val_fraction)\n    test_size = int(total_size * test_fraction)\n    train_size = total_size - val_size - test_size\n\n    # Use random_split to create the datasets.\n    train_dataset, val_dataset, test_dataset = random_split(\n        dataset, [train_size, val_size, test_size]\n    )\n    return train_dataset, val_dataset, test_dataset\n</pre> def split_dataset(dataset, val_fraction=0.15, test_fraction=0.15):     \"\"\"     Split the dataset into training, validation, and test sets.          By default, this function splits the data into 70% for training,     15% for validation, and 15% for testing.     \"\"\"      # Calculate the sizes of each split.     total_size = len(dataset)     val_size = int(total_size * val_fraction)     test_size = int(total_size * test_fraction)     train_size = total_size - val_size - test_size      # Use random_split to create the datasets.     train_dataset, val_dataset, test_dataset = random_split(         dataset, [train_size, val_size, test_size]     )     return train_dataset, val_dataset, test_dataset In\u00a0[\u00a0]: Copied! <pre>train_dataset, val_dataset, test_dataset = split_dataset(dataset_transformed)\n</pre> train_dataset, val_dataset, test_dataset = split_dataset(dataset_transformed) <p>Check the sizes of the resulting datasets to ensure they match the expected proportions.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Length of training dataset:   {len(train_dataset)}\")\nprint(f\"Length of validation dataset: {len(val_dataset)}\")\nprint(f\"Length of test dataset:       {len(test_dataset)}\")\n</pre> print(f\"Length of training dataset:   {len(train_dataset)}\") print(f\"Length of validation dataset: {len(val_dataset)}\") print(f\"Length of test dataset:       {len(test_dataset)}\") In\u00a0[\u00a0]: Copied! <pre># Set the batch size for the data loaders.\nbatch_size = 32\n\n# Create the DataLoader for the training set, with shuffling enabled.\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\n# Create the DataLoader for the validation set, with shuffling disabled.\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n\n# Create the DataLoader for the test set, with shuffling disabled.\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n</pre> # Set the batch size for the data loaders. batch_size = 32  # Create the DataLoader for the training set, with shuffling enabled. train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)  # Create the DataLoader for the validation set, with shuffling disabled. val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)  # Create the DataLoader for the test set, with shuffling disabled. test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) <p>To understand how dataloaders work, you can iterate for two epochs over the training and validation dataloaders and, at the very end, iterate over the test dataloader. This will give you a sense of how data is served in batches during training and evaluation.</p> In\u00a0[\u00a0]: Copied! <pre># Define the total number of training epochs.\nn_epochs = 2\n</pre> # Define the total number of training epochs. n_epochs = 2 In\u00a0[\u00a0]: Copied! <pre># Start the main training loop for each epoch.\nfor epoch in range(n_epochs):\n    # Print a header to indicate the start of a new epoch.\n    print(f\"=== Processing epoch {epoch} ===\")\n\n    # Announce the start of the training phase.\n    print(f\"Pass number {epoch} through the training set\")\n    print('Training...')\n    # Get the total number of samples in the training set.\n    train_samples = len(train_dataset)\n    # Create a progress bar for the training data loader.\n    train_bar = helper_utils.get_dataloader_bar(train_dataloader, color='blue')\n    \n    # Iterate over the training data loader to get batches of images and labels.\n    for batch, (images, labels) in enumerate(train_dataloader):\n        # Update the training progress bar for the current batch.\n        helper_utils.update_dataloader_bar(train_bar, batch, batch_size, train_samples)\n\n    # Announce the start of the validation phase.\n    print(f\"\\nPass number {epoch} through the validation set\")\n    print('Validation...')\n    # Create a progress bar for the validation data loader.\n    val_bar = helper_utils.get_dataloader_bar(val_dataloader, color='orange')\n    # Get the total number of samples in the validation set.\n    val_samples = len(val_dataset)\n    \n    # Iterate over the validation data loader to get batches of images and labels.\n    for batch, (images, labels) in enumerate(val_dataloader):\n        # Update the validation progress bar for the current batch.\n        helper_utils.update_dataloader_bar(val_bar, batch, batch_size, val_samples)\n</pre> # Start the main training loop for each epoch. for epoch in range(n_epochs):     # Print a header to indicate the start of a new epoch.     print(f\"=== Processing epoch {epoch} ===\")      # Announce the start of the training phase.     print(f\"Pass number {epoch} through the training set\")     print('Training...')     # Get the total number of samples in the training set.     train_samples = len(train_dataset)     # Create a progress bar for the training data loader.     train_bar = helper_utils.get_dataloader_bar(train_dataloader, color='blue')          # Iterate over the training data loader to get batches of images and labels.     for batch, (images, labels) in enumerate(train_dataloader):         # Update the training progress bar for the current batch.         helper_utils.update_dataloader_bar(train_bar, batch, batch_size, train_samples)      # Announce the start of the validation phase.     print(f\"\\nPass number {epoch} through the validation set\")     print('Validation...')     # Create a progress bar for the validation data loader.     val_bar = helper_utils.get_dataloader_bar(val_dataloader, color='orange')     # Get the total number of samples in the validation set.     val_samples = len(val_dataset)          # Iterate over the validation data loader to get batches of images and labels.     for batch, (images, labels) in enumerate(val_dataloader):         # Update the validation progress bar for the current batch.         helper_utils.update_dataloader_bar(val_bar, batch, batch_size, val_samples) In\u00a0[\u00a0]: Copied! <pre># Announce the final evaluation on the test set.\nprint(\"\\nFinal pass through the test set for evaluation\")\n# Create a progress bar for the test data loader.\ntest_bar = helper_utils.get_dataloader_bar(test_dataloader, color='green')\n# Get the total number of samples in the test set.\ntest_samples = len(test_dataset)\n\n# Iterate over the test data loader to get batches of images and labels.\nfor batch, (images, labels) in enumerate(test_dataloader):\n    # Update the test progress bar for the current batch.\n    helper_utils.update_dataloader_bar(test_bar, batch, batch_size, test_samples)\n</pre> # Announce the final evaluation on the test set. print(\"\\nFinal pass through the test set for evaluation\") # Create a progress bar for the test data loader. test_bar = helper_utils.get_dataloader_bar(test_dataloader, color='green') # Get the total number of samples in the test set. test_samples = len(test_dataset)  # Iterate over the test data loader to get batches of images and labels. for batch, (images, labels) in enumerate(test_dataloader):     # Update the test progress bar for the current batch.     helper_utils.update_dataloader_bar(test_bar, batch, batch_size, test_samples)  You can observe that the training dataloader has a total of 180 batches (for a total of 5733 samples). The validation and test dataloaders each have 39 batches (for a total of 1228 samples each).    <p>Within <code>get_augmentation_transform</code>, you\u2019ll create a more advanced transformation pipeline that includes data augmentation techniques. This pipeline consists of the following:</p> <p>Transformations that augment the raw images:</p> <ul> <li><code>RandomHorizontalFlip(p=0.5)</code>: This transformation randomly flips the image horizontally with a probability of 0.5,</li> <li><code>RandomRotation(degrees=10)</code>: This transformation randomly rotates the image within a range of -10 to +10 degrees,</li> <li><code>ColorJitter(brightness=0.2)</code>: This transformation randomly changes the brightness, contrast, saturation, and hue of the image.</li> </ul> <p>Transformations that standardize the images as <code>transform</code>: <code>Resize</code>, <code>CenterCrop</code>, <code>ToTensor</code>, and <code>Normalize</code>.</p> <p>You then create a new instance of the <code>FlowerDataset</code> class, this time passing the augmentation transformation pipeline <code>augmentation_transform</code> as an argument.</p> In\u00a0[\u00a0]: Copied! <pre>def get_augmentation_transform(mean, std):\n    \"\"\"\n    Creates and returns a composition of image transformations for data augmentation\n    and preprocessing.\n\n    Args:\n        mean (list or tuple): A sequence of mean values for each channel.\n        std (list or tuple): A sequence of standard deviation values for each channel.\n\n    Returns:\n        torchvision.transforms.Compose: A composed pipeline of transformations.\n    \"\"\"\n    # Define a list of data augmentation transformations to be applied randomly.\n    augmentations_transforms = [\n        # Randomly flip the image horizontally with a 50% probability.\n        transforms.RandomHorizontalFlip(p=0.5),\n        # Randomly rotate the image within a range of +/- 10 degrees.\n        transforms.RandomRotation(degrees=10),\n        # Randomly adjust the brightness of the image.\n        transforms.ColorJitter(brightness=0.2),\n    ]\n    \n    # Define the main list of standard, non-random transformations.\n    main_transforms = [\n        # Resize the input image to 256x256 pixels.\n        transforms.Resize((256, 256)),\n        # Crop the center 224x224 pixels of the image.\n        transforms.CenterCrop(224),\n        # Convert the PIL Image to a PyTorch tensor.\n        transforms.ToTensor(),\n        # Normalize the tensor with the provided mean and standard deviation.\n        transforms.Normalize(mean=mean, std=std),\n    ]\n\n    # Combine the augmentation and main transformations into a single pipeline.\n    transform = transforms.Compose(augmentations_transforms + main_transforms)\n    # Return the final composed transform object.\n    return transform\n</pre> def get_augmentation_transform(mean, std):     \"\"\"     Creates and returns a composition of image transformations for data augmentation     and preprocessing.      Args:         mean (list or tuple): A sequence of mean values for each channel.         std (list or tuple): A sequence of standard deviation values for each channel.      Returns:         torchvision.transforms.Compose: A composed pipeline of transformations.     \"\"\"     # Define a list of data augmentation transformations to be applied randomly.     augmentations_transforms = [         # Randomly flip the image horizontally with a 50% probability.         transforms.RandomHorizontalFlip(p=0.5),         # Randomly rotate the image within a range of +/- 10 degrees.         transforms.RandomRotation(degrees=10),         # Randomly adjust the brightness of the image.         transforms.ColorJitter(brightness=0.2),     ]          # Define the main list of standard, non-random transformations.     main_transforms = [         # Resize the input image to 256x256 pixels.         transforms.Resize((256, 256)),         # Crop the center 224x224 pixels of the image.         transforms.CenterCrop(224),         # Convert the PIL Image to a PyTorch tensor.         transforms.ToTensor(),         # Normalize the tensor with the provided mean and standard deviation.         transforms.Normalize(mean=mean, std=std),     ]      # Combine the augmentation and main transformations into a single pipeline.     transform = transforms.Compose(augmentations_transforms + main_transforms)     # Return the final composed transform object.     return transform In\u00a0[\u00a0]: Copied! <pre># Create the augmentation and preprocessing pipeline, providing the normalization stats.\naugmentation_transform = get_augmentation_transform(mean=mean, std=std)\n\n# Initialize a new dataset instance that will use the augmentation pipeline.\ndataset_augmented = FlowerDataset(path_dataset, transform=augmentation_transform)\n</pre> # Create the augmentation and preprocessing pipeline, providing the normalization stats. augmentation_transform = get_augmentation_transform(mean=mean, std=std)  # Initialize a new dataset instance that will use the augmentation pipeline. dataset_augmented = FlowerDataset(path_dataset, transform=augmentation_transform) <p>Debugging a dataset pipeline is a relevant step to ensure that the transformations and data loading processes are functioning as intended. <code>visualize_augmentation</code> allows you to visualize the effect of the augmentation transformations on a sample image.</p> <p>For a given number of versions, it retrieves the same sample image from the dataset. As the dataset applies random transformations, each retrieved version of the image will be affected differently, allowing you to see the variety of augmentations applied.</p> In\u00a0[\u00a0]: Copied! <pre>def visualize_augmentations(dataset_aug, idx=0, num_versions=8):\n    \"\"\"\n    Displays multiple augmented versions of a single image from a dataset.\n\n    This function repeatedly retrieves an image from a dataset with augmentations\n    enabled, then displays each unique, randomly generated version in a grid\n    to help visualize the effect of the transformations.\n\n    Args:\n        dataset_aug: The dataset object with augmentation transforms applied.\n        idx (int): The index of the image in the dataset to visualize.\n        num_versions (int): The total number of augmented versions to display.\n    \"\"\"\n    # Create a denormalization transform to revert normalization for display.\n    denormalize = Denormalize(mean, std)\n\n    # Set the number of rows for the visualization grid.\n    n_rows = 2\n    # Calculate the number of columns needed based on the total versions to show.\n    n_cols = num_versions // n_rows\n    # Create a grid of subplots to display the images.\n    fig, axes = helper_utils.get_grid(n_rows, n_cols, figsize=(16, 8))\n\n    # Iterate through each subplot axis in the grid.\n    for ax in axes.flatten():\n        # Get a new, randomly augmented version of the same image by index.\n        img, label = dataset_aug[idx]\n\n        # Denormalize the image tensor so it can be displayed correctly.\n        img = denormalize(img)\n\n        # Plot the augmented image on the current subplot.\n        helper_utils.plot_img(img=img, ax=ax)\n\n    # Display the complete grid of augmented images.\n    plt.show()\n</pre> def visualize_augmentations(dataset_aug, idx=0, num_versions=8):     \"\"\"     Displays multiple augmented versions of a single image from a dataset.      This function repeatedly retrieves an image from a dataset with augmentations     enabled, then displays each unique, randomly generated version in a grid     to help visualize the effect of the transformations.      Args:         dataset_aug: The dataset object with augmentation transforms applied.         idx (int): The index of the image in the dataset to visualize.         num_versions (int): The total number of augmented versions to display.     \"\"\"     # Create a denormalization transform to revert normalization for display.     denormalize = Denormalize(mean, std)      # Set the number of rows for the visualization grid.     n_rows = 2     # Calculate the number of columns needed based on the total versions to show.     n_cols = num_versions // n_rows     # Create a grid of subplots to display the images.     fig, axes = helper_utils.get_grid(n_rows, n_cols, figsize=(16, 8))      # Iterate through each subplot axis in the grid.     for ax in axes.flatten():         # Get a new, randomly augmented version of the same image by index.         img, label = dataset_aug[idx]          # Denormalize the image tensor so it can be displayed correctly.         img = denormalize(img)          # Plot the augmented image on the current subplot.         helper_utils.plot_img(img=img, ax=ax)      # Display the complete grid of augmented images.     plt.show() In\u00a0[\u00a0]: Copied! <pre># Display 8 augmented versions of the selected image to see the transformations.\nvisualize_augmentations(dataset_augmented, idx=sel_idx, num_versions=8)\n</pre> # Display 8 augmented versions of the selected image to see the transformations. visualize_augmentations(dataset_augmented, idx=sel_idx, num_versions=8) <p>To address this, you will create a custom dataset class <code>SubsetWithTransform</code> that wraps around a <code>Subset</code> and allows you to specify a different transformation for it.</p> In\u00a0[\u00a0]: Copied! <pre>class SubsetWithTransform(Dataset):\n    \"\"\"\n    A wrapper for a PyTorch Subset that applies a specific transformation.\n\n    This class allows for applying a different set of transformations to a\n    subset of a dataset, which is useful for creating distinct training,\n    validation, or test sets with different preprocessing steps from the\n    same base dataset.\n    \"\"\"\n    def __init__(self, subset, transform=None):\n        \"\"\"\n        Initializes the SubsetWithTransform object.\n\n        Args:\n            subset: A PyTorch Subset object containing a portion of a dataset.\n            transform (callable, optional): An optional transform to be applied\n                to the samples within this subset.\n        \"\"\"\n        # Store the original subset of the dataset.\n        self.subset = subset\n        # Store the transformations to be applied.\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the subset.\n        \"\"\"\n        # Return the length of the underlying subset.\n        return len(self.subset)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a sample and applies the transform.\n\n        Args:\n            idx (int): The index of the sample to retrieve.\n\n        Returns:\n            tuple: A tuple containing the transformed image and its label.\n        \"\"\"\n        # Get the original image and label from the underlying subset.\n        image, label = self.subset[idx]\n        # Check if a transform has been provided.\n        if self.transform:\n            # Apply the transform to the image.\n            image = self.transform(image)\n        # Return the transformed image and its label.\n        return image, label\n</pre> class SubsetWithTransform(Dataset):     \"\"\"     A wrapper for a PyTorch Subset that applies a specific transformation.      This class allows for applying a different set of transformations to a     subset of a dataset, which is useful for creating distinct training,     validation, or test sets with different preprocessing steps from the     same base dataset.     \"\"\"     def __init__(self, subset, transform=None):         \"\"\"         Initializes the SubsetWithTransform object.          Args:             subset: A PyTorch Subset object containing a portion of a dataset.             transform (callable, optional): An optional transform to be applied                 to the samples within this subset.         \"\"\"         # Store the original subset of the dataset.         self.subset = subset         # Store the transformations to be applied.         self.transform = transform      def __len__(self):         \"\"\"         Returns the total number of samples in the subset.         \"\"\"         # Return the length of the underlying subset.         return len(self.subset)      def __getitem__(self, idx):         \"\"\"         Retrieves a sample and applies the transform.          Args:             idx (int): The index of the sample to retrieve.          Returns:             tuple: A tuple containing the transformed image and its label.         \"\"\"         # Get the original image and label from the underlying subset.         image, label = self.subset[idx]         # Check if a transform has been provided.         if self.transform:             # Apply the transform to the image.             image = self.transform(image)         # Return the transformed image and its label.         return image, label In\u00a0[\u00a0]: Copied! <pre># Apply the augmentation pipeline to the training subset.\ntrain_dataset = SubsetWithTransform(train_dataset, transform=augmentation_transform)\n# Apply the basic preprocessing transform to the validation subset.\nval_dataset = SubsetWithTransform(val_dataset, transform=transform)\n# Apply the basic preprocessing transform to the test subset.\ntest_dataset = SubsetWithTransform(test_dataset, transform=transform)\n</pre> # Apply the augmentation pipeline to the training subset. train_dataset = SubsetWithTransform(train_dataset, transform=augmentation_transform) # Apply the basic preprocessing transform to the validation subset. val_dataset = SubsetWithTransform(val_dataset, transform=transform) # Apply the basic preprocessing transform to the test subset. test_dataset = SubsetWithTransform(test_dataset, transform=transform) <p>You can check that indeed the training dataset has the augmentation transformations applied, while the validation and test datasets do not.</p> In\u00a0[\u00a0]: Copied! <pre>print(train_dataset.transform)\nprint(val_dataset.transform)\nprint(test_dataset.transform)\n</pre> print(train_dataset.transform) print(val_dataset.transform) print(test_dataset.transform) In\u00a0[\u00a0]: Copied! <pre>class RobustFlowerDataset(Dataset):\n    \"\"\"\n    A custom dataset class with robust error handling for loading images.\n\n    This class is designed to gracefully handle issues with individual data\n    samples, such as corrupted files or incorrect formats. It logs any errors\n    and attempts to load a different sample instead of crashing.\n    \"\"\"\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Initializes the dataset object.\n\n        Args:\n            root_dir (str): The root directory where the dataset is stored.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        # Store the root directory path.\n        self.root_dir = root_dir\n        # Construct the full path to the image directory.\n        self.img_dir = os.path.join(root_dir, \"jpg\")\n        # Store the optional transformations.\n        self.transform = transform\n        # Load and process the labels from the corresponding file.\n        self.labels = self.load_and_correct_labels()\n        # Initialize a list to keep track of any errors encountered.\n        self.error_logs = []\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a sample, handling errors by trying the next available item.\n\n        Args:\n            idx (int): The index of the sample to retrieve.\n\n        Returns:\n            tuple: A tuple containing the image and its label.\n        \"\"\"\n        # Loop to attempt loading a valid sample, preventing an infinite loop.\n        for attempt in range(len(self)):\n            # Attempt to load and process the sample.\n            try:\n                # Retrieve the image using the helper method.\n                image = self.retrieve_image(idx)\n                # Check if a transform has been provided.\n                if self.transform:\n                    # Apply the transform to the image.\n                    image = self.transform(image)\n                # Get the label for the current index.\n                label = self.labels[idx]\n                # Return the valid image and its corresponding label.\n                return image, label\n            # Catch any exception that occurs during the process.\n            except Exception as e:\n                # Log the error with its index and message.\n                self.log_error(idx, e)\n                # Move to the next index, wrapping around if necessary.\n                idx = (idx + 1) % len(self)\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n        \"\"\"\n        # The total number of samples is the number of labels.\n        return len(self.labels)\n\n    def retrieve_image(self, idx):\n        \"\"\"\n        Loads and validates a single image from disk.\n\n        Args:\n            idx (int): The index of the image to load.\n\n        Returns:\n            PIL.Image.Image: The validated and loaded image object.\n        \"\"\"\n        # Construct the image filename based on the index.\n        img_name = f\"image_{idx+1:05d}.jpg\"\n        # Construct the full path to the image file.\n        img_path = os.path.join(self.img_dir, img_name)\n        # Open the image file to check its integrity without loading fully.\n        with Image.open(img_path) as img:\n            # Perform a quick verification of the file's structure.\n            img.verify()\n        # Re-open the image file after successful verification.\n        image = Image.open(img_path)\n        # Fully load the image data into memory.\n        image.load()\n        # Check if the image dimensions are below a minimum threshold.\n        if image.size[0] &lt; 32 or image.size[1] &lt; 32:\n            # Raise an error for images that are too small.\n            raise ValueError(f\"Image too small: {image.size}\")\n        # Check if the image is not in the RGB color mode.\n        if image.mode != \"RGB\":\n            # Convert the image to RGB.\n            image = image.convert(\"RGB\")\n        # Return the fully loaded and validated image.\n        return image\n\n    def load_and_correct_labels(self):\n        \"\"\"\n        Loads labels from a .mat file and adjusts them.\n\n        Returns:\n            numpy.ndarray: An array of zero-indexed integer labels.\n        \"\"\"\n        # Load the MATLAB file containing the labels.\n        self.labels_mat = scipy.io.loadmat(\n            os.path.join(self.root_dir, \"imagelabels.mat\")\n        )\n        # Extract the labels array and correct for zero-based indexing.\n        labels = self.labels_mat[\"labels\"][0] - 1\n        # Truncate the dataset to the first 10 labels for quick testing.\n        labels = labels[:10]\n        # Return the processed labels.\n        return labels\n\n    def log_error(self, idx, e):\n        \"\"\"\n        Records the details of an error encountered during data loading.\n\n        Args:\n            idx (int): The index of the problematic sample.\n            e (Exception): The exception object that was raised.\n        \"\"\"\n        # Construct the filename of the problematic image.\n        img_name = f\"image_{idx + 1:05d}.jpg\"\n        # Construct the full path to the image file.\n        img_path = os.path.join(self.img_dir, img_name)\n        # Append a dictionary with error details to the log.\n        self.error_logs.append(\n            {\n                \"index\": idx,\n                \"error\": str(e),\n                \"path\": img_path if \"img_path\" in locals() else \"unknown\",\n            }\n        )\n        # Print a warning to the console about the skipped image.\n        print(f\"Warning: Skipping corrupted image {idx}: {e}\")\n\n    def get_error_summary(self):\n        \"\"\"\n        Prints a summary of all errors encountered during dataset processing.\n        \"\"\"\n        # Check if the error log is empty.\n        if not self.error_logs:\n            # Print a message indicating the dataset is clean.\n            print(\"No errors encountered - dataset is clean!\")\n        else:\n            # Print the total number of problematic images found.\n            print(f\"\\nEncountered {len(self.error_logs)} problematic images:\")\n            # Iterate through the first few logged errors.\n            for error in self.error_logs[:5]:\n                # Print the details of an individual error.\n                print(f\"  Index {error['index']}: {error['error']}\")\n            # Check if there are more errors than were displayed.\n            if len(self.error_logs) &gt; 5:\n                # Print a summary of the remaining errors.\n                print(f\"  ... and {len(self.error_logs) - 5} more\")\n</pre> class RobustFlowerDataset(Dataset):     \"\"\"     A custom dataset class with robust error handling for loading images.      This class is designed to gracefully handle issues with individual data     samples, such as corrupted files or incorrect formats. It logs any errors     and attempts to load a different sample instead of crashing.     \"\"\"     def __init__(self, root_dir, transform=None):         \"\"\"         Initializes the dataset object.          Args:             root_dir (str): The root directory where the dataset is stored.             transform (callable, optional): Optional transform to be applied                 on a sample.         \"\"\"         # Store the root directory path.         self.root_dir = root_dir         # Construct the full path to the image directory.         self.img_dir = os.path.join(root_dir, \"jpg\")         # Store the optional transformations.         self.transform = transform         # Load and process the labels from the corresponding file.         self.labels = self.load_and_correct_labels()         # Initialize a list to keep track of any errors encountered.         self.error_logs = []      def __getitem__(self, idx):         \"\"\"         Retrieves a sample, handling errors by trying the next available item.          Args:             idx (int): The index of the sample to retrieve.          Returns:             tuple: A tuple containing the image and its label.         \"\"\"         # Loop to attempt loading a valid sample, preventing an infinite loop.         for attempt in range(len(self)):             # Attempt to load and process the sample.             try:                 # Retrieve the image using the helper method.                 image = self.retrieve_image(idx)                 # Check if a transform has been provided.                 if self.transform:                     # Apply the transform to the image.                     image = self.transform(image)                 # Get the label for the current index.                 label = self.labels[idx]                 # Return the valid image and its corresponding label.                 return image, label             # Catch any exception that occurs during the process.             except Exception as e:                 # Log the error with its index and message.                 self.log_error(idx, e)                 # Move to the next index, wrapping around if necessary.                 idx = (idx + 1) % len(self)      def __len__(self):         \"\"\"         Returns the total number of samples in the dataset.         \"\"\"         # The total number of samples is the number of labels.         return len(self.labels)      def retrieve_image(self, idx):         \"\"\"         Loads and validates a single image from disk.          Args:             idx (int): The index of the image to load.          Returns:             PIL.Image.Image: The validated and loaded image object.         \"\"\"         # Construct the image filename based on the index.         img_name = f\"image_{idx+1:05d}.jpg\"         # Construct the full path to the image file.         img_path = os.path.join(self.img_dir, img_name)         # Open the image file to check its integrity without loading fully.         with Image.open(img_path) as img:             # Perform a quick verification of the file's structure.             img.verify()         # Re-open the image file after successful verification.         image = Image.open(img_path)         # Fully load the image data into memory.         image.load()         # Check if the image dimensions are below a minimum threshold.         if image.size[0] &lt; 32 or image.size[1] &lt; 32:             # Raise an error for images that are too small.             raise ValueError(f\"Image too small: {image.size}\")         # Check if the image is not in the RGB color mode.         if image.mode != \"RGB\":             # Convert the image to RGB.             image = image.convert(\"RGB\")         # Return the fully loaded and validated image.         return image      def load_and_correct_labels(self):         \"\"\"         Loads labels from a .mat file and adjusts them.          Returns:             numpy.ndarray: An array of zero-indexed integer labels.         \"\"\"         # Load the MATLAB file containing the labels.         self.labels_mat = scipy.io.loadmat(             os.path.join(self.root_dir, \"imagelabels.mat\")         )         # Extract the labels array and correct for zero-based indexing.         labels = self.labels_mat[\"labels\"][0] - 1         # Truncate the dataset to the first 10 labels for quick testing.         labels = labels[:10]         # Return the processed labels.         return labels      def log_error(self, idx, e):         \"\"\"         Records the details of an error encountered during data loading.          Args:             idx (int): The index of the problematic sample.             e (Exception): The exception object that was raised.         \"\"\"         # Construct the filename of the problematic image.         img_name = f\"image_{idx + 1:05d}.jpg\"         # Construct the full path to the image file.         img_path = os.path.join(self.img_dir, img_name)         # Append a dictionary with error details to the log.         self.error_logs.append(             {                 \"index\": idx,                 \"error\": str(e),                 \"path\": img_path if \"img_path\" in locals() else \"unknown\",             }         )         # Print a warning to the console about the skipped image.         print(f\"Warning: Skipping corrupted image {idx}: {e}\")      def get_error_summary(self):         \"\"\"         Prints a summary of all errors encountered during dataset processing.         \"\"\"         # Check if the error log is empty.         if not self.error_logs:             # Print a message indicating the dataset is clean.             print(\"No errors encountered - dataset is clean!\")         else:             # Print the total number of problematic images found.             print(f\"\\nEncountered {len(self.error_logs)} problematic images:\")             # Iterate through the first few logged errors.             for error in self.error_logs[:5]:                 # Print the details of an individual error.                 print(f\"  Index {error['index']}: {error['error']}\")             # Check if there are more errors than were displayed.             if len(self.error_logs) &gt; 5:                 # Print a summary of the remaining errors.                 print(f\"  ... and {len(self.error_logs) - 5} more\") <p>The Oxford Flowers 102 dataset is clean, but to illustrate robustness techniques, a subset of images in the dataset have been intentionally corrupted. The files of this subset are in the folder <code>./corrupted_flower_data</code>.</p> <p>You can create an instance of the <code>RobustFlowerDataset</code> class, passing the path to the corrupted dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Define the path to the directory containing the corrupted dataset.\ncorrupted_dataset_path = './corrupted_flower_data'\n\n# Initialize the robust dataset handler with the path to the corrupted data.\nrobust_dataset = RobustFlowerDataset(corrupted_dataset_path)\n</pre> # Define the path to the directory containing the corrupted dataset. corrupted_dataset_path = './corrupted_flower_data'  # Initialize the robust dataset handler with the path to the corrupted data. robust_dataset = RobustFlowerDataset(corrupted_dataset_path) In\u00a0[\u00a0]: Copied! <pre># Set the index to a known corrupted image. Image 2 is tiny\nidx = 2\n\n# Attempt to retrieve the image; the robust dataset will skip the bad one and return the next.\nimg, label = robust_dataset[idx]\n\n# Plot the retrieved image, which should be the one following the corrupted one.\nhelper_utils.plot_img(img)\n\n# Explicitly retrieve the next image in the sequence to verify.\nnext_img, next_label = robust_dataset[idx + 1]\n\n# Plot the next image; it should be identical to the one above.\nhelper_utils.plot_img(next_img)\n</pre> # Set the index to a known corrupted image. Image 2 is tiny idx = 2  # Attempt to retrieve the image; the robust dataset will skip the bad one and return the next. img, label = robust_dataset[idx]  # Plot the retrieved image, which should be the one following the corrupted one. helper_utils.plot_img(img)  # Explicitly retrieve the next image in the sequence to verify. next_img, next_label = robust_dataset[idx + 1]  # Plot the next image; it should be identical to the one above. helper_utils.plot_img(next_img) In\u00a0[\u00a0]: Copied! <pre># Set the index to a known grayscale image.\n# Image 4 is corrupted (grayscale)\nidx = 4\n\n# Reconstruct the path to the original image file.\noriginal_img_path = os.path.join(robust_dataset.img_dir, f\"image_{idx + 1:05d}.jpg\")\n# Open the original image directly to check its mode before correction.\noriginal_img = Image.open(original_img_path)\n# Print the mode of the original, uncorrected image.\nprint(f\"Mode of the original image file: {original_img.mode}\")  # Prints 'L' for 8-bit grayscale. A standard color image would be 'RGB'.\n</pre> # Set the index to a known grayscale image. # Image 4 is corrupted (grayscale) idx = 4  # Reconstruct the path to the original image file. original_img_path = os.path.join(robust_dataset.img_dir, f\"image_{idx + 1:05d}.jpg\") # Open the original image directly to check its mode before correction. original_img = Image.open(original_img_path) # Print the mode of the original, uncorrected image. print(f\"Mode of the original image file: {original_img.mode}\")  # Prints 'L' for 8-bit grayscale. A standard color image would be 'RGB'. In\u00a0[\u00a0]: Copied! <pre># Retrieve the image; the robust loader should automatically convert it to RGB.\nimg, label = robust_dataset[idx]\n\n# Plot the image to visually confirm it's now in color.\nhelper_utils.plot_img(img)\n\n# Print the image's mode to confirm it has been corrected to 'RGB'.\nprint(f\"Mode of the corrected image: {img.mode}\")\n</pre> # Retrieve the image; the robust loader should automatically convert it to RGB. img, label = robust_dataset[idx]  # Plot the image to visually confirm it's now in color. helper_utils.plot_img(img)  # Print the image's mode to confirm it has been corrected to 'RGB'. print(f\"Mode of the corrected image: {img.mode}\") In\u00a0[\u00a0]: Copied! <pre># Set the index to a known corrupted or unreadable image.\nidx = 6\n\n# Attempt to retrieve the image; the robust loader should skip the corrupted file and return the next one.\nrobust_img = robust_dataset[idx][0]\n\n# Plot the retrieved image, which should be the sample from the next index (7).\nhelper_utils.plot_img(robust_img)\n\n# Explicitly retrieve the next image in the sequence to verify the fallback logic.\n# Check next image to ensure it's correct\nnext_img, next_label = robust_dataset[idx + 1]\n\n# Plot the next image; it should be identical to the one above, confirming the skip.\nhelper_utils.plot_img(next_img)\n</pre> # Set the index to a known corrupted or unreadable image. idx = 6  # Attempt to retrieve the image; the robust loader should skip the corrupted file and return the next one. robust_img = robust_dataset[idx][0]  # Plot the retrieved image, which should be the sample from the next index (7). helper_utils.plot_img(robust_img)  # Explicitly retrieve the next image in the sequence to verify the fallback logic. # Check next image to ensure it's correct next_img, next_label = robust_dataset[idx + 1]  # Plot the next image; it should be identical to the one above, confirming the skip. helper_utils.plot_img(next_img) <p>You can see that images 2, 4 and 6 are corrupted in different ways, but the robust dataset class is able to handle those cases accordingly. You can get a summary of the errors encountered during data loading by inspecting the <code>error_logs</code> attribute.</p> In\u00a0[\u00a0]: Copied! <pre># Display the summary of any corrupted or problematic images found during loading.\nrobust_dataset.get_error_summary()\n</pre> # Display the summary of any corrupted or problematic images found during loading. robust_dataset.get_error_summary() <p>The <code>MonitoredDataset</code> class extends <code>RobustFlowerDataset</code> to add monitoring features for dataset access and loading performance.</p> <ul> <li><p>Access Tracking: Each time an image is loaded through <code>__getitem__</code>, the class increments a counter for that image index in <code>self.access_counts</code>. This allows you to track how often each sample is accessed during training or evaluation.</p> </li> <li><p>Load Time Measurement: This class records the time taken to load each image and stores these values in <code>self.load_times</code>. If loading an image takes longer than 1 second, a warning is printed with the image index and load time. This helps identify slow-loading samples that may affect training performance.</p> </li> <li><p>Statistics Reporting: The <code>print_stats()</code> method provides a summary of dataset usage, including:</p> <ul> <li>Total number of images in the dataset</li> <li>Number of unique images accessed</li> <li>Number of errors encountered (inherited from the parent class)</li> <li>Average and maximum image load times</li> <li>A warning if any images were never accessed, with examples</li> </ul> </li> </ul> <p>These monitoring capabilities help you spot bottlenecks, diagnose errors, and ensure the reliability of your data pipeline\u2014making it easier to debug and optimize dataset handling in deep learning workflows.</p> <p>The MonitoredDataset class extends RobustFlowerDataset to add monitoring features for dataset access and loading performance.</p> In\u00a0[\u00a0]: Copied! <pre>class MonitoredDataset(RobustFlowerDataset):\n    \"\"\"\n    Extends a robust dataset class to add performance monitoring.\n\n    This class tracks metrics such as how frequently each image is accessed,\n    how long each access takes, and which images are never loaded. It provides\n    a summary of these statistics to help diagnose data pipeline issues.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initializes the monitored dataset object.\n\n        Args:\n            *args: Variable length argument list passed to the parent class.\n            **kwargs: Arbitrary keyword arguments passed to the parent class.\n        \"\"\"\n        # Initialize the parent class with all provided arguments.\n        super().__init__(*args, **kwargs)\n        # Initialize a dictionary to count how many times each index is accessed.\n        self.access_counts = {}\n        # Initialize a list to store the load time for each access.\n        self.load_times = []\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a sample while monitoring access counts and load times.\n\n        Args:\n            idx (int): The index of the sample to retrieve.\n\n        Returns:\n            tuple: The data sample (e.g., image and label) from the parent class.\n        \"\"\"\n        # Import the time module for timing operations.\n        import time\n        # Record the start time of the operation.\n        start_time = time.time()\n        # Increment the access count for the given index.\n        self.access_counts[idx] = self.access_counts.get(idx, 0) + 1\n        # Call the parent class's method to load the data.\n        result = super().__getitem__(idx)\n        # Calculate the total time taken to load the sample.\n        load_time = time.time() - start_time\n        # Append the calculated load time to the list.\n        self.load_times.append(load_time)\n        # Check if the load time exceeds a certain threshold.\n        if load_time &gt; 1.0:\n            # Print a warning if a slow load time is detected.\n            print(f\"\u26a0\ufe0f Slow load: Image {idx} took {load_time:.2f}s\")\n        # Return the loaded sample from the parent class.\n        return result\n\n    def print_stats(self):\n        \"\"\"\n        Prints a summary of the dataset's access statistics and performance.\n        \"\"\"\n        # Print a header for the statistics report.\n        print(\"\\n=== Pipeline Statistics ===\")\n        # Display the total number of images in the dataset.\n        print(f\"Total images: {len(self)}\")\n        # Display the number of unique images that were accessed.\n        print(f\"Unique images accessed: {len(self.access_counts)}\")\n        # Display the total number of errors logged by the parent class.\n        print(f\"Errors encountered: {len(self.error_logs)}\")\n        # Check if any load times have been recorded.\n        if self.load_times:\n            # Calculate the average load time.\n            avg_time = sum(self.load_times) / len(self.load_times)\n            # Find the maximum (slowest) load time.\n            max_time = max(self.load_times)\n            # Print the average load time in milliseconds.\n            print(f\"Average load time: {avg_time*1000:.1f} ms\")\n            # Print the slowest load time in milliseconds.\n            print(f\"Slowest load: {max_time*1000:.1f} ms\")\n        # Create a set of all possible indices in the dataset.\n        all_indices = set(range(len(self)))\n        # Create a set of all indices that were actually accessed.\n        accessed_indices = set(self.access_counts.keys())\n        # Find the set of indices that were never accessed.\n        never_accessed = all_indices - accessed_indices\n        # Check if there are any images that were never loaded.\n        if never_accessed:\n            # Print a warning message with the count of never-accessed images.\n            print(f\"\\n\u26a0\ufe0f WARNING: {len(never_accessed)} images were never loaded!\")\n            # Show a few examples of the indices that were never accessed.\n            print(f\"   Examples: {list(never_accessed)[:5]}\")\n</pre> class MonitoredDataset(RobustFlowerDataset):     \"\"\"     Extends a robust dataset class to add performance monitoring.      This class tracks metrics such as how frequently each image is accessed,     how long each access takes, and which images are never loaded. It provides     a summary of these statistics to help diagnose data pipeline issues.     \"\"\"     def __init__(self, *args, **kwargs):         \"\"\"         Initializes the monitored dataset object.          Args:             *args: Variable length argument list passed to the parent class.             **kwargs: Arbitrary keyword arguments passed to the parent class.         \"\"\"         # Initialize the parent class with all provided arguments.         super().__init__(*args, **kwargs)         # Initialize a dictionary to count how many times each index is accessed.         self.access_counts = {}         # Initialize a list to store the load time for each access.         self.load_times = []      def __getitem__(self, idx):         \"\"\"         Retrieves a sample while monitoring access counts and load times.          Args:             idx (int): The index of the sample to retrieve.          Returns:             tuple: The data sample (e.g., image and label) from the parent class.         \"\"\"         # Import the time module for timing operations.         import time         # Record the start time of the operation.         start_time = time.time()         # Increment the access count for the given index.         self.access_counts[idx] = self.access_counts.get(idx, 0) + 1         # Call the parent class's method to load the data.         result = super().__getitem__(idx)         # Calculate the total time taken to load the sample.         load_time = time.time() - start_time         # Append the calculated load time to the list.         self.load_times.append(load_time)         # Check if the load time exceeds a certain threshold.         if load_time &gt; 1.0:             # Print a warning if a slow load time is detected.             print(f\"\u26a0\ufe0f Slow load: Image {idx} took {load_time:.2f}s\")         # Return the loaded sample from the parent class.         return result      def print_stats(self):         \"\"\"         Prints a summary of the dataset's access statistics and performance.         \"\"\"         # Print a header for the statistics report.         print(\"\\n=== Pipeline Statistics ===\")         # Display the total number of images in the dataset.         print(f\"Total images: {len(self)}\")         # Display the number of unique images that were accessed.         print(f\"Unique images accessed: {len(self.access_counts)}\")         # Display the total number of errors logged by the parent class.         print(f\"Errors encountered: {len(self.error_logs)}\")         # Check if any load times have been recorded.         if self.load_times:             # Calculate the average load time.             avg_time = sum(self.load_times) / len(self.load_times)             # Find the maximum (slowest) load time.             max_time = max(self.load_times)             # Print the average load time in milliseconds.             print(f\"Average load time: {avg_time*1000:.1f} ms\")             # Print the slowest load time in milliseconds.             print(f\"Slowest load: {max_time*1000:.1f} ms\")         # Create a set of all possible indices in the dataset.         all_indices = set(range(len(self)))         # Create a set of all indices that were actually accessed.         accessed_indices = set(self.access_counts.keys())         # Find the set of indices that were never accessed.         never_accessed = all_indices - accessed_indices         # Check if there are any images that were never loaded.         if never_accessed:             # Print a warning message with the count of never-accessed images.             print(f\"\\n\u26a0\ufe0f WARNING: {len(never_accessed)} images were never loaded!\")             # Show a few examples of the indices that were never accessed.             print(f\"   Examples: {list(never_accessed)[:5]}\") In\u00a0[\u00a0]: Copied! <pre># Initialize the monitored dataset with the path to the potentially corrupted data.\nmonitored_dataset = MonitoredDataset(corrupted_dataset_path)\n\n# Loop through every index in the dataset to simulate a full pass.\n# Iterate through the dataset to trigger monitoring\nfor idx in range(len(monitored_dataset)):\n    # Access the sample at the current index to trigger the monitoring and error-handling logic.\n    img, label = monitored_dataset[idx]\n</pre> # Initialize the monitored dataset with the path to the potentially corrupted data. monitored_dataset = MonitoredDataset(corrupted_dataset_path)  # Loop through every index in the dataset to simulate a full pass. # Iterate through the dataset to trigger monitoring for idx in range(len(monitored_dataset)):     # Access the sample at the current index to trigger the monitoring and error-handling logic.     img, label = monitored_dataset[idx] In\u00a0[\u00a0]: Copied! <pre># Print the statistics\nmonitored_dataset.print_stats()\n</pre> # Print the statistics monitored_dataset.print_stats() <p>The <code>print_stats</code> method provides a comprehensive overview of the dataset's usage and any issues encountered during data loading. This is a good practice to implement in your dataset classes, as it helps you monitor the quality of your data and the performance of your data pipeline.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#data-management","title":"Data Management\u00b6","text":"<p>Welcome to the Data Management lab!</p> <p>In previous labs, you focused on building and training models with well-structured data. However, in the real world, data is rarely perfect. Even the most powerful model architecture can fail if it's fed a messy, inefficient, or unreliable data stream. This is where a robust data pipeline becomes essential.</p> <p>This lab shifts your focus from the model to the data itself, tackling the common challenges of real-world datasets. You'll be working with the Oxford 102 Flowers dataset, a collection of images and labels that are stored in separate files, with inconsistent formatting, and possibly even some corrupted samples. To overcome these hurdles, you will use PyTorch's core data management tools: The <code>Dataset</code> and <code>DataLoader</code> classes.</p> <p>In this lab, you will:</p> <ul> <li>Explore a real-world dataset with unorganized files and separated labels.</li> <li>Build a custom PyTorch <code>Dataset</code> to load and preprocess images and labels on-the-fly.</li> <li>Apply transformations and data augmentation to prepare your data and improve model robustness.</li> <li>Use the <code>DataLoader</code> to efficiently create and shuffle batches for training.</li> <li>Split your data into training, validation, and test sets.</li> <li>Implement error-handling techniques to manage data issues and monitor your pipeline\u2019s performance.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#imports","title":"Imports\u00b6","text":""},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#data-access","title":"Data Access\u00b6","text":"<p>Every deep learning project starts with data, but not all data is ready to use. Sometimes it\u2019s already well organized, but often it\u2019s scattered or stored in formats that aren\u2019t directly compatible with model training. In this section, you\u2019ll learn how to manage data access and follow best practices for working with unorganized or inconsistent datasets.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#dataset-exploration","title":"Dataset Exploration\u00b6","text":"<p>The first step when working with a new dataset is to access and explore its structure. This helps you understand how the data is organized, and therefore, how to load and use it effectively.</p> <ul> <li>You will define a  <code>download_dataset</code> function that downloads the dataset from a given URL and extracts it to a specified directory.</li> <li>In this case, the dataset consists of two files: a <code>.tgz</code>  file with images and a <code>.mat</code> file containing the labels.</li> </ul> <p>Both files will be downloaded using the <code>requests</code> library and then extracted using the <code>tarfile</code> library.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#creating-a-custom-dataset-class","title":"Creating a Custom Dataset Class\u00b6","text":"<p>Having downloaded and briefly explored the dataset, the next step is to create a custom dataset class that organizes access to the data. The goal is to retrieve images and their corresponding labels consistently and efficiently for model training.</p> <p>You will create a class <code>FlowerDataset</code> that inherits from <code>torch.utils.data.Dataset</code>. A custom dataset class must implement the following methods:</p> <p><code>__init__</code>: Initializes the dataset object.</p> <ul> <li>Typically accepts the data path and any transforms to apply.</li> <li>Optionally loads metadata or labels associated with the data.</li> </ul> <p><code>__len__</code>: Returns the number of samples in the dataset.</p> <p><code>__getitem__</code>: Retrieves a single sample given an index.</p> <ul> <li>Loads the image and its label, applies transforms, and returns them.</li> </ul> <p>Additionally, you will define helper methods to make the code cleaner and more organized:</p> <p><code>load_and_correct_labels</code>: Loads image labels from the MATLAB .mat file.</p> <ul> <li>It uses <code>scipy.io.loadmat</code> to read the file and extract the labels array.</li> <li>The labels are adjusted by subtracting 1, which is necessary because MATLAB uses 1-based indexing, while Python uses 0-based indexing.<ul> <li>This correction prevents off-by-one errors during training and evaluation.</li> </ul> </li> </ul> <p><code>retrieve_image</code>: Loads an image from disk given its index <code>idx</code>.</p> <ul> <li>Given <code>idx</code>, it constructs the filename and path, opens the image using the Pillow library, converts it to RGB format (to ensure consistency), and returns the image object.</li> </ul> <p><code>get_label_description</code>: Given a label, returns a human-readable description from the text file.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#overview-of-the-images-in-the-dataset","title":"Overview of the Images in the Dataset\u00b6","text":"<p>Now that you have a working dataset class, you can explore the images in the dataset. You will make use of the <code>visual_exploration</code> function to visualize a few images to get a better understanding of the data.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#quality-problems","title":"Quality Problems\u00b6","text":"<p>As you have seen in the previous section, the size of the images in the dataset varies significantly. This can be a problem when training a model, as most models expect input images to have the same size. To address this issue, you will implement a series of transformations to standardize the size of the images.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#transformations","title":"Transformations\u00b6","text":"<p>After exploring the dataset and identifying potential quality issues, the next step is to apply transformations to preprocess the images. Transformations are operations that modify images to prepare them for model training. Common examples include resizing, cropping, flipping, and normalizing pixel values. The <code>torchvision.transforms</code> module provides a variety of pre-defined transformations that can be easily applied to images.</p> <p>You\u2019ll start by defining a simple transformation pipeline composed of two stages:</p> <p>Transformations applied directly to the raw images:</p> <ul> <li><code>Resize((256, 256))</code>: Resizes each image to a fixed size of 256\u00d7256 pixels.</li> <li><code>CenterCrop(224)</code>: Crops the center of the image to 224\u00d7224 pixels.</li> </ul> <p>Transformations that convert and standardize images:</p> <ul> <li><code>ToTensor()</code>: converts a PIL Image or NumPy array to a tensor and scales pixel values to the range [0, 1].</li> <li><code>Normalize(mean, std)</code>: normalizes the tensor using the specified mean and standard deviation.</li> </ul> <p>Note: The order of transformations often matters. Resizing and cropping should be applied before converting the image to a tensor.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#data-loading","title":"Data Loading\u00b6","text":"<p>In this section, you\u2019ll learn how to prepare and serve data efficiently for model training in PyTorch. A full training process typically includes three stages: training, validation, and evaluation.</p> <p>During training, the model learns from the training data by adjusting its weights based on the loss function. During validation, the model is evaluated on a separate dataset to tune hyperparameters and prevent overfitting. During evaluation, the model\u2019s performance is tested on unseen data to assess its generalization ability.</p> <p>Each of these stages requires splitting the dataset into distinct subsets and using data loaders to feed data efficiently during training and testing. In what follows, you will see how to do this using PyTorch.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#splitting-the-dataset","title":"Splitting the Dataset\u00b6","text":"<p>The <code>split_dataset</code> function will divide the dataset into training, validation, and test sets. To do this, you will:</p> <ul> <li>Define the sizes of each subset from the given fractions for validation and test,</li> <li>Make use of <code>random_split</code> from <code>torch.utils.data</code> to perform the actual split.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#dataloaders","title":"Dataloaders\u00b6","text":"<p>Once your dataset is split into training, validation, and test sets, the next step is to serve data efficiently to your model during training and evaluation. PyTorch\u2019s <code>DataLoader</code> class handles this by batching samples, shuffling data during training, and simplifying iteration over your dataset. Using dataloaders helps speed up training and ensures your model sees a diverse mix of samples in each batch. Now you will create dataloaders for each subset of the data (training, validation, and test). You will use the <code>DataLoader</code> class from <code>torch.utils.data</code> to create dataloaders. The arguments you will provide include:</p> <ul> <li><code>dataset</code>: The dataset to load data from (e.g., <code>train_dataset</code>, <code>val_dataset</code>, <code>test_dataset</code>),</li> <li><code>batch_size</code>: The number of samples per batch to load (e.g., <code>32</code>),</li> <li><code>shuffle</code>: Whether to shuffle the data at every epoch (typically <code>True</code> for training and <code>False</code> for validation and test).</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#augmentation","title":"Augmentation\u00b6","text":"<p>Data augmentation is an important technique for improving the robustness and generalization of deep learning models. By applying random transformations, such as flipping, rotating, or adjusting brightness, to training images, augmentation helps models recognize objects under varied real-world conditions.</p> <p>In PyTorch, augmentation can be performed \"on-the-fly,\" generating endless image variations without extra storage.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#splitting-with-augmentation","title":"Splitting with Augmentation\u00b6","text":"<p>Integrating proper random splitting with data augmentation requires careful handling to ensure that the training, validation, and test sets are distinct and that augmentation is applied only to the training data.</p> <p>One subtle point to consider is that when using <code>random_split</code>, each of the splits is a <code>Subset</code> object that references the original dataset. Therefore those subsets will inherit the transformations defined in the original dataset and can not be assigned different transformations directly.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#robust-datasets","title":"Robust Datasets\u00b6","text":"<p>In real-world projects, data pipelines must be resilient to unexpected issues such as corrupted files, inconsistent image formats, or problematic samples that can crash training runs. This section introduces robust dataset design strategies.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#error-handling","title":"Error Handling\u00b6","text":"<p>You will create a new custom dataset class <code>RobustFlowerDataset</code> that extends the original <code>FlowerDataset</code> class to handle potential issues with corrupted or problematic images.</p> <p>The <code>__init__</code>, <code>__len__</code> and <code>load_and_correct_labels</code> methods remain mostly the same as in the original <code>FlowerDataset</code> class.</p> <p>The main differences are:</p> <p><code>__getitem__</code>: Implements a try-except block to catch exceptions that may occur when loading an image.</p> <ul> <li>If an exception occurs (e.g., due to a corrupted image file), the code will print and log the error by calling the <code>log_error</code> method.</li> <li>It then attempts to retrieve the next image in the dataset instead of stopping execution.</li> </ul> <p><code>retrieve_image</code>: Handles the actual image loading process.</p> <ul> <li>Constructs the filename and path, verifies image integrity using Pillow\u2019s <code>verify()</code> method, and reloads the image to ensure it is fully loaded into memory.</li> <li>Checks the image size, raising an error if it\u2019s smaller than 32 pixels in either dimension, and converts grayscale images to RGB for consistency.</li> </ul> <p><code>get_error_summary</code>: Provides a summary of all errors encountered during data loading, useful for debugging and assessing dataset quality.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#tracking-errors","title":"Tracking Errors\u00b6","text":"<p>In real-world deep learning projects, data pipelines need to be robust not only to corrupted files but also to subtle issues that can silently degrade model performance. Beyond handling exceptions, it\u2019s important to systematically track and analyze the errors and anomalies that occur during data loading and preprocessing.</p> <p>This final section introduces practical strategies for error tracking within your dataset pipeline. You\u2019ll learn how to log problematic samples, monitor which images are accessed (and how often), and review error summaries after training. These techniques help ensure that your pipeline is resilient, transparent, and production-ready, allowing you to detect and resolve data quality issues before they affect your model\u2019s results.</p>"},{"location":"tools/pytorch/Examples/C1_M3_Lab_data_management/#conclusion","title":"Conclusion\u00b6","text":"<p>Congratulations! You\u2019ve now built a complete, production-ready data pipeline in PyTorch. In this lab, you moved beyond working with perfectly curated data and learned how to manage the real-world complexities of data handling\u2014from initial access to efficient batching.</p> <p>You saw how to organize a messy dataset by creating a custom <code>Dataset</code> class to load images and labels from separate files. You applied essential preprocessing steps, including transformations to normalize your data and data augmentation to make your future model more robust.</p> <p>You then used  <code>DataLoader</code> to prepare your data by batching and shuffling it for training. Finally, you made your pipeline more reliable by implementing error handling and performance monitoring, ensuring that your model\u2019s training process is both stable and efficient.</p> <p>You\u2019ve built a strong foundation in data management that will support everything you do next.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/","title":"Building a CNN for Nature Classification","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport helper_utils\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as transforms from torch.utils.data import DataLoader  import helper_utils In\u00a0[2]: Copied! <pre># Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n</pre> # Device configuration device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> In\u00a0[3]: Copied! <pre>cifar100_mean = (0.5071, 0.4867, 0.4408)\ncifar100_std = (0.2675, 0.2565, 0.2761)\n</pre> cifar100_mean = (0.5071, 0.4867, 0.4408) cifar100_std = (0.2675, 0.2565, 0.2761) <ul> <li>Define two separate pipelines using <code>transforms.Compose</code>.<ul> <li>One for the training set that includes data augmentation and another for the validation set.</li> </ul> </li> </ul> In\u00a0[4]: Copied! <pre># Training set transformation pipeline\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(cifar100_mean, cifar100_std)\n])\n\n# Validation set transformation pipeline\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(cifar100_mean, cifar100_std)\n])\n</pre> # Training set transformation pipeline train_transform = transforms.Compose([     transforms.RandomHorizontalFlip(),     transforms.RandomRotation(15),     transforms.ToTensor(),     transforms.Normalize(cifar100_mean, cifar100_std) ])  # Validation set transformation pipeline val_transform = transforms.Compose([     transforms.ToTensor(),     transforms.Normalize(cifar100_mean, cifar100_std) ]) <ul> <li>Create a Python list containing the names of the 9 classes you'll use for the initial prototype.</li> </ul> In\u00a0[5]: Copied! <pre>subset_target_classes = [\n    # Flowers\n    'orchid', 'poppy', 'sunflower',\n    # Mammals\n    'fox', 'raccoon', 'skunk',\n    # Insects\n    'butterfly', 'caterpillar', 'cockroach'\n]\n</pre> subset_target_classes = [     # Flowers     'orchid', 'poppy', 'sunflower',     # Mammals     'fox', 'raccoon', 'skunk',     # Insects     'butterfly', 'caterpillar', 'cockroach' ] <ul> <li>Use the <code>load_cifar100_subset</code> helper function, passing in your <code>subset_target_classes</code> list and both transformation pipelines.</li> <li>This function handles the entire loading process: it downloads the full CIFAR-100 dataset, applies your specified transformations, and then filters the result to include only the 9 classes you selected.</li> <li>It returns the final training and validation dataset objects, ready for the next step.</li> </ul> In\u00a0[6]: Copied! <pre># Call the helper function to prepare the datasets\ntrain_dataset_proto, val_dataset_proto = helper_utils.load_cifar100_subset(subset_target_classes, train_transform, val_transform)\n</pre> # Call the helper function to prepare the datasets train_dataset_proto, val_dataset_proto = helper_utils.load_cifar100_subset(subset_target_classes, train_transform, val_transform) <pre>Dataset found in './cifar_100'. Loading from local files.\nDataset loaded successfully.\n\nFiltering for 9 classes...\nFiltering complete. Returning training and validation datasets.\n</pre> <ul> <li>With your <code>Dataset</code> objects ready, the final step in the data pipeline is to create <code>DataLoaders</code>.</li> </ul> In\u00a0[7]: Copied! <pre># Set the number of samples to be processed in each batch\nbatch_size = 64\n\n# Create a data loader for the training set, with shuffling enabled\ntrain_loader_proto = DataLoader(train_dataset_proto, batch_size=batch_size, shuffle=True)\n\n# Create a data loader for the validation set, without shuffling\nval_loader_proto = DataLoader(val_dataset_proto, batch_size=batch_size, shuffle=False)\n</pre> # Set the number of samples to be processed in each batch batch_size = 64  # Create a data loader for the training set, with shuffling enabled train_loader_proto = DataLoader(train_dataset_proto, batch_size=batch_size, shuffle=True)  # Create a data loader for the validation set, without shuffling val_loader_proto = DataLoader(val_dataset_proto, batch_size=batch_size, shuffle=False) In\u00a0[8]: Copied! <pre># Visualize a 3x3 grid of random training images\nhelper_utils.visualise_images(train_dataset_proto, grid=(3, 3))\n</pre> # Visualize a 3x3 grid of random training images helper_utils.visualise_images(train_dataset_proto, grid=(3, 3)) In\u00a0[9]: Copied! <pre>class SimpleCNN(nn.Module):\n    \"\"\"\n    A simple Convolutional Neural Network model.\n\n    The architecture consists of three convolutional blocks followed by two\n    fully connected layers for classification.\n    \"\"\"\n    def __init__(self, num_classes):\n        \"\"\"\n        Initializes the layers of the neural network.\n\n        Args:\n            num_classes: The number of output classes for the final layer.\n        \"\"\"\n        # Call the constructor of the parent class (nn.Module)\n        super(SimpleCNN, self).__init__()\n        \n        # Define the first convolutional block\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Define the second convolutional block\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Define the third convolutional block\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU()\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Define the layer to flatten the feature maps\n        self.flatten = nn.Flatten()\n\n        # Define the fully connected (dense) layers\n        # Input image is 32x32, after 3 pooling layers: 4x4\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.relu4 = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(512, num_classes)\n\n\n    def forward(self, x):\n        \"\"\"\n        Defines the forward pass of the model.\n\n        Args:\n            x: The input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            The output tensor containing the logits for each class.\n        \"\"\"\n        # Pass input through the first convolutional block\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n\n        # Pass feature maps through the second convolutional block\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        \n        # Pass feature maps through the third convolutional block\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.pool3(x)\n\n        # Flatten the output for the fully connected layers\n        x = self.flatten(x)\n\n        # Pass the flattened features through the fully connected layers\n        x = self.fc1(x)\n        x = self.relu4(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        # Return the final output logits\n        return x\n</pre> class SimpleCNN(nn.Module):     \"\"\"     A simple Convolutional Neural Network model.      The architecture consists of three convolutional blocks followed by two     fully connected layers for classification.     \"\"\"     def __init__(self, num_classes):         \"\"\"         Initializes the layers of the neural network.          Args:             num_classes: The number of output classes for the final layer.         \"\"\"         # Call the constructor of the parent class (nn.Module)         super(SimpleCNN, self).__init__()                  # Define the first convolutional block         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)         self.relu1 = nn.ReLU()         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                  # Define the second convolutional block         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)         self.relu2 = nn.ReLU()         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                  # Define the third convolutional block         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)         self.relu3 = nn.ReLU()         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)                  # Define the layer to flatten the feature maps         self.flatten = nn.Flatten()          # Define the fully connected (dense) layers         # Input image is 32x32, after 3 pooling layers: 4x4         self.fc1 = nn.Linear(128 * 4 * 4, 512)         self.relu4 = nn.ReLU()         self.dropout = nn.Dropout(0.5)         self.fc2 = nn.Linear(512, num_classes)       def forward(self, x):         \"\"\"         Defines the forward pass of the model.          Args:             x: The input tensor of shape (batch_size, channels, height, width).          Returns:             The output tensor containing the logits for each class.         \"\"\"         # Pass input through the first convolutional block         x = self.conv1(x)         x = self.relu1(x)         x = self.pool1(x)          # Pass feature maps through the second convolutional block         x = self.conv2(x)         x = self.relu2(x)         x = self.pool2(x)                  # Pass feature maps through the third convolutional block         x = self.conv3(x)         x = self.relu3(x)         x = self.pool3(x)          # Flatten the output for the fully connected layers         x = self.flatten(x)          # Pass the flattened features through the fully connected layers         x = self.fc1(x)         x = self.relu4(x)         x = self.dropout(x)         x = self.fc2(x)          # Return the final output logits         return x <p>With the <code>SimpleCNN</code> architecture defined, the next step is to create an instance of the model for your prototype.</p> <ul> <li>First, dynamically determine the number of output classes by checking the length of the class list in your <code>train_dataset_proto</code>.</li> <li>Create an instance of your <code>SimpleCNN</code>, passing <code>num_classes</code> to its constructor.</li> </ul> In\u00a0[10]: Copied! <pre># Get the number of classes\nnum_classes = len(train_dataset_proto.classes)\n\n# Instantiate the model\nprototype_model = SimpleCNN(num_classes)\n</pre> # Get the number of classes num_classes = len(train_dataset_proto.classes)  # Instantiate the model prototype_model = SimpleCNN(num_classes) <p>Before you start training, it's very helpful to visualize how the shape of your data changes as it flows through the CNN. This will confirm that your architecture is set up correctly and show you how the spatial dimensions shrink while the number of channels grows with each convolutional block.</p> <ul> <li>Define the <code>print_data_flow</code> helper function.<ul> <li>This function will pass a sample 32x32 color image through your model, layer by layer, printing the tensor's shape at each key step to trace its journey from input to final prediction.</li> </ul> </li> </ul> In\u00a0[11]: Copied! <pre>def print_data_flow(model):\n    \"\"\"\n    Prints the shape of a tensor as it flows through each layer of the model.\n\n    Args:\n        model: An instance of the PyTorch model to inspect.\n    \"\"\"\n    # Create a sample input tensor (batch_size, channels, height, width)\n    x = torch.randn(1, 3, 32, 32)\n\n    # Track the tensor shape at each stage\n    print(f\"Input shape: \\t\\t{x.shape}\")\n\n    # First conv block\n    x = model.conv1(x)\n    print(f\"After conv1: \\t\\t{x.shape}\")\n    x = model.relu1(x)\n    x = model.pool1(x)\n    print(f\"After pool1: \\t\\t{x.shape}\")\n\n    # Second conv block\n    x = model.conv2(x)\n    print(f\"After conv2: \\t\\t{x.shape}\")\n    x = model.relu2(x)\n    x = model.pool2(x)\n    print(f\"After pool2: \\t\\t{x.shape}\")\n\n    # Third conv block\n    x = model.conv3(x)\n    print(f\"After conv3: \\t\\t{x.shape}\")\n    x = model.relu3(x)\n    x = model.pool3(x)\n    print(f\"After pool3: \\t\\t{x.shape}\")\n\n    # Flatten using the model's flatten layer\n    x = model.flatten(x)\n    print(f\"After flatten: \\t\\t{x.shape}\")\n\n    # Fully connected layers\n    x = model.fc1(x)\n    print(f\"After fc1: \\t\\t{x.shape}\")\n    x = model.relu4(x)\n    x = model.dropout(x)\n    x = model.fc2(x)\n    print(f\"Output shape (fc2): \\t{x.shape}\")\n</pre> def print_data_flow(model):     \"\"\"     Prints the shape of a tensor as it flows through each layer of the model.      Args:         model: An instance of the PyTorch model to inspect.     \"\"\"     # Create a sample input tensor (batch_size, channels, height, width)     x = torch.randn(1, 3, 32, 32)      # Track the tensor shape at each stage     print(f\"Input shape: \\t\\t{x.shape}\")      # First conv block     x = model.conv1(x)     print(f\"After conv1: \\t\\t{x.shape}\")     x = model.relu1(x)     x = model.pool1(x)     print(f\"After pool1: \\t\\t{x.shape}\")      # Second conv block     x = model.conv2(x)     print(f\"After conv2: \\t\\t{x.shape}\")     x = model.relu2(x)     x = model.pool2(x)     print(f\"After pool2: \\t\\t{x.shape}\")      # Third conv block     x = model.conv3(x)     print(f\"After conv3: \\t\\t{x.shape}\")     x = model.relu3(x)     x = model.pool3(x)     print(f\"After pool3: \\t\\t{x.shape}\")      # Flatten using the model's flatten layer     x = model.flatten(x)     print(f\"After flatten: \\t\\t{x.shape}\")      # Fully connected layers     x = model.fc1(x)     print(f\"After fc1: \\t\\t{x.shape}\")     x = model.relu4(x)     x = model.dropout(x)     x = model.fc2(x)     print(f\"Output shape (fc2): \\t{x.shape}\") <p>You can now print a summary of your model and trace the data flow to see it in action.</p> <ul> <li><p>Call your helper function to print the tensor's shape at each step.</p> <ul> <li><p>The tensor starts as a <code>(1, 3, 32, 32)</code> image. As it passes through the <code>conv</code> and <code>pool</code> blocks, the number of channels increases while the spatial size is halved at each step.</p> </li> <li><p>The final <code>(1, 128, 4, 4)</code> feature map is flattened into a 1D vector to be processed by the linear layers. The model's final output shape is <code>(1, 9)</code>, providing one score for each of the 9 classes.</p> </li> </ul> </li> </ul> In\u00a0[12]: Copied! <pre># Print the model's architecture\nprint(prototype_model)\n\n# Call the helper function to visualize the data flow\nprint(\"\\n--- Tracing Data Flow ---\")\nprint_data_flow(prototype_model)\n</pre> # Print the model's architecture print(prototype_model)  # Call the helper function to visualize the data flow print(\"\\n--- Tracing Data Flow ---\") print_data_flow(prototype_model) <pre>SimpleCNN(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu3): ReLU()\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n  (relu4): ReLU()\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=512, out_features=9, bias=True)\n)\n\n--- Tracing Data Flow ---\nInput shape: \t\ttorch.Size([1, 3, 32, 32])\nAfter conv1: \t\ttorch.Size([1, 32, 32, 32])\nAfter pool1: \t\ttorch.Size([1, 32, 16, 16])\nAfter conv2: \t\ttorch.Size([1, 64, 16, 16])\nAfter pool2: \t\ttorch.Size([1, 64, 8, 8])\nAfter conv3: \t\ttorch.Size([1, 128, 8, 8])\nAfter pool3: \t\ttorch.Size([1, 128, 4, 4])\nAfter flatten: \t\ttorch.Size([1, 2048])\nAfter fc1: \t\ttorch.Size([1, 512])\nOutput shape (fc2): \ttorch.Size([1, 9])\n</pre> In\u00a0[13]: Copied! <pre># Loss function\nloss_function = nn.CrossEntropyLoss()\n\n# Optimizer for the prototype model\noptimizer_prototype = optim.Adam(prototype_model.parameters(), lr=0.001)\n</pre> # Loss function loss_function = nn.CrossEntropyLoss()  # Optimizer for the prototype model optimizer_prototype = optim.Adam(prototype_model.parameters(), lr=0.001) In\u00a0[14]: Copied! <pre>def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):\n    \"\"\"\n    Trains and validates a PyTorch neural network model.\n\n    Args:\n        model: The neural network model to be trained.\n        train_loader: DataLoader for the training dataset.\n        val_loader: DataLoader for the validation dataset.\n        loss_function: The loss function to use for training.\n        optimizer: The optimization algorithm.\n        num_epochs: The total number of epochs to train for.\n        device: The device (e.g., 'cpu' or 'cuda') to run the training on.\n\n    Returns:\n        A tuple containing:\n        - The trained model.\n        - A list of metrics [train_losses, val_losses, val_accuracies].\n    \"\"\"\n    # Move the model to the specified device (CPU or GPU)\n    model.to(device)\n    \n    # Initialize lists to store training and validation metrics\n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    \n    # Print a message indicating the start of the training process\n    print(\"--- Training Started ---\")\n    \n    # Loop over the specified number of epochs\n    for epoch in range(num_epochs):\n        # Set the model to training mode\n        model.train()\n        # Initialize running loss for the current epoch\n        running_loss = 0.0\n        # Iterate over batches of data in the training loader\n        for images, labels in train_loader:\n            # Move images and labels to the specified device\n            images, labels = images.to(device), labels.to(device)\n            \n            # Clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # Perform a forward pass to get model outputs\n            outputs = model(images)\n            # Calculate the loss\n            loss = loss_function(outputs, labels)\n            # Perform a backward pass to compute gradients\n            loss.backward()\n            # Update the model parameters\n            optimizer.step()\n            \n            # Accumulate the training loss for the batch\n            running_loss += loss.item() * images.size(0)\n            \n        # Calculate the average training loss for the epoch\n        epoch_loss = running_loss / len(train_loader.dataset)\n        # Append the epoch loss to the list of training losses\n        train_losses.append(epoch_loss)\n        \n        # Set the model to evaluation mode\n        model.eval()\n        # Initialize running validation loss and correct predictions count\n        running_val_loss = 0.0\n        correct = 0\n        total = 0\n        # Disable gradient calculations for validation\n        with torch.no_grad():\n            # Iterate over batches of data in the validation loader\n            for images, labels in val_loader:\n                # Move images and labels to the specified device\n                images, labels = images.to(device), labels.to(device)\n                \n                # Perform a forward pass to get model outputs\n                outputs = model(images)\n                \n                # Calculate the validation loss for the batch\n                val_loss = loss_function(outputs, labels)\n                # Accumulate the validation loss\n                running_val_loss += val_loss.item() * images.size(0)\n                \n                # Get the predicted class labels\n                _, predicted = torch.max(outputs, 1)\n                # Update the total number of samples\n                total += labels.size(0)\n                # Update the number of correct predictions\n                correct += (predicted == labels).sum().item()\n                \n        # Calculate the average validation loss for the epoch\n        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n        # Append the epoch validation loss to the list\n        val_losses.append(epoch_val_loss)\n        \n        # Calculate the validation accuracy for the epoch\n        epoch_accuracy = 100.0 * correct / total\n        # Append the epoch accuracy to the list\n        val_accuracies.append(epoch_accuracy)\n        \n        # Print the metrics for the current epoch\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")\n        \n    # Print a message indicating the end of the training process\n    print(\"--- Finished Training ---\")\n    \n    # Consolidate all metrics into a single list\n    metrics = [train_losses, val_losses, val_accuracies]\n    \n    # Return the trained model and the collected metrics\n    return model, metrics\n</pre> def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):     \"\"\"     Trains and validates a PyTorch neural network model.      Args:         model: The neural network model to be trained.         train_loader: DataLoader for the training dataset.         val_loader: DataLoader for the validation dataset.         loss_function: The loss function to use for training.         optimizer: The optimization algorithm.         num_epochs: The total number of epochs to train for.         device: The device (e.g., 'cpu' or 'cuda') to run the training on.      Returns:         A tuple containing:         - The trained model.         - A list of metrics [train_losses, val_losses, val_accuracies].     \"\"\"     # Move the model to the specified device (CPU or GPU)     model.to(device)          # Initialize lists to store training and validation metrics     train_losses = []     val_losses = []     val_accuracies = []          # Print a message indicating the start of the training process     print(\"--- Training Started ---\")          # Loop over the specified number of epochs     for epoch in range(num_epochs):         # Set the model to training mode         model.train()         # Initialize running loss for the current epoch         running_loss = 0.0         # Iterate over batches of data in the training loader         for images, labels in train_loader:             # Move images and labels to the specified device             images, labels = images.to(device), labels.to(device)                          # Clear the gradients of all optimized variables             optimizer.zero_grad()             # Perform a forward pass to get model outputs             outputs = model(images)             # Calculate the loss             loss = loss_function(outputs, labels)             # Perform a backward pass to compute gradients             loss.backward()             # Update the model parameters             optimizer.step()                          # Accumulate the training loss for the batch             running_loss += loss.item() * images.size(0)                      # Calculate the average training loss for the epoch         epoch_loss = running_loss / len(train_loader.dataset)         # Append the epoch loss to the list of training losses         train_losses.append(epoch_loss)                  # Set the model to evaluation mode         model.eval()         # Initialize running validation loss and correct predictions count         running_val_loss = 0.0         correct = 0         total = 0         # Disable gradient calculations for validation         with torch.no_grad():             # Iterate over batches of data in the validation loader             for images, labels in val_loader:                 # Move images and labels to the specified device                 images, labels = images.to(device), labels.to(device)                                  # Perform a forward pass to get model outputs                 outputs = model(images)                                  # Calculate the validation loss for the batch                 val_loss = loss_function(outputs, labels)                 # Accumulate the validation loss                 running_val_loss += val_loss.item() * images.size(0)                                  # Get the predicted class labels                 _, predicted = torch.max(outputs, 1)                 # Update the total number of samples                 total += labels.size(0)                 # Update the number of correct predictions                 correct += (predicted == labels).sum().item()                          # Calculate the average validation loss for the epoch         epoch_val_loss = running_val_loss / len(val_loader.dataset)         # Append the epoch validation loss to the list         val_losses.append(epoch_val_loss)                  # Calculate the validation accuracy for the epoch         epoch_accuracy = 100.0 * correct / total         # Append the epoch accuracy to the list         val_accuracies.append(epoch_accuracy)                  # Print the metrics for the current epoch         print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")              # Print a message indicating the end of the training process     print(\"--- Finished Training ---\")          # Consolidate all metrics into a single list     metrics = [train_losses, val_losses, val_accuracies]          # Return the trained model and the collected metrics     return model, metrics <p>With all the components in place, you're ready to start training.</p> <ul> <li>Run the <code>training_loop</code> function with your prototype model (for 9 classes), its corresponding data loaders, the loss function, and the optimizer.</li> <li>You'll train for <code>15 epochs</code>, and the function will return the trained model along with the collected performance metrics.</li> <li>After training is complete, you'll use the <code>plot_training_metrics</code> helper function to visualize the training and validation loss, along with the validation accuracy.</li> </ul> In\u00a0[15]: Copied! <pre># Start the training process by calling the training loop function\ntrained_proto_model, training_metrics_proto = training_loop(\n    model=prototype_model, \n    train_loader=train_loader_proto, \n    val_loader=val_loader_proto, \n    loss_function=loss_function, \n    optimizer=optimizer_prototype, \n    num_epochs=15, \n    device=device\n)\n\n# Visualize the training metrics (loss and accuracy)\nprint(\"\\n--- Training Plots ---\\n\")\nhelper_utils.plot_training_metrics(training_metrics_proto)\n</pre> # Start the training process by calling the training loop function trained_proto_model, training_metrics_proto = training_loop(     model=prototype_model,      train_loader=train_loader_proto,      val_loader=val_loader_proto,      loss_function=loss_function,      optimizer=optimizer_prototype,      num_epochs=15,      device=device )  # Visualize the training metrics (loss and accuracy) print(\"\\n--- Training Plots ---\\n\") helper_utils.plot_training_metrics(training_metrics_proto) <pre>--- Training Started ---\nEpoch [1/15], Train Loss: 1.5991, Val Loss: 1.2501, Val Accuracy: 52.78%\nEpoch [2/15], Train Loss: 1.1933, Val Loss: 1.0173, Val Accuracy: 64.56%\nEpoch [3/15], Train Loss: 1.0468, Val Loss: 1.0142, Val Accuracy: 65.00%\nEpoch [4/15], Train Loss: 0.9488, Val Loss: 0.8790, Val Accuracy: 70.11%\nEpoch [5/15], Train Loss: 0.8565, Val Loss: 0.8224, Val Accuracy: 72.22%\nEpoch [6/15], Train Loss: 0.8153, Val Loss: 0.9911, Val Accuracy: 67.22%\nEpoch [7/15], Train Loss: 0.7519, Val Loss: 0.7471, Val Accuracy: 74.89%\nEpoch [8/15], Train Loss: 0.7205, Val Loss: 0.6986, Val Accuracy: 75.78%\nEpoch [9/15], Train Loss: 0.6686, Val Loss: 0.7398, Val Accuracy: 75.00%\nEpoch [10/15], Train Loss: 0.6244, Val Loss: 0.6902, Val Accuracy: 75.67%\nEpoch [11/15], Train Loss: 0.5797, Val Loss: 0.6727, Val Accuracy: 78.89%\nEpoch [12/15], Train Loss: 0.5460, Val Loss: 0.7393, Val Accuracy: 76.56%\nEpoch [13/15], Train Loss: 0.5180, Val Loss: 0.6468, Val Accuracy: 79.89%\nEpoch [14/15], Train Loss: 0.4806, Val Loss: 0.6700, Val Accuracy: 77.44%\nEpoch [15/15], Train Loss: 0.4388, Val Loss: 0.7010, Val Accuracy: 78.33%\n--- Finished Training ---\n\n--- Training Plots ---\n\n</pre> <p>Excellent work! The prototype model is trained, and the results look very promising. Achieving a validation accuracy of over 75% on the 9-class subset is a great result and confirms that your CNN architecture is well-suited for this task.</p> <p>This successful prototype gives you the green light to move forward with the next phase: training a full-scale model on all 15 classes for the butterfly house. But before you do, it's helpful to perform one last qualitative check to see how your model \"thinks.\"</p> In\u00a0[16]: Copied! <pre># Visualize model predictions on a sample of validation images\nhelper_utils.visualise_predictions(\n    model=trained_proto_model, \n    data_loader=val_loader_proto, \n    device=device, \n    grid=(3, 3)\n)\n</pre> # Visualize model predictions on a sample of validation images helper_utils.visualise_predictions(     model=trained_proto_model,      data_loader=val_loader_proto,      device=device,      grid=(3, 3) ) In\u00a0[17]: Copied! <pre># Define the full class list.\nall_target_classes = [\n    # Flowers\n    'orchid', 'poppy', 'rose', 'sunflower', 'tulip',\n    # Mammals\n    'fox', 'porcupine', 'possum', 'raccoon', 'skunk',\n    # Insects\n    'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'\n]\n\n# Load the full datasets.\ntrain_dataset, val_dataset = helper_utils.load_cifar100_subset(all_target_classes, train_transform, val_transform)\n</pre> # Define the full class list. all_target_classes = [     # Flowers     'orchid', 'poppy', 'rose', 'sunflower', 'tulip',     # Mammals     'fox', 'porcupine', 'possum', 'raccoon', 'skunk',     # Insects     'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach' ]  # Load the full datasets. train_dataset, val_dataset = helper_utils.load_cifar100_subset(all_target_classes, train_transform, val_transform) <pre>Dataset found in './cifar_100'. Loading from local files.\nDataset loaded successfully.\n\nFiltering for 15 classes...\nFiltering complete. Returning training and validation datasets.\n</pre> <ul> <li>Wrap your new 15-class datasets in <code>DataLoader</code> instances, using the same <code>batch_size=64</code>.</li> </ul> In\u00a0[18]: Copied! <pre># Create a data loader for the training set, with shuffling enabled\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Create a data loader for the validation set, without shuffling\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n</pre> # Create a data loader for the training set, with shuffling enabled train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Create a data loader for the validation set, without shuffling val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) <ul> <li>Display a sample of images from your new 15-class training set to confirm it has been loaded correctly.</li> </ul> In\u00a0[19]: Copied! <pre># Visualize a 3x5 grid of random training images\nhelper_utils.visualise_images(train_dataset, grid=(3, 5))\n</pre> # Visualize a 3x5 grid of random training images helper_utils.visualise_images(train_dataset, grid=(3, 5)) <ul> <li>Create a new instance of your <code>SimpleCNN</code> model, this time configured for all 15 classes.</li> </ul> In\u00a0[20]: Copied! <pre># Get the number of classes\nnum_classes = len(train_dataset.classes)\n\n# Instantiate the full model\nmodel = SimpleCNN(num_classes)\n\n# Print the model's architecture (notice, it now has 15 output classes)\nprint(model)\n</pre> # Get the number of classes num_classes = len(train_dataset.classes)  # Instantiate the full model model = SimpleCNN(num_classes)  # Print the model's architecture (notice, it now has 15 output classes) print(model) <pre>SimpleCNN(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu3): ReLU()\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n  (relu4): ReLU()\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=512, out_features=15, bias=True)\n)\n</pre> <ul> <li>Create a new <code>Adam</code> optimizer for your full 15-class model.</li> </ul> In\u00a0[21]: Copied! <pre># Optimizer for the full model\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n</pre> # Optimizer for the full model optimizer = optim.Adam(model.parameters(), lr=0.001) <ul> <li>Call the <code>training_loop</code> to train your 15-class model for <code>25 epochs</code>. The <code>plot_training_metrics</code> function will then immediately visualize the loss and accuracy curves from this final training run.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Start the training process for the full model on all 15 classes\ntrained_model, training_metrics = training_loop(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    loss_function=loss_function, \n    optimizer=optimizer, \n    num_epochs=25, \n    device=device\n)\n\n# Visualize the training metrics for the full model\nprint(\"\\n--- Training Plots ---\\n\")\nhelper_utils.plot_training_metrics(training_metrics)\n</pre> # Start the training process for the full model on all 15 classes trained_model, training_metrics = training_loop(     model=model,      train_loader=train_loader,      val_loader=val_loader,      loss_function=loss_function,      optimizer=optimizer,      num_epochs=25,      device=device )  # Visualize the training metrics for the full model print(\"\\n--- Training Plots ---\\n\") helper_utils.plot_training_metrics(training_metrics) <pre>--- Training Started ---\nEpoch [1/25], Train Loss: 2.2245, Val Loss: 1.8269, Val Accuracy: 37.40%\nEpoch [2/25], Train Loss: 1.8091, Val Loss: 1.7099, Val Accuracy: 40.87%\nEpoch [3/25], Train Loss: 1.6538, Val Loss: 1.4649, Val Accuracy: 50.93%\nEpoch [4/25], Train Loss: 1.5331, Val Loss: 1.3664, Val Accuracy: 53.13%\nEpoch [5/25], Train Loss: 1.4460, Val Loss: 1.3160, Val Accuracy: 55.60%\nEpoch [6/25], Train Loss: 1.3652, Val Loss: 1.3082, Val Accuracy: 56.93%\nEpoch [7/25], Train Loss: 1.3003, Val Loss: 1.2124, Val Accuracy: 57.87%\nEpoch [8/25], Train Loss: 1.2166, Val Loss: 1.1685, Val Accuracy: 59.93%\nEpoch [9/25], Train Loss: 1.1706, Val Loss: 1.1420, Val Accuracy: 61.73%\nEpoch [10/25], Train Loss: 1.1178, Val Loss: 1.1394, Val Accuracy: 61.80%\nEpoch [11/25], Train Loss: 1.0708, Val Loss: 1.0840, Val Accuracy: 64.13%\nEpoch [12/25], Train Loss: 1.0238, Val Loss: 1.1512, Val Accuracy: 62.00%\nEpoch [13/25], Train Loss: 0.9812, Val Loss: 1.0564, Val Accuracy: 64.47%\nEpoch [14/25], Train Loss: 0.9448, Val Loss: 1.0666, Val Accuracy: 63.87%\nEpoch [15/25], Train Loss: 0.9100, Val Loss: 1.0919, Val Accuracy: 62.60%\nEpoch [16/25], Train Loss: 0.8688, Val Loss: 1.0477, Val Accuracy: 65.47%\nEpoch [17/25], Train Loss: 0.8432, Val Loss: 1.0485, Val Accuracy: 65.13%\n</pre> <p>After training the full model, you can analyze the results. But wait, something isn't right here. Your prototype model trained successfully, showing steady improvement. However, the performance on the full 15-class dataset seems to have hit a wall. What happened?</p> <p>A close look at the plots reveals the problem. While the Training Loss consistently decreases, the Validation Loss drops for a while and then begins to rise and fluctuate. At the same time, the Validation Accuracy gets stuck, plateauing without making further significant progress. This is a classic case of overfitting.</p> <p>Overfitting occurs when a model learns the training data too well, including its noise and specific quirks, instead of the general, underlying patterns that would help it perform on new, unseen data. The widening gap between your training and validation loss is a clear sign your model is memorizing the training set instead of learning to generalize.</p> <p>You might wonder why this happened now and not with the 9-class prototype. The reason is the significant increase in task complexity. Distinguishing between 15 classes is much harder than 9, requiring the model to learn more subtle features. Faced with this harder challenge, your powerful CNN model found an easier path to lowering the training loss: it started to memorize the training data instead of learning to generalize.</p> <p>This overfitting problem presents a realistic challenge, similar to what you'd encounter in a real-world project. In this module's graded assignment, you'll tackle this issue by making several updates to your entire pipeline to see if you can improve the model's ability to generalize.</p> In\u00a0[\u00a0]: Copied! <pre># ### Optional: Uncomment and run this cell to see the predictions made by the full model\n\n# helper_utils.visualise_predictions(\n#     model=trained_model, \n#     data_loader=val_loader, \n#     device=device, \n#     grid=(3, 5)\n# )\n</pre> # ### Optional: Uncomment and run this cell to see the predictions made by the full model  # helper_utils.visualise_predictions( #     model=trained_model,  #     data_loader=val_loader,  #     device=device,  #     grid=(3, 5) # ) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#building-a-cnn-for-nature-classification","title":"Building a CNN for Nature Classification\u00b6","text":"<p>Welcome! The butterfly house next door has approached the botanical garden with a new idea: an app that can classify not just flowers, but also insects and small animals. Your task is to design and build the model that will make this possible.</p> <p>This problem is more complex than what you\u2019ve tackled before. The linear layers you used previously won\u2019t be enough to capture the rich visual patterns in these diverse images. To meet this new challenge, you\u2019ll build a Convolutional Neural Network (CNN), a model designed to recognize shapes, textures, and features in visual data.</p> <p>In this lab, you\u2019ll go through the end-to-end process of building a CNN for this classification task. You\u2019ll not only implement the architecture but also follow an iterative workflow\u2014starting with a smaller prototype before scaling up\u2014and learn how to diagnose common training issues.</p> <p>You will:</p> <ul> <li><p>Prepare a Diverse Dataset: Load and transform a specialized subset of images for your multi-class nature classifier.</p> </li> <li><p>Build a CNN Architecture: Define a complete CNN from scratch, combining convolutional, pooling, and fully connected layers to create a powerful feature extractor.</p> </li> <li><p>Train a Prototype Model: Follow a realistic workflow by first training your model on a smaller, 9-class subset to build a working prototype and establish a performance baseline.</p> </li> <li><p>Scale Up and Diagnose Challenges: Train the full model on all 15 classes and analyze the results to identify common machine learning challenges like overfitting.</p> </li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#imports","title":"Imports\u00b6","text":""},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#preparing-the-nature-dataset","title":"Preparing the Nature Dataset\u00b6","text":"<p>For this lab, you'll work with a collection of images taken from the well-known CIFAR-100 dataset. This dataset is a fantastic resource for computer vision tasks, containing thousands of small, 32x32 color images that are perfect for training a CNN. It\u2019s a diverse collection, which is exactly what you need for the expanded nature classifier app.</p> <p>While CIFAR-100 has 100 different classes, you won't need all of them. To meet the new requirements for your app, you'll focus on a curated selection of 15 classes that fit the theme of a nature classifier. This selection will include flowers, insects and mammals. Specifically, you'll be working with:</p> <ul> <li><p>Flowers: 'orchid', 'poppy', 'rose', 'sunflower', 'tulip'</p> </li> <li><p>Mammals: 'fox', 'porcupine', 'possum', 'raccoon', 'skunk'</p> </li> <li><p>Insects: 'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'</p> </li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#image-transformations","title":"Image Transformations\u00b6","text":"<p>Before you load the dataset, first you need to define the transformation pipelines for it. Since all images in the dataset are already a standard 32x32 size, you don't need to add a resizing step. Your training pipeline will include data augmentation, while both pipelines will convert images to tensors and normalize them using the standard mean and standard deviation for the CIFAR-100 dataset.</p> <ul> <li>Define the specific mean and standard deviation for the CIFAR-100 dataset.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#preparing-the-data-pipeline","title":"Preparing the Data Pipeline\u00b6","text":"<p>With your transformations ready, it's time to load the data. To quickly show the butterfly house next door a working prototype, it's a smart strategy to start with a smaller, more manageable dataset. This allows you to test your entire pipeline and build a baseline model without the long wait times required for the full dataset.</p> <p>Therefore, instead of using all 15 classes at once, you'll begin with a balanced subset of 9 classes (3 from each category). This iterative approach is a common and efficient practice in real-world machine learning.</p> <p>For this initial prototype, you'll use the following classes:</p> <ul> <li><p>Flowers: 'orchid', 'poppy', 'sunflower'</p> </li> <li><p>Mammals: 'fox', 'raccoon', 'skunk'</p> </li> <li><p>Insects: 'butterfly', 'caterpillar', 'cockroach'</p> </li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#visualizing-the-training-images","title":"Visualizing the Training Images\u00b6","text":"<p>With your data pipeline complete, it's always a good idea to look at a few examples from your training set. This helps confirm that your data has been loaded and processed correctly. The following helper function will display a random sample of your training images.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#building-the-cnn-architecture","title":"Building the CNN Architecture\u00b6","text":"<p>With your data ready, it's time to build the core of your nature classifier. For a complex task like identifying different species in images, the linear layers you've used before aren't enough, as they look at pixels individually without understanding their spatial relationships.</p> <p>You'll now build a Convolutional Neural Network (CNN), an architecture specifically designed to \"see\" and recognize patterns, edges, and textures in images through a series of learnable filters. You'll define your model's structure using PyTorch's <code>nn.Module</code>, combining several types of layers to create a powerful image classifier.</p> <p>Here's a breakdown of the key layers you'll use:</p> <p>Convolutional Layer (<code>nn.Conv2d</code>)</p> <p>This is the core building block of a CNN, using learnable filters to scan the image for visual features. The output is a set of \"feature maps\" that highlight where in the image these patterns appear.</p> <ul> <li><code>in_channels</code>: The number of channels from the previous layer; for the first layer, this is 3 for the RGB color channels.</li> <li><code>out_channels</code>: The number of filters the layer will learn, determining the number of output feature maps.</li> <li><code>kernel_size</code>: The dimensions of the filter, such as a 3x3 grid that examines a pixel and its immediate neighbors.</li> <li><code>padding</code>: Adds a border around the image, allowing the kernel to process edge pixels while preserving the image's dimensions.</li> </ul> <p>ReLU Activation Function (<code>nn.ReLU</code>)</p> <p>An activation function that introduces non-linearity by changing all negative values in the feature maps to zero. This helps the model learn more complex patterns.</p> <p>Max Pooling Layer (<code>nn.MaxPool2d</code>)</p> <p>This layer downsamples the feature maps by reducing their height and width, which makes the network more efficient. It slides a window over the feature map and keeps only the single largest value from that window, discarding the rest.</p> <ul> <li><code>kernel_size</code>: The size of the window to perform pooling on, such as a 2x2 area.</li> <li><code>stride</code>: The step size the window moves across the image. A stride of 2 with a 2x2 kernel will halve the feature map's dimensions.</li> </ul> <p>Flatten Layer (<code>nn.Flatten</code>)</p> <p>A utility layer that unrolls the 2D feature maps into a single 1D vector. This is a necessary step to prepare the data for the fully connected linear layers.</p> <p>Linear Layer (<code>nn.Linear</code>)</p> <p>Also known as a fully connected layer, it performs the final classification. It combines the features learned by the convolutional layers into a final prediction.</p> <p>Dropout Layer (<code>nn.Dropout</code>)</p> <p>A regularization technique that helps prevent overfitting by randomly setting a fraction of neuron activations to zero during training. This forces the network to learn more robust features instead of relying too heavily on any single pattern.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#training-the-model","title":"Training the Model\u00b6","text":"<p>With your model defined and the data pipeline prepared, you're ready to set up the training process. This involves initializing a loss function to measure your model's error and an optimizer to update its weights based on that error.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#initialize-loss-function-and-optimizer","title":"Initialize Loss Function and Optimizer\u00b6","text":"<p>Before starting the training loop, you'll define two key components:</p> <ul> <li>You'll use <code>nn.CrossEntropyLoss</code>. This is the standard loss function for multi-class classification tasks as it's designed to measure the error when a model has to choose one class from several possibilities.</li> <li>You'll use the <code>Adam</code> optimizer. This is a popular and efficient algorithm that updates the model's weights to minimize the loss.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#the-training-loop","title":"The Training Loop\u00b6","text":"<ul> <li>Next, you'll define the <code>training_loop</code> function. This function encapsulates the entire process of training and validating your model over multiple epochs.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#visualizing-predictions","title":"Visualizing Predictions\u00b6","text":"<p>While the plots show your model's overall performance, looking at individual predictions provides a more intuitive feel for its strengths and weaknesses. You can now use a helper function to see your model in action, visualizing its predictions on random images from the validation set. This will show you concrete examples of where it succeeds and where it might be making mistakes.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#scaling-up-training-the-full-model","title":"Scaling Up: Training the Full Model\u00b6","text":"<p>The prototype was a success! Now it's time to train the final model for the butterfly house app. You'll repeat the same steps as before, but this time using the full, more challenging dataset of 15 classes.</p> <ul> <li>First, create a new list containing all 15 target classes.</li> <li>Use the <code>load_cifar100_subset</code> helper function again to create the new training and validation datasets based on this full list.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_1_cnn_nature_classifier/#conclusion","title":"Conclusion\u00b6","text":"<p>Congratulations on completing the lab! You have successfully navigated the entire machine learning pipeline, from data preparation to building, training, and analyzing your very own Convolutional Neural Network.</p> <p>You've put theory into practice by building a CNN architecture capable of learning complex visual patterns. More importantly, you've experienced a realistic, iterative development workflow by first creating a successful prototype and then scaling up to a more complex model. This process led you to encounter and diagnose overfitting, a fundamental challenge that every machine learning practitioner must learn to solve.</p> <p>The skills you've developed here have prepared you for the next step. You've identified the problem, and in the graded assignment, you'll get to solve it. Well done!</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/","title":"Model Debugging, Inspection, and Modularization","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import SqueezeNet\n</pre> import torch import torch.nn as nn import torchvision.transforms as transforms from torch.utils.data import DataLoader from torchvision.models import SqueezeNet In\u00a0[\u00a0]: Copied! <pre>import helper_utils\n</pre> import helper_utils In\u00a0[\u00a0]: Copied! <pre>dataset = helper_utils.get_dataset()\n\ntransform = transforms.ToTensor()\ndataset.transform = transform\n</pre> dataset = helper_utils.get_dataset()  transform = transforms.ToTensor() dataset.transform = transform In\u00a0[\u00a0]: Copied! <pre>batch_size = 64\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n</pre> batch_size = 64 dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False) In\u00a0[\u00a0]: Copied! <pre>img_batch, label_batch = next(iter(dataloader))\nprint(\"Batch shape:\", img_batch.shape)  # Should be [batch_size, 1, 28, 28]\n</pre> img_batch, label_batch = next(iter(dataloader)) print(\"Batch shape:\", img_batch.shape)  # Should be [batch_size, 1, 28, 28] In\u00a0[\u00a0]: Copied! <pre>class SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Convolutional Block\n        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully Connected Block\n        # For Fashion MNIST: input images are 28x28,\n        # after conv+pool: 32x14x14\n        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n        self.relu_fc = nn.ReLU()\n        self.fc2 = nn.Linear(128, 10)  # 10 classes for Fashion MNIST\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv(x)))\n        x = self.relu_fc(self.fc1(x))\n        x = self.fc2(x)\n        return x\n</pre> class SimpleCNN(nn.Module):     def __init__(self):         super().__init__()         # Convolutional Block         self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)         self.relu = nn.ReLU()         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # Fully Connected Block         # For Fashion MNIST: input images are 28x28,         # after conv+pool: 32x14x14         self.fc1 = nn.Linear(32 * 14 * 14, 128)         self.relu_fc = nn.ReLU()         self.fc2 = nn.Linear(128, 10)  # 10 classes for Fashion MNIST      def forward(self, x):         x = self.pool(self.relu(self.conv(x)))         x = self.relu_fc(self.fc1(x))         x = self.fc2(x)         return x In\u00a0[\u00a0]: Copied! <pre>simple_cnn = SimpleCNN()\n\ntry:\n    output = simple_cnn(img_batch)  \nexcept Exception as e:\n    print(f\"\\033[91mError during forward pass: {e}\\033[0m\")\n</pre> simple_cnn = SimpleCNN()  try:     output = simple_cnn(img_batch)   except Exception as e:     print(f\"\\033[91mError during forward pass: {e}\\033[0m\") <p>Indeed, the model as provided contains some errors that require debugging. The message provided by PyTorch when an error occurs can sometimes be cryptic. It describes that two matrices (<code>mat1</code> and <code>mat2</code>) cannot be multiplied and provides their shapes. This indicates that there is a mismatch in the dimensions of the tensors being multiplied, which is a common issue in neural network implementations.</p> <p>However, the error message does not specify why and where in the model the error occurs. That is when the <code>forward</code> method of the model comes into play. The dynamic graph nature of PyTorch allows you to insert print statements or use debugging tools to inspect the values and shapes of tensors at various points in the <code>forward</code> method.</p> <p>You will define a new class that inherits from the original model and overrides the <code>forward</code> method to include print statements that display the shape of the tensor after each layer. A first try might be to explicitly separate the layers in the <code>forward</code> method and, for each layer:</p> <ul> <li>print the shape of the tensor before the layer (input shape),</li> <li>print the shape of some parameters of the layer (e.g., weights and biases),</li> <li>print the shape of the activation tensor after the layer (output shape), which will be the input for the next layer.</li> </ul> <p>You can now run the forward pass again and observe the printed shapes to identify where the mismatch occurs.</p> In\u00a0[\u00a0]: Copied! <pre>class SimpleCNNDebug(SimpleCNN):\n    def __init__(self):\n        super().__init__()\n        # The super().__init__() call above properly initializes all layers from SimpleCNN\n        # No need to redefine the layers here\n\n    def forward(self, x):\n        print(\"Input shape:\", x.shape)\n        print(\n            \" (Layer components) Conv layer parameters (weights, biases):\",\n            self.conv.weight.shape,\n            self.conv.bias.shape,\n        )\n        x_conv = self.relu(self.conv(x))\n\n        print(\"===\")\n\n        print(\"(Activation) After convolution and ReLU:\", x_conv.shape)\n        x_pool = self.pool(x_conv)\n        print(\"(Activation) After pooling:\", x_pool.shape)\n\n        print(\n            \"(Layer components) Linear layer fc1 parameters (weights, biases):\",\n            self.fc1.weight.shape,\n            self.fc1.bias.shape,\n        )\n\n        x_fc1 = self.relu_fc(self.fc1(x_pool))\n\n        print(\"===\")\n\n        print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)\n\n        print(\n            \"(Layer components) Linear layer fc2 parameters (weights, biases):\",\n            self.fc2.weight.shape,\n            self.fc2.bias.shape,\n        )\n        x = self.fc2(x_fc1)\n\n        print(\"===\")\n\n        print(\"(Activation) After fc2 (output):\", x.shape)\n        return x\n</pre> class SimpleCNNDebug(SimpleCNN):     def __init__(self):         super().__init__()         # The super().__init__() call above properly initializes all layers from SimpleCNN         # No need to redefine the layers here      def forward(self, x):         print(\"Input shape:\", x.shape)         print(             \" (Layer components) Conv layer parameters (weights, biases):\",             self.conv.weight.shape,             self.conv.bias.shape,         )         x_conv = self.relu(self.conv(x))          print(\"===\")          print(\"(Activation) After convolution and ReLU:\", x_conv.shape)         x_pool = self.pool(x_conv)         print(\"(Activation) After pooling:\", x_pool.shape)          print(             \"(Layer components) Linear layer fc1 parameters (weights, biases):\",             self.fc1.weight.shape,             self.fc1.bias.shape,         )          x_fc1 = self.relu_fc(self.fc1(x_pool))          print(\"===\")          print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)          print(             \"(Layer components) Linear layer fc2 parameters (weights, biases):\",             self.fc2.weight.shape,             self.fc2.bias.shape,         )         x = self.fc2(x_fc1)          print(\"===\")          print(\"(Activation) After fc2 (output):\", x.shape)         return x In\u00a0[\u00a0]: Copied! <pre>simple_cnn_debug = SimpleCNNDebug()\n\ntry:\n    output_debug = simple_cnn_debug(img_batch)  \nexcept Exception as e:\n    print(f\"\\033[91mError during forward pass in debug model: {e}\\033[0m\")\n</pre> simple_cnn_debug = SimpleCNNDebug()  try:     output_debug = simple_cnn_debug(img_batch)   except Exception as e:     print(f\"\\033[91mError during forward pass in debug model: {e}\\033[0m\") <p>This is already a cleaner output. You can already see that all the layers of the convolutional block are working fine, and the shapes are as expected (<code>batch_size=64</code> and <code>out_channels=32</code>).</p> <p>The error occurs in the fully connected block, specifically at the first linear layer: <code>x_pool</code> has shape <code>[64, 32, 14, 14]</code>, but the linear layer expects an input of shape <code>[64, 2048]</code> (its weight matrix has shape <code>[128, 6272]</code>).</p> <p>As the linear layer <code>fc1</code> expects a 2D input of shape <code>[batch_size, input_features]</code>, the <code>x_pool</code> is flattened to a 2D tensor with shape <code>[64*32*14, 14]</code> before being passed to <code>fc1</code>. This is not the intended shape, and it leads to the dimension mismatch error.</p> <p>Once you have identified the issue, you can fix it by adding a flattening operation before the first linear layer in the <code>forward</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>class SimpleCNNFixed(SimpleCNN):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        print(\"Input shape:\", x.shape)\n        print(\n            \" (Neuron components) Conv layer parameters (weights, biases):\",\n            self.conv.weight.shape,\n            self.conv.bias.shape,\n        )\n        x_conv = self.relu(self.conv(x))\n\n        print(\"===\")\n\n        print(\"(Activation) After convolution and ReLU:\", x_conv.shape)\n        x_pool = self.pool(x_conv)\n        print(\"(Activation) After pooling:\", x_pool.shape)\n\n        x_flattened = torch.flatten(\n            x_pool, start_dim=1\n        )  # Flatten all dimensions except batch\n        print(\"(Activation) After flattening:\", x_flattened.shape)\n\n        print(\n            \"(Neuron components) Linear layer fc1 parameters (weights, biases):\",\n            self.fc1.weight.shape,\n            self.fc1.bias.shape,\n        )\n\n        x_fc1 = self.relu_fc(self.fc1(x_flattened))\n\n        print(\"===\")\n\n        print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)\n\n        print(\n            \"(Neuron components) Linear layer fc2 parameters (weights, biases):\",\n            self.fc2.weight.shape,\n            self.fc2.bias.shape,\n        )\n        x = self.fc2(x_fc1)\n\n        print(\"===\")\n\n        print(\"(Activation) After fc2 (output):\", x.shape)\n        return x\n</pre> class SimpleCNNFixed(SimpleCNN):     def __init__(self):         super().__init__()      def forward(self, x):         print(\"Input shape:\", x.shape)         print(             \" (Neuron components) Conv layer parameters (weights, biases):\",             self.conv.weight.shape,             self.conv.bias.shape,         )         x_conv = self.relu(self.conv(x))          print(\"===\")          print(\"(Activation) After convolution and ReLU:\", x_conv.shape)         x_pool = self.pool(x_conv)         print(\"(Activation) After pooling:\", x_pool.shape)          x_flattened = torch.flatten(             x_pool, start_dim=1         )  # Flatten all dimensions except batch         print(\"(Activation) After flattening:\", x_flattened.shape)          print(             \"(Neuron components) Linear layer fc1 parameters (weights, biases):\",             self.fc1.weight.shape,             self.fc1.bias.shape,         )          x_fc1 = self.relu_fc(self.fc1(x_flattened))          print(\"===\")          print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)          print(             \"(Neuron components) Linear layer fc2 parameters (weights, biases):\",             self.fc2.weight.shape,             self.fc2.bias.shape,         )         x = self.fc2(x_fc1)          print(\"===\")          print(\"(Activation) After fc2 (output):\", x.shape)         return x In\u00a0[\u00a0]: Copied! <pre># Fixed version\nsimple_cnn_fixed = SimpleCNNFixed()\n\noutput = simple_cnn_fixed(img_batch)\n</pre> # Fixed version simple_cnn_fixed = SimpleCNNFixed()  output = simple_cnn_fixed(img_batch) <p>The issue is now fixed, and the model runs without errors! You can see that the shapes of the tensors are as expected after each layer, and the final output has the correct shape of <code>[64, 10]</code>, corresponding to the batch size and the number of classes.</p> <p>Once the model is running without errors, you can jump the next section to refactor the model using <code>nn.Sequential</code> for a cleaner and more modular implementation.</p> In\u00a0[\u00a0]: Copied! <pre>class SimpleCNN2Seq(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Convolutional Block\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        # Fully Connected Block\n        # For Fashion MNIST: input images are 28x28,\n        # after conv+pool: 32x14x14\n        flattened_size = 32 * 14 * 14\n        self.fc_block = nn.Sequential(\n            nn.Linear(flattened_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),  # 10 classes for Fashion MNIST\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n        x = self.fc_block(x)\n        return x\n</pre> class SimpleCNN2Seq(nn.Module):     def __init__(self):         super().__init__()         # Convolutional Block         self.conv_block = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2, stride=2),         )          # Fully Connected Block         # For Fashion MNIST: input images are 28x28,         # after conv+pool: 32x14x14         flattened_size = 32 * 14 * 14         self.fc_block = nn.Sequential(             nn.Linear(flattened_size, 128),             nn.ReLU(),             nn.Linear(128, 10),  # 10 classes for Fashion MNIST         )      def forward(self, x):         x = self.conv_block(x)         x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch         x = self.fc_block(x)         return x In\u00a0[\u00a0]: Copied! <pre>simple_cnn_seq = SimpleCNN2Seq()\noutput = simple_cnn_seq(img_batch)\n\nprint(\"Output shape from sequential model:\", output.shape)\n</pre> simple_cnn_seq = SimpleCNN2Seq() output = simple_cnn_seq(img_batch)  print(\"Output shape from sequential model:\", output.shape) In\u00a0[\u00a0]: Copied! <pre>class SimpleCNN2SeqDebug(SimpleCNN2Seq):\n    def __init__(self):\n        super().__init__()\n        # The super().__init__() call above properly initializes all layers from SimpleCNN2Seq\n        # No need to redefine the layers here\n\n    def get_statistics(self, activation):\n        mean = activation.mean().item()\n        std = activation.std().item()\n        min_val = activation.min().item()\n        max_val = activation.max().item()\n\n        print(f\" Mean: {mean}\")\n        print(f\" Std: {std}\")\n        print(f\" Min: {min_val}\")\n        print(f\" Max: {max_val}\")\n        return mean, std, min_val, max_val\n\n    def forward(self, x):\n        features = self.conv_block(x)\n        x = torch.flatten(features, start_dim=1)  # Flatten all dimensions except batch\n\n        print(\"After conv_block, the activation statistics are:\")\n        self.get_statistics(features)\n\n        x = self.fc_block(x)\n        print(\"After fc_block, the activation statistics are:\")\n        self.get_statistics(x)\n        return x\n</pre> class SimpleCNN2SeqDebug(SimpleCNN2Seq):     def __init__(self):         super().__init__()         # The super().__init__() call above properly initializes all layers from SimpleCNN2Seq         # No need to redefine the layers here      def get_statistics(self, activation):         mean = activation.mean().item()         std = activation.std().item()         min_val = activation.min().item()         max_val = activation.max().item()          print(f\" Mean: {mean}\")         print(f\" Std: {std}\")         print(f\" Min: {min_val}\")         print(f\" Max: {max_val}\")         return mean, std, min_val, max_val      def forward(self, x):         features = self.conv_block(x)         x = torch.flatten(features, start_dim=1)  # Flatten all dimensions except batch          print(\"After conv_block, the activation statistics are:\")         self.get_statistics(features)          x = self.fc_block(x)         print(\"After fc_block, the activation statistics are:\")         self.get_statistics(x)         return x In\u00a0[\u00a0]: Copied! <pre>simple_cnn_seq_debug = SimpleCNN2SeqDebug()\n\nfor idx, (img_batch, _) in enumerate(dataloader):\n    if idx &lt; 5:\n        print(f\"=== Batch {idx} ===\")\n        output_debug = simple_cnn_seq_debug(img_batch)\n</pre> simple_cnn_seq_debug = SimpleCNN2SeqDebug()  for idx, (img_batch, _) in enumerate(dataloader):     if idx &lt; 5:         print(f\"=== Batch {idx} ===\")         output_debug = simple_cnn_seq_debug(img_batch) <p>This is a sanity check to ensure that the model is initialized correctly and that the activations are not exploding or vanishing. Those issues can lead to poor training performance or convergence problems.</p> In\u00a0[\u00a0]: Copied! <pre># Load SqueezeNet model\ncomplex_model = SqueezeNet()\n\nprint(complex_model)\n</pre> # Load SqueezeNet model complex_model = SqueezeNet()  print(complex_model) <p>For complex models, printing the entire model architecture can be overwhelming. Instead, you can make use of <code>named_children()</code> and <code>children()</code> to iterate through the top-level blocks of the model.</p> In\u00a0[\u00a0]: Copied! <pre># Iterate through the main blocks\nfor name, block in complex_model.named_children():\n    print(f\"Block {name} has a total of {len(list(block.children()))} layers:\")\n    \n    # List all children layers in the block\n    for idx, layer in enumerate(block.children()):\n        # Check if the layer is terminal (no children) or not\n        if len(list(layer.children())) == 0:\n            print(f\"\\t {idx} - Layer {layer}\")\n        # If the layer has children, it's a sub-block, then print only the number of children and its name\n        else:\n            layer_name = layer._get_name()  # More user-friendly name\n            print(f\"\\t {idx} - Sub-block {layer_name} with {len(list(layer.children()))} layers\")            \n</pre> # Iterate through the main blocks for name, block in complex_model.named_children():     print(f\"Block {name} has a total of {len(list(block.children()))} layers:\")          # List all children layers in the block     for idx, layer in enumerate(block.children()):         # Check if the layer is terminal (no children) or not         if len(list(layer.children())) == 0:             print(f\"\\t {idx} - Layer {layer}\")         # If the layer has children, it's a sub-block, then print only the number of children and its name         else:             layer_name = layer._get_name()  # More user-friendly name             print(f\"\\t {idx} - Sub-block {layer_name} with {len(list(layer.children()))} layers\")             <p>This provides a cleaner overview of the model's structure, allowing you to focus on the main components without getting lost in the details of every single layer. You will now zoom into one of the <code>Fire</code> modules to see its internal structure.</p> <p>For that you can use <code>modules()</code> to iterate through all the layers and sub-modules of the model.</p> In\u00a0[\u00a0]: Copied! <pre>first_fire_module = complex_model.features[3]\n\nfor idx, module in enumerate(first_fire_module.modules()):\n    # Avoid printing the top-level module itself\n    if idx &gt; 0 :\n        print(module)\n</pre> first_fire_module = complex_model.features[3]  for idx, module in enumerate(first_fire_module.modules()):     # Avoid printing the top-level module itself     if idx &gt; 0 :         print(module) <p>Now the model's architecture is neatly printed, showing the main components and their configurations. You can now do some specific inspections such as counting the number of specific layer types or calculating the total number of parameters in the model.</p> In\u00a0[\u00a0]: Copied! <pre>type_layer = nn.Conv2d\n\nselected_layers = [layer for layer in complex_model.modules() if isinstance(layer, type_layer)]\n\nprint(f\"Number of {type_layer.__name__} layers: {len(selected_layers)}\")\n</pre> type_layer = nn.Conv2d  selected_layers = [layer for layer in complex_model.modules() if isinstance(layer, type_layer)]  print(f\"Number of {type_layer.__name__} layers: {len(selected_layers)}\") <p>You will now count the total number of parameters in the model. This gives you an idea of the model's complexity and capacity.</p> In\u00a0[\u00a0]: Copied! <pre># total number of parameters in the model\ntotal_params = sum(p.numel() for p in complex_model.parameters())\nprint(f\"Total number of parameters in the model: {total_params}\")\n</pre> # total number of parameters in the model total_params = sum(p.numel() for p in complex_model.parameters()) print(f\"Total number of parameters in the model: {total_params}\") <p>Now, you can take this further by inspecting the parameters of each terminal layer (layers without children) in the model. For each terminal layer, you will print its name and the total number of parameters it contains. This helps to identify which layers contribute most to the model's parameter count and can be useful for model optimization, pruning, or understanding where the model's capacity lies.</p> In\u00a0[\u00a0]: Copied! <pre>counting_params = {}\n\n# For each terminal layer print its number of parameters\nfor layer in complex_model.named_modules():\n    n_children = len(list(layer[1].children()))\n    if n_children == 0:  # Terminal layer\n        layer_name = layer[0]\n        n_parameters = sum(p.numel() for p in layer[1].parameters())\n        counting_params[layer_name] = n_parameters\n        print(f\"Layer {layer_name} has {n_parameters} parameters\")\n\n# Plotting the distribution of parameters per layer\nhelper_utils.plot_counting(counting_params)\n</pre> counting_params = {}  # For each terminal layer print its number of parameters for layer in complex_model.named_modules():     n_children = len(list(layer[1].children()))     if n_children == 0:  # Terminal layer         layer_name = layer[0]         n_parameters = sum(p.numel() for p in layer[1].parameters())         counting_params[layer_name] = n_parameters         print(f\"Layer {layer_name} has {n_parameters} parameters\")  # Plotting the distribution of parameters per layer helper_utils.plot_counting(counting_params)"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#model-debugging-inspection-and-modularization","title":"Model Debugging, Inspection, and Modularization\u00b6","text":"<p>So far, you've focused on building and training models. But in the real world, your first attempt at a model rarely works perfectly. You'll often encounter cryptic error messages about mismatched tensor shapes, or worse, your model will run without errors but fail to produce meaningful results.</p> <p>This is where debugging, inspection, and modularization become essential skills. In this lab, you'll step into the role of a model investigator. You'll start with a broken Convolutional Neural Network (CNN) and use systematic debugging techniques to find and fix the bug. Then, you'll learn how to refactor your code for clarity and reuse, and finally, you'll dissect a complex, pre-trained model to understand its inner workings.</p> <p>In this lab, you will:</p> <ul> <li>Debug a broken CNN by inserting print statements into the <code>forward</code> pass to identify and correct a critical tensor shape mismatch.</li> <li>Refactor the corrected model using <code>nn.Sequential</code> to create a cleaner, more modular, and less error-prone architecture.</li> <li>Inspect the activation statistics of your model to perform a sanity check for issues like exploding or vanishing gradients.</li> <li>Explore the architecture of a complex, pre-existing model (<code>SqueezeNet</code>) to count its layers and analyze its parameter distribution.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#imports","title":"Imports\u00b6","text":""},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#data-loading","title":"Data Loading\u00b6","text":"<p>To debug and inspect a model effectively, you'll first need a dataset to work with. The goal of this lab is to practice an end-to-end debugging and inspection workflow, so you'll use a simple dataset that lets you focus on the model architecture rather than complex data preprocessing. For this purpose, you'll use the Fashion MNIST dataset, which consists of grayscale images of clothing items and serves as a straightforward benchmark for image classification tasks. You\u2019ll begin by loading the dataset using PyTorch\u2019s torchvision library, and then create a DataLoader to efficiently handle the data in batches during training and evaluation.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#debugging-through-forward-pass","title":"Debugging through forward pass\u00b6","text":"<p>When starting to work with a new model, it is common to encounter errors. These errors can be due to various reasons, such as incorrect tensor shapes, incompatible operations, or unexpected values. Sometimes, the model may run without errors but produce incorrect outputs.</p> <p>In this section, you will explore how to debug a PyTorch model by examining its forward pass.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#a-first-exploration-of-the-model","title":"A first exploration of the model\u00b6","text":"<p>It is now time to explore the model. This model is a simple network with:</p> <ul> <li>a convolutional block: consisting of a convolutional layer, a ReLU activation function, and a max pooling layer,</li> <li>a fully connected block: consisting of a linear layer, a ReLU activation function, and a final linear layer that outputs the class scores.</li> </ul> <p>You will first instantiate the model and try to run a forward pass with a batch from the dataloader. To get a cleaner output in case of errors, you will use <code>try/except</code> to catch any exceptions that may arise during the forward pass.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#nnsequential-for-modularization","title":"<code>nn.Sequential</code> for Modularization\u00b6","text":"<p>The model is now working correctly, but the <code>forward</code> method is quite verbose and repetitive. To make the code cleaner and more modular, you can use <code>nn.Sequential</code> to define the convolutional and fully connected blocks.</p> <p>In this way you gain several advantages:</p> <ul> <li>Modularity: Each block is defined as a separate module, making it easier to understand and modify.</li> <li>Reusability: You can easily reuse the blocks in other models or experiments.</li> <li>Cleaner Code: The <code>forward</code> method becomes much simpler, as it only needs to call the blocks sequentially.</li> <li>Less Error-Prone: By defining the blocks in one place, you reduce the chances of making mistakes when implementing the <code>forward</code> method.</li> </ul>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#statistical-inspection-of-the-initialization","title":"Statistical Inspection of the Initialization\u00b6","text":"<p>A common check when inspecting a model is to look at the statistics of some activations to ensure that they are within a reasonable range.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#model-inspection","title":"Model Inspection\u00b6","text":"<p>With the previous model working correctly, you will now inspect a pre-existing complex model from <code>torchvision.models</code>, such as <code>SqueezeNet</code>.</p> <p>In this section you will make use of the inspection utilities provided by PyTorch to explore the model's architecture, layers, and parameters. These inspection techniques are foundational for effective debugging and for making informed modifications to your neural network designs.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#architecture-overview","title":"Architecture Overview\u00b6","text":""},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#detail-inspection","title":"Detail Inspection\u00b6","text":"<p>You will now count how many <code>Conv2d</code> layers are in the model.</p>"},{"location":"tools/pytorch/Examples/C1_M4_Lab_2_debugging/#conclusion","title":"Conclusion\u00b6","text":"<p>You have now successfully debugged, refactored, and inspected PyTorch models. In this lab, you saw firsthand that a model's <code>forward</code> pass is not a black box and that by strategically adding print statements, you can diagnose and solve common but frustrating errors like shape mismatches.</p> <p>You have moved beyond simply writing model code and can now make it more robust and readable by grouping layers into logical blocks with <code>nn.Sequential</code>. This practice of modularization makes your architectures easier to understand, reuse, and adapt. You also learned how to perform essential sanity checks by inspecting activation statistics and how to systematically explore any PyTorch model, no matter how complex, using inspection utilities like <code>.modules()</code> and <code>.named_children()</code>.</p> <p>With these fundamental skills of debugging and inspection, you are well-prepared for more advanced challenges.</p>"}]}