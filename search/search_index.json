{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mathematics-for-machine-learning","title":"Mathematics for Machine Learning","text":"<p>Welcome to Mathematics for Machine Learning, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p> <p>This digital book provides a unified, intuition-driven exploration of key mathematical tools \u2014 from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p> <ul> <li>Concise derivations of essential results</li> <li>Geometric intuition and figures where helpful</li> <li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li> <li>Appendices that extend into more advanced or specialized topics</li> </ul> <p>Whether you\u2019re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p>"},{"location":"appendices/120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"appendices/120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"appendices/120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"appendices/120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"appendices/120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"appendices/120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"appendices/120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"appendices/120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"appendices/120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"appendices/120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"appendices/130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"appendices/140_support/","title":"Appendix C - Support Functions and Dual Geometry (Advanced)","text":""},{"location":"appendices/140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"appendices/140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"appendices/140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"appendices/140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"appendices/140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"appendices/140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"appendices/140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"appendices/140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"appendices/160_conjugates/","title":"Appendix D - Convex Conjugates and Fenchel Duality (Advanced)","text":""},{"location":"appendices/160_conjugates/#appendix-d-convex-conjugates-and-fenchel-duality","title":"Appendix D: Convex Conjugates and Fenchel Duality","text":"<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis. They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p> <ul> <li>Convex conjugates convert a function into its \u201cslope-space\u201d representation \u2014 capturing its tightest linear overestimates.  </li> <li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li> </ul> <p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>"},{"location":"appendices/160_conjugates/#d1-intuitive-picture","title":"D.1 Intuitive Picture","text":"<p>Imagine a convex function \\(f(x)\\) drawn as a bowl in space. Each point \\(y\\) defines a line (or hyperplane) of slope \\(y\\):  The convex conjugate \\(f^*(y)\\) is the smallest height \\(b\\) such that this line always stays above \\(f(x)\\). In other words:</p> <p>\\(f^*(y)\\) measures the tightest linear overestimate of \\(f\\) in direction \\(y\\).</p> <p>So \\(f^*\\) encodes how \u201csteep\u201d \\(f\\) can be in every direction \u2014 it transforms the geometry of \\(f\\) into a new convex function on slope-space.</p>"},{"location":"appendices/160_conjugates/#d2-definition-and-key-properties","title":"D.2 Definition and Key Properties","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}\\) be a proper convex function. Its convex (Fenchel) conjugate is  </p> <p>Interpretation - \\(y\\): a slope or linear functional. - The supremum seeks the largest gap between the linear function \\(\\langle y,x\\rangle\\) and the graph of \\(f\\). - \\(f^*(y)\\) is always convex, even if \\(f\\) isn\u2019t strictly convex.</p>"},{"location":"appendices/160_conjugates/#fundamental-identities","title":"Fundamental Identities","text":"<ol> <li> <p>Fenchel\u2013Young inequality        with equality iff \\(y \\in \\partial f(x)\\).</p> </li> <li> <p>Biconjugation        This tells us the conjugate transform loses no information for convex functions.</p> </li> <li> <p>Order reversal    \\(f \\le g \\;\\Rightarrow\\; f^* \\ge g^*\\).</p> </li> <li> <p>Scaling and shift</p> </li> <li>\\((f + a)^*(y) = f^*(y) - a\\),</li> <li>\\((\\alpha f)^*(y) = \\alpha f^*(y/\\alpha)\\) for \\(\\alpha&gt;0.\\)</li> </ol>"},{"location":"appendices/160_conjugates/#d3-canonical-examples","title":"D.3 Canonical Examples","text":"Function \\(f(x)\\) Conjugate \\(f^*(y)\\) Notes \\( \\tfrac{1}{2}\\|x\\|_2^2 \\) \\( \\tfrac{1}{2}\\|y\\|_2^2 \\) Self-conjugate quadratic \\( \\|x\\|_1 \\) \\( \\delta_{\\{\\|y\\|_\\infty \\le 1\\}}(y) \\) Dual norm indicator \\( \\delta_C(x) \\) \\( \\sigma_C(y)=\\sup_{x\\in C}\\langle y,x\\rangle \\) Support function of set \\(C\\) \\( e^x \\) \\( y\\log y - y,\\, y&gt;0 \\) Appears in entropy and KL-divergence <p>These examples illustrate how conjugation connects: - Norms \u2194 dual norms, - Sets \u2194 support functions, - Exponentials \u2194 entropy, - Quadratics \u2194 themselves.</p>"},{"location":"appendices/160_conjugates/#d4-geometric-interpretation","title":"D.4 Geometric Interpretation","text":"<ul> <li>Each point on \\(f\\) has a tangent hyperplane whose slope is a subgradient.  </li> <li>The collection of all such hyperplanes forms the epigraph of \\(f^*\\).  </li> <li>The transformation \\(f \\mapsto f^*\\) swaps the roles of \u201cposition\u201d and \u201cslope\u201d:   convex geometry \u2194 supporting hyperplanes.</li> </ul> <p>Visually: - \\(f\\) describes a bowl in \\((x,t)\\)-space. - \\(f^*\\) describes the envelope of tangent planes to that bowl.</p>"},{"location":"appendices/160_conjugates/#d5-from-conjugates-to-duality-fenchel-duality","title":"D.5 From Conjugates to Duality \u2014 Fenchel Duality","text":"<p>Many convex optimization problems can be written as  where \\(f,g\\) are convex and \\(A\\) is linear. Fenchel duality uses conjugates to build a dual problem in terms of \\(f^*\\) and \\(g^*\\).</p>"},{"location":"appendices/160_conjugates/#the-fenchel-dual-problem","title":"The Fenchel Dual Problem","text":"\\[ \\max_y \\; -f^*(A^\\top y) - g^*(-y). \\] <p>Interpretation - \\(y\\) is the dual variable (similar to Lagrange multipliers). - The dual objective collects the best linear lower bounds on the primal cost.</p>"},{"location":"appendices/160_conjugates/#d6-weak-and-strong-duality","title":"D.6 Weak and Strong Duality","text":"<ul> <li> <p>Weak duality: For any \\(x,y\\),      So the dual value always underestimates the primal value.</p> </li> <li> <p>Strong duality:   If \\(f,g\\) are closed convex and a mild constraint qualification holds (e.g. Slater\u2019s condition \u2014 existence of strictly feasible \\(x\\)), then    </p> </li> </ul> <p>At the optimum:  These are the Fenchel\u2013KKT conditions, directly linking primal and dual subgradients.</p>"},{"location":"appendices/160_conjugates/#d7-illustrative-examples","title":"D.7 Illustrative Examples","text":""},{"location":"appendices/160_conjugates/#a-linear-programming","title":"(a) Linear Programming","text":"<p>Primal:  </p> <p>Take \\(f(x) = c^\\top x + \\delta_{\\{x\\ge0\\}}(x)\\), \\(g(z)=\\delta_{\\{z=b\\}}(z)\\).</p> <p>Then  </p> <p>Dual:  which is the standard LP dual.</p>"},{"location":"appendices/160_conjugates/#b-quadratic-set-constraint","title":"(b) Quadratic + Set Constraint","text":"<p>Primal:  </p> <p>Then  so the dual is  Optimality gives \\(x^*=y^*\\), the projection condition in Euclidean geometry.</p>"},{"location":"appendices/160_conjugates/#d8-practical-significance","title":"D.8 Practical Significance","text":"Area How Fenchel Duality Appears Optimization theory Derives general dual problems beyond inequality constraints. Algorithm design Basis for primal\u2013dual and splitting methods (ADMM, Chambolle\u2013Pock, Mirror Descent). Geometry Dual problem finds the \u201cbest supporting hyperplane\u201d to the primal epigraph. Machine Learning Loss\u2013regularizer pairs (hinge \u2194 clipped loss, logistic \u2194 log-sum-exp) often form conjugate pairs. Proximal operators Linked via Moreau identity:  \\(\\mathrm{prox}_{f^*}(y) = y - \\mathrm{prox}_f(y)\\)."},{"location":"appendices/160_conjugates/#d9-conceptual-unification","title":"D.9 Conceptual Unification","text":"<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p> <ul> <li>From geometry: support functions, projections, subgradients (Appendices B\u2013C).  </li> <li>From analysis: inequalities like Fenchel\u2019s and Jensen\u2019s (Appendix A).  </li> <li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7\u20138).  </li> <li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9\u201310).</li> </ul> <p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>"},{"location":"appendices/160_conjugates/#d10-summary-and-takeaways","title":"D.10 Summary and Takeaways","text":"<ul> <li>The convex conjugate \\(f^*\\) expresses \\(f\\) through its linear support planes.  </li> <li>The Fenchel\u2013Young inequality connects primal variables and dual slopes.  </li> <li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li> <li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li> <li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li> </ul> <p>Further Reading</p> <ul> <li>Rockafellar, R. T. (1970). Convex Analysis. Princeton UP.  </li> <li>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization, Chs. 3 &amp; 5.  </li> <li>Bauschke, H. H., &amp; Combettes, P. L. (2017). Convex Analysis and Monotone Operator Theory.  </li> <li>Hiriart-Urruty, J.-B., &amp; Lemar\u00e9chal, C. (2001). Fundamentals of Convex Analysis.  </li> </ul>"},{"location":"appendices/170_probability/","title":"Appendix E - Convexity in Probability and Statistics (Advanced)","text":""},{"location":"appendices/170_probability/#appendix-e-convexity-in-probability-and-statistics","title":"Appendix E : Convexity in Probability and Statistics","text":"<p>Convex analysis is not just geometry and optimization \u2014 it is deeply woven into probability, statistics, and information theory. Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p> <p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>"},{"location":"appendices/170_probability/#e1-convexity-of-expectations","title":"E.1 Convexity of Expectations","text":"<p>Let \\(f:\\mathbb{R}^n\\!\\to\\!\\mathbb{R}\\) be convex and \\(X\\) a random vector. Then by Jensen\u2019s inequality (Appendix A):</p> \\[ f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]. \\]"},{"location":"appendices/170_probability/#consequences","title":"Consequences","text":"<ul> <li>Expectations preserve convexity:   if each \\(f(\\cdot,\\xi)\\) is convex, then \\(F(x)=\\mathbb{E}_\\xi[f(x,\\xi)]\\) is convex.</li> <li>Stochastic objectives in ML \u2014 e.g. expected loss \\(\\mathbb{E}_{(a,b)}[\\ell(a^\\top x,b)]\\) \u2014 are convex when the sample-wise loss is convex.</li> </ul> <p>Hence almost all empirical risk minimization problems are discrete approximations of convex expectations.</p>"},{"location":"appendices/170_probability/#e2-convexity-of-log-partition-and-moment-generating-functions","title":"E.2 Convexity of Log-Partition and Moment-Generating Functions","text":"<p>For a random variable \\(X\\), the moment-generating function (MGF) and cumulant-generating function (CGF) are</p> \\[ M_X(t)=\\mathbb{E}[e^{tX}], \\qquad K_X(t)=\\log M_X(t). \\] <p>Fact: \\(K_X(t)\\) is always convex in \\(t\\).</p> <p>Reason: \\(K_X''(t)=\\mathrm{Var}_t(X)\\ge0\\); variance is nonnegative.  </p>"},{"location":"appendices/170_probability/#implications","title":"Implications","text":"<ul> <li>\\(K_X(t)\\) acts as a convex \u201cpotential\u201d controlling exponential families.</li> <li>The log-partition function in statistics,      is convex in \\(\\theta\\) (strictly convex for full exponential families).</li> <li>Its gradient gives the mean parameter: \\(\\nabla A(\\theta)=\\mathbb{E}_\\theta[T(X)]\\).</li> </ul> <p>Thus convexity of \\(A\\) guarantees a one-to-one mapping between natural and mean parameters \u2014 a foundation of exponential-family inference.</p>"},{"location":"appendices/170_probability/#e3-exponential-families-and-dual-convexity","title":"E.3 Exponential Families and Dual Convexity","text":"<p>An exponential-family density has the form  </p> <p>Properties:</p> <ol> <li>\\(A(\\theta)\\) is convex, smooth, and serves as a potential function.</li> <li>Its convex conjugate \\(A^*(\\mu)\\) defines the entropy of the family:        where \\(H\\) is the Shannon entropy of the distribution with mean \\(\\mu\\).</li> </ol> <p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>"},{"location":"appendices/170_probability/#e4-convex-divergences-and-information-measures","title":"E.4 Convex Divergences and Information Measures","text":""},{"location":"appendices/170_probability/#a-kullbackleibler-kl-divergence","title":"(a) Kullback\u2013Leibler (KL) Divergence","text":"<p>For densities \\(p,q\\),  </p> <ul> <li>\\(D_{\\mathrm{KL}}\\) is jointly convex in \\((p,q)\\).  </li> <li>Proof: the function \\((u,v)\\mapsto u\\log(u/v)\\) is convex on \\(\\mathbb{R}_+^2\\).  </li> <li>Consequently, mixtures of distributions cannot increase KL divergence \u2014 a key fact in variational inference and EM.</li> </ul>"},{"location":"appendices/170_probability/#b-bregman-divergences","title":"(b) Bregman Divergences","text":"<p>Given a differentiable convex \\(\\phi\\), define  KL divergence is a Bregman divergence for \\(\\phi(p)=\\sum_i p_i\\log p_i\\). Thus information-theoretic distances are geometric shadows of convex functions.</p>"},{"location":"appendices/170_probability/#c-f-divergences","title":"(c) f-Divergences","text":"<p>A general convex generator \\(f\\) with \\(f(1)=0\\) yields  Convexity of \\(f\\) \u21d2 convexity of \\(D_f\\). Common choices recover KL, \u03c7\u00b2, Hellinger, and Jensen\u2013Shannon divergences.</p>"},{"location":"appendices/170_probability/#e5-convex-loss-functions-in-statistics-and-machine-learning","title":"E.5 Convex Loss Functions in Statistics and Machine Learning","text":"<p>Convexity ensures estimators are globally optimal and algorithms converge.</p> Setting Loss / Negative Log-Likelihood Convexity Gaussian noise \\(\\tfrac12\\|Ax-b\\|_2^2\\) quadratic, strongly convex Laplace noise \\(\\|Ax-b\\|_1\\) convex, nonsmooth Logistic regression \\(\\log(1+e^{-y a^\\top x})\\) convex, smooth Poisson regression \\(e^{a^\\top x}-y a^\\top x\\) convex, exponential Huber loss piecewise quadratic/linear convex, robust <p>Convexity of the negative log-likelihood follows from convexity of the log-partition function \\(A(\\theta)\\) in exponential families.</p>"},{"location":"appendices/170_probability/#e6-convexity-and-bayesian-inference","title":"E.6 Convexity and Bayesian Inference","text":"<p>In Bayesian inference, convexity appears in:</p> <ul> <li> <p>Log-concave posteriors:   If the likelihood and prior are log-concave, the posterior \\(p(x|y)\\propto \\exp(-f(x))\\) is also log-concave \u21d2 \\(\\log p(x|y)\\) concave, \\(f(x)\\) convex.</p> </li> <li> <p>MAP estimation:   Maximizing \\(\\log p(x|y)\\) \u2261 minimizing a convex function when \\(p(x|y)\\) is log-concave \u21d2 global optimum guaranteed.</p> </li> <li> <p>Variational inference:   The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).   Optimizing it is equivalent to minimizing a convex divergence.</p> </li> </ul> <p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>"},{"location":"appendices/170_probability/#e7-statistical-risk-and-convex-surrogates","title":"E.7 Statistical Risk and Convex Surrogates","text":"<p>Convex surrogate losses replace nonconvex 0\u20131 loss with convex approximations:</p> <ul> <li>Hinge loss (\\(\\max(0,1-y a^\\top x)\\)) \u2192 support-vector machines.  </li> <li>Logistic loss \u2192 probabilistic classification (cross-entropy).  </li> <li>Exponential loss \u2192 AdaBoost.</li> </ul> <p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p>"},{"location":"appendices/180_subgradient_methods/","title":"Appendix F - Subgradient Method and Variants (Advanced)","text":""},{"location":"appendices/180_subgradient_methods/#appendix-f-subgradient-method-derivation-geometry-and-convergence","title":"Appendix F: Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>This appendix presents the subgradient method\u2014the fundamental algorithm for minimizing nonsmooth convex functions. It generalizes gradient descent to functions such as the \\(\\ell_1\\) norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>"},{"location":"appendices/180_subgradient_methods/#f1-problem-setup","title":"F.1 Problem Setup","text":"<p>We consider</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex but possibly nondifferentiable and \\(\\mathcal{X}\\) is a convex feasible set.</p>"},{"location":"appendices/180_subgradient_methods/#f2-subgradients-and-geometry","title":"F.2 Subgradients and Geometry","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) satisfies</p> \\[ f(y) \\ge f(x_t) + \\langle g_t,\\, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>If \\(f\\) is differentiable, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\).  </li> <li>At a nonsmooth point (e.g. \\(|x|\\) at \\(x=0\\)), \\(\\partial f(x_t)\\) is a set of supporting slopes.  </li> <li>Each subgradient defines a supporting hyperplane below the graph of \\(f\\).</li> </ul> <p>Hence a subgradient gives a descent direction even when \\(f\\) lacks a unique gradient.</p>"},{"location":"appendices/180_subgradient_methods/#f3-update-rule-and-projection-view","title":"F.3 Update Rule and Projection View","text":"<p>The projected subgradient step is</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}}\\!\\big(x_t - \\eta_t g_t\\big), \\] <p>where - \\(g_t \\in \\partial f(x_t)\\), - \\(\\eta_t&gt;0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) projects onto \\(\\mathcal{X}\\).</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\), projection disappears:  </p> <p>Geometric view: move in a subgradient direction, then project back to feasibility. The method \u201cslides\u201d along the edges of \\(f\\)\u2019s epigraph.</p>"},{"location":"appendices/180_subgradient_methods/#f4-distance-analysis","title":"F.4 Distance Analysis","text":"<p>Let \\(x^\\star\\) be an optimal solution. Expanding the squared distance:</p> \\[ \\|x_{t+1}-x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t\\langle g_t, x_t - x^\\star\\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity,  </p> <p>Substitute to get</p> \\[ \\|x_{t+1}-x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t\\big(f(x_t)-f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"appendices/180_subgradient_methods/#f5-bounding-suboptimality","title":"F.5 Bounding Suboptimality","text":"<p>Rearranging:</p> \\[ f(x_t)-f(x^\\star) \\le \\frac{\\|x_t-x^\\star\\|^2 - \\|x_{t+1}-x^\\star\\|^2}{2\\eta_t} + \\frac{\\eta_t}{2}\\|g_t\\|^2. \\] <p>This shows a trade-off:</p> <ul> <li>Large \\(\\eta_t\\) \u2192 faster steps but higher error term.  </li> <li>Small \\(\\eta_t\\) \u2192 more precise but slower progress.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f6-convergence-rate","title":"F.6 Convergence Rate","text":"<p>Assume \\(\\|g_t\\| \\le G\\). Summing over \\(t=0,\\dots,T-1\\):</p> \\[ \\sum_{t=0}^{T-1}\\!\\big(f(x_t)-f(x^\\star)\\big) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Define \\(\\bar{x}_T = \\tfrac{1}{T}\\sum_{t=0}^{T-1} x_t\\). By convexity of \\(f\\),</p> \\[ f(\\bar{x}_T)-f(x^\\star) \\le \\frac{\\|x_0-x^\\star\\|^2}{2\\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing \\(\\eta_t = \\tfrac{R}{G\\sqrt{T}}\\) with \\(R=\\|x_0-x^\\star\\|\\) yields</p> <p>  i.e. a sublinear rate \\(O(1/\\sqrt{T})\\).</p>"},{"location":"appendices/180_subgradient_methods/#f7-interpretation-and-practice","title":"F.7 Interpretation and Practice","text":"<ul> <li>Works for any convex function, smooth or not.  </li> <li>Converges slower than smooth-gradient methods (\\(O(1/T)\\) or linear), but applies more generally.  </li> <li>Step size schedule is crucial: \\(\\eta_t \\!\\downarrow 0\\) for convergence, or fixed \\(\\eta\\) for steady error.  </li> <li>Averaging \\(\\bar{x}_T\\) improves stability.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#typical-ml-uses","title":"Typical ML Uses","text":"Model Objective Nonsmooth Term LASSO \\(\\tfrac12\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1\\) \\(\\ell_1\\) penalty SVM \\(\\tfrac12\\|w\\|^2 + C\\sum_i \\max(0,1-y_i w^\\top x_i)\\) hinge loss Robust regression $\\sum_i a_i^\\top x - b_i Neural nets \\(\\|w\\|_1\\) or ReLU activations piecewise linear"},{"location":"appendices/180_subgradient_methods/#f8-beyond-basic-subgradients","title":"F.8 Beyond Basic Subgradients","text":"<p>Many advanced methods refine or accelerate the basic idea:</p> <ul> <li>Stochastic subgradients: sample-based updates for large-scale ML.  </li> <li>Mirror descent: adapt geometry via Bregman divergences.  </li> <li>Proximal methods: replace step with proximal operator (see Appendix B).  </li> <li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li> </ul>"},{"location":"appendices/180_subgradient_methods/#f9-summary","title":"F.9 Summary","text":"<ul> <li>Subgradients generalize gradients to nondifferentiable convex functions.  </li> <li>The projected subgradient method provides a universal, robust minimization algorithm.  </li> <li>Achieves \\(O(1/\\sqrt{T})\\) convergence under bounded subgradients.  </li> <li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9\u201310.</li> </ul>"},{"location":"appendices/190_proximal/","title":"Appendix G - Proximal Operators","text":""},{"location":"appendices/190_proximal/#appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization","title":"Appendix G | Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>Many convex optimization problems involve constraints or nonsmooth penalties. This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>"},{"location":"appendices/190_proximal/#g1-problem-setup","title":"G.1 Problem Setup","text":"<p>We wish to minimize a convex, differentiable function \\( f(x) \\) subject to a convex feasible set \\( \\mathcal{X} \\subseteq \\mathbb{R}^n \\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>A plain gradient step,</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>may leave \\( x_{t+1} \\notin \\mathcal{X} \\). We fix this by projecting the iterate back into the feasible region.</p>"},{"location":"appendices/190_proximal/#g2-projection-operator","title":"G.2 Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2. \\] <p>Hence, the projected gradient descent update is</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\]"},{"location":"appendices/190_proximal/#geometric-insight","title":"Geometric Insight","text":"<ul> <li>Take a descent step possibly outside the feasible set.  </li> <li>Project back to the closest feasible point.  </li> <li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li> </ul> <p>Example \u2014 Euclidean ball: If \\( \\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\} \\), then</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)}. \\] <ul> <li>Inside the ball \u2192 unchanged.  </li> <li>Outside \u2192 scaled back to the boundary.</li> </ul>"},{"location":"appendices/190_proximal/#g3-from-projections-to-proximal-operators","title":"G.3 From Projections to Proximal Operators","text":"<p>Projections handle explicit constraints, but many problems use implicit penalties \u2014 e.g. sparsity (\\(\\|x\\|_1\\)), total variation, or nonnegativity penalties.</p> <p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>"},{"location":"appendices/190_proximal/#definition","title":"Definition","text":"<p>For a convex (possibly nondifferentiable) function \\( g(x) \\),</p> <p>  where \\( \\lambda &gt; 0 \\) balances regularization vs. proximity.</p>"},{"location":"appendices/190_proximal/#interpretation","title":"Interpretation","text":"<ul> <li>The quadratic term \\( \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\) keeps \\(x\\) close to \\(y\\).  </li> <li>The function \\( g(x) \\) encourages structure (sparsity, smoothness, feasibility).  </li> <li>Small \\(\\lambda\\): conservative correction; large \\(\\lambda\\): stronger regularization.</li> </ul> <p>The proximal step acts as a soft correction after a gradient step.</p>"},{"location":"appendices/190_proximal/#g4-projection-as-a-special-case","title":"G.4 Projection as a Special Case","text":"<p>Define the indicator function of a convex set \\(\\mathcal{X}\\):</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X}, \\\\[4pt] +\\infty, &amp; x \\notin \\mathcal{X}. \\end{cases} \\] <p>Substitute \\(g(x)=I_{\\mathcal{X}}(x)\\) into the proximal definition:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\tfrac{1}{2\\lambda}\\|x - y\\|^2 \\Big) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y). \\] <p>\u2705 Projection is just a proximal operator for an indicator function.</p>"},{"location":"appendices/190_proximal/#g5-proximal-gradient-method","title":"G.5 Proximal Gradient Method","text":"<p>When minimizing a composite convex objective  where \\(f\\) is smooth and \\(g\\) convex (possibly nonsmooth), the proximal gradient method updates:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big). \\] <ul> <li>The gradient step reduces the smooth part \\(f(x)\\).  </li> <li>The proximal step enforces structure via \\(g(x)\\). This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li> </ul>"},{"location":"appendices/190_proximal/#g6-example-proximal-operator-of-the-ell_1-norm","title":"G.6 Example: Proximal Operator of the \\(\\ell_1\\)-Norm","text":"<p>We seek</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda\\|x\\|_1 + \\tfrac{1}{2}\\|x - y\\|^2 \\right). \\]"},{"location":"appendices/190_proximal/#step-1-coordinate-separation","title":"Step 1. Coordinate Separation","text":"<p>The problem is separable across coordinates:  so each coordinate solves  </p>"},{"location":"appendices/190_proximal/#step-2-subgradient-optimality","title":"Step 2. Subgradient Optimality","text":"<p>Optimality condition:  Thus,  </p>"},{"location":"appendices/190_proximal/#step-3-case-analysis","title":"Step 3. Case Analysis","text":"Case Condition Solution \\(x^\\star&gt;0\\) \\(y&gt;\\lambda\\) \\(x^\\star = y - \\lambda\\) \\(x^\\star&lt;0\\) \\(y&lt;-\\lambda\\) \\(x^\\star = y + \\lambda\\) \\(x^\\star=0\\) ( y"},{"location":"appendices/190_proximal/#step-4-compact-form","title":"Step 4. Compact Form","text":"\\[ \\boxed{ \\text{prox}_{\\lambda|\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda,\\, 0) } \\] <p>This is the soft-thresholding operator.</p>"},{"location":"appendices/190_proximal/#step-5-vector-case","title":"Step 5. Vector Case","text":"<p>For \\(y \\in \\mathbb{R}^n\\),</p> \\[ \\big(\\text{prox}_{\\lambda\\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i)\\cdot\\max(|y_i| - \\lambda, 0). \\] <p>Each coordinate is independently shrunk toward zero \u2014 producing sparse solutions.</p>"},{"location":"appendices/190_proximal/#step-6-interpretation","title":"Step 6. Interpretation","text":"<ul> <li>Coordinates with \\(|y_i| \\le \\lambda\\) \u2192 set to zero (promotes sparsity).  </li> <li>Coordinates with \\(|y_i| &gt; \\lambda\\) \u2192 shrink by \\(\\lambda\\).  </li> <li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li> </ul>"},{"location":"appendices/190_proximal/#g7-geometry-and-connection-to-algorithms","title":"G.7 Geometry and Connection to Algorithms","text":"<ul> <li>Projection = nearest feasible point \u2192 handles hard constraints.  </li> <li>Proximal operator = nearest structured point \u2192 handles soft regularization.  </li> <li>Proximal gradient = combines both, yielding algorithms like:</li> <li>ISTA / FISTA (sparse recovery, LASSO),</li> <li>Projected gradient (feasibility),</li> <li>ADMM (splitting into subproblems).</li> </ul> <p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>"},{"location":"appendices/190_proximal/#g8-summary","title":"G.8 Summary","text":"<ul> <li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li> <li>Projection is a special case of the proximal operator for an indicator function.  </li> <li>Proximal mappings handle nonsmooth regularizers (e.g., \\(\\ell_1\\)-norm).  </li> <li>The proximal gradient method unifies constrained and regularized optimization.  </li> <li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li> </ul>"},{"location":"appendices/200_mirror/","title":"Appendix H - Mirror Descent and Bregman Geometry","text":""},{"location":"appendices/200_mirror/#appendix-h-mirror-descent-and-bregman-geometry","title":"Appendix H: Mirror Descent and Bregman Geometry","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry. In many structured domains\u2014such as probability simplices or sparse models\u2014Euclidean updates can destroy problem structure or cause instability.  </p> <p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence. It performs gradient-like updates in a dual space, respecting the intrinsic geometry of the domain.</p>"},{"location":"appendices/200_mirror/#h1-motivation-and-limitations-of-euclidean-gd","title":"H.1 Motivation and Limitations of Euclidean GD","text":"<p>Standard GD update:  assumes Euclidean distance  </p> <p>This works well in \\(\\mathbb{R}^n\\) without structure, but fails to respect constraints or sparsity.</p> <p>In practice:</p> <ul> <li>Many parameters are nonnegative or normalized (probabilities, weights).  </li> <li>Euclidean steps can violate constraints or zero out coordinates.  </li> <li>The \u201cflat\u201d \\(\\ell_2\\) geometry treats all directions equally.</li> </ul> <p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the metric via a mirror map.</p>"},{"location":"appendices/200_mirror/#h2-geometry-in-optimization","title":"H.2 Geometry in Optimization","text":"<p>The \u201csteepest descent\u201d direction depends on the notion of distance. GD implicitly minimizes a linearized loss plus a Euclidean proximity term.</p> Scenario Natural Constraint Appropriate Geometry Probability vectors \\(x_i\\ge0, \\sum_i x_i=1\\) KL / entropy geometry Sparse models \\(\\|x\\|_1\\)-structured \\(\\ell_1\\) geometry Online learning multiplicative updates log-space geometry <p>Using Euclidean projections in these domains can cause:</p> <ul> <li>abrupt projection onto boundaries,</li> <li>loss of positivity or sparsity,</li> <li>geometric inconsistency.</li> </ul>"},{"location":"appendices/200_mirror/#h3-mirror-descent-framework","title":"H.3 Mirror Descent Framework","text":"<p>Let \\(\\psi(x)\\) be a mirror map \u2014 a strictly convex, differentiable potential encoding the geometry.</p> <p>Define the dual coordinate:  and its inverse mapping through the convex conjugate \\(\\psi^*\\):  </p>"},{"location":"appendices/200_mirror/#bregman-divergence","title":"Bregman Divergence","text":"<p>The geometry is quantified by the Bregman divergence:  </p> <ul> <li>Measures how nonlinear \\(\\psi\\) is between \\(x\\) and \\(y\\).  </li> <li>When \\(\\psi(x)=\\tfrac12\\|x\\|_2^2\\), \\(D_\\psi\\) becomes \\(\\tfrac12\\|x-y\\|_2^2\\).  </li> <li>When \\(\\psi(x)=\\sum_i x_i\\log x_i\\), \\(D_\\psi\\) becomes KL divergence.</li> </ul>"},{"location":"appendices/200_mirror/#h4-mirror-descent-update-rule","title":"H.4 Mirror Descent Update Rule","text":"<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:  </p> <p>Equivalent dual-space form:  </p> <p>\u2705 MD is gradient descent in dual coordinates, where distances are measured by \\(D_\\psi\\) instead of \\(\\|x-y\\|_2\\).</p>"},{"location":"appendices/200_mirror/#h5-comparing-gd-projected-gd-and-md","title":"H.5 Comparing GD, Projected GD, and MD","text":"Method Update Rule Geometry Comments Gradient Descent \\(x - \\eta\\nabla f\\) Euclidean may leave feasible set Projected GD \\(\\text{Proj}(x - \\eta\\nabla f)\\) Euclidean + projection can cause discontinuous jumps Mirror Descent \\(\\arg\\min_x \\langle\\nabla f, x - x_t\\rangle + \\frac{1}{\\eta}D_\\psi(x\\|x_t)\\) Bregman smooth, structure-preserving"},{"location":"appendices/200_mirror/#h6-simplex-example-kl-geometry","title":"H.6 Simplex Example (KL Geometry)","text":"<p>Let \\(x\\in\\Delta^2=\\{x\\ge0, x_1+x_2=1\\}\\), objective \\(f(x)=x_1^2+2x_2\\), \\(\\eta=0.3\\).</p>"},{"location":"appendices/200_mirror/#euclidean-gd-projection","title":"Euclidean GD + Projection","text":"<ol> <li>\\(\\nabla f=(2x_1,2)=(1,2)\\),  </li> <li>\\(y=x-\\eta\\nabla f=(0.2,-0.1)\\),  </li> <li>Project \u2192 \\(x_{new}=(1,0)\\).</li> </ol> <p>\u2192 Projection kills one coordinate \u21d2 lost smoothness.</p>"},{"location":"appendices/200_mirror/#mirror-descent-with-negative-entropy","title":"Mirror Descent with Negative Entropy","text":"<p>Mirror map \\(\\psi(x)=\\sum_i x_i\\log x_i\\). Update:  Gives \\(x\\approx(0.57,0.43)\\) \u2014 smooth, positive, stays in simplex.</p> <p>MD follows the manifold of the simplex naturally\u2014no harsh projection.</p>"},{"location":"appendices/200_mirror/#h7-choosing-the-mirror-map","title":"H.7 Choosing the Mirror Map","text":"Mirror Map \\(\\psi(x)\\) Bregman Divergence \\(D_\\psi\\) Typical Domain / Application \\(\\tfrac12\\|x\\|_2^2\\) Euclidean distance unconstrained \\(\\mathbb{R}^n\\) \\(\\sum_i x_i\\log x_i\\) KL divergence simplex, probabilities \\(\\|x\\|_1\\) or variants \\(\\ell_1\\) geometry sparse models log-barrier \\(\\sum_i -\\log x_i\\) barrier divergence positive orthant <p>Mirror maps act as design choices defining the optimization geometry.</p>"},{"location":"appendices/200_mirror/#h8-practical-remarks","title":"H.8 Practical Remarks","text":"<p>When to prefer Mirror Descent:</p> <ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates desired</li> <li>Avoiding discontinuous projections</li> </ul> <p>Computational notes:</p> <ul> <li>Some \\(\\psi\\) yield closed-form updates (e.g. multiplicative weights).  </li> <li>Works with adaptive or momentum step-size schemes.  </li> <li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li> </ul>"},{"location":"appendices/200_mirror/#h9-convergence-at-a-glance","title":"H.9 Convergence at a Glance","text":"<p>For convex \\(f\\) with bounded gradients \\(\\|\\nabla f\\|\\le G\\) and strong convex mirror map \\(\\psi\\), Mirror Descent achieves the same sublinear rate as projected subgradient methods:  but with improved geometry-adapted constants that exploit curvature of \\(\\psi\\).</p>"},{"location":"appendices/300_matrixfactorization/","title":"Appendix I - Matrix Factorization","text":""},{"location":"appendices/300_matrixfactorization/#numerical-linear-algebra-for-convex-optimization","title":"Numerical Linear Algebra for Convex Optimization","text":"<p>Numerical linear algebra is the computational foundation of convex optimization. Every modern optimization algorithm \u2014 from Newton\u2019s method to interior-point or proximal algorithms \u2014 ultimately requires solving a structured linear system:  where \\(H\\) may represent a Hessian, a normal equations matrix, or a KKT (Karush\u2013Kuhn\u2013Tucker) system.</p> <p>In practice, we never compute \\(H^{-1}\\) directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>"},{"location":"appendices/300_matrixfactorization/#1-why-linear-algebra-matters-in-convex-optimization","title":"1. Why Linear Algebra Matters in Convex Optimization","text":"<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p> <ul> <li> <p>Newton\u2019s method:    </p> </li> <li> <p>Interior-point methods (KKT systems):</p> </li> </ul> \\[ \\begin{bmatrix} H &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\[3pt] \\Delta \\lambda \\end{bmatrix} = \\begin{bmatrix} -r_d \\\\[3pt] -r_p \\end{bmatrix} \\] <ul> <li>Least-squares problems: \\(A^T A x = A^T b\\)</li> </ul> <p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>"},{"location":"appendices/300_matrixfactorization/#2-the-matrix-factorization-toolbox","title":"2. The Matrix Factorization Toolbox","text":"<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure. They enable efficient triangular solves instead of direct inversion.</p> Factorization Applies To Form Common Use Key Notes LU Any nonsingular matrix \\(A = L U\\) General linear systems Requires pivoting for stability QR Any (rectangular) matrix \\(A = Q R\\) Least-squares Orthogonal, stable Cholesky Symmetric positive definite \\(A = L L^T\\) SPD systems, normal equations Fastest for SPD \\(LDL^T\\) Symmetric indefinite \\(A = L D L^T\\) KKT systems Handles indefiniteness Eigen Symmetric/Hermitian \\(A = Q \\Lambda Q^T\\) Curvature, convexity checks Diagonalizes \\(A\\) SVD Any matrix \\(A = U \\Sigma V^T\\) Rank, conditioning, pseudoinverse Most stable, expensive <p>Each factorization corresponds to a numerically preferred strategy for certain classes of problems.</p>"},{"location":"appendices/300_matrixfactorization/#3-lu-factorization-the-general-purpose-workhorse","title":"3. LU Factorization \u2014 The General-Purpose Workhorse","text":"<p>Form:  where \\(P\\) is a permutation matrix ensuring stability.</p> <ul> <li>Used for: General linear systems, nonsymmetric matrices.</li> <li>Cost: \\(\\approx \\tfrac{2}{3}n^3\\) (dense).</li> <li>Stability: Requires partial pivoting (\\(PA=LU\\)) to avoid numerical blow-up.</li> </ul> <p>Example use case:</p> <ul> <li>Solving KKT systems in linear programming (LP simplex tableau).</li> <li>Small dense systems with no symmetry or SPD property.</li> </ul> <p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or \\(LDL^T\\).</p>"},{"location":"appendices/300_matrixfactorization/#4-qr-factorization-orthogonal-and-stable","title":"4. QR Factorization \u2014 Orthogonal and Stable","text":"<p>Form:  </p> <ul> <li>Used for: Least-squares problems      Instead of forming normal equations (\\(A^T A x = A^T b\\)), we solve:    </li> <li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li> </ul> <p>Example use cases:</p> <ul> <li>Linear regression via least squares.</li> <li>ADMM and proximal steps with overdetermined systems.</li> <li>Orthogonal projections in signal processing.</li> </ul> <p>Variants:</p> <ul> <li>Householder QR: numerically robust, used in LAPACK.</li> <li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#5-cholesky-factorization-fastest-for-spd-systems","title":"5. Cholesky Factorization \u2014 Fastest for SPD Systems","text":"<p>Form:  Applicable when \\(A\\) is symmetric positive definite (SPD) \u2014 common in convex problems.</p> <p>Why it\u2019s central: Convexity ensures \\(A \\succeq 0\\). For strictly convex problems, \\(A \\succ 0\\) and Cholesky is the most efficient and stable method.</p> <p>Cost: \\(\\tfrac{1}{3}n^3\\) operations \u2014 half of LU.</p> <p>Example use cases:</p> <ul> <li>Newton\u2019s method on unconstrained convex functions.</li> <li>Solving normal equations \\(A^T A x = A^T b\\).</li> <li>QP subproblems and ridge regression.</li> </ul> <p>Implementation detail: No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>"},{"location":"appendices/300_matrixfactorization/#6-ldlt-factorization-for-indefinite-symmetric-systems","title":"6. LDL\u1d40 Factorization \u2014 For Indefinite Symmetric Systems","text":"<p>Form:  where \\(D\\) is block diagonal (1\u00d71 or 2\u00d72 blocks), and \\(L\\) is unit lower triangular.</p> <p>Used when \\(A\\) is symmetric but not SPD (e.g., KKT systems).</p> <p>Example use cases:</p> <ul> <li> <p>Interior-point methods for QPs and SDPs:    </p> </li> <li> <p>Equality-constrained least-squares.</p> </li> <li>Sparse symmetric indefinite systems in primal-dual algorithms.</li> </ul> <p>Algorithmic note: Uses Bunch\u2013Kaufman pivoting to maintain numerical stability. In practice, LDL\u1d40 is used with sparse reordering and partial elimination.</p>"},{"location":"appendices/300_matrixfactorization/#7-block-systems-and-the-schur-complement","title":"7. Block Systems and the Schur Complement","text":"<p>Many KKT or structured systems naturally appear in block form:  </p> <p>Assuming \\(A_{11}\\) is invertible:</p> <ol> <li>Eliminate \\(x_1\\):     </li> <li>Substitute into the second block:     </li> </ol> <p>The matrix  is the Schur complement of \\(A_{11}\\) in \\(A\\).</p>"},{"location":"appendices/300_matrixfactorization/#schur-complement-in-optimization","title":"Schur Complement in Optimization","text":"<ul> <li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li> <li>Preserves symmetry and often positive definiteness.</li> <li>Foundation of block elimination and reduced Hessian methods.</li> </ul> <p>Example use cases:</p> <ul> <li>Interior-point Newton systems (eliminate \\(\\Delta x\\) to get a system in \\(\\Delta \\lambda\\)).</li> <li>Partial elimination in sequential quadratic programming (SQP).</li> <li>Covariance conditioning and Gaussian marginalization.</li> </ul> <p>Numerical caution: Never form \\(A_{11}^{-1}\\) explicitly \u2014 use triangular solves via Cholesky or LU.</p>"},{"location":"appendices/300_matrixfactorization/#8-block-elimination-algorithm","title":"8. Block Elimination Algorithm","text":"<p>Given a nonsingular \\(A_{11}\\):</p> <ol> <li>Compute \\(A_{11}^{-1}A_{12}\\) and \\(A_{11}^{-1}b_1\\) by solving triangular systems.</li> <li>Form \\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\\), \\(\\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\\).</li> <li>Solve \\(Sx_2 = \\tilde{b}\\).</li> <li>Recover \\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\\).</li> </ol> <p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p> <p>Example use case:</p> <ul> <li>Partitioned least-squares with fixed and variable parameters.</li> <li>Constrained optimization where some variables can be analytically eliminated.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#9-structured-plus-low-rank-matrices","title":"9. Structured Plus Low-Rank Matrices","text":"<p>Suppose we need to solve:  where:</p> <ul> <li>\\(A \\in \\mathbb{R}^{n \\times n}\\) is structured or easily invertible (e.g., diagonal or sparse),</li> <li>\\(B \\in \\mathbb{R}^{n \\times p}\\), \\(C \\in \\mathbb{R}^{p \\times n}\\) are low rank.</li> </ul> <p>This situation arises when updating an existing system with a small modification.</p>"},{"location":"appendices/300_matrixfactorization/#block-reformulation","title":"Block Reformulation","text":"<p>Introduce \\(y = Cx\\), yielding:</p> <p>$$   =</p> <p> . $$</p> <p>Block elimination gives:  </p>"},{"location":"appendices/300_matrixfactorization/#matrix-inversion-lemma-woodbury-identity","title":"Matrix Inversion Lemma (Woodbury Identity)","text":"<p>If \\(A\\) and \\(A + BC\\) are nonsingular:  </p> <p>Example use cases:</p> <ul> <li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li> <li>Ridge regression / kernel methods: low-rank updates to \\((X^T X + \\lambda I)^{-1}\\).</li> <li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li> </ul> <p>Numerical note: Avoid explicit inversion; use solves with \\(A\\) and small dense matrices.</p>"},{"location":"appendices/300_matrixfactorization/#10-conditioning-stability-and-sparsity","title":"10. Conditioning, Stability, and Sparsity","text":""},{"location":"appendices/300_matrixfactorization/#conditioning","title":"Conditioning","text":"<ul> <li>Condition number: \\(\\kappa(A) = |A||A^{-1}|\\) measures sensitivity to perturbations.</li> <li>High \\(\\kappa(A)\\) \u21d2 round-off errors amplified \u21d2 ill-conditioning.</li> <li>Regularization (adding \\(\\lambda I\\)) improves numerical stability.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#stability","title":"Stability","text":"<ul> <li>Orthogonal transformations (QR, SVD) are backward stable.</li> <li>LU needs partial pivoting.</li> <li>LDL\u1d40 needs symmetric pivoting (Bunch\u2013Kaufman).</li> <li>Cholesky is stable for SPD matrices.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#sparsity-and-fill-in","title":"Sparsity and Fill-In","text":"<ul> <li>Large convex solvers exploit sparse Cholesky / LDL\u1d40.</li> <li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li> <li>Symbolic factorization determines the pattern before numeric factorization.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#11-iterative-solvers-and-preconditioning","title":"11. Iterative Solvers and Preconditioning","text":"<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>"},{"location":"appendices/300_matrixfactorization/#common-iterative-methods","title":"Common Iterative Methods","text":"Method For Description CG SPD systems Uses matrix\u2013vector products; converges in \u2264 n steps MINRES / SYMMLQ Symmetric indefinite Handles KKT and saddle-point systems GMRES / BiCGSTAB Nonsymmetric General-purpose Krylov solvers"},{"location":"appendices/300_matrixfactorization/#preconditioning","title":"Preconditioning","text":"<p>Preconditioners \\(M \\approx A^{-1}\\) improve convergence:</p> <ul> <li>Jacobi (diagonal): \\(M = \\text{diag}(A)^{-1}\\)</li> <li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li> <li>Block preconditioners: use Schur complement approximations for KKT systems</li> </ul> <p>Example use case:</p> <ul> <li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li> <li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#12-eigenvalue-and-svd-decompositions","title":"12. Eigenvalue and SVD Decompositions","text":""},{"location":"appendices/300_matrixfactorization/#eigenvalue-decomposition","title":"Eigenvalue Decomposition","text":"<p>  Reveals curvature, stability, and definiteness:</p> <ul> <li>Convexity \u21d4 \\(\\Lambda \\ge 0\\).</li> <li>Used in semidefinite programming (SDP) and spectral analysis.</li> </ul>"},{"location":"appendices/300_matrixfactorization/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>  with \\(\\Sigma = \\text{diag}(\\sigma_i) \\ge 0\\).</p> <p>Applications:</p> <ul> <li>Rank and condition number estimation (\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\)).</li> <li>Low-rank approximation (\\(A_k = U_k \\Sigma_k V_k^T\\)).</li> <li>Pseudoinverse: \\(A^+ = V \\Sigma^{-1} U^T\\).</li> <li>Convex relaxations: nuclear-norm minimization (matrix completion).</li> </ul>"},{"location":"appendices/300_matrixfactorization/#13-computational-complexity-summary","title":"13. Computational Complexity Summary","text":"Factorization Dense Cost Notes LU \\(\\frac{2}{3}n^3\\) Needs pivoting Cholesky \\(\\frac{1}{3}n^3\\) Fastest for SPD QR \\(\\approx \\frac{2}{3}n^3\\) Stable, more memory LDL\u1d40 \\(\\approx \\frac{2}{3}n^3\\) For indefinite SVD \\(\\approx \\frac{4}{3}n^3\\) Most accurate CG / MINRES Variable Depends on condition number and preconditioning <p>Sparse systems reduce cost to roughly \\(O(n^{1.5})\\)\u2013\\(O(n^2)\\) depending on fill-in.</p>"},{"location":"appendices/300_matrixfactorization/#14-example-applications-overview","title":"14. Example Applications Overview","text":"Problem Type Typical Matrix Solver / Factorization Example Unconstrained Newton step SPD Hessian Cholesky Convex quadratic, ridge regression Equality-constrained QP Symmetric indefinite KKT LDL\u1d40 Interior-point QP solver Overdetermined LS Rectangular \\(A\\) QR Linear regression, ADMM subproblem KKT block system Block-symmetric Schur complement Primal-dual method Low-rank correction \\(A + U U^T\\) Woodbury Kalman filter, online update Rank-deficient system Any SVD Matrix completion, regularization Large-scale Hessian SPD CG + preconditioner Logistic regression, large ML models"},{"location":"cheatsheets/20a_cheatsheet/","title":"Optimization Algos - Cheat Sheet","text":""},{"location":"cheatsheets/20a_cheatsheet/#comprehensive-optimization-algorithm-cheat-sheet","title":"Comprehensive Optimization Algorithm Cheat Sheet","text":"<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search. It balances theoretical precision with practical intuition\u2014from gradient-based solvers to black-box evolutionary methods.</p>"},{"location":"cheatsheets/20a_cheatsheet/#how-to-read-this-table","title":"\ud83e\udded How to Read This Table","text":"<p>Each method lists: - Problem Type \u2014 the class of objectives it applies to. - Assumptions \u2014 smoothness, convexity, or structural conditions. - Core Update Rule \u2014 canonical iteration. - Scalability \u2014 computational feasibility. - Per-Iteration Cost \u2014 approximate computational complexity. - Applications \u2014 typical ML or engineering use cases.</p>"},{"location":"cheatsheets/20a_cheatsheet/#first-order-methods","title":"\ud83d\ude80 First-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Gradient Descent (GD) Unconstrained smooth (convex/nonconvex) Differentiable; \\(L\\)-smooth \\(x_{k+1} = x_k - \\eta \\nabla f(x_k)\\) Medium \\(O(nd)\\) Logistic regression, least squares Nesterov\u2019s Accelerated GD Smooth convex (fast rate) Convex, \\(L\\)-smooth \\(y_k = x_k + \\frac{k-1}{k+2}(x_k - x_{k-1})\\); \\(x_{k+1} = y_k - \\eta \\nabla f(y_k)\\) Medium \\(O(nd)\\) Accelerated convex models (Polyak) Heavy-Ball Momentum Unconstrained smooth Differentiable, \\(\\beta \\in (0,1)\\) \\(x_{k+1} = x_k - \\eta \\nabla f(x_k) + \\beta(x_k - x_{k-1})\\) Large \\(O(nd)\\) Deep networks, convex smooth losses Conjugate Gradient (CG) Quadratic or linear systems \\(Ax=b\\) \\(A\\) symmetric positive definite \\(p_{k+1}=r_{k+1}+\\beta_k p_k\\), \\(x_{k+1}=x_k+\\alpha_k p_k\\) Large \\(O(nd)\\) Large-scale least squares, implicit Newton steps Mirror Descent Non-Euclidean geometry Convex; mirror map \\(\\psi\\) strongly convex \\(x_{k+1} = \\nabla \\psi^*(\\nabla \\psi(x_k) - \\eta \\nabla f(x_k))\\) Medium \\(O(nd)\\) Probability simplex, online learning <p>Conjugate Gradient (CG) bridges first- and second-order methods: it achieves exact convergence in at most \\(d\\) steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>"},{"location":"cheatsheets/20a_cheatsheet/#second-order-methods","title":"\u2699\ufe0f Second-Order Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Per-Iteration Cost Applications Newton\u2019s Method Smooth convex Twice differentiable; \\(\\nabla^2 f(x)\\) PD \\(x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\\) Small\u2013Medium \\(O(d^3)\\) Logistic regression (IRLS), convex solvers BFGS / L-BFGS Smooth convex Differentiable, approximate Hessian Solve \\(B_k p_k=-\\nabla f(x_k)\\); update \\(B_k\\) via secant rule Medium \\(O(d^2)\\) GLMs, medium ML models Trust-Region Smooth convex/nonconvex Twice differentiable \\(\\min_p \\tfrac{1}{2}p^\\top \\nabla^2 f(x_k)p + \\nabla f(x_k)^\\top p\\) s.t. \\(\\|p\\|\\le\\Delta_k\\) Medium \\(O(d^2)\\) TRPO, physics-based ML"},{"location":"cheatsheets/20a_cheatsheet/#proximal-projected-splitting-methods","title":"\ud83e\uddee Proximal, Projected &amp; Splitting Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Proximal Gradient (ISTA) Composite \\(f=g+h\\) \\(g\\) smooth, \\(h\\) convex \\(x_{k+1}=\\operatorname{prox}_{\\alpha h}(x_k-\\alpha\\nabla g(x_k))\\) Medium \\(O(nd)\\) LASSO, sparse recovery FISTA Same as ISTA Convex, \\(L\\)-smooth \\(g\\) Like ISTA with momentum Medium \\(O(nd)\\) Compressed sensing Projected Gradient (PG) Convex constrained \\(f\\) smooth; easy projection \\(x_{k+1}=\\Pi_C(x_k-\\eta\\nabla f(x_k))\\) Medium \\(O(nd)\\) + projection Box/simplex constraints ADMM Separable convex + linear constraints \\(f,g\\) convex Alternating minimization + dual update Medium \\(O(nd)\\) per block Distributed ML, consensus Majorization\u2013Minimization (MM) Convex/nonconvex $g(x x_k)\\ge f(x)$ $x_{k+1}=\\arg\\min g(x x_k)$ Medium"},{"location":"cheatsheets/20a_cheatsheet/#coordinate-block-methods","title":"\ud83e\udde9 Coordinate &amp; Block Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Coordinate Descent (CD) Separable convex Convex, differentiable Update one coordinate: \\(x_{i}^{k+1}=x_i^k-\\eta\\partial_i f(x^k)\\) Large \\(O(d)\\) LASSO, SVM duals Block Coordinate Descent (BCD) Block separable Convex per block Minimize over \\(x^{(j)}\\) while fixing others Large \\(O(nd_j)\\) Matrix factorization, alternating minimization <p>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</p>"},{"location":"cheatsheets/20a_cheatsheet/#stochastic-mini-batch-methods","title":"\ud83c\udfb2 Stochastic &amp; Mini-Batch Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Stochastic Gradient Descent (SGD) Large-scale / streaming Unbiased stochastic gradients \\(x_{k+1}=x_k-\\eta_t\\nabla f_{i_k}(x_k)\\) Very Large \\(O(bd)\\) Deep learning, online learning Variance-Reduced (SVRG/SAGA/SARAH) Finite-sum convex Smooth, strongly convex \\(v_k=\\nabla f_{i_k}(x_k)-\\nabla f_{i_k}(\\tilde{x})+\\nabla f(\\tilde{x})\\) Large \\(O(bd)\\) Logistic regression, GLMs Adaptive SGD (Adam/RMSProp/Adagrad) Nonconvex stochastic Bounded variance \\(m_k=\\beta_1m_{k-1}+(1-\\beta_1)g_k\\), \\(v_k=\\beta_2v_{k-1}+(1-\\beta_2)g_k^2\\) Very Large \\(O(bd)\\) Neural networks Proximal Stochastic (Prox-SGD / Prox-SAGA) Nonsmooth stochastic \\(f=g+h\\) with prox of \\(h\\) known \\(x_{k+1}=\\operatorname{prox}_{\\eta h}(x_k-\\eta\\widehat{\\nabla g}(x_k))\\) Large \\(O(bd)\\) Sparse online learning"},{"location":"cheatsheets/20a_cheatsheet/#interior-point-augmented-methods","title":"\ud83e\uddf1 Interior-Point &amp; Augmented Methods","text":"Method Problem Type Assumptions Core Update Rule Scalability Cost Applications Interior-Point Convex with inequalities Slater\u2019s condition, self-concordant barrier Solve \\(\\min f_0(x)-\\tfrac{1}{t}\\sum_i\\log(-g_i(x))\\) Small\u2013Medium \\(O(d^3)\\) LP, QP, SDP Augmented Lagrangian (ALM) Constrained convex \\(f,g\\) convex; equality constraints \\(L_\\rho(x,\\lambda)=f(x)+\\lambda^T g(x)+\\tfrac{\\rho}{2}\\|g(x)\\|^2\\) Medium \\(O(nd)\\) Penalty methods, PDEs"},{"location":"cheatsheets/20a_cheatsheet/#derivative-free-black-box-optimization","title":"\ud83c\udf10 Derivative-Free &amp; Black-Box Optimization","text":"Method Problem Type Assumptions Core Idea Scalability Cost Applications Nelder\u2013Mead Simplex Low-dimensional, smooth or noisy No gradients; continuous \\(f\\) Maintain simplex of \\(d+1\\) points; reflect\u2013expand\u2013contract\u2013shrink operations Small \\(O(d^2)\\) Parameter tuning, physics models Simulated Annealing Nonconvex, global Stochastic exploration via temperature Random perturbations accepted w.p. \\(\\exp(-\\Delta f/T)\\); \\(T\\downarrow\\) Medium High (many samples) Hyperparameter tuning, design optimization Multi-start Local Search Nonconvex None; relies on restart diversity Run local solver from multiple random inits, pick best result Medium \\(k\\times\\) local solver Avoids local minima; cheap global heuristic Evolutionary Algorithms (EA) Black-box, global Population-based; fitness function only Mutate, select, recombine candidates Large \\(O(Pd)\\) per gen Global optimization, control, AutoML Genetic Algorithms (GA) Combinatorial / continuous Chromosomal encoding of solutions Apply selection, crossover, mutation; evolve over generations Medium\u2013Large \\(O(Pd)\\) Feature selection, neural architecture search Evolution Strategies (ES) Continuous, black-box Gaussian mutation around mean \\(\\theta_{k+1} = \\theta_k + \\eta \\sum_i w_i \\epsilon_i f(\\theta_k+\\sigma \\epsilon_i)\\) Large \\(O(Pd)\\) Reinforcement learning, black-box control Derivative-Free Optimization (DFO) Black-box, noisy \\(f\\) Only function values available Gradient estimated via random perturbations: \\(g\\approx\\frac{f(x+hu)-f(x)}{h}u\\) Medium \\(O(d)\\)\u2013\\(O(d^2)\\) Robotics, policy search, design Black-Box Optimization Framework General No analytical gradients; often stochastic Unified term covering EA, GA, ES, and DFO Medium\u2013Large varies Hyperparameter search, AutoML, reinforcement learning Numerical Encodings Used in GA/EA Represents variables in binary, integer, or floating-point form Choice of encoding impacts mutation/crossover behavior N/A negligible Optimization of mixed or discrete variables <p>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</p>"},{"location":"cheatsheets/20a_cheatsheet/#convergence-complexity-snapshot","title":"\ud83d\udcc8 Convergence &amp; Complexity Snapshot","text":"Method Type Convergence (Convex) Notes Subgradient \\(O(1/\\sqrt{k})\\) Nonsmooth convex Gradient Descent \\(O(1/k)\\) Smooth convex Accelerated Gradient \\(O(1/k^2)\\) Optimal first-order Newton / Quasi-Newton Quadratic / Superlinear Local only Strongly Convex \\((1-\\mu/L)^k\\) Linear rate Variance-Reduced Linear (strongly convex) Finite-sum optimization ADMM / Proximal \\(O(1/k)\\) Composite convex Interior-Point Polynomial time High-accuracy convex Derivative-Free / Heuristics No formal bound Empirical convergence only"},{"location":"cheatsheets/20a_cheatsheet/#practitioner-summary","title":"\ud83e\udde0 Practitioner Summary","text":"Situation Recommended Methods Gradients available, smooth convex Gradient Descent, Nesterov Curvature matters, moderate scale Newton, BFGS, Conjugate Gradient Nonsmooth regularizer Proximal Gradient, ADMM Simple constraints Projected Gradient Large-scale / streaming SGD, Adam, RMSProp Finite-sum convex SVRG, SAGA Online / adaptive Mirror Descent, FTRL No gradients (black-box) DFO, Nelder\u2013Mead, ES, GA Global nonconvex search Simulated Annealing, Multi-starts, Evolutionary Algorithms Distributed / separable ADMM, ALM High-precision convex programs Interior-Point, Trust-Region"},{"location":"cheatsheets/20a_cheatsheet/#notes-on-global-black-box-optimization","title":"\ud83e\udde9 Notes on Global &amp; Black-Box Optimization","text":"<ul> <li>Conjugate Gradient: memory-efficient quasi-second-order method for large convex quadratics.  </li> <li>Nelder\u2013Mead: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li> <li>Simulated Annealing: probabilistic global search inspired by thermodynamics.  </li> <li>Multi-Starts: pragmatic global exploration by repeated local optimization.  </li> <li>Evolutionary / Genetic / ES: population-based global heuristics; robust to noise and discontinuity.  </li> <li>Derivative-Free Optimization (DFO): umbrella for random, surrogate-based, or adaptive black-box methods.  </li> <li>Numerical Encoding: crucial in discrete search\u2014how real or binary variables are represented determines performance.</li> </ul> <p>Summary Insight: - Convex + differentiable \u2192 use gradient-based or Newton-type methods. - Convex + nonsmooth \u2192 use proximal, ADMM, or coordinate descent. - Large-scale or stochastic \u2192 use SGD or adaptive variants. - No gradients or nonconvex \u2192 use derivative-free or evolutionary methods. - The structure of the objective, not its size alone, determines the optimal solver family.</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1  Introduction and Overview","text":"<p>Optimization is the mathematical foundation of nearly all modern machine learning, signal processing, and control systems. Every learning algorithm \u2014 from linear regression to deep neural networks \u2014 is ultimately an optimization procedure: it adjusts model parameters to minimize a loss or maximize a performance criterion based on observed data.</p> <p>Convex optimization is a special and profoundly important subset of optimization. It provides structure, guarantees, and tractability that general nonlinear optimization often lacks. When the objective and constraints are convex, we obtain three fundamental advantages:</p> <ol> <li> <p>Global optimality:    Any local minimum is also a global minimum \u2014 eliminating the risk of getting trapped in suboptimal solutions.</p> </li> <li> <p>Algorithmic stability and efficiency:     Convex problems admit well-understood convergence behavior and can be solved reliably by gradient, Newton, or interior-point methods.</p> </li> <li> <p>Theoretical guarantees and interpretability:    Duality theory and KKT (Karush-Kuhn-Tucker) conditions provide verifiable optimality certificates and often lend economic or geometric meaning to solutions.</p> </li> </ol> <p>These properties make convex optimization the \u201clanguage of guarantees\u201d in machine learning. While deep learning and other modern methods are largely nonconvex, many of their building blocks \u2014 such as linear models, regularizers, and convex losses \u2014 originate from convex analysis. Understanding convex optimization equips us with the principles that ensure robustness, efficiency, and insight across all areas of data-driven modeling.</p> <p>Convexity \u21d2 Robustness. Convex problems are stable: small perturbations to inputs cause proportionally small shifts in the solution. Many difficult non-convex problems are attacked by constructing convex relaxations, whose solutions yield bounds or high-quality approximations.</p> <p>This web-book is written for ML practitioners who want to understand why convex optimization works,  and how to use its geometry, duality, and algorithms to build and tune models in practice.</p>"},{"location":"convex/11_intro/#11-motivation-optimization-in-machine-learning","title":"1.1 Motivation: Optimization in Machine Learning","text":"<p>Most supervised learning problems can be viewed as minimizing a regularized empirical risk:</p> \\[ \\min_x \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i) + \\lambda R(x) \\quad \\text{s.t. } x \\in \\mathcal{X}. \\] <p>Here:</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function measuring fit to data,  </li> <li>\\(R(x)\\) is a regularizer controlling complexity or promoting structure,  </li> <li>\\(\\mathcal{X}\\) encodes simple constraints (box, simplex, or norm ball).</li> </ul> <p>Many of these objectives \u2014 least squares, logistic loss, hinge loss, \\(\\ell_1\\) or \\(\\ell_2\\) regularizers \u2014 are convex. That convexity is what makes them reliably solvable at scale.</p> <p>Key idea: Convex optimization is the quiet engine under the hood of most practical ML methods, even in settings that appear nonconvex.</p>"},{"location":"convex/11_intro/#12-convex-sets-and-convex-functions-first-intuition","title":"1.2 Convex Sets and Convex Functions \u2014 First Intuition","text":"<p>A set \\(\\mathcal{C}\\subseteq\\mathbb{R}^n\\) is convex if for all \\(x,y\\in\\mathcal{C}\\) and any \\(\\theta\\in[0,1]\\),  This means the line segment joining any two points in \\(\\mathcal{C}\\) stays inside \\(\\mathcal{C}\\).</p> <p>A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if its epigraph is a convex set, or equivalently if for all \\(x,y\\) and \\(\\theta\\in[0,1]\\),  </p> <p>Intuitively, the graph of \\(f\\) lies below the chord connecting any two points \u2014 it curves upward but never downward.</p> <p>Clarification: Affine functions (linear + constant) are both convex and concave. They define flat surfaces \u2014 neither bowl-shaped nor peaked.</p>"},{"location":"convex/11_intro/#13-why-convex-optimization-still-matters-in-ml","title":"1.3 Why Convex Optimization Still Matters in ML","text":"<p>Convex optimization remains vital in ML for three reasons:</p> <ol> <li> <p>Convex surrogates    Losses such as logistic, hinge, or Huber are convex approximations to difficult nonconvex objectives (like 0\u20131 loss). They make training tractable while preserving predictive performance.</p> </li> <li> <p>Convex subproblems in nonconvex training    Even deep learning routinely solves convex inner loops: least-squares layers, proximal updates, line searches, or trust-region substeps.</p> </li> <li> <p>Implicit bias and geometry    Gradient descent on convex models (e.g., least squares) naturally converges to the minimum-norm solution \u2014 a property used to analyze implicit regularization in overparameterized regimes.</p> </li> </ol> <p>Convex optimization provides both the tools and the theory for understanding why first-order methods generalize and converge.</p>"},{"location":"convex/11_intro/#14-from-global-optima-to-algorithms","title":"1.4 From Global Optima to Algorithms","text":"<p>Convexity eliminates local traps. For a differentiable convex \\(f\\) on a convex domain \\(\\mathcal{X}\\):</p> \\[ \\nabla f(x^\\star) = 0 \\;\\Rightarrow\\; x^\\star \\text{ is a global minimizer.} \\] <p>There are no local minima or saddle points distinct from the global solution.  For nondifferentiable convex \\(f\\), the same holds with subgradients: \\(0\\in\\partial f(x^\\star)\\).</p> <p>Practical meaning: You can trust gradient-based methods to find the best possible solution \u2014 not just a good one \u2014 if the problem is convex.</p>"},{"location":"convex/11_intro/#15-canonical-convex-ml-problems-at-a-glance","title":"1.5 Canonical Convex ML Problems at a Glance","text":"Problem Objective Typical Solver Least squares \\(\\|A x - b\\|_2^2\\) Gradient descent, CG Ridge regression \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_2^2\\) Closed form / GD LASSO \\(\\|A x - b\\|_2^2 + \\lambda\\|x\\|_1\\) Prox-gradient (ISTA/FISTA) Logistic regression \\(\\sum_i \\log(1+\\exp(-y_i a_i^\\top x)) + \\lambda\\|x\\|_2^2\\) Newton, SGD SVM (hinge loss) \\(\\tfrac{1}{2}\\|x\\|^2 + C\\sum_i \\max(0,1-y_i a_i^\\top x)\\) Subgradient, SMO Robust regression \\(\\|A x - b\\|_1\\) Linear programming Elastic Net \\(\\|A x-b\\|_2^2+\\lambda_1\\|x\\|_1+\\lambda_2\\|x\\|_2^2\\) Coordinate descent <p>These patterns appear repeatedly in later chapters and unify much of convex ML.</p>"},{"location":"convex/11_intro/#16-web-book-roadmap-and-how-to-use-it","title":"1.6 Web-Book Roadmap and How to Use It","text":"Question Where to Look Key Idea What makes a function or set convex? Ch. 2 \u2013 5 Geometry &amp; calculus of convexity How do gradients, subgradients, and KKT conditions certify optimality? Ch. 6 \u2013 9 Optimality &amp; duality How are convex problems actually solved? Ch. 10 \u2013 14 First-order, second-order, interior-point methods How do I pick a solver for my ML model? Ch. 15 \u2013 17 Large-scale, structured, and modeling patterns"},{"location":"convex/11_intro/#17-next-steps","title":"1.7 Next Steps","text":"<p>In the next chapters we move from intuition to structure:</p> <p>Geometry first (convex sets, functions), then calculus (gradients, subgradients), then optimality and algorithms.</p> <p>This geometric foundation will make every later algorithm \u2014 gradient descent, Newton, or interior-point \u2014 feel natural and inevitable.</p>"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra is the geometry of optimization. A simple motivating example: fitting a linear model \\(x\\) to data \\((A,b)\\) solves:</p> \\[ \\min_x \\ \\|A x - b\\|_2^2. \\] <p>We will see that this is a projection of \\(b\\) onto the column space of \\(A\\), and that ideas like rank, nullspace, orthogonality, and conditioning directly control optimization algorithms and convergence.</p> <p>This chapter builds the geometric toolkit used throughout convex optimization and machine learning.</p>"},{"location":"convex/12_vector/#21-vector-spaces-subspaces-and-affine-sets","title":"2.1 Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set \\(V\\) equipped with addition and scalar multiplication satisfying the usual axioms: closure, associativity, commutativity of addition, distributivity, existence of an additive identity and additive inverses, and compatibility with scalar multiplication.</p> <p>A subspace of a vector space \\(V\\) is a subset \\(S \\subseteq V\\) that 1. contains the zero vector, 2. is closed under addition, and 3. is closed under scalar multiplication.  </p>"},{"location":"convex/12_vector/#affine-sets","title":"Affine Sets","text":"<p>A set \\(A \\subseteq V\\) is affine if for any \\(x, y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\), </p> <p>That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\).</p> <p>By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set.</p> <p>Every affine set can be written as  where \\(S\\) is a subspace of \\(V\\). Affine sets arise as the solution sets to linear equality constraints \\(A x = b\\).</p>"},{"location":"convex/12_vector/#affine-transformations","title":"Affine Transformations","text":"<p>An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as  where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex.</p>"},{"location":"convex/12_vector/#22-linear-combinations-span-basis-dimension","title":"2.2 Linear combinations, span, basis, dimension","text":"<p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form  is a linear combination. The set of all linear combinations is called the span:  </p> <p>A set of vectors is linearly independent if no vector can be written as a combination of others.  A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). The number of basis vectors is the dimension \\(\\dim(V)\\).</p> <p>Rank and nullity facts:</p> <ul> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states:  where \\(n\\) is the number of columns of \\(A\\).</li> </ul>"},{"location":"convex/12_vector/#column-space","title":"Column Space:","text":"<p>The column space of a matrix \\( A \\), denoted \\( C(A) \\), is the set of all possible output vectors \\( b \\) that can be written as \\( Ax \\) for some \\( x \\). In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system \\( Ax = b \\) have a solution?\u201d is equivalent to asking whether \\( b \\in C(A) \\). If \\( b \\) lies in the column space, a solution exists; otherwise, it does not.</p>"},{"location":"convex/12_vector/#null-space","title":"Null Space:","text":"<p>The null space (or kernel) of \\( A \\), denoted \\( N(A) \\), is the set of all input vectors \\( x \\) that are mapped to zero:  \\( N(A) = \\{ x : Ax = 0 \\} \\). It answers a different question: If a solution to \\( Ax = b \\) exists, is it unique? If the null space contains only the zero vector (\\( \\mathrm{nullity}(A) = 0 \\)), the solution is unique. But if \\( N(A) \\) contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p>"},{"location":"convex/12_vector/#multicollinearity","title":"Multicollinearity:","text":"<p>When one feature in the data matrix \\( A \\) is a linear combination of others\u2014for example, \\( \\text{feature}_3 = 2 \\times \\text{feature}_1 + \\text{feature}_2 \\)\u2014the columns of \\( A \\) become linearly dependent. This creates a nonzero vector in the null space of \\( A \\), meaning multiple weight vectors \\( x \\) can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and \\( A^\\top A \\) becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution\u2014favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p>"},{"location":"convex/12_vector/#feasible-directions","title":"Feasible Directions:","text":"<p>In a constrained optimization problem of the form \\( Ax = b \\), the null space of \\( A \\) characterizes the directions along which one can move without violating the constraints. If \\( d \\in N(A) \\), then moving from a feasible point \\( x \\) to \\( x + d \\) preserves feasibility, since  \\( A(x + d) = Ax + Ad = b + 0 = b \\). Thus, the null space defines the space of free movement\u2014directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p>"},{"location":"convex/12_vector/#row-space","title":"Row Space:","text":"<p>The row space of \\( A \\), denoted \\( R(A) \\), is the span of the rows of \\( A \\) (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to \\( \\mathrm{rank}(A) \\). The row space is orthogonal to the null space of \\( A \\):  \\( R(A) \\perp N(A) \\). In optimization, the row space corresponds to the set of active constraints or the directions along which changes in \\( x \\) affect the constraints.</p>"},{"location":"convex/12_vector/#left-null-space","title":"Left Null Space:","text":"<p>The left null space, denoted \\( N(A^\\top) \\), is the set of all vectors \\( y \\) such that \\( A^\\top y = 0 \\). These vectors are orthogonal to the columns of \\( A \\), and therefore orthogonal to the column space itself. In least squares problems, \\( N(A^\\top) \\) represents residual directions\u2014components of \\( b \\) that cannot be explained by the model \\( Ax = b \\).</p>"},{"location":"convex/12_vector/#projection-interpretation-least-squares","title":"Projection Interpretation (Least Squares):","text":"<p>When \\( Ax = b \\) has no exact solution (as in overdetermined systems), the least squares solution finds \\( x \\) such that \\( Ax \\) is the projection of \\( b \\) onto the column space of \\( A \\):  and the residual  lies in the left null space \\( N(A^\\top) \\). This provides a geometric view: the solution projects \\( b \\) onto the closest point in the subspace that \\( A \\) can reach.</p>"},{"location":"convex/12_vector/#ranknullity-relationship","title":"Rank\u2013Nullity Relationship:","text":"<p>The rank of \\( A \\) is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where \\( n \\) is the number of columns of \\( A \\). This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in \\( x \\).</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by \\( A \\).  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map \\( A: \\mathbb{R}^n \\to \\mathbb{R}^m \\).</p>"},{"location":"convex/12_vector/#23-inner-products-and-orthogonality","title":"2.3 Inner products and orthogonality","text":"<p>An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\):  with equality iff \\(x\\) and \\(y\\) are linearly dependent Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#24-norms-and-distances","title":"2.4 Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms: Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), and the dual of \\(\\ell_2\\) is itself.</p> <p>Imagine the vector \\(x\\) lives inside the original norm ball (\\(\\|x\\| \\le 1\\)). The term \\(x^\\top y\\) is the dot product, which measures the alignment between \\(x\\) and \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible value you can get by taking the dot product of \\(y\\) with any vector \\(x\\) that fits inside the original norm ball.If the dual norm \\(\\|y\\|_*\\) is large, it means \\(y\\) is strongly aligned with a direction \\(x\\) that is \"small\" (size \\(\\le 1\\)) according to the original norm.If the dual norm is small, \\(y\\) must be poorly aligned with all vectors \\(x\\) in the ball.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"convex/12_vector/#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p>"},{"location":"convex/12_vector/#26-orthogonal-projections-and-least-squares","title":"2.6 Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\).</p> <p>If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#27-advanced-concepts","title":"2.7 Advanced Concepts","text":"<p>Operator norm: Given a matrix (linear map) \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and given a choice of vector norms on input and output, one can define the induced operator norm. If we use \\(|\\cdot|_p\\) on \\(\\mathbb{R}^n\\) and \\(|\\cdot|_q\\) on \\(\\mathbb{R}^m\\), the operator norm is</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q \\] <p>This gives the maximum factor by which \\(A\\) can stretch a vector (measuring \\(x\\) in norm \\(p\\) and \\(Ax\\) in norm \\(q\\)).pecial cases are common: with \\(p = q = 2\\), \\(|A|_{2 \\to 2}\\) (often just written \\(|A|_2\\)) is the spectral norm, which equals the largest singular value of \\(A\\) (more on singular values below). If \\(p = q = 1\\), \\(|A|_{1 \\to 1}\\) is the maximum absolute column sum of \\(A\\). If \\(p = q = \\infty\\), \\(|A|{\\infty \\to \\infty}\\) is the maximum absolute row sum.</p> <p>Operator norms tell us the worst-case amplification of signals by \\(A\\). In gradient descent on \\(f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x\\) (a quadratic form), the step size must be \\(\\le \\tfrac{2}{|A|_2}\\) for convergence; here \\(|A|_2 = \\lambda_{\\max}(A)\\) if \\(A\\) is symmetric (it\u2019s related to Hessian eigenvalues, Chapter 5). In general, controlling \\(|A|\\) controls stability: if \\(|A| &lt; 1\\), the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p> <p>Singular Value Decomposition (SVD): Any matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as</p> \\[ A = U \\Sigma V^\\top \\] <p>where \\(U \\in \\mathbb{R}^{m\\times m}\\) and \\(V \\in \\mathbb{R}^{n\\times n}\\) are orthogonal matrices (their columns are orthonormal bases of \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\), respectively), and \\(\\Sigma\\) is an \\(m\\times n\\) diagonal matrix with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0\\) on the diagonal. The \\(\\sigma_i\\) are the singular values of \\(A\\). Geometrically, \\(A\\) sends the unit ball in \\(\\mathbb{R}^n\\) to an ellipsoid in \\(\\mathbb{R}^m\\) whose principal semi-axes lengths are the singular values and directions are the columns of \\(V\\) (mapped to columns of \\(U\\)). The largest singular value \\(\\sigma_{\\max} = |A|_2\\) is the spectral norm. The smallest (if \\(n \\le m\\), \\(\\sigma{\\min}\\) of those \\(n\\)) indicates how \\(A\\) contracts the least \u2013 if \\(\\sigma_{\\min} = 0\\), \\(A\\) is rank-deficient.</p> <p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number \\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\) (when \\(\\sigma_{\\min}&gt;0\\)), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in \\(x\\)-space hardly change \\(Ax\\) (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means \\(A\\) is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of \\(A\\) (Eckart\u2013Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p> <p>Low-rank structure: The rank of \\(A\\) equals the number of nonzero singular values. If \\(A\\) has rank \\(r \\ll \\min(n,m)\\), it means \\(A\\) effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem\u2019s data matrix \\(A\\) is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p> <p>Operator norm in optimization: Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\), the Hessian is \\(Q\\) and gradient descent converges if \\(\\alpha &lt; 2/\\lambda_{\\max}(Q)\\). Preconditioning aims to transform \\(Q\\) into one with a smaller condition number by multiplying by some \\(P\\) (like using \\(P^{-1}Q\\)) \u2014 effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex \\(f\\), the Lipschitz constant of \\(\\nabla f\\) (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p> <p>Summary of spectral properties:</p> <ul> <li> <p>The spectral norm \\(|A|_2 = \\sigma_{\\max}(A)\\) quantifies the largest stretching. It determines stability and step sizes.</p> </li> <li> <p>The smallest singular value \\(\\sigma_{\\min}\\) (if \\(A\\) is tall full-rank) tells if \\(A\\) is invertible and how sensitive the inverse is. If \\(\\sigma_{\\min}\\) is tiny, small changes in output cause huge changes in solving \\(Ax=b\\).</p> </li> <li> <p>The condition number \\(\\kappa = \\sigma_{\\max}/\\sigma_{\\min}\\) is a figure of merit for algorithms: gradient descent iterations needed often scale with \\(\\kappa\\) (worse conditioning = slower). Regularization like adding \\(\\mu I\\) increases \\(\\sigma_{\\min}\\), thereby reducing \\(\\kappa\\) and accelerating convergence (at the expense of bias).</p> </li> <li> <p>Nuclear norm (sum of singular values) and spectral norm often appear in optimization as convex surrogates for rank and as constraints to limit the operator\u2019s impact.</p> </li> </ul> <p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization seeks to find points that minimize or maximize a real-valued function. To analyze and solve such problems, we rely on tools from multivariable calculus \u2014 gradients, Jacobians, Hessians, and Taylor expansions \u2014 which describe how a function changes locally.</p> <p>This chapter provides the differential calculus foundation essential for convex analysis and gradient-based learning algorithms.  It links geometric intuition and analytical tools that underlie optimization methods such as gradient descent, Newton\u2019s method, and backpropagation.</p>"},{"location":"convex/13_calculus/#31-gradients-and-directional-derivatives","title":"3.1 Gradients and Directional Derivatives","text":"<p>Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\). Differentiability at \\(x\\) means there exists a linear map (the gradient) such that</p> \\[ f(x+h)=f(x)+\\nabla f(x)^\\top h+o(\\|h\\|). \\] <p>Equivalently, for every direction \\(v\\in\\mathbb{R}^n\\), the directional derivative exists and matches the gradient pairing:</p> \\[ D_v f(x)=\\lim_{t \\rightarrow 0}\\frac{f(x+tv)-f(x)}{t}=\\nabla f(x)^\\top v. \\] <p>This shows that \\(\\nabla f(x)\\) is the unique vector giving the best linear approximation of \\(f\\) near \\(x\\). Among all unit directions \\(u\\),  is maximized when \\(u\\) aligns with \\(\\nabla f(x)\\) \u2014 the direction of steepest ascent. The opposite direction, \\(- \\nabla f(x)\\), gives the steepest descent.</p> <p>A level set of a differentiable function \\(f\\) is  </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#32-jacobians","title":"3.2  Jacobians","text":"<p>When dealing with optimization or learning, functions rarely map one number to another. They often map many inputs to many outputs \u2014 for example, a neural network layer, a physical model, or a vector-valued transformation. The Jacobian matrix captures how each output reacts to each input \u2014 the complete local sensitivity of a system.</p> <p>Derivative to Gradient:  For a scalar function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the derivative generalizes to the gradient:</p> \\[ \\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}(x) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(x) \\end{bmatrix}. \\] <ul> <li>Each component \\(\\frac{\\partial f}{\\partial x_i}\\) tells how \\(f\\) changes as we vary \\(x_i\\) alone.  </li> <li>Collectively, \\(\\nabla f(x)\\) forms the vector of steepest ascent, pointing toward the direction of maximal increase.  </li> <li>The magnitude \\(\\|\\nabla f(x)\\|\\) measures how sharply \\(f\\) rises.</li> </ul> <p>From Gradient to Jacobian \u2014 Many Inputs, Many Outputs: Now let \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a vector-valued function:</p> \\[ F(x) = \\begin{bmatrix} F_1(x) \\\\[4pt] F_2(x) \\\\[4pt] \\vdots \\\\[4pt] F_m(x) \\end{bmatrix}. \\] <p>Each \\(F_i(x)\\) is a scalar function with its own gradient \\(\\nabla F_i(x)^\\top\\). Stacking these row vectors gives the Jacobian matrix:  </p> <p>For \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian is the \\(m \\times n\\) matrix  </p> <p>It represents the best linear map approximating \\(F\\) near \\(x\\):  </p> <p>Just as a tangent line approximates a scalar curve, the Jacobian defines the tangent linear map that approximates \\(F\\) near \\(x\\):  </p> <ul> <li>The small displacement \\(h\\) in input space is transformed linearly into an output change \\(J_F(x)h\\).  </li> <li>Thus, \\(J_F(x)\\) acts like the microscopic blueprint of \\(F\\) around \\(x\\).  </li> <li>Locally, \\(F\\) behaves like a matrix transformation \u2014 stretching, rotating, or skewing space.</li> </ul> Part of \\(J_F(x)\\) Interpretation Row \\(i\\) Gradient of the \\(i\\)-th output \\(F_i(x)\\) \u2014 how that output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to input \\(x_j\\) \u2014 how changing \\(x_j\\) influences the entire output vector. Determinant (if \\(m=n\\)) Local volume scaling \u2014 how much \\(F\\) expands or compresses space near \\(x\\). Rank of \\(J_F(x)\\) Local dimensionality of the image of \\(F\\) \u2014 tells if directions are lost or preserved."},{"location":"convex/13_calculus/#33-the-hessian-and-curvature","title":"3.3 The Hessian and Curvature","text":"<p>If \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is twice differentiable, its Hessian is</p> \\[ \\nabla^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}. \\] <p>The Hessian encodes curvature information: - \\(\\nabla^2 f(x) \\succeq 0\\) (positive semidefinite) \u27f9 \\(f\\) is convex near \\(x\\). - \\(\\nabla^2 f(x) \\succ 0\\) \u27f9 \\(f\\) is strictly convex. - Negative eigenvalues \u27f9 directions of local decrease.</p> <p>Example \u2013 quadratic function: \\(f(x) = \\frac{1}{2}x^TQx - b^T x\\). Here \\(\\nabla f(x) = Qx - b\\) (linear), and \\(\\nabla^2 f(x) = Q\\). Solving \\(\\nabla f=0\\) yields \\(Qx=b\\), so if \\(Q \\succ 0\\) the unique minimizer is \\(x^* = Q^{-1}b\\). The Hessian being \\(Q \\succ 0\\) confirms convexity. If \\(Q\\) has large eigenvalues, gradient \\(Qx - b\\) changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of \\(Q\\) controls difficulty of minimizing \\(f\\).</p> <p>Eigenvalues of the Hessian describe curvature along principal directions. Large eigenvalues correspond to steep curvature; small ones correspond to flat regions. Understanding this curvature is essential in designing stable optimization algorithms.</p>"},{"location":"convex/13_calculus/#34-taylor-approximation","title":"3.4 Taylor approximation","text":"<p>For differentiable \\(f\\), we have the first-order Taylor expansion around \\(x\\):  </p> <p>The gradient gives the best local linear approximation, predicting how \\(f\\) changes for a small move \\(d\\). This is the foundation of first-order optimization methods such as gradient descent.</p> <p>If f is twice differentiable, we have the second-order expansion  </p> <p>If \\(\\nabla^2 f(x)\\) is positive semidefinite, the quadratic term is always \\(\\ge 0\\). Locally, \\(x\\) is in a \u201cbowl\u201d. If \\(\\nabla^2 f(x)\\) is indefinite, the landscape can curve up in some directions and down in others \u2014 typical of saddle points.</p> <p>This quadratic model is the foundation of Newton\u2019s method and trust-region algorithms:  the linear term drives direction; the quadratic term refines curvature.</p>"},{"location":"convex/13_calculus/#35-convexity-and-the-hessian","title":"3.5 Convexity and the Hessian","text":"<p>A twice-differentiable function \\(f\\) is convex on a convex set if and only if</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\forall x \\text{ in the domain.} \\] <p>The Hessian describes how the gradient changes.</p>"},{"location":"convex/13_calculus/#36-first-and-second-order-optimality-conditions","title":"3.6 First and Second-Order Optimality Conditions","text":"<p>Suppose we want to solve an unconstrained optimization problem:  </p> <p>A point \\(x^\\star\\) is called a critical point if  </p>"},{"location":"convex/13_calculus/#first-order-necessary-condition","title":"First-Order Necessary Condition","text":"<p>If \\(f\\) is differentiable and \\(x^\\star\\) is a local minimizer, then necessarily  </p> <p>Intuitively, this means the slope in every direction must vanish \u2014 there is no infinitesimal move that decreases \\(f\\) further.</p>"},{"location":"convex/13_calculus/#second-order-necessary-condition","title":"Second-Order Necessary Condition","text":"<p>If \\(f\\) is twice differentiable and \\(x^\\star\\) is a local minimizer, then  </p> <p>meaning the Hessian is positive semidefinite (PSD). The function curves upward (or flat) in all local directions.</p>"},{"location":"convex/13_calculus/#second-order-sufficient-condition","title":"Second-Order Sufficient Condition","text":"<p>If  i.e. the Hessian is positive definite (PD), then \\(x^\\star\\) is a strict local minimizer \u2014 the point lies at the bottom of a strictly convex bowl.</p> <p>The gradient gives the best local linear approximation, predicting how f changes for a small move d.  This is the foundation of first-order optimization methods such as gradient descent. If \\(\\nabla f(x^\\star)\\) has both positive and negative eigenvalues, \\(x^\\star\\) is a saddle point \u2014 neither a minimum nor a maximum.</p> <p>Convexity makes everything simpler.If \\(f\\) is convex, then any point \\(x^\\star\\) satisfying \\(\\nabla f(x^\\star) = 0\\)  is not only a local minimizer \u2014 it is a global minimizer.</p>"},{"location":"convex/13_calculus/#37-lipschitz-continuity-and-smoothness","title":"3.7 Lipschitz Continuity and Smoothness","text":"<p>A function \\(f\\) has a Lipschitz continuous gradient with constant \\(L &gt; 0\\) if  </p> <p>Such a function is called L-smooth. This condition bounds how quickly the gradient can change, ensuring the function is not excessively curved.</p> <p>This property implies the Descent Lemma:  </p> <p>Smoothness bounds how fast \\(f\\) can curve. In gradient descent, choosing a step size \\(\\eta \\le 1/L\\) guarantees convergence for convex functions.</p> <p>In ML training, \\(L\\) controls how \u201caggressive\u201d the learning rate can be \u2014 smoother losses allow larger steps.</p>"},{"location":"convex/13_calculus/#38-strong-convexity-functions-with-guaranteed-curvature","title":"3.8 Strong Convexity \u2014 Functions with Guaranteed Curvature","text":"<p>A differentiable function \\(f\\) is said to be \\(\\mu\\)-strongly convex if for some \\(\\mu &gt; 0\\),  </p> <p>This inequality means f always lies above its tangent plane by at least a quadratic term of curvature \u03bc. Strong convexity ensures a minimum curvature: f grows at least as fast as a parabola away from its minimizer.</p>"},{"location":"convex/13_calculus/#39-subgradients-and-nonsmooth-extensions","title":"3.9 Subgradients and Nonsmooth Extensions","text":"<p>Many useful convex functions are nonsmooth \u2014 e.g., hinge loss, \\(\\ell_1\\) norm, ReLU. They lack a gradient at certain points but admit a subgradient.</p> <p>A vector \\(g\\) is a subgradient of \\(f\\) at \\(x\\) if  </p> <p>The set of all such vectors is the subdifferential \\(\\partial f(x)\\).  </p> <p>Examples: - \\(f(x) = \\|x\\|_1\\) has    - For \\(f(x) = \\max_i x_i\\), any unit vector supported on the active index is a subgradient.</p> <p>Subgradients generalize the gradient concept, allowing optimization even when derivatives do not exist. They are the backbone of nonsmooth convex optimization and proximal methods.  In machine learning, they make it possible to minimize losses such as the hinge or absolute deviation, where gradients are undefined at corners.</p>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Optimisation problems almost always include constraints. The feasible region, the set of points allowed by those constraints, is often a convex set. This chapter builds geometric intuition for convex sets, affine sets, hyperplanes, polyhedra, and supporting hyperplanes.</p>"},{"location":"convex/14_convexsets/#41-convex-sets","title":"4.1 Convex sets","text":"<p>A set \\(C \\subseteq \\mathbb{R}^n\\) is convex if for any \\(x,y \\in C\\) and any \\(\\theta \\in [0,1]\\),  </p> <p>For any two feasible points, the entire line segment between them is also feasible. No \u201cindentations.\u201d</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>An affine subspace: \\(\\{ x : Ax = b \\}\\).</li> <li>A halfspace: \\(\\{ x : a^\\top x \\le b \\}\\).</li> <li>An \\(\\ell_2\\) ball: \\(\\{ x : \\|x\\|_2 \\le r \\}\\).</li> <li>An \\(\\ell_\\infty\\) ball: \\(\\{ x : \\|x\\|_\\infty \\le r \\}\\), which is a box.</li> <li>The probability simplex: \\(\\{ x \\in \\mathbb{R}^n : x \\ge 0, \\sum_i x_i = 1 \\}\\).</li> </ul> <p>Convexity means \u201cno caves or holes\u201d \u2014 you can mix any two feasible points and stay feasible.</p> <p>Geometrically:  </p> <ul> <li>Convex sets are closed under averaging.  </li> <li>Optimization over convex sets is stable: small perturbations to data do not create new local minima.  </li> <li>Convex sets preserve feasibility under stochastic mixing \u2014 a key property in probabilistic ML models.</li> </ul> <p>A set that is not convex: a crescent shape or annulus. The defining failure is: there exist \\(x,y\\) in the set such that some convex combination leaves the set.</p>"},{"location":"convex/14_convexsets/#42-affine-sets-hyperplanes-and-halfspaces","title":"4.2 Affine sets, hyperplanes, and halfspaces","text":"<p>An affine set is a translate of a subspace:  where \\(S\\) is a subspace. Affine sets are convex.</p> <p>A hyperplane in \\(\\mathbb{R}^n\\) is a set of the form  for some nonzero \\(a \\in \\mathbb{R}^n\\).</p> <p>A halfspace is  </p> <p>Every affine set is convex, and every hyperplane is both affine and convex.</p> <p>Halfspaces are convex; intersections of halfspaces are convex. Linear inequality constraints define intersections of halfspaces, and therefore give convex feasible regions.</p>"},{"location":"convex/14_convexsets/#43-convex-combinations-convex-hulls","title":"4.3 Convex combinations, convex hulls","text":"<p>A convex combination of points \\(x_1,\\dots,x_k\\) is  </p> <p>The convex hull of a set \\(S\\) is the set of all convex combinations of finitely many points of \\(S\\). It is the \u201csmallest\u201d convex set containing \\(S\\).</p> <p>Convex hulls matter because:</p> <ul> <li>Polytopes (bounded polyhedra) can be described as convex hulls of finitely many points (their vertices).</li> <li>Many relaxations in optimisation replace a complicated nonconvex feasible set by its convex hull.</li> </ul> <p>Convex hulls appear everywhere:</p> <ul> <li>In machine learning, convex combinations define mixtures (e.g., mixture of experts, convex combination of base classifiers).</li> <li>In optimization, many relaxations approximate hard discrete sets by their convex hull \u2014 making problems solvable with convex tools.</li> </ul> <p>Geometric intuition: the convex hull of points is like wrapping a rubber band around them.</p>"},{"location":"convex/14_convexsets/#44-polyhedra-and-polytopes","title":"4.4 Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are convex and cane be unbounded. If \\(P\\) is also bounded, it is called a polytope.</p> <p>In linear programming, we minimise a linear objective \\(c^\\top x\\) over a polyhedron. The optimal solution, if it exists, is always attained at an extreme point (vertex) of the feasible polyhedron.</p> <p>Linear programs (LPs) minimize \\(c^\\top x\\) over polyhedra. Since the objective is linear, the optimum lies at a vertex (extreme point) of the feasible region.</p> <p>In ML:</p> <ul> <li>LPs describe support vector machines (linear margin constraints).  </li> <li>Polyhedral feasible regions define sparsity-inducing \\(\\ell_1\\) regularizers (via linear inequalities).</li> </ul>"},{"location":"convex/14_convexsets/#45-extreme-points","title":"4.5 Extreme points","text":"<p>Let \\(C\\) be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be expressed as a strict convex combination of two other distinct points in \\(C\\). Formally, \\(x\\) is extreme in \\(C\\) if whenever  then \\(y = z = x\\).</p> <p>Geometric meaning:</p> <ul> <li>Extreme points are the \u201ccorners.\u201d</li> <li>In a polytope, extreme points are precisely the vertices.</li> </ul> <p>This is why linear programming solutions are found at vertices.</p>"},{"location":"convex/14_convexsets/#46-cones","title":"4.6 Cones","text":"<p>A set \\(K\\) is a cone if for any \\(x \\in K\\) and \\(\\alpha \\ge 0\\), \\(\\alpha x \\in K\\). A cone is convex if additionally \\(x,y\\in K\\) implies \\(x+y \\in K\\). Convex cones are important (e.g. nonnegative orthant, PSD matrices cone) because many optimization problems can be cast as cone programs. Cones have extreme rays instead of points (directions that generate edges of the cone). For instance, the extreme rays of the positive orthant in \\(\\mathbb{R}^n\\) are the coordinate axes (each axis direction can\u2019t be formed by positive combos of others).</p> <ul> <li>Cones are closed under nonnegative scaling, but not necessarily addition.  </li> <li>Conic hull (convex cone): Collection of all conic combinations of points in \\(S\\).</li> <li>A cone is not necessarily a subspace (negative multiples may not be included).  </li> <li>A convex cone is closed under addition and nonnegative scaling.  </li> <li> <p>Polar Cones: Given a cone \\(K \\subseteq \\mathbb{R}^n\\), the polar cone is:</p> \\[ K^\\circ = \\{ y \\in \\mathbb{R}^n \\mid \\langle y, x \\rangle \\le 0, \\; \\forall x \\in K \\}. \\] <ul> <li>Intuition: polar cone vectors form non-acute angles with every vector in \\(K\\).  </li> <li>Properties:  <ul> <li>Always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, \\(K^\\circ\\) is the orthogonal complement.  </li> <li>Duality: \\((K^\\circ)^\\circ = K\\) for closed convex cones.  </li> </ul> </li> </ul> </li> <li> <p>Tangent Cone: For a set \\(C\\) and point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) contains all directions in which one can \u201cmove infinitesimally\u201d while remaining in \\(C\\):</p> \\[ T_C(x) = \\Big\\{ d \\in \\mathbb{R}^n \\;\\Big|\\; \\exists t_k \\downarrow 0, \\; x_k \\in C, \\; x_k \\to x, \\; \\frac{x_k - x}{t_k} \\to d \\Big\\}. \\] <ul> <li>Interior point: \\(T_C(x) = \\mathbb{R}^n\\).  </li> <li>Boundary point: \\(T_C(x)\\) restricts movement to directions staying inside \\(C\\). </li> <li>Tangent cones define feasible directions for projected gradient steps or constrained optimization.</li> </ul> </li> <li> <p>Normal Cone: For a convex set \\(C\\) at point \\(x \\in C\\):      </p> <ul> <li>Each \\(v \\in N_C(x)\\) defines a supporting hyperplane at \\(x\\).  </li> <li>Relation: \\(N_C(x) = \\big(T_C(x)\\big)^\\circ\\) \u2014 polar of tangent cone.  </li> <li>Interior point: \\(N_C(x) = \\{0\\}\\).  </li> <li>Boundary/corner: \\(N_C(x)\\) is a cone of outward normals.- Appears in first-order optimality conditions:  where the subgradient of \\(f\\) is balanced by the \u201cpush-back\u201d of constraints.</li> </ul> </li> </ul>"},{"location":"convex/14_convexsets/#47-supporting-hyperplanes-and-separation","title":"4.7 Supporting hyperplanes and separation","text":"<p>Convex sets can be \u201ctouched\u201d or \u201cseparated\u201d by hyperplanes.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting hyperplane theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty closed convex set, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero \\(a\\) such that  In words, there is a hyperplane \\(a^\\top x = a^\\top x_0\\) that \u201csupports\u201d \\(C\\) at \\(x_0\\): it touches \\(C\\) but does not cut through it.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating hyperplane theorem","text":"<p>If \\(C\\) and \\(D\\) are two disjoint nonempty convex sets, then there exists a hyperplane that separates them: some nonzero \\(a\\) and scalar \\(b\\) such that  </p> <p>Why do we care?</p> <ul> <li>These theorems are the geometric heart of duality.</li> <li>KKT conditions can be interpreted as existence of a supporting hyperplane that is simultaneously aligned with objective and constraints.</li> <li>Subgradients of convex functions correspond to supporting hyperplanes of epigraphs.</li> </ul>"},{"location":"convex/14_convexsets/#47-feasible-regions-in-convex-optimisation","title":"4.7 Feasible regions in convex optimisation","text":"<p>In convex optimisation, the feasible set is typically something like  </p> <ul> <li>If each \\(g_i\\) is convex and each \\(h_j\\) is affine, then the feasible set is convex.</li> <li>If \\(f\\) is also convex, then the entire problem is a convex optimisation problem.</li> </ul>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>Convex functions are the objectives we minimise. Understanding them is essential, because convexity of the objective is what turns an optimisation problem from \"possibly impossible\" to \"provably solvable\".</p>"},{"location":"convex/15_convexfunctions/#51-definitions-of-convexity","title":"5.1 Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),</p> \\[ f(\\theta x + (1-\\theta) y) \\le \\theta f(x) + (1-\\theta) f(y)~. \\] <p>If the inequality is strict whenever \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\), then \\(f\\) is strictly convex.</p> <p>Geometrically, the chord between two points on the graph of f lies above the graph itself. This means that f never \u201cbends downward\u201d \u2014 it has a single valley rather than multiple dips. Equivalently, the epigraph of f,</p> \\[ \\mathrm{epi}(f) = \\{ (x, t) \\in \\mathbb{R}^n \\times \\mathbb{R} : f(x) \\le t \\}. \\] <p>is a convex set.</p>"},{"location":"convex/15_convexfunctions/#52-first-order-characterisation","title":"5.2 First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y - x) \\quad \\text{for all } x,y. \\] <p>Interpretation:</p> <ul> <li>The first-order Taylor approximation is always a global underestimator.</li> <li>The gradient at \\(x\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x, f(x))\\).</li> </ul> <p>This inequality is sometimes called the first-order condition for convexity.</p> <p>This is a powerful characterization: it says the tangent hyperplane at any point \\(x\\) lies below the graph of \\(f\\) everywhere. In other words, the gradient at \\(x\\) provides a global underestimator of \\(f\\) (supporting hyperplane to epigraph). Geometrically, this means no tangent line ever goes above the function. </p> <p>For a convex differentiable \\(f\\), we have \\(f(y) - f(x) \\ge \\nabla f(x)^T (y-x)\\), so moving from \\(x\\) in any direction, the actual increase in \\(f\\) is at least as large as the linear prediction by \\(\\nabla f(x)\\) (since the function bends upward or straight). At optimum \\(\\hat{x}\\), a necessary and sufficient condition (for convex differentiable \\(f\\)) is \\(\\nabla f(\\hat{x}) = 0\\). This ties to optimality: \\(\\nabla f(\\hat{x})=0\\) means \\(f(y)\\ge f(\\hat{x}) + \\nabla f(\\hat{x})^T (y-\\hat{x}) = f(\\hat{x})\\) for all \\(y\\), so \\(\\hat{x}\\) is global minimizer.</p> <p>If \\(f\\) is not differentiable, a similar condition holds with subgradients (see next chapter): \\(f\\) is convex iff for all \\(x,y\\) there exists a (sub)gradient \\(g \\in \\partial f(x)\\) such that \\(f(y) \\ge f(x) + g^T(y-x)\\). The set of all subgradients \\(\\partial f(x)\\) is a convex set (the subdifferential). At optimum, \\(0 \\in \\partial f(\\hat{x})\\) is the condition. </p>"},{"location":"convex/15_convexfunctions/#53-second-order-characterisation","title":"5.3 Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then \\(f\\) is convex if and only if its Hessian is positive semidefinite everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{for all } x~. \\] <p>If \\(\\nabla^2 f(x) \\succ 0\\) for all \\(x\\), then \\(f\\) is strictly convex.</p>"},{"location":"convex/15_convexfunctions/#54-examples-of-convex-functions","title":"5.4 Examples of convex functions","text":"<ol> <li> <p>Affine functions: \\(f(x) = a^\\top x + b\\).    Always convex (and concave).</p> </li> <li> <p>Quadratic functions with PSD Hessian: \\(f(x) = \\tfrac12 x^\\top Q x + c^\\top x + d\\),    where \\(Q \\succeq 0\\) (symmetric positive semidefinite).    Convex because \\(\\nabla^2 f(x) = Q \\succeq 0\\).</p> </li> <li> <p>Norms: \\(f(x) = \\|x\\|\\) for any norm.    All norms are convex.</p> </li> <li> <p>Maximum of affine functions: \\(f(x) = \\max_i (a_i^\\top x + b_i)\\).    Convex because it is the pointwise maximum of convex functions.</p> </li> <li> <p>Log-sum-exp function: \\(f(x) = \\log \\left( \\sum_{i=1}^k \\exp(a_i^\\top x + b_i) \\right)\\).    This function is convex and is ubiquitous in statistics and machine learning (softmax, logistic regression). The convexity follows from Jensen\u2019s inequality and properties of the exponential (Boyd and Vandenberghe, 2004).</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#55-jensens-inequality","title":"5.5 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable taking values in the domain of \\(f\\). Then  </p> <p>the function value at the mean is always less than or equal to the mean of function values.</p> <p>This is Jensen\u2019s inequality. It generalises the definition of convexity from two-point averages to arbitrary expectations. As a special case, for scalars \\(x_1,\\dots,x_n\\) and weights \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality has many uses: in machine learning, it justifies algorithms like EM (which use the inequality to create surrogate objectives), and it provides bounds like \\(\\log(\\mathbb{E}[e^X]) \\ge \\mathbb{E}[X]\\) (by convexity of \\(\\log\\) or \\(e^x\\)). As a simple example, taking \\(f(x)=x^2\\) and \\(X\\) uniform in \\({-1,1}\\), Jensen says \\((\\mathbb{E}[X])^2 = 0^2 \\le \\mathbb{E}[X^2] = 1\\), which is true. Or \\(f(x)=\\frac{1}{x}\\) convex on \\((0,\\infty)\\) implies \\(\\frac{1}{\\mathbb{E}[X]} \\le \\mathbb{E}[\\frac{1}{X}]\\) for positive \\(X\\). In optimization, Jensen\u2019s inequality often helps in proving convexity of expectations: if you mix some distributions or uncertain inputs, the expected loss is convex if the loss function is convex.</p>"},{"location":"convex/15_convexfunctions/#56-operations-that-preserve-convexity","title":"5.6 Operations that preserve convexity","text":"<p>If \\(f\\) and \\(g\\) are convex, then:</p> <ul> <li>\\(f + g\\) is convex.</li> <li>\\(\\alpha f\\) is convex for any \\(\\alpha \\ge 0\\).</li> <li>\\(\\max\\{f,g\\}\\) is convex.</li> <li>Composition with an affine map preserves convexity:   If \\(A\\) is a matrix and \\(b\\) a vector, then \\(x \\mapsto f(Ax + b)\\) is convex.</li> </ul> <p>If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex, then the composition \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex. This helps you build new convex functions from known ones.</p>"},{"location":"convex/15_convexfunctions/#57-level-sets-of-convex-functions","title":"5.7 Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), define the sublevel set  </p> <p>Geometrically, these are the \u201ccontour slices\u201d of a convex bowl \u2014 cross-sections below certain heights.  </p> <p>If \\(f\\) is convex, then every sublevel set is convex. This is crucial: constraints of the form \\(f(x) \\le \\alpha\\) are convex constraints.</p> <p>For example, the set  is convex because \\(x \\mapsto \\|Ax-b\\|_2\\) is convex.</p>"},{"location":"convex/15_convexfunctions/#58-strict-and-strong-convexity","title":"5.8 Strict and strong convexity","text":"<ul> <li>\\(f\\) is strictly convex if  for all \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\).</li> </ul> <p>Strict convexity ensures uniqueness of the minimizer: there can be only one bottom to the bowl.</p> <ul> <li>\\(f\\) is strongly convex with parameter \\(m&gt;0\\) if  </li> </ul> <p>Strong convexity implies a unique minimiser and gives quantitative convergence rates for gradient methods.</p>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many of the most important convex functions are not differentiable everywhere:</p> <ul> <li>\\(\\|x\\|_1 = \\sum_i |x_i|\\) has corners at \\(x_i = 0\\),</li> <li>\\(f(x) = \\max\\{a_1^\\top x + b_1, \\dots, a_k^\\top x + b_k\\}\\) is piecewise affine,</li> <li>the hinge loss \\(\\max\\{0, 1 - y w^\\top x\\}\\) (used in SVMs) is not smooth at the kink.</li> </ul> <p>For a convex but nonsmooth \\(f\\), the usual condition \u201c\\(\\nabla f(x^*) = 0\\)\u201d may not make sense, because \\(\\nabla f(x^*)\\) may not exist. But geometrically, convex functions still have supporting hyperplanes at every point. That is the key.</p>"},{"location":"convex/16_subgradients/#61-subgradients-and-the-subdifferential","title":"6.1 Subgradients and the subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex. A vector \\(g \\in \\mathbb{R}^n\\) is called a subgradient of \\(f\\) at \\(x\\) if, for all \\(y\\),  </p> <p>Interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top (y-x)\\) is a global underestimator of \\(f\\).</li> <li>\\(g\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x,f(x))\\).</li> </ul> <p>The set of all subgradients of \\(f\\) at \\(x\\) is called the subdifferential of \\(f\\) at \\(x\\):  </p> <p>If \\(f\\) is differentiable at \\(x\\), then  </p> <p>If \\(f\\) is not differentiable at \\(x\\), \\(\\partial f(x)\\) is typically a nonempty convex set.</p> <p>Geometrically, each \\(g \\in \\partial f(x)\\) defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).  At smooth points, there is a single supporting hyperplane (the tangent); at corners, there are many possible supporting hyperplanes, forming a convex set of slopes.</p> <p>Intuitively:</p> <ul> <li>Gradients describe \u201ctilt\u201d at smooth points.</li> <li>Subgradients describe all possible valid tilts at kinks.</li> </ul>"},{"location":"convex/16_subgradients/#62-examples","title":"6.2 Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\).</p> <ul> <li>For \\(t&gt;0\\), \\(\\partial f(t) = \\{1\\}\\).</li> <li>For \\(t&lt;0\\), \\(\\partial f(t) = \\{-1\\}\\).</li> <li>For \\(t=0\\),     </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) is a valid supporting line from below.</p>"},{"location":"convex/16_subgradients/#ell_1-norm","title":"\\(\\ell_1\\) norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\), we have  So</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This is exactly what shows up in LASSO optimality conditions in statistics.</p>"},{"location":"convex/16_subgradients/#pointwise-max-of-affine-functions","title":"Pointwise max of affine functions","text":"<p>Let  </p> <p>If a single index \\(i^*\\) achieves the max at \\(x\\), then  </p> <p>If multiple \\(i\\) are tied at the max, then  the convex hull of all active slopes.</p>"},{"location":"convex/16_subgradients/#63-subgradient-optimality-condition","title":"6.3 Subgradient optimality condition","text":"<p>Suppose we want to solve the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>Then a point \\(x^*\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^*). \\] <p>The condition \\(0\\in\\partial f(x^*)\\) means no subgradient points in a direction that can reduce \\(f\\).  In optimization geometry, this corresponds to the supporting hyperplane being horizontal at \\(x^*\\) \u2014 the flat bottom of the convex bowl. In constrained problems, this generalizes to \\(0\\in\\partial f(x^)+A^\\top\\lambda^\\), the KKT stationarity condition (see Chapter 8).</p>"},{"location":"convex/16_subgradients/#64-subgradient-calculus-useful-rules","title":"6.4 Subgradient calculus (useful rules)","text":"<p>If \\(f\\) and \\(g\\) are convex:</p> <ul> <li> <p>\\(\\partial (f+g)(x) \\subseteq \\partial f(x) + \\partial g(x)\\), i.e.    </p> </li> <li> <p>If \\(A\\) is a matrix and \\(h(x) = f(Ax)\\), then    </p> </li> <li> <p>If \\(f(x) = \\max_i f_i(x)\\) and each \\(f_i\\) is convex, then    </p> </li> </ul> <p>These rules make it possible to compute subgradients of complicated nonsmooth objectives.</p>"},{"location":"convex/16_subgradients/#65-subgradient-methods","title":"6.5 Subgradient Methods","text":"<p>Even though nonsmooth functions lack gradients, we can still minimize them using subgradient descent. Given a subgradient \\(g_k \\in \\partial f(x_k)\\), the iteration</p> \\[ x_{k+1} = x_k - \\alpha_k g_k \\] <p>moves in the direction of the negative subgradient. Unlike in smooth optimization, the step sizes \\(\\alpha_k\\) typically decrease with \\(k\\) (for example, \\(\\alpha_k = c / \\sqrt{k}\\)) to guarantee convergence. Subgradient descent converges to the global minimum for convex \\(f\\), though at a slower rate than smooth gradient descent. While smooth convex functions enjoy \\(\\mathcal{O}(1/k^2)\\) or linear convergence under strong convexity, nonsmooth convex functions converge at rate \\(\\mathcal{O}(1/\\sqrt{k})\\). In practice, many machine learning algorithms\u2014such as SVM training with hinge loss, \\(\\ell_1\\)-regularized models, and even certain deep learning optimizers\u2014operate as subgradient methods in disguise. Their stability and robustness stem from convexity rather than smoothness.</p>"},{"location":"convex/16_subgradients/#66-proximal-and-smoothed-alternatives","title":"6.6  Proximal and Smoothed Alternatives","text":"<p>When the subgradient method converges too slowly, we often smooth or separate the nonsmooth term.</p> <ul> <li> <p>Proximal methods (Chapter 12) compute \\(\\mathrm{prox}_{\\alpha f}(y)=\\arg\\min_x f(x)+\\tfrac1{2\\alpha}\\|x-y\\|^2\\) \u2014 this yields faster and more stable updates.</p> </li> <li> <p>Smoothed approximations replace \\(\\max\\) or \\(|t|\\) by smooth surrogates (e.g. Huber loss, softplus) retaining convexity but enabling gradient-based solvers.</p> </li> </ul> <p>These bridges connect subgradient theory with modern first-order algorithms.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions provide precise geometric and analytic criteria for such points to be optimal. They generalize the idea of \u201czero gradient\u201d to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) theory developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#71-understanding-nth-order-optimality-conditions","title":"7.1  Understanding \u201cnth-Order\u201d Optimality Conditions","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we inspect around a candidate minimizer \\(x^*\\):</p> Order Uses Meaning First-order \\(\\nabla f(x^*)\\) (or subgradients) Checks whether any local descent direction exists Second-order Hessian \\(\\nabla^2 f(x^*)\\) Examines curvature to ensure the point is bowl-shaped (no local maxima or saddle) Third and higher Higher derivatives Rarely used; detect flat or degenerate cases when curvature vanishes <p>In general optimization, these successive tests ensure a point is truly a local minimizer. However, in convex optimization, this hierarchy collapses beautifully:</p>"},{"location":"convex/16a_optimality_conditions/#why-only-first-order-conditions-matter-for-convex-functions","title":"Why Only First-Order Conditions Matter for Convex Functions","text":"<p>A convex function already has non-negative curvature everywhere \u2014 its Hessian is automatically positive semidefinite wherever it exists:</p> \\[ \\nabla^2 f(x) \\succeq 0, \\quad \\forall x. \\] <p>Therefore, once the first-order condition is satisfied, no direction can decrease \\(f\\) \u2014 not locally, but globally.  Convexity guarantees that the landscape is bowl-shaped everywhere, so a stationary point is the unique global minimum. In contrast, nonconvex functions can have zero gradient points that are maxima or saddles; second- or higher-order checks are needed to tell them apart. Hence, convex optimization requires only the first-order condition \u2014 it captures both necessity and sufficiency for global optimality. This remarkable simplification is one of the reasons convex analysis is so powerful and elegant.</p>"},{"location":"convex/16a_optimality_conditions/#72-motivation","title":"7.2 Motivation","text":"<p>Consider minimizing a convex function \\(f\\) over a convex set \\(\\mathcal{X}\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>Even for simple convex objectives, we need a way to check when a point \\(\\hat{x}\\) is optimal. In unconstrained problems, this means no direction can reduce \\(f\\). With constraints, we only consider feasible directions \u2014 those that stay inside \\(\\mathcal{X}\\).</p> <p>In both cases, optimality can be understood as an equilibrium condition: the gradient (or subgradient) of \\(f\\) is balanced by the \u201cforces\u2019\u2019 from the constraints. These equilibrium conditions are the first-order optimality conditions.</p> <p>In machine learning, this reasoning appears everywhere \u2014 verifying when a trained model reaches stationarity (zero gradient) or when a sparsity constraint (like \\(\\ell_1\\) regularization) is active.</p>"},{"location":"convex/16a_optimality_conditions/#73-unconstrained-convex-problems","title":"7.3 Unconstrained Convex Problems","text":"<p>For the unconstrained problem</p> \\[ \\min_x f(x), \\] <p>the first-order optimality condition is simple:</p> <p>If \\(f\\) is differentiable, \\(\\hat{x}\\) is optimal if and only if</p> \\[ \\nabla f(\\hat{x}) = 0. \\] <p>If \\(f\\) is convex but possibly nonsmooth, the gradient is replaced by the subdifferential:</p> \\[ 0 \\in \\partial f(\\hat{x}). \\] <p>Intuitively, this means that the origin lies inside the set of all subgradients at \\(\\hat{x}\\).  Geometrically, every supporting hyperplane to the graph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\) has zero slope \u2014 the function cannot be decreased by moving in any direction.</p> <p>For smooth functions, \\(\\nabla f(\\hat{x}) = 0\\) means that the tangent plane is horizontal. For nonsmooth functions, \\(0 \\in \\partial f(\\hat{x})\\) means there exists at least one horizontal supporting plane among all possible tangents.</p>"},{"location":"convex/16a_optimality_conditions/#74-constrained-convex-problems","title":"7.4 Constrained Convex Problems","text":"<p>Now consider minimizing \\(f(x)\\) over a closed convex set \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>If \\(\\hat{x} \\in \\operatorname{int}(\\mathcal{X})\\) (the interior), the situation is identical to the unconstrained case: no boundary prevents motion, so</p> \\[ 0 \\in \\partial f(\\hat{x}). \\] <p>However, if \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\), we must restrict movement to feasible directions \u2014 those that stay within the set. At such points, not all directions are allowed, and the gradient (or subgradient) may point outward from the feasible region.</p>"},{"location":"convex/16a_optimality_conditions/#tangent-and-normal-cones","title":"Tangent and Normal Cones","text":"<p>At a point \\(x \\in \\mathcal{X}\\), define the tangent cone \\(T_x(\\mathcal{X})\\) as the set of feasible directions:</p> \\[ T_x(\\mathcal{X}) = \\{\\, d : \\exists\\, t_k \\downarrow 0,\\; x + t_k d \\in \\mathcal{X} \\,\\}. \\] <p>It captures all directions in which one can move infinitesimally without leaving \\(\\mathcal{X}\\).  </p> <p>The normal cone is its polar:</p> \\[ N_x(\\mathcal{X}) = \\{\\, v : v^\\top d \\le 0,\\; \\forall d \\in T_x(\\mathcal{X}) \\,\\}. \\] <p>The normal cone consists of vectors pointing outward from \\(\\mathcal{X}\\) \u2014 the directions orthogonal (or opposing) to every feasible direction.</p> <p>Geometrically, at an optimal boundary point, the gradient of \\(f\\) must lie inside the normal cone: if you try to move within \\(\\mathcal{X}\\) (inside the tangent cone), \\(f\\) cannot decrease further.</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-Order Condition with Constraints","text":"<p>For constrained convex optimization, the unified first-order condition is</p> \\[ 0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x}). \\] <p>This means that there exists a subgradient \\(g \\in \\partial f(\\hat{x})\\) and a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\) such that \\(g + v = 0\\). Equivalently, the objective\u2019s slope is exactly counterbalanced by the constraint\u2019s normal pressure.</p> <p>If \\(\\hat{x}\\) lies in the interior of \\(\\mathcal{X}\\), \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so the condition reduces to the unconstrained one (\\(0 \\in \\partial f(\\hat{x})\\)). If \\(\\hat{x}\\) is on the boundary, the constraint pushes back against the descent direction.</p>"},{"location":"convex/16a_optimality_conditions/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>At optimality:</p> <ul> <li>The gradient (or a subgradient) points into the normal cone of the feasible set.  </li> <li>The tangent cone defines all directions along which the function cannot decrease.  </li> <li>The inclusion \\(0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x})\\) encodes equilibrium between descent forces and boundary constraints.</li> </ul> <p>This picture generalizes the intuitive idea from single-variable calculus: the derivative changes sign at a minimum, while with constraints, the derivative at the boundary is balanced by the constraint\u2019s barrier.</p>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-kkt-framework","title":"Chapter 8: Lagrange Multipliers and KKT Framework","text":"<p>At this point we understand:</p> <ul> <li>how to recognise convex functions,</li> <li>how to talk about feasible sets,</li> <li>how to describe optimality with gradients or subgradients.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained optimization and the gradient descent principle, then extend to equality and inequality constraints \u2014 culminating in the Karush\u2013Kuhn\u2013Tucker (KKT) conditions, the cornerstone of constrained convex optimization.</p> <p>In constrained convex optimization, the gradient cannot vanish freely\u2014it must be counteracted by constraint forces. Lagrange multipliers quantify these forces. The Karush\u2013Kuhn\u2013Tucker (KKT) conditions express this balance algebraically.</p>"},{"location":"convex/17_kkt/#81-unconstrained-convex-minimisation","title":"8.1 Unconstrained convex minimisation","text":"<p>Consider  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent is the iterative method:  for some step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Move opposite the gradient to reduce \\(f\\).</li> <li>Under suitable conditions on step size (e.g. Lipschitz gradient), this converges to a global minimiser if \\(f\\) is convex.</li> </ul> <p>If \\(f\\) is strongly convex, we get uniqueness of the minimiser and faster convergence.</p> <p>In machine learning, this is the foundation of training via backpropagation \u2014 each step reduces the loss by following the negative gradient of the cost function with respect to model parameters.</p>"},{"location":"convex/17_kkt/#82-equality-constrained-optimisation-and-lagrange-multipliers","title":"8.2 Equality-constrained optimisation and Lagrange multipliers","text":"<p>Now suppose we add equality constraints:</p> <p>  where \\(f\\) and each \\(h_j\\) are differentiable.</p> <p>We define the Lagrangian  where \\(\\lambda_j\\) are the Lagrange multipliers.</p> <p>A necessary condition for \\(x^*\\) to be optimal (under suitable regularity assumptions) is:</p> <ol> <li>Stationarity:     </li> <li>Primal feasibility:    Primal feasibility simply means that the point \\(x^*\\) satisfies all the original constraints of the optimization problem.</li> </ol> <p> </p> <p>Geometrically, stationarity says: At an equality-constrained optimum, the gradient of \\(f\\) is orthogonal to the feasible set \u2014 it points in a direction that cannot reduce \\(f\\) without violating a constraint.</p>"},{"location":"convex/17_kkt/#83-inequality-constraints-and-kkt","title":"8.3 Inequality constraints and KKT","text":"<p>Now consider the general convex problem:  </p> <p>We form the Lagrangian  with dual varibales \\(\\lambda \\in \\mathbb{R}^p\\) (unrestricted) and \\(\\mu \\in \\mathbb{R}^m\\) with \\(\\mu_i \\ge 0\\).</p> <p>The Karush\u2013Kuhn\u2013Tucker (KKT) conditions consist of:</p> <ol> <li> <p>Primal feasibility:     </p> </li> <li> <p>Dual feasibility:     </p> </li> </ol> <p>Dual feasibility says the \u201cpenalty coefficients\u201d for inequality constraints can only push you inward (not reward you for violating constraints).</p> <ol> <li> <p>Stationarity:</p> <p>\\(\\nabla f(x^*)    + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*)   + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*)   = 0\\)</p> </li> <li> <p>Complementary slackness:</p> </li> </ol> <p> </p> <p>Complementary slackness means:</p> <ul> <li>If a constraint \\(g_i(x) \\le 0\\) is strictly inactive at \\(x^*\\) (i.e. \\(g_i(x^*) &lt; 0\\)), then \\(\\mu_i^* = 0\\).</li> <li>If \\(\\mu_i^* &gt; 0\\), then the constraint is tight: \\(g_i(x^*) = 0\\).</li> </ul> <p>This matches geometric intuition: only active constraints can \u201cpush back\u201d on the optimiser.</p>"},{"location":"convex/17_kkt/#84-slaters-condition-ensuring-strong-duality","title":"8.4 Slater\u2019s Condition \u2013 Ensuring Strong Duality","text":"<p>For the KKT conditions to not only hold but also guarantee optimality and zero duality gap, the problem must satisfy a regularity condition known as Slater\u2019s condition.</p>"},{"location":"convex/17_kkt/#definition","title":"Definition","text":"<p>For the convex problem above, if all \\(f\\) and \\(g_i\\) are convex and all \\(h_j\\) are affine, then Slater\u2019s condition holds if there exists at least one strictly feasible point:</p> \\[ \\exists\\, x^{\\text{slater}} \\text{ such that }  h_j(x^{\\text{slater}}) = 0, \\ \\forall j,  \\quad \\text{and} \\quad  g_i(x^{\\text{slater}}) &lt; 0, \\ \\forall i. \\] <p>This means there is some point that satisfies the equalities exactly and the inequalities strictly \u2014 i.e., a point inside the feasible region, not merely on its boundary.</p>"},{"location":"convex/17_kkt/#dual-problems-and-the-duality-gap","title":"Dual Problems and the Duality Gap","text":"<p>Every constrained problem (the primal) has a dual:  where \\(g(\\lambda,\\mu)\\) is the dual function obtained from the Lagrangian.</p> <p>By weak duality, for all feasible \\(x,(\\lambda,\\mu)\\),  The difference  measures how far apart the primal and dual optima are.</p> <ul> <li>If \\(p^*&gt;d^*\\), the gap is positive (weak duality only).  </li> <li>If \\(p^*=d^*\\), we have strong duality \u2014 the primal and dual attain the same optimum.</li> </ul>"},{"location":"convex/17_kkt/#what-slaters-condition-guarantees","title":"What Slater\u2019s Condition Guarantees","text":"<p>For convex problems, Slater\u2019s condition ensures: 1. Strong duality: \\(p^*=d^*\\) (duality gap = 0). 2. Dual attainment: finite \\((\\lambda^*,\\mu^*)\\) exist. 3. KKT conditions are necessary and sufficient for optimality.</p> <p>Intuitively, Slater\u2019s condition says the feasible region has \u201cbreathing room\u2019\u2019; it\u2019s not pinched to the boundary. Then the dual hyperplanes can exactly touch the primal surface \u2014 they kiss at the optimum, eliminating the gap.</p>"},{"location":"convex/17_kkt/#examples","title":"Examples","text":"<p>(a) Condition Holds \u2192 Zero Gap  Strictly feasible point \\(x=2\\) satisfies \\(g(x)=-1&lt;0\\). Hence \\(p^*=d^*=1\\) \u2014 no duality gap.</p> <p>(b) Condition Fails \u2192 Positive Gap  Feasible set \\(\\{0\\}\\) has no interior; no strictly feasible point. Dual cannot attain equality: \\(p^*&gt;d^*\\).</p>"},{"location":"convex/17_kkt/#85-geometric-and-physical-interpretation","title":"8.5 Geometric and Physical Interpretation","text":"<ul> <li>The gradient of the objective is balanced by the weighted gradients of the active constraints.  </li> <li>Each multiplier \\(\\mu_i\\) or \\(\\lambda_j\\) acts like a tension or shadow price that enforces feasibility.</li> <li>The KKT system generalizes the condition \\(\\nabla f(x^*) = 0\\):</li> <li>In the unconstrained case, there are no forces \u2014 pure gradient equilibrium.</li> <li>With constraints, these forces push the solution back into the feasible region.</li> </ul> <p>Physically, you can imagine optimization as minimizing potential energy subject to rigid walls (constraints). At equilibrium, the total force \u2014 gradient of \\(f\\) plus constraint reactions \u2014 equals zero. Convexity ensures the landscape is bowl-shaped. Slater\u2019s condition ensures the bowl has interior volume so that primal and dual solutions coincide. Together they make the KKT framework both elegant and powerful \u2014 the foundation upon which Lagrange duality theory is built.</p>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the most beautiful and useful ideas in convex optimisation. Every constrained optimisation problem (the primal) has an associated dual problem. The dual problem:</p> <ul> <li>provides a lower bound on the optimal primal value,</li> <li>often has structure that is easier to analyse,</li> <li>gives certificates of optimality,</li> <li>interprets multipliers as \u201cprices\u201d of constraints.</li> </ul> <p>In convex optimisation, under mild assumptions, the primal and dual optimal values are equal.</p>"},{"location":"convex/18_duality/#91-the-primal-problem","title":"9.1 The primal problem","text":"<p>We consider the general problem:  </p> <p>Assume \\(f\\) and the \\(g_i\\) are convex, and \\(h_j\\) are affine. This is a convex optimisation problem.</p> <p>We call \\(f^\\star\\) the optimal value:  </p> <p>Here, infimum means the smallest value \\(f(x)\\) can approach \u2014 even if it is not exactly attained.</p>"},{"location":"convex/18_duality/#92-why-duality","title":"9.2 Why Duality?","text":"<p>Before any equations, let\u2019s understand the high-level idea.</p> <p>A constrained optimization problem can be viewed as a trade-off:</p> <p>Minimize \\(f(x)\\) while paying penalties for violating constraints.</p> <p>If we allow violations but penalize them proportionally, we get a relaxed problem. How high should these penalties be? That\u2019s what the dual variables \\(\\mu_i, \\lambda_j\\) represent.</p> <ul> <li>\\(\\mu_i\\) (for inequalities): how costly it is to violate constraint \\(g_i(x) \\le 0\\).  </li> <li>\\(\\lambda_j\\) (for equalities): how much the objective changes when equality constraint \\(h_j(x)=0\\) is relaxed.</li> </ul> <p>Thus, duality converts constraints into prices or forces that shape the optimization landscape.</p>"},{"location":"convex/18_duality/#92-the-lagrangian","title":"9.2 The Lagrangian","text":"<p>The Lagrangian function incorporates both the objective and constraints:  with dual variables \\(\\mu \\in \\mathbb{R}^m\\), \\(\\lambda \\in \\mathbb{R}^p\\).</p> <ul> <li>\\(\\mu_i \\ge 0\\) are multipliers for inequality constraints,</li> <li>\\(\\lambda_j\\) are free (can be any sign) for equality constraints.</li> </ul> <p>If \\(\\mu_i &gt; 0\\), violating the \\(i\\)th constraint is penalized heavily; if \\(\\mu_i = 0\\), that constraint is inactive.</p>"},{"location":"convex/18_duality/#94-the-dual-function-lower-bounds-from-penalties","title":"9.4 The Dual Function \u2013 Lower Bounds from Penalties","text":"<p>For fixed \\((\\lambda, \\mu)\\), we define the dual function:  </p> <p>Intuitively:</p> <ul> <li>We pick penalties \\((\\lambda, \\mu)\\) for constraint violations.</li> <li>We minimize \\(L\\) with respect to \\(x\\) \u2014 allowing constraint violations but paying for them.</li> <li>The resulting value \\(\\theta(\\lambda, \\mu)\\) is a lower bound on the original problem\u2019s optimum \\(f^*\\).</li> </ul> <p>Formally, for any feasible \\(x\\) and any \\(\\mu \\ge 0\\),  since \\(g_i(x) \\le 0\\) and \\(\\mu_i \\ge 0\\). Taking the infimum over all \\(x\\) gives:  </p> <p>This property is known as weak duality:</p> <p>The dual function provides lower bounds on the primal optimum.</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda, \\mu)\\) is concave, even if \\(f\\) itself is not convex.   (Infimum of affine functions in \\((\\lambda, \\mu)\\) is concave.)</li> <li>It may take the value \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul> <p>The dual function defines a new optimization problem \u2014 the dual problem \u2014 where we maximize this lower bound.</p>"},{"location":"convex/18_duality/#95-the-dual-problem","title":"9.5 The Dual Problem","text":"<p>We now maximize the dual function subject to \\(\\mu \\ge 0\\):  </p> <p>This is the Lagrange dual problem.</p> <p>Let \\(d^*\\) denote the optimal dual value. From weak duality, we always have:  </p> <p>The dual problem is always a concave maximization \u2014 equivalently, a convex optimization problem in the variables \\((\\lambda, \\mu)\\).</p>"},{"location":"convex/18_duality/#96-strong-duality-and-the-zero-duality-gap","title":"9.6 Strong Duality and the Zero Duality Gap","text":"<p>When \\(d^* = f^*\\), we say strong duality holds. The difference \\(f^* - d^*\\) is called the duality gap.</p> <ul> <li>Weak duality: \\(d^* \\le f^*\\) (always true).  </li> <li>Strong duality: \\(d^* = f^*\\) (no gap).</li> </ul> <p>Strong duality means the dual gives exactly the same value as the primal \u2014 and optimal multipliers \\((\\lambda^*, \\mu^*)\\) exist.</p> <p>For convex problems, this beautiful property holds under Slater\u2019s condition (see Chapter 8):</p> <p>If there exists a strictly feasible point \\(\\tilde{x}\\) such that \\(g_i(\\tilde{x}) &lt; 0\\) for all \\(i\\), and \\(h_j(\\tilde{x}) = 0\\) for all \\(j\\), then strong duality holds \u2014 the duality gap is zero.</p> <p>Consequences:</p> <ul> <li>\\(f^* = d^*\\) (zero gap).  </li> <li>Dual variables \\((\\lambda^*, \\mu^*)\\) exist and are finite.  </li> <li>The KKT conditions are both necessary and sufficient for optimality.</li> </ul>"},{"location":"convex/18_duality/#97-duality-and-the-kkt-conditions","title":"9.7 Duality and the KKT Conditions","text":"<p>The KKT conditions (from Chapter 8) are the points where:</p> <ol> <li>The primal is feasible (\\(g_i(x^*) \\le 0\\), \\(h_j(x^*) = 0\\)),</li> <li>The dual is feasible (\\(\\mu_i^* \\ge 0\\)),</li> <li>The gradients balance (stationarity):     </li> <li>Complementary slackness holds (\\(\\mu_i^* g_i(x^*) = 0\\)).</li> </ol> <p>When these hold, \\((x^*, \\lambda^*, \\mu^*)\\) is a primal\u2013dual optimal pair and </p> <p>Geometrically, the primal and dual surfaces touch at the optimum \u2014 the tangent plane defined by \\((\\lambda^*, \\mu^*)\\) supports \\(f\\) exactly.</p>"},{"location":"convex/18_duality/#98-interpreting-dual-variables","title":"9.8 Interpreting Dual Variables","text":"<p>Dual variables have rich interpretations:</p> <ul> <li>\\(\\mu_i^*\\): the shadow price of relaxing inequality \\(g_i(x) \\le 0\\).   A large \\(\\mu_i^*\\) means this constraint is expensive \u2014 small relaxation significantly reduces \\(f\\).</li> <li>\\(\\lambda_j^*\\): the price or force associated with equality \\(h_j(x)=0\\).   Changing the equality\u2019s right-hand side shifts the objective by roughly \\(\\lambda_j^*\\).</li> </ul> <p>In economic or resource allocation problems: - The dual problem represents pricing of limited resources. - The primal problem represents allocation given prices.</p> <p>In machine learning: - SVMs: dual variables correspond to support vectors. - Lasso and Elastic Net: \\(\\ell_1\\) penalties can be viewed as dual constraints on coefficient magnitudes. - Regularized losses: duality expresses the trade-off between data fit and model complexity.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-pareto-optimality-and-multi-objective-convex-optimization","title":"Chapter 10: Pareto Optimality and Multi-Objective Convex Optimization","text":"<p>Optimization often focuses on a single objective function \u2014 minimizing one measure of performance. However, real-world problems rarely involve a single criterion. In practice, we must balance multiple conflicting goals: accuracy vs. complexity, fairness vs. utility, return vs. risk, etc.  </p> <p>This chapter introduces Pareto optimality, which generalizes classical convex optimization to the multi-objective setting, and explores how scalarisation connects multi-objective problems to duality and regularisation.</p>"},{"location":"convex/18a_pareto/#101-classical-optimality","title":"10.1 Classical Optimality","text":"<p>In standard convex optimization, we minimize one convex function:  where \\(\\mathcal{X}\\) is a convex feasible set.</p> <p>Optimality is absolute: there exists (or can exist) one best point minimizing a single criterion. But what happens when we must minimize several convex functions simultaneously?</p>"},{"location":"convex/18a_pareto/#102-multi-objective-convex-optimization","title":"10.2 Multi-Objective Convex Optimization","text":"<p>In many learning and design problems, several objectives compete:</p> Application Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs complexity Fair ML Prediction loss Fairness metric Accuracy vs fairness Portfolio design Return Risk Profit vs stability Information theory Accuracy Compression Fit vs simplicity <p>Formally:  where each \\(f_i(x)\\) is convex. Because no single \\(x\\) minimizes all \\(f_i\\) simultaneously, we define optimality differently \u2014 not as a single point, but as a set of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#103-pareto-optimality","title":"10.3 Pareto Optimality","text":""},{"location":"convex/18a_pareto/#a-strong-pareto-optimality","title":"(a) Strong Pareto Optimality","text":"<p>A point \\(x^* \\in \\mathcal{X}\\) is Pareto optimal if no other \\(x \\in \\mathcal{X}\\) satisfies:  with strict inequality for at least one \\(j\\).</p> <p>Intuitively: no feasible solution can improve one objective without worsening another.</p>"},{"location":"convex/18a_pareto/#b-weak-pareto-optimality","title":"(b) Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no \\(x\\) satisfies:  </p> <p>That is, no feasible solution strictly improves all objectives simultaneously.</p>"},{"location":"convex/18a_pareto/#c-geometric-intuition","title":"(c) Geometric Intuition","text":"<p>In two dimensions \\((f_1, f_2)\\), the Pareto frontier forms the lower-left boundary of the feasible region (for minimization):</p> <ul> <li>Points on the frontier are Pareto optimal \u2014 non-dominated.  </li> <li>Points above or inside are dominated (inferior in all respects).</li> </ul> <p>The frontier visualizes the fundamental trade-offs in the problem.</p>"},{"location":"convex/18a_pareto/#104-scalarisation-reducing-many-objectives-to-one","title":"10.4 Scalarisation: Reducing Many Objectives to One","text":"<p>Since multi-objective problems rarely have a unique minimizer, we often scalarise them \u2014 combine all objectives into a single composite scalar that we can minimize using standard methods.</p>"},{"location":"convex/18a_pareto/#a-weighted-sum-scalarisation","title":"(a) Weighted Sum Scalarisation","text":"\\[ \\min_{x \\in \\mathcal{X}} \\; \\sum_{i=1}^k w_i f_i(x), \\quad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights \\(w_i\\) encode relative importance of objectives.</li> <li>Each choice of \\(w\\) yields a different point on the Pareto frontier.</li> <li>Larger \\(w_i\\) emphasizes objective \\(f_i\\).</li> </ul> <p>Convexity caveat: If the objectives and feasible set are convex, the weighted-sum method recovers the convex portion of the Pareto frontier. Nonconvex parts cannot be reached with simple weights.</p>"},{"location":"convex/18a_pareto/#b-varepsilon-constraint-scalarisation","title":"(b) \\(\\varepsilon\\)-Constraint Scalarisation","text":"<p>Alternatively, minimize one objective while turning others into constraints:</p> \\[ \\min_x f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\; i=2,\\dots,k. \\] <ul> <li>The tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Varying them explores different Pareto-optimal trade-offs.  </li> </ul> <p>Example connection: Ridge regression minimizes fit error subject to a bound on model complexity:  The Lagrangian form:  is a weighted-sum scalarisation \u2014 \\(\\lambda\\) acts as a trade-off parameter.</p>"},{"location":"convex/18a_pareto/#c-duality-and-scalarisation","title":"(c) Duality and Scalarisation","text":"<p>Scalarisation is closely related to duality (Chapter 9):</p> <ul> <li>The weights \\(w_i\\) or Lagrange multipliers \\(\\lambda_i\\) act as dual variables.  </li> <li>Changing them selects different Pareto-optimal points on the frontier.  </li> <li>Regularisation parameters in ML (like \\(\\lambda\\)) are dual to constraint levels \u2014 they move along the Pareto frontier.</li> </ul>"},{"location":"convex/18a_pareto/#105-examples-and-applications","title":"10.5 Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1 \u2013 Regularized Least Squares","text":"<p>Objectives:  </p> <p>Two equivalent forms: 1. Weighted sum:     2. \\(\\varepsilon\\)-constraint:     </p> <p>Both yield Pareto-optimal solutions; \\(\\lambda\\) and \\(\\tau\\) trace the same curve \u2014 the bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2 \u2013 Portfolio Optimization (Risk\u2013Return)","text":"<p>Decision variable: portfolio weights \\(w \\in \\mathbb{R}^n\\). Objectives:  </p> <p>Weighted formulation:  </p> <ul> <li>Varying \\(\\alpha\\) traces the efficient frontier in risk\u2013return space.</li> <li>This forms the basis of Modern Portfolio Theory (Markowitz, 1952).</li> </ul>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-trade-off-in-ml","title":"Example 3 \u2013 Fairness\u2013Accuracy Trade-off in ML","text":"<p>In fair machine learning, we minimize prediction loss while maintaining fairness:  where \\(D\\) is a fairness measure (e.g., demographic disparity).</p> <p>Equivalent scalarized form:  Different \\(\\lambda\\) values trace the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-beta-vae","title":"Example 4 \u2013 Variational Autoencoders and \\(\\beta\\)-VAE","text":"<p>The ELBO in variational inference:  </p> <p>Here we balance two objectives: - Reconstruction accuracy (\\(f_1\\)) - Latent simplicity (\\(f_2\\))</p> <p>The scalarized \\(\\beta\\)-VAE objective:  Parameter \\(\\beta\\) moves the solution along the Pareto frontier between data fidelity and disentanglement.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-regularized-approximation-balancing-fit-and-complexity","title":"Chapter 11: Regularized Approximation \u2013 Balancing Fit and Complexity","text":"<p>Many practical optimization problems involve a trade-off between fitting observed data and controlling model complexity. Regularization formalizes this trade-off as a convex optimization problem that balances these two competing goals.  </p> <p>Building on Chapter 10 (Pareto Optimality), this chapter shows that regularized models correspond to specific points on a Pareto frontier between data fidelity and simplicity.  </p>"},{"location":"convex/18b_regularization/#111-motivation-fit-vs-complexity","title":"11.1 Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we want both:</p> <ol> <li>Data fidelity: minimize the loss or error \\(f(x)\\),  </li> <li>Model simplicity: penalize unnecessary complexity \\(R(x)\\).</li> </ol> <p>This yields a bicriterion problem:  </p> <p>Since improving both simultaneously is typically impossible, we form a scalarized problem:  </p> <ul> <li>\\(f(x)\\) \u2014 data-fitting term (e.g. \\(\\|Ax-b\\|_2^2\\))  </li> <li>\\(R(x)\\) \u2014 regularizer (e.g. \\(\\|x\\|_1\\), \\(\\|x\\|_2^2\\), total variation)  </li> <li>\\(\\lambda\\) \u2014 trade-off parameter controlling bias\u2013variance or fit\u2013complexity balance.</li> </ul> <p>Small \\(\\lambda\\) \u2192 better fit, possible overfitting. Large \\(\\lambda\\) \u2192 simpler, possibly underfit model.</p>"},{"location":"convex/18b_regularization/#112-bicriterion-optimization-and-the-pareto-frontier","title":"11.2 Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization is a scalarised multi-objective problem (Chapter 10). A solution \\(x^*\\) is Pareto optimal if no other feasible \\(x\\) can reduce \\(f(x)\\) without increasing \\(R(x)\\).</p> <ul> <li>For convex \\(f\\) and \\(R\\), each \\(\\lambda \\ge 0\\) yields a Pareto-optimal solution.  </li> <li>The mapping between \\(\\lambda\\) (Lagrange multiplier) and constraint level \\(R(x)\\le t\\) is monotonic.  </li> <li>Each \\(\\lambda\\) thus corresponds to one point on the fit\u2013complexity frontier.</li> </ul> <p>If both \\(f\\) and \\(R\\) are convex, this frontier is also convex; otherwise, scalarisation may miss nonconvex trade-offs.</p>"},{"location":"convex/18b_regularization/#113-why-keep-x-small","title":"11.3 Why Keep \\(x\\) Small?","text":"<p>Ill-posed or noisy inverse problems (\\(Ax \\approx b\\) with ill-conditioned \\(A\\)) often admit many unstable solutions. If \\(A\\) is ill-conditioned, small perturbations in \\(b\\) cause large changes in \\(x\\). Regularization controls this instability by constraining the size or structure of \\(x\\).</p> <p>Example: Ridge Regression  The optimality condition (normal equations):  </p> <ul> <li>The matrix \\(A^\\top A + \\lambda I\\) is positive definite for \\(\\lambda&gt;0\\).  </li> <li>Even if \\(A\\) is rank-deficient, the solution is unique and stable.  </li> <li>Larger \\(\\lambda\\) improves conditioning but increases bias.</li> </ul> <p>Interpretation: Regularization trades variance for stability \u2014 \u201csmooths\u201d the solution landscape and tames directions where data provides weak information.</p>"},{"location":"convex/18b_regularization/#114-constrained-and-lagrangian-forms","title":"11.4 Constrained and Lagrangian Forms","text":"<p>Regularized problems can be equivalently written as constrained convex programs:  </p>"},{"location":"convex/18b_regularization/#lagrangian-formulation","title":"Lagrangian Formulation","text":"<p>The Lagrangian is  The associated penalized form  corresponds to solving the constrained problem for some \\(t&gt;0\\).</p> <p>Under convexity and Slater\u2019s condition, the KKT conditions become:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*), \\quad \\lambda^* \\ge 0, \\quad R(x^*) \\le t, \\quad \\lambda^*(R(x^*)-t)=0. \\] <ul> <li>Penalized and constrained forms yield the same optimality structure.  </li> <li>The mapping \\(\\lambda \\leftrightarrow t\\) is monotonic but not bijective.  </li> <li>Regularization parameters thus act as Lagrange multipliers, weighting one objective against another (Chapter 10).</li> </ul>"},{"location":"convex/18b_regularization/#115-common-regularizers","title":"11.5 Common Regularizers","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"<ul> <li>Smooth, strongly convex \u2192 unique minimizer.  </li> <li>Shrinks coefficients uniformly; improves numerical conditioning.  </li> <li>Bayesian view: corresponds to Gaussian prior \\(x\\sim \\mathcal{N}(0,\\tau^2I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"<ul> <li>Convex but not smooth \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball\u2019s corners align with coordinate axes, leading to zeros in the solution.  </li> <li>Proximal operator (soft-thresholding):    </li> <li>Bayesian view: Laplace prior \\(\\sim e^{-|x_i|/\\tau}\\).</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"<ul> <li>Combines sparsity (L1) with stability (L2).  </li> <li>Ensures uniqueness under correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2","title":"(d) Beyond L1/L2","text":"Regularizer Definition Effect General Tikhonov \\(R(x)=\\|Lx\\|_2^2\\) smoothness via linear operator \\(L\\) Total Variation (TV) \\(R(x)=\\|\\nabla x\\|_1\\) piecewise-constant signals Group Lasso \\(R(x)=\\sum_g \\|x_g\\|_2\\) structured sparsity Nuclear Norm \\(R(X)=\\|X\\|_* = \\sum_i \\sigma_i(X)\\) low-rank matrix recovery <p>Each regularizer defines a distinct geometry \u2014 spheres, diamonds, polytopes \u2014 shaping the solution\u2019s structure.</p>"},{"location":"convex/18b_regularization/#116-choosing-the-regularization-parameter-lambda","title":"11.6 Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-off Behavior","text":"<ul> <li>Small \\(\\lambda\\) \u2192 high fit, high variance.  </li> <li>Large \\(\\lambda\\) \u2192 smoother, more biased solution. \\(\\lambda\\) determines the location on the Pareto frontier.</li> </ul>"},{"location":"convex/18b_regularization/#b-cross-validation-cv","title":"(b) Cross-Validation (CV)","text":"<p>Most common selection strategy:</p> <ol> <li>Split data into \\(k\\) folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining one.  </li> <li>Average validation error, choose \\(\\lambda\\) minimizing it.</li> </ol> <p>Best practices</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>For time series: use blocked or rolling CV.  </li> <li>Use nested CV for fair model comparison.  </li> <li>One-standard-error rule: choose simplest model within 1 SE of best error.</li> </ul>"},{"location":"convex/18b_regularization/#c-analytical-heuristic-alternatives","title":"(c) Analytical / Heuristic Alternatives","text":"<ul> <li>Closed-form rules (ridge shrinkage factor).  </li> <li>Information criteria (AIC/BIC for Lasso).  </li> <li>Regularization paths: trace \\(x^*(\\lambda)\\) as \\(\\lambda\\) varies.  </li> <li>Inverse problems: discrepancy principle or L-curve method.</li> </ul>"},{"location":"convex/18b_regularization/#117-algorithmic-perspective","title":"11.7 Algorithmic Perspective","text":"<p>Regularized convex problems typically take the form</p> <p>  where \\(f\\) is smooth convex and \\(R\\) convex, possibly nonsmooth.</p> <p>Key algorithms:</p> Method Idea Suitable For Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), prox step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update one coordinate at a time Lasso, Elastic Net ADMM Split \\(f\\) and \\(R\\) for parallel structure Large-scale structured problems <p>Proximal operators (Appendix G) handle the nonsmooth term efficiently:</p> <ul> <li>L2 \u2192 scaling (shrinkage)  </li> <li>L1 \u2192 soft-thresholding  </li> <li>TV/Nuclear \u2192 more advanced proximal maps</li> </ul>"},{"location":"convex/18b_regularization/#118-bayesian-interpretation","title":"11.8 Bayesian Interpretation","text":"<p>Regularization corresponds to MAP estimation in probabilistic models.</p> <p>Given the linear model:  and prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\), the MAP estimator is:  </p> <p>The connection:  </p> <ul> <li>Gaussian prior \u2192 L2 penalty (ridge).  </li> <li>Laplace prior \u2192 L1 penalty (sparse MAP estimate).  </li> </ul> <p>Thus, regularization = prior knowledge: it encodes our beliefs about what solutions are likely before seeing data.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization \u2014 convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#121-problem-classes-vs-method-classes","title":"12.1 Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#122-first-order-methods-gradient-descent","title":"12.2 First-order methods: Gradient descent","text":""},{"location":"convex/19_optimizationalgo/#1221-setting","title":"12.2.1 Setting","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  Smoothness lets us control step sizes.</p>"},{"location":"convex/19_optimizationalgo/#1222-algorithm","title":"12.2.2 Algorithm","text":"<p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). A common choice is a constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known, or a backtracking line search when it is not.</p> <p>Derivation: Around \\(x_t\\), we approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <ul> <li>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  </li> <li>If we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable.</li> </ul> <p>This motivates adding a locality restriction \u2014 we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\]"},{"location":"convex/19_optimizationalgo/#1223-geometric-meaning","title":"12.2.3 Geometric meaning","text":"<p>From Chapter 3, the first-order Taylor model is  This is minimised (under a step length constraint) by taking \\(d\\) in the direction \\(-\\nabla f(x)\\). So gradient descent is just \u201ctake a cautious step downhill\u201d.</p>"},{"location":"convex/19_optimizationalgo/#1224-convergence","title":"12.2.4 Convergence","text":"<p>For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p>"},{"location":"convex/19_optimizationalgo/#1225-when-to-use-gradient-descent","title":"12.2.5 When to use gradient descent","text":"<ul> <li>Problems with millions of variables (large-scale ML).</li> <li>You can afford many cheap iterations.</li> <li>You only have access to gradients (or stochastic gradients).</li> <li>You do not need very high precision.</li> </ul> <p>Gradient descent is the baseline first-order method. But we can do better.</p>"},{"location":"convex/19_optimizationalgo/#123-accelerated-first-order-methods","title":"12.3 Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#1231-nesterov-acceleration","title":"12.3.1 Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common presentation is:</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra \\(\\beta_k\\) term \u201clooks ahead,\u201d helping the method exploit curvature better than plain gradient descent.</p>"},{"location":"convex/19_optimizationalgo/#1232-optimal-first-order-rate","title":"12.3.2 Optimal first-order rate","text":"<p>For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives. In other words, you cannot beat \\(O(1/k^2)\\) in the worst case using only first-order oracle calls.</p>"},{"location":"convex/19_optimizationalgo/#1233-when-to-use-acceleration","title":"12.3.3 When to use acceleration","text":"<ul> <li>Same setting as gradient descent (large-scale smooth convex problems),</li> <li>but you want to converge in fewer iterations.</li> <li>You can tolerate a little more instability/parameter tuning (acceleration can overshoot if step sizes are not chosen carefully).</li> </ul> <p>Acceleration is the default upgrade from vanilla gradient descent in many smooth convex machine learning problems.</p> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical)\u2014the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence.</p> <p>In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#124-steepest-descent-method","title":"12.4 Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization. Gradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>"},{"location":"convex/19_optimizationalgo/#1241-mathematical-properties","title":"12.4.1. Mathematical Properties","text":""},{"location":"convex/19_optimizationalgo/#a-normalized-direction","title":"(a) Normalized direction","text":"<p>  \u2192 unit vector with the most negative directional derivative.</p>"},{"location":"convex/19_optimizationalgo/#b-unnormalized-direction","title":"(b) Unnormalized direction","text":"<p>  This gives the actual direction and magnitude used in updates.</p>"},{"location":"convex/19_optimizationalgo/#c-key-identity","title":"(c) Key identity","text":"<p>  The directional derivative equals the negative squared dual norm of the gradient.</p>"},{"location":"convex/19_optimizationalgo/#1242-the-steepest-descent-method","title":"12.4.2. The Steepest Descent Method","text":"<p>The iterative update rule is:  where \\(t_k &gt; 0\\) is a step size (from line search or a fixed rule).</p> <ul> <li>For the Euclidean norm, this reduces to ordinary gradient descent.</li> <li>For other norms, it adapts the search direction to the geometry of the problem.</li> </ul> <p>Convergence: Similar to gradient descent \u2014 linear for general convex functions, potentially faster when level sets are well-conditioned.</p>"},{"location":"convex/19_optimizationalgo/#1243-role-of-the-norm-and-its-influence","title":"12.4.3. Role of the Norm and Its Influence","text":"<p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#a-euclidean-norm-v_2","title":"(a) Euclidean Norm \\(|v|_2\\)","text":"\\[ \\Delta x_{\\text{nsd}} = -\\frac{\\nabla f(x)}{|\\nabla f(x)|*2}, \\quad \\Delta x*{\\text{sd}} = -\\nabla f(x) \\] <p>This is standard gradient descent. The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>"},{"location":"convex/19_optimizationalgo/#b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0","title":"(b) Quadratic Norm \\(|v|_P = (v^T P v)^{1/2}\\), with \\(P \\succ 0\\)","text":"<p>Here, \\(P\\) defines an ellipsoidal metric. The dual norm is \\(|y|_* = (y^T P^{-1} y)^{1/2}\\).</p> \\[ \\Delta x_{\\text{sd}} = -P^{-1}\\nabla f(x) \\] <p>This corresponds to preconditioned gradient descent, where \\(P\\) rescales directions to counter anisotropy in level sets.</p> <p>Interpretation:</p> <ul> <li>If \\(P\\) approximates the Hessian, this becomes Newton\u2019s method.</li> <li>If \\(P\\) is diagonal, it acts like an adaptive step size per coordinate.</li> </ul>"},{"location":"convex/19_optimizationalgo/#c-ell_1-norm","title":"(c) \\(\\ell_1\\)-Norm","text":"\\[ \\Delta x_{\\text{nsd}} = -e_i, \\quad i = \\arg\\max_j \\left|\\frac{\\partial f}{\\partial x_j}\\right| $$ and $$ \\Delta x_{\\text{sd}} = -|\\nabla f(x)|_\\infty e_i \\] <p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p> <p>Geometric intuition: The \\(\\ell_1\\)-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p> <ul> <li>In \\(\\ell_2\\)-norm: the unit ball is a circle \u2192 the steepest direction is exactly opposite the gradient.</li> <li>In \\(\\ell_1\\)-norm: the unit ball is a diamond \u2192 the steepest direction points to a corner (one coordinate).</li> <li>In quadratic norms: the unit ball is an ellipsoid \u2192 the steepest direction follows the metric-adjusted gradient.</li> </ul> <p>Hence, the norm defines the geometry of what \u201csteepest\u201d means.</p>"},{"location":"convex/19_optimizationalgo/#125-conjugate-gradient-method-efficient-optimization-for-quadratic-objectives","title":"12.5 Conjugate Gradient Method \u2014 Efficient Optimization for Quadratic Objectives","text":"<p>Gradient descent can be slow when the objective\u2019s level sets are highly elongated, a symptom of ill-conditioning in the Hessian. For quadratic functions of the form</p> \\[ f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x, \\quad A \\succ 0, \\] <p>plain gradient descent takes many small steps along shallow directions of \\(A\\).</p> <p>The Conjugate Gradient (CG) method accelerates convergence dramatically for such problems \u2014 it exploits the structure of the quadratic and uses curvature-aware search directions without explicitly forming or inverting the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#problem-setup","title":"Problem Setup","text":"<p>Minimize a strictly convex quadratic:</p> \\[ \\min_x f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x, \\quad A \\in \\mathbb{R}^{n \\times n}, \\; A \\succ 0. \\] <p>This is equivalent to solving the linear system</p> \\[ A x = b. \\]"},{"location":"convex/19_optimizationalgo/#algorithm-linear-cg","title":"Algorithm (Linear CG)","text":"<p>Given an initial \\(x_0\\), define the residual \\(r_0 = b - A x_0\\)  and the initial direction \\(p_0 = r_0\\).</p> <p>For \\(k = 0, 1, 2, \\dots\\) until convergence:</p> <ol> <li>Compute step size </li> <li>Update the iterate </li> <li>Update the residual </li> <li>Compute the new direction coefficient </li> <li>Update the direction </li> </ol> <p>Terminate when \\(\\|r_k\\|\\) is below tolerance \\(\\varepsilon\\).</p>"},{"location":"convex/19_optimizationalgo/#geometric-intuition","title":"Geometric Intuition","text":"<p>Each search direction \\(p_k\\) is \\(A\\)-conjugate to the previous ones:</p> \\[ p_i^\\top A p_j = 0 \\quad \\text{for } i \\ne j. \\] <p>That means successive steps explore independent curvature directions of the quadratic. The residual \\(r_k\\) (the negative gradient) becomes orthogonal to all previous directions, so the method never re-searches the same subspace.</p> <p>As a result, in exact arithmetic, CG finds the exact minimizer in at most \\(n\\) steps.</p>"},{"location":"convex/19_optimizationalgo/#convergence-properties","title":"Convergence Properties","text":"<ul> <li>For SPD \\(A\\), CG converges monotonically to the minimizer \\(x^\\star = A^{-1} b\\).</li> <li>The rate depends on the condition number \\(\\kappa(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\):    </li> <li>Preconditioning (using \\(M^{-1}A\\) with well-chosen \\(M\\)) further improves convergence.</li> </ul>"},{"location":"convex/19_optimizationalgo/#machine-learning-context","title":"Machine Learning Context","text":"<p>In ML, CG is widely used for large-scale convex quadratic subproblems:</p> Application Formulation Notes Ridge regression \\(\\min_x \\|A x - b\\|_2^2 + \\lambda\\|x\\|_2^2\\) Normal equations are SPD; CG avoids explicit inversion. Kernel ridge regression \\((K + \\lambda I)\\alpha = y\\) CG solves this efficiently without forming full \\(K^{-1}\\). Linear least squares \\(\\min_x \\tfrac{1}{2}\\|A x - b\\|^2\\) Equivalent to solving \\(A^\\top A x = A^\\top b\\). Large-scale Newton steps Solve \\(\\nabla^2 f(x_k)p = -\\nabla f(x_k)\\) CG acts as an inner solver for the Newton direction."},{"location":"convex/19_optimizationalgo/#practical-notes","title":"Practical Notes","text":"<ul> <li>CG requires only matrix\u2013vector products with \\(A\\), not explicit storage.   It\u2019s ideal when \\(A\\) is large, sparse, or implicitly defined.</li> <li>Sensitive to rounding errors; residual re-orthogonalization may be needed for long runs.</li> <li>Preconditioners (Jacobi, incomplete Cholesky, etc.) can drastically reduce iterations.</li> </ul>"},{"location":"convex/19_optimizationalgo/#comparison-summary","title":"Comparison Summary","text":"Method Memory Curvature Use Convergence Typical Use Gradient Descent \\(O(n)\\) None \\(O(1/k)\\) General smooth convex Newton\u2019s Method \\(O(n^2)\\) Full Hessian Quadratic (local) Small/medium convex Conjugate Gradient \\(O(n)\\) Implicit (via \\(A\\)-conjugacy) Fast linear / finite-step Large quadratic systems <p>-</p>"},{"location":"convex/19_optimizationalgo/#key-insight","title":"Key Insight","text":"<p>The Conjugate Gradient method is the exact gradient method for quadratic objectives that automatically builds curvature information through orthogonalized directions, achieving Newton-like efficiency without forming the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#126-newtons-method-and-second-order-methods","title":"12.6 Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p>"},{"location":"convex/19_optimizationalgo/#1261-local-quadratic-model","title":"12.6.1 Local quadratic model","text":"<p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step points toward the minimizer of the local quadratic model, and near the true minimizer, Newton\u2019s method exhibits quadratic convergence.</p>"},{"location":"convex/19_optimizationalgo/#1262-convergence-behaviour","title":"12.6.2 Convergence behaviour","text":"<ul> <li>Near the minimiser of a strictly convex, twice-differentiable \\(f\\), Newton\u2019s method converges quadratically: roughly, the number of correct digits doubles every iteration.  </li> <li>This is dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order methods \u2014 but only once the iterates enter the basin of attraction.  </li> <li>Far from the minimiser, Newton\u2019s method can behave erratically or even diverge.   To stabilise it, we typically pair it with a line search or trust region strategy to control step size.</li> </ul>"},{"location":"convex/19_optimizationalgo/#1263-implementation","title":"12.6.3 Implementation","text":"<p>The main computational effort in each iteration lies in evaluating derivatives and solving the Newton system:</p> \\[ H \\, \\Delta x = -g, \\] <p>where</p> \\[ H = \\nabla^2 f(x), \\quad g = \\nabla f(x). \\]"},{"location":"convex/19_optimizationalgo/#solving-via-cholesky-factorization","title":"Solving via Cholesky factorization","text":"<p>If \\(H\\) is symmetric and positive definite, we can efficiently solve this system using a Cholesky factorization:</p> \\[ H = L L^{\\top}, \\] <p>where \\(L\\) is lower triangular. The Newton step is then:</p> \\[ \\Delta x_{\\text{nt}} = -L^{-\\top} L^{-1} g. \\] <p>This involves two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^{\\top} \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids explicitly computing \\(H^{-1}\\) and ensures numerical stability.</p>"},{"location":"convex/19_optimizationalgo/#newton-decrement","title":"Newton decrement","text":"<p>A useful measure of progress is the Newton decrement:</p> \\[ \\lambda(x) = \\| L^{-1} g \\|_2, \\] <p>which approximates how far we are from the optimum. A common stopping criterion is \\(\\lambda(x)^2 / 2 &lt; \\varepsilon\\).</p>"},{"location":"convex/19_optimizationalgo/#1264-computational-cost","title":"12.6.4 Computational cost","text":"<p>Each Newton step requires solving a linear system involving \\(\\nabla^2 f(x_k)\\), which costs about as much as factoring the Hessian (or an approximation).</p> <ul> <li>For an unstructured, dense Hessian, Cholesky factorization requires approximately \\((1/3) n^3\\) floating-point operations.  </li> <li>If \\(H\\) is sparse, banded, or has special structure, the cost can be much lower.  </li> <li>Because of this cubic scaling, Newton\u2019s method is most attractive for medium-scale problems where high accuracy is required.</li> </ul>"},{"location":"convex/19_optimizationalgo/#1265-why-convexity-helps","title":"12.6.5 Why convexity helps","text":"<p>If \\(f\\) is convex, then \\(\\nabla^2 f(x_k)\\) is positive semidefinite (Chapter 5). This has two important implications:</p> <ul> <li>The local quadratic model is bowl-shaped, so the Newton direction points toward a minimiser.  </li> <li>Regularised Newton steps (e.g. using \\(H + \\mu I\\) for small \\(\\mu &gt; 0\\)) are guaranteed to be descent directions and behave predictably.</li> </ul>"},{"location":"convex/19_optimizationalgo/#1266-quasi-newton-methods","title":"12.6.6 Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory</p> <p>They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods.</p> <p>For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p>"},{"location":"convex/19_optimizationalgo/#1267-when-to-use-newton-or-quasi-newton-methods","title":"12.6.7 When to use Newton or quasi-Newton methods","text":"<p>Use Newton or quasi-Newton methods when:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p> <p>Newton-Raphson: The Newton step solves \\(\\nabla^2 f(x_k) p_k=-\\nabla f(x_k)\\) and updates \\(x_{k+1}=x_k+p_k\\) with line search or trust-region safeguards. Complexity hinges on solving linear systems; use sparse Cholesky, conjugate gradients with preconditioning, or low-rank structure to scale. For generalized linear models, iteratively reweighted least squares converges in few iterations, but regularization and damping are needed when data are nearly separable.</p> <p>Gauss-Newton: For nonlinear least squares \\(f(x)=\\tfrac12\\|r(x)\\|^2\\), the Gauss\u2013Newton approximation uses \\(H\\approx J^\\top J\\) where \\(J\\) is the Jacobian of \\(r\\). Solve \\((J^\\top J)\\Delta=-J^\\top r\\) to get a step; Levenberg\u2013Marquardt adds damping \\((J^\\top J+\\lambda I)\\Delta=-J^\\top r\\) interpolating between gradient and Gauss\u2013Newton. Effective for residual models where second-order residual terms are small; widely used in curve fitting and some deep learning layerwise updates.</p>"},{"location":"convex/19_optimizationalgo/#127-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"12.7 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex optimization problems are not purely smooth. They often include:</p> <ul> <li>Constraints: \\(x \\in \\mathcal{X}\\),</li> <li>Nonsmooth regularisers: such as \\(\\|x\\|_1\\),</li> <li>Penalties: promoting robustness or sparsity (see Chapter 6).</li> </ul> <p>Two core strategies handle such settings:</p> <ol> <li>Projected gradient methods \u2014 where we project each iterate back into the feasible set \\(\\mathcal{X}\\).  </li> <li>Proximal gradient methods \u2014 which generalize projection to handle nonsmooth but structured terms.</li> </ol> <p>These methods extend the ideas of gradient and Newton updates to the broader world of constrained and composite optimization.</p>"},{"location":"convex/19_optimizationalgo/#1272-convergence-behaviour","title":"12.7.2 Convergence behaviour","text":"<ul> <li>Near the minimiser of a strictly convex, twice-differentiable \\(f\\), Newton\u2019s method converges quadratically: roughly, the number of correct digits doubles every iteration.</li> <li>This is dramatically faster than \\(O(1/k)\\) or \\(O(1/k^2)\\), but only once you\u2019re in the \u201cbasin of attraction.\u201d</li> <li>Far from the minimiser, Newton can misbehave, so we pair it with a line search or trust region.</li> </ul>"},{"location":"convex/19_optimizationalgo/#1273-computational-cost","title":"12.7.3 Computational cost","text":"<p>Each Newton step requires solving a linear system involving \\(\\nabla^2 f(x_k)\\), which costs about as much as factoring the Hessian (or an approximation). This is expensive in very high dimensions, which is why Newton is most attractive for medium-scale problems where high accuracy matters.</p>"},{"location":"convex/19_optimizationalgo/#1274-why-convexity-helps","title":"12.7.4 Why convexity helps","text":"<p>If \\(f\\) is convex, then \\(\\nabla^2 f(x_k)\\) is positive semidefinite (Chapter 5). This means:</p> <ul> <li>The quadratic model is bowl-shaped, so the Newton step makes sense.</li> <li>Regularised Newton steps (adding a multiple of the identity to the Hessian) behave very predictably.</li> </ul>"},{"location":"convex/19_optimizationalgo/#1275-quasi-newton","title":"12.7.5 Quasi-Newton","text":"<p>When Hessians are too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. Famous examples include BFGS and L-BFGS. These methods keep much of Newton\u2019s fast local convergence but with per-iteration cost closer to first-order methods.</p>"},{"location":"convex/19_optimizationalgo/#1276-when-to-use-newton-quasi-newton","title":"12.7.6 When to use Newton / quasi-Newton","text":"<ul> <li>You need high-accuracy solutions.</li> <li>The problem is smooth and reasonably well-conditioned.</li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g. via sparse linear algebra).</li> </ul>"},{"location":"convex/19_optimizationalgo/#128-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"12.8 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#1281-projected-gradient-descent","title":"12.8.1 Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#1282-proximal-gradient-forwardbackward-splitting","title":"12.8.2 Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#129-penalties-barriers-and-interior-point-methods","title":"12.9 Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\)? Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#1291-penalty-methods","title":"12.9.1 Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints.</p> <p>Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul> <p>Penalty methods are closely linked to robust formulations and Huber-like losses: you replace a hard requirement by a soft cost. This is exactly what you do in robust regression and in \\(\\epsilon\\)-insensitive / Huber losses (see Section 9.7).</p>"},{"location":"convex/19_optimizationalgo/#1292-barrier-methods","title":"12.9.2 Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary.</p> <p>For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul> <p>This is the core idea of interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#1293-interior-point-methods-in-practice","title":"12.9.3 Interior-point methods in practice","text":"<p>Interior-point methods:</p> <ul> <li>Are globally convergent for convex problems under mild assumptions (Slater\u2019s condition; see Chapter 8).</li> <li>Solve a series of smooth, strictly feasible subproblems.</li> <li>Use Newton-like steps to update primal (and, implicitly, dual) variables.</li> <li>Produce both primal and dual iterates \u2014 so they naturally produce a duality gap, which certifies how close you are to optimality (Chapter 8).</li> </ul> <p>Interior-point methods are the engine behind modern general-purpose convex solvers for:</p> <ul> <li>linear programs (LP),</li> <li>quadratic programs (QP),</li> <li>second-order cone programs (SOCP),</li> <li>semidefinite programs (SDP).</li> </ul> <p>They give high-accuracy answers and KKT-based optimality certificates. They are more expensive per iteration than gradient methods, but need far fewer iterations, and they handle fully general convex constraints.</p> <p>Summary: Penalty vs Barrier vs Interior-Point</p> Method Feasibility During Iteration Mechanism Typical Behavior Penalty May violate constraints Adds large penalty outside feasible region Easy to implement but can be ill-conditioned Barrier Stays strictly feasible Adds infinite cost near constraint boundary Smooth approximation to constrained problem Interior-Point Always feasible (uses barrier) Solves a sequence of barrier problems with increasing precision Follows central path to true optimum"},{"location":"convex/19_optimizationalgo/#1210-choosing-the-right-method-in-practice","title":"12.10 Choosing the right method in practice","text":"<p>Let\u2019s summarise the chapter in the form of a decision guide.</p> <p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever the variables must satisfy one or more exact relations \u2014 such as conservation laws, normalization, or fairness criteria. We study algorithms for minimizing a function subject to linear or nonlinear equality constraints:</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>Such problems are fundamental in convex optimization, quadratic programming, and many ML formulations involving exact invariants.</p>"},{"location":"convex/19a_optimization_constraints/#131-geometric-view-optimization-on-an-affine-manifold","title":"13.1 Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set, a lower-dimensional plane within \\(\\mathbb{R}^n\\). The feasible region is:</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank (\\(\\operatorname{rank}(A)=p\\)), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine manifold.</p> <p>Geometrically, optimization proceeds not over all \\(\\mathbb{R}^n\\), but along this manifold. At the optimum, the gradient \\(\\nabla f(x^\\star)\\) cannot point in a direction that stays feasible\u2014hence it must be orthogonal to the feasible surface. This gives the first key optimality relation:</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star, \\] <p>where \\(\\nu^\\star\\) is a vector of Lagrange multipliers capturing how sensitive the objective is to constraint perturbations.</p> <p>Intuition: The gradient of the objective at the optimum lies in the span of the constraint normals (rows of \\(A\\)). Any feasible direction must lie in the null space of \\(A\\), orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#132-lagrange-function-and-kkt-system","title":"13.2 Lagrange Function and KKT System","text":"<p>Define the Lagrangian:</p> \\[ \\mathcal{L}(x, \\nu) = f(x) + \\nu^\\top (A x - b). \\] <p>The first-order (KKT) conditions for a feasible point \\((x^\\star, \\nu^\\star)\\) to be optimal are:</p> \\[ \\begin{aligned} \\nabla f(x^\\star) + A^\\top \\nu^\\star &amp;= 0, \\\\ A x^\\star &amp;= b. \\end{aligned} \\] <p>These equations express stationarity and feasibility simultaneously. They can be combined into the KKT linear system:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>At the optimum, the right-hand side is zero.</p> <p>ML Connection: Lagrange multipliers \\(\\nu\\) quantify trade-offs between objectives and hard constraints \u2014 for instance, enforcing weight normalization in a neural layer, balance constraints in fair classification, or conservation laws in physics-informed networks.</p>"},{"location":"convex/19a_optimization_constraints/#133-the-quadratic-case","title":"13.3 The Quadratic Case","text":"<p>For a quadratic objective  with \\(P \\succeq 0\\), the KKT conditions reduce to a linear system:</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>This is a saddle-point system, solvable by factorization or elimination. If \\(P \\succ 0\\) and \\(A\\) has full row rank, the solution \\((x^\\star, \\nu^\\star)\\) is unique.</p> <p>In ML, such systems appear in constrained least squares, e.g. enforcing \\(\\sum_i w_i = 1\\) in portfolio optimization or convex combination weights in mixture models.</p>"},{"location":"convex/19a_optimization_constraints/#134-the-null-space-reduced-variable-method","title":"13.4 The Null-Space (Reduced Variable) Method","text":"<p>If \\(A\\) has full row rank, we can find a particular feasible point \\(x_0\\) such that \\(A x_0 = b\\), and a basis \\(Z\\) for the null space of \\(A\\) satisfying \\(A Z = 0\\). Then any feasible \\(x\\) can be written as:</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective gives a reduced problem:</p> \\[ \\min_y \\; f(x_0 + Z y). \\] <p>This is an unconstrained problem in \\(y\\), solvable by gradient or Newton methods. The reduced gradient and Hessian are:</p> \\[ \\nabla_y f = Z^\\top \\nabla_x f, \\qquad \\nabla_y^2 f = Z^\\top \\nabla_x^2 f \\, Z. \\] <p>Interpretation: Optimization proceeds only along feasible directions \u2014 those that do not violate the constraints (i.e., within \\(\\operatorname{Null}(A)\\)). This is equivalent to projecting all gradient steps onto the tangent space of the constraint manifold.</p>"},{"location":"convex/19a_optimization_constraints/#135-newtons-method-for-equality-constrained-problems","title":"13.5 Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice differentiable \\(f\\), the equality-constrained Newton step solves the quadratic subproblem:</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d, \\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>This produces the step \\((d, \\lambda)\\) from the linearized KKT system:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>The update is \\(x_{k+1} = x_k + \\alpha d\\), ensuring \\(A x_{k+1} = b\\) if \\(A x_k = b\\).</p> <p>Geometric insight: The Newton direction is the projection of the unconstrained Newton step onto the tangent space of the feasible set (directions satisfying \\(A d = 0\\)). Thus, each step stays within the affine constraint manifold.</p> <p>In practice: The KKT system is typically solved by Schur complement factorization:  which then yields \\(d = -(\\nabla^2 f)^{-1} (\\nabla f + A^\\top \\lambda)\\).</p>"},{"location":"convex/19a_optimization_constraints/#136-infeasible-start-newton-method","title":"13.6 Infeasible Start Newton Method","text":"<p>When starting from an infeasible point (\\(A x_0 \\ne b\\)), we relax the constraint and drive feasibility progressively. At iteration \\(k\\), compute \\((\\Delta x, \\Delta \\nu)\\) by solving:</p> \\[ \\begin{bmatrix} \\nabla^2 f(x_k) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x_k) + A^\\top \\nu_k \\\\ A x_k - b \\end{bmatrix}. \\] <p>Then update:</p> \\[ x_{k+1} = x_k + \\alpha \\Delta x, \\quad \\nu_{k+1} = \\nu_k + \\alpha \\Delta \\nu. \\] <p>This method enforces feasibility gradually, converging to \\((x^\\star, \\nu^\\star)\\) under mild conditions.</p> <p>In ML contexts, infeasible starts are typical \u2014 we rarely have feasible initialization (e.g., in constrained autoencoders or regularized fairness models). The infeasible Newton method ensures consistent progress in both primal feasibility (\\(A x = b\\)) and dual stationarity (\\(\\nabla f + A^\\top \\nu = 0\\)).</p>"},{"location":"convex/19a_optimization_constraints/#137-computational-considerations","title":"13.7 Computational Considerations","text":"<ul> <li>Factorization: KKT systems can be large but structured. Exploiting sparsity in \\(\\nabla^2 f\\) and \\(A\\) is essential in high-dimensional problems.</li> <li>Stability: Adding small regularization to the (0,0) block of the KKT matrix improves conditioning:    </li> <li>Schur Complement: Eliminating \\(\\Delta x\\) yields a smaller linear system in \\(\\Delta \\nu\\), which can be more efficient when \\(p \\ll n\\).</li> </ul>"},{"location":"convex/19a_optimization_constraints/#138-connections-to-machine-learning","title":"13.8 Connections to Machine Learning","text":"<p>Equality-constrained optimization appears in several ML and signal processing settings:</p> Example Equality Constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights must sum to 1 Fair classification \\(A w = 0\\) Enforces equal outcomes across groups Orthogonal embeddings \\(W^\\top W = I\\) Preserves independence / energy Normalization layers \\(\\|w\\|_2^2 = 1\\) Scale invariance constraint Physics-informed models \\(\\text{div}(F)=0\\) Conservation of mass / charge"},{"location":"convex/19a_optimization_constraints/#summary-approaches-to-equality-constrained-optimization","title":"Summary: Approaches to Equality-Constrained Optimization","text":"Approach Constraint Type Feasibility (Local/Global) Core Idea Advantages Limitations / Drawbacks Typical ML / Optimization Use Null-Space (Variable Elimination) Linear, full-rank \\(A\\) Global Parameterize feasible \\(x = x_0 + Z y\\) with \\(A Z = 0\\) Converts to unconstrained problem; dimension reduction; exact Requires null-space basis \\(Z\\); destroys sparsity; expensive for large \\(A\\) Constrained least squares, small-scale convex programs Local Parameterization (Manifold Method) Nonlinear \\(g(x) = 0\\) Local (around feasible point) Use implicit function theorem: locally express \\(x = x(y)\\) Captures nonlinear manifold structure; geometric insight Valid only locally; requires Jacobians; expensive Manifold learning, orthogonal embeddings, equality-regularized networks KKT / Lagrange System Linear or nonlinear Global (if convex) Solve coupled system \\(\\nabla f + A^\\top \\nu = 0\\), \\(A x = b\\) Keeps structure; allows dual interpretation; works for large sparse systems Larger system; more variables Quadratic programming, convex solvers, equality-constrained ML models Primal\u2013Dual Newton Method Linear or nonlinear Global (convex) Newton\u2019s method on full KKT system Quadratic convergence near optimum; stable numerically Requires Hessians and factorizations Interior-point solvers, primal\u2013dual optimization, barrier methods Penalty / Augmented Lagrangian General (convex or nonconvex) Approximate (drives feasibility) Add penalty term \\(\\tfrac{\\rho}{2}\\|A x - b\\|^2\\) or dual updates Simple to implement; smooth transition from unconstrained Needs tuning of \\(\\rho\\); slow convergence to exact feasibility Regularized fairness, soft constraints, physics-informed networks Projection / Normalization Step Linear or nonlinear (simple form) Iterative (after each step) Project back to feasible set: \\(x_{k+1} = \\Pi_{\\{A x = b\\}}(x_{k+1})\\) Keeps updates feasible; easy for simple constraints Costly for complex \\(A\\); may distort gradient direction Normalization layers, unit-norm or balance constraints"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In practice, optimization problems often include inequalities that restrict feasible solutions to a convex region. Examples include nonnegativity of variables, margin constraints in support vector machines, fairness or safety limits, and physical conservation laws. This chapter introduces algorithms for solving such problems efficiently, focusing on the logarithmic barrier and interior-point methods that underpin modern convex solvers.</p>"},{"location":"convex/19b_optimization_constraints/#141-problem-setup","title":"14.1 Problem Setup","text":"<p>We consider the general convex optimization problem with both equality and inequality constraints:</p> \\[ \\begin{aligned} \\text{minimize}   &amp;\\quad f_0(x) \\\\ \\text{subject to} &amp;\\quad f_i(x) \\le 0, \\quad i=1,\\dots,m,\\\\ &amp;\\quad A x = b. \\end{aligned} \\] <p>Assumptions:</p> <ul> <li>Each \\(f_i\\) is convex and twice differentiable.  </li> <li>\\(A \\in \\mathbb{R}^{p\\times n}\\) has full row rank (\\(\\mathrm{rank}(A)=p\\)).  </li> <li>There exists a strictly feasible point \\(\\bar{x}\\) such that \\(f_i(\\bar{x})&lt;0\\) and \\(A\\bar{x}=b\\) (Slater\u2019s condition).  </li> </ul> <p>Under these assumptions, strong duality holds and the KKT conditions are necessary and sufficient for optimality.</p>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem \\(f_0(x)\\) \\(f_i(x)\\) Notes / ML context Linear Program (LP) \\(c^T x\\) \\(a_i^T x - b_i\\) Feature selection, resource allocation Quadratic Program (QP) \\(\\tfrac{1}{2}x^T P x + q^T x\\) Linear \\(a_i^T x - b_i\\) SVM training, ridge regression QCQP Quadratic Quadratic Portfolio optimization, control Geometric Program (log domain) Convex in \\(\\log x\\) Linear in \\(\\log x\\) Network flow, resource allocation Entropy minimization \\(\\sum_i x_i \\log x_i\\) \\(F x \\le g\\) Probability calibration, information bottleneck"},{"location":"convex/19b_optimization_constraints/#142-indicator-function-reformulation","title":"14.2 Indicator-Function Reformulation","text":"<p>Define the indicator of the nonpositive orthant:</p> \\[ I_-(u)= \\begin{cases} 0, &amp; u \\le 0,\\\\ +\\infty, &amp; u &gt; 0. \\end{cases} \\] <p>Then the constrained problem is equivalent to</p> \\[ \\min_x \\; f_0(x) + \\sum_{i=1}^m I_-(f_i(x)) \\quad \\text{s.t. } A x = b. \\] <p>This form is conceptually clear but nondifferentiable since \\(I_-\\) is discontinuous. To apply Newton-type algorithms, we replace \\(I_-\\) with a smooth approximation: the logarithmic barrier.</p>"},{"location":"convex/19b_optimization_constraints/#143-logarithmic-barrier-approximation","title":"14.3 Logarithmic-Barrier Approximation","text":"<p>We approximate each \\(I_-(f_i(x))\\) by a differentiable barrier function \\(\\Phi(u) = -\\tfrac{1}{t} \\log(-u)\\) for \\(u &lt; 0\\). The smoothed subproblem becomes</p> \\[ \\min_x \\; f_0(x) - \\frac{1}{t} \\sum_{i=1}^m \\log(-f_i(x)) \\quad \\text{s.t. } A x = b. \\] <ul> <li>For small \\(t\\): the barrier is strong and keeps points deep inside the feasible region.  </li> <li>As \\(t \\to \\infty\\): the barrier weakens and the solution approaches the true optimum.</li> </ul> <p>Hence, the original inequality-constrained problem is replaced by a sequence of smooth equality-constrained subproblems.</p>"},{"location":"convex/19b_optimization_constraints/#144-properties-of-the-barrier-function","title":"14.4 Properties of the Barrier Function","text":"<p>Define</p> \\[ \\phi(x) = -\\sum_{i=1}^m \\log(-f_i(x)), \\qquad \\mathrm{dom}\\,\\phi = \\{x : f_i(x) &lt; 0\\}. \\] <p>Then \\(\\phi\\) is convex and twice differentiable:</p> \\[ \\nabla \\phi(x) = \\sum_i \\frac{1}{-f_i(x)} \\nabla f_i(x), \\] \\[ \\nabla^2 \\phi(x) = \\sum_i \\frac{1}{f_i(x)^2} \\nabla f_i(x)\\nabla f_i(x)^T + \\sum_i \\frac{1}{-f_i(x)} \\nabla^2 f_i(x). \\] <p>Near the boundary \\(f_i(x)=0\\), the gradient norm grows without bound \u2014 producing a repulsive force that prevents violation of constraints.</p>"},{"location":"convex/19b_optimization_constraints/#145-central-path-and-approximate-kkt-conditions","title":"14.5 Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^*(t)\\) minimize the barrier problem</p> \\[ \\min_x\\; t f_0(x) + \\phi(x) \\quad \\text{s.t. } A x = b. \\] <p>The curve \\(\\{x^*(t) : t &gt; 0\\}\\) is the central path. As \\(t \\to \\infty\\), \\(x^*(t)\\) approaches the true optimal solution \\(x^*\\). Along this path there exist dual variables \\((\\lambda^*(t), v^*(t))\\) satisfying</p> \\[ \\begin{aligned} \\nabla f_0(x^*(t)) + \\sum_i \\lambda_i^*(t) \\nabla f_i(x^*(t)) + A^T v^*(t) &amp;= 0,\\\\ A x^*(t) &amp;= b,\\\\ -\\lambda_i^*(t) f_i(x^*(t)) &amp;= \\tfrac{1}{t}, \\quad \\lambda_i^*(t) \\ge 0. \\end{aligned} \\] <p>The complementarity condition is relaxed: \\(\\lambda_i f_i(x) = -1/t\\) instead of \\(0\\). As \\(t \\to \\infty\\), these approximate KKT conditions converge to the exact ones.</p>"},{"location":"convex/19b_optimization_constraints/#146-geometric-and-physical-intuition","title":"14.6 Geometric and Physical Intuition","text":"<p>The centering subproblem</p> \\[ \\min_x\\; t f_0(x) - \\sum_i \\log(-f_i(x)) \\] <p>can be viewed as a particle system in a potential field:</p> <ul> <li>The objective \\(f_0(x)\\) pulls toward lower cost (external force).  </li> <li>Each constraint \\(f_i(x)\\le0\\) creates a repulsive potential that diverges near the boundary.  </li> </ul> <p>At equilibrium, these forces balance:</p> \\[ \\nabla f_0(x^*(t)) + \\sum_i \\frac{1}{t(-f_i(x^*(t)))} \\nabla f_i(x^*(t)) = 0. \\] <p>Thus, the solution remains strictly feasible \u2014 this is the essence of the interior-point philosophy.</p>"},{"location":"convex/19b_optimization_constraints/#147-barrier-method-algorithm","title":"14.7 Barrier-Method Algorithm","text":"<p>The barrier method converts the original inequality-constrained problem into a sequence of smooth equality-constrained subproblems. Each subproblem is solved exactly (to high precision) while a barrier parameter \\(t\\) is gradually increased, allowing the iterates to approach the boundary and the true constrained optimum.</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-outline","title":"Algorithm Outline","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (so \\(f_i(x) &lt; 0\\) for all \\(i\\)),</li> <li>an initial barrier parameter \\(t &gt; 0\\),</li> <li>a barrier scaling factor \\(\\mu &gt; 1\\) (usually between 10 and 20),</li> <li>and a desired accuracy \\(\\varepsilon &gt; 0\\) (for stopping),</li> </ul> <p>the algorithm proceeds as follows:</p> <ol> <li> <p>Centering step: Solve     using Newton\u2019s method for equality-constrained optimization. The result \\(x^*(t)\\) is the centering point for the current \\(t\\).</p> </li> <li> <p>Update iterate:  Set \\(x := x^*(t)\\).</p> </li> <li> <p>Stopping criterion:  Stop if         Here \\(m\\) is the number of inequality constraints, and \\(\\varepsilon\\) is the desired tolerance on suboptimality. This rule is derived from the duality gap bound:        meaning that if \\(m/t\\) is smaller than \\(\\varepsilon\\), the current solution is guaranteed to be within \\(\\varepsilon\\) of the true optimum.</p> </li> <li> <p>Increase barrier parameter: Set \\(t := \\mu t\\) and return to Step 1.    Each centering subproblem maintains strict feasibility, and increasing \\(t\\) gradually weakens the barrier, allowing the iterates to approach the true constraint boundary. A typical choice is \\(\\mu \\in [10, 20]\\).</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#understanding-varepsilon-the-accuracy-parameter","title":"Understanding \\(\\varepsilon\\) \u2014 the Accuracy Parameter","text":"<p>The parameter \\(\\varepsilon\\) controls how close to the optimal solution we wish to stop.</p> <ul> <li> <p>Mathematically, \\(\\varepsilon\\) specifies an upper bound on the duality gap:    </p> </li> <li> <p>Conceptually, \\(\\varepsilon\\) represents the trade-off between accuracy and computational cost:</p> </li> <li>Smaller \\(\\varepsilon\\) \u2192 more iterations (larger \\(t\\) required).</li> <li>Larger \\(\\varepsilon\\) \u2192 faster termination, but lower accuracy.</li> </ul> <p>In practice:</p> <ul> <li>For numerical optimization or ML training, \\(\\varepsilon\\) is often set between \\(10^{-3}\\) and \\(10^{-8}\\) depending on problem size and desired precision.  </li> <li>Convex solvers (like CVX, MOSEK, or ECOS) typically use \\(\\varepsilon \\approx 10^{-6}\\) as a default high-accuracy target.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#intuitive-interpretation","title":"Intuitive Interpretation","text":"<ul> <li>Think of \\(\\varepsilon\\) as the \u201cdistance\u201d between the current point and the true optimum in terms of objective value.  </li> <li>The ratio \\(m/t\\) acts like a thermometer for this distance \u2014 as \\(t\\) grows, the temperature (error) cools down.</li> <li>Once \\(m/t &lt; \\varepsilon\\), we know the algorithm has cooled sufficiently: the point lies extremely close to the optimal constrained solution.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#summary-of-key-parameters","title":"Summary of Key Parameters","text":"Symbol Meaning Typical Value / Range Intuitive Role \\(m\\) Number of inequality constraints problem dependent Total number of barrier terms \\(t\\) Barrier parameter starts small (1\u201310), grows by \\(\\mu\\) Controls strength of barrier \\(\\mu\\) Barrier growth factor 10\u201320 Controls how fast we approach constraint boundary \\(\\varepsilon\\) Desired accuracy (tolerance) \\(10^{-3}\\) to \\(10^{-8}\\) Stopping threshold based on duality gap"},{"location":"convex/19b_optimization_constraints/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Each centering step finds the best interior point for a given barrier strength \\(1/t\\).  </li> <li>Increasing \\(t\\) reduces the barrier effect, letting \\(x\\) approach the boundary.  </li> <li>The stopping rule \\(m/t &lt; \\varepsilon\\) ensures that the objective value of \\(x\\) differs from the true optimum by less than \\(\\varepsilon\\).  </li> <li>Smaller \\(\\varepsilon\\) means tighter optimality, but more work (larger \\(t\\) and more iterations).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#148-computational-and-practical-notes","title":"14.8 Computational and Practical Notes","text":"<ul> <li>Each centering problem is solved by equality-constrained Newton steps (KKT system).  </li> <li>Barrier methods inherit superlinear convergence near the optimum.  </li> <li>Initialization must be strictly feasible; feasibility restoration can be costly.  </li> <li>Large \\(t\\) makes the barrier steep, so line search and step damping are essential.</li> </ul> <p>In machine learning: - SVM and logistic regression margin constraints fit naturally in this form. - Interior-point solvers for QPs are used in sparse regression and convex relaxations. - Barrier penalties act as smooth approximations to hard constraints in physics-informed and fairness-aware models.</p>"},{"location":"convex/19b_optimization_constraints/#149-comparison-equality-vs-inequality-constrained-methods","title":"14.9 Comparison: Equality vs Inequality-Constrained Methods","text":"Aspect Equality Constraints Inequality Constraints Feasible set Affine manifold Convex region with boundary Algorithms Newton, projected Newton, KKT Barrier, interior-point, primal\u2013dual Feasibility handling Exact Maintained via barrier term Complementarity \\(A x = b\\) \\(\\lambda_i f_i(x) = 0\\) (or \\(= -1/t\\)) Feasible start Optional Required (strict) ML relevance Normalization, fairness, balance Nonnegativity, margins, sparsity, safety constraints"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often operates at massive scales \u2014 millions of variables, billions of data points, or constraints distributed across devices and networks. Classical Newton or interior-point algorithms, while theoretically elegant, become computationally impractical in these regimes.  </p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to solve large-scale convex problems efficiently. These ideas underpin the optimization engines behind most machine learning systems.</p>"},{"location":"convex/20_advanced/#151-motivation-structure-and-scale","title":"15.1 Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the difficulty lies not in theory but in computation.</p> <ul> <li>Memory limits: Storing the full Hessian or even the gradient can be infeasible.  </li> <li>Data size: Evaluating the objective over the full dataset is expensive.  </li> <li>Distributed data: Information may be spread across machines or devices.  </li> <li>Sparsity and separability: Many objectives decompose nicely into smaller components.</li> </ul> <p>Thus, the goal is to design algorithms that make incremental or local progress while exploiting the structure of the problem.</p> <p>Typical forms include:  where: - each \\(f_i(x)\\) represents a data-sample loss term, and - \\(R(x)\\) is a regularizer (possibly nonsmooth, such as \\(\\lambda\\|x\\|_1\\)).</p>"},{"location":"convex/20_advanced/#152-coordinate-descent","title":"15.2 Coordinate Descent","text":"<p>Coordinate descent updates a single variable (or a small block) at a time while holding others fixed.  </p>"},{"location":"convex/20_advanced/#algorithm","title":"Algorithm","text":"<p>Given \\(x^{(k)}\\), choose coordinate \\(i\\) and update:  </p> <p>This can be seen as projecting the gradient onto the coordinate directions. For separable problems, it is computationally much cheaper than full gradient updates.</p> <ul> <li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li> <li>For problems with separable structure \u2014 e.g. sums over features, or regularisers like \\(\\|x\\|_1 = \\sum_i |x_i|\\) \u2014 the coordinate update is extremely cheap.</li> <li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li> </ul> <p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>"},{"location":"convex/20_advanced/#convergence","title":"Convergence","text":"<p>If \\(f\\) is convex with Lipschitz-continuous partial derivatives, cyclic or randomized coordinate descent converges to the global optimum.</p>"},{"location":"convex/20_advanced/#ml-context","title":"ML Context","text":"<p>Coordinate descent is widely used in: - LASSO and Elastic Net regression (where updates are closed-form soft-thresholding), - logistic regression with \\(\\ell_1\\) penalty, - matrix factorization and dictionary learning.</p>"},{"location":"convex/20_advanced/#153-stochastic-gradient-and-variance-reduced-methods","title":"15.3 Stochastic Gradient and Variance-Reduced Methods","text":"<p>When the dataset is large, computing the full gradient  can be prohibitively expensive, since it requires evaluating all \\(N\\) samples at every iteration. Stochastic methods overcome this by using unbiased gradient estimates based on small random subsets (mini-batches) of the data.</p>"},{"location":"convex/20_advanced/#1531-stochastic-gradient-descent-sgd","title":"15.3.1 Stochastic Gradient Descent (SGD)","text":"<p>At each iteration, choose a random sample (or mini-batch) \\(\\mathcal{B}_k\\) and perform the update:</p> \\[ x_{k+1} = x_k - \\eta_k \\, \\widehat{\\nabla f}(x_k), $$ where $$ \\widehat{\\nabla f}(x_k) = \\frac{1}{|\\mathcal{B}_k|} \\sum_{i \\in \\mathcal{B}_k} \\nabla f_i(x_k) \\] <p>is a stochastic estimate of the true gradient, and \\(\\eta_k &gt; 0\\) is the step size (learning rate).</p>"},{"location":"convex/20_advanced/#interpretation","title":"Interpretation","text":"<ul> <li>SGD performs a noisy gradient step: it moves in approximately the right direction on average.</li> <li>The noise introduced by sampling allows exploration of the parameter space and helps escape shallow local minima in nonconvex problems.</li> <li>In convex settings, it trades accuracy for computational efficiency \u2014 each iteration is much cheaper, so we can afford many more of them.</li> </ul>"},{"location":"convex/20_advanced/#1532-step-size-and-averaging","title":"15.3.2 Step Size and Averaging","text":"<p>The step size \\(\\eta_k\\) controls the bias\u2013variance tradeoff: - If \\(\\eta_k\\) is too large \u2192 iterates oscillate due to stochastic noise. - If \\(\\eta_k\\) is too small \u2192 progress slows down.</p> <p>Common choices:  </p> <p>Two popular stabilization strategies:</p> <ol> <li> <p>Decay learning rate.</p> </li> <li> <p>Polyak\u2013Ruppert averaging:    Instead of returning the last iterate, return the running average        Averaging cancels gradient noise and ensures convergence to the optimal solution in expectation.</p> </li> <li> <p>Increasing mini-batch size:    As optimization proceeds, increasing \\(|\\mathcal{B}_k|\\) gradually reduces gradient variance while keeping updates efficient.</p> </li> </ol>"},{"location":"convex/20_advanced/#1533-convergence-properties","title":"15.3.3 Convergence Properties","text":"<p>For convex objectives: - \\(\\mathbb{E}[f(x_k)] - f^\\star = O(1/\\sqrt{k})\\) with diminishing \\(\\eta_k\\).</p> <p>For strongly convex \\(f\\), with \\(\\eta_k = O(1/k)\\): - \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</p> <p>These are optimal rates for stochastic first-order methods:  no unbiased stochastic optimizer using the same amount of data can asymptotically converge faster than SGD with Polyak averaging.</p>"},{"location":"convex/20_advanced/#1534-variance-reduction","title":"15.3.4 Variance Reduction","text":"<p>Although SGD is simple, the stochastic noise prevents it from reaching very high accuracy.  Variance-reduced methods (SVRG, SAGA, SARAH) correct this by mixing stochastic and full-gradient information.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <p>At outer iteration \\(s\\), compute a full gradient snapshot \\(\\nabla f(\\tilde{x})\\). Then, for inner iterations:  </p> <ul> <li>\\(v_k\\) is an unbiased estimate of \\(\\nabla f(x_k)\\) but with reduced variance.</li> <li>For strongly convex \\(f\\), SVRG and SAGA achieve linear convergence, bridging the gap between SGD and full gradient descent.</li> </ul> <p>Intuitively, these methods \u201canchor\u201d stochastic gradients around a periodically refreshed reference point, preventing the gradient noise from accumulating.</p>"},{"location":"convex/20_advanced/#1535-stochastic-second-order-and-momentum-methods","title":"15.3.5 Stochastic Second-Order and Momentum Methods","text":"<p>SGD can be further improved by incorporating curvature or momentum information.</p> <ol> <li> <p>Momentum / Nesterov acceleration:    Maintains an exponential moving average of past gradients:        Momentum accelerates convergence in smooth regions and damps oscillations in narrow valleys.</p> </li> <li> <p>Adaptive methods (Adam, RMSProp, Adagrad):    Use coordinate-wise scaling based on running averages of squared gradients to handle ill-conditioned curvature.</p> </li> <li> <p>Stochastic second-order methods:    Approximate curvature matrices (e.g., Fisher or Hessian) via stochastic estimates and maintain them with exponential decay:        Though theoretically limited by SGD\u2019s asymptotic rate, they often yield better pre-asymptotic performance \u2014 crucial in practical deep learning where only a few passes over the data are feasible.</p> </li> </ol>"},{"location":"convex/20_advanced/#1536-machine-learning-context-and-insights","title":"15.3.6 Machine Learning Context and Insights","text":"<ul> <li>Deep neural networks rely almost exclusively on SGD and its adaptive or momentum-based variants. The stochasticity helps generalization by acting as implicit regularization.</li> <li>Large-scale convex ML problems \u2014 logistic regression, SVMs, ridge regression \u2014 use SGD or variance-reduced methods (SVRG/SAGA) for scalability.</li> <li>The balance between variance reduction and computational cost defines practical performance.</li> </ul>"},{"location":"convex/20_advanced/#1537-summary","title":"15.3.7 Summary","text":"Method Key Idea Convergence Practical Use SGD Uses mini-batch gradients \\(O(1/\\sqrt{k})\\) Deep learning, online learning SGD + Polyak averaging Averaged iterates \\(O(1/k)\\) Theoretically optimal stochastic convergence SVRG / SAGA Variance-reduced updates Linear for strongly convex Convex ML, GLMs Momentum / Adam Smoothed gradient estimates Empirical acceleration Deep nets Stochastic 2nd-order Curvature tracking Better pre-asymptotic Large-batch training"},{"location":"convex/20_advanced/#154-proximal-and-composite-optimization","title":"15.4 Proximal and Composite Optimization","text":"<p>Many modern objectives combine a smooth loss and a nonsmooth regularizer:  where \\(g\\) is differentiable with Lipschitz gradient and \\(R\\) is convex but possibly nonsmooth.</p> <p>The proximal gradient method updates as:  where the proximal operator is:  </p>"},{"location":"convex/20_advanced/#intuition","title":"Intuition","text":"<ul> <li>The gradient step moves in a descent direction for \\(g\\).  </li> <li>The proximal step performs a local \u201cdenoising\u201d or shrinkage under \\(R\\) (e.g., soft-thresholding for \\(\\ell_1\\) norms).</li> </ul>"},{"location":"convex/20_advanced/#ml-context_1","title":"ML Context","text":"<p>Proximal methods underpin: - Sparse regression (LASSO, Elastic Net), - matrix completion and compressed sensing, - total-variation image denoising, - low-rank and structured regularization.</p>"},{"location":"convex/20_advanced/#155-alternating-direction-method-of-multipliers-admm","title":"15.5 Alternating Direction Method of Multipliers (ADMM)","text":"<p>When an objective separates into parts that depend on different variables, ADMM enables efficient distributed optimization.</p> <p>Consider:  </p>"},{"location":"convex/20_advanced/#augmented-lagrangian","title":"Augmented Lagrangian","text":"\\[ L_\\rho(x,z,y) = f(x) + g(z) + y^T(Ax + Bz - c) + \\frac{\\rho}{2}\\|A x + B z - c\\|^2. \\]"},{"location":"convex/20_advanced/#iterations","title":"Iterations","text":"<p>ADMM performs alternating updates:  </p>"},{"location":"convex/20_advanced/#interpretation_1","title":"Interpretation","text":"<p>Each step solves an easier subproblem involving only part of the variables, followed by a dual update to enforce consistency. ADMM thus merges ideas from dual ascent and penalty methods.</p>"},{"location":"convex/20_advanced/#convergence_1","title":"Convergence","text":"<p>For convex \\(f\\) and \\(g\\), ADMM converges to the global optimum. It is particularly effective when the subproblems are simple (e.g., proximal operators).</p>"},{"location":"convex/20_advanced/#ml-context_2","title":"ML Context","text":"<p>ADMM is a key tool for: - distributed LASSO and logistic regression, - matrix decomposition and factorization, - consensus optimization in federated learning, - distributed deep learning regularization.</p>"},{"location":"convex/20_advanced/#156-majorizationminimization-mm-and-em-algorithms","title":"15.6 Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The MM principle iteratively minimizes a surrogate function that upper-bounds the objective.</p> <p>Given a current point \\(x_k\\), construct a surrogate \\(g(x|x_k)\\) such that:  </p> <p>Then update:  </p> <p>Each iteration ensures \\(f(x_{k+1}) \\le f(x_k)\\).</p>"},{"location":"convex/20_advanced/#ml-context_3","title":"ML Context","text":"<ul> <li>The Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models.  </li> <li>IRLS (Iteratively Reweighted Least Squares) for logistic regression and \\(\\ell_p\\) regression follows the same idea.  </li> <li>MM methods guarantee descent even for complex, nonconvex objectives.</li> </ul>"},{"location":"convex/20_advanced/#157-distributed-and-parallel-optimization","title":"15.7 Distributed and Parallel Optimization","text":"<p>For large-scale convex problems distributed across multiple nodes, parallel methods are essential.</p>"},{"location":"convex/20_advanced/#synchronous-and-asynchronous-updates","title":"Synchronous and Asynchronous Updates","text":"<ul> <li>Synchronous: all workers compute updates and synchronize (used in federated averaging).  </li> <li>Asynchronous: updates proceed without waiting, improving throughput but increasing variance.</li> </ul>"},{"location":"convex/20_advanced/#consensus-optimization","title":"Consensus Optimization","text":"<p>In distributed convex optimization, one solves  which can be handled by ADMM or primal\u2013dual methods. Each machine optimizes its local copy \\(x_i\\), and the shared variable \\(z\\) enforces consensus.</p>"},{"location":"convex/20_advanced/#ml-context_4","title":"ML Context","text":"<ul> <li>Federated learning and parameter-server training frameworks (e.g., TensorFlow Distributed, PyTorch DDP) follow this model.  </li> <li>Decentralized convex optimization appears in sensor networks and multi-agent control.</li> </ul>"},{"location":"convex/20_advanced/#158-handling-structure-sparsity-and-low-rank","title":"15.8 Handling Structure: Sparsity and Low Rank","text":"<p>Many convex problems exhibit special structures that algorithms can exploit:</p> Structure Typical Regularizer Algorithmic Advantage Sparsity \\(\\ell_1\\) or group lasso Coordinate updates, proximal shrinkage Low rank nuclear norm \\(\\|X\\|_*\\) SVD-based proximal step Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed updates Graph structure total variation norm Local neighborhood computations Simplex or probability constraints entropy or KL penalty Mirror descent, projected methods <p>Exploiting such structure yields orders-of-magnitude speedups in both memory and computation.</p>"},{"location":"convex/20_advanced/#159-summary-and-practical-guidance","title":"15.9 Summary and Practical Guidance","text":"Method Gradient Access Scalability Parallelization Convexity Required Typical ML Uses Coordinate Descent Partial / coordinate High Easy Convex LASSO, sparse models SGD / SVRG / SAGA Stochastic Excellent Natural Convex / nonconvex Deep learning, logistic regression Proximal Gradient Full gradient + prox Moderate\u2013High Easy Convex Composite objectives ADMM Separable subproblems High Distributed Convex Consensus, large convex solvers MM / EM Surrogate-based Moderate Model-specific Convex / nonconvex Probabilistic models, IRLS Distributed / Federated Local gradients Very high Essential Convex / smooth Federated learning, large-scale convex optimization"},{"location":"convex/20_advanced/#1510-key-takeaways","title":"15.10 Key Takeaways","text":"<ul> <li>Large-scale convex optimization relies on exploiting structure, stochasticity, and separability.  </li> <li>Coordinate and proximal methods handle sparse and composite problems efficiently.  </li> <li>Stochastic and variance-reduced methods scale to massive data.  </li> <li>ADMM and distributed optimization enable multi-machine or federated settings.  </li> <li>MM and EM extend convex ideas to broader nonconvex inference tasks.</li> </ul>"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#161-regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"16.1 Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithmic pointers for 11.1:  </p> <ul> <li>Smooth+\u2113\u2082 (strongly convex) \u2192 Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li> <li>Mixed penalties (\u2113\u2081+\u2113\u2082) \u2192 Still convex; often handle like \u2113\u2081 case since smooth part dominates curvature.  </li> <li>Large-scale data \u2192 Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li> </ul> <p>Remarks:  Choose \\(\\lambda\\) via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (\\(n\\) large), coordinate or stochastic methods often outperform direct second-order methods.</p>"},{"location":"convex/21_models/#162-robust-regression-and-outlier-resistance","title":"16.2 Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#1621-least-absolute-deviations-l1-loss","title":"16.2.1 Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#1622-huber-loss","title":"16.2.2 Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints.     That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.     This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#1623-worst-case-robust-regression","title":"16.2.3 Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#163-maximum-likelihood-and-loss-design","title":"16.3 Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#164-structured-constraints-in-engineering-and-design","title":"16.4 Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers for 11.4:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#165-linear-and-conic-programming-the-canonical-models","title":"16.5 Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#166-risk-safety-margins-and-robust-design","title":"16.6 Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#167-cheat-sheet-if-your-problem-looks-like-this-use-that","title":"16.7 Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#167-matching-model-structure-to-algorithm-type","title":"16.7 Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes. Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#171-hierarchy-of-canonical-problems","title":"17.1 Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#172-linear-programming-lp","title":"17.2 Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#173-quadratic-programming-qp","title":"17.3 Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#174-quadratically-constrained-qp-qcqp","title":"17.4 Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#175-second-order-cone-programming-socp","title":"17.5 Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#176-semidefinite-programming-sdp","title":"17.6 Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#177-geometric-programming-gp","title":"17.7 Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#178-likelihood-based-convex-models-mle-and-glms","title":"17.8 Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#179-solver-selection-summary","title":"17.9 Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"deeplearning/1_mlp/","title":"1. Introduction to Deep Learning Optimization","text":""},{"location":"deeplearning/1_mlp/#an-introduction-to-neural-networks","title":"An Introduction to Neural Networks","text":""},{"location":"deeplearning/1_mlp/#1-neural-networks-as-computation-graphs","title":"1. Neural Networks as Computation Graphs","text":"<p>Modern neural networks are best understood as differentiable computation graphs.  They are not just layered algebraic systems but structured compositions of primitive mathematical operations.</p> <p>Each node in this graph corresponds to a function:</p> \\[z_i = f_i(x_1, \\dots, x_k)\\] <p>and the entire network defines a composite function:</p> \\[f_\\theta(x) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(x)\\] <p>where \\(\\theta = \\{W_i, b_i\\}\\) denotes all learnable parameters.</p>"},{"location":"deeplearning/1_mlp/#formal-structure","title":"Formal Structure","text":"<p>For a Multilayer Perceptron (MLP):</p> \\[h_0 = x, \\quad h_i = \\sigma(W_i h_{i-1} + b_i), \\quad i=1,\\dots,L-1, \\quad \\hat{y} = W_L h_{L-1} + b_L\\] <p>with: \\(W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}, \\quad b_i \\in \\mathbb{R}^{d_i}\\)</p> <p>Each layer is a small differentiable function. When we connect them, we form a composite map \u2014 the fundamental abstraction underlying autodiff, backprop, and learning.</p> <p>Key property: Because every node in the graph is differentiable, the entire function \\(f_\\theta(x)\\) is differentiable with respect to both input \\(x\\) and parameters \\(\\theta\\).</p> <p>Graphically, the network is a directed acyclic graph (DAG):</p> <ul> <li>Edges: carry tensor values.</li> <li>Nodes: represent differentiable functions.</li> <li>Forward pass: evaluates node outputs.</li> <li>Backward pass: propagates sensitivities (gradients) backward.</li> </ul> <p>This graph abstraction unifies all architectures \u2014 CNNs, RNNs, Transformers, Diffusion Models \u2014 as differentiable computation graphs.</p>"},{"location":"deeplearning/1_mlp/#2-gradients-jacobians-and-differentiation","title":"2. Gradients, Jacobians, and Differentiation","text":"<p>For any function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian matrix \\(J_f(x)\\) encodes local derivatives:</p> \\[[J_f(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\] <p>In neural networks, we often deal with a scalar loss function:</p> \\[L(\\theta) = \\ell(f_\\theta(x), y)\\] <p>and want: </p> \\[\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\] <p>However, computing full Jacobians is computationally infeasible \u2014 for a network with millions of parameters, explicit Jacobians would have trillions of entries. Instead, automatic differentiation (autodiff) computes vector\u2013Jacobian products efficiently.</p> <p>For scalar loss \\(L\\): \\(\\nabla_\\theta L = J_{f_\\theta}(x)^T \\nabla_{f_\\theta} L\\)</p> <p>where \\(J_{f_\\theta}(x)\\) is the Jacobian of the output w.r.t. parameters.</p> <p>This operation can be done efficiently in reverse-mode autodiff \u2014 the heart of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#3-forward-and-backward-passes","title":"3. Forward and Backward Passes","text":""},{"location":"deeplearning/1_mlp/#forward-pass","title":"Forward Pass","text":"<p>Given input \\(x\\) and parameters \\(\\theta\\):</p> <ol> <li>Compute layer outputs sequentially: \\(h_i = \\sigma(W_i h_{i-1} + b_i)\\)</li> <li>Compute loss \\(L = \\ell(f_\\theta(x), y)\\)</li> <li>Store intermediate activations \\(h_i\\) for reuse during backpropagation.</li> </ol> <p>This pass evaluates the function \\(L(\\theta)\\).</p>"},{"location":"deeplearning/1_mlp/#backward-pass","title":"Backward Pass","text":"<p>The backward pass applies the chain rule in reverse, computing derivatives of the loss with respect to each parameter:</p> <p>\\(\\frac{\\partial L}{\\partial \\theta_i} =  \\frac{\\partial L}{\\partial h_L} \\frac{\\partial h_L}{\\partial h_{L-1}} \\dots \\frac{\\partial h_{i+1}}{\\partial \\theta_i}\\)</p> <p>The chain rule guarantees that this derivative can be factored into local derivatives of each layer, which can be computed efficiently.</p> <p>Reverse-mode autodiff (backprop) algorithm:</p> <ol> <li>Initialize \\(\\bar{h}_L = \\frac{\\partial L}{\\partial h_L} = 1\\).</li> <li>For each layer \\(l = L, L-1, \\dots, 1\\):</li> <li>Compute local derivative \\(\\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Accumulate gradient: \\(\\bar{h}_{l-1} = \\bar{h}_l \\frac{\\partial h_l}{\\partial h_{l-1}}\\)</li> <li>Compute parameter gradients: \\(\\frac{\\partial L}{\\partial W_l} = \\bar{h}_l (h_{l-1})^T\\)</li> <li>Return all \\(\\nabla_\\theta L\\).</li> </ol> <p>This process requires the cached activations from the forward pass, which explains the memory cost of backpropagation.</p>"},{"location":"deeplearning/1_mlp/#4-chain-rule-backpropagation-and-automatic-differentiation","title":"4. Chain Rule, Backpropagation, and Automatic Differentiation","text":"<p>The chain rule underpins all gradient computation. For scalar functions:</p> <p>\\(\\frac{dL}{dx} = \\frac{dL}{dz} \\frac{dz}{dx}\\)</p> <p>and recursively for multivariate functions:</p> <p>\\(\\nabla_x L = J_{z}(x)^T \\nabla_z L\\)</p> <p>Autodiff implements this automatically, performing either:</p> <ul> <li>Forward-mode AD: propagates derivatives forward, efficient when #inputs \u226a #outputs.</li> <li>Reverse-mode AD: propagates derivatives backward, efficient when #outputs \u226a #inputs (our case).</li> </ul> <p>Reverse-mode AD \u2261 backpropagation.</p> <p>Computational Complexity: - Cost \u2248 2\u00d7 forward pass (one forward, one backward). - Memory \u2248 size of stored activations.</p> <p>Optimization viewpoint:   Autodiff converts the learning problem into an optimization problem over parameters:</p> <p>\\(\\min_\\theta L(\\theta)\\)</p> <p>where \\(L\\) is differentiable but nonconvex. Backprop provides the exact gradient needed by optimization algorithms. s</p>"},{"location":"deeplearning/1_mlp/#5-from-gradients-to-optimization","title":"5. From Gradients to Optimization","text":"<p>The Learning Problem - Training a neural network means solving:</p> <p>\\(\\min_\\theta \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} [\\,\\ell(f_\\theta(x), y)\\,]\\)</p> <p>Since the true data distribution \\(\\mathcal{D}\\) is unknown, we use empirical risk minimization (ERM):</p> <p>\\(\\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\ell(f_\\theta(x_i), y_i)\\)</p> <p>This is a high-dimensional, nonconvex optimization problem. The parameter space may have millions (or billions) of dimensions.Despite this, gradient-based methods \u2014 powered by backpropagation \u2014 reliably find good solutions.</p>"},{"location":"deeplearning/1_mlp/#first-order-optimization-algorithms","title":"First-Order Optimization Algorithms","text":"<p>All modern deep learning optimization relies on gradients:</p> <p>\\(\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\theta}\\)</p> <p>The basic rule: update parameters in the direction of negative gradient:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_t\\)</p> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"deeplearning/1_mlp/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>We use mini-batches instead of full data:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\frac{1}{|B_t|}\\sum_{i \\in B_t} \\ell(f_\\theta(x_i), y_i)\\)</p> <ul> <li>Cheap per-step computation.</li> <li>Introduces gradient noise, which helps escape shallow minima and saddle points.</li> </ul>"},{"location":"deeplearning/1_mlp/#momentum","title":"Momentum","text":"<p>Accelerates learning by accumulating a velocity vector:</p> <p>\\(v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L_t, \\quad \\theta_{t+1} = \\theta_t + v_{t+1}\\)</p> <p>Momentum smooths oscillations and stabilizes descent on curved loss surfaces.</p>"},{"location":"deeplearning/1_mlp/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Maintains exponentially weighted averages of gradients and squared gradients:</p> <p>\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla_\\theta L_t\\)</p> <p>\\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla_\\theta L_t)^2\\)</p> <p>Bias-corrected updates:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\\)</p> <p>Adam adapts the learning rate per-parameter, combining momentum with RMS normalization.</p>"},{"location":"deeplearning/1_mlp/#second-order-and-curvature-aware-methods","title":"Second-Order and Curvature-Aware Methods","text":"<p>While first-order methods use only gradients, second-order methods consider curvature (Hessian):</p> <p>\\(H = \\frac{\\partial^2 L}{\\partial \\theta^2}\\)</p> <p>Newton\u2019s update:</p> <p>\\(\\theta_{t+1} = \\theta_t - H^{-1}\\nabla_\\theta L\\)</p> <p>is theoretically optimal for quadratic loss but computationally infeasible for deep nets. Approximations like L-BFGS, K-FAC, and natural gradient descent use low-rank or structured approximations to curvature.</p>"},{"location":"deeplearning/1_mlp/#optimization-landscape-and-gradient-flow","title":"Optimization Landscape and Gradient Flow","text":"<p>Although neural network loss surfaces are highly nonconvex, they possess favorable geometry:</p> <ul> <li>Most critical points are saddle points, not local minima.</li> <li>Wide, flat minima generalize better (implicit regularization of SGD).</li> <li>Gradient noise helps explore valleys in high-dimensional space.</li> </ul> <p>Gradient flow (continuous limit of SGD):</p> <p>\\(\\frac{d\\theta(t)}{dt} = - \\nabla_\\theta L(\\theta(t))\\)</p> <p>describes a trajectory in parameter space governed by the vector field of gradients.</p> <p>The optimization algorithm defines the dynamics of this flow (e.g., momentum adds inertia).</p>"},{"location":"deeplearning/1_mlp/#6-what-mlps-cant-do","title":"6. What MLPs Can\u2019t Do?","text":""},{"location":"deeplearning/1_mlp/#a-multiplicative-interactions","title":"(a) Multiplicative Interactions","text":"<p>MLPs compute sums of weighted activations \u2014 inherently additive operations:</p> <p>\\(h = \\sigma(Wx + b)\\)</p> <p>They cannot naturally represent multiplicative relationships (like \\(x_1 x_2\\)) unless approximated via nonlinear stacking, which is inefficient.</p> <p>Architectures with multiplicative gates (LSTMs, Transformers) encode such interactions directly, improving optimization dynamics by linearizing multiplicative effects.</p>"},{"location":"deeplearning/1_mlp/#b-attention-and-dynamic-routing","title":"(b) Attention and Dynamic Routing","text":"<p>MLPs have static connectivity. Attention mechanisms compute data-dependent weights, enabling context-sensitive computation:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V\\)</p> <p>Optimization over attention parameters effectively learns a dynamic kernel, something MLPs cannot emulate efficiently.</p>"},{"location":"deeplearning/1_mlp/#c-metric-learning-and-inductive-bias","title":"(c) Metric Learning and Inductive Bias","text":"<p>MLPs lack structural priors about similarity or geometry. Optimization in unstructured parameter spaces can overfit and fail to generalize relational properties.</p> <p>Architectures like CNNs (translation equivariance), GNNs (permutation invariance), and Transformers (contextual attention) bake inductive biases into the computation graph, making optimization more efficient \u2014 the landscape becomes smoother and gradients more informative.</p>"},{"location":"deeplearning/1_mlp/#7-beyond-backprop-curvature-generalization-and-geometry","title":"7. Beyond Backprop: Curvature, Generalization, and Geometry","text":"<p>Advanced optimization in neural networks goes beyond plain gradient descent.</p>"},{"location":"deeplearning/1_mlp/#natural-gradient","title":"Natural Gradient","text":"<p>Instead of minimizing loss directly in parameter space, we minimize it in function space:</p> <p>\\(\\Delta \\theta = - \\eta F^{-1} \\nabla_\\theta L\\)</p> <p>where \\(F\\) is the Fisher information matrix:</p> <p>\\(F = \\mathbb{E}\\left[\\nabla_\\theta \\log p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x)^T\\right]\\)</p> <p>Natural gradients move along directions that respect the underlying information geometry of the model.</p>"},{"location":"deeplearning/1_mlp/#implicit-bias-of-gradient-descent","title":"Implicit Bias of Gradient Descent","text":"<p>Even in overparameterized models, gradient descent tends to find low-norm or flat minima that generalize better \u2014 a phenomenon not yet fully understood but deeply tied to the optimization path and noise structure of SGD.</p>"},{"location":"deeplearning/1_mlp/#optimization-as-inference","title":"Optimization as Inference","text":"<p>Many modern perspectives view training as approximate inference:</p> <p>\\(p(\\theta | D) \\propto e^{-L(\\theta)/T}\\)</p> <p>Gradient descent samples from this energy landscape as \\(T \\to 0\\); stochastic variants like SGD approximate Bayesian inference under certain limits.</p>"},{"location":"deeplearning/2_convnets/","title":"2. Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#chapter-2-convolutional-neural-networks-cnns","title":"Chapter 2: Convolutional Neural Networks (CNNs)","text":""},{"location":"deeplearning/2_convnets/#1-core-principles-locality-and-translation-invariance","title":"1. Core Principles: Locality and Translation Invariance","text":"<p>Before understanding convolutional networks, it\u2019s crucial to grasp why they exist \u2014 the structural priors they impose on data.</p>"},{"location":"deeplearning/2_convnets/#11-locality","title":"1.1 Locality","text":"<p>In many real-world signals (e.g., images, audio, text), nearby elements are highly correlated, while distant ones are less related. This is called the principle of locality.</p> <p>For example:</p> <ul> <li>Adjacent pixels in an image often belong to the same object or texture.</li> <li>Neighboring audio samples belong to the same phoneme.</li> <li>Nearby words in a sentence influence each other\u2019s meaning.</li> </ul> <p>MLPs treat every input dimension as independent, ignoring these spatial correlations. CNNs fix this by restricting connections: each neuron sees only a small, local region of the input, called its receptive field.</p> <p>Formally, for an input \\(x \\in \\mathbb{R}^{H \\times W}\\), a neuron at position \\((i,j)\\) in a CNN depends only on values in a small window \\(\\Omega(i,j)\\):  This allows CNNs to learn spatially local filters, like edge detectors or texture extractors.</p>"},{"location":"deeplearning/2_convnets/#12-translation-invariance","title":"1.2 Translation Invariance","text":"<p>Natural patterns are repeatable across locations \u2014 the same feature (e.g., an edge, a cat\u2019s ear) can appear anywhere in the image.</p> <p>An MLP would need to learn a separate detector for each position. CNNs overcome this through weight sharing: the same filter \\(W\\) is applied across all spatial positions.</p> <p>Mathematically:  </p> <p>This operation \u2014 convolution \u2014 ensures translation equivariance:  meaning if the input shifts by \\(\\Delta\\), the output shifts by the same amount. After pooling, this becomes translation invariance, i.e. the output doesn\u2019t change under small shifts.</p> <p>These two properties \u2014 locality and translation invariance \u2014 are the foundation of convolutional architectures.</p>"},{"location":"deeplearning/2_convnets/#2-motivation-why-convolutions","title":"2. Motivation: Why Convolutions?","text":"<p>While MLPs are universal function approximators, they are inefficient for data with spatial or local structure, such as images, audio, or videos. An MLP flattens input data into a 1D vector, destroying spatial relationships and requiring a huge number of parameters.</p> <p>Example: For a 256\u00d7256 RGB image (\u2248200K input features), even one hidden layer with 1,000 neurons requires: \\(\\((256 \\times 256 \\times 3) \\times 1000 = 196\\,\\text{million weights}.\\)\\)</p> <p>Moreover, the MLP learns redundant patterns (e.g., the same edge in multiple regions).</p> <p>Convolutional Neural Networks address this by exploiting spatial locality, translation invariance, and weight sharing.</p>"},{"location":"deeplearning/2_convnets/#3-the-convolution-operation","title":"3. The Convolution Operation","text":""},{"location":"deeplearning/2_convnets/#31-discrete-convolution","title":"3.1 Discrete Convolution","text":"<p>A convolution is a linear operation where a small filter (kernel) slides over an input and computes local weighted sums.</p> <p>For 2D inputs (e.g. images):</p> \\[ S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n) \\] <ul> <li>\\(I\\) \u2014 input (image)</li> <li>\\(K\\) \u2014 kernel (filter)</li> <li>\\(S\\) \u2014 output feature map</li> </ul> <p>Each filter detects a specific local pattern (edges, corners, textures).</p>"},{"location":"deeplearning/2_convnets/#32-convolution-in-neural-networks","title":"3.2 Convolution in Neural Networks","text":"<p>In CNNs, the convolution becomes a learnable operation:</p> \\[ h_{i,j,k} = \\sigma\\left( \\sum_{c=1}^{C_\\text{in}} (W_{k,c} * x_c)_{i,j} + b_k \\right) \\] <ul> <li>\\(x_c\\): input channel \\(c\\) (e.g. R, G, B)</li> <li>\\(W_{k,c}\\): kernel for output channel \\(k\\) and input channel \\(c\\)</li> <li>\\(b_k\\): bias for output channel \\(k\\)</li> <li>\\(\\sigma\\): nonlinearity (ReLU, etc.)</li> </ul> <p>This produces \\(C_\\text{out}\\) feature maps, each representing a learned spatial pattern.</p> <p>Weight sharing drastically reduces parameters: Each kernel might be \\(3 \\times 3\\) or \\(5 \\times 5\\) \u2014 independent of image size.</p>"},{"location":"deeplearning/2_convnets/#4-building-blocks-of-cnns","title":"4. Building Blocks of CNNs","text":""},{"location":"deeplearning/2_convnets/#41-convolutional-layer","title":"4.1 Convolutional Layer","text":"<p>Performs learnable filtering and produces feature maps.</p> <p>If input has shape \\((H, W, C_\\text{in})\\): - Kernel: \\((k_H, k_W, C_\\text{in}, C_\\text{out})\\) - Output: \\((H', W', C_\\text{out})\\)</p>"},{"location":"deeplearning/2_convnets/#42-nonlinear-activation","title":"4.2 Nonlinear Activation","text":"<p>After convolution, apply nonlinearity (commonly ReLU):  </p>"},{"location":"deeplearning/2_convnets/#43-pooling-layer","title":"4.3 Pooling Layer","text":"<p>Reduces spatial dimensions and increases invariance.</p> <ul> <li>Max pooling: selects the largest value in a patch.</li> <li>Average pooling: takes mean value.</li> </ul> <p>Formally:  </p> <p>Pooling introduces translation invariance \u2014 small shifts in input don\u2019t drastically change outputs.</p>"},{"location":"deeplearning/2_convnets/#44-flatten-fully-connected-layers","title":"4.4 Flatten + Fully Connected Layers","text":"<p>At the top of CNNs, feature maps are flattened and passed into MLP layers for classification or regression.</p>"},{"location":"deeplearning/2_convnets/#5-cnn-architecture-as-a-computation-graph","title":"5. CNN Architecture as a Computation Graph","text":"<p>A typical CNN defines a differentiable map:</p> \\[ f_\\theta(x) = W_L (\\text{Flatten}(h_{L-1})) + b_L \\] <p>where each layer \\(h_l\\) is defined recursively as:</p> \\[ h_l = \\sigma(\\text{Conv}(h_{l-1}; W_l) + b_l), \\quad l = 1, \\dots, L-1 \\] <p>Here, <code>Conv</code> represents the convolution operation.</p> <p>Each layer is spatially local, translation-equivariant, and differentiable \u2014 meaning backpropagation works seamlessly, just as in MLPs.</p>"},{"location":"deeplearning/2_convnets/#6-backpropagation-through-convolutions","title":"6. Backpropagation Through Convolutions","text":"<p>The gradient computation is a direct extension of the chain rule.</p>"},{"location":"deeplearning/2_convnets/#61-forward-pass","title":"6.1 Forward Pass","text":"<p>Compute:  </p>"},{"location":"deeplearning/2_convnets/#62-backward-pass","title":"6.2 Backward Pass","text":"<p>We need: - Gradient w.r.t. weights: \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) - Gradient w.r.t. input: \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\)</p> <p>The flipping arises from the mathematical property of convolution. Modern frameworks handle this efficiently via convolution transpose operations.</p> <p>Optimization viewpoint: Convolution layers remain linear in their weights \u2014 the nonlinearity and local parameter sharing define their expressive power.</p>"},{"location":"deeplearning/2_convnets/#7-inductive-biases-in-cnns","title":"7. Inductive Biases in CNNs","text":"<p>Convolutional architectures embed strong inductive biases:</p> Property Mathematical Mechanism Effect Local connectivity Small kernels (3\u00d73, 5\u00d75) Exploits spatial locality Weight sharing Same filter across space Reduces parameters drastically Translation equivariance Convolution operation Same pattern detection anywhere Pooling invariance Spatial downsampling Robust to small shifts/noise <p>These biases make CNNs data-efficient and easy to train \u2014 especially compared to fully connected networks on images.</p>"},{"location":"deeplearning/2_convnets/#8-optimization-and-training-dynamics","title":"8. Optimization and Training Dynamics","text":"<p>Training CNNs is similar to MLPs \u2014 we use gradient-based optimizers (SGD, Adam, etc.) \u2014 but with different landscape geometry:</p> <ul> <li>Parameter sharing makes the loss smoother (less overfitting).</li> <li>Batch normalization stabilizes gradient flow:    </li> <li>Regularization via dropout or weight decay improves generalization.</li> <li>Learning rate scheduling (cosine, step decay, warm restarts) accelerates convergence.</li> </ul> <p>Empirical finding: CNNs optimize faster and generalize better on spatial data due to structured parameterization.</p>"},{"location":"deeplearning/2_convnets/#9-cnn-architectures-through-history","title":"9. CNN Architectures Through History","text":"Model Year Key Innovation Depth Inductive Bias LeNet-5 1998 First practical CNN for handwritten digits 7 layers Local receptive fields AlexNet 2012 GPU training, ReLU, dropout 8 layers Data augmentation VGG 2014 Deep stacks of small 3\u00d73 filters 19 layers Uniform architecture ResNet 2015 Skip connections for gradient flow 152 layers Identity mapping DenseNet 2016 Feature reuse via dense connectivity 201 layers Multi-scale learning EfficientNet 2019 Compound scaling variable Optimized parameter scaling"},{"location":"deeplearning/2_convnets/#10-cnns-and-the-optimization-landscape","title":"10. CNNs and the Optimization Landscape","text":"<p>CNNs reshape the optimization problem compared to MLPs:</p> <ul> <li>Reduced parameter redundancy \u2192 fewer degenerate directions in gradient space.</li> <li>Structured weight sharing \u2192 smoother loss surface, fewer sharp minima.</li> <li>Skip connections (ResNets) introduce identity mappings, improving conditioning of the Jacobian and preventing vanishing gradients.</li> </ul> <p>In optimization terms, CNNs are better-conditioned models of the input\u2013output mapping.</p>"},{"location":"deeplearning/2_convnets/#11-beyond-classical-cnns","title":"11. Beyond Classical CNNs","text":"<p>Modern vision architectures have evolved: - Residual Networks (ResNets): skip connections allow training very deep models. - Depthwise Separable Convolutions (MobileNet, EfficientNet): reduce parameter count. - Dilated Convolutions: expand receptive field without extra parameters. - Convolution + Attention hybrids: combine locality (CNN) with global context (Transformers).</p>"},{"location":"deeplearning/2_convnets/#12-mathematical-summary","title":"12. Mathematical Summary","text":"Concept Formula Description Convolution \\((I * K)(i,j) = \\sum_m \\sum_n I(i+m,j+n) K(m,n)\\) Weighted local sum CNN Layer \\(h = \\sigma(W * x + b)\\) Convolution + nonlinearity Pooling \\(y_{i,j} = \\max_{(m,n)\\in \\Omega(i,j)} h_{m,n}\\) Downsampling Gradient wrt weights \\(\\frac{\\partial L}{\\partial W} = x * \\frac{\\partial L}{\\partial y}\\) Backprop step Gradient wrt input \\(\\frac{\\partial L}{\\partial x} = \\text{flip}(W) * \\frac{\\partial L}{\\partial y}\\) Sensitivity propagation"},{"location":"deeplearning/2_convnets/#13-intuitive-summary","title":"13. Intuitive Summary","text":"<p>Convolutional networks are: - Local \u2192 they process neighborhoods of data. - Hierarchical \u2192 deeper layers build on lower-level features. - Translation-equivariant \u2192 same pattern anywhere is treated the same. - Efficient \u2192 far fewer parameters than MLPs.</p> <p>They form the backbone of modern computer vision, speech recognition, and even some transformer hybrids (ConvNeXt, ViT hybrids).</p>"},{"location":"deeplearning/3_sequence_data/","title":"3. Sequence Data and Recurrent Neural Networks (RNNs)","text":""},{"location":"deeplearning/3_sequence_data/#chapter-3-modeling-sequence-data-in-deep-learning","title":"Chapter 3: Modeling Sequence Data in Deep Learning","text":"<p>In machine learning, a sequence is an ordered list of elements (e.g. words, time-series measurements) where the order of elements carries meaning. Formally, a sequence of length \\(T\\) can be written as \\((x_1,x_2,\\dots,x_T)\\), where each element \\(x_t\\) is indexed by its position in the sequence. Elements can repeat (e.g. the word \u201cthe\u201d may appear multiple times), and different sequences may have different lengths. Thus sequence data is inherently variable-length and order-dependent.</p> <p>Sequences are collection of elements where:</p> <ul> <li>Elements can be repeated.</li> <li>Order matters.</li> <li>Of variable length.</li> </ul>"},{"location":"deeplearning/3_sequence_data/#limitations-of-traditional-supervised-models","title":"Limitations of Traditional Supervised Models:","text":"<p>Traditional supervised models (e.g. fixed-size feedforward neural networks or classifiers) expect inputs of a fixed dimension and have no built-in notion of order or memory. In practice, applying a standard feedforward net to sequence data \u2013 by, say, collapsing the sequence into a fixed-size feature vector \u2013 ignores the important temporal or sequential structure. As one summary notes, \u201cfeedforward neural networks are severely limited when it comes to sequential data\u201d. Indeed, trying to predict a time-series or next word in a sentence by a fixed snapshot yields poor results. The key missing capability in traditional networks is memory of the past: they cannot readily model how earlier parts of the sequence influence later outputs. </p> <p>Concretely, most classifiers assume each input example is independent and fixed-size. A sentence of variable length or a time-series with long-term correlations violates this assumption. Thus, classical models fail because they have no mechanism to store or process long-term context: they either throw away order information or arbitrarily truncate sequences. Feedforward networks also do not share parameters over time, so each time-step would have its own weights (infeasible for long sequences).</p>"},{"location":"deeplearning/3_sequence_data/#the-simplest-assumption-independent-words-bag-of-words","title":"The Simplest Assumption: Independent Words (Bag-of-Words)","text":"<p>A na\u00efve approach to sequence (especially text) is to assume all elements are independent. In language, this is like a bag-of-words model (or unigram model) that ignores word order. In a bag-of-words representation, one simply counts or models each word\u2019s occurrence, treating all words as \u201cindependent features.\u201d This ignores sequence structure: \u201cthe order of words in the original documents is irrelevant\u201d. Such a model can still do document classification by word frequency, but it cannot predict the next word or capture meaning that depends on word order. Critically, bag-of-words assumes word occurrences are uncorrelated: \u201cbag-of-words assumes words are independent of one another\u201d. In reality, words co-occur in context (\u201cpeanut butter\u201d versus \u201cpeanut giraffe\u201d) \u2013 bag-of-words misses all such dependencies. Thus the independent-words assumption breaks down for sequence modeling, motivating models that explicitly use ordering and context.</p>"},{"location":"deeplearning/3_sequence_data/#n-gram-models-and-fixed-context-assumptions","title":"N-gram Models and Fixed-Context Assumptions","text":"<p>To go beyond complete independence, one can incorporate local context by using \\(n\\)-gram models. An \\(n\\)-gram model makes the (Markov) assumption that the probability of each element depends only on the previous \\(n-1\\) elements. For language, a bigram model (2-gram) assumes \\(P(w_t\\mid w_{t-1})\\), a trigram (3-gram) uses \\(P(w_t\\mid w_{t-2},w_{t-1})\\), etc. In general, the chain rule with an \\(N\\)-gram approximation is</p> \\[ P(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{t-N+1}, \\ldots, x_{t-1}) \\, . \\] <p>This preserves some order information: the window of the last \\(N-1\\) items is used to predict the next. However, \\(n\\)-gram models have well-known downsides:</p> <ul> <li> <p>Limited context length: They cannot capture dependencies beyond the fixed window. As noted in the literature, language \u201ccannot reason about context beyond the immediate \\(n\\)-gram window\u201d, and dependencies span entire sentences or documents. For example, a 3-gram model cannot connect a subject at the start of a sentence to its verb at the end if they are more than two words apart. Thus any longer-range dependency is missed by an \\(n\\)-gram.</p> </li> <li> <p>Data sparsity and scalability: The number of possible \\(n\\)-grams grows exponentially with vocabulary size \\(V\\). For a vocabulary of size \\(V\\), there are \\(V^N\\) possible \\(N\\)-grams. Jurafsky &amp; Martin observe that even for Shakespeare\u2019s corpus (\\(V\\approx 29{,}066\\)), there are \\(V^2\\approx8.4\\times 10^8\\) possible bigrams and \\(V^4\\approx 7\\times 10^{17}\\) possible 4-grams. Most of these never occur, so the resulting probability tables are extremely sparse. Training requires huge corpora to observe enough \\(n\\)-gram counts, and storing these tables is impractical for large \\(N\\) or \\(V\\). In practice, language models become \u201cridiculously sparse\u201d and unwieldy.</p> </li> <li> <p>No parametrization (non-differentiable): Traditional \\(n\\)-gram models are simply tables of counts with smoothing. They are not learned via gradient descent, so integrating them into larger neural pipelines (or backpropagating through them) is not straightforward. They lack nonlinearity and share no features across contexts.</p> </li> </ul> <p>In summary, while \\(n\\)-grams preserve local order up to length \\(N\\), they suffer from fixed-window limitations and massive tables, motivating more compact, learnable alternatives.</p>"},{"location":"deeplearning/3_sequence_data/#learnable-context-models-vectorization-and-neural-nets","title":"Learnable Context Models: Vectorization and Neural Nets","text":"<p>Modern sequence models address these issues by representing context with vectors and training parametric models. Key features of a learnable sequential model include:</p> <ul> <li> <p>Vector representation (embedding) of words and context: Each element (e.g. a word) is mapped to a continuous vector. Context (the recent history) can be summarized by combining or encoding these vectors into a fixed-size context vector. This preserves order by using the positions of the context vectors in the encoding.</p> </li> <li> <p>Order sensitivity: Unlike bag-of-words, the model output depends on the order of context elements. For example, we might concatenate or otherwise encode a sequence of word embeddings, ensuring different sequences yield different context vectors.</p> </li> <li> <p>Variable-length compatibility: The model should handle inputs of differing lengths. For instance, recurrent or attention models can process a variable number of inputs sequentially. Context-vectors built from the sequence (such as by a recurrent state) grow as needed. As noted, context-vector methods can \u201coperate in variable length of sequences\u201d.</p> </li> <li> <p>Differentiability: The mapping from context vector to next-word probability should be a differentiable function (e.g. a neural network) so we can train by gradient descent. This requires using continuous, learnable transformations (matrices, nonlinearities) instead of fixed count tables.</p> </li> <li> <p>Nonlinearity: Neural networks allow complex (nonlinear) interactions among inputs. A simple linear model on concatenated embeddings might be too weak, so one often uses at least one hidden layer with a nonlinear activation (e.g. tanh, ReLU).</p> </li> </ul> <p>For example, one could take the last few words, map each to an embedding \\(\\mathbf{x}{t-N+1},\\dots,\\mathbf{x}{t-1}\\), concatenate them into one large vector, and feed it into a multilayer perceptron (MLP) to predict the next word\u2019s probability. This would be order-sensitive and differentiable. However, it still fixes the context window size (\\(N-1\\)) and uses a separate weight for each position, so it\u2019s not efficient or variable-length. </p> <p>A more flexible approach is to encode arbitrary prefixes of the sequence into a single context (memory) vector using a recurrent or recursive process. One introduces a context vector \\(\\mathbf{h}_t\\) that evolves as the sequence is read. Such a context-vector \u201cacts as memory\u201d summarizing the past. A context-vector model has crucial advantages: it preserves order, handles variable-length inputs, and is fully trainable (differentiable). In short, vectorized context models can \u201clearn\u201d how much each part of the past matters, via backpropagation, while maintaining the sequence structure.</p>"},{"location":"deeplearning/3_sequence_data/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<p>These considerations lead naturally to Recurrent Neural Networks (RNNs) \u2013 models specifically designed for sequences. An RNN processes one element at a time, maintaining a hidden state (context vector) that is updated recurrently. At each time step \\(t\\), the RNN takes the current input \\(\\mathbf{x}t\\) and the previous hidden state \\(\\mathbf{h}{t-1}\\) and computes a new hidden state \\(\\mathbf{h}_t\\). The simplest RNN update is:</p> \\[ h_t = \\phi(W_h h_{t-1} + W_x x_t + b) \\, . \\] <p>where \\(\\phi\\) is a nonlinear activation (often \\(\\tanh\\)) and \\(W_h,W_x\\) are weight matrices. The same weight matrices \\(W_h,W_x\\) are reused at every time step (this is parameter sharing), which gives the RNN the ability to handle sequences of any length. As noted, this weight sharing means the model uses constant parameters across time.</p> <p>Intuitively, the RNN\u2019s hidden state \\(\\mathbf{h}_t\\) \u201cremembers\u201d the information from all prior inputs up to time \\(t\\). The final hidden state (or the hidden state at each step) can then be fed to an output layer to make predictions. Typically, we compute an output distribution over the next element via a softmax layer:</p> \\[ y_t = \\mathrm{softmax}(W_y h_t + b_y) \\, . \\] <p>so that \\(P(x_{t+1}=w \\mid \\mathbf{h}_t)\\) is given by the corresponding component of \\(\\mathbf{y}_t\\). In language modeling, for instance, \\(y_t\\) gives a probability for each word in the vocabulary. As described in practice, \u201cRNNs predict the output from the last hidden state along with output parameter \\(W_y\\); a softmax function to ensure the probability over all possible words\u201d. </p> <p>In summary, RNNs explicitly model order and context via their hidden state updates and shared parameters. They can be seen as a recurrent generalization of feedforward networks: an \u201cMLP with shared weights across time.\u201d At time \\(t\\), the RNN effectively takes the previous state and new input and feeds them through a nonlinear layer to compute the new state. Because information flows from each state to the next, the RNN can, in principle, capture long-range dependencies: any input can influence all future hidden states.</p>"},{"location":"deeplearning/3_sequence_data/#unrolling-and-backpropagation-through-time-bptt","title":"Unrolling and Backpropagation Through Time (BPTT)","text":"<p>Training an RNN is done by backpropagation through time. Conceptually, we unfold or unroll the RNN across \\(T\\) time steps, creating a deep feedforward network of depth \\(T\\) (each layer corresponds to one time step) with tied weights. One then applies standard backpropagation on this unfolded network. Formally, the total loss (e.g. sum of cross-entropies at each step) depends on the sequence of outputs, and gradients are computed by propagating errors backward through the unfolded time dimension. As one overview explains, \u201cthe network needs to be expanded, or unfolded, so that the parameters could be differentiated ... \u2013 hence backpropagation through time (BPTT)\u201d. In practice, each weight matrix \\(W\\) receives gradient contributions from each time step, effectively summing gradients as they propagate back. BPTT thus accounts for how current errors depend on all previous inputs through the recurrent hidden state. Because parameters are shared across time, the gradient at each step flows through multiple copies of the layer. BPTT differs from ordinary backpropagation only in that errors are summed at each time step due to weight sharing. Concretely, if \\(L = -\\sum_t \\log P(x_t\\mid \\mathbf{h}_{t-1})\\) is the loss, then for each \\(W\\) we compute</p> \\[ \\frac{\\partial L}{\\partial W} = \\sum_{t} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} \\, . \\] <p>taking into account the influence of \\(W\\) at every time step. In implementation, we typically use truncated BPTT (backprop through a limited number of steps) for efficiency on long sequences. But in principle, gradients propagate through all time steps, linking distant inputs to distant outputs.</p>"},{"location":"deeplearning/3_sequence_data/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>A critical challenge in training RNNs is that the repeated nonlinear transformations can cause gradients to vanish or explode during BPTT. Mathematically, the derivative \\(\\partial \\mathbf{h}t/\\partial \\mathbf{h}{t-1}\\) involves the Jacobian of the activation and the recurrent weights. Over many steps, the gradient involves a product of many such Jacobians. Just as multiplying many numbers less than 1 quickly goes to zero, multiplying many matrices with spectral radius \\(&lt;1\\) causes the gradients to shrink exponentially (vanishing), while if the spectral radius is \\(&gt;1\\) they blow up (exploding). The exploding gradient problem arises when the norm of the gradient grows exponentially (due to eigenvalues \\(&gt;1\\)), whereas the vanishing gradient problem occurs when long-term components of the gradient go \u201cexponentially fast to norm 0\u201d. Formally, for a linearized RNN one can show that if the largest eigenvalue \\(\\lambda_{\\max}\\) of the recurrent weight matrix satisfies \\(|\\lambda_{\\max}|&lt;1\\), long-term gradients vanish as \\(t\\to\\infty\\), and if \\(|\\lambda_{\\max}|&gt;1\\) they explode. </p> <p>Vanishing gradients mean that inputs from the distant past have almost no effect on the gradient of the loss, so the model learns only short-term dependencies. Exploding gradients make training unstable (weights take huge jumps). Both phenomena are well-documented: \u201cwhen long term components go to zero, the model cannot learn correlation between distant events.\u201d In practice, it is common to observe gradients either shrinking toward zero over time or blowing up and causing numerical issues in RNNs, especially with long sequences.</p>"},{"location":"deeplearning/3_sequence_data/#gated-architectures-lstm-and-gru","title":"Gated Architectures: LSTM and GRU","text":"<p>To mitigate the vanishing gradient, gated RNN architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These architectures incorporate learnable \u201cgates\u201d that control the flow of information and create paths for gradients to propagate more easily. Long Short-Term Memory (LSTM): An LSTM cell augments the basic RNN with a cell state \\(\\mathbf{C}_t\\) and three gates: input (\\(\\mathbf{i}_t\\)), forget (\\(\\mathbf{f}_t\\)), and output (\\(\\mathbf{o}t\\)) gates. Each gate is a sigmoid unit that decides how much information to let through. Formally, at time \\(t\\) with input \\(\\mathbf{x}t\\) and previous hidden \\(\\mathbf{h}{t-1}\\) and cell \\(\\mathbf{C}{t-1}\\), the gates and cell update are given by (all operations are elementwise):</p> <p>\u200b </p> <p>The new cell state \\(\\mathbf{C}_t\\) is then updated by combining the old state and the candidate:</p> \\[ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\, . \\] <p>where \\(\\odot\\) denotes elementwise multiplication. Finally, the hidden state (output of the LSTM) is</p> \\[ h_t = o_t \\odot \\tanh(C_t) \\, \\] <p>The intuition is that the forget gate \\(\\mathbf{f_t}\\) can reset or retain the old memory \\(\\mathbf{C}_{t-1}\\), the input gate \\(\\mathbf{i}_t\\) controls how much new information \\(\\tilde{\\mathbf{C}}_t\\) to write, and the output gate \\(\\mathbf{o}_t\\) controls how much of the cell state to expose as \\(\\mathbf{h}_t\\). By design, if the forget gate is near 1 and input gate near 0, the cell state is simply carried forward unchanged; gradients can flow through this constant path, avoiding vanishing. In practice, LSTMs \u201calleviate the vanishing gradient problem,\u201d making it easier to train on long sequences. The gating architecture enables the network to learn to keep or discard information over many time steps. </p> <p>In practice, using LSTM or GRU units yields much better performance on sequence tasks like language modeling or translation than vanilla RNNs.</p>"},{"location":"deeplearning/3_sequence_data/#optimization-challenges-and-solutions","title":"Optimization Challenges and Solutions","text":"<p>Even with gating, training RNNs can be tricky. Besides architectural fixes, optimization techniques are crucial:</p> <ul> <li> <p>Gradient clipping: To handle exploding gradients, one common technique is gradient clipping. Before updating parameters, one clips the norm of the gradient vector to some threshold (rescaling if too large). This prevents any single update from blowing up. As Pascanu et al. note, clipping \u201csolves the exploding gradients problem\u201d by limiting gradient norm. Clipping was key to many RNN successes (e.g. in language modeling), and it is standard practice in modern frameworks.</p> </li> <li> <p>Orthogonal (or careful) initialization: Choosing a good initial recurrent weight matrix can help. Initializing \\(W_h\\) as an (scaled) orthogonal matrix ensures its eigenvalues have magnitude 1, which prevents immediate vanishing/exploding. In fact, orthogonal matrices preserve the norm of vectors, so repeated multiplications neither decay nor explode. As one tutorial explains, \u201cOrthogonal initialization is a simple yet relatively effective way of combating exploding and vanishing gradients,\u201d ensuring stable gradient propagation. In practice, some implementations initialize \\(W_h\\) to random orthogonal (or unitary) matrices to encourage long memory.</p> </li> <li> <p>Layer normalization or gating enhancements: Techniques like layer normalization inside LSTM cells, or using newer architectures (e.g. LayerNorm-LSTM, transformer-like attention), also alleviate training difficulties.</p> </li> <li> <p>Regularization: Some works add penalties to encourage \\(W_h\\) to have a controlled spectral radius, or use techniques like weight noise or dropout to stabilize training.</p> </li> </ul> <p>In summary, sequence modeling requires architectures and training methods that explicitly handle order, context, and long-range information. Traditional models fail because they lack memory and flexibility. N-gram models give a glimpse of sequential structure but cannot scale or generalize. Recurrent models \u2013 especially gated RNNs \u2013 provide a powerful framework: mathematically, they define hidden states \\(\\mathbf{h}_t\\) updated by \\(\\mathbf{h}t = f(\\mathbf{h}{t-1},\\mathbf{x}_t)\\) with shared weights, and training via BPTT. Gating (LSTM/GRU) adds control mechanisms that preserve gradients and selective memory. With appropriate initialization, clipping, and optimization, these RNN-based models form the foundation of modern sequence learning. </p>"},{"location":"deeplearning/4_nlp/","title":"4. Natural Language Processing (NLP) with Deep Learning","text":""},{"location":"deeplearning/4_nlp/#deep-learning-for-natural-language-processing","title":"Deep Learning for Natural Language Processing","text":"<ul> <li>Natural language is context-dependent, compositional, and ambiguous.</li> <li>Deep neural networks (DNNs) handle parallel, distributed, and interactive computation \u2014 ideal for modeling contextual relationships.</li> <li>Early symbolic NLP struggled with discrete word tokens and rigid grammar rules; deep models learn continuous representations that encode meaning and similarity.</li> </ul>"},{"location":"deeplearning/4_nlp/#key-challenges-of-language","title":"Key Challenges of Language","text":"<p>Human language presents a unique set of challenges for computational models. Unlike artificial symbol systems, linguistic meaning is contextual, compositional, and dynamic, requiring models to infer relationships that go far beyond surface form.</p> <ul> <li> <p>Words are not discrete symbols.   The same word can have several related senses depending on context \u2014 for example: <code>face\u2081</code> (human face), <code>face\u2082</code> (clock face), <code>face\u2083</code> (to confront), and <code>face\u2084</code> (a person or presence).   Treating these as independent dictionary entries loses the shared semantic structure between them.   A more effective representation encodes meaning as distributed patterns in a continuous vector space, where related senses occupy nearby regions.</p> </li> <li> <p>Need for distributed representations.   Because meanings overlap and interact, we represent words not as atomic tokens but as vectors of features (syntactic, semantic, pragmatic).   This allows similarity, analogy, and composition to emerge geometrically \u2014 for instance, <code>king - man + woman \u2248 queen</code>.</p> </li> <li> <p>Disambiguation depends on context.   The meaning of a word or phrase is determined by its linguistic surroundings.   For example, in \u201cThe man who ate the pepper sneezed,\u201d the subject of sneezed is determined by a non-adjacent clause (the man), demonstrating how interpretation depends on sentence structure and longer-range dependencies.</p> </li> <li> <p>Non-local dependencies.   Natural language contains relationships between words that may be far apart in sequence.   Classical RNNs capture these dependencies only through sequential recurrence, which limits parallel computation and struggles with long-range information.   Transformers, through self-attention, handle these dependencies efficiently and in parallel by allowing each token to directly attend to every other token in the sequence.</p> </li> <li> <p>Compositionality.   The meaning of larger expressions arises from the meanings of their parts and how they are combined.   However, this combination is not purely linear.   For example, <code>carnivorous plant</code> is not simply the sum of carnivore and plant \u2014 its interpretation depends on how the features interact (a plant that eats insects).   Deep neural models capture this by learning nonlinear composition functions that reflect semantic interactions rather than mere addition.</p> </li> </ul> <p>In summary, natural language understanding requires models that can represent overlapping meanings, integrate long-range contextual information, and compose new meanings dynamically. Transformers achieve this by combining distributed representations with global attention mechanisms, providing a unified solution to these fundamental linguistic challenges.</p>"},{"location":"deeplearning/4_nlp/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Sequence models (RNNs, LSTMs) process tokens sequentially \u2014 limiting parallelism and long-range context.</li> <li>Transformers replace recurrence with self-attention, allowing the model to relate all words to all others simultaneously.</li> </ul>"},{"location":"deeplearning/4_nlp/#core-mechanism-self-attention","title":"Core Mechanism: Self-Attention","text":"<p>Given token embeddings :</p> \\[ q_i = e_i W^Q, \\quad k_i = e_i W^K, \\quad v_i = e_i W^V \\] <p>Attention weights:</p> \\[ \\alpha_{ij} = \\mathrm{softmax}_j \\left( \\frac{q_i k_j^\\top}{\\sqrt{d}} \\right) \\] <p>Output:</p> \\[ z_i = \\sum_j \\alpha_{ij} v_j \\] <p>Each token\u2019s new representation  is a contextual blend of all others. Captures semantic and syntactic relations without explicit recurrence.</p>"},{"location":"deeplearning/4_nlp/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Use multiple projections \\((W^Q_h, W^K_h, W^V_h)\\) \u2192 multiple \u201cheads.\u201d Each head focuses on different relations (e.g. subject\u2013verb, modifier\u2013noun). Outputs are concatenated and projected back to dimension \\(d\\):</p> \\[ \\text{MHA}(E) = [Z_1; Z_2; \\dots; Z_H] W^O \\]"},{"location":"deeplearning/4_nlp/#position-encoding","title":"Position Encoding","text":"<p>Since attention is permutation-invariant, Transformers add position information:</p> \\[ \\text{PE}_{(pos,2i)} = \\sin(pos / 10000^{2i/d}), \\quad \\text{PE}_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d}) \\] <p>\u2192 These sinusoidal signals are added to embeddings to encode word order.</p>"},{"location":"deeplearning/4_nlp/#full-transformer-block","title":"Full Transformer Block","text":"<pre><code>Input\n  \u2193\nMulti-Head Self-Attention\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nFeedforward Network (ReLU)\n  \u2193\n+ Skip Connection\n  \u2193\nLayer Normalization\n  \u2193\nOutput\n</code></pre> <p>Skip connections enable gradient flow and top-down influence. Stacking \\(N\\) blocks yields hierarchical contextualization of meaning.</p>"},{"location":"deeplearning/4_nlp/#intuition","title":"Intuition","text":"<ul> <li>Self-attention handles non-local relations.</li> <li>Multi-head captures multiple semantic dimensions simultaneously.</li> <li>Stacked layers build abstraction \u2014 from word-level to phrase- and discourse-level features.</li> </ul>"},{"location":"deeplearning/4_nlp/#unsupervised-learning-and-bert","title":"Unsupervised Learning and BERT","text":""},{"location":"deeplearning/4_nlp/#the-need-for-contextualized-representations","title":"The Need for Contextualized Representations","text":"<ul> <li>Word embeddings like Word2Vec are static: one vector per word.</li> <li>Language understanding requires contextual embeddings: \u201cbank\u201d (river vs. finance).</li> <li>Transformers enable bidirectional context \u2014 understanding a word from both sides.</li> </ul>"},{"location":"deeplearning/4_nlp/#bert-pretraining-objectives","title":"BERT Pretraining Objectives","text":"<ol> <li>Masked Language Modeling (MLM) Randomly mask 15% of tokens, predict them:</li> </ol> \\[ \\text{Loss}_{MLM} = - \\sum_{i \\in M} \\log P(w_i | \\text{context}) \\] <p>Encourages bidirectional encoding of meaning.</p> <ol> <li>Next Sentence Prediction (NSP) Model predicts if sentence B follows sentence A. Builds discourse-level coherence and world knowledge.</li> </ol>"},{"location":"deeplearning/4_nlp/#architecture","title":"Architecture","text":"<ul> <li>Deep bidirectional Transformer encoder.</li> <li>Uses special tokens:</li> <li><code>[CLS]</code> \u2013 sentence-level classification embedding</li> <li><code>[SEP]</code> \u2013 separates segments</li> <li>Pretrained on massive text (e.g. Wikipedia, BooksCorpus).</li> <li>Fine-tuned for downstream tasks (QA, sentiment, NER, etc.) by adding a simple classifier.</li> </ul>"},{"location":"deeplearning/4_nlp/#significance","title":"Significance","text":"<p>BERT shows self-supervised pretraining \u2192 transfer learning pipeline:</p> <pre><code>Pretrain (unsupervised)\n   \u2193\nFine-tune (supervised)\n   \u2193\nTask-specific adaptation\n</code></pre> <p>Achieves state-of-the-art on multiple NLP benchmarks with minimal labeled data. Learns semantic similarity, coreference, and discourse relations implicitly.</p>"},{"location":"deeplearning/4_nlp/#grounded-and-embodied-language-learning","title":"Grounded and Embodied Language Learning","text":""},{"location":"deeplearning/4_nlp/#motivation","title":"Motivation","text":"<ul> <li>Language understanding ultimately involves relating words to the world.</li> <li>Humans learn language in context \u2014 perception, action, and social interaction.</li> <li>Grounded learning aims to give agents multimodal grounding (vision, action, language).</li> </ul>"},{"location":"deeplearning/4_nlp/#grounded-agents","title":"Grounded Agents","text":"<ul> <li>Combine perceptual input (vision), motor control (actions), and linguistic input/output.</li> <li>Train via predictive modeling \u2014 anticipate sensory outcomes from language-conditioned actions.</li> <li>Enables semantic grounding: linking word \u201cred\u201d to visual color, \u201cpick up\u201d to motor command.</li> </ul>"},{"location":"deeplearning/4_nlp/#predictive-and-self-supervised-paradigms","title":"Predictive and Self-Supervised Paradigms","text":"<p>Agents learn representations by predicting future sensory or linguistic states:</p> \\[ \\min_\\theta \\mathbb{E} [ \\| f_\\theta(s_t, a_t) - s_{t+1} \\|^2 ] \\] <p>\u2192 Connects to world models and predictive coding principles in neuroscience. The agent\u2019s internal model encodes both linguistic meaning and causal structure of the environment.</p>"},{"location":"deeplearning/4_nlp/#insights-from-deepmind-work","title":"Insights from DeepMind Work","text":"<ul> <li>Embodied agents trained in simulated environments exhibit:</li> <li>Systematic generalization (e.g., learning \u201cpick up red object\u201d \u2192 generalize to unseen colors).</li> <li>Question answering and instruction following grounded in perception.</li> <li>Transfer from text to embodied tasks, using pretrained linguistic encoders (like BERT) as initialization.</li> </ul>"},{"location":"deeplearning/4_nlp/#conceptual-shift","title":"Conceptual Shift","text":"<p>From pipeline \u2192 integrated model:</p> Classic Pipeline Embodied / Interactive Model Letters \u2192 Words \u2192 Syntax \u2192 Meaning \u2192 Action Multimodal loops: Perception \u2194 Action \u2194 Language \u2194 Prediction"},{"location":"deeplearning/4_nlp/#conceptual-map-from-representation-to-understanding","title":"Conceptual Map: From Representation to Understanding","text":"<pre><code>Word Input\n   \u2193\nDistributed Representations (embedding)\n   \u2193\nSelf-Attention Mechanism\n   \u2193\nMulti-Head Parallel Processing\n   \u2193\nHierarchical Transformer Layers\n   \u2193\nContextualized Embeddings (BERT)\n   \u2193\nTransfer Learning to Tasks\n   \u2193\nEmbodied Agents (Grounded Semantics)\n   \u2193\nLanguage Understanding as Prediction + Interaction\n</code></pre>"},{"location":"deeplearning/4_nlp/#key-transitions","title":"Key Transitions","text":"<p>Symbol \u2192 Vector: Continuous representations enable learning of semantic gradients.</p> <p>Sequence \u2192 Attention: Parallel context integration replaces recurrence.</p> <p>Text \u2192 Context: Pretraining captures knowledge without explicit supervision.</p> <p>Language \u2192 World: Grounding links linguistic representations to sensory and causal models.</p>"},{"location":"deeplearning/4_nlp/#unifying-principle","title":"Unifying Principle","text":"<p>Deep language understanding = predictive modeling of structured context across both linguistic and environmental domains.</p> Concept Core Idea Model / Mechanism Distributed representations Meanings as patterns, not symbols Embeddings Context dependence Sense resolution via interaction Self-attention Parallelism All words attend to all others Transformer Bidirectionality Context from both sides BERT encoder Transfer learning Self-supervised \u2192 supervised Fine-tuning Grounding Language tied to perception/action Embodied agents Predictive learning Understanding as anticipation World models"},{"location":"deeplearning/5_attention/","title":"5. Transformers and Attention Mechanisms","text":""},{"location":"deeplearning/5_attention/#1-attention-memory-and-cognition","title":"1. Attention, Memory, and Cognition","text":"<ul> <li>Attention = ability to focus on relevant signals and ignore distractions.  </li> <li>Enables selective processing (e.g. cocktail party effect).  </li> <li> <p>Allows focusing on one thought or event at a time.</p> </li> <li> <p>Memory provides continuity: keeping information over time to guide behavior or reasoning.</p> </li> <li> <p>Together, they form the basis of cognition \u2014 controlling what to process, store, and recall.</p> </li> <li> <p>Neural networks can model aspects of this by learning what to attend to and what to remember.</p> </li> <li> <p>Goal of attention in DL:   Reduce complexity by focusing computation on the most informative parts of data or internal state.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#2-implicit-attention-in-neural-networks","title":"2. Implicit Attention in Neural Networks","text":"<ul> <li> <p>Neural networks are parametric nonlinear functions \\(y = f_\\theta(x)\\) mapping inputs to outputs.   They naturally exhibit implicit attention: certain input dimensions influence outputs more.</p> </li> <li> <p>The Jacobian \\(J = \\frac{\\partial y}{\\partial x}\\) quantifies this sensitivity \u2014 shows which input parts the model \u201cpays attention\u201d to.</p> </li> <li> <p>Example:   In deep RL, sensitivity maps reveal focus on state-value vs action-advantage components.</p> </li> <li> <p>Recurrent Neural Networks (RNNs) extend this to sequences:  </p> </li> <li>Hidden state \\(h_t\\) stores past info.  </li> <li>The sequential Jacobian \\(\\frac{\\partial y_t}{\\partial x_{t-k}}\\) shows which past inputs are remembered.  </li> <li> <p>Implicitly attends to relevant time steps (memory through recurrence).</p> </li> <li> <p>In tasks like machine translation, implicit attention lets models reorder tokens:</p> <p>\u201cto reach\u201d \u2192 \u201czu erreichen\u201d</p> </li> </ul>"},{"location":"deeplearning/5_attention/#3-explicit-hard-attention","title":"3. Explicit (Hard) Attention","text":"<ul> <li>Explicit attention introduces a separate attention mechanism that decides where to look or what to read.   It restricts the data fed to the main network.</li> </ul>"},{"location":"deeplearning/5_attention/#why-explicit-attention","title":"Why explicit attention?","text":"<ul> <li>Efficiency: processes only selected parts of input.  </li> <li>Scalability: works on large or variable-size data.  </li> <li>Sequential processing: e.g. moving \u201cgaze\u201d across static images.  </li> <li>Interpretability: easier to visualize focus regions.</li> </ul>"},{"location":"deeplearning/5_attention/#model-structure","title":"Model structure","text":"<ul> <li>Network outputs attention parameters \\(a\\) that define a glimpse distribution \\(p(g|a)\\) over possible data regions.  </li> <li>A glimpse \\(g\\) (subset or window of data) is sampled and passed back as input.  </li> <li>System becomes recurrent, even if the base network is not.</li> </ul>"},{"location":"deeplearning/5_attention/#training-non-differentiable","title":"Training (non-differentiable)","text":"<ul> <li> <p>When glimpse selection is discrete or stochastic, use REINFORCE:      where \\(R\\) is the reward (e.g. task loss) and \\(b\\) a baseline for variance reduction.</p> </li> <li> <p>Thus, attention acts as a policy \\(\\pi_\\theta(g)\\) over glimpses.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#examples","title":"Examples","text":"<ul> <li>Recurrent Models of Visual Attention (Mnih et al., 2014): learns a sequence of foveal glimpses for image classification.  </li> <li>Multiple Object Recognition with Visual Attention (Ba et al., 2014): attends sequentially to multiple objects.</li> </ul>"},{"location":"deeplearning/5_attention/#4-soft-attention","title":"4. Soft Attention","text":"<ul> <li>Hard attention samples discrete glimpses \u2192 non-differentiable \u2192 needs RL.  </li> <li>Soft attention computes a weighted average over all glimpses \u2192 differentiable \u2192 trainable by backprop.</li> </ul>"},{"location":"deeplearning/5_attention/#basic-idea","title":"Basic idea","text":"<ul> <li> <p>Attention parameters \\(a\\) define weights \\(w_i\\) over input features \\(v_i\\):      The readout \\(v\\) is a smooth combination of inputs.</p> </li> <li> <p>Replaces sampling by expectation \u2192 continuous, differentiable.</p> </li> </ul>"},{"location":"deeplearning/5_attention/#benefits","title":"Benefits","text":"<ul> <li>Trained end-to-end with gradients.  </li> <li>Easier and more stable than hard attention.  </li> <li>Allows focus distribution rather than a single point.</li> </ul>"},{"location":"deeplearning/5_attention/#variants","title":"Variants","text":"<ul> <li>Location-based attention: focuses by spatial position (e.g. Gaussian over coordinates).  </li> <li>Content-based attention: focuses by similarity of key \\(k\\) to data vectors \\(x_i\\) via score \\(S(k, x_i)\\), usually normalized by softmax:    </li> </ul>"},{"location":"deeplearning/5_attention/#applications","title":"Applications","text":"<ul> <li>Handwriting synthesis: RNN learns soft \u201cwindow\u201d over text sequence.  </li> <li>Neural Machine Translation: associative attention aligns words between languages.  </li> <li> <p>DRAW model: uses Gaussian filters to read/write parts of an image.</p> </li> <li> <p>Soft attention = data-dependent dynamic weighting (similar to convolution with adaptive filters).</p> </li> </ul>"},{"location":"deeplearning/5_attention/#5-introspective-attention-and-memory","title":"5. Introspective Attention and Memory","text":"<ul> <li>So far: attention over external data.  </li> <li>Now: attention over internal state or memory \u2192 \u201cintrospective attention.\u201d  </li> <li>Lets the network read or write selectively to memory locations.  </li> <li>Enables reasoning, recall, and algorithmic behavior.</li> </ul>"},{"location":"deeplearning/5_attention/#neural-turing-machine-ntm","title":"Neural Turing Machine (NTM)","text":"<ul> <li>Adds a differentiable memory matrix \\(M \\in \\mathbb{R}^{N \\times W}\\).  </li> <li>Controller (RNN) interacts with memory using differentiable attention mechanisms.</li> </ul> <p>Operations - Write: modify selected rows in \\(M\\) using attention weights \\(w_t\\). - Read: output weighted sum of memory slots:    - Addressing modes:   - Content-based: match key vector \\(k_t\\) to memory contents (via cosine similarity).   - Location-based: shift attention by relative position.</p> <p>Training: fully differentiable \u2014 end-to-end via backprop.</p> <p>Example task: copying sequences of variable length \u2014 learns algorithmic generalization.</p>"},{"location":"deeplearning/5_attention/#differentiable-neural-computer-dnc","title":"Differentiable Neural Computer (DNC)","text":"<ul> <li>Successor to NTM with richer memory access:</li> <li>Tracks temporal links between writes.  </li> <li>Supports dynamic memory allocation.  </li> <li>Improves stability and scalability.</li> </ul> <p>Application: synthetic QA tasks (bAbI dataset) \u2014 answers questions requiring multiple supporting facts and temporal reasoning.</p> <p>Key insight: Attention provides selective access to memory, acting like \u201caddressing\u201d in a differentiable data structure.</p>"},{"location":"deeplearning/5_attention/#6-transformers-and-self-attention","title":"6. Transformers and Self-Attention","text":"<ul> <li>Transformers: remove recurrence and convolution entirely \u2014 rely only on attention.</li> </ul>"},{"location":"deeplearning/5_attention/#self-attention","title":"Self-Attention","text":"<ul> <li>Each token attends to all others in the sequence:      where:</li> <li>\\(Q, K, V\\) are query, key, and value matrices (learned linear projections of input embeddings).</li> <li>Produces context-aware representations for all tokens in parallel.</li> </ul>"},{"location":"deeplearning/5_attention/#multi-head-attention","title":"Multi-Head Attention","text":"<ul> <li>Multiple attention \u201cheads\u201d (\\(H\\)) learn different relationships:      Each head captures a distinct pattern (syntax, semantics, position, etc.).</li> </ul>"},{"location":"deeplearning/5_attention/#transformer-block","title":"Transformer Block","text":"<ul> <li>Structure:</li> <li>Multi-head self-attention  </li> <li>Add &amp; LayerNorm  </li> <li>Feedforward (ReLU + linear)  </li> <li>Add &amp; LayerNorm  </li> <li>Skip connections improve gradient flow and allow top-down signal mixing.</li> </ul>"},{"location":"deeplearning/5_attention/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Since model is permutation-invariant, inject position information:       Added to input embeddings.</li> </ul>"},{"location":"deeplearning/5_attention/#intuition","title":"Intuition","text":"<ul> <li>Self-attention generalizes RNN memory:</li> <li>Recurrent \u2192 sequential access  </li> <li>Transformer \u2192 direct pairwise access between all tokens.</li> <li>Enables long-range dependencies and parallelization.</li> </ul>"},{"location":"deeplearning/5_attention/#key-result","title":"Key result","text":"<ul> <li>Attention-only models achieve SOTA in translation and NLP tasks.  </li> <li>Forms basis for BERT, GPT, and modern large language models.</li> </ul>"},{"location":"deeplearning/5_attention/#7-adaptive-computation-time-act-and-summary","title":"7. Adaptive Computation Time (ACT) and Summary","text":""},{"location":"deeplearning/5_attention/#adaptive-computation-time-act","title":"Adaptive Computation Time (ACT)","text":"<ul> <li>Proposed by Graves (2016): allows networks to \u201cponder\u201d variable amounts of time per input.  </li> <li>Each step computes a halting probability \\(p_t\\); total halt when \\(\\sum_t p_t = 1\\).</li> <li>Output is a weighted sum of intermediate states:    </li> <li>Encourages efficient use of computation \u2014 more steps for harder inputs, fewer for easy ones.</li> <li>Regularized by a time penalty to avoid overthinking.</li> </ul>"},{"location":"deeplearning/5_attention/#universal-transformers","title":"Universal Transformers","text":"<ul> <li>Extend Transformers with recurrence in depth (same block applied multiple times).  </li> <li>Shares parameters across layers \u2014 like an RNN unrolled over depth.</li> <li>Combine parallel self-attention + iterative refinement + ACT.</li> <li>Achieves better generalization and adaptive reasoning on sequence tasks.</li> </ul>"},{"location":"deeplearning/5_attention/#summary","title":"Summary","text":"<ul> <li>Attention = selective processing of relevant information.  </li> <li>Implicit attention occurs naturally in deep nets (via sensitivity).  </li> <li>Explicit attention can be hard (sampled) or soft (differentiable).  </li> <li>Memory networks (NTM, DNC) use attention to read/write differentiable external memory.  </li> <li>Transformers unify attention as the core mechanism \u2014 fully parallel, context-rich.  </li> <li>Adaptive computation gives flexibility in processing time and complexity.</li> </ul> <p>Takeaway: Selective attention and memory \u2014 biological inspirations \u2014 are now core architectural principles driving modern deep learning.</p>"},{"location":"deeplearning/6_gans/","title":"6. Generative Models and GANs","text":""},{"location":"deeplearning/6_gans/#1-overview-generative-models","title":"1. Overview: Generative Models","text":"<ul> <li>Goal: learn a model of the true data distribution \\(p^*(x)\\) from samples.</li> </ul>"},{"location":"deeplearning/6_gans/#types-of-generative-models","title":"Types of Generative Models","text":"<ol> <li>Explicit likelihood models \u2013 define tractable \\(p_\\theta(x)\\)</li> <li>Max. likelihood: PPCA, Mixture Models, PixelCNN, Wavenet, autoregressive LMs.</li> <li> <p>Approx. likelihood: Boltzmann Machines, Variational Autoencoders (VAE).</p> </li> <li> <p>Implicit models \u2013 define sampling procedure, not explicit \\(p_\\theta(x)\\) </p> </li> <li>Examples: GANs, Moment Matching Networks.</li> </ol>"},{"location":"deeplearning/6_gans/#11-the-gan-idea","title":"1.1 The GAN Idea","text":"<ul> <li>Two-player minimax game:</li> <li>Generator (G): maps noise \\(z \\sim p(z)\\) to data space \\(G(z)\\).</li> <li> <p>Discriminator (D): classifies samples as real (from \\(p^*(x)\\)) or fake (\\(G(z)\\)).</p> </li> <li> <p>Objectives:    </p> </li> <li> <p>Interpretation:</p> </li> <li>\\(D\\) learns to distinguish real from fake.</li> <li>\\(G\\) learns to fool \\(D\\).</li> <li>Training reaches equilibrium when \\(p_G(x) = p^*(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#12-alternative-view-teacherstudent-analogy","title":"1.2 Alternative View \u2014 Teacher\u2013Student Analogy","text":"<ul> <li>Teacher (D): distinguishes real vs fake, providing feedback.</li> <li>Student (G): improves by making fake data look real.</li> <li>Cooperative interpretation of the adversarial process.</li> </ul>"},{"location":"deeplearning/6_gans/#13-gans-as-a-game","title":"1.3 GANs as a Game","text":"<ul> <li>Zero-sum, bi-level optimization \u2192 strong connection to game theory.</li> <li>GAN equilibrium = Nash equilibrium between \\(G\\) and \\(D\\).</li> <li>Training alternates between optimizing \\(D\\) and \\(G\\).</li> </ul> <p>Key Intuition: GANs learn by competition between a generator and discriminator rather than direct likelihood maximization.</p>"},{"location":"deeplearning/6_gans/#2-gan-objective-as-divergence-minimization","title":"2. GAN Objective as Divergence Minimization","text":"<ul> <li>Generative modeling often aims to minimize a distance or divergence between   the true data distribution \\(p^*(x)\\) and model distribution \\(p_G(x)\\).</li> </ul>"},{"location":"deeplearning/6_gans/#21-kl-and-related-divergences","title":"2.1 KL and Related Divergences","text":"<ul> <li> <p>Maximum Likelihood Estimation (MLE):      \u2192 drives \\(p_\\theta\\) to assign high probability to observed data.</p> </li> <li> <p>But: implicit models (like GANs) don\u2019t have explicit likelihoods, so MLE can\u2019t be used directly.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#22-gan-as-jensenshannon-js-divergence-minimization","title":"2.2 GAN as Jensen\u2013Shannon (JS) Divergence Minimization","text":"<ul> <li> <p>If discriminator \\(D\\) is optimal:      Plugging into the GAN loss shows that the generator minimizes:      \u2192 GAN \u2248 JS divergence minimization.</p> </li> <li> <p>However, this relies on an optimal discriminator \u2014 not true in practice.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#23-limitations-of-kl-js-divergences","title":"2.3 Limitations of KL / JS Divergences","text":"<ul> <li>If \\(p_G\\) and \\(p^*\\) have non-overlapping support,   \u2192 no useful gradient signal (zero gradient problem).</li> <li>The density ratio \\(\\frac{p^*(x)}{p_G(x)}\\) becomes infinite where \\(p_G=0\\).</li> <li>Thus, GANs can fail to learn when supports are disjoint.</li> </ul>"},{"location":"deeplearning/6_gans/#24-alternative-distances-divergences","title":"2.4 Alternative Distances &amp; Divergences","text":""},{"location":"deeplearning/6_gans/#a-wasserstein-distance-earth-movers","title":"(a) Wasserstein Distance (Earth Mover\u2019s)","text":"<ul> <li>Measures minimal \u201ccost\u201d of moving probability mass:    </li> <li>Provides smooth, non-vanishing gradients even when supports don\u2019t overlap.</li> <li>WGAN: enforce 1-Lipschitz \\(D\\) via:</li> <li>weight clipping,</li> <li>gradient penalty (WGAN-GP),</li> <li>spectral normalization.</li> </ul>"},{"location":"deeplearning/6_gans/#b-mmd-maximum-mean-discrepancy","title":"(b) MMD (Maximum Mean Discrepancy)","text":"<ul> <li>Compares distributions via embeddings in a Reproducing Kernel Hilbert Space (RKHS):    </li> <li>MMD-GAN: learns kernel features \\(\\phi\\) jointly with \\(D\\).</li> </ul>"},{"location":"deeplearning/6_gans/#c-f-divergences","title":"(c) f-divergences","text":"<ul> <li>General framework using convex functions \\(f\\):    </li> <li>GAN training derived via variational lower bound on \\(D_f\\).</li> </ul>"},{"location":"deeplearning/6_gans/#25-practical-view","title":"2.5 Practical View","text":"<ul> <li>GANs are not pure divergence minimizers in practice:</li> <li>\\(D\\) not optimal \u2192 approximate divergence.</li> <li>Neural discriminator learns a smooth approximation to density ratio.</li> <li>Provides useful gradients even when the true divergence would fail.</li> </ul>"},{"location":"deeplearning/6_gans/#26-summary-table","title":"2.6 Summary Table","text":"Perspective Example Key Idea KL Divergence MLE, VAEs Explicit likelihoods JS Divergence Original GAN Adversarial training Wasserstein WGAN Smooth gradients MMD MMD-GAN Kernel mean embedding f-divergence f-GAN Variational bound family <p>Insight: GANs can be viewed as learning a neural divergence measure that provides a stable, informative training signal.</p>"},{"location":"deeplearning/6_gans/#3-evaluating-gans","title":"3. Evaluating GANs","text":"<ul> <li>Evaluating generative models is difficult \u2014 no single metric captures all aspects.</li> <li>Must assess:</li> <li>Sample quality (fidelity, realism)</li> <li>Diversity / generalization</li> <li>Representation learning (usefulness of learned features)</li> </ul>"},{"location":"deeplearning/6_gans/#31-why-not-log-likelihood","title":"3.1 Why Not Log-Likelihood?","text":"<ul> <li>GANs are implicit models \u2014 no tractable \\(p(x)\\).</li> <li>Estimating log-likelihood is expensive and unreliable.</li> <li>Hence: use feature-based or classifier-based proxies.</li> </ul>"},{"location":"deeplearning/6_gans/#32-inception-score-is","title":"3.2 Inception Score (IS)","text":"<ul> <li>Uses a pretrained Inception v3 classifier.</li> <li>Compares predicted label distributions of generated samples.</li> </ul> <p>Formula:  </p> <p>Intuition: - High-quality images \u2192 confident predictions (\\(p(y|x)\\) low entropy). - Diverse images \u2192 marginal label distribution \\(p(y)\\) high entropy.</p> <p>Properties: - Measures sample quality and diversity. - Correlates with human judgment. - Fails to capture intra-class variation or features beyond ImageNet classes.</p> <p>Higher is better.</p>"},{"location":"deeplearning/6_gans/#33-frechet-inception-distance-fid","title":"3.3 Fr\u00e9chet Inception Distance (FID)","text":"<ul> <li>Compares statistics of features (from pretrained Inception network) for real vs fake samples.</li> </ul> <p>Formula:  </p> <p>where \\((\\mu_r, \\Sigma_r)\\) and \\((\\mu_g, \\Sigma_g)\\) are mean and covariance of real and generated data features.</p> <p>Properties: - Sensitive to mode dropping and artifacts. - Correlates strongly with human evaluation. - Lower is better. - Biased for small sample sizes \u2192 use KID (Kernel Inception Distance) for correction.</p>"},{"location":"deeplearning/6_gans/#34-overfitting-check-nearest-neighbours","title":"3.4 Overfitting Check \u2014 Nearest Neighbours","text":"<ul> <li>Compute nearest real images to generated samples in pretrained feature space.</li> <li>Helps detect memorization (copying training images).</li> </ul>"},{"location":"deeplearning/6_gans/#35-evaluation-depends-on-goal","title":"3.5 Evaluation Depends on Goal","text":"Goal Metric Example Measures Image quality FID, IS Fidelity &amp; diversity Representation learning Linear probe accuracy Feature usefulness Data generation Human evaluation Perceptual quality RL / control Policy reward Functional realism <p>Key Takeaway: Use multiple complementary metrics \u2014 quantitative (IS, FID) + qualitative (visual inspection, diversity).</p>"},{"location":"deeplearning/6_gans/#4-the-gan-zoo","title":"4. The GAN Zoo","text":"<p>GANs have evolved rapidly \u2014 from simple MLPs on MNIST to massive multi-GPU models like BigGAN and StyleGAN.</p>"},{"location":"deeplearning/6_gans/#41-the-original-gan","title":"4.1 The Original GAN","text":"<ul> <li>First formulation of adversarial training.</li> <li>Architecture: simple multilayer perceptrons (MLPs).</li> <li>Trained on small images (e.g. 32\u00d732).  </li> <li>Ignored spatial structure (flattened pixels).  </li> <li>Introduced the minimax objective still used today.</li> </ul>"},{"location":"deeplearning/6_gans/#42-conditional-gan","title":"4.2 Conditional GAN","text":"<ul> <li> <p>Adds conditioning information \\(y\\) (e.g. class label or input image). </p> </li> <li> <p>Enables controlled generation \u2014 specify category or domain.   Examples:</p> </li> <li>Class-conditional image synthesis (e.g., \"generate a dog\").  </li> <li>Image-to-image translation (later: Pix2Pix, CycleGAN).</li> </ul>"},{"location":"deeplearning/6_gans/#43-laplacian-gan","title":"4.3 Laplacian GAN","text":"<ul> <li>Generates images progressively, starting from low resolution.  </li> <li>Each level adds high-frequency detail via residual (Laplacian) generation.  </li> <li>Fully convolutional \u2014 can produce arbitrarily large outputs.</li> <li>Improves high-res synthesis through multi-scale structure.</li> </ul>"},{"location":"deeplearning/6_gans/#44-deep-convolutional-gan","title":"4.4 Deep Convolutional GAN","text":"<ul> <li>Replaces MLPs with deep convnets for both \\(G\\) and \\(D\\).</li> <li>Uses Batch Normalization and ReLU/LeakyReLU for stability.</li> <li>Enables smooth interpolation in latent space:</li> <li>\\(G(z_1)\\) \u2192 \\(G(\\frac{1}{2}(z_1 + z_2))\\) \u2192 \\(G(z_2)\\) produces semantically meaningful transitions.</li> <li>Latent space exhibits semantic arithmetic (e.g. \u201cman + glasses \u2013 woman\u201d).</li> </ul>"},{"location":"deeplearning/6_gans/#45-spectrally-normalized-gan","title":"4.5 Spectrally Normalized GAN","text":"<ul> <li> <p>Enforces 1-Lipschitz constraint on \\(D\\) via spectral normalization:      where \\(\\sigma_{\\max}(W)\\) is the largest singular value.</p> </li> <li> <p>Stabilizes training and improves generalization.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#46-projection-discriminator","title":"4.6 Projection Discriminator","text":"<ul> <li> <p>Adds class embedding projection inside \\(D\\):    where \\(v_y\\) is the embedding for class \\(y\\).</p> </li> <li> <p>Theoretically consistent probabilistic discriminator formulation.  </p> </li> <li>Strong empirical results on class-conditional image synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#47-self-attention-gan","title":"4.7 Self-Attention GAN","text":"<ul> <li>Introduces self-attention layers to capture long-range dependencies.  </li> <li>Improves global structure and coherence in generated images.</li> <li>Inspired by Transformer attention.</li> </ul>"},{"location":"deeplearning/6_gans/#48-biggan","title":"4.8 BigGAN","text":"<ul> <li>Scaled-up GANs with massive compute + large datasets (ImageNet, JFT).  </li> <li>Key ingredients:</li> <li>Hinge loss for \\(D\\) </li> <li>Spectral normalization  </li> <li>Self-attention  </li> <li>Projection discriminator  </li> <li>Orthogonal regularization  </li> <li>Skip connections from noise  </li> <li>Shared class embeddings  </li> <li>Truncation trick: reduce noise magnitude to increase fidelity (trade-off with diversity).</li> </ul>"},{"location":"deeplearning/6_gans/#49-logan","title":"4.9 LOGAN","text":"<ul> <li>Introduces latent optimization \u2014 optimize \\(z\\) via gradient updates to improve adversarial dynamics.  </li> <li>Uses natural gradient descent in latent space.  </li> <li>Yields higher FID/IS improvements over BigGAN.</li> </ul>"},{"location":"deeplearning/6_gans/#410-progressive-gan","title":"4.10 Progressive GAN","text":"<ul> <li>Trains from low to high resolution (4\u00d74 \u2192 8\u00d78 \u2192 16\u00d716 \u2026).  </li> <li>Each stage adds new layers to \\(G\\) and \\(D\\).  </li> <li>Dramatically improves stability and image quality (especially faces).</li> </ul>"},{"location":"deeplearning/6_gans/#411-stylegan","title":"4.11 StyleGAN","text":"<ul> <li>Adds style-based generator architecture:</li> <li>Latent vector \\(z\\) transformed by MLP to intermediate \\(w\\).</li> <li>AdaIN (Adaptive Instance Normalization): modulates style per channel.</li> <li> <p>Injects per-pixel noise for local details.</p> </li> <li> <p>Learns disentangled representations \u2014 global attributes (style) vs local (texture).</p> </li> </ul>"},{"location":"deeplearning/6_gans/#412-takeaways","title":"4.12 Takeaways","text":"<ul> <li>GAN progress driven by:</li> <li>Better architectures (Conv, Attention, Progressive, Style-based)</li> <li>Normalization &amp; regularization</li> <li>Stability techniques</li> <li>Large-scale training</li> </ul> <p>Trend: From small MLPs \u2192 Conv architectures \u2192 Attention-based, scalable, stable models like BigGAN &amp; StyleGAN.</p>"},{"location":"deeplearning/6_gans/#5-representation-learning-with-gans","title":"5. Representation Learning with GANs","text":"<p>Beyond generating samples, GANs can learn rich latent representations of data.</p>"},{"location":"deeplearning/6_gans/#51-motivation","title":"5.1 Motivation","text":"<ul> <li>GANs implicitly learn latent spaces that capture high-level semantics.</li> <li>Exploring or constraining this latent space enables unsupervised representation learning.</li> </ul>"},{"location":"deeplearning/6_gans/#52-evidence-from-dcgan","title":"5.2 Evidence from DCGAN","text":"<ul> <li>DCGAN latent vectors encode meaningful directions:</li> <li>Smooth interpolation between points \u2192 semantic transformations.</li> <li>Linear arithmetic in latent space (e.g., smiling woman \u2013 woman + man \u2192 smiling man).</li> <li>Suggests disentangled feature representations emerge naturally.</li> </ul>"},{"location":"deeplearning/6_gans/#53-infogan","title":"5.3 InfoGAN","text":"<ul> <li>Extends GAN with information maximization objective:</li> <li>Encourages some latent codes \\(c\\) to be interpretable and disentangled.</li> </ul> <p>Objective:  where \\(I(c; G(z, c))\\) is mutual information between latent code and generated output.</p> <ul> <li>Adds an auxiliary network to infer \\(c\\) from \\(G(z, c)\\).</li> <li>Learns to associate:</li> <li>Discrete codes \u2192 categories (digits, shapes)</li> <li>Continuous codes \u2192 attributes (rotation, scale)</li> </ul>"},{"location":"deeplearning/6_gans/#54-ali-bigan","title":"5.4 ALI / BiGAN","text":"<ul> <li>Adds an encoder \\(E(x)\\) mapping real data to latent space.</li> <li> <p>Joint discriminator distinguishes pairs:    </p> </li> <li> <p>At equilibrium:</p> </li> <li> <p>\\(E\\) and \\(G\\) become approximate inverses:</p> <ul> <li>\\(x \\approx G(E(x))\\)</li> <li>\\(z \\approx E(G(z))\\)</li> </ul> </li> <li> <p>Enables inference and representation learning simultaneously.</p> </li> </ul>"},{"location":"deeplearning/6_gans/#55-bigbigan","title":"5.5 BigBiGAN","text":"<ul> <li>Scales BiGAN to BigGAN architecture.</li> <li>Uses large-scale encoders (\\(E\\)) with ResNet blocks.</li> <li>Learns strong unsupervised representations competitive with self-supervised models.</li> </ul> <p>Observations: - Reconstructions \\(G(E(x))\\) preserve semantic content, not exact pixels. - Encoder features yield high ImageNet classification accuracy after linear probing.</p>"},{"location":"deeplearning/6_gans/#56-summary","title":"5.6 Summary","text":"Model Key Idea Outcome DCGAN Implicitly semantic latent space Interpolations meaningful InfoGAN Maximize info between codes and outputs Disentangled features BiGAN / ALI Add encoder, joint training Bidirectional mapping BigBiGAN Large-scale BiGAN Competitive unsupervised features <p>Key Insight: GANs not only generate, but also encode \u2014 their latent structure can act as a rich, learned representation space.</p>"},{"location":"deeplearning/6_gans/#6-gans-for-other-modalities-and-problems","title":"6. GANs for Other Modalities and Problems","text":"<p>GANs extend far beyond images \u2014 used for translation, audio, video, RL, and even art.</p>"},{"location":"deeplearning/6_gans/#61-image-to-image-translation","title":"6.1 Image-to-Image Translation","text":""},{"location":"deeplearning/6_gans/#a-pix2pix","title":"(a) Pix2Pix","text":"<ul> <li>Conditional GAN trained on paired datasets \\((x, y)\\).</li> <li>Learns deterministic mapping between domains (e.g., edges \u2192 photos).</li> <li>Loss combines adversarial term + L1 reconstruction:    </li> </ul>"},{"location":"deeplearning/6_gans/#b-cyclegan","title":"(b) CycleGAN","text":"<ul> <li>Unpaired domain translation \u2014 no 1:1 correspondence.</li> <li>Uses cycle consistency:</li> <li>\\(x \\in A \\to G_B(x) \\to F_A(G_B(x)) \\approx x\\)</li> <li>Enforces invertibility between domains.</li> <li>Enables tasks like horse \u2194 zebra, summer \u2194 winter.</li> </ul>"},{"location":"deeplearning/6_gans/#62-audio-synthesis","title":"6.2 Audio Synthesis","text":""},{"location":"deeplearning/6_gans/#a-wavegan","title":"(a) WaveGAN","text":"<ul> <li>Adapts convolutional GANs to 1D waveforms.</li> <li>Fully unsupervised raw-audio synthesis.</li> </ul>"},{"location":"deeplearning/6_gans/#b-melgan","title":"(b) MelGAN","text":"<ul> <li>Conditional GAN trained to generate mel-spectrogram waveforms.</li> <li>Used in text-to-speech (GAN-TTS).</li> </ul>"},{"location":"deeplearning/6_gans/#c-gan-tts","title":"(c) GAN-TTS","text":"<ul> <li>High-fidelity speech synthesis model.</li> <li>Achieves human-like audio quality via adversarial losses.</li> </ul>"},{"location":"deeplearning/6_gans/#63-video-synthesis-prediction","title":"6.3 Video Synthesis &amp; Prediction","text":"<ul> <li>GANs extended to spatiotemporal data:</li> <li>TGAN-v2 (Saito &amp; Saito, 2018): multi-layer subsampling for video generation.</li> <li>DVD-GAN (Clark et al., 2019): scalable adversarial model for long, complex videos.</li> <li>TriVD-GAN (Luc et al., 2020): transformation-based video prediction.</li> </ul>"},{"location":"deeplearning/6_gans/#64-gans-in-reinforcement-learning-imitation-control","title":"6.4 GANs in Reinforcement Learning (Imitation &amp; Control)","text":"<ul> <li>GAIL (Ho &amp; Ermon, 2016): Generative Adversarial Imitation Learning </li> <li>Discriminator distinguishes expert vs policy trajectories.</li> <li>Generator = policy network optimizing to mimic experts.</li> </ul>"},{"location":"deeplearning/6_gans/#65-creative-applied-uses","title":"6.5 Creative &amp; Applied Uses","text":"<ul> <li>GauGAN (Park et al., 2019): semantic image synthesis using spatially-adaptive normalization (SPADE).  </li> <li>SPIRAL (Ganin et al., 2018): program synthesis from images via adversarial reinforcement learning.  </li> <li>Everybody Dance Now (Chan et al., 2019): motion transfer via adversarial video mapping.  </li> <li>DANN (Ganin et al., 2016): domain-adversarial training for domain adaptation.  </li> <li>Learning to See (Memo Akten, 2017): interactive GAN-based digital art.</li> </ul>"},{"location":"deeplearning/6_gans/#66-summary","title":"6.6 Summary","text":"Domain Example Key Idea Paired image translation Pix2Pix Conditional GAN + L1 loss Unpaired translation CycleGAN Cycle consistency Audio MelGAN, WaveGAN Conditional waveform generation Video DVD-GAN, TGAN-v2 Temporal adversarial modeling RL / Imitation GAIL Adversarial trajectory matching Art / Creativity GauGAN, SPIRAL Adversarial synthesis and style transfer <p>Insight: Adversarial learning generalizes across domains \u2014 GANs serve as a universal generator\u2013critic framework for structured data.</p>"},{"location":"deeplearning/7_unsuper/","title":"7. Unsupervised and Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#1-what-is-unsupervised-learning","title":"1. What is Unsupervised Learning?","text":""},{"location":"deeplearning/7_unsuper/#definition","title":"Definition","text":"<ul> <li>Goal: discover structure in data without explicit labels or rewards.  </li> <li>Learns a compact, informative representation of input data.</li> </ul> Learning Type Goal Supervision Supervised Map inputs \u2192 labels Requires labeled data Reinforcement Learn actions maximizing future reward Requires reward signal Unsupervised Find hidden structure No labels or rewards"},{"location":"deeplearning/7_unsuper/#core-ideas","title":"Core Ideas","text":"<ul> <li>Model latent structure or relationships between observations.  </li> <li>Examples:</li> <li>Clustering: group similar data points.  </li> <li>Dimensionality reduction: project data to low-dimensional latent space.  </li> <li>Manifold learning / disentangling: uncover independent factors of variation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#evaluation-challenges","title":"Evaluation Challenges","text":"<p>How do we know if unsupervised learning worked?</p> <ul> <li>Ambiguity of structure: multiple valid clusterings possible.   e.g., cluster by leg count, arm number, or height in robot dataset.</li> <li>Metrics depend on downstream use:   useful representations should improve data efficiency, generalization, or transfer.</li> </ul>"},{"location":"deeplearning/7_unsuper/#classic-methods","title":"Classic Methods","text":"<ul> <li>PCA (Principal Component Analysis): orthogonal basis capturing variance.  </li> <li>ICA (Independent Component Analysis): separates statistically independent components.  </li> <li>Modern goal: move beyond orthogonality \u2192 learn disentangled factors.</li> </ul>"},{"location":"deeplearning/7_unsuper/#summary","title":"Summary","text":"<p>Unsupervised learning discovers patterns, dependencies, or latent variables from data itself \u2014 forming the foundation for representation learning.</p>"},{"location":"deeplearning/7_unsuper/#2-why-is-unsupervised-learning-important","title":"2. Why is Unsupervised Learning Important?","text":""},{"location":"deeplearning/7_unsuper/#21-historical-context-of-representation-learning","title":"2.1 Historical Context of Representation Learning","text":"Era Key Milestone Approach 1950s\u20132000s Arthur Samuel (1959): Machine Learning coined Feature engineering, clustering 2000s Kernel methods (Hofmann et al., 2008) Hand-crafted similarity functions 2006 Hinton &amp; Salakhutdinov: RBMs &amp; Autoencoders Layer-wise unsupervised pretraining 2012 Krizhevsky et al.: AlexNet End-to-end supervised learning dominates <ul> <li>Progress came from more data, deeper models, and better hardware \u2014 but not necessarily more efficient learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#22-limitations-of-purely-supervised-learning","title":"2.2 Limitations of Purely Supervised Learning","text":"<p>Supervised models are: - Data inefficient \u2014 need millions of labeled samples. - Brittle \u2014 vulnerable to adversarial perturbations. - Poor at transfer \u2014 struggle with new domains or tasks. - Lack common sense \u2014 limited abstraction and reasoning.</p>"},{"location":"deeplearning/7_unsuper/#23-evidence-of-current-gaps","title":"2.3 Evidence of Current Gaps","text":"Challenge Example Reference Data efficiency Learning from few examples Lake et al. (2017) Robustness Adversarial examples, brittle decisions Goodfellow et al. (2015) Generalization CoinRun, DMLab-30 Cobbe (2018), DeepMind Transfer Schema Networks Kansky et al. (2017) Common sense Conceptual reasoning Lake et al. (2015)"},{"location":"deeplearning/7_unsuper/#24-why-unsupervised-learning-matters","title":"2.4 Why Unsupervised Learning Matters","text":"<ul> <li>Enables data-efficient adaptation to new tasks.</li> <li>Provides robust, generalizable features.</li> <li>Promotes transfer learning by separating invariant factors.</li> <li>Encourages abstract reasoning and causal understanding.</li> </ul>"},{"location":"deeplearning/7_unsuper/#25-towards-general-ai","title":"2.5 Towards General AI","text":"<p>Unsupervised learning provides shared representations enabling: - Rapid multi-task adaptation. - Reuse across vision, language, and control. - Reduced supervision in real-world learning.</p> <p>Summary: Unsupervised representation learning addresses the core limits of current AI \u2014 aiming for data efficiency, robustness, generalization, transfer, and common sense.</p>"},{"location":"deeplearning/7_unsuper/#3-what-makes-a-good-representation","title":"3. What Makes a Good Representation?","text":"<p>A representation is an internal model of the world \u2014 an abstraction that makes reasoning and prediction efficient.</p>"},{"location":"deeplearning/7_unsuper/#31-what-is-a-representation","title":"3.1 What is a Representation?","text":"<p>\u201cA formal system for making explicit certain entities or types of information, together with a specification of how the system does this.\u201d</p> <ul> <li>Represents information about the world in a way useful for computation.  </li> <li>Not about a single feature, but the geometry or manifold shape in representational space.</li> </ul>"},{"location":"deeplearning/7_unsuper/#32-why-representation-form-matters","title":"3.2 Why Representation Form Matters","text":"<ul> <li>Determines which computations are easy.  </li> <li>Should make relevant variations simple (e.g., object position) and irrelevant ones invariant (e.g., lighting).</li> </ul>"},{"location":"deeplearning/7_unsuper/#33-desirable-properties","title":"3.3 Desirable Properties","text":"Property Description Intuition Untangling Simplifies complex input manifolds Enables linear decoding Attention Allows selective focus on relevant factors Supports task-specific filtering Clustering Groups similar experiences together Facilitates generalization Latent Information Encodes hidden or inferred causes Predicts unobserved aspects Compositionality Builds complex concepts from simple parts Enables open-ended reasoning"},{"location":"deeplearning/7_unsuper/#34-information-bottleneck-principle","title":"3.4 Information Bottleneck Principle","text":"<ul> <li>Good representations compress inputs while preserving information about outputs.    </li> <li>Encourages minimal sufficient representations \u2014 compact yet predictive.</li> </ul>"},{"location":"deeplearning/7_unsuper/#4-evaluating-the-merit-of-a-representation","title":"4. Evaluating the Merit of a Representation","text":"<p>The value of a representation lies in how well it supports efficient, generalizable behavior across tasks.</p>"},{"location":"deeplearning/7_unsuper/#41-the-evaluation-challenge","title":"4.1 The Evaluation Challenge","text":"<ul> <li>No single metric defines a \u201cgood\u201d representation.</li> <li>The test: How well does it help solve new, diverse, unseen tasks efficiently?</li> </ul> <p>Representations should enable: - Data efficiency \u2014 learn new tasks from few examples. - Robustness \u2014 resist noise or perturbations. - Generalization \u2014 perform well on new data. - Transfer \u2014 reuse knowledge in new settings. - Common sense \u2014 support reasoning and abstraction.</p>"},{"location":"deeplearning/7_unsuper/#42-example-evaluating-representations-via-symmetries","title":"4.2 Example: Evaluating Representations via Symmetries","text":"<p>Let: - \\(W\\) = world space - \\(Z\\) = representational space - \\(G = G_x \\times G_y \\times G_c\\) = group of transformations (e.g., position, color)</p> <p>A good representation \\(f: W \\rightarrow Z\\) should satisfy:  </p> <p>That is, transformations in the world (translation, color shift) correspond to predictable transformations in representation space \u2192 equivariance.</p>"},{"location":"deeplearning/7_unsuper/#43-desirable-evaluation-criteria","title":"4.3 Desirable Evaluation Criteria","text":"Criterion Desired Property Example / Metric Equivariance Transformations map consistently Translation \u2192 shift in latent Compositionality Combine factors to form new concepts Modular latent factors Metric structure Smooth distances reflect similarity \\(L_2\\), cosine Attention Selectively focus on task-relevant parts Masking or gating mechanisms Symmetries Invariance to irrelevant transformations Rotation, scale invariance"},{"location":"deeplearning/7_unsuper/#44-downstream-evaluation-tasks","title":"4.4 Downstream Evaluation Tasks","text":"Evaluation Setting Example Task Reference Perception / Control Predict object color or position Gens &amp; Domingos, Deep Symmetry Networks (2014) Robustness Classify images under adversarial noise Gowal et al., 2019 Sequential Attention Learn task-focused vision Zoran et al., 2020 Transfer / RL Zero-shot navigation (DARLA) Higgins et al., ICML 2017 Lifelong Learning Maintain latent structure over domains Achille et al., NeurIPS 2018 Reasoning / Imagination Compositional concept inference Lake et al., Science 2015; Higgins et al., ICLR 2018"},{"location":"deeplearning/7_unsuper/#45-why-evaluation-matters","title":"4.5 Why Evaluation Matters","text":"<p>A good representation supports simple mappings to downstream tasks: - Linear classifiers for vision tasks (e.g., color or position recognition). - Efficient policy learning in RL with fewer samples. - Abstract reasoning and imagination \u2014 \u201cIf rainbow elephants live in big cities, can we expect one in London?\u201d</p>"},{"location":"deeplearning/7_unsuper/#5-representation-learning-techniques","title":"5. Representation Learning Techniques","text":"<p>Modern unsupervised representation learning spans generative, contrastive, and self-supervised approaches \u2014 all aiming to extract structure from data without labels.</p>"},{"location":"deeplearning/7_unsuper/#51-categories-of-methods","title":"5.1 Categories of Methods","text":"Category Core Idea Typical Example Generative Modeling Learn \\(p(x)\\) or a model that can reconstruct data VAE, \u03b2-VAE, MONet, GQN, GANs Contrastive Learning Learn by discriminating similar vs dissimilar samples CPC, SimCLR, word2vec Self-Supervised Learning Design pretext tasks that predict missing or reordered parts BERT, Colorization, Context Prediction"},{"location":"deeplearning/7_unsuper/#52-generative-modeling","title":"5.2 Generative Modeling","text":""},{"location":"deeplearning/7_unsuper/#521-motivation","title":"5.2.1 Motivation","text":"<ul> <li>Goal: learn the underlying data distribution \\(p(x)\\) to reveal hidden structure and causal factors.  </li> <li>Unsupervised generative modeling captures common regularities in data \u2014 enabling representation learning, synthesis, and reasoning.  </li> <li>Instead of directly memorizing examples, the model learns a probabilistic process that could have generated them.</li> </ul> <p>Generative models explain the data by learning how it might have arisen.</p>"},{"location":"deeplearning/7_unsuper/#522-from-maximum-likelihood-to-latent-variable-models","title":"5.2.2 From Maximum Likelihood to Latent Variable Models","text":""},{"location":"deeplearning/7_unsuper/#maximum-likelihood-principle","title":"Maximum Likelihood Principle","text":"<p>The ideal objective for learning a generative model is to maximize the likelihood of the observed data:  where \\(p^*(x)\\) is the true data distribution and \\(p_\\theta(x)\\) is the model.</p>"},{"location":"deeplearning/7_unsuper/#latent-variable-formulation","title":"Latent Variable Formulation","text":"<ul> <li>Assume data arises from hidden (latent) variables \\(z\\):    </li> <li>Here:</li> <li>\\(p(z)\\) \u2014 prior over latent variables (e.g., \\(\\mathcal{N}(0, I)\\))  </li> <li>\\(p_\\theta(x|z)\\) \u2014 likelihood or decoder mapping latent codes to data</li> </ul> <p>This defines a latent variable model: the data-generating process maps from a low-dimensional latent space to the observed space.</p>"},{"location":"deeplearning/7_unsuper/#523-inference-in-latent-variable-models","title":"5.2.3 Inference in Latent Variable Models","text":"<p>Goal: infer the posterior  to identify which latent factors \\(z\\) most likely generated observation \\(x\\).</p> <ul> <li>Intuition:   Recover the underlying causes that explain the data \u2014 along with uncertainty estimates.</li> <li>Problem:   Computing \\(p(z|x)\\) is often intractable, since \\(p_\\theta(x)\\) involves integrating over all \\(z\\).   \u2192 We must approximate inference using neural networks.</li> </ul> <p>Thus, generative models combine:</p> <ul> <li>Generation: \\(z \\rightarrow x\\) (decode latent causes into data)</li> <li>Inference: \\(x \\rightarrow z\\) (encode data into latent causes)</li> </ul>"},{"location":"deeplearning/7_unsuper/#524-variational-autoencoders-vaes","title":"5.2.4 Variational Autoencoders (VAEs)","text":"<p>To make inference tractable, VAEs introduce an approximate posterior \\(q_\\phi(z|x)\\) and optimize a variational bound on the likelihood:</p>"},{"location":"deeplearning/7_unsuper/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"\\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}[q_\\phi(z|x)\\,||\\,p(z)] \\]"},{"location":"deeplearning/7_unsuper/#terms","title":"Terms","text":"<ol> <li> <p>Reconstruction term    Encourages the model to faithfully reproduce the input from its latent code.</p> </li> <li> <p>KL divergence term    Regularizes the latent posterior to match the prior \u2014 ensuring smoothness and preventing overfitting.</p> </li> </ol>"},{"location":"deeplearning/7_unsuper/#neural-implementation","title":"Neural Implementation","text":"<ul> <li>Encoder \\(q_\\phi(z|x)\\): approximates inference (maps data \u2192 latent code).  </li> <li>Decoder \\(p_\\theta(x|z)\\): generates data from the latent space (latent \u2192 data).  </li> <li>Both are parameterized by deep neural networks.</li> </ul> <p>Reparameterization trick (Kingma &amp; Welling, 2014):  enables backpropagation through stochastic latent sampling.</p>"},{"location":"deeplearning/7_unsuper/#why-vaes-matter","title":"Why VAEs Matter","text":"<ul> <li>Provide continuous, structured latent spaces capturing generative factors.  </li> <li>Support smooth interpolation and semantic manipulation.  </li> <li>Foundation for disentangled and interpretable representation learning (e.g., \u03b2-VAE).  </li> <li>Bridge probabilistic modeling with deep learning.</li> </ul> <p>VAEs turn probabilistic inference into a scalable neural optimization problem \u2014 the cornerstone of modern generative representation learning.</p>"},{"location":"deeplearning/7_unsuper/#524-vae","title":"5.2.4 \u03b2-VAE","text":"<ul> <li>Adds weight \u03b2 to KL term:    </li> <li>Encourages disentangled latent factors (position, shape, rotation, color).</li> <li> <p>Provides interpretable, semantically meaningful representations.</p> </li> <li> <p>DARLA (Higgins et al., 2017): \u03b2-VAE for reinforcement learning \u2192 improved transfer and sim2real generalization.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#525-sequential-and-layered-models","title":"5.2.5 Sequential and Layered Models","text":"<p>ConvDRAW (Gregor et al., 2016) - Sequential VAE with recurrent refinement. - Models temporal and spatial dependencies.</p> <p>MONet (Burgess et al., 2019) - Attention-based scene decomposition. - Each latent corresponds to one object \u2192 compositional representations. - Enables object-centric reasoning and RL transfer.</p> <p>GQN (Eslami et al., 2018) - Generative Query Networks: learn neural scene representations. - Given partial observations, predict unseen viewpoints (3D reasoning).</p> <p>VQ-VAE (van den Oord et al., 2017) - Learns discrete latent variables via vector quantization. - Enables hierarchical or symbolic structure. - Useful for speech, images, and video. -</p>"},{"location":"deeplearning/7_unsuper/#526-gans-goodfellow-et-al-2014","title":"5.2.6 GANs (Goodfellow et al., 2014)","text":"<ul> <li>Implicit generative models \u2014 learn by adversarial game:</li> <li>Generator creates samples.</li> <li>Discriminator provides learning signal (no reconstruction loss).</li> <li>BigBiGAN (Donahue et al., 2019):</li> <li>Adds encoder for inference.</li> <li>Learns rich, high-level representations \u2192 SOTA semi-supervised performance on ImageNet.</li> </ul>"},{"location":"deeplearning/7_unsuper/#527-large-scale-generative-models","title":"5.2.7 Large-Scale Generative Models","text":"<ul> <li>GPT (Radford et al., 2019):  </li> <li>Large transformer trained via language modeling.</li> <li>Learns general representations useful for multiple downstream tasks (few-shot transfer).</li> </ul>"},{"location":"deeplearning/7_unsuper/#53-contrastive-learning","title":"5.3 Contrastive Learning","text":""},{"location":"deeplearning/7_unsuper/#core-idea","title":"Core Idea","text":"<ul> <li>No need to model \\(p(x)\\) explicitly.</li> <li>Learn representations that maximize mutual information between related samples.</li> </ul>"},{"location":"deeplearning/7_unsuper/#531-word2vec-mikolov-et-al-2013","title":"5.3.1 word2vec (Mikolov et al., 2013)","text":"<ul> <li>Predict context words given a target word.  </li> <li>Contrastive objective: classify positive (true context) vs negative (random) samples.</li> <li>Learns semantic embeddings; supports few-shot translation.</li> </ul>"},{"location":"deeplearning/7_unsuper/#532-contrastive-predictive-coding-cpc-van-den-oord-et-al-2018","title":"5.3.2 Contrastive Predictive Coding (CPC, van den Oord et al., 2018)","text":"<ul> <li>Maximize mutual information between current representation and future observations.  </li> <li>Trains a classifier to distinguish real future samples from negatives.  </li> <li> <p>Learns features useful across modalities (vision, speech).</p> </li> <li> <p>Data-efficient Image Recognition (H\u00e9naff et al., 2019):   contrastive features outperform pixel-level training in low-data regimes.</p> </li> </ul>"},{"location":"deeplearning/7_unsuper/#533-simclr-chen-et-al-2020","title":"5.3.3 SimCLR (Chen et al., 2020)","text":"<ul> <li>Simple, scalable contrastive framework:</li> <li>Generate two augmented views of the same image.</li> <li>Maximize agreement via contrastive loss (NT-Xent).</li> <li>Achieves state-of-the-art performance on ImageNet with linear evaluation.</li> <li>Demonstrates that contrastive signals + strong augmentations suffice for representation learning.</li> </ul>"},{"location":"deeplearning/7_unsuper/#54-self-supervised-learning","title":"5.4 Self-Supervised Learning","text":""},{"location":"deeplearning/7_unsuper/#idea","title":"Idea","text":"<ul> <li>Design pretext tasks that use natural structure in data as supervision.  </li> <li>Representations are deterministic and transferable to new tasks.</li> </ul>"},{"location":"deeplearning/7_unsuper/#541-examples","title":"5.4.1 Examples","text":"Task Description Reference Colorization Predict color from grayscale image Zhang et al., 2016 Context Prediction Predict position of image patches Doersch et al., 2015 Sequence Sorting Predict correct frame order in videos Lee et al., 2017 BERT (Devlin et al., 2019) Masked language modeling + next sentence prediction Revolutionized NLP"},{"location":"deeplearning/7_unsuper/#542-key-benefits","title":"5.4.2 Key Benefits","text":"<ul> <li>Requires no labels \u2014 just structure in data.  </li> <li>Produces general features useful for:</li> <li>Semi-supervised classification  </li> <li>Transfer learning  </li> <li>Downstream reasoning tasks</li> </ul>"},{"location":"deeplearning/7_unsuper/#55-design-principles","title":"5.5 Design Principles","text":"Consideration Desired Property Modality Align architecture with data type (image, text, audio) Task Design Choose pretext that aligns with useful features Consistency Maintain temporal/spatial coherence Discrete + Continuous Latents Enable symbolic and continuous reasoning Adaptivity Representations should evolve with experience <p>Summary: Unsupervised representation learning uses three complementary lenses: - Generative \u2192 model what the world looks like. - Contrastive \u2192 learn what is similar or different. - Self-supervised \u2192 create pseudo-tasks that reveal structure. Together, they aim for data-efficient, transferable, and interpretable representations.</p>"},{"location":"deeplearning/8_latentvariables/","title":"8. Latent Variable Models","text":""},{"location":"deeplearning/8_latentvariables/#1-generative-modelling","title":"1. Generative Modelling","text":""},{"location":"deeplearning/8_latentvariables/#11-what-are-generative-models","title":"1.1 What Are Generative Models?","text":"<ul> <li>Probabilistic models of high-dimensional data.</li> <li>Describe how observations are generated from underlying processes.</li> <li>Key focus: modelling dependencies between dimensions and capturing the full data distribution.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#12-why-they-matter","title":"1.2 Why They Matter","text":"<p>Generative models can: - Estimate data density (detect outliers, anomalies). - Enable compression (encode \u2192 decode). - Map between domains (e.g., translation, text-to-speech). - Support model-based RL (predict future states). - Learn representations from raw data. - Improve understanding of data structure.</p>"},{"location":"deeplearning/8_latentvariables/#13-types-of-generative-models-in-deep-learning","title":"1.3 Types of Generative Models in Deep Learning","text":""},{"location":"deeplearning/8_latentvariables/#a-autoregressive-models","title":"(a) Autoregressive Models","text":"<p>Model joint distribution via chain rule:  </p> <p>Trained with maximum likelihood</p> <p>Examples:</p> <ul> <li>RNN/Transformer LMs  </li> <li>NADE  </li> <li>PixelCNN / WaveNet</li> </ul> <p>Pros:</p> <ul> <li>Easy training (max. likelihood).</li> <li>No sampling during training.</li> </ul> <p>Cons:</p> <ul> <li>Slow generation (sequential).</li> <li>Often capture local structure better than global structure.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#b-latent-variable-models","title":"(b) Latent Variable Models","text":"<p>Introduce an unobserved latent variable \\(z\\):</p> <ul> <li>Prior: \\(p(z)\\) </li> <li>Likelihood: \\(p_\\theta(x\\mid z)\\) </li> </ul> <p>Joint:  </p> <p>Pros</p> <ul> <li>Flexible &amp; interpretable  </li> <li>Natural for representation learning  </li> <li>Fast generation  </li> </ul> <p>Cons - Require approximate inference unless specially designed (e.g., invertible models).</p>"},{"location":"deeplearning/8_latentvariables/#c-implicit-models-gans","title":"(c) Implicit Models (GANs)","text":"<ul> <li>Define a generator \\(G(z)\\) with no explicit likelihood.</li> <li>Trained adversarially using a discriminator.</li> </ul> <p>Pros</p> <ul> <li>Extremely realistic samples  </li> <li>Fast sampling  </li> </ul> <p>Cons</p> <ul> <li>Cannot evaluate \\(p(x)\\) </li> <li>Mode collapse  </li> <li>Training instability  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-latent-variable-models-inference","title":"2. Latent Variable Models &amp; Inference","text":""},{"location":"deeplearning/8_latentvariables/#21-what-is-a-latent-variable-model-lvm","title":"2.1 What is a Latent Variable Model (LVM)?","text":"<p>A latent variable model introduces an unobserved variable \\(z\\) that explains the observed data \\(x\\).</p> <p>Model components:</p> <ul> <li>Prior over latent variables:  </li> </ul> <p> </p> <ul> <li>Likelihood / decoder mapping latent \u2192 observation:  </li> </ul> <p> </p> <p>Joint distribution:</p> \\[ p_\\theta(x, z) = p_\\theta(x \\mid z)\\,p(z) \\] <p>Marginal likelihood (what we want to maximize when training):</p> \\[ p_\\theta(x) = \\int p_\\theta(x \\mid z)\\,p(z)\\,dz \\]"},{"location":"deeplearning/8_latentvariables/#22-intuition-latents-as-explanations","title":"2.2 Intuition: Latents as \u201cExplanations\u201d","text":"<ul> <li> <p>A particular value of \\(z\\) is a hypothesis about hidden causes that produced \\(x\\).</p> </li> <li> <p>Generation = sample latent \u2192 map it to data:    </p> </li> </ul> <p>Most of the article focuses on the inverse of this: recovering \\(z\\) from \\(x\\).</p>"},{"location":"deeplearning/8_latentvariables/#23-what-is-inference","title":"2.3 What Is Inference?","text":"<p>Inference means computing the posterior:  </p> <p>Why it matters:</p> <ul> <li>Explains the observation (which latents likely produced it?)</li> <li>Needed inside maximum-likelihood training   (the gradient depends on the posterior!)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#24-inference-requires-the-marginal-likelihood","title":"2.4 Inference Requires the Marginal Likelihood","text":"<p>To compute the posterior, we need:  This integral is often intractable.</p> <p>Thus exact inference usually fails except in special models (e.g., mixture models, linear-Gaussian).</p>"},{"location":"deeplearning/8_latentvariables/#25-example-mixture-of-gaussians","title":"2.5 Example: Mixture of Gaussians","text":"<p>Model:</p> <ul> <li>Choose cluster \\(k\\) </li> <li>Sample \\(x\\) from Gaussian for that cluster</li> </ul> <p>Posterior:  </p> <p>This model is tractable because:</p> <ul> <li>Finite number of discrete states  </li> <li>Closed-form posterior</li> </ul>"},{"location":"deeplearning/8_latentvariables/#26-the-need-for-inference-in-learning","title":"2.6 The Need for Inference in Learning","text":""},{"location":"deeplearning/8_latentvariables/#maximum-likelihood-as-the-core-training-principle","title":"Maximum Likelihood as the Core Training Principle","text":"<p>Maximum Likelihood Estimation (MLE) is the dominant method for fitting probabilistic models.  We choose parameters \\(\\theta\\) that make the observed training data as probable as possible:</p> \\[ \\theta^* = \\arg\\max_\\theta \\sum_{i} \\log p_\\theta(x^{(i)}) \\] <p>For latent variable models, the marginal likelihood is:  This integral is rarely tractable, which makes direct maximization difficult.</p>"},{"location":"deeplearning/8_latentvariables/#why-optimization-is-hard-in-latent-variable-models","title":"Why Optimization Is Hard in Latent Variable Models","text":"<ul> <li>The log-likelihood involves an integral (or sum) over the latent variables \\(z\\).  </li> <li>Because this integral usually has no closed form, we must use iterative optimization methods.</li> </ul> <p>Common approaches:</p> <ol> <li>Gradient-based optimization (e.g., gradient descent)</li> <li>Expectation-Maximization (EM)</li> </ol> <p>Below we explain why inference (computing the posterior \\(p_\\theta(z \\mid x)\\)) is essential for both.</p>"},{"location":"deeplearning/8_latentvariables/#261-gradient-based-learning-requires-the-posterior","title":"2.6.1 Gradient-Based Learning Requires the Posterior","text":"<p>Using the identity:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)}[\\nabla_\\theta \\log p_\\theta(x, z)] \\] <p>Differentiate the log-marginal</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{\\nabla_\\theta p_\\theta(x)}{p_\\theta(x)}\\] <p>Using: \\(p_\\theta(x)=\\int p_\\theta(x,z)\\,dz,\\)</p> <p>differentiate under the integral:</p> \\[\\nabla_\\theta p_\\theta(x) = \\nabla_\\theta \\int p_\\theta(x,z)\\,dz = \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Combine:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int \\nabla_\\theta p_\\theta(x,z)\\,dz\\] <p>Apply the log-derivative identity</p> <p>The identity:</p> \\[\\nabla_\\theta p_\\theta(x,z) = p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\] <p>Substitute:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\frac{1}{p_\\theta(x)} \\int p_\\theta(x,z)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Recognize the posterior</p> <p>Bayes\u2019 rule:</p> \\[p_\\theta(z\\mid x) = \\frac{p_\\theta(x,z)}{p_\\theta(x)}\\] <p>Substitute into the integral:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int \\frac{p_\\theta(x,z)}{p_\\theta(x)} \\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>This becomes:</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\int p_\\theta(z\\mid x)\\,\\nabla_\\theta \\log p_\\theta(x,z)\\,dz\\] <p>Write as an expectation</p> \\[\\nabla_\\theta \\log p_\\theta(x) = \\mathbb{E}_{p_\\theta(z\\mid x)} \\left[ \\nabla_\\theta \\log p_\\theta(x,z) \\right]\\] <p>To compute the gradient of the marginal likelihood, we must take an expectation under the posterior \\(p_\\theta(z\\mid x)\\).</p> <p>So:</p> <ul> <li>We cannot compute \\(\\nabla_\\theta \\log p_\\theta(x)\\) without knowing the posterior.</li> <li>Inference becomes part of every gradient step.</li> <li>If inference is intractable \u2192 gradient is intractable.</li> </ul> <p>This is why approximate inference (variational inference, MCMC) is essential for deep latent-variable models.</p>"},{"location":"deeplearning/8_latentvariables/#262-expectation-maximization-em-also-requires-inference","title":"2.6.2 Expectation-Maximization (EM) Also Requires Inference","text":"<p>EM is an alternative to gradient descent for maximizing likelihood.</p>"},{"location":"deeplearning/8_latentvariables/#e-step","title":"E-step:","text":"<p>Compute (or approximate) the posterior:  This assigns responsibilities to each latent configuration.</p>"},{"location":"deeplearning/8_latentvariables/#m-step","title":"M-step:","text":"<p>Update parameters by maximizing the expected complete-data log-likelihood:  </p> <p>Thus, the E-step directly requires inference.</p>"},{"location":"deeplearning/8_latentvariables/#27-why-exact-inference-is-hard","title":"2.7 Why Exact Inference Is Hard","text":""},{"location":"deeplearning/8_latentvariables/#continuous-latents","title":"Continuous latents:","text":"<ul> <li>Require multidimensional integration over nonlinear likelihoods.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#discrete-latents","title":"Discrete latents:","text":"<ul> <li>Require summing over exponentially many configurations.</li> </ul> <p>Only a few cases allow closed-form inference:</p> <ul> <li>Mixture models  </li> <li>Linear Gaussian systems  </li> <li>Invertible / flow-based models (covered next)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#28-two-strategies-to-handle-intractability","title":"2.8 Two Strategies to Handle Intractability","text":""},{"location":"deeplearning/8_latentvariables/#1-design-tractable-models","title":"1. Design tractable models","text":"<ul> <li>Invertible models (normalizing flows)</li> <li>Autoregressive latent structures Pros: exact inference Cons: restricted model class</li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-approximate-inference","title":"2. Approximate inference","text":"<ul> <li>Use approximations to posterior \\(p(z \\mid x)\\) </li> <li>Variational Inference or MCMC Pros: flexible, expressive models Cons: introduces approximation error</li> </ul>"},{"location":"deeplearning/8_latentvariables/#3-invertible-models-exact-inference","title":"3. Invertible Models &amp; Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#31-what-are-invertible-models","title":"3.1 What Are Invertible Models?","text":"<p>Invertible models (also called normalizing flows) are latent variable models where:</p> <ul> <li>The latent variable \\(z\\) and data \\(x\\) have the same dimensionality</li> <li>There exists an invertible, differentiable mapping </li> <li>Because \\(f_\\theta\\) is invertible:    </li> </ul> <p>Key property: Inference is exact and trivial \u2014 simply apply the inverse function.</p>"},{"location":"deeplearning/8_latentvariables/#32-generative-process","title":"3.2 Generative Process","text":"<p>To generate a sample:</p> <ol> <li>Sample \\(z \\sim p(z)\\) (usually a simple prior like \\(\\mathcal{N}(0, I)\\))</li> <li>Transform via </li> </ol> <p>Thus, the model pushes forward the prior distribution through a sequence of invertible transformations.</p>"},{"location":"deeplearning/8_latentvariables/#33-why-are-invertible-models-attractive","title":"3.3 Why Are Invertible Models Attractive?","text":"<ul> <li> <p>Exact inference:    is computed by a single function evaluation (no approximation needed).</p> </li> <li> <p>Exact likelihood:   Can compute \\(\\log p_\\theta(x)\\) exactly using the change-of-variables formula.</p> </li> </ul>"},{"location":"deeplearning/8_latentvariables/#34-change-of-variables-for-likelihood","title":"3.4 Change of Variables for Likelihood","text":"<p>Given an invertible mapping \\(x = f_\\theta(z)\\):</p> \\[ p_\\theta(x) = p(z) \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(x)}{\\partial x} \\right) \\right| \\] <p>Equivalently, using \\(z = f_\\theta^{-1}(x)\\):</p> \\[ \\log p_\\theta(x) = \\log p(z) + \\log \\left| \\det J_{f_\\theta^{-1}}(x) \\right| \\] <p>Where:</p> <ul> <li>\\(J_{f_\\theta^{-1}}\\) is the Jacobian matrix of the inverse map  </li> <li>The determinant accounts for volume change introduced by transformation</li> </ul>"},{"location":"deeplearning/8_latentvariables/#35-example-independent-component-analysis-ica","title":"3.5 Example: Independent Component Analysis (ICA)","text":"<p>ICA is the simplest invertible model:</p> <ul> <li>Latent prior:  factorial prior      with non-Gaussian heavy-tailed components</li> <li>Linear invertible mixing: </li> </ul> <p>Inference:  </p> <p>ICA recovers independent sources that explain the observed signal.</p>"},{"location":"deeplearning/8_latentvariables/#36-building-complex-invertible-models","title":"3.6 Building Complex Invertible Models","text":"<p>Modern flows build \\(f_\\theta\\) by composing many simple invertible layers:</p> \\[ f_\\theta = f_K \\circ f_{K-1} \\circ \\dots \\circ f_1 \\] <p>Composition of invertible functions is invertible.</p> <p>Building blocks:</p> <ul> <li>Linear transforms</li> <li>Autoregressive flows (IAF, MAF)</li> <li>Coupling layers (RealNVP, Glow)</li> <li>Residual flows</li> <li>Sylvester flows</li> </ul> <p>Design goal:</p> <p>Each layer must have a tractable inverse and a tractable Jacobian determinant.</p>"},{"location":"deeplearning/8_latentvariables/#37-advantages-limitations","title":"3.7 Advantages &amp; Limitations","text":""},{"location":"deeplearning/8_latentvariables/#advantages","title":"Advantages","text":"<ul> <li>Exact inference  </li> <li>Exact log-likelihood  </li> <li>Fast, parallel sampling  </li> <li>Useful as components in larger probabilistic models</li> </ul>"},{"location":"deeplearning/8_latentvariables/#limitations","title":"Limitations","text":"<ul> <li>Latent and data dimensions must match  </li> <li>Latents must be continuous  </li> <li>Observations must be continuous or quantized  </li> <li>Very deep flows require large memory  </li> <li>Hard to encode strong structure or sparsity  </li> </ul> <p>Flows are powerful but rigid: they trade flexibility in modeling for tractability in inference.</p>"},{"location":"deeplearning/8_latentvariables/#mar","title":"Mar","text":""},{"location":"deeplearning/8_latentvariables/#4-variational-inference-vi","title":"4. Variational Inference (VI)","text":""},{"location":"deeplearning/8_latentvariables/#41-why-variational-inference","title":"4.1 Why Variational Inference?","text":"<p>In many latent variable models, the true posterior  is intractable because computing  is impossible in closed form.</p> <p>We still need the posterior for:</p> <ul> <li>Inference (explaining the observation)</li> <li>Learning (MLE gradient depends on it)</li> <li>EM algorithm E-step</li> </ul>"},{"location":"deeplearning/8_latentvariables/#approximate-inference","title":"Approximate Inference","text":"<p>There are two major classes of approaches to approximate inference:</p>"},{"location":"deeplearning/8_latentvariables/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>Generate samples from the exact posterior using a Markov chain.</p> <ul> <li>Very general; exact in the limit of infinite time / computation  </li> <li>Computationally expensive  </li> <li>Convergence is hard to diagnose  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#2-variational-inference-vi","title":"2. Variational Inference (VI)","text":"<p>Approximate the posterior with a tractable distribution (e.g., fully factorized, mixture, or autoregressive).</p> <ul> <li>Fairly efficient \u2014 inference reduces to optimization of distribution parameters  </li> <li>Fast at test time (single forward pass of the inference network)  </li> <li>Cannot easily trade computation for accuracy (unlike MCMC)  </li> </ul> <p>MCMC = flexible, asymptotically exact, but slow. VI = fast and scalable, but biased due to restricted approximating family.</p>"},{"location":"deeplearning/8_latentvariables/#42-core-idea-of-variational-inference","title":"4.2 Core Idea of Variational Inference","text":"<p>Turns inference into a optimization problem. Faster compared to MCMC as optimization is faster than sampleing. Approximate the posterior with a simpler distribution:</p> \\[ q_\\phi(z \\mid x) \\approx p_\\theta(z \\mid x) \\] <p>Where:</p> <ul> <li>\\(q_\\phi\\) is the variational posterior</li> <li>\\(\\phi\\) are variational parameters (learned)</li> </ul> <p>Requirements:</p> <ol> <li>We can sample from \\(q_\\phi(z \\mid x)\\) </li> <li>We can compute \\(\\log q_\\phi(z \\mid x)\\) and its gradient wrt \\(\\phi\\) </li> </ol> <p>Common choice: mean-field approximation</p> \\[ q_\\phi(z \\mid x) = \\prod_i q_\\phi(z_i \\mid x) \\]"},{"location":"deeplearning/8_latentvariables/#43-training-with-variational-inference","title":"4.3 Training with Variational Inference","text":"<p>Goal: maximize the marginal likelihood</p> \\[ \\log p_\\theta(x) \\] <p>Since it's intractable, VI uses a lower bound on this quantity.</p>"},{"location":"deeplearning/8_latentvariables/#variational-lower-bound-elbo","title":"Variational Lower Bound (ELBO)","text":"<p>Using Jensen\u2019s inequality:</p> \\[ \\log p_\\theta(x) \\ge  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x, z)] - \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log q_\\phi(z \\mid x)] \\] <p>This is the Evidence Lower Bound (ELBO):</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi}\\!\\left[\\log p_\\theta(x, z)\\right] - \\mathbb{E}_{q_\\phi}\\!\\left[\\log q_\\phi(z \\mid x)\\right] \\] <p>We maximize ELBO w.r.t both \\(\\theta\\) and \\(\\phi\\).</p>"},{"location":"deeplearning/8_latentvariables/#44-kl-interpretation-variational-gap","title":"4.4 KL Interpretation (Variational Gap)","text":"<p>Rewrite ELBO:</p> \\[ \\log p_\\theta(x) = \\text{ELBO}(\\theta, \\phi) + D_{\\text{KL}}(q_\\phi(z \\mid x) \\,\\|\\, p_\\theta(z \\mid x)) \\] <p>Thus:</p> <ul> <li> <p>Maximizing ELBO wrt \\(\\phi\\)   \u2192 minimizes the KL divergence between \\(q_\\phi\\) and the true posterior.</p> </li> <li> <p>The variational gap is </p> </li> </ul> <p>If \\(q_\\phi\\) is expressive enough:  </p>"},{"location":"deeplearning/8_latentvariables/#45-what-happens-when-updating-each-parameter-set","title":"4.5 What Happens When Updating Each Parameter Set?","text":""},{"location":"deeplearning/8_latentvariables/#updating-variational-parameters-phi","title":"Updating variational parameters \\(\\phi\\):","text":"<ul> <li>Minimizes the variational gap  </li> <li>Makes \\(q_\\phi(z \\mid x)\\) closer to the true posterior  </li> <li>Does not affect the model directly</li> </ul>"},{"location":"deeplearning/8_latentvariables/#updating-model-parameters-theta","title":"Updating model parameters \\(\\theta\\):","text":"<ul> <li>Increases \\(\\log p_\\theta(x)\\) (good)</li> <li>BUT often also reduces the gap by making the posterior simpler   \u2192 Risk: posterior collapse / variational pruning</li> </ul> <p>This motivates using expressive variational families (flows, mixtures, autoregressive).</p>"},{"location":"deeplearning/8_latentvariables/#46-variational-pruning-posterior-collapse","title":"4.6 Variational Pruning (Posterior Collapse)","text":"<p>Because VI pushes \\(p_\\theta(z \\mid x)\\) towards \\(q_\\phi(z\\mid x)\\), the model may choose to ignore some latent dimensions:</p> \\[ p_\\theta(z_i \\mid x) = p(z_i) \\] <p>Meaning the latent variable carries no information about \\(x\\).</p> <p>Pros:</p> <ul> <li>Automatically learns effective latent dimensionality</li> </ul> <p>Cons:</p> <ul> <li>Prevents fully utilizing the latent capacity  </li> <li>Common issue in VAEs (particularly with strong decoders)</li> </ul>"},{"location":"deeplearning/8_latentvariables/#47-choosing-the-variational-posterior-family","title":"4.7 Choosing the Variational Posterior Family","text":""},{"location":"deeplearning/8_latentvariables/#simple-mean-field-gaussian","title":"Simple: Mean-field Gaussian","text":"<ul> <li>Fast</li> <li>Easy to optimize</li> <li>But limited expressivity</li> </ul>"},{"location":"deeplearning/8_latentvariables/#more-expressive-options","title":"More expressive options:","text":"<ul> <li>Mixture posteriors</li> <li>Gaussians with full covariance</li> <li>Autoregressive posteriors</li> <li>Normalizing-flow posteriors</li> </ul> <p>Trade-off: accuracy vs speed.</p>"},{"location":"deeplearning/8_latentvariables/#48-amortized-variational-inference","title":"4.8 Amortized Variational Inference","text":"<p>Classic VI:</p> <ul> <li>Each datapoint \\(x\\) has its own variational parameters  </li> <li>Requires iterative optimization per datapoint  </li> <li>Too slow for deep learning</li> </ul> <p>Amortized VI:</p> <ul> <li>Use an inference network (encoder)    </li> <li>Fast inference  </li> <li>Works with SGD  </li> <li>Introduced in Helmholtz Machines  </li> <li>Popularized by Variational Autoencoders</li> </ul>"},{"location":"deeplearning/8_latentvariables/#49-variational-vs-exact-inference","title":"4.9 Variational vs Exact Inference","text":""},{"location":"deeplearning/8_latentvariables/#advantages-of-vi","title":"Advantages of VI","text":"<ul> <li>Scalable to modern deep models  </li> <li>Fast inference  </li> <li>Enables flexible model design  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#disadvantages","title":"Disadvantages","text":"<ul> <li>Approximation bias  </li> <li>Posterior may be oversimplified  </li> <li>Can limit expressiveness of the full model  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#410-summary-of-section-4","title":"4.10 Summary of Section 4","text":"<ul> <li>Variational inference approximates the true posterior with a tractable distribution.  </li> <li>ELBO gives a trainable lower bound on the marginal likelihood.  </li> <li>VI converts inference into optimization.  </li> <li>Amortized VI enables neural inference (encoders).  </li> <li>Variational pruning can arise naturally and must be managed.  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#5-gradient-estimation-in-variational-inference","title":"5. Gradient Estimation in Variational Inference","text":""},{"location":"deeplearning/8_latentvariables/#51-why-do-we-need-gradient-estimators","title":"5.1 Why Do We Need Gradient Estimators?","text":"<p>To train a latent variable model with variational inference, we maximize the ELBO:</p> \\[ \\text{ELBO}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z\\mid x)}\\Big[ \\log p_\\theta(x, z) - \\log q_\\phi(z\\mid x) \\Big] \\] <p>We need gradients with respect to:</p> <ol> <li>Model parameters \\(\\theta\\)</li> <li>Variational parameters \\(\\phi\\)</li> </ol> <p>The expectation makes these gradients intractable in closed form, so we estimate them using Monte Carlo samples.</p>"},{"location":"deeplearning/8_latentvariables/#52-gradients-wrt-model-parameters-theta","title":"5.2 Gradients w.r.t. Model Parameters (\\(\\theta\\))","text":"<p>This part is easy.</p> <p>Because \\(q_\\phi(z\\mid x)\\) does not depend on \\(\\theta\\):</p> \\[ \\nabla_\\theta \\text{ELBO} = \\mathbb{E}_{q_\\phi(z\\mid x)} \\big[ \\nabla_\\theta \\log p_\\theta(x, z) \\big] \\] <p>We estimate this using samples:</p> <ol> <li>Draw \\(z \\sim q_\\phi(z\\mid x)\\) </li> <li>Compute \\(\\nabla_\\theta \\log p_\\theta(x,z)\\) </li> <li>Average across samples</li> </ol> <p>No special techniques required.</p>"},{"location":"deeplearning/8_latentvariables/#53-gradients-wrt-variational-parameters-phi","title":"5.3 Gradients w.r.t. Variational Parameters (\\(\\phi\\))","text":"<p>This is more difficult.</p> <p>We want:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] \\] <p>But \\(q_\\phi(z\\mid x)\\) depends on \\(\\phi\\). Two main strategies exist to handle this dependence:</p>"},{"location":"deeplearning/8_latentvariables/#54-two-families-of-gradient-estimators","title":"5.4 Two Families of Gradient Estimators","text":""},{"location":"deeplearning/8_latentvariables/#1-likelihood-ratio-reinforce-estimator","title":"\ud83d\udd37 1. Likelihood-Ratio / REINFORCE Estimator","text":"<p>Uses the identity:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}[f(z)] = \\mathbb{E}_{q_\\phi(z)}[f(z)\\,\\nabla_\\phi \\log q_\\phi(z)] \\] <p>This allows gradients for: - Discrete latent variables - Non-differentiable \\(f(z)\\) - Any distribution where we can compute \\(\\log q_\\phi(z)\\)</p> <p>Pros - Very general - Works for discrete and continuous latents  </p> <p>Cons - High variance - Requires variance reduction (baselines, control variates)</p> <p>This is the same gradient estimator used in policy gradients in RL.</p>"},{"location":"deeplearning/8_latentvariables/#2-reparameterization-pathwise-estimator","title":"\ud83d\udd37 2. Reparameterization / Pathwise Estimator","text":"<p>Instead of sampling \\(z \\sim q_\\phi(z\\mid x)\\) directly, write it as a differentiable transformation of noise:</p> \\[ z = g_\\phi(\\epsilon, x), \\quad \\epsilon \\sim p(\\epsilon) \\] <p>Then:</p> \\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z\\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)} \\big[ \\nabla_\\phi f(g_\\phi(\\epsilon, x)) \\big] \\] <p>This pushes the dependence on \\(\\phi\\) inside a differentiable function.</p>"},{"location":"deeplearning/8_latentvariables/#example-gaussian-posterior","title":"Example: Gaussian posterior","text":"<p>If  then:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1) \\] <p>Pros - Low variance - Enables stable VAE training  </p> <p>Cons - Only works for continuous latent variables - Requires differentiable sampling procedure</p>"},{"location":"deeplearning/8_latentvariables/#55-comparison-table","title":"5.5 Comparison Table","text":"Property REINFORCE Reparameterization Works for discrete latent variables \u2705 \u274c Works for continuous latent variables \u2705 \u2705 Low-variance gradients \u274c \u2705 Requires differentiable sampling \u274c \u2705 Used in VAEs sometimes always"},{"location":"deeplearning/8_latentvariables/#56-practical-notes","title":"5.6 Practical Notes","text":"<ul> <li>Modern VAEs always use the reparameterization trick.  </li> <li>More expressive posteriors (flows, mixtures) require more advanced reparameterization methods (e.g., implicit gradients).  </li> <li>Discrete VAEs use:</li> <li>Gumbel-Softmax  </li> <li>NVIL / REINFORCE with baselines  </li> <li>VIMCO  </li> </ul>"},{"location":"deeplearning/8_latentvariables/#57-summary-of-section-5","title":"5.7 Summary of Section 5","text":"<ul> <li>Gradient estimation is essential for training VI models.  </li> <li>\\(\\nabla_\\theta\\) is easy: just sample from the variational posterior.  </li> <li>\\(\\nabla_\\phi\\) is hard because sampling depends on parameters.  </li> <li>Two estimators solve this:</li> <li>Likelihood-ratio (REINFORCE)  </li> <li>Reparameterization trick  </li> <li>Reparameterization yields low-variance gradients and powers modern VAEs.</li> </ul>"},{"location":"deeplearning/8_latentvariables/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":""},{"location":"deeplearning/8_latentvariables/#61-what-is-a-vae","title":"6.1 What Is a VAE?","text":"<p>A VAE is a latent variable generative model with:</p> <ul> <li>Continuous latent variables \\(z\\)</li> <li>Neural networks for both:</li> <li>Encoder (variational posterior) \\(q_\\phi(z \\mid x)\\) </li> <li>Decoder (likelihood) \\(p_\\theta(x \\mid z)\\)</li> <li>Training through amortized variational inference  </li> <li>Gradients computed using the reparameterization trick</li> </ul> <p>VAEs were introduced in 2014 by Kingma &amp; Welling and Rezende et al., and marked a major breakthrough in tractable, scalable generative modeling.</p>"},{"location":"deeplearning/8_latentvariables/#62-vae-model-components","title":"6.2 VAE Model Components","text":""},{"location":"deeplearning/8_latentvariables/#prior","title":"Prior","text":"<p>Usually a factorized standard Gaussian:  </p>"},{"location":"deeplearning/8_latentvariables/#likelihood-decoder","title":"Likelihood / Decoder","text":"<p>Maps latents to a distribution over observations.</p> <p>For binary data:  </p> <p>For real-valued data:  </p>"},{"location":"deeplearning/8_latentvariables/#variational-posterior-encoder","title":"Variational Posterior / Encoder","text":"\\[ q_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu_\\phi(x), \\sigma_\\phi^2(x)) \\] <p>All of these functions (encoder &amp; decoder) can be implemented with: - MLPs - ConvNets - ResNets - Transformers depending on the domain.</p>"},{"location":"deeplearning/8_latentvariables/#63-training-objective-the-elbo","title":"6.3 Training Objective: The ELBO","text":"<p>VAEs maximize the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}(x) = \\mathbb{E}_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - D_{\\text{KL}}\\!\\Big(q_\\phi(z\\mid x)\\,\\|\\, p(z)\\Big) \\] <p>Interpretation:</p> <ol> <li> <p>Reconstruction Term    Measures how well the model predicts \\(x\\) from \\(z\\).    Encourages informative latents.</p> </li> <li> <p>KL Regularization Term    Encourages \\(q_\\phi(z\\mid x)\\) to stay close to the prior \\(p(z)\\).    Prevents overfitting and encourages smooth latent spaces.</p> </li> </ol> <p>The KL term often has closed-form for Gaussian distributions.</p>"},{"location":"deeplearning/8_latentvariables/#64-reparameterization-trick-key-to-vaes","title":"6.4 Reparameterization Trick (Key to VAEs)","text":"<p>Direct backprop through a sample \\(z \\sim q_\\phi(z\\mid x)\\) is impossible.</p> <p>Solution: rewrite sampling as a differentiable transformation of noise:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>This allows gradient flow through \\(z\\) and makes VAE training practical.</p>"},{"location":"deeplearning/8_latentvariables/#65-vae-as-a-framework","title":"6.5 VAE as a Framework","text":"<p>The term \u201cVAE\u201d now refers to a broad family of models: - Continuous latent variables - Amortized inference - Reparameterization-based gradients - Trained by maximizing ELBO (or its variants)</p> <p>Modern VAEs extend the basic version in many ways: - Multiple latent layers - More expressive posteriors (flows, mixtures) - More expressive priors (hierarchical, autoregressive) - More expressive decoders (ResNets, autoregressive PixelCNN decoders) - Iterative inference networks - Variance reduction techniques</p> <p>The VAE framework is flexible and underlies many state-of-the-art generative models.</p>"},{"location":"deeplearning/8_latentvariables/#66-summary-of-section-6","title":"6.6 Summary of Section 6","text":"<ul> <li>VAEs are tractable generative models with continuous latent variables.</li> <li>They pair:</li> <li>a decoder \\(p_\\theta(x\\mid z)\\) and  </li> <li>an encoder \\(q_\\phi(z\\mid x)\\)   using amortized VI.</li> <li>Training uses ELBO + reparameterization trick.</li> <li>VAEs balance reconstruction quality with regularized latent structure.</li> <li>The VAE framework is highly extensible and central to modern deep generative modeling.</li> </ul>"},{"location":"deeplearning/alogirthmic_detials/","title":"Alogirthmic detials","text":"<ol> <li>Batch normalization</li> <li>contrastive loss</li> <li>Sematic segmentation</li> <li>class, bounding box</li> <li>poly nn</li> <li>Representation learning</li> </ol>"},{"location":"deeplearning/plan/","title":"Plan","text":"<p>diffusion  GAN</p> <p>articles</p>"},{"location":"distributedsystems/0_intro/","title":"Introduction","text":""},{"location":"distributedsystems/0_intro/#introduction","title":"Introduction","text":"<p>What is \"distributed system\":</p> <p>A group of computers cooperating to provide a service</p>"},{"location":"distributedsystems/0_intro/#why","title":"Why?","text":"<ol> <li>to increase capacity via parallel processing</li> <li>to tolerate faults via replication</li> <li>to match distribution of physical devices e.g. sensors</li> <li>to increase security via isolation</li> </ol>"},{"location":"distributedsystems/0_intro/#challanges","title":"Challanges:","text":"<ul> <li>concurrency</li> <li>complex interactions</li> <li>performance bottlenecks</li> <li>partial failure</li> </ul>"},{"location":"distributedsystems/0_intro/#key-topics","title":"Key Topics","text":""},{"location":"distributedsystems/0_intro/#fault-tolerance","title":"Fault tolerance:","text":"<ul> <li>1000s of servers, big network -&gt; always something broken</li> <li>We'd like to hide these failures from the application.</li> <li>\"High availability\": service continues despite failures</li> <li>Big idea: replicated servers. If one server crashes, can proceed using the other(s).</li> </ul>"},{"location":"distributedsystems/0_intro/#consistency","title":"Consistency:","text":"<ul> <li>General-purpose infrastructure needs well-defined behavior. E.g. \"read(x) yields the value from the most recent write(x).\"</li> <li>Achieving good behavior is hard! e.g. \"replica\" servers are hard to keep identical.</li> </ul>"},{"location":"distributedsystems/0_intro/#performance","title":"Performance:","text":"<ul> <li>The goal: scalable throughput. Nx servers -&gt; Nx total throughput via parallel CPU, RAM, disk, net.</li> <li>Scaling gets harder as N grows:<ul> <li>Load imbalance.</li> <li>Slowest-of-N latency.</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#tradeoffs","title":"Tradeoffs:","text":"<ul> <li>Fault-tolerance, consistency, and performance are enemies.</li> <li>Fault tolerance and consistency require communication<ul> <li>e.g., send data to backup server</li> <li>e.g., check if cached data is up-to-date</li> <li>communication is often slow and non-scalable</li> </ul> </li> <li>Many designs sacrifice consistency to gain speed.<ul> <li>e.g. read(x) might not yield the latest write(x)!</li> <li>Painful for application programmers (or users).</li> </ul> </li> </ul>"},{"location":"distributedsystems/0_intro/#implementation","title":"Implementation:","text":"<ul> <li>RPC, threads, concurrency control, configuration.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/","title":"1. MapReduce","text":""},{"location":"distributedsystems/1_mapreduce/#mapreduce-a-complete-guide","title":"MapReduce: A Complete Guide","text":""},{"location":"distributedsystems/1_mapreduce/#introduction","title":"Introduction","text":"<p>Modern data analysis often involves multi-hour computations on multi-terabyte datasets \u2014  for example, building search indexes, sorting massive logs, or analyzing web graphs.  Such tasks are only practical using thousands of computers working in parallel.</p> <p>MapReduce (MR) is a programming model designed to make large-scale data processing  easy for non-specialist programmers. It lets you write simple sequential code, while the framework handles parallel execution, fault tolerance, and data distribution.</p>"},{"location":"distributedsystems/1_mapreduce/#core-concept","title":"Core Concept","text":"<p>The programmer defines just two functions:</p> <ul> <li><code>Map()</code> \u2013 processes input data and emits key-value pairs.</li> <li><code>Reduce()</code> \u2013 aggregates or summarizes all values associated with a given key.</li> </ul> <p>Everything else \u2014 input splitting, task scheduling, network communication, and fault recovery \u2014  is handled automatically by the MapReduce framework.</p>"},{"location":"distributedsystems/1_mapreduce/#how-mapreduce-works-word-count-example","title":"How MapReduce Works (Word Count Example)","text":""},{"location":"distributedsystems/1_mapreduce/#abstract-view","title":"Abstract View","text":"<pre><code>Input1 -&gt; Map -&gt; a,1 b,1\nInput2 -&gt; Map -&gt;     b,1\nInput3 -&gt; Map -&gt; a,1     c,1\n                    |   |   |\n                    |   |   -&gt; Reduce -&gt; c,1\n                    |   -----&gt; Reduce -&gt; b,2\n                    ---------&gt; Reduce -&gt; a,2\n</code></pre>"},{"location":"distributedsystems/1_mapreduce/#steps","title":"Steps","text":"<ol> <li>Input Splitting \u2014 Data is divided into <code>M</code> splits (files or blocks).</li> <li>Map Phase \u2014 Each split is processed by a Map task, generating <code>(key, value)</code> pairs.</li> <li>Shuffle Phase \u2014 Intermediate pairs are grouped by key and distributed to Reduce tasks.</li> <li>Reduce Phase \u2014 Each Reduce task processes one group and outputs final results.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#word-count-example","title":"Word Count Example","text":"<pre><code># Map function\ndef Map(document):\n    words = document.split()\n    for word in words:\n        emit(word, 1)\n\n# Reduce function\ndef Reduce(word, values):\n    emit(word, sum(values))\n</code></pre> <p>Final Output: </p><pre><code>a: 2\nb: 2\nc: 1\n</code></pre><p></p>"},{"location":"distributedsystems/1_mapreduce/#why-mapreduce-scales-so-well","title":"Why MapReduce Scales So Well","text":"<ul> <li>Parallelism: Map and Reduce tasks run independently, enabling massive parallelism.</li> <li>Automatic Management: The framework handles failures, scheduling, and communication.</li> <li>Simplicity: Developers only implement <code>Map()</code> and <code>Reduce()</code>.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#input-output-storage-via-gfs","title":"Input &amp; Output Storage (via GFS)","text":"<p>MapReduce typically uses a distributed file system such as Google File System (GFS).</p> <ul> <li>Files split into 64 MB chunks, distributed across many servers.</li> <li>Maps read input in parallel; Reduces write output in parallel.</li> <li>Replication (2\u20133 copies) ensures fault tolerance.</li> <li>Data locality: Tasks are often scheduled on the same machine where their data resides.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#inside-the-mapreduce-framework","title":"Inside the MapReduce Framework","text":""},{"location":"distributedsystems/1_mapreduce/#coordinators-role","title":"Coordinator\u2019s Role","text":"<ol> <li>Map Phase</li> <li>Assigns Map tasks to workers.</li> <li>Each Map writes intermediate output to its local disk.</li> <li> <p>Intermediate data is partitioned by <code>hash(key) mod R</code> (R = number of Reduces).</p> </li> <li> <p>Reduce Phase</p> </li> <li>Coordinator assigns Reduce tasks.</li> <li>Each Reduce fetches its partition (bucket) from all Maps.</li> <li> <p>Sorts data by key and processes each group.</p> </li> <li> <p>Output</p> </li> <li>Each Reduce writes its final output to GFS.</li> </ol>"},{"location":"distributedsystems/1_mapreduce/#performance-and-bottlenecks","title":"Performance and Bottlenecks","text":""},{"location":"distributedsystems/1_mapreduce/#what-limits-performance","title":"What Limits Performance?","text":"<p>Often, network speed is the main bottleneck \u2014 not CPU or disk speed.</p> <p>Network Transfers Include:</p> <ul> <li>Maps reading input from GFS.</li> <li>Reduces fetching intermediate (shuffled) data from Maps.</li> <li>Reduces writing output to GFS.</li> </ul> <p>Because the shuffle phase may move data as large as the original input, network optimization is critical.</p>"},{"location":"distributedsystems/1_mapreduce/#network-optimizations","title":"Network Optimizations","text":"<ul> <li>Data Locality: Run Map tasks where their input data is stored.</li> <li>Single Network Transfer: Intermediate data stored locally, not in GFS.</li> <li>Hash Partitioning: Reduces transfer large data batches (buckets), minimizing small transfers.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#load-balancing","title":"Load Balancing","text":"<p>Why it matters:  Uneven load causes idle workers waiting for \u201cstragglers\u201d. Solution:  </p> <ul> <li>Create many more tasks than workers.</li> <li>The Coordinator dynamically assigns tasks to free workers.</li> <li>Faster machines handle more tasks; slower ones handle fewer.</li> </ul> <p>This keeps the cluster well-balanced and efficient.</p>"},{"location":"distributedsystems/1_mapreduce/#fault-tolerance","title":"Fault Tolerance","text":"<p>Failures are expected in large clusters. MapReduce handles them gracefully.</p>"},{"location":"distributedsystems/1_mapreduce/#worker-failures","title":"Worker Failures","text":"<ul> <li> <p>Map worker crash:</p> </li> <li> <p>Intermediate data (stored locally) is lost.</p> </li> <li>Coordinator reassigns those Map tasks to new workers.</li> <li> <p>No need to rerun if Reduces already fetched the data.</p> </li> <li> <p>Reduce worker crash:</p> </li> <li> <p>Completed results are safe (stored in GFS).</p> </li> <li>Unfinished Reduce tasks are rerun elsewhere.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#deterministic-functions-required","title":"Deterministic Functions Required","text":"<p>Because tasks may be re-executed:</p> <ul> <li><code>Map()</code> and <code>Reduce()</code> must be pure functions \u2014 deterministic and side-effect-free.</li> <li>No external state, random numbers, or I/O beyond the framework.</li> </ul> <p>This guarantees identical results across re-runs.</p>"},{"location":"distributedsystems/1_mapreduce/#handling-other-failures","title":"Handling Other Failures","text":"<ul> <li>Duplicate task execution:   Coordinator accepts output from only one instance.</li> <li>Simultaneous Reduce outputs:   GFS\u2019s atomic rename ensures one consistent final file.</li> <li>Stragglers:   Coordinator launches backup copies of slow tasks.</li> <li>Corrupted output or bad hardware:   Not handled \u2014 MR assumes fail-stop (crash, not corrupt) behavior.</li> <li>Coordinator crash:   Not fully addressed in the original paper.</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-works-well","title":"Where MapReduce Works Well","text":"<p>Ideal Use Cases:</p> <ul> <li>Batch processing of huge datasets (TB\u2013PB scale)</li> <li>Log analysis (e.g., counting queries, clickstream analytics)</li> <li>Index building for search engines</li> <li>Data transformations (ETL pipelines)</li> <li>Large-scale machine learning preprocessing</li> <li>Sorting and aggregation across distributed data</li> </ul> <p>These workloads share common traits:</p> <ul> <li>Large, independent input records</li> <li>Deterministic, parallel-friendly computation</li> <li>No need for real-time feedback</li> </ul>"},{"location":"distributedsystems/1_mapreduce/#where-mapreduce-falls-short","title":"Where MapReduce Falls Short","text":"<p>Not Suitable For:</p> <ul> <li> <p>Real-time or streaming data processing   MR is inherently batch-oriented; results appear only after job completion.</p> </li> <li> <p>Interactive querying   Jobs take minutes to hours; unsuitable for low-latency analytics.</p> </li> <li> <p>Iterative algorithms   Machine learning or graph algorithms (e.g., PageRank, K-means) need multiple    passes over data, causing heavy I/O.</p> </li> <li> <p>Stateful or dependent tasks   MR disallows inter-task communication or shared state.</p> </li> <li> <p>Small or medium datasets   Overhead of distributing tasks outweighs benefits.</p> </li> </ul> <p>Modern systems like Apache Spark, Flink, or Beam were designed to overcome these limitations by enabling in-memory and streaming computation.</p>"},{"location":"distributedsystems/2_threads/","title":"2. Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#65840-lecture-2-2025-threads-and-rpc","title":"6.5840 \u2014 Lecture 2 (2025): Threads and RPC","text":""},{"location":"distributedsystems/2_threads/#introduction-implementing-distributed-systems","title":"Introduction: Implementing Distributed Systems","text":"<p>This lecture introduces: - Go threads (goroutines) - Concurrency challenges - The web crawler example - Remote Procedure Calls (RPC)</p> <p>Go is the language used for this writeup.</p>"},{"location":"distributedsystems/2_threads/#why-go","title":"Why Go?","text":"<p>Go is well-suited for distributed systems:</p> <ul> <li>Excellent thread (goroutine) support  </li> <li>Convenient RPC library</li> <li>Type- and memory-safe</li> <li>Garbage-collected (safe with concurrency)</li> <li>Simpler than many other languages</li> <li>Commonly used in production distributed systems</li> </ul> <p>\ud83d\udc49 After the tutorial, read Effective Go: https://golang.org/doc/effective_go.html</p>"},{"location":"distributedsystems/2_threads/#threads-goroutines","title":"Threads (Goroutines)","text":""},{"location":"distributedsystems/2_threads/#what-is-a-thread","title":"What is a Thread?","text":"<p>A thread is a \u201cthread of execution\u201d:</p> <ul> <li>Allows a program to do multiple things at once</li> <li>Executes sequentially (like a program), but shares memory with other threads</li> <li>Has its own program counter, registers, and stack</li> </ul> <p>In Go, threads are called goroutines.</p>"},{"location":"distributedsystems/2_threads/#why-use-threads","title":"Why Use Threads?","text":"<p>Three type of threads:</p>"},{"location":"distributedsystems/2_threads/#1-io-concurrency","title":"1. I/O Concurrency","text":"<ul> <li>Client sends requests to many servers at once  </li> <li>Server handles many clients concurrently  </li> <li>When one thread blocks on I/O, another can run</li> </ul>"},{"location":"distributedsystems/2_threads/#2-multicore-performance","title":"2. Multicore Performance","text":"<p>Use multiple CPU cores simultaneously.</p>"},{"location":"distributedsystems/2_threads/#3-convenience","title":"3. Convenience","text":"<p>Run background tasks (e.g., periodic health checks).</p>"},{"location":"distributedsystems/2_threads/#alternative-event-driven-systems","title":"Alternative: Event-Driven Systems","text":"<p>Instead of threads:</p> <ul> <li>Use a single-threaded system with an event loop</li> <li>Explicitly interleave different activities</li> <li>Maintain state tables for each ongoing operation</li> </ul> <p>Pros:  </p> <ul> <li>Good for I/O concurrency  </li> <li>No thread overhead</li> </ul> <p>Cons:  </p> <ul> <li>No multicore usage  </li> <li>Hard to program and maintain</li> </ul>"},{"location":"distributedsystems/2_threads/#threading-challenges","title":"Threading Challenges","text":""},{"location":"distributedsystems/2_threads/#1-safe-data-sharing","title":"1. Safe Data Sharing","text":"<p>Race example: </p><pre><code>n = n + 1\n</code></pre> Two threads modifying <code>n</code> at the same time \u2192 race condition.<p></p> <p>A race is when: - Two threads access the same memory - At least one is a write - And there's no synchronization</p> <p>Fixes: - Use <code>sync.Mutex</code> - Avoid sharing mutable data</p>"},{"location":"distributedsystems/2_threads/#2-coordination-producerconsumer","title":"2. Coordination (Producer\u2013Consumer)","text":"<ul> <li>One thread produces data  </li> <li>Another consumes it  </li> <li>Need a way for consumers to wait and wake up</li> </ul> <p>Tools: - Go channels - <code>sync.Cond</code> - <code>sync.Wait</code>, <code>sync.WaitGroup</code></p>"},{"location":"distributedsystems/2_threads/#3-deadlock","title":"3. Deadlock","text":"<p>When threads wait on each other forever. Can happen via:</p> <ul> <li>Locks</li> <li>Channels</li> <li>RPC</li> </ul>"},{"location":"distributedsystems/2_threads/#web-crawler-example","title":"Web Crawler Example","text":"<p>A web crawler:</p> <ul> <li>Fetches web pages recursively starting from a URL</li> <li>Follows links</li> <li>Avoids revisiting pages</li> <li>Avoids cycles</li> <li>Exploits I/O concurrency for speed</li> </ul>"},{"location":"distributedsystems/2_threads/#1-serial-crawler","title":"1. Serial Crawler","text":"<ul> <li>Depth-first traversal  </li> <li>A shared map tracks visited URLs  </li> <li>Simple and correct  </li> <li>Very slow \u2014 only fetches one page at a time  </li> </ul> <p>Adding <code>go</code> before recursive calls breaks correctness:</p> <ul> <li>Many threads may fetch same URL  </li> <li>Finishing detection becomes difficult</li> </ul>"},{"location":"distributedsystems/2_threads/#2-concurrent-crawler-with-mutex","title":"2. Concurrent Crawler with Mutex","text":""},{"location":"distributedsystems/2_threads/#how-it-works","title":"How it Works","text":"<ul> <li>Launch a goroutine per page</li> <li>Shared <code>fetched</code> map  </li> <li>Mutex ensures only one thread fetches each URL</li> </ul>"},{"location":"distributedsystems/2_threads/#why-the-mutex","title":"Why the Mutex?","text":""},{"location":"distributedsystems/2_threads/#1-avoid-logical-races","title":"1. Avoid Logical Races","text":"<p>Two threads may check the same URL at once: - Both see <code>fetched[url] == false</code> - Both fetch \u2192 wrong</p> <p>Mutex ensures: - One thread checks + sets at a time</p>"},{"location":"distributedsystems/2_threads/#2-avoid-map-corruption","title":"2. Avoid Map Corruption","text":"<p>Go maps are not thread-safe.</p>"},{"location":"distributedsystems/2_threads/#what-if-lock-is-removed","title":"What If Lock Is Removed?","text":"<ul> <li>Program may appear to work sometimes  </li> <li>But races still occur  </li> <li>Use the race detector:</li> </ul> <pre><code>go run -race crawler.go\n</code></pre>"},{"location":"distributedsystems/2_threads/#completion-detection-using-syncwaitgroup","title":"Completion Detection Using sync.WaitGroup","text":"<ul> <li><code>Add(n)</code> increments  </li> <li><code>Done()</code> decrements  </li> <li><code>Wait()</code> blocks until count is zero  </li> </ul> <p>Ensures main thread waits for all children.</p>"},{"location":"distributedsystems/2_threads/#3-concurrent-crawler-with-channels","title":"3. Concurrent Crawler with Channels","text":"<p>Channels provide:</p> <ul> <li>Communication  </li> <li>Synchronization  </li> </ul>"},{"location":"distributedsystems/2_threads/#channel-basics","title":"Channel Basics","text":"<pre><code>ch := make(chan int)\n\nch &lt;- x   // send (blocks)\ny := &lt;-ch // receive (blocks)\n</code></pre>"},{"location":"distributedsystems/2_threads/#coordinator-workers-model","title":"Coordinator + Workers Model","text":"<ul> <li>Coordinator creates workers via goroutines  </li> <li>Workers fetch a page and send resulting URLs via channel  </li> <li>Coordinator receives URLs, checks visited set</li> </ul>"},{"location":"distributedsystems/2_threads/#why-no-mutex","title":"Why No Mutex?","text":"<ul> <li>Shared state is only in coordinator  </li> <li>Workers never mutate shared maps  </li> <li>Therefore no races</li> </ul>"},{"location":"distributedsystems/2_threads/#channel-safety","title":"Channel Safety","text":"<p>Example:</p> <ul> <li>Worker creates slice of URLs</li> <li>Sends it to channel</li> <li>Coordinator reads it</li> </ul> <p>Safe because:</p> <ul> <li>Worker writes slice before send completes</li> <li>Coordinator reads slice after receive completes</li> </ul> <p>No overlap \u2192 no race.</p>"},{"location":"distributedsystems/2_threads/#why-some-sends-need-a-goroutine","title":"Why Some Sends Need a Goroutine?","text":"<p>Without a goroutine:</p> <ul> <li>send blocks  </li> <li>coordinator may not reach the receive  </li> <li>\u2192 deadlock</li> </ul>"},{"location":"distributedsystems/2_threads/#locks-vs-channels","title":"Locks vs Channels","text":"<p>Both are powerful. Use whichever matches intuition:</p> <ul> <li>State-focused logic \u2192 locks</li> <li>Communication-focused logic \u2192 channels</li> </ul> <p>In 6.5840 labs: - Use sharing + locks for state - Use channels, <code>sync.Cond</code>, or sleep-based polling for notifications</p>"},{"location":"distributedsystems/2_threads/#remote-procedure-call-rpc","title":"Remote Procedure Call (RPC)","text":"<p>RPC enables easy client-server communication.</p>"},{"location":"distributedsystems/2_threads/#goals","title":"Goals","text":"<ul> <li>Hide network details</li> <li>Provide a procedure-call interface</li> <li>Automatically marshal/unmarshal data</li> <li>Enable portability across systems</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-architecture","title":"RPC Architecture","text":"<pre><code>Client               Server\n  request ----&gt;\n            &lt;---- response\n</code></pre> <p>Software structure: </p><pre><code>Client App        Server Handlers\nClient Stubs      Dispatcher\nRPC Library  ---- RPC Library\n Network     ---- Network\n</code></pre><p></p>"},{"location":"distributedsystems/2_threads/#go-rpc-example-keyvalue-store","title":"Go RPC Example: Key/Value Store","text":"<p>Handlers: - <code>Put(key, value)</code> - <code>Get(key) -&gt; value</code></p>"},{"location":"distributedsystems/2_threads/#client-side","title":"Client Side","text":"<ul> <li>Use <code>Dial()</code> to connect  </li> <li>Call RPC using:</li> </ul> <pre><code>Call(\"KVServer.Get\", args, &amp;reply)\n</code></pre> <p>RPC library: - Marshals args - Sends request - Waits for reply - Unmarshals reply - Returns error if something went wrong  </p>"},{"location":"distributedsystems/2_threads/#server-side","title":"Server Side","text":"<p>Server must: 1. Declare a type with exported RPC methods 2. Register the type 3. Accept TCP connections and let RPC library handle them  </p> <p>RPC library:</p> <ul> <li>Creates goroutine per request  </li> <li>Unmarshals request  </li> <li>Dispatches handler  </li> <li>Marshals reply  </li> <li>Sends reply  </li> </ul> <p>Handlers must use locks since multiple RPCs run concurrently.</p>"},{"location":"distributedsystems/2_threads/#rpc-details","title":"RPC Details","text":""},{"location":"distributedsystems/2_threads/#binding","title":"Binding","text":"<p>Client must know <code>\"server:port\"</code> to dial.</p>"},{"location":"distributedsystems/2_threads/#marshalling-rules","title":"Marshalling Rules","text":"<ul> <li>Sends strings, arrays, structs, maps  </li> <li>Cannot send channels or functions  </li> <li>Only exported fields in structs are marshaled  </li> <li>Pointers are sent by copying the pointee</li> </ul>"},{"location":"distributedsystems/2_threads/#rpc-failures","title":"RPC Failures","text":"<p>Client may never get a reply:</p> <p>Could mean:</p> <ul> <li>Server never received request  </li> <li>Server crashed after executing  </li> <li>Reply lost in network  </li> <li>Network or server slow  </li> </ul> <p>RPC \u2260 local function call.</p>"},{"location":"distributedsystems/2_threads/#best-effort-rpc","title":"Best-Effort RPC","text":"<p>Algorithm: 1. Send request 2. Wait 3. If no reply, resend 4. After several tries \u2192 give up  </p>"},{"location":"distributedsystems/2_threads/#problems","title":"Problems","text":"<p>Example: </p><pre><code>Put(\"k\", 10)\nPut(\"k\", 20)\n</code></pre><p></p> <p>Retries can reorder or duplicate operations.</p>"},{"location":"distributedsystems/2_threads/#when-is-best-effort-ok","title":"When Is Best-Effort OK?","text":"<ul> <li>Read-only operations  </li> <li>Idempotent operations (safe to repeat)</li> </ul>"},{"location":"distributedsystems/2_threads/#at-most-once-semantics","title":"At-Most-Once Semantics","text":"<p>Go RPC provides:</p> <ul> <li>One TCP connection  </li> <li>Sends each request once  </li> <li>No retries \u2192 no duplicates  </li> </ul> <p>But:</p> <ul> <li>Errors returned on timeouts  </li> <li>Hard to build replicated fault-tolerant systems without retries  </li> </ul> <p>Later labs explore stronger semantics.</p>"},{"location":"informationtheory/1_intro_to_infotheory/","title":"1. Introduction","text":""},{"location":"informationtheory/1_intro_to_infotheory/#chapter-1-introduction-to-information-theory-for-machine-learning","title":"Chapter 1 \u2014 Introduction to Information Theory (for Machine Learning)","text":"<p>Information theory provides a mathematical foundation for uncertainty, compression, communication, and learning. In modern ML and DL, information theory underlies:</p> <ul> <li>loss functions (cross-entropy, NLL)</li> <li>representation learning and contrastive learning</li> <li>variational inference and VAEs</li> <li>generative modeling (GANs, flows, diffusion)</li> <li>reinforcement learning (entropy bonuses, policy KL constraints)</li> <li>model capacity, generalization, and bottlenecks</li> </ul> <p>This chapter introduces the core motivations and conceptual tools.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#1-why-information-theory-matters-for-ml","title":"1. Why Information Theory Matters for ML","text":"<p>Information theory answers questions fundamental to ML:</p> <ul> <li>How much uncertainty does a model reduce?</li> <li>How do we quantify the difference between two probability distributions?</li> <li>How do we measure dependence between variables?</li> <li>What is the maximum information a neural network layer can transmit?</li> <li>How do we formalize compression and generalization?</li> </ul> <p>In ML, information theory is not abstract mathematics \u2014 it provides the language for describing learning itself:</p> <p>Learning = finding distributions that compress data optimally  while preserving information relevant for prediction.</p> <p>This viewpoint unifies:</p> <ul> <li>Maximum likelihood  </li> <li>Variational inference  </li> <li>Contrastive learning  </li> <li>GAN objectives  </li> <li>Representation learning  </li> <li>Reinforcement learning signal shaping  </li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#2-the-communication-view-shannons-formulation","title":"2. The Communication View (Shannon\u2019s Formulation)","text":"<p>A classical communication system consists of:</p> <ol> <li> <p>Source:    Generates data (symbols, images, text, states).</p> </li> <li> <p>Encoder: Transforms data into a compressed or structured representation (ML analogy: neural encoders, feature extraction, token embedding).</p> </li> <li> <p>Channel: Communication medium; may be noisy or bandwidth-limited  (ML analogy: stochastic layers, dropout, variational noise).</p> </li> <li> <p>Decoder: Reconstructs the data (ML analogy: neural decoders, autoregressive models).</p> </li> <li> <p>Receiver:   Obtains the final predictions or reconstructions.</p> </li> </ol> <p>Information theory studies:</p> <ul> <li>Limits of efficient communication  </li> <li>Optimal encoding and representation  </li> <li>Tradeoffs between compression and fidelity  </li> <li>Effect of noise on learnability</li> </ul>"},{"location":"informationtheory/1_intro_to_infotheory/#3-the-uncertainty-view-shannonbayesian-perspective","title":"3. The Uncertainty View (Shannon\u2013Bayesian Perspective)","text":"<p>Information theory also quantifies uncertainty:</p> <ul> <li>More uncertainty \u2192 more information needed  </li> <li>Less uncertainty \u2192 easier prediction and compression  </li> </ul> <p>Key idea: Information is the reduction of uncertainty.</p> <p>In ML:</p> <ul> <li>Entropy measures label uncertainty  </li> <li>Cross-entropy measures model fit  </li> <li>KL divergence measures mismatch  </li> <li>Mutual information measures representation quality  </li> <li>ELBO measures how well a generative model explains data  </li> </ul> <p>Thus, learning and compression are mathematically the same problem.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#4-machine-learning-as-communication","title":"4. Machine Learning as Communication","text":"<p>Modern ML pipelines resemble a communication system:</p>"},{"location":"informationtheory/1_intro_to_infotheory/#data-encoder-latent-representation-decoder-output","title":"Data \u2192 Encoder \u2192 Latent Representation \u2192 Decoder \u2192 Output","text":"<p>Examples:</p> <ul> <li>Autoencoders / VAEs: compress \\(x\\) into \\(z\\), then reconstruct</li> <li>Transformers: compress sequences into features, decode predictions</li> <li>Contrastive models (SimCLR, CPC): maximize MI between views of data</li> <li>GANs: learn generator distributions close to data distribution</li> <li>RL agents: compress sensory input into state representations</li> </ul> <p>Thus, the principles governing communication capacity, coding, and noise apply directly to network design.</p>"},{"location":"informationtheory/1_intro_to_infotheory/#5-roadmap-for-this-web-book","title":"5. Roadmap for This Web-book","text":"<p>This web-book is structured to build information theory specifically for ML:</p> <ol> <li> <p>Entropy &amp; Self-Information    Foundations of uncertainty, coding length, and compression.</p> </li> <li> <p>Cross-Entropy &amp; Negative Log-Likelihood    Core ML loss; the bridge between probability and training objectives.</p> </li> <li> <p>KL Divergence &amp; f-Divergences    Quantifying model mismatch, VI, GAN divergences.</p> </li> <li> <p>Jensen\u2013Shannon &amp; Wasserstein Distances    GAN stability, geometric learning, distribution metrics.</p> </li> <li> <p>Mutual Information &amp; Estimation Bounds    Representation learning, contrastive learning, InfoNCE.</p> </li> <li> <p>Variational Inference &amp; ELBO    VAEs, Bayesian deep learning, posterior approximations.</p> </li> <li> <p>Information Bottleneck &amp; Representation Theory    Why deep networks compress, and how representations generalize.</p> </li> <li> <p>Summary &amp; Concept Map    Unifying view of entropy \u2192 KL \u2192 MI \u2192 VI \u2192 representation learning.</p> </li> </ol>"},{"location":"informationtheory/2_entropy/","title":"2. Entropy, Self-Information & Cross-Entropy","text":""},{"location":"informationtheory/2_entropy/#chapter-2-entropy-self-information-cross-entropy-information-measures","title":"Chapter 2 \u2014 Entropy, Self-Information, Cross-Entropy &amp; Information Measures","text":""},{"location":"informationtheory/2_entropy/#1-self-information-surprisal","title":"1. Self-Information (Surprisal)","text":"<p>Self-information quantifies the surprise of observing an event.</p> <p>For an event with probability \\(p(x)\\):</p> \\[ I(x) = - \\log_2 p(x) \\] <p>Why the log?</p> <ul> <li>Additivity of independent events  </li> <li>Probability \u2192 information monotonicity  </li> <li>Log base 2 \u2192 units in bits  </li> <li>Log-likelihoods become additive \u2192 ML becomes convex (in many models)</li> </ul> <p>Interpretations:</p> <ul> <li>Unlikely events carry more information  </li> <li>Certain events carry zero information  </li> <li>Foundation of cross-entropy and negative log-likelihood  </li> </ul> <p>In ML:  </p> <ul> <li>The loss used in classification is simply the surprisal of the correct class.</li> </ul>"},{"location":"informationtheory/2_entropy/#2-entropy-expected-uncertainty","title":"2. Entropy \u2014 Expected Uncertainty","text":"<p>Entropy is the expected self-information:</p> \\[ H(X) = -\\sum_x p(x) \\log p(x) \\] <p>Entropy measures:</p> <ul> <li>Uncertainty  </li> <li>Randomness  </li> <li>Compressibility  </li> <li>Difficulty of prediction  </li> </ul>"},{"location":"informationtheory/2_entropy/#key-properties","title":"Key properties:","text":"<ul> <li>\\(H(X) = 0\\) if a variable is deterministic  </li> <li>Maximum when distribution is uniform  </li> <li>Upper bound on achievable compression (Shannon)</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation","title":"ML Interpretation:","text":"<ul> <li>High entropy labels \u2192 noisy dataset \u2192 harder learning</li> <li>Activation entropy reflects network expressiveness</li> <li>Entropy of output distribution measures model confidence</li> <li>Entropy regularization improves exploration in RL</li> </ul>"},{"location":"informationtheory/2_entropy/#3-differential-entropy-continuous-entropy","title":"3. Differential Entropy (Continuous Entropy)","text":"<p>For continuous variables:</p> \\[ h(X) = -\\int p(x) \\log p(x)\\,dx \\] <p>Important differences:</p> <ul> <li>Can be negative</li> <li>Not invariant under reparameterization</li> <li>Not comparable between different coordinate systems</li> </ul>"},{"location":"informationtheory/2_entropy/#why-it-matters-in-ml","title":"Why it matters in ML:","text":"<ul> <li>VAEs use continuous latent variables \\(z\\)</li> <li>Flows and diffusion models use continuous densities</li> <li>Score-based models estimate gradients of log-densities, not densities directly</li> </ul> <p>Differential entropy is not the same thing as Shannon entropy \u2014 a common source of confusion.</p>"},{"location":"informationtheory/2_entropy/#4-joint-conditional-and-total-entropy","title":"4. Joint, Conditional, and Total Entropy","text":""},{"location":"informationtheory/2_entropy/#joint-entropy","title":"Joint entropy:","text":"\\[ H(X,Y) = -\\sum_{x,y} p(x,y)\\log p(x,y) \\]"},{"location":"informationtheory/2_entropy/#conditional-entropy","title":"Conditional entropy:","text":"\\[ H(Y|X) = -\\sum_{x,y} p(x,y)\\log p(y|x) \\] <p>Interpretation:</p> <ul> <li>Average residual uncertainty in \\(Y\\) after observing \\(X\\)</li> </ul>"},{"location":"informationtheory/2_entropy/#chain-rule-of-entropy","title":"Chain rule of entropy:","text":"\\[ H(X,Y) = H(X) + H(Y|X) \\] <p>This rule is foundational for:</p> <ul> <li>Autoregressive modeling  </li> <li>Sequence modeling  </li> <li>Transformers (predictive factorization)  </li> <li>Bayesian networks  </li> </ul>"},{"location":"informationtheory/2_entropy/#5-cross-entropy-coding-p-using-q","title":"5. Cross-Entropy \u2014 Coding \\(p\\) Using \\(q\\)","text":"<p>Cross-entropy is the expected surprise under model \\(q\\):</p> \\[ H(p, q) = -\\sum_x p(x)\\log q(x) \\]"},{"location":"informationtheory/2_entropy/#crucial-identity","title":"Crucial identity:","text":"\\[ H(p, q) = H(p) + D_{\\text{KL}}(p\\|q) \\] <p>Meaning:</p> <ul> <li>True entropy + penalty for using the wrong distribution</li> <li>Cross-entropy \u2265 entropy</li> </ul>"},{"location":"informationtheory/2_entropy/#ml-interpretation_1","title":"ML Interpretation:","text":"<p>Cross-entropy = Negative Log Likelihood:</p> \\[ \\mathcal{L} = - \\log q(y_{\\text{true}}) \\] <p>This powers:</p> <ul> <li>Softmax classifiers  </li> <li>Logistic regression  </li> <li>Transformers (next-token prediction)  </li> <li>Language models (autoregressive LM)  </li> <li>Image segmentation (pixel-wise CE)  </li> </ul> <p>Minimizing cross-entropy is equivalent to making model probabilities match the data distribution.</p>"},{"location":"informationtheory/2_entropy/#6-perplexity-entropy-in-language-modeling","title":"6. Perplexity \u2014 Entropy in Language Modeling","text":"<p>Perplexity is:</p> \\[ \\text{PPL} = 2^{H} \\] <p>Interpretation:</p> <ul> <li>The \u201ceffective vocabulary size\u201d the model thinks it must guess from</li> <li>Lower perplexity = better language model</li> </ul> <p>Transformers and LLMs are explicitly evaluated using this entropy-derived metric.</p>"},{"location":"informationtheory/2_entropy/#7-mutual-information-information-shared-between-variables","title":"7. Mutual Information \u2014 Information Shared Between Variables","text":"\\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)) \\] <p>MI measures:</p> <ul> <li>How much knowing \\(X\\) tells us about \\(Y\\)</li> <li>Reduction in entropy of one variable after observing the other</li> </ul>"},{"location":"informationtheory/2_entropy/#equivalent-forms","title":"Equivalent forms:","text":"\\[ I(X;Y) = H(X) - H(X|Y) \\] \\[ I(X;Y) = H(X) + H(Y) - H(X,Y) \\] <p>MI links entropy and KL divergence into a unified measure of dependence.</p>"},{"location":"informationtheory/2_entropy/#why-mi-is-critical-in-ml","title":"Why MI is critical in ML:","text":"<ul> <li>Representation learning (maximize MI with labels)</li> <li>Contrastive learning (InfoNCE is a lower bound to MI)</li> <li>InfoGAN (maximize MI between latent code and output)</li> <li>Feature selection (choose features with highest MI to labels)</li> <li>Stochastic encoders control MI with constraints</li> </ul>"},{"location":"informationtheory/2_entropy/#8-the-data-processing-inequality-dpi","title":"8. The Data Processing Inequality (DPI)","text":"<p>If:</p> \\[ X \\rightarrow Z \\rightarrow Y \\] <p>is a Markov chain, then:</p> \\[ I(X;Y) \\le I(X;Z) \\] <p>Meaning:</p> <ul> <li>Processing or compressing data cannot add information</li> <li>Neural networks cannot create information about the input   \u2014 they can only discard or transform it</li> </ul> <p>ML relevance:</p> <ul> <li>Explains why deeper layers become more task-specialized  </li> <li>Supports the Information Bottleneck theory in deep learning  </li> <li>Ensures that any learned representation is bounded by input information  </li> <li>Helps analyze generalization and compression in deep nets</li> </ul>"},{"location":"informationtheory/2_entropy/#9-entropy-in-neural-networks","title":"9. Entropy in Neural Networks","text":"<p>Entropy plays multiple roles in deep learning:</p>"},{"location":"informationtheory/2_entropy/#output-entropy","title":"Output entropy","text":"<p>Low entropy \u2192 confident predictions High entropy \u2192 uncertainty</p>"},{"location":"informationtheory/2_entropy/#entropy-of-hidden-representations","title":"Entropy of hidden representations","text":"<ul> <li>Early layers: reduce entropy (denoising)  </li> <li>Deep layers: compress irrelevant information  </li> <li>Good representations retain low entropy but high MI with labels</li> </ul>"},{"location":"informationtheory/2_entropy/#entropy-regularization-in-rl","title":"Entropy regularization in RL","text":"<p>  encourages exploration.</p>"},{"location":"informationtheory/2_entropy/#dropout-increases-entropy","title":"Dropout increases entropy","text":"<p>forcing models to encode more robust representations.</p>"},{"location":"informationtheory/3_KL/","title":"3. Kullback-Leibler Divergence","text":"<p>Chapter 3 \u2014 KL Divergence, f-Divergences, Jensen\u2013Shannon Divergence, and Wasserstein Distance</p> <p>This chapter introduces the major ways to quantify how different two probability distributions are. These measures underpin many areas of modern machine learning, including generative models (VAEs, GANs, flows), reinforcement learning, Bayesian inference, and representation learning. The goal is to build an intuitive and mathematical understanding suitable for a beginner, while still maintaining the depth needed for practical ML reasoning.</p>"},{"location":"informationtheory/3_KL/#1-kl-divergence-measuring-distribution-mismatch","title":"1. KL Divergence: Measuring Distribution Mismatch","text":"<p>The Kullback\u2013Leibler (KL) divergence measures how different two probability distributions are. For distributions \\(p\\) and \\(q\\):</p> \\[ D_{\\text{KL}}(p\\|q) = \\sum_x p(x)\\log\\frac{p(x)}{q(x)}. \\] <p>KL divergence quantifies the inefficiency incurred when encoding samples drawn from \\(p\\) using a code optimized for \\(q\\). If \\(q\\) assigns very low probability to events that occur frequently under \\(p\\), the KL divergence becomes large.</p>"},{"location":"informationtheory/3_KL/#key-properties-of-kl-divergence","title":"Key properties of KL divergence","text":"<ol> <li> <p>Non-negative </p> </li> <li> <p>Zero only when the two distributions are identical.</p> </li> <li> <p>Asymmetric </p> </li> <li> <p>Not a true metric, since it fails the triangle inequality.</p> </li> <li> <p>Can be infinite when \\(p(x) &gt; 0\\) but \\(q(x) = 0\\).    This is a crucial issue in generative modeling, where such mismatches occur frequently.</p> </li> </ol>"},{"location":"informationtheory/3_KL/#2-kl-divergence-in-machine-learning","title":"2. KL Divergence in Machine Learning","text":"<p>KL divergence appears throughout machine learning, often in subtle ways. The direction of KL used in an algorithm profoundly affects how the resulting model behaves.</p>"},{"location":"informationtheory/3_KL/#21-maximum-likelihood-as-forward-kl-minimization","title":"2.1 Maximum likelihood as forward KL minimization","text":"<p>Training a model by maximum likelihood is equivalent to minimizing the forward KL divergence:</p> \\[ \\theta^* = \\arg\\min_\\theta D_{\\text{KL}}(p_{\\text{data}} \\,\\|\\, q_\\theta). \\] <p>The model is penalized heavily for failing to assign probability mass to any region where real data occurs. As a result, maximum-likelihood models attempt to cover all modes of the data distribution.</p> <p>This produces mode-covering behavior, which is characteristic of:</p> <ul> <li>normalizing flows  </li> <li>autoregressive models  </li> <li>density estimation models trained via log-likelihood  </li> </ul>"},{"location":"informationtheory/3_KL/#22-kl-divergence-in-variational-inference-vi","title":"2.2 KL divergence in variational inference (VI)","text":"<p>Variational inference relies on minimizing the reverse KL divergence between an approximate posterior \\(q(z|x)\\) and the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Since the true posterior is typically intractable, VAEs approximate this with:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>Reverse KL heavily penalizes placing probability mass in regions where the target distribution has little or none. This leads the model to concentrate on a single high-density mode and avoid uncertain areas.</p> <p>This behavior is known as mode seeking. In VAEs, it contributes to smooth or blurry reconstructions, because the model often collapses to a conservative \u201csafe\u201d solution.</p>"},{"location":"informationtheory/3_KL/#23-kl-divergence-in-reinforcement-learning","title":"2.3 KL divergence in reinforcement learning","text":"<p>Modern policy gradient methods constrain policy updates using KL divergence. For example, TRPO and PPO penalize large deviations between the previous policy and the new one:</p> \\[ D_{\\text{KL}}(\\pi_{\\text{old}} \\,\\|\\, \\pi_{\\text{new}}). \\] <p>This keeps learning stable by preventing abrupt policy changes that might harm performance.</p>"},{"location":"informationtheory/3_KL/#24-kl-divergence-in-distillation-and-compression","title":"2.4 KL divergence in distillation and compression","text":"<p>KL divergence compares two probability distributions directly and is used for:</p> <ul> <li>teacher\u2013student distillation  </li> <li>compressing large models into smaller ones  </li> <li>aligning probability distributions across layers  </li> <li>calibrating output probabilities  </li> </ul> <p>Whenever we want one model to imitate another, KL divergence naturally appears.</p>"},{"location":"informationtheory/3_KL/#3-understanding-kl-behavior-mode-covering-vs-mode-seeking","title":"3. Understanding KL Behavior: Mode Covering vs. Mode Seeking","text":"<p>The two directions of KL divergence behave very differently. Understanding this distinction is central to understanding why VAEs blur, GANs collapse, and flows cover all modes.</p>"},{"location":"informationtheory/3_KL/#forward-kl-d_textklpq","title":"Forward KL: \\(D_{\\text{KL}}(p\\|q)\\)","text":"<p>(Used in maximum likelihood, flows \u2192 mode covering)</p> <p>Forward KL asks whether the model \\(q\\) assigns sufficient probability wherever the data distribution \\(p\\) has mass:</p> <p>\u201cDoes the model assign enough probability to every place where the data occurs?\u201d</p> <p>If \\(q\\) misses even a small region where \\(p\\) has mass, the divergence becomes very large. The model is therefore encouraged to spread probability across all data modes.</p> <p>Result: mode covering The model covers every part of the data distribution, even rare modes. It tolerates false positives (assigning probability where there is no data) but avoids false negatives (missing data modes).</p> <p>Flows and MLE-based models display this behavior.</p>"},{"location":"informationtheory/3_KL/#reverse-kl-d_textklqp","title":"Reverse KL: \\(D_{\\text{KL}}(q\\|p)\\)","text":"<p>(Used in VI, VAEs, GAN-like behavior \u2192 mode seeking)</p> <p>Reverse KL asks the opposite question:</p> <p>\u201cIs the model placing probability in places where the data distribution is very small or zero?\u201d</p> <p>Reverse KL heavily penalizes placing mass in low-density regions of \\(p\\), making the model conservative.</p> <p>Result: mode seeking The model places most of its mass at a single safe mode, often ignoring minor modes. This produces sharp or collapsed samples, depending on the context.</p> <p>VAEs, many VI methods, and GAN-like formulations exhibit mode seeking.</p>"},{"location":"informationtheory/3_KL/#4-f-divergences-a-unified-family-of-divergences","title":"4. f-Divergences: A Unified Family of Divergences","text":"<p>KL divergence belongs to a larger family called f-divergences. An f-divergence is defined by a convex function \\(f\\):</p> \\[ D_f(p\\|q) = \\sum_x q(x)\\, f\\!\\left(\\frac{p(x)}{q(x)}\\right). \\]"},{"location":"informationtheory/3_KL/#5-jensenshannon-divergence-the-original-gan-divergence","title":"5. Jensen\u2013Shannon Divergence: The Original GAN Divergence","text":"<p>The Jensen\u2013Shannon (JS) divergence measures how different two distributions are using a mixture distribution:</p> \\[ \\text{JS}(p\\|q) = \\frac12 D_{\\text{KL}}(p\\|m) + \\frac12 D_{\\text{KL}}(q\\|m) \\] <p>where the mixture is:</p> \\[ m = \\frac12(p+q). \\] <p>JS divergence is symmetric and always lies between 0 and \\(\\log 2\\).</p>"},{"location":"informationtheory/3_KL/#why-js-appears-in-gans","title":"Why JS appears in GANs","text":"<p>GANs train a discriminator using binary cross entropy. When the discriminator is trained to optimality, the resulting generator objective becomes:</p> \\[ \\text{JS}(p\\|q) - \\log 2. \\] <p>Thus, GANs naturally minimize JS divergence without explicitly choosing it. This symmetry and boundedness initially made JS seem ideal.</p>"},{"location":"informationtheory/3_KL/#6-why-js-divergence-causes-gan-instability","title":"6. Why JS Divergence Causes GAN Instability","text":"<p>At the beginning of GAN training, real samples and generated samples usually do not overlap. When the supports of \\(p\\) and \\(q\\) are disjoint:</p> \\[ \\text{JS}(p\\|q) = \\log 2. \\] <p>In this regime, JS divergence becomes constant and the gradient becomes zero.</p> <p>Consequences:</p> <ol> <li>The discriminator immediately becomes perfect.  </li> <li>The generator stops receiving meaningful gradients.  </li> <li>Training often collapses, oscillates, or diverges.  </li> </ol> <p>This gradient-vanishing problem motivated the development of Wasserstein GANs.</p>"},{"location":"informationtheory/3_KL/#7-total-variation-and-hellinger-distances","title":"7. Total Variation and Hellinger Distances","text":"<p>Unlike KL or JS, these are true metrics: symmetric, finite, and geometrically meaningful.</p>"},{"location":"informationtheory/3_KL/#71-total-variation-tv-distance","title":"7.1 Total Variation (TV) Distance","text":"\\[ \\text{TV}(p,q) = \\frac12\\sum_x |p(x)-q(x)|. \\] <p>TV measures the maximum possible difference in probabilities assigned to events by the two distributions. It corresponds to the minimum amount of probability mass that must be moved to transform \\(p\\) into \\(q\\).</p> <p>Applications in ML:</p> <ul> <li>Robustness under distribution shift  </li> <li>Generalization bounds (PAC-Bayes)  </li> <li>Fairness and safety  </li> </ul>"},{"location":"informationtheory/3_KL/#72-hellinger-distance","title":"7.2 Hellinger Distance","text":"\\[ H^2(p,q) = \\frac12 \\sum_x\\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^2. \\] <p>Hellinger distance compares the square roots of probabilities, producing a smooth and bounded measure between 0 and 1.</p> <p>Uses in ML include:</p> <ul> <li>Robust statistics  </li> <li>Domain adaptation  </li> <li>Generalization theory  </li> <li>Some GAN formulations  </li> </ul>"},{"location":"informationtheory/3_KL/#8-wasserstein-distance-geometry-of-probability-distributions","title":"8. Wasserstein Distance: Geometry of Probability Distributions","text":"<p>The Wasserstein-1 (Earth Mover) distance measures how much work is needed to move probability mass from one distribution to another.</p>"},{"location":"informationtheory/3_KL/#81-primal-form-earth-mover-interpretation","title":"8.1 Primal form (Earth Mover interpretation)","text":"\\[ W(p,q) = \\inf_{\\gamma \\in \\Gamma(p,q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]. \\] <p>It seeks the transport plan \\(\\gamma\\) requiring the least expected effort to turn \\(p\\) into \\(q\\).</p>"},{"location":"informationtheory/3_KL/#82-dual-form-used-in-wgan","title":"8.2 Dual form (used in WGAN)","text":"\\[ W(p,q) = \\sup_{\\|f\\|_L\\le 1} \\left(\\mathbb{E}_p[f(x)]      - \\mathbb{E}_q[f(x)]\\right). \\] <p>GANs implement \\(f\\) as a neural network called a critic. The critic must be 1-Lipschitz to ensure stable gradients.</p>"},{"location":"informationtheory/3_KL/#83-why-wasserstein-solves-gan-instability","title":"8.3 Why Wasserstein solves GAN instability","text":"<p>Wasserstein distance has several advantages:</p> <ul> <li>Provides informative gradients even with no overlap  </li> <li>Reflects the actual geometry of the data space  </li> <li>Avoids the saturation and vanishing gradients of JS divergence  </li> <li>Works reliably in high-dimensional spaces  </li> </ul> <p>These properties make Wasserstein GANs far more stable than classical GANs.</p>"},{"location":"informationtheory/3_KL/#84-wgan-gp-gradient-penalty","title":"8.4 WGAN-GP: Gradient Penalty","text":"<p>To enforce the Lipschitz condition, WGAN-GP adds a gradient penalty:</p> \\[ \\lambda(\\|\\nabla_x f(x)\\|_2 - 1)^2. \\] <p>This produces smoother and more stable training compared to weight clipping.</p>"},{"location":"informationtheory/3_KL/#9-divergence-versus-distance","title":"9. Divergence versus Distance","text":"<p>Divergences such as KL and JS:</p> <ul> <li>may be infinite  </li> <li>are asymmetric  </li> <li>do not behave well when distributions have disjoint support  </li> </ul> <p>Distances such as Wasserstein, TV, and Hellinger:</p> <ul> <li>are symmetric  </li> <li>obey triangle inequality  </li> <li>remain meaningful under distribution shift  </li> </ul> <p>In machine learning:</p> <ul> <li>Divergences are useful for inference and likelihood  </li> <li>Distances are useful for generative modeling and geometry  </li> </ul>"},{"location":"informationtheory/3_KL/#10-why-divergences-fail-in-high-dimensions","title":"10. Why Divergences Fail in High Dimensions","text":"<p>In high-dimensional spaces:</p> <ul> <li>Real and generated samples rarely overlap  </li> <li>KL divergence often becomes infinite  </li> <li>JS divergence becomes flat  </li> <li>Gradients vanish  </li> </ul> <p>Wasserstein distance solves these issues by relying on geometric structure rather than probability ratios.</p> <p>KL divergence quantifies mismatch between distributions and plays a central role in likelihood-based learning, variational inference, reinforcement learning, and distillation. The choice between forward and reverse KL determines whether a model exhibits mode-covering or mode-seeking behavior.</p> <p>The f-divergence family generalizes KL and provides a unified view of GAN objectives. Jensen\u2013Shannon divergence arises naturally in classical GAN training but suffers from gradient-vanishing problems when real and fake data do not overlap.</p> <p>Total Variation and Hellinger distances offer robust, metric-based ways to compare distributions. Wasserstein distance introduces a geometric perspective that overcomes the limitations of KL and JS, enabling stable GAN training via WGAN and WGAN-GP.</p>"},{"location":"informationtheory/4_bayes/","title":"4. Bayesian Inference","text":"<p>Bayesian inference provides a principled framework for reasoning about uncertainty in machine learning models. It describes how to update beliefs about hidden variables when new data is observed. Many modern generative models, including VAEs and diffusion models, are based on Bayesian ideas, and variational inference is a direct approximation to Bayesian posterior inference.</p> <p>This chapter introduces the core concepts of Bayesian inference, why posterior inference is difficult, and how these ideas set the stage for variational inference and the ELBO in the next chapter.</p>"},{"location":"informationtheory/4_bayes/#1-bayes-rule","title":"1. Bayes\u2019 Rule","text":"<p>Bayes\u2019 theorem relates prior beliefs, likelihoods, and posterior beliefs. For a hidden variable \\(z\\) and an observed variable \\(x\\):</p> \\[ p(z|x) = \\frac{p(x|z)\\,p(z)}{p(x)}. \\] <p>Each term has a clear interpretation.</p> <ul> <li>\\(p(z)\\): prior belief about the unknown variable  </li> <li>\\(p(x|z)\\): likelihood of observing \\(x\\) given \\(z\\) </li> <li>\\(p(x)\\): marginal likelihood or evidence  </li> <li>\\(p(z|x)\\): posterior distribution after observing data  </li> </ul> <p>Bayesian inference is the task of computing \\(p(z|x)\\).</p>"},{"location":"informationtheory/4_bayes/#2-priors-encoding-assumptions-about-hidden-variables","title":"2. Priors: Encoding Assumptions About Hidden Variables","text":"<p>The prior \\(p(z)\\) expresses what we believe about the latent variable before observing the data. Priors serve several purposes in machine learning.</p>"},{"location":"informationtheory/4_bayes/#21-regularization","title":"2.1 Regularization","text":"<p>A prior can prevent overfitting. For example, a Gaussian prior on weights yields \\(L_2\\) regularization.</p>"},{"location":"informationtheory/4_bayes/#22-structural-assumptions","title":"2.2 Structural assumptions","text":"<p>Priors can encode assumptions such as smoothness, sparsity, or low-dimensional structure.</p>"},{"location":"informationtheory/4_bayes/#23-uncertainty","title":"2.3 Uncertainty","text":"<p>The prior makes explicit that before observing data, we do not know the true value of \\(z\\).</p>"},{"location":"informationtheory/4_bayes/#24-generative-modeling","title":"2.4 Generative modeling","text":"<p>In latent-variable models like VAEs, the prior determines the structure of the latent space.</p>"},{"location":"informationtheory/4_bayes/#3-likelihood-connecting-latent-variables-to-observed-data","title":"3. Likelihood: Connecting Latent Variables to Observed Data","text":"<p>The likelihood \\(p(x|z)\\) describes how the data are generated from latent causes. In many generative models:</p> <ul> <li>\\(z\\) represents latent structure  </li> <li>\\(x\\) represents an image, time series, or text  </li> <li>\\(p(x|z)\\) is parameterized by a neural network decoder  </li> </ul> <p>The likelihood term encourages the latent variable \\(z\\) to explain the observed data.</p>"},{"location":"informationtheory/4_bayes/#4-the-posterior-what-we-really-want-to-compute","title":"4. The Posterior: What We Really Want to Compute","text":"<p>The goal of Bayesian inference is the posterior:</p> \\[ p(z|x) = \\frac{p(x|z)p(z)}{p(x)}. \\] <p>The posterior expresses how our belief about \\(z\\) changes after seeing \\(x\\). It incorporates both:</p> <ul> <li>prior knowledge  </li> <li>evidence from data  </li> </ul> <p>Unfortunately, computing this posterior is usually intractable.</p>"},{"location":"informationtheory/4_bayes/#5-why-exact-inference-is-hard","title":"5. Why Exact Inference Is Hard","text":"<p>The denominator in Bayes\u2019 rule is the marginal likelihood:</p> \\[ p(x) = \\int p(x,z)\\,dz. \\] <p>This integral is often impossible to evaluate directly because:</p> <ul> <li>the latent space \\(z\\) can be high-dimensional  </li> <li>the joint distribution \\(p(x,z)\\) may involve a complex neural network  </li> <li>the integral has no analytic form  </li> </ul> <p>Computing the exact posterior is rarely feasible in modern models. This makes approximate inference essential.</p>"},{"location":"informationtheory/4_bayes/#6-maximum-a-posteriori-map-vs-full-bayesian-inference","title":"6. Maximum a Posteriori (MAP) vs Full Bayesian Inference","text":"<p>There are two kinds of Bayesian computation.</p>"},{"location":"informationtheory/4_bayes/#61-map-estimation","title":"6.1 MAP estimation","text":"<p>MAP finds the single most likely value of \\(z\\):</p> \\[ z_{\\text{MAP}} = \\arg\\max_z\\, p(z|x). \\] <p>MAP is similar to maximum likelihood but includes the prior. MAP is easier to compute but does not provide uncertainty.</p>"},{"location":"informationtheory/4_bayes/#62-full-posterior-inference","title":"6.2 Full posterior inference","text":"<p>The full posterior \\(p(z|x)\\) describes a distribution over possible values of \\(z\\), reflecting uncertainty. Most Bayesian methods aim for the full posterior, not MAP. However, because it is intractable, we approximate it.</p>"},{"location":"informationtheory/4_bayes/#7-bayesian-latent-variable-models","title":"7. Bayesian Latent-Variable Models","text":"<p>Many generative models are Bayesian latent-variable models with:</p> <ol> <li> <p>a prior over latent variables </p> </li> <li> <p>a conditional likelihood </p> </li> <li> <p>a posterior </p> </li> </ol> <p>Examples include:</p> <ul> <li>VAEs  </li> <li>mixture models  </li> <li>topic models  </li> <li>probabilistic PCA  </li> <li>diffusion models (in a specific sense)  </li> </ul> <p>Bayesian inference is the foundation of these models.</p>"},{"location":"informationtheory/4_bayes/#8-the-evidence-and-its-importance","title":"8. The Evidence and Its Importance","text":"<p>The marginal likelihood, also called the evidence:</p> \\[ p(x) = \\int p(x,z)\\,dz \\] <p>plays several roles:</p> <ul> <li>It normalizes the posterior.  </li> <li>It evaluates how well a model explains data.  </li> <li>It is used in Bayesian model comparison.  </li> <li>Its logarithm appears in training objectives for VAEs and diffusion models.</li> </ul> <p>Maximizing evidence corresponds to learning a model that explains the data well.</p>"},{"location":"informationtheory/4_bayes/#9-bayesian-interpretation-of-kl-divergence","title":"9. Bayesian Interpretation of KL Divergence","text":"<p>KL divergence naturally appears when comparing an approximate posterior \\(q(z|x)\\) with the true posterior \\(p(z|x)\\):</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Minimizing this KL divergence means making the approximation \\(q\\) as close as possible to the exact posterior.</p> <p>This forms the basis of variational inference.</p>"},{"location":"informationtheory/4_bayes/#10-why-we-need-variational-inference","title":"10. Why We Need Variational Inference","text":"<p>Because the true posterior is intractable, we introduce a simpler distribution \\(q(z|x)\\) and optimize it to approximate \\(p(z|x)\\).</p> <p>We cannot compute:</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)) \\] <p>directly, because \\(p(z|x)\\) depends on \\(p(x)\\), which is the intractable integral.</p> <p>Variational inference resolves this by rewriting \\(\\log p(x)\\) and isolating the KL divergence from quantities we can compute. This leads to the Evidence Lower Bound (ELBO), which forms the training objective of VAEs.</p> <p>This is the topic of the next chapter.</p> <p>Bayesian inference describes how to update beliefs in light of new evidence using Bayes\u2019 rule. The posterior distribution combines the prior and likelihood to capture all information about latent variables. However, direct computation of the posterior is often intractable due to the marginal likelihood integral.</p> <p>Approximate inference methods are therefore necessary. Variational inference replaces the true posterior with a tractable approximation and optimizes it by minimizing KL divergence. Understanding Bayesian inference is essential for understanding the ELBO, VAEs, Bayesian neural networks, and modern probabilistic deep learning methods.</p>"},{"location":"informationtheory/5_mc_intro/","title":"5. Probability toolbox","text":"<p>Many problems in machine learning require computing expectations, marginal likelihoods, or posterior distributions of the form</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}, \\qquad  p(x) = \\int p(x,z)\\,dz. \\] <p>For most realistic models, the integral in the denominator is intractable. Modern machine learning therefore relies on several approximation strategies, each with different assumptions, strengths, and limitations. These approaches form a probability toolbox for inference.</p> <p>This section introduces four major families of methods:</p> <ol> <li>complete enumeration  </li> <li>Laplace approximation  </li> <li>Monte Carlo methods  </li> <li>variational methods  </li> </ol> <p>Subsequent chapters expand on these ideas, beginning with a deeper discussion of Monte Carlo sampling.</p>"},{"location":"informationtheory/5_mc_intro/#1-complete-enumeration","title":"1. Complete Enumeration","text":"<p>Complete enumeration computes the integral exactly by summing or integrating over all possible latent configurations:</p> \\[ p(x) = \\sum_z p(x,z) \\quad \\text{or} \\quad p(x) = \\int p(x,z)\\,dz. \\] <p>This is feasible only when:</p> <ul> <li>the latent variable is low dimensional  </li> <li>the domain is small or discrete  </li> <li>the joint distribution has a simple closed form  </li> </ul> <p>Although conceptually straightforward, complete enumeration becomes impossible as dimensionality increases. It serves mainly as a theoretical reference point.</p>"},{"location":"informationtheory/5_mc_intro/#2-laplace-approximation","title":"2. Laplace Approximation","text":"<p>The Laplace method approximates an intractable posterior by a Gaussian distribution centered at its mode.</p> <p>Given a posterior</p> \\[ p(z|x) \\propto p(x,z), \\] <p>the Laplace approximation fits a Gaussian distribution</p> \\[ q(z|x) \\approx \\mathcal{N}(z_{\\text{MAP}}, H^{-1}), \\] <p>where:</p> <ul> <li>\\(z_{\\text{MAP}}\\) is the mode of \\(p(z|x)\\) </li> <li>\\(H\\) is the Hessian of \\(-\\log p(z|x)\\) at the mode  </li> </ul> <p>This method assumes the posterior is approximately unimodal and locally Gaussian. It is fast and easy to compute, but may be inaccurate when the posterior is skewed or multimodal.</p>"},{"location":"informationtheory/5_mc_intro/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":"<p>Monte Carlo methods approximate integrals using random samples. The central idea is:</p> \\[ \\mathbb{E}_{p(z|x)}[f(z)]  \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\qquad z_i \\sim p(z|x). \\] <p>Monte Carlo estimators do not require closed-form integrals and scale well to high dimensions. They are widely used in Bayesian inference, reinforcement learning, generative modeling, and probabilistic programming.</p> <p>Sampling strategies fall into two groups:</p> <ul> <li>independent sampling  </li> <li>Markov chain\u2013based sampling (MCMC)  </li> </ul> <p>The next chapter explains Monte Carlo and sampling methods in detail.</p>"},{"location":"informationtheory/5_mc_intro/#4-variational-methods","title":"4. Variational Methods","text":"<p>Variational methods replace an intractable posterior with a tractable family of approximations. Instead of sampling directly from \\(p(z|x)\\), we introduce a distribution \\(q(z|x)\\) and optimize it to be close to the true posterior. The objective is to minimize</p> \\[ D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) is unknown, variational inference rewrites this quantity using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x) + D_{\\text{KL}}(q(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian inference. Variational methods power VAEs, Bayesian neural networks, diffusion models, and many modern probabilistic approaches.</p>"},{"location":"informationtheory/5_mc_intro/#summary","title":"Summary","text":"<p>Approximate inference methods can be understood as four major strategies:</p> <ul> <li>complete enumeration: exact but rarely feasible  </li> <li>Laplace approximation: fast Gaussian approximation near the mode  </li> <li>Monte Carlo methods: sampling-based numerical estimation  </li> <li>variational methods: optimization-based posterior approximation  </li> </ul> <p>Monte Carlo sampling is the most flexible approach and serves as the backbone of Bayesian computation. The next chapter develops Monte Carlo and sampling techniques in detail.</p>"},{"location":"informationtheory/6_mc/","title":"6. Monte Carlo Methods","text":"<p>Sampling methods provide numerical techniques for approximating integrals, expectations, and posterior distributions that are analytically intractable. They are an essential component of Bayesian inference and appear in many areas of machine learning, including reinforcement learning, probabilistic modeling, and generative models.</p> <p>This chapter introduces sampling in a structured sequence, beginning with independent sampling, progressing to Monte Carlo estimation, extending to Markov chain Monte Carlo (MCMC), and concluding with advanced techniques and ML-specific applications.</p>"},{"location":"informationtheory/6_mc/#1-the-goal-of-sampling","title":"1. The Goal of Sampling","text":"<p>Many problems require computing expectations of the form</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\int f(z)\\,p(z)\\,dz, \\] <p>or evaluating posterior quantities such as</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>Direct computation is rarely feasible because the integral may be high-dimensional or have no closed form.</p> <p>Sampling provides a way to approximate these quantities using draws from the distribution.</p>"},{"location":"informationtheory/6_mc/#2-independent-sampling","title":"2. Independent Sampling","text":"<p>Independent sampling methods produce samples where each draw does not depend on the previous one.</p> <p>These methods work best when:</p> <ul> <li>sampling directly from \\(p(z)\\) is tractable  </li> <li>the distribution is low-dimensional  </li> <li>the support is simple (e.g., Gaussian, uniform)  </li> </ul>"},{"location":"informationtheory/6_mc/#21-direct-sampling","title":"2.1 Direct Sampling","text":"<p>When the distribution has an invertible CDF \\(F(z)\\):</p> <ol> <li>sample \\(u \\sim \\text{Uniform}(0,1)\\) </li> <li>compute \\(z = F^{-1}(u)\\) </li> </ol> <p>This yields exact samples. It is commonly used in:</p> <ul> <li>uniform sampling  </li> <li>exponential distributions  </li> <li>simple discrete distributions  </li> </ul>"},{"location":"informationtheory/6_mc/#22-importance-sampling","title":"2.2 Importance Sampling","text":"<p>When sampling from \\(p(z)\\) is difficult but evaluating \\(p(z)\\) is easy, importance sampling draws samples from a proposal distribution \\(q(z)\\) and reweights them:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] = \\mathbb{E}_{q(z)}\\left[f(z)\\frac{p(z)}{q(z)}\\right]. \\] <p>Importance sampling is widely used for:</p> <ul> <li>likelihood estimation  </li> <li>off-policy reinforcement learning  </li> <li>correcting distribution mismatch  </li> </ul> <p>It suffers when \\(p(z)/q(z)\\) has high variance.</p>"},{"location":"informationtheory/6_mc/#23-rejection-sampling","title":"2.3 Rejection Sampling","text":"<p>Rejection sampling uses a proposal distribution \\(q(z)\\) and a constant \\(M\\) such that</p> \\[ p(z) \\le M q(z) \\quad \\text{for all } z. \\] <p>Procedure:</p> <ol> <li>sample \\(z \\sim q(z)\\) </li> <li>accept with probability \\(\\frac{p(z)}{M q(z)}\\) </li> </ol> <p>It produces exact samples from \\(p(z)\\), but can be extremely inefficient in high dimensions.</p>"},{"location":"informationtheory/6_mc/#3-monte-carlo-estimation","title":"3. Monte Carlo Estimation","text":"<p>Monte Carlo approximates expectations by:</p> \\[ \\mathbb{E}_{p(z)}[f(z)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(z_i), \\quad z_i \\sim p(z). \\] <p>Key properties:</p> <ul> <li>error scales as \\(\\mathcal{O}(1/\\sqrt{N})\\) </li> <li>works in high dimensions  </li> <li>accuracy depends on sampling quality  </li> </ul> <p>Monte Carlo is the backbone of almost all probabilistic computation.</p>"},{"location":"informationtheory/6_mc/#4-markov-chain-monte-carlo-mcmc","title":"4. Markov Chain Monte Carlo (MCMC)","text":"<p>When sampling directly from \\(p(z)\\) is hard, Markov Chain Monte Carlo constructs a Markov chain</p> \\[ z_1 \\to z_2 \\to z_3 \\to \\cdots \\] <p>whose stationary distribution is \\(p(z)\\).</p> <p>After a burn-in period, samples approximate \\(p(z)\\) even if individual states are dependent.</p> <p>MCMC is widely applicable because it does not require the normalization constant of \\(p(z)\\):</p> \\[ p(z|x) \\propto p(x,z). \\]"},{"location":"informationtheory/6_mc/#41-metropolishastings-algorithm","title":"4.1 Metropolis\u2013Hastings Algorithm","text":"<p>Given the current state \\(z\\), propose \\(z'\\) using a proposal distribution \\(q(z'|z)\\). Accept with probability:</p> \\[ \\alpha = \\min\\left(1,  \\frac{p(z')\\,q(z|z')}      {p(z)\\,q(z'|z)} \\right). \\] <p>If accepted, set \\(z_{t+1} = z'\\), otherwise keep \\(z_{t+1} = z\\).</p> <p>Metropolis\u2013Hastings forms the foundation for most MCMC methods.</p>"},{"location":"informationtheory/6_mc/#42-gibbs-sampling","title":"4.2 Gibbs Sampling","text":"<p>Gibbs sampling updates one variable at a time by sampling from its conditional distribution:</p> \\[ z_i \\sim p(z_i \\mid z_{-i}). \\] <p>This requires all conditionals to be tractable.</p> <p>Applications include:</p> <ul> <li>topic models (LDA)  </li> <li>hidden Markov models  </li> <li>Bayesian networks  </li> </ul>"},{"location":"informationtheory/6_mc/#43-slice-sampling","title":"4.3 Slice Sampling","text":"<p>Slice sampling chooses a height \\(u\\) and samples uniformly along the slice:</p> \\[ \\{z : p(z) &gt; u \\}. \\] <p>It adapts automatically to the local shape of the distribution and requires minimal tuning.</p>"},{"location":"informationtheory/6_mc/#5-reducing-random-walk-behaviour","title":"5. Reducing Random-Walk Behaviour","text":"<p>Basic MCMC methods suffer from slow exploration due to random-walk behavior. Advanced methods reduce this inefficiency.</p>"},{"location":"informationtheory/6_mc/#51-hamiltonian-monte-carlo-hmc","title":"5.1 Hamiltonian Monte Carlo (HMC)","text":"<p>HMC introduces momentum variables and uses Hamiltonian dynamics to propose long-distance moves with high acceptance probability.</p> <p>Advantages:</p> <ul> <li>avoids random walk behaviour  </li> <li>efficient in high dimensions  </li> <li>uses gradients of \\(\\log p(z)\\) </li> </ul> <p>HMC is widely used in probabilistic programming systems (Stan, PyMC).</p>"},{"location":"informationtheory/6_mc/#52-overrelaxation","title":"5.2 Overrelaxation","text":"<p>Overrelaxation proposes samples that are negatively correlated with the previous ones, improving mixing speed.</p> <p>Sampling methods approximate expectations and posterior distributions when closed-form solutions are unavailable. Independent methods such as importance and rejection sampling are simple but limited. Monte Carlo estimation provides a general framework for approximating integrals, and MCMC allows sampling from complex, high-dimensional distributions by constructing Markov chains. Advanced methods such as Hamiltonian Monte Carlo improve mixing and efficiency.</p> <p>Sampling is a central tool for Bayesian inference and underlies many modern machine learning models, from deep generative architectures to reinforcement learning algorithms.</p>"},{"location":"informationtheory/7a_vi_intro/","title":"8. Optimization-Based Inference","text":""},{"location":"informationtheory/7a_vi_intro/#chapter-optimization-based-inference-map-em-and-the-path-to-variational-inference","title":"Chapter \u2014 Optimization-Based Inference: MAP, EM, and the Path to Variational Inference","text":"<p>Monte Carlo methods provide a sampling-based approach to approximate expectations and posterior distributions. Although sampling is flexible and asymptotically exact, it can be computationally expensive, difficult to tune, or slow to converge in high dimensions. For many models, especially those involving latent variables or large datasets, it is more practical to replace sampling with optimization.</p> <p>This chapter introduces three optimization-based inference strategies:</p> <ol> <li>Maximum a posteriori (MAP) estimation  </li> <li>Expectation\u2013Maximization (EM)  </li> <li>Variational inference (VI), in its simplest introductory form  </li> </ol> <p>Together, these methods motivate the full treatment of variational inference in the following chapter.</p>"},{"location":"informationtheory/7a_vi_intro/#1-motivation-for-optimization-based-inference","title":"1. Motivation for Optimization-Based Inference","text":"<p>Bayesian inference requires the posterior</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The challenge lies in computing the marginal likelihood</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is almost always intractable. Monte Carlo sampling approximates this integral using samples, but sampling may be slow or unreliable for:</p> <ul> <li>high-dimensional latent spaces  </li> <li>multimodal posteriors  </li> <li>large datasets  </li> <li>models requiring gradient-based learning  </li> </ul> <p>This motivates an alternative strategy: instead of drawing samples, we can transform inference into an optimization problem.</p>"},{"location":"informationtheory/7a_vi_intro/#2-maximum-a-posteriori-map-estimation","title":"2. Maximum A Posteriori (MAP) Estimation","text":"<p>MAP estimation finds the most likely value of a latent variable or parameter after observing the data. Starting from Bayes\u2019 rule:</p> \\[ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}, \\] <p>MAP chooses the mode of the posterior:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta p(\\theta|x). \\] <p>Since \\(p(x)\\) does not depend on \\(\\theta\\), this is equivalent to:</p> \\[ \\theta_{\\text{MAP}}  = \\arg\\max_\\theta \\big[ \\log p(x|\\theta) + \\log p(\\theta) \\big]. \\] <p>MAP is efficient and easy to compute. It reduces inference to optimization and incorporates prior knowledge through \\(p(\\theta)\\). However, it returns only a point estimate and does not capture uncertainty.</p> <p>MAP is thus a limited but useful form of Bayesian inference, often interpreted as maximum likelihood augmented with a regularization term.</p>"},{"location":"informationtheory/7a_vi_intro/#3-expectationmaximization-em","title":"3. Expectation\u2013Maximization (EM)","text":"<p>EM is designed for models with latent variables. The log-likelihood of the observed data is:</p> \\[ \\log p_\\theta(x)  = \\log \\sum_z p_\\theta(x,z). \\] <p>Direct optimization is difficult because of the sum over latent variables. EM solves this using two alternating steps:</p>"},{"location":"informationtheory/7a_vi_intro/#e-step","title":"E-step","text":"<p>Compute the posterior over latent variables under the current parameters:</p> \\[ q(z) = p_\\theta(z|x). \\]"},{"location":"informationtheory/7a_vi_intro/#m-step","title":"M-step","text":"<p>Maximize the expected complete-data log-likelihood:</p> \\[ \\theta \\leftarrow  \\arg\\max_\\theta  \\mathbb{E}_{q(z)}[\\log p_\\theta(x,z)]. \\] <p>EM guarantees that the likelihood increases with each iteration. It is widely used in:</p> <ul> <li>mixture of Gaussians  </li> <li>hidden Markov models  </li> <li>probabilistic PCA  </li> <li>clustering and density estimation  </li> </ul> <p>EM can be interpreted as a form of variational inference where the variational distribution is constrained to be the exact posterior \\(q(z) = p_\\theta(z|x)\\).</p>"},{"location":"informationtheory/7a_vi_intro/#4-em-and-map-map-em","title":"4. EM and MAP: MAP-EM","text":"<p>EM typically performs maximum likelihood estimation, but it can be modified to perform MAP estimation by including a prior:</p> \\[ \\theta_{\\text{MAP}}  =  \\arg\\max_\\theta  \\left[ \\mathbb{E}_{p(z|x,\\theta)}[\\log p(x,z|\\theta)] + \\log p(\\theta) \\right]. \\] <p>This version, often called MAP-EM, incorporates prior structure into the estimation procedure.</p>"},{"location":"informationtheory/7a_vi_intro/#5-limitations-of-map-and-em","title":"5. Limitations of MAP and EM","text":"<p>Both MAP and EM have limitations that motivate more general methods:</p> <ol> <li>MAP returns only a point estimate and discards posterior uncertainty.  </li> <li>EM requires exact posterior computation in the E-step:        which is often intractable.  </li> <li>EM struggles with:</li> <li>multimodal posteriors  </li> <li>high-dimensional latent spaces  </li> <li>arbitrary likelihood forms  </li> </ol> <p>These limitations lead naturally to variational inference.</p>"},{"location":"informationtheory/7a_vi_intro/#6-a-brief-introduction-to-variational-inference-vi","title":"6. A Brief Introduction to Variational Inference (VI)","text":"<p>Variational inference generalizes EM by replacing the exact posterior with a tractable approximation. Instead of requiring</p> \\[ q(z) = p_\\theta(z|x), \\] <p>VI chooses a family of distributions</p> \\[ q_\\phi(z|x) \\in \\mathcal{Q} \\] <p>and optimizes it to be close to the true posterior. The objective is:</p> \\[ \\phi^* =  \\arg\\min_\\phi D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Because \\(p(z|x)\\) contains the intractable marginal likelihood, VI rewrites this using the Evidence Lower Bound (ELBO):</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Maximizing the ELBO yields a tractable approximation to Bayesian posterior inference.</p> <p>VI:</p> <ul> <li>generalizes MAP (when \\(q\\) is a delta function)  </li> <li>generalizes EM (when \\(q = p_\\theta(z|x)\\))  </li> <li>supports flexible approximations  </li> <li>scales to large datasets  </li> <li>is the backbone of VAEs, Bayesian deep models, and many modern generative models  </li> </ul> <p>The next chapter explores variational inference in detail.</p>"},{"location":"informationtheory/7a_vi_intro/#7-summary-of-the-chapter","title":"7. Summary of the Chapter","text":"<p>Monte Carlo sampling approximates integrals using random samples, but can be slow or difficult to tune. Optimization-based inference provides an alternative strategy.</p> <p>MAP estimation chooses the most likely parameter value given the data and the prior. EM handles models with latent variables by alternating between inference (E-step) and optimization (M-step). Variational inference generalizes EM by allowing the E-step to use tractable approximations rather than the exact posterior.</p> <p>MAP, EM, and variational inference all represent the shift from sampling-based methods toward optimization-based approaches. These methods form the conceptual foundation for the next chapter on full variational inference and the ELBO.</p>"},{"location":"informationtheory/7b_vi/","title":"7. Variatonal Inference","text":"<p>Variational inference (VI) is a fundamental technique in modern machine learning. It provides a way to approximate complicated probability distributions with simpler, tractable ones. VAEs, Bayesian neural networks, probabilistic latent-variable models, and diffusion models all rely on variational inference.</p> <p>This chapter explains VI step by step, beginning with the inference problem itself, then deriving the Evidence Lower Bound (ELBO), and finally connecting these ideas to VAEs and deep learning.</p>"},{"location":"informationtheory/7b_vi/#1-the-problem-of-inference","title":"1. The Problem of Inference","text":"<p>Many machine-learning models introduce hidden variables to explain observed data. For example:</p> <ul> <li>A VAE introduces latent variables \\(z\\) describing an image \\(x\\).  </li> <li>A Bayesian neural network introduces weight distributions.  </li> <li>Mixture models introduce cluster assignments.  </li> </ul> <p>In each case, the goal is to understand the posterior distribution:</p> \\[ p(z|x) = \\frac{p(x,z)}{p(x)}. \\] <p>The difficulty is the denominator:</p> \\[ p(x) = \\int p(x,z)\\,dz, \\] <p>which is often impossible to compute exactly because the integral spans a complicated, high-dimensional space. Because computing the true posterior is intractable, we approximate it.</p>"},{"location":"informationtheory/7b_vi/#2-the-idea-of-variational-inference","title":"2. The Idea of Variational Inference","text":"<p>Variational inference turns inference into an optimization problem. Instead of trying to compute \\(p(z|x)\\) exactly, we choose a simpler, tractable family of distributions \\(q_\\phi(z|x)\\) and try to make it as close as possible to the true posterior.</p> <p>This is done by minimizing the KL divergence:</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>However, since \\(p(z|x)\\) is unknown (that was the original problem), we cannot compute this KL divergence directly. Instead, VI reformulates the problem using quantities we can compute.</p>"},{"location":"informationtheory/7b_vi/#3-deriving-the-elbo","title":"3. Deriving the ELBO","text":"<p>We begin with the log marginal likelihood:</p> \\[ \\log p(x) = \\log \\int p(x,z)\\,dz. \\] <p>We introduce the variational distribution \\(q_\\phi(z|x)\\) and use the identity:</p> \\[ \\log p(x) = \\mathbb{E}_{q_\\phi(z|x)}\\left[ \\log\\frac{p(x,z)}{q_\\phi(z|x)} \\right] + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>Rearranging gives:</p> \\[ \\log p(x) = \\mathcal{L}(x;\\phi,\\theta) + D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)), \\] <p>where</p> \\[ \\mathcal{L}(x;\\phi,\\theta) = \\mathbb{E}_{q_\\phi(z|x)} \\left[\\log p_\\theta(x|z)\\right] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>Since the KL divergence is non-negative, we have the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}(x;\\phi,\\theta) \\le \\log p(x). \\] <p>Maximizing the ELBO minimizes the divergence between the approximate and true posteriors. Variational inference becomes an optimization problem.</p>"},{"location":"informationtheory/7b_vi/#4-interpreting-the-elbo","title":"4. Interpreting the ELBO","text":"<p>The ELBO consists of two terms:</p>"},{"location":"informationtheory/7b_vi/#reconstruction-term","title":"Reconstruction term","text":"\\[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\] <p>This ensures that the latent variable \\(z\\) contains enough information to reconstruct \\(x\\). It corresponds to reconstruction accuracy in VAEs and log-likelihood in generative models.</p>"},{"location":"informationtheory/7b_vi/#regularization-term","title":"Regularization term","text":"\\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)) \\] <p>This forces the approximate posterior to stay close to the prior. In VAEs, \\(p(z)\\) is usually a standard Gaussian, so this term pushes the latent codes to be smooth and structured.</p>"},{"location":"informationtheory/7b_vi/#interpretation","title":"Interpretation","text":"<p>The reconstruction term encourages expressiveness, while the KL term encourages simplicity. Together, they balance the trade-off between data fidelity and model complexity.</p>"},{"location":"informationtheory/7b_vi/#5-why-vi-uses-reverse-kl","title":"5. Why VI Uses Reverse KL","text":"<p>Variational inference minimizes:</p> \\[ D_{\\text{KL}}(q_\\phi(z|x)\\|p(z|x)). \\] <p>This is reverse KL, which behaves conservatively:</p> <ul> <li>It avoids placing mass in regions where \\(p(z|x)\\) is small.  </li> <li>It prefers single high-density modes.  </li> <li>It is willing to ignore alternative modes of the posterior.  </li> </ul> <p>As a result, VI tends to produce mode-seeking solutions, which explains why VAEs sometimes prefer \u201csafe,\u201d blurry reconstructions rather than sharp, multimodal samples. This behavior contrasts with forward KL, which tries to cover all modes.</p>"},{"location":"informationtheory/7b_vi/#6-variational-autoencoders-vaes","title":"6. Variational Autoencoders (VAEs)","text":"<p>VAEs are a practical deep-learning implementation of variational inference. They combine:</p> <ul> <li>a generative model \\(p_\\theta(x|z)\\) </li> <li>an approximate posterior \\(q_\\phi(z|x)\\) </li> <li>a prior distribution \\(p(z)\\) </li> </ul> <p>The VAE is trained by maximizing the ELBO over the dataset.</p>"},{"location":"informationtheory/7b_vi/#61-the-generative-model","title":"6.1 The generative model","text":"<p>A latent variable \\(z\\) is sampled from a prior:</p> \\[ z \\sim p(z), \\] <p>and an observation \\(x\\) is generated from the decoder:</p> \\[ x \\sim p_\\theta(x|z). \\]"},{"location":"informationtheory/7b_vi/#62-the-inference-model","title":"6.2 The inference model","text":"<p>A neural network encoder approximates the posterior:</p> \\[ q_\\phi(z|x) \\approx p(z|x), \\] <p>typically using a Gaussian distribution whose mean and variance depend on \\(x\\).</p>"},{"location":"informationtheory/7b_vi/#63-training-objective","title":"6.3 Training objective","text":"<p>The VAE maximizes the ELBO:</p> \\[ \\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)\\|p(z)). \\] <p>The first term encourages accurate reconstruction; the second encourages structured latent representations.</p>"},{"location":"informationtheory/7b_vi/#7-the-reparameterization-trick","title":"7. The Reparameterization Trick","text":"<p>To compute gradients through the expectation:</p> \\[ z \\sim q_\\phi(z|x), \\] <p>VAEs use the reparameterization:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\odot\\epsilon, \\qquad \\epsilon\\sim\\mathcal{N}(0,I). \\] <p>This converts sampling into a differentiable transformation, enabling backpropagation.</p>"},{"location":"informationtheory/7b_vi/#8-consequences-of-reverse-kl-in-vaes","title":"8. Consequences of Reverse KL in VAEs","text":"<p>The use of reverse KL causes VAEs to produce:</p> <ul> <li>smooth, conservative outputs  </li> <li>blurry reconstructions in image models  </li> <li>latent spaces with clear structure  </li> <li>stable training dynamics  </li> </ul> <p>Mode-seeking behavior is not always ideal, which is why researchers develop:</p> <ul> <li>\\(\\beta\\)-VAEs (more control over KL term)  </li> <li>hierarchical VAEs  </li> <li>richer posterior distributions  </li> </ul> <p>These extensions aim to improve sample quality or increase flexibility.</p>"},{"location":"informationtheory/7b_vi/#9-vi-beyond-vaes","title":"9. VI Beyond VAEs","text":"<p>Variational inference is far broader than VAEs.</p>"},{"location":"informationtheory/7b_vi/#bayesian-neural-networks","title":"Bayesian neural networks","text":"<p>Approximate weight posteriors:</p> \\[ q(w)\\approx p(w|D). \\]"},{"location":"informationtheory/7b_vi/#diffusion-models","title":"Diffusion models","text":"<p>Have a variational interpretation through score matching and likelihood bounds.</p>"},{"location":"informationtheory/7b_vi/#normalizing-flows","title":"Normalizing flows","text":"<p>Can be used to create more expressive variational posteriors.</p>"},{"location":"informationtheory/7b_vi/#reinforcement-learning","title":"Reinforcement learning","text":"<p>Entropy-regularized RL can be derived using variational principles.</p> <p>VI provides a general framework for approximating intractable distributions.</p> <p>Variational inference transforms posterior inference into an optimization problem by introducing a tractable distribution \\(q_\\phi(z|x)\\) and maximizing the ELBO. The ELBO decomposes into a reconstruction term and a KL regularization term, revealing a trade-off between accuracy and simplicity. VAEs apply this framework in deep learning by parameterizing both the generative model and the approximate posterior with neural networks.</p> <p>Reverse KL divergence drives the behavior of VI and explains why VAEs tend to produce smooth, conservative samples. The concepts developed here provide the foundation for understanding probabilistic deep learning, posterior approximations, representation learning, and modern generative models.</p>"},{"location":"informationtheory/8_reperesentation/","title":"8 reperesentation","text":""},{"location":"informationtheory/8_reperesentation/#chapter-5-representation-learning-mutual-information-and-the-information-bottleneck","title":"Chapter 5 \u2014 Representation Learning, Mutual Information, and the Information Bottleneck","text":"<p>Representation learning seeks transformations of data that make tasks such as prediction, compression, and reasoning easier. A representation \\(Z\\) is typically obtained by applying an encoder to an input \\(X\\). Information theory provides a natural way to formalize what makes a representation useful by analyzing the mutual information between \\(Z\\), the input \\(X\\), and the target \\(Y\\).</p> <p>This chapter introduces mutual information as a measure of shared structure, explains the Information Bottleneck framework, and connects these ideas to deep learning methods such as contrastive learning, VAEs, and self-supervised learning.</p>"},{"location":"informationtheory/8_reperesentation/#1-what-is-a-representation","title":"1. What Is a Representation?","text":"<p>A representation is a transformed form of input data:</p> \\[ Z = f_\\theta(X), \\] <p>where \\(f_\\theta\\) is usually a neural network. A good representation should satisfy two goals:</p> <ol> <li>It should retain information that is relevant for predicting \\(Y\\).  </li> <li>It should discard noise or irrelevant aspects of \\(X\\).</li> </ol> <p>Information theory allows us to express these goals using mutual information.</p>"},{"location":"informationtheory/8_reperesentation/#2-mutual-information-connecting-two-variables","title":"2. Mutual Information: Connecting Two Variables","text":"<p>Mutual information (MI) measures how much knowledge of one variable reduces uncertainty about another:</p> \\[ I(X;Y) = H(X) - H(X|Y). \\] <p>It can also be written as a KL divergence:</p> \\[ I(X;Y) = D_{\\text{KL}}(p(x,y)\\|p(x)p(y)). \\] <p>MI is zero when \\(X\\) and \\(Y\\) are independent and increases as \\(Y\\) becomes more predictable from \\(X\\).</p> <p>In representation learning, we are often interested in the two quantities:</p> \\[ I(Z;Y), \\qquad I(Z;X). \\] <p>These measure how informative the representation \\(Z\\) is regarding the target \\(Y\\) and how much irrelevant detail from \\(X\\) is still present.</p>"},{"location":"informationtheory/8_reperesentation/#3-the-role-of-mi-in-representation-learning","title":"3. The Role of MI in Representation Learning","text":"<p>A representation \\(Z\\) is desirable when:</p> <ul> <li> <p>\\(I(Z;Y)\\) is large   (the representation captures features relevant to prediction)</p> </li> <li> <p>\\(I(Z;X)\\) is small   (the representation removes noise and redundancy)</p> </li> </ul> <p>This idea appears in supervised learning, contrastive methods, and generative modeling.</p> <p>Some examples:</p> <ul> <li>In supervised learning, we want features that preserve label information.  </li> <li>In contrastive learning, we want features that preserve the information common across augmented views.  </li> <li>In generative models, latent variables should retain structure that explains the data while avoiding unnecessary detail.</li> </ul>"},{"location":"informationtheory/8_reperesentation/#4-the-information-bottleneck-principle","title":"4. The Information Bottleneck Principle","text":"<p>The Information Bottleneck (IB) formalizes the trade-off between informativeness and compression. The goal is:</p> \\[ \\max I(Z;Y) \\quad \\text{s.t.} \\quad  I(Z;X) \\text{ is small}. \\] <p>This can be written as the Lagrangian:</p> \\[ \\mathcal{L}_{\\text{IB}} = I(Z;Y) - \\beta I(Z;X). \\] <p>The parameter \\(\\beta\\) controls how aggressively the representation is compressed.</p> <ul> <li>Large \\(\\beta\\) leads to simpler, more compressed representations.  </li> <li>Small \\(\\beta\\) allows more expressive, detailed representations.</li> </ul> <p>IB provides a theoretical explanation for the behavior of learned features in deep neural networks.</p>"},{"location":"informationtheory/8_reperesentation/#5-deep-learning-and-the-information-bottleneck","title":"5. Deep Learning and the Information Bottleneck","text":"<p>IB theory suggests several statements about deep networks:</p> <ol> <li>Early layers preserve much of the information in \\(X\\).  </li> <li>Later layers tend to compress \\(X\\) while emphasizing information predictive of \\(Y\\).  </li> <li>Networks may first memorize and later compress during training.  </li> <li>Generalization is linked to discarding unnecessary information.</li> </ol> <p>Although the exact dynamics remain debated, the overall perspective helps interpret the evolution of features during training.</p>"},{"location":"informationtheory/8_reperesentation/#6-variational-information-bottleneck-vib","title":"6. Variational Information Bottleneck (VIB)","text":"<p>Mutual information terms are often difficult to compute directly. The Variational Information Bottleneck approximates them using variational distributions.</p> <p>We treat the representation as a random variable drawn from \\(q(z|x)\\) and estimate MI with tractable terms. The VIB objective is:</p> \\[ \\mathcal{L}_{\\text{VIB}} = \\mathbb{E}_{p(x,y)}\\! \\left[ \\mathbb{E}_{q(z|x)}\\![\\log p(y|z)] \\right] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)). \\] <p>This resembles the VAE objective, but \\(p(y|z)\\) replaces the reconstruction term. The first term encourages predictive features, while the KL term compresses the representation.</p> <p>VIB therefore provides a practical implementation of the Information Bottleneck.</p>"},{"location":"informationtheory/8_reperesentation/#7-mutual-information-and-contrastive-learning","title":"7. Mutual Information and Contrastive Learning","text":"<p>Contrastive learning uses mutual information to learn representations without labels. The idea is:</p> <ul> <li>Generate two augmented views of the same input: \\((x_1, x_2)\\).  </li> <li>Encode them as \\((z_1, z_2)\\).  </li> <li>Encourage \\(z_1\\) and \\(z_2\\) to be similar.  </li> <li>Encourage representations of different inputs to be dissimilar.</li> </ul> <p>This encourages \\(Z\\) to retain the information that is preserved under augmentation, while ignoring irrelevant aspects of the input.</p> <p>Many methods follow this structure:</p> <ul> <li>SimCLR  </li> <li>MoCo  </li> <li>BYOL  </li> <li>InfoNCE  </li> <li>CPC  </li> <li>Deep InfoMax  </li> </ul> <p>The InfoNCE objective is:</p> \\[ \\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}\\left[ \\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)} {\\sum_k \\exp(\\text{sim}(z_i,z_k)/\\tau)} \\right], \\] <p>where \\((i,j)\\) is a positive pair. InfoNCE is a variational lower bound on \\(I(Z_1;Z_2)\\).</p>"},{"location":"informationtheory/8_reperesentation/#8-mi-in-generative-modeling","title":"8. MI in Generative Modeling","text":"<p>Mutual information also appears in generative models:</p>"},{"location":"informationtheory/8_reperesentation/#81-vaes","title":"8.1 VAEs","text":"<p>The KL term controls the structure and redundancy of \\(Z\\), and the decoder ensures \\(I(Z;X)\\) stays large enough for accurate reconstruction.</p>"},{"location":"informationtheory/8_reperesentation/#82-infogan","title":"8.2 InfoGAN","text":"<p>This model maximizes:</p> \\[ I(c; G(z,c)), \\] <p>encouraging the generator to learn interpretable latent factors.</p>"},{"location":"informationtheory/8_reperesentation/#83-normalizing-flows","title":"8.3 Normalizing flows","text":"<p>Flows maintain \\(I(X;Z) = H(X)\\) because they are invertible; they do not compress the input.</p>"},{"location":"informationtheory/8_reperesentation/#84-diffusion-models","title":"8.4 Diffusion models","text":"<p>Diffusion models gradually reduce noise and can be interpreted using information-theoretic ideas related to KL divergence and score matching.</p>"},{"location":"informationtheory/8_reperesentation/#9-mi-and-disentanglement","title":"9. MI and Disentanglement","text":"<p>Disentangled representations aim to separate independent generative factors such as orientation or color. The \\(\\beta\\)-VAE objective:</p> \\[ \\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - \\beta D_{\\text{KL}}(q(z|x)\\|p(z)) \\] <p>encourages disentanglement by increasing compression in the latent space. A larger \\(\\beta\\) pushes different dimensions of \\(Z\\) to encode more independent aspects of the data.</p>"},{"location":"informationtheory/8_reperesentation/#10-estimating-mi-in-high-dimensions","title":"10. Estimating MI in High Dimensions","text":"<p>Mutual information is difficult to compute exactly in high dimensions. Neural estimation relies on variational bounds such as:</p> <ul> <li>InfoNCE  </li> <li>NWJ bound  </li> <li>Donsker\u2013Varadhan bound  </li> <li>MINE estimator  </li> <li>f-divergence lower bounds  </li> </ul> <p>These allow MI to be used in representation learning even when the true quantities are intractable.</p>"},{"location":"informationtheory/8_reperesentation/#11-summary-of-chapter-5","title":"11. Summary of Chapter 5","text":"<p>Mutual information provides a principled measure of what makes a useful representation: it should retain information relevant for prediction and discard irrelevant detail. The Information Bottleneck formalizes this trade-off and motivates practical methods such as the Variational Information Bottleneck.</p> <p>Contrastive learning methods maximize MI between augmented views, enabling self-supervised representation learning. Generative models such as VAEs, GANs, flows, and diffusion models each manipulate mutual information in different ways, leading to distinct behaviors and capabilities.</p> <p>Information theory therefore provides a unified lens through which to understand representation learning in modern deep networks.</p>"},{"location":"informationtheory/intro/","title":"Intro","text":"<p>https://www.inference.org.uk/itprnn_lectures/ https://www.youtube.com/watch?v=BCiZc0n6COY</p> <p>Application in  1. GANS</p>"},{"location":"informationtheory/intro/#game-theory","title":"Game theory","text":"<ul> <li>zero sum?</li> <li>min max V(D,G)</li> <li>nash equiblria</li> </ul> <p>Application in GAN</p>"},{"location":"llms/chahllanges/","title":"Chahllanges","text":"<p>Data Challenges: This pertains to the data used for training and how the model addresses gaps or missing data. Ethical Challenges: This involves addressing issues such as mitigating biases, ensuring privacy, and preventing the generation of harmful content in the deployment of LLMs. Technical Challenges: These challenges focus on the practical implementation of LLMs. Deployment Challenges: Concerned with the specific processes involved in transitioning fully-functional LLMs into real-world use-cases (productionization) Data Challenges:</p> <p>Data Bias: The presence of prejudices and imbalances in the training data leading to biased model outputs. Limited World Knowledge and Hallucination: LLMs may lack comprehensive understanding of real-world events and information and tend to hallucinate information. Note that training them on new data is a long and expensive process. Dependency on Training Data Quality: LLM performance is heavily influenced by the quality and representativeness of the training data. Ethical and Social Challenges:</p> <p>Ethical Concerns: Concerns regarding the responsible and ethical use of language models, especially in sensitive contexts. Bias Amplification: Biases present in the training data may be exacerbated, resulting in unfair or discriminatory outputs. Legal and Copyright Issues: Potential legal complications arising from generated content that infringes copyrights or violates laws. User Privacy Concerns: Risks associated with generating text based on user inputs, especially when dealing with private or sensitive information. Technical Challenges:</p> <p>Computational Resources: Significant computing power required for training and deploying large language models. Interpretability: Challenges in understanding and explaining the decision-making process of complex models. Evaluation: Evaluation presents a notable challenge as assessing models across diverse tasks and domains is inadequately designed, particularly due to the challenges posed by freely generated content. Fine-tuning Challenges: Difficulties in adapting pre-trained models to specific tasks or domains. Contextual Understanding: LLMs may face challenges in maintaining coherent context over longer passages or conversations. Robustness to Adversarial Attacks: Vulnerability to intentional manipulations of input data leading to incorrect outputs. Long-Term Context: Struggles in maintaining context and coherence over extended pieces of text or discussions. Deployment Challenges:</p> <p>Scalability: Ensuring that the model can scale efficiently to handle increased workloads and demand in production environments. Latency: Minimizing the response time or latency of the model to provide quick and efficient interactions, especially in real-time applications. Monitoring and Maintenance: Implementing robust monitoring systems to track model performance, detect issues, and perform regular maintenance to avoid downtime. Integration with Existing Systems: Ensuring smooth integration of LLMs with existing software, databases, and infrastructure within an organization. Cost Management: Optimizing the cost of deploying and maintaining large language models, as they can be resource-intensive in terms of both computation and storage. Security Concerns: Addressing potential security vulnerabilities and risks associated with deploying language models in production, including safeguarding against malicious attacks. Interoperability: Ensuring compatibility with other tools, frameworks, or systems that may be part of the overall production pipeline. User Feedback Incorporation: Developing mechanisms to incorporate user feedback to continuously improve and update the model in a production environment. Regulatory Compliance: Adhering to regulatory requirements and compliance standards, especially in industries with strict data protection and privacy regulations. Dynamic Content Handling: Managing the generation of text in dynamic environments where content and user interactions change frequently.</p> <p>Types of Domain Adaptation Methods There are several methods to incorporate domain-specific knowledge into LLMs, each with its own advantages and limitations. Here are three classes of approaches:</p> <p>Domain-Specific Pre-Training:</p> <p>Training Duration: Days to weeks to months Summary: Requires a large amount of domain training data; can customize model architecture, size, tokenizer, etc. In this method, LLMs are pre-trained on extensive datasets representing various natural language use cases. For instance, models like PaLM 540B, GPT-3, and LLaMA 2 have been pre-trained on datasets with sizes ranging from 499 billion to 2 trillion tokens. Examples of domain-specific pre-training include models like ESMFold, ProGen2 for protein sequences, Galactica for science, BloombergGPT for finance, and StarCoder for code. These models outperform generalist models within their domains but still face limitations in terms of accuracy and potential hallucinations.</p> <p>Domain-Specific Fine-Tuning:</p> <p>Training Duration: Minutes to hours Summary: Adds domain-specific data; tunes for specific tasks; updates LLM model Fine-tuning involves training a pre-trained LLM on a specific task or domain, adapting its knowledge to a narrower context. Examples include Alpaca (fine-tuned LLaMA-7B model for general tasks), xFinance (fine-tuned LLaMA-13B model for financial-specific tasks), and ChatDoctor (fine-tuned LLaMA-7B model for medical chat). The costs for fine-tuning are significantly smaller compared to pre-training.</p> <p>Retrieval Augmented Generation (RAG):</p> <p>Training Duration: Not required Summary: No model weights; external information retrieval system can be tuned RAG involves grounding the LLM's parametric knowledge with external or non-parametric knowledge from an information retrieval system. This external knowledge is provided as additional context in the prompt to the LLM. The advantages of RAG include no training costs, low expertise requirement, and the ability to cite sources for human verification. This approach addresses limitations such as hallucinations and allows for precise manipulation of knowledge. The knowledge base is easily updatable without changing the LLM. Strategies to combine non-parametric knowledge with an LLM's parametric knowledge are actively researched.</p> <p>Use Domain-Specific Pre-Training When: Exclusive Domain Focus: Pre-training is suitable when you require a model exclusively trained on data from a specific domain, creating a specialized language model for that domain. Customizing Model Architecture: It allows you to customize various aspects of the model architecture, size, tokenizer, etc., based on the specific requirements of the domain. Extensive Training Data Available: Effective pre-training often requires a large amount of domain-specific training data to ensure the model captures the intricacies of the chosen domain. Use Domain-Specific Fine-Tuning When: Specialization Needed: Fine-tuning is suitable when you already have a pre-trained LLM, and you want to adapt it for specific tasks or within a particular domain. Task Optimization: It allows you to adjust the model's parameters related to the task, such as architecture, size, or tokenizer, for optimal performance in the chosen domain. Time and Resource Efficiency: Fine-tuning saves time and computational resources compared to training a model from scratch since it leverages the knowledge gained during the pre-training phase. Use RAG When: Information Freshness Matters: RAG provides up-to-date, context-specific data from external sources. Reducing Hallucination is Crucial: Ground LLMs with verifiable facts and citations from an external knowledge base. Cost-Efficiency is a Priority: Avoid extensive model training or fine-tuning; implement without the need for training.</p>"},{"location":"llms/foundations/","title":"Foundations","text":""},{"location":"monetcarlo_simulations/plan/","title":"Plan","text":"<ol> <li>Markov Chain</li> </ol>"},{"location":"nonconvex/41_intro/","title":"1. Introduction","text":""},{"location":"nonconvex/41_intro/#chapter-1-non-convex-optimization-fundamentals","title":"Chapter 1: Non-Convex Optimization Fundamentals","text":""},{"location":"nonconvex/41_intro/#11-why-non-convexity-matters","title":"1.1 Why Non-Convexity Matters","text":"<p>Convex optimization ensures a unique global minimum and strong theoretical guarantees. However, many practical problems in machine learning, deep learning, control, and physics are non-convex:</p> <ul> <li>Neural network loss surfaces</li> <li>Reinforcement learning value functions</li> <li>Matrix factorization</li> <li>Clustering and combinatorial tasks</li> </ul> <p>These landscapes cannot be handled efficiently with traditional convex methods.</p>"},{"location":"nonconvex/41_intro/#12-characteristics-of-non-convex-landscapes","title":"1.2 Characteristics of Non-Convex Landscapes","text":"Property Description Consequence Multiple local minima Many suboptimal valleys Gradient descent may get trapped Saddle points Flat or neutral zones Slow or no convergence Discontinuities Non-differentiable regions Gradients undefined Non-linearity Coupled variables Non-trivial curvature and topology <p>Visualization of 2D loss surfaces often reveals chaotic or fractal-like geometry.</p>"},{"location":"nonconvex/41_intro/#13-gradient-based-methods-and-their-limits","title":"1.3 Gradient-Based Methods and Their Limits","text":"<p>Even though SGD, Adam, and similar methods dominate deep learning, they:</p> <ul> <li>Depend heavily on initialization</li> <li>May converge to poor local minima or plateaus</li> <li>Are sensitive to learning rate and batch size</li> <li>Cannot handle discrete or combinatorial variables</li> </ul> <p>This motivates global search strategies that can explore the space more broadly.</p>"},{"location":"nonconvex/41_intro/#14-toward-global-optimization","title":"1.4 Toward Global Optimization","text":"<p>Global optimization aims to find near-optimal solutions without convexity assumptions. Two main families exist:</p> <ol> <li>Deterministic global methods \u2014 exhaustive, branch-and-bound, interval analysis</li> <li>Stochastic and metaheuristic methods \u2014 probabilistic, adaptive, and nature-inspired</li> </ol> <p>The remainder of this book focuses on the latter, due to their flexibility and robustness in black-box settings.</p>"},{"location":"nonconvex/42_meta/","title":"2. Metaheuristic Optimization Methods","text":""},{"location":"nonconvex/42_meta/#chapter-2-metaheuristic-optimization-methods","title":"Chapter 2: Metaheuristic Optimization Methods","text":"<p>Metaheuristics are general-purpose stochastic search algorithms inspired by natural or social processes. They do not require gradient information and are particularly powerful for non-convex, discrete, and black-box problems.</p>"},{"location":"nonconvex/42_meta/#21-core-principles","title":"2.1 Core Principles","text":"<p>Metaheuristics operate by balancing two key dynamics:</p> <ul> <li>Exploration: Searching new, unvisited regions of the solution space</li> <li>Exploitation: Refining promising areas to improve solution quality</li> </ul> <p>Algorithms differ in how they maintain this balance \u2014 through temperature schedules, populations, or probabilistic moves.</p>"},{"location":"nonconvex/42_meta/#22-major-families-of-metaheuristics","title":"2.2 Major Families of Metaheuristics","text":"Category Examples Inspiration Trajectory-based Simulated Annealing (SA) Thermodynamics Evolutionary algorithms Genetic Algorithms (GA), Differential Evolution (DE) Natural selection Swarm intelligence Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO) Collective animal behavior Physics/chemistry inspired Harmony Search, Firefly Algorithm, Gravitational Search Physical processes"},{"location":"nonconvex/42_meta/#23-simulated-annealing-sa","title":"2.3 Simulated Annealing (SA)","text":"<ul> <li>Mimics the cooling of metals.</li> <li>Accepts worse moves with a probability <code>exp(-\u0394E/T)</code>, allowing escape from local minima.</li> <li>Temperature <code>T</code> decreases over time.</li> </ul> <p>Key idea: Controlled randomness to avoid premature convergence.</p>"},{"location":"nonconvex/42_meta/#24-genetic-algorithms-ga","title":"2.4 Genetic Algorithms (GA)","text":"<ul> <li>Maintain a population of solutions.</li> <li>Apply selection, crossover, and mutation to evolve toward better candidates.</li> <li>Works well for discrete, combinatorial, or mixed-variable problems.</li> </ul> <p>Mathematical insight: Balances exploitation (selection) and exploration (mutation).</p>"},{"location":"nonconvex/42_meta/#25-swarm-based-methods","title":"2.5 Swarm-Based Methods","text":"<p>Inspired by collective behaviors in nature:</p> <ul> <li>PSO: Particles move based on personal and social bests</li> <li>ACO: Agents deposit pheromones to guide search</li> <li>Firefly Algorithm: Movement toward brighter (better) peers</li> </ul> <p>These methods excel in continuous search spaces and dynamic environments.</p>"},{"location":"nonconvex/42_meta/#26-theoretical-considerations","title":"2.6 Theoretical Considerations","text":"<p>Although metaheuristics lack strong convex guarantees, their stochastic convergence can be studied via:</p> <ul> <li>Markov chain analysis</li> <li>Expected improvement over iterations</li> <li>Diversity measures within populations</li> </ul> <p>They often converge probabilistically to a near-optimal region rather than a single guaranteed optimum.</p>"},{"location":"nonconvex/43_hybrid/","title":"3. Hybrid and Modern Optimization Methods","text":""},{"location":"nonconvex/43_hybrid/#chapter-3-hybrid-and-modern-optimization-methods","title":"Chapter 3: Hybrid and Modern Optimization Methods","text":"<p>Modern optimization integrates heuristic exploration with mathematical precision. Hybrid and adaptive approaches leverage the strengths of both global and local methods.</p>"},{"location":"nonconvex/43_hybrid/#31-hybrid-metaheuristics","title":"3.1 Hybrid Metaheuristics","text":"<p>Combine metaheuristics with classical optimization to accelerate convergence:</p> <ul> <li>Memetic algorithms: GA + local search refinement</li> <li>Hybrid PSO: PSO with gradient descent fine-tuning</li> <li>Adaptive Simulated Annealing: Dynamic temperature and step size</li> </ul> <p>The goal: exploit global search for exploration, and analytical methods for exploitation.</p>"},{"location":"nonconvex/43_hybrid/#32-multi-objective-optimization","title":"3.2 Multi-Objective Optimization","text":"<p>Many real-world problems have conflicting objectives, e.g., accuracy vs interpretability.</p> <ul> <li>Represent trade-offs via the Pareto front</li> <li>Search for non-dominated solutions using evolutionary multi-objective algorithms (e.g., NSGA-II, MOEA/D)</li> <li>Use crowding distance and rank-based selection for diversity</li> </ul> <p>These methods underpin design trade-offs in engineering and AutoML pipelines.</p>"},{"location":"nonconvex/43_hybrid/#33-constraint-handling","title":"3.3 Constraint Handling","text":"<p>Constraints are incorporated via:</p> <ul> <li>Penalty functions: Add cost for violations</li> <li>Repair mechanisms: Project invalid solutions back into feasible space</li> <li>Decoders: Convert unconstrained representations into feasible solutions</li> </ul> <p>These are essential for optimization in robotics, control, and combinatorial planning.</p>"},{"location":"nonconvex/43_hybrid/#34-modern-directions","title":"3.4 Modern Directions","text":""},{"location":"nonconvex/43_hybrid/#1-reinforcement-learning-and-evolution","title":"1. Reinforcement Learning and Evolution","text":"<ul> <li>Neuroevolution (e.g., NEAT)</li> <li>Policy optimization via evolutionary strategies</li> </ul>"},{"location":"nonconvex/43_hybrid/#2-bayesian-and-surrogate-optimization","title":"2. Bayesian and Surrogate Optimization","text":"<ul> <li>Gaussian processes + exploration policies</li> <li>Efficient black-box optimization (used in hyperparameter tuning)</li> </ul>"},{"location":"nonconvex/43_hybrid/#3-quantum-inspired-and-neuro-symbolic-search","title":"3. Quantum-Inspired and Neuro-symbolic Search","text":"<ul> <li>Quantum annealing analogies</li> <li>Neural controllers guiding metaheuristics</li> <li>AutoML as meta-level optimization</li> </ul>"},{"location":"nonconvex/43_hybrid/#35-summary","title":"3.5 Summary","text":"<p>Hybrid and modern metaheuristics represent a convergence of mathematics, biology, and computation. They embrace stochasticity not as noise, but as a powerful tool for discovering high-quality solutions in complex, non-convex landscapes.</p>"},{"location":"statistics/introd/","title":"Introd","text":"<p>Variational Inference Exact Inference Monte Carlo  Markov Chain MCMC Sampling vs Optimization</p>"},{"location":"tools/intro/","title":"Intro","text":"<p>Snowflake</p> <p>Proven experience in agentic frameworks (using CruxAI, Google ADK, LangGraph).</p> <p>Multi-agent systems using frameworks such as LangChain, LangGraph, AutoGen, or CrewAI, with practical understanding of LLM orchestration, retrieval augmentation (RAG), tool calling, and dynamic reasoning.</p> <p>Experience integrating agentic systems into enterprise data and workflow environments, ensuring robustness, and maintainability.</p> <p>Proficiency in Python, with experience extending orchestration components and building APIs or tool interfaces.</p> <p>Experience deploying AI systems on cloud platforms using Docker, Kubernetes, and microservices integration.</p> <p>Experience deploying and optimising GenAI and LLM-based systems, including performance evaluation and monitoring.</p>"}]}