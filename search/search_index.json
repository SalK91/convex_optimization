{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#lectures-on-optimization","title":"Lectures on Optimization","text":"<p>Welcome to Lectures on Optimization, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and implement modern optimization algorithms, with a strong focus on convex analysis and scalable algorithmic methods central to machine learning.</p> <p>These notes are inspired by and draw heavily on material from:</p> <ul> <li>Convex Optimization\u2014 Stephen Boyd &amp; Lieven Vandenberghe Cambridge University Press, 2004 link</li> <li>Stanford EE364A: Convex Optimization I - Stephen Boyd (2023) link</li> <li>Optimization: The University of Texas at Austin \u2014 Spring 2020 link</li> </ul> <p>The goal is not to reproduce the content from these courses, but to distill, reorganize, and extend its core ideas into a machine-learning-aligned, optimization-first framework, prioritizing:</p> <ul> <li>Convex problem structure, geometry, and theoretical principles</li> <li>Algorithmic design, convergence behavior, and computational efficiency</li> <li>Scalable and practical methods used in ML, deep learning, and NLP optimization</li> </ul> <p>These lectures develop intuition, mathematical rigor, and practical algorithmic insight across major areas including: - Convex sets, convex functions, and problem modeling - Gradient, subgradient, Newton, and proximal methods - Constrained optimization, duality, and KKT theory - Decomposition, interior-point methods, and path-following algorithms - Large-scale optimization strategies that power real-world machine learning systems</p>"},{"location":"convex/11_intro/","title":"1. Introduction and Overview","text":""},{"location":"convex/11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1:  Introduction and Overview","text":"<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p> <ol> <li>No bad local minima: any local minimum is also the global minimum.  </li> <li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li> <li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li> </ol>"},{"location":"convex/11_intro/#motivation-optimization-in-machine-learning","title":"Motivation: Optimization in Machine Learning","text":"<p>Many supervised learning problems can be written in a common form:</p> \\[ \\min_{x \\in \\mathcal{X}}  \\; \\frac{1}{N}\\sum_{i=1}^{N} \\ell(a_i^\\top x, b_i)  + \\lambda R(x), \\] <p>where</p> <ul> <li>\\(\\ell(\\cdot,\\cdot)\\) is a loss function that measures how well the model predicts \\(b_i\\) from \\(a_i\\),  </li> <li>\\(R(x)\\) is a regularizer that encourages certain structure (such as sparsity or small weights),  </li> <li>\\(\\mathcal{X}\\) is a set of allowed parameter values, often simple and convex.</li> </ul> <p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the \\(\\ell_1\\) norm, and the \\(\\ell_2\\) norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>"},{"location":"convex/11_intro/#why-convex-optimization-remains-central-in-ml","title":"Why Convex Optimization Remains Central in ML","text":"<p>Although many modern models are nonconvex, convex optimization continues to play a major role:</p> <ol> <li> <p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the \\(0\\text{\u2013}1\\) loss. They make optimization practical while still leading to models that generalize well.</p> </li> <li> <p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p> </li> </ol> <p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>"},{"location":"convex/11_intro/#web-book-roadmap-and-how-to-use-it","title":"Web-Book Roadmap and How to Use It","text":"<ul> <li> <p>Chapter 2: Linear Algebra Foundations. Basic vector/matrix operations and linear algebra needed for optimization.</p> </li> <li> <p>Chapter 3: Multivariable Calculus. Differentiation and derivatives of functions of many variables (gradients, Hessians).</p> </li> <li> <p>Chapter 4: Convex Sets and Geometry. Definitions and examples of convex sets, cones, affine spaces, and geometric properties.</p> </li> <li> <p>Chapter 5: Convex Functions. Convexity for functions, epigraphs, and key examples (norms, quadratic functions, log-sum-exp, etc.).</p> </li> <li> <p>Chapter 6: Nonsmooth Optimization \u2013 Subgradients. Generalized derivatives for convex functions that are not differentiable, and subgradient methods.</p> </li> <li> <p>Chapter 7: First-Order Optimality Conditions. Gradient-based optimality for smooth problems, supporting theory for necessary and sufficient conditions.</p> </li> <li> <p>Chapter 8: Optimization Principles \u2013 From Gradient Descent to KKT. Unconstrained and constrained gradient methods, culminating in the Karush\u2013Kuhn\u2013Tucker (KKT) conditions.</p> </li> <li> <p>Chapter 9: Lagrange Duality Theory. Duality principles, weak/strong duality, and interpretations of Lagrange multipliers.</p> </li> <li> <p>Chapter 10: Pareto Optimality and Multi-Objective Optimization. Trade-offs in optimizing multiple goals and Pareto efficiency.</p> </li> <li> <p>Chapter 11: Regularized Approximation. Balancing fit vs. complexity with regularization (\u2113\u2081, \u2113\u2082, elastic net, etc.).</p> </li> <li> <p>Chapter 12: Algorithms for Convex Optimization. General convex optimization solvers and algorithmic frameworks (interior-point, gradient methods, etc.).</p> </li> <li> <p>Chapter 13: Equality-Constrained Problems. Specialized methods (e.g. KKT with only equalities, reduced-space methods).</p> </li> <li> <p>Chapter 14: Inequality-Constrained Problems. Algorithms handling general inequality constraints, barrier methods.</p> </li> <li> <p>Chapter 15: Advanced Large-Scale and Structured Methods. Techniques for very large or structured problems (decomposition, coordinate descent, etc.).</p> </li> <li> <p>Chapter 16: Modeling Patterns and Algorithm Selection. Practical guidance on modeling choices and selecting the right algorithm in practice.</p> </li> <li> <p>Chapter 17: Canonical Problems in Convex Optimization. Well-known problem templates (linear, quadratic, SOCP, SDP) and how to recognize them.</p> </li> <li> <p>Chapter 18: Modern Optimizers in Machine Learning Frameworks. How convex optimization appears in ML libraries and frameworks.</p> </li> <li> <p>Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization. Overview of nonconvex problems and global methods (to contrast with convex theory).</p> </li> <li> <p>Chapter 20: Derivative-Free and Black-Box Optimization. Techniques when gradients are not available.</p> </li> <li> <p>Chapter 21: Metaheuristic and Evolutionary Optimization. Heuristic algorithms (genetic algorithms, simulated annealing) for hard problems.</p> </li> <li> <p>Chapter 22: Advanced Topics in Combinatorial Optimization. Combinatorial optimization problems and convex relaxations.</p> </li> </ul> <p>This roadmap helps the reader see how the material progresses from foundations (Ch.2\u20135) to theory (Ch.6\u201311) to algorithms (Ch.12\u201315) and on to specialized and modern topics (Ch.16\u201322).</p>"},{"location":"convex/11_intro/#acknowledgments","title":"Acknowledgments","text":"<p>The content and structure of this web book are strongly informed by the Stanford University course EE364A: Convex Optimization I, taught by Stephen Boyd. </p>"},{"location":"convex/12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"convex/12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas: projections, subspaces, orthogonality, eigenvalues, singular values, and norms  because these ideas directly shape how optimization behaves.</p>"},{"location":"convex/12_vector/#vector-spaces-subspaces-and-affine-sets","title":"Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set of vectors that can be added and scaled without leaving the set. The familiar example is \\(\\mathbb{R}^n\\), where operations like \\(\\alpha x + \\beta y\\) keep us within the same space.</p> <p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are \u201cflat\u201d objects that always pass through the origin, such as lines or planes in \\(\\mathbb{R}^3\\). </p> <p>Affine sets extend this idea by allowing a shift away from the origin. A set \\(A\\) is affine if it contains the entire line passing through any two of its points. Equivalently, for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  \\(\\theta x + (1 - \\theta) y \\in A.\\) That is, the entire line passing through any two points in \\(A\\) lies within \\(A\\). By contrast, a convex set only requires this property for \\(\\theta \\in [0,1]\\), meaning only the line segment between \\(x\\) and \\(y\\) must lie within the set. </p> <p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: \\(A = x_0 + S = \\{\\, x_0 + s : s \\in S \\,\\},\\) where \\(S\\) is a subspace and \\(x_0\\) is any point in the set. This representation is extremely useful in optimization. If \\(Ax = b\\) is a linear constraint, then its solution set is an affine set. A single particular solution \\(x_0\\) gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of \\(A\\). Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p> <p>Affine Transformations: An affine transformation (or affine map) is a function \\(f : V \\to W\\) that can be written as \\(f(x) = A x + b,\\) where \\(A\\) is a linear map and \\(b\\) is a fixed vector. Affine transformations preserve both affinity and convexity: if \\(C\\) is convex, then \\(A C + b\\) is also convex. is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p>"},{"location":"convex/12_vector/#linear-combinations-span-basis-dimension","title":"Linear combinations, span, basis, dimension","text":"<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p> <p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form is a linear combination. The set of all linear combinations is called the span:  The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p> <p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p> <ul> <li>A basis of a space \\(V\\) is a linearly independent set whose span equals \\(V\\). </li> <li>The number of basis vectors is the dimension \\(\\dim(V)\\).</li> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem states: \\(\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n,\\) where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p></p> <p>Figure : Four Fundamental Subspaces: MIT Linear Algebrea</p> <p>Column Space: The column space of a matrix , denoted , is the set of all possible output vectors  that can be written as  for some . In other words, it contains all vectors that the matrix can \u201creach\u201d through linear combinations of its columns. The question \u201cDoes the system  have a solution?\u201d is equivalent to asking whether . If  lies in the column space, a solution exists; otherwise, it does not.</p> <p>Null Space: The null space (or kernel) of , denoted , is the set of all input vectors  that are mapped to zero:  . It answers a different question: If a solution to  exists, is it unique? If the null space contains only the zero vector (), the solution is unique. But if  contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p> <p>Multicollinearity: When one feature in the data matrix  is a linear combination of others for example, \u2014the columns of  become linearly dependent. This creates a nonzero vector in the null space of , meaning multiple weight vectors  can produce the same predictions. The model is then unidentifiable (Underdetermined \u2013 the number of unknowns (parameters) exceeds the number of independent equations (information)), and  becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p> <p>Regularization introduces an additional constraint or penalty that selects a single, stable solution from among the infinite possibilities.</p> <ul> <li> <p>Ridge regression (L2 regularization) adds a penalty on the norm of \\(x\\):      which modifies the normal equations to      The added term \\(\\lambda I\\) ensures invertibility and numerical stability.</p> </li> <li> <p>Lasso regression (L1 regularization) instead penalizes \\(\\|x\\|_1\\), promoting sparsity by driving some coefficients exactly to zero.</p> </li> </ul> <p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors\u2014and making the regression problem well-posed even when \\(A\\) is rank-deficient.</p> <p>Feasible Directions: In a constrained optimization problem of the form , the null space of  characterizes the directions along which one can move without violating the constraints. If , then moving from a feasible point  to  preserves feasibility, since  . Thus, the null space defines the space of free movement directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p> <p>Row Space: The row space of , denoted , is the span of the rows of  (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to . The row space is orthogonal to the null space of :  .  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in  affect the constraints.</p> <p>Left Null Space: The left null space, denoted , is the set of all vectors  such that . These vectors are orthogonal to the columns of , and therefore orthogonal to the column space itself. In least squares problems,  represents residual directions\u2014components of  that cannot be explained by the model .</p> <p>Projection Interpretation (Least Squares):  When  has no exact solution (as in overdetermined systems), the least squares solution finds  such that  is the projection of  onto the column space of :  and the residual  lies in the left null space . This provides a geometric view: the solution projects  onto the closest point in the subspace that  can reach.</p> <p>Rank\u2013Nullity Relationship: The rank of  is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank\u2013Nullity Theorem:  where  is the number of columns of . This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in .</p> <p>Geometric Interpretation:  </p> <ul> <li>The column space represents all reachable outputs.  </li> <li>The null space represents all indistinguishable inputs that map to zero.  </li> <li>The row space represents all independent constraints imposed by .  </li> <li>The left null space captures inconsistencies or residual directions that cannot be explained by the model.  </li> </ul> <p>Together, these four subspaces define the complete geometry of the linear map .</p>"},{"location":"convex/12_vector/#inner-products-and-orthogonality","title":"Inner products and orthogonality","text":"<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality. An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:  </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\): \\(|\\langle x,y\\rangle| \\le \\|x\\|\\|y\\|~,\\) &gt;with equality iff \\(x\\) and \\(y\\) are linearly dependent. Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"convex/12_vector/#norms-and-distances","title":"Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions.</p>"},{"location":"convex/12_vector/#eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. </p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>"},{"location":"convex/12_vector/#orthogonal-projections-and-least-squares","title":"Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\). If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. \\] <p>The solution \\(x^*\\) satisfies the normal equations</p> \\[ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"convex/12_vector/#operator-norms-singular-values-and-spectral-structure","title":"Operator norms, singular values, and spectral structure","text":"<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>"},{"location":"convex/12_vector/#operator-norms","title":"Operator norms","text":"<p>Given a matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and norms \\(\\|\\cdot\\|_p\\) on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_q\\) on \\(\\mathbb{R}^m\\), the induced operator norm is defined as  This quantity measures the largest amount by which \\(A\\) can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p> <ul> <li>\\(\\|A\\|_{2 \\to 2}\\), the spectral norm, equals the largest singular value of \\(A\\).</li> <li>\\(\\|A\\|_{1 \\to 1}\\) is the maximum absolute column sum.</li> <li>\\(\\|A\\|_{\\infty \\to \\infty}\\) is the maximum absolute row sum.</li> </ul> <p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function  converges for step sizes \\(\\alpha &lt; 2 / \\|Q\\|_2\\). This shows that controlling the operator norm of the Hessian\u2014or a Lipschitz constant of the gradient\u2014directly governs how aggressively an algorithm can move.</p>"},{"location":"convex/12_vector/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) admits a factorization  where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is diagonal with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots\\). The \\(\\sigma_i\\) are the singular values of \\(A\\).</p> <p>Geometrically, the SVD shows how \\(A\\) transforms the unit ball into an ellipsoid. The columns of \\(V\\) give the principal input directions, the singular values are the lengths of the ellipsoid\u2019s axes, and the columns of \\(U\\) give the output directions. The largest singular value \\(\\sigma_{\\max}\\) equals the spectral norm \\(\\|A\\|_2\\), while the smallest \\(\\sigma_{\\min}\\) describes the least expansion (or exact flattening if \\(\\sigma_{\\min} = 0\\)).</p> <p>SVD is a powerful diagnostic tool in optimization. The ratio  is the condition number of \\(A\\). A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means \\(A\\) behaves more like a uniform scaling, which is ideal for optimization.</p>"},{"location":"convex/12_vector/#low-rank-structure","title":"Low-rank structure","text":"<p>The rank of \\(A\\) is the number of nonzero singular values. When \\(A\\) has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p>"},{"location":"convex/12_vector/#mental-map","title":"Mental Map","text":"<pre><code>                 Linear Algebra Foundations for Convex Optimization\n               Geometry + Computation for Understanding Algorithms\n                                   \u2502\n                                   \u25bc\n                        Objects: Vectors, Matrices, Maps\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Vector Spaces / Subspaces / Affine Sets                      \u2502\n      \u2502  - Feasible sets of Ax=b are affine: x = x0 + N(A)            \u2502\n      \u2502  - Feasible directions live in the null space                 \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  The Four Fundamental Subspaces of A                            \u2502\n      \u2502  - Column space C(A): reachable outputs (Ax)                    \u2502\n      \u2502  - Null space N(A): indistinguishable inputs (Ax=0)             \u2502\n      \u2502  - Row space R(A): constraint directions in x-space             \u2502\n      \u2502  - Left null space N(A\u1d40): residual directions (orthogonal to C) \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Inner Products \u2192 Orthogonality \u2192 Projections                  \u2502\n      \u2502  - Defines angles/lengths                                      \u2502\n      \u2502  - Least squares = projection of b onto C(A)                   \u2502\n      \u2502  - QR / Gram\u2013Schmidt give stable numerical tools               \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Norms &amp; Dual Norms: \"How we measure size\"                     \u2502\n      \u2502  - Unit balls define geometry of constraints/regularizers      \u2502\n      \u2502  - Dual norms bound dot products and appear in optimality      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Spectral Structure: Eigenvalues, PSD, SVD                    \u2502\n      \u2502  - PSD \u21d4 convex quadratics (Hessians of quadratic objectives)\u2502\n      \u2502  - SVD shows stretching directions and conditioning           \u2502\n      \u2502  - Condition number \u2194 convergence speed / numerical stability \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"convex/13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p> <p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton\u2019s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p> <p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work.</p>"},{"location":"convex/13_calculus/#gradients-and-directional-derivatives","title":"Gradients and Directional Derivatives","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The function is differentiable at a point \\(x\\) if there exists a vector \\(\\nabla f(x)\\) such that  meaning that the linear function \\(h \\mapsto \\nabla f(x)^\\top h\\) provides the best local approximation to \\(f\\) near \\(x\\). The gradient is the unique vector with this property.</p> <p>A closely related concept is the directional derivative. For any direction \\(v \\in \\mathbb{R}^n\\), the directional derivative of \\(f\\) at \\(x\\) in the direction \\(v\\) is  If \\(f\\) is differentiable, then  Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction \\(v\\) tells us how rapidly \\(f\\) increases when we move infinitesimally along \\(v\\).</p> <p>This immediately yields an important geometric fact. Among all unit directions \\(u\\),  is maximized when \\(u\\) points in the direction of \\(\\nabla f(x)\\), the direction of steepest ascent. The steepest descent direction is therefore \\(-\\nabla f(x)\\), which motivates gradient-descent algorithms for minimizing functions.</p> <p>For any real number \\(c\\), the level set of \\(f\\) is   </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them \u2014 in the direction of the steepest ascent of \\(f\\). If we wish to decrease \\(f\\), we move roughly in the opposite direction, \\(-\\nabla f(x)\\) (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"convex/13_calculus/#jacobians","title":"Jacobians","text":"<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>"},{"location":"convex/13_calculus/#from-derivative-to-gradient","title":"From derivative to gradient","text":"<p>For a scalar function , differentiability means that near any point ,  The gradient vector  collects all partial derivatives. Each component measures how sensitive \\(f\\) is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>"},{"location":"convex/13_calculus/#from-gradient-to-jacobian","title":"From gradient to Jacobian","text":"<p>Now consider a vector-valued function \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\),  Each output \\(F_i\\) has its own gradient. Stacking these row vectors yields the Jacobian matrix:  </p> <p>The Jacobian provides the best linear approximation of \\(F\\) near \\(x\\):  Thus, locally, the nonlinear map \\(F\\) behaves like the linear map \\(h \\mapsto J_F(x)h\\). A small displacement \\(h\\) in input space is transformed into an output change governed by the Jacobian.</p>"},{"location":"convex/13_calculus/#interpreting-the-jacobian","title":"Interpreting the Jacobian","text":"Component of \\(J_F(x)\\) Meaning Row \\(i\\) Gradient of output \\(F_i(x)\\): how the \\(i\\)-th output changes with each input variable. Column \\(j\\) Sensitivity of all outputs to \\(x_j\\): how varying input \\(x_j\\) affects the entire output vector. Determinant (when \\(m=n\\)) Local volume scaling: how \\(F\\) expands or compresses space near \\(x\\). Rank Local dimension of the image: whether any input directions are lost or collapsed. <p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>"},{"location":"convex/13_calculus/#the-hessian-and-curvature","title":"The Hessian and Curvature","text":"<p>For a twice\u2013differentiable function , the Hessian matrix collects all second-order partial derivatives:  </p> <p>The Hessian describes the local curvature of the function. While the gradient indicates the direction of steepest change, the Hessian tells us how that directional change itself varies\u2014whether the surface curves upward, curves downward, or remains nearly flat.</p>"},{"location":"convex/13_calculus/#curvature-and-positive-definiteness","title":"Curvature and positive definiteness","text":"<p>The eigenvalues of the Hessian determine its geometric behavior:</p> <ul> <li>If  (all eigenvalues nonnegative), the function is locally convex near \\(x\\).  </li> <li>If , the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li> <li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li> </ul> <p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>"},{"location":"convex/13_calculus/#taylor-approximation","title":"Taylor approximation","text":"<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>"},{"location":"convex/13_calculus/#first-order-approximation","title":"First-order approximation","text":"<p>If \\(f\\) is differentiable at \\(x\\), then for small steps \\(d\\),  The gradient gives the best linear model of the function near \\(x\\). This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>"},{"location":"convex/13_calculus/#second-order-approximation","title":"Second-order approximation","text":"<p>If \\(f\\) is twice differentiable, we can include curvature information:  The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p> <ul> <li>If , the quadratic term is nonnegative and the function curves upward\u2014locally bowl-shaped.</li> <li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others\u2014characteristic of saddle points.</li> </ul>"},{"location":"convex/13_calculus/#role-in-optimization-algorithms","title":"Role in optimization algorithms","text":"<p>Second-order Taylor models are the basis of Newton-type methods. Newton\u2019s method chooses \\(d\\) by approximately minimizing the quadratic model,  which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p> <p>Thus, Taylor expansions connect a function\u2019s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>"},{"location":"convex/13_calculus/#smoothness-and-strong-convexity","title":"Smoothness and Strong Convexity","text":"<p>In optimization, the behavior of a function\u2019s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>"},{"location":"convex/13_calculus/#lipschitz-continuous-gradients-l-smoothness","title":"Lipschitz continuous gradients (L-smoothness)","text":"<p>A differentiable function  has an \\(L\\)-Lipschitz continuous gradient if  This condition limits how quickly the gradient can change. Intuitively, an \\(L\\)-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:  This inequality states that every \\(L\\)-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p> <p>In gradient descent, smoothness directly determines a safe step size: choosing  ensures that each update decreases the function value for convex objectives. In machine learning, the constant \\(L\\) effectively controls how large the learning rate can be before training becomes unstable.</p>"},{"location":"convex/13_calculus/#strong-convexity","title":"Strong convexity","text":"<p>A differentiable function  is -strongly convex if, for some ,  This condition guarantees that \\(f\\) has at least \\(\\mu\\) amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p> <p>Strong convexity has major optimization implications:</p> <ul> <li>The minimizer is unique.  </li> <li>Gradient descent converges linearly with step size \\(\\eta \\le 1/L\\).  </li> <li>The ratio \\(L / \\mu\\) (the condition number) dictates convergence speed.</li> </ul>"},{"location":"convex/13_calculus/#curvature-in-both-directions","title":"Curvature in both directions","text":"<p>Together, smoothness and strong convexity bound the curvature of \\(f\\):  Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p> <p>These concepts, imiting curvature from above via \\(L\\) and from below via \\(\\mu\\), form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>"},{"location":"convex/13_calculus/#mental-map","title":"Mental map","text":"<pre><code>                Multivariable Calculus for Optimization\n        How objectives change, how curvature shapes algorithms\n                              \u2502\n                              \u25bc\n                 Local change of a scalar function f(x)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Differentiability &amp; First-Order Model                     \u2502\n     \u2502 f(x+h) = f(x) + \u2207f(x)\u1d40h + o(\u2016h\u2016)                          \u2502\n     \u2502 - \u2207f(x): best linear approximation                        \u2502\n     \u2502 - Directional derivative: D_v f(x) = \u2207f(x)\u1d40v              \u2502\n     \u2502 - Steepest descent: move along -\u2207f(x)                     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Geometry of Level Sets                                      \u2502\n     \u2502 L_c = {x : f(x)=c}                                          \u2502\n     \u2502 - If \u2207f(x) \u2260 0, then \u2207f(x) \u27c2 level set at x                 \u2502\n     \u2502 - Connects to constrained optimality (later: KKT)           \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Vector-Valued Maps &amp; Jacobians                              \u2502\n     \u2502 F: \u211d\u207f \u2192 \u211d\u1d50                                                  \u2502\n     \u2502 - Jacobian J_F(x) stacks gradients of outputs               \u2502\n     \u2502 - Linearization: F(x+h) \u2248 F(x) + J_F(x) h                   \u2502\n     \u2502 - Chain rule foundation for backprop / sensitivity analysis \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Structure: Hessian &amp; Curvature               \u2502\n     \u2502 \u2207\u00b2f(x): matrix of second partials                         \u2502\n     \u2502 - Curvature along v: v\u1d40\u2207\u00b2f(x)v                            \u2502\n     \u2502 - Eigenvalues quantify steep/flat directions              \u2502\n     \u2502 - PSD/PD Hessian ties directly to convexity (Ch.5)        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Taylor Models \u2192 Algorithm Design                          \u2502\n     \u2502 First-order:  f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d                     \u2502\n     \u2502 Second-order: f(x+d) \u2248 f(x) + \u2207f(x)\u1d40d + \u00bd d\u1d40\u2207\u00b2f(x)d       \u2502\n     \u2502 - Gradient descent uses the linear model                  \u2502\n     \u2502 - Newton uses the quadratic model: d \u2248 -(\u2207\u00b2f)^{-1}\u2207f      \u2502\n     \u2502 - Trust-region / quasi-Newton approximate curvature       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Global Control of Local Behavior: Smoothness &amp; Strong Convexity\u2502\n     \u2502 L-smooth: \u2016\u2207f(x)-\u2207f(y)\u2016 \u2264 L\u2016x-y\u2016                               \u2502\n     \u2502 - Descent Lemma gives a quadratic upper bound                  \u2502\n     \u2502 - Sets safe step size: \u03b7 \u2264 1/L (for convex objectives)         \u2502\n     \u2502 \u03bc-strongly convex: f lies above tangents by (\u03bc/2)\u2016y-x\u2016\u00b2        \u2502\n     \u2502 - Unique minimizer, linear convergence of gradient descent     \u2502\n     \u2502 Combined curvature bounds: \u03bcI \u2aaf \u2207\u00b2f(x) \u2aaf LI                   \u2502\n     \u2502 - Condition number \u03ba = L/\u03bc governs difficulty                  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"convex/14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p>"},{"location":"convex/14_convexsets/#convex-sets","title":"Convex sets","text":"<p>A set  is convex if for any two points  and any ,  That is, the entire line segment between \\(x\\) and \\(y\\) lies inside the set. Convex sets have no \u201choles\u201d or \u201cindentations,\u201d and this geometric regularity is what makes optimization over them tractable.</p>"},{"location":"convex/14_convexsets/#examples","title":"Examples","text":"<ul> <li>Affine subspaces: .  </li> <li>Halfspaces: .  </li> <li>Euclidean balls: .  </li> <li>  balls (axis-aligned boxes): .  </li> <li>Probability simplex: .  </li> </ul> <p>A set fails to be convex whenever some segment between two feasible points leaves the set; for example, a crescent or an annulus.</p>"},{"location":"convex/14_convexsets/#affine-sets-hyperplanes-and-halfspaces","title":"Affine sets, hyperplanes, and halfspaces","text":"<p>Affine sets generalize linear subspaces by allowing a shift. A set \\(A\\) is affine if for some point \\(x_0\\) and subspace \\(S\\),  Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p> <p>A hyperplane is an affine set defined by a single linear equation:  Hyperplanes act as the \u201cflat boundaries\u201d of higher-dimensional space and are the fundamental building blocks of polyhedra.</p> <p>A halfspace is one side of a hyperplane:  Halfspaces are convex and serve as basic local approximations to general convex sets.</p>"},{"location":"convex/14_convexsets/#convex-combinations-and-convex-hulls","title":"Convex combinations and convex hulls","text":"<p>A convex combination of points  is a weighted average  Convex sets are precisely those that contain all convex combinations of their points.</p> <p>The convex hull of a set \\(S\\), denoted \\(\\operatorname{conv}(S)\\), is the set of all convex combinations of finitely many points in \\(S\\). It is the smallest convex set containing \\(S\\). Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p>"},{"location":"convex/14_convexsets/#polyhedra-and-polytopes","title":"Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are always convex; they may be bounded or unbounded.</p> <p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>"},{"location":"convex/14_convexsets/#extreme-points","title":"Extreme points","text":"<p>Let  be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if  implies .</p> <p>Geometrically, extreme points are the \u201ccorners\u201d of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems, such as linear programs achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>"},{"location":"convex/14_convexsets/#cones","title":"Cones","text":"<p>Cones generalize the idea of \u201cdirections\u201d in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>"},{"location":"convex/14_convexsets/#basic-definition","title":"Basic definition","text":"<p>A set \\(K \\subseteq \\mathbb{R}^n\\) is a cone if  A cone is convex if it is also closed under addition:  </p> <p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant , each coordinate axis direction is an extreme ray.</p>"},{"location":"convex/14_convexsets/#conic-hull","title":"Conic hull","text":"<p>Given any set \\(S\\), its conic hull is the set of all conic combinations:  This is the smallest convex cone containing \\(S\\). Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>"},{"location":"convex/14_convexsets/#polar-cones","title":"Polar cones","text":"<p>For a cone \\(K\\), the polar cone is defined as  </p> <p>Intuition:</p> <ul> <li>Polar vectors make a nonacute angle with every vector in \\(K\\).  </li> </ul> <p>Key properties:</p> <ul> <li>\\(K^\\circ\\) is always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, then \\(K^\\circ\\) is the orthogonal complement.  </li> <li>For any closed convex cone, </li> </ul> <p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>"},{"location":"convex/14_convexsets/#tangent-cones","title":"Tangent cones","text":"<p>For a set \\(C\\) and a point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) consists of all feasible \u201cinfinitesimal directions\u201d from \\(x\\):  </p> <p>Intuition:</p> <ul> <li>At an interior point, \\(T_C(x) = \\mathbb{R}^n\\): all small moves are allowed.  </li> <li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li> </ul> <p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>"},{"location":"convex/14_convexsets/#normal-cones","title":"Normal cones","text":"<p>For a convex set \\(C\\), the normal cone at a point \\(x \\in C\\) is  </p> <p>Interpretation:</p> <ul> <li>Every \\(v \\in N_C(x)\\) defines a supporting hyperplane to \\(C\\) at \\(x\\).  </li> <li>At interior points, the normal cone is \\(\\{0\\}\\).  </li> <li>At boundary or corner points, it becomes a pointed cone of outward normals.</li> </ul> <p>A fundamental relationship ties tangent and normal cones together:  </p> <p>Normal cones appear directly in first-order optimality conditions. For a constrained problem  </p> \\[\\min_{x \\in C} f(x), \\] <p>a point \\(x^*\\) is optimal only if</p> \\[0 \\in \\nabla f(x^*) + N_C(x^*).\\] <p>This expresses a balance between the objective\u2019s slope and the \u201cpushback\u2019\u2019 from the constraint set.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplanes-and-separation","title":"Supporting Hyperplanes and Separation","text":"<p>One of the most important geometric facts about convex sets is that they can be supported or separated by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape.</p>"},{"location":"convex/14_convexsets/#supporting-hyperplane-theorem","title":"Supporting Hyperplane Theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be nonempty, closed, and convex, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero vector \\(a\\) such that</p> \\[ a^\\top x \\le a^\\top x_0 \\qquad \\forall x \\in C. \\] <p>This means that the hyperplane</p> \\[ a^\\top x = a^\\top x_0 \\] <p>touches \\(C\\) at \\(x_0\\) but does not cut through it. The vector \\(a\\) is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>"},{"location":"convex/14_convexsets/#separating-hyperplane-theorem","title":"Separating Hyperplane Theorem","text":"<p>If \\(C\\) and \\(D\\) are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector \\(a\\) and scalar \\(b\\) such that</p> \\[ a^\\top x \\le b \\quad \\forall x \\in C, \\qquad a^\\top y \\ge b \\quad \\forall y \\in D. \\] <p>The hyperplane \\(a^\\top x = b\\) places all points of \\(C\\) on one side and all points of \\(D\\) on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p>"},{"location":"convex/14_convexsets/#mental-map","title":"Mental Map","text":"<pre><code>               Convex Sets &amp; Geometric Fundamentals\n     Feasible regions, geometry of constraints, and separation\n                              \u2502\n                              \u25bc\n                 Core idea: convexity removes \"bad geometry\"\n        (segments stay inside \u2192 no holes/indentations \u2192 tractable)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Definition of Convex Set                                  \u2502\n     \u2502 C convex \u21d4  \u03b8x + (1-\u03b8)y \u2208 C  for all x,y\u2208C, \u03b8\u2208[0,1]      \u2502\n     \u2502 - Geometry: every chord lies inside                       \u2502\n     \u2502 - Optimization: feasible region supports global reasoning \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Affine Geometry: the \"flat\" building blocks               \u2502\n     \u2502 - Affine set: x0 + S                                      \u2502\n     \u2502 - Hyperplane: {x : a\u1d40x = b}                               \u2502\n     \u2502 - Halfspace:  {x : a\u1d40x \u2264 b}                               \u2502\n     \u2502 Role: linear constraints and local linear boundaries      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Convex Combinations &amp; Convex Hull                         \u2502\n     \u2502 - Convex combination: \u03a3 \u03b8_i x_i, \u03b8_i\u22650, \u03a3\u03b8_i=1            \u2502\n     \u2502 - conv(S): all convex combos of points in S               \u2502\n     \u2502 Why it matters: convexification / relaxations / geometry  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Polyhedra &amp; Polytopes                                     \u2502\n     \u2502 - Polyhedron: intersection of finitely many halfspaces    \u2502\n     \u2502   P = {x : Ax \u2264 b}                                        \u2502\n     \u2502 - Polytope: bounded polyhedron                            \u2502\n     \u2502 Why it matters: LP feasible sets; two views (H- vs V-form)\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Extreme Points (Corners)                                     \u2502\n     \u2502 - x extreme \u21d4 cannot be written as nontrivial convex combo  \u2502\n     \u2502 - For polytopes: extremes = vertices                         \u2502\n     \u2502 Optimization link: linear objectives attain optima at corners\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Cones: scaling geometry for constraints &amp; duality         \u2502\n     \u2502 - Cone: x\u2208K, \u03b1\u22650 \u21d2 \u03b1x\u2208K                                  \u2502\n     \u2502 - Convex cone: also closed under addition                 \u2502\n     \u2502 - Conic hull cone(S): smallest convex cone containing S   \u2502\n     \u2502 - Extreme rays replace extreme points                     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Local Directional Geometry at a Point x                     \u2502\n     \u2502 Tangent cone T_C(x): feasible infinitesimal directions      \u2502\n     \u2502 - Interior point: T_C(x)=\u211d\u207f                                 \u2502\n     \u2502 - Boundary: directions restricted                           \u2502\n     \u2502 Normal cone N_C(x): outward normals / supporting directions \u2502\n     \u2502 - Interior point: N_C(x)={0}                                \u2502\n     \u2502 - Boundary/corner: pointed cone of normals                  \u2502\n     \u2502 Duality relation: N_C(x) = (T_C(x))\u00b0                        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Supporting Hyperplanes &amp; Separation                       \u2502\n     \u2502 Supporting hyperplane at boundary point x0:               \u2502\n     \u2502   \u2203a\u22600 s.t. a\u1d40x \u2264 a\u1d40x0  for all x\u2208C                       \u2502\n     \u2502 Separating hyperplane for disjoint convex sets C,D:       \u2502\n     \u2502   \u2203a,b s.t. a\u1d40x \u2264 b \u2264 a\u1d40y  for x\u2208C, y\u2208D                   \u2502\n     \u2502 Why it matters: geometry behind subgradients and duality  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"convex/15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>"},{"location":"convex/15_convexfunctions/#definitions-of-convexity","title":"Definitions of convexity","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),  </p> <p>The graph of \\(f\\) never dips below the straight line between \\((x,f(x))\\) and \\((y,f(y))\\). If the inequality is strict whenever \\(x \\neq y\\), the function is strictly convex.</p> <p>A powerful geometric viewpoint comes from the epigraph:  The function \\(f\\) is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p> <p></p> <p>A function is convex if and only if the region above its grapH is a convex set. This region is the function's epigraph.: Wikipedia</p>"},{"location":"convex/15_convexfunctions/#first-order-characterisation","title":"First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if  </p> <p>Interpretation:</p> <ul> <li>The tangent plane at any point \\(x\\) lies below the function everywhere.</li> <li>\\(\\nabla f(x)\\) defines a supporting hyperplane to the epigraph.</li> <li>The gradient provides a global linear underestimator of \\(f\\).</li> </ul> <p>This geometric picture is crucial in optimisation: at a minimiser \\(x^\\star\\), convexity implies </p> <p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>"},{"location":"convex/15_convexfunctions/#second-order-characterisation","title":"Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then convexity can be checked via curvature:</p> \\[ f \\text{ is convex } \\iff \\nabla^2 f(x) \\succeq 0 \\text{ for all } x. \\] <ul> <li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li> <li>If \\(\\nabla^2 f(x) \\succ 0\\) everywhere, the function is strictly convex.  </li> <li>Negative eigenvalues indicate directions of negative curvature \u2014 impossible for convex functions.</li> </ul>"},{"location":"convex/15_convexfunctions/#examples-of-convex-functions","title":"Examples of convex functions","text":"<ol> <li> <p>Affine functions:     Always convex (and concave). They define supporting hyperplanes.</p> </li> <li> <p>Quadratic functions with PSD Hessian:     Convex because the curvature matrix \\(Q\\) is PSD.</p> </li> <li> <p>Norms:     All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p> </li> <li> <p>Maximum of affine functions:     Convex because the maximum of convex functions is convex.    (Important in SVM hinge loss.)</p> </li> <li> <p>Log-sum-exp:     A smooth approximation to the max; convex by Jensen\u2019s inequality. Appears in softmax, logistic regression, partition functions.</p> </li> </ol>"},{"location":"convex/15_convexfunctions/#jensens-inequality","title":"Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex and \\(X\\) a random variable in its domain. Then:  </p> <p>This generalises the definition of convexity from finite averages to expectations. Practically:</p> <ul> <li>convex functions \u201cpull upward\u201d under averaging,</li> <li>log-sum-exp is convex because exponential is convex,</li> <li>EM and variational methods rely on Jensen to construct lower bounds.</li> </ul> <p>As a finite form, for \\(\\theta_i \\ge 0\\) with \\(\\sum \\theta_i = 1\\),  </p>"},{"location":"convex/15_convexfunctions/#operations-that-preserve-convexity","title":"Operations that preserve convexity","text":"<p>Convexity is preserved under many natural constructions:</p> <ul> <li> <p>Nonnegative scaling:   If \\(f\\) is convex and \\(\\alpha \\ge 0\\), then \\(\\alpha f\\) is convex.</p> </li> <li> <p>Addition:   If \\(f\\) and \\(g\\) are convex, then \\(f+g\\) is convex.</p> </li> <li> <p>Maximum: \\(\\max\\{f,g\\}\\) is convex.</p> </li> <li> <p>Affine pre-composition:   If \\(A\\) is a matrix,      is convex.</p> </li> <li> <p>Monotone composition rule:   If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex,   then \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex.</p> </li> </ul>"},{"location":"convex/15_convexfunctions/#level-sets-of-convex-functions","title":"Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), the sublevel set is  </p> <p>If \\(f\\) is convex, every sublevel set is convex. This property is crucial because inequalities \\(f(x) \\le \\alpha\\) are ubiquitous in constraints.</p> <p>Examples:</p> <ul> <li>Norm balls: \\(\\{ x : \\|x\\|_2 \\le r \\}\\) </li> <li>Linear regression confidence ellipsoids: \\(\\{ x : \\|Ax - b\\|_2 \\le \\epsilon \\}\\)</li> </ul> <p>These sets enable convex constrained optimisation formulations.</p>"},{"location":"convex/15_convexfunctions/#mental-map","title":"Mental Map","text":"<pre><code>                      Convex Functions\n      Objective landscapes with predictable geometry and guarantees\n                              \u2502\n                              \u25bc\n                Core idea: no bad local minima\n        (every local minimum is global; geometry is well-behaved)\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Definition of Convexity                                   \u2502\n     \u2502 f(\u03b8x+(1\u2212\u03b8)y) \u2264 \u03b8f(x)+(1\u2212\u03b8)f(y)                            \u2502\n     \u2502 - Graph lies below all chords                             \u2502\n     \u2502 - Strict convexity: inequality is strict                  \u2502\n     \u2502 - Epigraph view: f convex \u21d4 epi(f) is a convex set       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Geometry (Supporting Hyperplanes)              \u2502\n     \u2502 f(y) \u2265 f(x)+\u2207f(x)\u1d40(y\u2212x)                                    \u2502\n     \u2502 - Tangent plane globally underestimates f                  \u2502\n     \u2502 - \u2207f(x) defines a supporting hyperplane to epi(f)          \u2502\n     \u2502 - Optimality: \u2207f(x*)=0 \u21d4 x* global minimizer (smooth case)\u2502\n     \u2502 - Nonsmooth extension: subgradients (next chapter)         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Characterisation                              \u2502\n     \u2502 \u2207\u00b2f(x) \u2ab0 0  for all x                                      \u2502\n     \u2502 - PSD Hessian \u21d4 upward curvature everywhere               \u2502\n     \u2502 - PD Hessian \u21d4 strict convexity                           \u2502\n     \u2502 - Links convexity to eigenvalues and curvature (Ch.3)      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Canonical Examples                                          \u2502\n     \u2502 - Affine functions: supporting hyperplanes                  \u2502\n     \u2502 - Quadratics (Q\u2ab00): curvature from Hessian                  \u2502\n     \u2502 - Norms: regularization geometry                            \u2502\n     \u2502 - Max of affine functions: hinge loss, LPs                  \u2502\n     \u2502 - Log-sum-exp: smooth max, softmax, logistic regression     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Jensen\u2019s Inequality                                         \u2502\n     \u2502 f(E[X]) \u2264 E[f(X)]                                           \u2502\n     \u2502 - Convex functions penalize variability                     \u2502\n     \u2502 - Basis for EM, variational bounds, log-sum-exp convexity   \u2502\n     \u2502 - Extends convexity from points to expectations             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Convexity-Preserving Operations                             \u2502\n     \u2502 - Scaling (\u03b1\u22650), addition                                   \u2502\n     \u2502 - Max of convex functions                                   \u2502\n     \u2502 - Affine pre-composition f(Ax+b)                            \u2502\n     \u2502 - Monotone composition rules                                \u2502\n     \u2502 Role: modular construction of convex models                 \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Sublevel Sets                                               \u2502\n     \u2502 {x : f(x) \u2264 \u03b1}                                              \u2502\n     \u2502 - Always convex for convex f                                \u2502\n     \u2502 - Enables convex inequality constraints                     \u2502\n     \u2502 - Norm balls, confidence ellipsoids, feasibility regions    \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"convex/16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p> <ul> <li>the  norm  (nondifferentiable at zero),</li> <li>pointwise-max functions such as ,</li> <li>the hinge loss  used in SVMs,</li> <li>regularisers like total variation or indicator functions of convex sets.</li> </ul> <p>Although these functions have \u201ckinks\u201d, they remain convex and convexity guarantees the existence of supporting hyperplanes at every point.</p>"},{"location":"convex/16_subgradients/#subgradients-and-the-subdifferential","title":"Subgradients and the Subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex.  A vector \\(g \\in \\mathbb{R}^n\\) is a subgradient of \\(f\\) at \\(x\\) if</p> \\[ f(y) \\ge f(x) + g^\\top (y - x) \\quad \\text{for all } y. \\] <p>Geometric interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top(y-x)\\) is a global underestimator of \\(f\\).</li> <li>Each subgradient defines a supporting hyperplane touching the epigraph of \\(f\\) at \\((x, f(x))\\).</li> <li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li> <li>At kinks, there may be infinitely many supporting hyperplanes.</li> </ul> <p>The subdifferential of \\(f\\) at \\(x\\) is the set  </p> <p>Properties:</p> <ul> <li>  is always a nonempty convex set (if \\(x\\) is in the interior of the domain).</li> <li>If \\(f\\) is differentiable at \\(x\\), then </li> <li>If \\(f\\) is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li> </ul> <p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>"},{"location":"convex/16_subgradients/#examples","title":"Examples","text":""},{"location":"convex/16_subgradients/#absolute-value-in-1d","title":"Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\). Then:</p> <ul> <li>If \\(t &gt; 0\\),  \\(\\partial f(t) = \\{1\\}\\).</li> <li>If \\(t &lt; 0\\),  \\(\\partial f(t) = \\{-1\\}\\).</li> <li>If \\(t = 0\\), </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) supports the graph from below.</p>"},{"location":"convex/16_subgradients/#the-ell_1-norm","title":"The  norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\):</p> \\[ g \\in \\partial \\|x\\|_1 \\quad\\Longleftrightarrow\\quad g_i \\in \\partial |x_i|. \\] <p>Thus:</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>"},{"location":"convex/16_subgradients/#pointwise-maximum-of-affine-functions","title":"Pointwise maximum of affine functions","text":"<p>Let </p> <ul> <li> <p>If only one index \\(i^\\star\\) achieves the maximum at \\(x\\), then </p> </li> <li> <p>If multiple indices are tied, then    the convex hull of the active slopes.</p> </li> </ul> <p>This structure underlies SVM hinge loss and ReLU-type functions.</p>"},{"location":"convex/16_subgradients/#subgradient-optimality-condition","title":"Subgradient Optimality Condition","text":"<p>For the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>a point \\(x^\\star\\) is optimal if and only if</p> \\[ 0 \\in \\partial f(x^\\star). \\] <p>Interpretation:</p> <ul> <li>At optimality, no subgradient points to a direction that would decrease \\(f\\).</li> <li>Geometrically, the supporting hyperplane at \\(x^\\star\\) is horizontal, forming the flat bottom of the convex bowl.</li> <li>This generalises the smooth condition .</li> </ul>"},{"location":"convex/16_subgradients/#subgradient-calculus","title":"Subgradient Calculus","text":"<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let \\(f\\) and \\(g\\) be convex.</p>"},{"location":"convex/16_subgradients/#sum-rule","title":"Sum rule","text":"\\[ \\partial(f+g)(x) \\subseteq \\partial f(x) + \\partial g(x) = \\{ u+v : u \\in \\partial f(x),\\ v \\in \\partial g(x) \\}. \\] <p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>"},{"location":"convex/16_subgradients/#affine-composition","title":"Affine composition","text":"<p>If \\(h(x) = f(Ax + b)\\), then  </p> <p>This rule is heavily used in machine learning models, where losses depend on linear predictions \\(Ax\\).</p>"},{"location":"convex/16_subgradients/#maximum-of-convex-functions","title":"Maximum of convex functions","text":"<p>If \\(f(x) = \\max_i f_i(x)\\), then  </p> <p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>"},{"location":"convex/16a_optimality_conditions/","title":"7. First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"convex/16a_optimality_conditions/#chapter-7-first-order-and-geometric-optimality-conditions","title":"Chapter 7: First-Order and Geometric Optimality Conditions","text":"<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar \u201czero gradient\u201d condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p> <p>These conditions form the conceptual bridge between unconstrained minimization and the Karush\u2013Kuhn\u2013Tucker (KKT) framework developed in the next chapter.</p>"},{"location":"convex/16a_optimality_conditions/#orders-of-optimality-why-first-order-is-enough-in-convex-optimization","title":"Orders of Optimality: Why First Order is Enough in Convex Optimization","text":"<p>For a differentiable function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), the \u201corder\u2019\u2019 of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer \\(x^\\star\\):</p> Order Object inspected Role First-order \\(\\nabla f(x^\\star)\\) or subgradients Detects existence of a local descent direction Second-order Hessian \\(\\nabla^2 f(x^\\star)\\) Examines curvature (minimum vs saddle vs maximum) Higher-order Third derivative and beyond Rarely used; only for degenerate cases with vanishing curvature <p>In general nonconvex optimization, these conditions are used together: a point may have \\(\\nabla f(x^\\star) = 0\\) but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p> <p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{whenever the Hessian exists}. \\] <p>Therefore:</p> <ul> <li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li> <li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li> </ul> <p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>"},{"location":"convex/16a_optimality_conditions/#motivation","title":"Motivation","text":"<p>Consider the basic convex problem  where \\(f\\) is convex and \\(\\mathcal{X}\\) is a convex set.</p> <p>Intuitively, a point \\(\\hat{x}\\) is optimal if there is no feasible direction in which we can move and strictly decrease \\(f\\). In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside \\(\\mathcal{X}\\) are allowed.</p> <p>Thus, optimality can be seen as an equilibrium:</p> <ul> <li>the objective\u2019s tendency to decrease (captured by its gradient or subgradient)  </li> <li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li> </ul>"},{"location":"convex/16a_optimality_conditions/#unconstrained-convex-problems","title":"Unconstrained Convex Problems","text":"<p>For the unconstrained problem  with \\(f\\) convex, the optimality conditions are especially simple.</p>"},{"location":"convex/16a_optimality_conditions/#smooth-case","title":"Smooth case","text":"<p>If \\(f\\) is differentiable, then a point \\(\\hat{x}\\) is optimal if and only if  </p> <p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>"},{"location":"convex/16a_optimality_conditions/#nonsmooth-case","title":"Nonsmooth case","text":"<p>If \\(f\\) is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes  </p> <p>Interpretation:</p> <ul> <li>The origin lies in the set of all subgradients at \\(\\hat{x}\\).  </li> <li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of \\(f\\) at \\((\\hat{x}, f(\\hat{x}))\\).  </li> <li>No direction in \\(\\mathbb{R}^n\\) gives a first-order improvement in the objective.</li> </ul> <p>For smooth \\(f\\), this reduces to the usual condition \\(\\nabla f(\\hat{x}) = 0\\).</p>"},{"location":"convex/16a_optimality_conditions/#constrained-convex-problems","title":"Constrained Convex Problems","text":"<p>Now consider the constrained problem  where \\(f\\) is convex and \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) is a nonempty closed convex set.</p> <p>If \\(\\hat{x}\\) lies strictly inside \\(\\mathcal{X}\\), then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,  remains the necessary and sufficient condition for optimality.</p> <p>The interesting case is when \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\).</p>"},{"location":"convex/16a_optimality_conditions/#first-order-condition-with-constraints","title":"First-order condition with constraints","text":"<p>The general first-order optimality condition for the constrained convex problem is:  </p> <p>That is, there exist</p> <ul> <li>a subgradient \\(g \\in \\partial f(\\hat{x})\\), and  </li> <li>a normal vector \\(v \\in N_{\\mathcal{X}}(\\hat{x})\\)</li> </ul> <p>such that  </p> <p>Interpretation:</p> <ul> <li>The objective\u2019s slope \\(g\\) is exactly balanced by a normal vector \\(v\\) coming from the constraint set.  </li> <li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which \\(f\\) can decrease.  </li> <li>Geometrically, the epigraph of \\(f\\) and the feasible set meet with aligned supporting hyperplanes at \\(\\hat{x}\\).</li> </ul> <p>Special cases:</p> <ul> <li>If \\(\\hat{x}\\) is an interior point, then \\(N_{\\mathcal{X}}(\\hat{x}) = \\{0\\}\\), so we recover the unconstrained condition \\(0 \\in \\partial f(\\hat{x})\\).  </li> <li>If \\(\\mathcal{X}\\) is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li> </ul>"},{"location":"convex/17_kkt/","title":"8. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"convex/17_kkt/#chapter-8-lagrange-multipliers-and-the-kkt-framework","title":"Chapter 8: Lagrange Multipliers and the KKT Framework","text":"<p>We now have the ingredients for understanding optimality in convex optimization:</p> <ul> <li>convex functions define well-behaved objectives,</li> <li>convex sets describe feasible regions,</li> <li>gradients and subgradients encode descent directions.</li> </ul> <p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions\u2014the Karush\u2013Kuhn\u2013Tucker (KKT) conditions\u2014is the central optimality framework for constrained convex optimization.</p> <p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by \u201cforces\u2019\u2019 coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>"},{"location":"convex/17_kkt/#unconstrained-convex-minimization","title":"Unconstrained Convex Minimization","text":"<p>Consider the problem  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent iteratively updates  with step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Moving opposite the gradient decreases \\(f\\).</li> <li>If the gradient is Lipschitz continuous and the step size is small enough (\\(\\alpha_k \\le 1/L\\)), then gradient descent converges to a global minimizer.</li> <li>If \\(f\\) is strongly convex, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li> </ul>"},{"location":"convex/17_kkt/#equality-constrained-problems-and-lagrange-multipliers","title":"Equality-Constrained Problems and Lagrange Multipliers","text":"<p>Now consider minimizing \\(f\\) subject to equality constraints:  </p> <p>Define the Lagrangian  where \\(\\lambda = (\\lambda_1,\\dots,\\lambda_p)\\) are the Lagrange multipliers.</p> <p>Under differentiability and regularity assumptions, a point \\(x^*\\) is optimal only if:</p> <ol> <li> <p>Primal feasibility     </p> </li> <li> <p>Stationarity     </p> </li> </ol> <p>Geometric meaning:</p> <ul> <li>The feasible set  is typically a smooth manifold.</li> <li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li> <li>The multipliers \\(\\lambda_j^*\\) weight the constraint normals to exactly cancel the objective\u2019s gradient.</li> </ul> <p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>"},{"location":"convex/17_kkt/#inequality-constraints-and-the-kkt-conditions","title":"Inequality Constraints and the KKT Conditions","text":"<p>Now consider the general convex problem:  </p> <p>Form the Lagrangian  with:</p> <ul> <li>  (equality multipliers),</li> <li>  (inequality multipliers).</li> </ul> <p>A point \\(x^*\\) with multipliers \\((\\lambda^*,\\mu^*)\\) satisfies the KKT conditions:</p>"},{"location":"convex/17_kkt/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^*) \\le 0,\\quad \\forall i, \\qquad h_j(x^*) = 0,\\quad \\forall j. \\]"},{"location":"convex/17_kkt/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^* \\ge 0,\\quad \\forall i. \\]"},{"location":"convex/17_kkt/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^*)  + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*) = 0. \\]"},{"location":"convex/17_kkt/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^*\\, g_i(x^*) = 0, \\quad i=1,\\dots,m. \\] <p>Complementary slackness expresses a clear dichotomy:</p> <ul> <li>If constraint \\(g_i(x) \\le 0\\) is inactive (strictly \\(&lt;0\\)), then it applies no force: \\(\\mu_i^* = 0\\).</li> <li>If a constraint is active at the boundary, it may exert a force: \\(\\mu_i^* &gt; 0\\), and then \\(g_i(x^*) = 0\\).</li> </ul> <p>Only active constraints can push back against the objective.</p> <p> Wikipedia</p>"},{"location":"convex/17_kkt/#slaters-condition-guaranteeing-strong-duality","title":"Slater\u2019s Condition \u2014 Guaranteeing Strong Duality","text":"<p>The KKT conditions always provide necessary conditions for optimality. For them to also be sufficient (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p> <p>For convex problems with convex \\(g_i\\) and affine \\(h_j\\), Slater\u2019s condition holds if there exists a strictly feasible point:  </p> <p>Interpretation:</p> <ul> <li>The feasible region contains an interior point.</li> <li>The constraints are not \u201ctight\u201d everywhere.</li> <li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li> </ul> <p>When Slater\u2019s condition holds:</p> <ol> <li> <p>Strong duality holds: </p> </li> <li> <p>The dual optimum is attained.</p> </li> <li> <p>The KKT conditions are both necessary and sufficient for optimality.</p> </li> </ol>"},{"location":"convex/17_kkt/#duality-gap","title":"Duality gap","text":"<p>For a primal problem with optimum \\(p^*\\) and its dual with optimum \\(d^*\\), the duality gap is  </p> <ul> <li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li> <li>Slater\u2019s condition ensures the gap is zero.</li> </ul> <p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>"},{"location":"convex/17_kkt/#geometric-and-physical-interpretation","title":"Geometric and Physical Interpretation","text":"<p>The KKT conditions describe an equilibrium of forces:</p> <ul> <li>The objective gradient pushes the point in the direction of steepest decrease.</li> <li>Active constraints push back through normal vectors scaled by multipliers.</li> <li>At optimality, these forces exactly cancel.</li> </ul> <p>Physically:</p> <ul> <li>Lagrange multipliers are \u201creaction forces\u2019\u2019 keeping a system on the constraint surface.</li> <li>In economics, they are \u201cshadow prices\u2019\u2019 indicating how much the objective would improve if a constraint were relaxed.</li> <li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li> </ul>"},{"location":"convex/18_duality/","title":"9. Lagrange Duality Theory","text":""},{"location":"convex/18_duality/#chapter-9-lagrange-duality-theory","title":"Chapter 9: Lagrange Duality Theory","text":"<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p> <ul> <li>lower bounds on the primal optimal value,</li> <li>certificates of optimality,</li> <li>interpretations of constraint \u201cprices,\u201d</li> <li>and alternative algorithmic routes to solutions.</li> </ul> <p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality \u2014 strong duality \u2014 lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>"},{"location":"convex/18_duality/#the-primal-problem","title":"The Primal Problem","text":"<p>Consider the general convex problem</p> \\[ \\begin{array}{ll} \\text{minimize} &amp; f(x) \\\\ \\text{subject to} &amp; g_i(x) \\le 0,\\quad i=1,\\dots,m, \\\\  &amp; h_j(x) = 0,\\quad j=1,\\dots,p, \\end{array} \\] <p>where:</p> <ul> <li>\\(f\\) and each \\(g_i\\) are convex,</li> <li>each equality constraint \\(h_j\\) is affine.</li> </ul> <p>The optimal value is</p> \\[ f^\\star = \\inf\\{ f(x) : g_i(x) \\le 0,\\ h_j(x)=0 \\}. \\] <p>The infimum allows for the possibility that the best value is approached but not attained.</p>"},{"location":"convex/18_duality/#why-duality","title":"Why Duality?","text":"<p>A constrained problem can be viewed as:</p> <p>minimize \\(f(x)\\) but pay a penalty whenever constraints are violated.</p> <p>If the penalties are chosen \u201ccorrectly,\u201d one can recover the original constrained problem from an unconstrained penalized problem. Dual variables \u2014 \\(\\mu_i\\) for inequalities and \\(\\lambda_j\\) for equalities \u2014 precisely encode these penalties:</p> <ul> <li>\\(\\mu_i\\) measures how costly it is to violate \\(g_i(x)\\le 0\\),</li> <li>\\(\\lambda_j\\) measures the sensitivity of the objective to relaxing \\(h_j(x)=0\\).</li> </ul> <p>Duality converts constraints into prices, and transforms geometry into algebra.</p>"},{"location":"convex/18_duality/#the-lagrangian","title":"The Lagrangian","text":"<p>The Lagrangian function is</p> \\[ L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^p \\lambda_j h_j(x), \\] <p>with:</p> <ul> <li>\\(\\mu_i \\ge 0\\) for inequality constraints,</li> <li>\\(\\lambda_j \\in \\mathbb{R}\\) unrestricted for equalities.</li> </ul> <p>Interpretation:</p> <ul> <li>If \\(\\mu_i &gt; 0\\), violating \\(g_i(x)\\le 0\\) incurs a penalty proportional to \\(\\mu_i\\).</li> <li>If \\(\\mu_i = 0\\), that constraint does not influence the Lagrangian at that point.</li> </ul>"},{"location":"convex/18_duality/#the-dual-function-lower-bounds-from-penalties","title":"The Dual Function: Lower Bounds from Penalties","text":"<p>Fix \\((\\lambda,\\mu)\\) and minimize the Lagrangian with respect to \\(x\\):</p> \\[ \\theta(\\lambda, \\mu) = \\inf_x L(x,\\lambda,\\mu). \\] <p>Because \\(g_i(x) \\le 0\\) for feasible \\(x\\) and \\(\\mu_i \\ge 0\\),</p> \\[ L(x,\\lambda,\\mu) \\le f(x), \\] <p>so taking the infimum over all \\(x\\) yields</p> \\[ \\theta(\\lambda,\\mu) \\le f^\\star. \\] <p>Thus \\(\\theta\\) always produces lower bounds on the true optimal value (weak duality).</p>"},{"location":"convex/18_duality/#properties-of-the-dual-function","title":"Properties of the Dual Function","text":"<ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\) (infimum of affine functions).</li> <li>It may be \\(-\\infty\\) if the Lagrangian is unbounded below.</li> </ul>"},{"location":"convex/18_duality/#the-dual-problem","title":"The Dual Problem","text":"<p>The dual problem maximizes these lower bounds:</p> \\[ \\begin{array}{ll} \\text{maximize}_{\\lambda,\\mu} &amp; \\theta(\\lambda,\\mu) \\\\ \\text{subject to} &amp; \\mu \\ge 0. \\end{array} \\] <p>Let \\(d^\\star\\) be the optimal dual value. Weak duality guarantees:</p> \\[ d^\\star \\le f^\\star. \\] <p>The dual problem is always a concave maximization, i.e., a convex optimization problem in \\((\\lambda,\\mu)\\).</p>"},{"location":"convex/18_duality/#strong-duality-and-the-duality-gap","title":"Strong Duality and the Duality Gap","text":"<p>If</p> \\[ d^\\star = f^\\star, \\] <p>we say strong duality holds. The duality gap is zero.</p>"},{"location":"convex/18_duality/#slaters-condition","title":"Slater\u2019s Condition","text":"<p>If:</p> <ul> <li>\\(g_i\\) are convex,</li> <li>\\(h_j\\) are affine,</li> <li>and there exists a \\(\\tilde{x}\\) such that </li> </ul> <p>then:</p> <ul> <li>strong duality holds (\\(f^\\star = d^\\star\\)),</li> <li>dual maximizers exist,</li> <li>KKT conditions fully characterize primal\u2013dual optimality.</li> </ul> <p>Slater\u2019s condition ensures the feasible region has interior \u2014 the constraints are not tight everywhere.</p>"},{"location":"convex/18_duality/#duality-and-the-kkt-conditions","title":"Duality and the KKT Conditions","text":"<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>"},{"location":"convex/18_duality/#primal-feasibility","title":"Primal feasibility","text":"\\[ g_i(x^\\star) \\le 0,\\qquad h_j(x^\\star)=0. \\]"},{"location":"convex/18_duality/#dual-feasibility","title":"Dual feasibility","text":"\\[ \\mu_i^\\star \\ge 0. \\]"},{"location":"convex/18_duality/#stationarity","title":"Stationarity","text":"\\[ \\nabla f(x^\\star) + \\sum_{i=1}^m \\mu_i^\\star \\nabla g_i(x^\\star) + \\sum_{j=1}^p \\lambda_j^\\star \\nabla h_j(x^\\star) = 0. \\]"},{"location":"convex/18_duality/#complementary-slackness","title":"Complementary slackness","text":"\\[ \\mu_i^\\star g_i(x^\\star) = 0,\\qquad \\forall i. \\] <p>Together these conditions ensure:</p> \\[ f(x^\\star) = \\theta(\\lambda^\\star,\\mu^\\star) = f^\\star = d^\\star. \\] <p>Geometrically, the gradients of the active constraints form a supporting hyperplane that \u201ctouches\u2019\u2019 the objective exactly at the optimum.</p>"},{"location":"convex/18_duality/#interpretation-of-dual-variables","title":"Interpretation of Dual Variables","text":"<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>"},{"location":"convex/18_duality/#shadow-prices-constraint-forces","title":"Shadow Prices / Constraint Forces","text":"<ul> <li> <p>\\(\\mu_i^\\star\\): the shadow price for relaxing \\(g_i(x)\\le 0\\).   Large \\(\\mu_i^\\star\\) means the constraint is tight and costly to relax.</p> </li> <li> <p>\\(\\lambda_j^\\star\\): sensitivity of the optimal value to perturbations of \\(h_j(x)=0\\).</p> </li> </ul>"},{"location":"convex/18_duality/#ml-interpretations","title":"ML Interpretations","text":"<ul> <li>Support Vector Machines: dual variables select support vectors (only points with \\(\\mu_i^\\star &gt; 0\\) matter).</li> <li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li> <li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li> </ul> <p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p>"},{"location":"convex/18a_pareto/","title":"10. Pareto Optimality and Multi-Objective Convex Optimization","text":""},{"location":"convex/18a_pareto/#chapter-10-multi-objective-convex-optimization","title":"Chapter 10: Multi-Objective Convex Optimization","text":"<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p> <ul> <li>loss vs. fairness,</li> <li>return vs. risk,</li> <li>energy use vs. performance.</li> </ul> <p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p>"},{"location":"convex/18a_pareto/#classical-optimality-one-objective","title":"Classical Optimality (One Objective)","text":"<p>In standard convex optimization, we solve:</p> \\[ x^* \\in \\arg\\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) is convex and \\(\\mathcal{X}\\) is convex. In this setting, it is natural to speak of the minimizer \u2014 or set of minimizers \u2014 because the task is governed by a single quantitative measure.</p> <p>However, when multiple objectives \\((f_1,\\dots,f_k)\\) must be minimized simultaneously, a single \u201cbest\u201d point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>"},{"location":"convex/18a_pareto/#multi-objective-convex-optimization","title":"Multi-Objective Convex Optimization","text":"<p>A multi-objective optimization problem takes the form</p> \\[ \\min_{x \\in \\mathcal{X}} F(x) = (f_1(x), \\dots, f_k(x)), \\] <p>where each \\(f_i\\) is convex. This framework appears in many ML and statistical tasks:</p> Domain Objective 1 Objective 2 Trade-off Regression Fit error Regularization Accuracy vs. complexity Fair ML Loss Fairness metric Utility vs. fairness Portfolio Return Risk Profit vs. stability Autoencoders Reconstruction KL divergence Fidelity vs. disentanglement <p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes Pareto efficiency.</p>"},{"location":"convex/18a_pareto/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"convex/18a_pareto/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A point \\(x^*\\) is Pareto optimal if there is no other \\(x\\) such that</p> \\[ f_i(x) \\le f_i(x^*)\\quad \\forall i, \\] <p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>"},{"location":"convex/18a_pareto/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A point \\(x^*\\) is weakly Pareto optimal if no feasible point satisfies</p> \\[ f_i(x) &lt; f_i(x^*)\\quad \\forall i. \\] <p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>"},{"location":"convex/18a_pareto/#geometric-view","title":"Geometric View","text":"<p>For two objectives \\((f_1, f_2)\\), the feasible set in objective space is a region in \\(\\mathbb{R}^2\\). Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p> <ul> <li>Points on the frontier are the best achievable trade-offs.</li> <li>Points above or inside the region are dominated and thus suboptimal.</li> </ul> <p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>"},{"location":"convex/18a_pareto/#scalarization-turning-many-objectives-into-one","title":"Scalarization: Turning Many Objectives into One","text":"<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>"},{"location":"convex/18a_pareto/#weighted-sum-scalarization","title":"Weighted-Sum Scalarization","text":"\\[ \\min_{x \\in \\mathcal{X}} \\sum_{i=1}^k w_i f_i(x), \\qquad w_i \\ge 0,\\quad \\sum_i w_i = 1. \\] <ul> <li>The weights encode relative importance.  </li> <li>Varying \\(w\\) traces (part of) the Pareto frontier.  </li> <li>When \\(f_i\\) and \\(\\mathcal{X}\\) are convex, this method recovers the convex portion of the frontier.</li> </ul>"},{"location":"convex/18a_pareto/#-constraint-method","title":"\u03b5-Constraint Method","text":"\\[ \\min_{x} \\ f_1(x) \\quad \\text{s.t. } f_i(x) \\le \\varepsilon_i,\\ \\ i = 2,\\dots,k. \\] <ul> <li>Here the tolerances \\(\\varepsilon_i\\) act as performance budgets.  </li> <li>Each choice of \\(\\varepsilon\\) yields a different Pareto-efficient point.</li> </ul> <p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>"},{"location":"convex/18a_pareto/#duality-connection","title":"Duality Connection","text":"<p>Scalarization has a tight relationship with duality (Chapter 9):</p> <ul> <li>Weights \\(w_i\\) in a weighted sum act like dual variables.</li> <li>Regularization parameters (e.g., the \\(\\lambda\\) in L2 or L1 regularization) correspond to dual multipliers.</li> <li>Moving along \\(\\lambda\\) traces the Pareto frontier between data fit and model complexity.</li> </ul> <p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>"},{"location":"convex/18a_pareto/#examples-and-applications","title":"Examples and Applications","text":""},{"location":"convex/18a_pareto/#example-1-regularized-least-squares","title":"Example 1: Regularized Least Squares","text":"<p>Consider</p> \\[ f_1(x) = \\|Ax - b\\|_2^2,\\qquad  f_2(x) = \\|x\\|_2^2. \\] <p>Two scalarizations:</p> <ol> <li> <p>Weighted:     </p> </li> <li> <p>\u03b5-constraint:     </p> </li> </ol> <p>\\(\\lambda\\) and \\(\\tau\\) trace the same Pareto curve \u2014 the classical bias\u2013variance trade-off.</p>"},{"location":"convex/18a_pareto/#example-2-portfolio-optimization-riskreturn","title":"Example 2: Portfolio Optimization (Risk\u2013Return)","text":"<p>Let \\(w\\) be portfolio weights, \\(\\mu\\) expected returns, and \\(\\Sigma\\) the covariance matrix. Objectives:</p> \\[ f_1(w) = -\\mu^\\top w, \\qquad f_2(w) = w^\\top \\Sigma w. \\] <p>Weighted scalarization:</p> \\[ \\min_w \\ -\\alpha \\mu^\\top w + (1-\\alpha) w^\\top \\Sigma w, \\quad 0 \\le \\alpha \\le 1. \\] <p>Varying \\(\\alpha\\) recovers the efficient frontier of Modern Portfolio Theory.</p>"},{"location":"convex/18a_pareto/#example-3-fairnessaccuracy-in-ml","title":"Example 3: Fairness\u2013Accuracy in ML","text":"\\[ \\min_\\theta \\ \\mathbb{E}[\\ell(y, f_\\theta(x))] \\quad \\text{s.t. } D(f_\\theta(x),y) \\le \\varepsilon, \\] <p>where \\(D\\) is a fairness metric. Scalarized form:</p> \\[ \\min_\\theta\\  \\mathbb{E}[\\ell(y, f_\\theta(x))] + \\lambda D(f_\\theta(x), y). \\] <p>Tuning \\(\\lambda\\) walks across the fairness\u2013accuracy Pareto frontier.</p>"},{"location":"convex/18a_pareto/#example-4-variational-autoencoders-and-vae","title":"Example 4: Variational Autoencoders and \u03b2-VAE","text":"<p>The ELBO is:</p> \\[ \\mathbb{E}_{q(z)}[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z)). \\] <p>Objectives:</p> <ul> <li>Reconstruction fidelity,</li> <li>Latent simplicity.</li> </ul> <p>\u03b2-VAE scalarization:</p> \\[ \\max_q \\ \\mathbb{E}[\\log p(x|z)] - \\beta \\,\\mathrm{KL}(q(z)\\|p(z)). \\] <p>\\(\\beta\\) controls the trade-off between reconstruction and disentanglement \u2014 a Pareto frontier in latent space.</p> <p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p>"},{"location":"convex/18b_regularization/","title":"11. Regularized Approximation \u2013 Balancing Fit and Complexity","text":""},{"location":"convex/18b_regularization/#chapter-11-balancing-fit-and-complexity","title":"Chapter 11:  Balancing Fit and Complexity","text":"<p>Most real-world learning and estimation problems must balance two competing goals:</p> <ol> <li>Fit the observed data well, and  </li> <li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li> </ol> <p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>"},{"location":"convex/18b_regularization/#motivation-fit-vs-complexity","title":"Motivation: Fit vs. Complexity","text":"<p>Suppose we wish to estimate parameters \\(x\\) from data via a loss function \\(f(x)\\). If the data are noisy or the model is high-dimensional, solutions minimizing \\(f\\) alone may be unstable or overly complex. We introduce a regularizer \\(R(x)\\), typically convex, to encourage desirable structure:</p> \\[ \\min_{x} \\; f(x) + \\lambda R(x), \\qquad \\lambda &gt; 0. \\] <ul> <li>\\(f(x)\\): measures data misfit (e.g., squared loss, logistic loss).  </li> <li>\\(R(x)\\): penalizes complexity (e.g., \\(\\ell_1\\) norm for sparsity, \\(\\ell_2\\) norm for smoothness).  </li> <li>\\(\\lambda\\): controls the trade-off.<ul> <li>Small \\(\\lambda\\): excellent data fit, potentially overfitting.  </li> <li>Large \\(\\lambda\\): simpler model, potentially underfitting.</li> </ul> </li> </ul> <p>This is a scalarized multi-objective optimization problem of \\((f, R)\\).</p>"},{"location":"convex/18b_regularization/#bicriterion-optimization-and-the-pareto-frontier","title":"Bicriterion Optimization and the Pareto Frontier","text":"<p>Regularization corresponds to the bicriterion objective:</p> \\[ \\min_{x} \\; (f(x), R(x)). \\] <p>A point \\(x^*\\) is Pareto optimal if there is no feasible \\(x\\) such that:  with strict inequality in at least one component.</p> <p>For convex \\(f\\) and \\(R\\):</p> <ul> <li>Every \\(\\lambda \\ge 0\\) yields a Pareto-optimal point,</li> <li>The mapping from \\(\\lambda\\) to constraint level \\(R(x^*)\\) is monotone,</li> <li>The Pareto frontier is convex and can be traced continuously by varying \\(\\lambda\\).</li> </ul> <p>Thus, tuning \\(\\lambda\\) moves the solution along the fit\u2013complexity frontier.</p>"},{"location":"convex/18b_regularization/#why-control-the-size-of-the-solution","title":"Why Control the Size of the Solution?","text":"<p>Inverse problems such as \\(Ax \\approx b\\) are often ill-posed or ill-conditioned:</p> <ul> <li>Small noise in \\(b\\) may cause large variability in the solution \\(x\\).  </li> <li>If \\(A\\) is rank-deficient or nearly singular, infinitely many solutions exist.</li> </ul> <p>Example: ridge regression</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2. \\] <p>The optimality condition is</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b. \\] <p>Benefits of L2 regularization:</p> <ul> <li>\\(A^\\top A + \\lambda I\\) becomes positive definite for any \\(\\lambda &gt; 0\\),  </li> <li>the solution becomes unique and stable,  </li> <li>small singular directions of \\(A\\) are suppressed.</li> </ul> <p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>"},{"location":"convex/18b_regularization/#constrained-vs-penalized-formulations","title":"Constrained vs. Penalized Formulations","text":"<p>Regularized problems can be expressed equivalently as constrained problems:</p> \\[ \\min_x f(x)  \\quad \\text{s.t. } R(x) \\le t. \\] <p>The Lagrangian is</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda (R(x) - t), \\qquad \\lambda \\ge 0. \\] <p>The penalized form</p> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>is the dual of the constrained form. Under convexity and Slater\u2019s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p> \\[ 0 \\in \\partial f(x^*) + \\lambda^* \\partial R(x^*),  \\] \\[ R(x^*) \\le t,\\qquad \\lambda^* \\ge 0,\\qquad \\lambda^*(R(x^*) - t) = 0. \\] <p>Here:</p> <ul> <li>If \\(R(x^*) &lt; t\\), then \\(\\lambda^* = 0\\).  </li> <li>If \\(\\lambda^* &gt; 0\\), then \\(R(x^*) = t\\) (constraint active).</li> </ul> <p>Thus \\(\\lambda\\) is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>"},{"location":"convex/18b_regularization/#common-regularizers-and-their-effects","title":"Common Regularizers and Their Effects","text":""},{"location":"convex/18b_regularization/#a-l2-regularization-ridge","title":"(a) L2 Regularization (Ridge)","text":"\\[ R(x) = \\|x\\|_2^2. \\] <ul> <li>Smooth and strongly convex.  </li> <li>Shrinks coefficients uniformly.  </li> <li>Improves conditioning.  </li> <li>MAP interpretation: Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\).</li> </ul>"},{"location":"convex/18b_regularization/#b-l1-regularization-lasso","title":"(b) L1 Regularization (Lasso)","text":"\\[ R(x) = \\|x\\|_1 = \\sum_i |x_i|. \\] <ul> <li>Convex but not differentiable \u2192 promotes sparsity.  </li> <li>The \\(\\ell_1\\) ball has corners aligned with coordinate axes, encouraging zeros in \\(x\\).  </li> <li>Proximal operator (soft-thresholding):</li> </ul> \\[ \\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(v) = \\operatorname{sign}(v)\\,\\max(|v|-\\tau, 0). \\] <ul> <li>MAP interpretation: Laplace prior.</li> </ul>"},{"location":"convex/18b_regularization/#c-elastic-net","title":"(c) Elastic Net","text":"\\[ R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2. \\] <ul> <li>Combines sparsity with numerical stability.  </li> <li>Useful with correlated features.</li> </ul>"},{"location":"convex/18b_regularization/#d-beyond-l1l2-structured-regularizers","title":"(d) Beyond L1/L2: Structured Regularizers","text":"Regularizer Formula Effect Tikhonov \\(\\|Lx\\|_2^2\\) smoothness via operator \\(L\\) Total Variation \\(\\|\\nabla x\\|_1\\) piecewise-constant signals/images Group Lasso \\(\\sum_g \\|x_g\\|_2\\) structured sparsity across groups Nuclear Norm \\(\\|X\\|_* = \\sum_i \\sigma_i\\) low-rank matrices <p>Each regularizer defines a geometry for the solution \u2014 ellipsoids, diamonds, polytopes, or spectral shapes.</p>"},{"location":"convex/18b_regularization/#choosing-the-regularization-parameter-lambda","title":"Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"convex/18b_regularization/#a-trade-off-behavior","title":"(a) Trade-Off Behavior","text":"<ul> <li>\\(\\lambda \\downarrow\\): favors small training error, high variance.  </li> <li>\\(\\lambda \\uparrow\\): favors simplicity, higher bias.  </li> </ul> <p>\\(\\lambda\\) selects a point on the fit\u2013complexity Pareto frontier.</p>"},{"location":"convex/18b_regularization/#b-cross-validation","title":"(b) Cross-Validation","text":"<p>The most common practice:</p> <ol> <li>Split data into folds.  </li> <li>Train on \\(k-1\\) folds, validate on the remaining fold.  </li> <li>Choose \\(\\lambda\\) minimizing average validation error.</li> </ol> <p>Guidelines:</p> <ul> <li>Standardize features for L1/Elastic Net.  </li> <li>Use time-aware CV for dependent data.  </li> <li>Use the \u201cone-standard-error rule\u201d for simpler models.</li> </ul>"},{"location":"convex/18b_regularization/#c-other-selection-methods","title":"(c) Other Selection Methods","text":"<ul> <li>Information criteria (AIC, BIC) for sparsity.  </li> <li>L-curve or discrepancy principle in inverse problems.  </li> <li>Regularization paths: computing \\(x^*(\\lambda)\\) for many \\(\\lambda\\).</li> </ul>"},{"location":"convex/18b_regularization/#algorithmic-view","title":"Algorithmic View","text":"<p>Most regularized problems have the form:</p> \\[ \\min_x \\ f(x) + R(x), \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex (possibly nonsmooth).</p> <p>Common algorithms:</p> Method Idea When Useful Proximal Gradient (ISTA/FISTA) Gradient step on \\(f\\), proximal step on \\(R\\) L1, TV, nuclear norm Coordinate Descent Update coordinates cyclically Lasso, Elastic Net ADMM Split problem to exploit structure Large-scale or distributed settings <p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal \\(O(1/k^2)\\) rate for smooth+convex problems.</p>"},{"location":"convex/18b_regularization/#bayesian-interpretation","title":"Bayesian Interpretation","text":"<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p> <p>Linear model:</p> \\[ b = Ax + \\varepsilon,\\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I). \\] <p>With prior \\(x \\sim p(x)\\), MAP estimation solves:</p> \\[ \\min_x \\ \\frac{1}{2\\sigma^2}\\|Ax - b\\|_2^2 - \\log p(x). \\] <p>Examples:</p> <ul> <li>Gaussian prior \\(p(x) \\propto e^{-\\|x\\|_2^2 / (2\\tau^2)}\\)   \u2192 L2 penalty with \\(\\lambda = \\sigma^2/(2\\tau^2)\\).  </li> <li>Laplace prior   \u2192 L1 penalty and sparse MAP estimate.</li> </ul> <p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p> <p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p>"},{"location":"convex/19_optimizationalgo/","title":"12. Algorithms for Convex Optimization","text":""},{"location":"convex/19_optimizationalgo/#chapter-12-algorithms-for-convex-optimization","title":"Chapter 12: Algorithms for Convex Optimization","text":"<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p> <p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3\u201311) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept \u2014 gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13\u201315) extend these ideas to constrained, stochastic, and large-scale environments.</p>"},{"location":"convex/19_optimizationalgo/#problem-classes-vs-method-classes","title":"Problem classes vs method classes","text":"<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p> Problem Type Typical Formulation Representative Methods Examples Smooth, unconstrained \\(\\min_x f(x)\\), convex and differentiable Gradient descent, Accelerated gradient, Newton Logistic regression, least squares Smooth with simple constraints \\(\\min_x f(x)\\) s.t. \\(x \\in \\mathcal{X}\\) (box, ball, simplex) Projected gradient Constrained regression, probability simplex Composite convex (smooth + nonsmooth) \\(\\min_x f(x) + R(x)\\) Proximal gradient, coordinate descent Lasso, Elastic Net, TV minimization General constrained convex \\(\\min f(x)\\) s.t. \\(g_i(x) \\le 0, h_j(x)=0\\) Interior-point, primal\u2013dual methods LP, QP, SDP, SOCP"},{"location":"convex/19_optimizationalgo/#first-order-methods-gradient-descent","title":"First-order methods: Gradient descent","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  </p> <p>Smoothness lets us control step sizes.</p> <p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). Typical choices:</p> <ul> <li>constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known,</li> <li>backtracking line search when \\(L\\) is unknown,</li> <li>diminishing step sizes in some settings.</li> </ul> <p>Derivation: </p> <p>Around \\(x_t\\), we can approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\).  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near \\(x_t\\), not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <ul> <li>The linear term pulls \\(x\\) in the steepest descent direction.</li> <li>The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\).</li> <li>\\(\\eta\\) trades off aggressive progress vs stability:<ul> <li>Small \\(\\eta\\) \u2192 cautious updates.</li> <li>Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</li> </ul> </li> </ul> <p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Convergence: For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p> <p>When to use gradient descent:</p> <ul> <li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li> <li>You can compute gradients cheaply.</li> <li>You only need moderate accuracy.</li> <li>Memory constraints rule out storing or factoring Hessians.</li> </ul>"},{"location":"convex/19_optimizationalgo/#accelerated-first-order-methods","title":"Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"convex/19_optimizationalgo/#nesterov-acceleration","title":"Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences \\(x_k\\) and \\(y_k\\):</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra momentum term \\(\\beta_k (x_{k+1}-x_k)\\) uses past iterates to \u201clook ahead\u201d and can significantly accelerate convergence.</p> <p>Convergece: For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives.</p> <ul> <li>Acceleration is effective for well-behaved smooth convex problems.</li> <li>It can be more sensitive to step size and noise than plain gradient descent.</li> <li>Variants such as FISTA apply acceleration in the composite setting \\(f + R\\).</li> </ul> <p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned\u2014that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>"},{"location":"convex/19_optimizationalgo/#subgradient-methods","title":"Subgradient Methods","text":"<p>Even when \\(f\\) is not differentiable, we can minimise it using subgradient descent:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k, \\qquad g_k \\in \\partial f(x_k). \\] <p>Key features:</p> <ul> <li>Requires only a subgradient (no differentiability needed).</li> <li>Works for any convex function.</li> <li>Stepsizes must typically decrease (e.g. , ).</li> <li>Guaranteed convergence for convex \\(f\\), but generally slow.</li> </ul>"},{"location":"convex/19_optimizationalgo/#convergence-rates-worst-case","title":"Convergence rates (worst case)","text":"<ul> <li>Smooth convex gradient descent: \\(O(1/k)\\) or \\(O(1/k^2)\\).  </li> <li>Nonsmooth subgradient descent: </li> </ul> <p>This slower rate reflects the lack of curvature information at kinks.</p>"},{"location":"convex/19_optimizationalgo/#proximal-and-smoothed-alternatives","title":"Proximal and Smoothed Alternatives","text":"<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>"},{"location":"convex/19_optimizationalgo/#1-proximal-methods","title":"(1) Proximal methods","text":"<p>For a convex function \\(f\\), the proximal operator is  </p> <p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p> <ul> <li>  regularisation,</li> <li>indicator functions of convex sets,</li> <li>total variation penalties.</li> </ul> <p>They achieve faster and more stable convergence than basic subgradient descent.</p>"},{"location":"convex/19_optimizationalgo/#2-smoothing-techniques","title":"(2) Smoothing techniques","text":"<p>Many nonsmooth convex functions have smooth approximations:</p> <ul> <li>Replace  with the Huber loss.</li> <li>Replace  with softplus.</li> <li>Replace  with log-sum-exp, a smooth convex approximation.</li> </ul> <p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>"},{"location":"convex/19_optimizationalgo/#steepest-descent-method","title":"Steepest Descent Method","text":"<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of maximum decrease of the objective function under a unit norm constraint.</p> <p>The norm defines the \u201cgeometry\u201d of optimization. Gradient descent is steepest descent under the Euclidean norm. Changing the norm changes what \u201csteepest\u201d means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems. The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p> <p>At a point \\(x\\), and for a chosen norm \\(|\\cdot|\\):</p> \\[ \\Delta x_{\\text{nsd}} = \\arg\\min_{|v| = 1} \\nabla f(x)^T v \\] <p>This defines the normalized steepest descent direction \u2014 the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of \\(f\\)).</p> <ul> <li>\\(\\Delta x_{\\text{nsd}}\\): normalized steepest descent direction</li> <li>\\(\\Delta x_{\\text{sd}}\\): unnormalized direction (scaled by the gradient norm)</li> </ul> <p>For small steps \\(v\\),  The term \\(\\nabla f(x)^T v\\) describes how fast \\(f\\) increases in direction \\(v\\). To decrease \\(f\\) most rapidly, we pick \\(v\\) that minimizes this inner product \u2014 subject to \\(|v| = 1\\).</p> <ul> <li>The result depends on which norm we use to measure the \u201csize\u201d of \\(v\\).</li> <li>The corresponding dual norm \\(|\\cdot|_*\\) determines how we measure the gradient\u2019s magnitude.</li> </ul> <p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p> <p>The choice of norm determines:</p> <ol> <li>The shape of the unit ball \\({v : |v| \\le 1}\\),</li> <li>The direction of steepest descent, since the minimization is constrained by that shape,</li> <li>The dual norm \\(|\\nabla f(x)|_*\\) that measures the gradient\u2019s size.</li> </ol> <p>Different norms yield different \u201cgeometries\u201d of descent:</p> Norm Unit Ball Shape Dual Norm Effect on Direction \\(\\ell_2\\) Circle / sphere \\(\\ell_2\\) Direction is opposite to gradient \\(\\ell_1\\) Diamond \\(\\ell_\\infty\\) Moves along coordinate of largest gradient \\(\\ell_\\infty\\) Square \\(\\ell_1\\) Moves opposite to sum of all gradient signs Quadratic \\((x^T P x)^{1/2}\\) Ellipsoid Weighted \\(\\ell_2\\) Scales direction by preconditioner \\(P^{-1}\\) <p>Thus, the norm defines how \u201cdistance\u201d and \u201csteepness\u201d are perceived, shaping how the algorithm moves through the landscape of \\(f(x)\\).</p>"},{"location":"convex/19_optimizationalgo/#conjugate-gradient-method-fast-optimization-for-quadratic-objectives","title":"Conjugate Gradient Method \u2014 Fast Optimization for Quadratic Objectives","text":"<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p> <p>CG is a first-order method that behaves like a second-order method for quadratics.</p> <p>For a quadratic objective function:</p> \\[ f(x) = \\tfrac12 x^\\top A x - b^\\top x  \\] <p>with \\(A \\succ 0\\), the level sets are ellipses shaped by the eigenvalues of \\(A\\). If \\(A\\) is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p> <p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors \\(p_i, p_j\\) are said to be A-conjugate if</p> \\[ p_i^\\top A p_j = 0. \\] <p>This is orthogonality measured in the geometry induced by the Hessian \\(A\\). Why is this useful?</p> <ul> <li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of \\(A\\).</li> <li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li> <li>After \\(n\\) mutually A-conjugate directions, all curvature directions are resolved \u2192 exact solution.</li> </ul> <p>In contrast, gradient descent repeatedly re-corrects previous progress.</p> <p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system \\(Ax = b\\). Let</p> \\[ r_0 = b - A x_0, \\qquad p_0 = r_0. \\] <p>For \\(k = 0,1,2,\\dots\\):</p> <ol> <li> <p>Step size     </p> </li> <li> <p>Update iterate     </p> </li> <li> <p>Update residual (negative gradient)     </p> </li> <li> <p>Direction scaling     </p> </li> <li> <p>New conjugate direction     </p> </li> </ol> <p>Stop when \\(\\|r_k\\|\\) is below tolerance.</p> <p>Every new direction \\(p_{k+1}\\) is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p> <p>Why CG Is Fast: For an \\(n\\)-dimensional quadratic, CG solves the problem in at most \\(n\\) iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in \\(O(\\sqrt{\\kappa})\\) iterations, where \\(\\kappa = \\lambda_{\\max}/\\lambda_{\\min}\\) is the condition number. The convergence bound in the A-norm is:</p> \\[ \\|x_k - x^\\star\\|_A \\le  2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k  \\|x_0 - x^\\star\\|_A. \\] <p>This is dramatically better than the \\(O(1/k)\\) rate of gradient descent.</p> <p>CG is ideal when:</p> <ul> <li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix \\(A\\).</li> <li>\\(A\\) is large and sparse or available as a matrix\u2013vector product.</li> <li>You cannot form or store \\(A^{-1}\\) or even the full matrix \\(A\\).</li> <li>You want a Hessian-aware method but cannot afford Newton\u2019s method.</li> </ul> <p>Typical scenarios:</p> Application Why CG fits Large linear systems \\(A x = b\\) Only requires \\(A p\\), not factorization. Ridge regression Normal equations form an SPD matrix. Kernel ridge regression Solves \\((K+\\lambda I)\\alpha = y\\) efficiently. Newton steps in ML Inner solver for Hessian systems without forming Hessian. PDEs and scientific computing Sparse SPD matrices, ideal for CG. <p>Assumptions Required for CG: To guarantee correctness of linear CG, we require:</p> <ul> <li>\\(A\\) is symmetric</li> <li>\\(A\\) is positive definite</li> <li>Objective is strictly convex quadratic</li> <li>Arithmetic is exact (for the finite-step guarantee)</li> </ul> <p>If the function is not quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p> <p>Practical Notes:</p> <ul> <li>You only need matrix\u2013vector products \\(Ap\\).  </li> <li>Storage cost is \\(O(n)\\).  </li> <li>Preconditioning (replacing the system with \\(M^{-1} A\\)) improves conditioning and accelerates convergence dramatically.  </li> <li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li> </ul> <p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>"},{"location":"convex/19_optimizationalgo/#newtons-method-and-second-order-methods","title":"Newton\u2019s method and second-order methods","text":"<p>First-order methods (like gradient descent) only use gradient information. Newton\u2019s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p> <p>From Chapter 3, the second-order Taylor approximation of \\(f(x)\\) around a point \\(x_k\\) is:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x_k) d. \\] <p>If we temporarily trust this quadratic model, we can choose \\(d\\) to minimize the right-hand side. Differentiating with respect to \\(d\\) and setting to zero gives:</p> \\[ \\nabla^2 f(x_k) \\, d_{\\text{newton}} = - \\nabla f(x_k). \\] <p>Hence, the Newton step is:</p> \\[ d_{\\text{newton}} = - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k), \\quad x_{k+1} = x_k + d_{\\text{newton}}. \\] <p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex \\(f\\), Newton\u2019s method achieves quadratic convergence\u2014dramatically faster than the \\(O(1/k)\\) or \\(O(1/k^2)\\) rates typical of first-order algorithms.</p> <p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton\u2019s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>"},{"location":"convex/19_optimizationalgo/#solving-the-newton-system","title":"Solving the Newton System","text":"<p>Each iteration requires solving</p> \\[ H \\,\\Delta x = -g, \\qquad H = \\nabla^2 f(x), \\;\\; g = \\nabla f(x). \\] <p>If \\(H\\) is symmetric positive definite, a Cholesky factorization</p> \\[ H = L L^\\top \\] <p>allows efficient and numerically stable solution via two triangular solves:</p> <ol> <li>\\(L y = -g\\)</li> <li>\\(L^\\top \\Delta x_{\\text{nt}} = y\\)</li> </ol> <p>This avoids forming \\(H^{-1}\\) explicitly.</p> <p>The Newton decrement:</p> \\[ \\lambda(x) = \\|L^{-1} g\\|_2 \\] <p>gauges proximity to the optimum and provides a natural stopping criterion: \\(\\lambda(x)^2/2 &lt; \\varepsilon\\).</p> <p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs \\(\\approx (1/3)n^3\\) operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton\u2019s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix\u2013free iterative methods.</p>"},{"location":"convex/19_optimizationalgo/#gaussnewton-method","title":"Gauss\u2013Newton Method","text":"<p>The Gauss\u2013Newton method is a specialization of Newton\u2019s method for nonlinear least squares problems</p> \\[ f(x) = \\tfrac12 \\| r(x) \\|^2, \\] <p>where \\(r(x)\\) is a vector of residual functions and a nonlinear function of \\(x\\) and \\(J\\) is its Jacobian. Newton\u2019s Hessian decomposes as</p> \\[ \\nabla^2 f(x) = J^\\top J \\;+\\; \\sum_i r_i(x)\\, \\nabla^2 r_i(x). \\] <p>The second term involves the curvature of the residuals. When \\(r(x)\\) is approximately linear near the optimum, this term is small. Gauss\u2013Newton drops it, giving the approximation</p> \\[ \\nabla^2 f(x) \\approx J^\\top J, \\] <p>leading to the Gauss\u2013Newton step:</p> \\[ (J^\\top J)\\, \\Delta = -J^\\top r. \\] <p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg\u2013Marquardt method adds a damping term,</p> \\[ (J^\\top J + \\lambda I)\\, \\Delta = -J^\\top r, \\] <p>which interpolates smoothly between  </p> <ul> <li>gradient descent (large \\(\\lambda\\)), and  </li> <li>Gauss\u2013Newton (small \\(\\lambda\\)).</li> </ul> <p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss\u2013Newton and Levenberg\u2013Marquardt are highly effective when the residuals are nearly linear\u2014common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning\u2014yielding fast convergence without the expense of full second derivatives.</p>"},{"location":"convex/19_optimizationalgo/#quasi-newton-methods","title":"Quasi-Newton methods","text":"<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. These methods use gradient information from previous steps to estimate curvature.</p> <p>The most famous examples are:</p> <ul> <li>BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno)  </li> <li>DFP (Davidon\u2013Fletcher\u2013Powell)  </li> <li>L-BFGS (Limited-memory BFGS) \u2014 for very large-scale problems.</li> </ul> <p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton\u2019s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation \\(B_k \\approx \\nabla^2 f(x_k)^{-1}\\) updated via gradient and step differences:</p> \\[ B_{k+1} = B_k + \\frac{(s_k^\\top y_k + y_k^\\top B_k y_k)}{(s_k^\\top y_k)^2} s_k s_k^\\top - \\frac{B_k y_k s_k^\\top + s_k y_k^\\top B_k}{s_k^\\top y_k}, \\] <p>where \\(s_k = x_{k+1} - x_k\\) and \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\).</p> <p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p> <p>When to use Newton or quasi-Newton methods:</p> <ul> <li>You need high-accuracy solutions.  </li> <li>The problem is smooth and reasonably well-conditioned.  </li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li> </ul> <p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>"},{"location":"convex/19_optimizationalgo/#constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"convex/19_optimizationalgo/#projected-gradient-descent","title":"Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm:</p> <ol> <li>Gradient step:     </li> <li>Projection:     </li> </ol> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"convex/19_optimizationalgo/#proximal-gradient-forwardbackward-splitting","title":"Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method:</p> <ol> <li>Gradient step on \\(f\\):     </li> <li>Proximal step on \\(R\\):     </li> </ol> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"convex/19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"convex/19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"convex/19_optimizationalgo/#penalties-barriers-and-interior-point-methods","title":"Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\): Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"convex/19_optimizationalgo/#penalty-methods","title":"Penalty methods","text":"<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-basic-penalty-method-quadratic-or-general-penalization","title":"Algorithm: Basic Penalty Method (Quadratic or General Penalization)","text":"<p>Goal:  Solve </p> <p>Penalty formulation:  where  </p> <ul> <li>\\(\\phi(r) = 0\\) if \\(r \\le 0\\),  </li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (e.g., \\(\\phi(r)=\\max\\{0,r\\}^2\\)),  </li> <li>\\(\\rho &gt; 0\\) is the penalty weight.</li> </ul> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>constraints \\(g_i(x)\\) </li> <li>penalty function \\(\\phi\\) </li> <li>initial point \\(x_0\\) </li> <li>initial penalty parameter \\(\\rho_0 &gt; 0\\) </li> <li>penalty update factor \\(\\gamma &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose \\(x_0\\), \\(\\rho_0 &gt; 0\\).  </li> <li>For \\(k = 0, 1, 2, \\dots\\):  <ol> <li>Solve the penalized subproblem  \\(x_{k+1} = \\arg\\min_x F_{\\rho_k}(x)\\) using Newton\u2019s method, gradient descent, quasi-Newton, etc.  </li> <li>Check feasibility / stopping:  If \\(\\max_i g_i(x_{k+1}) \\le \\varepsilon, \\quad   \\|x_{k+1} - x_k\\| \\le \\varepsilon\\)  stop and return \\(x_{k+1}\\).  </li> <li>Increase penalty parameter  \\(\\rho_{k+1} = \\gamma\\, \\rho_k\\)   with typical \\(\\gamma \\in [5,10]\\).  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#barrier-methods","title":"Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul>"},{"location":"convex/19_optimizationalgo/#algorithm-barrier-method-logarithmic-barrier-interior-approximation","title":"Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)","text":"<p>Goal: Solve the constrained problem </p> <p>Logarithmic barrier:  defined only for strictly feasible points \\(g_i(x)&lt;0\\).</p> <p>Barrier subproblem:  where \\(t&gt;0\\) is the barrier parameter.</p> <p>As \\(t \\to \\infty\\), minimizers of \\(F_t\\) approach the constrained optimum.</p> <p>Inputs:  </p> <ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>barrier function \\(b(x)\\) </li> <li>strictly feasible starting point \\(x_0\\) (\\(g_i(x_0) &lt; 0\\))  </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>barrier growth factor \\(\\mu &gt; 1\\) (often \\(\\mu = 10\\))  </li> <li>tolerance \\(\\varepsilon\\)</li> </ul> <p>Procedure:</p> <ol> <li>Choose strictly feasible \\(x_0\\), and pick \\(t_0 &gt; 0\\).  </li> <li>For \\(k = 0,1,2,\\dots\\):  <ol> <li>Centering step (inner loop):  Solve the barrier subproblem    Typically use Newton\u2019s method (damped) on \\(F_{t_k}\\).  Stop when the Newton decrement satisfies  \\(\\lambda(x_{k+1})^2/2 \\le \\varepsilon\\)</li> <li>Optimality / stopping test:    If  \\(\\frac{m}{t_k} \\le \\varepsilon,\\)   then \\(x_{k+1}\\) is an \\(\\varepsilon\\)-approximate solution of the original constrained problem; stop and return \\(x_{k+1}\\).  </li> <li>Increase barrier parameter:  \\(t_{k+1} = \\mu\\, t_k,\\)   which tightens the approximation and moves closer to the boundary.  </li> </ol> </li> <li>End.</li> </ol>"},{"location":"convex/19_optimizationalgo/#interior-point-methods","title":"Interior-point methods","text":"<p>Interior-point methods combine barrier functions with Newton\u2019s method to solve general convex programs:</p> <ul> <li>They maintain strict feasibility throughout.</li> <li>Each iteration solves a Newton system for the barrier-augmented objective.</li> <li>They naturally generate primal\u2013dual pairs and duality gap estimates.</li> <li>Under standard assumptions (e.g., Slater\u2019s condition), they converge in a predictable number of iterations.</li> </ul> <p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>"},{"location":"convex/19_optimizationalgo/#algorithm-primaldual-interior-point-method-for-convex-inequality-constraints","title":"Algorithm: Primal\u2013Dual Interior-Point Method (for convex inequality constraints)","text":"<p>We consider the problem  </p> <p>Introduce Lagrange multipliers \\(\\lambda \\ge 0\\). The KKT conditions are  </p> <p>Interior-point methods enforce the relaxed condition  which keeps iterates strictly feasible.</p>"},{"location":"convex/19_optimizationalgo/#inputs","title":"Inputs","text":"<ul> <li>objective \\(f(x)\\) </li> <li>inequality constraints \\(g_i(x)\\) </li> <li>initial primal point \\(x_0\\) with \\(g_i(x_0)&lt;0\\) </li> <li>initial dual variable \\(\\lambda_0 &gt; 0\\) </li> <li>initial barrier parameter \\(t_0 &gt; 0\\) </li> <li>growth factor \\(\\mu &gt; 1\\) </li> <li>tolerance \\(\\varepsilon\\)</li> </ul>"},{"location":"convex/19_optimizationalgo/#procedure","title":"Procedure","text":"<ol> <li> <p>Choose strictly feasible \\(x_0\\), positive \\(\\lambda_0\\), and \\(t_0\\).</p> </li> <li> <p>For \\(k = 0,1,2,\\dots\\):</p> <p>(a) Form the perturbed KKT system.  Solve for the Newton direction \\((\\Delta x, \\Delta \\lambda)\\):</p> <p> </p> <p>(b) Line search to keep strict feasibility. Choose the maximum \\(\\alpha\\in(0,1]\\) such that:</p> <ul> <li>\\(g_i(x + \\alpha \\Delta x) &lt; 0\\),</li> <li>\\(\\lambda + \\alpha \\Delta \\lambda &gt; 0\\).</li> </ul> <p>(c) Update: \\(x \\leftarrow x + \\alpha \\Delta x,    \\qquad  \\lambda \\leftarrow \\lambda + \\alpha \\Delta \\lambda.\\)</p> <p>(d) Check duality gap: \\(\\text{gap} = - g(x)^\\top \\lambda\\) If \\(\\text{gap} \\le \\varepsilon\\), stop.</p> <p>(e) Increase barrier parameter \\(t \\leftarrow \\mu t.\\)</p> </li> <li> <p>Return \\(x\\).</p> </li> </ol>"},{"location":"convex/19_optimizationalgo/#choosing-the-right-method-in-practice","title":"Choosing the right method in practice","text":"<p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"convex/19_optimizationalgo/#mental-map","title":"Mental Map","text":"<pre><code>                Algorithms for Convex Optimization\n     Turning convex geometry + optimality conditions into solvers\n                              \u2502\n                              \u25bc\n            Core question: how do we actually solve problems?\n   Choose an algorithm class that matches problem structure + scale\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Problem Classes  \u2192  Method Classes                         \u2502\n     \u2502 Smooth unconstrained:        GD, Acceleration, Newton      \u2502\n     \u2502 Smooth + simple constraints: Projected gradient            \u2502\n     \u2502 Composite (smooth+nonsmooth): Proximal/coordinate/splitting\u2502\n     \u2502 General constrained convex:  Interior-point / primal\u2013dual  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 First-Order Core: Gradient Descent                        \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k \u2207f(x_k)                               \u2502\n     \u2502 Requirements: convex + L-smooth                           \u2502\n     \u2502 - Step size from L or line search                         \u2502\n     \u2502 - Rate (smooth convex): O(1/k)                            \u2502\n     \u2502 Geometry: move opposite supporting hyperplane slope       \n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Acceleration (Nesterov / Momentum)                        \u2502\n     \u2502 Two sequences: gradient at y_k + extrapolation to y_{k+1} \u2502\n     \u2502 - Rate (smooth convex): O(1/k^2)                          \u2502\n     \u2502 - Best possible for gradient-only methods                 \u2502\n     \u2502 Tradeoff: faster but more sensitive to tuning/noise       \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Nonsmooth First-Order: Subgradient Descent                   \u2502\n     \u2502 x_{k+1} = x_k \u2212 \u03b1_k g_k,   g_k \u2208 \u2202f(x_k)                     \u2502\n     \u2502 - Works for convex nonsmooth objectives                      \u2502\n     \u2502 - Needs diminishing step sizes                               \u2502\n     \u2502 - Worst-case rate: O(1/\u221ak)                                   \u2502\n     \u2502 Use when: only subgradients available / simple implementation\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Fixing Nonsmoothness: Proximal &amp; Smoothing                \u2502\n     \u2502 Prox operator: prox_{\u03b1R}(y)=argmin_x R(x)+(1/2\u03b1)\u2016x\u2212y\u2016\u00b2    \u2502\n     \u2502 - Handles \u2113\u2081, indicators, TV, etc.                        \u2502\n     \u2502 Smoothing: Huber / softplus / log-sum-exp                 \u2502\n     \u2502 - Enables fast smooth methods                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Steepest Descent = Gradient Descent under a chosen norm     \u2502\n     \u2502 \u0394x_nsd = argmin_{\u2016v\u2016=1} \u2207f(x)\u1d40v                             \u2502\n     \u2502 - Dual norm controls gradient magnitude                     \u2502\n     \u2502 - \u2113\u2082 \u2192 standard GD                                          \u2502\n     \u2502 - Quadratic norm \u2192 preconditioned GD / Newton-like          \u2502\n     \u2502 - \u2113\u2081 \u2192 coordinate-like steps                                \u2502\n     \u2502 Purpose: change geometry to reduce anisotropy / improve rate\u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Quadratic Structure: Conjugate Gradient (CG)                \u2502\n     \u2502 Solve: \u00bdx\u1d40Ax \u2212 b\u1d40x, A\u227b0  (equivalently Ax=b)                \u2502\n     \u2502 - Builds A-conjugate directions (Hessian-orthogonal)        \u2502\n     \u2502 - Uses only matrix\u2013vector products Ap                       \u2502\n     \u2502 - Converges in \u2264 n steps (exact arithmetic)                 \u2502\n     \u2502 - Practical iterations ~ O(\u221a\u03ba) with \u03ba=\u03bb_max/\u03bb_min           \u2502\n     \u2502 - Preconditioning is key for speed                          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Second-Order Methods: Newton &amp; Variants                     \u2502\n     \u2502 Newton step:  \u2207\u00b2f(x_k) d = \u2212\u2207f(x_k)                         \u2502\n     \u2502 - Quadratic local model \u2192 fast local convergence            \u2502\n     \u2502 - Needs line search / trust region for robustness           \u2502\n     \u2502 Gauss\u2013Newton / Levenberg\u2013Marquardt: least-squares structure \u2502\n     \u2502 Quasi-Newton (BFGS/L-BFGS): Hessian inverse approximations  \u2502\n     \u2502 Use when: moderate dimension or efficient linear solves     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Handling Constraints: Projection &amp; Proximal Splitting        \u2502\n     \u2502 Projected GD:  y=x\u2212\u03b1\u2207f(x);  x\u207a=\u03a0_X(y)                        \u2502\n     \u2502 Proximal gradient: y=x\u2212\u03b1\u2207f(x); x\u207a=prox_{\u03b1R}(y)               \u2502\n     \u2502 Unification: indicator_R(X) \u21d2 prox = projection              \u2502\n     \u2502 Use when: constraints/regularizers have cheap prox/project   \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 General Inequalities: Penalties \u2192 Barriers \u2192 Interior-Point \u2502\n     \u2502 Penalty: f(x)+\u03c1 \u03a3 \u03c6(g_i(x))                                 \u2502\n     \u2502 Barrier: tf(x) \u2212 \u03a3 log(\u2212g_i(x))  (strict feasibility)       \u2502\n     \u2502 Interior-point: Newton steps on (primal\u2013dual) perturbed KKT \u2502\n     \u2502 - Few iterations, high accuracy, solver backbone for LP/QP  \u2502\n     \u2502 - Uses duality gap certificates                             \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                 Practical selection (the decision logic)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Huge-scale smooth \u2192 GD / accelerated / L-BFGS               \u2502\n     \u2502 Moderate smooth, high accuracy \u2192 Newton / quasi-Newton      \u2502\n     \u2502 Simple constraints \u2192 projected gradient                     \u2502\n     \u2502 Smooth + nonsmooth regularizer \u2192 proximal gradient / FISTA  \u2502\n     \u2502 General constraints \u2192 interior-point / primal\u2013dual          \u2502\n     \u2502 Quadratic / SPD linear systems \u2192 CG (+ preconditioning)     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"convex/19a_optimization_constraints/","title":"13. Optimization Algorithms for Equality-Constrained Problems","text":""},{"location":"convex/19a_optimization_constraints/#chapter-13-optimization-algorithms-for-equality-constrained-problems","title":"Chapter 13: Optimization Algorithms for Equality-Constrained Problems","text":"<p>Equality-constrained optimization arises whenever variables must satisfy exact relationships, such as conservation laws, normalization, or linear invariants. In this chapter we focus on problems of the form</p> \\[ \\min_x \\; f(x) \\quad \\text{s.t.} \\quad A x = b. \\] <p>where \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is (typically convex and differentiable) and \\(A \\in \\mathbb{R}^{p \\times n}\\) has rank \\(p\\). This linear equality structure appears in constrained least squares, portfolio optimization, and many ML formulations that impose exact balance or normalization constraints.</p>"},{"location":"convex/19a_optimization_constraints/#geometric-view-optimization-on-an-affine-manifold","title":"Geometric View \u2014 Optimization on an Affine Manifold","text":"<p>The constraint \\(A x = b\\) defines an affine set</p> \\[ \\mathcal{X} = \\{ x \\in \\mathbb{R}^n \\mid A x = b \\}. \\] <p>If \\(\\operatorname{rank}(A) = p\\), then \\(\\mathcal{X}\\) is an \\((n-p)\\)-dimensional affine subspace of \\(\\mathbb{R}^n\\): a \u201cflat\u201d lower-dimensional plane embedded in the ambient space. Optimization now happens along this plane, not in all of \\(\\mathbb{R}^n\\). Any feasible direction \\(d\\) must keep us in \\(\\mathcal{X}\\), so it must satisfy</p> \\[ A (x + d) = b \\quad \\Rightarrow \\quad A d = 0. \\] <p>Thus, feasible directions lie in the null space of \\(A\\):</p> \\[ \\mathcal{D}_{\\text{feas}} = \\{ d \\in \\mathbb{R}^n \\mid A d = 0 \\} = \\operatorname{Null}(A). \\] <p>At an optimal point \\(x^\\star \\in \\mathcal{X}\\), moving in any feasible direction \\(d\\) cannot decrease \\(f\\). For differentiable \\(f\\), this means</p> \\[ \\nabla f(x^\\star)^\\top d \\ge 0 \\quad \\text{for all } d \\text{ with } A d = 0. \\] <p>Equivalently, \\(\\nabla f(x^\\star)\\) must be orthogonal to all feasible directions, i.e. it lies in the row space of \\(A\\). Therefore there exists a vector of Lagrange multipliers \\(\\nu^\\star\\) such that</p> \\[ \\nabla f(x^\\star) = A^\\top \\nu^\\star. \\] <p>This is the basic geometric optimality condition: at the optimum, the gradient of \\(f\\) is a linear combination of the constraint normals (rows of \\(A\\)), and every feasible direction is orthogonal to \\(\\nabla f(x^\\star)\\).</p>"},{"location":"convex/19a_optimization_constraints/#lagrange-function-and-kkt-system","title":"Lagrange Function and KKT System","text":"<p>The Lagrangian for the equality-constrained problem is</p> \\[ \\mathcal{L}(x,\\nu) = f(x) + \\nu^\\top (A x - b), \\] <p>where \\(\\nu \\in \\mathbb{R}^p\\) are Lagrange multipliers. The first-order (KKT) conditions for a point \\((x^\\star,\\nu^\\star)\\) to be optimal are</p> \\[ \\begin{aligned} \\nabla_x \\mathcal{L}(x^\\star,\\nu^\\star) &amp;= \\nabla f(x^\\star) + A^\\top \\nu^\\star = 0  \\quad &amp;\\text{(stationarity)},\\\\ A x^\\star &amp;= b  \\quad &amp;\\text{(primal feasibility)}. \\end{aligned} \\] <p>When \\(f\\) is convex and \\(A\\) has full row rank, these conditions are necessary and sufficient for global optimality. For Newton-type methods we linearize these conditions around a current iterate \\((x,\\nu)\\) and solve for corrections \\((\\Delta x,\\Delta \\nu)\\) from</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta \\nu \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) + A^\\top \\nu \\\\ A x - b \\end{bmatrix}. \\] <p>This linear system is called the (equality-constrained) KKT system. At the optimum the right-hand side is zero.</p>"},{"location":"convex/19a_optimization_constraints/#quadratic-objectives","title":"Quadratic Objectives","text":"<p>A particularly important case is a convex quadratic objective</p> \\[ f(x) = \\tfrac{1}{2} x^\\top P x + q^\\top x + r, \\] <p>with \\(P \\succeq 0\\). The equality-constrained problem</p> \\[ \\min_x \\tfrac{1}{2} x^\\top P x + q^\\top x + r  \\quad \\text{s.t.} \\quad A x = b \\] <p>has KKT conditions</p> \\[ \\begin{bmatrix} P &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} x^\\star \\\\ \\nu^\\star \\end{bmatrix} = - \\begin{bmatrix} q \\\\ -b \\end{bmatrix}. \\] <p>If \\(P \\succ 0\\) and \\(A\\) has full row rank, this system has a unique solution \\((x^\\star,\\nu^\\star)\\). This is the standard linear system solved in equality-constrained least squares and quadratic programming.</p> <p>Examples in ML and statistics:</p> <ul> <li>constrained least squares with sum-to-one constraints on coefficients;  </li> <li>portfolio optimization with ;  </li> <li>quadratic surrogate subproblems inside second-order methods.</li> </ul> <p>The structure of the KKT matrix (symmetric, indefinite, with blocks \\(P\\), \\(A\\)) can be exploited by specialized linear solvers and factorizations.</p>"},{"location":"convex/19a_optimization_constraints/#null-space-reduced-variable-method","title":"Null-Space (Reduced Variable) Method","text":"<p>When the constraints are linear and of full row rank, a natural approach is to eliminate them explicitly.</p> <p>Choose:</p> <ul> <li>a particular feasible point \\(x_0\\) satisfying \\(A x_0 = b\\),  </li> <li>a matrix \\(Z \\in \\mathbb{R}^{n \\times (n-p)}\\) whose columns form a basis of the null space of \\(A\\):    </li> </ul> <p>Then every feasible \\(x\\) can be written as</p> \\[ x = x_0 + Z y, \\quad y \\in \\mathbb{R}^{n-p}. \\] <p>Substituting into the objective yields an unconstrained reduced problem in the smaller variable \\(y\\):</p> \\[ \\min_{y} \\; \\phi(y) := f(x_0 + Z y). \\] <p>Gradients and Hessians transform as</p> \\[ \\nabla_y \\phi(y) = Z^\\top \\nabla_x f(x_0 + Z y), \\qquad \\nabla_y^2 \\phi(y) = Z^\\top \\nabla_x^2 f(x_0 + Z y) \\, Z. \\] <p>We can now apply any unconstrained method (gradient descent, CG, Newton) to \\(\\phi(y)\\). The corresponding updates in the original space are mapped back via \\(x = x_0 + Z y\\).</p> <p>Key points:</p> <ul> <li>Optimization is restricted to feasible directions \\(\\operatorname{Null}(A)\\) by construction.  </li> <li>The dimension drops from \\(n\\) to \\(n-p\\), which can be advantageous if \\(p\\) is large.  </li> <li>The cost is computing and storing a suitable null-space basis \\(Z\\), which may destroy sparsity and be expensive for large-scale problems.</li> </ul> <p>Null-space methods are attractive when:</p> <ul> <li>the number of constraints is moderate,  </li> <li>a good factorization of \\(A\\) is available,  </li> <li>and we want an unconstrained algorithm in reduced coordinates.</li> </ul>"},{"location":"convex/19a_optimization_constraints/#newtons-method-for-equality-constrained-problems","title":"Newton\u2019s Method for Equality-Constrained Problems","text":"<p>For a twice-differentiable convex \\(f\\), we can derive an equality-constrained Newton step by solving a local quadratic approximation subject to linearized constraints.</p> <p>At a point \\(x\\), approximate \\(f(x+d)\\) by its second-order Taylor expansion:</p> \\[ f(x+d) \\approx f(x) + \\nabla f(x)^\\top d + \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d. \\] <p>We seek a step \\(d\\) that approximately minimizes this quadratic model while remaining feasible to first order, i.e.</p> \\[ \\begin{aligned} \\min_d &amp; \\quad \\tfrac{1}{2} d^\\top \\nabla^2 f(x) d + \\nabla f(x)^\\top d\\\\ \\text{s.t.} &amp; \\quad A d = 0. \\end{aligned} \\] <p>The KKT conditions for this quadratic subproblem are</p> \\[ \\begin{bmatrix} \\nabla^2 f(x) &amp; A^\\top \\\\ A &amp; 0 \\end{bmatrix} \\begin{bmatrix} d \\\\ \\lambda \\end{bmatrix} = - \\begin{bmatrix} \\nabla f(x) \\\\ 0 \\end{bmatrix}. \\] <p>Solving this system gives the Newton step \\(d_{\\text{nt}}\\) and a multiplier update \\(\\lambda\\). The primal update is</p> \\[ x_{k+1} = x_k + \\alpha_k d_{\\text{nt}}, \\] <p>with a step size \\(\\alpha_k \\in (0,1]\\) chosen by line search to ensure sufficient decrease and preservation of feasibility (for equality constraints, \\(A d_{\\text{nt}} = 0\\) guarantees \\(A x_{k+1} = b\\) whenever \\(A x_k = b\\)).</p> <p>Geometrically:</p> <ul> <li>unconstrained Newton would move by \\(-\\nabla^2 f(x)^{-1} \\nabla f(x)\\);  </li> <li>equality-constrained Newton projects this step onto the tangent space \\(\\{ d : A d = 0 \\}\\) of the affine constraint set.</li> </ul> <p>For strictly convex \\(f\\) with positive definite Hessian on the feasible directions, this method enjoys quadratic convergence near the solution, much like the unconstrained Newton method.</p>"},{"location":"convex/19a_optimization_constraints/#connections-to-machine-learning-and-signal-processing","title":"Connections to Machine Learning and Signal Processing","text":"<p>Linear equality constraints appear naturally in ML and related areas:</p> Setting Equality constraint Interpretation Portfolio optimization \\(\\mathbf{1}^\\top w = 1\\) Weights sum to one (full investment) Constrained regression \\(C x = d\\) Enforce domain-specific linear relations between coefficients Mixture models / convex combinations \\(\\mathbf{1}^\\top \\alpha = 1, \\; \\alpha \\ge 0\\) Mixture weights form a probability simplex Fairness constraints (linearized) \\(A w = 0\\) Enforce equal averages across groups or balance conditions Physics-informed models (discretized) \\(A x = b\\) Discrete conservation laws (mass, charge, energy) <p>More generally, nonlinear equality constraints (e.g. \\(W^\\top W = I\\) for orthonormal embeddings, or \\(\\|w\\|_2^2 = 1\\) for normalized weights) lead to optimization on curved manifolds. Techniques from this chapter extend to those settings when combined with Riemannian optimization or local parameterizations, but here we focus on the linear case as the fundamental building block.</p>"},{"location":"convex/19b_optimization_constraints/","title":"14. Optimization Algorithms for Inequality-Constrained Problems","text":""},{"location":"convex/19b_optimization_constraints/#chapter-14-optimization-algorithms-for-inequality-constrained-problems","title":"Chapter 14: Optimization Algorithms for Inequality-Constrained Problems","text":"<p>In many applications, we must optimize an objective while respecting inequality constraints: nonnegativity of variables, margin constraints in SVMs, capacity or safety limits, physical bounds, fairness budgets, and more. Mathematically, the feasible region is now a convex set with a boundary, and the optimizer often lies on that boundary.</p> <p>This chapter introduces algorithms for solving such problems, focusing on logarithmic barrier and interior-point methods. These are the workhorses behind modern general-purpose convex solvers (for LP, QP, SOCP, SDP) and provide a smooth way to enforce inequalities while still using Newton-type methods.</p>"},{"location":"convex/19b_optimization_constraints/#problem-setup","title":"Problem Setup","text":"<p>We consider the general convex problem with inequality and equality constraints  where</p> <ul> <li>\\(f_0, f_1,\\dots,f_m\\) are convex, typically twice differentiable,</li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\) has full row rank,</li> <li>there exists a strictly feasible point \\(\\bar{x}\\) such that   \\(f_i(\\bar{x}) &lt; 0\\) for all \\(i\\) and \\(A\\bar{x} = b\\) (Slater\u2019s condition).</li> </ul> <p>Under these assumptions:</p> <ul> <li>the problem is convex,</li> <li>strong duality holds (zero duality gap),</li> <li>and the KKT conditions characterize optimality.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#examples","title":"Examples","text":"Problem type \\(f_0(x)\\) Constraints \\(f_i(x)\\le0\\) ML / applications Linear program (LP) \\(c^\\top x\\) \\(a_i^\\top x - b_i \\le 0\\) resource allocation, feature selection Quadratic program \\(\\tfrac12 x^\\top P x + q^\\top x\\) linear SVMs, ridge with box constraints QCQP quadratic quadratic portfolio optimization, control Entropy models \\(\\sum_i x_i \\log x_i\\) \\(F x - g \\le 0\\) probability calibration, max-entropy Nonnegativity arbitrary convex \\(-x_i \\le 0\\) sparse coding, nonnegative factorization <p>Many machine-learning training problems can be written in this template by expressing regularization, margins, fairness, or safety conditions as convex inequalities.</p>"},{"location":"convex/19b_optimization_constraints/#indicator-function-view-of-constraints","title":"Indicator-Function View of Constraints","text":"<p>Conceptually, we can write inequality constraints using an indicator function. Define  </p> <p>Then the inequality-constrained problem is equivalent to  </p> <ul> <li>If \\(x\\) is feasible (\\(f_i(x) \\le 0\\) for all \\(i\\)), the indicators contribute \\(0\\).</li> <li>If any constraint is violated (\\(f_i(x) &gt; 0\\)), the objective becomes \\(+\\infty\\).</li> </ul> <p>This formulation is clean but not numerically friendly:</p> <ul> <li>\\(I_{-}\\) is discontinuous and nonsmooth.</li> <li>We cannot directly apply Newton-type methods.</li> </ul> <p>The key idea of barrier methods is to replace the hard indicator with a smooth approximation that grows to \\(+\\infty\\) as we approach the boundary.</p>"},{"location":"convex/19b_optimization_constraints/#logarithmic-barrier-approximation","title":"Logarithmic Barrier Approximation","text":"<p>We approximate the indicator \\(I_{-}\\) with a smooth barrier function  </p> <p>For each inequality \\(f_i(x) \\le 0\\), we introduce a barrier term \\(-\\log(-f_i(x))\\). For a given parameter \\(t &gt; 0\\), we solve the barrier subproblem  where  </p> <p>Equivalently,  </p> <p>Interpretation:</p> <ul> <li>The barrier term \\(\\phi(x)\\) is finite only for strictly feasible points (\\(f_i(x) &lt; 0\\)).</li> <li>As \\(x\\) approaches the boundary \\(f_i(x) \\to 0^-\\), the term \\(-\\log(-f_i(x)) \\to +\\infty\\).</li> <li>The parameter \\(t\\) controls the trade-off:</li> <li>small \\(t\\) (large \\(1/t\\)) \u2192 strong barrier, solution stays deep inside the feasible set;</li> <li>large \\(t\\) \u2192 barrier is weaker, solutions can move closer to the boundary.</li> </ul> <p>As \\(t \\to \\infty\\), solutions of the barrier subproblem approach the solution of the original constrained problem.</p>"},{"location":"convex/19b_optimization_constraints/#derivatives-of-the-barrier","title":"Derivatives of the Barrier","text":"<p>Let  Then \\(\\phi\\) is convex and twice differentiable on its domain. Its gradient and Hessian are  </p> <p>Key features:</p> <ul> <li>As \\(f_i(x) \\uparrow 0\\) (approaching the boundary from inside), the factor \\(1/(-f_i(x))\\) blows up, so \\(\\|\\nabla \\phi(x)\\|\\) becomes very large.</li> <li>This creates a strong repulsive force that prevents iterates from crossing the boundary.</li> <li>The barrier \u201cpushes\u201d the solution away from constraint violation, while the original objective \\(f_0(x)\\) pulls toward lower cost.</li> </ul> <p>The barrier subproblem  is a smooth equality-constrained problem. We can therefore apply equality-constrained Newton methods (Chapter 13) at each fixed \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#central-path-and-approximate-kkt-conditions","title":"Central Path and Approximate KKT Conditions","text":"<p>For each \\(t &gt; 0\\), let \\(x^\\star(t)\\) be a minimizer of the barrier problem  </p> <p>The set \\(\\{x^\\star(t) : t &gt; 0\\}\\) is called the central path. As \\(t \\to \\infty\\), \\(x^\\star(t)\\) converges to a solution \\(x^\\star\\) of the original inequality-constrained problem.</p> <p>We can associate approximate dual variables to \\(x^\\star(t)\\):  </p> <p>Then the KKT-like relations hold:  </p> <p>Compare with the exact KKT conditions (for optimal \\((x^\\star,\\lambda^\\star,v^\\star)\\)):  </p> <p>Along the central path we have the relaxed complementarity condition  which tends to \\(0\\) as \\(t \\to \\infty\\). Hence the barrier formulation naturally yields approximate primal\u2013dual solutions whose KKT residuals shrink as we increase \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#geometric-and-physical-intuition","title":"Geometric and Physical Intuition","text":"<p>Consider the barrier-augmented objective  </p> <p>We can interpret this as:</p> <ul> <li>\\(t f_0(x)\\): an \u201cexternal potential\u201d pulling us toward low objective values.</li> <li>\\(-\\log(-f_i(x))\\): repulsive potentials that become infinite near the boundary \\(f_i(x)=0\\).</li> </ul> <p>At a minimizer \\(x^\\star(t)\\), we have  </p> <p>The gradient of the objective is exactly balanced by a weighted sum of constraint gradients. This is a force-balance condition:</p> <ul> <li>constraints \u201cpush back\u201d more strongly when \\(x\\) is close to their boundary,</li> <li>the interior-point iterates follow a smooth path that stays strictly feasible   and moves gradually toward the optimal boundary point.</li> </ul> <p>This picture explains both:</p> <ul> <li>why iterates never leave the feasible region, and  </li> <li>why the method naturally generates dual variables (the weights on constraint gradients).</li> </ul>"},{"location":"convex/19b_optimization_constraints/#the-barrier-method","title":"The Barrier Method","text":"<p>The barrier method solves the original inequality-constrained problem by solving a sequence of barrier subproblems with increasing \\(t\\).</p>"},{"location":"convex/19b_optimization_constraints/#algorithm-barrier-method-conceptual-form","title":"Algorithm: Barrier Method (Conceptual Form)","text":"<p>Given:</p> <ul> <li>a strictly feasible starting point \\(x\\) (\\(f_i(x) &lt; 0\\), \\(A x = b\\)),</li> <li>initial barrier parameter \\(t &gt; 0\\),</li> <li>barrier growth factor \\(\\mu &gt; 1\\) (e.g. \\(\\mu \\in [10,20]\\)),</li> <li>accuracy tolerance \\(\\varepsilon &gt; 0\\),</li> </ul> <p>repeat:</p> <ol> <li> <p>Centering step    Solve the equality-constrained problem        using an equality-constrained Newton method.    (In practice, we start from the previous solution and take a small number of Newton steps rather than \u201csolve exactly\u201d.)</p> </li> <li> <p>Update iterate    Let \\(x\\) be the resulting point (the approximate minimizer for current \\(t\\)).</p> </li> <li> <p>Check stopping criterion    For the barrier problem, one can show        where \\(p^\\star\\) is the optimal value of the original problem.    If        then stop: \\(x\\) is guaranteed to be within \\(\\varepsilon\\) (in objective value) of optimal.</p> </li> <li> <p>Increase \\(t\\)    Set \\(t := \\mu t\\) to weaken the barrier and move closer to the true boundary, then go back to Step 1.</p> </li> </ol> <p>Key parameters:</p> Symbol Role \\(t\\) barrier strength (larger \\(t\\) = weaker barrier, closer to solution) \\(\\mu\\) growth factor for \\(t\\) \\(\\varepsilon\\) desired accuracy (duality-gap based) \\(m\\) number of inequality constraints <p>In practice:</p> <ul> <li>\\(\\varepsilon\\) is often in the range \\(10^{-3}\\)\u2013\\(10^{-8}\\),</li> <li>\\(\\mu\\) is chosen to balance outer iterations vs inner Newton steps,</li> <li>the centering step is usually solved to modest accuracy, not exactness.</li> </ul>"},{"location":"convex/19b_optimization_constraints/#from-barrier-methods-to-interior-point-methods","title":"From Barrier Methods to Interior-Point Methods","text":"<p>Pure barrier methods conceptually \u201csolve a sequence of problems for increasing \\(t\\)\u201d. Modern interior-point methods refine this idea:</p> <ul> <li>they update both primal variables \\(x\\) and dual variables \\((\\lambda, v)\\),</li> <li>they use Newton\u2019s method on the (perturbed) KKT system,</li> <li>they follow the central path by simultaneously enforcing:</li> <li>primal feasibility (\\(f_i(x) \\le 0\\), \\(A x = b\\)),</li> <li>dual feasibility (\\(\\lambda_i \\ge 0\\)),</li> <li>relaxed complementarity (\\(-\\lambda_i f_i(x) \\approx 1/t\\)).</li> </ul> <p>A typical primal\u2013dual step solves a linearized KKT system of the form  </p> <p>Newton\u2019s method applied to these equations yields search directions for \\((x,\\lambda,v)\\) that move toward the central path and reduce primal and dual residuals simultaneously. This is what modern LP/QP/SOCP/SDP solvers implement.</p> <p>You do not need to implement these methods from scratch to use them: in practice, you describe your problem in a modeling language (e.g. CVX, CVXPY, JuMP) and rely on an interior-point solver under the hood.</p>"},{"location":"convex/19b_optimization_constraints/#computational-and-practical-notes","title":"Computational and Practical Notes","text":"<p>Some important practical aspects:</p> <ol> <li> <p>Equality-constrained Newton inside    Each barrier subproblem is solved by equality-constrained Newton (Chapter 13). The main cost is solving the KKT linear system at each Newton step.</p> </li> <li> <p>Strict feasibility    Barrier and interior-point methods require a strictly feasible starting point \\(x\\) with \\(f_i(x) &lt; 0\\).  </p> </li> <li>Sometimes this is easy (e.g. nonnegativity constraints with a positive initial vector).  </li> <li> <p>Otherwise, a separate phase I problem is solved to find such a point or to certify infeasibility.</p> </li> <li> <p>Step size control    Because the barrier blows up near the boundary, too aggressive Newton steps may try to leave the feasible region. A backtracking line search is used to ensure:</p> </li> <li>sufficient decrease in the barrier objective,</li> <li> <p>and preservation of strict feasibility (\\(f_i(x) &lt; 0\\) remains true).</p> </li> <li> <p>Accuracy vs cost    The duality-gap bound \\(m/t\\) provides a clear trade-off:</p> </li> <li>small \\(m/t\\) (large \\(t\\)) \u2192 high accuracy, more iterations,</li> <li> <p>larger \\(m/t\\) \u2192 faster but less precise.</p> </li> <li> <p>Sparsity and structure    For large problems, exploiting sparsity in \\(A\\) and in the Hessians \\(\\nabla^2 f_i(x)\\) is crucial. Interior-point methods scale well when linear algebra is carefully optimized.</p> </li> </ol>"},{"location":"convex/19b_optimization_constraints/#equality-vs-inequality-constrained-algorithms","title":"Equality vs Inequality-Constrained Algorithms","text":"<p>Finally, it is helpful to contrast the equality-only case (Chapter 13) with the inequality case.</p> Aspect Equality constraints \\(A x = b\\) Inequality constraints \\(f_i(x) \\le 0\\) Feasible set Affine subspace General convex region with boundary Typical algorithms Lagrange/KKT, equality-constrained Newton, null-space Barrier methods, primal\u2013dual interior-point methods Feasibility during iteration Can start infeasible and converge to \\(A x = b\\) Iterates kept strictly feasible (\\(f_i(x) &lt; 0\\)) Complementarity Not present (only equalities) \\(\\lambda_i f_i(x) = 0\\) at optimum, or \\(\\approx -1/t\\) along central path Geometric picture Optimization on a flat manifold Optimization in a convex region, repelled from boundary ML relevance Normalization, linear invariants, balance constraints Nonnegativity, margin constraints, safety/fairness limits <p>In summary:</p> <ul> <li>Equality-constrained methods operate directly on an affine manifold using KKT and Newton.  </li> <li>Inequality-constrained methods use smooth barriers (or primal\u2013dual perturbed KKT systems) to stay in the interior and gradually approach the boundary and the optimal point.</li> </ul> <p>Interior-point methods unify these perspectives and are the backbone of modern convex optimization software.</p>"},{"location":"convex/20_advanced/","title":"15. Advanced Large-Scale and Structured Methods","text":""},{"location":"convex/20_advanced/#chapter-15-advanced-large-scale-and-structured-methods","title":"Chapter 15: Advanced Large-Scale and Structured Methods","text":"<p>Modern convex optimization often runs at massive scale: millions (or billions) of variables, datasets too large to fit in memory, and constraints spread across machines or devices. Per-iteration cost and memory usage often of often makes classical solutions impractical for these regimes.</p> <p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to make convex optimization scalable. These ideas underpin the optimization engines behind most modern machine learning systems.</p>"},{"location":"convex/20_advanced/#motivation-structure-and-scale","title":"Motivation: Structure and Scale","text":"<p>In large-scale convex optimization, the challenge is not \u201cdoes a solution exist?\u201d but rather \u201ccan we compute it in time and memory?\u201d.</p> <p>Bottlenecks include:</p> <ul> <li>Memory: storing Hessians (or even full gradients) may be impossible.</li> <li>Data size: one full pass over all samples can already be expensive.</li> <li>Distributed data: samples are spread across devices / workers.</li> <li>Sparsity and separability: the problem often decomposes into many small pieces.</li> </ul> <p>A common template is the empirical risk + regularizer form  where</p> <ul> <li>each \\(f_i(x)\\) is a loss term for sample \\(i\\),</li> <li>\\(R(x)\\) is a regularizer (possibly nonsmooth, e.g. \\(\\lambda\\|x\\|_1\\)).</li> </ul> <p>The methods in this chapter are designed to exploit this structure:</p> <ul> <li>update only parts of \\(x\\) at a time (coordinate/block methods),</li> <li>use only some data per step (stochastic methods),</li> <li>split the problem into simpler subproblems (proximal / ADMM),</li> <li>or distribute computation across multiple machines (consensus methods).</li> </ul>"},{"location":"convex/20_advanced/#coordinate-descent","title":"Coordinate Descent","text":"<p>Coordinate descent updates one coordinate (or a small block of coordinates) at a time, holding all others fixed. It is especially effective when updates along a single coordinate are cheap to compute. Given \\(x^{(k)}\\), choose coordinate \\(i_k\\) and define</p> \\[ x^{(k+1)}_i = \\begin{cases} \\displaystyle \\arg\\min_{z \\in \\mathbb{R}} \\; f\\big(x_1^{(k+1)},\\dots,x_{i_k-1}^{(k+1)},z,x_{i_k+1}^{(k)},\\dots,x_n^{(k)}\\big), &amp; i = i_k,\\\\[4pt] x_i^{(k)}, &amp; i \\ne i_k. \\end{cases} \\] <p>In practice:</p> <ul> <li>\\(i_k\\) is chosen either cyclically (\\(1,2,\\dots,n,1,2,\\dots\\)) or randomly.</li> <li>Each coordinate update often has a closed form (e.g. soft-thresholding for LASSO).</li> <li>You never form or store the full gradient; you only need partial derivatives.</li> </ul> <p>Why it scales:</p> <ul> <li>Each step is very cheap \u2014 often \\(O(1)\\) or proportional to the number of nonzeros in the column corresponding to coordinate \\(i_k\\).</li> <li>In high dimensions (e.g., millions of features), this can be far more efficient than updating all coordinates at once.</li> </ul> <p>Convergence: If \\(f\\) is convex and has Lipschitz-continuous partial derivatives, coordinate descent (cyclic or randomized) converges to the global minimizer. Randomized coordinate descent often has clean expected convergence rates.</p> <p>ML context:</p> <ul> <li>LASSO / Elastic Net regression (coordinate updates are soft-thresholding),</li> <li>\\(\\ell_1\\)-penalized logistic regression,</li> <li>matrix factorization and dictionary learning (updating one factor vector at a time),</li> <li>problems where \\(R(x)\\) is separable: \\(R(x) = \\sum_i R_i(x_i)\\).</li> </ul>"},{"location":"convex/20_advanced/#stochastic-gradient-and-variance-reduced-methods","title":"Stochastic Gradient and Variance-Reduced Methods","text":"<p>When \\(N\\) (number of samples) is huge, computing the full gradient  every iteration is too expensive. Stochastic methods replace this full gradient with cheap, unbiased estimates based on small random subsets (mini-batches) of data.</p>"},{"location":"convex/20_advanced/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>At iteration \\(k\\):</p> <ol> <li>Sample a mini-batch \\(\\mathcal{B}_k \\subset \\{1,\\dots,N\\}\\).</li> <li>Form the stochastic gradient     </li> <li>Update        where \\(\\eta_k &gt; 0\\) is the learning rate.</li> </ol> <p>Properties:</p> <ul> <li>\\(\\mathbb{E}[\\widehat{\\nabla f}(x_k) \\mid x_k] = \\nabla f(x_k)\\) (unbiased),</li> <li>Each iteration is cheap (depends only on \\(|\\mathcal{B}_k|\\), not \\(N\\)),</li> <li>The noise can help escape shallow nonconvex traps (in deep learning).</li> </ul> <p>In convex settings, SGD trades off per-iteration cost against convergence speed: many cheap noisy steps instead of fewer expensive precise ones.</p>"},{"location":"convex/20_advanced/#step-sizes-and-averaging","title":"Step Sizes and Averaging","text":"<p>The step size \\(\\eta_k\\) is crucial:</p> <ul> <li>Too large \u2192 iterates diverge or oscillate.</li> <li>Too small \u2192 extremely slow progress.</li> </ul> <p>Typical schedules for convex problems:</p> <ul> <li>General convex: \\(\\eta_k = \\frac{c}{\\sqrt{k}}\\),</li> <li>Strongly convex: \\(\\eta_k = \\frac{c}{k}\\).</li> </ul> <p>Two important stabilization techniques:</p> <ol> <li>Decay the learning rate over time.</li> <li>Polyak\u2013Ruppert averaging: return the average        instead of the last iterate. Averaging cancels noise and leads to optimal \\(O(1/k)\\) rates in strongly convex settings.</li> </ol> <p>Mini-batch size can also grow with \\(k\\), gradually reducing variance while keeping early iterations cheap.</p>"},{"location":"convex/20_advanced/#convergence-rates","title":"Convergence Rates","text":"<p>For convex \\(f\\):</p> <ul> <li>With appropriate diminishing \\(\\eta_k\\), \\(\\mathbb{E}[f(x_k)] - f^\\star = O(k^{-1/2})\\).</li> </ul> <p>For strongly convex \\(f\\):</p> <ul> <li>With \\(\\eta_k = O(1/k)\\) and averaging, \\(\\mathbb{E}[\\|x_k - x^\\star\\|^2] = O(1/k)\\).</li> </ul> <p>These rates are optimal for unbiased first-order stochastic methods.</p>"},{"location":"convex/20_advanced/#variance-reduced-methods-svrg-saga-sarah","title":"Variance-Reduced Methods (SVRG, SAGA, SARAH)","text":"<p>Plain SGD cannot easily reach very high accuracy because the gradient noise never fully disappears. Variance-reduced methods reduce this noise, especially near the solution, by periodically using the full gradient.</p> <p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p> <ul> <li>Pick a reference point \\(\\tilde{x}\\) and compute \\(\\nabla f(\\tilde{x})\\).</li> <li>For inner iterations:      where \\(i_k\\) is a random sample index.</li> </ul> <p>Here \\(v_k\\) is still an unbiased estimator of \\(\\nabla f(x_k)\\), but its variance decays as \\(x_k\\) approaches \\(\\tilde{x}\\). For strongly convex \\(f\\), methods like SVRG and SAGA achieve linear convergence, comparable to full gradient descent but at near-SGD cost.</p>"},{"location":"convex/20_advanced/#momentum-and-adaptive-methods","title":"Momentum and Adaptive Methods","text":"<p>In practice, large-scale learning often uses SGD with various modifications:</p> <ul> <li> <p>Momentum / Nesterov: keep a moving average of gradients      which accelerates progress along consistent directions and damps oscillations.</p> </li> <li> <p>Adaptive methods (Adagrad, RMSProp, Adam): maintain coordinate-wise scales based on past squared gradients, effectively using a diagonal preconditioner that adapts to curvature and feature scales.</p> </li> </ul> <p>These methods are especially popular in deep learning. For convex problems, their theoretical behavior is subtle, but empirically they often converge faster in wall-clock time.</p>"},{"location":"convex/20_advanced/#proximal-and-composite-optimization","title":"Proximal and Composite Optimization","text":"<p>Many large-scale convex problems are composite:  where</p> <ul> <li>\\(g\\) is smooth convex with Lipschitz gradient (e.g. data-fitting term),</li> <li>\\(R\\) is convex but possibly nonsmooth (e.g. \\(\\lambda\\|x\\|_1\\), indicator of a constraint set, nuclear norm).</li> </ul> <p>The proximal gradient method (a.k.a. ISTA) updates as  where the proximal operator is  </p> <p>Intuition:</p> <ul> <li>The gradient step moves \\(x\\) in a direction that lowers the smooth term \\(g\\).</li> <li>The prox step solves a small \u201cregularized\u201d problem, pulling \\(x\\) toward a structure favored by \\(R\\) (sparsity, low rank, feasibility, etc.).</li> </ul> <p>Examples of prox operators:</p> <ul> <li>\\(R(x) = \\lambda\\|x\\|_1\\) \u2192 soft-thresholding (coordinate-wise shrinkage).</li> <li>\\(R\\) = indicator of a convex set \\(\\mathcal{X}\\) \u2192 projection onto \\(\\mathcal{X}\\) (so projected gradient is a special case).</li> <li>\\(R(X) = \\|X\\|_*\\) (nuclear norm) \u2192 singular value soft-thresholding.</li> </ul> <p>For large-scale problems:</p> <ul> <li>Proximal gradient scales like gradient descent: each iteration uses only \\(\\nabla g\\) and a prox (often cheap and parallelizable).</li> <li>Accelerated variants (FISTA) achieve \\(O(1/k^2)\\) rates for smooth \\(g\\).</li> </ul>"},{"location":"convex/20_advanced/#alternating-direction-method-of-multipliers-admm","title":"Alternating Direction Method of Multipliers (ADMM)","text":"<p>When objectives naturally split into simpler pieces depending on different variables, ADMM is a powerful tool. It is especially useful when:</p> <ul> <li>\\(f\\) and \\(g\\) have simple prox operators,</li> <li>the problem is distributed or separable across machines.</li> </ul> <p>Consider  </p> <p>The augmented Lagrangian is  with dual variable \\(y\\) and penalty parameter \\(\\rho &gt; 0\\).</p> <p>ADMM performs the iterations:  </p> <p>Interpretation:</p> <ul> <li>The \\(x\\)-update solves a subproblem involving \\(f\\) only.</li> <li>The \\(z\\)-update solves a subproblem involving \\(g\\) only.</li> <li>The \\(y\\)-update nudges the constraint \\(A x + B z = c\\) toward satisfaction.</li> </ul> <p>For convex \\(f,g\\), ADMM converges to a primal\u2013dual optimal point. It is particularly effective when the \\(x\\)- and \\(z\\)-subproblems have closed-form prox solutions or can be solved cheaply in parallel.</p> <p>ML use cases:</p> <ul> <li>Distributed LASSO / logistic regression,</li> <li>matrix completion and robust PCA,</li> <li>consensus optimization (each worker has local data but shares a global model),</li> <li>some federated learning formulations.</li> </ul>"},{"location":"convex/20_advanced/#majorizationminimization-mm-and-em-algorithms","title":"Majorization\u2013Minimization (MM) and EM Algorithms","text":"<p>The Majorization\u2013Minimization (MM) principle constructs at each iterate \\(x_k\\) a surrogate function \\(g(\\cdot \\mid x_k)\\) that upper-bounds \\(f\\) and is easier to minimize.</p> <p>Requirements:  </p> <p>Then define  </p> <p>This guarantees monotone decrease:  </p> <p>The famous Expectation\u2013Maximization (EM) algorithm is an MM method for latent-variable models, where the surrogate arises from Jensen\u2019s inequality and missing-data structure.</p> <p>Other examples:</p> <ul> <li>Iteratively Reweighted Least Squares (IRLS) for logistic regression and robust regression,</li> <li>MM surrogates for nonconvex penalties (e.g. smoothly approximating \\(\\ell_0\\)),</li> <li>mixture models and variational inference.</li> </ul>"},{"location":"convex/20_advanced/#distributed-and-parallel-optimization","title":"Distributed and Parallel Optimization","text":"<p>When data or variables are split across machines, we need distributed or parallel optimization schemes.</p>"},{"location":"convex/20_advanced/#synchronous-vs-asynchronous","title":"Synchronous vs Asynchronous","text":"<ul> <li>Synchronous methods: all workers compute local gradients or updates, then synchronize (e.g. parameter server, federated averaging).</li> <li>Asynchronous methods: workers update parameters without global synchronization, improving hardware utilization but introducing staleness and variance.</li> </ul>"},{"location":"convex/20_advanced/#consensus-optimization","title":"Consensus Optimization","text":"<p>A standard pattern is consensus form:  where \\(f_i\\) is the local objective on worker \\(i\\) and \\(z\\) is the global consensus variable.</p> <p>ADMM applied to this problem:</p> <ul> <li>Each worker updates its local \\(x_i\\) using only local data,</li> <li>The global variable \\(z\\) is updated by averaging or aggregation,</li> <li>Dual variables enforce agreement \\(x_i \\approx z\\).</li> </ul> <p>This template underlies many federated learning and parameter-server architectures.</p>"},{"location":"convex/20_advanced/#ml-context","title":"ML Context","text":"<ul> <li>Federated learning (phone/edge devices update local models and send summaries to a server),</li> <li>Large-scale convex optimization over sharded datasets,</li> <li>Distributed sparse regression, matrix factorization, and graphical model learning.</li> </ul>"},{"location":"convex/20_advanced/#handling-structure-sparsity-and-low-rank","title":"Handling Structure: Sparsity and Low Rank","text":"<p>Large-scale convex problems often have additional structure that we can exploit algorithmically:</p> Structure Typical Regularizer / Constraint Algorithmic Benefit Sparsity \\(\\ell_1\\), group lasso Cheap coordinate updates, soft-thresholding Low rank Nuclear norm \\(\\|X\\|_*\\) SVD-based prox; rank truncation Block separability \\(\\sum_i f_i(x_i)\\) Parallel or distributed block updates Graph structure Total variation on graphs Local neighborhood computations Probability simplex simplex constraint or entropy term Mirror descent, simplex projections <p>Examples:</p> <ul> <li>In compressed sensing, \\(\\ell_1\\) regularization + sparse sensing matrices \u2192 very cheap mat\u2013vecs + prox operations.</li> <li>In matrix completion, nuclear norm structure + low-rank iterates \u2192 approximate SVD instead of full SVD.</li> <li>In TV denoising, local difference structure \u2192 each prox step involves only neighboring pixels/vertices.</li> </ul> <p>Exploiting structure can yield orders-of-magnitude speedups compared to generic solvers.</p>"},{"location":"convex/20_advanced/#summary-and-practical-guidance","title":"Summary and Practical Guidance","text":"<p>Different large-scale methods are appropriate in different regimes:</p> Method Gradient Access Scalability Parallelization Convexity Needed Typical Uses Coordinate Descent Partial gradients High Easy (blockwise) Convex LASSO, sparse GLMs, matrix factorization SGD / Mini-batch SGD Stochastic gradients Excellent Natural (data parallel) Convex / nonconvex Deep learning, logistic regression SVRG / SAGA (VR methods) Stochastic + periodic full gradient High Data parallel Convex, often strongly convex Large-scale convex ML, GLMs Proximal Gradient (ISTA/FISTA) Full gradient + prox Moderate\u2013High Easy Convex Composite objectives with structure ADMM Local subproblems High Designed for distributed Convex Consensus, distributed convex solvers MM / EM Surrogates Moderate Model-specific Convex / nonconvex Latent-variable models, IRLS Distributed / Federated Local gradients Very high Essential Often convex / smooth Federated learning, multi-agent systems"},{"location":"convex/21_models/","title":"16. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"convex/21_models/#chapter-16-modelling-patterns-and-algorithm-selection","title":"Chapter 16: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"convex/21_models/#regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithms Summary:  </p> <ul> <li>Smooth + \u2113\u2082 (strongly convex):   Newton / quasi-Newton, conjugate gradient, or accelerated gradient.</li> <li>Smooth + \u2113\u2081 (and variants):   proximal gradient or coordinate descent; for huge data, stochastic/proximal variants.</li> <li>Mixed penalties (\u2113\u2081 + \u2113\u2082):   treat as composite smooth + nonsmooth; prox and coordinate methods still apply.</li> <li>Large \\(N\\) or \\(n\\):   mini-batch / stochastic gradients (SGD, SVRG, etc.) on the smooth part + prox for the regulariser.</li> </ul> <p>Regularisation strength \\(\\lambda\\) is usually chosen via cross-validation or a validation set, exploring the accuracy\u2013simplicity trade-off.</p>"},{"location":"convex/21_models/#robust-regression-and-outlier-resistance","title":"Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"convex/21_models/#least-absolute-deviations-l1-loss","title":"Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale.  </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"convex/21_models/#huber-loss","title":"Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints. That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly. This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"convex/21_models/#worst-case-robust-regression","title":"Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"convex/21_models/#maximum-likelihood-and-loss-design","title":"Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"convex/21_models/#structured-constraints-in-engineering-and-design","title":"Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"convex/21_models/#linear-and-conic-programming-the-canonical-models","title":"Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> <li> <p>Practical patterns:</p> <ol> <li>Resource allocation/flow (LP): linear costs and constraints.</li> <li>Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).</li> <li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li> </ol> </li> </ul> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"convex/21_models/#risk-safety-margins-and-robust-design","title":"Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"convex/21_models/#cheat-sheet-if-your-problem-looks-like-this-use-that","title":"Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"convex/21_models/#matching-model-structure-to-algorithm-type","title":"Matching Model Structure to Algorithm Type","text":"Model Type Problem Form Recommended Algorithms Notes / ML Examples Smooth unconstrained \\(\\min f(x)\\) Gradient descent, Newton, LBFGS Small to medium problems; logistic regression, ridge regression Nonsmooth unconstrained \\(\\min f(x) + R(x)\\) Subgradient, proximal (ISTA/FISTA), coordinate descent LASSO, hinge loss SVM Equality-constrained \\(\\min f(x)\\) s.t. \\(A x = b\\) Projected gradient, augmented Lagrangian Constrained least squares, balance conditions Inequality-constrained \\(\\min f(x)\\) s.t. \\(f_i(x)\\le 0\\) Barrier, primal\u2013dual, interior-point Quadratic programming, LPs, constrained regression Separable / block structure \\(\\min \\sum_i f_i(x_i)\\) ADMM, coordinate updates Distributed optimization, federated learning Stochastic / large data \\(\\min \\frac{1}{N}\\sum_i f_i(x_i)\\) SGD, SVRG, adaptive variants Deep learning, online convex optimization Low-rank / matrix structure \\(\\min f(X) + \\lambda \\|X\\|_*\\) Proximal (SVD shrinkage), ADMM Matrix completion, PCA variants"},{"location":"convex/30_canonical_problems/","title":"17. Canonical Problems in Convex Optimization","text":""},{"location":"convex/30_canonical_problems/#chapter-17-canonical-problems-in-convex-optimization","title":"Chapter 17: Canonical Problems in Convex Optimization","text":"<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms \u2014 each with characteristic geometry, structure, and algorithms.</p>"},{"location":"convex/30_canonical_problems/#hierarchy-of-canonical-problems","title":"Hierarchy of Canonical Problems","text":"<p>Convex programs form a nested hierarchy:</p> \\[ \\text{LP} \\subseteq \\text{QP} \\subseteq \\text{SOCP} \\subseteq \\text{SDP}. \\] <p>Each inclusion represents an extension of representational power \u2014 from linear to quadratic, to conic, and finally to semidefinite constraints. Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p> Class Canonical Form Key Condition Typical Algorithms ML / Applied Examples LP \\(\\min_x c^\\top x\\) s.t. \\(A x=b,\\,x\\ge0\\) Linear constraints Simplex, Interior-point Resource allocation, Chebyshev regression QP \\(\\min_x \\tfrac12 x^\\top Q x + c^\\top x\\) s.t. \\(A x\\le b\\) \\(Q\\succeq0\\) Interior-point, Active-set, CG Ridge, SVM, Portfolio optimization QCQP \\(\\min_x \\tfrac12 x^\\top P_0 x + q_0^\\top x\\) s.t. \\(\\tfrac12 x^\\top P_i x + q_i^\\top x \\le0\\) All \\(P_i\\succeq0\\) Interior-point, SOCP reformulation Robust regression, trust-region SOCP \\(\\min_x f^\\top x\\) s.t. \\(\\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i\\) Cone constraints Conic interior-point Robust least-squares, risk limits SDP \\(\\min_X \\mathrm{Tr}(C^\\top X)\\) s.t. \\(\\mathrm{Tr}(A_i^\\top X)=b_i\\), \\(X\\succeq0\\) Matrix PSD constraint Interior-point, low-rank first-order Covariance estimation, control GP \\(\\min_{x&gt;0} f_0(x)\\) s.t. \\(f_i(x)\\le1,\\,g_j(x)=1\\) Log-convex after \\(y=\\log x\\) Log-transform + IPM Circuit design, power control MLE / GLM $\\min_x -\\sum_i \\log p(b_i a_i^\\top x)+\\mathcal{R}(x)$ Log-concave likelihood Newton, L-BFGS, Prox / SGD"},{"location":"convex/30_canonical_problems/#linear-programming-lp","title":"Linear Programming (LP)","text":"<p>Form</p> \\[ \\min_x c^\\top x \\quad \\text{s.t. } A x=b,\\, x\\ge0 \\] <p>Geometry: Feasible region = polyhedron; optimum = vertex. Applications: Resource allocation, shortest path, flow, scheduling. Algorithms:</p> <ol> <li>Simplex: walks along edges (vertex-based).  </li> <li>Interior-point: moves through the interior using log barriers.  </li> <li>Decomposition: exploits block structure for large LPs.</li> </ol>"},{"location":"convex/30_canonical_problems/#quadratic-programming-qp","title":"Quadratic Programming (QP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top Q x + c^\\top x  \\quad \\text{s.t. } A x \\le b,\\, F x = g, \\quad Q\\succeq0 \\] <p>Geometry: Objective = ellipsoids; feasible = polyhedron. Examples: Ridge regression, Markowitz portfolio, SVM. Algorithms: - Interior-point (smooth path). - Active-set (edge-following). - Conjugate Gradient for large unconstrained QPs. - First-order methods for massive \\(n\\).</p>"},{"location":"convex/30_canonical_problems/#quadratically-constrained-qp-qcqp","title":"Quadratically Constrained QP (QCQP)","text":"<p>Form</p> \\[ \\min_x \\tfrac12 x^\\top P_0x + q_0^\\top x \\quad \\text{s.t. } \\tfrac12 x^\\top P_i x + q_i^\\top x + r_i \\le 0 \\] <p>Convex if all \\(P_i\\succeq0\\). Geometry: Intersection of ellipsoids and half-spaces. Applications: Robust control, filter design, trust-region. Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>"},{"location":"convex/30_canonical_problems/#second-order-cone-programming-socp","title":"Second-Order Cone Programming (SOCP)","text":"<p>Form</p> \\[ \\min_x f^\\top x \\quad \\text{s.t. }  \\|A_i x + b_i\\|_2 \\le c_i^\\top x + d_i,\\; F x = g \\] <p>Interpretation: Linear objective, norm-bounded constraints. Applications: Robust regression, risk-aware portfolio, engineering design. Algorithms: Conic interior-point; scalable ADMM variants. Special case: Any QP or norm constraint can be written as an SOCP.</p>"},{"location":"convex/30_canonical_problems/#semidefinite-programming-sdp","title":"Semidefinite Programming (SDP)","text":"<p>Form</p> \\[ \\min_X \\mathrm{Tr}(C^\\top X) \\quad \\text{s.t. } \\mathrm{Tr}(A_i^\\top X)=b_i,\\; X\\succeq0 \\] <p>Meaning: Variable = PSD matrix \\(X\\); constraints = affine. Geometry: Feasible region = intersection of affine space with PSD cone. Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion. Algorithms: Interior-point for moderate \\(n\\); low-rank proximal / Frank\u2013Wolfe for large-scale.</p>"},{"location":"convex/30_canonical_problems/#geometric-programming-gp","title":"Geometric Programming (GP)","text":"<p>Original form</p> \\[ \\min_{x&gt;0} f_0(x) \\quad \\text{s.t. } f_i(x)\\le1,\\; g_j(x)=1 \\] <p>where \\(f_i\\) are posynomials and \\(g_j\\) monomials.  </p> <p>Log transformation: With \\(y=\\log x\\), the problem becomes convex in \\(y\\). Applications: Circuit sizing, communication power control, resource allocation. Solvers: Convert to convex form \u2192 interior-point or primal-dual methods.</p>"},{"location":"convex/30_canonical_problems/#likelihood-based-convex-models-mle-and-glms","title":"Likelihood-Based Convex Models (MLE and GLMs)","text":"<p>General form</p> \\[ \\min_x -\\sum_i \\log p(b_i|a_i^\\top x) + \\mathcal{R}(x) \\] <p>Examples</p> Noise Model Objective Equivalent Problem Gaussian \\(\\|A x - b\\|_2^2\\) Least squares Laplacian \\(\\|A x - b\\|_1\\) Robust regression (LP) Bernoulli \\(\\sum_i \\log(1+e^{-y_i a_i^\\top x})\\) Logistic regression Poisson \\(\\sum_i [a_i^\\top x - y_i\\log(a_i^\\top x)]\\) Poisson GLM <p>Algorithms - Newton or IRLS (small\u2013medium). - Quasi-Newton / L-BFGS (moderate). - Proximal or SGD (large-scale).</p>"},{"location":"convex/30_canonical_problems/#solver-selection-summary","title":"Solver Selection Summary","text":"Problem Type Convex Form Key Solvers ML Examples LP Linear Simplex, Interior-point Minimax regression QP Quadratic Interior-point, CG, Active-set Ridge, SVM QCQP Quadratic + constraints IPM, SOCP / SDP reformulation Robust regression SOCP Cone Conic IPM, ADMM Robust least-squares SDP PSD cone Interior-point, low-rank FW Covariance, Max-cut relaxations GP Log-convex Log-transform + IPM Power allocation MLE / GLM Log-concave Newton, L-BFGS, Prox-SGD Logistic regression"},{"location":"convex/35_modern/","title":"18. Modern Optimizers in Machine Learning Frameworks","text":""},{"location":"convex/35_modern/#chapter-18-modern-optimizers-in-machine-learning","title":"Chapter 18: Modern Optimizers in Machine Learning","text":"<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization\u2014balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p> <p>This chapter connects the principles of convex optimization to the modern optimizers that power today\u2019s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>"},{"location":"convex/35_modern/#stochastic-optimization-overview","title":"Stochastic Optimization Overview","text":"<p>In machine learning, we often minimize an empirical risk:  where \\(\\ell(x; z_i)\\) is the loss on data sample \\(z_i\\).</p> <p>Computing the full gradient \\(\\nabla f(x)\\) is infeasible when \\(N\\) is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p> \\[ g_k = \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla \\ell(x_k; z_i). \\] <p>This yields the Stochastic Gradient Descent (SGD) update:</p> \\[ x_{k+1} = x_k - \\alpha_k g_k. \\] <p>SGD is the foundation for nearly all deep learning optimizers.</p>"},{"location":"convex/35_modern/#momentum-and-acceleration","title":"Momentum and Acceleration","text":"<p>SGD\u2019s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p> <p>  where \\(\\beta \\in [0,1)\\) controls inertia.</p> <p>Nesterov momentum adds a correction term anticipating the future position:</p> \\[ v_{k+1} = \\beta v_k + g(x_k - \\alpha \\beta v_k), \\quad x_{k+1} = x_k - \\alpha v_{k+1}. \\] <p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>"},{"location":"convex/35_modern/#adaptive-learning-rate-methods","title":"Adaptive Learning Rate Methods","text":"<p>Different parameters often require different step sizes. Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>"},{"location":"convex/35_modern/#adagrad","title":"AdaGrad","text":"<p>Keeps a cumulative sum of squared gradients:</p> <p>  and updates parameters as:</p> <p>  Good for sparse data, but the learning rate can shrink too quickly.</p>"},{"location":"convex/35_modern/#rmsprop","title":"RMSProp","text":"<p>A refinement of AdaGrad using exponential averaging:</p> \\[ E[g^2]_k = \\beta E[g^2]_{k-1} + (1-\\beta) g_k^2, \\] \\[ x_{k+1} = x_k - \\frac{\\alpha}{\\sqrt{E[g^2]_k + \\epsilon}} g_k. \\] <p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>"},{"location":"convex/35_modern/#adam-adaptive-moment-estimation","title":"Adam: Adaptive Moment Estimation","text":"<p>Adam combines momentum and adaptive scaling:</p> \\[ m_k = \\beta_1 m_{k-1} + (1-\\beta_1) g_k, \\quad v_k = \\beta_2 v_{k-1} + (1-\\beta_2) g_k^2, \\] \\[ \\hat{m}_k = \\frac{m_k}{1-\\beta_1^k}, \\quad \\hat{v}_k = \\frac{v_k}{1-\\beta_2^k}, \\] \\[ x_{k+1} = x_k - \\alpha \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\epsilon}. \\] <p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>"},{"location":"convex/35_modern/#variants-and-modern-extensions","title":"Variants and Modern Extensions","text":"Optimizer Key Idea Notes AdamW Decoupled weight decay from gradient update Better regularization RAdam Rectified Adam\u2014adaptive variance correction Improves stability early in training Lookahead Combines fast and slow weights Enhances robustness and convergence AdaBelief Uses prediction error instead of raw gradient variance More adaptive learning rates Lion Uses sign-based updates and momentum Efficient for large-scale training <p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>"},{"location":"convex/35_modern/#implicit-regularization-and-generalization","title":"Implicit Regularization and Generalization","text":"<p>Modern optimizers not only minimize loss\u2014they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p> <p>Empirical findings suggest:</p> <ul> <li>Large-batch training finds sharper minima (risk of overfitting).  </li> <li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li> <li>Adaptive optimizers may converge faster but generalize slightly worse.</li> </ul> <p>This trade-off drives ongoing research into optimizer design.</p>"},{"location":"convex/35_modern/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Learning Rate Most critical hyperparameter; use warm-up and decay schedules Batch Size Balances gradient noise and hardware efficiency Initialization Affects early dynamics, especially for Adam variants Gradient Clipping Prevents instability in exploding gradients Mixed Precision Use with adaptive optimizers for speed and memory savings"},{"location":"convex/35_modern/#comparative-behavior","title":"Comparative Behavior","text":"Method Adaptivity Speed Memory Typical Use SGD + Momentum Moderate Slow-medium Low General-purpose, good generalization RMSProp Adaptive per-parameter Medium-fast Medium Recurrent networks, nonstationary data Adam / AdamW Fully adaptive Fast High Deep networks, large-scale training RAdam / AdaBelief / Lion Advanced adaptivity Fast Medium Cutting-edge training tasks"},{"location":"convex/35_modern/#optimization-in-modern-deep-networks","title":"Optimization in Modern Deep Networks","text":"<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p> <ul> <li>Batch normalization modifies effective learning rates.  </li> <li>Skip connections ease gradient flow.  </li> <li>Large-scale distributed training relies on adaptive optimizers for stability.  </li> </ul> <p>Optimization is no longer an isolated procedure but part of the model\u2019s design philosophy.</p> <p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning\u2014adapting dynamically, balancing efficiency and generalization.</p>"},{"location":"convex/40_nonconvex/","title":"19. Beyond Convexity \u2013 Nonconvex and Global Optimization","text":""},{"location":"convex/40_nonconvex/#chapter-19-beyond-convexity-nonconvex-and-global-optimization","title":"Chapter 19: Beyond Convexity \u2013 Nonconvex and Global Optimization","text":"<p>Optimization extends far beyond the comfortable world of convexity. In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points. Convex optimization gives us strong guarantees; every local minimum is global, and algorithms converge predictably. But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>"},{"location":"convex/40_nonconvex/#the-landscape-of-nonconvex-optimization","title":"The Landscape of Nonconvex Optimization","text":"<p>A nonconvex function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) violates convexity; i.e., for some \\(x, y\\) and \\(\\theta \\in (0,1)\\),  Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p> <p>A typical nonconvex landscape looks like a mountainous terrain: smooth in some regions, rugged in others. An optimization algorithm\u2019s path depends strongly on initialization and stochastic effects.</p>"},{"location":"convex/40_nonconvex/#example-a-simple-nonconvex-function","title":"Example: A Simple Nonconvex Function","text":"<p>  This function has multiple stationary points:</p> <ul> <li>\\((0,0)\\) (a saddle),</li> <li>\\((1,1)\\) and \\((-1,-1)\\) (local minima),</li> <li>\\((1,-1)\\) and \\((-1,1)\\) (local maxima).</li> </ul> <p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>"},{"location":"convex/40_nonconvex/#local-vs-global-minima","title":"Local vs. Global Minima","text":"<p>A point \\(x^*\\) is a local minimum if:  </p> <p>A global minimum satisfies the stronger condition:  </p> <p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad and there may be exponentially many of them.</p>"},{"location":"convex/40_nonconvex/#classes-of-nonconvex-problems","title":"Classes of Nonconvex Problems","text":"<p>Nonconvex problems appear in several distinct forms:</p> Type Example Challenge Smooth nonconvex Neural network training Multiple minima, saddle points Nonsmooth nonconvex Sparse regularization, ReLU activations Undefined gradients Discrete / combinatorial Scheduling, routing, integer programs Exponential search space <p>Each category requires different algorithmic strategies: from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>"},{"location":"convex/40_nonconvex/#local-optimization-strategies","title":"Local Optimization Strategies","text":"<p>Even in nonconvex settings, local optimization remains useful when:</p> <ul> <li>The problem is nearly convex (e.g., locally convex around good minima),</li> <li>The initialization is close to a desired basin of attraction,</li> <li>Or the goal is approximate, not exact, optimality.</li> </ul>"},{"location":"convex/40_nonconvex/#gradient-descent-and-its-variants","title":"Gradient Descent and Its Variants","text":"<p>Gradient descent behaves well if \\(f\\) is smooth and Lipschitz-continuous:  However, convergence is only to a stationary point; not necessarily a minimum. Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>"},{"location":"convex/40_nonconvex/#global-optimization-strategies","title":"Global Optimization Strategies","text":"<p>To seek the global minimum, algorithms must explore the search space more broadly. Common strategies include:</p> <ol> <li> <p>Multiple Starts: Run local optimization from diverse random initial points and keep the best solution.</p> </li> <li> <p>Continuation and Homotopy Methods: Start from a smooth, convex approximation \\(f_\\lambda\\) of \\(f\\) and gradually transform it into the true objective as \\(\\lambda \\to 0\\).</p> </li> <li> <p>Stochastic Search and Simulated Annealing: Introduce randomness in updates to jump between basins.</p> </li> <li> <p>Population-Based Methods:  Maintain a swarm or population of candidate solutions evolving by selection and variation \u2014 leading to metaheuristic algorithms like Genetic Algorithm and Particle Swarm Optimization.</p> </li> </ol>"},{"location":"convex/40_nonconvex/#deterministic-vs-stochastic-global-methods","title":"Deterministic vs. Stochastic Global Methods","text":"Deterministic Methods Stochastic Methods Systematic exploration of space (branch &amp; bound, interval analysis) Randomized search (simulated annealing, evolutionary algorithms) Can provide certificates of global optimality Typically approximate but scalable High computational cost Naturally parallelizable <p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>"},{"location":"convex/40_nonconvex/#a-taxonomy-of-optimization-beyond-convexity","title":"A Taxonomy of Optimization Beyond Convexity","text":"Family Typical Algorithms When to Use Derivative-Free (Black-Box) Nelder\u2013Mead, CMA-ES, Bayesian Opt. When gradients unavailable Metaheuristic (Evolutionary) GA, PSO, DE, ACO Complex landscapes, combinatorial problems Modern Stochastic Gradient Adam, RMSProp, Lion Deep learning, large-scale models Combinatorial / Discrete Branch &amp; Bound, Tabu, SA Integer or graph-based problems Learning-Based Optimizers Meta-learning, Reinforcement methods Adaptive, data-driven optimization"},{"location":"convex/42_derivativefree/","title":"20. Derivative-Free and Black-Box Optimization","text":""},{"location":"convex/42_derivativefree/#chapter-20-derivative-free-and-black-box-optimization","title":"Chapter 20: Derivative-Free and Black-Box Optimization","text":"<p>In many optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p> <p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. </p>"},{"location":"convex/42_derivativefree/#motivation-and-challenges","title":"Motivation and Challenges","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be an objective function.  </p> <p>A derivative-free algorithm seeks to minimize \\(f(x)\\) using only evaluations of \\(f(x)\\), without access to \\(\\nabla f(x)\\) or \\(\\nabla^2 f(x)\\).</p> <p>Key challenges:</p> <ul> <li>No gradient information \u2192 difficult to infer descent directions.  </li> <li>Expensive evaluations \u2192 every call to \\(f(x)\\) might require a simulation or experiment.  </li> <li>Noise and stochasticity \u2192 evaluations may be corrupted by measurement or sampling error.  </li> <li>High-dimensionality \u2192 sampling-based methods scale poorly with \\(n\\).</li> </ul> <p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>"},{"location":"convex/42_derivativefree/#classification-of-derivative-free-methods","title":"Classification of Derivative-Free Methods","text":"Category Representative Algorithms Main Idea Direct Search Nelder\u2013Mead, Pattern Search, MADS Explore the space via geometric moves or meshes Model-Based BOBYQA, Trust-Region DFO Build local quadratic or surrogate models of \\(f\\) Evolutionary / Population-Based CMA-ES, Differential Evolution Evolve a population using stochastic operators Probabilistic / Bayesian Bayesian Optimization Use probabilistic surrogate models to guide exploration"},{"location":"convex/42_derivativefree/#direct-search-methods","title":"Direct Search Methods","text":"<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>"},{"location":"convex/42_derivativefree/#neldermead-simplex-method","title":"Nelder\u2013Mead Simplex Method","text":"<p>Perhaps the most famous derivative-free algorithm, Nelder\u2013Mead maintains a simplex: a polytope of \\(n+1\\) vertices in \\(\\mathbb{R}^n\\).</p> <p>At each iteration:</p> <ol> <li>Evaluate \\(f\\) at all simplex vertices.</li> <li>Reflect, expand, contract, or shrink the simplex depending on performance.</li> <li>Continue until simplex collapses near a minimum.</li> </ol> <p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>"},{"location":"convex/42_derivativefree/#pattern-search-methods","title":"Pattern Search Methods","text":"<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p> <p>Typical update rule:  </p> <p>where \\(d_i\\) is a direction from a finite set (e.g., coordinate axes). If a direction yields improvement, move there; otherwise, shrink \\(\\Delta_k\\).</p> <p>Mesh Adaptive Direct Searcs: MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>"},{"location":"convex/42_derivativefree/#model-based-methods","title":"Model-Based Methods","text":"<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>"},{"location":"convex/42_derivativefree/#trust-region-dfo","title":"Trust-Region DFO","text":"<p>A local model \\(m_k(x)\\) (often quadratic) is built to approximate \\(f\\) near the current iterate \\(x_k\\):  The next iterate solves a trust-region subproblem:  The trust region size \\(\\Delta_k\\) adapts based on how well \\(m_k\\) predicts true function values.</p> <p>Bound Optimization BY Quadratic Approximation: BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>"},{"location":"convex/42_derivativefree/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>Model the objective as a random function \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))\\) (Gaussian Process prior). After each evaluation, update the posterior mean and variance to quantify uncertainty.</p> <p>Use an acquisition function \\(a(x)\\) to select the next evaluation point:  balancing exploration (high uncertainty) and exploitation (low expected value).</p> <p>Common acquisition functions:</p> <ul> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Upper Confidence Bound (UCB)</li> </ul>"},{"location":"convex/42_derivativefree/#surrogate-models-beyond-gaussian-processes","title":"Surrogate Models Beyond Gaussian Processes","text":"<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs: - Tree-structured Parzen Estimators (TPE) - Random forests (SMAC) - Neural network surrogates (Bayesian neural networks)</p> <p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>"},{"location":"convex/42_derivativefree/#hybrid-and-adaptive-approaches","title":"Hybrid and Adaptive Approaches","text":"<p>Modern applications often combine derivative-free and gradient-based techniques:</p> <ul> <li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li> <li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li> <li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li> </ul> <p>Such hybridization reflects a pragmatic view: no single optimizer is best \u2014 adaptability matters most.</p>"},{"location":"convex/42_derivativefree/#practical-considerations","title":"Practical Considerations","text":"Aspect Guideline Function evaluations expensive Use Bayesian or model-based methods Noisy evaluations Use averaging, smoothing, or robust estimators High dimension (\\(n &gt; 50\\)) Prefer CMA-ES or evolutionary strategies Box constraints Methods like BOBYQA, DE, or PSO Parallel computation available Population-based methods excel <p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize anything we can evaluate. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p> <p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors; turning randomness into a powerful search strategy.</p>"},{"location":"convex/44_metaheuristic/","title":"21. Metaheuristic and Evolutionary Optimization","text":""},{"location":"convex/44_metaheuristic/#chapter-21-metaheuristic-and-evolutionary-algorithms","title":"Chapter 21: Metaheuristic and Evolutionary Algorithms","text":"<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions.  In these settings, metaheuristic algorithms\u2014inspired by nature, biology, and collective behavior\u2014provide robust and flexible alternatives. Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas. They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>"},{"location":"convex/44_metaheuristic/#principles-of-metaheuristic-optimization","title":"Principles of Metaheuristic Optimization","text":"<p>All metaheuristics share three key principles:</p> <ol> <li> <p>Population-Based Search: Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p> </li> <li> <p>Variation Operators:Create new solutions via mutation, recombination, or stochastic perturbations.</p> </li> <li> <p>Selection and Adaptation:Favor candidates with better objective values, guiding the search toward promising regions.</p> </li> </ol> <p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>"},{"location":"convex/44_metaheuristic/#genetic-algorithms","title":"Genetic Algorithms","text":"<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>"},{"location":"convex/44_metaheuristic/#representation","title":"Representation","text":"<p>A solution (individual) is represented as a chromosome, often a binary string, vector of reals, or permutation. Each position (gene) encodes part of the decision variable.</p>"},{"location":"convex/44_metaheuristic/#algorithm-outline","title":"Algorithm Outline","text":"<ol> <li>Initialize a population \\(\\{x_i\\}_{i=1}^N\\) randomly.  </li> <li>Evaluate fitness \\(f(x_i)\\) for all individuals.  </li> <li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li> <li> <p>Apply:</p> <ul> <li>Crossover: combine genetic material of two parents.  </li> <li>Mutation: randomly alter some genes to maintain diversity.  </li> </ul> </li> <li> <p>Form a new population and repeat until convergence.</p> </li> </ol>"},{"location":"convex/44_metaheuristic/#crossover-and-mutation-examples","title":"Crossover and Mutation Examples","text":"<ul> <li>Single-point crossover: exchange genes after a random index.  </li> <li>Gaussian mutation: add small noise to continuous parameters.  </li> </ul> Strengths Weaknesses Highly parallel, robust, domain-independent Requires many function evaluations Effective for combinatorial and discrete optimization Parameter tuning (mutation, crossover rates) is nontrivial"},{"location":"convex/44_metaheuristic/#differential-evolution","title":"Differential Evolution","text":"<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization. Mutation is performed using differences of population members:</p> <p>  where \\(r1, r2, r3\\) are random distinct indices and \\(F \\in [0,2]\\) controls mutation amplitude.</p> <p>Then crossover forms trial vectors:  and selection chooses between \\(x_i\\) and \\(u_i\\) based on objective value.</p>"},{"location":"convex/44_metaheuristic/#features","title":"Features","text":"<ul> <li>Self-adaptive exploration of the search space.</li> <li>Suitable for continuous, multimodal functions.</li> <li>Simple to implement, with few control parameters.</li> </ul>"},{"location":"convex/44_metaheuristic/#particle-swarm-optimization","title":"Particle Swarm Optimization","text":"<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space. Each particle \\(i\\) has position \\(x_i\\) and velocity \\(v_i\\), updated as:   where:</p> <ul> <li>\\(p_i\\) = personal best position of particle \\(i\\),</li> <li>\\(g\\) = best global position found by the swarm,</li> <li>\\(w\\), \\(c_1\\), \\(c_2\\) are weight and learning coefficients,</li> <li>\\(r_1\\), \\(r_2\\) are random numbers in \\([0,1]\\).</li> </ul> <p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>"},{"location":"convex/44_metaheuristic/#simulated-annealing","title":"Simulated Annealing","text":"<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy, a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual \u201ccooling\u201d in the search for a global minimum.</p>"},{"location":"convex/44_metaheuristic/#physical-analogy","title":"Physical Analogy","text":"<p>In thermodynamics, a system at temperature \\(T\\) has probability of occupying a state with energy \\(E\\) given by the Boltzmann distribution:</p> \\[ P(E) \\propto e^{-E / (kT)}. \\] <p>At high temperature, the system freely explores many states. As \\(T\\) decreases, it becomes increasingly likely to remain near states of minimal energy.</p> <p>Simulated Annealing maps this principle to optimization by treating:</p> <ul> <li>The objective function \\(f(x)\\) as the system\u2019s energy.</li> <li>The solution vector \\(x\\) as a configuration.</li> <li>The temperature \\(T\\) as a control parameter determining randomness.</li> </ul>"},{"location":"convex/44_metaheuristic/#algorithm-outline_1","title":"Algorithm Outline","text":"<ol> <li> <p>Initialization</p> <ul> <li>Choose an initial solution \\(x_0\\) and initial temperature \\(T_0\\).</li> <li>Set a cooling schedule \\(T_{k+1} = \\alpha T_k\\), with \\(\\alpha \\in (0,1)\\).</li> </ul> </li> <li> <p>Iteration</p> <ul> <li>Generate a candidate \\(x'\\) from \\(x_k\\) via a small random perturbation.</li> <li>Compute \\(\\Delta f = f(x') - f(x_k)\\).</li> <li>Accept or reject based on the Metropolis criterion:</li> </ul> <p> </p> </li> <li> <p>Cooling</p> <ul> <li> <p>Reduce the temperature gradually according to the schedule.</p> </li> <li> <p>Repeat until \\(T\\) becomes sufficiently small or the system stabilizes.</p> </li> </ul> </li> <li> <p>At high temperatures, SA accepts both better and worse moves \u2192 exploration.  </p> </li> <li> <p>At low temperatures, it becomes increasingly selective \u2192 exploitation.</p> </li> </ol> <p>This balance allows SA to escape local minima and approach the global optimum over time.</p> <p>The temperature schedule determines convergence quality:</p> Type Formula Behavior Exponential \\(T_{k+1} = \\alpha T_k\\) Simple, widely used Linear \\(T_{k+1} = T_0 - \\beta k\\) Faster cooling, less exploration Logarithmic \\(T_k = \\frac{T_0}{\\log(k + c)}\\) Theoretically convergent (slow) Adaptive Adjust based on recent acceptance rates Practical and self-tuning <p>A slower cooling schedule improves accuracy but increases computational cost.</p>"},{"location":"convex/44_metaheuristic/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>Every metaheuristic must balance:</p> <ul> <li>Exploration: sampling diverse regions to escape local minima.  </li> <li>Exploitation: refining known good solutions to reach local optima.</li> </ul> High Exploration High Exploitation GA with strong mutation PSO with low inertia DE with high \\(F\\) ACO with low evaporation rate Random restarts Local refinement <p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>"},{"location":"convex/44_metaheuristic/#performance-and-practical-tips","title":"Performance and Practical Tips","text":"Aspect Guideline Initialization Use wide, random distributions to promote diversity Parameter Tuning Use adaptive schedules (e.g., cooling, inertia decay) Population Size Larger for global search, smaller for fine-tuning Parallelism Evaluate populations concurrently for efficiency Stopping Criteria Use both iteration limits and stagnation detection <p>Metaheuristics are heuristic by design \u2014 they do not guarantee global optimality, but offer practical success across many fields. Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p>"},{"location":"convex/48_advanced_combinatorial/","title":"22. Advanced Topics in Combinatorial Optimization","text":""},{"location":"convex/48_advanced_combinatorial/#chapter-22-advanced-topics-in-combinatorial-optimization","title":"Chapter 22: Advanced Topics in Combinatorial Optimization","text":"<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics. Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>"},{"location":"convex/48_advanced_combinatorial/#nature-of-combinatorial-problems","title":"Nature of Combinatorial Problems","text":"<p>A combinatorial optimization problem can be expressed as:</p> \\[ \\min_{x \\in \\mathcal{F}} f(x), \\] <p>where \\(\\mathcal{F}\\) is a finite or countable set of feasible solutions, often exponentially large in size.</p> <p>Example forms include:</p> <ul> <li>Binary decisions: \\(x_i \\in \\{0,1\\}\\)</li> <li>Integer constraints: \\(x_i \\in \\mathbb{Z}\\)</li> <li>Permutations: ordering or ranking elements</li> </ul> <p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>"},{"location":"convex/48_advanced_combinatorial/#graph-theoretic-foundations","title":"Graph-Theoretic Foundations","text":"<p>Many combinatorial problems are naturally represented as graphs \\(G = (V, E)\\).</p>"},{"location":"convex/48_advanced_combinatorial/#shortest-path-problem","title":"Shortest Path Problem","text":"<p>Given edge weights \\(w_{ij}\\), find a path from \\(s\\) to \\(t\\) minimizing total weight:  Efficiently solvable by Dijkstra\u2019s or Bellman\u2013Ford algorithms.</p>"},{"location":"convex/48_advanced_combinatorial/#minimum-spanning-tree-mst","title":"Minimum Spanning Tree (MST)","text":"<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal\u2019s or Prim\u2019s algorithm in \\(O(E\\log V)\\) time.</p>"},{"location":"convex/48_advanced_combinatorial/#maximum-flow-minimum-cut","title":"Maximum Flow / Minimum Cut","text":"<p>Determine how much \u201cflow\u201d can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>"},{"location":"convex/48_advanced_combinatorial/#integer-linear-programming-ilp","title":"Integer Linear Programming (ILP)","text":"<p>An integer program seeks:  </p> <p>It generalizes many classical problems:</p> <ul> <li>Knapsack  </li> <li>Assignment  </li> <li>Scheduling  </li> <li>Facility location</li> </ul> <p>Relaxing \\(x \\in \\mathbb{Z}^n\\) to \\(x \\in \\mathbb{R}^n\\) yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>"},{"location":"convex/48_advanced_combinatorial/#relaxation-and-rounding","title":"Relaxation and Rounding","text":"<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>"},{"location":"convex/48_advanced_combinatorial/#lp-relaxation","title":"LP Relaxation","text":"<p>For binary variables \\(x_i \\in \\{0,1\\}\\), relax to \\(0 \\le x_i \\le 1\\) and solve via simplex or interior-point methods.</p>"},{"location":"convex/48_advanced_combinatorial/#semidefinite-relaxation","title":"Semidefinite Relaxation","text":"<p>For quadratic binary problems, lift to a positive semidefinite matrix \\(X = xx^\\top\\):  Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>"},{"location":"convex/48_advanced_combinatorial/#randomized-rounding","title":"Randomized Rounding","text":"<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>"},{"location":"convex/48_advanced_combinatorial/#branch-and-bound-and-search-trees","title":"Branch-and-Bound and Search Trees","text":"<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>"},{"location":"convex/48_advanced_combinatorial/#basic-principle","title":"Basic Principle","text":"<ol> <li>Partition the feasible set into subsets (branching).  </li> <li>Compute upper/lower bounds for each subset.  </li> <li>Prune branches that cannot contain the optimum.  </li> </ol> <p>The algorithm systematically explores a search tree, guided by bounds.</p>"},{"location":"convex/48_advanced_combinatorial/#bounding-via-relaxations","title":"Bounding via Relaxations","text":"<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>"},{"location":"convex/48_advanced_combinatorial/#dynamic-programming","title":"Dynamic Programming","text":"<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p> \\[ \\text{OPT}(S) = \\min_{x \\in S} \\{ c(x) + \\text{OPT}(S') \\}. \\] <p>It is exact but can suffer from exponential growth (\u201ccurse of dimensionality\u201d).</p> <p>Applications:</p> <ul> <li>Shortest paths</li> <li>Sequence alignment</li> <li>Knapsack</li> <li>Resource allocation</li> </ul> <p>DP offers exact solutions when structure allows sequential decomposition.</p>"},{"location":"convex/48_advanced_combinatorial/#heuristics-and-metaheuristics-for-combinatorial-problems","title":"Heuristics and Metaheuristics for Combinatorial Problems","text":"<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>"},{"location":"convex/48_advanced_combinatorial/#greedy-heuristics","title":"Greedy Heuristics","text":"<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal\u2019s MST). Fast but not always globally optimal.</p>"},{"location":"convex/48_advanced_combinatorial/#local-search-and-hill-climbing","title":"Local Search and Hill Climbing","text":"<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>"},{"location":"convex/48_advanced_combinatorial/#metaheuristic-extensions","title":"Metaheuristic Extensions","text":"<ul> <li>Simulated Annealing: controlled random acceptance of worse moves.  </li> <li>Tabu Search: memory-based diversification.  </li> <li>Ant Colony Optimization: probabilistic path construction.  </li> <li>Genetic Algorithms and PSO: population-based evolution.  </li> </ul> <p>These approaches generalize to discrete structures with minimal problem-specific design.</p>"},{"location":"convex/48_advanced_combinatorial/#approximation-algorithms","title":"Approximation Algorithms","text":"<p>Some combinatorial problems are provably intractable but allow approximation guarantees:  where \\(\\alpha \\ge 1\\) is the approximation ratio.</p> <p>Examples:</p> <ul> <li>Greedy Set Cover: \\(\\alpha = \\ln n + 1\\) </li> <li>Christofides\u2019 Algorithm for TSP: \\(\\alpha = 1.5\\) </li> <li>MAX-CUT SDP Relaxation: \\(\\alpha \\approx 0.878\\)</li> </ul> <p>Approximation theory blends combinatorics with convex relaxation insights.</p> <p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p>"},{"location":"convex/tutorials/1_lp_transport/","title":"I - Transportation Optimization: A Linear Programming Case Study.","text":"In\u00a0[2]: Copied! <pre>#!pip uninstall -y pandas numpy matplotlib scipy\n#!pip install --no-cache-dir numpy pandas  matplotlib scipy\n</pre> #!pip uninstall -y pandas numpy matplotlib scipy #!pip install --no-cache-dir numpy pandas  matplotlib scipy In\u00a0[3]: Copied! <pre>import re\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional, List\nimport numpy as np\nimport pandas as pd\nimport cvxpy as cp\nfrom scipy import sparse\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import PowerNorm\n</pre> import re from pathlib import Path from typing import Dict, Tuple, Optional, List import numpy as np import pandas as pd import cvxpy as cp from scipy import sparse import matplotlib.pyplot as plt from matplotlib.colors import PowerNorm In\u00a0[4]: Copied! <pre>rng = np.random.default_rng(42)\nnp.random.seed(42)\n</pre> rng = np.random.default_rng(42) np.random.seed(42) In\u00a0[5]: Copied! <pre>#!pip install cvxpy[glpk]\n#!pip install ecos\n</pre> #!pip install cvxpy[glpk] #!pip install ecos In\u00a0[6]: Copied! <pre># Data Cleaning Utilities\n\ndef canon(s: str) -&gt; str:\n    \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"\n    s = s.strip().lower()\n    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s\n\n\ndef clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = df.copy()\n    df.columns = [canon(c) for c in df.columns]\n    # Normalize common missing value tokens\n    df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})\n    return df\n\n\ndef load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:\n    print(f\"Loading workbook: {xlsx_path.resolve()}\")\n    xls = pd.ExcelFile(xlsx_path)\n    sheets = {}\n    for sheet_name in xls.sheet_names:\n        df = pd.read_excel(xls, sheet_name=sheet_name)\n        df = clean_df(df)\n        sheets[canon(sheet_name)] = df\n    print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")\n    return sheets\n</pre> # Data Cleaning Utilities  def canon(s: str) -&gt; str:     \"\"\"Canonical column name: lowercase, strip, replace non-alnum with underscore.\"\"\"     s = s.strip().lower()     s = re.sub(r\"[^a-z0-9]+\", \"_\", s)     s = re.sub(r\"_+\", \"_\", s).strip(\"_\")     return s   def clean_df(df: pd.DataFrame) -&gt; pd.DataFrame:     df = df.copy()     df.columns = [canon(c) for c in df.columns]     # Normalize common missing value tokens     df = df.replace({\"\": np.nan, \"NA\": np.nan, \"N/A\": np.nan, \"null\": np.nan})     return df   def load_excel_sheets(xlsx_path: Path) -&gt; Dict[str, pd.DataFrame]:     print(f\"Loading workbook: {xlsx_path.resolve()}\")     xls = pd.ExcelFile(xlsx_path)     sheets = {}     for sheet_name in xls.sheet_names:         df = pd.read_excel(xls, sheet_name=sheet_name)         df = clean_df(df)         sheets[canon(sheet_name)] = df     print(f\"Loaded {len(sheets)} sheets: {list(sheets.keys())}\")     return sheets In\u00a0[7]: Copied! <pre>xlsx_path = Path(\"data/SupplyChainLogisticsProblems.xlsx\")\nsheets = load_excel_sheets(xlsx_path)\n</pre> xlsx_path = Path(\"data/SupplyChainLogisticsProblems.xlsx\") sheets = load_excel_sheets(xlsx_path) <pre>Loading workbook: C:\\Users\\salmank\\Documents\\convex_optimization\\docs\\convex\\tutorials\\data\\SupplyChainLogisticsProblems.xlsx\nLoaded 7 sheets: ['orderlist', 'freightrates', 'whcosts', 'whcapacities', 'productsperplant', 'vmicustomers', 'plantports']\n</pre> In\u00a0[8]: Copied! <pre># Demand\norders = sheets['orderlist'][['order_id', 'weight']]\norders.columns = [\"order_id\", \"demand\"]\norders = orders[orders[\"demand\"] &gt; 0].copy()\norders[\"order_id\"] = orders[\"order_id\"].astype(str)\n\n# Retain top 1000 orders by demand\nk = 1000\norders = (\n    orders\n    .sort_values(\"demand\", ascending=False)\n    .head(k)\n    .reset_index(drop=True)\n)\n\nprint(f\"Orders retained: {len(orders)}\")\norders.head()\n\n\n# Total Demand capacity\ntotal_demand = orders['demand'].sum()\nprint(f\"Total orders: {len(orders)}\")\nprint(f\"Total demand capacity: {total_demand}\")\n\n#orders.head()\n</pre> # Demand orders = sheets['orderlist'][['order_id', 'weight']] orders.columns = [\"order_id\", \"demand\"] orders = orders[orders[\"demand\"] &gt; 0].copy() orders[\"order_id\"] = orders[\"order_id\"].astype(str)  # Retain top 1000 orders by demand k = 1000 orders = (     orders     .sort_values(\"demand\", ascending=False)     .head(k)     .reset_index(drop=True) )  print(f\"Orders retained: {len(orders)}\") orders.head()   # Total Demand capacity total_demand = orders['demand'].sum() print(f\"Total orders: {len(orders)}\") print(f\"Total demand capacity: {total_demand}\")  #orders.head()  <pre>Orders retained: 1000\nTotal orders: 1000\nTotal demand capacity: 129698.9734391597\n</pre> In\u00a0[9]: Copied! <pre># Plant capacities\n\nplants = sheets['whcapacities']\nplants.columns = [\"plant_id\", \"supply_cap\"]\nplants[\"plant_id\"] = plants[\"plant_id\"].astype(str)\nplants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\")\nplants = plants.dropna(subset=[\"supply_cap\"])\nplants[\"supply_cap\"] = plants[\"supply_cap\"].astype(float) * 30\n\n# Retain top 10 plants by supply capacity\nplants = (\n    plants\n    .sort_values(\"supply_cap\", ascending=False)\n    .head(10)\n    .reset_index(drop=True)\n)\n\n# Total Supply capacity\ntotal_supply = plants[\"supply_cap\"].sum()\nprint(f\"Total plants: {len(plants)}\")\nprint(f\"Total supply capacity: {total_supply}\")\n\n#plants.head()\n</pre> # Plant capacities  plants = sheets['whcapacities'] plants.columns = [\"plant_id\", \"supply_cap\"] plants[\"plant_id\"] = plants[\"plant_id\"].astype(str) plants[\"supply_cap\"] = pd.to_numeric(plants[\"supply_cap\"], errors=\"coerce\") plants = plants.dropna(subset=[\"supply_cap\"]) plants[\"supply_cap\"] = plants[\"supply_cap\"].astype(float) * 30  # Retain top 10 plants by supply capacity plants = (     plants     .sort_values(\"supply_cap\", ascending=False)     .head(10)     .reset_index(drop=True) )  # Total Supply capacity total_supply = plants[\"supply_cap\"].sum() print(f\"Total plants: {len(plants)}\") print(f\"Total supply capacity: {total_supply}\")  #plants.head() <pre>Total plants: 10\nTotal supply capacity: 159720.0\n</pre> In\u00a0[10]: Copied! <pre># Check Supply is enough to meet Demand\nif total_supply &lt; total_demand:\n    raise ValueError(f\"Total supply ({total_supply}) is less than total demand ({total_demand}). Problem is infeasible.\")\n</pre> # Check Supply is enough to meet Demand if total_supply &lt; total_demand:     raise ValueError(f\"Total supply ({total_supply}) is less than total demand ({total_demand}). Problem is infeasible.\") In\u00a0[11]: Copied! <pre># ----------------------------\n# Build lane table (arc list) and unit costs\n# ----------------------------\n# NOTE: In this notebook we generate a mostly-dense plant\u00d7order lane set\n#       with dummy unit costs, but randomly skip ~x% of connections\n#       to mimic missing/infeasible shipping lanes in real data.\n\nrng = np.random.default_rng(7)\n\nplant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]}\nplant_ids = plants[\"plant_id\"].tolist()\n\nskip_prob = 0.5  # probability of skipping a plant-order connection\n\nlanes = []\nfor _, o in orders.iterrows():\n    oid = o[\"order_id\"]\n    dem = float(o[\"demand\"])\n    order_jitter = 0.75 + 0.25 * rng.random()\n\n    for pid in plant_ids:\n        # randomly skip some connections\n        if rng.random() &lt; skip_prob:\n            continue\n\n        unit_cost = 1 * plant_factor[pid] * order_jitter\n        lanes.append((pid, oid, unit_cost))\n\nlanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"])\n\nprint(f\"Total lanes (plant-order pairs): {len(lanes)}\")\nprint(f\"Total lanes (plant-order pairs) if we had full connectivity: {len(plants) * len(orders)}\")\n</pre> # ---------------------------- # Build lane table (arc list) and unit costs # ---------------------------- # NOTE: In this notebook we generate a mostly-dense plant\u00d7order lane set #       with dummy unit costs, but randomly skip ~x% of connections #       to mimic missing/infeasible shipping lanes in real data.  rng = np.random.default_rng(7)  plant_factor = {pid: 0.8 + 0.4 * rng.random() for pid in plants[\"plant_id\"]} plant_ids = plants[\"plant_id\"].tolist()  skip_prob = 0.5  # probability of skipping a plant-order connection  lanes = [] for _, o in orders.iterrows():     oid = o[\"order_id\"]     dem = float(o[\"demand\"])     order_jitter = 0.75 + 0.25 * rng.random()      for pid in plant_ids:         # randomly skip some connections         if rng.random() &lt; skip_prob:             continue          unit_cost = 1 * plant_factor[pid] * order_jitter         lanes.append((pid, oid, unit_cost))  lanes = pd.DataFrame(lanes, columns=[\"plant_id\", \"order_id\", \"unit_cost\"])  print(f\"Total lanes (plant-order pairs): {len(lanes)}\") print(f\"Total lanes (plant-order pairs) if we had full connectivity: {len(plants) * len(orders)}\") <pre>Total lanes (plant-order pairs): 5048\nTotal lanes (plant-order pairs) if we had full connectivity: 10000\n</pre> In\u00a0[12]: Copied! <pre># Drop plants / orders with no lanes\n\nprint(\n    f\"Before cleanup: {len(plants)} plants, \"\n    f\"{len(orders)} orders, \"\n    f\"{len(lanes)} lanes\"\n)\n\nplants_with_lane = set(lanes[\"plant_id\"].unique())\norders_with_lane = set(lanes[\"order_id\"].unique())\n\n# Identify disconnected nodes\ndrop_plants = set(plants[\"plant_id\"]) - plants_with_lane\ndrop_orders = set(orders[\"order_id\"]) - orders_with_lane\n\nif drop_plants:\n    print(f\"Dropping {len(drop_plants)} plant(s) with no lanes: {sorted(drop_plants)}\")\n    plants_with_lanes = plants[~plants[\"plant_id\"].isin(drop_plants)].reset_index(drop=True)\nelse:\n    plants_with_lanes = plants.copy()\n\nif drop_orders:\n    print(f\"Dropping {len(drop_orders)} order(s) with no lanes: {sorted(drop_orders)}\")\n    orders_with_lanes = orders[~orders[\"order_id\"].isin(drop_orders)].reset_index(drop=True)\nelse:\n    orders_with_lanes = orders.copy()\n\n# Keep lanes consistent with remaining plants and orders\nlanes = lanes[\n    lanes[\"plant_id\"].isin(plants_with_lanes[\"plant_id\"]) &amp;\n    lanes[\"order_id\"].isin(orders_with_lanes[\"order_id\"])\n].reset_index(drop=True)\n\nprint(\n    f\"After cleanup: {len(plants_with_lanes)} plants, \"\n    f\"{len(orders_with_lanes)} orders, \"\n    f\"{len(lanes)} lanes\"\n)\n</pre> # Drop plants / orders with no lanes  print(     f\"Before cleanup: {len(plants)} plants, \"     f\"{len(orders)} orders, \"     f\"{len(lanes)} lanes\" )  plants_with_lane = set(lanes[\"plant_id\"].unique()) orders_with_lane = set(lanes[\"order_id\"].unique())  # Identify disconnected nodes drop_plants = set(plants[\"plant_id\"]) - plants_with_lane drop_orders = set(orders[\"order_id\"]) - orders_with_lane  if drop_plants:     print(f\"Dropping {len(drop_plants)} plant(s) with no lanes: {sorted(drop_plants)}\")     plants_with_lanes = plants[~plants[\"plant_id\"].isin(drop_plants)].reset_index(drop=True) else:     plants_with_lanes = plants.copy()  if drop_orders:     print(f\"Dropping {len(drop_orders)} order(s) with no lanes: {sorted(drop_orders)}\")     orders_with_lanes = orders[~orders[\"order_id\"].isin(drop_orders)].reset_index(drop=True) else:     orders_with_lanes = orders.copy()  # Keep lanes consistent with remaining plants and orders lanes = lanes[     lanes[\"plant_id\"].isin(plants_with_lanes[\"plant_id\"]) &amp;     lanes[\"order_id\"].isin(orders_with_lanes[\"order_id\"]) ].reset_index(drop=True)  print(     f\"After cleanup: {len(plants_with_lanes)} plants, \"     f\"{len(orders_with_lanes)} orders, \"     f\"{len(lanes)} lanes\" ) <pre>Before cleanup: 10 plants, 1000 orders, 5048 lanes\nDropping 1 order(s) with no lanes: ['1447183486.7']\nAfter cleanup: 10 plants, 999 orders, 5048 lanes\n</pre> In\u00a0[13]: Copied! <pre># ----------------------------\n# Data integrity checks\n# ----------------------------\n# Ensure IDs are unique before we rely on index-based alignment later.\n\nif not plants_with_lanes[\"plant_id\"].is_unique:\n    raise ValueError(\"plants.plant_id is not unique. Deduplicate or aggregate supplies first.\")\nif not orders_with_lanes[\"order_id\"].is_unique:\n    raise ValueError(\"orders.order_id is not unique. Deduplicate or aggregate demands first.\")\n</pre> # ---------------------------- # Data integrity checks # ---------------------------- # Ensure IDs are unique before we rely on index-based alignment later.  if not plants_with_lanes[\"plant_id\"].is_unique:     raise ValueError(\"plants.plant_id is not unique. Deduplicate or aggregate supplies first.\") if not orders_with_lanes[\"order_id\"].is_unique:     raise ValueError(\"orders.order_id is not unique. Deduplicate or aggregate demands first.\") In\u00a0[14]: Copied! <pre># ----------------------------\n# Clean / validate lanes\n# ----------------------------\n# 1) Drop lanes whose plant_id/order_id do not exist in the node tables.\n# 2) If duplicate (plant_id, order_id) lanes exist, aggregate them (here: keep the minimum unit cost).\n\n# Keep only lanes that connect to known plants/orders (avoid silent mismatches)\nlanes = lanes.merge(plants[[\"plant_id\"]], on=\"plant_id\", how=\"inner\")\nlanes = lanes.merge(orders[[\"order_id\"]], on=\"order_id\", how=\"inner\")\n\nif lanes.empty:\n    raise ValueError(\"No valid lanes after matching plant_id/order_id against plants/orders.\")\n\nlanes = (lanes.groupby([\"plant_id\", \"order_id\"], as_index=False).agg({'unit_cost': \"min\"}))\n\n#lanes.head()\n</pre> # ---------------------------- # Clean / validate lanes # ---------------------------- # 1) Drop lanes whose plant_id/order_id do not exist in the node tables. # 2) If duplicate (plant_id, order_id) lanes exist, aggregate them (here: keep the minimum unit cost).  # Keep only lanes that connect to known plants/orders (avoid silent mismatches) lanes = lanes.merge(plants[[\"plant_id\"]], on=\"plant_id\", how=\"inner\") lanes = lanes.merge(orders[[\"order_id\"]], on=\"order_id\", how=\"inner\")  if lanes.empty:     raise ValueError(\"No valid lanes after matching plant_id/order_id against plants/orders.\")  lanes = (lanes.groupby([\"plant_id\", \"order_id\"], as_index=False).agg({'unit_cost': \"min\"}))  #lanes.head() In\u00a0[15]: Copied! <pre># ----------------------------\n# Define a *single source of truth* for indices\n# ----------------------------\nplant_ids = plants_with_lanes[\"plant_id\"].to_numpy()\norder_ids = orders_with_lanes[\"order_id\"].to_numpy()\n\nplant_to_i = {pid: i for i, pid in enumerate(plant_ids)}\norder_to_j = {oid: j for j, oid in enumerate(order_ids)}\n\n# Arc index = row index of lanes after reset\nlanes = lanes.reset_index(drop=True)\nnA = len(lanes)\nnP = len(plant_ids)\nnO = len(order_ids)\n\n # Build arc endpoint index arrays\narc_i = lanes[\"plant_id\"].map(plant_to_i).to_numpy()\narc_j = lanes[\"order_id\"].map(order_to_j).to_numpy()\n\n\narc_i = arc_i.astype(int)\narc_j = arc_j.astype(int)\n</pre> # ---------------------------- # Define a *single source of truth* for indices # ---------------------------- plant_ids = plants_with_lanes[\"plant_id\"].to_numpy() order_ids = orders_with_lanes[\"order_id\"].to_numpy()  plant_to_i = {pid: i for i, pid in enumerate(plant_ids)} order_to_j = {oid: j for j, oid in enumerate(order_ids)}  # Arc index = row index of lanes after reset lanes = lanes.reset_index(drop=True) nA = len(lanes) nP = len(plant_ids) nO = len(order_ids)   # Build arc endpoint index arrays arc_i = lanes[\"plant_id\"].map(plant_to_i).to_numpy() arc_j = lanes[\"order_id\"].map(order_to_j).to_numpy()   arc_i = arc_i.astype(int) arc_j = arc_j.astype(int)  In\u00a0[16]: Copied! <pre># ----------------------------\n# Build aligned vectors: cost c, supply, demand\n# ----------------------------\n# IMPORTANT: `c[a]` must correspond to the same arc as decision variable `x[a]`.\n# Here, arc index a = row index of `lanes` after reset_index.\n\n# Cost vector corresponds to arc order (lanes row order).\nc = lanes['unit_cost'].to_numpy(dtype=float)\n\nsupply = plants.set_index(\"plant_id\").loc[plant_ids, 'supply_cap'].to_numpy(dtype=float)\ndemand = orders.set_index(\"order_id\").loc[order_ids, 'demand'].to_numpy(dtype=float)\n\n# Ensure every order has at least one incoming lane\nincoming_counts = np.bincount(arc_j, minlength=nO)\nmissing_orders = order_ids[incoming_counts == 0]\nif len(missing_orders) &gt; 0:\n    raise ValueError(f\"Orders with no incoming lanes: {missing_orders[:10]}{'...' if len(missing_orders) &gt; 10 else ''}\")\n</pre> # ---------------------------- # Build aligned vectors: cost c, supply, demand # ---------------------------- # IMPORTANT: `c[a]` must correspond to the same arc as decision variable `x[a]`. # Here, arc index a = row index of `lanes` after reset_index.  # Cost vector corresponds to arc order (lanes row order). c = lanes['unit_cost'].to_numpy(dtype=float)  supply = plants.set_index(\"plant_id\").loc[plant_ids, 'supply_cap'].to_numpy(dtype=float) demand = orders.set_index(\"order_id\").loc[order_ids, 'demand'].to_numpy(dtype=float)  # Ensure every order has at least one incoming lane incoming_counts = np.bincount(arc_j, minlength=nO) missing_orders = order_ids[incoming_counts == 0] if len(missing_orders) &gt; 0:     raise ValueError(f\"Orders with no incoming lanes: {missing_orders[:10]}{'...' if len(missing_orders) &gt; 10 else ''}\") In\u00a0[17]: Copied! <pre># ----------------------------\n# Visuals: supply capacity by plant &amp; demand by order\n# ----------------------------\n \nfig, ax = plt.subplots(figsize=(10, 4))\nax.bar(np.arange(len(supply)), supply)\nax.set_title(\"Supply capacity by plant (index order)\")\nax.set_xlabel(\"Plant index i\")\nax.set_ylabel(\"Supply capacity\")\nplt.show()\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.bar(np.arange(len(demand)), demand)\nax.set_title(\"Demand by order (index order)\")\nax.set_xlabel(\"Order index j\")\nax.set_ylabel(\"Demand\")\nplt.show()\n</pre> # ---------------------------- # Visuals: supply capacity by plant &amp; demand by order # ----------------------------   fig, ax = plt.subplots(figsize=(10, 4)) ax.bar(np.arange(len(supply)), supply) ax.set_title(\"Supply capacity by plant (index order)\") ax.set_xlabel(\"Plant index i\") ax.set_ylabel(\"Supply capacity\") plt.show()  fig, ax = plt.subplots(figsize=(10, 4)) ax.bar(np.arange(len(demand)), demand) ax.set_title(\"Demand by order (index order)\") ax.set_xlabel(\"Order index j\") ax.set_ylabel(\"Demand\") plt.show()  In\u00a0[18]: Copied! <pre># ----------------------------\n# Build sparse incidence matrices\n# ----------------------------\n# A_order[j, a] = 1 if arc a goes into order j\nA_order = sparse.coo_matrix(\n    (np.ones(nA), (arc_j, np.arange(nA))),\n    shape=(nO, nA)\n).tocsr()\n\n# A_plant[i, a] = 1 if arc a goes out of plant i\nA_plant = sparse.coo_matrix(\n    (np.ones(nA), (arc_i, np.arange(nA))),\n    shape=(nP, nA)\n).tocsr()\n\nA_order\n</pre> # ---------------------------- # Build sparse incidence matrices # ---------------------------- # A_order[j, a] = 1 if arc a goes into order j A_order = sparse.coo_matrix(     (np.ones(nA), (arc_j, np.arange(nA))),     shape=(nO, nA) ).tocsr()  # A_plant[i, a] = 1 if arc a goes out of plant i A_plant = sparse.coo_matrix(     (np.ones(nA), (arc_i, np.arange(nA))),     shape=(nP, nA) ).tocsr()  A_order Out[18]: <pre>&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 5048 stored elements and shape (999, 5048)&gt;</pre> In\u00a0[19]: Copied! <pre># ----------------------------\n# Solve the LP in CVXPY\n# ----------------------------\n# Decision variable:\n#   x[a] = shipment quantity on lane (arc) a\nx = cp.Variable(nA, nonneg=True)\n\n# Constraints:\n#   - every order demand must be met exactly\n#   - plant shipments cannot exceed supply capacity\nconstraints = [\n    A_order @ x == demand,\n    A_plant @ x &lt;= supply\n]\n\n# Objective: min total shipping cost\nobjective = cp.Minimize(c @ x)\nprob = cp.Problem(objective, constraints)\n\n# Choose a good LP solver explicitly (fallback if unavailable)\npreferred = \"HIGHS\"\ninstalled = set(cp.installed_solvers())\nsolver = preferred if preferred in installed else (\"GLPK\" if \"GLPK\" in installed else None)\nif solver is None:\n    raise RuntimeError(f\"No suitable LP solver found. Installed solvers: {sorted(installed)}\")\n\n_ = prob.solve(solver=solver, verbose=False)\n\nprint(\"Status:\", prob.status)\nprint(\"Objective value:\", prob.value)\n\nif prob.status not in (\"optimal\", \"optimal_inaccurate\"):\n    raise RuntimeError(f\"Solve failed: status={prob.status}\")\n\n# Attach the solution back to the lane table (arc ordering!)\nlanes_sol = lanes.copy()\nlanes_sol[\"x\"] = np.asarray(x.value).reshape(-1)\n\n# ----------------------------\n# Sanity checks (residuals)\n# ----------------------------\norder_resid = (A_order @ lanes_sol[\"x\"].to_numpy()) - demand\nplant_resid = (A_plant @ lanes_sol[\"x\"].to_numpy()) - supply  # should be &lt;= 0\n\ndiagnostics = {\n    \"status\": prob.status,\n    \"objective\": float(prob.value),\n    \"max_abs_order_residual\": float(np.max(np.abs(order_resid))) if len(order_resid) else 0.0,\n    \"max_plant_violation\": float(np.max(plant_resid)) if len(plant_resid) else 0.0,\n    \"solver_used\": solver,\n    \"n_plants\": int(nP),\n    \"n_orders\": int(nO),\n    \"n_lanes\": int(nA),\n}\n\ndiagnostics\n</pre> # ---------------------------- # Solve the LP in CVXPY # ---------------------------- # Decision variable: #   x[a] = shipment quantity on lane (arc) a x = cp.Variable(nA, nonneg=True)  # Constraints: #   - every order demand must be met exactly #   - plant shipments cannot exceed supply capacity constraints = [     A_order @ x == demand,     A_plant @ x &lt;= supply ]  # Objective: min total shipping cost objective = cp.Minimize(c @ x) prob = cp.Problem(objective, constraints)  # Choose a good LP solver explicitly (fallback if unavailable) preferred = \"HIGHS\" installed = set(cp.installed_solvers()) solver = preferred if preferred in installed else (\"GLPK\" if \"GLPK\" in installed else None) if solver is None:     raise RuntimeError(f\"No suitable LP solver found. Installed solvers: {sorted(installed)}\")  _ = prob.solve(solver=solver, verbose=False)  print(\"Status:\", prob.status) print(\"Objective value:\", prob.value)  if prob.status not in (\"optimal\", \"optimal_inaccurate\"):     raise RuntimeError(f\"Solve failed: status={prob.status}\")  # Attach the solution back to the lane table (arc ordering!) lanes_sol = lanes.copy() lanes_sol[\"x\"] = np.asarray(x.value).reshape(-1)  # ---------------------------- # Sanity checks (residuals) # ---------------------------- order_resid = (A_order @ lanes_sol[\"x\"].to_numpy()) - demand plant_resid = (A_plant @ lanes_sol[\"x\"].to_numpy()) - supply  # should be &lt;= 0  diagnostics = {     \"status\": prob.status,     \"objective\": float(prob.value),     \"max_abs_order_residual\": float(np.max(np.abs(order_resid))) if len(order_resid) else 0.0,     \"max_plant_violation\": float(np.max(plant_resid)) if len(plant_resid) else 0.0,     \"solver_used\": solver,     \"n_plants\": int(nP),     \"n_orders\": int(nO),     \"n_lanes\": int(nA), }  diagnostics  <pre>Status: optimal\nObjective value: 113265.87945592123\n</pre> Out[19]: <pre>{'status': 'optimal',\n 'objective': 113265.87945592123,\n 'max_abs_order_residual': 2.808064891723916e-11,\n 'max_plant_violation': 2.546585164964199e-11,\n 'solver_used': 'GLPK',\n 'n_plants': 10,\n 'n_orders': 999,\n 'n_lanes': 5048}</pre> In\u00a0[20]: Copied! <pre># ----------------------------\n# Heatmap of shipped amounts (plants x orders)\n# ----------------------------\n# lanes_sol must contain: plant_id, order_id, x\n# plant_ids, order_ids are the index orders used elsewhere (strings)\n\n# Build shipment matrix: rows=plants, cols=orders\nship = (lanes_sol\n        .pivot_table(index=\"plant_id\", columns=\"order_id\", values=\"x\", aggfunc=\"sum\", fill_value=0.0)\n       )\n\n# Ensure full row/col order (so it matches your indexing)\nship = ship.reindex(index=plant_ids, columns=order_ids, fill_value=0.0)\norder_totals = ship.sum(axis=0).sort_values(ascending=False)\nship = ship[order_totals.index]\nplant_totals = ship.sum(axis=1).sort_values(ascending=False)\n#ship = ship.loc[plant_totals.index]\n\nX = ship.to_numpy()\n\nplt.figure(figsize=(18, 5))\nim = plt.imshow(\n    X,\n    aspect=\"auto\",\n    cmap=\"viridis\",\n    norm=PowerNorm(gamma=0.3)\n)\n\nplt.title(\"Shipment heatmap\")\nplt.xlabel(\"Orders (Order ID)\")\nplt.ylabel(\"Plants (Plant ID)\")\n\nplt.yticks(np.arange(len(ship.index)), ship.index)\n\n#step = max(1, len(ship.columns)//50)\nstep = 20\nxt = np.arange(0, len(ship.columns), step)\nplt.xticks(xt, ship.columns[xt], rotation=90)\n\nplt.colorbar(im, label=\"Shipped quantity (power-law scaled)\")\nplt.tight_layout()\nplt.show()\n</pre> # ---------------------------- # Heatmap of shipped amounts (plants x orders) # ---------------------------- # lanes_sol must contain: plant_id, order_id, x # plant_ids, order_ids are the index orders used elsewhere (strings)  # Build shipment matrix: rows=plants, cols=orders ship = (lanes_sol         .pivot_table(index=\"plant_id\", columns=\"order_id\", values=\"x\", aggfunc=\"sum\", fill_value=0.0)        )  # Ensure full row/col order (so it matches your indexing) ship = ship.reindex(index=plant_ids, columns=order_ids, fill_value=0.0) order_totals = ship.sum(axis=0).sort_values(ascending=False) ship = ship[order_totals.index] plant_totals = ship.sum(axis=1).sort_values(ascending=False) #ship = ship.loc[plant_totals.index]  X = ship.to_numpy()  plt.figure(figsize=(18, 5)) im = plt.imshow(     X,     aspect=\"auto\",     cmap=\"viridis\",     norm=PowerNorm(gamma=0.3) )  plt.title(\"Shipment heatmap\") plt.xlabel(\"Orders (Order ID)\") plt.ylabel(\"Plants (Plant ID)\")  plt.yticks(np.arange(len(ship.index)), ship.index)  #step = max(1, len(ship.columns)//50) step = 20 xt = np.arange(0, len(ship.columns), step) plt.xticks(xt, ship.columns[xt], rotation=90)  plt.colorbar(im, label=\"Shipped quantity (power-law scaled)\") plt.tight_layout() plt.show()"},{"location":"convex/tutorials/1_lp_transport/#i-transportation-optimization-a-linear-programming-case-study","title":"I - Transportation Optimization: A Linear Programming Case Study.\u00b6","text":"<p>We consider a classical transportation optimization problem, also known as the minimum-cost flow problem, arising in supply chain and logistics planning. A set of production facilities (plants) must ship goods to a set of customer orders at minimum total transportation cost, subject to supply, demand, and routing constraints.</p>"},{"location":"convex/tutorials/1_lp_transport/#formulation","title":"Formulation\u00b6","text":"<p>We assume the following information is known:</p> <ul> <li>Supply capacity at each plant</li> <li>Demand requirement for each order</li> <li>Transportation cost between feasible plant order pairs</li> <li>Feasible shipping lanes (not all plants can serve all orders)</li> </ul> <p>Question - determine how much product to ship from each plant to each order in order to minimize total transportation cost while fully satisfying demand and respecting supply limits</p>"},{"location":"convex/tutorials/1_lp_transport/#objective-function","title":"Objective Function\u00b6","text":"<ul> <li>Minimize total transportation cost:  $\\min \\sum_{i \\in P} \\sum_{j \\in O} c_{ij} x_{ij}$</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$x_{ij} \\ge 0$: quantity shipped from plant $i$ to order $j$</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$c_{ij}$: unit transportation cost from plant $i$ to order $j$</li> <li>$s_i$: supply capacity of plant $i$</li> <li>$d_j$: demand requirement of order $j$</li> <li>$A \\subseteq P \\times O$: set of feasible shipping lanes</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Supply constraints (plant capacity):   $\\sum_{j \\in O} x_{ij} \\le s_i \\quad \\forall i \\in P$</li> <li>Demand constraints (order fulfillmen): $\\sum_{i \\in P} x_{ij} = d_j \\quad \\forall j \\in O$</li> <li>Route feasibility constraints:   $x_{ij} = 0 \\quad \\forall (i,j) \\notin A$</li> <li>Non-negativity:  $x_{ij} \\ge 0 \\quad \\forall i,j$</li> </ul> <p>This problem belongs to the class of linear programming problems, characterized by continuous decision variables, convex feasible regions, and polynomial-time solvability.</p>"},{"location":"convex/tutorials/1_lp_transport/#dataset-source","title":"Dataset Source\u00b6","text":"<p>The data used in this notebook comes from the Supply Chain Logistics Problem Dataset, originally published by researchers at Brunel University London on Figshare.</p> <p>Source details:</p> <ul> <li>Title: Supply Chain Logistics Problem Dataset</li> <li>Publisher: Brunel University London</li> <li>Link</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#dataset-structure","title":"Dataset Structure\u00b6","text":"<p>The dataset provides a complete snapshot of a multi-node supply chain, organized into several tables, including:</p> <ul> <li>OrderList \u2013 customer orders that must be fulfilled</li> <li>Plants / Facilities \u2013 supply locations with capacity limits</li> <li>Shipping Lanes / Routes \u2013 feasible connections between plants and orders</li> <li>Cost &amp; Attributes \u2013 transportation cost and lane-specific properties</li> </ul> <p>Together, these tables allow us to construct a realistic transportation network with real-world constraints.</p>"},{"location":"convex/tutorials/1_lp_transport/#data-description","title":"Data description\u00b6","text":"<ul> <li><p>Plants (<code>plants</code>)</p> <ul> <li><code>plant_id</code>: unique identifier of a plant</li> <li><code>supply_cap</code>: maximum supply capacity available at the plant</li> </ul> </li> <li><p>Orders (<code>orders</code>)</p> <ul> <li><code>order_id</code>: unique identifier of an order</li> <li><code>demand</code>: required demand that must be satisfied</li> </ul> </li> <li><p>Lanes (<code>lanes</code>)</p> <ul> <li>Each row represents a feasible shipping lane from a plant to an order</li> <li><code>plant_id</code>, <code>order_id</code>: endpoints of the lane</li> <li><code>unit_cost</code>: per-unit shipping cost on that lane</li> </ul> </li> </ul> <p>After preprocessing:</p> <ul> <li>all plants and orders included in the model have at least one feasible lane</li> <li>total supply is sufficient to meet total demand</li> </ul>"},{"location":"convex/tutorials/1_lp_transport/#cvxpy-solver","title":"CVXPY Solver\u00b6","text":""},{"location":"convex/tutorials/2_portfolio/","title":"II - Mean\u2013Variance Portfolio Optimization: A Pareto-Optimal Case Study.","text":"In\u00a0[15]: Copied! <pre>#!pip install  pandas_datareader\n</pre> #!pip install  pandas_datareader In\u00a0[1]: Copied! <pre># Install and import necessary libraries\n#!pip install yfinance cvxpy pandas numpy matplotlib seaborn\nimport pandas as pd\nimport numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_datareader import data as pdr\n</pre> # Install and import necessary libraries #!pip install yfinance cvxpy pandas numpy matplotlib seaborn import pandas as pd import numpy as np import cvxpy as cp import matplotlib.pyplot as plt import seaborn as sns from pandas_datareader import data as pdr  In\u00a0[2]: Copied! <pre>tickers_dict = {\n    # Broad Equity\n    \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",\n    \"VTI.US\": \"Total U.S. stock market\",\n\n    # Sector-Specific Equities\n    \"XLK.US\": \"Technology sector\",\n    \"XLF.US\": \"Financials sector\",\n    \"XLE.US\": \"Energy sector\",\n    \"XLV.US\": \"Healthcare sector\",\n    \"XLY.US\": \"Consumer Discretionary sector\",\n    \"XLI.US\": \"Industrials sector\",\n    \"XLRE.US\": \"Real Estate sector\",\n\n    # Size / Style\n    \"IWM.US\": \"Russell 2000 ETF\",\n    \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",\n\n    # International Equity\n    \"EFA.US\": \"Developed markets (ex-US)\",\n    \"EEM.US\": \"Emerging markets\",\n    \"VXUS.US\": \"Total International equity\",\n\n    # Government Bonds\n    \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",\n    \"TLT.US\": \"U.S. Treasuries 20+Y\",\n    \"SHY.US\": \"Short-term Treasuries\",\n\n    # Corporate / High Yield\n    \"LQD.US\": \"Investment-grade corporate bonds\",\n    \"HYG.US\": \"High-yield corporate bonds\",\n\n    # Commodities\n    \"GLD.US\": \"Gold\",\n    \"SLV.US\": \"Silver\",\n    \"DBC.US\": \"Broad commodities\",\n    \"USO.US\": \"Crude Oil\",\n    \"UNG.US\": \"Natural Gas\",\n\n    # Alternative / Real Assets\n    \"VNQ.US\": \"U.S. Real Estate (REITs)\",\n    \"ICLN.US\": \"Global Clean Energy\"\n\n    # Cryptocurrency\n    #\"BTC-USD\": \"Bitcoin (USD-denominated)\"\n}\ntickers = list(tickers_dict.keys())\n\nprint(len(tickers))\n</pre> tickers_dict = {     # Broad Equity     \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",     \"VTI.US\": \"Total U.S. stock market\",      # Sector-Specific Equities     \"XLK.US\": \"Technology sector\",     \"XLF.US\": \"Financials sector\",     \"XLE.US\": \"Energy sector\",     \"XLV.US\": \"Healthcare sector\",     \"XLY.US\": \"Consumer Discretionary sector\",     \"XLI.US\": \"Industrials sector\",     \"XLRE.US\": \"Real Estate sector\",      # Size / Style     \"IWM.US\": \"Russell 2000 ETF\",     \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",      # International Equity     \"EFA.US\": \"Developed markets (ex-US)\",     \"EEM.US\": \"Emerging markets\",     \"VXUS.US\": \"Total International equity\",      # Government Bonds     \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",     \"TLT.US\": \"U.S. Treasuries 20+Y\",     \"SHY.US\": \"Short-term Treasuries\",      # Corporate / High Yield     \"LQD.US\": \"Investment-grade corporate bonds\",     \"HYG.US\": \"High-yield corporate bonds\",      # Commodities     \"GLD.US\": \"Gold\",     \"SLV.US\": \"Silver\",     \"DBC.US\": \"Broad commodities\",     \"USO.US\": \"Crude Oil\",     \"UNG.US\": \"Natural Gas\",      # Alternative / Real Assets     \"VNQ.US\": \"U.S. Real Estate (REITs)\",     \"ICLN.US\": \"Global Clean Energy\"      # Cryptocurrency     #\"BTC-USD\": \"Bitcoin (USD-denominated)\" } tickers = list(tickers_dict.keys())  print(len(tickers)) <pre>26\n</pre> In\u00a0[3]: Copied! <pre># Define asset tickers and download data (3 years of daily data)\nstart_date = \"2023-01-01\"\nend_date = \"2025-12-20\"\n\nprices = pd.DataFrame()\n\nfor ticker in tickers:\n    print(f\"Downloading data for {ticker}...\")\n    data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)\n    prices[ticker] = data[\"Close\"]\n\nprices = prices.sort_index()\n</pre> # Define asset tickers and download data (3 years of daily data) start_date = \"2023-01-01\" end_date = \"2025-12-20\"  prices = pd.DataFrame()  for ticker in tickers:     print(f\"Downloading data for {ticker}...\")     data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)     prices[ticker] = data[\"Close\"]  prices = prices.sort_index() <pre>Downloading data for SPY.US...\nDownloading data for VTI.US...\nDownloading data for XLK.US...\nDownloading data for XLF.US...\nDownloading data for XLE.US...\nDownloading data for XLV.US...\nDownloading data for XLY.US...\nDownloading data for XLI.US...\nDownloading data for XLRE.US...\nDownloading data for IWM.US...\nDownloading data for QQQ.US...\nDownloading data for EFA.US...\nDownloading data for EEM.US...\nDownloading data for VXUS.US...\nDownloading data for IEF.US...\nDownloading data for TLT.US...\nDownloading data for SHY.US...\nDownloading data for LQD.US...\nDownloading data for HYG.US...\nDownloading data for GLD.US...\nDownloading data for SLV.US...\nDownloading data for DBC.US...\nDownloading data for USO.US...\nDownloading data for UNG.US...\nDownloading data for VNQ.US...\nDownloading data for ICLN.US...\n</pre> In\u00a0[4]: Copied! <pre>print(\"Missing values per ticker:\")\n#print(prices.isna().sum())\n\n# Drop rows with any missing values\nprices = prices.dropna()\n</pre> print(\"Missing values per ticker:\") #print(prices.isna().sum())  # Drop rows with any missing values prices = prices.dropna() <pre>Missing values per ticker:\n</pre> In\u00a0[5]: Copied! <pre>plt.figure(figsize=(28, 14))\n\nfor ticker in prices.columns:\n    plt.plot(prices.index, prices[ticker], label=tickers_dict[ticker])\n\nplt.title(\"Asset Prices (Daily Close)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.legend(loc='upper left', bbox_to_anchor=(1,1))\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(28, 14))  for ticker in prices.columns:     plt.plot(prices.index, prices[ticker], label=tickers_dict[ticker])  plt.title(\"Asset Prices (Daily Close)\") plt.xlabel(\"Date\") plt.ylabel(\"Price\") plt.legend(loc='upper left', bbox_to_anchor=(1,1)) plt.grid(True) plt.show()  In\u00a0[6]: Copied! <pre>## Returns &amp; Risk Estimation\n\n# Daily returns\nreturns = prices.pct_change().dropna()\n\ntrading_days_per_year = len(returns) / ((returns.index[-1] - returns.index[0]).days / 365)\n\n# Annualized statistics\nmu = returns.mean() * trading_days_per_year\nSigma = returns.cov() * trading_days_per_year\n\n\n\n# --- Weekly returns ---\nweekly_returns = prices.resample('W').last().pct_change().dropna()\ntrading_weeks_per_year = 52  # standard\nmu_weekly = weekly_returns.mean() * trading_weeks_per_year\n\n# --- Monthly returns ---\nmonthly_returns = prices.resample('ME').last().pct_change().dropna()\ntrading_months_per_year = 12\nmu_monthly = monthly_returns.mean() * trading_months_per_year\n\n# --- Combine into DataFrame ---\nmu_df = pd.DataFrame({\n    'Asset': [tickers_dict[t] for t in prices.columns],\n    'Annualized Daily Return': mu.values,\n    'Annualized Weekly Return': mu_weekly.values,\n    'Annualized Monthly Return': mu_monthly.values\n})\n\n# Sort by Annualized Daily Return descending\nmu_df = mu_df.sort_values(by='Annualized Monthly Return', ascending=False).reset_index(drop=True)\nmu_df\n</pre> ## Returns &amp; Risk Estimation  # Daily returns returns = prices.pct_change().dropna()  trading_days_per_year = len(returns) / ((returns.index[-1] - returns.index[0]).days / 365)  # Annualized statistics mu = returns.mean() * trading_days_per_year Sigma = returns.cov() * trading_days_per_year    # --- Weekly returns --- weekly_returns = prices.resample('W').last().pct_change().dropna() trading_weeks_per_year = 52  # standard mu_weekly = weekly_returns.mean() * trading_weeks_per_year  # --- Monthly returns --- monthly_returns = prices.resample('ME').last().pct_change().dropna() trading_months_per_year = 12 mu_monthly = monthly_returns.mean() * trading_months_per_year  # --- Combine into DataFrame --- mu_df = pd.DataFrame({     'Asset': [tickers_dict[t] for t in prices.columns],     'Annualized Daily Return': mu.values,     'Annualized Weekly Return': mu_weekly.values,     'Annualized Monthly Return': mu_monthly.values })  # Sort by Annualized Daily Return descending mu_df = mu_df.sort_values(by='Annualized Monthly Return', ascending=False).reset_index(drop=True) mu_df  Out[6]: Asset Annualized Daily Return Annualized Weekly Return Annualized Monthly Return 0 Silver 0.384067 0.387958 0.393563 1 Gold 0.299218 0.292404 0.286397 2 NASDAQ 100 ETF (tech-heavy) 0.311032 0.304409 0.271663 3 U.S. broad-market equity (S&amp;P 500) 0.218446 0.210848 0.195445 4 Total U.S. stock market 0.204311 0.196916 0.179124 5 Industrials sector 0.168020 0.159802 0.157793 6 Financials sector 0.172198 0.162923 0.152073 7 Emerging markets 0.145270 0.128816 0.115537 8 Russell 2000 ETF 0.147370 0.137934 0.111278 9 Developed markets (ex-US) 0.134415 0.125603 0.105827 10 Total International equity 0.131797 0.121534 0.103827 11 Technology sector 0.144699 0.137155 0.103551 12 Healthcare sector 0.054585 0.054160 0.060555 13 High-yield corporate bonds 0.030829 0.022513 0.018823 14 Crude Oil 0.045700 0.060544 0.018543 15 Real Estate sector 0.044486 0.036616 0.011133 16 Consumer Discretionary sector 0.068886 0.055095 0.009095 17 Short-term Treasuries 0.006527 0.005279 0.004174 18 U.S. Real Estate (REITs) 0.039841 0.031769 0.003941 19 Investment-grade corporate bonds 0.015634 0.007656 0.001178 20 U.S. Treasuries 7\u201310Y 0.001794 -0.004978 -0.007952 21 Broad commodities -0.005557 0.000307 -0.022092 22 U.S. Treasuries 20+Y -0.008399 -0.021982 -0.029124 23 Global Clean Energy -0.030941 -0.036411 -0.050184 24 Energy sector -0.129084 -0.142963 -0.158374 25 Natural Gas -0.312402 -0.285856 -0.263702 In\u00a0[7]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute correlation matrix\ncorr = returns.corr()\n\n# Rename rows and columns using full asset names\ncorr_named = corr.rename(index=tickers_dict, columns=tickers_dict)\n\ng= sns.clustermap(\n    corr_named,\n    cmap=\"magma\",\n    figsize=(16, 14),\n    linewidths=0.5,\n    cbar_kws={'label': 'Correlation'},\n    method='average',\n    metric='correlation')\ng.fig.text(\n    0.5,                # x-position (center)\n    0.875,               # y-position (near bottom)\n    \"Correlation Matrix with Clustered Ordering (Dendrogram Hidden)\",\n    ha='center',\n    fontsize=16\n)\nplt.show()\n</pre> import seaborn as sns import matplotlib.pyplot as plt  # Compute correlation matrix corr = returns.corr()  # Rename rows and columns using full asset names corr_named = corr.rename(index=tickers_dict, columns=tickers_dict)  g= sns.clustermap(     corr_named,     cmap=\"magma\",     figsize=(16, 14),     linewidths=0.5,     cbar_kws={'label': 'Correlation'},     method='average',     metric='correlation') g.fig.text(     0.5,                # x-position (center)     0.875,               # y-position (near bottom)     \"Correlation Matrix with Clustered Ordering (Dendrogram Hidden)\",     ha='center',     fontsize=16 ) plt.show()  In\u00a0[8]: Copied! <pre>## CVXPY Implementation\n\nn = len(tickers)\nw = cp.Variable(n)\n\n# Weighted scalarization function\ndef solve_portfolio(alpha):\n    objective = cp.Minimize(-alpha * mu.values @ w + (1 - alpha) * cp.quad_form(w, Sigma.values))\n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 0.25]\n    prob = cp.Problem(objective, constraints)\n    prob.solve()\n    return w.value\n</pre> ## CVXPY Implementation  n = len(tickers) w = cp.Variable(n)  # Weighted scalarization function def solve_portfolio(alpha):     objective = cp.Minimize(-alpha * mu.values @ w + (1 - alpha) * cp.quad_form(w, Sigma.values))     constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 0.25]     prob = cp.Problem(objective, constraints)     prob.solve()     return w.value  In\u00a0[9]: Copied! <pre>alphas = np.linspace(0, 1, 100)\nfrontier_returns = []\nfrontier_risks = []\nweights_list = []\n\nfor alpha in alphas:\n    weights = solve_portfolio(alpha)\n    weights_list.append(weights.round(4))\n    r_p = mu.values @ weights\n    sigma_p = np.sqrt(weights.T @ Sigma.values @ weights)\n    frontier_returns.append(r_p)\n    frontier_risks.append(sigma_p)\n</pre> alphas = np.linspace(0, 1, 100) frontier_returns = [] frontier_risks = [] weights_list = []  for alpha in alphas:     weights = solve_portfolio(alpha)     weights_list.append(weights.round(4))     r_p = mu.values @ weights     sigma_p = np.sqrt(weights.T @ Sigma.values @ weights)     frontier_returns.append(r_p)     frontier_risks.append(sigma_p) In\u00a0[10]: Copied! <pre># Convert weights to DataFrame\nweights_df = pd.DataFrame(weights_list, columns=list(tickers_dict.keys()))\nportfolio_index = np.arange(len(frontier_risks)) + 1\nnum_assets = len(tickers_dict)\n\n# Colors: first 20 solid, next 10 lighter\nbase_colors = plt.get_cmap('tab20').colors\nextra_colors = plt.get_cmap('tab20b').colors\ncolors = list(base_colors) + list(extra_colors)\ncolors = colors[:num_assets]\n\n# Hatches: first 20 none, rest diagonal '/'\n\nif num_assets &lt;= 10:\n    hatches = [\"\"]*num_assets\nelif num_assets &lt;= 20:\n    hatches = [\"\"]*10 + [\"/\"]*(num_assets-10)\nelse:\n    hatches = [\"\"]*10 + [\"/\"]*(10) + [\"\\\\\"]*(num_assets-20)\n\n# Create figure with 2 subplots\nfig, axes = plt.subplots(2, 1, figsize=(22, 16))\n\n# Top plot: Efficient Frontier\naxes[0].plot(frontier_risks, frontier_returns, marker='o', color='b', linewidth=2)\naxes[0].set_xlabel(\"Portfolio Risk (Std Dev)\", fontsize=12)\naxes[0].set_ylabel(\"Portfolio Return\", fontsize=12)\naxes[0].set_title(\"Efficient Frontier: Risk vs Return\", fontsize=14)\naxes[0].grid(True)\n\n# Bottom plot: Asset Weights along Frontier with hatches\nbottom = np.zeros(len(portfolio_index))\nfor i, col in enumerate(weights_df.columns):\n    axes[1].fill_between(portfolio_index,\n                         bottom,\n                         bottom + weights_df[col].values,\n                         color=colors[i],\n                         hatch=hatches[i],\n                         edgecolor='k',\n                         linewidth=0.5,\n                         label=tickers_dict[col])\n    bottom += weights_df[col].values\n\n# Overlay scaled risk and return\naxes[1].plot(portfolio_index, np.array(frontier_risks)/max(frontier_risks), color='k', linestyle='--', linewidth=2, label='Risk (scaled)')\naxes[1].plot(portfolio_index, np.array(frontier_returns)/max(frontier_returns), color='r', linestyle='-', linewidth=2, label='Return (scaled)')\n\naxes[1].set_xlabel(\"Portfolio along Efficient Frontier\", fontsize=12)\naxes[1].set_ylabel(\"Asset Weights / Scaled Risk &amp; Return\", fontsize=12)\naxes[1].set_title(\"Asset Allocation Along Efficient Frontier\", fontsize=14)\naxes[1].legend(loc='upper left', bbox_to_anchor=(1,1), fontsize=10)\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n</pre> # Convert weights to DataFrame weights_df = pd.DataFrame(weights_list, columns=list(tickers_dict.keys())) portfolio_index = np.arange(len(frontier_risks)) + 1 num_assets = len(tickers_dict)  # Colors: first 20 solid, next 10 lighter base_colors = plt.get_cmap('tab20').colors extra_colors = plt.get_cmap('tab20b').colors colors = list(base_colors) + list(extra_colors) colors = colors[:num_assets]  # Hatches: first 20 none, rest diagonal '/'  if num_assets &lt;= 10:     hatches = [\"\"]*num_assets elif num_assets &lt;= 20:     hatches = [\"\"]*10 + [\"/\"]*(num_assets-10) else:     hatches = [\"\"]*10 + [\"/\"]*(10) + [\"\\\\\"]*(num_assets-20)  # Create figure with 2 subplots fig, axes = plt.subplots(2, 1, figsize=(22, 16))  # Top plot: Efficient Frontier axes[0].plot(frontier_risks, frontier_returns, marker='o', color='b', linewidth=2) axes[0].set_xlabel(\"Portfolio Risk (Std Dev)\", fontsize=12) axes[0].set_ylabel(\"Portfolio Return\", fontsize=12) axes[0].set_title(\"Efficient Frontier: Risk vs Return\", fontsize=14) axes[0].grid(True)  # Bottom plot: Asset Weights along Frontier with hatches bottom = np.zeros(len(portfolio_index)) for i, col in enumerate(weights_df.columns):     axes[1].fill_between(portfolio_index,                          bottom,                          bottom + weights_df[col].values,                          color=colors[i],                          hatch=hatches[i],                          edgecolor='k',                          linewidth=0.5,                          label=tickers_dict[col])     bottom += weights_df[col].values  # Overlay scaled risk and return axes[1].plot(portfolio_index, np.array(frontier_risks)/max(frontier_risks), color='k', linestyle='--', linewidth=2, label='Risk (scaled)') axes[1].plot(portfolio_index, np.array(frontier_returns)/max(frontier_returns), color='r', linestyle='-', linewidth=2, label='Return (scaled)')  axes[1].set_xlabel(\"Portfolio along Efficient Frontier\", fontsize=12) axes[1].set_ylabel(\"Asset Weights / Scaled Risk &amp; Return\", fontsize=12) axes[1].set_title(\"Asset Allocation Along Efficient Frontier\", fontsize=14) axes[1].legend(loc='upper left', bbox_to_anchor=(1,1), fontsize=10) axes[1].grid(True)  plt.tight_layout() plt.show()"},{"location":"convex/tutorials/2_portfolio/#ii-meanvariance-portfolio-optimization-a-pareto-optimal-case-study","title":"II - Mean\u2013Variance Portfolio Optimization: A Pareto-Optimal Case Study.\u00b6","text":"<p>Modern Portfolio Theory provides a mathematical framework for constructing investment portfolios that balance expected return against risk, where risk is measured by the variance (or standard deviation) of portfolio returns. This framework\u2014commonly referred to as mean\u2013variance analysis\u2014was introduced by Harry Markowitz and forms the foundation of quantitative portfolio construction.</p> <ul> <li>Mean\u2013Variance Optimization: Select portfolio weights to trade off expected return against return variance.</li> <li>Efficient Frontier: The set of portfolios that achieve the maximum expected return for a given level of risk, or equivalently, the minimum risk for a given expected return.</li> <li>Pareto Optimality: A portfolio is Pareto-optimal if no other feasible portfolio offers higher return at the same level of risk.</li> </ul> <p>In this case study, we analyze a diversified universe of liquid financial assets using publicly available historical market data.</p> <ul> <li>Assets consist of widely traded exchange-traded funds (ETFs) spanning multiple asset classes</li> <li>Historical price data covers several years</li> <li>Expected returns and the covariance matrix are estimated from historical returns</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#formulation","title":"Formulation\u00b6","text":"<p>We seek to determine portfolio weights that optimally balance expected return and risk, subject to realistic investment constraints commonly encountered in practice.</p>"},{"location":"convex/tutorials/2_portfolio/#objective-function","title":"Objective Function\u00b6","text":"<p>Two equivalent formulations are commonly used:</p> <ul> <li><p>Minimize portfolio variance for a target return:  $\\min \\; w^\\top \\Sigma w$</p> </li> <li><p>Maximize risk-adjusted return:  $\\max \\; \\mu^\\top w - \\lambda \\, w^\\top \\Sigma w$</p> </li> </ul> <p>where:</p> <ul> <li>$w$ is the vector of portfolio weights</li> <li>$\\mu$ is the vector of expected asset returns</li> <li>$\\Sigma$ is the covariance matrix of asset returns</li> <li>$\\lambda &gt; 0$ is a risk-aversion parameter</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$w_i$: portfolio weight allocated to asset $i$</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$\\mu_i$: expected return of asset $i$</li> <li>$\\Sigma_{ij}$: covariance between returns of assets $i$ and $j$</li> <li>$\\lambda$: risk-aversion coefficient</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Long-only: portfolio weights must be non-negative ($w_i \\ge 0$).</li> <li>Fully invested: portfolio weights sum to one ($\\sum_i w_i = 1$).</li> <li>Maximum per-asset allocation: $w_i \\le 0.25$ to prevent excessive concentration.</li> </ul> <p>This optimization problem is quadratic programming (QP) problem</p> <ul> <li>The objective function contains a quadratic term $w^\\top \\Sigma w$</li> <li>The covariance matrix $\\Sigma$ is positive semidefinite</li> <li>All constraints are linear</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#data-acquisition-and-cleaning","title":"Data Acquisition and Cleaning\u00b6","text":"<p>We begin by selecting a representative universe of financial assets and obtaining their historical price data from publicly available sources. To ensure reproducibility and eliminate reliance on proprietary services or API keys, we use historical market data from Stooq, a freely accessible financial data repository .</p> <p>The asset universe consists of liquid ETFs spanning multiple asset classes, such as U.S. equities, sector-specific equities, government bonds, and commodities. Historical daily closing prices are retrieved over a multi-year horizon to capture different market conditions and provide sufficient data for robust estimation of expected returns and risk.</p>"},{"location":"convex/tutorials/2_portfolio/#mathematical-formulation","title":"Mathematical Formulation\u00b6","text":"<p>Let:</p> <ul> <li>$\\mathbf{w} = [w_1, w_2, \\dots, w_n]^\\top$ be the portfolio weights,</li> <li>$\\mathbf{r} = [r_1, r_2, \\dots, r_n]^\\top$ be the expected returns,</li> <li>$\\mathbf{C}$ be the covariance matrix of asset returns.</li> </ul> <p>The portfolio expected return and variance are:</p> <p>$$ r_p = \\mathbf{w}^\\top \\mathbf{r}, \\qquad \\sigma_p^2 = \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w}. $$</p>"},{"location":"convex/tutorials/2_portfolio/#objectives","title":"Objectives\u00b6","text":"<p>We aim to maximize return and minimize risk simultaneously. Define:</p> <p>$$ f_1(\\mathbf{w}) = -\\mathbf{r}^\\top \\mathbf{w} \\quad \\text{(maximize return)},  \\qquad f_2(\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w} \\quad \\text{(minimize risk)}. $$</p> <p>A weighted scalarization of the two objectives allows us to trace the efficient frontier:</p> <p>$$ \\min_{\\mathbf{w}} \\ -\\alpha \\, \\mathbf{r}^\\top \\mathbf{w} + (1-\\alpha) \\, \\mathbf{w}^\\top \\mathbf{C} \\mathbf{w},  \\quad 0 \\le \\alpha \\le 1. $$</p> <ul> <li>$\\alpha = 0$ \u2192 minimize risk only.</li> <li>$\\alpha = 1$ \u2192 maximize return only.</li> <li>Varying $\\alpha$ from 0 to 1 produces the Pareto-optimal frontier.</li> </ul>"},{"location":"convex/tutorials/2_portfolio/#constraints","title":"Constraints\u00b6","text":"<p>$$ \\sum_{i=1}^n w_i = 1, \\quad 0 \\le w_i \\le 0.25, \\quad \\forall i. $$</p> <p>By sweeping the trade-off parameter $\\alpha$ in the weighted scalarization problem, we can compute a set of optimal portfolios that balance return and risk under realistic investment constraints. This approach provides both practical portfolio recommendations and a visualization of the efficient frontier, aiding investment decision-making in multi-asset portfolios.</p>"},{"location":"convex/tutorials/3_meta/","title":"III - Vehicle Routing: A Metaheuristic Case Study.","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import product\nimport time\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from itertools import product import time In\u00a0[2]: Copied! <pre>#!pip install ortools\n#!pip install python-tsp\n</pre> #!pip install ortools #!pip install python-tsp In\u00a0[3]: Copied! <pre>from python_tsp.exact import solve_tsp_dynamic_programming\n</pre> from python_tsp.exact import solve_tsp_dynamic_programming In\u00a0[4]: Copied! <pre>from ortools.constraint_solver import routing_enums_pb2\nfrom ortools.constraint_solver import pywrapcp\n</pre> from ortools.constraint_solver import routing_enums_pb2 from ortools.constraint_solver import pywrapcp <p>For our case-study we use a benchmark VRPTW dataset based on the classical Solomon instances. Specifically, we utilize the extended Multi-Time-Window VRP dataset recently published on Zenodo, which augments Solomon\u2019s problems with multiple time windows for each customer. This provides realistic distance coordinates, demands, and time-window constraints for a multi-vehicle delivery scenario.</p> In\u00a0[13]: Copied! <pre>df_raw = pd.read_csv('data/multiple time window VRP dataset/multiple time window VRP dataset/data/solomon/C101_MTW.csv')\n\n# randomly select k customers along with the depot (CUST_NO 0)\n\nk = 19\ndf = df_raw.sample(n=k, random_state=1).reset_index(drop=True)\ndf = pd.concat([pd.DataFrame([df_raw.iloc[0]]), df]).reset_index(drop=True)\n\n\nprint(len(df))\n#df.head()\n#print(df.columns)\n</pre> df_raw = pd.read_csv('data/multiple time window VRP dataset/multiple time window VRP dataset/data/solomon/C101_MTW.csv')  # randomly select k customers along with the depot (CUST_NO 0)  k = 19 df = df_raw.sample(n=k, random_state=1).reset_index(drop=True) df = pd.concat([pd.DataFrame([df_raw.iloc[0]]), df]).reset_index(drop=True)   print(len(df)) #df.head() #print(df.columns) <pre>20\n</pre> In\u00a0[14]: Copied! <pre>depot = df[df['CUST_NO'] == 0]\ncustomers = df[df['CUST_NO'] != 0]\n\n# Create the plot\nplt.figure(figsize=(8, 6))\n\n# Plot depot with red square\nplt.scatter(depot['XCOORD'], depot['YCOORD'], color='red', marker='s', s=100, label='Depot (Customer 0)')\n\n# Plot other customers with blue circles\nplt.scatter(customers['XCOORD'], customers['YCOORD'], color='blue', marker='o', s=60, label='Customers')\n\n# Annotate all points with their customer number\nfor _, row in df.iterrows():\n    plt.text(row['XCOORD'] + 0.5, row['YCOORD'] + 0.5, str(row['CUST_NO']), fontsize=9)\n\n# Set plot labels and title\nplt.xlabel('X Coordinate')\nplt.ylabel('Y Coordinate')\nplt.title('Customer Locations with Depot')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</pre> depot = df[df['CUST_NO'] == 0] customers = df[df['CUST_NO'] != 0]  # Create the plot plt.figure(figsize=(8, 6))  # Plot depot with red square plt.scatter(depot['XCOORD'], depot['YCOORD'], color='red', marker='s', s=100, label='Depot (Customer 0)')  # Plot other customers with blue circles plt.scatter(customers['XCOORD'], customers['YCOORD'], color='blue', marker='o', s=60, label='Customers')  # Annotate all points with their customer number for _, row in df.iterrows():     plt.text(row['XCOORD'] + 0.5, row['YCOORD'] + 0.5, str(row['CUST_NO']), fontsize=9)  # Set plot labels and title plt.xlabel('X Coordinate') plt.ylabel('Y Coordinate') plt.title('Customer Locations with Depot') plt.legend() plt.grid(True) plt.tight_layout() plt.show()  In\u00a0[15]: Copied! <pre># sort customers\ndf_plot = df.sort_values(\"CUST_NO\")\n# remove 0 customer number\ndf_plot = df_plot[df_plot[\"CUST_NO\"] != 0]\n# y positions\ny_pos = range(len(df_plot))\n\n# time window lengths\nwindow_length = df_plot[\"DUE_TIME_1\"] - df_plot[\"READY_TIME_1\"]\n\nplt.figure(figsize=(10, 6))\n\n# horizontal bars\nplt.barh(\n    y_pos,\n    window_length,\n    left=df_plot[\"READY_TIME_1\"],\n    alpha=0.8\n)\n\n# y-axis labels\nplt.yticks(y_pos, df_plot[\"CUST_NO\"])\n\n# labels and title\nplt.xlabel(\"Time (minutes)\")\nplt.ylabel(\"Customer\")\nplt.title(\"Customer Time Windows\")\n\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n</pre> # sort customers df_plot = df.sort_values(\"CUST_NO\") # remove 0 customer number df_plot = df_plot[df_plot[\"CUST_NO\"] != 0] # y positions y_pos = range(len(df_plot))  # time window lengths window_length = df_plot[\"DUE_TIME_1\"] - df_plot[\"READY_TIME_1\"]  plt.figure(figsize=(10, 6))  # horizontal bars plt.barh(     y_pos,     window_length,     left=df_plot[\"READY_TIME_1\"],     alpha=0.8 )  # y-axis labels plt.yticks(y_pos, df_plot[\"CUST_NO\"])  # labels and title plt.xlabel(\"Time (minutes)\") plt.ylabel(\"Customer\") plt.title(\"Customer Time Windows\")  plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5) plt.tight_layout() plt.show()  In\u00a0[16]: Copied! <pre>depot_index = int(df.index[df['CUST_NO'] == 0][0])\n\n# Compute Euclidean distance matrix\ncoords_array = df[['XCOORD','YCOORD']].to_numpy()\ndist_matrix = np.sqrt(((coords_array[:,None,:] - coords_array[None,:,:])**2).sum(axis=2))\n</pre> depot_index = int(df.index[df['CUST_NO'] == 0][0])  # Compute Euclidean distance matrix coords_array = df[['XCOORD','YCOORD']].to_numpy() dist_matrix = np.sqrt(((coords_array[:,None,:] - coords_array[None,:,:])**2).sum(axis=2)) In\u00a0[17]: Copied! <pre>scale = 1\nscaled_dist_matrix = np.rint(dist_matrix * scale).astype(int)\n</pre> scale = 1 scaled_dist_matrix = np.rint(dist_matrix * scale).astype(int) In\u00a0[18]: Copied! <pre>## Data Model\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = 1\ndata['depot'] = depot_index\n</pre> ## Data Model data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = 1 data['depot'] = depot_index In\u00a0[19]: Copied! <pre>D = np.array(scaled_dist_matrix)\n# Reorder so the depot becomes node 0 (python-tsp assumes start at 0)\nn = D.shape[0]\nperm = [depot_index] + [i for i in range(n) if i != depot_index]\nD2 = D[np.ix_(perm, perm)]\n# Exact TSP (cycle). Returns permutation starting at 0 and the optimal tour length.\n\nt_start = time.perf_counter()\n# Exact TSP (Held\u2013Karp)\n_, total_distance = solve_tsp_dynamic_programming(D2)\nt_end = time.perf_counter()\nprint(f\"Exact TSP (Held\u2013Karp) took {t_end - t_start:.4f} seconds\")\nprint(\"Optimal total distance:\", total_distance)\n</pre> D = np.array(scaled_dist_matrix) # Reorder so the depot becomes node 0 (python-tsp assumes start at 0) n = D.shape[0] perm = [depot_index] + [i for i in range(n) if i != depot_index] D2 = D[np.ix_(perm, perm)] # Exact TSP (cycle). Returns permutation starting at 0 and the optimal tour length.  t_start = time.perf_counter() # Exact TSP (Held\u2013Karp) _, total_distance = solve_tsp_dynamic_programming(D2) t_end = time.perf_counter() print(f\"Exact TSP (Held\u2013Karp) took {t_end - t_start:.4f} seconds\") print(\"Optimal total distance:\", total_distance) <pre>Exact TSP (Held\u2013Karp) took 82.1020 seconds\nOptimal total distance: 305\n</pre> In\u00a0[20]: Copied! <pre># Create the routing model\n# The pywrapcp.RoutingIndexManager is responsible for translating between external \n# node indices (which you use in your problem definition) and internal solver indices.\n# The inputs to RoutingIndexManager are:\n# - The number of rows of the distance matrix, which is the number of locations (including the depot).\n# - The number of vehicles in the problem.\n# - The node corresponding to the depot.\n\nmanager = pywrapcp.RoutingIndexManager(\n    len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"]\n)\nrouting = pywrapcp.RoutingModel(manager)\n\n# Create the distance callback\n# To use the routing solver, you need to create a distance (or transit) callback: \n# a function that takes any pair of locations and returns the distance between them.\n# The easiest way to do this is using the distance matrix.\n\ndef distance_callback(from_index, to_index):\n    \"\"\"Returns the distance between the two nodes.\"\"\"\n    # Convert from routing variable Index to distance matrix NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    return data[\"distance_matrix_scaled\"][from_node][to_node]\n\ntransit_callback_index = routing.RegisterTransitCallback(distance_callback)\n\n# Set the cost of travel\n# The arc cost evaluator tells the solver how to calculate the cost of travel \n# between any two locations \u2014 in other words, the cost of the edge (or arc) \n# joining them in the graph for the problem. The following code sets the arc cost \n# evaluator.\n\nrouting.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n\n# Set search parameters\n# The code sets the first solution strategy to PATH_CHEAPEST_ARC, \n# which creates an initial route for the solver by repeatedly adding edges \n# with the least weight that don't lead to a previously visited node \n# (other than the depot).\n\nsearch_parameters = pywrapcp.DefaultRoutingSearchParameters()\nsearch_parameters.first_solution_strategy = (\n    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n)\n</pre> # Create the routing model # The pywrapcp.RoutingIndexManager is responsible for translating between external  # node indices (which you use in your problem definition) and internal solver indices. # The inputs to RoutingIndexManager are: # - The number of rows of the distance matrix, which is the number of locations (including the depot). # - The number of vehicles in the problem. # - The node corresponding to the depot.  manager = pywrapcp.RoutingIndexManager(     len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"] ) routing = pywrapcp.RoutingModel(manager)  # Create the distance callback # To use the routing solver, you need to create a distance (or transit) callback:  # a function that takes any pair of locations and returns the distance between them. # The easiest way to do this is using the distance matrix.  def distance_callback(from_index, to_index):     \"\"\"Returns the distance between the two nodes.\"\"\"     # Convert from routing variable Index to distance matrix NodeIndex.     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     return data[\"distance_matrix_scaled\"][from_node][to_node]  transit_callback_index = routing.RegisterTransitCallback(distance_callback)  # Set the cost of travel # The arc cost evaluator tells the solver how to calculate the cost of travel  # between any two locations \u2014 in other words, the cost of the edge (or arc)  # joining them in the graph for the problem. The following code sets the arc cost  # evaluator.  routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)  # Set search parameters # The code sets the first solution strategy to PATH_CHEAPEST_ARC,  # which creates an initial route for the solver by repeatedly adding edges  # with the least weight that don't lead to a previously visited node  # (other than the depot).  search_parameters = pywrapcp.DefaultRoutingSearchParameters() search_parameters.first_solution_strategy = (     routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC )  In\u00a0[21]: Copied! <pre># Solution Printer\ndef print_solution(manager, routing, solution):\n    \"\"\"Prints solution on console.\"\"\"\n    print(f\"Objective: {solution.ObjectiveValue()} miles\")\n    index = routing.Start(0)\n    plan_output = \"Route for vehicle 0:\\n\"\n    route_distance = 0\n    while not routing.IsEnd(index):\n        plan_output += f\" {manager.IndexToNode(index)} -&gt;\"\n        previous_index = index\n        index = solution.Value(routing.NextVar(index))\n        route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)\n    plan_output += f\" {manager.IndexToNode(index)}\\n\"\n    plan_output += f\"Route distance: {route_distance}miles\\n\"\n    print(plan_output)\n</pre> # Solution Printer def print_solution(manager, routing, solution):     \"\"\"Prints solution on console.\"\"\"     print(f\"Objective: {solution.ObjectiveValue()} miles\")     index = routing.Start(0)     plan_output = \"Route for vehicle 0:\\n\"     route_distance = 0     while not routing.IsEnd(index):         plan_output += f\" {manager.IndexToNode(index)} -&gt;\"         previous_index = index         index = solution.Value(routing.NextVar(index))         route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)     plan_output += f\" {manager.IndexToNode(index)}\\n\"     plan_output += f\"Route distance: {route_distance}miles\\n\"     print(plan_output)  In\u00a0[22]: Copied! <pre>t_start = time.perf_counter()\nsolution = routing.SolveWithParameters(search_parameters)\nt_end = time.perf_counter()\nprint(f\"OR Tools took {t_end - t_start:.4f} seconds\")\n\nif solution:\n    print_solution(manager, routing, solution)\n</pre> t_start = time.perf_counter() solution = routing.SolveWithParameters(search_parameters) t_end = time.perf_counter() print(f\"OR Tools took {t_end - t_start:.4f} seconds\")  if solution:     print_solution(manager, routing, solution) <pre>OR Tools took 0.0358 seconds\nObjective: 305 miles\nRoute for vehicle 0:\n 0 -&gt; 15 -&gt; 17 -&gt; 7 -&gt; 12 -&gt; 10 -&gt; 13 -&gt; 16 -&gt; 5 -&gt; 3 -&gt; 4 -&gt; 14 -&gt; 1 -&gt; 11 -&gt; 6 -&gt; 8 -&gt; 9 -&gt; 18 -&gt; 2 -&gt; 19 -&gt; 0\nRoute distance: 305miles\n\n</pre> In\u00a0[23]: Copied! <pre>def plot_tsp_route_ortools(df, manager, routing, solution,\n                          vehicle_id=0,\n                          distance_matrix=None,   # e.g., data['distance_matrix_scaled'] or data['distance_matrix']\n                          scale=1,                # e.g., 1000 if using scaled costs, else 1\n                          x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",\n                          show_labels=True):\n    \"\"\"\n    One-stop function:\n      1) Extracts the OR-Tools route for `vehicle_id`\n      2) Plots points + arrows\n      3) Computes total route distance using your EXISTING distance matrix\n         (so it matches what OR-Tools optimized)\n\n    Notes:\n      - Assumes IndexToNode outputs indices aligned with df row order (df reset_index recommended).\n      - distance_matrix should be the same one you used for costs, typically scaled.\n    \"\"\"\n    if solution is None:\n        raise ValueError(\"No solution found (solution is None).\")\n\n    if distance_matrix is None:\n        raise ValueError(\"Please pass distance_matrix (e.g., data['distance_matrix_scaled']).\")\n\n    xs = df[x_col].to_numpy()\n    ys = df[y_col].to_numpy()\n\n    # --- Extract route nodes (including end depot) ---\n    index = routing.Start(vehicle_id)\n    route = [manager.IndexToNode(index)]\n    while not routing.IsEnd(index):\n        index = solution.Value(routing.NextVar(index))\n        route.append(manager.IndexToNode(index))\n\n    # --- Compute total distance from existing matrix ---\n    total_cost = 0\n    for a, b in zip(route[:-1], route[1:]):\n        total_cost += int(distance_matrix[a][b])\n    total_distance = total_cost / scale\n\n    # --- Plot ---\n    plt.figure(figsize=(8, 6))\n    plt.scatter(xs, ys)\n\n    # depot highlight (assumes depot is route[0])\n    depot_node = route[0]\n    plt.scatter([xs[depot_node]], [ys[depot_node]], s=140, marker=\"s\", label=\"Depot\")\n\n    # labels\n    if show_labels:\n        labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))\n        for i in range(len(df)):\n            plt.text(xs[i], ys[i], str(labels[i]), fontsize=9, ha=\"left\", va=\"bottom\")\n\n    # arrows\n    for a, b in zip(route[:-1], route[1:]):\n        plt.annotate(\n            \"\",\n            xy=(xs[b], ys[b]),\n            xytext=(xs[a], ys[a]),\n            arrowprops=dict(arrowstyle=\"-&gt;\", lw=1),\n        )\n\n    plt.title(f\"TSP (vehicle {vehicle_id}) | Total distance = {total_distance:.2f}\")\n    plt.xlabel(x_col)\n    plt.ylabel(y_col)\n    plt.axis(\"equal\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    return route, total_distance\n</pre> def plot_tsp_route_ortools(df, manager, routing, solution,                           vehicle_id=0,                           distance_matrix=None,   # e.g., data['distance_matrix_scaled'] or data['distance_matrix']                           scale=1,                # e.g., 1000 if using scaled costs, else 1                           x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",                           show_labels=True):     \"\"\"     One-stop function:       1) Extracts the OR-Tools route for `vehicle_id`       2) Plots points + arrows       3) Computes total route distance using your EXISTING distance matrix          (so it matches what OR-Tools optimized)      Notes:       - Assumes IndexToNode outputs indices aligned with df row order (df reset_index recommended).       - distance_matrix should be the same one you used for costs, typically scaled.     \"\"\"     if solution is None:         raise ValueError(\"No solution found (solution is None).\")      if distance_matrix is None:         raise ValueError(\"Please pass distance_matrix (e.g., data['distance_matrix_scaled']).\")      xs = df[x_col].to_numpy()     ys = df[y_col].to_numpy()      # --- Extract route nodes (including end depot) ---     index = routing.Start(vehicle_id)     route = [manager.IndexToNode(index)]     while not routing.IsEnd(index):         index = solution.Value(routing.NextVar(index))         route.append(manager.IndexToNode(index))      # --- Compute total distance from existing matrix ---     total_cost = 0     for a, b in zip(route[:-1], route[1:]):         total_cost += int(distance_matrix[a][b])     total_distance = total_cost / scale      # --- Plot ---     plt.figure(figsize=(8, 6))     plt.scatter(xs, ys)      # depot highlight (assumes depot is route[0])     depot_node = route[0]     plt.scatter([xs[depot_node]], [ys[depot_node]], s=140, marker=\"s\", label=\"Depot\")      # labels     if show_labels:         labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))         for i in range(len(df)):             plt.text(xs[i], ys[i], str(labels[i]), fontsize=9, ha=\"left\", va=\"bottom\")      # arrows     for a, b in zip(route[:-1], route[1:]):         plt.annotate(             \"\",             xy=(xs[b], ys[b]),             xytext=(xs[a], ys[a]),             arrowprops=dict(arrowstyle=\"-&gt;\", lw=1),         )      plt.title(f\"TSP (vehicle {vehicle_id}) | Total distance = {total_distance:.2f}\")     plt.xlabel(x_col)     plt.ylabel(y_col)     plt.axis(\"equal\")     plt.grid(True)     plt.legend()     plt.show()      return route, total_distance  In\u00a0[24]: Copied! <pre>route, dist = plot_tsp_route_ortools(df, manager, \n                                     routing, solution,\n                                     vehicle_id=0,\n                                     distance_matrix=data['distance_matrix_scaled'],\n                                     scale=1)\n</pre> route, dist = plot_tsp_route_ortools(df, manager,                                       routing, solution,                                      vehicle_id=0,                                      distance_matrix=data['distance_matrix_scaled'],                                      scale=1) In\u00a0[25]: Copied! <pre>## Data Model\n\nK = 4\nvehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K))\nprint(vehicle_capacity)\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = K\ndata['demands'] = df['DEMAND'].tolist()\ndata['vehicle_capacities'] = [vehicle_capacity]*K\ndata['depot'] = depot_index\n</pre> ## Data Model  K = 4 vehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K)) print(vehicle_capacity) data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = K data['demands'] = df['DEMAND'].tolist() data['vehicle_capacities'] = [vehicle_capacity]*K data['depot'] = depot_index <pre>105\n</pre> In\u00a0[26]: Copied! <pre># Create the routing model\nmanager = pywrapcp.RoutingIndexManager(\n    len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"]\n)\n\nrouting = pywrapcp.RoutingModel(manager)\n\n# Create the distance callback\ndef distance_callback(from_index, to_index):\n    \"\"\"Returns the distance between the two nodes.\"\"\"\n    # Convert from routing variable Index to distance matrix NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    to_node = manager.IndexToNode(to_index)\n    return int(data[\"distance_matrix_scaled\"][from_node][to_node])\n\ntransit_callback_index = routing.RegisterTransitCallback(distance_callback)\n\n# Define cost of each arc.\nrouting.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n\n# Add Capacity constraint.\ndef demand_callback(from_index):\n    \"\"\"Returns the demand of the node.\"\"\"\n    # Convert from routing variable Index to demands NodeIndex.\n    from_node = manager.IndexToNode(from_index)\n    return int(data[\"demands\"][from_node])\n\ndemand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)\nrouting.AddDimensionWithVehicleCapacity(\n        demand_callback_index,\n        0,  # null capacity slack\n        data[\"vehicle_capacities\"],  # vehicle maximum capacities\n        True,  # start cumul to zero\n        \"Capacity\",\n)\n\n# Set search parameters\n# Setting first solution heuristic.\nsearch_parameters = pywrapcp.DefaultRoutingSearchParameters()\nsearch_parameters.first_solution_strategy = (\n    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n)\nsearch_parameters.local_search_metaheuristic = (\n    routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH\n)\nsearch_parameters.time_limit.FromSeconds(1)\n</pre> # Create the routing model manager = pywrapcp.RoutingIndexManager(     len(data[\"distance_matrix_scaled\"]), data[\"num_vehicles\"], data[\"depot\"] )  routing = pywrapcp.RoutingModel(manager)  # Create the distance callback def distance_callback(from_index, to_index):     \"\"\"Returns the distance between the two nodes.\"\"\"     # Convert from routing variable Index to distance matrix NodeIndex.     from_node = manager.IndexToNode(from_index)     to_node = manager.IndexToNode(to_index)     return int(data[\"distance_matrix_scaled\"][from_node][to_node])  transit_callback_index = routing.RegisterTransitCallback(distance_callback)  # Define cost of each arc. routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)  # Add Capacity constraint. def demand_callback(from_index):     \"\"\"Returns the demand of the node.\"\"\"     # Convert from routing variable Index to demands NodeIndex.     from_node = manager.IndexToNode(from_index)     return int(data[\"demands\"][from_node])  demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback) routing.AddDimensionWithVehicleCapacity(         demand_callback_index,         0,  # null capacity slack         data[\"vehicle_capacities\"],  # vehicle maximum capacities         True,  # start cumul to zero         \"Capacity\", )  # Set search parameters # Setting first solution heuristic. search_parameters = pywrapcp.DefaultRoutingSearchParameters() search_parameters.first_solution_strategy = (     routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC ) search_parameters.local_search_metaheuristic = (     routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH ) search_parameters.time_limit.FromSeconds(1) In\u00a0[27]: Copied! <pre>solution = routing.SolveWithParameters(search_parameters)\n</pre> solution = routing.SolveWithParameters(search_parameters) In\u00a0[28]: Copied! <pre>def plot_cvrp_routes_ortools(df, manager, routing, solution,\n                            distance_matrix=None,\n                            scale=1,\n                            x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",\n                            show_labels=True,\n                            show_arrows=True):\n    \"\"\"\n    CVRP visualizer with:\n      - per-vehicle colored routes\n      - per-vehicle distance + load in legend\n      - total distance + total load in title\n    \"\"\"\n    if solution is None:\n        raise ValueError(\"No solution found.\")\n    if distance_matrix is None:\n        raise ValueError(\"Please pass distance_matrix.\")\n\n    xs = df[x_col].to_numpy()\n    ys = df[y_col].to_numpy()\n\n    labels = None\n    if show_labels:\n        labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))\n\n    depot_node = manager.IndexToNode(routing.Start(0))\n\n    # Capacity dimension (if present)\n    try:\n        cap_dim = routing.GetDimensionOrDie(\"Capacity\")\n    except Exception:\n        cap_dim = None\n\n    routes = {}\n    per_vehicle_distance = {}\n    per_vehicle_load = {}\n\n    total_cost = 0\n    total_load = 0\n\n    # --- Extract routes, distance, load ---\n    for v in range(routing.vehicles()):\n        start_index = routing.Start(v)\n        if solution.Value(routing.NextVar(start_index)) == routing.End(v):\n            continue\n\n        index = start_index\n        route = [manager.IndexToNode(index)]\n        route_cost = 0\n\n        while not routing.IsEnd(index):\n            node = manager.IndexToNode(index)\n            next_index = solution.Value(routing.NextVar(index))\n            next_node = manager.IndexToNode(next_index)\n\n            route.append(next_node)\n            route_cost += int(distance_matrix[node][next_node])\n            index = next_index\n\n        routes[v] = route\n        per_vehicle_distance[v] = route_cost / scale\n        total_cost += route_cost\n\n        if cap_dim is not None:\n            load = solution.Value(cap_dim.CumulVar(routing.End(v)))\n        else:\n            load = 0\n\n        per_vehicle_load[v] = load\n        total_load += load\n\n    total_distance = total_cost / scale\n\n    # --- Plot base points ---\n    plt.figure(figsize=(9, 7))\n    plt.scatter(xs, ys, label=\"Customers\")\n    plt.scatter([xs[depot_node]], [ys[depot_node]],\n                s=180, marker=\"s\", label=\"Depot\")\n\n    if show_labels:\n        for i in range(len(df)):\n            plt.text(xs[i], ys[i], str(labels[i]),\n                     fontsize=9, ha=\"left\", va=\"bottom\")\n\n    # --- Colors ---\n    used_vehicles = list(routes.keys())\n    cmap = plt.get_cmap(\"tab10\") if len(used_vehicles) &lt;= 10 else plt.get_cmap(\"tab20\")\n\n    # --- Draw routes ---\n    for idx, v in enumerate(used_vehicles):\n        route = routes[v]\n        color = cmap(idx % cmap.N)\n\n        for a, b in zip(route[:-1], route[1:]):\n            x1, y1 = xs[a], ys[a]\n            x2, y2 = xs[b], ys[b]\n\n            plt.plot([x1, x2], [y1, y2], color=color, lw=1.8)\n\n            if show_arrows:\n                plt.annotate(\n                    \"\",\n                    xy=(x2, y2),\n                    xytext=(x1, y1),\n                    arrowprops=dict(arrowstyle=\"-&gt;\", lw=1.8, color=color),\n                )\n\n        # legend entry\n        plt.plot(\n            [],\n            [],\n            color=color,\n            lw=3,\n            label=f\"Vehicle {v} | dist={per_vehicle_distance[v]:.2f}, load={per_vehicle_load[v]}\"\n        )\n\n    plt.title(\n        f\"CVRP Routes | Vehicles used = {len(routes)} | \"\n        f\"Total distance = {total_distance:.2f} | Total load = {total_load}\"\n    )\n    plt.xlabel(x_col)\n    plt.ylabel(y_col)\n    plt.axis(\"equal\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    return routes, total_distance, per_vehicle_distance, per_vehicle_load\n</pre> def plot_cvrp_routes_ortools(df, manager, routing, solution,                             distance_matrix=None,                             scale=1,                             x_col=\"XCOORD\", y_col=\"YCOORD\", node_col=\"CUST_NO\",                             show_labels=True,                             show_arrows=True):     \"\"\"     CVRP visualizer with:       - per-vehicle colored routes       - per-vehicle distance + load in legend       - total distance + total load in title     \"\"\"     if solution is None:         raise ValueError(\"No solution found.\")     if distance_matrix is None:         raise ValueError(\"Please pass distance_matrix.\")      xs = df[x_col].to_numpy()     ys = df[y_col].to_numpy()      labels = None     if show_labels:         labels = df[node_col].to_numpy() if node_col in df.columns else np.arange(len(df))      depot_node = manager.IndexToNode(routing.Start(0))      # Capacity dimension (if present)     try:         cap_dim = routing.GetDimensionOrDie(\"Capacity\")     except Exception:         cap_dim = None      routes = {}     per_vehicle_distance = {}     per_vehicle_load = {}      total_cost = 0     total_load = 0      # --- Extract routes, distance, load ---     for v in range(routing.vehicles()):         start_index = routing.Start(v)         if solution.Value(routing.NextVar(start_index)) == routing.End(v):             continue          index = start_index         route = [manager.IndexToNode(index)]         route_cost = 0          while not routing.IsEnd(index):             node = manager.IndexToNode(index)             next_index = solution.Value(routing.NextVar(index))             next_node = manager.IndexToNode(next_index)              route.append(next_node)             route_cost += int(distance_matrix[node][next_node])             index = next_index          routes[v] = route         per_vehicle_distance[v] = route_cost / scale         total_cost += route_cost          if cap_dim is not None:             load = solution.Value(cap_dim.CumulVar(routing.End(v)))         else:             load = 0          per_vehicle_load[v] = load         total_load += load      total_distance = total_cost / scale      # --- Plot base points ---     plt.figure(figsize=(9, 7))     plt.scatter(xs, ys, label=\"Customers\")     plt.scatter([xs[depot_node]], [ys[depot_node]],                 s=180, marker=\"s\", label=\"Depot\")      if show_labels:         for i in range(len(df)):             plt.text(xs[i], ys[i], str(labels[i]),                      fontsize=9, ha=\"left\", va=\"bottom\")      # --- Colors ---     used_vehicles = list(routes.keys())     cmap = plt.get_cmap(\"tab10\") if len(used_vehicles) &lt;= 10 else plt.get_cmap(\"tab20\")      # --- Draw routes ---     for idx, v in enumerate(used_vehicles):         route = routes[v]         color = cmap(idx % cmap.N)          for a, b in zip(route[:-1], route[1:]):             x1, y1 = xs[a], ys[a]             x2, y2 = xs[b], ys[b]              plt.plot([x1, x2], [y1, y2], color=color, lw=1.8)              if show_arrows:                 plt.annotate(                     \"\",                     xy=(x2, y2),                     xytext=(x1, y1),                     arrowprops=dict(arrowstyle=\"-&gt;\", lw=1.8, color=color),                 )          # legend entry         plt.plot(             [],             [],             color=color,             lw=3,             label=f\"Vehicle {v} | dist={per_vehicle_distance[v]:.2f}, load={per_vehicle_load[v]}\"         )      plt.title(         f\"CVRP Routes | Vehicles used = {len(routes)} | \"         f\"Total distance = {total_distance:.2f} | Total load = {total_load}\"     )     plt.xlabel(x_col)     plt.ylabel(y_col)     plt.axis(\"equal\")     plt.grid(True)     plt.legend()     plt.show()      return routes, total_distance, per_vehicle_distance, per_vehicle_load  In\u00a0[29]: Copied! <pre>routes, total_dist, per_v_dist, per_v_load  = plot_cvrp_routes_ortools(\n    df, manager, routing, solution,\n    distance_matrix=data['distance_matrix_scaled'],\n    scale=1,           \n    show_labels=True,\n    show_arrows=True\n)\nprint(\"Total distance:\", total_dist)\nprint(\"Per-vehicle:\", per_v_dist)\n</pre> routes, total_dist, per_v_dist, per_v_load  = plot_cvrp_routes_ortools(     df, manager, routing, solution,     distance_matrix=data['distance_matrix_scaled'],     scale=1,                show_labels=True,     show_arrows=True ) print(\"Total distance:\", total_dist) print(\"Per-vehicle:\", per_v_dist) <pre>Total distance: 447.0\nPer-vehicle: {0: 120.0, 1: 92.0, 2: 74.0, 3: 161.0}\n</pre> In\u00a0[30]: Copied! <pre>## Data Model\n\nK = 4\n\nvehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K))\nprint(vehicle_capacity)\ndata = {}\ndata['distance_matrix_scaled'] = scaled_dist_matrix\ndata['num_vehicles'] = K\ndata['demands'] = df['DEMAND'].tolist()\n#data['vehicle_capacities'] = [vehicle_capacity]*K\ndata['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1']))  \ndata['depot'] = depot_index\n</pre> ## Data Model  K = 4  vehicle_capacity = int(1.2*np.ceil(sum(df['DEMAND']) / K)) print(vehicle_capacity) data = {} data['distance_matrix_scaled'] = scaled_dist_matrix data['num_vehicles'] = K data['demands'] = df['DEMAND'].tolist() #data['vehicle_capacities'] = [vehicle_capacity]*K data['time_windows'] = list(zip(df['READY_TIME_1'], df['DUE_TIME_1']))   data['depot'] = depot_index <pre>105\n</pre>"},{"location":"convex/tutorials/3_meta/#iii-vehicle-routing-a-metaheuristic-case-study","title":"III - Vehicle Routing: A Metaheuristic Case Study.\u00b6","text":""},{"location":"convex/tutorials/3_meta/#problem-definition","title":"Problem Definition\u00b6","text":"<p>We consider a real-world logistics challenge: the Vehicle Routing Problem (VRP). In VRP a fleet of vehicles must deliver goods from a depot to a set of geographically dispersed customers</p> <ul> <li>Travelling Salesman Problem (TSP): Given a single vehicle, find the minimum-cost tour that visits each customer exactly once and returns to the depot.</li> <li>Vehicle Routing Problem (VRP): A generalization of the TSP in which multiple vehicles are available; customers must be partitioned among vehicles, and each vehicle performs a route starting and ending at the depot, subject to constraints such as vehicle capacity.</li> <li>Vehicle Routing Problem with Time Windows (VRPTW): An extension of the VRP where each customer must be serviced within a specified time interval, and vehicle routes must respect both capacity constraints and time-window feasibility.</li> </ul>"},{"location":"convex/tutorials/3_meta/#formulation","title":"Formulation\u00b6","text":"<p>We know:</p> <ul> <li>The demand (quantity required) of each customer</li> <li>The travel cost (or distance/time) between every pair of locations (customers and depot)</li> <li>The number of available vehicles</li> <li>The capacity of each vehicle</li> <li>A time window for each customer $i$, denoted by $[a_i, b_i]$</li> </ul> <p>We must determine which customers are assigned to each vehicle and the order in which they are served, so as to minimize the total routing cost while satisfying all capacity and time-window constraints.</p>"},{"location":"convex/tutorials/3_meta/#objective-function","title":"Objective Function\u00b6","text":"<ul> <li>Minimize total travel cost: $\\min \\sum_{i,j} c_{ij} \\sum_k x_{ijk}$</li> </ul>"},{"location":"convex/tutorials/3_meta/#decision-variables","title":"Decision Variables\u00b6","text":"<ul> <li>$x_{ijk} = 1$ if vehicle $k$ travels directly from $i$ to $j$, $0$ otherwise</li> <li>$T_{ik}$ = service start time of vehicle $k$ at customer $i$</li> </ul>"},{"location":"convex/tutorials/3_meta/#parameters","title":"Parameters\u00b6","text":"<ul> <li>$c_{ij}$ = travel cost from $i$ to $j$</li> <li>$t_{ij}$ = travel time from $i$ to $j$</li> <li>$q_i$ = demand of customer $i$</li> <li>$Q_k$ = capacity of vehicle $k$</li> <li>$[a_i, b_i]$ = time window of customer $i$</li> <li>$s_i$ = service time at customer $i$ (assumed 0 in this case-study)</li> <li>$M$ = sufficiently large constant</li> </ul>"},{"location":"convex/tutorials/3_meta/#constraints","title":"Constraints\u00b6","text":"<ul> <li>Exactly one vehicle enters each customer (except depot): $\\sum_i \\sum_k x_{ijk} = 1 \\;\\forall j \\neq \\text{depot}$</li> <li>Exactly one vehicle leaves each customer (except depot): $\\sum_j \\sum_k x_{ijk} = 1 \\;\\forall i \\neq \\text{depot}$</li> <li>Flow conservation (same vehicle enters and leaves): $\\sum_i x_{ihk} - \\sum_j x_{hjk} = 0 \\;\\forall h,k$</li> <li>Vehicle capacity: $\\sum_i q_i \\sum_j x_{ijk} \\le Q_k \\;\\forall k$</li> <li>Subtour elimination (Prevent disconnected cycles that do not include the depot): $\\sum_{i,j \\in S} x_{ijk} \\le |S| - 1 \\;\\forall S \\subseteq N,\\; S \\neq \\emptyset,\\; \\forall k$</li> <li>Service must start within the allowed time window: $a_i \\le T_{ik} \\le b_i \\;\\forall i \\neq \\text{depot},\\forall k$</li> <li>Time consistency along routes: $T_{jk} \\ge T_{ik} + s_i + t_{ij} - M(1 - x_{ijk}) \\;\\forall i,j,k$</li> </ul> <p>M is a large constant used in Big-M constraints to turn constraints on or off depending on a binary decision variable.</p> <p>This problem is combinatorial and non-convex, it generalizes the Traveling Salesman Problem and is therefore NP-hard (i.e. solution time grows exponentially with problem size).</p> <ul> <li><p>Binary routing variables: The decision variables  $x_{ijk} \\in \\{0,1\\}$ make the Vehicle Routing Problem with Time Windows (VRPTW) a mixed-integer optimization problem. Each variable indicates whether vehicle ( k ) travels directly from customer ( i ) to customer ( j ). There is no concept of a fractional route or \u201chalf a vehicle,\u201d unlike continuous flow variables in linear programming.</p> </li> <li><p>Exponential combinations: Assigning $n$ customers to $m$ vehicles and determining the service sequence for each vehicle leads to a factorial / exponential growth in the number of possible solutions. Even for moderately sized instances, the number of feasible routing combinations is astronomically large.</p> </li> </ul> <p>This combinatorial explosion is the fundamental reason why exact solution methods scale poorly and why heuristic and metaheuristic approaches are commonly used in practice.</p>"},{"location":"convex/tutorials/3_meta/#data-source","title":"Data Source\u00b6","text":""},{"location":"convex/tutorials/3_meta/#scenario-1-traveling-salesman-problem","title":"Scenario 1 -  Traveling Salesman Problem\u00b6","text":"<p>In this scenario, a single vehicle starts from a depot, visits all customers exactly once, and returns to the depot. The objective is to minimize the total travel distance.</p> <ul> <li>One vehicle</li> <li>No capacity constraints</li> <li>No time window constraints</li> </ul>"},{"location":"convex/tutorials/3_meta/#option-1-exact-solution","title":"Option 1 - Exact Solution\u00b6","text":"<p>We will first approach this using this problem using an exact dynamic programming algorithm (Held\u2013Karp). The method is exact and guarantees optimality, but it is slow because it must enumerate all $2^\ud835\udc5b$ subsets of customers, leading to exponential time complexity</p>"},{"location":"convex/tutorials/3_meta/#option-2-metaheuristic-solution","title":"Option 2 \u2013 Metaheuristic Solution\u00b6","text":"<p>To overcome the computational limitations of exact methods, we employ a metaheuristic approach using Google OR-Tools. The Traveling Salesman Problem is modeled as a weighted graph, and a feasible tour is first constructed using a greedy heuristic (Path Cheapest Arc), which incrementally selects the lowest-cost admissible edges. This initial solution is then refined through local search techniques that explore neighboring solutions to reduce total travel cost. While this approach does not guarantee optimality, it efficiently produces high-quality, near-optimal solutions and scales well to large problem instances where exact optimization is impractical.</p>"},{"location":"convex/tutorials/3_meta/#scenario-2-capacitated-vehicle-routing-problem","title":"Scenario 2 \u2013 Capacitated Vehicle Routing Problem\u00b6","text":"<p>In this scenario, there are K vehicles available at a central depot. Each vehicle has a maximum load capacity, and customer demands must be delivered without exceeding that capacity. The objective is to design a set of routes that minimizes total travel cost (or distance) while ensuring all customer demands are served.</p> <ul> <li>K vehicles</li> <li>Capacity constraints (total demand on each route must not exceed vehicle capacity)</li> <li>No time window constraints on customer visits</li> </ul>"},{"location":"convex/tutorials/3_meta/#scenario-3-vehicle-routing-with-time-windows","title":"Scenario 3 \u2013 Vehicle Routing with Time Windows\u00b6","text":"<p>In this scenario, a fleet of $K$ vehicles is available at a central depot. Each vehicle has a fixed load capacity, and customer demands must be delivered without exceeding vehicle capacity constraints. Each customer must also be visited within a specified time window.</p> <p>The objective is to design a set of vehicle routes that minimizes total travel cost (or distance) while ensuring that all customer demands are satisfied and all operational constraints are respected.</p> <ul> <li>$K$ vehicles starting and ending at a central depot</li> <li>Capacity constraints: total demand served on each route must not exceed vehicle capacity</li> <li>Time window constraints on customer service times</li> </ul>"},{"location":"convex/tutorials/4_ga/","title":"IV \u2013 Cancer Gene Expression Feature Selection \u2013 Genetic Algorithm Case Study","text":"In\u00a0[11]: Copied! <pre>#!pip install sklearn-genetic-opt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n</pre> #!pip install sklearn-genetic-opt  import warnings warnings.filterwarnings(\"ignore\", category=FutureWarning)  In\u00a0[12]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nfrom sklearn.base import clone\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn_genetic import GAFeatureSelectionCV\nfrom sklearn.compose import ColumnTransformer\n\nSEED = 42\nnp.random.seed(SEED)\n</pre> import numpy as np import pandas as pd  from sklearn.base import clone from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.inspection import permutation_importance from sklearn.metrics import accuracy_score, f1_score from sklearn.linear_model import LogisticRegression from sklearn_genetic import GAFeatureSelectionCV from sklearn.compose import ColumnTransformer  SEED = 42 np.random.seed(SEED) In\u00a0[13]: Copied! <pre>df = pd.read_csv('data/brca_data_w_subtypes/brca_data_w_subtypes.csv')\n\nprint(\"Dataset shape:\", df.shape)\ndf.head()\n\ntarget_col = \"vital.status\"\nX = df.drop(columns=[target_col])\ny = df[target_col]\n\nprint(\"Unique values:\", y.unique())\nprint(\"Value counts:\\n\", y.value_counts())\n\nfeature_names = X.columns.tolist()\n</pre> df = pd.read_csv('data/brca_data_w_subtypes/brca_data_w_subtypes.csv')  print(\"Dataset shape:\", df.shape) df.head()  target_col = \"vital.status\" X = df.drop(columns=[target_col]) y = df[target_col]  print(\"Unique values:\", y.unique()) print(\"Value counts:\\n\", y.value_counts())  feature_names = X.columns.tolist() <pre>Dataset shape: (705, 1941)\nUnique values: [0 1]\nValue counts:\n vital.status\n0    611\n1     94\nName: count, dtype: int64\n</pre> In\u00a0[18]: Copied! <pre># Train / test split\n# Larger test split to stress generalization (common in genomics)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.8,\n    stratify=y,\n    random_state=42\n)\n\nprint(f\"Samples: {X.shape[0]}\")\nprint(f\"Genes: {X.shape[1]}\")\nprint(f\"Train samples: {X_train.shape[0]}\")\nprint(f\"Test samples: {X_test.shape[0]}\")\n</pre> # Train / test split # Larger test split to stress generalization (common in genomics) X_train, X_test, y_train, y_test = train_test_split(     X,     y,     test_size=0.8,     stratify=y,     random_state=42 )  print(f\"Samples: {X.shape[0]}\") print(f\"Genes: {X.shape[1]}\") print(f\"Train samples: {X_train.shape[0]}\") print(f\"Test samples: {X_test.shape[0]}\") <pre>Samples: 705\nGenes: 1940\nTrain samples: 141\nTest samples: 564\n</pre> In\u00a0[19]: Copied! <pre>num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\nprint(f\"Numeric features: {len(num_cols)}\")\nprint(f\"Categorical features: {len(cat_cols)}\")\n</pre> num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns cat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns print(f\"Numeric features: {len(num_cols)}\") print(f\"Categorical features: {len(cat_cols)}\") <pre>Numeric features: 1936\nCategorical features: 4\n</pre> In\u00a0[20]: Copied! <pre># preprocessing\npreprocess_only = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n# Fit transform to get numeric matrix\nX_train_enc = preprocess_only.fit_transform(X_train)\nX_test_enc  = preprocess_only.transform(X_test)\n\n# Get encoded feature names (for reporting)\nfeat_names = preprocess_only.get_feature_names_out()\n\n# Now GA runs on numeric encoded matrix\nlr = LogisticRegression( max_iter=10000,\n    solver=\"saga\",\n    class_weight=\"balanced\",\n    n_jobs=-1,\n    random_state=SEED)\n\nprint(f\"Total features: {len(feat_names)}\")\n</pre> # preprocessing preprocess_only = ColumnTransformer(     transformers=[         (\"num\", StandardScaler(), num_cols),         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),     ],     remainder=\"drop\" )  # Fit transform to get numeric matrix X_train_enc = preprocess_only.fit_transform(X_train) X_test_enc  = preprocess_only.transform(X_test)  # Get encoded feature names (for reporting) feat_names = preprocess_only.get_feature_names_out()  # Now GA runs on numeric encoded matrix lr = LogisticRegression( max_iter=10000,     solver=\"saga\",     class_weight=\"balanced\",     n_jobs=-1,     random_state=SEED)  print(f\"Total features: {len(feat_names)}\")  <pre>Total features: 1953\n</pre> In\u00a0[21]: Copied! <pre>cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n</pre> cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED) In\u00a0[22]: Copied! <pre># Helper function: train/eval logistic regression on a given set of columns\ndef eval_logreg(col_idx, label, model=lr, scoring_cv=\"f1\"):\n    \"\"\"\n    col_idx: list/array of integer feature indices OR boolean mask\n    \"\"\"\n\n    m = clone(model)\n\n    Xtr = X_train_enc[:, col_idx]\n    Xte = X_test_enc[:, col_idx]\n\n    # Fit\n    m.fit(Xtr, y_train)\n\n    # Test metrics\n    y_pred = m.predict(Xte)\n    test_acc = accuracy_score(y_test, y_pred)\n    test_f1  = f1_score(y_test, y_pred)\n\n    # CV on training\n    cv_score = cross_val_score(m, Xtr, y_train, cv=cv, scoring=scoring_cv).mean()\n\n    return {\n        \"Approach\": label,\n        \"NumFeatures\": Xtr.shape[1],\n        f\"CV_{scoring_cv}_mean(train)\": cv_score,\n        \"Test_Accuracy\": test_acc,\n        \"Test_F1\": test_f1,\n    }\n\n\nresults = []\n</pre> # Helper function: train/eval logistic regression on a given set of columns def eval_logreg(col_idx, label, model=lr, scoring_cv=\"f1\"):     \"\"\"     col_idx: list/array of integer feature indices OR boolean mask     \"\"\"      m = clone(model)      Xtr = X_train_enc[:, col_idx]     Xte = X_test_enc[:, col_idx]      # Fit     m.fit(Xtr, y_train)      # Test metrics     y_pred = m.predict(Xte)     test_acc = accuracy_score(y_test, y_pred)     test_f1  = f1_score(y_test, y_pred)      # CV on training     cv_score = cross_val_score(m, Xtr, y_train, cv=cv, scoring=scoring_cv).mean()      return {         \"Approach\": label,         \"NumFeatures\": Xtr.shape[1],         f\"CV_{scoring_cv}_mean(train)\": cv_score,         \"Test_Accuracy\": test_acc,         \"Test_F1\": test_f1,     }   results = []  In\u00a0[23]: Copied! <pre>## Baseline - all features\nall_idx = np.arange(X_train_enc.shape[1])\nresults.append(eval_logreg(all_idx, \"All features (LogReg)\"))\n</pre> ## Baseline - all features all_idx = np.arange(X_train_enc.shape[1]) results.append(eval_logreg(all_idx, \"All features (LogReg)\")) In\u00a0[26]: Copied! <pre>results\n</pre> results Out[26]: <pre>[{'Approach': 'All features (LogReg)',\n  'NumFeatures': 1953,\n  'CV_f1_mean(train)': 0.22952958152958153,\n  'Test_Accuracy': 0.7890070921985816,\n  'Test_F1': 0.2874251497005988}]</pre> <p>The genetic algorithm maintains a population of candidate feature subsets and iteratively evolves them through selection, crossover, and mutation. Population size determines the diversity of candidate solutions explored in each generation, the number of generations controls the depth of evolutionary search, and elitism ensures that high-performing feature subsets are preserved across generations. A sparsity constraint limits the maximum number of selected features, guiding the search toward compact and generalizable solutions. Together, these parameters balance exploration and exploitation in a highly non-convex combinatorial search space.</p> In\u00a0[\u00a0]: Copied! <pre>ga_selector = GAFeatureSelectionCV(\n    estimator=lr,              # LogisticRegression(...)\n    cv=cv,\n    scoring=\"average_precision\",              \n    population_size=100,\n    generations=100,\n    keep_top_k=5,\n    max_features=1000,           \n    verbose=False,\n    n_jobs=1,\n)\n\n\nga_selector.fit(X_train_enc, y_train)\nga_mask = np.array(ga_selector.best_features_, dtype=bool)\nga_idx = np.where(ga_mask)[0]\n\nga_cols = list(np.array(feat_names)[ga_idx])\n\nprint(f\"Selected {len(ga_cols)} encoded features\")\nprint(ga_cols[:20])\n</pre> ga_selector = GAFeatureSelectionCV(     estimator=lr,              # LogisticRegression(...)     cv=cv,     scoring=\"average_precision\",                   population_size=100,     generations=100,     keep_top_k=5,     max_features=1000,                verbose=False,     n_jobs=1, )   ga_selector.fit(X_train_enc, y_train) ga_mask = np.array(ga_selector.best_features_, dtype=bool) ga_idx = np.where(ga_mask)[0]  ga_cols = list(np.array(feat_names)[ga_idx])  print(f\"Selected {len(ga_cols)} encoded features\") print(ga_cols[:20]) In\u00a0[\u00a0]: Copied! <pre>results.append(eval_logreg(ga_idx, f\"GA-selected (n={len(ga_idx)})\", scoring_cv=\"f1\"))\n</pre> results.append(eval_logreg(ga_idx, f\"GA-selected (n={len(ga_idx)})\", scoring_cv=\"f1\")) In\u00a0[\u00a0]: Copied! <pre>results\n</pre> results Out[\u00a0]: <pre>[{'Approach': 'All features (LogReg)',\n  'NumFeatures': 1955,\n  'CV_f1_mean(train)': 0.27673350041771094,\n  'Test_Accuracy': 0.8632075471698113,\n  'Test_F1': 0.32558139534883723},\n {'Approach': 'GA-selected (n=48)',\n  'NumFeatures': 48,\n  'CV_f1_mean(train)': 0.2914684603471789,\n  'Test_Accuracy': 0.8584905660377359,\n  'Test_F1': 0.21052631578947367},\n {'Approach': 'GA-selected (n=88)',\n  'NumFeatures': 88,\n  'CV_f1_mean(train)': 0.40913804713804713,\n  'Test_Accuracy': 0.839622641509434,\n  'Test_F1': 0.2608695652173913}]</pre>"},{"location":"convex/tutorials/4_ga/#iv-cancer-gene-expression-feature-selection-genetic-algorithm-case-study","title":"IV \u2013 Cancer Gene Expression Feature Selection \u2013 Genetic Algorithm Case Study\u00b6","text":"<p>Genetic algorithms (GAs) are population-based evolutionary optimization methods inspired by the principles of natural selection. They iteratively evolve a set of candidate solutions through selection, crossover, and mutation, enabling effective exploration of large and complex search spaces. GAs are particularly well suited for non-convex, discrete, and combinatorial optimization problems, where the objective function is non-differentiable and characterized by many local optima. In this case study, we apply a GA to the problem of gene selection from high-dimensional cancer gene expression data, with the goal of identifying a compact subset of genes that maximizes predictive performance.</p> <p>Gene expression datasets are a canonical example of a challenging optimization problem in biomedical machine learning. They typically contain thousands to tens of thousands of gene expression measurements for only a few hundred or thousand samples, resulting in an extreme high-dimensional, low-sample-size regime. Each sample corresponds to a patient, and the target label represents a clinical outcome such as cancer subtype or disease status. Predictive signal in such data rarely resides in individual genes; instead, it emerges from complex, nonlinear interactions among small groups of genes, often reflecting underlying biological pathways.</p>"},{"location":"convex/tutorials/4_ga/#genetic-algorithm-formulation","title":"Genetic Algorithm Formulation\u00b6","text":"<ul> <li><p>Variables: Each candidate solution is represented as a binary chromosome of length d, where d is the number of genes. A value of 1 indicates that the corresponding gene is included in the feature subset, while 0 indicates exclusion.</p> </li> <li><p>Objective: Maximize classifier accuracy on held-out validation folds. In our GA, the fitness of a mask is the cross-validated accuracy of a logistic regression model trained on the selected features.</p> </li> <li><p>Constraints: To encourage interpretability and biological plausibility, we optionally impose a maximum number of selected genes (e.g., 30\u201350). The GA naturally handles this discrete constraint without requiring relaxation or approximation.</p> </li> <li><p>GA Steps: An initial population of random masks is generated. In each generation, masks are evaluated (fitness), then the best are selected for reproduction. New offspring are created via crossover (combining bits from two parents) and mutation (flipping bits) to introduce diversity. Elite individuals may be carried over. This process repeats for many generations, gradually improving the feature subset.</p> </li> </ul>"},{"location":"convex/tutorials/4_ga/#implementation","title":"Implementation\u00b6","text":"<p>We use the open-source Python library sklearn-genetic-opt (which is built on DEAP) to handle the GA mechanics. This library implements GAFeatureSelectionCV, which wraps scikit-learn estimators in a GA that optimizes cross-validation score while minimizing feature count.</p>"},{"location":"convex/tutorials/4_ga/#why-ga-for-feature-selection","title":"Why GA for feature selection?\u00b6","text":"<p>GA is particularly well suited for this problem because feature selection is a non-convex, combinatorial optimization task with an exponentially large search space ($2^d$ possible subsets). Ensemble-based permutation feature importance methods (e.g., random permutation in random forests or gradient-boosted trees) evaluate features largely in isolation or under conditional perturbations, implicitly assuming that the contribution of each feature can be separated from the rest. This assumption fails in the presence of feature interactions, redundancy, and multicollinearity, where the predictive power emerges only from specific combinations of features rather than individual ones.</p> <p>More importantly, permutation importance is diagnostic rather than optimization-driven: it explains a trained ensemble model but does not directly solve a well-defined optimization problem. Selecting features based on importance thresholds is heuristic and does not guarantee near-optimal performance for a downstream model. In contrast, a GA directly optimizes the target objective (e.g., cross-validated accuracy with a sparsity constraint) by evaluating entire feature subsets at once. By maintaining a population of candidate solutions and using crossover and mutation, GAs explore multiple regions of the non-convex landscape simultaneously and are far less prone to getting trapped in poor local optima. From an optimization standpoint, GAs are explicitly designed for discrete, non-differentiable, and non-convex problems, making them a principled and academically appropriate choice when the goal is global subset discovery rather than feature ranking.</p>"},{"location":"convex/tutorials/4_ga/#just-700-sample-and-2000-features","title":"Just ~700 Sample and ~2000 features!!\u00b6","text":""},{"location":"convex/tutorials/5_cvar_portfolio/","title":"V. Tail-Risk Portfolio Optimization.","text":"In\u00a0[34]: Copied! <pre># (Optional) Install dependencies if needed\n# !pip install -q yfinance pandas_datareader cvxpy\n\n# If your environment lacks an LP solver, you can also install one of:\n# !pip install -q ecos\n# !pip install -q osqp\n# !pip install -q clarabel\n</pre> # (Optional) Install dependencies if needed # !pip install -q yfinance pandas_datareader cvxpy  # If your environment lacks an LP solver, you can also install one of: # !pip install -q ecos # !pip install -q osqp # !pip install -q clarabel  In\u00a0[35]: Copied! <pre># Install and import necessary libraries\n#!pip install yfinance cvxpy pandas numpy matplotlib seaborn\nimport pandas as pd\nimport numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_datareader import data as pdr\nfrom pandas_datareader.stooq import StooqDailyReader\n</pre> # Install and import necessary libraries #!pip install yfinance cvxpy pandas numpy matplotlib seaborn import pandas as pd import numpy as np import cvxpy as cp import matplotlib.pyplot as plt import seaborn as sns from pandas_datareader import data as pdr from pandas_datareader.stooq import StooqDailyReader   In\u00a0[\u00a0]: Copied! <pre>tickers_dict = {\n    # Broad Equity\n    \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",\n    \"VTI.US\": \"Total U.S. stock market\",\n\n    # Big Tech / Mega-Cap Equities\n    \"AAPL.US\": \"Apple Inc.\",\n    \"MSFT.US\": \"Microsoft Corp.\",\n    \"GOOGL.US\": \"Alphabet Inc.\",\n    \"AMZN.US\": \"Amazon.com Inc.\",\n    \"TSLA.US\": \"Tesla Inc.\",\n    \"NVDA.US\": \"NVIDIA Corp.\",\n    \"META.US\": \"Meta Platforms Inc.\",\n    \"JPM.US\": \"JPMorgan Chase &amp; Co.\",\n    \"JNJ.US\": \"Johnson &amp; Johnson\",\n    \"V.US\": \"Visa Inc.\",\n    \"WMT.US\": \"Walmart Inc.\",\n    \"PG.US\": \"Procter &amp; Gamble Co.\",\n    \"UNH.US\": \"UnitedHealth Group Inc.\",\n    \"HD.US\": \"Home Depot Inc.\",\n    \"MA.US\": \"Mastercard Inc.\",\n    \"DIS.US\": \"The Walt Disney Co.\",\n    \"BAC.US\": \"Bank of America Corp.\",\n    \"XOM.US\": \"Exxon Mobil Corp.\",\n    \"PFE.US\": \"Pfizer Inc.\",\n    \"KO.US\": \"Coca-Cola Co.\",\n    \"VZ.US\": \"Verizon Communications Inc.\",\n    \"ADBE.US\": \"Adobe Inc.\",\n    \"CMCSA.US\": \"Comcast Corp.\",\n    \"NFLX.US\": \"Netflix Inc.\",\n    \"T.US\": \"AT&amp;T Inc.\",\n    \"CSCO.US\": \"Cisco Systems Inc.\",\n    \"PEP.US\": \"PepsiCo Inc.\",\n    \"INTC.US\": \"Intel Corp.\",\n    \"CRM.US\": \"Salesforce Inc.\",\n    \"ABNB.US\": \"Airbnb Inc.\",\n    \"PYPL.US\": \"PayPal Holdings Inc.\",\n    \"AVGO.US\": \"Broadcom Inc.\",\n    \"COST.US\": \"Costco Wholesale Corp.\",\n    \"QCOM.US\": \"Qualcomm Inc.\",\n    \"TXN.US\": \"Texas Instruments Inc.\",\n    \"AMD.US\": \"Advanced Micro Devices Inc.\",\n    \"ORCL.US\": \"Oracle Corp.\",\n    \"NKE.US\": \"Nike Inc.\",\n    \"SBUX.US\": \"Starbucks Corp.\",\n    \"CVX.US\": \"Chevron Corp.\",\n    \"BMY.US\": \"Bristol-Myers Squibb Co.\",\n    \"MDLZ.US\": \"Mondelez International Inc.\",\n\n    # Sector-Specific Equities\n    \"XLK.US\": \"Technology sector\",\n    \"XLF.US\": \"Financials sector\",\n    \"XLE.US\": \"Energy sector\",\n    \"XLV.US\": \"Healthcare sector\",\n    \"XLY.US\": \"Consumer Discretionary sector\",\n    \"XLI.US\": \"Industrials sector\",\n    \"XLRE.US\": \"Real Estate sector\",\n\n    # Size / Style\n    \"IWM.US\": \"Russell 2000 ETF\",\n    \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",\n\n    # International Equity\n    \"EFA.US\": \"Developed markets (ex-US)\",\n    \"EEM.US\": \"Emerging markets\",\n    \"VXUS.US\": \"Total International equity\",\n\n    # Government Bonds\n    \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",\n    \"TLT.US\": \"U.S. Treasuries 20+Y\",\n    \"SHY.US\": \"Short-term Treasuries\",\n\n    # Corporate / High Yield\n    \"LQD.US\": \"Investment-grade corporate bonds\",\n    \"HYG.US\": \"High-yield corporate bonds\",\n\n    # Commodities\n    \"GLD.US\": \"Gold\",\n    \"SLV.US\": \"Silver\",\n    \"DBC.US\": \"Broad commodities\",\n    \"USO.US\": \"Crude Oil\",\n    \"UNG.US\": \"Natural Gas\",\n\n    # Alternative / Real Assets\n    \"VNQ.US\": \"U.S. Real Estate (REITs)\",\n    \"ICLN.US\": \"Global Clean Energy\"\n\n    # Cryptocurrency\n    #\"BTC-USD\": \"Bitcoin (USD-denominated)\"\n}\ntickers = list(tickers_dict.keys())\n\nprint(len(tickers))\n\nprint(len(tickers))\n</pre> tickers_dict = {     # Broad Equity     \"SPY.US\": \"U.S. broad-market equity (S&amp;P 500)\",     \"VTI.US\": \"Total U.S. stock market\",      # Big Tech / Mega-Cap Equities     \"AAPL.US\": \"Apple Inc.\",     \"MSFT.US\": \"Microsoft Corp.\",     \"GOOGL.US\": \"Alphabet Inc.\",     \"AMZN.US\": \"Amazon.com Inc.\",     \"TSLA.US\": \"Tesla Inc.\",     \"NVDA.US\": \"NVIDIA Corp.\",     \"META.US\": \"Meta Platforms Inc.\",     \"JPM.US\": \"JPMorgan Chase &amp; Co.\",     \"JNJ.US\": \"Johnson &amp; Johnson\",     \"V.US\": \"Visa Inc.\",     \"WMT.US\": \"Walmart Inc.\",     \"PG.US\": \"Procter &amp; Gamble Co.\",     \"UNH.US\": \"UnitedHealth Group Inc.\",     \"HD.US\": \"Home Depot Inc.\",     \"MA.US\": \"Mastercard Inc.\",     \"DIS.US\": \"The Walt Disney Co.\",     \"BAC.US\": \"Bank of America Corp.\",     \"XOM.US\": \"Exxon Mobil Corp.\",     \"PFE.US\": \"Pfizer Inc.\",     \"KO.US\": \"Coca-Cola Co.\",     \"VZ.US\": \"Verizon Communications Inc.\",     \"ADBE.US\": \"Adobe Inc.\",     \"CMCSA.US\": \"Comcast Corp.\",     \"NFLX.US\": \"Netflix Inc.\",     \"T.US\": \"AT&amp;T Inc.\",     \"CSCO.US\": \"Cisco Systems Inc.\",     \"PEP.US\": \"PepsiCo Inc.\",     \"INTC.US\": \"Intel Corp.\",     \"CRM.US\": \"Salesforce Inc.\",     \"ABNB.US\": \"Airbnb Inc.\",     \"PYPL.US\": \"PayPal Holdings Inc.\",     \"AVGO.US\": \"Broadcom Inc.\",     \"COST.US\": \"Costco Wholesale Corp.\",     \"QCOM.US\": \"Qualcomm Inc.\",     \"TXN.US\": \"Texas Instruments Inc.\",     \"AMD.US\": \"Advanced Micro Devices Inc.\",     \"ORCL.US\": \"Oracle Corp.\",     \"NKE.US\": \"Nike Inc.\",     \"SBUX.US\": \"Starbucks Corp.\",     \"CVX.US\": \"Chevron Corp.\",     \"BMY.US\": \"Bristol-Myers Squibb Co.\",     \"MDLZ.US\": \"Mondelez International Inc.\",      # Sector-Specific Equities     \"XLK.US\": \"Technology sector\",     \"XLF.US\": \"Financials sector\",     \"XLE.US\": \"Energy sector\",     \"XLV.US\": \"Healthcare sector\",     \"XLY.US\": \"Consumer Discretionary sector\",     \"XLI.US\": \"Industrials sector\",     \"XLRE.US\": \"Real Estate sector\",      # Size / Style     \"IWM.US\": \"Russell 2000 ETF\",     \"QQQ.US\": \"NASDAQ 100 ETF (tech-heavy)\",      # International Equity     \"EFA.US\": \"Developed markets (ex-US)\",     \"EEM.US\": \"Emerging markets\",     \"VXUS.US\": \"Total International equity\",      # Government Bonds     \"IEF.US\": \"U.S. Treasuries 7\u201310Y\",     \"TLT.US\": \"U.S. Treasuries 20+Y\",     \"SHY.US\": \"Short-term Treasuries\",      # Corporate / High Yield     \"LQD.US\": \"Investment-grade corporate bonds\",     \"HYG.US\": \"High-yield corporate bonds\",      # Commodities     \"GLD.US\": \"Gold\",     \"SLV.US\": \"Silver\",     \"DBC.US\": \"Broad commodities\",     \"USO.US\": \"Crude Oil\",     \"UNG.US\": \"Natural Gas\",      # Alternative / Real Assets     \"VNQ.US\": \"U.S. Real Estate (REITs)\",     \"ICLN.US\": \"Global Clean Energy\"      # Cryptocurrency     #\"BTC-USD\": \"Bitcoin (USD-denominated)\" } tickers = list(tickers_dict.keys())  print(len(tickers))  print(len(tickers)) <pre>69\n69\n</pre> In\u00a0[37]: Copied! <pre>#tickers = [\"SPY.US\", \"AAPL.US\", \"MSFT.US\"]\n</pre> #tickers = [\"SPY.US\", \"AAPL.US\", \"MSFT.US\"] In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[38]: Copied! <pre># Define asset tickers and download data \nstart_date = \"2023-01-01\"\nend_date = \"2025-12-24\"\n\nprices = pd.DataFrame()\n\n\nfor ticker in tickers:\n    print(f\"Downloading data for {ticker}...\")\n\n    try:\n        #data = StooqDailyReader(ticker, start=start_date, end=end_date).read()\n        #prices[ticker] = data[\"Close\"]\n        data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)\n        prices[ticker] = data[\"Close\"]\n\n    except Exception as e:\n        print(f\"Failed to download data for {ticker}: {e}\")\nprices = prices.sort_index()\nprices.tail()\n</pre> # Define asset tickers and download data  start_date = \"2023-01-01\" end_date = \"2025-12-24\"  prices = pd.DataFrame()   for ticker in tickers:     print(f\"Downloading data for {ticker}...\")      try:         #data = StooqDailyReader(ticker, start=start_date, end=end_date).read()         #prices[ticker] = data[\"Close\"]         data = pdr.DataReader(ticker, \"stooq\", start_date, end_date)         prices[ticker] = data[\"Close\"]      except Exception as e:         print(f\"Failed to download data for {ticker}: {e}\") prices = prices.sort_index() prices.tail()  <pre>Downloading data for SPY.US...\nDownloading data for VTI.US...\nDownloading data for AAPL.US...\nDownloading data for MSFT.US...\nDownloading data for GOOGL.US...\nDownloading data for AMZN.US...\nDownloading data for TSLA.US...\nDownloading data for NVDA.US...\nDownloading data for META.US...\nDownloading data for BRK.B.US...\nFailed to download data for BRK.B.US: 'Close'\nDownloading data for JPM.US...\nDownloading data for JNJ.US...\nDownloading data for V.US...\nDownloading data for WMT.US...\nDownloading data for PG.US...\nDownloading data for UNH.US...\nDownloading data for HD.US...\nDownloading data for MA.US...\nDownloading data for DIS.US...\nDownloading data for BAC.US...\nDownloading data for XOM.US...\nDownloading data for PFE.US...\nDownloading data for KO.US...\nDownloading data for VZ.US...\nDownloading data for ADBE.US...\nDownloading data for CMCSA.US...\nDownloading data for NFLX.US...\nDownloading data for T.US...\nDownloading data for CSCO.US...\nDownloading data for PEP.US...\nDownloading data for INTC.US...\nDownloading data for CRM.US...\nDownloading data for ABNB.US...\nDownloading data for PYPL.US...\nDownloading data for AVGO.US...\nDownloading data for COST.US...\nDownloading data for QCOM.US...\nDownloading data for TXN.US...\nDownloading data for AMD.US...\nDownloading data for ORCL.US...\nDownloading data for NKE.US...\nDownloading data for SBUX.US...\nDownloading data for CVX.US...\nDownloading data for BMY.US...\nDownloading data for MDLZ.US...\nDownloading data for XLK.US...\nDownloading data for XLF.US...\nDownloading data for XLE.US...\nDownloading data for XLV.US...\nDownloading data for XLY.US...\nDownloading data for XLI.US...\nDownloading data for XLRE.US...\nDownloading data for IWM.US...\nDownloading data for QQQ.US...\nDownloading data for EFA.US...\nDownloading data for EEM.US...\nDownloading data for VXUS.US...\nDownloading data for IEF.US...\nDownloading data for TLT.US...\nDownloading data for SHY.US...\nDownloading data for LQD.US...\nDownloading data for HYG.US...\nDownloading data for GLD.US...\nDownloading data for SLV.US...\nDownloading data for DBC.US...\nDownloading data for USO.US...\nDownloading data for UNG.US...\nDownloading data for VNQ.US...\nDownloading data for ICLN.US...\n</pre> Out[38]: SPY.US VTI.US AAPL.US MSFT.US GOOGL.US AMZN.US TSLA.US NVDA.US META.US JPM.US ... SHY.US LQD.US HYG.US GLD.US SLV.US DBC.US USO.US UNG.US VNQ.US ICLN.US Date 2025-12-18 676.47 333.25 272.19 483.98 302.46 226.76 483.37 174.14 664.45 313.00 ... 83.03 110.85 80.78 398.57 59.32 22.69 67.19 12.03 88.93 16.18 2025-12-19 680.59 336.22 273.67 485.92 307.16 227.35 481.20 180.99 658.77 317.21 ... 82.76 110.13 80.36 399.02 60.93 22.85 68.03 12.19 88.59 16.44 2025-12-22 684.83 337.60 270.97 484.92 309.78 228.43 488.73 183.69 661.50 323.09 ... 82.72 110.11 80.43 408.23 62.47 22.39 69.73 11.85 88.24 16.65 2025-12-23 687.96 338.72 272.36 486.85 314.35 232.14 485.56 189.21 664.94 325.93 ... 82.68 110.22 80.49 413.64 64.84 22.64 70.30 12.90 88.18 16.54 2025-12-24 690.38 339.88 273.81 488.02 314.09 232.38 485.40 188.61 667.55 329.17 ... 82.73 110.65 80.64 411.93 65.22 22.63 70.20 12.39 88.76 16.58 <p>5 rows \u00d7 68 columns</p> In\u00a0[39]: Copied! <pre># Daily simple returns\nreturns = prices.pct_change().dropna()\n\n# Train/test split (e.g. 70/30)\nsplit = int(0.7 * len(returns))\nR_train = returns.iloc[:split].copy()\nR_test  = returns.iloc[split:].copy()\n\nprint(\"Train days:\", len(R_train), \"Test days:\", len(R_test))\nreturns.describe()\n</pre> # Daily simple returns returns = prices.pct_change().dropna()  # Train/test split (e.g. 70/30) split = int(0.7 * len(returns)) R_train = returns.iloc[:split].copy() R_test  = returns.iloc[split:].copy()  print(\"Train days:\", len(R_train), \"Test days:\", len(R_test)) returns.describe()  <pre>C:\\Users\\salmank\\AppData\\Local\\Temp\\ipykernel_15256\\3148299028.py:2: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  returns = prices.pct_change().dropna()\n</pre> <pre>Train days: 522 Test days: 225\n</pre> Out[39]: SPY.US VTI.US AAPL.US MSFT.US GOOGL.US AMZN.US TSLA.US NVDA.US META.US JPM.US ... SHY.US LQD.US HYG.US GLD.US SLV.US DBC.US USO.US UNG.US VNQ.US ICLN.US count 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 ... 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 747.000000 mean 0.000885 0.000824 0.001194 0.001081 0.001875 0.001537 0.002721 0.003953 0.002528 0.001299 ... 0.000025 0.000068 0.000127 0.001229 0.001614 -0.000035 0.000224 -0.001209 0.000161 -0.000111 std 0.009688 0.009829 0.016162 0.014655 0.019125 0.020169 0.037896 0.031712 0.024087 0.014613 ... 0.001440 0.005060 0.003890 0.010174 0.017981 0.009729 0.018637 0.037059 0.011284 0.015196 min -0.058543 -0.058683 -0.092456 -0.061809 -0.095085 -0.089791 -0.154262 -0.169682 -0.113348 -0.080502 ... -0.005380 -0.019489 -0.016328 -0.064269 -0.082355 -0.049955 -0.080727 -0.152493 -0.043399 -0.073333 25% -0.003385 -0.003649 -0.006667 -0.006762 -0.008756 -0.009610 -0.018546 -0.013673 -0.009695 -0.005433 ... -0.000615 -0.002874 -0.001874 -0.004473 -0.008661 -0.005594 -0.011671 -0.027269 -0.006174 -0.009618 50% 0.001092 0.001029 0.001498 0.001270 0.002540 0.000909 0.001628 0.003672 0.001393 0.001856 ... 0.000121 0.000091 0.000250 0.001122 0.001030 0.000444 0.000393 -0.001577 0.000204 -0.000626 75% 0.005975 0.006249 0.008772 0.009165 0.011635 0.013333 0.022602 0.021652 0.013098 0.008802 ... 0.000731 0.003190 0.002280 0.007078 0.011803 0.005966 0.012552 0.023378 0.006585 0.009261 max 0.105019 0.101456 0.153288 0.101337 0.102241 0.119770 0.226900 0.243701 0.232824 0.115445 ... 0.009974 0.016769 0.026799 0.036991 0.063932 0.036210 0.068887 0.144036 0.059657 0.075327 <p>8 rows \u00d7 68 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[40]: Copied! <pre>R_train.isnull().sum().sum()\n</pre> R_train.isnull().sum().sum() Out[40]: <pre>0</pre> In\u00a0[41]: Copied! <pre>alpha = 0.9  # tail confidence level\nN, n = R_train.shape\n\n# Scenario loss matrix (N x n) for CVaR constraints\n# L_j = - w^T r_j\nR = R_train.values\n\n# Decision variables\nw = cp.Variable(n)\ngamma = cp.Variable()        # VaR-like threshold\nz = cp.Variable(N)           # tail slack variables\n\n# Optional: minimum expected daily return constraint\nmu = R_train.mean().values\n\n# --- Convert annualized expected return to daily ---\n# If returns are arithmetic daily returns, a common conversion is:\n# (1 + r_annual)^(1/252) - 1\nr_target_annual = 0.10\ntrading_days = 252\nrho_target = (1 + r_target_annual)**(1 / trading_days) - 1\n\n#rho_target = 10/(100*252) # a mild target, adaptive to data scale\nprint(f\"Using rho_target = {rho_target:.6f} (~{100*r_target_annual}% annualized approx)\")\n# You can also set something like rho_target = 0.0002 (~5% annualized approx) as a fixed target.\n\nconstraints = [\n    cp.sum(w) == 1,\n    w &gt;= 0,\n    z &gt;= 0,\n    z &gt;= -(R @ w) - gamma,          # z_j &gt;= L_j - gamma\n    mu @ w &gt;= rho_target            # expected return constraint (optional but useful)\n]\n\nobjective = cp.Minimize(gamma + (1 / ((1 - alpha) * N)) * cp.sum(z))\nprob = cp.Problem(objective, constraints)\n\n# Solve\nresult = prob.solve(solver=cp.ECOS, verbose=False)\n\nprint(\"Status:\", prob.status)\nprint(\"Optimal objective (CVaR proxy):\", result)\n</pre> alpha = 0.9  # tail confidence level N, n = R_train.shape  # Scenario loss matrix (N x n) for CVaR constraints # L_j = - w^T r_j R = R_train.values  # Decision variables w = cp.Variable(n) gamma = cp.Variable()        # VaR-like threshold z = cp.Variable(N)           # tail slack variables  # Optional: minimum expected daily return constraint mu = R_train.mean().values  # --- Convert annualized expected return to daily --- # If returns are arithmetic daily returns, a common conversion is: # (1 + r_annual)^(1/252) - 1 r_target_annual = 0.10 trading_days = 252 rho_target = (1 + r_target_annual)**(1 / trading_days) - 1  #rho_target = 10/(100*252) # a mild target, adaptive to data scale print(f\"Using rho_target = {rho_target:.6f} (~{100*r_target_annual}% annualized approx)\") # You can also set something like rho_target = 0.0002 (~5% annualized approx) as a fixed target.  constraints = [     cp.sum(w) == 1,     w &gt;= 0,     z &gt;= 0,     z &gt;= -(R @ w) - gamma,          # z_j &gt;= L_j - gamma     mu @ w &gt;= rho_target            # expected return constraint (optional but useful) ]  objective = cp.Minimize(gamma + (1 / ((1 - alpha) * N)) * cp.sum(z)) prob = cp.Problem(objective, constraints)  # Solve result = prob.solve(solver=cp.ECOS, verbose=False)  print(\"Status:\", prob.status) print(\"Optimal objective (CVaR proxy):\", result)  <pre>Using rho_target = 0.000378 (~10.0% annualized approx)\nStatus: optimal\nOptimal objective (CVaR proxy): 0.002875933039936678\n</pre> In\u00a0[42]: Copied! <pre>weights_cvar = pd.Series(w.value, index=R_train.columns).sort_values(ascending=False)\nweights_cvar\n</pre> weights_cvar = pd.Series(w.value, index=R_train.columns).sort_values(ascending=False) weights_cvar Out[42]: <pre>SHY.US     7.991167e-01\nWMT.US     4.165347e-02\nMETA.US    3.078826e-02\nJPM.US     3.008830e-02\nV.US       2.925812e-02\n               ...     \nAMD.US     1.997300e-16\nADBE.US    1.490573e-16\nUNG.US     1.171536e-16\nINTC.US    8.628544e-17\nICLN.US   -5.853315e-18\nLength: 68, dtype: float64</pre> In\u00a0[43]: Copied! <pre># --- Estimates from training data (daily returns) ---\nmu = R_train.mean().values          # daily mean returns (n,)\nSigma = R_train.cov().values        # daily covariance (n,n)\n\nn = len(returns.columns)\nw = cp.Variable(n)\n\n\n# --- Min-variance portfolio with target return constraint ---\nobjective = cp.Minimize(cp.quad_form(w, Sigma))\n\nconstraints = [\n    cp.sum(w) == 1,           # fully invested\n    w &gt;= 0,                   # long-only\n    w &lt;= 0.25,                # max 25% per asset\n    mu @ w &gt;= rho_target  # target expected return (daily)\n]\n\nprob = cp.Problem(objective, constraints)\nprob.solve(solver=cp.OSQP)    # OSQP is a good default for QPs\n\nprint(\"Status:\", prob.status)\nprint(\"Target daily return:\", rho_target)\n\nweights_baseline = w.value\nweights_b = pd.Series(weights_baseline, index=R_train.columns).sort_values(ascending=False)\nweights_b\n</pre> # --- Estimates from training data (daily returns) --- mu = R_train.mean().values          # daily mean returns (n,) Sigma = R_train.cov().values        # daily covariance (n,n)  n = len(returns.columns) w = cp.Variable(n)   # --- Min-variance portfolio with target return constraint --- objective = cp.Minimize(cp.quad_form(w, Sigma))  constraints = [     cp.sum(w) == 1,           # fully invested     w &gt;= 0,                   # long-only     w &lt;= 0.25,                # max 25% per asset     mu @ w &gt;= rho_target  # target expected return (daily) ]  prob = cp.Problem(objective, constraints) prob.solve(solver=cp.OSQP)    # OSQP is a good default for QPs  print(\"Status:\", prob.status) print(\"Target daily return:\", rho_target)  weights_baseline = w.value weights_b = pd.Series(weights_baseline, index=R_train.columns).sort_values(ascending=False) weights_b  <pre>Status: optimal\nTarget daily return: 0.0003782865315342665\n</pre> Out[43]: <pre>HYG.US     2.500000e-01\nSHY.US     2.500000e-01\nIEF.US     1.593812e-01\nWMT.US     7.535102e-02\nGLD.US     4.291713e-02\n               ...     \nAAPL.US   -8.946613e-18\nNFLX.US   -9.181773e-18\nHD.US     -1.484294e-17\nLQD.US    -3.260698e-17\nTLT.US    -5.775274e-17\nLength: 68, dtype: float64</pre> In\u00a0[44]: Copied! <pre>def portfolio_returns(R_df: pd.DataFrame, w: pd.Series) -&gt; pd.Series:\n    w = w.reindex(R_df.columns).fillna(0.0)\n    return (R_df * w.values).sum(axis=1)\n\n# Test portfolio returns\np_cvar_train = portfolio_returns(R_train, weights_cvar)\np_b_train   = portfolio_returns(R_train, weights_b)\n\np_cvar = portfolio_returns(R_test, weights_cvar)\np_b   = portfolio_returns(R_test, weights_b)\n\ndef var_cvar(x: pd.Series, alpha=0.95):\n    # x is returns; losses are -x\n    losses = -x.values\n    var = np.quantile(losses, alpha)\n    cvar = losses[losses &gt;= var].mean()\n    return var, cvar\n\nvar_cvar_cvar = var_cvar(p_cvar, alpha=alpha)\nvar_cvar_b   = var_cvar(p_b, alpha=alpha)\n\nprint(f\"Test VaR@{alpha:.2f} (CVaR-opt): {var_cvar_cvar[0]:.4f} | Test CVaR@{alpha:.2f}: {var_cvar_cvar[1]:.4f}\")\nprint(f\"Test VaR@{alpha:.2f} (Equal-wt): {var_cvar_b[0]:.4f} | Test CVaR@{alpha:.2f}: {var_cvar_b[1]:.4f}\")\n</pre> def portfolio_returns(R_df: pd.DataFrame, w: pd.Series) -&gt; pd.Series:     w = w.reindex(R_df.columns).fillna(0.0)     return (R_df * w.values).sum(axis=1)  # Test portfolio returns p_cvar_train = portfolio_returns(R_train, weights_cvar) p_b_train   = portfolio_returns(R_train, weights_b)  p_cvar = portfolio_returns(R_test, weights_cvar) p_b   = portfolio_returns(R_test, weights_b)  def var_cvar(x: pd.Series, alpha=0.95):     # x is returns; losses are -x     losses = -x.values     var = np.quantile(losses, alpha)     cvar = losses[losses &gt;= var].mean()     return var, cvar  var_cvar_cvar = var_cvar(p_cvar, alpha=alpha) var_cvar_b   = var_cvar(p_b, alpha=alpha)  print(f\"Test VaR@{alpha:.2f} (CVaR-opt): {var_cvar_cvar[0]:.4f} | Test CVaR@{alpha:.2f}: {var_cvar_cvar[1]:.4f}\") print(f\"Test VaR@{alpha:.2f} (Equal-wt): {var_cvar_b[0]:.4f} | Test CVaR@{alpha:.2f}: {var_cvar_b[1]:.4f}\") <pre>Test VaR@0.90 (CVaR-opt): 0.0025 | Test CVaR@0.90: 0.0040\nTest VaR@0.90 (Equal-wt): 0.0037 | Test CVaR@0.90: 0.0058\n</pre> In\u00a0[45]: Copied! <pre># Cumulative returns\ncum_cvar = (1 + p_cvar_train).cumprod()\ncum_b = (1 + p_b_train).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cum_cvar.index, cum_cvar, label=\"CVaR-min (test)\")\nplt.plot(cum_b.index, cum_b, label=\"Mean-variance (test)\")\nplt.title(\"Insample cumulative growth\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Growth of $1\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Cumulative returns cum_cvar = (1 + p_cvar_train).cumprod() cum_b = (1 + p_b_train).cumprod()  plt.figure(figsize=(10,5)) plt.plot(cum_cvar.index, cum_cvar, label=\"CVaR-min (test)\") plt.plot(cum_b.index, cum_b, label=\"Mean-variance (test)\") plt.title(\"Insample cumulative growth\") plt.xlabel(\"Date\") plt.ylabel(\"Growth of $1\") plt.legend() plt.grid(True) plt.show()  In\u00a0[46]: Copied! <pre># Cumulative returns\ncum_cvar = (1 + p_cvar).cumprod()\ncum_b = (1 + p_b).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cum_cvar.index, cum_cvar, label=\"CVaR-min (test)\")\nplt.plot(cum_b.index, cum_b, label=\"Mean-variance (test)\")\nplt.title(\"Out-of-sample cumulative growth\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Growth of $1\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Cumulative returns cum_cvar = (1 + p_cvar).cumprod() cum_b = (1 + p_b).cumprod()  plt.figure(figsize=(10,5)) plt.plot(cum_cvar.index, cum_cvar, label=\"CVaR-min (test)\") plt.plot(cum_b.index, cum_b, label=\"Mean-variance (test)\") plt.title(\"Out-of-sample cumulative growth\") plt.xlabel(\"Date\") plt.ylabel(\"Growth of $1\") plt.legend() plt.grid(True) plt.show()  In\u00a0[47]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_tail_comparison_shaded(\n    losses_a,\n    losses_b,\n    alpha=0.95,\n    label_a=\"CVaR-min\",\n    label_b=\"Mean\u2013Variance\",\n    title=\"Tail risk comparison (test set)\"\n):\n    # Compute VaR / CVaR\n    var_a = np.quantile(losses_a, alpha)\n    cvar_a = losses_a[losses_a &gt;= var_a].mean()\n\n    var_b = np.quantile(losses_b, alpha)\n    cvar_b = losses_b[losses_b &gt;= var_b].mean()\n\n    # Common bins for fair comparison\n    xmin = min(losses_a.min(), losses_b.min())\n    xmax = max(losses_a.max(), losses_b.max())\n    bins = np.linspace(xmin, xmax, 80)\n\n    plt.figure(figsize=(11, 5))\n\n    # Histograms (density)\n    plt.hist(\n        losses_a, bins=bins, density=True, alpha=0.6,\n        label=label_a\n    )\n    plt.hist(\n        losses_b, bins=bins, density=True, alpha=0.6,\n        label=label_b\n    )\n\n    # VaR lines\n    plt.axvline(\n        var_a, linestyle=\"--\", linewidth=2,\n        label=f\"{label_a} VaR@{alpha:.2f}\"\n    )\n    plt.axvline(\n        var_b, linestyle=\"--\", linewidth=2,\n        label=f\"{label_b} VaR@{alpha:.2f}\"\n    )\n\n    # Shade tail regions\n    y_max = plt.ylim()[1]\n\n    plt.fill_betweenx(\n        [0, y_max],\n        var_a,\n        xmax,\n        alpha=0.15\n    )\n    plt.fill_betweenx(\n        [0, y_max],\n        var_b,\n        xmax,\n        alpha=0.15\n    )\n\n    # CVaR annotations (not vertical lines)\n    plt.text(\n        cvar_a,\n        y_max * 0.85,\n        f\"'{label_a}' - Avg Loss in Tail = {100*cvar_a:.2f}%\",\n        rotation=90,\n        va=\"top\",\n        ha=\"right\"\n    )\n\n    plt.text(\n        cvar_b,\n        y_max * 0.85,\n        f\"'{label_b}' - Avg Loss in Tail = {100*cvar_b:.2f}%\",\n        rotation=90,\n        va=\"top\",\n        ha=\"right\"\n    )\n\n    # Labels and styling\n    plt.title(title)\n    plt.xlabel(\"Daily loss\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  def plot_tail_comparison_shaded(     losses_a,     losses_b,     alpha=0.95,     label_a=\"CVaR-min\",     label_b=\"Mean\u2013Variance\",     title=\"Tail risk comparison (test set)\" ):     # Compute VaR / CVaR     var_a = np.quantile(losses_a, alpha)     cvar_a = losses_a[losses_a &gt;= var_a].mean()      var_b = np.quantile(losses_b, alpha)     cvar_b = losses_b[losses_b &gt;= var_b].mean()      # Common bins for fair comparison     xmin = min(losses_a.min(), losses_b.min())     xmax = max(losses_a.max(), losses_b.max())     bins = np.linspace(xmin, xmax, 80)      plt.figure(figsize=(11, 5))      # Histograms (density)     plt.hist(         losses_a, bins=bins, density=True, alpha=0.6,         label=label_a     )     plt.hist(         losses_b, bins=bins, density=True, alpha=0.6,         label=label_b     )      # VaR lines     plt.axvline(         var_a, linestyle=\"--\", linewidth=2,         label=f\"{label_a} VaR@{alpha:.2f}\"     )     plt.axvline(         var_b, linestyle=\"--\", linewidth=2,         label=f\"{label_b} VaR@{alpha:.2f}\"     )      # Shade tail regions     y_max = plt.ylim()[1]      plt.fill_betweenx(         [0, y_max],         var_a,         xmax,         alpha=0.15     )     plt.fill_betweenx(         [0, y_max],         var_b,         xmax,         alpha=0.15     )      # CVaR annotations (not vertical lines)     plt.text(         cvar_a,         y_max * 0.85,         f\"'{label_a}' - Avg Loss in Tail = {100*cvar_a:.2f}%\",         rotation=90,         va=\"top\",         ha=\"right\"     )      plt.text(         cvar_b,         y_max * 0.85,         f\"'{label_b}' - Avg Loss in Tail = {100*cvar_b:.2f}%\",         rotation=90,         va=\"top\",         ha=\"right\"     )      # Labels and styling     plt.title(title)     plt.xlabel(\"Daily loss\")     plt.ylabel(\"Density\")     plt.legend()     plt.grid(True)     plt.show()  In\u00a0[48]: Copied! <pre>plot_tail_comparison_shaded(\n    losses_a = -p_cvar_train.values,\n    losses_b = -p_b_train.values,\n    alpha = alpha,\n    title = \"Tail risk comparison: CVaR vs Mean\u2013Variance (test)\"\n)\n</pre> plot_tail_comparison_shaded(     losses_a = -p_cvar_train.values,     losses_b = -p_b_train.values,     alpha = alpha,     title = \"Tail risk comparison: CVaR vs Mean\u2013Variance (test)\" )  In\u00a0[49]: Copied! <pre>plot_tail_comparison_shaded(\n    losses_a = -p_cvar.values,\n    losses_b = -p_b.values,\n    alpha = alpha,\n    title = \"Tail risk comparison: CVaR vs Mean\u2013Variance (test)\"\n)\n</pre> plot_tail_comparison_shaded(     losses_a = -p_cvar.values,     losses_b = -p_b.values,     alpha = alpha,     title = \"Tail risk comparison: CVaR vs Mean\u2013Variance (test)\" )"},{"location":"convex/tutorials/5_cvar_portfolio/#v-tail-risk-portfolio-optimization","title":"V. Tail-Risk Portfolio Optimization.\u00b6","text":"<p>This case study shows how to build a tail-risk-aware portfolio using Conditional Value-at-Risk (CVaR) (also called Expected Shortfall). Compared to mean\u2013variance optimization, CVaR focuses explicitly on rare but severe losses, which is often what matters in practice.</p> <p>We will:</p> <ol> <li>Pull daily price data for a basket of liquid ETFs (real market data).</li> <li>Convert prices to return scenarios.</li> <li>Solve a convex linear program that minimizes CVaR at confidence level $\\alpha$ (e.g. 95%).</li> <li>Backtest out-of-sample and compare against a simple baseline (equal-weight).</li> </ol>"},{"location":"convex/tutorials/5_cvar_portfolio/#why-cvar","title":"Why CVaR?\u00b6","text":"<ul> <li>VaR at level $\\alpha$ is the loss threshold exceeded only $(1-\\alpha)$ of the time.</li> <li>CVaR at level $\\alpha$ is the average loss conditional on exceeding VaR. In other words: \u201chow bad are things in the worst tail?\u201d</li> </ul> <p>Rockafellar &amp; Uryasev showed CVaR minimization can be written as a linear program using auxiliary variables, making it scalable and globally solvable.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#data-acquisition","title":"Data acquisition\u00b6","text":"<p>We\u2019ll use a handful of liquid ETFs spanning broad equities, bonds, commodities, and defensive exposures. You can change the universe based on your use-case (e.g., region-specific, factor ETFs, credit, etc.).</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#convert-to-returns-and-create-scenario-matrix","title":"Convert to returns and create scenario matrix\u00b6","text":"<p>Let $r_{t}\\in\\mathbb{R}^n$ be the vector of asset returns at day $t$. We will treat historical daily returns as scenarios for the CVaR optimization.</p> <p>We then split into:</p> <ul> <li>Train window: used to fit the portfolio weights.</li> <li>Test window: used to evaluate out-of-sample.</li> </ul>"},{"location":"convex/tutorials/5_cvar_portfolio/#cvar","title":"CVaR\u00b6","text":"<p>Let the portfolio return in scenario $j$ be  $R_j = w^\\top r_j,$ and define the loss  $L_j = -R_j = -w^\\top r_j.$</p> <ul> <li>Risk management is fundamentally about controlling large losses, not just average behavior.</li> <li>Variance-based risk treats upside and downside symmetrically and does not distinguish between frequent small losses and rare catastrophic ones.</li> <li>CVaR (Conditional Value-at-Risk) directly targets the worst-case tail of the loss distribution.</li> </ul>"},{"location":"convex/tutorials/5_cvar_portfolio/#value-at-risk-var","title":"Value-at-Risk (VaR)\u00b6","text":"<p>For a confidence level $\\alpha \\in (0,1)$, the Value-at-Risk is defined as the $\\alpha$-quantile of the loss distribution:</p> <p>$$\\text{VaR}_\\alpha(L) = \\inf\\{\\gamma : \\mathbb{P}(L \\le \\gamma) \\ge \\alpha\\}.$$</p> <p>Interpretation:</p> <ul> <li>With probability $\\alpha$, the loss will not exceed $\\gamma$.</li> <li>VaR tells us how bad losses can get, but not how bad they are beyond that threshold.</li> </ul> <p>VaR is non-convex and difficult to optimize directly.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#conditional-value-at-risk-cvar","title":"Conditional Value-at-Risk (CVaR)\u00b6","text":"<p>CVaR addresses this limitation by measuring the expected loss conditional on exceeding VaR:</p> <p>$$ \\text{CVaR}_\\alpha(L) = \\mathbb{E}[L \\mid L \\ge \\text{VaR}_\\alpha(L)]. $$</p> <p>Intuition:</p> <ul> <li>CVaR is the average of the worst $(1-\\alpha)$ fraction of losses</li> <li>It explicitly penalizes tail severity, not just tail location</li> </ul> <p>Importantly CVaR is Convex.</p> <p>CVaR can be written as:</p> <p>$$ \\text{CVaR}_\\alpha(L) = \\min_{\\gamma} \\left[ \\gamma + \\frac{1}{1-\\alpha} \\mathbb{E}\\big[(L - \\gamma)_+\\big] \\right], $$ where $$ (x)_+ = \\max(x, 0). $$</p> <p>Interpretation:</p> <ul> <li>$\\gamma$ acts as a candidate VaR level</li> <li>$(L - \\gamma)_+$ measures how much losses exceed $\\gamma$</li> <li>CVaR balances:<ul> <li>Choosing $\\gamma$ high enough to cover most losses</li> <li>Penalizing excess losses beyond $\\gamma$</li> </ul> </li> </ul> <p>In practice, the loss distribution is unknown. We use historical or simulated data: $\\{L_1, L_2, \\dots, L_N\\}.$ The expectation becomes a sample average:</p> <p>$$ \\text{CVaR}_\\alpha(L) \\approx \\min_{\\gamma} \\left[ \\gamma + \\frac{1}{(1-\\alpha)N} \\sum_{j=1}^N (L_j - \\gamma)_+ \\right]. $$</p> <p>The function $(L_j - \\gamma)_+$ is convex but non-smooth. We linearize it using auxiliary variables $z_j$: $z_j \\ge 0, \\qquad z_j \\ge L_j - \\gamma.$ At the optimum: $z_j = \\max(L_j - \\gamma, 0)$. Substituting into the objective yields:</p> <p>$$ \\min_{w,\\gamma,z} \\quad \\gamma + \\frac{1}{(1-\\alpha)N}\\sum_{j=1}^N z_j. $$</p> <ul> <li>$\\gamma$ controls where the tail begins (VaR estimate)</li> <li>$z_j$ penalizes only the scenarios where losses exceed $\\gamma$</li> <li>The factor $\\frac{1}{1-\\alpha}$ rescales the tail average</li> <li>Only the worst $(1-\\alpha)$ fraction of scenarios contribute meaningfully</li> </ul> <p>This formulation:</p> <ul> <li>Ignores upside returns</li> <li>Ignores moderate losses</li> <li>Focuses optimization power on catastrophic events</li> </ul> <p>Putting everything together:</p> <p>$$ \\begin{aligned} \\min_{w,\\gamma,z}\\quad &amp; \\gamma + \\frac{1}{(1-\\alpha)N}\\sum_{j=1}^N z_j \\\\ \\text{s.t.}\\quad &amp; z_j \\ge 0, \\\\ &amp; z_j \\ge -w^\\top r_j - \\gamma, \\quad j=1,\\dots,N, \\\\ &amp; \\sum_{i=1}^n w_i = 1, \\\\ &amp; w_i \\ge 0. \\end{aligned} $$</p> <p>This is a linear program:</p> <ul> <li>Linear objective</li> <li>Linear constraints</li> <li>Global optimum guaranteed</li> </ul> <p>Pure CVaR minimization may lead to overly conservative portfolios. To enforce performance:</p> <p>$$ \\mathbb{E}[w^\\top r] \\ge \\rho_{\\text{target}}. $$</p> <p>Using the sample mean:</p> <p>$$ \\frac{1}{N}\\sum_{j=1}^N w^\\top r_j \\ge \\rho_{\\text{target}}. $$</p> <p>This adds a linear constraint and preserves convexity.</p> <ul> <li>CVaR minimizes how bad things get when things go really bad</li> <li>The optimization explicitly models tail losses</li> <li>Auxiliary variables isolate extreme scenarios</li> <li>Convexity ensures reliability, scalability, and robustness</li> </ul> <p>This makes CVaR minimization a modern risk optimization tool used in asset management, insurance, and energy markets.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#cvar-minimization-as-a-convex-optimization-problem","title":"CVaR minimization as a convex optimization problem\u00b6","text":"<p>We define portfolio weights $w \\in \\mathbb{R}^n$ with constraints:</p> <ul> <li>Fully invested: $\\sum_i w_i = 1$</li> <li>Long-only (typical for many ETF mandates): $w_i \\ge 0$</li> </ul> <p>Portfolio return on scenario $j$ is $w^\\top r_j$ and portfolio loss is $L_j = -w^\\top r_j$.</p> <p>For confidence level $\\alpha \\in (0,1)$, the CVaR minimization can be written as:</p> <p>$$ \\min_{w,\\gamma,z}\\; \\gamma + \\frac{1}{(1-\\alpha)N}\\sum_{j=1}^N z_j $$ subject to $$ z_j \\ge 0,\\qquad z_j \\ge L_j - \\gamma, $$ plus portfolio constraints.</p> <p>This is a linear program (LP) because everything is linear in $(w,\\gamma,z)$.</p> <p>In practice, you often want \u201cmin CVaR subject to at least some expected return.\u201d We can add:</p> <p>$$ \\mathbb{E}[w^\\top r] \\ge \\rho_{\\text{target}} $$</p> <p>Using sample mean return as an estimate.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#baseline-meanvariance-portfolio-optimization-from-case-study-ii","title":"Baseline: Mean\u2013Variance Portfolio Optimization from Case Study II\u00b6","text":"<p>Compared to Case Study II, we use the minimum-variance formulation with a return constraint rather than a weighted mean\u2013variance objective.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#out-of-sample-backtest","title":"Out-of-sample backtest\u00b6","text":"<p>We compute:</p> <ul> <li>Daily portfolio returns on the test set</li> <li>Cumulative returns</li> <li>Realized VaR/CVaR estimates from test returns</li> </ul> <p>Important: CVaR is a tail quantity; to estimate it reliably you want enough test observations. If your test window is short, interpret tail metrics with care.</p>"},{"location":"convex/tutorials/5_cvar_portfolio/#tail-loss-visualization","title":"Tail-loss visualization\u00b6","text":""},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/","title":"VI \u2014 Matchmaking - An Optimal Transport Case Study","text":"<p>Suppose we have two groups of people participating in speed dating event.</p> <ul> <li>For each person we can build a feature vector (age, interests, stated preferences, etc.).</li> <li>For any pair (person from group A, person from group B), we can compute a compatibility cost: low cost means \"good match\".</li> </ul> <p>What we want: a global matching policy that answers:</p> <p>\"How should mass (attention / recommendations) flow from group A to group B to minimize total incompatibility, while treating everyone fairly?\"</p> <p>Optimal Transport is a natural fit because it enforces global consistency:</p> <ul> <li>Everyone in group A must distribute all their mass.</li> <li>Everyone in group B must receive the right total mass.</li> </ul> <p>This prevents a common failure mode of naive scoring systems: everyone gets matched to the same few \"top\" profiles.</p> <p>A common baseline is:</p> <ol> <li>Score every possible pair.</li> <li>For each person, recommend the top-k matches by score.</li> </ol> <p>This is not a valid matching policy at the population level:</p> <ul> <li>It ignores competition (many people choose the same top profiles).</li> <li>It ignores scarcity (limited attention / limited capacity).</li> <li>It can create extreme exposure imbalance (a few people receive nearly all recommendations).</li> </ul> <p>Optimal Transport fixes this by adding hard constraints (marginals) and solving a global optimization.</p> In\u00a0[1]: Copied! <pre>#!pip install POT\n</pre> #!pip install POT  In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.preprocessing import StandardScaler\nimport ot\not.__version__\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import re from sklearn.preprocessing import StandardScaler import ot ot.__version__  Out[2]: <pre>'0.9.6.post1'</pre> In\u00a0[3]: Copied! <pre># Load the Speed Dating dataset \ndf = pd.read_csv('data/SpeedDating.csv')\ndf.shape, df.columns[:20]\n</pre> # Load the Speed Dating dataset  df = pd.read_csv('data/SpeedDating.csv') df.shape, df.columns[:20] Out[3]: <pre>((8378, 123),\n Index(['has_null', 'wave', 'gender', 'age', 'age_o', 'd_age', 'd_d_age',\n        'race', 'race_o', 'samerace', 'importance_same_race',\n        'importance_same_religion', 'd_importance_same_race',\n        'd_importance_same_religion', 'field', 'pref_o_attractive',\n        'pref_o_sincere', 'pref_o_intelligence', 'pref_o_funny',\n        'pref_o_ambitious'],\n       dtype='object'))</pre> In\u00a0[4]: Copied! <pre>df['gender'].value_counts(dropna=False).head()\n</pre> df['gender'].value_counts(dropna=False).head() Out[4]: <pre>gender\nmale      4194\nfemale    4184\nName: count, dtype: int64</pre> In\u00a0[5]: Copied! <pre>candidate_cols = [\n 'gender', #Gender of self  \n 'age', #Age of self  \n 'race', #Race of self  \n 'importance_same_race', #How important is it that partner is of same race?\n 'importance_same_religion', #How important is it that partner has same religion?  \n 'field', #Field of study  \n 'pref_o_attractive', #How important does partner rate attractiveness  \n 'pref_o_sincere', #How important does partner rate sincerity\n 'pref_o_intelligence', #How important does partner rate intelligence\n 'pref_o_funny', #How important does partner rate being funny\n 'pref_o_ambitious', #How important does partner rate ambition  \n 'pref_o_shared_interests', #How important does partner rate having shared interests  \n 'attractive_important', #What do you look for in a partner - attractiveness  \n 'sincere_important', #What do you look for in a partner - sincerity  \n 'intellicence_important', #What do you look for in a partner - intelligence  \n 'funny_important', #What do you look for in a partner - being funny  \n 'ambtition_important', #What do you look for in a partner - ambition  \n 'shared_interests_important', #What do you look for in a partner - shared interests  \n 'attractive', #Rate yourself - attractiveness  \n 'sincere', #Rate yourself - sincerity   \n 'intelligence', #Rate yourself - intelligence   \n 'funny', #Rate yourself - being funny   \n 'ambition', #Rate yourself - ambition \n 'sports', #Your own interests [1-10]  \n 'tvsports', #Your own interests [1-10]  \n 'exercise', #Your own interests [1-10]  \n 'dining', #Your own interests [1-10]  \n 'museums', #Your own interests [1-10]  \n 'art', #Your own interests [1-10]  \n 'hiking', #Your own interests [1-10]  \n 'gaming', #Your own interests [1-10]  \n 'clubbing', #Your own interests [1-10]  \n 'reading', #Your own interests [1-10]  \n 'tv', #Your own interests [1-10]  \n 'theater', #Your own interests [1-10]  \n 'movies', #Your own interests [1-10]  \n 'concerts', #Your own interests [1-10]  \n 'music', #Your own interests [1-10]  \n 'shopping', #Your own interests [1-10]  \n 'yoga', #Your own interests [1-10]  \n 'expected_happy_with_sd_people', #How happy do you expect to be with the people you meet during the speed-dating event?  \n 'expected_num_interested_in_me', #Out of the 20 people you will meet, how many do you expect will be interested in dating you?  \n 'expected_num_matches' #How many matches do you expect to get?  \n]\n\nnot_available_cols = [c for c in candidate_cols if c not in df.columns]\navailable_cols = [c for c in candidate_cols if c in df.columns]\nprint(\"Not available columns:\", not_available_cols)\n</pre> candidate_cols = [  'gender', #Gender of self    'age', #Age of self    'race', #Race of self    'importance_same_race', #How important is it that partner is of same race?  'importance_same_religion', #How important is it that partner has same religion?    'field', #Field of study    'pref_o_attractive', #How important does partner rate attractiveness    'pref_o_sincere', #How important does partner rate sincerity  'pref_o_intelligence', #How important does partner rate intelligence  'pref_o_funny', #How important does partner rate being funny  'pref_o_ambitious', #How important does partner rate ambition    'pref_o_shared_interests', #How important does partner rate having shared interests    'attractive_important', #What do you look for in a partner - attractiveness    'sincere_important', #What do you look for in a partner - sincerity    'intellicence_important', #What do you look for in a partner - intelligence    'funny_important', #What do you look for in a partner - being funny    'ambtition_important', #What do you look for in a partner - ambition    'shared_interests_important', #What do you look for in a partner - shared interests    'attractive', #Rate yourself - attractiveness    'sincere', #Rate yourself - sincerity     'intelligence', #Rate yourself - intelligence     'funny', #Rate yourself - being funny     'ambition', #Rate yourself - ambition   'sports', #Your own interests [1-10]    'tvsports', #Your own interests [1-10]    'exercise', #Your own interests [1-10]    'dining', #Your own interests [1-10]    'museums', #Your own interests [1-10]    'art', #Your own interests [1-10]    'hiking', #Your own interests [1-10]    'gaming', #Your own interests [1-10]    'clubbing', #Your own interests [1-10]    'reading', #Your own interests [1-10]    'tv', #Your own interests [1-10]    'theater', #Your own interests [1-10]    'movies', #Your own interests [1-10]    'concerts', #Your own interests [1-10]    'music', #Your own interests [1-10]    'shopping', #Your own interests [1-10]    'yoga', #Your own interests [1-10]    'expected_happy_with_sd_people', #How happy do you expect to be with the people you meet during the speed-dating event?    'expected_num_interested_in_me', #Out of the 20 people you will meet, how many do you expect will be interested in dating you?    'expected_num_matches' #How many matches do you expect to get?   ]  not_available_cols = [c for c in candidate_cols if c not in df.columns] available_cols = [c for c in candidate_cols if c in df.columns] print(\"Not available columns:\", not_available_cols) <pre>Not available columns: []\n</pre> In\u00a0[6]: Copied! <pre>df_relevant = df[available_cols].copy()\n# check for missing values\ndf_relevant.isnull().sum()\n\n# remove null values\ndf_relevant = df_relevant.dropna().reset_index(drop=True)\n#df_relevant.isnull().sum()\ndf_relevant.shape\n\ndf_relevant\n\nmen = df_relevant[df_relevant[\"gender\"] == 'female'].copy()\nwomen = df_relevant[df_relevant[\"gender\"] == 'male'].copy()\n\nmen = men.drop(columns=['gender'])\nwomen = women.drop(columns=['gender'])\n</pre> df_relevant = df[available_cols].copy() # check for missing values df_relevant.isnull().sum()  # remove null values df_relevant = df_relevant.dropna().reset_index(drop=True) #df_relevant.isnull().sum() df_relevant.shape  df_relevant  men = df_relevant[df_relevant[\"gender\"] == 'female'].copy() women = df_relevant[df_relevant[\"gender\"] == 'male'].copy()  men = men.drop(columns=['gender']) women = women.drop(columns=['gender']) In\u00a0[7]: Copied! <pre># preprocess df\ndef preprocess_mixed_table(df: pd.DataFrame, cols=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Minimal preprocessing for mixed-type columns.\n\n    Rules:\n    1) Detect column type by inspection.\n    2) Numeric columns: impute missing with MODE.\n    3) \"Range-of-integers\" string columns (e.g. '6-10'): convert to midpoint float,\n       then impute missing with MEDIAN.\n    4) Categorical columns: one-hot encode; missing -&gt; all zeros (i.e., no 'missing' dummy).\n\n    Returns:\n        A new DataFrame with:\n          - numeric columns kept (imputed)\n          - range columns converted to numeric (imputed)\n          - categorical columns replaced by one-hot columns\n    \"\"\"\n    df = df.copy()\n    if cols is None:\n        cols = list(df.columns)\n    else:\n        cols = [c for c in cols if c in df.columns]\n\n    # --- helpers ---\n    range_pat = re.compile(r\"^\\s*(\\d+)\\s*-\\s*(\\d+)\\s*$\")\n\n    def is_range_string_series(s: pd.Series, sample_n=200, min_hits=3) -&gt; bool:\n        # detect columns that look like \"6-10\"\n        x = s.dropna().astype(str)\n        if len(x) == 0:\n            return False\n        x = x.sample(min(sample_n, len(x)), random_state=0)\n        hits = sum(bool(range_pat.match(v)) for v in x)\n        return hits &gt;= min_hits\n\n    def range_to_midpoint(v):\n        if pd.isna(v):\n            return np.nan\n        if isinstance(v, (int, float, np.number)):\n            return float(v)\n        s = str(v).strip()\n        m = range_pat.match(s)\n        if m:\n            lo, hi = float(m.group(1)), float(m.group(2))\n            return 0.5 * (lo + hi)\n        # try numeric string\n        try:\n            return float(s)\n        except Exception:\n            return np.nan\n\n    out_parts = []\n\n    for c in cols:\n        s = df[c]\n\n        # Numeric dtype \u2192 mode impute\n        if pd.api.types.is_numeric_dtype(s):\n            mode_vals = s.dropna().mode()\n            fill = mode_vals.iloc[0] if len(mode_vals) else 0.0\n            out_parts.append(s.fillna(fill).to_frame(c))\n            continue\n\n        # Object/category: check if it's mostly \"a-b\" integer range\n        if is_range_string_series(s):\n            s_num = s.apply(range_to_midpoint).astype(float)\n            med = float(np.nanmedian(s_num.values)) if np.isfinite(s_num.values).any() else 0.0\n            out_parts.append(s_num.fillna(med).to_frame(c))\n            continue\n\n        # Otherwise treat as categorical: one-hot; missing =&gt; all zeros\n        s_cat = s.astype(\"object\")\n        dummies = pd.get_dummies(s_cat, prefix=c, dummy_na=False).astype(int)\n        # Ensure missing rows become all-zeros\n        missing_mask = s_cat.isna()\n        if missing_mask.any() and dummies.shape[1] &gt; 0:\n            dummies.loc[missing_mask, :] = 0\n        out_parts.append(dummies)\n\n    return pd.concat(out_parts, axis=1)\n</pre> # preprocess df def preprocess_mixed_table(df: pd.DataFrame, cols=None) -&gt; pd.DataFrame:     \"\"\"     Minimal preprocessing for mixed-type columns.      Rules:     1) Detect column type by inspection.     2) Numeric columns: impute missing with MODE.     3) \"Range-of-integers\" string columns (e.g. '6-10'): convert to midpoint float,        then impute missing with MEDIAN.     4) Categorical columns: one-hot encode; missing -&gt; all zeros (i.e., no 'missing' dummy).      Returns:         A new DataFrame with:           - numeric columns kept (imputed)           - range columns converted to numeric (imputed)           - categorical columns replaced by one-hot columns     \"\"\"     df = df.copy()     if cols is None:         cols = list(df.columns)     else:         cols = [c for c in cols if c in df.columns]      # --- helpers ---     range_pat = re.compile(r\"^\\s*(\\d+)\\s*-\\s*(\\d+)\\s*$\")      def is_range_string_series(s: pd.Series, sample_n=200, min_hits=3) -&gt; bool:         # detect columns that look like \"6-10\"         x = s.dropna().astype(str)         if len(x) == 0:             return False         x = x.sample(min(sample_n, len(x)), random_state=0)         hits = sum(bool(range_pat.match(v)) for v in x)         return hits &gt;= min_hits      def range_to_midpoint(v):         if pd.isna(v):             return np.nan         if isinstance(v, (int, float, np.number)):             return float(v)         s = str(v).strip()         m = range_pat.match(s)         if m:             lo, hi = float(m.group(1)), float(m.group(2))             return 0.5 * (lo + hi)         # try numeric string         try:             return float(s)         except Exception:             return np.nan      out_parts = []      for c in cols:         s = df[c]          # Numeric dtype \u2192 mode impute         if pd.api.types.is_numeric_dtype(s):             mode_vals = s.dropna().mode()             fill = mode_vals.iloc[0] if len(mode_vals) else 0.0             out_parts.append(s.fillna(fill).to_frame(c))             continue          # Object/category: check if it's mostly \"a-b\" integer range         if is_range_string_series(s):             s_num = s.apply(range_to_midpoint).astype(float)             med = float(np.nanmedian(s_num.values)) if np.isfinite(s_num.values).any() else 0.0             out_parts.append(s_num.fillna(med).to_frame(c))             continue          # Otherwise treat as categorical: one-hot; missing =&gt; all zeros         s_cat = s.astype(\"object\")         dummies = pd.get_dummies(s_cat, prefix=c, dummy_na=False).astype(int)         # Ensure missing rows become all-zeros         missing_mask = s_cat.isna()         if missing_mask.any() and dummies.shape[1] &gt; 0:             dummies.loc[missing_mask, :] = 0         out_parts.append(dummies)      return pd.concat(out_parts, axis=1)   In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre>df_preprocessed = preprocess_mixed_table(df_relevant)\nprint(df_preprocessed.shape)\nprint(df_preprocessed.isna().sum().sort_values(ascending=False).head())\nnon_numeric = df_preprocessed.select_dtypes(exclude=[np.number]).columns\nprint(non_numeric)\n\nmen = df_preprocessed[df_preprocessed[\"gender_male\"] == 1].copy()\nwomen = df_preprocessed[df_preprocessed[\"gender_female\"] == 1].copy()\n\nmen = men.drop(columns=['gender_male', 'gender_female'])\nwomen = women.drop(columns=['gender_male', 'gender_female'])\n</pre> df_preprocessed = preprocess_mixed_table(df_relevant) print(df_preprocessed.shape) print(df_preprocessed.isna().sum().sort_values(ascending=False).head()) non_numeric = df_preprocessed.select_dtypes(exclude=[np.number]).columns print(non_numeric)  men = df_preprocessed[df_preprocessed[\"gender_male\"] == 1].copy() women = df_preprocessed[df_preprocessed[\"gender_female\"] == 1].copy()  men = men.drop(columns=['gender_male', 'gender_female']) women = women.drop(columns=['gender_male', 'gender_female']) <pre>(1452, 103)\ngender_female             0\nfield_psychology          0\nfunny_important           0\nintellicence_important    0\nsincere_important         0\ndtype: int64\nIndex([], dtype='object')\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[9]: Copied! <pre>pd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\n\n\nmen_desc = men.describe().T\nwomen_desc = women.describe().T\n\n# Add top-level column labels\nmen_desc.columns = pd.MultiIndex.from_product([[\"Men\"], men_desc.columns])\nwomen_desc.columns = pd.MultiIndex.from_product([[\"Women\"], women_desc.columns])\n\n# Concatenate side by side\ndesc_side_by_side = pd.concat([men_desc, women_desc], axis=1)\n\ndesc_side_by_side\n\nimport matplotlib.pyplot as plt\n\nfeatures = men_desc.index  # or a subset list\n\nmen_means = men_desc[(\"Men\", \"mean\")]\nwomen_means = women_desc[(\"Women\", \"mean\")]\n\nx = range(len(features))\nwidth = 0.35\n\nplt.figure(figsize=(20, 10))\nplt.bar(x, men_means, width, label=\"Men\")\nplt.bar([i + width for i in x], women_means, width, label=\"Women\")\n\nplt.xticks([i + width/2 for i in x], features, rotation=90, ha=\"right\")\nplt.ylabel(\"Mean value\")\nplt.title(\"Feature-wise mean comparison\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> pd.set_option(\"display.max_rows\", None) pd.set_option(\"display.max_columns\", None)   men_desc = men.describe().T women_desc = women.describe().T  # Add top-level column labels men_desc.columns = pd.MultiIndex.from_product([[\"Men\"], men_desc.columns]) women_desc.columns = pd.MultiIndex.from_product([[\"Women\"], women_desc.columns])  # Concatenate side by side desc_side_by_side = pd.concat([men_desc, women_desc], axis=1)  desc_side_by_side  import matplotlib.pyplot as plt  features = men_desc.index  # or a subset list  men_means = men_desc[(\"Men\", \"mean\")] women_means = women_desc[(\"Women\", \"mean\")]  x = range(len(features)) width = 0.35  plt.figure(figsize=(20, 10)) plt.bar(x, men_means, width, label=\"Men\") plt.bar([i + width for i in x], women_means, width, label=\"Women\")  plt.xticks([i + width/2 for i in x], features, rotation=90, ha=\"right\") plt.ylabel(\"Mean value\") plt.title(\"Feature-wise mean comparison\") plt.legend() plt.tight_layout() plt.show()   In\u00a0[10]: Copied! <pre>X_raw.shape, Y_raw.shape\n</pre> X_raw.shape, Y_raw.shape  <pre>---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 X_raw.shape, Y_raw.shape\n\nNameError: name 'X_raw' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>X_raw = men.values\nY_raw = women.values\n\n# Standardize features (important for Euclidean costs)\nscaler = StandardScaler()\nscaler.fit(np.vstack([X_raw, Y_raw]))\n\nX = scaler.transform(X_raw)\nY = scaler.transform(Y_raw)\n\nX.shape, Y.shape\n</pre> X_raw = men.values Y_raw = women.values  # Standardize features (important for Euclidean costs) scaler = StandardScaler() scaler.fit(np.vstack([X_raw, Y_raw]))  X = scaler.transform(X_raw) Y = scaler.transform(Y_raw)  X.shape, Y.shape Out[\u00a0]: <pre>((718, 101), (734, 101))</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport ot  # POT\n\nn, m = X.shape[0], Y.shape[0]\na = np.ones(n) / n\nb = np.ones(m) / m\n\n# Compute the cost matrix C_ij = ||x_i - y_j||^2\n# POT provides fast distance computation:\nC = ot.dist(X, Y, metric=\"euclidean\") ** 2   # shape (n, m)\n\n\n# Optional: cost normalization (often improves numerical stability)\nC = C / C.max()\n\n# Choose entropy regularization strength (epsilon)\n# Larger reg =&gt; smoother plan; smaller reg =&gt; sharper plan but more numerically sensitive\nreg = 0.001  \n\n# Solve entropic OT with Sinkhorn\nPi = ot.sinkhorn(a, b, C, reg, numItermax=5000, stopThr=1e-9)\n\nPi.shape, Pi.min(), Pi.max(), Pi.sum()\n</pre> import numpy as np import ot  # POT  n, m = X.shape[0], Y.shape[0] a = np.ones(n) / n b = np.ones(m) / m  # Compute the cost matrix C_ij = ||x_i - y_j||^2 # POT provides fast distance computation: C = ot.dist(X, Y, metric=\"euclidean\") ** 2   # shape (n, m)   # Optional: cost normalization (often improves numerical stability) C = C / C.max()  # Choose entropy regularization strength (epsilon) # Larger reg =&gt; smoother plan; smaller reg =&gt; sharper plan but more numerically sensitive reg = 0.001    # Solve entropic OT with Sinkhorn Pi = ot.sinkhorn(a, b, C, reg, numItermax=5000, stopThr=1e-9)  Pi.shape, Pi.min(), Pi.max(), Pi.sum()  Out[\u00a0]: <pre>((718, 734), 0.0, 0.0013623977870042849, 1.0)</pre> In\u00a0[\u00a0]: Copied! <pre>def random_transport(a, b, seed=0):\n    rng = np.random.default_rng(seed)\n    Pi_rand = rng.random((len(a), len(b)))\n    Pi_rand = Pi_rand / Pi_rand.sum(axis=1, keepdims=True)\n    Pi_rand = Pi_rand * a[:, None]\n    return Pi_rand\nPi_rand = random_transport(a, b, seed=0)\nPi_rand.sum()\n</pre> def random_transport(a, b, seed=0):     rng = np.random.default_rng(seed)     Pi_rand = rng.random((len(a), len(b)))     Pi_rand = Pi_rand / Pi_rand.sum(axis=1, keepdims=True)     Pi_rand = Pi_rand * a[:, None]     return Pi_rand Pi_rand = random_transport(a, b, seed=0) Pi_rand.sum() Out[\u00a0]: <pre>1.0000000000000002</pre> In\u00a0[\u00a0]: Copied! <pre>def nearest_neighbor_transport(C, a, b=None):\n    \"\"\"\n    Nearest-neighbor (greedy) baseline transport.\n\n    For each source i (man), send all its mass a_i to the single closest target j*.\n    This uses pairwise costs only and does NOT enforce column marginals.\n\n    Args:\n        C: (n, m) cost matrix\n        a: (n,) source weights (e.g., uniform 1/n)\n        b: unused (kept for interface symmetry)\n\n    Returns:\n        Pi_nn: (n, m) transport plan with exactly one nonzero per row.\n    \"\"\"\n    n, m = C.shape\n    j_star = np.argmin(C, axis=1)          # closest woman for each man\n    Pi_nn = np.zeros((n, m), dtype=float)\n    Pi_nn[np.arange(n), j_star] = a        # send all mass to nearest neighbor\n    return Pi_nn\n\n# Example usage:\nPi_nn = nearest_neighbor_transport(C, a)\nPi_nn.shape, Pi_nn.sum(), Pi_nn.min(), Pi_nn.max()\n</pre> def nearest_neighbor_transport(C, a, b=None):     \"\"\"     Nearest-neighbor (greedy) baseline transport.      For each source i (man), send all its mass a_i to the single closest target j*.     This uses pairwise costs only and does NOT enforce column marginals.      Args:         C: (n, m) cost matrix         a: (n,) source weights (e.g., uniform 1/n)         b: unused (kept for interface symmetry)      Returns:         Pi_nn: (n, m) transport plan with exactly one nonzero per row.     \"\"\"     n, m = C.shape     j_star = np.argmin(C, axis=1)          # closest woman for each man     Pi_nn = np.zeros((n, m), dtype=float)     Pi_nn[np.arange(n), j_star] = a        # send all mass to nearest neighbor     return Pi_nn  # Example usage: Pi_nn = nearest_neighbor_transport(C, a) Pi_nn.shape, Pi_nn.sum(), Pi_nn.min(), Pi_nn.max() Out[\u00a0]: <pre>((718, 734), 0.9999999999999996, 0.0, 0.001392757660167131)</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_mass_weighted_cost_hist(Pi_rand, Pi_nn, Pi_ot, C, bins=100):\n    # Flatten everything\n    C_flat = C.ravel()\n    Pr = Pi_rand.ravel()\n    Pn = Pi_nn.ravel()\n    Po = Pi_ot.ravel()\n\n    plt.figure(figsize=(8,5))\n\n    plt.hist(\n        C_flat, bins=bins, weights=Pr,\n        alpha=0.5, density=True, label=\"Random baseline\"\n    )\n    plt.hist(\n        C_flat, bins=bins, weights=Pn,\n        alpha=0.5, density=True, label=\"Nearest neighbor\"\n    )\n    plt.hist(\n        C_flat, bins=bins, weights=Po,\n        alpha=0.5, density=True, label=\"Optimal Transport\"\n    )\n\n    plt.xlabel(\"Cost $C_{ij}$\")\n    plt.ylabel(\"Mass-weighted density\")\n    plt.title(\"How Different Matching Methods allocate 'matches' Across Costs\")\n    plt.legend()\n    plt.grid(True)\n\n        # --- Interpretation box ---\n    text = (\n        \"Random baseline:\\n\"\n        \"\u2022 Mass mirrors the underlying cost distribution\\n\"\n        \"\u2022 No preferential concentration at low cost\\n\\n\"\n        \"Nearest neighbor:\\n\"\n        \"\u2022 Mass collapses at minimum cost\\n\\n\"\n        \"Optimal Transport:\\n\"\n        \"\u2022 Mass shifted toward low cost\\n\"\n        \"\u2022 Balanced across the population\"\n    )\n\n    plt.text(\n        0.55, 0.95, text,\n        transform=plt.gca().transAxes,\n        fontsize=9,\n        va=\"top\",\n        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9)\n    )\n\n\n    plt.tight_layout()\n    plt.show()\n\nplot_mass_weighted_cost_hist(Pi_rand, Pi_nn, Pi, C, bins=100)\n</pre> def plot_mass_weighted_cost_hist(Pi_rand, Pi_nn, Pi_ot, C, bins=100):     # Flatten everything     C_flat = C.ravel()     Pr = Pi_rand.ravel()     Pn = Pi_nn.ravel()     Po = Pi_ot.ravel()      plt.figure(figsize=(8,5))      plt.hist(         C_flat, bins=bins, weights=Pr,         alpha=0.5, density=True, label=\"Random baseline\"     )     plt.hist(         C_flat, bins=bins, weights=Pn,         alpha=0.5, density=True, label=\"Nearest neighbor\"     )     plt.hist(         C_flat, bins=bins, weights=Po,         alpha=0.5, density=True, label=\"Optimal Transport\"     )      plt.xlabel(\"Cost $C_{ij}$\")     plt.ylabel(\"Mass-weighted density\")     plt.title(\"How Different Matching Methods allocate 'matches' Across Costs\")     plt.legend()     plt.grid(True)          # --- Interpretation box ---     text = (         \"Random baseline:\\n\"         \"\u2022 Mass mirrors the underlying cost distribution\\n\"         \"\u2022 No preferential concentration at low cost\\n\\n\"         \"Nearest neighbor:\\n\"         \"\u2022 Mass collapses at minimum cost\\n\\n\"         \"Optimal Transport:\\n\"         \"\u2022 Mass shifted toward low cost\\n\"         \"\u2022 Balanced across the population\"     )      plt.text(         0.55, 0.95, text,         transform=plt.gca().transAxes,         fontsize=9,         va=\"top\",         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9)     )       plt.tight_layout()     plt.show()  plot_mass_weighted_cost_hist(Pi_rand, Pi_nn, Pi, C, bins=100)   <p>This figure shows how different matching methods allocate transport mass across pairwise costs. The random baseline assigns mass independently of cost, so its distribution simply reflects the natural distribution of distances. Nearest-neighbor matching collapses mass onto the lowest-cost pairs, ignoring global balance. Optimal transport shifts mass toward low-cost pairs while maintaining balanced participation across the population.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_participation_boxplot(Pi_rand, Pi_nn, Pi_ot):\n    r_rand = Pi_rand.sum(axis=0)\n    r_nn   = Pi_nn.sum(axis=0)\n    r_ot   = Pi_ot.sum(axis=0)\n\n    data = [r_rand, r_nn, r_ot]\n    labels = [\"Random baseline\", \"Nearest neighbor\", \"Optimal Transport\"]\n\n    plt.figure(figsize=(7,5))\n    plt.boxplot(\n        data,\n        labels=labels,\n        showfliers=True,\n        whis=(5, 95)\n    )\n\n    plt.ylabel(\"Received mass per woman\")\n    plt.title(\"Participation Distribution Across Women\")\n    plt.grid(True, axis=\"y\")\n    plt.tight_layout()\n    plt.show()\n\nplot_participation_boxplot(Pi_rand, Pi_nn, Pi)\n</pre> def plot_participation_boxplot(Pi_rand, Pi_nn, Pi_ot):     r_rand = Pi_rand.sum(axis=0)     r_nn   = Pi_nn.sum(axis=0)     r_ot   = Pi_ot.sum(axis=0)      data = [r_rand, r_nn, r_ot]     labels = [\"Random baseline\", \"Nearest neighbor\", \"Optimal Transport\"]      plt.figure(figsize=(7,5))     plt.boxplot(         data,         labels=labels,         showfliers=True,         whis=(5, 95)     )      plt.ylabel(\"Received mass per woman\")     plt.title(\"Participation Distribution Across Women\")     plt.grid(True, axis=\"y\")     plt.tight_layout()     plt.show()  plot_participation_boxplot(Pi_rand, Pi_nn, Pi)   <pre>C:\\Users\\salmank\\AppData\\Local\\Temp\\ipykernel_17708\\4258920207.py:10: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot(\n</pre> <p>The box plot summarizes how participation is distributed across individuals. Nearest-neighbor matching produces extreme imbalance, with many individuals receiving no mass, while optimal transport tightly concentrates participation around the target level.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#vi-matchmaking-an-optimal-transport-case-study","title":"VI \u2014 Matchmaking - An Optimal Transport Case Study\u00b6","text":""},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#data-source","title":"Data source\u00b6","text":"<p>We use the public SpeedDating dataset (participants in speed dating events, 2002\u20132004)  Link</p> <p>Each row corresponds to an interaction (a date) between two participants and includes:</p> <ul> <li>participant id (<code>iid</code>) and partner id (<code>pid</code>)</li> <li>participant attributes (e.g. <code>age</code>, <code>field</code>, ...)</li> <li>participant stated preference weights (e.g. how much they value attractiveness, sincerity, ...)</li> <li>partner ratings (e.g. <code>attr_o</code>, <code>intel_o</code>, ...)</li> <li>decision / match outcome (e.g. <code>dec</code>, <code>match</code>)</li> </ul>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#build-person-level-feature-vectors","title":"Build person-level feature vectors\u00b6","text":"<p>The raw table is interaction-level (one row per date). For OT we want one vector per person.</p> <p>We'll construct a compact numeric feature vector per participant using:</p> <ul> <li>basic demographics (e.g. age)</li> <li>stated preference weights (e.g. how much they care about attractiveness / intelligence / etc.)</li> </ul> <p>We then split into two distributions:</p> <ul> <li>$X = \\{x_i\\}$: men</li> <li>$Y = \\{y_j\\}$: women</li> </ul> <p>(The dataset is heterosexual speed dating; we treat <code>gender</code> as the group indicator.)</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#formulation","title":"Formulation:\u00b6","text":"<p>We have two populations represented as feature vectors embedded in a shared metric space:</p> <ul> <li>Men: $X = \\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$</li> <li>Women: $Y = \\{y_j \\in \\mathbb{R}^d\\}_{j=1}^m$</li> </ul> <p>Each vector is a standardized profile (e.g., age, preferences, interests). Our goal is not to predict individual outcomes, but to compute a globally consistent, population-level matching between the two distributions.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#cost-function","title":"Cost Function\u00b6","text":"<p>We define a pairwise compatibility cost: $$ C_{ij} = \\|x_i - y_j\\|_2^2 $$</p> <p>Lower cost indicates higher similarity. The cost matrix $C$ is fixed and exogenous; defining it is a modeling choice made prior to optimization.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#decision-variable","title":"Decision Variable\u00b6","text":"<p>We optimize over a transport plan: $$ \\Pi \\in \\mathbb{R}_+^{n \\times m} $$</p> <p>$\\Pi_{ij}$ represents the amount of probability mass assigned between man $i$ and woman $j$. $\\Pi$ is a soft coupling (a joint distribution over pairs), not a one-to-one assignment.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#marginal-constraints-population-balance","title":"Marginal Constraints (Population Balance)\u00b6","text":"<p>Let $a \\in \\Delta^n$ and $b \\in \\Delta^m$ be probability weights (typically uniform): $$ a_i = \\tfrac{1}{n}, \\quad b_j = \\tfrac{1}{m} $$</p> <p>The transport plan must satisfy: $$ \\Pi \\mathbf{1}_m = a, \\qquad \\Pi^\\top \\mathbf{1}_n = b $$</p> <p>Thus, each individual contributes and receives the correct total mass in aggregate.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#optimal-transport-objective","title":"Optimal Transport Objective\u00b6","text":"<p>Unregularized OT (Linear Program): $$ \\min_{\\Pi \\ge 0} \\ \\langle C, \\Pi \\rangle \\quad \\text{s.t.} \\quad \\Pi \\mathbf{1}_m = a,\\ \\Pi^\\top \\mathbf{1}_n = b $$</p> <p>This is a linear program. Its solutions often lie at sparse extreme points and may be non-unique.</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#entropic-optimal-transport-regularized-practical","title":"Entropic Optimal Transport (Regularized, Practical)\u00b6","text":"<p>To obtain a smooth, stable, and computationally efficient solution, we add entropy regularization: $$ \\min_{\\Pi \\ge 0} \\ \\langle C, \\Pi \\rangle - \\varepsilon H(\\Pi), \\qquad H(\\Pi) = -\\sum_{i,j} \\Pi_{ij}\\log \\Pi_{ij} $$</p> <p>Equivalently: $$ \\min_{\\Pi \\ge 0} \\ \\langle C, \\Pi \\rangle + \\varepsilon \\sum_{i,j} \\Pi_{ij}(\\log \\Pi_{ij}-1) $$</p> <p>where $\\varepsilon &gt; 0$ controls the smoothness of the matching.</p> <p>This problem is strictly convex (under mild conditions) and admits a unique solution. This is a well-defined convex optimization problem and is solved with Sinkhor iteration algorithm.</p> <p>Sinkhorn iterations are the numerical algorithm used to compute the solution of the entropically regularized optimal transport problem.</p> <ul> <li>Decision variable: $\\Pi$ (a joint distribution over pairs)</li> <li>Objective: minimize total mismatch cost while encouraging smooth matchings</li> <li>Constraints: preserve population-level balance</li> <li>Problem class: convex optimization<ul> <li>unregularized OT: linear program</li> <li>entropic OT: strictly convex, efficiently solvable</li> </ul> </li> </ul>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#interpreting-ot-results-with-baselines","title":"Interpreting OT Results (with Baselines)\u00b6","text":"<p>We compare optimal transport to two baselines:</p> <ul> <li>random matching, which ignores costs,</li> <li>a nearest-neighbor baseline, which matches each man to the most similar woman based on pairwise distance only.</li> </ul> <p>Random matching distributes mass independently of cost, while nearest-neighbor concentrates mass on low-cost pairs but can overuse the same popular profiles and violate global balance. In contrast, optimal transport concentrates mass on low-cost pairs while simultaneously enforcing balanced participation across the entire population.</p> <p>This highlights the key advantage of OT: it performs global cost minimization under balancing constraints, whereas baselines are either cost-blind (random) or locally optimal but globally inconsistent (nearest-neighbor).</p>"},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#random-transport-baseline-for-comparison","title":"Random transport baseline for comparison\u00b6","text":""},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#nearest-neighbor-greedy-baseline-transport","title":"Nearest-neighbor (greedy) baseline transport\u00b6","text":""},{"location":"convex/tutorials/6_sinkhorn_speed_dating_ot/#how-evenly-does-each-method-distribute-participation-across-women","title":"How evenly does each method distribute participation across women?\u00b6","text":""},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/","title":"VII \u2014 Vehicle Tracking \u2014 Robust Kalman Filtering Case Study","text":"In\u00a0[\u00a0]: Copied! <pre># --- Imports &amp; reproducibility ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\n\nSEED_MAIN = 6\nSEED_OUTLIERS = 0\nnp.random.seed(SEED_MAIN)\n\nimport sys, platform\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Platform:\", platform.platform())\nprint(\"NumPy:\", np.__version__)\nprint(\"CVXPY:\", cp.__version__)\n</pre> # --- Imports &amp; reproducibility --- import numpy as np import matplotlib.pyplot as plt import cvxpy as cp  SEED_MAIN = 6 SEED_OUTLIERS = 0 np.random.seed(SEED_MAIN)  import sys, platform print(\"Python:\", sys.version.split()[0]) print(\"Platform:\", platform.platform()) print(\"NumPy:\", np.__version__) print(\"CVXPY:\", cp.__version__) In\u00a0[\u00a0]: Copied! <pre>import matplotlib\n\ndef plot_state(ts, actual, estimated=None, suptitle=None):\n    \"\"\"Plot position, velocity, and input over time in x and y.\"\"\"\n    trajectories = [actual]\n    labels = [\"True\"]\n    if estimated is not None:\n        trajectories.append(estimated)\n        labels.append(\"Estimated\")\n\n    fig, ax = plt.subplots(3, 2, sharex='col', sharey='row', figsize=(10, 8))\n    for (x, w), lab in zip(trajectories, labels):\n        ax[0,0].plot(ts, x[0,:-1], label=lab)\n        ax[0,1].plot(ts, x[1,:-1], label=lab)\n        ax[1,0].plot(ts, x[2,:-1], label=lab)\n        ax[1,1].plot(ts, x[3,:-1], label=lab)\n        ax[2,0].plot(ts, w[0,:], label=lab)\n        ax[2,1].plot(ts, w[1,:], label=lab)\n\n    ax[0,0].set_ylabel('x position')\n    ax[1,0].set_ylabel('x velocity')\n    ax[2,0].set_ylabel('x input')\n    ax[0,1].set_ylabel('y position')\n    ax[1,1].set_ylabel('y velocity')\n    ax[2,1].set_ylabel('y input')\n\n    for r in range(3):\n        ax[r,1].yaxis.tick_right()\n        ax[r,1].yaxis.set_label_position(\"right\")\n\n    ax[2,0].set_xlabel('time')\n    ax[2,1].set_xlabel('time')\n\n    ax[0,0].legend(loc=\"best\")\n    if suptitle:\n        fig.suptitle(suptitle, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_positions(traj, labels, axis=None):\n    \"\"\"Point clouds for (x,y) positions.\"\"\"\n    matplotlib.rcParams.update({'font.size': 12})\n    n = len(traj)\n    fig, ax = plt.subplots(1, n, sharex=True, sharey=True, figsize=(4*n, 4))\n    if n == 1:\n        ax = [ax]\n    for i, x in enumerate(traj):\n        ax[i].plot(x[0,:], x[1,:], 'o', alpha=.15, markersize=4)\n        ax[i].set_title(labels[i])\n        ax[i].set_xlabel(\"x\")\n        if i == 0:\n            ax[i].set_ylabel(\"y\")\n        if axis is not None:\n            ax[i].axis(axis)\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib  def plot_state(ts, actual, estimated=None, suptitle=None):     \"\"\"Plot position, velocity, and input over time in x and y.\"\"\"     trajectories = [actual]     labels = [\"True\"]     if estimated is not None:         trajectories.append(estimated)         labels.append(\"Estimated\")      fig, ax = plt.subplots(3, 2, sharex='col', sharey='row', figsize=(10, 8))     for (x, w), lab in zip(trajectories, labels):         ax[0,0].plot(ts, x[0,:-1], label=lab)         ax[0,1].plot(ts, x[1,:-1], label=lab)         ax[1,0].plot(ts, x[2,:-1], label=lab)         ax[1,1].plot(ts, x[3,:-1], label=lab)         ax[2,0].plot(ts, w[0,:], label=lab)         ax[2,1].plot(ts, w[1,:], label=lab)      ax[0,0].set_ylabel('x position')     ax[1,0].set_ylabel('x velocity')     ax[2,0].set_ylabel('x input')     ax[0,1].set_ylabel('y position')     ax[1,1].set_ylabel('y velocity')     ax[2,1].set_ylabel('y input')      for r in range(3):         ax[r,1].yaxis.tick_right()         ax[r,1].yaxis.set_label_position(\"right\")      ax[2,0].set_xlabel('time')     ax[2,1].set_xlabel('time')      ax[0,0].legend(loc=\"best\")     if suptitle:         fig.suptitle(suptitle, y=1.02)     plt.tight_layout()     plt.show()  def plot_positions(traj, labels, axis=None):     \"\"\"Point clouds for (x,y) positions.\"\"\"     matplotlib.rcParams.update({'font.size': 12})     n = len(traj)     fig, ax = plt.subplots(1, n, sharex=True, sharey=True, figsize=(4*n, 4))     if n == 1:         ax = [ax]     for i, x in enumerate(traj):         ax[i].plot(x[0,:], x[1,:], 'o', alpha=.15, markersize=4)         ax[i].set_title(labels[i])         ax[i].set_xlabel(\"x\")         if i == 0:             ax[i].set_ylabel(\"y\")         if axis is not None:             ax[i].axis(axis)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre># --- Problem size (change N if you want faster/slower solves) ---\nN = 600              # timesteps\nT = 50               # time horizon (seconds)\nts, delt = np.linspace(0, T, N, endpoint=True, retstep=True)\n\ngamma = 0.05         # damping (0 = no damping)\n\n# Dynamics matrices (state: [px, py, vx, vy])\nA = np.zeros((4, 4))\nB = np.zeros((4, 2))\nC = np.zeros((2, 4))\n\nA[0,0] = 1\nA[1,1] = 1\nA[0,2] = (1 - gamma*delt/2) * delt\nA[1,3] = (1 - gamma*delt/2) * delt\nA[2,2] = 1 - gamma*delt\nA[3,3] = 1 - gamma*delt\n\nB[0,0] = delt2 / 2\nB[1,1] = delt2 / 2\nB[2,0] = delt\nB[3,1] = delt\n\nC[0,0] = 1\nC[1,1] = 1\n\n# Noise / outlier settings\nsigma_outlier = 20.0\np_outlier = 0.20\n\n# Simulate\nnp.random.seed(SEED_MAIN)\nx_true = np.zeros((4, N+1))\nx_true[:, 0] = [0, 0, 0, 0]\ny = np.zeros((2, N))\n\nw_true = np.random.randn(2, N)          # drive force\nv = np.random.randn(2, N)               # nominal measurement noise\n\n# Inject outliers in the measurement noise\nnp.random.seed(SEED_OUTLIERS)\noutlier_mask = (np.random.rand(N) &lt;= p_outlier)\nv[:, outlier_mask] = sigma_outlier * np.random.randn(2, N)[:, outlier_mask]\n\n# Roll forward\nfor t in range(N):\n    y[:, t] = C @ x_true[:, t] + v[:, t]\n    x_true[:, t+1] = A @ x_true[:, t] + B @ w_true[:, t]\n\nprint(f\"N={N}, outliers={outlier_mask.mean()*100:.1f}%\")\n</pre> # --- Problem size (change N if you want faster/slower solves) --- N = 600              # timesteps T = 50               # time horizon (seconds) ts, delt = np.linspace(0, T, N, endpoint=True, retstep=True)  gamma = 0.05         # damping (0 = no damping)  # Dynamics matrices (state: [px, py, vx, vy]) A = np.zeros((4, 4)) B = np.zeros((4, 2)) C = np.zeros((2, 4))  A[0,0] = 1 A[1,1] = 1 A[0,2] = (1 - gamma*delt/2) * delt A[1,3] = (1 - gamma*delt/2) * delt A[2,2] = 1 - gamma*delt A[3,3] = 1 - gamma*delt  B[0,0] = delt2 / 2 B[1,1] = delt2 / 2 B[2,0] = delt B[3,1] = delt  C[0,0] = 1 C[1,1] = 1  # Noise / outlier settings sigma_outlier = 20.0 p_outlier = 0.20  # Simulate np.random.seed(SEED_MAIN) x_true = np.zeros((4, N+1)) x_true[:, 0] = [0, 0, 0, 0] y = np.zeros((2, N))  w_true = np.random.randn(2, N)          # drive force v = np.random.randn(2, N)               # nominal measurement noise  # Inject outliers in the measurement noise np.random.seed(SEED_OUTLIERS) outlier_mask = (np.random.rand(N) &lt;= p_outlier) v[:, outlier_mask] = sigma_outlier * np.random.randn(2, N)[:, outlier_mask]  # Roll forward for t in range(N):     y[:, t] = C @ x_true[:, t] + v[:, t]     x_true[:, t+1] = A @ x_true[:, t] + B @ w_true[:, t]  print(f\"N={N}, outliers={outlier_mask.mean()*100:.1f}%\") In\u00a0[\u00a0]: Copied! <pre>plot_positions(\n    traj=[x_true[:2, :-1], y],\n    labels=[\"True positions\", \"Noisy measurements\"],\n    axis=[-4, 14, -5, 20]\n)\n</pre> plot_positions(     traj=[x_true[:2, :-1], y],     labels=[\"True positions\", \"Noisy measurements\"],     axis=[-4, 14, -5, 20] ) In\u00a0[\u00a0]: Copied! <pre>def solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False):\n    \"\"\"Quadratic Kalman smoothing as a QP.\"\"\"\n    n = y.shape[1]\n    x = cp.Variable((4, n+1))\n    w = cp.Variable((2, n))\n    v = cp.Variable((2, n))\n\n    obj = cp.Minimize(cp.sum_squares(w) + tau * cp.sum_squares(v))\n\n    constr = []\n    for t in range(n):\n        constr += [\n            x[:, t+1] == A @ x[:, t] + B @ w[:, t],\n            y[:, t]   == C @ x[:, t] + v[:, t],\n        ]\n\n    prob = cp.Problem(obj, constr)\n    try:\n        prob.solve(solver=solver, verbose=verbose)\n    except Exception:\n        prob.solve(verbose=verbose)\n\n    return np.array(x.value), np.array(w.value), prob.value, prob.status\n\nx_kf, w_kf, obj_kf, status_kf = solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False)\nprint(\"status:\", status_kf)\nprint(\"objective:\", obj_kf)\n</pre> def solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False):     \"\"\"Quadratic Kalman smoothing as a QP.\"\"\"     n = y.shape[1]     x = cp.Variable((4, n+1))     w = cp.Variable((2, n))     v = cp.Variable((2, n))      obj = cp.Minimize(cp.sum_squares(w) + tau * cp.sum_squares(v))      constr = []     for t in range(n):         constr += [             x[:, t+1] == A @ x[:, t] + B @ w[:, t],             y[:, t]   == C @ x[:, t] + v[:, t],         ]      prob = cp.Problem(obj, constr)     try:         prob.solve(solver=solver, verbose=verbose)     except Exception:         prob.solve(verbose=verbose)      return np.array(x.value), np.array(w.value), prob.value, prob.status  x_kf, w_kf, obj_kf, status_kf = solve_kalman_least_squares(A, B, C, y, tau=0.08, solver=\"OSQP\", verbose=False) print(\"status:\", status_kf) print(\"objective:\", obj_kf) In\u00a0[\u00a0]: Copied! <pre>plot_state(ts, (x_true, w_true), (x_kf, w_kf), suptitle=\"Standard (quadratic) Kalman smoothing\")\nplot_positions([x_true[:2, :-1], x_kf[:2, :-1]], [\"True positions\", \"KF recovery\"], axis=[-4, 14, -5, 20])\n</pre> plot_state(ts, (x_true, w_true), (x_kf, w_kf), suptitle=\"Standard (quadratic) Kalman smoothing\") plot_positions([x_true[:2, :-1], x_kf[:2, :-1]], [\"True positions\", \"KF recovery\"], axis=[-4, 14, -5, 20]) In\u00a0[\u00a0]: Copied! <pre>def solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False):\n    \"\"\"Robust Kalman smoothing with Huber measurement loss (conic program).\"\"\"\n    n = y.shape[1]\n    x = cp.Variable((4, n+1))\n    w = cp.Variable((2, n))\n    v = cp.Variable((2, n))\n\n    huber_terms = [tau * cp.huber(cp.norm(v[:, t], 2), rho) for t in range(n)]\n    obj = cp.Minimize(cp.sum_squares(w) + cp.sum(huber_terms))\n\n    constr = []\n    for t in range(n):\n        constr += [\n            x[:, t+1] == A @ x[:, t] + B @ w[:, t],\n            y[:, t]   == C @ x[:, t] + v[:, t],\n        ]\n\n    prob = cp.Problem(obj, constr)\n    try:\n        prob.solve(solver=solver, verbose=verbose)\n    except Exception:\n        prob.solve(verbose=verbose)\n\n    return np.array(x.value), np.array(w.value), prob.value, prob.status\n\nx_rkf, w_rkf, obj_rkf, status_rkf = solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False)\nprint(\"status:\", status_rkf)\nprint(\"objective:\", obj_rkf)\n</pre> def solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False):     \"\"\"Robust Kalman smoothing with Huber measurement loss (conic program).\"\"\"     n = y.shape[1]     x = cp.Variable((4, n+1))     w = cp.Variable((2, n))     v = cp.Variable((2, n))      huber_terms = [tau * cp.huber(cp.norm(v[:, t], 2), rho) for t in range(n)]     obj = cp.Minimize(cp.sum_squares(w) + cp.sum(huber_terms))      constr = []     for t in range(n):         constr += [             x[:, t+1] == A @ x[:, t] + B @ w[:, t],             y[:, t]   == C @ x[:, t] + v[:, t],         ]      prob = cp.Problem(obj, constr)     try:         prob.solve(solver=solver, verbose=verbose)     except Exception:         prob.solve(verbose=verbose)      return np.array(x.value), np.array(w.value), prob.value, prob.status  x_rkf, w_rkf, obj_rkf, status_rkf = solve_kalman_robust_huber(A, B, C, y, tau=2.0, rho=2.0, solver=\"SCS\", verbose=False) print(\"status:\", status_rkf) print(\"objective:\", obj_rkf) In\u00a0[\u00a0]: Copied! <pre>plot_state(ts, (x_true, w_true), (x_rkf, w_rkf), suptitle=\"Robust Kalman smoothing (Huber measurements)\")\nplot_positions([x_true[:2, :-1], x_rkf[:2, :-1]], [\"True positions\", \"Robust KF recovery\"], axis=[-4, 14, -5, 20])\n</pre> plot_state(ts, (x_true, w_true), (x_rkf, w_rkf), suptitle=\"Robust Kalman smoothing (Huber measurements)\") plot_positions([x_true[:2, :-1], x_rkf[:2, :-1]], [\"True positions\", \"Robust KF recovery\"], axis=[-4, 14, -5, 20]) In\u00a0[\u00a0]: Copied! <pre>def rmse(a, b):\n    return np.sqrt(np.mean((a - b)2))\n\npos_true = x_true[:2, :-1]\nvel_true = x_true[2:, :-1]\n\npos_kf = x_kf[:2, :-1]\nvel_kf = x_kf[2:, :-1]\n\npos_rkf = x_rkf[:2, :-1]\nvel_rkf = x_rkf[2:, :-1]\n\nmetrics = {\n    \"KF_pos_RMSE\": rmse(pos_kf, pos_true),\n    \"RKF_pos_RMSE\": rmse(pos_rkf, pos_true),\n    \"KF_vel_RMSE\": rmse(vel_kf, vel_true),\n    \"RKF_vel_RMSE\": rmse(vel_rkf, vel_true),\n}\nmetrics\n</pre> def rmse(a, b):     return np.sqrt(np.mean((a - b)2))  pos_true = x_true[:2, :-1] vel_true = x_true[2:, :-1]  pos_kf = x_kf[:2, :-1] vel_kf = x_kf[2:, :-1]  pos_rkf = x_rkf[:2, :-1] vel_rkf = x_rkf[2:, :-1]  metrics = {     \"KF_pos_RMSE\": rmse(pos_kf, pos_true),     \"RKF_pos_RMSE\": rmse(pos_rkf, pos_true),     \"KF_vel_RMSE\": rmse(vel_kf, vel_true),     \"RKF_vel_RMSE\": rmse(vel_rkf, vel_true), } metrics In\u00a0[\u00a0]: Copied! <pre>def residual_norms(C, x_hat, y):\n    v_hat = y - (C @ x_hat[:, :-1])\n    return np.linalg.norm(v_hat, axis=0)\n\nr_kf = residual_norms(C, x_kf, y)\nr_rkf = residual_norms(C, x_rkf, y)\n\nplt.figure(figsize=(10, 3))\nplt.plot(ts, r_kf, label=\"KF residual norm\", alpha=0.9)\nplt.plot(ts, r_rkf, label=\"Robust KF residual norm\", alpha=0.9)\nplt.scatter(ts[outlier_mask], r_kf[outlier_mask], s=12, alpha=0.6, label=\"Outlier timesteps (KF)\")\nplt.xlabel(\"time\")\nplt.ylabel(r\"$\\|y_t - C\\hat x_t\\|_2$\")\nplt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n</pre> def residual_norms(C, x_hat, y):     v_hat = y - (C @ x_hat[:, :-1])     return np.linalg.norm(v_hat, axis=0)  r_kf = residual_norms(C, x_kf, y) r_rkf = residual_norms(C, x_rkf, y)  plt.figure(figsize=(10, 3)) plt.plot(ts, r_kf, label=\"KF residual norm\", alpha=0.9) plt.plot(ts, r_rkf, label=\"Robust KF residual norm\", alpha=0.9) plt.scatter(ts[outlier_mask], r_kf[outlier_mask], s=12, alpha=0.6, label=\"Outlier timesteps (KF)\") plt.xlabel(\"time\") plt.ylabel(r\"$\\|y_t - C\\hat x_t\\|_2$\") plt.legend(loc=\"best\") plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>taus = [0.5, 1.0, 2.0, 4.0]\nrhos = [0.5, 1.0, 2.0]\n\nresults = []\nfor tau in taus:\n    for rho in rhos:\n        x_hat, w_hat, obj, status = solve_kalman_robust_huber(A, B, C, y, tau=tau, rho=rho, solver=\"SCS\", verbose=False)\n        pos_rmse = rmse(x_hat[:2, :-1], pos_true)\n        vel_rmse = rmse(x_hat[2:, :-1], vel_true)\n        results.append((tau, rho, pos_rmse, vel_rmse, status))\n\nresults_sorted = sorted(results, key=lambda t: t[2])\nfor tau, rho, prmse, vrmse, st in results_sorted[:8]:\n    print(f\"tau={tau:&gt;4}, rho={rho:&gt;3} | pos_RMSE={prmse:.3f}, vel_RMSE={vrmse:.3f} | status={st}\")\n</pre> taus = [0.5, 1.0, 2.0, 4.0] rhos = [0.5, 1.0, 2.0]  results = [] for tau in taus:     for rho in rhos:         x_hat, w_hat, obj, status = solve_kalman_robust_huber(A, B, C, y, tau=tau, rho=rho, solver=\"SCS\", verbose=False)         pos_rmse = rmse(x_hat[:2, :-1], pos_true)         vel_rmse = rmse(x_hat[2:, :-1], vel_true)         results.append((tau, rho, pos_rmse, vel_rmse, status))  results_sorted = sorted(results, key=lambda t: t[2]) for tau, rho, prmse, vrmse, st in results_sorted[:8]:     print(f\"tau={tau:&gt;4}, rho={rho:&gt;3} | pos_RMSE={prmse:.3f}, vel_RMSE={vrmse:.3f} | status={st}\")"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#vii-vehicle-tracking-robust-kalman-filtering-case-study","title":"VII \u2014 Vehicle Tracking \u2014 Robust Kalman Filtering Case Study\u00b6","text":"<p>We track a moving vehicle in 2D from noisy position measurements.</p> <ul> <li>State: position + velocity in x/y</li> <li>Dynamics: linear (discrete-time) with unknown input (drive force) and damping</li> <li>Noise: mostly mild Gaussian, but with occasional large outliers</li> </ul> <p>A classical Kalman smoother corresponds to a least-squares problem. To reduce sensitivity to outliers, we replace the quadratic measurement penalty with a Huber loss, yielding a convex problem solved with CVXPY.</p> <p>Adapted from the official CVXPY example \u201cRobust Kalman filtering for vehicle tracking\u201d.</p> <p>Kalman Filter Explained</p> <p>https://www.youtube.com/watch?v=R63dU5w_djQ</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#theory-and-pre-requisite-knowledge","title":"Theory and Pre-requisite Knowledge\u00b6","text":""},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#recursive-least-squares","title":"Recursive Least Squares\u00b6","text":"<p>Suppose we observe a sequence of data pairs $(\\phi_t, y_t)$, where $y_t \\in \\mathbb{R}$ is a scalar measurement and $\\phi_t \\in \\mathbb{R}^d$ is a known feature (regressor) vector. We assume a linear measurement model</p> <p>$$ y_t = \\phi_t^\\top \\theta + \\epsilon_t, \\qquad \\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2), $$</p> <p>where $\\theta \\in \\mathbb{R}^d$ is an unknown parameter vector to be estimated.</p> <p>A standard approach is least squares, which estimates $\\theta$ by minimizing the sum of squared residuals over all observed data. However, recomputing a batch least-squares solution every time a new data point arrives is computationally expensive and impractical for online or real-time applications. Recursive least squares addresses this issue by updating the parameter estimate incrementally as new measurements become available, without revisiting the full data history. A common formulation is exponentially weighted least squares:</p> <p>$$ \\hat\\theta_t = \\arg\\min_\\theta \\sum_{k=1}^t \\lambda^{\\,t-k}\\, \\big(y_k - \\phi_k^\\top \\theta\\big)^2, \\qquad 0 &lt; \\lambda \\le 1, $$</p> <p>where $\\lambda$ is a forgetting factor that discounts older data. Values of $\\lambda$ close to 1 emphasize long-term consistency, while smaller values allow the estimator to adapt more quickly to changing parameters. The RLS algorithm produces a recursive update of the parameter estimate:</p> <p>$$ \\hat\\theta_t = \\hat\\theta_{t-1} + K_t \\big(y_t - \\phi_t^\\top \\hat\\theta_{t-1}\\big), $$</p> <p>where the term</p> <p>$$ y_t - \\phi_t^\\top \\hat\\theta_{t-1} $$</p> <p>is the prediction error (or residual), measuring how well the current estimate explains the new observation. The gain vector $K_t$ determines how strongly the estimate should be corrected in response to this residual:</p> <p>$$ K_t = \\frac{P_{t-1}\\phi_t} {\\lambda + \\phi_t^\\top P_{t-1}\\phi_t}. $$</p> <p>Here, $P_t \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive-definite matrix that quantifies the uncertainty in the parameter estimate. When uncertainty is large or the new data is highly informative, the correction is larger; when uncertainty is small, updates are more conservative.</p> <p>RLS is particularly useful in online parameter estimation problems, such as system identification, adaptive control, and signal processing, where data arrives sequentially and model parameters may change slowly over time. The central idea behind RLS is to maintain both an estimate of the unknown parameters and a measure of confidence in that estimate, updating both efficiently as new information becomes available.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#kalman-filter-from-recursive-least-squares-to-dynamic-state-estimation","title":"Kalman filter: from recursive least squares to dynamic state estimation\u00b6","text":"<p>Recursive least squares is designed to estimate unknown but essentially static parameters from sequential data. While it is effective for online regression and system identification, it does not explicitly model how the quantity being estimated evolves over time. Any temporal variation must be handled indirectly through mechanisms such as forgetting factors. In many real-world problems, however, the unknown quantity is not static. The state of a physical system\u2014such as the position and velocity of a car\u2014changes continuously according to known physical laws, while also being subject to random disturbances. In these settings, we need an estimator that explicitly accounts for system dynamics rather than treating each observation as an independent regression problem.</p> <p>The Kalman filter addresses this limitation by introducing a motion model that describes how the state evolves from one time step to the next, together with a measurement model that links the hidden state to noisy sensor observations. Both models are assumed to be linear, and uncertainty is modeled using Gaussian noise.</p> <p>The objective of the Kalman filter is to estimate the hidden state of a dynamical system by optimally combining:</p> <ol> <li>predictions generated by the motion model, and</li> <li>information provided by noisy measurements.</li> </ol> <p>Like recursive least squares, the Kalman filter operates recursively and maintains both an estimate and an uncertainty matrix. However, unlike RLS, it explicitly propagates uncertainty through time using the system dynamics and corrects predictions using new measurements. This allows the filter to track time-varying states in a principled and computationally efficient manner.</p> <p>Under linear dynamics and Gaussian noise assumptions, the Kalman filter is optimal in the sense that it minimizes the mean squared estimation error and produces the posterior mean and covariance of the system state at each time step.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#state-space-model","title":"State-space model\u00b6","text":"<p>We model a discrete-time linear dynamical system as</p> <p>$$ \\begin{aligned} \\text{Motion model (State Equation):}\\quad x_{t+1} &amp;= A x_t + B u_t + w_t, \\\\ \\text{Measurement model (Observation Equation):}\\quad y_t &amp;= C x_t + v_t, \\end{aligned} \\qquad t = 0,\\dots,N-1. $$</p> <p>The motion model describes how the system state evolves from one time step to the next, while the measurement model specifies how the hidden state gives rise to observable sensor readings.</p> <p>State and observations: In a typical vehicle tracking example, the state vector is $x_t = (p_x, p_y, v_x, u_y) \\in \\mathbb{R}^4$ where $(p_x, p_y)$ denotes position and $(u_x, u_y)$ denotes velocity. The measurement vector is $y_t \\in \\mathbb{R}^2$  representing observed position data, for example from a GPS sensor.</p> <p>Inputs and noise: The model includes both control inputs and stochastic disturbances:</p> <ul> <li>$u_t$ is a known control input, such as commanded acceleration.</li> <li>$w_t$ is process (motion) noise, capturing unmodeled dynamics and random disturbances. It is typically modeled as $w_t \\sim \\mathcal{N}(0, Q)$ where $Q$ is the process noise covariance.</li> <li>$v_t$ is measurement noise, capturing sensor inaccuracies, modeled as $v_t \\sim \\mathcal{N}(0, R)$ where $R$ is the measurement noise covariance.</li> </ul> <p>Interpretation of system matrices:</p> <ul> <li>$A$ is the state transition matrix; it describes the deterministic evolution of the state.</li> <li>$B$ maps the control input $u_t$ into the state.</li> <li>$C$ maps the state to the measurement space, indicating which components of the state are observable.</li> <li>$Q$ and $R$ quantify uncertainty in the motion and measurement models, respectively.</li> </ul> <p>Importantly, $A$, $B$, and $C$ describe system structure, not uncertainty; only $Q$ and $R$ are covariance matrices.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#what-is-being-estimated","title":"What is being estimated?\u00b6","text":"<p>At each time step $t$, the Kalman filter maintains two coupled quantities:</p> <ul> <li>$\\hat x_{t+1|t}$: the estimate of the state after incorporating measurements up to time $t$,</li> <li>$P_{t|t}$: the covariance of the estimation error, $$ P_{t|t} = \\mathbb{E}\\big[(x_t - \\hat x_{t|t})(x_t - \\hat x_{t|t})^\\top\\big]. $$</li> </ul> <p>These quantities summarize all past information relevant for future estimation.</p> <p>The recursion is initialized with a prior belief</p> <p>$$ x_0 \\sim \\mathcal{N}(\\hat x_{0|0}, P_{0|0}), $$</p> <p>which encodes initial knowledge and uncertainty about the system state.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#the-two-step-recursion","title":"The two-step recursion\u00b6","text":"<p>As in recursive least squares, estimation proceeds by alternating between a prediction based on the current model and a correction driven by new data. However, here both the estimate and its uncertainty are explicitly propagated through the system dynamics.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#step-1-prediction-time-update","title":"Step 1: Prediction (time update)\u00b6","text":"<p>Using the motion model, the filter predicts the next state before observing $y_t$:</p> <p>$$ \\hat x_{t|t-1} = A \\hat x_{t-1|t-1} + B u_{t-1}. $$</p> <p>Uncertainty is propagated forward as</p> <p>$$ P_{t|t-1} = A P_{t-1|t-1} A^\\top + Q. $$</p> <p>This step extrapolates the current belief through the dynamics and increases uncertainty to account for process noise.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#step-2-update-measurement-correction","title":"Step 2: Update (measurement correction)\u00b6","text":"<p>Once a new measurement arrives, the filter computes the innovation (or surprise term):</p> <p>$$ r_t = y_t - C \\hat x_{t|t-1}, $$</p> <p>which measures the discrepancy between the observed measurement and its predicted value.</p> <p>The uncertainty associated with this discrepancy is</p> <p>$$ S_t = C P_{t|t-1} C^\\top + R. $$</p> <p>The Kalman gain is then defined as</p> <p>$$ K_t = P_{t|t-1} C^\\top S_t^{-1}. $$</p> <p>This gain determines how much the prediction should be corrected using the new measurement.</p> <p>The state estimate is updated according to</p> <p>$$ \\hat x_{t|t} = \\hat x_{t|t-1} + K_t r_t, $$</p> <p>and the corresponding uncertainty is reduced as</p> <p>$$ P_{t|t} = (I - K_t C) P_{t|t-1}. $$</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#interpretation","title":"Interpretation\u00b6","text":"<ul> <li>Large measurement noise (large $R$) leads to a smaller Kalman gain, causing the filter to rely more heavily on the model prediction.</li> <li>Large prediction uncertainty (large $P_{t|t-1}$) leads to a larger Kalman gain, causing the filter to rely more heavily on the measurement.</li> </ul> <p>Thus, the Kalman filter automatically balances model-based prediction and data-driven correction in an uncertainty-aware manner, extending the core ideas of recursive least squares to time-evolving dynamical systems.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#1-problem-setup","title":"1. Problem setup\u00b6","text":"<p>Give breief introd to Recursive leastime qaure estimator?</p> <p>Kalman alos include a motion model</p> <p>We model a discrete-time linear dynamical system:</p> <p>$$ \\begin{aligned} {Motion Model}:\\\\ x_{t+1} &amp;= A x_t + B u_t + w_{t} \\\\ {Measurement Model}:\\\\ y_t &amp;= C x_t + v_t, \\end{aligned} \\qquad t=0,\\dots,N-1 $$</p> <ul> <li>$x_t \\in \\mathbb{R}^4$: state $=(p_x,p_y,u_x,u_y)$ (position + velocity)</li> <li>$u_t \\in \\mathbb{R}^2$: unknown input (drive force in x/y)</li> <li>$w_t \\in \\mathbb{R}^2$: Process or Mitoion noise $2_t ~Normal (0, Q_t)$</li> <li>$y_t \\in \\mathbb{R}^2$: observed position (Measuring insturement such as GPS)</li> <li>$v_t \\in \\mathbb{R}^2$: measurement noise  $v ~Norma (0, R_t$)$</li> <li>A is covraiance matric?</li> <li>B?</li> <li>C?</li> </ul> <p>Question: how to best estimate the posiiton of car using initial estiamte and measurment both of whcih are noise? Kalman filter is an algorithm to estimate the state of the system using past and possibley observations and current and possibly noisy measurments of that systement - more specidfically?</p> <p>Its a two step process:</p> <ol> <li>Pedictition step $X^hat_t+1 = A x_t + B_u_t$ $</li> <li>Update step</li> </ol> <p>Suprise term? Kalman Ratio?</p> <p>Oprimal estimation algorithm</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#standard-quadratic-kalman-smoothing","title":"Standard (quadratic) Kalman smoothing\u00b6","text":"<p>$$ \\begin{aligned} \\min_{\\{x_t,w_t,v_t\\}} &amp;\\sum_{t=0}^{N-1} \\left(\\|w_t\\|_2^2 + \\tau\\,\\|v_t\\|_2^2\\right) \\\\ \\text{s.t. } &amp;x_{t+1}=Ax_t + Bw_t,\\quad y_t=Cx_t + v_t. \\end{aligned} $$</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#robust-kalman-smoothing-huber-measurements","title":"Robust Kalman smoothing (Huber measurements)\u00b6","text":"<p>Define the Huber penalty $$ \\phi_\\rho(a)= \\begin{cases} \\|a\\|_2^2, &amp; \\|a\\|_2\\le \\rho \\\\ 2\\rho\\|a\\|_2 - \\rho^2, &amp; \\|a\\|_2 &gt; \\rho \\end{cases} $$</p> <p>and solve $$ \\min \\sum_{t=0}^{N-1}\\left(\\|w_t\\|_2^2 + \\tau\\,\\phi_\\rho(v_t)\\right) \\quad \\text{s.t. dynamics + measurement constraints.} $$</p> <p>Interpretation: small residuals are penalized quadratically, but large residuals only linearly, reducing outlier influence.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#2-helper-plotting-functions","title":"2. Helper plotting functions\u00b6","text":""},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#3-generate-synthetic-tracking-data-with-outliers","title":"3. Generate synthetic tracking data (with outliers)\u00b6","text":"<p>We simulate $N$ timesteps. The vehicle starts at the origin with zero velocity.</p> <ul> <li>Inputs $w_t$ are standard Gaussian.</li> <li>Measurement noise $v_t$ is standard Gaussian except that a fraction $p$ of timesteps are outliers with a much larger standard deviation $\\sigma$.</li> </ul>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#4-standard-kalman-smoothing-as-least-squares-cvxpy","title":"4. Standard Kalman smoothing as least squares (CVXPY)\u00b6","text":"<p>We solve the quadratic objective (least squares). This tends to \u201cchase\u201d outliers because large residuals are penalized quadratically.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#5-robust-kalman-smoothing-huber-measurements","title":"5. Robust Kalman smoothing (Huber measurements)\u00b6","text":"<p>We replace the quadratic measurement penalty with a Huber penalty on the residual norm $\\|v_t\\|_2$.</p> <p>In CVXPY: <code>huber(norm(v[:,t]), rho)</code></p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#6-quantitative-comparison","title":"6. Quantitative comparison\u00b6","text":"<p>We compare RMSE of position and velocity:</p> <ul> <li>Position RMSE uses $(p_x,p_y)$</li> <li>Velocity RMSE uses $(u_x,u_y)$</li> </ul>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#7-residual-diagnostics-who-is-chasing-outliers","title":"7. Residual diagnostics (who is \u201cchasing\u201d outliers?)\u00b6","text":"<p>The estimated measurement residual is $\\hat v_t = y_t - C\\hat x_t$. We compare residual norms for quadratic vs robust smoothing and mark outlier timesteps.</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#8-hyperparameters-tau-rho-quick-sweep","title":"8. Hyperparameters ($\\tau$, $\\rho$): quick sweep\u00b6","text":"<ul> <li>Larger $\tau$ forces the model to fit measurements more (less smoothing).</li> <li>Smaller $ ho$ makes the Huber loss switch to linear earlier (more robust).</li> </ul> <p>We do a tiny sweep (keep it small to avoid long runtimes).</p>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#9-takeaways","title":"9. Takeaways\u00b6","text":"<ol> <li>Standard quadratic smoothing is optimal under Gaussian noise, but outliers can warp the estimate.</li> <li>A Huber measurement penalty is a clean convex way to robustify the estimator.</li> <li>CVXPY makes it easy to switch between models by changing only the objective.</li> </ol>"},{"location":"convex/tutorials/7_robust_kalman_vehicle_tracking/#exercises-good-for-reports-assignments","title":"Exercises (good for reports / assignments)\u00b6","text":"<ul> <li>Set <code>p_outlier = 0</code> and verify quadratic and robust behave similarly.</li> <li>Increase <code>sigma_outlier</code> and see when the quadratic model collapses.</li> <li>Replace the Huber penalty with an $\\ell_1$ penalty on $v_t$ and compare.</li> <li>Try different solvers (e.g., ECOS if available) and compare runtime/accuracy.</li> </ul>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/","title":"Taxi / Ride-Hailing Fleet Repositioning with Rolling-Horizon Convex Optimization (MPC)","text":"In\u00a0[7]: Copied! <pre>#!pip install pyarrow\n</pre> #!pip install pyarrow In\u00a0[1]: Copied! <pre>import os\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport pyarrow \nimport cvxpy as cp\n\nSEED = 7\nrng = np.random.default_rng(SEED)\n\nprint(\"cvxpy installed solvers:\", cp.installed_solvers())\n</pre>  import os import math import numpy as np import pandas as pd import matplotlib.pyplot as plt from tqdm.auto import tqdm import pyarrow  import cvxpy as cp  SEED = 7 rng = np.random.default_rng(SEED)  print(\"cvxpy installed solvers:\", cp.installed_solvers()) <pre>c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>cvxpy installed solvers: ['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLOP', 'GLPK', 'GLPK_MI', 'OSQP', 'PDLP', 'SCIPY', 'SCS']\n</pre> In\u00a0[2]: Copied! <pre>import urllib.request\n\nDATA_DIR = \"data_tlc\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Choose a month. You can change this to any month available on the TLC page.\nMONTH = \"2024-01\"\n\nYELLOW_URL = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{MONTH}.parquet\"\nZONE_URL   = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n\nYELLOW_PATH = os.path.join(DATA_DIR, f\"yellow_tripdata_{MONTH}.parquet\")\nZONE_PATH   = os.path.join(DATA_DIR, \"taxi_zone_lookup.csv\")\n\nDOWNLOAD = True\n\ndef download(url: str, path: str):\n    if os.path.exists(path):\n        print(f\"\u2705 Exists: {path}\")\n        return\n    print(f\"\u2b07\ufe0f Downloading: {url}\")\n    urllib.request.urlretrieve(url, path)\n    print(f\"\u2705 Saved to: {path}\")\n\nif DOWNLOAD:\n    download(ZONE_URL, ZONE_PATH)\n    download(YELLOW_URL, YELLOW_PATH)\n</pre>  import urllib.request  DATA_DIR = \"data_tlc\" os.makedirs(DATA_DIR, exist_ok=True)  # Choose a month. You can change this to any month available on the TLC page. MONTH = \"2024-01\"  YELLOW_URL = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{MONTH}.parquet\" ZONE_URL   = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"  YELLOW_PATH = os.path.join(DATA_DIR, f\"yellow_tripdata_{MONTH}.parquet\") ZONE_PATH   = os.path.join(DATA_DIR, \"taxi_zone_lookup.csv\")  DOWNLOAD = True  def download(url: str, path: str):     if os.path.exists(path):         print(f\"\u2705 Exists: {path}\")         return     print(f\"\u2b07\ufe0f Downloading: {url}\")     urllib.request.urlretrieve(url, path)     print(f\"\u2705 Saved to: {path}\")  if DOWNLOAD:     download(ZONE_URL, ZONE_PATH)     download(YELLOW_URL, YELLOW_PATH) <pre>\u2705 Exists: data_tlc\\taxi_zone_lookup.csv\n\u2705 Exists: data_tlc\\yellow_tripdata_2024-01.parquet\n</pre> In\u00a0[14]: Copied! <pre># Column subset (keeps memory reasonable)\nCOLS = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\"]\n\n# Optional row sampling after load (set to None for full month)\nN_SAMPLE = 3_000_000  # adjust based on your machine\n\ndf = pd.read_parquet(YELLOW_PATH, columns=COLS,  engine=\"pyarrow\")\n\n# Basic cleaning\ndf = df.dropna(subset=[\"PULocationID\", \"DOLocationID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]).copy()\n\n\n# Ensure integer zone IDs\ndf[\"PULocationID\"] = df[\"PULocationID\"].astype(int)\ndf[\"DOLocationID\"] = df[\"DOLocationID\"].astype(int)\n\npickup  = pd.to_datetime(df[\"tpep_pickup_datetime\"])\ndropoff = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\ndf[\"duration_min\"] = (dropoff - pickup).dt.total_seconds() / 60.0\ndf['pickup_dt'] = pickup\n\ndf.head()\n</pre> # Column subset (keeps memory reasonable) COLS = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\"]  # Optional row sampling after load (set to None for full month) N_SAMPLE = 3_000_000  # adjust based on your machine  df = pd.read_parquet(YELLOW_PATH, columns=COLS,  engine=\"pyarrow\")  # Basic cleaning df = df.dropna(subset=[\"PULocationID\", \"DOLocationID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]).copy()   # Ensure integer zone IDs df[\"PULocationID\"] = df[\"PULocationID\"].astype(int) df[\"DOLocationID\"] = df[\"DOLocationID\"].astype(int)  pickup  = pd.to_datetime(df[\"tpep_pickup_datetime\"]) dropoff = pd.to_datetime(df[\"tpep_dropoff_datetime\"]) df[\"duration_min\"] = (dropoff - pickup).dt.total_seconds() / 60.0 df['pickup_dt'] = pickup  df.head()  Out[14]: tpep_pickup_datetime tpep_dropoff_datetime PULocationID DOLocationID duration_min pickup_dt 0 2024-01-01 00:57:55 2024-01-01 01:17:43 186 79 19.800000 2024-01-01 00:57:55 1 2024-01-01 00:03:00 2024-01-01 00:09:36 140 236 6.600000 2024-01-01 00:03:00 2 2024-01-01 00:17:06 2024-01-01 00:35:01 236 79 17.916667 2024-01-01 00:17:06 3 2024-01-01 00:36:38 2024-01-01 00:44:56 79 211 8.300000 2024-01-01 00:36:38 4 2024-01-01 00:46:51 2024-01-01 00:52:57 211 148 6.100000 2024-01-01 00:46:51 In\u00a0[12]: Copied! <pre>zones = pd.read_csv(ZONE_PATH)\nzones\n</pre> zones = pd.read_csv(ZONE_PATH) zones Out[12]: LocationID Borough Zone service_zone 0 1 EWR Newark Airport EWR 1 2 Queens Jamaica Bay Boro Zone 2 3 Bronx Allerton/Pelham Gardens Boro Zone 3 4 Manhattan Alphabet City Yellow Zone 4 5 Staten Island Arden Heights Boro Zone ... ... ... ... ... 260 261 Manhattan World Trade Center Yellow Zone 261 262 Manhattan Yorkville East Yellow Zone 262 263 Manhattan Yorkville West Yellow Zone 263 264 Unknown NaN NaN 264 265 NaN Outside of NYC NaN <p>265 rows \u00d7 4 columns</p> In\u00a0[15]: Copied! <pre># train/test split\nmonth_start = df[\"pickup_dt\"].min().normalize()\ntrain_start = month_start\ntrain_end   = train_start + pd.Timedelta(days=21)\ntest_start  = train_end\ntest_end    = test_start + pd.Timedelta(days=7)\n\ndf_train = df[(df[\"pickup_dt\"] &gt;= train_start) &amp; (df[\"pickup_dt\"] &lt; train_end)].copy()\ndf_test  = df[(df[\"pickup_dt\"] &gt;= test_start)  &amp; (df[\"pickup_dt\"] &lt; test_end)].copy()\n\nprint(\"Train window:\", train_start, \"to\", train_end, \"rows:\", len(df_train))\nprint(\"Test  window:\", test_start,  \"to\", test_end,  \"rows:\", len(df_test))\n</pre> # train/test split month_start = df[\"pickup_dt\"].min().normalize() train_start = month_start train_end   = train_start + pd.Timedelta(days=21) test_start  = train_end test_end    = test_start + pd.Timedelta(days=7)  df_train = df[(df[\"pickup_dt\"] &gt;= train_start) &amp; (df[\"pickup_dt\"] &lt; train_end)].copy() df_test  = df[(df[\"pickup_dt\"] &gt;= test_start)  &amp; (df[\"pickup_dt\"] &lt; test_end)].copy()  print(\"Train window:\", train_start, \"to\", train_end, \"rows:\", len(df_train)) print(\"Test  window:\", test_start,  \"to\", test_end,  \"rows:\", len(df_test))  <pre>Train window: 2002-12-31 00:00:00 to 2003-01-21 00:00:00 rows: 2\nTest  window: 2003-01-21 00:00:00 to 2003-01-28 00:00:00 rows: 0\n</pre> In\u00a0[21]: Copied! <pre>DT_MIN = 15\nK_ZONES = 100              # keep moderate; increase if you want\nH = 12                    # horizon length in steps (12*15min = 3 hours)\nTAU_MAX = 8               # max travel-time lag in steps (8*15min = 2 hours)\n\ntop_zones = df_train[\"PULocationID\"].value_counts().head(K_ZONES).index.to_numpy()\nzone_to_idx = {z:i for i,z in enumerate(top_zones)}\nidx_to_zone = {i:z for z,i in zone_to_idx.items()}\n\nprint(\"Example zones:\", top_zones[:10])\n</pre>  DT_MIN = 15 K_ZONES = 100              # keep moderate; increase if you want H = 12                    # horizon length in steps (12*15min = 3 hours) TAU_MAX = 8               # max travel-time lag in steps (8*15min = 2 hours)  top_zones = df_train[\"PULocationID\"].value_counts().head(K_ZONES).index.to_numpy() zone_to_idx = {z:i for i,z in enumerate(top_zones)} idx_to_zone = {i:z for z,i in zone_to_idx.items()}  print(\"Example zones:\", top_zones[:10]) <pre>Example zones: [170]\n</pre> In\u00a0[22]: Copied! <pre>def discretize_time(ts: pd.Series, dt_min: int) -&gt; pd.Series:\n    return ts.dt.floor(f\"{dt_min}min\")\n\ndf_train_k = df_train[df_train[\"PULocationID\"].isin(top_zones) &amp; df_train[\"DOLocationID\"].isin(top_zones)].copy()\ndf_test_k  = df_test[df_test[\"PULocationID\"].isin(top_zones) &amp; df_test[\"DOLocationID\"].isin(top_zones)].copy()\n\ndf_train_k[\"tbin\"] = discretize_time(df_train_k[\"pickup_dt\"], DT_MIN)\ndf_test_k[\"tbin\"]  = discretize_time(df_test_k[\"pickup_dt\"], DT_MIN)\n\ndf_train_k\n</pre> def discretize_time(ts: pd.Series, dt_min: int) -&gt; pd.Series:     return ts.dt.floor(f\"{dt_min}min\")  df_train_k = df_train[df_train[\"PULocationID\"].isin(top_zones) &amp; df_train[\"DOLocationID\"].isin(top_zones)].copy() df_test_k  = df_test[df_test[\"PULocationID\"].isin(top_zones) &amp; df_test[\"DOLocationID\"].isin(top_zones)].copy()  df_train_k[\"tbin\"] = discretize_time(df_train_k[\"pickup_dt\"], DT_MIN) df_test_k[\"tbin\"]  = discretize_time(df_test_k[\"pickup_dt\"], DT_MIN)  df_train_k  Out[22]: tpep_pickup_datetime tpep_dropoff_datetime PULocationID DOLocationID duration_min pickup_dt tbin 53119 2002-12-31 22:59:39 2002-12-31 23:05:41 170 170 6.033333 2002-12-31 22:59:39 2002-12-31 22:45:00 53120 2002-12-31 22:59:39 2002-12-31 23:05:41 170 170 6.033333 2002-12-31 22:59:39 2002-12-31 22:45:00 In\u00a0[\u00a0]: Copied! <pre>t_index = pd.date_range(df_test_k[\"tbin\"].min(), df_test_k[\"tbin\"].max(), freq=f\"{DT_MIN}min\")\nt_to_idx = {t:i for i,t in enumerate(t_index)}\n\nT_test = len(t_index)\nK = len(top_zones)\n\nd_real = np.zeros((T_test, K, K), dtype=np.float32)\n\nfor row in df_test_k.itertuples(index=False):\n    t = row.tbin\n    if t not in t_to_idx:\n        continue\n    ti = t_to_idx[t]\n    i = zone_to_idx[row.PULocationID]\n    j = zone_to_idx[row.DOLocationID]\n    d_real[ti, i, j] += 1.0\n\nprint(\"Test tensor shape:\", d_real.shape, \"nonzeros:\", np.count_nonzero(d_real))\n</pre>  t_index = pd.date_range(df_test_k[\"tbin\"].min(), df_test_k[\"tbin\"].max(), freq=f\"{DT_MIN}min\") t_to_idx = {t:i for i,t in enumerate(t_index)}  T_test = len(t_index) K = len(top_zones)  d_real = np.zeros((T_test, K, K), dtype=np.float32)  for row in df_test_k.itertuples(index=False):     t = row.tbin     if t not in t_to_idx:         continue     ti = t_to_idx[t]     i = zone_to_idx[row.PULocationID]     j = zone_to_idx[row.DOLocationID]     d_real[ti, i, j] += 1.0  print(\"Test tensor shape:\", d_real.shape, \"nonzeros:\", np.count_nonzero(d_real)) In\u00a0[\u00a0]: Copied! <pre>od_med = (df_train_k\n          .groupby([\"PULocationID\", \"DOLocationID\"])[\"duration_min\"]\n          .median()\n          .reset_index())\n\nglobal_med = float(df_train_k[\"duration_min\"].median())\ntt_min = np.full((K, K), global_med, dtype=np.float32)\n\nfor row in od_med.itertuples(index=False):\n    i = zone_to_idx[row.PULocationID]\n    j = zone_to_idx[row.DOLocationID]\n    tt_min[i, j] = float(row.duration_min)\n\ntau = np.clip(np.ceil(tt_min / DT_MIN).astype(int), 1, TAU_MAX)\nc_repo = tt_min.copy()\n\nprint(\"Median duration (min) global:\", global_med)\nprint(\"tau stats (steps): min\", tau.min(), \"max\", tau.max())\n</pre>  od_med = (df_train_k           .groupby([\"PULocationID\", \"DOLocationID\"])[\"duration_min\"]           .median()           .reset_index())  global_med = float(df_train_k[\"duration_min\"].median()) tt_min = np.full((K, K), global_med, dtype=np.float32)  for row in od_med.itertuples(index=False):     i = zone_to_idx[row.PULocationID]     j = zone_to_idx[row.DOLocationID]     tt_min[i, j] = float(row.duration_min)  tau = np.clip(np.ceil(tt_min / DT_MIN).astype(int), 1, TAU_MAX) c_repo = tt_min.copy()  print(\"Median duration (min) global:\", global_med) print(\"tau stats (steps): min\", tau.min(), \"max\", tau.max()) In\u00a0[\u00a0]: Copied! <pre>steps_per_day = int(24*60/DT_MIN)\n\ndf_train_k[\"weekday\"] = df_train_k[\"tbin\"].dt.weekday\ndf_train_k[\"step_of_day\"] = (df_train_k[\"tbin\"].dt.hour * 60 + df_train_k[\"tbin\"].dt.minute) // DT_MIN\ndf_train_k[\"date\"] = df_train_k[\"tbin\"].dt.date\n\nweekday_days = df_train_k.groupby(\"weekday\")[\"date\"].nunique().to_dict()\n\ngrp = (df_train_k\n       .groupby([\"weekday\",\"step_of_day\",\"PULocationID\",\"DOLocationID\"])\n       .size()\n       .reset_index(name=\"cnt\"))\n\navg = np.zeros((7, steps_per_day, K, K), dtype=np.float32)\n\nfor row in grp.itertuples(index=False):\n    w, s = int(row.weekday), int(row.step_of_day)\n    i = zone_to_idx[row.PULocationID]\n    j = zone_to_idx[row.DOLocationID]\n    denom = max(1, weekday_days.get(w, 1))\n    avg[w, s, i, j] = float(row.cnt) / denom\n\ndef forecast_demand(t0: pd.Timestamp, H: int) -&gt; np.ndarray:\n    out = np.zeros((H, K, K), dtype=np.float32)\n    for h in range(H):\n        th = t0 + pd.Timedelta(minutes=DT_MIN*h)\n        w = th.weekday()\n        s = (th.hour*60 + th.minute)//DT_MIN\n        out[h] = avg[w, s]\n    return out\n\nprint(\"avg tensor shape:\", avg.shape)\n</pre>  steps_per_day = int(24*60/DT_MIN)  df_train_k[\"weekday\"] = df_train_k[\"tbin\"].dt.weekday df_train_k[\"step_of_day\"] = (df_train_k[\"tbin\"].dt.hour * 60 + df_train_k[\"tbin\"].dt.minute) // DT_MIN df_train_k[\"date\"] = df_train_k[\"tbin\"].dt.date  weekday_days = df_train_k.groupby(\"weekday\")[\"date\"].nunique().to_dict()  grp = (df_train_k        .groupby([\"weekday\",\"step_of_day\",\"PULocationID\",\"DOLocationID\"])        .size()        .reset_index(name=\"cnt\"))  avg = np.zeros((7, steps_per_day, K, K), dtype=np.float32)  for row in grp.itertuples(index=False):     w, s = int(row.weekday), int(row.step_of_day)     i = zone_to_idx[row.PULocationID]     j = zone_to_idx[row.DOLocationID]     denom = max(1, weekday_days.get(w, 1))     avg[w, s, i, j] = float(row.cnt) / denom  def forecast_demand(t0: pd.Timestamp, H: int) -&gt; np.ndarray:     out = np.zeros((H, K, K), dtype=np.float32)     for h in range(H):         th = t0 + pd.Timedelta(minutes=DT_MIN*h)         w = th.weekday()         s = (th.hour*60 + th.minute)//DT_MIN         out[h] = avg[w, s]     return out  print(\"avg tensor shape:\", avg.shape) In\u00a0[\u00a0]: Copied! <pre>def solve_mpc_lp(\n    s0: np.ndarray,\n    d_hat: np.ndarray,     # (H,K,K)\n    c_repo: np.ndarray,    # (K,K)\n    tau: np.ndarray,       # (K,K) integer steps\n    lambda_unmet: float = 5.0,\n    eta_smooth: float = 0.0,\n    solver: str | None = None,\n):\n    H, K, _ = d_hat.shape\n    s0 = s0.astype(float)\n\n    x = cp.Variable((H, K, K), nonneg=True)\n    r = cp.Variable((H, K, K), nonneg=True)\n    s = cp.Variable((H+1, K), nonneg=True)\n    u = cp.Variable((H, K, K), nonneg=True)\n\n    cons = [s[0, :] == s0, x &lt;= d_hat, u == d_hat - x]\n\n    for t in range(H):\n        cons += [cp.sum(x[t, :, :], axis=1) + cp.sum(r[t, :, :], axis=1) &lt;= s[t, :]]\n\n    for t in range(H):\n        departures = cp.sum(x[t, :, :], axis=1) + cp.sum(r[t, :, :], axis=1)\n\n        arrivals_expr = []\n        for i in range(K):\n            incoming_terms = []\n            for k in range(K):\n                lag = int(tau[k, i])\n                dep_t = t - lag + 1\n                if 0 &lt;= dep_t &lt; H:\n                    incoming_terms.append(x[dep_t, k, i] + r[dep_t, k, i])\n            arrivals_expr.append(cp.sum(cp.hstack(incoming_terms)) if incoming_terms else 0.0)\n\n        arrivals = cp.hstack(arrivals_expr)\n        cons += [s[t+1, :] == s[t, :] - departures + arrivals]\n\n    obj = cp.sum(cp.multiply(c_repo[None, :, :], r)) + lambda_unmet * cp.sum(u)\n    if eta_smooth &gt; 0:\n        obj += eta_smooth * cp.sum_squares(s[1:, :] - s[:-1, :])\n\n    prob = cp.Problem(cp.Minimize(obj), cons)\n\n    if solver is None:\n        for cand in [\"ECOS\", \"OSQP\", \"CLARABEL\", \"SCS\"]:\n            if cand in cp.installed_solvers():\n                solver = cand\n                break\n\n    prob.solve(solver=solver, verbose=False)\n    if prob.status not in (\"optimal\", \"optimal_inaccurate\"):\n        raise RuntimeError(f\"MPC solve failed: status={prob.status}\")\n\n    x0 = np.maximum(x.value[0], 0)\n    r0 = np.maximum(r.value[0], 0)\n    return x0, r0, float(prob.value), prob.status, solver\n</pre>  def solve_mpc_lp(     s0: np.ndarray,     d_hat: np.ndarray,     # (H,K,K)     c_repo: np.ndarray,    # (K,K)     tau: np.ndarray,       # (K,K) integer steps     lambda_unmet: float = 5.0,     eta_smooth: float = 0.0,     solver: str | None = None, ):     H, K, _ = d_hat.shape     s0 = s0.astype(float)      x = cp.Variable((H, K, K), nonneg=True)     r = cp.Variable((H, K, K), nonneg=True)     s = cp.Variable((H+1, K), nonneg=True)     u = cp.Variable((H, K, K), nonneg=True)      cons = [s[0, :] == s0, x &lt;= d_hat, u == d_hat - x]      for t in range(H):         cons += [cp.sum(x[t, :, :], axis=1) + cp.sum(r[t, :, :], axis=1) &lt;= s[t, :]]      for t in range(H):         departures = cp.sum(x[t, :, :], axis=1) + cp.sum(r[t, :, :], axis=1)          arrivals_expr = []         for i in range(K):             incoming_terms = []             for k in range(K):                 lag = int(tau[k, i])                 dep_t = t - lag + 1                 if 0 &lt;= dep_t &lt; H:                     incoming_terms.append(x[dep_t, k, i] + r[dep_t, k, i])             arrivals_expr.append(cp.sum(cp.hstack(incoming_terms)) if incoming_terms else 0.0)          arrivals = cp.hstack(arrivals_expr)         cons += [s[t+1, :] == s[t, :] - departures + arrivals]      obj = cp.sum(cp.multiply(c_repo[None, :, :], r)) + lambda_unmet * cp.sum(u)     if eta_smooth &gt; 0:         obj += eta_smooth * cp.sum_squares(s[1:, :] - s[:-1, :])      prob = cp.Problem(cp.Minimize(obj), cons)      if solver is None:         for cand in [\"ECOS\", \"OSQP\", \"CLARABEL\", \"SCS\"]:             if cand in cp.installed_solvers():                 solver = cand                 break      prob.solve(solver=solver, verbose=False)     if prob.status not in (\"optimal\", \"optimal_inaccurate\"):         raise RuntimeError(f\"MPC solve failed: status={prob.status}\")      x0 = np.maximum(x.value[0], 0)     r0 = np.maximum(r.value[0], 0)     return x0, r0, float(prob.value), prob.status, solver In\u00a0[\u00a0]: Copied! <pre>from dataclasses import dataclass\n\n@dataclass\nclass SimResult:\n    service_rate: np.ndarray\n    unmet: np.ndarray\n    deadhead_cost: np.ndarray\n    idle_total: np.ndarray\n    solver_time: np.ndarray\n    x_served_total: float\n    demand_total: float\n    deadhead_total: float\n\ndef run_backtest(policy: str, H_policy: int, lambda_unmet=5.0, eta_smooth=0.0, fleet_size=4000):\n    pickup_counts = df_train_k[\"PULocationID\"].value_counts().reindex(top_zones).fillna(0).to_numpy(dtype=float)\n    p = pickup_counts / max(1.0, pickup_counts.sum())\n    s_idle = np.round(fleet_size * p).astype(float)\n\n    transit = [np.zeros(K, dtype=float) for _ in range(TAU_MAX)]\n\n    service_rate = np.zeros(T_test, dtype=float)\n    unmet = np.zeros(T_test, dtype=float)\n    deadhead_cost = np.zeros(T_test, dtype=float)\n    idle_total = np.zeros(T_test, dtype=float)\n    solver_time = np.zeros(T_test, dtype=float)\n\n    x_served_total = 0.0\n    demand_total = float(d_real.sum())\n    deadhead_total = 0.0\n\n    for t in tqdm(range(T_test), desc=f\"Backtest: {policy}\"):\n        arrivals_now = transit.pop(0)\n        transit.append(np.zeros(K, dtype=float))\n        s_idle += arrivals_now\n\n        idle_total[t] = s_idle.sum()\n        d_t = d_real[t].astype(float)\n\n        if policy == \"none\":\n            x_plan = d_t.copy()\n            r_plan = np.zeros((K, K), dtype=float)\n        else:\n            t0 = t_index[t]\n            d_hat = forecast_demand(t0, H_policy)\n\n            import time\n            t_start = time.time()\n            x0, r0, _, _, _ = solve_mpc_lp(\n                s0=s_idle,\n                d_hat=d_hat,\n                c_repo=c_repo,\n                tau=tau,\n                lambda_unmet=lambda_unmet,\n                eta_smooth=eta_smooth,\n            )\n            solver_time[t] = time.time() - t_start\n            x_plan, r_plan = x0, r0\n\n        x_serv = np.minimum(x_plan, d_t)\n\n        for i in range(K):\n            avail = s_idle[i]\n            serve_out = x_serv[i, :].sum()\n            if serve_out &gt; avail + 1e-9:\n                x_serv[i, :] *= (avail / serve_out)\n                serve_out = avail\n\n            avail_left = avail - serve_out\n            repo_out = r_plan[i, :].sum()\n            if repo_out &gt; avail_left + 1e-9 and repo_out &gt; 0:\n                r_plan[i, :] *= (avail_left / repo_out)\n\n        dep = x_serv.sum(axis=1) + r_plan.sum(axis=1)\n        s_idle -= dep\n        s_idle = np.maximum(s_idle, 0.0)\n\n        for i in range(K):\n            for j in range(K):\n                flow = x_serv[i, j] + r_plan[i, j]\n                if flow &lt;= 0:\n                    continue\n                lag = int(tau[i, j])\n                lag = max(1, min(TAU_MAX, lag))\n                transit[lag-1][j] += flow\n\n        served = float(x_serv.sum())\n        unmet_t = float(d_t.sum() - served)\n        cost_t = float((c_repo * r_plan).sum())\n\n        x_served_total += served\n        deadhead_total += cost_t\n\n        service_rate[t] = served / (float(d_t.sum()) + 1e-9)\n        unmet[t] = unmet_t\n        deadhead_cost[t] = cost_t\n\n    return SimResult(\n        service_rate=service_rate,\n        unmet=unmet,\n        deadhead_cost=deadhead_cost,\n        idle_total=idle_total,\n        solver_time=solver_time,\n        x_served_total=x_served_total,\n        demand_total=demand_total,\n        deadhead_total=deadhead_total,\n    )\n</pre>  from dataclasses import dataclass  @dataclass class SimResult:     service_rate: np.ndarray     unmet: np.ndarray     deadhead_cost: np.ndarray     idle_total: np.ndarray     solver_time: np.ndarray     x_served_total: float     demand_total: float     deadhead_total: float  def run_backtest(policy: str, H_policy: int, lambda_unmet=5.0, eta_smooth=0.0, fleet_size=4000):     pickup_counts = df_train_k[\"PULocationID\"].value_counts().reindex(top_zones).fillna(0).to_numpy(dtype=float)     p = pickup_counts / max(1.0, pickup_counts.sum())     s_idle = np.round(fleet_size * p).astype(float)      transit = [np.zeros(K, dtype=float) for _ in range(TAU_MAX)]      service_rate = np.zeros(T_test, dtype=float)     unmet = np.zeros(T_test, dtype=float)     deadhead_cost = np.zeros(T_test, dtype=float)     idle_total = np.zeros(T_test, dtype=float)     solver_time = np.zeros(T_test, dtype=float)      x_served_total = 0.0     demand_total = float(d_real.sum())     deadhead_total = 0.0      for t in tqdm(range(T_test), desc=f\"Backtest: {policy}\"):         arrivals_now = transit.pop(0)         transit.append(np.zeros(K, dtype=float))         s_idle += arrivals_now          idle_total[t] = s_idle.sum()         d_t = d_real[t].astype(float)          if policy == \"none\":             x_plan = d_t.copy()             r_plan = np.zeros((K, K), dtype=float)         else:             t0 = t_index[t]             d_hat = forecast_demand(t0, H_policy)              import time             t_start = time.time()             x0, r0, _, _, _ = solve_mpc_lp(                 s0=s_idle,                 d_hat=d_hat,                 c_repo=c_repo,                 tau=tau,                 lambda_unmet=lambda_unmet,                 eta_smooth=eta_smooth,             )             solver_time[t] = time.time() - t_start             x_plan, r_plan = x0, r0          x_serv = np.minimum(x_plan, d_t)          for i in range(K):             avail = s_idle[i]             serve_out = x_serv[i, :].sum()             if serve_out &gt; avail + 1e-9:                 x_serv[i, :] *= (avail / serve_out)                 serve_out = avail              avail_left = avail - serve_out             repo_out = r_plan[i, :].sum()             if repo_out &gt; avail_left + 1e-9 and repo_out &gt; 0:                 r_plan[i, :] *= (avail_left / repo_out)          dep = x_serv.sum(axis=1) + r_plan.sum(axis=1)         s_idle -= dep         s_idle = np.maximum(s_idle, 0.0)          for i in range(K):             for j in range(K):                 flow = x_serv[i, j] + r_plan[i, j]                 if flow &lt;= 0:                     continue                 lag = int(tau[i, j])                 lag = max(1, min(TAU_MAX, lag))                 transit[lag-1][j] += flow          served = float(x_serv.sum())         unmet_t = float(d_t.sum() - served)         cost_t = float((c_repo * r_plan).sum())          x_served_total += served         deadhead_total += cost_t          service_rate[t] = served / (float(d_t.sum()) + 1e-9)         unmet[t] = unmet_t         deadhead_cost[t] = cost_t      return SimResult(         service_rate=service_rate,         unmet=unmet,         deadhead_cost=deadhead_cost,         idle_total=idle_total,         solver_time=solver_time,         x_served_total=x_served_total,         demand_total=demand_total,         deadhead_total=deadhead_total,     ) In\u00a0[\u00a0]: Copied! <pre># Run policies\nres_none = run_backtest(\"none\",   H_policy=1,  fleet_size=4000)\nres_myo  = run_backtest(\"myopic\", H_policy=1,  fleet_size=4000, lambda_unmet=8.0)\nres_mpc  = run_backtest(\"mpc\",    H_policy=H,  fleet_size=4000, lambda_unmet=8.0, eta_smooth=1e-3)\n\ndef summarize(name, res: SimResult):\n    return {\n        \"Policy\": name,\n        \"Total demand\": res.demand_total,\n        \"Total served\": res.x_served_total,\n        \"Service rate\": res.x_served_total / max(1e-9, res.demand_total),\n        \"Total deadhead cost\": res.deadhead_total,\n        \"Avg solver time (s)\": float(np.mean(res.solver_time[res.solver_time&gt;0])) if np.any(res.solver_time&gt;0) else 0.0,\n    }\n\nsummary = pd.DataFrame([\n    summarize(\"No reposition\", res_none),\n    summarize(\"Myopic H=1\",    res_myo),\n    summarize(f\"MPC H={H}\",    res_mpc),\n])\n\nsummary\n</pre>  # Run policies res_none = run_backtest(\"none\",   H_policy=1,  fleet_size=4000) res_myo  = run_backtest(\"myopic\", H_policy=1,  fleet_size=4000, lambda_unmet=8.0) res_mpc  = run_backtest(\"mpc\",    H_policy=H,  fleet_size=4000, lambda_unmet=8.0, eta_smooth=1e-3)  def summarize(name, res: SimResult):     return {         \"Policy\": name,         \"Total demand\": res.demand_total,         \"Total served\": res.x_served_total,         \"Service rate\": res.x_served_total / max(1e-9, res.demand_total),         \"Total deadhead cost\": res.deadhead_total,         \"Avg solver time (s)\": float(np.mean(res.solver_time[res.solver_time&gt;0])) if np.any(res.solver_time&gt;0) else 0.0,     }  summary = pd.DataFrame([     summarize(\"No reposition\", res_none),     summarize(\"Myopic H=1\",    res_myo),     summarize(f\"MPC H={H}\",    res_mpc), ])  summary In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.plot(res_none.service_rate, label=\"No reposition\")\nplt.plot(res_myo.service_rate,  label=\"Myopic H=1\")\nplt.plot(res_mpc.service_rate,  label=f\"MPC H={H}\")\nplt.xlabel(\"Time step (15-min)\")\nplt.ylabel(\"Service rate (served / demand)\")\nplt.title(\"Service rate over time\")\nplt.legend()\nplt.show()\n</pre>  plt.figure(figsize=(12,4)) plt.plot(res_none.service_rate, label=\"No reposition\") plt.plot(res_myo.service_rate,  label=\"Myopic H=1\") plt.plot(res_mpc.service_rate,  label=f\"MPC H={H}\") plt.xlabel(\"Time step (15-min)\") plt.ylabel(\"Service rate (served / demand)\") plt.title(\"Service rate over time\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.plot(np.cumsum(res_none.deadhead_cost), label=\"No reposition\")\nplt.plot(np.cumsum(res_myo.deadhead_cost),  label=\"Myopic H=1\")\nplt.plot(np.cumsum(res_mpc.deadhead_cost),  label=f\"MPC H={H}\")\nplt.xlabel(\"Time step (15-min)\")\nplt.ylabel(\"Cumulative deadhead cost (minutes)\")\nplt.title(\"Cumulative deadhead cost over time\")\nplt.legend()\nplt.show()\n</pre>  plt.figure(figsize=(12,4)) plt.plot(np.cumsum(res_none.deadhead_cost), label=\"No reposition\") plt.plot(np.cumsum(res_myo.deadhead_cost),  label=\"Myopic H=1\") plt.plot(np.cumsum(res_mpc.deadhead_cost),  label=f\"MPC H={H}\") plt.xlabel(\"Time step (15-min)\") plt.ylabel(\"Cumulative deadhead cost (minutes)\") plt.title(\"Cumulative deadhead cost over time\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>def apply_demand_shock(d_real_base: np.ndarray, zone_subset_idx, t_start, t_end, factor=1.5):\n    d = d_real_base.copy()\n    d[t_start:t_end, zone_subset_idx, :] *= factor\n    return d\n\nshock_zones = list(range(min(5, K)))\nt_start_shock, t_end_shock = 24, 36  # adjust based on your test start time\n\nd_backup = d_real.copy()\nd_real = apply_demand_shock(d_real, shock_zones, t_start_shock, t_end_shock, factor=1.8)\n\nres_mpc_shock = run_backtest(\"mpc\", H_policy=H, fleet_size=4000, lambda_unmet=8.0, eta_smooth=1e-3)\n\nd_real = d_backup\n\nprint(\"Base MPC service rate:\", res_mpc.x_served_total / res_mpc.demand_total)\nprint(\"Shock MPC service rate:\", res_mpc_shock.x_served_total / res_mpc_shock.demand_total)\n</pre>  def apply_demand_shock(d_real_base: np.ndarray, zone_subset_idx, t_start, t_end, factor=1.5):     d = d_real_base.copy()     d[t_start:t_end, zone_subset_idx, :] *= factor     return d  shock_zones = list(range(min(5, K))) t_start_shock, t_end_shock = 24, 36  # adjust based on your test start time  d_backup = d_real.copy() d_real = apply_demand_shock(d_real, shock_zones, t_start_shock, t_end_shock, factor=1.8)  res_mpc_shock = run_backtest(\"mpc\", H_policy=H, fleet_size=4000, lambda_unmet=8.0, eta_smooth=1e-3)  d_real = d_backup  print(\"Base MPC service rate:\", res_mpc.x_served_total / res_mpc.demand_total) print(\"Shock MPC service rate:\", res_mpc_shock.x_served_total / res_mpc_shock.demand_total) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.plot(res_mpc.service_rate, label=\"MPC (base)\")\nplt.plot(res_mpc_shock.service_rate, label=\"MPC (demand shock)\")\nplt.axvspan(t_start_shock, t_end_shock, alpha=0.2, label=\"Shock window\")\nplt.xlabel(\"Time step\")\nplt.ylabel(\"Service rate\")\nplt.title(\"MPC robustness under a demand shock\")\nplt.legend()\nplt.show()\n</pre>  plt.figure(figsize=(12,4)) plt.plot(res_mpc.service_rate, label=\"MPC (base)\") plt.plot(res_mpc_shock.service_rate, label=\"MPC (demand shock)\") plt.axvspan(t_start_shock, t_end_shock, alpha=0.2, label=\"Shock window\") plt.xlabel(\"Time step\") plt.ylabel(\"Service rate\") plt.title(\"MPC robustness under a demand shock\") plt.legend() plt.show()"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#taxi-ride-hailing-fleet-repositioning-with-rolling-horizon-convex-optimization-mpc","title":"Taxi / Ride-Hailing Fleet Repositioning with Rolling-Horizon Convex Optimization (MPC)\u00b6","text":"<p>Taxi/Ride-hailing systems must continuously decide where to position idle vehicles so that future trip requests can be served with low waiting times. When vehicles move without a passenger\u2014often called deadheading (or empty miles) they incur operational cost without generating revenue, so good policies must balance service quality against empty repositioning cost.</p> <p>In this notebook, we study dynamic fleet rebalancing using public trip records from the NYC Taxi &amp; Limousine Commission (TLC). The TLC yellow-taxi data contains pickup/dropoff timestamps and pickup/dropoff zone identifiers, enabling us to aggregate origin\u2013destination demand over time and space.</p> <p>We formulate this rebalancing as a convex, multi-period flow optimization solved in a receding-horizon (Model Predictive Control, MPC) loop: at each time step we</p> <ol> <li>State: idle vehicles per zone at the current time.</li> <li>Forecast: expected demand for the next few hours (simple time-of-week averages from training data).</li> <li>Optimize (convex): solve a multi-period, time-expanded flow model that decides:<ul> <li>how many trips to serve in each zone pair, and</li> <li>how many vehicles to move empty between zones (reposition / deadhead flows), trading off customer service vs. deadheading cost.</li> </ul> </li> <li>Apply only the first step of the plan.</li> <li>Simulate reality: reveal the actual demand from held-out TLC data, update vehicle locations with travel-time lags, and repeat.</li> </ol> <p>Data:</p> <ul> <li>NYC TLC Trip Record Data (monthly Parquet download links)</li> <li>Taxi zone lookup table (<code>LocationID \u2192 Borough/Zone/Service zone</code>)</li> <li>Yellow taxi data dictionary for column meanings</li> </ul>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#problem-at-a-glance","title":"Problem at a glance\u00b6","text":"<ul> <li>City is partitioned into zones $i \\in \\{1,\\dots,K\\}$.</li> <li>Time is discretized into steps $t = 0,1,\\dots$ (e.g., 15 minutes).</li> <li>State $s_{i,t}$: idle vehicles in zone $i$ at step $t$.</li> <li>Demand $d_{ij,t}$: ride requests from zone $i$ to $j$ at step $t$.</li> <li>Decisions:<ul> <li>$x_{ij,t}$: rides served from $i \\to j$ (bounded by demand)</li> <li>$r_{ij,t}$: empty reposition moves from $i \\to j$</li> </ul> </li> </ul> <p>We solve a multi-period, time-expanded network-flow problem and run it in a rolling horizon loop.</p>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#mathematical-formulation","title":"Mathematical formulation\u00b6","text":"<p>Let $H$ be the planning horizon (number of future time steps optimized each time).</p> <p>Decision variables (continuous):</p> <ul> <li>$x_{ij,t} \\ge 0$ served demand (rides) for $t=0,\\dots,H-1$</li> <li>$r_{ij,t} \\ge 0$ reposition (deadhead) flows</li> <li>$u_{ij,t} \\ge 0$ unmet demand slack</li> <li>$s_{i,t} \\ge 0$ idle vehicles for $t=0,\\dots,H$</li> </ul> <p>Constraints</p> <ol> <li><p>Demand bounds $0 \\le x_{ij,t} \\le \\hat d_{ij,t}$ and we set $u_{ij,t} = \\hat d_{ij,t} - x_{ij,t}$.</p> </li> <li><p>Idle vehicle availability $$ \\sum_j x_{ij,t} + \\sum_j r_{ij,t} \\le s_{i,t} $$</p> </li> <li><p>Fleet dynamics with travel-time lags</p> </li> </ol> <p>We estimate an integer travel-time lag $\\tau_{ij}\\in\\{1,\\dots,\\tau_{\\max}\\}$ (in steps) from data.</p> <p>Vehicles that depart at time $t$ from $i\\to j$ arrive at $j$ at time $t+\\tau_{ij}$. Therefore, for each zone $i$ and time $t$:</p> <p>$$ s_{i,t+1} = s_{i,t} - \\sum_j (x_{ij,t} + r_{ij,t}) + \\sum_k \\Big( x_{k i, t-\\tau_{ki}+1} + r_{k i, t-\\tau_{ki}+1} \\Big) $$ where terms with negative indices are treated as 0.</p> <p>Objective (LP) $$ \\min \\sum_{t,i,j} c^{repo}_{ij}\\, r_{ij,t} \\;+\\; \\lambda \\sum_{t,i,j} u_{ij,t} $$</p> <ul> <li>$c^{repo}_{ij}$: deadhead cost (we use median travel time from data as a proxy)</li> <li>$\\lambda$: penalty for unmet demand (service-level priority)</li> </ul> <p>Optional convex smoothing (QP): Add $\\eta \\sum_t \\|s_{\\cdot,t+1}-s_{\\cdot,t}\\|_2^2$ to reduce oscillatory repositioning.</p>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#download-nyc-tlc-data-parquet-zone-lookup-csv","title":"Download NYC TLC data (Parquet) + zone lookup (CSV)\u00b6","text":""},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#load-and-clean-the-data","title":"Load and clean the data\u00b6","text":"<p>We only need:</p> <ul> <li>pickup datetime (<code>tpep_pickup_datetime</code>)</li> <li>dropoff datetime (<code>tpep_dropoff_datetime</code>)</li> <li>pickup zone ID (<code>PULocationID</code>)</li> <li>dropoff zone ID (<code>DOLocationID</code>)</li> </ul> <p>We filter to a manageable time window and optionally sample rows.</p>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#build-demand-tensors-and-travel-time-estimates","title":"Build demand tensors and travel-time estimates\u00b6","text":"<p>We choose:</p> <ul> <li>time step length <code>DT_MIN</code> (e.g., 15 minutes)</li> <li>a subset of zones (<code>K_ZONES</code>) to keep optimization fast</li> <li>a train/test split within the month:<ul> <li>train: compute demand profiles and travel-time medians</li> <li>test: backtest the MPC policy</li> </ul> </li> </ul>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#4-demand-forecasting-baseline-time-of-week-average","title":"4) Demand forecasting baseline (time-of-week average)\u00b6","text":""},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#5-convex-mpc-optimizer-cvxpy","title":"5) Convex MPC optimizer (CVXPY)\u00b6","text":""},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#6-simulator-backtesting-loop","title":"6) Simulator + backtesting loop\u00b6","text":""},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#7-stress-testing-scenarios","title":"7) Stress testing scenarios\u00b6","text":"<p>Because we have a simulator, you can create \"what-if\" scenarios without changing the optimizer:</p> <ul> <li>Demand shock: multiply demand in a set of zones for a period (event letting out).</li> <li>Congestion shock: increase travel times $tt_{ij}$ (and thus $\\tau_{ij}$, $c^{repo}_{ij}$) for certain OD pairs.</li> <li>Fleet shortage: reduce <code>fleet_size</code>.</li> </ul> <p>Below is a simple demand shock example.</p>"},{"location":"convex/tutorials/7_taxi_fleet_repositioning_mpc/#next-upgrades-optional","title":"Next upgrades (optional)\u00b6","text":"<ul> <li>Add fairness constraints (minimum service per borough / zone group)</li> <li>Use HVFHV trip records for a closer ride-hailing proxy (also on the TLC page)</li> <li>Use the taxi zone shapefile to plot maps (geopandas)</li> </ul>"}]}