{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"0b%203Convex%20Conjugates/","title":"Convex Conjugates","text":""},{"location":"0b%203Convex%20Conjugates/#convex-conjugates","title":"Convex Conjugates","text":"<p>Convex conjugates, also known as Fenchel conjugates, are a fundamental tool in convex analysis and optimization. They provide a dual representation of functions and are central to:</p> <ul> <li>Deriving dual problems in convex optimization  </li> <li>Understanding subgradients and supporting hyperplanes  </li> <li>Designing efficient algorithms such as proximal and primal-dual methods  </li> </ul> <p>Intuition: the convex conjugate of a function \\(f\\) captures the maximum linear overestimate of \\(f\\). It tells us, for any direction \\(y\\), what is the steepest slope that does not underestimate \\(f\\).</p>"},{"location":"0b%203Convex%20Conjugates/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) be a proper convex function. The convex conjugate \\(f^*: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) is defined as:</p> \\[ f^*(y) = \\sup_{x \\in \\mathbb{R}^n} \\left( \\langle y, x \\rangle - f(x) \\right) \\] <ul> <li>\\(y\\) is the dual variable, representing a slope or linear functional.  </li> <li>Geometric intuition: for each \\(y\\), \\(f^*(y)\\) is the height of the tightest linear overestimate of \\(f\\) along \\(y\\).</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#key-properties","title":"Key Properties","text":"<ul> <li>\\(f^*\\) is always convex, even if \\(f\\) is not strictly convex.  </li> <li>The biconjugate \\(f^{**}\\) equals \\(f\\) if \\(f\\) is proper, convex, and lower semicontinuous: </li> <li>Fenchel-Young inequality: For all \\(x, y \\in \\mathbb{R}^n\\):  Equality holds if and only if \\(y \\in \\partial f(x)\\) (subgradient condition).</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex function \\(f(x)\\).  </li> <li>For a given dual vector \\(y\\), compute </li> <li>Use the properties:  </li> <li>For norm functions, conjugates give dual norms.  </li> <li>For indicator functions, conjugates give support functions.  </li> <li>Fenchel conjugates are used to construct dual problems in convex optimization.  </li> <li>Apply in optimization algorithms:  </li> <li>Proximal operators of \\(f^*\\) relate to those of \\(f\\) (Moreau identity)  </li> <li>Subgradients of \\(f^*\\) are linked to primal solutions</li> </ol>"},{"location":"0b%203Convex%20Conjugates/#examples","title":"Examples","text":""},{"location":"0b%203Convex%20Conjugates/#example-1-quadratic-function","title":"Example 1: Quadratic Function","text":"<p>Let \\(f(x) = \\frac{1}{2} \\|x\\|_2^2\\). Then</p> \\[ f^*(y) = \\sup_x \\left( \\langle y, x \\rangle - \\frac{1}{2} \\|x\\|_2^2 \\right) \\] <ul> <li>Differentiate: \\(\\nabla_x (\\langle y, x \\rangle - \\frac{1}{2} \\|x\\|_2^2) = y - x = 0 \\implies x = y\\) </li> <li>Substitute back: </li> </ul> <p>Observation: quadratic is self-conjugate.</p>"},{"location":"0b%203Convex%20Conjugates/#example-2-ell_1-norm","title":"Example 2: \\(\\ell_1\\) Norm","text":"<p>Let \\(f(x) = \\|x\\|_1\\). Then</p> \\[ f^*(y) = \\sup_{\\|x\\|_1 \\le \\infty} \\left( \\langle y, x \\rangle - \\|x\\|_1 \\right) \\] <ul> <li>The supremum is finite only if \\(\\|y\\|_\\infty \\le 1\\), otherwise \\(+\\infty\\).  </li> <li>Therefore:  where \\(\\delta\\) is the indicator function.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#example-3-indicator-function","title":"Example 3: Indicator Function","text":"<p>Let \\(f(x) = \\delta_C(x)\\), the indicator of a convex set \\(C\\). Then</p> \\[ f^*(y) = \\sup_{x \\in C} \\langle y, x \\rangle = \\sigma_C(y) \\] <ul> <li>Observation: support functions are conjugates of indicator functions.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Dual Optimization Problems: Fenchel duals are derived by taking conjugates of primal objectives and constraints.  </li> <li>Subgradient Methods: Fenchel-Young inequality provides a direct link between primal and dual subgradients.  </li> <li>Proximal Algorithms: Proximal operators of conjugates allow efficient updates in primal-dual splitting methods.  </li> <li>Norm Duality: Conjugates of norm functions yield dual norms, connecting directly to constrained optimization theory.</li> </ul>"},{"location":"0b%203Convex%20Conjugates/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Convex conjugates provide a dual perspective of convex functions, capturing maximal linear overestimates.  </li> <li>Biconjugation recovers the original function if it is convex and lower semicontinuous.  </li> <li>Fenchel-Young inequality links primal and dual variables via subgradients.  </li> <li>Indicator functions and norm functions have conjugates that reveal support functions and dual norms.  </li> <li>Conjugates are foundational in dual optimization, proximal algorithms, and geometric analysis.</li> </ul>"},{"location":"0b%205Fenchel%20Duality/","title":"Fenchel Duality","text":""},{"location":"0b%205Fenchel%20Duality/#fenchel-duality","title":"Fenchel Duality","text":"<p>Fenchel duality generalizes the idea of Lagrange duality to general convex optimization problems. It provides a systematic way to:</p> <ul> <li>Derive dual problems for convex programs  </li> <li>Analyze optimality conditions via subgradients  </li> <li>Link primal and dual solutions geometrically and algorithmically  </li> </ul> <p>Intuition: Fenchel duality captures the interplay between a convex function and its conjugate. By representing constraints and objectives via conjugates, we can often solve a dual problem that is simpler or more structured than the primal.</p>"},{"location":"0b%205Fenchel%20Duality/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) and \\(g: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) be proper convex functions. Consider the primal problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\, f(x) + g(Ax) \\] <p>where \\(A \\in \\mathbb{R}^{m \\times n}\\).</p>"},{"location":"0b%205Fenchel%20Duality/#fenchel-dual-problem","title":"Fenchel Dual Problem","text":"<p>The Fenchel dual is defined as:</p> \\[ \\max_{y \\in \\mathbb{R}^m} \\, -f^*(A^\\top y) - g^*(-y) \\] <ul> <li>\\(f^*\\) and \\(g^*\\) are the convex conjugates of \\(f\\) and \\(g\\).  </li> <li>\\(y\\) is the dual variable, often representing Lagrange multipliers for the linear mapping \\(Ax\\).</li> </ul>"},{"location":"0b%205Fenchel%20Duality/#weak-and-strong-duality","title":"Weak and Strong Duality","text":"<ul> <li>Weak duality: For any primal feasible \\(x\\) and dual feasible \\(y\\),</li> </ul> \\[ f(x) + g(Ax) \\ge -f^*(A^\\top y) - g^*(-y) \\] <ul> <li>Strong duality: If \\(f\\) and \\(g\\) are convex and satisfy constraint qualifications (e.g., Slater's condition), then</li> </ul> \\[ \\min_x f(x) + g(Ax) = \\max_y -f^*(A^\\top y) - g^*(-y) \\] <p>and the dual optimal solution \\(y^*\\) gives information about the primal optimal \\(x^*\\) via subgradients:</p> \\[ A^\\top y^* \\in \\partial f(x^*), \\quad -y^* \\in \\partial g(Ax^*) \\]"},{"location":"0b%205Fenchel%20Duality/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex functions \\(f\\) and \\(g\\) and linear map \\(A\\).  </li> <li>Compute the convex conjugates \\(f^*\\) and \\(g^*\\).  </li> <li>Form the dual problem:</li> </ol> \\[ \\max_y -f^*(A^\\top y) - g^*(-y) \\] <ol> <li>Solve the dual problem (often easier than the primal).  </li> <li>Recover the primal solution from dual optimality using subgradients:</li> </ol> \\[ x^* \\in \\partial f^*(A^\\top y^*) \\]"},{"location":"0b%205Fenchel%20Duality/#examples","title":"Examples","text":""},{"location":"0b%205Fenchel%20Duality/#example-1-linear-programming","title":"Example 1: Linear Programming","text":"<p>Primal LP:</p> \\[ \\min_{x \\ge 0} c^\\top x \\quad \\text{s.t.} \\quad Ax = b \\] <ul> <li>Let \\(f(x) = c^\\top x + \\delta_{\\{x \\ge 0\\}}(x)\\), \\(g(z) = \\delta_{\\{z = b\\}}(z)\\) </li> <li>Conjugates: \\(f^*(y) = \\delta_{\\{y \\le c\\}}(y)\\), \\(g^*(y) = b^\\top y\\) </li> <li>Fenchel dual:</li> </ul> \\[ \\max_y b^\\top y \\quad \\text{s.t.} \\quad A^\\top y \\le c \\] <p>This is the standard LP dual.</p>"},{"location":"0b%205Fenchel%20Duality/#example-2-quadratic-problem","title":"Example 2: Quadratic Problem","text":"<p>Primal: \\(\\min_x \\frac{1}{2}\\|x\\|_2^2 + \\delta_C(x)\\), where \\(C\\) is convex.  </p> <ul> <li>\\(f(x) = \\frac{1}{2}\\|x\\|_2^2\\), \\(g(x) = \\delta_C(x)\\) </li> <li>Conjugates: \\(f^*(y) = \\frac{1}{2}\\|y\\|_2^2\\), \\(g^*(y) = \\sigma_C(y)\\) </li> <li> <p>Fenchel dual: \\(\\max_y -\\frac{1}{2}\\|y\\|_2^2 - \\sigma_C(y)\\) </p> </li> <li> <p>Optimal \\(x^*\\) recovered via: \\(x^* = y^*\\) (gradient of \\(f^*\\))</p> </li> </ul>"},{"location":"0b%205Fenchel%20Duality/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Algorithm Design: Fenchel duality is the foundation for primal-dual algorithms and proximal splitting methods.  </li> <li>Optimality Checks: Dual solutions provide bounds on primal objectives via weak duality.  </li> <li>Geometric Interpretation: The dual problem represents the tightest linear lower bounds on the primal objective.  </li> <li>Norms and Conjugates: Links to support functions and dual norms are direct consequences of Fenchel duality.</li> </ul>"},{"location":"0b%205Fenchel%20Duality/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Fenchel duality generalizes Lagrange duality using convex conjugates.  </li> <li>Weak duality always holds; strong duality requires convexity and constraint qualifications.  </li> <li>Dual solutions provide bounds, optimality conditions, and subgradient relationships for the primal.  </li> <li>Fenchel duality is fundamental in convex optimization, primal-dual algorithms, and nonsmooth analysis.</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/","title":"Moreau Envelopes &amp; Proximal Operators","text":""},{"location":"0b%206Moreau%20and%20Proximal/#moreau-envelopes-proximal-operators","title":"Moreau Envelopes &amp; Proximal Operators","text":"<p>Moreau envelopes and proximal operators are central concepts in nonsmooth convex optimization. They allow us to:</p> <ul> <li>Smooth nonsmooth functions for easier optimization  </li> <li>Define proximal updates that generalize gradient steps  </li> <li>Connect primal and dual problems via Fenchel conjugates </li> </ul> <p>Intuition: A Moreau envelope is a smoothed version of a convex function that approximates it while retaining its convexity. The proximal operator finds the point closest to a given input while balancing the original function, effectively performing a \u201csoft minimization.\u201d</p>"},{"location":"0b%206Moreau%20and%20Proximal/#definitions-and-formal-statements","title":"Definitions and Formal Statements","text":""},{"location":"0b%206Moreau%20and%20Proximal/#moreau-envelope","title":"Moreau Envelope","text":"<p>For a proper convex function \\(f: \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}\\) and parameter \\(\\lambda &gt; 0\\), the Moreau envelope \\(f_\\lambda\\) is defined as:</p> \\[ f_\\lambda(x) = \\min_{y \\in \\mathbb{R}^n} \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ul> <li>\\(f_\\lambda(x)\\) is smooth, even if \\(f\\) is nonsmooth.  </li> <li>It provides a lower bound on \\(f\\), approaching \\(f\\) as \\(\\lambda \\to 0\\).  </li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#proximal-operator","title":"Proximal Operator","text":"<p>The proximal operator of \\(f\\) is defined as:</p> \\[ \\text{prox}_{\\lambda f}(x) = \\arg\\min_{y \\in \\mathbb{R}^n} \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ul> <li>The proximal operator returns the point \\(y\\) that balances minimizing \\(f\\) with staying close to \\(x\\).  </li> <li>Relation: \\(f_\\lambda(x) = f(\\text{prox}_{\\lambda f}(x)) + \\frac{1}{2\\lambda} \\| \\text{prox}_{\\lambda f}(x) - x \\|_2^2\\)</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#step-by-step-analysis-how-to-use","title":"Step-by-Step Analysis / How to Use","text":"<ol> <li>Identify the convex function \\(f\\) and choose a parameter \\(\\lambda &gt; 0\\).  </li> <li>Compute the proximal operator:  </li> </ol> \\[ y^* = \\text{prox}_{\\lambda f}(x) = \\arg\\min_y \\left\\{ f(y) + \\frac{1}{2\\lambda} \\|y - x\\|_2^2 \\right\\} \\] <ol> <li>Compute the Moreau envelope if a smooth approximation is desired:  </li> </ol> \\[ f_\\lambda(x) = f(y^*) + \\frac{1}{2\\lambda} \\|y^* - x\\|_2^2 \\] <ol> <li>Use in optimization algorithms:  </li> <li>Proximal gradient descent: \\(x_{k+1} = \\text{prox}_{\\lambda g}(x_k - \\lambda \\nabla f(x_k))\\) </li> <li>Splitting methods: Handle \\(f\\) and \\(g\\) separately using their proximal operators  </li> </ol>"},{"location":"0b%206Moreau%20and%20Proximal/#examples","title":"Examples","text":""},{"location":"0b%206Moreau%20and%20Proximal/#example-1-ell_1-norm-soft-thresholding","title":"Example 1: \\(\\ell_1\\) Norm (Soft Thresholding)","text":"<p>Let \\(f(x) = \\|x\\|_1\\). Then</p> \\[ \\text{prox}_{\\lambda \\| \\cdot \\|_1}(x)_i = \\text{sign}(x_i) \\cdot \\max(|x_i| - \\lambda, 0) \\] <ul> <li>Each component is shrunk toward zero, promoting sparsity.  </li> <li>Widely used in LASSO regression and compressed sensing.</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#example-2-indicator-function","title":"Example 2: Indicator Function","text":"<p>Let \\(f(x) = \\delta_C(x)\\), the indicator of a convex set \\(C\\). Then</p> \\[ \\text{prox}_{\\lambda \\delta_C}(x) = \\arg\\min_{y \\in C} \\frac{1}{2\\lambda} \\|y - x\\|_2^2 = P_C(x) \\] <ul> <li>The proximal operator reduces to the metric projection onto \\(C\\).  </li> <li>Intuition: move \\(x\\) to the closest feasible point in \\(C\\).</li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#example-3-quadratic-function","title":"Example 3: Quadratic Function","text":"<p>Let \\(f(x) = \\frac{1}{2} \\|x\\|_2^2\\). Then</p> \\[ \\text{prox}_{\\lambda f}(x) = \\frac{x}{1 + \\lambda}, \\quad f_\\lambda(x) = \\frac{1}{2(1+\\lambda)} \\|x\\|_2^2 \\] <ul> <li>Smooths the function and scales down the input.  </li> </ul>"},{"location":"0b%206Moreau%20and%20Proximal/#applications-implications","title":"Applications / Implications","text":"<ul> <li>Proximal Gradient Methods: Handle composite objectives \\(f(x) + g(x)\\) where \\(f\\) is smooth and \\(g\\) is nonsmooth.  </li> <li>Splitting Algorithms: Alternating updates with proximal operators allow decomposition in high-dimensional problems.  </li> <li>Regularization: \\(\\ell_1\\), nuclear norm, and indicator functions are easily handled using proximal operators.  </li> <li>Duality: Proximal operators relate to Fenchel conjugates via the Moreau decomposition:</li> </ul> \\[ x = \\text{prox}_{\\lambda f}(x) + \\lambda \\, \\text{prox}_{f^*/\\lambda}(x/\\lambda) \\]"},{"location":"0b%206Moreau%20and%20Proximal/#summary-key-takeaways","title":"Summary / Key Takeaways","text":"<ul> <li>Moreau envelopes provide smooth approximations of nonsmooth convex functions.  </li> <li>Proximal operators generalize gradient steps to nonsmooth settings.  </li> <li>Proximal updates often have closed-form solutions for many common functions.  </li> <li>They are central to modern optimization algorithms, including proximal gradient, ADMM, and primal-dual splitting.  </li> <li>The connection with duality and conjugates makes them a versatile and powerful tool in convex optimization.</li> </ul>"},{"location":"0e1%20Gradient%20Descent/","title":"Gradient Descent: Derivation and Convergence","text":""},{"location":"0e1%20Gradient%20Descent/#gradient-descent-derivation-and-convergence","title":"Gradient Descent: Derivation and Convergence","text":"<p>Gradient descent is one of the most fundamental algorithms in optimization and machine learning. It forms the backbone of training neural networks, logistic regression, matrix factorization, and many other models.</p>"},{"location":"0e1%20Gradient%20Descent/#problem-setup","title":"Problem Setup","text":"<p>We aim to minimize a differentiable function over a feasible convex set \\(\\mathcal{X}\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x). \\] <p>At each iteration \\(t\\), we hold a current iterate \\(x_t\\) and wish to take a step that reduces the objective. But instead of minimizing \\(f\\) directly (which may be complex), we construct a local surrogate model.</p>"},{"location":"0e1%20Gradient%20Descent/#local-linear-approximation-first-order-model","title":"Local Linear Approximation (First-order Model)","text":"<p>Around \\(x_t\\), we approximate \\(f\\) using its Taylor expansion:</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle. \\] <p>Intuition: - We assume \\(f\\) behaves approximately like its tangent plane near \\(x_t\\). - If we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent \\(-\\nabla f(x_t)\\), which is not realistic or stable.</p> <p>This motivates adding a locality restriction \u2014 we trust the linear approximation near \\(x_t\\), not globally.</p>"},{"location":"0e1%20Gradient%20Descent/#adding-a-quadratic-regularization-term","title":"Adding a Quadratic Regularization Term","text":"<p>To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from \\(x_t\\):</p> \\[ f(x) \\approx f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2, \\] <p>where \\(\\eta &gt; 0\\) is the learning rate or step size.</p> <p>Geometric Interpretation: - The linear term pulls \\(x\\) in the steepest descent direction. - The quadratic term acts like a trust region, discouraging large deviations from \\(x_t\\). - \\(\\eta\\) trades off aggressive progress vs stability:   - Small \\(\\eta\\) \u2192 cautious updates.   - Large \\(\\eta\\) \u2192 bold updates (risk of divergence).</p>"},{"location":"0e1%20Gradient%20Descent/#deriving-the-gradient-descent-update","title":"Deriving the Gradient Descent Update","text":"<p>We define the next iterate as the minimizer of the surrogate objective:</p> \\[ x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\Big[ f(x_t) + \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{2\\eta} \\|x - x_t\\|^2 \\Big]. \\] <p>Ignoring the constant term \\(f(x_t)\\) and differentiating w.r.t. \\(x\\):</p> \\[ \\nabla f(x_t) + \\frac{1}{\\eta}(x - x_t) = 0 \\] <p>Solving:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Gradient Descent Update: </p>"},{"location":"0e1%20Gradient%20Descent/#convergence-analysis","title":"Convergence Analysis","text":"<p>To analyze convergence, we assume:</p>"},{"location":"0e1%20Gradient%20Descent/#smoothness-lipschitz-gradient","title":"Smoothness (Lipschitz Gradient)","text":"\\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|, \\quad \\forall x, y. \\] <p>This says the gradient does not change too abruptly. Most ML objectives satisfy this.</p>"},{"location":"0e1%20Gradient%20Descent/#strong-convexity","title":"Strong Convexity","text":"<p>If, in addition, \\(f\\) is \\(\\mu\\)-strongly convex, then:</p> \\[ f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2} \\|y-x\\|^2. \\] <p>This implies \\(f\\) has a unique minimizer \\(x^\\*\\) and its level sets are bowl-shaped, not flat.</p>"},{"location":"0e1%20Gradient%20Descent/#descent-lemma-why-gradient-descent-decreases-f","title":"Descent Lemma: Why Gradient Descent Decreases \\(f\\)","text":"<p>For an \\(L\\)-smooth function,</p> \\[ f(x_{t+1}) \\le f(x_t) + \\langle \\nabla f(x_t), x_{t+1}-x_t \\rangle + \\frac{L}{2} \\|x_{t+1}-x_t\\|^2. \\] <p>Using \\(x_{t+1} = x_t - \\eta \\nabla f(x_t)\\):</p> \\[ \\begin{aligned} f(x_{t+1})  &amp;\\le f(x_t) - \\eta \\|\\nabla f(x_t)\\|^2 + \\frac{L \\eta^2}{2} \\|\\nabla f(x_t)\\|^2 \\\\ &amp;= f(x_t) - \\left( \\eta - \\frac{L\\eta^2}{2} \\right) \\|\\nabla f(x_t)\\|^2. \\end{aligned} \\] <p>If \\(\\eta \\le \\frac{1}{L}\\), then the decrease term is positive \u21d2 every step reduces the objective.</p>"},{"location":"0e1%20Gradient%20Descent/#convergence-rates","title":"Convergence Rates","text":""},{"location":"0e1%20Gradient%20Descent/#convex-but-not-strongly-convex","title":"Convex but not strongly convex","text":"\\[ f(x_T) - f(x^*) \\le \\frac{L \\|x_0 - x^*\\|^2}{2T} \\quad \\Rightarrow \\quad O(1/T) \\text{ convergence}. \\] <p>This is called sublinear convergence.</p>"},{"location":"0e1%20Gradient%20Descent/#strongly-convex-case-linear-convergence","title":"Strongly Convex Case: Linear Convergence","text":"<p>If \\(f\\) is \\(\\mu\\)-strongly convex and \\(\\eta = \\frac{2}{\\mu + L}\\):</p> \\[ \\|x_{t+1} - x^*\\| \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right) \\|x_t - x^*\\| \\] <p>This gives:</p> \\[ \\|x_T - x^*\\| \\le \\left( \\frac{L - \\mu}{L + \\mu} \\right)^T \\|x_0 - x^*\\| \\quad \\Rightarrow \\quad \\text{Linear (geometric) convergence}. \\] <p>Meaning: Error shrinks by a constant factor every iteration.</p>"},{"location":"0e1%20Gradient%20Descent/#summary-and-ml-interpretation","title":"Summary and ML Interpretation","text":"Assumption on \\(f\\) Convergence Rate Typical ML Scenario Convex + Smooth \\(O(1/T)\\) Unregularized logistic regression, basic convex losses Strongly Convex + Smooth \\(O(\\rho^T)\\) (linear) L2-regularized models, ridge regression"},{"location":"0e2%20subgradient%20method/","title":"Subgradient Method: Derivation, Geometry, and Convergence","text":""},{"location":"0e2%20subgradient%20method/#subgradient-method-derivation-geometry-and-convergence","title":"Subgradient Method: Derivation, Geometry, and Convergence","text":"<p>Let's consider the problem of minimizing a convex, possibly nonsmooth, function:</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\] <p>where \\(f\\) may not be differentiable everywhere (e.g., hinge loss, \\(L_1\\) norm, ReLU penalties\u2014common in ML). Classical gradient descent cannot be applied directly, so we use subgradients.</p>"},{"location":"0e2%20subgradient%20method/#subgradients-and-geometric-meaning","title":"Subgradients and Geometric Meaning","text":"<p>A subgradient \\(g_t \\in \\partial f(x_t)\\) is any vector that supports the function from below:</p> \\[ f(y) \\ge f(x_t) + \\langle g_t, y - x_t \\rangle, \\quad \\forall y \\in \\mathcal{X}. \\] <ul> <li>When \\(f\\) is smooth, \\(\\partial f(x_t) = \\{\\nabla f(x_t)\\}\\) and \\(g_t\\) coincides with the gradient.</li> <li>When \\(f\\) is nonsmooth (like \\(|x|\\) at \\(x=0\\)), the subdifferential \\(\\partial f(x_t)\\) is a set of valid slopes.</li> <li>Intuitively, any subgradient defines a supporting hyperplane that lies below the graph of \\(f\\) and touches it at \\(x_t\\).</li> </ul> <p>This generalization allows us to move in a descent direction even when a unique gradient does not exist.</p>"},{"location":"0e2%20subgradient%20method/#subgradient-update-and-projection-view","title":"Subgradient Update and Projection View","text":"<p>The update rule of the projected subgradient method is:</p> \\[ x_{t+1} = \\Pi_{\\mathcal{X}} \\big( x_t - \\eta_t g_t \\big), \\] <p>where: - \\(g_t \\in \\partial f(x_t)\\) is a valid subgradient, - \\(\\eta_t &gt; 0\\) is the step size, - \\(\\Pi_{\\mathcal{X}}\\) denotes projection onto \\(\\mathcal{X}\\) to ensure feasibility.</p> <p>If \\(\\mathcal{X} = \\mathbb{R}^n\\) (unconstrained case), projection disappears:</p> \\[ x_{t+1} = x_t - \\eta_t g_t. \\] <p>Geometric insight: we move in the direction of a subgradient and then \"snap back\" to the feasible region if needed. This is analogous to gradient descent but more flexible, tolerating kinks in the objective.</p>"},{"location":"0e2%20subgradient%20method/#distance-analysis-and-role-of-convexity","title":"Distance Analysis and Role of Convexity","text":"<p>Let \\(x^\\star\\) be an optimal solution. Consider the squared distance after an update:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - \\eta_t g_t - x^\\star\\|^2. \\] <p>Expanding the norm:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 = \\|x_t - x^\\star\\|^2 - 2\\eta_t \\langle g_t, x_t - x^\\star \\rangle + \\eta_t^2 \\|g_t\\|^2. \\] <p>By convexity of \\(f\\):</p> \\[ f(x_t) - f(x^\\star) \\le \\langle g_t, x_t - x^\\star \\rangle. \\] <p>Substitute this into the distance inequality to relate movement to function decrease:</p> \\[ \\|x_{t+1} - x^\\star\\|^2 \\le \\|x_t - x^\\star\\|^2 - 2\\eta_t \\big(f(x_t) - f(x^\\star)\\big) + \\eta_t^2 \\|g_t\\|^2. \\]"},{"location":"0e2%20subgradient%20method/#bounding-suboptimality","title":"Bounding Suboptimality","text":"<p>Rearranging gives a direct bound on how far we are from optimum in function value:</p> \\[ f(x_t) - f(x^\\star) \\le \\frac{\\|x_t - x^\\star\\|^2 - \\|x_{t+1} - x^\\star\\|^2}{2 \\eta_t} + \\frac{\\eta_t}{2} \\|g_t\\|^2. \\] <p>This shows a trade-off: large step sizes make faster jumps but increase the \\(\\eta_t \\|g_t\\|^2\\) error; small step sizes ensure precision but slow progress.</p>"},{"location":"0e2%20subgradient%20method/#convergence-rate-and-step-size-insight","title":"Convergence Rate and Step Size Insight","text":"<p>Summing over \\(t = 0, \\dots, T-1\\) and assuming \\(\\|g_t\\| \\le G\\):</p> \\[ \\sum_{t=0}^{T-1} \\big(f(x_t) - f(x^\\star)\\big) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2\\eta} + \\frac{\\eta G^2 T}{2}. \\] <p>Dividing by \\(T\\) and using \\(\\bar{x}_T = \\frac{1}{T} \\sum_{t=0}^{T-1} x_t\\):</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{\\|x_0 - x^\\star\\|^2}{2 \\eta T} + \\frac{\\eta G^2}{2}. \\] <p>Choosing:</p> \\[ \\eta_t = \\frac{R}{G \\sqrt{T}}, \\quad R = \\|x_0 - x^\\star\\|, \\] <p>gives the sublinear convergence rate:</p> \\[ f(\\bar{x}_T) - f(x^\\star) \\le \\frac{R G}{\\sqrt{T}} \\quad \\Rightarrow \\quad O\\left(\\frac{1}{\\sqrt{T}}\\right). \\] <p>This is slower than gradient descent on smooth functions (\\(O(1/T)\\) or linear), reflecting the cost of nonsmoothness.</p>"},{"location":"0e2%20subgradient%20method/#practical-and-ml-perspective","title":"Practical and ML Perspective","text":"<ul> <li>Subgradients power many ML methods with nonsmooth penalties: L1 regularization, hinge loss (SVMs), ReLU activations, TV regularization in imaging.</li> <li>Step size choice is everything. Too large \u2192 oscillation. Too small \u2192 stagnation.</li> <li>Averaging iterates improves convergence behavior and stability.</li> <li>Unlike gradient descent, the method does not converge to a single point but to a region near the optimum unless step size goes to zero.</li> </ul> <p>In high-dimensional ML models, the subgradient method's simplicity and robustness often outweigh its slower convergence rate\u2014especially when structure (sparsity, hinge-like losses) matters more than raw speed.</p>"},{"location":"0e3%20accelerated%20gs/","title":"Accelerated Gradient Descent: Momentum","text":""},{"location":"0e3%20accelerated%20gs/#accelerated-gradient-descent-momentum","title":"Accelerated Gradient Descent: Momentum","text":"<p>The standard gradient descent (GD) update is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t), \\] <p>where: - \\(x_t\\) is the current iterate, - \\(\\eta &gt; 0\\) is the step size (learning rate), - \\(\\nabla f(x_t)\\) is the gradient at \\(x_t\\).</p> <p>Intuition: Imagine rolling a ball on a hill. The ball moves in the steepest downhill direction at each step. In long, narrow valleys, standard GD can zig-zag, taking many small steps to reach the bottom.</p>"},{"location":"0e3%20accelerated%20gs/#1-momentum-adding-inertia","title":"1. Momentum: Adding Inertia","text":"<p>Momentum adds the idea of velocity, allowing the optimization to \"remember\" previous directions:</p> \\[ v_t = x_t - x_{t-1}, \\] <p>where \\(v_t\\) represents the velocity of the iterate.  </p> <ul> <li>The update now combines the current gradient and the previous motion.  </li> <li>Momentum helps move faster along flat or consistent slopes and reduces zig-zagging in steep valleys.  </li> </ul> <p>Analogy: - No momentum \u2192 ball stops after each step, carefully following the slope. - With momentum \u2192 ball keeps rolling, building speed along the valley, only slowing when gradients push against it.</p>"},{"location":"0e3%20accelerated%20gs/#2-gradient-descent-with-momentum","title":"2. Gradient Descent with Momentum","text":"<p>The update rule with momentum is:</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) + \\beta (x_t - x_{t-1}), \\] <p>where: - \\(\\beta \\in [0,1)\\) is the momentum parameter, controlling how much past velocity is retained.</p>"},{"location":"0e3%20accelerated%20gs/#breakdown","title":"Breakdown","text":"<ol> <li>Gradient step: \\(-\\eta \\nabla f(x_t)\\) \u2192 moves downhill.  </li> <li>Momentum step: \\(\\beta (x_t - x_{t-1})\\) \u2192 continues moving along previous direction.</li> </ol> <p>Intuition: - If gradients consistently point in the same direction, momentum accelerates the steps. - If gradients oscillate, momentum smooths the path, reducing overshooting.</p>"},{"location":"0e3%20accelerated%20gs/#3-alternative-form-velocity-update","title":"3. Alternative Form: Velocity Update","text":"<p>Another common formulation introduces an explicit velocity variable \\(v_t\\):</p> \\[ \\begin{aligned} v_{t+1} &amp;= \\beta v_t - \\eta \\nabla f(x_t) \\\\ x_{t+1} &amp;= x_t + v_{t+1} \\end{aligned} \\] <ul> <li>Here, \\(v_t\\) accumulates the past updates weighted by \\(\\beta\\).  </li> <li>This makes the analogy to a rolling ball more explicit.</li> </ul>"},{"location":"0e3%20accelerated%20gs/#4-convergence-intuition","title":"4. Convergence Intuition","text":"<ul> <li>For convex and smooth functions, momentum accelerates convergence:  </li> <li>Standard GD: \\(O(1/t)\\) </li> <li> <p>GD + Momentum / Nesterov: \\(O(1/t^2)\\)</p> </li> <li> <p>Momentum combines current slope and accumulated speed from past steps.  </p> </li> <li>Acts like a frictionless ball in a valley: keeps moving in the right direction, accelerating convergence.</li> </ul> <p>Key idea: Momentum builds up speed along consistent gradient directions and smooths oscillations along steep valleys.</p>"},{"location":"0e3%20accelerated%20gs/#5-practical-remarks","title":"5. Practical Remarks","text":"<ol> <li>Momentum is memory: it remembers the direction of previous steps.  </li> <li>Reduces oscillations in narrow valleys.  </li> <li>Accelerates convergence along consistent gradient directions.  </li> <li>Hyperparameter \\(\\beta\\) controls inertia:  </li> <li>Higher \\(\\beta\\) \u2192 longer memory, faster but potentially riskier steps.  </li> <li>Typical values: \\(\\beta = 0.9\\) or \\(0.99\\).  </li> <li>Can be combined with Nesterov acceleration for theoretically optimal rates.</li> </ol>"},{"location":"0e3%20accelerated%20gs/#6-summary","title":"6. Summary","text":"<p>Momentum modifies gradient descent by combining:</p> <ul> <li>Immediate gradient information (\\(-\\eta \\nabla f(x_t)\\))  </li> <li>Past velocity (\\(\\beta (x_t - x_{t-1})\\))  </li> </ul> <p>Effectively, it allows the optimizer to roll through valleys faster, reduce zig-zagging, and achieve accelerated convergence, especially for convex and smooth functions.</p>"},{"location":"0f%20Convergence/","title":"Function Properties for Optimization: Strong Convexity, Smoothness, and Conditioning","text":""},{"location":"0f%20Convergence/#function-properties-for-optimization-strong-convexity-smoothness-and-conditioning","title":"Function Properties for Optimization: Strong Convexity, Smoothness, and Conditioning","text":""},{"location":"0f%20Convergence/#strong-convexity","title":"Strong Convexity","text":"<p>A function \\(f\\) is \\(\\mu\\)-strongly convex if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) + \\frac{\\mu}{2}\\|y-x\\|^2. \\] <p>If \\(f\\) is twice differentiable, this is equivalent to</p> \\[ \\nabla^2 f(x) \\succeq \\mu I \\quad \\text{for all } x. \\] <ul> <li>Guarantees a unique minimizer.  </li> <li>Gradient-based methods achieve linear convergence.  </li> <li>Prevents flat regions where optimization would stall.  </li> </ul>"},{"location":"0f%20Convergence/#why-convergence-may-be-slow-without-strong-convexity","title":"Why Convergence May Be Slow Without Strong Convexity","text":"<ul> <li>If \\(f\\) is convex but not strongly convex, it can have flat regions (zero curvature).  </li> <li>Gradients may be very small in these directions \u2192 gradient steps shrink, and convergence becomes sublinear:  </li> </ul> \\[ f(x_t) - f(x^\\star) = O\\left(\\frac{1}{t}\\right). \\] <ul> <li> <p>Example: \\(f(x) = x^4\\) is convex but not strongly convex near \\(x=0\\). Gradient descent steps become tiny near the minimum \u2192 slow convergence.  </p> </li> <li> <p>Contrast: \\(f(x) = x^2\\) is strongly convex (\\(\\mu=2\\)) \u2192 linear convergence.  </p> </li> </ul>"},{"location":"0f%20Convergence/#examples","title":"Examples","text":"<ol> <li> <p>Quadratic function: \\(f(x) = x^2\\) \u2192 \\(\\mu=2\\), strongly convex \u2192 fast convergence.  </p> </li> <li> <p>Quartic function: \\(f(x) = x^4\\) \u2192 convex but not strongly convex near \\(0\\) \u2192 slow convergence.  </p> </li> <li> <p>Ridge Regression (L2 Regularization): </p> <ul> <li>The first term \\(\\|Xw - y\\|^2\\) is convex.  </li> <li>The L2 term \\(\\lambda \\|w\\|^2\\) is strongly convex (\\(\\nabla^2 (\\lambda\\|w\\|^2) = 2\\lambda I \\succeq \\lambda I\\)).  </li> <li>Adding the L2 penalty makes the entire objective strongly convex with \\(\\mu = \\lambda\\).  </li> <li>Implications: <ul> <li>Unique solution:  even if \\(X^\\top X\\) is singular or ill-conditioned.  </li> <li>Stable optimization: gradient-based methods converge linearly.  </li> <li>Prevents overfitting by controlling the size of weights.  </li> </ul> </li> </ul> </li> </ol>"},{"location":"0f%20Convergence/#smoothness-l-smoothness","title":"Smoothness (L-smoothness)","text":"<p>A function \\(f\\) is \\(L\\)-smooth if</p> \\[ \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|. \\] <p>If twice differentiable:</p> \\[ \\nabla^2 f(x) \\preceq L I \\quad \\text{for all } x. \\] <ul> <li>Limits how steep \\(f\\) can be.  </li> <li>Ensures gradients change gradually \u2192 stable gradient steps.  </li> <li>Guarantees safe step sizes: \\(\\alpha &lt; 1/L\\) for gradient descent.  </li> </ul>"},{"location":"0f%20Convergence/#why-smoothness-matters-for-convergence","title":"Why Smoothness Matters for Convergence","text":"<ul> <li>Without smoothness, the gradient can change abruptly.  </li> <li>A large gradient could lead to overshooting, oscillation, or divergence.  </li> <li>Smoothness ensures predictable, stable progress along the gradient.  </li> </ul> <p>Examples: - Quadratic \\(f(x) = \\frac{1}{2}x^\\top Qx\\): \\(L = \\lambda_{\\max}(Q)\\). - Logistic regression loss: smooth with \\(L\\) depending on \\(\\|X\\|^2\\). - Non-smooth case: \\(f(x) = |x|\\) \u2192 gradient jumps at \\(x=0\\), cannot guarantee smooth progress \u2192 need subgradient methods.  </p>"},{"location":"0f%20Convergence/#condition-number","title":"Condition Number","text":"<p>The condition number is defined as</p> \\[ \\kappa = \\frac{L}{\\mu}. \\] <ul> <li>Measures how \u201cstretched\u201d the optimization landscape is.  </li> <li>High \\(\\kappa\\) \u2192 narrow, elongated valleys \u2192 gradient descent zig-zags, converges slowly.  </li> <li>Low \\(\\kappa\\) \u2192 round bowl \u2192 fast convergence.  </li> </ul> <p>Examples: - \\(Q=I\\): \\(\\mu=L=1\\), \\(\\kappa=1\\) \u2192 fastest convergence. - \\(Q=\\text{diag}(1,1000)\\): \\(\\mu=1\\), \\(L=1000\\), \\(\\kappa=1000\\) \u2192 ill-conditioned, very slow. - In ML, normalization (batch norm, feature scaling, whitening) reduces \\(\\kappa\\), improving training speed.  </p>"},{"location":"0f%20Convergence/#use-cases-and-benefits","title":"Use Cases and Benefits","text":""},{"location":"0f%20Convergence/#strong-convexity_1","title":"Strong Convexity","text":"<ul> <li>Unique solution (ridge regression).  </li> <li>Linear convergence of gradient-based methods.  </li> <li>Stabilizes optimization by avoiding flatness.  </li> </ul>"},{"location":"0f%20Convergence/#smoothness","title":"Smoothness","text":"<ul> <li>Ensures safe and predictable step sizes.  </li> <li>Avoids overshooting or divergence.  </li> <li>Justifies constant learning rates for many ML losses.  </li> </ul>"},{"location":"0f%20Convergence/#condition-number_1","title":"Condition Number","text":"<ul> <li>Predicts convergence speed.  </li> <li>Guides preprocessing: scaling, normalization, whitening.  </li> <li>Central in designing adaptive optimizers and preconditioning methods.  </li> </ul>"},{"location":"0f%20Convergence/#convergence-rates-of-first-order-methods","title":"Convergence Rates of First-Order Methods","text":"Function Property Gradient Descent Rate Accelerated Gradient (Nesterov) Subgradient Method Rate Convex (not strongly convex) \\(O(1/t)\\) \\(O(1/t^2)\\) \\(O(1/\\sqrt{t})\\) \\(\\mu\\)-Strongly Convex Linear: \\(O\\big((1-\\eta\\mu)^t\\big)\\) Linear: faster than GD \\(O(\\log t / t)\\) Condition Number \\(\\kappa\\) Iterations \\(\\sim O(\\kappa \\log(1/\\epsilon))\\) Iterations \\(\\sim O(\\sqrt{\\kappa}\\log(1/\\epsilon))\\) \u2013"},{"location":"0f%20Convergence/#intuitive-summary","title":"Intuitive Summary","text":"<ul> <li>Strong convexity: bowl is always curved enough \u2192 unique and fast convergence.  </li> <li>Smoothness: bowl is not too steep \u2192 safe steps, avoids overshooting.  </li> <li>Condition number: how round vs stretched the bowl is \u2192 dictates optimization difficulty.  </li> <li>Without strong convexity \u2192 flat regions \u2192 slow sublinear convergence.  </li> <li>Without smoothness \u2192 steep gradient changes \u2192 possible divergence or oscillations.</li> </ul>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/","title":"0g Proximal and Projected Gradient Descent","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projections-and-proximal-operators-in-constrained-convex-optimization","title":"Projections and Proximal Operators in Constrained Convex Optimization","text":"<p>In many convex optimization problems, we want to minimize a convex, differentiable function \\(f(x)\\) subject to some constraint that limits \\(x\\) to a feasible region \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\):</p> \\[ \\min_{x \\in \\mathcal{X}} f(x) \\] <p>A standard gradient descent step is</p> \\[ x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>but this update might move \\(x_{t+1}\\) outside the feasible region \\(\\mathcal{X}\\). To fix that, we add a projection step that brings the point back into the allowed set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#projection-operator","title":"Projection Operator","text":"<p>The projection of a point \\(y\\) onto a convex set \\(\\mathcal{X}\\) is the closest point in the set to \\(y\\):</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 \\] <p>So the projected gradient descent update becomes</p> \\[ x_{t+1} = \\text{Proj}_{\\mathcal{X}}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#geometric-intuition","title":"Geometric intuition","text":"<p>Think of taking a gradient step in the direction of steepest descent, possibly leaving the feasible region. The projection then \u201csnaps\u201d that point back to the nearest feasible location. This ensures all iterates \\(x_t\\) stay within \\(\\mathcal{X}\\) while still moving downhill with respect to \\(f\\).</p> <p>Example: If \\(\\mathcal{X} = \\{x : \\|x\\|_2 \\le 1\\}\\) (the unit ball), the projection is</p> \\[ \\text{Proj}_{\\mathcal{X}}(y) = \\frac{y}{\\max(1, \\|y\\|_2)} \\] <p>That means: - If \\(y\\) is inside the ball, it stays there (\\(\\|y\\|_2 \\le 1\\)). - If \\(y\\) is outside, scale it down to lie exactly on the boundary.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#from-projections-to-proximal-operators","title":"From Projections to Proximal Operators","text":"<p>Projection helps when constraints are explicitly defined by a set (e.g., nonnegativity or norm bounds). But many optimization problems include non-smooth regularization terms instead \u2014 for example, \\(g(x) = \\lambda \\|x\\|_1\\) to promote sparsity.</p> <p>The proximal operator generalizes projection to handle such non-smooth functions directly.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#definition","title":"Definition","text":"<p>For a convex (possibly non-differentiable) function \\(g(x)\\), its proximal operator is defined as:</p> \\[ \\text{prox}_{\\lambda g}(y) = \\arg\\min_x \\left( g(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#interpretation","title":"Interpretation","text":"<p>The proximal operator finds a point \\(x\\) that balances two objectives:</p> <ol> <li>Stay close to \\(y\\) \u2014 enforced by the squared term \\(\\frac{1}{2\\lambda}\\|x - y\\|^2\\).</li> <li>Reduce \\(g(x)\\) \u2014 the regularization or penalty term.</li> </ol> <p>The parameter \\(\\lambda &gt; 0\\) controls this trade-off:</p> <ul> <li>A small \\(\\lambda\\) \u2192 stronger pull toward \\(y\\) (less movement).  </li> <li>A large \\(\\lambda\\) \u2192 more freedom to reduce \\(g(x)\\).</li> </ul> <p>The squared distance term acts as a soft tether, keeping \\(x\\) near \\(y\\) while allowing it to move toward regions where \\(g(x)\\) is smaller or structured.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#indicator-function-and-connection-to-projection","title":"Indicator Function and Connection to Projection","text":"<p>Let\u2019s see how projection appears as a special case of the proximal operator.</p> <p>Define the indicator function of a convex set \\(\\mathcal{X}\\) as:</p> \\[ I_{\\mathcal{X}}(x) = \\begin{cases} 0, &amp; x \\in \\mathcal{X} \\\\ +\\infty, &amp; x \\notin \\mathcal{X} \\end{cases} \\] <p>Now, substitute \\(g(x) = I_{\\mathcal{X}}(x)\\) into the definition of the proximal operator:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_x \\Big( I_{\\mathcal{X}}(x) + \\frac{1}{2\\lambda}\\|x - y\\|^2 \\Big) \\] <p>Because \\(I_{\\mathcal{X}}(x)\\) is infinite outside \\(\\mathcal{X}\\), the minimization is effectively restricted to \\(x \\in \\mathcal{X}\\). Thus we get:</p> \\[ \\text{prox}_{\\lambda I_{\\mathcal{X}}}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x - y\\|^2 = \\text{Proj}_{\\mathcal{X}}(y) \\] <p>\u2705 Therefore, projection is just a proximal operator for the indicator of a set.</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#understanding-the-proximal-step","title":"Understanding the Proximal Step","text":"<p>The proximal operator can be viewed as a correction step:</p> <ul> <li>The gradient step moves toward minimizing the smooth part \\(f(x)\\).</li> <li>The proximal step adjusts that move to respect the structure imposed by \\(g(x)\\) \u2014 e.g., sparsity, nonnegativity, or feasibility.</li> </ul> <p>When combining both, we get the proximal gradient method:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>This algorithm generalizes projected gradient descent \u2014 it works for both constraint sets (through indicator functions) and regularizers (like \\(\\ell_1\\)-norms).</p>"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#example-proximal-of-the-ell_1-norm","title":"Example: Proximal of the \\(\\ell_1\\)-Norm","text":"<p>We want to compute the proximal operator of the \\(\\ell_1\\)-norm:</p> \\[ \\text{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\arg\\min_x \\left( \\lambda \\|x\\|_1 + \\frac{1}{2}\\|x - y\\|^2 \\right) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-1-coordinate-wise-separation","title":"Step 1. Coordinate-wise separation","text":"<p>Because both \\(\\|x\\|_1\\) and \\(\\|x - y\\|^2\\) are separable across coordinates, we can solve for each component independently:</p> \\[ \\min_x \\left( \\lambda |x| + \\frac{1}{2}(x - y)^2 \\right) \\] <p>Thus, we only need to handle the scalar problem for one coordinate \\(y \\in \\mathbb{R}\\):</p> \\[ \\phi(x) = \\lambda |x| + \\frac{1}{2}(x - y)^2 \\] <p>and find</p> \\[ x^\\star = \\arg\\min_x \\phi(x) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-2-subgradient-optimality-condition","title":"Step 2. Subgradient optimality condition","text":"<p>Since \\(\\phi\\) is convex (but not differentiable at \\(x = 0\\)), the optimality condition is</p> \\[ 0 \\in \\partial \\phi(x^\\star) \\] <p>Compute the subgradient:</p> \\[ \\partial \\phi(x) = \\lambda \\, \\partial |x| + (x - y) \\] <p>where</p> \\[ \\partial |x| = \\begin{cases} \\{1\\}, &amp; x &gt; 0 \\\\[4pt] [-1, 1], &amp; x = 0 \\\\[4pt] \\{-1\\}, &amp; x &lt; 0 \\end{cases} \\] <p>Hence, the optimality condition becomes</p> \\[ 0 \\in \\lambda s + (x^\\star - y), \\quad s \\in \\partial |x^\\star| \\] <p>Rewriting:</p> \\[ x^\\star = y - \\lambda s, \\quad s \\in \\partial |x^\\star| \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-3-case-analysis","title":"Step 3. Case Analysis","text":""},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-1-xstar-0","title":"Case 1: \\(x^\\star &gt; 0\\)","text":"<p>Then \\(s = 1\\), so</p> \\[ x^\\star = y - \\lambda \\] <p>This is valid only if \\(x^\\star &gt; 0 \\implies y &gt; \\lambda\\).</p> <p>Hence, when \\(y &gt; \\lambda\\), the minimizer is:</p> \\[ x^\\star = y - \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-2-xstar-0","title":"Case 2: \\(x^\\star &lt; 0\\)","text":"<p>Then \\(s = -1\\), so</p> \\[ x^\\star = y + \\lambda \\] <p>This is valid only if \\(x^\\star &lt; 0 \\implies y &lt; -\\lambda\\).</p> <p>Hence, when \\(y &lt; -\\lambda\\), the minimizer is:</p> \\[ x^\\star = y + \\lambda \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#case-3-xstar-0","title":"Case 3: \\(x^\\star = 0\\)","text":"<p>Then \\(s \\in [-1, 1]\\), and the condition</p> \\[ 0 \\in \\lambda s + (0 - y) \\] <p>means there exists \\(s \\in [-1, 1]\\) such that \\(y = \\lambda s\\). This happens exactly when \\(y \\in [-\\lambda, \\lambda]\\).</p> <p>Hence, when \\(|y| \\le \\lambda\\), the minimizer is:</p> \\[ x^\\star = 0 \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-4-combine-the-cases","title":"Step 4. Combine the cases","text":"<p>Putting the three cases together:</p> \\[ \\text{prox}_{\\lambda |\\cdot|}(y) = \\begin{cases} y - \\lambda, &amp; y &gt; \\lambda \\\\[6pt] 0, &amp; |y| \\le \\lambda \\\\[6pt] y + \\lambda, &amp; y &lt; -\\lambda \\end{cases} \\] <p>Or equivalently, in compact form:</p> \\[ \\boxed{ \\text{prox}_{\\lambda |\\cdot|}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda, 0) } \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-5-extend-to-vector-case","title":"Step 5. Extend to vector case","text":"<p>For a vector \\(y \\in \\mathbb{R}^n\\), the proximal operator applies coordinate-wise:</p> \\[ \\big(\\text{prox}_{\\lambda \\|\\cdot\\|_1}(y)\\big)_i = \\text{sign}(y_i) \\cdot \\max(|y_i| - \\lambda, 0) \\]"},{"location":"0g%20Proximal%20and%20Projected%20Gradient%20Descent/#step-6-intuition","title":"Step 6. Intuition","text":"<ul> <li>When \\(|y_i| \\le \\lambda\\), the quadratic term cannot compensate for the \\(\\ell_1\\) penalty, so the coordinate shrinks to zero (sparsity).</li> <li>When \\(|y_i| &gt; \\lambda\\), the coordinate is shrunk by \\(\\lambda\\) toward zero but remains nonzero.</li> <li>This behavior is called soft-thresholding, and it is the key to algorithms like LASSO and ISTA for sparse recovery.</li> </ul>"},{"location":"0g1%20proximal%20ga/","title":"Proximal Gradient Algorithm","text":""},{"location":"0g1%20proximal%20ga/#proximal-gradient-algorithm","title":"Proximal Gradient Algorithm","text":"<p>Many optimization problems involve composite objectives of the form:</p> \\[ \\min_{x \\in \\mathbb{R}^n} F(x) := f(x) + g(x) \\] <p>where:  </p> <ul> <li>\\(f(x)\\) is convex and differentiable with a Lipschitz continuous gradient (\\(\\nabla f\\) exists and is \\(L\\)-Lipschitz).  </li> <li>\\(g(x)\\) is convex but possibly non-differentiable (e.g., \\(\\ell_1\\)-norm, indicator of a constraint set).  </li> </ul> <p>This structure appears in many applications: LASSO (\\(f = \\text{least squares}, g = \\lambda \\|x\\|_1\\)), elastic net, constrained optimization, etc.</p>"},{"location":"0g1%20proximal%20ga/#1-motivation","title":"1. Motivation","text":"<ul> <li>Standard gradient descent cannot handle \\(g(x)\\) if it is non-differentiable.  </li> <li>Projected gradient descent works only if \\(g\\) is an indicator function of a set.  </li> <li>The proximal gradient method generalizes both approaches and allows efficient updates even when \\(g\\) is non-smooth.</li> </ul>"},{"location":"0g1%20proximal%20ga/#2-proximal-gradient-update","title":"2. Proximal Gradient Update","text":"<p>For step size \\(\\eta &gt; 0\\), the proximal gradient update is:</p> \\[ x_{t+1} = \\text{prox}_{\\eta g}\\big(x_t - \\eta \\nabla f(x_t)\\big) \\] <p>Interpretation:</p> <ol> <li>Take a gradient step on the smooth part \\(f\\):</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply the proximal operator of \\(g\\) to handle the non-smooth part:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <p>This ensures:</p> <ul> <li>\\(f(x)\\) decreases via the gradient step.  </li> <li>\\(g(x)\\) is accounted for via the proximal step.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#3-step-size-selection","title":"3. Step Size Selection","text":"<p>For convergence, the step size \\(\\eta\\) is typically chosen as:</p> \\[ 0 &lt; \\eta \\le \\frac{1}{L} \\] <p>where \\(L\\) is the Lipschitz constant of \\(\\nabla f\\).  </p> <ul> <li>Smaller \\(\\eta\\) \u2192 conservative steps.  </li> <li>Larger \\(\\eta\\) may overshoot and break convergence guarantees.  </li> <li>Adaptive strategies (like backtracking line search) can also be used.</li> </ul>"},{"location":"0g1%20proximal%20ga/#4-algorithm-proximal-gradient-method-ista","title":"4. Algorithm (Proximal Gradient Method / ISTA)","text":"<p>Input: \\(x_0\\), step size \\(\\eta &gt; 0\\) </p> <p>Repeat for \\(t = 0, 1, 2, \\dots\\):  </p> <ol> <li>Compute gradient step:</li> </ol> \\[ y_t = x_t - \\eta \\nabla f(x_t) \\] <ol> <li>Apply proximal operator:</li> </ol> \\[ x_{t+1} = \\text{prox}_{\\eta g}(y_t) \\] <ol> <li>Check convergence (e.g., \\(\\|x_{t+1} - x_t\\| &lt; \\epsilon\\)).</li> </ol>"},{"location":"0g1%20proximal%20ga/#5-special-cases","title":"5. Special Cases","text":"Non-smooth term \\(g(x)\\) Proximal operator \\(\\text{prox}_{\\eta g}(y)\\) Interpretation \\(\\lambda \\|x\\|_1\\) Soft-thresholding: $\\text{sign}(y_i)\\max( y_i Indicator \\(I_{\\mathcal{X}}(x)\\) Projection: \\(\\text{Proj}_{\\mathcal{X}}(y)\\) Constrained optimization \\(\\lambda \\|x\\|_2^2\\) Shrinkage: \\(y / (1 + 2\\eta\\lambda)\\) Smooth regularization"},{"location":"0g1%20proximal%20ga/#6-properties-of-proximal-operators","title":"6. Properties of Proximal Operators","text":"<p>Proximal operators have several useful mathematical properties:</p>"},{"location":"0g1%20proximal%20ga/#non-expansiveness-lipschitz-continuity","title":"Non-expansiveness (Lipschitz continuity):","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2 \\le \\|x - y\\|_2, \\quad \\forall x, y \\]"},{"location":"0g1%20proximal%20ga/#firmly-non-expansive","title":"Firmly non-expansive:","text":"\\[ \\|\\text{prox}_{g}(x) - \\text{prox}_{g}(y)\\|_2^2 \\le \\langle \\text{prox}_{g}(x) - \\text{prox}_{g}(y), x - y \\rangle \\]"},{"location":"0g1%20proximal%20ga/#fixed-point-characterization","title":"Fixed point characterization:","text":"\\[ x^\\star = \\text{prox}_{g}(x^\\star - \\eta \\nabla f(x^\\star)) \\quad \\Longleftrightarrow \\quad 0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\]"},{"location":"0g1%20proximal%20ga/#translation-property","title":"Translation property:","text":"\\[ \\text{prox}_{g}(x + c) = \\text{prox}_{g(\\cdot - c)}(x) + c \\]"},{"location":"0g1%20proximal%20ga/#separable-for-sums-over-coordinates","title":"Separable for sums over coordinates:","text":"<p>If \\(g(x) = \\sum_i g_i(x_i)\\), then</p> \\[ \\text{prox}_{g}(x) = \\big( \\text{prox}_{g_1}(x_1), \\dots, \\text{prox}_{g_n}(x_n) \\big) \\] <p>This is why soft-thresholding works coordinate-wise.</p>"},{"location":"0g1%20proximal%20ga/#7-why-proximal-gradient-works","title":"7. Why Proximal Gradient Works","text":"<ul> <li>The proximal gradient method splits the objective into smooth and non-smooth parts.  </li> <li>The gradient step moves toward minimizing \\(f(x)\\) (smooth).  </li> <li>The proximal step moves toward minimizing \\(g(x)\\) (structure or constraints).  </li> <li> <p>Geometrically, the proximal operator finds a point close to the gradient update but also reduces the non-smooth term, ensuring convergence under convexity and Lipschitz continuity.  </p> </li> <li> <p>If \\(g = 0\\), it reduces to gradient descent.  </p> </li> <li>If \\(g\\) is an indicator function, it reduces to projected gradient descent.  </li> </ul>"},{"location":"0g1%20proximal%20ga/#8-convergence","title":"8. Convergence","text":"<p>For convex \\(f\\) and \\(g\\):</p> \\[ F(x_t) - F(x^\\star) = \\mathcal{O}\\Big(\\frac{1}{t}\\Big) \\] <ul> <li>Accelerated variants (like FISTA) improve the rate to \\(\\mathcal{O}(1/t^2)\\).  </li> <li>Requires convexity and Lipschitz continuity of \\(\\nabla f\\).</li> </ul>"},{"location":"0g1%20proximal%20ga/#9-fast-iterative-shrinkage-thresholding-algorithm-fista","title":"9. Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)","text":"<p>While ISTA converges at \\(\\mathcal{O}(1/t)\\), FISTA introduces a clever momentum / extrapolation term that accelerates convergence to \\(\\mathcal{O}(1/t^2)\\).</p> <p>Algorithm (FISTA):</p> <p>Initialize \\(x_0\\), set \\(y_0 = x_0\\), \\(t_0 = 1\\).</p> <p>For \\(k = 0, 1, 2, \\dots\\):</p> <ol> <li>Gradient and proximal update:</li> </ol> \\[ x_{k+1} = \\text{prox}_{\\eta g}(y_k - \\eta \\nabla f(y_k)) \\] <ol> <li>Update momentum parameter:</li> </ol> \\[ t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2} \\] <ol> <li>Extrapolation (Nesterov acceleration):</li> </ol> \\[ y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}}(x_{k+1} - x_k) \\]"},{"location":"0g1%20proximal%20ga/#key-insight","title":"Key Insight","text":"<ul> <li>ISTA updates only based on \\(x_k\\).  </li> <li>FISTA introduces a look-ahead point \\(y_k\\) that combines past iterates, similar to momentum methods in deep learning.</li> <li>This extrapolation step dramatically speeds up convergence without changing the proximal operator.</li> </ul>"},{"location":"0g1%20proximal%20ga/#comparison-summary","title":"Comparison Summary","text":"Method Update Uses Convergence Rate Gradient Descent (\\(g = 0\\)) \\(\\nabla f(x_t)\\) \\(\\mathcal{O}(1/t)\\) ISTA \\(\\nabla f(x_t)\\) + prox \\(\\mathcal{O}(1/t)\\) FISTA \\(\\nabla f(y_t)\\) + prox + momentum \\(\\mathcal{O}(1/t^2)\\) \u2705"},{"location":"0h%20lasso/","title":"0h lasso","text":""},{"location":"0h%20lasso/#lasso-and-optimization-methods","title":"LASSO and Optimization Methods","text":"<p>The LASSO problem is formulated as:</p> \\[ \\min_{x \\in \\mathbb{R}^p} \\ \\|Ax - y\\|_2^2 + \\lambda \\|x\\|_1 \\] <ul> <li>\\(A \\in \\mathbb{R}^{n \\times p}\\): measurement/design matrix.</li> <li>\\(y \\in \\mathbb{R}^n\\): observations.</li> <li>\\(\\|x\\|_1 = \\sum_{i=1}^p |x_i|\\): promotes sparsity.</li> <li>Assumption: \\(x\\) is \\(s\\)-sparse, meaning only \\(s \\ll p\\) entries are nonzero \u2192 compressed sensing.</li> </ul>"},{"location":"0h%20lasso/#1-lasso-via-subgradient-descent","title":"1.  LASSO via Subgradient Descent","text":"<p>Since \\(\\|x\\|_1\\) is non-smooth, we use subgradient descent:</p> <p>Update rule: </p> <p>where the subgradient \\(z \\in \\partial \\|x\\|_1\\) is:</p> \\[ z_i = \\begin{cases} +1, &amp; x_i &gt; 0 \\\\ -1, &amp; x_i &lt; 0 \\\\ [-1, 1], &amp; x_i = 0 \\end{cases} \\] <ul> <li>Convergence rate: \\(\\mathcal{O}(1/\\sqrt{t})\\) \u2014 slow for practical use.</li> <li>Still important conceptually but inefficient compared to proximal methods.</li> </ul>"},{"location":"0h%20lasso/#2-proximal-gradient-for-lasso-ista","title":"2. Proximal Gradient for LASSO (ISTA)","text":"<p>To efficiently handle the non-smooth \\(\\ell_1\\) term, we use the proximal gradient (ISTA) method.</p>"},{"location":"0h%20lasso/#step-1-gradient-update-on-smooth-term-ax-y_22","title":"Step 1 \u2014 Gradient update on smooth term \\(\\|Ax - y\\|_2^2\\):","text":"\\[ y_t = x_t - \\eta \\cdot 2A^\\top(Ax_t - y) \\]"},{"location":"0h%20lasso/#step-2-apply-proximal-operator-of-ell_1-soft-thresholding","title":"Step 2 \u2014 Apply proximal operator of \\(\\ell_1\\) (soft thresholding):","text":"\\[ x_{t+1} = \\text{prox}_{\\eta \\lambda \\|\\cdot\\|_1}(y_t) \\] <p>The soft-thresholding operator:</p> \\[ \\text{prox}_{\\alpha \\|\\cdot\\|_1}(z)_i = \\text{sign}(z_i) \\cdot \\max(|z_i| - \\alpha, 0) \\] <p>Thus the ISTA update becomes:</p> \\[ x_{t+1} = \\text{sign}(y_{t,i}) \\cdot \\max(|y_{t,i}| - \\eta \\lambda, 0) \\] <p>Interpretation: Gradient descent + shrinkage toward zero \u2192 automatically induces sparsity.</p>"},{"location":"0h%20lasso/#3-fista-accelerated-proximal-gradient-for-lasso","title":"3. FISTA \u2014 Accelerated Proximal Gradient for LASSO","text":"<p>ISTA has convergence rate \\(\\mathcal{O}(1/t)\\). FISTA (Fast ISTA) improves it to:</p> \\[ \\mathcal{O}(1/t^2) \\quad \\text{(optimal for first-order methods)} \\] <p>Algorithm (FISTA):</p> <ul> <li>Initialize: \\(x_0\\), set \\(y_0 = x_0\\), \\(t_0 = 1\\).</li> <li> <p>For \\(k = 0, 1, 2, \\dots\\):</p> </li> <li> <p>Proximal gradient step: </p> </li> <li> <p>Update momentum parameter: </p> </li> <li> <p>Nesterov extrapolation step: </p> </li> </ul> <p>Key idea: Instead of updating only from \\(x_k\\) (like ISTA), FISTA uses a look-ahead point \\(y_k\\) to inject momentum and accelerate convergence.</p>"},{"location":"0h%20lasso/#method-comparison","title":"\ud83d\udcca Method Comparison","text":"Method Handles \\(\\ell_1\\)? Uses Prox? Convergence Rate Gradient Descent \u274c \u274c Fast (smooth only) Subgradient Descent \u2705 \u274c \\(\\mathcal{O}(1/\\sqrt{t})\\) (slow) ISTA (Proximal GD) \u2705 \u2705 \\(\\mathcal{O}(1/t)\\) FISTA (Accelerated) \u2705 \u2705 \u2705 \\(\\mathcal{O}(1/t^2)\\) \u2705"},{"location":"0k%20AdvancedAlgos/","title":"0k AdvancedAlgos","text":""},{"location":"0k%20AdvancedAlgos/#first-order-gradient-based-methods","title":"First-Order Gradient-Based Methods","text":"<p>Used when: Only gradient information is available; scalable to high-dimensional problems.</p>"},{"location":"0k%20AdvancedAlgos/#gradient-descent-gd","title":"Gradient Descent (GD)","text":"<ul> <li>Problem: Minimize smooth or convex functions.  </li> <li>Update: </li> <li>Convergence: Convex \u2192 \\(O(1/k)\\); Strongly convex \u2192 linear.  </li> <li>Use case: Small convex problems, theoretical baseline.  </li> <li>Pitfalls: Step size too small \u2192 slow; too large \u2192 divergence.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<ul> <li>Problem: Minimize empirical risk over large datasets.  </li> <li>Update:   (mini-batch gradient)  </li> <li>Pros: Scales to huge datasets; cheap per iteration.  </li> <li>Cons: Noisy updates \u2192 requires learning rate schedules.  </li> <li>ML use: Deep learning, large-scale logistic regression.  </li> </ul> <p>Best Practices: Learning rate warmup, linear scaling with batch size, momentum to stabilize updates, cyclic learning rates for exploration.</p>"},{"location":"0k%20AdvancedAlgos/#momentum-nesterov-accelerated-gradient","title":"Momentum &amp; Nesterov Accelerated Gradient","text":"<ul> <li>Problem: Reduce oscillations and accelerate convergence in ill-conditioned problems.  </li> <li>Momentum: </li> <li>Nesterov: Gradient computed at lookahead point \u2192 theoretically optimal for convex problems.  </li> <li>ML use: CNNs, ResNets, EfficientNet.  </li> <li>Pitfalls: High momentum \u2192 oscillations; careful learning rate tuning required.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#adaptive-methods-adagrad-rmsprop-adam-adamw","title":"Adaptive Methods (AdaGrad, RMSProp, Adam, AdamW)","text":"<ul> <li>Problem: Adjust learning rate per parameter for fast/stable convergence.  </li> <li>Behavior: </li> <li>AdaGrad \u2192 aggressive decay, good for sparse features.  </li> <li>RMSProp \u2192 fixes AdaGrad\u2019s rapid decay.  </li> <li>Adam \u2192 RMSProp + momentum.  </li> <li>AdamW \u2192 decouples weight decay for better generalization.  </li> <li>ML use: Transformers, NLP, sparse models.  </li> <li>Pitfalls: Adam may converge to sharp minima \u2192 worse generalization than SGD in CNNs.  </li> <li>Best Practices: Warmup, cosine LR decay, weight decay with AdamW.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#second-order-curvature-aware-methods","title":"Second-Order &amp; Curvature-Aware Methods","text":"<p>Used when: Hessian or curvature information improves convergence; mostly for small/medium models.</p>"},{"location":"0k%20AdvancedAlgos/#newtons-method","title":"Newton\u2019s Method","text":"<ul> <li>Problem: Solve  with smooth Hessian.  </li> <li>Update: </li> <li>Pros: Quadratic convergence.  </li> <li>Cons: Hessian expensive in high dimensions.  </li> <li>ML use: GLMs, small convex models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#quasi-newton-bfgs-l-bfgs","title":"Quasi-Newton (BFGS, L-BFGS)","text":"<ul> <li>Problem: Approximate Hessian using low-rank updates.  </li> <li>Pros: Efficient for medium-scale problems.  </li> <li>Cons: BFGS memory-heavy; L-BFGS preferred.  </li> <li>ML use: Logistic regression, Cox models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#conjugate-gradient","title":"Conjugate Gradient","text":"<ul> <li>Problem: Solve large linear/quadratic problems efficiently.  </li> <li>ML use: Hessian-free optimization; combined with Pearlmutter trick for Hessian-vector products.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#natural-gradient-k-fac","title":"Natural Gradient &amp; K-FAC","text":"<ul> <li>Problem: Precondition gradients using Fisher Information \u2192 invariant to parameterization.  </li> <li>ML use: Large CNNs, transformers; improves convergence in distributed training.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#constrained-specialized-optimization","title":"Constrained &amp; Specialized Optimization","text":""},{"location":"0k%20AdvancedAlgos/#interior-point","title":"Interior-Point","text":"<ul> <li>Problem: Constrained optimization via barrier functions.  </li> <li>ML use: Structured convex problems, LP/QP.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#admm-augmented-lagrangian","title":"ADMM / Augmented Lagrangian","text":"<ul> <li>Problem: Split constraints into easier subproblems with dual updates.  </li> <li>ML use: Distributed optimization, structured sparsity.  </li> </ul>"},{"location":"0k%20AdvancedAlgos/#frankwolfe","title":"Frank\u2013Wolfe","text":"<ul> <li>Problem: Projection-free constrained optimization; linear subproblem instead of projection.  </li> <li>ML use: Simplex, nuclear norm problems.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#coordinate-descent","title":"Coordinate Descent","text":"<ul> <li>Problem: Update one variable at a time.  </li> <li>ML use: Lasso, GLMs, sparse regression.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#proximal-methods","title":"Proximal Methods","text":"<ul> <li>Problem: Efficiently handle nonsmooth penalties.  </li> <li>Algorithms: ISTA (\\(O(1/k)\\)), FISTA (\\(O(1/k^2)\\))  </li> <li>ML use: Sparse coding, Lasso, elastic net.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#derivative-free-black-box","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Optimize when gradients unavailable or unreliable.  </li> <li>Algorithms: Nelder\u2013Mead, CMA-ES, Bayesian Optimization  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#optimization-problem-styles","title":"Optimization Problem Styles","text":""},{"location":"0k%20AdvancedAlgos/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<ul> <li>Problem: Maximize likelihood or minimize negative log-likelihood: </li> <li>Algorithms: Newton/Fisher scoring, L-BFGS, SGD, EM, Proximal/Coordinate.  </li> <li>ML use: Logistic regression, GLMs, Gaussian mixture models, HMMs.  </li> <li>Notes: EM guarantees monotonic likelihood increase; Fisher scoring uses expected curvature \u2192 stable.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#empirical-risk-minimization-erm","title":"Empirical Risk Minimization (ERM)","text":"<ul> <li>Problem: Minimize average loss with optional regularization: </li> <li>Algorithms: GD, SGD, Momentum, Adam, L-BFGS, Proximal.  </li> <li>ML use: Regression, classification, deep learning.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#regularized-penalized-optimization","title":"Regularized / Penalized Optimization","text":"<ul> <li>Problem: Add penalties to encourage sparsity or smoothness: </li> <li>Algorithms: Proximal gradient, ADMM, Coordinate Descent, ISTA/FISTA.  </li> <li>ML use: Lasso, Elastic Net, sparse dictionary learning.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#constrained-optimization","title":"Constrained Optimization","text":"<ul> <li>Problem: Minimize with equality/inequality constraints.  </li> <li>Algorithms: Interior-point, ADMM, Frank\u2013Wolfe, penalty/barrier methods.  </li> <li>ML use: Fairness constraints, structured prediction.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#bayesian-map-optimization","title":"Bayesian / MAP Optimization","text":"<ul> <li>Problem: Maximize posterior: </li> <li>Algorithms: Gradient-based, Laplace approximation, Variational Inference, MCMC.  </li> <li>ML use: Bayesian neural networks, probabilistic models.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#minimax-adversarial-optimization","title":"Minimax / Adversarial Optimization","text":"<ul> <li>Problem: </li> <li>Algorithms: Gradient descent/ascent, extragradient, mirror descent.  </li> <li>ML use: GANs, adversarial training, robust optimization.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#reinforcement-learning-policy-optimization","title":"Reinforcement Learning / Policy Optimization","text":"<ul> <li>Problem: Maximize expected cumulative reward: </li> <li>Algorithms: Policy gradient, Actor-Critic, Natural Gradient.  </li> <li>ML use: RL agents, sequential decision-making.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<ul> <li>Problem: Optimize multiple competing objectives \u2192 Pareto front.  </li> <li>Algorithms: Scalarization, weighted sum, evolutionary algorithms.  </li> <li>ML use: Multi-task learning, accuracy vs fairness trade-offs.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#metric-embedding-learning","title":"Metric / Embedding Learning","text":"<ul> <li>Problem: Learn embeddings preserving similarity/distance: </li> <li>Algorithms: SGD/Adam with careful sampling.  </li> <li>ML use: Contrastive learning, triplet loss, Siamese networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#combinatorial-discrete-optimization","title":"Combinatorial / Discrete Optimization","text":"<ul> <li>Problem: Optimize discrete/integer variables.  </li> <li>Algorithms: Branch-and-bound, integer programming, RL-based relaxation, Gumbel-softmax.  </li> <li>ML use: Feature selection, neural architecture search, graph matching.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#derivative-free-black-box_1","title":"Derivative-Free / Black-Box","text":"<ul> <li>Problem: Gradients unavailable or noisy.  </li> <li>Algorithms: Bayesian Optimization, CMA-ES, Nelder\u2013Mead.  </li> <li>ML use: Hyperparameter tuning, neural architecture search, small networks.</li> </ul>"},{"location":"0k%20AdvancedAlgos/#learning-rate-practical-tips","title":"Learning Rate &amp; Practical Tips","text":"<ul> <li>Step decay, cosine annealing, OneCycle, warmup.  </li> <li>Gradient clipping (global norm 1\u20135), batch/layer normalization, FP16 mixed precision.  </li> <li>Decouple weight decay from Adam (AdamW).</li> </ul>"},{"location":"0k%20AdvancedAlgos/#summary","title":"Summary","text":"Algorithm Problem Type ML / AI Use Case GD Smooth / convex Small convex models, baseline SGD Large-scale ERM Deep learning, logistic regression SGD + Momentum Ill-conditioned / deep nets CNNs (ResNet, EfficientNet) Nesterov Accelerated GD Convex / ill-conditioned CNNs, small convex models AdaGrad Sparse features NLP, sparse embeddings RMSProp Stabilized adaptive LR RNNs, sequence models Adam Adaptive large-scale Transformers, small nets AdamW Adaptive + weight decay Transformers, NLP Newton / Fisher Scoring Smooth convex GLMs, small MLE BFGS / L-BFGS Medium convex Logistic regression, Cox models Conjugate Gradient Linear / quadratic Hessian-free optimization, linear regression Natural Gradient / K-FAC Deep nets CNNs, transformers Proximal / ISTA / FISTA Nonsmooth / sparse Lasso, sparse coding, elastic net Coordinate Descent Separable / sparse Lasso, GLMs Interior-Point Constrained convex LP/QP problems ADMM Distributed convex Sparse or structured optimization Frank\u2013Wolfe Projection-free constraints Simplex, nuclear norm problems EM Algorithm Latent variable MLE GMM, HMM, LDA Policy Gradient / Actor-Critic Sequential / RL RL agents Bayesian Optimization Black-box / derivative-free Hyperparameter tuning, NAS CMA-ES / Nelder-Mead Black-box Small networks, continuous black-box Minimax / Gradient Ascent-Descent Adversarial GANs, robust optimization Multi-Objective / Evolutionary Multiple objectives Multi-task learning, fairness Metric Learning / Triplet Loss Similarity embedding Contrastive learning, Siamese nets"},{"location":"0k%20mirror/","title":"0k mirror","text":"<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, it implicitly assumes Euclidean geometry, which may not respect the natural structure of many problems. Mirror Descent generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence, making it particularly suitable for constrained, probabilistic, and sparse domains. </p>"},{"location":"0k%20mirror/#1-introduction-and-motivation","title":"1. Introduction and Motivation","text":"<p>Gradient Descent is often introduced as the default optimization method:</p> \\[x_{t+1} = x_t - \\eta \\nabla f(x_t)\\] <p>This seemingly simple update assumes that the underlying optimization space is Euclidean, where distance is measured using the \\(\\ell_2\\) norm:</p> \\[\\|x - y\\|_2 = \\sqrt{\\sum_i (x_i - y_i)^2}\\] <p>Works well for unconstrained problems in \\(\\mathbb{R}^n\\) with no additional structure.  </p> <p>However, in real-world machine learning and optimization problems:</p> <ul> <li>Parameters often live in structured spaces like probability simplices or sparse domains.</li> <li>Euclidean distance is often not the most natural notion of distance.</li> <li>Applying Euclidean updates can destroy problem structure or create instabilities.</li> </ul> <p>Key insight: Gradient Descent is not inherently \u201cwrong\u201d\u2014it\u2019s just geometry-specific. Mirror Descent generalizes GD to respect the intrinsic geometry of the problem.</p>"},{"location":"0k%20mirror/#2-geometry-in-optimization-why-it-matters","title":"2. Geometry in Optimization \u2014 Why It Matters","text":"<p>Standard GD treats all directions equally. The steepest descent direction is simply the gradient \\(\\nabla f(x)\\), derived from Euclidean distance. This is equivalent to asking: \"In which direction does \\(f(x)\\) decrease fastest if distance is measured by the \\(\\ell_2\\) norm?\"</p> <p>While sufficient for many unconstrained problems, this implicitly assumes:</p> <ul> <li>The feasible set is unbounded or flat.</li> <li>Movement along all axes is equally \u201ccostly\u201d.</li> <li>There are no constraints like positivity or normalization.</li> </ul>"},{"location":"0k%20mirror/#when-euclidean-geometry-fails","title":"When Euclidean Geometry Fails","text":"<p>Many modern optimization problems involve structured domains:</p> Scenario Constraint / Structure Natural Geometry Probability vectors \\(x_i \\ge 0, \\sum_i x_i = 1\\) KL divergence / simplex geometry Attention weights Positive and normalized Entropy geometry Sparse models Preference for zeros \\(\\ell_1\\) geometry Online learning Avoid drastic updates Multiplicative weights / log-space <p>Using Euclidean GD in these settings can lead to:</p> <ul> <li>Harsh projections that instantly zero out components.</li> <li>Violation of sparsity or positivity constraints.</li> <li>Loss of smoothness or natural probabilistic interpretation.</li> </ul> <p>Observation: Gradient Descent works \u201clocally,\u201d but may be incompatible with global geometry of the feasible domain.</p>"},{"location":"0k%20mirror/#3-mirror-descent-a-geometric-generalization","title":"3. Mirror Descent: A Geometric Generalization","text":"<p>Mirror Descent adapts Gradient Descent to non-Euclidean geometries, encoding the structure of the optimization space.</p>"},{"location":"0k%20mirror/#mirror-maps-and-dual-coordinates","title":"Mirror Maps and Dual Coordinates","text":"<p>A mirror map \\(\\psi(x)\\) is a strictly convex, differentiable function representing the geometry:</p> \\[u = \\nabla \\psi(x)\\] <ul> <li>\\(x\\) = primal variable  </li> <li>\\(u\\) = dual variable (coordinates in transformed space)</li> </ul> <p>Updates occur in the dual space, then are mapped back using the convex conjugate:</p> \\[x = \\nabla \\psi^*(u)\\] <p>This allows GD-like updates while respecting geometry constraints.</p>"},{"location":"0k%20mirror/#bregman-divergence-and-interpretation","title":"Bregman Divergence and Interpretation","text":"<p>The Bregman divergence associated with \\(\\psi\\) generalizes squared Euclidean distance:</p> \\[D_\\psi(x \\| y) = \\psi(x) - \\psi(y) - \\langle \\nabla \\psi(y), x - y \\rangle\\]"},{"location":"0k%20mirror/#intuition","title":"Intuition:","text":"<ul> <li>Think of \\(D_\\psi(x \\| y)\\) as a geometry-aware distance measure.  </li> <li>It captures how far \\(x\\) is from \\(y\\) in the space defined by \\(\\psi\\), not just in straight-line Euclidean distance.</li> <li> <p>Conceptually, it measures the error between the linear approximation of \\(\\psi\\) at \\(y\\) and the true value at \\(x\\):</p> </li> <li> <p>\\(\\psi(y) + \\langle \\nabla \\psi(y), x - y \\rangle\\) = linear approximation  </p> </li> <li> <p>\\(\\psi(x) - (\\text{linear approximation})\\) = how \"nonlinear\" the space feels from \\(y\\) to \\(x\\) </p> </li> <li> <p>When \\(\\psi(x) = \\frac12 \\|x\\|_2^2\\), the Bregman divergence reduces to Euclidean distance squared.  </p> </li> <li> <p>For negative entropy (common in probability spaces), it reduces to KL divergence.</p> </li> <li> <p>Purpose in MD: Ensures updates respect the intrinsic geometry, balancing movement in the objective with staying \u201cclose\u201d in the right geometry.</p> </li> </ul> <p>Mirror Descent update (primal form):</p> \\[x_{t+1} = \\arg\\min_{x \\in \\mathcal{X}} \\left\\{ \\langle \\nabla f(x_t), x - x_t \\rangle + \\frac{1}{\\eta} D_\\psi(x \\| x_t) \\right\\}\\] <p>Move in a descent direction while staying \u201cclose\u201d according to Bregman divergence, not Euclidean distance.</p>"},{"location":"0k%20mirror/#4-gradient-descent-vs-mirror-descent","title":"4. Gradient Descent vs Mirror Descent","text":""},{"location":"0k%20mirror/#primal-view-projection-vs-bregman-step","title":"Primal View (Projection vs Bregman Step)","text":"<ul> <li>GD: Step in Euclidean space; may leave the feasible domain \u2192 project back.  </li> <li>MD: Step along geometry-aware Bregman divergence; no harsh projection needed.</li> </ul> Method Update Rule Notes Gradient Descent \\(x - \\eta \\nabla f\\) Euclidean, may leave domain Projected GD \\(\\text{Proj}(x - \\eta \\nabla f)\\) Projection may destroy smoothness Mirror Descent \\(\\arg\\min_x \\langle \\nabla f, x - x_t \\rangle + \\frac{1}{\\eta} D_\\psi(x\\|x_t)\\) Structure-preserving"},{"location":"0k%20mirror/#dual-view-gd-in-dual-space","title":"Dual View (GD in Dual Space)","text":"<p>Mirror Descent can also be understood as Gradient Descent in dual coordinates:</p> \\[ \\begin{aligned} u_t &amp;= \\nabla \\psi(x_t) \\\\ u_{t+1} &amp;= u_t - \\eta \\nabla f(x_t) \\\\ x_{t+1} &amp;= \\nabla \\psi^*(u_{t+1}) \\end{aligned} \\] <p>\u2705 MD is GD in a warped coordinate system, where distance and directions are geometry-aware.</p>"},{"location":"0k%20mirror/#5-intuitive-example-on-the-simplex","title":"5. Intuitive Example on the Simplex","text":"<p>Consider \\(x \\in \\Delta^2 = \\{ x \\ge 0, x_1 + x_2 = 1 \\}\\) and objective:</p> \\[f(x) = x_1^2 + 2 x_2\\] <p>Initial point: \\(x = (0.5, 0.5)\\), step size \\(\\eta = 0.3\\).</p>"},{"location":"0k%20mirror/#behavior-of-gd-projection","title":"Behavior of GD + Projection","text":"<ol> <li>Gradient: \\(\\nabla f = (2x_1, 2) = (1,2)\\)</li> <li>Step: \\(y = x - \\eta \\nabla f = (0.2, -0.1)\\)</li> <li>Project onto simplex: \\(x_{\\text{new}} = (1, 0)\\)</li> </ol> <p>\u274c Projection abruptly kills one component. Smoothness and probabilistic structure are lost.</p>"},{"location":"0k%20mirror/#behavior-of-mirror-descent-kl-negative-entropy","title":"Behavior of Mirror Descent (KL / Negative Entropy)","text":"<p>Mirror map: \\(\\psi(x) = \\sum_i x_i \\log x_i\\) </p> <p>Update rule:</p> <p>\\(x_i^{\\text{new}} \\propto x_i \\exp(-\\eta \\nabla_i f(x))\\)</p> <p>Normalized:</p> <p>\\(x \\approx (0.57, 0.43)\\)</p> <p>\u2705 Smooth, positive, stays in the simplex, no harsh projection.</p>"},{"location":"0k%20mirror/#interpretation","title":"Interpretation","text":"Method Intuition GD + Projection Walks straight \u2192 hits boundary \u2192 forced back Mirror Descent Walks along curved, geometry-aware space \u2192 never violates constraints <p>Mirror Descent = optimization with geometry turned ON.</p>"},{"location":"0k%20mirror/#6-choosing-the-mirror-map-geometry-as-a-design-choice","title":"6. Choosing the Mirror Map \u2014 Geometry as a Design Choice","text":""},{"location":"0k%20mirror/#entropy-geometry-simplex","title":"Entropy Geometry (Simplex)","text":"<ul> <li>Mirror map: \\(\\psi(x) = \\sum_i x_i \\log x_i\\)</li> <li>Divergence: KL divergence</li> <li>Update: multiplicative weights  </li> <li>Applications: probability vectors, attention mechanisms</li> </ul>"},{"location":"0k%20mirror/#ell_1-geometry-sparsity","title":"\\(\\ell_1\\) Geometry (Sparsity)","text":"<ul> <li>Mirror map encourages sparse updates</li> <li>Useful in compressed sensing, feature selection</li> </ul>"},{"location":"0k%20mirror/#euclidean-as-a-special-case","title":"Euclidean as a Special Case","text":"<ul> <li>Mirror map: \\(\\psi(x) = \\frac12 \\|x\\|_2^2\\)</li> <li>Divergence: squared Euclidean distance</li> <li>Recovers standard GD</li> </ul>"},{"location":"0k%20mirror/#7-practical-guidance-for-practitioners","title":"7. Practical Guidance for Practitioners","text":""},{"location":"0k%20mirror/#when-to-prefer-mirror-descent","title":"When to Prefer Mirror Descent","text":"<ul> <li>Structured domains (simplex, positive vectors, sparse spaces)</li> <li>Smooth, structure-preserving updates required</li> <li>Avoiding costly or disruptive Euclidean projections</li> </ul>"},{"location":"0k%20mirror/#computational-remarks","title":"Computational Remarks","text":"<ul> <li>Choice of mirror map affects efficiency (some dual mappings are cheap, others expensive)</li> <li>Often simple closed-form updates exist (multiplicative weights, exponentiated gradient)</li> <li>Integration with adaptive step sizes or momentum is possible</li> </ul> <p>Mirror Descent is a powerful generalization of Gradient Descent, making the geometry of the domain explicit in the update rule. By carefully choosing a mirror map, one can design updates that:</p> <ul> <li>Preserve constraints naturally</li> <li>Avoid projection shocks</li> <li>Respect sparsity or probability structure</li> <li>Connect elegantly to modern ML methods like attention, boosting, and natural gradient</li> </ul>"},{"location":"0m%20sgd/","title":"0m sgd","text":"<p>So far, all optimization methods we have discussed, including Gradient Descent (GD) and Mirror Descent (MD), assume exact evaluation of gradients or subgradients. In practice, however, computing exact gradients is often impossible or computationally expensive. Stochastic Gradient Descent (SGD) and related methods address this by using noisy or approximate gradients, enabling scalable optimization in large-scale or complex settings</p>"},{"location":"0m%20sgd/#1-motivation","title":"1. Motivation","text":"<p>Classical optimization relies on computing the exact gradient \\(\\nabla f(x)\\) at each iteration. However, in many real-world scenarios, this is impractical:</p> <ol> <li>Exact gradient unavailable </li> <li>Some functions are non-differentiable or analytically intractable.  </li> <li> <p>Example: complex composite objectives or non-smooth loss functions.</p> </li> <li> <p>Computational cost is prohibitive </p> </li> <li>Large datasets make full gradient computation expensive.  </li> <li> <p>Neural networks require backpropagation over all samples per step, which can be infeasible.</p> </li> <li> <p>Stochastic optimization naturally arises </p> </li> <li>Data scale: Objective involves a sum or expectation over massive datasets.  </li> <li>Intrinsic randomness: Objective defined as an expectation over a stochastic process, e.g., in reinforcement learning, online optimization, or probabilistic modeling.</li> </ol>"},{"location":"0m%20sgd/#2-stochastic-gradient-descent-sgd","title":"2. Stochastic Gradient Descent (SGD)","text":"<p>Instead of computing the full gradient, SGD uses a stochastic estimate \\(g_t\\) such that:</p> \\[ \\mathbb{E}[g_t \\mid x_t] = \\nabla f(x_t) \\] <p>The update rule becomes:</p> \\[ x_{t+1} = x_t - \\eta g_t \\] <p>where:</p> <ul> <li>\\(g_t\\) is a stochastic (noisy) gradient,  </li> <li>\\(\\eta\\) is the step size or learning rate.</li> </ul> <p>Intuition: We follow a \u201cnoisy compass\u201d that points roughly in the right direction instead of computing the exact gradient at every step.</p>"},{"location":"0m%20sgd/#21-sources-of-gradient-noise","title":"2.1 Sources of Gradient Noise","text":"<ul> <li>Finite datasets: Only a subset (mini-batch) of data is used.  </li> <li>Random sampling: Randomly select samples to approximate the full gradient.  </li> <li>Inherent stochasticity: In reinforcement learning or simulation-based optimization, gradients are intrinsically noisy.</li> </ul> <p>Example: Empirical Risk Minimization (ERM)</p> <p>For a dataset \\(\\{z_1, \\dots, z_n\\}\\) and loss function \\(\\ell(x; z_i)\\):</p> \\[ f(x) = \\frac{1}{n} \\sum_{i=1}^n \\ell(x; z_i) \\] <ul> <li>Exact gradient: \\(\\nabla f(x) = \\frac{1}{n} \\sum_i \\nabla \\ell(x; z_i)\\) </li> <li>SGD gradient: \\(g_t = \\nabla \\ell(x; z_{i_t})\\), where \\(i_t\\) is randomly sampled</li> </ul> <p>\u2705 Reduces per-step computation from \\(O(n)\\) to \\(O(1)\\) or \\(O(\\text{batch size})\\).</p>"},{"location":"0m%20sgd/#22-sgd-update-rule","title":"2.2 SGD Update Rule","text":"<ol> <li>Sample a stochastic gradient \\(g_t\\) (single sample or mini-batch).  </li> <li>Update:</li> </ol> \\[ x_{t+1} = x_t - \\eta g_t \\] <ul> <li>Converges in expectation under standard assumptions (convexity, bounded variance).  </li> <li>Simple, yet highly effective for large-scale optimization.</li> </ul>"},{"location":"0m%20sgd/#23-comparison-gd-vs-sgd","title":"2.3 Comparison: GD vs SGD","text":"Feature Gradient Descent Stochastic Gradient Descent Gradient Full / exact Noisy / approximate Step cost High (entire dataset) Low (single sample / mini-batch) Update direction Accurate Approximate, stochastic Trajectory Smooth Noisy, zig-zag Convergence Deterministic In expectation, slower per iteration, but scalable <p>Visual intuition: SGD zig-zags along the gradient surface but gradually converges toward a minimum.</p>"},{"location":"0m%20sgd/#3-practical-considerations","title":"3. Practical Considerations","text":""},{"location":"0m%20sgd/#31-step-size-learning-rate","title":"3.1 Step Size / Learning Rate","text":"<ul> <li>Constant or decaying \\(\\eta_t\\) </li> <li>Too large \u2192 divergence or oscillation  </li> <li>Too small \u2192 slow convergence</li> </ul>"},{"location":"0m%20sgd/#32-mini-batching","title":"3.2 Mini-batching","text":"<ul> <li>Trades off variance vs computation:  </li> <li>Smaller batches \u2192 noisier but cheaper updates  </li> <li>Larger batches \u2192 smoother updates but more computation</li> </ul>"},{"location":"0m%20sgd/#33-momentum-and-variants","title":"3.3 Momentum and Variants","text":"<ul> <li>Momentum, RMSProp, Adam: reduce stochastic noise, improve convergence.</li> </ul>"},{"location":"0m%20sgd/#34-noise-as-a-feature","title":"3.4 Noise as a Feature","text":"<ul> <li>Noise can help escape shallow local minima.  </li> <li>SGD is more robust in non-convex landscapes, e.g., deep neural networks.</li> </ul>"},{"location":"0m%20sgd/#4-convergence-guarantees","title":"4. Convergence Guarantees","text":"<p>For convex \\(f\\) with bounded gradient variance:</p> \\[ \\mathbb{E}[f(\\bar{x}_T)] - f(x^*) \\le O\\left(\\frac{1}{\\sqrt{T}}\\right) \\] <ul> <li>\\(\\bar{x}_T = \\frac{1}{T} \\sum_{t=1}^T x_t\\) </li> <li>\\(T\\) = number of iterations  </li> </ul> <p>Slower than exact GD (\\(O(1/T)\\) for smooth convex problems), but much cheaper per iteration.</p>"},{"location":"0m%20sgd/#5-example-stochastic-optimization-in-regression","title":"5. Example: Stochastic Optimization in Regression","text":"<p>Consider minimizing expected squared error:</p> \\[ \\min_x \\mathbb{E}_\\xi \\big[ (y - x^\\top \\xi)^2 \\big] \\] <p>Given a dataset \\(\\{(y_1, \\xi_1), \\dots, (y_n, \\xi_n)\\}\\), the empirical risk is:</p> \\[ \\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n (y_i - x^\\top \\xi_i)^2 \\]"},{"location":"0m%20sgd/#51-gradient-descent-gd","title":"5.1 Gradient Descent (GD)","text":"\\[ x_{t+1} = x_t - \\eta \\nabla \\hat{F}(x_t)  = x_t - \\eta \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(x_t) \\] <ul> <li>Full gradient requires a pass over all \\(n\\) samples \u2192 expensive for large datasets.</li> </ul>"},{"location":"0m%20sgd/#52-stochastic-gradient-descent-sgd","title":"5.2 Stochastic Gradient Descent (SGD)","text":"<p>Randomly sample an index \\(i_t \\in \\{1, \\dots, n\\}\\) and compute:</p> \\[ g_t = \\nabla f_{i_t}(x_t) \\] <p>Update rule:</p> \\[ x_{t+1} = x_t - \\eta_t g_t \\] <ul> <li>Key property:</li> </ul> \\[ \\mathbb{E}[g_t] = \\nabla \\hat{F}(x_t) \\] <p>Thus, \\(g_t\\) is an unbiased estimate of the true gradient.</p>"},{"location":"0m%20sgd/#6-randomized-coordinate-descent-rcd","title":"6. Randomized Coordinate Descent (RCD)","text":"<p>Another approach to reduce computational cost is Randomized Coordinate Descent (RCD). Instead of computing the full gradient, RCD updates only a randomly selected coordinate (or block of coordinates) at each iteration:</p> \\[ x_{t+1}^{(i)} = x_t^{(i)} - \\eta \\frac{\\partial f(x_t)}{\\partial x^{(i)}}, \\quad i \\sim \\text{Uniform}(\\{1, \\dots, d\\}) \\] <ul> <li>Key idea: Update only a subset of coordinates to reduce per-iteration cost from \\(O(d)\\) to \\(O(1)\\) (or \\(O(\\text{block size})\\)).  </li> <li>Connection to SGD: Both are stochastic approximations of full gradient descent:</li> <li>SGD introduces randomness via data sampling.  </li> <li>RCD introduces randomness via coordinate selection.  </li> </ul>"},{"location":"0m%20sgd/#61-benefits-of-rcd","title":"6.1 Benefits of RCD","text":"<ul> <li>Efficient for high-dimensional problems.  </li> <li>Exploits sparsity in variables.  </li> <li>Supports parallel and distributed updates through block-coordinate schemes.</li> </ul>"},{"location":"0m%20sgd/#62-variants","title":"6.2 Variants","text":"<ul> <li>Cyclic coordinate descent: systematically updates each coordinate in order.  </li> <li>Importance sampling: selects coordinates with probability proportional to the magnitude of partial derivatives to accelerate convergence.</li> </ul>"},{"location":"0m%20sgd/#63-comparison-to-gd-and-sgd","title":"6.3 Comparison to GD and SGD","text":"Method Gradient Computation Iteration Cost Convergence Behavior GD Full gradient High (\\(O(d)\\)) Deterministic, fast for small-scale problems SGD Stochastic gradient Low (\\(O(\\text{mini-batch})\\)) Noisy updates, scalable for large datasets RCD Partial gradient (coordinate) Very low (\\(O(1)\\) or block) Stochastic, efficient for high-dimensional sparse problems <p>RCD and SGD illustrate a unified principle: use a cheaper, stochastic approximation of the true gradient to achieve scalable optimization.</p>"},{"location":"0m%20sgd/#7-mini-batching-and-variance-reduction","title":"7. Mini-batching and Variance Reduction","text":""},{"location":"0m%20sgd/#71-mini-batch-sgd","title":"7.1 Mini-batch SGD","text":"<p>Compute the gradient over a small subset (mini-batch) of size \\(b\\):</p> \\[ g_t = \\frac{1}{b} \\sum_{i \\in \\mathcal{B}_t} \\nabla f_i(x_t) \\] <ul> <li>Benefits: </li> <li>Reduces variance of the gradient estimate.  </li> <li>Parallelizable on GPUs/TPUs.  </li> <li> <p>Smoother update trajectory than single-sample SGD.</p> </li> <li> <p>Trade-offs: </p> </li> <li>Smaller batch \u2192 cheaper but noisier updates.  </li> <li>Larger batch \u2192 more computation, lower variance.</li> </ul>"},{"location":"0m%20sgd/#72-variance-reduced-gradient-methods-svrg","title":"7.2 Variance-Reduced Gradient Methods (SVRG)","text":"<p>SVRG (Stochastic Variance Reduced Gradient) improves convergence by correcting stochastic gradients:</p> \\[ g_t = \\nabla f_{i_t}(x_t) - \\nabla f_{i_t}(\\tilde{x}) + \\nabla f(\\tilde{x}) \\] <p>where:</p> <ul> <li>\\(i_t\\) is a random index,  </li> <li> <p>\\(\\tilde{x}\\) is a reference point with precomputed full gradient \\(\\nabla f(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\tilde{x})\\).</p> </li> <li> <p>Benefits: </p> </li> <li>Faster convergence than standard SGD for strongly convex problems.  </li> <li>Maintains low per-iteration cost similar to SGD.  </li> <li> <p>Reduces stochastic oscillations by anchoring gradients around a reference.</p> </li> <li> <p>Intuition: Standard SGD \u201cwanders\u201d due to noise; SVRG periodically corrects it using a reference full gradient.</p> </li> </ul>"},{"location":"0n%20newtons/","title":"Newton\u2019s Method","text":""},{"location":"0n%20newtons/#newtons-method","title":"Newton\u2019s Method","text":"<p>Optimization algorithms can be broadly classified based on the type of information they use. Gradient Descent (GD) uses only first-order information (the gradient), while Newton\u2019s Method incorporates second-order information via the Hessian matrix, allowing it to adaptively rescale updates based on local curvature.</p>"},{"location":"0n%20newtons/#1-taylor-expansions-and-local-models","title":"1. Taylor Expansions and Local Models","text":""},{"location":"0n%20newtons/#11-gradient-descent-linear-approximation","title":"1.1 Gradient Descent \u2014 Linear Approximation","text":"<p>Gradient Descent is based on a first-order Taylor approximation of \\(f\\) around \\(x\\):</p> \\[ f(x + d) \\approx f(x) + \\nabla f(x)^\\top d \\] <p>To prevent uncontrolled steps, we regularize with a quadratic trust term:</p> \\[ \\min_d \\; \\nabla f(x)^\\top d + \\frac{1}{2\\eta}\\|d\\|^2 \\] <p>This yields the solution:</p> \\[ d = -\\eta \\nabla f(x), \\quad x_{t+1} = x_t - \\eta \\nabla f(x_t) \\] <p>Thus, GD follows the steepest descent direction with respect to the Euclidean metric.</p>"},{"location":"0n%20newtons/#12-newtons-method-quadratic-approximation","title":"1.2 Newton\u2019s Method \u2014 Quadratic Approximation","text":"<p>Newton's Method instead constructs a second-order Taylor approximation:</p> \\[ f(x + d) \\approx f(x) + \\nabla f(x)^\\top d + \\frac{1}{2} d^\\top \\nabla^2 f(x) d \\] <p>Minimizing this quadratic model:</p> \\[ \\min_d \\; \\nabla f(x)^\\top d + \\frac{1}{2} d^\\top H(x) d \\quad \\text{with} \\quad H(x) = \\nabla^2 f(x) \\] <p>Setting derivative to zero gives:</p> \\[ H(x)d = -\\nabla f(x) \\quad \\Rightarrow \\quad d = -H(x)^{-1} \\nabla f(x) \\] <p>Update rule:</p> \\[ x_{t+1} = x_t - H(x_t)^{-1} \\nabla f(x_t) \\] <p>Newton\u2019s step directly targets the minimizer of the local quadratic model, adjusting the direction based on curvature.</p>"},{"location":"0n%20newtons/#2-smoothness-and-strong-convexity-assumptions","title":"2. Smoothness and Strong Convexity Assumptions","text":"<p>Assume:</p> <ul> <li>\\(f\\) is \\(\\beta\\)-smooth: \\(\\|\\nabla f(x) - \\nabla f(y)\\| \\le \\beta \\|x - y\\|\\) </li> <li>\\(f\\) is \\(\\alpha\\)-strongly convex: \\(f(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) + \\frac{\\alpha}{2}\\|y-x\\|^2\\)</li> </ul> <p>These assumptions enable convergence analysis for both GD and Newton\u2019s method.</p>"},{"location":"0n%20newtons/#3-convergence-rates","title":"3. Convergence Rates","text":""},{"location":"0n%20newtons/#31-gradient-descent-linear-convergence","title":"3.1 Gradient Descent \u2014 Linear Convergence","text":"<p>For smooth and strongly convex \\(f\\):</p> \\[ f(x_t) - f(x^*) \\le \\left(1 - \\frac{\\alpha}{\\beta}\\right)^t (f(x_0) - f(x^*)) \\] <ul> <li>Convergence is linear.</li> <li>Each iteration reduces the error by a constant factor.</li> </ul>"},{"location":"0n%20newtons/#32-newtons-method-quadratic-convergence","title":"3.2 Newton\u2019s Method \u2014 Quadratic Convergence","text":"<p>Assuming \\(f\\) has Lipschitz-continuous Hessian and \\(x_t\\) is sufficiently close to \\(x^*\\):</p> \\[ \\|x_{t+1} - x^*\\| \\le C \\|x_t - x^*\\|^2 \\] <ul> <li>Convergence is quadratic.</li> <li>The number of correct digits doubles after each iteration.</li> </ul>"},{"location":"0n%20newtons/#4-damped-vs-non-damped-newton-method","title":"4. Damped vs. Non-Damped Newton Method","text":""},{"location":"0n%20newtons/#41-classical-non-damped-newton","title":"4.1 Classical (Non-Damped) Newton","text":"<p>Uses the raw step:</p> \\[ x_{t+1} = x_t - H(x_t)^{-1} \\nabla f(x_t) \\] <ul> <li>Extremely fast near optimum.</li> <li>However, if \\(x_t\\) is far from \\(x^*\\), the quadratic model may be inaccurate, causing divergence.</li> </ul>"},{"location":"0n%20newtons/#42-damped-newton-method","title":"4.2 Damped Newton Method","text":"<p>To improve global stability, introduce a damping factor \\(\\lambda_t \\in (0,1]\\):</p> \\[ x_{t+1} = x_t - \\lambda_t H(x_t)^{-1} \\nabla f(x_t) \\] <ul> <li>\\(\\lambda_t\\) is often chosen via line search to ensure sufficient decrease.</li> <li>Damping makes Newton\u2019s method globally convergent, transitioning to full Newton steps when close to optimum.</li> </ul> <p>Insight: - Non-damped Newton is extremely fast but potentially unstable. - Damped Newton trades some speed for robustness during early iterations.</p>"},{"location":"0n%20newtons/#5-affine-transformation-perspective-geometry-matters","title":"5. Affine Transformation Perspective \u2014 Geometry Matters","text":"<p>A powerful perspective is to analyze both methods under an affine change of coordinates:</p> \\[ x = Ay + b, \\quad A \\in \\mathbb{R}^{d \\times d} \\text{ invertible} \\]"},{"location":"0n%20newtons/#effect-on-gradient-descent","title":"Effect on Gradient Descent","text":"<ul> <li>Gradient transforms as \\(\\nabla_y f = A^\\top \\nabla_x f\\).</li> <li>Update becomes dependent on the coordinate system.</li> <li>GD is not affine invariant \u2014 performance heavily depends on scaling and conditioning.</li> </ul>"},{"location":"0n%20newtons/#effect-on-newtons-method","title":"Effect on Newton\u2019s Method","text":"<ul> <li>Hessian transforms as \\(H_y = A^\\top H_x A\\).</li> <li>Update:</li> </ul> \\[ y_{t+1} = y_t - (A^\\top H_x A)^{-1} (A^\\top \\nabla_x f) \\] <p>which simplifies to the same geometric step as in original coordinates.</p> <p>Newton\u2019s Method is affine invariant \u2014 it adapts to the local curvature, effectively removing ill-conditioning.</p> <p>Interpretation:</p> <ul> <li>Gradient Descent uses isotropic steps \u2014 same metric in every direction.</li> <li>Newton Method uses the Hessian-induced metric, effectively rescaling space so level sets become spherical.</li> </ul>"},{"location":"0n%20newtons/#6-computational-trade-off-summary","title":"6. Computational Trade-Off Summary","text":"Method Local Model Metric Used Per-Step Cost Convergence Affine Invariance GD Linear Euclidean (\\(I\\)) \\(O(d)\\) Linear No Newton Quadratic Hessian-inverse (\\(H^{-1}\\)) \\(O(d^3)\\) Quadratic Yes Damped Newton Quadratic + Line Search Hessian-inverse \\(O(d^3)\\) + line search Quadratic near optimum, stable globally Yes"},{"location":"0n%20newtons/#7-strategic-use","title":"7. Strategic Use","text":"<ul> <li>Use GD/SGD for large-scale problems or as a warm start.</li> <li>Switch to (damped) Newton or quasi-Newton methods when close to optimal region.</li> <li>The Hessian captures intrinsic geometry, removing conditioning issues that slow down GD.</li> </ul>"},{"location":"0o%20quasi_n/","title":"Quasi-Newton Methods","text":""},{"location":"0o%20quasi_n/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Optimization often struggles not because of bad algorithms but because the geometry of the loss landscape is distorted \u2014 stretched, skewed, ill-conditioned.</p> <p>Newton\u2019s method fixes this by using curvature, but computing full Hessians is expensive.</p> <p>Quasi-Newton methods are a clever hack: They learn curvature from past gradients without ever computing second derivatives explicitly.</p>"},{"location":"0o%20quasi_n/#newtons-update","title":"Newton's Update","text":"<p>The classical Newton update is:</p> \\[ x_{t+1} = x_t - H^{-1}(x_t) \\, \\nabla f(x_t) \\] <ul> <li>\\(H(x_t)\\) is the Hessian (curvature matrix).</li> <li>Fast convergence but requires computing and inverting \\(H\\).</li> <li>For dimension \\(d\\), storing \\(H\\) costs \\(O(d^2)\\) and inverting costs \\(O(d^3)\\) \u2192 impractical at scale.</li> </ul>"},{"location":"0o%20quasi_n/#key-idea-of-quasi-newton-methods","title":"Key Idea of Quasi-Newton Methods","text":"<p>Instead of computing \\(H^{-1}\\), build an approximation, call it \\(B_t \\approx H^{-1}(x_t)\\).</p> <p>We extract curvature information from how gradients change after each step.</p> <p>Define:</p> <ul> <li>Step vector: </li> <li>Gradient change: </li> </ul> <p>Then we impose the secant condition:</p> \\[ B_{t+1} \\, y_t = s_t \\] <p>\ud83d\udcac Interpretation: \"If moving by \\(s_t\\) caused gradient to change by \\(y_t\\), then our internal curvature model should map \\(y_t \\mapsto s_t\\).\"</p> <p>This imitates Newton's relation without computing actual Hessians.</p>"},{"location":"0o%20quasi_n/#bfgs-the-core-quasi-newton-algorithm","title":"BFGS \u2014 The Core Quasi-Newton Algorithm","text":"<p>BFGS maintains and updates an approximation of the inverse Hessian.</p> <p>The update rule is:</p> \\[ B_{t+1} = \\left(I - \\frac{s_t y_t^\\top}{y_t^\\top s_t}\\right) B_t  \\left(I - \\frac{y_t s_t^\\top}{y_t^\\top s_t}\\right)  + \\frac{s_t s_t^\\top}{y_t^\\top s_t} \\] <p>Properties: - Rank-2 update \u2192 efficient - Preserves symmetry and positive definiteness - No Hessian needed \u2014 only gradients</p> <p>Update the parameters using:</p> \\[ x_{t+1} = x_t - B_{t+1} \\nabla f(x_t) \\]"},{"location":"0o%20quasi_n/#intuitive-geometry","title":"Intuitive Geometry","text":"<ul> <li>Gradient Descent assumes the world is a sphere \u2192 same learning rate in all directions.</li> <li>Newton knows the true ellipse shape of level sets and reshapes space so it becomes spherical.</li> <li>BFGS starts blind like GD but learns to reshape space gradually based on past gradient motion.</li> </ul> <p>Think of BFGS as an optimizer that reconstructs a mental map of terrain curvature from memory.</p>"},{"location":"0o%20quasi_n/#why-bfgs-works-memory-of-curvature","title":"Why BFGS Works \u2014 Memory of Curvature","text":"<p>Each \\((s_t, y_t)\\) pair captures 1D curvature information in the direction of motion.</p> <p>Storing many of these directions lets \\(B_t\\) approximate true curvature more and more accurately. Eventually, the loss surface feels spherical, and optimization becomes fast and direct.</p>"},{"location":"0o%20quasi_n/#l-bfgs-making-bfgs-scalable","title":"L-BFGS \u2014 Making BFGS Scalable","text":"<p>Storing full \\(B_t\\) takes \\(O(d^2)\\) memory \u2192 too large when \\(d\\) is in millions.</p> <p>Limited-memory BFGS (L-BFGS): - Keep only the last \\(m\\) pairs \\((s_t, y_t)\\) (with \\(m \\ll d\\)) - Reconstruct the vector product \\(B_t \\cdot \\nabla f(x_t)\\) using a two-loop recursion - Memory: \\(O(md)\\) instead of \\(O(d^2)\\)</p> <p>This makes L-BFGS practical for high-dimensional problems \u2014 used in: - Logistic regression - NLP models - Deep learning fine-tuning - SciPy &amp; PyTorch optimizers</p>"},{"location":"0o%20quasi_n/#comparison-table","title":"Comparison Table","text":"Method Memory Uses Hessian? Update Direction Convergence Speed Affine-Aware? Gradient Descent \\(O(d)\\) \u274c \\(-\\nabla f\\) Linear \u274c Newton \\(O(d^2)\\) \u2705 Full \\(-H^{-1} \\nabla f\\) Quadratic \u2705 BFGS \\(O(d^2)\\) \u2705 Approx. \\(-B_t \\nabla f\\) Superlinear \u2705 (approx) L-BFGS \\(O(md)\\) \u2705 Approx. (limited) Fast via recursion Superlinear \u2705 (approx)"},{"location":"0o%20quasi_n/#final-mental-model","title":"Final Mental Model","text":"<p>Gradient Descent: Walks downhill with fixed stride \u2014 doesn't care about terrain shape. Newton: Has a full curvature map \u2014 picks the fastest path, but expensive. BFGS/L-BFGS: Starts blind like GD but learns terrain structure from past steps, gradually adapting stride and direction like Newton \u2014 without ever seeing the actual Hessian.</p>"},{"location":"0q%20interior/","title":"Motivation","text":""},{"location":"0q%20interior/#motivation","title":"Motivation","text":"<p>We begin with the linear programming (LP) problem:</p> \\[ \\min_x \\; c^\\top x \\quad \\text{subject to} \\quad A x \\le b \\] <p>Equivalent standard form:</p> \\[ \\min_x \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\quad x \\ge 0 \\]"},{"location":"0q%20interior/#why-is-lp-so-powerful","title":"Why is LP so powerful?","text":"<ul> <li>LP appears in resource allocation, scheduling, routing, and many combinatorial optimization problems.</li> <li>If an optimal solution exists, it is always located at a vertex (extreme point) of the polytope defined by \\(A x \\le b\\).</li> <li>The feasible region \\(\\mathcal{P} = \\{ x \\in \\mathbb{R}^n \\mid A x \\le b \\}\\) is a polyhedron, which typically has many corners.</li> </ul>"},{"location":"0q%20interior/#extreme-points-and-combinatorial-explosion","title":"Extreme Points and Combinatorial Explosion","text":"<p>Let:</p> \\[ \\mathcal{P} = \\{x \\mid A x \\le b\\} \\] <ul> <li>With \\(m\\) inequality constraints in \\(\\mathbb{R}^n\\), the number of extreme points can be exponential in \\(m\\).</li> <li>Simple example: hypercube \\([0,1]^n\\) \u2014 it has \\(2^n\\) vertices.</li> <li>Thus, corner-based search (like Simplex) may require visiting many corners in the worst case.</li> </ul>"},{"location":"0q%20interior/#classical-methods-simplex-and-projected-gradient","title":"Classical Methods: Simplex and Projected Gradient","text":""},{"location":"0q%20interior/#1-projected-gradient-descent-for-lp","title":"1. Projected Gradient Descent for LP","text":"<p>Minimize a quadratic surrogate:</p> \\[ \\min_x \\|x - x_k\\|^2 \\quad \\text{subject to} \\quad A x \\le b \\] <p>Main challenge: projection onto polytope is expensive.</p>"},{"location":"0q%20interior/#2-simplex-method","title":"2. Simplex Method","text":"<ul> <li>Moves from corner to corner.</li> <li>Efficient in practice, but not polynomial-time guaranteed.</li> <li>Geometrically: slides along edges of polytope \u2014 stays on the boundary.</li> </ul>"},{"location":"0q%20interior/#motivation-for-interior-point-methods","title":"Motivation for Interior Point Methods","text":"<ul> <li>Simplex walks on the boundary, potentially traversing exponentially many vertices.</li> <li>Gradient projection struggles with feasibility projection.</li> <li>Interior Point Methods take a different path:</li> <li>They stay strictly inside the polytope.</li> <li>They follow a central trajectory rather than bouncing between corners.</li> <li>They use Newton\u2019s method on a barrier-regularized objective, giving both feasibility and fast convergence.</li> </ul> <p>n</p>"},{"location":"0q%20interior/#interior-point-methods-barrier-and-newton-fusion","title":"Interior Point Methods \u2014 Barrier and Newton Fusion","text":"<p>We now revisit the constrained problem:</p> \\[ \\min_x \\; f(x) \\quad \\text{subject to } A x \\le b \\] <p>Introduce the log-barrier for each constraint:</p> \\[ B(x) = -\\sum_{i=1}^m \\log(b_i - a_i^\\top x) \\] <p>Construct the barrier objective:</p> \\[ \\Phi_t(x) = t f(x) + B(x), \\quad t &gt; 0 \\] <p>Interpretation: As we increase \\(t\\), we push harder toward the true optimum, while the barrier keeps us in the interior.</p>"},{"location":"0q%20interior/#newton-method-applied-to-barrier-objective","title":"Newton Method Applied to Barrier Objective","text":"<p>Compute:</p> \\[ \\nabla \\Phi_t(x) = t \\nabla f(x) + \\sum_{i=1}^m \\frac{1}{b_i - a_i^\\top x} a_i \\] \\[ \\nabla^2 \\Phi_t(x) = t \\nabla^2 f(x) + \\sum_{i=1}^m \\frac{1}{(b_i - a_i^\\top x)^2} a_i a_i^\\top \\] <p>Newton Direction:</p> \\[ d = - \\left[\\nabla^2 \\Phi_t(x)\\right]^{-1} \\nabla \\Phi_t(x) \\] <p>Update with damping to keep strict feasibility:</p> \\[ x_{k+1} = x_k + \\lambda_k d \\quad \\text{such that } A x_{k+1} &lt; b \\]"},{"location":"0q%20interior/#interior-point-algorithm-flow","title":"Interior Point Algorithm Flow","text":"<p>``` Given strictly feasible x\u2080 and initial t: repeat:     Solve the barrier problem: minimize \u03a6\u209c(x) using damped Newton     Increase parameter t \u2190 \u03bc t  (\u03bc &gt; 1) until m / t &lt; \u03b5   # duality gap condition</p>"},{"location":"11_intro/","title":"1. Introduction and Overview","text":""},{"location":"11_intro/#chapter-1-introduction-and-overview","title":"Chapter 1: Introduction and Overview","text":"<p>Convex optimization is the subfield of mathematical optimization that deals with problems where both the objective function and the constraints are convex. A generic convex optimization problem can be written in standard form as: </p> \\[ \\begin{array}{ll} \\text{minimize}_{x} &amp; f(x) \\\\ \\text{subject to}   &amp; g_i(x) \\le 0, \\quad i=1,\\dots,m, \\\\ &amp; h_j(x) = 0, \\quad j=1,\\dots,p~, \\end{array} \\] <p>where \\(f(x)\\) is a convex objective function, each \\(g_i(x)\\) is a convex inequality constraint, and each \\(h_j(x)\\) is an affine equality constraint (affine functions are both convex and concave). Such problems are convex in that any local minimum is guaranteed to be a global minimum (Boyd and Vandenberghe, 2004). This is the crucial property that makes convex optimization tractable: there are efficient algorithms to find the global optimum of convex problems, even in high dimensions, with no risk of getting stuck in local optima. In contrast, nonconvex optimization problems can have many local minima and are generally hard to solve in a globally optimal way (Boyd and Vandenberghe, 2004).</p>"},{"location":"11_intro/#why-convexity-matters","title":"Why Convexity Matters","text":"<p>The key concept that underpins the efficiency of convex optimization is the convexity of sets and functions. Geometrically, a set \\(C \\subset \\mathbb{R}^n\\) is convex if for any two points in \\(C\\), the straight line segment joining them lies entirely in \\(C\\). Similarly, a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if its epigraph (the set of points lying on or above its graph) is a convex set; equivalently, \\(f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y)\\) for all \\(0\\le\\theta\\le 1\\) and all \\(x,y\\) in the domain. Convexity implies a single \"bowl-shaped\" landscape with no local dips aside from the global minimum. This structure allows powerful theoretical guarantees. For example, any local minimum of a convex function (subject to a convex feasible region) must be a global minimum. Additionally, convex problems enjoy strong duality properties, which means we can often find the optimal value by solving a related dual problem. These properties will be explored in later sections.</p> <p>Convexity also leads to robustness. Small perturbations to problem data typically result in small changes to the solution in convex optimization, a desirable feature in practice. Moreover, many approximation techniques (like convex relaxations of NP-hard problems) rely on solving a convex problem to get bounds or heuristic solutions for the original task. In summary, convex optimization provides a toolkit that form the foundation for more complex or application-specific techniques.</p>"},{"location":"11_intro/#outline-of-this-website","title":"Outline of this Website","text":"<p>This Web-book is intended for early graduate students and assumes only basic linear algebra and multivariable calculus. We will build from fundamental mathematical concepts up to the core theory of convex optimization. Each chapter introduces prerequisite material with a focus on clarity and pedagogy, preparing the reader for advanced optimization topics. The chapters are organized as follows:</p> <ul> <li> <p>Chapter 2: Linear Algebra Foundations. Reviews the vector space concepts used throughout optimization. We cover vectors, matrices, norms, inner products, positive definiteness, and related topics from linear algebra that are essential in convex analysis (e.g., understanding quadratic forms and orthogonality).</p> </li> <li> <p>Chapter 3: Multivariable Calculus for Optimization. Summarizes key results from calculus in \\(\\mathbb{R}^n\\). We discuss gradients, Hessians, Taylor expansions, and the conditions for optima of unconstrained problems. These tools are necessary for understanding how to characterize and find extrema in optimization.</p> </li> <li>Chapter 4: Convex Sets and Geometric Fundamentals. Introduces convex sets, their properties, and geometric insights. We define convex combinations, affine sets, polyhedra, and other fundamental geometric objects. We also discuss operations that preserve convexity of sets and the importance of convex hulls.</li> <li>Chapter 5: Convex Functions. Focuses on properties of convex functions. We formally define convex (and concave) functions, give many examples, and derive conditions for convexity (including second-derivative tests). We cover important inequalities like Jensen\u2019s inequality and operations that preserve convexity of functions.</li> <li>Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients. Extends the concept of gradients to convex functions that are not differentiable. We introduce subgradients and subdifferentials as fundamental tools to handle nondifferentiable convex functions. This chapter explains how optimality conditions can still be expressed via subgradients and lays the groundwork for algorithms like the subgradient method.</li> <li>Chapter 7: Optimization Principles \u2013 From Gradient Descent to KKT. Bridges unconstrained and constrained optimization. We begin with the basic gradient descent method for unconstrained convex problems and then develop the Karush\u2013Kuhn\u2013Tucker (KKT) conditions for constrained problems. The KKT conditions generalize the method of Lagrange multipliers to handle inequality constraints (they are the first-order optimality conditions in nonlinear programming). We explain the meaning of each KKT condition and how they characterize optimal solutions.</li> <li> <p>Chapter 8: Lagrange Duality Theory. Delves into the powerful theory of duality in convex optimization. We define the Lagrangian, derive the dual function and dual problem, and discuss weak and strong duality. This chapter shows how every convex optimization problem has a corresponding dual problem which provides lower bounds on the optimum and often leads to insightful optimality conditions (duality ties back to KKT). We also introduce Slater\u2019s condition as a sufficient condition for strong duality in convex problems.</p> </li> <li> <p>Chapter 9: Algorithms for Convex Optimization. Surveys practical solvers: gradient methods, Newton and quasi-Newton methods, proximal algorithms, coordinate descent, and interior-point methods \u2014 emphasizing implementation and convergence in ML settings.</p> </li> <li> <p>Chapter 10: Advanced Large-Scale and Structured Methods. Focuses on methods for large datasets and structured problems, including stochastic gradient methods, ADMM, distributed optimization, and sparsity-exploiting algorithms.</p> </li> <li> <p>Chapter 11: Modelling Patterns and Algorithm Selection in Practice. Discusses how to model real-world ML problems as convex programs, choose appropriate solvers, handle non-convex extensions, and integrate optimization into end-to-end ML workflows.</p> </li> <li> <p>Appendix A: Common Inequalities and Identities. A handy reference list of fundamental mathematical inequalities and identities frequently used in proofs and exercises in convex optimization. For example, Cauchy\u2013Schwarz, Jensen\u2019s inequality, AM-GM inequality, and other algebraic facts.</p> </li> <li> <p>Appendix B: Support Functions and Dual Geometry (Advanced). An advanced topic for further study, introducing support functions of convex sets and related concepts of dual geometry (polar and dual cones). These tools provide deeper geometric insight into convex analysis and optimization duality but are not required for the core chapters. They are included for completeness and for readers interested in convex geometry.</p> </li> </ul> <p>References </p> <ul> <li>Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.  </li> <li>Nesterov, Y. (2018). Lectures on Convex Optimization. Springer.  </li> </ul>"},{"location":"11_intro/#missing-elements","title":"MIssing elements","text":"<ol> <li>Subgradient descent</li> </ol>"},{"location":"120_ineqaulities/","title":"Appendix A - Common Inequalities and Identities","text":""},{"location":"120_ineqaulities/#appendix-a-common-inequalities-and-identities","title":"Appendix A: Common Inequalities and Identities","text":"<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the \u201calgebraic tools\u201d you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemar\u00e9chal, 2001).</p>"},{"location":"120_ineqaulities/#a1-cauchyschwarz-inequality","title":"A.1 Cauchy\u2013Schwarz inequality","text":"<p>For any \\(x,y \\in \\mathbb{R}^n\\),  </p> <p>Equality holds if and only if \\(x\\) and \\(y\\) are linearly dependent.</p> <p>Consequences:</p> <ul> <li>Defines the notion of angle between vectors.</li> <li>Justifies dual norms.</li> </ul>"},{"location":"120_ineqaulities/#a2-jensens-inequality","title":"A.2 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable. Then  </p> <p>In finite form: for \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>Jensen\u2019s inequality is equivalent to convexity: it says \u201cthe function at the average is no more than the average of the function values.\u201d It is used constantly to prove convexity of expectations and log-sum-exp.</p>"},{"location":"120_ineqaulities/#a3-amgm-inequality","title":"A.3 AM\u2013GM inequality","text":"<p>For \\(x_1,\\dots,x_n \\ge 0\\),  </p> <p>This can be proved using Jensen\u2019s inequality with \\(f(t) = \\log t\\), which is concave. AM\u2013GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>"},{"location":"120_ineqaulities/#a4-holders-inequality-generalised-cauchyschwarz","title":"A.4 H\u00f6lder\u2019s inequality (generalised Cauchy\u2013Schwarz)","text":"<p>For \\(p,q \\ge 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) (conjugate exponents),  </p> <ul> <li>When \\(p=q=2\\), H\u00f6lder becomes Cauchy\u2013Schwarz.</li> <li>H\u00f6lder underlies dual norms: the dual of \\(\\ell_p\\) is \\(\\ell_q\\).</li> </ul>"},{"location":"120_ineqaulities/#a5-youngs-inequality","title":"A.5 Young\u2019s inequality","text":"<p>For \\(a,b \\ge 0\\) and \\(p,q &gt; 1\\) with \\(\\frac{1}{p} + \\frac{1}{q} = 1\\),  </p> <p>This is useful in bounding cross terms in convergence proofs.</p>"},{"location":"120_ineqaulities/#a6-fenchels-inequality","title":"A.6 Fenchel\u2019s inequality","text":"<p>Let \\(f\\) be a convex function and let \\(f^*\\) be its convex conjugate:  </p> <p>Then for all \\(x,y\\),  </p> <p>Fenchel\u2019s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel\u2019s inequality.</p>"},{"location":"120_ineqaulities/#a7-supporting-hyperplane-inequality","title":"A.7 Supporting hyperplane inequality","text":"<p>If \\(f\\) is convex, then for any \\(x\\) and any \\(g \\in \\partial f(x)\\),  </p> <p>This can be viewed as \u201c\\(f\\) lies above all its tangent hyperplanes,\u201d even when it\u2019s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>"},{"location":"120_ineqaulities/#a8-summary","title":"A.8 Summary","text":"<ul> <li>Cauchy\u2013Schwarz and H\u00f6lder bound inner products.</li> <li>Jensen shows convexity and expectation interact cleanly.</li> <li>Fenchel\u2019s inequality is the algebra of duality.</li> <li>Supporting hyperplane inequality is the geometry of convexity.</li> </ul> <p>These inequalities are used implicitly all over convex optimisation.</p>"},{"location":"12_vector/","title":"2. Linear Algebra Foundations","text":""},{"location":"12_vector/#chapter-2-linear-algebra-foundations","title":"Chapter 2: Linear Algebra Foundations","text":"<p>Convex optimisation is geometric. To talk about convex sets, supporting hyperplanes, projections, and quadratic forms, we need linear algebra. This chapter reviews the specific linear algebra tools we will use throughout: vector spaces, inner products, norms, projections, eigenvalues, and positive semidefinite matrices.</p>"},{"location":"12_vector/#21-vector-spaces-subspaces-and-affine-sets","title":"2.1 Vector spaces, subspaces, and affine sets","text":"<p>A vector space over \\(\\mathbb{R}\\) is a set \\(V\\) equipped with addition and scalar multiplication satisfying the usual axioms: closure, associativity, distributivity, etc. In this book we mostly work with \\(V = \\mathbb{R}^n\\).</p> <p>A subspace \\(S \\subseteq \\mathbb{R}^n\\) is a subset that:</p> <ol> <li>contains \\(0\\),</li> <li>is closed under addition,</li> <li>is closed under scalar multiplication.</li> </ol> <p>For example, the set of all solutions to \\(Ax = 0\\) is a subspace, called the nullspace or kernel of \\(A\\).</p> <p>An affine set is a translated subspace. A set \\(A\\) is affine if for any \\(x,y \\in A\\) and any \\(\\theta \\in \\mathbb{R}\\),  Every affine set can be written as  where \\(S\\) is a subspace. Affine sets appear as the solution sets to linear equality constraints \\(Ax = b\\).</p> <p>Affine sets are important in optimisation because:</p> <ul> <li>Feasible sets defined by equality constraints are affine.</li> <li>Affine functions preserve convexity.</li> </ul>"},{"location":"12_vector/#22-linear-combinations-span-basis-dimension","title":"2.2 Linear combinations, span, basis, dimension","text":"<p>Given vectors \\(v_1,\\dots,v_k\\), any vector of the form  is a linear combination. The set of all linear combinations is called the span:  </p> <p>A list of vectors is linearly independent if no nontrivial linear combination gives \\(0\\). A basis of a subspace \\(S\\) is a set of linearly independent vectors whose span is \\(S\\). The number of vectors in a basis is the dimension of \\(S\\).</p> <p>Rank and nullity facts:</p> <ul> <li>The column space of \\(A\\) is the span of its columns. Its dimension is \\(\\mathrm{rank}(A)\\).</li> <li>The nullspace of \\(A\\) is \\(\\{ x : Ax = 0 \\}\\).</li> <li>The rank-nullity theorem** states:  where \\(n\\) is the number of columns of \\(A\\).</li> </ul> <p>In constrained optimisation, \\(\\mathrm{rank}(A)\\) encodes the \u201cnumber of independent constraints\u201d, and the nullspace encodes feasible directions that do not violate certain constraints.</p> <p>Column Space is a set of all possible \"outputs\" you can create by computing \\(Ax\\). Its answers the question does a solution \\(x\\) even exist for \\(Ax = b\\). Solution exists if only if vector \\(b\\) lives inside the column space \\(C(A)\\). </p> <p>Null Space is all sets of \"inputs\" \\(x\\) that get \"squashed to zero\" by A i.e. All \\(x\\) such that \\(Ax = 0\\). It answers the question if a solution exisits, is it the only one.If the nullspace contains non-zero vectors (\\(\\mathrm{nullity}(A) &gt; 0\\)), there are infinitely many solutions.</p> <p>Multicollinearity (ML): If one feature in your data matrix \\(X\\) is a combination of others (e.g., \\(feature_3 = 2 \\times feature_1 + feature_2\\)), this creates a non-zero vector in the nullspace. This means there are infinitely many different weight vectors \\(w\\) that produce the exact same predictions. The model is \"unidentifiable.\" This is why \\(X^T X\\) becomes non-invertible, and it\u2019s the primary motivation for using regularization (like Ridge or Lasso) to pick one \"good\" solution from this infinite set.</p> <p>Feasible Directions (Optimization): As you noted, in a constrained problem like \\(Ax = b\\), the nullspace is the set of all directions \\(d\\) you can move from a feasible point \\(x\\) without violating the constraints. If you are at \\(x\\) and move to \\(x+d\\) (where \\(d \\in N(A)\\)), your constraints are still met: \\(A(x+d) = Ax + Ad = b + 0 = b\\). This tells you your \"space of free movement\"</p>"},{"location":"12_vector/#23-inner-products-and-orthogonality","title":"2.3 Inner products and orthogonality","text":"<p>An inner product on \\(\\mathbb{R}^n\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\) such that for all \\(x,y,z\\) and all scalars \\(\\alpha\\):</p> <ol> <li>\\(\\langle x,y \\rangle = \\langle y,x\\rangle\\) (symmetry),</li> <li>\\(\\langle x+y,z \\rangle = \\langle x,z \\rangle + \\langle y,z\\rangle\\) (linearity in first argument),</li> <li>\\(\\langle \\alpha x, y\\rangle = \\alpha \\langle x, y\\rangle\\),</li> <li>\\(\\langle x, x\\rangle \\ge 0\\) with equality iff \\(x=0\\) (positive definiteness).</li> </ol> <p>In \\(\\mathbb{R}^n\\), the standard inner product is the dot product:  </p> <p>The inner product induces:</p> <ul> <li>length (norm): \\(\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}\\),</li> <li>angle:   </li> </ul> <p>Two vectors are orthogonal if \\(\\langle x,y\\rangle = 0\\). A set of vectors \\(\\{v_i\\}\\) is orthonormal if each \\(\\|v_i\\| = 1\\) and \\(\\langle v_i, v_j\\rangle = 0\\) for \\(i\\ne j\\).</p> <p>More generally, an inner product endows \\(V\\) with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p> <p>Geometry from the inner product: An inner product induces a norm \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\) and a notion of distance \\(d(x,y) = \\|x-y\\|\\). It also defines angles: \\(\\langle x,y \\rangle = 0\\) means \\(x\\) and \\(y\\) are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: \\(\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\\).  </p> <p>The Cauchy\u2013Schwarz inequality: For any \\(x,y \\in \\mathbb{R}^n\\):  with equality iff \\(x\\) and \\(y\\) are linearly dependent Geometrically, it means the absolute inner product is maximized when \\(x\\) and \\(y\\) point in the same or opposite direction. </p> <p>Examples of inner products:</p> <ul> <li> <p>Standard (Euclidean) inner product: \\(\\langle x,y\\rangle = x^\\top y = \\sum_i x_i y_i\\). This underlies most optimization algorithms on \\(\\mathbb{R}^n\\), where \\(\\nabla f(x)\\) is defined via this inner product (so that \\(\\langle \\nabla f(x), h\\rangle\\) gives the directional derivative in direction \\(h\\)).  </p> </li> <li> <p>Weighted inner product: \\(\\langle x,y\\rangle_W = x^\\top W y\\) for some symmetric positive-definite matrix \\(W\\). Here \\(\\|x\\|_W = \\sqrt{x^\\top W x}\\) is a weighted length. Such inner products appear in preconditioning: by choosing \\(W\\) cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses \\(W = \\Sigma^{-1}\\) for covariance \\(\\Sigma\\)).  </p> </li> <li> <p>Function space inner product: \\(\\langle f, g \\rangle = \\int_a^b f(t)\\,g(t)\\,dt\\). This turns the space of square-integrable functions on \\([a,b]\\) into an inner product space (a Hilbert space, \\(L^2[a,b]\\)). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p> </li> </ul> <p>Any vector space with an inner product has an orthonormal basis (via the Gram\u2013Schmidt process). Gram\u2013Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as \\(A = QR\\) where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve \\(Ax=b\\) and to analyze subspaces. For example, for an overdetermined system (\\(m&gt;n\\) i.e. more equations than unknowns), \\(Ax=b\\) has a least-squares solution \\(x = R^{-1}(Q^\\top b)\\), and for underdetermined (\\(m&lt;n\\)), \\(Ax=b\\) has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p> <p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p> <ul> <li> <p>Gradients: The gradient \\(\\nabla f(x)\\) is defined as the vector satisfying \\(f(x+h)\\approx f(x) + \\langle \\nabla f(x), h\\rangle\\). Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction \\(-\\nabla f(x)\\) because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix \\(W\\)), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p> </li> <li> <p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints \\(Ax=b\\) (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of \\(b\\) onto \\(\\mathrm{range}(A)\\)) and quadratic programs (where each iteration might involve a projection).  </p> </li> <li> <p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p> </li> <li> <p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix \\(G_{ij} = \\langle x_i, x_j\\rangle\\) for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: \\(X^\\top X\\) is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix\u2019s condition number and thus algorithm performance.</p> </li> </ul>"},{"location":"12_vector/#24-norms-and-distances","title":"2.4 Norms and distances","text":"<p>A function \\(\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}\\) is a norm if for all \\(x,y\\) and scalar \\(\\alpha\\):</p> <ol> <li>\\(\\|x\\| \\ge 0\\) and \\(\\|x\\| = 0 \\iff x=0\\),</li> <li>\\(\\|\\alpha x\\| = |\\alpha|\\|x\\|\\) (absolute homogeneity),</li> <li>\\(\\|x+y\\| \\le \\|x\\| + \\|y\\|\\) (triangle inequality).</li> </ol> <p>If the vector space has an inner product, the norm \\(\\|x\\| = \\sqrt{\\langle x,x\\rangle}\\) is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry. Common examples on \\(\\mathbb{R}^n\\):  </p> <ul> <li> <p>\\(\\ell_2\\) norm (Euclidean): \\(\\|x\\|_2 = \\sqrt{\\sum_i x_i^2}\\), the usual length in space.  </p> </li> <li> <p>\\(\\ell_1\\) norm: \\(\\|x\\|_1 = \\sum_i |x_i|\\), measuring taxicab distance. In \\(\\mathbb{R}^2\\), its unit ball is a diamond.  </p> </li> <li> <p>\\(\\ell_\\infty\\) norm: \\(\\|x\\|_\\infty = \\max_i |x_i|\\), measuring the largest coordinate magnitude. Its unit ball in \\(\\mathbb{R}^2\\) is a square.  </p> </li> <li> <p>General \\(\\ell_p\\) norm: \\(\\|x\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\) for \\(p\\ge1\\). This interpolates between \\(\\ell_1\\) and \\(\\ell_2\\), and approaches \\(\\ell_\\infty\\) as \\(p\\to\\infty\\). All \\(\\ell_p\\) norms are convex and satisfy the norm axioms.  </p> </li> </ul> <p>Every norm induces a metric (distance) \\(d(x,y) = |x-y|\\) on the space. Norms thus define the shape of \u201cballs\u201d (sets \\({x: |x|\\le \\text{constant}}\\)) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm\u2019s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p> <p></p> <p>Unit-ball geometry: The shape of the unit ball \\({x: |x| \\le 1}\\) reveals how a norm treats different directions. For example, the \\(\\ell_2\\) unit ball in \\(\\mathbb{R}^2\\) is a perfect circle, treating all directions uniformly, whereas the \\(\\ell_1\\) unit ball is a diamond with corners along the axes, indicating that \\(\\ell_1\\) treats the coordinate axes as special (those are \u201ccheaper\u201d directions since the ball extends further along axes, touching them at \\((\\pm1,0)\\) and \\((0,\\pm1)\\)). The \\(\\ell_\\infty\\) unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (\\(\\ell_1\\)), green circle (\\(\\ell_2\\)), and blue square (\\(\\ell_\\infty\\)) in \\(\\mathbb{R}^2\\) . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an \\(\\ell_1\\) norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an \\(\\ell_2\\) ball encourages more evenly-distributed changes. An \\(\\ell_\\infty\\) constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p> <p>Dual norms: Each norm \\(\\|\\cdot\\|\\) has a dual norm \\(\\|\\cdot\\|_*\\) defined by  For example, the dual of \\(\\ell_1\\) is \\(\\ell_\\infty\\), and the dual of \\(\\ell_2\\) is itself.</p> <p>Imagine the vector \\(x\\) lives inside the original norm ball (\\(\\|x\\| \\le 1\\)). The term \\(x^\\top y\\) is the dot product, which measures the alignment between \\(x\\) and \\(y\\). The dual norm \\(\\|y\\|_*\\) is the maximum possible value you can get by taking the dot product of \\(y\\) with any vector \\(x\\) that fits inside the original norm ball.If the dual norm \\(\\|y\\|_*\\) is large, it means \\(y\\) is strongly aligned with a direction \\(x\\) that is \"small\" (size \\(\\le 1\\)) according to the original norm.If the dual norm is small, \\(y\\) must be poorly aligned with all vectors \\(x\\) in the ball.</p> <p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use \\(\\ell_\\infty\\) (since one coordinate move at a time is like a step in \\(\\ell_\\infty\\) unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using \\(\\ell_1\\) norm for sparse problems). The norm also figures in complexity bounds: an algorithm\u2019s convergence rate may depend on the diameter of the feasible set in the chosen norm, \\(D = \\max_{\\text{feasible}}|x - x^*|\\). For instance, in subgradient methods, having a smaller \\(\\ell_2\\) diameter or \\(\\ell_1\\) diameter can improve bounds. Moreover, when constraints are given by norms (like \\(|x|_1 \\le t\\)), projections and proximal operators with respect to that norm become subroutines in algorithms.</p> <p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (\\(|x_k - x^*|\\)), how to constrain solutions (\\(|x| \\le R\\)), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>"},{"location":"12_vector/#25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices","title":"2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices","text":"<p>If \\(A \\in \\mathbb{R}^{n\\times n}\\) is linear, a nonzero \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if</p> \\[ Av = \\lambda v~. \\] <p>When \\(A\\) is symmetric (\\(A = A^\\top\\)), it has:</p> <ul> <li>real eigenvalues,</li> <li>an orthonormal eigenbasis,</li> <li>a spectral decomposition</li> </ul> <p>  where \\(Q\\) is orthonormal and \\(\\Lambda\\) is diagonal.</p> <p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along \\(n\\) orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by \\(\\lambda_i\\).</p> <p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p> <p>In optimization, the Hessian matrix of a multivariate function \\(f(x)\\) is symmetric. Its eigenvalues \\(\\lambda_i(\\nabla^2 f(x))\\) tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there\u2019s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p> <p>Positive semidefinite matrices: A symmetric matrix \\(Q\\) is positive semidefinite (PSD) if</p> \\[ x^\\top Q x \\ge 0 \\quad \\text{for all } x~. \\] <p>If \\(x^\\top Q x &gt; 0\\) for all \\(x\\ne 0\\), then \\(Q\\) is positive definite (PD).</p> <p>Why this matters: if \\(f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x + d\\), then</p> \\[ \\nabla^2 f(x) = Q~. \\] <p>So \\(f\\) is convex iff \\(Q\\) is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not (Boyd and Vandenberghe, 2004). This is the algebraic test for convexity of quadratic forms.</p> <p>Implications of definiteness: If \\(A \\succ 0\\), the quadratic function \\(x^T A x\\) is strictly convex and has a unique minimizer at \\(x=0\\). If \\(A \\succeq 0\\), \\(x^T A x\\) is convex but could be flat in some directions (if some \\(\\lambda_i = 0\\), those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian \\(\\nabla^2 f(x) \\succ 0\\) means \\(f\\) has a unique local (and global, if domain convex) minimum at that \\(x\\) (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater\u2019s condition for strong duality.</p> <p>Condition number and convergence: For iterative methods on convex quadratics \\(f(x) = \\frac{1}{2}x^T Q x - b^T x\\), the eigenvalues of \\(Q\\) dictate convergence speed. Gradient descent\u2019s error after \\(k\\) steps satisfies roughly \\(|x_k - x^*| \\le (\\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}})^k |x_0 - x^*|\\) (for normalized step). So the ratio \\(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(Q)\\) appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton\u2019s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to \\(\\kappa\\) (locally). This explains why second-order methods shine on ill-conditioned problems: they \u201cwhiten\u201d the curvature by dividing by eigenvalues.</p> <p>Optimization interpretation of eigenvectors: The eigenvectors of \\(\\nabla^2 f(x^*)\\) at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton\u2019s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). For constrained problems, the Hessian of the Lagrangian (the KKT matrix) being PSD relates to second-order optimality conditions.</p>"},{"location":"12_vector/#26-orthogonal-projections-and-least-squares","title":"2.6 Orthogonal projections and least squares","text":"<p>Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal projection of a vector \\(b\\) onto \\(S\\) is the unique vector \\(p \\in S\\) minimising \\(\\|b - p\\|_2\\). Geometrically, \\(p\\) is the closest point in \\(S\\) to \\(b\\).</p> <p>If \\(S = \\mathrm{span}\\{a_1,\\dots,a_k\\}\\) and \\(A = [a_1~\\cdots~a_k]\\), then projecting \\(b\\) onto \\(S\\) is equivalent to solving the least-squares problem</p> \\[ \\min_x \\|Ax - b\\|_2^2~. $$ The solution $x^*$ satisfies the normal equations $$ A^\\top A x^* = A^\\top b~. \\] <p>This is our first real convex optimisation problem:</p> <ul> <li>the objective \\(\\|Ax-b\\|_2^2\\) is convex,</li> <li>there are no constraints,</li> <li>we can solve it in closed form.</li> </ul>"},{"location":"12_vector/#27-advanced-concepts","title":"2.7 Advanced Concepts","text":"<p>Operator norm: Given a matrix (linear map) \\(A: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and given a choice of vector norms on input and output, one can define the induced operator norm. If we use \\(|\\cdot|_p\\) on \\(\\mathbb{R}^n\\) and \\(|\\cdot|_q\\) on \\(\\mathbb{R}^m\\), the operator norm is</p> \\[ \\|A\\|_{p \\to q} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|_q}{\\|x\\|_p} = \\sup_{\\|x\\|_p \\le 1} \\|Ax\\|_q \\] <p>This gives the maximum factor by which \\(A\\) can stretch a vector (measuring \\(x\\) in norm \\(p\\) and \\(Ax\\) in norm \\(q\\)).pecial cases are common: with \\(p = q = 2\\), \\(|A|_{2 \\to 2}\\) (often just written \\(|A|_2\\)) is the spectral norm, which equals the largest singular value of \\(A\\) (more on singular values below). If \\(p = q = 1\\), \\(|A|_{1 \\to 1}\\) is the maximum absolute column sum of \\(A\\). If \\(p = q = \\infty\\), \\(|A|{\\infty \\to \\infty}\\) is the maximum absolute row sum.</p> <p>Operator norms tell us the worst-case amplification of signals by \\(A\\). In gradient descent on \\(f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x\\) (a quadratic form), the step size must be \\(\\le \\tfrac{2}{|A|_2}\\) for convergence; here \\(|A|_2 = \\lambda_{\\max}(A)\\) if \\(A\\) is symmetric (it\u2019s related to Hessian eigenvalues, Chapter 5). In general, controlling \\(|A|\\) controls stability: if \\(|A| &lt; 1\\), the map brings vectors closer (contraction mapping), important in fixed-point algorithms.</p> <p>Singular Value Decomposition (SVD): Any matrix \\(A \\in \\mathbb{R}^{m\\times n}\\) can be factored as</p> \\[ A = U \\Sigma V^\\top \\] <p>where \\(U \\in \\mathbb{R}^{m\\times m}\\) and \\(V \\in \\mathbb{R}^{n\\times n}\\) are orthogonal matrices (their columns are orthonormal bases of \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\), respectively), and \\(\\Sigma\\) is an \\(m\\times n\\) diagonal matrix with nonnegative entries \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0\\) on the diagonal. The \\(\\sigma_i\\) are the singular values of \\(A\\). Geometrically, \\(A\\) sends the unit ball in \\(\\mathbb{R}^n\\) to an ellipsoid in \\(\\mathbb{R}^m\\) whose principal semi-axes lengths are the singular values and directions are the columns of \\(V\\) (mapped to columns of \\(U\\)). The largest singular value \\(\\sigma_{\\max} = |A|_2\\) is the spectral norm. The smallest (if \\(n \\le m\\), \\(\\sigma{\\min}\\) of those \\(n\\)) indicates how \\(A\\) contracts the least \u2013 if \\(\\sigma_{\\min} = 0\\), \\(A\\) is rank-deficient.</p> <p>The SVD is a fundamental tool for analyzing linear maps in optimization: it reveals the condition number \\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\) (when \\(\\sigma_{\\min}&gt;0\\)), which measures how stretched the map is in one direction versus another. High condition number means ill-conditioning: some directions in \\(x\\)-space hardly change \\(Ax\\) (flat curvature), making it hard for algorithms to progress uniformly. Low condition number means \\(A\\) is close to an orthogonal scaling, which is ideal. SVD is also used for dimensionality reduction: truncating small singular values gives the best low-rank approximation of \\(A\\) (Eckart\u2013Young theorem), widely used in PCA and compressive sensing. In convex optimization, many second-order methods or constraint eliminations use eigen or singular values to simplify problems.</p> <p>Low-rank structure: The rank of \\(A\\) equals the number of nonzero singular values. If \\(A\\) has rank \\(r \\ll \\min(n,m)\\), it means \\(A\\) effectively operates in a low-dimensional subspace. This often can be exploited: the data or constraints have some latent low-dimensional structure. Many convex optimization techniques (like nuclear norm minimization) aim to produce low-rank solutions by leveraging singular values. Conversely, if an optimization problem\u2019s data matrix \\(A\\) is low-rank, one can often compress it (via SVD) to speed up computations or reduce variables.</p> <p>Operator norm in optimization: Operator norms also guide step sizes and preconditioning. As noted, for a quadratic problem \\(f(x) = \\frac{1}{2}x^TQx - b^Tx\\), the Hessian is \\(Q\\) and gradient descent converges if \\(\\alpha &lt; 2/\\lambda_{\\max}(Q)\\). Preconditioning aims to transform \\(Q\\) into one with a smaller condition number by multiplying by some \\(P\\) (like using \\(P^{-1}Q\\)) \u2014 effectively changing the norm in which we measure lengths, so the operator norm becomes smaller. In first-order methods for general convex \\(f\\), the Lipschitz constant of \\(\\nabla f\\) (which often equals a spectral norm of a Hessian or Jacobian) determines convergence rates.</p> <p>Summary of spectral properties:</p> <ul> <li> <p>The spectral norm \\(|A|_2 = \\sigma_{\\max}(A)\\) quantifies the largest stretching. It determines stability and step sizes.</p> </li> <li> <p>The smallest singular value \\(\\sigma_{\\min}\\) (if \\(A\\) is tall full-rank) tells if \\(A\\) is invertible and how sensitive the inverse is. If \\(\\sigma_{\\min}\\) is tiny, small changes in output cause huge changes in solving \\(Ax=b\\).</p> </li> <li> <p>The condition number \\(\\kappa = \\sigma_{\\max}/\\sigma_{\\min}\\) is a figure of merit for algorithms: gradient descent iterations needed often scale with \\(\\kappa\\) (worse conditioning = slower). Regularization like adding \\(\\mu I\\) increases \\(\\sigma_{\\min}\\), thereby reducing \\(\\kappa\\) and accelerating convergence (at the expense of bias).</p> </li> <li> <p>Nuclear norm (sum of singular values) and spectral norm often appear in optimization as convex surrogates for rank and as constraints to limit the operator\u2019s impact.</p> </li> </ul> <p>In machine learning, one often whitens data (via SVD of the covariance) to improve conditioning, or uses truncated SVD to compress features. In sum, understanding singular values and operator norms equips us to diagnose and improve algorithmic performance for convex optimization problems.</p>"},{"location":"130_projections/","title":"Appendix B - Projection and Proximal Operators","text":"<p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about \u201cdropping perpendiculars\u201d to a subspace or convex set.</p> <p>Projection onto a subspace: Let \\(W \\subseteq \\mathbb{R}^n\\) be a subspace (e.g. defined by a set of linear equations \\(Ax=0\\) or spanned by some basis vectors). The orthogonal projection of any \\(x \\in \\mathbb{R}^n\\) onto \\(W\\) is the unique point \\(P_W(x) \\in W\\) such that \\(x - P_W(x)\\) is orthogonal to \\(W\\). If \\({q_1,\\dots,q_k}\\) is an orthonormal basis of \\(W\\), then \u200b  </p> <p>as mentioned earlier. This \\(P_W(x)\\) minimizes the distance \\(|x - y|2\\) over all \\(y\\in W\\). The residual \\(r = x - P_W(x)\\) is orthogonal to every direction in \\(W\\). For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In \\(\\mathbb{R}^n\\), \\(P_W\\) is an \\(n \\times n\\) matrix (if \\(W\\) is \\(k\\)-dimensional, \\(P_W\\) has rank \\(k\\)) that satisfies \\(P_W^2 = P_W\\) (idempotent) and \\(P_W = P_W^T\\) (symmetric). In optimization, if we are constrained to \\(W\\), a projected gradient step does \\(x_{k+1} = P_W(x_k - \\alpha \\nabla f(x_k))\\) to ensure \\(x_{k+1} \\in W\\).</p> <p>Projection onto a convex set: More generally, for a closed convex set \\(C \\subset \\mathbb{R}^n\\), the projection \\(\\operatorname{proj}_C(x)\\) is defined as the unique point in \\(C\\) closest to \\(x\\):</p> \\[ P_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|_2 \\] <p>For convex \\(C\\), this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box \\([l,u]\\) just clips each coordinate between \\(l\\) and \\(u\\)). Some properties of convex projections: </p> <ul> <li> <p>\\(P_C(x)\\) lies on the boundary of \\(C\\) along the direction of \\(x\\) if \\(x \\notin C\\). </p> </li> <li> <p>The first-order optimality condition for the minimization above says \\((x - P_C(x))\\) is orthogonal to the tangent of \\(C\\) at \\(P_C(x)\\), or equivalently \\(\\langle x - P_C(x), y - P_C(x)\\rangle \\le 0\\) for all \\(y \\in C\\). This means the line from \\(P_C(x)\\) to \\(x\\) forms a supporting hyperplane to \\(C\\) at \\(P_C(x)\\). </p> </li> <li>Also, projections are firmly non-expansive: \\(|P_C(x)-P_C(y)|^2 \\le \\langle P_C(x)-P_C(y), x-y \\rangle \\le |x-y|^2\\). Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li> </ul> <p>Examples:</p> <ul> <li> <p>Projection onto an affine set \\(Ax=b\\) (assuming it\u2019s consistent) can be derived via normal equations: one finds a correction \\(\\delta x\\) in the row space of \\(A^T\\) such that \\(A(x+\\delta x)=b\\). The solution is \\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\\) (for full row rank \\(A\\)).</p> </li> <li> <p>Projection onto the nonnegative orthant \\({x: x_i\\ge0}\\) just sets \\(x_i^- = \\min(x_i,0)\\) to zero (i.e. \\([x]^+ = \\max{x,0}\\) componentwise). This is used in nonnegativity constraints.</p> </li> <li> <p>Projection onto an \\(\\ell_2\\) ball \\({|x|_2 \\le \\alpha}\\) scales \\(x\\) down to have length \\(\\alpha\\) if \\(|x|&gt;\\alpha\\), or does nothing if \\(|x|\\le\\alpha\\).</p> </li> <li> <p>Projection onto an \\(\\ell_1\\) ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal \\(\\alpha\\).</p> </li> </ul> <p>Why projections matter in optimization: Many convex optimization problems involve constraints \\(x \\in C\\) where \\(C\\) is convex. If we can compute \\(P_C(x)\\) easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to \\(C\\), we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that \\((x - P_C(x))\\) is orthogonal to the feasible region at \\(P_C(x)\\) connects to KKT conditions: at optimum \\(\\hat{x}\\) with \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(\\hat{x}))\\), the vector \\(-\\nabla f(\\hat{x})\\) must lie in the normal cone of \\(C\\) at \\(\\hat{x}\\), meaning the gradient is \u201cbalanced\u201d by the constraint boundary \u2014 this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that \\(\\hat{x} = P_C(x^* - \\alpha \\nabla f(x^*))\\) for some step \\(\\alpha\\), i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p> <p>Orthogonal decomposition: Any vector \\(x\\) can be uniquely decomposed relative to a subspace \\(W\\) as \\(x = P_W(x) + r\\) with \\(r \\perp W\\). Moreover, \\(|x|^2 = |P_W(x)|^2 + |r|^2\\) (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint \\(x\\in W\\), any descent direction \\(d\\) can be split into a part tangent to \\(W\\) (which actually moves within \\(W\\)) and a part normal to \\(W\\) (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient \\(\\nabla f(x^)\\) being orthogonal to the feasible region means \\(\\nabla f(x^)\\) lies entirely in the normal subspace \\(W^\\perp\\) \u2014 no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: \\(\\nabla f(x^)\\) is in the row space of \\(A\\) if \\(Ax^=b\\) are active constraints, which leads to \\(\\nabla f(x^*) = A^T \\lambda\\) for some \\(\\lambda\\) (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p> <p>Projection algorithms: The simplicity or difficulty of computing \\(P_C(x)\\) often determines if we can solve a problem efficiently. If \\(C\\) is something like a polyhedron given by linear inequalities, \\(P_C\\) might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing \\(\\operatorname{prox}{\\gamma g}(x) = \\arg\\min_y {g(y) + \\frac{1}{2\\gamma}|y-x|^2}\\), which for indicator functions of set \\(C\\) yields \\(\\operatorname{prox}{\\delta_C}(x) = P_C(x)\\). Thus projection is a special proximal operator (one for constraints).</p> <p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in \\(C_1 \\cap C_2\\) by \\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\\)), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections \u2014 that the closest point condition yields orthogonality conditions and that projections do not expand distances \u2014 is crucial for understanding how constraint-handling algorithms converge.</p>"},{"location":"13_calculus/","title":"3. Multivariable Calculus for Optimization","text":""},{"location":"13_calculus/#chapter-3-multivariable-calculus-for-optimization","title":"Chapter 3: Multivariable Calculus for Optimization","text":"<p>Optimisation is about finding points that minimise (or maximise) a function. To do that analytically, we need to understand gradients, Hessians, Taylor expansions, and first-/second-order optimality conditions.</p> <p>We work in \\(\\mathbb{R}^n\\).</p>"},{"location":"13_calculus/#31-gradients-jacobians-and-hessians","title":"3.1 Gradients, Jacobians, and Hessians","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The gradient of \\(f\\) at \\(x\\) is the column vector  It points in the direction of steepest increase of \\(f\\).</p> <p>The directional derivative in direction \\(u\\) is \\(D_u f(x) = \\lim_{t\\to0} \\frac{f(x+tu)-f(x)}{t} = \\langle \\nabla f(x), u\\rangle\\). This shows how the gradient inner product with \\(u\\) gives the instantaneous rate of change of \\(f\\) along \\(u\\). In particular, \\(D_u f(x)\\) is maximized when \\(u\\) points along \\(\\nabla f(x)\\) (steepest ascent) and minimized when \\(u\\) is opposite.</p> <p>If \\(F : \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian \\(J_F(x)\\) is the \\(m \\times n\\) matrix of partial derivatives.</p> <p>Gradient Lipschitz continuity: A concept often used in convergence analysis is Lipschitz continuity of the gradient. If there exists \\(L\\) such that \\(|\\nabla f(x) - \\nabla f(y)| \\le L |x-y|\\) for all \\(x,y\\), we say the gradient is \\(L\\)-Lipschitz (or \\(f\\) is \\(L\\)-smooth). \\(L\\) is essentially an upper bound on the Hessian eigenvalues (for \\(\\ell_2\\) norm): \\(L \\ge \\lambda_{\\max}(\\nabla^2 f(x))\\) for all \\(x\\). Smoothness is important because it ensures gradient descent with step \\(\\alpha = 1/L\\) converges, and it gives a bound \\(f(x_{k+1}) \\le f(x_k) - \\frac{1}{2L}|\\nabla f(x_k)|^2\\) (so the function value decreases at least proportionally to the squared gradient norm). Many convex functions in optimization are \\(L\\)-smooth (e.g. quadratic forms with \\(\\lambda_{\\max}(Q)=L\\)). Smoothness together with strong convexity (defined shortly) yields linear convergence rates for gradient descent.</p> <p>Strong convexity: A differentiable function \\(f\\) is \\(\\mu\\)-strongly convex if \\(f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\mu}{2}|y-x|^2\\) for all \\(x,y\\). Equivalently, \\(f(x) - \\frac{\\mu}{2}|x|^2\\) is convex, which implies \\(\\nabla^2 f(x) \\succeq \\mu I\\) (Hessian bounded below by \\(\\mu\\)) when \\(f\\) is twice differentiable. Strong convexity means \\(f\\) has a quadratic curvature of at least \\(\\mu\\) \u2013 it grows at least as fast as a parabola. Strongly convex functions have unique minimizers (the bowl can\u2019t flatten out). They also yield much faster convergence: for \\(\\mu\\)-strongly convex and \\(L\\)-smooth \\(f\\), gradient descent with \\(\\alpha=1/L\\) converges like \\((1-\\mu/L)^k\\) (linear rate). Intuitively, the condition number \\(\\kappa = L/\\mu\\) comes into play. Examples: the quadratic form above is strongly convex with \\(\\mu = \\lambda_{\\min}(Q)\\). Adding a small ridge term \\(\\frac{\\mu}{2}|x|^2\\) to any convex \\(f\\) makes it \\(\\mu\\)-strongly convex and improves conditioning at the cost of bias.</p> <p>The Hessian of \\(f\\) is the \\(n \\times n\\) matrix of second partial derivatives:  </p> <p>If \\(f\\) is twice continuously differentiable, then \\(\\nabla^2 f(x)\\) is symmetric (Clairaut\u2019s theorem).</p> <p>Example \u2013 quadratic function: \\(f(x) = \\frac{1}{2}x^TQx - b^T x\\). Here \\(\\nabla f(x) = Qx - b\\) (linear), and \\(\\nabla^2 f(x) = Q\\). Solving \\(\\nabla f=0\\) yields \\(Qx=b\\), so if \\(Q \\succ 0\\) the unique minimizer is \\(x^* = Q^{-1}b\\). The Hessian being \\(Q \\succ 0\\) confirms convexity. If \\(Q\\) has large eigenvalues, gradient \\(Qx - b\\) changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of \\(Q\\) controls difficulty of minimizing \\(f\\).</p>"},{"location":"13_calculus/#32-first-order-taylor-approximation","title":"3.2 First-order Taylor approximation","text":"<p>For differentiable \\(f\\), we have the first-order Taylor expansion around \\(x\\):  </p> <p>Interpretation:</p> <ul> <li>\\(\\nabla f(x)\\) gives the best linear approximation.</li> <li>The linear model predicts how \\(f\\) changes if we move by \\(d\\).</li> </ul> <p>This is the basis of first-order optimisation methods like gradient descent.</p>"},{"location":"13_calculus/#33-second-order-taylor-approximation","title":"3.3 Second-order Taylor approximation","text":"<p>If \\(f\\) is twice differentiable, then  </p> <p>If \\(\\nabla^2 f(x)\\) is positive semidefinite, the quadratic term is always \\(\\ge 0\\). Locally, \\(x\\) is in a \u201cbowl\u201d. If \\(\\nabla^2 f(x)\\) is indefinite, the landscape can curve up in some directions and down in others \u2014 typical of saddle points.</p>"},{"location":"13_calculus/#34-unconstrained-optimality-conditions","title":"3.4 Unconstrained optimality conditions","text":"<p>Suppose we want to solve  with no constraints.</p> <p>A point \\(x^*\\) is called a critical point if \\(\\nabla f(x^*) = 0\\).</p> <ul> <li> <p>First-order necessary condition:   If \\(x^*\\) is a (local) minimiser and \\(f\\) is differentiable, then    </p> </li> <li> <p>Second-order necessary condition:   If \\(x^*\\) is a (local) minimiser and \\(f\\) is twice differentiable,    </p> </li> <li> <p>Second-order sufficient condition:   If      then \\(x^*\\) is a strict local minimiser.</p> </li> </ul> <p>Now, here is where convexity changes everything.</p> <p>If \\(f\\) is convex, then any point \\(x^*\\) with \\(\\nabla f(x^*) = 0\\) is not just a local minimiser \u2014 it is a global minimiser (Boyd and Vandenberghe, 2004). No second-order check is even needed.</p>"},{"location":"13_calculus/#35-gradients-as-normals-to-level-sets","title":"3.5 Gradients as normals to level sets","text":"<p>A level set of a differentiable function \\(f\\) is  </p> <p>At any point \\(x\\) with \\(\\nabla f(x) \\ne 0\\), the gradient \\(\\nabla f(x)\\) is orthogonal to the level set \\(L_{f(x)}\\). Intuitively, the level set is like a contour line, and the gradient is perpendicular to it, pointing toward larger values of \\(f\\).</p> <p>In optimisation, if we want to decrease \\(f\\), we move roughly in direction \\(-\\nabla f(x)\\).</p> <p>This geometric fact recurs later in constrained optimisation and KKT: at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>"},{"location":"13_calculus/#36-convexity-and-the-hessian","title":"3.6 Convexity and the Hessian","text":"<p>If \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is twice differentiable, then:</p> <ul> <li>\\(f\\) is convex if and only if \\(\\nabla^2 f(x)\\) is positive semidefinite for all \\(x\\) in its domain (Boyd and Vandenberghe, 2004).</li> <li>\\(f\\) is strictly convex if \\(\\nabla^2 f(x)\\) is positive definite for all \\(x\\).</li> </ul>"},{"location":"140_support/","title":"Appendix C - Support Functions and Dual Geometry (Advanced)","text":""},{"location":"140_support/#appendix-b-support-functions-and-dual-geometry-advanced","title":"Appendix B: Support Functions and Dual Geometry (Advanced)","text":"<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>"},{"location":"140_support/#b1-support-functions","title":"B.1 Support functions","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty set. The support function of \\(C\\) is  </p> <p>Interpretation:</p> <ul> <li>For a given direction \\(y\\), \\(\\sigma_C(y)\\) tells you how far you can go in that direction while staying in \\(C\\).</li> <li>It is the value of the linear maximisation problem    </li> </ul> <p>Key facts:</p> <ol> <li>\\(\\sigma_C\\) is always convex, even if \\(C\\) is not convex.</li> <li>If \\(C\\) is convex and closed, \\(\\sigma_C\\) essentially characterises \\(C\\).    In particular, \\(C\\) can be recovered as the intersection of halfspaces     </li> </ol> <p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>"},{"location":"140_support/#b2-support-functions-and-dual-norms","title":"B.2 Support functions and dual norms","text":"<p>If \\(C\\) is the unit ball of a norm \\(\\|\\cdot\\|\\), i.e.  then  the dual norm of \\(\\|\\cdot\\|\\).</p> <p>Example:</p> <ul> <li>For \\(\\ell_2\\), \\(\\|\\cdot\\|_2\\) is self-dual, so \\(\\|y\\|_2^* = \\|y\\|_2\\).</li> <li>For \\(\\ell_1\\), the dual norm is \\(\\ell_\\infty\\).</li> <li>For \\(\\ell_\\infty\\), the dual norm is \\(\\ell_1\\).</li> </ul> <p>This shows that dual norms are just support functions of norm balls.</p>"},{"location":"140_support/#b3-indicator-functions-and-conjugates","title":"B.3 Indicator functions and conjugates","text":"<p>Define the indicator function of a set \\(C\\):  </p> <p>Its convex conjugate is  </p> <p>Thus,</p> <p>The support function \\(\\sigma_C\\) is the convex conjugate of the indicator of \\(C\\).</p> <p>This is extremely important conceptually:</p> <ul> <li>Conjugates turn sets into functions.</li> <li>Duality in optimisation is often conjugacy in disguise.</li> </ul>"},{"location":"140_support/#b4-hyperplane-separation-revisited","title":"B.4 Hyperplane separation revisited","text":"<p>Recall: if \\(C\\) is closed and convex, then at any boundary point \\(x_0 \\in C\\) there is a supporting hyperplane  </p> <p>This \\(a\\) is exactly the kind of vector we would use in a support function evaluation. In fact, \\(a^\\top x_0 = \\sigma_C(a)\\) if \\(x_0\\) is an extreme point (or exposed point) in direction \\(a\\).</p> <p>Geometric interpretation:</p> <ul> <li>Lagrange multipliers in the dual problem play the role of these \\(a\\)\u2019s.</li> <li>They identify supporting hyperplanes that \u201cwitness\u201d optimality.</li> </ul>"},{"location":"140_support/#b5-duality-as-support","title":"B.5 Duality as support","text":"<p>Consider the (convex) primal problem  where \\(C\\) is a convex feasible set.</p> <p>We can rewrite the problem as minimising  </p> <p>The convex conjugate of \\(f + \\delta_C\\) is  </p> <p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on \\(f(x)\\) over \\(x \\in C\\) using conjugates and support functions (Rockafellar, 1970).</p> <p>This view makes precise the slogan:</p> <p>\u201cDual variables are hyperplanes that support the feasible set and the objective from below.\u201d</p>"},{"location":"140_support/#b6-geometry-of-kkt-and-multipliers","title":"B.6 Geometry of KKT and multipliers","text":"<p>At the optimal point \\(x^*\\) of a convex problem, there is typically a hyperplane that supports the feasible set at \\(x^*\\) and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p> <ul> <li>The multipliers form a certificate that \\(x^*\\) cannot be improved without violating feasibility.</li> <li>The dual problem is the search for the \u201cbest\u201d such certificate.</li> </ul> <p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater\u2019s condition (Boyd and Vandenberghe, 2004).</p>"},{"location":"140_support/#b7-why-this-matters","title":"B.7 Why this matters","text":"<p>This geometric point of view is not just pretty:</p> <ul> <li>It explains why strong duality holds.</li> <li>It explains what \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) \u201cmean.\u201d</li> <li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li> </ul>"},{"location":"14_convexsets/","title":"4. Convex Sets and Geometric Fundamentals","text":""},{"location":"14_convexsets/#chapter-4-convex-sets-and-geometric-fundamentals","title":"Chapter 4: Convex Sets and Geometric Fundamentals","text":"<p>Optimisation problems almost always include constraints. The feasible region \u2014 the set of points allowed by those constraints \u2014 is often a convex set. This chapter builds geometric intuition for convex sets, affine sets, hyperplanes, polyhedra, and supporting hyperplanes (Boyd and Vandenberghe, 2004; Rockafellar, 1970).</p>"},{"location":"14_convexsets/#41-convex-sets","title":"4.1 Convex sets","text":"<p>A set \\(C \\subseteq \\mathbb{R}^n\\) is convex if for any \\(x,y \\in C\\) and any \\(\\theta \\in [0,1]\\),  </p> <p>Interpretation: for any two feasible points, the entire line segment between them is also feasible. No \u201cindentations.\u201d</p>"},{"location":"14_convexsets/#examples","title":"Examples","text":"<ul> <li>An affine subspace: \\(\\{ x : Ax = b \\}\\).</li> <li>A halfspace: \\(\\{ x : a^\\top x \\le b \\}\\).</li> <li>An \\(\\ell_2\\) ball: \\(\\{ x : \\|x\\|_2 \\le r \\}\\).</li> <li>An \\(\\ell_\\infty\\) ball: \\(\\{ x : \\|x\\|_\\infty \\le r \\}\\), which is a box.</li> <li>The probability simplex: \\(\\{ x \\in \\mathbb{R}^n : x \\ge 0, \\sum_i x_i = 1 \\}\\).</li> </ul> <p>A set that is not convex: a crescent shape or annulus. The defining failure is: there exist \\(x,y\\) in the set such that some convex combination leaves the set.</p>"},{"location":"14_convexsets/#42-affine-sets-hyperplanes-and-halfspaces","title":"4.2 Affine sets, hyperplanes, and halfspaces","text":"<p>An affine set is a translate of a subspace:  where \\(S\\) is a subspace. Affine sets are convex.</p> <p>A hyperplane in \\(\\mathbb{R}^n\\) is a set of the form  for some nonzero \\(a \\in \\mathbb{R}^n\\).</p> <p>A halfspace is  </p> <p>Halfspaces are convex; intersections of halfspaces are convex. Linear inequality constraints define intersections of halfspaces, and therefore give convex feasible regions.</p>"},{"location":"14_convexsets/#43-convex-combinations-convex-hulls","title":"4.3 Convex combinations, convex hulls","text":"<p>A convex combination of points \\(x_1,\\dots,x_k\\) is  </p> <p>The convex hull of a set \\(S\\) is the set of all convex combinations of finitely many points of \\(S\\). It is the \u201csmallest\u201d convex set containing \\(S\\).</p> <p>Convex hulls matter because:</p> <ul> <li>Polytopes (bounded polyhedra) can be described as convex hulls of finitely many points (their vertices).</li> <li>Many relaxations in optimisation replace a complicated nonconvex feasible set by its convex hull.</li> </ul>"},{"location":"14_convexsets/#44-polyhedra-and-polytopes","title":"4.4 Polyhedra and polytopes","text":"<p>A polyhedron is an intersection of finitely many halfspaces:  Polyhedra are convex and cane be unbounded. If \\(P\\) is also bounded, it is called a polytope.</p> <p>In linear programming, we minimise a linear objective \\(c^\\top x\\) over a polyhedron. The optimal solution, if it exists, is always attained at an extreme point (vertex) of the feasible polyhedron (Boyd and Vandenberghe, 2004).</p>"},{"location":"14_convexsets/#45-extreme-points","title":"4.5 Extreme points","text":"<p>Let \\(C\\) be a convex set. A point \\(x \\in C\\) is an extreme point if it cannot be expressed as a strict convex combination of two other distinct points in \\(C\\). Formally, \\(x\\) is extreme in \\(C\\) if whenever  then \\(y = z = x\\).</p> <p>Geometric meaning:</p> <ul> <li>Extreme points are the \u201ccorners.\u201d</li> <li>In a polytope, extreme points are precisely the vertices.</li> </ul> <p>This is why linear programming solutions are found at vertices.</p>"},{"location":"14_convexsets/#46-cones","title":"4.6 Cones","text":"<p>A set \\(K\\) is a cone if for any \\(x \\in K\\) and \\(\\alpha \\ge 0\\), \\(\\alpha x \\in K\\). A cone is convex if additionally \\(x,y\\in K\\) implies \\(x+y \\in K\\). Convex cones are important (e.g. nonnegative orthant, PSD matrices cone) because many optimization problems can be cast as cone programs. Cones have extreme rays instead of points (directions that generate edges of the cone). For instance, the extreme rays of the positive orthant in \\(\\mathbb{R}^n\\) are the coordinate axes (each axis direction can\u2019t be formed by positive combos of others).</p> <ul> <li>Cones are closed under nonnegative scaling, but not necessarily addition.  </li> <li>Conic hull (convex cone): Collection of all conic combinations of points in \\(S\\).</li> <li>A cone is not necessarily a subspace (negative multiples may not be included).  </li> <li>A convex cone is closed under addition and nonnegative scaling.  </li> <li> <p>Polar Cones: Given a cone \\(K \\subseteq \\mathbb{R}^n\\), the polar cone is:</p> \\[ K^\\circ = \\{ y \\in \\mathbb{R}^n \\mid \\langle y, x \\rangle \\le 0, \\; \\forall x \\in K \\}. \\] <ul> <li>Intuition: polar cone vectors form non-acute angles with every vector in \\(K\\).  </li> <li>Properties:  <ul> <li>Always a closed convex cone.  </li> <li>If \\(K\\) is a subspace, \\(K^\\circ\\) is the orthogonal complement.  </li> <li>Duality: \\((K^\\circ)^\\circ = K\\) for closed convex cones.  </li> </ul> </li> </ul> </li> <li> <p>Tangent Cone: For a set \\(C\\) and point \\(x \\in C\\), the tangent cone \\(T_C(x)\\) contains all directions in which one can \u201cmove infinitesimally\u201d while remaining in \\(C\\):</p> \\[ T_C(x) = \\Big\\{ d \\in \\mathbb{R}^n \\;\\Big|\\; \\exists t_k \\downarrow 0, \\; x_k \\in C, \\; x_k \\to x, \\; \\frac{x_k - x}{t_k} \\to d \\Big\\}. \\] <ul> <li>Interior point: \\(T_C(x) = \\mathbb{R}^n\\).  </li> <li>Boundary point: \\(T_C(x)\\) restricts movement to directions staying inside \\(C\\). </li> <li>Tangent cones define feasible directions for projected gradient steps or constrained optimization.</li> </ul> </li> <li> <p>Normal Cone: For a convex set \\(C\\) at point \\(x \\in C\\):      </p> <ul> <li>Each \\(v \\in N_C(x)\\) defines a supporting hyperplane at \\(x\\).  </li> <li>Relation: \\(N_C(x) = \\big(T_C(x)\\big)^\\circ\\) \u2014 polar of tangent cone.  </li> <li>Interior point: \\(N_C(x) = \\{0\\}\\).  </li> <li>Boundary/corner: \\(N_C(x)\\) is a cone of outward normals.- Appears in first-order optimality conditions:  where the subgradient of \\(f\\) is balanced by the \u201cpush-back\u201d of constraints.</li> </ul> </li> </ul>"},{"location":"14_convexsets/#47-supporting-hyperplanes-and-separation","title":"4.7 Supporting hyperplanes and separation","text":"<p>Convex sets can be \u201ctouched\u201d or \u201cseparated\u201d by hyperplanes.</p>"},{"location":"14_convexsets/#supporting-hyperplane-theorem","title":"Supporting hyperplane theorem","text":"<p>Let \\(C \\subseteq \\mathbb{R}^n\\) be a nonempty closed convex set, and let \\(x_0\\) be a boundary point of \\(C\\). Then there exists a nonzero \\(a\\) such that  In words, there is a hyperplane \\(a^\\top x = a^\\top x_0\\) that \u201csupports\u201d \\(C\\) at \\(x_0\\): it touches \\(C\\) but does not cut through it.</p>"},{"location":"14_convexsets/#separating-hyperplane-theorem","title":"Separating hyperplane theorem","text":"<p>If \\(C\\) and \\(D\\) are two disjoint nonempty convex sets, then there exists a hyperplane that separates them: some nonzero \\(a\\) and scalar \\(b\\) such that  </p> <p>Why do we care?</p> <ul> <li>These theorems are the geometric heart of duality.</li> <li>KKT conditions can be interpreted as existence of a supporting hyperplane that is simultaneously aligned with objective and constraints.</li> <li>Subgradients of convex functions correspond to supporting hyperplanes of epigraphs.</li> </ul>"},{"location":"14_convexsets/#47-feasible-regions-in-convex-optimisation","title":"4.7 Feasible regions in convex optimisation","text":"<p>In convex optimisation, the feasible set is typically something like  </p> <ul> <li>If each \\(g_i\\) is convex and each \\(h_j\\) is affine, then the feasible set is convex.</li> <li>If \\(f\\) is also convex, then the entire problem is a convex optimisation problem.</li> </ul> <p>Thus, convex sets formalise \u201cwhat it means for feasible directions to be allowed,\u201d and this is what gives us global optimality guarantees later.</p>"},{"location":"15_convexfunctions/","title":"5. Convex Functions","text":""},{"location":"15_convexfunctions/#chapter-5-convex-functions","title":"Chapter 5: Convex Functions","text":"<p>Convex functions are the objectives we minimise. Understanding them is essential, because convexity of the objective is what turns an optimisation problem from \"possibly impossible\" to \"provably solvable\".</p>"},{"location":"15_convexfunctions/#51-definitions-of-convexity","title":"5.1 Definitions of convexity","text":""},{"location":"15_convexfunctions/#511-basic-definition","title":"5.1.1 Basic definition","text":"<p>A function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x,y\\) in its domain and all \\(\\theta \\in [0,1]\\),</p> \\[ f(\\theta x + (1-\\theta) y) \\le \\theta f(x) + (1-\\theta) f(y)~. \\] <p>If the inequality is strict whenever \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\), then \\(f\\) is strictly convex.</p>"},{"location":"15_convexfunctions/#512-epigraph-definition","title":"5.1.2 Epigraph definition","text":"<p>The epigraph of \\(f\\) is  </p> <p>\\(f\\) is convex if and only if \\(\\mathrm{epi}(f)\\) is a convex set. This links convex functions to convex sets, and unlocks the geometry: tangent hyperplanes to \\(\\mathrm{epi}(f)\\) correspond to subgradients (Chapter 6).</p>"},{"location":"15_convexfunctions/#52-first-order-characterisation","title":"5.2 First-order characterisation","text":"<p>If \\(f\\) is differentiable, then \\(f\\) is convex if and only if</p> \\[ f(y) \\ge f(x) + \\nabla f(x)^\\top (y - x) \\quad \\text{for all } x,y. \\] <p>Interpretation:</p> <ul> <li>The first-order Taylor approximation is always a global underestimator.</li> <li>The gradient at \\(x\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x, f(x))\\).</li> </ul> <p>This inequality is sometimes called the first-order condition for convexity.</p> <p>This is a powerful characterization: it says the tangent hyperplane at any point \\(x\\) lies below the graph of \\(f\\) everywhere. In other words, the gradient at \\(x\\) provides a global underestimator of \\(f\\) (supporting hyperplane to epigraph). This inequality is essentially the definition above in the limit \\(\\lambda\\to0\\). Geometrically, this means no tangent line ever goes above the function. For a convex differentiable \\(f\\), we have \\(f(y) - f(x) \\ge \\nabla f(x)^T (y-x)\\), so moving from \\(x\\) in any direction, the actual increase in \\(f\\) is at least as large as the linear prediction by \\(\\nabla f(x)\\) (since the function bends upward or straight). At optimum \\(\\hat{x}\\), a necessary and sufficient condition (for convex differentiable \\(f\\)) is \\(\\nabla f(x^) = 0\\). This ties to optimality: \\(\\nabla f(x^)=0\\) means \\(f(y)\\ge f(\\hat{x}) + \\nabla f(\\hat{x})^T (y-\\hat{x}) = f(\\hat{x})\\) for all \\(y\\), so \\(\\hat{x}\\) is global minimizer.</p> <p>If \\(f\\) is not differentiable, a similar condition holds with subgradients: \\(f\\) is convex iff for all \\(x,y\\) there exists a (sub)gradient \\(g \\in \\partial f(x)\\) such that \\(f(y) \\ge f(x) + g^T(y-x)\\). The set of all subgradients \\(\\partial f(x)\\) is a convex set (the subdifferential). At optimum, \\(0 \\in \\partial f(x^)\\) is the condition. For example, \\(f(x)=|x|\\) has subgradient \\(g=\\mathrm{sign}(x)\\) (multivalued at 0, where any \\(g\\in[-1,1]\\) is subgradient). Setting \\(0\\) in subdifferential yields \\(0\\in[-1,1]\\), so indeed \\(x^=0\\) is minimizer.</p>"},{"location":"15_convexfunctions/#53-second-order-characterisation","title":"5.3 Second-order characterisation","text":"<p>If \\(f\\) is twice continuously differentiable, then \\(f\\) is convex if and only if its Hessian is positive semidefinite everywhere:</p> \\[ \\nabla^2 f(x) \\succeq 0 \\quad \\text{for all } x~. \\] <p>If \\(\\nabla^2 f(x) \\succ 0\\) for all \\(x\\), then \\(f\\) is strictly convex.</p> <p>This is a computational test for convexity of smooth functions: check the Hessian.</p>"},{"location":"15_convexfunctions/#54-examples-of-convex-functions","title":"5.4 Examples of convex functions","text":"<ol> <li> <p>Affine functions: \\(f(x) = a^\\top x + b\\).    Always convex (and concave).</p> </li> <li> <p>Quadratic functions with PSD Hessian: \\(f(x) = \\tfrac12 x^\\top Q x + c^\\top x + d\\),    where \\(Q \\succeq 0\\) (symmetric positive semidefinite).    Convex because \\(\\nabla^2 f(x) = Q \\succeq 0\\).</p> </li> <li> <p>Norms: \\(f(x) = \\|x\\|\\) for any norm.    All norms are convex.</p> </li> <li> <p>Maximum of affine functions: \\(f(x) = \\max_i (a_i^\\top x + b_i)\\).    Convex because it is the pointwise maximum of convex functions.</p> </li> <li> <p>Log-sum-exp function: \\(f(x) = \\log \\left( \\sum_{i=1}^k \\exp(a_i^\\top x + b_i) \\right)\\).    This function is convex and is ubiquitous in statistics and machine learning (softmax, logistic regression). The convexity follows from Jensen\u2019s inequality and properties of the exponential (Boyd and Vandenberghe, 2004).</p> </li> </ol>"},{"location":"15_convexfunctions/#55-jensens-inequality","title":"5.5 Jensen\u2019s inequality","text":"<p>Let \\(f\\) be convex, and let \\(X\\) be a random variable taking values in the domain of \\(f\\). Then  </p> <p>This is Jensen\u2019s inequality. It generalises the definition of convexity from two-point averages to arbitrary expectations. As a special case, for scalars \\(x_1,\\dots,x_n\\) and weights \\(\\theta_i \\ge 0\\) with \\(\\sum_i \\theta_i = 1\\),  </p> <p>which is exactly convexity. Jensen\u2019s inequality has many uses: in machine learning, it justifies algorithms like EM (which use the inequality to create surrogate objectives), and it provides bounds like \\(\\log(\\mathbb{E}[e^X]) \\ge \\mathbb{E}[X]\\) (by convexity of \\(\\log\\) or \\(e^x\\)). As a simple example, taking \\(f(x)=x^2\\) and \\(X\\) uniform in \\({-1,1}\\), Jensen says \\((\\mathbb{E}[X])^2 = 0^2 \\le \\mathbb{E}[X^2] = 1\\), which is true. Or \\(f(x)=\\frac{1}{x}\\) convex on \\((0,\\infty)\\) implies \\(\\frac{1}{\\mathbb{E}[X]} \\le \\mathbb{E}[\\frac{1}{X}]\\) for positive \\(X\\). In optimization, Jensen\u2019s inequality often helps in proving convexity of expectations: if you mix some distributions or uncertain inputs, the expected loss is convex if the loss function is convex.</p>"},{"location":"15_convexfunctions/#56-operations-that-preserve-convexity","title":"5.6 Operations that preserve convexity","text":"<p>If \\(f\\) and \\(g\\) are convex, then:</p> <ul> <li>\\(f + g\\) is convex.</li> <li>\\(\\alpha f\\) is convex for any \\(\\alpha \\ge 0\\).</li> <li>\\(\\max\\{f,g\\}\\) is convex.</li> <li>Composition with an affine map preserves convexity:   If \\(A\\) is a matrix and \\(b\\) a vector, then \\(x \\mapsto f(Ax + b)\\) is convex.</li> </ul> <p>If \\(f\\) is convex and nondecreasing in each argument, and each \\(g_i\\) is convex, then the composition \\(x \\mapsto f(g_1(x), \\dots, g_k(x))\\) is convex. This helps you build new convex functions from known ones.</p>"},{"location":"15_convexfunctions/#57-level-sets-of-convex-functions","title":"5.7 Level sets of convex functions","text":"<p>For \\(\\alpha \\in \\mathbb{R}\\), define the sublevel set </p> <p>If \\(f\\) is convex, then every sublevel set is convex (Boyd and Vandenberghe, 2004). This is crucial: constraints of the form \\(f(x) \\le \\alpha\\) are convex constraints.</p> <p>For example, the set  is convex because \\(x \\mapsto \\|Ax-b\\|_2\\) is convex.</p>"},{"location":"15_convexfunctions/#58-strict-and-strong-convexity","title":"5.8 Strict and strong convexity","text":"<ul> <li> <p>\\(f\\) is strictly convex if  for all \\(x \\ne y\\) and \\(\\theta \\in (0,1)\\).</p> </li> <li> <p>\\(f\\) is strongly convex with parameter \\(m&gt;0\\) if  </p> </li> </ul> <p>Strong convexity implies a unique minimiser and gives quantitative convergence rates for gradient methods.</p>"},{"location":"16_subgradients/","title":"6. Nonsmooth Convex Optimization \u2013 Subgradients","text":""},{"location":"16_subgradients/#chapter-6-nonsmooth-convex-optimization-subgradients","title":"Chapter 6: Nonsmooth Convex Optimization \u2013 Subgradients","text":"<p>Many of the most important convex functions are not differentiable everywhere:</p> <ul> <li>\\(\\|x\\|_1 = \\sum_i |x_i|\\) has corners at \\(x_i = 0\\),</li> <li>\\(f(x) = \\max\\{a_1^\\top x + b_1, \\dots, a_k^\\top x + b_k\\}\\) is piecewise affine,</li> <li>the hinge loss \\(\\max\\{0, 1 - y w^\\top x\\}\\) (used in SVMs) is not smooth at the kink.</li> </ul> <p>We still want optimality conditions, descent methods, and dual variables. Subgradients give us that (Hiriart-Urruty and Lemar\u00e9chal, 2001; Boyd and Vandenberghe, 2004).</p>"},{"location":"16_subgradients/#61-the-problem-with-nabla-f","title":"6.1 The problem with \\(\\nabla f\\)","text":"<p>For a convex but nonsmooth \\(f\\), the usual condition \u201c\\(\\nabla f(x^*) = 0\\)\u201d may not make sense, because \\(\\nabla f(x^*)\\) may not exist.</p> <p>But geometrically, convex functions still have supporting hyperplanes at every point. That is the key.</p>"},{"location":"16_subgradients/#62-subgradients-and-the-subdifferential","title":"6.2 Subgradients and the subdifferential","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) be convex. A vector \\(g \\in \\mathbb{R}^n\\) is called a subgradient of \\(f\\) at \\(x\\) if, for all \\(y\\),  </p> <p>Interpretation:</p> <ul> <li>The affine function \\(y \\mapsto f(x) + g^\\top (y-x)\\) is a global underestimator of \\(f\\).</li> <li>\\(g\\) defines a supporting hyperplane to the epigraph of \\(f\\) at \\((x,f(x))\\).</li> </ul> <p>The set of all subgradients of \\(f\\) at \\(x\\) is called the subdifferential of \\(f\\) at \\(x\\):  </p> <p>If \\(f\\) is differentiable at \\(x\\), then  </p> <p>If \\(f\\) is not differentiable at \\(x\\), \\(\\partial f(x)\\) is typically a nonempty convex set.</p>"},{"location":"16_subgradients/#63-examples","title":"6.3 Examples","text":""},{"location":"16_subgradients/#631-absolute-value-in-1d","title":"6.3.1 Absolute value in 1D","text":"<p>Let \\(f(t) = |t|\\).</p> <ul> <li>For \\(t&gt;0\\), \\(\\partial f(t) = \\{1\\}\\).</li> <li>For \\(t&lt;0\\), \\(\\partial f(t) = \\{-1\\}\\).</li> <li>For \\(t=0\\),     </li> </ul> <p>At the kink, any slope between \\(-1\\) and \\(1\\) is a valid supporting line from below.</p>"},{"location":"16_subgradients/#632-ell_1-norm","title":"6.3.2 \\(\\ell_1\\) norm","text":"<p>For \\(f(x) = \\|x\\|_1 = \\sum_i |x_i|\\), we have  So</p> <ul> <li>if \\(x_i &gt; 0\\), then \\(g_i = 1\\),</li> <li>if \\(x_i &lt; 0\\), then \\(g_i = -1\\),</li> <li>if \\(x_i = 0\\), then \\(g_i \\in [-1,1]\\).</li> </ul> <p>This is exactly what shows up in LASSO optimality conditions in statistics.</p>"},{"location":"16_subgradients/#633-pointwise-max-of-affine-functions","title":"6.3.3 Pointwise max of affine functions","text":"<p>Let  </p> <p>If a single index \\(i^*\\) achieves the max at \\(x\\), then  </p> <p>If multiple \\(i\\) are tied at the max, then  the convex hull of all active slopes.</p>"},{"location":"16_subgradients/#64-subgradient-optimality-condition","title":"6.4 Subgradient optimality condition","text":"<p>Suppose we want to solve the unconstrained convex minimisation problem</p> \\[ \\min_x f(x), \\] <p>Then a point \\(x^*\\) is optimal if and only if  </p> <p>This is the nonsmooth analogue of \\(\\nabla f(x^*) = 0\\) (Boyd and Vandenberghe, 2004). It says:</p> <p>at the minimiser, there is no subgradient pointing into a direction that would reduce \\(f\\).</p> <p>Equivalently: every subgradient \"pushes up\" from \\(x^*\\).</p>"},{"location":"16_subgradients/#65-subgradient-calculus-useful-rules","title":"6.5 Subgradient calculus (useful rules)","text":"<p>If \\(f\\) and \\(g\\) are convex:</p> <ul> <li> <p>\\(\\partial (f+g)(x) \\subseteq \\partial f(x) + \\partial g(x)\\), i.e.    </p> </li> <li> <p>If \\(A\\) is a matrix and \\(h(x) = f(Ax)\\), then    </p> </li> <li> <p>If \\(f(x) = \\max_i f_i(x)\\) and each \\(f_i\\) is convex, then    </p> </li> </ul> <p>These rules make it possible to compute subgradients of complicated nonsmooth objectives.</p>"},{"location":"16_subgradients/#66-connection-to-duality-and-conjugates","title":"6.6 Connection to duality and conjugates","text":"<p>For a convex function \\(f\\), its convex conjugate (also called Legendre\u2013Fenchel transform) is  </p> <p>A deep fact (Hiriart-Urruty and Lemar\u00e9chal, 2001) is:  </p> <p>This symmetry is the algebraic heart of Lagrangian duality, and it reappears when we discuss Fenchel duality in Chapter 8 and Appendix B.</p>"},{"location":"16_subgradients/#67-takeaways","title":"6.7 Takeaways","text":"<ol> <li>Subgradients generalise gradients to nondifferentiable convex functions.</li> <li>Optimality in convex, nonsmooth problems is: \\(0 \\in \\partial f(x^*)\\).</li> <li>Subdifferentials are convex sets; at kinks they become intervals or polytopes.</li> <li>Subgradients are geometrically supporting hyperplanes to the epigraph.</li> </ol> <p>In the next chapter, we apply all of this to constrained optimisation: KKT.</p>"},{"location":"17_kkt/","title":"7. Optimization Principles \u2013 From Gradient Descent to KKT","text":""},{"location":"17_kkt/#chapter-7-optimization-principles-from-gradient-descent-to-kkt","title":"Chapter 7: Optimization Principles \u2013 From Gradient Descent to KKT","text":"<p>At this point we understand:</p> <ul> <li>how to recognise convex functions,</li> <li>how to talk about feasible sets,</li> <li>how to describe optimality with gradients or subgradients.</li> </ul> <p>Now we turn to constrained optimisation. We first recall unconstrained optimisation and gradient descent, then develop the Karush\u2013Kuhn\u2013Tucker (KKT) conditions, which are the first-order optimality conditions for constrained convex optimisation (Boyd and Vandenberghe, 2004).</p>"},{"location":"17_kkt/#71-unconstrained-convex-minimisation","title":"7.1 Unconstrained convex minimisation","text":"<p>Consider  where \\(f\\) is convex and differentiable.</p> <p>Gradient descent is the iterative method:  for some step size \\(\\alpha_k &gt; 0\\).</p> <p>Intuition:</p> <ul> <li>Move opposite the gradient to reduce \\(f\\).</li> <li>Under suitable conditions on step size (e.g. Lipschitz gradient), this converges to a global minimiser if \\(f\\) is convex.</li> </ul> <p>If \\(f\\) is strongly convex, we get uniqueness of the minimiser and faster convergence.</p>"},{"location":"17_kkt/#72-equality-constrained-optimisation-and-lagrange-multipliers","title":"7.2 Equality-constrained optimisation and Lagrange multipliers","text":"<p>Consider  where \\(f\\) and each \\(h_j\\) are differentiable.</p> <p>We define the Lagrangian  where \\(\\lambda_j\\) are the Lagrange multipliers.</p> <p>A necessary condition for \\(x^*\\) to be optimal (under suitable regularity assumptions) is:</p> <ol> <li>Stationarity:     </li> <li>Primal feasibility:     </li> </ol> <p>Geometrically, stationarity says: the gradient of \\(f\\) at \\(x^*\\) lies in the span of the gradients of the active constraints. In words, you cannot move in any feasible direction without increasing \\(f\\).</p>"},{"location":"17_kkt/#73-inequality-constraints-and-kkt","title":"7.3 Inequality constraints and KKT","text":"<p>Now consider the general problem:  </p> <p>We form the Lagrangian  with multipliers \\(\\lambda \\in \\mathbb{R}^p\\) (unrestricted) and \\(\\mu \\in \\mathbb{R}^m\\) with \\(\\mu_i \\ge 0\\).</p> <p>The Karush\u2013Kuhn\u2013Tucker (KKT) conditions consist of:</p> <ol> <li> <p>Primal feasibility:     </p> </li> <li> <p>Dual feasibility:     </p> </li> <li> <p>Stationarity:</p> </li> </ol> \\[\\nabla f(x^*)     + \\sum_{j=1}^p \\lambda_j^* \\nabla h_j(x^*)    + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(x^*)    = 0\\] <ol> <li>Complementary slackness:     </li> </ol> <p>Complementary slackness means:</p> <ul> <li>If a constraint \\(g_i(x) \\le 0\\) is strictly inactive at \\(x^*\\) (i.e. \\(g_i(x^*) &lt; 0\\)), then \\(\\mu_i^* = 0\\).</li> <li>If \\(\\mu_i^* &gt; 0\\), then the constraint is tight: \\(g_i(x^*) = 0\\).</li> </ul> <p>This matches geometric intuition: only active constraints can \u201cpush back\u201d on the optimiser.</p>"},{"location":"17_kkt/#74-kkt-and-convexity","title":"7.4 KKT and convexity","text":"<p>For general nonlinear problems, KKT conditions are necessary under regularity assumptions. For convex problems, KKT conditions are often necessary and sufficient for optimality (Boyd and Vandenberghe, 2004). In other words, if the problem is convex and a point satisfies KKT, that point is globally optimal.</p> <p>This is extremely powerful:</p> <ul> <li>You can certify optimality (and thus global optimality) just by finding multipliers \\(\\lambda^*, \\mu^*\\) that satisfy KKT.</li> <li>KKT conditions are constructive: they are what solvers try to satisfy.</li> </ul>"},{"location":"17_kkt/#75-geometric-picture","title":"7.5 Geometric picture","text":"<p>At the optimal point \\(x^*\\):</p> <ul> <li>\\(\\nabla f(x^*)\\) is balanced by a conic combination of the normals of the active inequality constraints plus a linear combination of the equality constraint normals.</li> <li>The objective cannot be decreased by moving in any feasible direction.</li> </ul> <p>Visually: the contour of \\(f\\) is \u201ctangent\u201d to the feasible region. The Lagrange multipliers encode the direction and strength of that tangency.</p>"},{"location":"17_kkt/#76-constraint-qualifications-brief-note","title":"7.6 Constraint qualifications (brief note)","text":"<p>To guarantee that KKT multipliers exist and KKT conditions apply cleanly, we usually need a mild regularity condition called a constraint qualification. The most common is Slater\u2019s condition for convex problems:</p> <p>If the problem is convex and there exists a strictly feasible point \\(\\tilde{x}\\) such that \\(g_i(\\tilde{x}) &lt; 0\\) for all \\(i\\) and \\(h_j(\\tilde{x}) = 0\\) for all \\(j\\), then strong duality holds and KKT conditions are necessary and sufficient (Boyd and Vandenberghe, 2004).</p>"},{"location":"17_kkt/#77-summary","title":"7.7 Summary","text":"<ul> <li>Gradient descent solves unconstrained convex problems.</li> <li>Lagrange multipliers extend optimality to equality constraints.</li> <li>KKT generalises this to inequality constraints.</li> <li>In convex optimisation, KKT is not just necessary, it is sufficient.</li> <li>Slater\u2019s condition guarantees everything behaves nicely.</li> </ul> <p>In the next chapter, we go deeper: duality. The dual problem gives us lower bounds, certificates of optimality, and interpretation of multipliers as \u201cprices\u201d.</p>"},{"location":"18_duality/","title":"8. Lagrange Duality Theory","text":""},{"location":"18_duality/#chapter-8-lagrange-duality-theory","title":"Chapter 8: Lagrange Duality Theory","text":"<p>Duality is one of the most beautiful and useful ideas in convex optimisation. Every constrained optimisation problem (the primal) has an associated dual problem. The dual problem:</p> <ul> <li>provides a lower bound on the optimal primal value,</li> <li>often has structure that is easier to analyse,</li> <li>gives certificates of optimality,</li> <li>interprets multipliers as \u201cprices\u201d of constraints.</li> </ul> <p>In convex optimisation, under mild assumptions, the primal and dual optimal values are equal (Boyd and Vandenberghe, 2004; Rockafellar, 1970).</p>"},{"location":"18_duality/#81-the-primal-problem","title":"8.1 The primal problem","text":"<p>We consider the general problem:  </p> <p>Assume \\(f\\) and the \\(g_i\\) are convex, and \\(h_j\\) are affine. This is a convex optimisation problem.</p> <p>We call \\(f^\\star\\) the optimal value:  </p> <p>Infimum (inf): the greatest lower bound of a set \u2014 the smallest value a function can approach, even if it is not attained.</p>"},{"location":"18_duality/#82-the-lagrangian","title":"8.2 The Lagrangian","text":"<p>We define the Lagrangian:  with multipliers \\(\\mu \\in \\mathbb{R}^m\\) and \\(\\lambda \\in \\mathbb{R}^p\\). For inequality constraints, we will later require \\(\\mu_i \\ge 0\\).</p> <p>Think of \\(\\mu_i\\) and \\(\\lambda_j\\) as \u201cpenalties\u201d for violating the constraints.</p>"},{"location":"18_duality/#83-the-dual-function","title":"8.3 The dual function","text":"<p>For fixed multipliers \\((\\lambda,\\mu)\\), define the dual function:  </p> <p>Important:</p> <ul> <li>\\(\\theta(\\lambda,\\mu)\\) is always concave in \\((\\lambda,\\mu)\\), even if \\(f\\) is not convex.</li> <li>For any \\(\\mu \\ge 0\\),  </li> </ul> <p>This last fact is called weak duality:</p> <p>The dual function gives lower bounds on the primal optimum.</p> <p>Proof sketch of weak duality: For any feasible \\(x\\) (i.e. satisfying \\(g_i(x) \\le 0\\), \\(h_j(x) = 0\\)) and any \\(\\mu \\ge 0\\),  because \\(g_i(x) \\le 0\\) and \\(\\mu_i \\ge 0\\). So \\(\\theta(\\lambda,\\mu) = \\inf_x L(x,\\lambda,\\mu) \\le f(x)\\) for all feasible \\(x\\). Taking the infimum over feasible \\(x\\) gives \\(\\theta(\\lambda,\\mu) \\le f^\\star\\).</p>"},{"location":"18_duality/#84-the-dual-problem","title":"8.4 The dual problem","text":"<p>We now maximise the lower bound. The Lagrange dual problem is:  </p> <p>Because \\(\\theta\\) is concave and we are maximising it, the dual problem is always a concave maximisation problem (i.e. a convex optimisation problem in standard form).</p> <p>Let \\(d^\\star\\) denote the optimal dual value. From weak duality, \\(d^\\star \\le f^\\star\\) always.</p>"},{"location":"18_duality/#85-strong-duality-and-slaters-condition","title":"8.5 Strong duality and Slater\u2019s condition","text":"<p>If \\(d^\\star = f^\\star\\), we say strong duality holds.</p> <p>For convex problems, strong duality typically holds under a mild regularity condition known as Slater\u2019s condition (Boyd and Vandenberghe, 2004):</p> <p>If the problem is convex and there exists a strictly feasible point \\(\\tilde{x}\\) such that \\(g_i(\\tilde{x}) &lt; 0\\) for all \\(i\\) and \\(h_j(\\tilde{x}) = 0\\) for all \\(j\\), then strong duality holds.</p> <p>Consequences of strong duality:</p> <ul> <li>The gap \\(f^\\star - d^\\star\\) is zero.</li> <li>There exist optimal multipliers \\((\\lambda^*, \\mu^*)\\).</li> <li>KKT conditions hold and characterise optimality.</li> </ul>"},{"location":"18_duality/#86-kkt-revisited-via-duality","title":"8.6 KKT revisited via duality","text":"<p>The Karush\u2013Kuhn\u2013Tucker (KKT) conditions from Chapter 7 can also be seen as the conditions under which:</p> <ol> <li>\\(x^*\\) minimises the Lagrangian over \\(x\\),</li> <li>\\((\\lambda^*, \\mu^*)\\) maximises \\(\\theta(\\lambda,\\mu)\\),</li> <li>complementary slackness holds,</li> <li>primal feasibility and dual feasibility hold.</li> </ol> <p>Under convexity + Slater, a point is optimal if and only if it satisfies KKT (Boyd and Vandenberghe, 2004). So KKT is both necessary and sufficient.</p> <p>This is the unification:</p> <ul> <li>primal feasibility,</li> <li>dual feasibility,</li> <li>complementary slackness,</li> <li>stationarity (zero subgradient of \\(L\\) w.r.t. \\(x\\)).</li> </ul>"},{"location":"18_duality/#87-interpretation-of-multipliers","title":"8.7 Interpretation of multipliers","text":"<p>The dual variables \\(\\mu_i^*\\) and \\(\\lambda_j^*\\) have interpretations:</p> <ul> <li>\\(\\mu_i^*\\) can be seen as the \u201cshadow price\u201d of relaxing constraint \\(g_i(x) \\le 0\\). If \\(\\mu_i^*\\) is large, then constraint \\(i\\) is \u201cexpensive\u201d to satisfy \u2014 it is strongly active.</li> <li>\\(\\lambda_j^*\\) plays a similar role for equality constraints.</li> </ul> <p>In resource allocation problems, these multipliers act like market prices. In regularised estimation, they act like trade-off coefficients chosen by the optimisation itself.</p>"},{"location":"18_duality/#88-example-linear-programming-dual","title":"8.8 Example: Linear programming dual","text":"<p>Consider a linear program in standard form:  </p> <p>Its dual is  </p> <p>This is a classical primal\u2013dual pair. Linear programming is convex, Slater\u2019s condition typically holds (assuming strict feasibility), and therefore strong duality holds. The LP duality theory you may have seen in undergraduate optimisation is just a special case of Lagrange duality (Boyd and Vandenberghe, 2004; Rockafellar, 1970).</p>"},{"location":"18_duality/#89-duality-as-geometry","title":"8.9 Duality as geometry","text":"<p>Duality is geometry in disguise. The dual problem is finding the \u201cbest supporting hyperplane\u201d that underestimates the primal objective over the feasible set. This is exactly the picture of supporting hyperplanes from Chapter 4, and exactly the subgradient picture from Chapter 6. Appendix B makes this geometric relationship precise in terms of support functions.</p>"},{"location":"18_duality/#810-summary","title":"8.10 Summary","text":"<ol> <li>The Lagrangian builds a bridge between constrained problems and unconstrained ones.</li> <li>The dual function gives lower bounds (weak duality).</li> <li>Maximising the dual function gives the dual problem.</li> <li>Under Slater\u2019s condition, strong duality holds: no duality gap.</li> <li>KKT conditions fall naturally out of duality.</li> <li>Dual variables are interpretable and useful in analysis, sensitivity, and economics.</li> </ol>"},{"location":"19_optimizationalgo/","title":"9. Algorithms for Convex Optimization","text":""},{"location":"19_optimizationalgo/#chapter-9-algorithms-for-convex-optimization","title":"Chapter 9: Algorithms for Convex Optimization","text":"<p>In Chapters 2\u20138 we built the mathematics of convex optimization: linear algebra (Chapter 2), gradients and Hessians (Chapter 3), convex sets (Chapter 4), convex functions (Chapter 5), subgradients (Chapter 6), KKT conditions (Chapter 7), and duality (Chapter 8). </p> <p>Now we answer the practical question:</p> <p>How do we actually solve convex optimization problems in practice?</p> <p>This chapter develops the major algorithmic families used to solve convex problems. Our goal is not only to describe each method, but to explain:</p> <ul> <li>what class of problem it solves,</li> <li>what information it needs (gradient, Hessian, projection, etc.),</li> <li>when you should use it,</li> <li>how it connects to the modelling choices you make.</li> </ul>"},{"location":"19_optimizationalgo/#91-problem-classes-vs-method-classes","title":"9.1 Problem classes vs method classes","text":"<p>Before we dive into algorithms, we need a map. Different algorithms are natural for different convex problem structures.</p>"},{"location":"19_optimizationalgo/#911-smooth-unconstrained-convex-minimisation","title":"9.1.1 Smooth unconstrained convex minimisation","text":"<p>We want  where \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is convex and differentiable.</p> <p>Typical methods:</p> <ul> <li>Gradient descent (first-order),</li> <li>Accelerated gradient,</li> <li>Newton / quasi-Newton (second-order).</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\), sometimes \\(\\nabla^2 f(x)\\).</li> </ul>"},{"location":"19_optimizationalgo/#912-smooth-convex-minimisation-with-simple-constraints","title":"9.1.2 Smooth convex minimisation with simple constraints","text":"<p>We want  where \\(\\mathcal{X}\\) is a \u201csimple\u201d closed convex set such as a box, a norm ball, or a simplex (Chapter 4).</p>"},{"location":"19_optimizationalgo/#practical-examples-of-simple-constraints","title":"Practical Examples of Simple Constraints","text":"Constraint Type Explanation Example Meaning Box Each variable is bounded independently within lower and upper limits. \\( 0 \\le x_i \\le 1 \\) Parameters are restricted to a fixed range (e.g., pixel intensities, control limits). Norm Ball All feasible points lie within a fixed radius from a center under some norm. \\( \\|x - x_0\\|_2 \\le 1 \\) Keeps the solution close to a reference point \u2014 controls total magnitude or deviation. Simplex Nonnegative variables that sum to one. \\( x_i \\ge 0,\\ \\sum_i x_i = 1 \\) Represents valid probability distributions or normalized weights (e.g., portfolio allocations). <p>Typical method:</p> <ul> <li>Projected gradient descent, which alternates a gradient step and Euclidean projection back to \\(\\mathcal{X}\\).</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\),</li> <li>the ability to compute \\(\\Pi_\\mathcal{X}(y) = \\arg\\min_{x \\in \\mathcal{X}} \\|x-y\\|_2^2\\) efficiently.</li> </ul>"},{"location":"19_optimizationalgo/#913-composite-convex-minimisation-smooth-nonsmooth","title":"9.1.3 Composite convex minimisation (smooth + nonsmooth)","text":"<p>We want  where \\(f\\) is convex and differentiable with Lipschitz gradient, and \\(R\\) is convex but possibly nonsmooth (Chapter 6). Examples:</p> <ul> <li>\\(f(x)=\\|Ax-b\\|_2^2\\), \\(R(x)=\\lambda\\|x\\|_1\\) (LASSO),</li> <li>\\(R(x)\\) is the indicator of a convex set, enforcing a hard constraint.</li> </ul> <p>Typical method:</p> <ul> <li>Proximal gradient / forward\u2013backward splitting,</li> <li>Projected gradient as a special case.</li> </ul> <p>Information required:</p> <ul> <li>\\(\\nabla f(x)\\),</li> <li>the proximal operator of \\(R\\).</li> </ul>"},{"location":"19_optimizationalgo/#914-general-convex-programs-with-inequality-constraints","title":"9.1.4 General convex programs with inequality constraints","text":"<p>We want  where \\(f\\) and \\(g_i\\) are convex, \\(h_j\\) are affine.  </p> <p>Typical method:</p> <ul> <li>Interior-point (barrier) methods.</li> </ul> <p>Information required:</p> <ul> <li>Gradients and Hessians of the barrier-augmented objective,</li> <li>ability to solve linear systems arising from Newton steps.</li> </ul>"},{"location":"19_optimizationalgo/#915-the-moral","title":"9.1.5 The moral","text":"<p>There is no single \u201cbest\u201d algorithm. There is a best algorithm for the structure you have.</p> <ul> <li>First-order methods scale to huge problems but converge relatively slowly.</li> <li>Newton and interior-point methods converge extremely fast in iterations but each iteration is more expensive (they solve linear systems involving Hessians).</li> <li>Proximal methods are designed for nonsmooth regularisers and constraints that appear everywhere in statistics and machine learning.</li> <li>Interior-point methods are the workhorse for general convex programs (including linear programs, quadratic programs, conic programs) and deliver high-accuracy solutions with strong certificates of optimality </li> </ul>"},{"location":"19_optimizationalgo/#92-first-order-methods-gradient-descent","title":"9.2 First-order methods: Gradient descent","text":""},{"location":"19_optimizationalgo/#921-setting","title":"9.2.1 Setting","text":"<p>We solve  where \\(f\\) is convex, differentiable, and (ideally) \\(L\\)-smooth: its gradient is Lipschitz with constant \\(L\\), meaning  Smoothness lets us control step sizes.</p>"},{"location":"19_optimizationalgo/#922-algorithm","title":"9.2.2 Algorithm","text":"<p>Gradient descent iterates  where \\(\\alpha_k&gt;0\\) is the step size (also called learning rate in machine learning). A common choice is a constant \\(\\alpha_k = 1/L\\) when \\(L\\) is known, or a backtracking line search when it is not.</p>"},{"location":"19_optimizationalgo/#923-geometric-meaning","title":"9.2.3 Geometric meaning","text":"<p>From Chapter 3, the first-order Taylor model is  This is minimised (under a step length constraint) by taking \\(d\\) in the direction \\(-\\nabla f(x)\\). So gradient descent is just \u201ctake a cautious step downhill\u201d.</p>"},{"location":"19_optimizationalgo/#924-convergence","title":"9.2.4 Convergence","text":"<p>For convex, \\(L\\)-smooth \\(f\\), gradient descent with a suitable fixed step size satisfies  where \\(f^\\star\\) is the global minimum. This \\(O(1/k)\\) sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need \\(\\nabla f(x_k)\\).</p>"},{"location":"19_optimizationalgo/#925-when-to-use-gradient-descent","title":"9.2.5 When to use gradient descent","text":"<ul> <li>Problems with millions of variables (large-scale ML).</li> <li>You can afford many cheap iterations.</li> <li>You only have access to gradients (or stochastic gradients).</li> <li>You do not need very high precision.</li> </ul> <p>Gradient descent is the baseline first-order method. But we can do better.</p>"},{"location":"19_optimizationalgo/#93-accelerated-first-order-methods","title":"9.3 Accelerated first-order methods","text":"<p>Plain gradient descent has an \\(O(1/k)\\) rate for smooth convex problems. Remarkably, we can do better \u2014 and in fact, provably optimal \u2014 by adding momentum.</p>"},{"location":"19_optimizationalgo/#931-nesterov-acceleration","title":"9.3.1 Nesterov acceleration","text":"<p>Nesterov\u2019s accelerated gradient method modifies the update using a momentum-like extrapolation. One common presentation is:</p> <ol> <li>Maintain two sequences \\(x_k\\) and \\(y_k\\).</li> <li>Take a gradient step from \\(y_k\\):     </li> <li>Extrapolate:     </li> </ol> <p>The extra \\(\\beta_k\\) term \u201clooks ahead,\u201d helping the method exploit curvature better than plain gradient descent.</p>"},{"location":"19_optimizationalgo/#932-optimal-first-order-rate","title":"9.3.2 Optimal first-order rate","text":"<p>For smooth convex \\(f\\), accelerated gradient achieves  which is optimal for any algorithm that uses only gradient information and not higher derivatives. In other words, you cannot beat \\(O(1/k^2)\\) in the worst case using only first-order oracle calls.</p>"},{"location":"19_optimizationalgo/#933-when-to-use-acceleration","title":"9.3.3 When to use acceleration","text":"<ul> <li>Same setting as gradient descent (large-scale smooth convex problems),</li> <li>but you want to converge in fewer iterations.</li> <li>You can tolerate a little more instability/parameter tuning (acceleration can overshoot if step sizes are not chosen carefully).</li> </ul> <p>Acceleration is the default upgrade from vanilla gradient descent in many smooth convex machine learning problems.</p>"},{"location":"19_optimizationalgo/#94-newtons-method-and-second-order-methods","title":"9.4 Newton\u2019s method and second-order methods","text":"<p>First-order methods only use gradient information. Newton\u2019s method uses curvature (the Hessian) to take smarter steps.</p>"},{"location":"19_optimizationalgo/#941-local-quadratic-model","title":"9.4.1 Local quadratic model","text":"<p>From Chapter 3, the second-order Taylor approximation at \\(x_k\\) is  </p> <p>If we (temporarily) trust this model, we choose \\(d\\) to minimise the RHS. Differentiating w.r.t. \\(d\\) and setting to zero gives the Newton step:  So  </p>"},{"location":"19_optimizationalgo/#942-convergence-behaviour","title":"9.4.2 Convergence behaviour","text":"<ul> <li>Near the minimiser of a strictly convex, twice-differentiable \\(f\\), Newton\u2019s method converges quadratically: roughly, the number of correct digits doubles every iteration.</li> <li>This is dramatically faster than \\(O(1/k)\\) or \\(O(1/k^2)\\), but only once you\u2019re in the \u201cbasin of attraction.\u201d</li> <li>Far from the minimiser, Newton can misbehave, so we pair it with a line search or trust region.</li> </ul>"},{"location":"19_optimizationalgo/#943-computational-cost","title":"9.4.3 Computational cost","text":"<p>Each Newton step requires solving a linear system involving \\(\\nabla^2 f(x_k)\\), which costs about as much as factoring the Hessian (or an approximation). This is expensive in very high dimensions, which is why Newton is most attractive for medium-scale problems where high accuracy matters.</p>"},{"location":"19_optimizationalgo/#944-why-convexity-helps","title":"9.4.4 Why convexity helps","text":"<p>If \\(f\\) is convex, then \\(\\nabla^2 f(x_k)\\) is positive semidefinite (Chapter 5). This means:</p> <ul> <li>The quadratic model is bowl-shaped, so the Newton step makes sense.</li> <li>Regularised Newton steps (adding a multiple of the identity to the Hessian) behave very predictably.</li> </ul>"},{"location":"19_optimizationalgo/#945-quasi-newton","title":"9.4.5 Quasi-Newton","text":"<p>When Hessians are too expensive, we can build low-rank approximations of \\(\\nabla^2 f(x_k)\\) or its inverse. Famous examples include BFGS and L-BFGS. These methods keep much of Newton\u2019s fast local convergence but with per-iteration cost closer to first-order methods.</p>"},{"location":"19_optimizationalgo/#946-when-to-use-newton-quasi-newton","title":"9.4.6 When to use Newton / quasi-Newton","text":"<ul> <li>You need high-accuracy solutions.</li> <li>The problem is smooth and reasonably well-conditioned.</li> <li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g. via sparse linear algebra).</li> </ul>"},{"location":"19_optimizationalgo/#95-constraints-and-nonsmooth-terms-projection-and-proximal-methods","title":"9.5 Constraints and nonsmooth terms: projection and proximal methods","text":"<p>In practice, most convex objectives are not just \u201cnice smooth \\(f(x)\\)\u201d. They often have:</p> <ul> <li>constraints \\(x \\in \\mathcal{X}\\),</li> <li>nonsmooth regularisers like \\(\\|x\\|_1\\),</li> <li>penalties that encode robustness or sparsity (Chapter 6).</li> </ul> <p>Two core ideas handle this: projected gradient and proximal gradient.</p>"},{"location":"19_optimizationalgo/#951-projected-gradient-descent","title":"9.5.1 Projected gradient descent","text":"<p>Setting: Minimise convex, differentiable \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a simple closed convex set (Chapter 4).</p> <p>Algorithm: 1. Gradient step:     2. Projection:     </p> <p>Interpretation:</p> <ul> <li>You take an unconstrained step downhill,</li> <li>then you \u201csnap back\u201d to feasibility by Euclidean projection.</li> </ul> <p>Examples of \\(\\mathcal{X}\\) where projection is cheap:</p> <ul> <li>A box: \\(l \\le x \\le u\\) (clip each coordinate).</li> <li>The probability simplex \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\) (there are fast projection routines).</li> <li>An \\(\\ell_2\\) ball \\(\\{x : \\|x\\|_2 \\le R\\}\\) (scale down if needed).</li> </ul> <p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>"},{"location":"19_optimizationalgo/#952-proximal-gradient-forwardbackward-splitting","title":"9.5.2 Proximal gradient (forward\u2013backward splitting)","text":"<p>Setting: Composite convex minimisation  where:</p> <ul> <li>\\(f\\) is convex, differentiable, with Lipschitz gradient,</li> <li>\\(R\\) is convex, possibly nonsmooth.</li> </ul> <p>Typical choices of \\(R(x)\\):</p> <ul> <li>\\(R(x) = \\lambda \\|x\\|_1\\) (sparsity),</li> <li>\\(R(x) = \\lambda \\|x\\|_2^2\\) (ridge),</li> <li>\\(R(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\), i.e. \\(R(x)=0\\) if \\(x \\in \\mathcal{X}\\) and \\(+\\infty\\) otherwise \u2014 this encodes a hard constraint.</li> </ul> <p>Define the proximal operator of \\(R\\):  </p> <p>Proximal gradient method: 1. Gradient step on \\(f\\):     2. Proximal step on \\(R\\):     </p> <p>This is also called forward\u2013backward splitting: \u201cforward\u201d = gradient step, \u201cbackward\u201d = prox step.</p>"},{"location":"19_optimizationalgo/#interpretation","title":"Interpretation:","text":"<ul> <li>The prox step \u201chandles\u201d the nonsmooth or constrained part exactly.</li> <li>For \\(R(x)=\\lambda \\|x\\|_1\\), \\(\\mathrm{prox}_{\\alpha R}\\) is soft-thresholding, which promotes sparsity in \\(x\\).   This is the heart of \\(\\ell_1\\)-regularised least-squares (LASSO) and many sparse recovery problems.</li> <li>For \\(R\\) as an indicator of \\(\\mathcal{X}\\), \\(\\mathrm{prox}_{\\alpha R} = \\Pi_\\mathcal{X}\\), so projected gradient is a special case of proximal gradient.</li> </ul> <p>This unifies constraints and regularisation.</p>"},{"location":"19_optimizationalgo/#when-to-use-proximal-projected-gradient","title":"When to use proximal / projected gradient","text":"<ul> <li>High-dimensional ML/statistics problems.</li> <li>Objectives with \\(\\ell_1\\), group sparsity, total variation, hinge loss, or indicator constraints.</li> <li>You can evaluate \\(\\nabla f\\) and compute \\(\\mathrm{prox}_{\\alpha R}\\) cheaply.</li> <li>You don\u2019t need absurdly high accuracy, but you do need scalability.</li> </ul> <p>This is the standard tool for modern large-scale convex learning problems.</p>"},{"location":"19_optimizationalgo/#96-penalties-barriers-and-interior-point-methods","title":"9.6 Penalties, barriers, and interior-point methods","text":"<p>So far we\u2019ve assumed either:</p> <ul> <li>simple constraints we can project onto,</li> <li>or nonsmooth terms we can prox.</li> </ul> <p>What if the constraints are general convex inequalities \\(g_i(x)\\le0\\)? Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>"},{"location":"19_optimizationalgo/#961-penalty-methods","title":"9.6.1 Penalty methods","text":"<p>Idea: Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints.</p> <p>Suppose we want  </p> <p>A penalty method solves instead  where:</p> <ul> <li>\\(\\phi(r)\\) is \\(0\\) when \\(r \\le 0\\) (feasible),</li> <li>\\(\\phi(r)\\) grows when \\(r&gt;0\\) (infeasible),</li> <li>\\(\\rho &gt; 0\\) is a penalty weight.</li> </ul> <p>As \\(\\rho \\to \\infty\\), infeasible points become extremely expensive, so minimisers approach feasibility.  </p> <p>This is conceptually simple and is sometimes effective, but:</p> <ul> <li>choosing \\(\\rho\\) is tricky,</li> <li>very large \\(\\rho\\) can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li> </ul> <p>Penalty methods are closely linked to robust formulations and Huber-like losses: you replace a hard requirement by a soft cost. This is exactly what you do in robust regression and in \\(\\epsilon\\)-insensitive / Huber losses (see Section 9.7).</p>"},{"location":"19_optimizationalgo/#962-barrier-methods","title":"9.6.2 Barrier methods","text":"<p>Penalty methods penalise violation after you cross the boundary. Barrier methods make it impossible to even touch the boundary.</p> <p>For inequality constraints \\(g_i(x) \\le 0\\), define the logarithmic barrier  This is finite only if \\(g_i(x) &lt; 0\\) for all \\(i\\), i.e. \\(x\\) is strictly feasible. As you approach the boundary \\(g_i(x)=0\\), \\(b(x)\\) blows up to \\(+\\infty\\).</p> <p>We then solve, for a sequence of increasing parameters \\(t\\):  subject to strict feasibility \\(g_i(x)&lt;0\\).</p> <p>As \\(t \\to \\infty\\), minimisers of \\(F_t\\) approach the true constrained optimum. The path of minimisers \\(x^*(t)\\) is called the central path.</p> <p>Key points:</p> <ul> <li>\\(F_t\\) is smooth on the interior of the feasible region.</li> <li>We can apply Newton\u2019s method to \\(F_t\\).</li> <li>Each Newton step solves a linear system involving the Hessian of \\(F_t\\), so the inner loop looks like a damped Newton method.</li> <li>Increasing \\(t\\) tightens the approximation; we \u201chome in\u201d on the boundary of feasibility.</li> </ul> <p>This is the core idea of interior-point methods.</p>"},{"location":"19_optimizationalgo/#963-interior-point-methods-in-practice","title":"9.6.3 Interior-point methods in practice","text":"<p>Interior-point methods:</p> <ul> <li>Are globally convergent for convex problems under mild assumptions (Slater\u2019s condition; see Chapter 8).</li> <li>Solve a series of smooth, strictly feasible subproblems.</li> <li>Use Newton-like steps to update primal (and, implicitly, dual) variables.</li> <li>Produce both primal and dual iterates \u2014 so they naturally produce a duality gap, which certifies how close you are to optimality (Chapter 8).</li> </ul> <p>Interior-point methods are the engine behind modern general-purpose convex solvers for:</p> <ul> <li>linear programs (LP),</li> <li>quadratic programs (QP),</li> <li>second-order cone programs (SOCP),</li> <li>semidefinite programs (SDP).</li> </ul> <p>They give high-accuracy answers and KKT-based optimality certificates. They are more expensive per iteration than gradient methods, but need far fewer iterations, and they handle fully general convex constraints.</p>"},{"location":"19_optimizationalgo/#98-choosing-the-right-method-in-practice","title":"9.8 Choosing the right method in practice","text":"<p>Let\u2019s summarise the chapter in the form of a decision guide.</p> <p>Case A. Smooth, unconstrained, very high dimensional. Example: logistic regression on millions of samples. Use: gradient descent or (better) accelerated gradient. Why: cheap iterations, easy to implement, scales.  </p> <p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy. Example: convex nonlinear fitting with well-behaved Hessian. Use: Newton or quasi-Newton. Why: quadratic (or near-quadratic) convergence near optimum.  </p> <p>Case C. Convex with simple feasible set \\(x \\in \\mathcal{X}\\) (box, ball, simplex). Use: projected gradient. Why: projection is easy, maintains feasibility at each step.  </p> <p>Case D. Composite objective \\(f(x) + R(x)\\) where \\(R\\) is nonsmooth (e.g. \\(\\ell_1\\), indicator of a constraint set). Use: proximal gradient. Why: prox handles nonsmooth/constraint part exactly each step.  </p> <p>Case E. General convex program with inequalities \\(g_i(x)\\le 0\\). Use: interior-point methods. Why: they solve smooth barrier subproblems via Newton steps and give primal\u2013dual certificates through KKT and duality (Chapters 7\u20138).  </p>"},{"location":"1_11_optimality/","title":"1 11 optimality","text":"<p>We now bring together geometry and calculus to characterize when a solution is optimal and to understand duality \u2013 a powerful perspective that every convex optimization problem comes paired with another (dual) problem providing bounds on the optimum.</p> <p>Unconstrained optimality: As noted, if \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex and differentiable, the necessary and sufficient condition for \\(x^*\\) to be a (global) minimizer is \\(\\nabla f(x^*) = 0\\). Geometrically, this means the tangent hyperplane is flat; algebraically, there\u2019s no descent direction. If \\(f\\) is not differentiable, the condition generalizes to \\(0 \\in \\partial f(x^*)\\) \u2013 the origin must be in the subgradient set, meaning there are subgradients pointing every way, so no single direction of descent. These conditions are first-order optimality conditions.</p> <p>Constrained optimality (KKT conditions): Consider a problem \\(\\min f(x)\\) s.t. \\(h_i(x)=0\\) (for \\(i=1,\\dots,m\\) equality constraints) and \\(g_j(x) \\le 0\\) (for \\(j=1,\\dots,p\\) inequality constraints), with \\(f\\) convex, \\(h_i\\) affine, \\(g_j\\) convex. The Karush\u2013Kuhn\u2013Tucker (KKT) conditions give a set of equations and inequalities that characterize optimal \\(x^*\\) and associated dual variables \\(\\lambda_i\\) (for equalities) and \\(\\mu_j \\ge 0\\) (for inequalities). They are:</p> <ol> <li> <p>Primal feasibility: \\(h_i(x^*)=0\\) for all \\(i\\), and \\(g_j(x^*) \\le 0\\) for all \\(j\\). (i.e. \\(x^*\\) satisfies the original constraints)</p> </li> <li> <p>Dual feasibility: \\(\\mu_j \\ge 0\\) for all \\(j\\). (Lagrange multipliers for inequalities are nonnegative)</p> </li> <li> <p>Stationarity: \\(\\nabla f(x^*) + \\sum_i \\lambda_i \\nabla h_i(x^*) + \\sum_j \\mu_j \\nabla g_j(x^*) = 0\\). This is essentially \\(\\nabla_x \\mathcal{L}(x^*,\\lambda,\\mu) = 0\\), where \\(\\mathcal{L}(x,\\lambda,\\mu) = f(x)+\\sum_i \\lambda_i h_i(x) + \\sum_j \\mu_j g_j(x)\\) is the Lagrangian. It generalizes \u201cgradient = 0\u201d to account for constraint forces.</p> </li> <li> <p>Complementary slackness: \\(\\mu_j, g_j(x^*) = 0\\) for all \\(j\\). This means for each inequality, either the constraint is tight (\\(g_j(x^*)=0\\)) in which case \\(\\mu_j\\) can be positive, or the constraint is loose (\\(g_j(x^*)&lt;0\\)) in which case optimal \\(\\mu_j\\) must be 0. Essentially \\(\\mu_j\\) \u201cactivates\u201d only on active constraints.</p> </li> </ol> <p>If strong duality holds (which it does under convexity and Slater\u2019s condition \u2013 existence of a strictly feasible point), these conditions are necessary and sufficient for optimality. They are the heart of convex optimization theory. Geometrically, the stationarity condition says \\(\\nabla f(x^*)\\) is a linear combination of the constraint normals (\\(\\nabla h_i, \\nabla g_j\\)) with certain weights. This is exactly the intuitive picture: at optimum \\(x^*\\), if constraints that are active form surfaces, the gradient of \\(f\\) must lie in their span \u2013 otherwise, there\u2019d be a direction along the surface to decrease \\(f\\). The multipliers \\(\\lambda,\\mu\\) are those weights.</p> <p>Consider a simple case: \\(\\min f(x)\\) s.t. \\(a^T x = b\\). At optimum \\(x^*\\), we expect \\(\\nabla f(x^)\\) to be orthogonal to the feasible plane (or else we could move along the plane and decrease \\(f\\)). The constraint normal is \\(a\\). So we expect \\(\\nabla f(x^*) = \\lambda a\\) for some \\(\\lambda\\). That is the stationarity KKT condition in this case, and \\(\\lambda\\) is the Lagrange multiplier. It measures how much increasing \\(b\\) would increase the optimal value (sensitivity).</p> <p>For inequality, say one constraint \\(g_1(x)\\le0\\) active. Then at optimum, \\(\\nabla f(x^) = \\mu_1 \\nabla g_1(x^)\\). If the constraint is not active, \\(\\mu_1 = 0\\) (since gradient of \\(f\\) has no reason to align with that constraint as it\u2019s not \u201cbinding\u201d).</p> <p>Dual problem and interpretation: The Lagrangian \\(\\mathcal{L}(x,\\lambda,\\mu)\\) introduces these multipliers. The Lagrange dual function is \\(q(\\lambda,\\mu) = \\inf_x \\mathcal{L}(x,\\lambda,\\mu)\\). For convex problems, one can often swap inf and sup and get \\(\\max_{\\lambda,\\mu \\ge 0} q(\\lambda,\\mu)\\) as the dual problem, which is concave maximization (or convex minimization after sign flip). The dual problem variables \\((\\lambda,\\mu)\\) can be thought of as setting prices for constraint violations. The dual function \\(q(\\lambda,\\mu)\\) gives for each \\((\\lambda,\\mu)\\) a lower bound on the primal optimum \\(p^*\\) (weak duality: \\(q(\\lambda,\\mu) \\le p^*\\)). Strong duality means \\(\\max_{\\lambda,\\mu\\ge0} q(\\lambda,\\mu) = p^*\\). At optimum, the maximizing \\((\\lambda^,\\mu^*)\\) are optimal dual variables. They often have economic interpretation: e.g. in resource allocation, \\(\\mu_j\\) is the shadow price of resource \\(j\\) (how much objective would worsen if resource \\(j\\) capacity is slightly tightened). Complementary slackness then says if a resource isn\u2019t fully used, its price is zero (it\u2019s not scarce so extra of it has no value).</p> <p>Geometrically, the dual variables define a hyperplane (through the origin in \\(\\mathbb{R}^n\\) dual space) \\(h(\\lambda,\\mu)(x) = \\sum_i \\lambda_i h_i(x) + \\sum_j \\mu_j g_j(x)\\) that supports the epigraph of \\(f\\) at the optimum. In fact, the stationary condition is exactly that \\(\\nabla f(x^) + \\sum_i \\lambda_i \\nabla h_i(x^) + \\sum_j \\mu_j \\nabla g_j(x^) = 0\\), which implies \\(\\nabla (f + \\sum_i \\lambda_i h_i + \\sum_j \\mu_j g_j)(x^*)=0\\). Since for feasible \\(x\\), \\(h_i(x)=0\\) and \\(g_j(x)\\le0\\), this means at \\(x^*\\), \\(f(x) \\ge f(x^*) + \\nabla f(x^*)^T (x-x^*)\\) and similarly linear terms for constraints give \\(0 \\ge \\lambda^T h(x^*) + \\mu^T g(x^) +\\) linear terms. Combining these weighted by \\(\\lambda,\\mu\\) yields an inequality \\(f(x) \\ge f(x^*) + (\\text{some linear form})\\) for all feasible \\(x\\) that matches with \\(f(x^*)\\) at \\(x^*\\). This linear form is precisely the dual hyperplane that touches \\(f\\) at \\(x^*\\).</p> <p>More concretely, for each active inequality \\(g_j\\), the dual \\(\\mu_j\\) can be seen as the slope of the supporting line to \\(f\\) along that constraint boundary. If we relax a constraint by a small \\(\\epsilon\\) (making feasible region slightly bigger if \\(\\mu_j&gt;0\\)), the optimal value should not increase; in fact one can show \\(d p^*/d(\\text{constraint}j)|{\\epsilon=0} = -\\mu_j\\). This sensitivity relation (from the envelope theorem or from dual solution) is why duals are important: they tell us marginal values of constraints.</p> <p>Complementary slackness intuition: If a constraint is not active, it doesn\u2019t affect the optimum, so its price \\(\\mu_j\\) should be zero. Conversely, if \\(\\mu_j&gt;0\\), it indicates a \u201cpush back\u201d from that constraint, meaning the solution is exactly on that constraint. Complementary slackness encodes this mutual dependency, reducing the KKT equations significantly (active set can be identified if one guesses which \\(\\mu_j&gt;0\\)).</p> <p>Strong duality and no duality gap: For convex problems satisfying Slater\u2019s condition (there exists a strictly interior feasible point for inequality constraints), we have zero duality gap (\\(p^* = d^*\\)). This is very useful: solving the dual (often easier, smaller dimensional or more separable) yields the primal solution indirectly. Sometimes the dual can be solved analytically when primal can\u2019t. Also, stopping criteria for iterative solvers often involve duality gap (if we have a primal and dual feasible solution with nearly equal objective, we\u2019re close to optimal).</p> <p>For example, in support vector machines (SVMs), one solves the dual problem (which is easier as a QP in dual variables bounded between 0 and C, with one equality constraint) and then recovers the primal (weights vector) from a weighted combination of support vectors (those with nonzero dual \\(\\alpha_i\\)). The support vectors are exactly the data points corresponding to active constraints (they lie on the margin, making the inequality \\(y_i(w^T x_i + b) - 1 \\le 0\\) tight). Their dual \\(\\alpha_i\\) are the support vector weights.</p> <p>Economic and geometric summary: The primal problem asks \u201cwhat is the smallest loss for a feasible plan?\u201d The dual asks \u201cwhat is the largest reward for a certifying combination of constraints?\u201d. At optimum, both meet. The dual variables are like a witness proving optimality: they provide a lower bound that matches the primal upper bound. For convex optimization, constructing dual variables that satisfy KKT is often how you prove something is optimal. E.g. for linear programming, complementary slackness can identify optimal basis, etc.</p> <p>Finally, Slater\u2019s condition: if there is a strictly feasible \\(x\\) (s.t. \\(h_i(x)=0\\), \\(g_j(x)&lt;0\\) for all \\(j\\)), then strong duality holds. If not, there can be a gap (though for linear problems there is always either a solution or an extreme ray proving infeasibility \u2013 dual helps with that too by Farkas\u2019 lemma). Slater\u2019s condition also allows us to derive KKT by Lagrange saddle-point arguments.</p> <p>In conclusion, optimality conditions and duality give us an alternative \u201cdual view\u201d of convex optimization: instead of directly searching in primal variable \\(x\\), we can think in terms of dual variables that measure constraint trade-offs. This viewpoint often simplifies problems (converting constrained problems to unconstrained duals), provides sensitivity analysis, and deepens understanding: every constraint induces a \u201cforce\u201d at optimum, and every unconstrained direction at optimum must have zero force (gradient zero). The harmony of these forces is succinctly captured by the KKT conditions, which can be seen as the equations of equilibrium for the optimization problem.</p>"},{"location":"1_8a_epigraphs/","title":"A.5 Epigraphs and Convexity","text":""},{"location":"1_8a_epigraphs/#a5-epigraphs-and-convexity","title":"A.5 Epigraphs and Convexity","text":"<p>Epigraphs are geometric tools that help us define and visualize convex functions. They serve as a bridge between convex functions and convex sets, allowing us to apply set-based reasoning (see [A.1 Convex Sets]) to analyze functions (see [A.4 Convex Functions]).</p>"},{"location":"1_8a_epigraphs/#1-what-is-an-epigraph","title":"1. What Is an Epigraph?","text":"<p>Let \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The epigraph of \\(f\\) is the set:</p> \\[ \\operatorname{epi}(f) = \\{ (x, t) \\in \\mathbb{R}^n \\times \\mathbb{R} \\;\\mid\\; f(x) \\le t \\}. \\] <ul> <li>Each point \\((x, t)\\) lies on or above the graph of \\(f\\).</li> <li>The epigraph lives in \\(\\mathbb{R}^{n+1}\\).</li> <li>It includes all superlevel points with respect to the function.</li> </ul>"},{"location":"1_8a_epigraphs/#2-intuition-and-visualization","title":"2. Intuition and Visualization","text":"<p>Think of the graph of \\(f\\) as a landscape. Then:</p> <ul> <li>The graph is the exact curve or surface traced out by \\(f\\).</li> <li>The epigraph is the solid region above the graph \u2014 like the sky above a hill.</li> </ul> <p>Example: If \\(f(x) = x^2\\), then \\(\\operatorname{epi}(f)\\) is the set of points above the parabola in 2D.</p> <p>The epigraph captures all points that are \u201csafe\u201d from below the function \u2014 a perspective crucial for defining convexity.</p>"},{"location":"1_8a_epigraphs/#3-convexity-from-epigraphs","title":"3. Convexity from Epigraphs","text":"<p>A function \\(f\\) is convex if and only if its epigraph is a convex set.</p> <p>This means: - If \\((x_1, t_1)\\) and \\((x_2, t_2)\\) lie in \\(\\operatorname{epi}(f)\\), then for any \\(\\theta \\in [0,1]\\), the convex combination      also lies in \\(\\operatorname{epi}(f)\\).</p> <p>This condition geometrically encodes Jensen\u2019s inequality and convexity.</p> <p>For background on convex sets and linear combinations, see [A.1 Convex Sets] and [A.2 Affine Sets and Hyperplanes].</p>"},{"location":"1_8a_epigraphs/#4-examples","title":"4. Examples","text":""},{"location":"1_8a_epigraphs/#example-1-convex-function","title":"Example 1: Convex Function","text":"<p>Let \\(f(x) = x^2\\).</p> <ul> <li>Epigraph: the set of points above the parabola.</li> <li>This set is convex.</li> <li>\\(\\Rightarrow\\) \\(f\\) is convex (see [A.4]).</li> </ul>"},{"location":"1_8a_epigraphs/#example-2-nonconvex-function","title":"Example 2: Nonconvex Function","text":"<p>Let \\(f(x) = -x^2\\).</p> <ul> <li>Epigraph: the region above the upside-down parabola.</li> <li>This set is not convex (you can draw line segments that dip below the curve).</li> <li>\\(\\Rightarrow\\) \\(f\\) is not convex.</li> </ul>"},{"location":"1_8a_epigraphs/#5-epigraph-form-in-optimization","title":"5. Epigraph Form in Optimization","text":"<p>Epigraphs allow us to reformulate optimization problems in a form that emphasizes convexity.</p> <p>Given:  </p> <p>We can write the epigraph form as:  </p> <p>This has advantages: - The feasible region becomes \\(\\operatorname{epi}(f)\\). - Makes it easier to check convexity of constraints. - Many solvers (e.g., CVXPY) require problems to be in epigraph-compatible form.</p> <p>This idea is a building block in disciplined convex programming \u2014 see [E.1 Modeling Convex Optimization Problems].</p>"},{"location":"1_8a_epigraphs/#6-why-epigraphs-matter","title":"6. Why Epigraphs Matter","text":"<p>Epigraphs unify: - Function analysis (shape of \\(f\\)) - Geometric reasoning (convex sets in higher dimensions) - Modeling flexibility (lift problems to introduce new variables) - Duality theory (used to characterize convex conjugates in [D.4 Dual Norms and Fenchel Conjugates])</p> <p>They also make nonsmooth functions (like norms or max operations) easier to analyze and optimize \u2014 see [A.6 Norms and Balls] and [A.7 Subgradients].</p>"},{"location":"1_8a_epigraphs/#summary-and-takeaways","title":"\u2705 Summary and Takeaways","text":"<ul> <li>The epigraph of a function is the region above its graph.</li> <li>A function is convex if and only if its epigraph is convex.</li> <li>Many optimization problems can be lifted into epigraph form to clarify structure and aid computation.</li> <li>Epigraphs provide an elegant bridge between function analysis and convex geometry.</li> </ul>"},{"location":"1a%20LP/","title":"Linear Programming (LP) Problem","text":""},{"location":"1a%20LP/#linear-programming-lp-problem","title":"Linear Programming (LP) Problem","text":"<p>Linear Programming (LP) is a cornerstone of convex optimization. It is used to find a decision vector \\(x\\) that minimizes a linear objective function subject to linear constraints:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where: - \\(x \\in \\mathbb{R}^n\\) \u2014 decision variables, - \\(c \\in \\mathbb{R}^n\\) \u2014 cost vector, - \\(d \\in \\mathbb{R}\\) \u2014 constant offset (shifts objective but does not affect optimizer), - \\(G \\in \\mathbb{R}^{m \\times n}, \\; h \\in \\mathbb{R}^m\\) \u2014 inequality constraints, - \\(A \\in \\mathbb{R}^{p \\times n}, \\; b \\in \\mathbb{R}^p\\) \u2014 equality constraints.  </p>"},{"location":"1a%20LP/#why-lps-are-convex-optimization-problems","title":"Why LPs Are Convex Optimization Problems","text":"<p>A problem is convex if: 1. The objective is convex, 2. The feasible region is convex.  </p>"},{"location":"1a%20LP/#convexity-of-the-objective","title":"Convexity of the Objective","text":"<p>The LP objective is affine:</p> \\[ f(x) = c^T x + d \\] <ul> <li>Affine functions are both convex and concave (zero curvature).  </li> <li>Thus, no spurious local minima: every local optimum is global.  </li> </ul>"},{"location":"1a%20LP/#convexity-of-the-feasible-set","title":"Convexity of the Feasible Set","text":"<ul> <li>Each inequality \\(a^T x \\leq b\\) defines a half-space \u2014 convex.  </li> <li>Each equality \\(a^T x = b\\) defines a hyperplane \u2014 convex.  </li> <li>The intersection of convex sets is convex.  </li> </ul> <p>Hence, the feasible region is a convex polyhedron.</p>"},{"location":"1a%20LP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>Inequalities act like flat walls, keeping feasible points on one side.  </li> <li>Equalities act like flat sheets, slicing through space.  </li> <li>The feasible region is a polyhedron (possibly unbounded).  </li> <li>LP solutions always occur at a vertex (extreme point) of this polyhedron \u2014 this fact powers the simplex algorithm.  </li> </ul>"},{"location":"1a%20LP/#canonical-and-standard-forms","title":"Canonical and Standard Forms","text":"<p>LPs are often reformulated for theory and solvers:</p> <ul> <li>Canonical form (minimization):</li> </ul> \\[ \\min \\; c^T x \\quad \\text{s.t. } A x = b, \\; x \\geq 0 \\] <ul> <li>Standard form (maximization):</li> </ul> \\[ \\max \\; c^T x \\quad \\text{s.t. } A x \\leq b, \\; x \\geq 0 \\] <p>Any LP can be transformed into one of these forms via slack variables and variable splitting.</p>"},{"location":"1a%20LP/#duality-in-linear-programming","title":"Duality in Linear Programming","text":"<p>Every LP has a dual problem:</p> <ul> <li>Primal (minimization):</li> </ul> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } Gx \\leq h, \\; A x = b \\] <ul> <li>Dual:</li> </ul> \\[ \\max_{\\lambda, \\nu} \\; -h^T \\lambda + b^T \\nu \\quad \\text{s.t. } G^T \\lambda + A^T \\nu = c, \\; \\lambda \\geq 0 \\]"},{"location":"1a%20LP/#properties","title":"Properties:","text":"<ul> <li>Weak duality: Dual objective \\(\\leq\\) Primal objective.  </li> <li>Strong duality: Holds under mild conditions (Slater\u2019s condition).  </li> <li>Complementary slackness provides optimality certificates.  </li> </ul> <p>Duality underpins modern algorithms like interior-point methods.</p>"},{"location":"1a%20LP/#robust-linear-programming-rlp","title":"Robust Linear Programming (RLP)","text":"<p>In many applications, the LP data (\\(c, A, G, b, h\\)) are uncertain due to noise, estimation errors, or worst-case planning requirements. Robust Optimization handles this by requiring constraints to hold for all possible realizations of the uncertain parameters within a given uncertainty set.</p>"},{"location":"1a%20LP/#general-robust-lp-formulation","title":"General Robust LP Formulation","text":"<p>Consider an uncertain LP:</p> \\[ \\min_{x} \\; c^T x \\quad \\text{s.t. } G(u) x \\leq h(u), \\quad \\forall u \\in \\mathcal{U} \\] <ul> <li>\\(\\mathcal{U}\\): uncertainty set (polyhedron, ellipsoid, box, etc.)  </li> <li>\\(u\\): uncertain parameters affecting \\(G, h\\).  </li> </ul> <p>The robust counterpart requires feasibility for all \\(u \\in \\mathcal{U}\\).</p>"},{"location":"1a%20LP/#box-uncertainty-interval-uncertainty","title":"Box Uncertainty (Interval Uncertainty)","text":"<p>Suppose \\(G = G_0 + \\Delta G\\), with each row uncertain in a box set:</p> \\[ \\{ g_i : g_i = g_i^0 + \\delta, \\; \\|\\delta\\|_\\infty \\leq \\rho \\} \\] <p>Robust constraint:</p> \\[ g_i^T x \\leq h_i, \\quad \\forall g_i \\in \\mathcal{U} \\] <p>This is equivalent to:</p> \\[ g_i^{0T} x + \\rho \\|x\\|_1 \\leq h_i \\] <p>Thus, a robust LP under box uncertainty is still a deterministic convex program (LP with additional \\(\\ell_1\\) terms).</p>"},{"location":"1a%20LP/#ellipsoidal-uncertainty","title":"Ellipsoidal Uncertainty","text":"<p>If uncertainty lies in an ellipsoid:</p> \\[ \\mathcal{U} = \\{ g : g = g^0 + Q^{1/2} u, \\; \\|u\\|_2 \\leq 1 \\} \\] <p>then the robust counterpart becomes:</p> \\[ g^{0T} x + \\| Q^{1/2} x \\|_2 \\leq h \\] <p>This is a Second-Order Cone Program (SOCP), still convex but more general than LP.</p>"},{"location":"1a%20LP/#robust-objective","title":"Robust Objective","text":"<p>When cost vector \\(c\\) is uncertain in \\(\\mathcal{U}_c\\):</p> \\[ \\min_{x} \\max_{c \\in \\mathcal{U}_c} c^T x \\] <ul> <li>If \\(\\mathcal{U}_c\\) is a box: inner max = \\(c^T x + \\rho \\|x\\|_1\\) </li> <li>If \\(\\mathcal{U}_c\\) is ellipsoidal: inner max = \\(c^T x + \\|Q^{1/2} x\\|_2\\) </li> </ul> <p>Thus, robust objectives often introduce regularization-like terms.  </p>"},{"location":"1a%20LP/#applications-of-robust-lp","title":"Applications of Robust LP","text":"<ul> <li>Supply chain optimization: demand uncertainty \u2192 robust inventory policies.  </li> <li>Finance: portfolio selection under uncertain returns.  </li> <li>Energy systems: robust scheduling under uncertain loads.  </li> <li>AI/ML: adversarial optimization, distributionally robust ML training.  </li> </ul>"},{"location":"1a%20LP/#how-lp-scales-in-practice","title":"How LP Scales in Practice","text":""},{"location":"1a%20LP/#polynomial-time-solvability","title":"Polynomial-Time Solvability","text":"<ul> <li>LPs can be solved in polynomial time using Interior-Point Methods (IPMs).  </li> <li>For an LP with \\(n\\) variables and \\(m\\) constraints, classical IPM complexity is roughly:</li> </ul> \\[ O((n+m)^3) \\] <ul> <li>But real-world performance depends on sparsity and problem structure. Sparse LPs are often solved in nearly linear time with specialized solvers.</li> </ul>"},{"location":"1a%20LP/#solver-ecosystem","title":"Solver Ecosystem","text":"<ul> <li>Commercial solvers: Gurobi, CPLEX, Mosek \u2014 highly optimized, exploit sparsity and parallelism, support warm-starts. These dominate large-scale industrial and financial problems.  </li> <li>Open-source solvers: HiGHS, GLPK, SCIP \u2014 robust for moderate problems, widely integrated into Python/Julia (via PuLP, Pyomo, CVXPY).  </li> <li>ML integration: CVXPY and PyTorch integrations make LP-based optimization easily callable inside ML pipelines.  </li> </ul>"},{"location":"1a%20LP/#algorithmic-tradeoffs","title":"Algorithmic Tradeoffs","text":"<ul> <li>Simplex method: moves along vertices of the feasible polyhedron.  </li> <li>Often very fast in practice, though exponential in theory.  </li> <li>Warm-starts make it excellent for iterative ML problems.  </li> <li>Interior-Point Methods (IPMs): follow a central path through the feasible region.  </li> <li>Polynomial worst-case guarantees.  </li> <li>Very robust to degeneracy, well-suited to dense problems.  </li> <li>First-order and decomposition methods:  </li> <li>ADMM, primal-dual splitting, stochastic coordinate descent.  </li> <li>Scale to massive LPs with billions of variables.  </li> <li>Sacrifice exactness for approximate but usable solutions.  </li> </ul>"},{"location":"1a%20LP/#comparison-of-lp-solvers","title":"Comparison of LP Solvers","text":"Method Complexity (theory) Scaling in practice Strengths Weaknesses ML/Engineering Use Cases Simplex Worst-case exponential Very fast in practice (near-linear for sparse LPs) Supports warm-starts, excellent for re-solving May stall on degenerate problems Iterative ML models, resource allocation, network flow Interior-Point (IPM) \\(O((n+m)^3)\\) Handles millions of variables if sparse Polynomial guarantees, robust, finds central solutions Memory-heavy (factorization of large matrices) Large dense LPs, convex relaxations in ML, finance First-order methods Sublinear (per iteration) Scales to billions of variables Memory-efficient, parallelizable Only approximate solutions MAP inference in CRFs, structured SVMs, massive embeddings Decomposition methods Problem-dependent Linear or near-linear scaling when structure exploited Breaks huge problems into smaller ones Requires separable structure Supply chain optimization, distributed training, scheduling"},{"location":"1a%20LP/#solving-large-scale-lps-in-ml-and-engineering","title":"Solving Large-Scale LPs in ML and Engineering","text":"<p>When problem sizes explode (e.g., \\(10^8\\) variables in embeddings or large-scale resource scheduling), standard solvers may fail due to memory or time.</p>"},{"location":"1a%20LP/#strategies","title":"Strategies","text":"<ul> <li>Decomposition methods:  </li> <li>Dantzig\u2013Wolfe, Benders, Lagrangian relaxation break problems into subproblems solved iteratively.  </li> <li>Column generation:  </li> <li>Introduces only a subset of variables initially, generating new ones as needed.  </li> <li>Stochastic and online optimization:  </li> <li>Replaces full LP solves with SGD-like updates, used in ML training pipelines.  </li> <li>Approximate relaxations:  </li> <li>In structured ML, approximate LP solutions often suffice (e.g., in structured prediction tasks).  </li> </ul>"},{"location":"1a%20LP/#ml-perspective","title":"ML Perspective","text":"<ul> <li>Structured prediction: LP relaxations approximate inference in CRFs, structured SVMs.  </li> <li>Adversarial robustness: Worst-case perturbation problems often reduce to LP relaxations, especially under \\(\\ell_\\infty\\) constraints.  </li> <li>Fairness: Linear constraints encode fairness requirements inside risk minimization objectives.  </li> <li>Large-scale systems: Recommender systems, resource allocation, energy scheduling \u2192 decomposition + approximate LP solvers.  </li> </ul>"},{"location":"1a%20LP/#where-lp-struggles-failure-modes","title":"Where LP Struggles (Failure Modes)","text":"<p>Despite its power, LPs face limitations:</p> <ol> <li>Nonlinearities </li> <li>Many ML objectives (e.g., log-likelihood, quadratic loss) are nonlinear.  </li> <li> <p>LP relaxations may be loose, requiring QP, SOCP, or nonlinear solvers.  </p> </li> <li> <p>Integrality </p> </li> <li>LP cannot enforce discrete decisions.  </li> <li> <p>Mixed-Integer Linear Programs (MILPs) are NP-hard, limiting scalability.  </p> </li> <li> <p>Uncertainty </p> </li> <li>Classical LP assumes perfect knowledge of data.  </li> <li> <p>Real problems require Robust LP or Stochastic LP.  </p> </li> <li> <p>Numerical conditioning </p> </li> <li>Poorly scaled coefficients lead to solver instability.  </li> <li> <p>Always normalize inputs for ML-scale LPs.  </p> </li> <li> <p>Memory bottlenecks </p> </li> <li>IPMs require factorizing large dense matrices \u2014 infeasible for extremely large-scale ML problems.  </li> </ol>"},{"location":"1b%20QP/","title":"Quadratic Programming (QP) Problem","text":""},{"location":"1b%20QP/#quadratic-programming-qp-problem","title":"Quadratic Programming (QP) Problem","text":"<p>Quadratic Programming (QP) is an optimization framework that generalizes Linear Programming by allowing a quadratic objective function, while keeping the constraints linear. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q x + c^T x + d \\\\ \\text{subject to} \\quad &amp; G x \\leq h \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector to be optimized.  </li> <li>\\(Q \\in \\mathbb{R}^{n \\times n}\\) \u2014 the Hessian matrix defining the curvature of the objective.  </li> <li>\\(c \\in \\mathbb{R}^n\\) \u2014 the linear cost term.  </li> <li>\\(d \\in \\mathbb{R}\\) \u2014 a constant offset (does not affect the minimizer\u2019s location).  </li> <li>\\(G \\in \\mathbb{R}^{m \\times n}\\), \\(h \\in \\mathbb{R}^m\\) \u2014 inequality constraints \\(Gx \\leq h\\).  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints \\(Ax = b\\).  </li> </ul>"},{"location":"1b%20QP/#why-qps-can-be-convex-optimization-problems","title":"Why QPs Can Be Convex Optimization Problems","text":"<p>Whether a QP is convex depends on one key condition:</p> <ul> <li>Convex QP: The Hessian \\(Q\\) is positive semidefinite (\\(Q \\succeq 0\\)).  </li> <li>Nonconvex QP: The Hessian has negative eigenvalues (some directions curve downward).</li> </ul>"},{"location":"1b%20QP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QP objective is:</p> \\[ f(x) = \\frac{1}{2} x^T Q x + c^T x + d \\] <p>This is a quadratic function, which is:</p> <ul> <li>Convex if \\(Q \\succeq 0\\) (all curvature is flat or bowl-shaped).</li> <li>Strictly convex if \\(Q \\succ 0\\) (curvature is strictly bowl-shaped, ensuring a unique minimizer).</li> <li>Nonconvex if \\(Q\\) has negative eigenvalues (some directions slope downward).</li> </ul> <p>The gradient and Hessian are:</p> \\[ \\nabla f(x) = Qx + c, \\quad \\nabla^2 f(x) = Q \\] <p>Since the Hessian is constant in QPs, checking convexity is straightforward:  </p> <p>Positive semidefinite Hessian \u2192 Convex objective \u2192 No spurious local minima.</p>"},{"location":"1b%20QP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>Exactly as in LPs:</p> <ul> <li>Each inequality \\(a^T x \\leq b\\) is a half-space (convex).  </li> <li>Each equality \\(a^T x = b\\) is a hyperplane (convex).  </li> </ul> <p>The feasible set:</p> \\[ \\mathcal{F} = \\{ x \\mid Gx \\leq h, \\quad Ax = b \\} \\] <p>is the intersection of convex sets, hence convex.</p>"},{"location":"1b%20QP/#the-feasible-set-is-a-convex-polyhedron","title":"The Feasible Set is a Convex Polyhedron","text":"<p>For convex QPs:</p> <ul> <li>The feasible region \\(\\mathcal{F}\\) is still a convex polyhedron (because constraints are the same as in LPs).  </li> <li>The objective is a convex quadratic \"bowl\" sitting over that polyhedron.  </li> <li>The optimal solution is where the bowl\u2019s lowest point touches the feasible polyhedron.</li> </ul>"},{"location":"1b%20QP/#geometric-intuition-visualizing-qp","title":"Geometric Intuition: Visualizing QP","text":"<ul> <li>In LP, the objective is a flat plane sliding over a polyhedron.  </li> <li>In convex QP, the objective is a curved bowl sliding over the same polyhedron.  </li> <li>If the bowl\u2019s center lies inside the feasible region, the optimum is at that center.  </li> <li>If not, the bowl \u201cleans\u201d against the polyhedron\u2019s faces, edges, or vertices \u2014 which is where the optimal solution lies.</li> </ul> <p>\u2705 Summary: A QP is a convex optimization problem if and only if \\(Q \\succeq 0\\). In that case:</p> <ul> <li>Objective: Convex quadratic.  </li> <li>Constraints: Linear, hence convex.  </li> <li>Feasible set: Convex polyhedron.  </li> <li>Solution: Found at the point in the feasible set where the quadratic surface reaches its lowest value.</li> </ul>"},{"location":"1c%20Least%20Square/","title":"Least Squares Problem","text":""},{"location":"1c%20Least%20Square/#least-squares-problem","title":"Least Squares Problem","text":"<p>Least Squares is one of the canonical convex optimization problems in statistics, machine learning, and signal processing. It seeks the vector \\(x \\in \\mathbb{R}^n\\) that minimizes the sum of squared errors:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\; \\|A x - b\\|_2^2 \\] <p>Where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) \u2014 data or measurement matrix, - \\(b \\in \\mathbb{R}^m\\) \u2014 observation or target vector, - \\(x \\in \\mathbb{R}^n\\) \u2014 decision vector (unknowns to estimate).  </p>"},{"location":"1c%20Least%20Square/#objective-expansion","title":"Objective Expansion","text":"<p>Expanding the squared norm:</p> \\[ \\|A x - b\\|_2^2 = (A x - b)^T (A x - b) = x^T A^T A x - 2 b^T A x + b^T b \\] <p>This is a quadratic convex function. In standard quadratic form:</p> \\[ f(x) = \\tfrac{1}{2} x^T Q x + c^T x + d \\] <p>with</p> \\[ Q = 2 A^T A, \\quad c = -2 A^T b, \\quad d = b^T b \\]"},{"location":"1c%20Least%20Square/#convexity","title":"Convexity","text":"<ul> <li>\\(Q = 2 A^T A \\succeq 0\\) since for any \\(z\\):  </li> </ul> \\[ z^T (A^T A) z = \\|A z\\|_2^2 \\geq 0 \\] <ul> <li>Hence LS is convex.  </li> <li>If \\(A\\) has full column rank, then \\(A^T A \\succ 0\\), making the problem strictly convex with a unique minimizer.  </li> </ul>"},{"location":"1c%20Least%20Square/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>When \\(m &gt; n\\) (overdetermined system), the equations \\(A x = b\\) may not be solvable.  </li> <li>LS finds \\(x^\\star\\) such that \\(A x^\\star\\) is the orthogonal projection of \\(b\\) onto the column space of \\(A\\).  </li> <li>The residual \\(r = b - A x^\\star\\) is orthogonal to \\(\\text{col}(A)\\):</li> </ul> \\[ A^T (b - A x^\\star) = 0 \\]"},{"location":"1c%20Least%20Square/#solutions","title":"Solutions","text":"<ul> <li>Normal Equations (full-rank case):</li> </ul> \\[ x^\\star = (A^T A)^{-1} A^T b \\] <ul> <li>General Case (possibly rank-deficient): The solution set is affine. The minimum-norm solution is given by the Moore\u2013Penrose pseudoinverse:</li> </ul> \\[ x^\\star = A^+ b \\] <ul> <li>Numerical Considerations: Normal equations can be ill-conditioned. In practice:</li> <li>Use QR decomposition or  </li> <li>SVD (stable, gives pseudoinverse).  </li> </ul>"},{"location":"1c%20Least%20Square/#constrained-least-squares-cls","title":"Constrained Least Squares (CLS)","text":"<p>Many practical problems require constraints on the solution. A general CLS formulation is:</p> \\[ \\begin{aligned} \\min_{x} \\quad &amp; \\|A x - b\\|_2^2 \\\\ \\text{s.t.} \\quad &amp; G x \\leq h \\\\ &amp; A_{\\text{eq}} x = b_{\\text{eq}} \\end{aligned} \\] <ul> <li>Objective: convex quadratic.  </li> <li>Constraints: linear.  </li> <li>Therefore: CLS is always a Quadratic Program (QP).</li> </ul>"},{"location":"1c%20Least%20Square/#example-1-wear-and-tear-allocation-cls-with-inequalities","title":"Example 1: Wear-and-Tear Allocation (CLS with Inequalities)","text":"<p>Suppose a landlord models annual apartment wear-and-tear costs as:</p> \\[ c_t \\approx a t + b, \\quad t = 1,\\dots,n \\] <p>with parameters \\(x = [a, b]^T\\).  </p> <p>CLS formulation:</p> \\[ \\min_{a,b} \\sum_{t=1}^n (a t + b - c_t)^2 \\] <p>Constraints (practical feasibility):</p> <ul> <li>Costs cannot be negative: </li> </ul> <p>This yields a CLS problem with linear inequality constraints, hence a QP.  </p> <p>-</p>"},{"location":"1c%20Least%20Square/#example-2-energy-consumption-fitting-cls-with-box-constraints","title":"\ud83d\udca1 Example 2: Energy Consumption Fitting (CLS with Box Constraints)","text":"<p>Suppose we fit energy usage from appliance data:  </p> <ul> <li>\\(A \\in \\mathbb{R}^{m \\times n}\\) usage matrix,  </li> <li>\\(b \\in \\mathbb{R}^m\\) observed energy bills.  </li> </ul> <p>CLS formulation:</p> \\[ \\min_{x} \\|A x - b\\|^2 \\] <p>Constraints: each appliance has a usage cap:  </p> \\[ 0 \\leq x_i \\leq u_i, \\quad i = 1,\\dots,n \\] <p>This is a QP with box constraints, often solved efficiently by projected gradient or interior-point methods.  </p>"},{"location":"1c%20Least%20Square/#regularized-least-squares-ridge-regression","title":"Regularized Least Squares (Ridge Regression)","text":"<p>A common extension in ML is regularized LS, e.g., ridge regression:</p> \\[ \\min_x \\|A x - b\\|^2 + \\lambda \\|x\\|_2^2 \\] <ul> <li>Equivalent to CLS with a quadratic penalty on \\(x\\).  </li> <li>Ensures uniqueness even if \\(A\\) is rank-deficient.  </li> <li>Solution:</li> </ul> \\[ x^\\star = (A^T A + \\lambda I)^{-1} A^T b \\]"},{"location":"1d%20QCQP/","title":"Quadratically Constrained Quadratic Programming (QCQP) Problem","text":""},{"location":"1d%20QCQP/#quadratically-constrained-quadratic-programming-qcqp-problem","title":"Quadratically Constrained Quadratic Programming (QCQP) Problem","text":"<p>A Quadratically Constrained Quadratic Program (QCQP) is an optimization problem in which both the objective and the constraints can be quadratic functions. Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\\\ \\text{subject to} \\quad &amp; \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0, \\quad i = 1, \\dots, m \\\\ &amp; A x = b \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(Q_0, Q_i \\in \\mathbb{R}^{n \\times n}\\) \u2014 symmetric matrices defining curvature of the objective and constraints.  </li> <li>\\(c_0, c_i \\in \\mathbb{R}^n\\) \u2014 linear terms in the objective and constraints.  </li> <li>\\(d_0, d_i \\in \\mathbb{R}\\) \u2014 constant offsets.  </li> <li>\\(A \\in \\mathbb{R}^{p \\times n}\\), \\(b \\in \\mathbb{R}^p\\) \u2014 equality constraints (linear).  </li> </ul>"},{"location":"1d%20QCQP/#why-qcqps-can-be-convex-optimization-problems","title":"Why QCQPs Can Be Convex Optimization Problems","text":"<p>QCQPs are not automatically convex \u2014 convexity requires specific conditions:</p> <ol> <li> <p>Objective convexity: \\(Q_0 \\succeq 0\\) (positive semidefinite Hessian for the objective).</p> </li> <li> <p>Constraint convexity:    For each inequality constraint \\(i\\), \\(Q_i \\succeq 0\\) so that  defines a convex set.</p> </li> <li> <p>Equality constraints:    Must be affine (linear), e.g., \\(A x = b\\).</p> </li> </ol>"},{"location":"1d%20QCQP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<p>The QCQP objective:</p> \\[ f_0(x) = \\frac{1}{2} x^T Q_0 x + c_0^T x + d_0 \\] <p>is convex iff \\(Q_0 \\succeq 0\\).</p>"},{"location":"1d%20QCQP/#2-convexity-of-constraints","title":"2. Convexity of Constraints","text":"<p>A single quadratic constraint:</p> \\[ f_i(x) = \\frac{1}{2} x^T Q_i x + c_i^T x + d_i \\leq 0 \\] <p>defines a convex feasible set iff \\(Q_i \\succeq 0\\).  </p> <p>If any \\(Q_i\\) is not positive semidefinite, the constraint set becomes nonconvex, and the overall problem is nonconvex.</p>"},{"location":"1d%20QCQP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>If all \\(Q_i \\succeq 0\\), inequality constraints define convex quadratic regions (ellipsoids, elliptic cylinders, or half-spaces).</li> <li>Equality constraints \\(A x = b\\) cut flat slices through these regions.</li> <li>The feasible set is the intersection of convex quadratic sets and affine sets \u2014 hence convex.</li> </ul>"},{"location":"1d%20QCQP/#geometric-intuition-visualizing-qcqp","title":"Geometric Intuition: Visualizing QCQP","text":"<ul> <li>In QP, only the objective is curved; constraints are flat.  </li> <li>In QCQP, constraints can also be curved \u2014 forming shapes like ellipsoids or paraboloids.  </li> <li>Convex QCQPs look like a \u201cbowl\u201d objective contained within (or pressed against) curved convex walls.  </li> <li>Nonconvex QCQPs can have holes or disconnected regions, making them much harder to solve.</li> </ul> <p>\u2705 Summary: A QCQP is a convex optimization problem if and only if:</p> <ul> <li>\\(Q_0 \\succeq 0\\) (objective convexity), and  </li> <li>\\(Q_i \\succeq 0\\) for all \\(i\\) (each quadratic inequality constraint convex), and  </li> <li>All equality constraints are affine.  </li> </ul> <p>When these hold: - Objective: Convex quadratic. - Constraints: Convex quadratic or affine. - Feasible set: Intersection of convex sets (can be curved). - Solution: Found where the objective\u2019s minimum touches the convex feasible region.</p>"},{"location":"1e%20SOCP/","title":"Second-Order Cone Programming (SOCP) Problem","text":""},{"location":"1e%20SOCP/#second-order-cone-programming-socp-problem","title":"Second-Order Cone Programming (SOCP) Problem","text":"<p>Second-Order Cone Programming (SOCP) is a class of convex optimization problems that generalizes Linear and (certain) Quadratic Programs by allowing constraints involving second-order (quadratic) cones.  </p> <p>Formally, the problem is:</p> \\[ \\begin{aligned} \\text{minimize} \\quad &amp; f^T x \\\\ \\text{subject to} \\quad &amp; \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i, \\quad i = 1, \\dots, m \\\\ &amp; F x = g \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) \u2014 the decision vector.  </li> <li>\\(f \\in \\mathbb{R}^n\\) \u2014 the linear objective coefficients.  </li> <li>\\(A_i \\in \\mathbb{R}^{k_i \\times n}\\), \\(b_i \\in \\mathbb{R}^{k_i}\\) \u2014 define the affine transformation inside the norm for cone \\(i\\).  </li> <li>\\(c_i \\in \\mathbb{R}^n\\), \\(d_i \\in \\mathbb{R}\\) \u2014 define the affine term on the right-hand side.  </li> <li>\\(F \\in \\mathbb{R}^{p \\times n}\\), \\(g \\in \\mathbb{R}^p\\) \u2014 define linear equality constraints.</li> </ul>"},{"location":"1e%20SOCP/#the-second-order-quadratic-cone","title":"The Second-Order (Quadratic) Cone","text":"<p>A second-order cone in \\(\\mathbb{R}^k\\) is:</p> \\[ \\mathcal{Q}^k = \\left\\{ (u,t) \\in \\mathbb{R}^{k-1} \\times \\mathbb{R} \\ \\middle|\\ \\|u\\|_2 \\leq t \\right\\} \\] <p>Key properties: - Convex set. - Rotationally symmetric around the \\(t\\)-axis. - Contains all rays pointing \u201cupward\u201d inside the cone.</p>"},{"location":"1e%20SOCP/#why-socp-is-a-convex-optimization-problem","title":"Why SOCP is a Convex Optimization Problem","text":""},{"location":"1e%20SOCP/#1-convexity-of-the-objective","title":"1. Convexity of the Objective","text":"<ul> <li>The SOCP objective \\(f^T x\\) is affine.</li> <li>Affine functions are both convex and concave \u2014 no curvature.</li> </ul>"},{"location":"1e%20SOCP/#2-convexity-of-the-constraints","title":"2. Convexity of the Constraints","text":"<p>Each second-order cone constraint:</p> \\[ \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i \\] <p>is convex because: - The left-hand side \\(\\|A_i x + b_i\\|_2\\) is a convex function of \\(x\\). - The right-hand side \\(c_i^T x + d_i\\) is affine. - The set \\(\\{x \\mid \\|A_i x + b_i\\|_2 \\leq c_i^T x + d_i\\}\\) is a convex set.</p> <p>Equality constraints \\(F x = g\\) define a hyperplane, which is convex.</p> <p>Since the feasible region is the intersection of convex sets, it is convex.</p>"},{"location":"1e%20SOCP/#feasible-set-geometry","title":"Feasible Set Geometry","text":"<ul> <li>Each SOCP constraint defines a rotated or shifted cone in \\(x\\)-space.</li> <li>Equality constraints slice the space with flat hyperplanes.</li> <li>The feasible set is the intersection of these cones and hyperplanes.</li> </ul>"},{"location":"1e%20SOCP/#special-cases-of-socp","title":"Special Cases of SOCP","text":"<ul> <li>Linear Programs (LP): If all \\(A_i = 0\\), cone constraints reduce to linear inequalities.</li> <li>Certain Quadratic Programs (QP): Quadratic inequalities of the form \\(\\|Q^{1/2}x\\|_2 \\leq a^T x + b\\) can be rewritten as SOCP constraints.</li> <li>Norm Constraints: Bounds on \\(\\ell_2\\)-norms (e.g., \\(\\|x\\|_2 \\leq t\\)) are directly SOCP constraints.</li> </ul>"},{"location":"1e%20SOCP/#geometric-intuition","title":"Geometric Intuition","text":"<ul> <li>In LP, constraints are flat walls.</li> <li>In QP, objective is curved but constraints are flat.</li> <li>In SOCP, constraints themselves are curved (cone-shaped), allowing more modeling flexibility.</li> <li>The optimal solution is where the objective plane just \u201ctouches\u201d the feasible cone-shaped region.</li> </ul> <p>\u2705 Summary: - Objective: Affine (linear) \u2192 convex. - Constraints: Intersection of affine equalities and convex second-order cone inequalities. - Feasible set: Convex \u2014 shaped by cones and hyperplanes. - Power: Captures LPs, norm minimization, robust optimization, and some QCQPs. - Solution: Found efficiently by interior-point methods specialized for conic programming.</p>"},{"location":"1f%20GeometricInterpretation/","title":"1f GeometricInterpretation","text":"Feature LP QP QCQP SOCP Objective Linear: \\(c^T x\\) Quadratic: \\(\\frac{1}{2} x^T Q x + c^T x\\), convex if \\(Q \\succeq 0\\), indefinite \\(Q\\) \u2192 nonconvex Quadratic: \\(\\frac{1}{2} x^T Q_0 x + c_0^T x\\), convex if \\(Q_0 \\succeq 0\\), indefinite \u2192 nonconvex Linear: \\(c^T x\\), always convex Objective level sets Hyperplanes (flat, parallel) Quadrics: ellipsoids/paraboloids if \\(Q \\succeq 0\\), hyperboloids if indefinite Quadrics: ellipsoids/paraboloids; shape depends on \\(Q_0\\) definiteness Hyperplanes (flat, parallel) Constraints Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Linear: \\(A x \\le b\\) \u2192 half-spaces, flat Quadratic: \\(\\frac{1}{2} x^T Q_i x + c_i^T x \\le b_i\\) \u2192 curved surfaces; convex if \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Second-order cone: \\(\\|A_i x + b_i\\|_2 \\le c_i^T x + d_i\\) \u2192 convex, curved conic surfaces Feasible region Polyhedron (flat faces, convex) Polyhedron (flat faces, convex) Curved region; convex if all \\(Q_i \\succeq 0\\), otherwise possibly nonconvex Intersection of convex cones; curved, convex region Optimum location Vertex (extreme point of polyhedron) Face, edge, vertex, or interior if unconstrained minimizer feasible Boundary or interior; multiple local minima possible if nonconvex Boundary or interior; linear objective touches cone tangentially 2D Example Max \\(x_1 + 2x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 4\\) \u2192 polygon Min \\(x_1^2 + x_2^2 + x_1 + x_2\\), s.t. \\(x_1 \\ge 0\\), \\(x_2 \\ge 0\\), \\(x_1 + x_2 \\le 3\\) \u2192 polygon feasible, elliptical contours Min \\(x_1^2 + x_2^2\\), s.t. \\(x_1^2 + x_2^2 \\le 4\\), \\(x_1 + x_2 \\le 3\\) \u2192 circular + linear \u2192 curved feasible Min \\(x_1 + x_2\\), s.t. \\(\\sqrt{x_1^2 + x_2^2} \\le 2 - 0.5 x_1\\) \u2192 tilted cone 3D Example Max \\(x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 polyhedron Min \\(x_1^2 + x_2^2 + x_3^2 + x_1 + x_2 + x_3\\), s.t. \\(x_i \\ge 0\\), \\(x_1 + x_2 + x_3 \\le 4\\) \u2192 polyhedron + ellipsoid Min \\(x_1^2 + x_2^2 + x_3^2\\), s.t. \\(x_1^2 + x_2^2 + x_3^2 \\le 9\\), \\(x_1 + x_2 + x_3 \\le 5\\) \u2192 spherical + plane \u2192 curved feasible Min \\(x_1 + x_2 + x_3\\), s.t. \\(\\sqrt{x_1^2 + x_2^2 + x_3^2} \\le 4 - 0.5 x_3\\) \u2192 3D cone 4D Intuition Polytope + 3D hyperplane Polytope + 4D ellipsoid Curved 4D region + 4D ellipsoid; convex if \\(Q_0,Q_i \\succeq 0\\) 4D cone + 3D hyperplane Curvature Hint Flat objective / flat constraints Curved objective / flat constraints Curved objective / curved constraints Flat objective / curved constraints"},{"location":"1g%20GP/","title":"\ud83d\udcd8 Geometric Programming (GP)","text":""},{"location":"1g%20GP/#geometric-programming-gp","title":"\ud83d\udcd8 Geometric Programming (GP)","text":"<p>Geometric Programming (GP) is a flexible, widely used optimization class (communications, circuit design, resource allocation, control, ML model fitting). In its natural variable form it looks nonconvex, but \u2014 crucially \u2014 there is a canonical change of variables and a monotone transformation that converts a GP into a convex optimization problem.</p>"},{"location":"1g%20GP/#definitions-monomials-posynomials-and-the-standard-gp","title":"Definitions: monomials, posynomials, and the standard GP","text":"<p>Let \\(x=(x_1,\\dots,x_n)\\) with \\(x_i&gt;0\\).</p> <ul> <li> <p>A monomial (in GP terminology) is a function of the form  where \\(c&gt;0\\) and the exponents \\(a_i\\in\\mathbb{R}\\) (real exponents allowed).  </p> <p>Note: in GP literature \"monomial\" means positive coefficient times a power product (not to be confused with polynomial monomial which has nonnegative integer powers).</p> </li> <li> <p>A posynomial is a sum of monomials:  </p> </li> <li> <p>Standard (inequality) form of a geometric program:  where each \\(p_i\\) is a posynomial and each \\(m_j\\) is a monomial. (Any GP with other RHS values can be normalized to this form by dividing.)</p> </li> </ul>"},{"location":"1g%20GP/#why-gp-in-the-original-x-variables-is-not-convex","title":"\u2757 Why GP (in the original \\(x\\) variables) is not convex","text":"<ul> <li> <p>A monomial \\(m(x)=c x^a\\) (single variable) is convex on \\(x&gt;0\\) only for certain exponent ranges (e.g. \\(a\\le 0\\) or \\(a\\ge 1\\)). For \\(0&lt;a&lt;1\\) it is concave; for general real \\(a\\) it can be neither globally convex nor concave over \\(x&gt;0\\).   Example: \\(f(x)=x^{1/2}\\) (\\(0&lt;a&lt;1\\)) is concave on \\((0,\\infty)\\) (second derivative \\(=\\tfrac{1}{4}x^{-3/2}&gt;0\\)? \u2014 check sign; in fact \\(f''(x)= -\\tfrac{1}{4} x^{-3/2}&lt;0\\) showing concavity).</p> </li> <li> <p>A posynomial is a sum of monomials. Sums of nonconvex (or concave) terms are generally nonconvex. There is no general convexity guarantee for posynomials in the original variables \\(x\\).</p> </li> <li> <p>Therefore the objective \\(p_0(x)\\) and constraints \\(p_i(x)\\le 1\\) are not convex functions/constraints in \\(x\\), so the GP is not a convex program in the \\(x\\)-space.</p> </li> </ul> <p>Concrete counterexample (1D): take \\(p(x)=x^{1/2}+x^{-1}\\). The term \\(x^{1/2}\\) is concave on \\((0,\\infty)\\), \\(x^{-1}\\) is convex, and the sum is neither convex nor concave. One can find points \\(x_1,x_2\\) and \\(\\theta\\in(0,1)\\) that violate the convexity inequality.</p>"},{"location":"1g%20GP/#how-to-make-gp-convex-the-log-change-of-variables-and-log-transformation","title":"\u2705 How to make GP convex: the log-change of variables and log transformation","text":"<p>Key facts that enable convexification:</p> <ul> <li> <p>Monomials become exponentials of affine functions in log-variables.   Define \\(y_i = \\log x_i\\) (so \\(x_i = e^{y_i}\\)) and write \\(y=(y_1,\\dots,y_n)\\). For a monomial      we have      which is affine in \\(y\\).</p> </li> <li> <p>Posynomials become sums of exponentials of affine functions. For a posynomial      where \\(a_k\\) is the exponent-vector for the \\(k\\)th monomial and \\(y=\\log x\\).</p> </li> <li> <p>Taking the log of a posynomial yields a log-sum-exp function, i.e.      where \\(\\operatorname{LSE}(z_1,\\dots,z_K)=\\log\\!\\sum_{k} e^{z_k}\\).</p> </li> <li> <p>The log-sum-exp function is convex. Hence constraints of the form \\(p_i(x)\\le 1\\) become      i.e. a convex constraint in \\(y\\) because \\(\\log p_i(e^y)\\) is convex.</p> </li> <li> <p>Since \\(\\log(\\cdot)\\) is monotone, minimizing \\(p_0(x)\\) is equivalent to minimizing \\(\\log p_0(x)\\). Therefore one may transform the GP to the equivalent convex program in \\(y\\):</p> </li> </ul> <p>\\(\\(\\boxed{%   \\begin{aligned}   \\min_{y\\in\\mathbb{R}^n}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_0} c_{0k} e^{a_{0k}^T y}\\Big) \\\\   \\text{s.t.}\\quad &amp; \\log\\!\\Big(\\sum_{k=1}^{K_i} c_{ik} e^{a_{ik}^T y}\\Big) \\le 0,\\quad i=1,\\dots,m,\\\\   &amp; a_{j}^T y + \\log c_j = 0,\\quad \\text{(for each monomial equality } m_j(x)=1).   \\end{aligned}}\\)\\)</p> <p>This \\(y\\)-problem is convex: log-sum-exp objective/constraints are convex; monomial equalities are affine in \\(y\\).</p>"},{"location":"1g%20GP/#why-log-sum-exp-is-convex-brief-proof-via-hessian","title":"\ud83d\udd0d Why log-sum-exp is convex (brief proof via Hessian)","text":"<p>Let \\(g(y)=\\log\\!\\sum_{k=1}^K e^{u_k(y)}\\) with \\(u_k(y)=a_k^T y + b_k\\) (affine). Define  - Gradient:  - Hessian:  where \\(\\bar a=\\sum_k p_k a_k\\). The Hessian is a weighted covariance matrix of the vectors \\(a_k\\) (weights \\(p_k\\ge0\\)), hence PSD. Thus \\(g\\) is convex.</p>"},{"location":"1g%20GP/#monomials-as-affine-constraints-in-y","title":"\u2733\ufe0f Monomials as affine constraints in \\(y\\)","text":"<p>A monomial equality \\(c x^{a} = 1\\) becomes  an affine equality in \\(y\\). So monomial equality constraints become linear equalities after the log change.</p>"},{"location":"1g%20GP/#equivalence-and-solving-workflow","title":"\ud83d\udd01 Equivalence and solving workflow","text":"<ol> <li>Start with GP in \\(x&gt;0\\): minimize \\(p_0(x)\\) subject to posynomial constraints and monomial equalities.  </li> <li>Change variables: \\(y=\\log x\\) (domain becomes all \\(\\mathbb{R}^n\\)).  </li> <li>Apply log to posynomials (objective + inequality LHS). Because \\(\\log\\) is monotone increasing, inequalities maintain direction.  </li> <li>Solve the convex problem in \\(y\\) (log-sum-exp objective, convex constraints). Use interior-point or other convex solvers.  </li> <li>Recover \\(x^\\star = e^{y^\\star}\\).</li> </ol> <p>Because \\(x\\mapsto \\log x\\) is a bijection for \\(x&gt;0\\), solutions correspond exactly.</p>"},{"location":"1g%20GP/#worked-out-simple-example-2-variables","title":"\ud83d\udd27 Worked-out simple example (2 variables)","text":"<p>Original GP (standard form): </p> <p>Change variables: \\(y_1=\\log x_1,\\; y_2=\\log x_2\\).</p> <ul> <li>Transform terms:</li> <li>\\(3 x_1^{-1} = 3 e^{-y_1}\\) with \\(\\log\\) term \\(\\log 3 - y_1\\).</li> <li>\\(2 x_1 x_2 = 2 e^{y_1+y_2}\\) with \\(\\log\\) term \\(\\log 2 + y_1 + y_2\\).</li> <li>Constraint posynomial: \\(0.5 e^{-y_1} + e^{y_2}\\).</li> </ul> <p>Convex form (in \\(y\\)): </p> <p>Both objective and constraint are log-sum-exp functions (convex). Solve for \\(y^\\star\\) with a convex solver; then \\(x^\\star = e^{y^\\star}\\).</p>"},{"location":"1g%20GP/#numerical-implementation-remarks","title":"\u2699\ufe0f Numerical &amp; implementation remarks","text":"<ul> <li> <p>Domain requirement: GP requires \\(x_i&gt;0\\). The log transform only works on the positive orthant. If some variables can be zero, model reformulation (introducing small positive lower bounds) may be necessary.</p> </li> <li> <p>Normalization: Standard GPs use constraints \\(p_i(x)\\le 1\\). If you have \\(p_i(x) \\le t\\), divide by \\(t\\) to normalize.</p> </li> <li> <p>Numerical stability: Use the stable log-sum-exp implementation:      to avoid overflow/underflow.</p> </li> <li> <p>Solvers: After convexification the problem can be passed to generic convex solvers (CVX, CVXOPT, MOSEK, SCS, ECOS). Many solvers accept the log-sum-exp cone directly. Interior-point methods are effective on moderate-size GPs.</p> </li> <li> <p>Interpretation: The convexified problem is not an LP; it is a convex program with log-sum-exp terms (equivalently representable using exponential/relative entropy cones or by second-order cone approximations in some cases).</p> </li> </ul>"},{"location":"1g%20GP/#limitations-and-extensions","title":"\ud83d\udeab Limitations and extensions","text":"<ul> <li> <p>Signomials: If the problem contains negative coefficients (e.g. sums of monomials with arbitrary signs), it is a signomial program and the log transform does not yield a convex problem. Signomial programs are generally nonconvex and require local optimization or sequential convex approximations (e.g., successive convex approximation / condensation, branch-and-bound heuristics).</p> </li> <li> <p>Robust GP: Uncertainty in coefficients \\(c_k\\) or exponents \\(a_k\\) can sometimes be handled by robustification. If uncertainty is modeled multiplicatively (log-convex uncertainty sets), the robust counterpart often remains convex in \\(y\\); specifics depend on the uncertainty set shape (box \u2192 additional terms, ellipsoidal \u2192 conic forms). Robust GP is a rich topic \u2014 it frequently yields convex reformulations after log-transform if the uncertainty preserves log-convexity.</p> </li> <li> <p>Sparsity/exponents: Exponents \\(a_{ik}\\) need not be integers \u2014 positive real exponents are allowed. Sparsity in exponent matrix (\\(a_{ik}\\)) often leads to efficient implementations.</p> </li> </ul>"},{"location":"1g%20GP/#takeaway-summary-concise","title":"\u2705 Takeaway / Summary (concise)","text":"<ul> <li>GP in \\(x\\) is typically nonconvex because posynomials are not convex in \\(x\\).  </li> <li>Key transformation: \\(y=\\log x\\), and take \\(\\log\\) of posynomials \u2192 posynomial \\(\\mapsto\\) log-sum-exp of affine functions in \\(y\\).  </li> <li>Result: The GP becomes a convex optimization problem in \\(y\\): log-sum-exp objective/constraints (convex), and monomial equalities become linear equalities.  </li> <li>Solve in \\(y\\), then exponentiate to get \\(x\\).  </li> <li>Caveats: requires \\(x&gt;0\\); signomials (negative coefficients) remain nonconvex and need other techniques.</li> </ul> <p>If you want, I can: - provide a numerical, step-by-step example solving the worked GP above (with numbers) and show the solver output and \\(x^\\star\\); - show how robust multiplicative uncertainty in coefficients is handled in log domain; or - give a short code snippet (CVX/MOSEK or CVXPY) that constructs and solves the convexified GP.</p> <p>Which of those would you like next?</p>"},{"location":"20_advanced/","title":"10. Advanced Large-Scale and Structured Methods","text":""},{"location":"20_advanced/#chapter-10-advanced-large-scale-and-structured-methods","title":"Chapter 10: Advanced Large-Scale and Structured Methods","text":"<p>In Chapter 9 we focused on \u201cclassical convex solvers\u201d: gradient methods, accelerated methods, Newton and quasi-Newton methods, projected/proximal methods, and interior-point methods. Those are the canonical tools of convex optimisation.</p> <p>This chapter moves one step further.</p> <p>Here we study methods that: - exploit problem structure (sparsity, separability, block structure), - scale to extremely high dimensions, - or are widely used in practice for machine learning and signal processing \u2014 including in problems that are not convex.</p> <p>Some of these methods were first analysed in the convex setting (often with strong guarantees), and then adopted \u2014 sometimes recklessly \u2014 in the nonconvex world (training neural nets, matrix factorisation, etc.). You\u2019ll absolutely see them in modern optimisation and ML code.</p> <p>We\u2019ll cover: 1. Coordinate (block) descent, 2. Stochastic gradient and mini-batch methods, 3. ADMM (Alternating Direction Method of Multipliers), 4. Proximal coordinate / coordinate proximal variants, 5. Majorization\u2013minimization and iterative reweighted schemes.</p> <p>Throughout we\u2019ll emphasise: - When they are provably correct for convex problems, - Why people also use them in nonconvex problems.</p>"},{"location":"20_advanced/#101-coordinate-descent-and-block-coordinate-descent","title":"10.1 Coordinate descent and block coordinate descent","text":""},{"location":"20_advanced/#1011-idea","title":"10.1.1 Idea","text":"<p>Instead of updating all coordinates of \\(x\\) at once using a full gradient or Newton direction, we update one coordinate (or one block of coordinates) at a time, holding the others fixed.</p> <p>Suppose we want to minimise a convex function  and write \\(x = (x_1, x_2, \\dots, x_p)\\) in coordinates or blocks. Coordinate descent cycles through \\(i = 1,2,\\dots,p\\) and solves (or approximately solves)  </p> <p>In other words: update coordinate \\(i\\) by optimising over just that coordinate (or block), treating the rest as constants.</p>"},{"location":"20_advanced/#1012-why-this-can-be-fast","title":"10.1.2 Why this can be fast","text":"<ul> <li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li> <li>For problems with separable structure \u2014 e.g. sums over features, or regularisers like \\(\\|x\\|_1 = \\sum_i |x_i|\\) \u2014 the coordinate update is extremely cheap.</li> <li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li> </ul> <p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>"},{"location":"20_advanced/#1013-convergence-in-convex-problems","title":"10.1.3 Convergence in convex problems","text":"<p>For many convex, continuously differentiable problems with certain regularity (e.g. strictly convex objective, or convex plus separable nonsmooth terms), cyclic coordinate descent is guaranteed to converge to the global minimiser. There are also randomized versions that pick a coordinate uniformly at random, which often give cleaner expected-rate guarantees.</p> <p>For \\(\\ell_1\\)-regularised least squares, i.e.  each coordinate update becomes a scalar soft-thresholding step \u2014 so coordinate descent becomes an extremely efficient sparse regression solver.</p>"},{"location":"20_advanced/#1014-block-coordinate-descent","title":"10.1.4 Block coordinate descent","text":"<p>When coordinates are naturally grouped (for example, \\(x\\) is really \\((x^{(1)}, x^{(2)}, \\dots)\\) where each \\(x^{(j)}\\) is a vector of parameters for a submodule or layer), we generalise to block coordinate descent. Each step solves  </p> <p>Block coordinate descent is the backbone of many alternating minimisation schemes in signal processing, matrix factorisation, dictionary learning, etc.</p>"},{"location":"20_advanced/#1015-use-in-nonconvex-problems","title":"10.1.5 Use in nonconvex problems","text":"<p>Even when \\(F\\) is not convex, people still run block coordinate descent (under names like \u201calternating minimisation\u201d or \u201calternating least squares\u201d), because:</p> <ul> <li>each block subproblem might be convex even if the joint problem isn\u2019t,</li> <li>it is easy to implement,</li> <li>it often works \u201cwell enough\u201d in practice.</li> </ul> <p>You see this in low-rank matrix factorisation (recommender systems), where you fix all user factors and update item factors, then swap. There are no global guarantees in general (no convexity), but empirically it converges to useful solutions.</p> <p>So: - In convex settings \u2192 provable global convergence. - In nonconvex settings \u2192 heuristic that often finds acceptable stationary points.</p>"},{"location":"20_advanced/#102-stochastic-gradient-and-mini-batch-methods","title":"10.2 Stochastic gradient and mini-batch methods","text":""},{"location":"20_advanced/#1021-full-gradient-vs-stochastic-gradient","title":"10.2.1 Full gradient vs stochastic gradient","text":"<p>In Chapter 9, gradient descent uses the full gradient \\(\\nabla f(x)\\) at each step. In large-scale learning problems, \\(f\\) is almost always an average over data:  where \\(\\ell_i\\) is the loss on sample \\(i\\).</p> <p>Computing \\(\\nabla f(x)\\) exactly costs \\(O(N)\\) per step, which is huge.</p> <p>Stochastic Gradient Descent (SGD) replaces \\(\\nabla f(x)\\) with an unbiased estimate. At each iteration we:</p> <ol> <li>Sample \\(i\\) uniformly from \\(\\{1,\\dots,N\\}\\),</li> <li>Use \\(g_k = \\nabla \\ell_i(x_k)\\),</li> <li>Update     </li> </ol> <p>This is extremely cheap: one data point (or a small mini-batch) per step.</p>"},{"location":"20_advanced/#1022-convergence-in-convex-problems","title":"10.2.2 Convergence in convex problems","text":"<p>For convex problems, with diminishing step sizes \\(\\alpha_k\\), SGD converges to the global optimum in expectation, and more refined analyses show \\(O(1/\\sqrt{k})\\) suboptimality rates for general convex Lipschitz losses, improving to \\(O(1/k)\\) in strongly convex smooth cases with appropriate averaging.</p> <p>That is slower (per iteration) than deterministic gradient descent in theory, but each iteration is much cheaper. So SGD wins in wall-clock time for huge \\(N\\).</p>"},{"location":"20_advanced/#1023-momentum-adam-rmsprop-nonconvex-practice-convex-roots","title":"10.2.3 Momentum, Adam, RMSProp (nonconvex practice, convex roots)","text":"<p>In modern machine learning, methods like momentum SGD, Adam, RMSProp, Adagrad, etc., are used routinely to train enormous nonconvex models (deep networks). These are variations of first-order methods with:</p> <ul> <li>adaptive step sizes,</li> <li>running averages of squared gradients,</li> <li>momentum terms.</li> </ul> <p>While the most common use is for nonconvex problems, many of these methods (e.g. Adagrad-type adaptive steps, momentum acceleration) have their theoretical roots in convex optimisation and mirror-descent style analyses.</p> <p>So stochastic first-order methods are:</p> <ul> <li>rigorous for convex problems,</li> <li>widely used heuristically for nonconvex problems.</li> </ul>"},{"location":"20_advanced/#103-admm-alternating-direction-method-of-multipliers","title":"10.3 ADMM: Alternating Direction Method of Multipliers","text":"<p>ADMM is one of the most important algorithms in modern convex optimisation for structured problems. It is used constantly in signal processing, sparse learning, distributed optimisation, and large-scale statistical estimation.</p>"},{"location":"20_advanced/#1031-problem-form","title":"10.3.1 Problem form","text":"<p>ADMM solves problems of the form  where \\(f\\) and \\(g\\) are convex.</p> <p>This form appears everywhere:</p> <ul> <li>\\(f\\) is a data-fit term,</li> <li>\\(g\\) is a regulariser or constraint indicator,</li> <li>\\(Ax + Bz = c\\) ties them together.</li> </ul> <p>For example, LASSO can be written by introducing a copy variable and enforcing \\(x=z\\).</p>"},{"location":"20_advanced/#1032-augmented-lagrangian","title":"10.3.2 Augmented Lagrangian","text":"<p>ADMM applies the augmented Lagrangian method, which is like dual ascent but with a quadratic penalty on constraint violation. The augmented Lagrangian is  with dual variable (Lagrange multiplier) \\(y\\) and penalty parameter \\(\\rho&gt;0\\).</p>"},{"location":"20_advanced/#1033-the-admm-updates-two-block-case","title":"10.3.3 The ADMM updates (two-block case)","text":"<p>Iterate the following: 1. \\(x\\)-update:     (holding \\(z,y\\) fixed). 2. \\(z\\)-update:  3. Dual update: </p> <p>That is: optimise \\(x\\) given \\(z\\), optimise \\(z\\) given \\(x\\), then update the multiplier.</p>"},{"location":"20_advanced/#1034-why-admm-is-powerful","title":"10.3.4 Why ADMM is powerful","text":"<ul> <li>Each subproblem often becomes simple and separable:</li> <li>The \\(x\\)-update might be a least-squares or a smooth convex minimisation,</li> <li>The \\(z\\)-update might be a proximal operator (soft-thresholding, projection, etc.).</li> <li>You never have to solve the full coupled problem in one shot.</li> <li>ADMM is embarrassingly parallel / distributable: different blocks can be solved on different machines then averaged via the multiplier step.</li> </ul>"},{"location":"20_advanced/#1035-convergence","title":"10.3.5 Convergence","text":"<p>For convex \\(f\\) and \\(g\\), under mild assumptions (closed proper convex functions, some regularity), ADMM converges to a solution of the primal problem, and the dual variable \\(y^k\\) converges to an optimal dual multiplier (Boyd and Vandenberghe, 2004, Ch. 5; also classical ADMM literature).</p> <p>This is deeply tied to duality (Chapter 8): ADMM is best understood as a method of solving the dual with decomposability, but returning primal iterates along the way.</p>"},{"location":"20_advanced/#1036-use-in-nonconvex-problems","title":"10.3.6 Use in nonconvex problems","text":"<p>In practice, ADMM is often extended to nonconvex problems by simply \u201cpretending it\u2019s fine.\u201d Each subproblem is solved anyway, and the dual variable is updated the same way. The method is no longer guaranteed to find a global minimiser \u2014 but it often finds a stationary point that is good enough (e.g. in nonconvex regularised matrix completion, dictionary learning, etc.).</p> <p>You will see ADMM used in imaging, sparse coding, variational inference, etc., even when parts of the model are not convex.</p>"},{"location":"20_advanced/#104-proximal-coordinate-and-coordinate-prox-methods","title":"10.4 Proximal coordinate and coordinate-prox methods","text":"<p>There\u2019s a natural fusion of the ideas in Sections 10.1 (coordinate descent) and 9.5 (proximal methods): proximal coordinate descent.</p>"},{"location":"20_advanced/#1041-problem-form","title":"10.4.1 Problem form","text":"<p>Consider composite convex objectives  with \\(f\\) smooth convex and \\(R\\) convex, possibly nonsmooth and separable across coordinates or blocks:  </p>"},{"location":"20_advanced/#1042-algorithm-sketch","title":"10.4.2 Algorithm sketch","text":"<p>At each iteration, pick coordinate (or block) \\(j\\), and update only \\(x_j\\) by solving the 1D (or low-dim) proximal subproblem:  </p> <p>Often we linearise \\(f\\) around the current point in that block and add a quadratic term, just like a proximal gradient step but on one coordinate at a time.</p>"},{"location":"20_advanced/#1043-why-its-useful","title":"10.4.3 Why it\u2019s useful","text":"<ul> <li>When \\(R\\) is separable (e.g. \\(\\ell_1\\) sparsity penalties), each coordinate subproblem becomes a scalar shrinkage / thresholding step.</li> <li>Memory footprint is tiny.</li> <li>You get sparsity \u201cfor free\u201d as many coordinates get driven to zero and stay there.</li> <li>Randomised versions (pick a coordinate at random) are simple and have good expected convergence guarantees in convex problems.</li> </ul>"},{"location":"20_advanced/#1044-use-in-nonconvex-settings","title":"10.4.4 Use in nonconvex settings","text":"<p>People run proximal coordinate descent in nonconvex sparse learning (e.g. \\(\\ell_0\\)-like surrogates, nonconvex penalties for variable selection). The convex convergence guarantees are gone, but empirically the method still often converges to a structured, interpretable solution.</p>"},{"location":"20_advanced/#105-majorizationminimization-mm-and-reweighted-schemes","title":"10.5 Majorization\u2013minimization (MM) and reweighted schemes","text":"<p>Majorization\u2013minimization (MM) is a general pattern:</p> <ol> <li>Build a simple convex surrogate that upper-bounds (majorises) your objective at the current iterate,</li> <li>Minimise the surrogate,</li> <li>Repeat.</li> </ol> <p>It is sometimes called \u201citerative reweighted\u201d or \u201csuccessive convex approximation.\u201d</p>"},{"location":"20_advanced/#1051-mm-template","title":"10.5.1 MM template","text":"<p>Suppose we want to minimise \\(F(x)\\) (convex or not). We construct \\(G(x \\mid x^{(k)})\\) such that:</p> <ul> <li>\\(G(x^{(k)} \\mid x^{(k)}) = F(x^{(k)})\\) (touches at current iterate),</li> <li>\\(G(x \\mid x^{(k)}) \\ge F(x)\\) for all \\(x\\) (majorises \\(F\\)),</li> <li>\\(G(\\cdot \\mid x^{(k)})\\) is easy to minimise (often convex, often separable).</li> </ul> <p>Then we set  </p> <p>This guarantees \\(F(x^{(k+1)}) \\le F(x^{(k)})\\). So the objective is monotonically nonincreasing.</p>"},{"location":"20_advanced/#1052-iterative-reweighted-ell_1-ell_2","title":"10.5.2 Iterative reweighted \\(\\ell_1\\) / \\(\\ell_2\\)","text":"<p>A classical example: to promote sparsity or robustness, you might want to minimise something like  or a concave penalty on residuals. You replace that concave / nonconvex penalty with a weighted convex penalty that depends on the previous iterate. Then you update the weights and solve again.</p> <p>In the convex world, MM is just another way to design descent methods. In the nonconvex world, MM is a way to attack nonconvex penalties using a sequence of convex subproblems.</p> <p>This is extremely common in robust regression, compressed sensing with nonconvex sparsity surrogates, and low-rank matrix recovery.</p>"},{"location":"20_advanced/#1053-relation-to-proximal-methods","title":"10.5.3 Relation to proximal methods","text":"<p>MM can often be interpreted as doing a proximal step on a locally quadratic or linearised upper bound. In that sense, it is philosophically close to proximal gradient (Chapter 9) and to Newton-like local quadratic approximation (Chapter 9), but with the additional twist that we are allowed to handle nonconvex \\(F\\) as long as we majorise it with something convex.</p>"},{"location":"20_advanced/#106-summary-and-perspective","title":"10.6 Summary and perspective","text":"<p>We\u2019ve now seen several algorithmic families that are particularly important at large scale and/or under structural constraints:</p> <ol> <li> <p>Coordinate descent / block coordinate descent </p> <ul> <li>Updates one coordinate block at a time.  </li> <li>Converges globally for many convex problems.  </li> <li>Scales extremely well in high dimensions.  </li> <li>Used heuristically in nonconvex alternating minimisation.</li> </ul> </li> <li> <p>Stochastic and mini-batch gradient methods </p> <ul> <li>Use noisy gradient estimates to get cheap iterations.  </li> <li>Converge (in expectation) for convex problems.  </li> <li>Power all of modern large-scale ML, including nonconvex deep learning.</li> </ul> </li> <li> <p>ADMM (Alternating Direction Method of Multipliers) </p> <ul> <li>Splits a problem into simpler subproblems linked by linear constraints.  </li> <li>Closely tied to duality and KKT (Chapters 7\u20138).  </li> <li>Converges for convex problems.  </li> <li>Used everywhere, including nonconvex settings, due to its modularity and parallelisability.</li> </ul> </li> <li> <p>Proximal coordinate / coordinate-prox methods </p> <ul> <li>Merge sparsity-inducing penalties (Chapter 6) with blockwise updates.  </li> <li>Ideal for \\(\\ell_1\\)-type structure, group lasso, etc.  </li> <li>Often extended to nonconvex penalties for even \u201cmore sparse\u201d solutions.</li> </ul> </li> <li> <p>Majorization\u2013minimization (MM) </p> <ul> <li>Iteratively builds and minimises convex surrogates.  </li> <li>Guarantees monotone descent of the true objective.  </li> <li>Provides a clean bridge from convex optimisation theory into heuristic nonconvex optimisation.</li> </ul> </li> </ol>"},{"location":"21_models/","title":"11. Modelling Patterns and Algorithm Selection in Practice","text":""},{"location":"21_models/#chapter-11-modelling-patterns-and-algorithm-selection","title":"Chapter 11: Modelling Patterns and Algorithm Selection","text":"<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often tells us which solver class to use.  In practice, solving machine learning problems looks like: modeling \u2192 recognize structure \u2192 pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>"},{"location":"21_models/#111-regularized-estimation-and-the-accuracysimplicity-tradeoff","title":"11.1 Regularized estimation and the accuracy\u2013simplicity tradeoff","text":"<p>Many learning tasks use a regularized risk minimization form:  Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing \\(\\lambda\\) trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p> <ul> <li> <p>Ridge regression (\u2113\u2082 penalty):    This arises from Gaussian noise (squared-error loss) plus a quadratic prior on \\(x\\).  It is a smooth, strongly convex quadratic problem (Hessian \\(A^TA + \\lambda I \\succ 0\\)).  One can solve it via Newton\u2019s method or closed\u2010form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p> </li> <li> <p>LASSO / Sparse regression (\u2113\u2081 penalty):    The \\(\\ell_1\\) penalty encourages many \\(x_i=0\\) (sparsity) for interpretability.  The problem is convex but nonsmooth (since \\(|\\cdot|\\) is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for \\(\\ell_1\\), which sets small entries to zero.  Coordinate descent is another popular solver \u2013 updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p> </li> <li> <p>Elastic net (mixed \u2113\u2081+\u2113\u2082):    This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for \\(\\lambda_2&gt;0\\)) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the \u2113\u2082 term, the objective is smooth and unique solution.</p> </li> <li> <p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block \\(\\ell_{2,1}\\) norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p> </li> </ul> <p>Algorithmic pointers for 11.1: </p> <ul> <li>Smooth+\u2113\u2082 (strongly convex) \u2192 Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li> <li>Mixed penalties (\u2113\u2081+\u2113\u2082) \u2192 Still convex; often handle like \u2113\u2081 case since smooth part dominates curvature.  </li> <li>Large-scale data \u2192 Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li> </ul> <p>Remarks:  Choose \\(\\lambda\\) via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (\\(n\\) large), coordinate or stochastic methods often outperform direct second-order methods.</p>"},{"location":"21_models/#112-robust-regression-and-outlier-resistance","title":"11.2 Robust regression and outlier resistance","text":"<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>"},{"location":"21_models/#1121-least-absolute-deviations-l1-loss","title":"11.2.1 Least absolute deviations (\u2113\u2081 loss)","text":"<p>Formulation:  </p> <p>Interpretation:</p> <ul> <li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li> <li>Unlike squared error, it penalizes big residuals linearly, not quadratically, so outliers hurt less.</li> </ul> <p>Geometry/structure: - The objective is convex but nondifferentiable at zero residual (the kink in \\(|r|\\) at \\(r=0\\)).</p> <p>How to solve it:</p> <ol> <li> <p>As a linear program (LP).    Introduce slack variables \\(t_i \\ge 0\\) and rewrite:</p> <ul> <li>constraints: \\(-t_i \\le a_i^\\top x - b_i \\le t_i\\),</li> <li>objective: \\(\\min \\sum_i t_i\\).</li> </ul> <p>This is now a standard LP. You can solve it with:</p> <ul> <li>an interior-point LP solver,</li> <li>or simplex.</li> </ul> <p>These methods give high-accuracy solutions and certificates.</p> </li> <li> <p>First-order methods for large scale. </p> <p>For very large problems (millions of samples/features), you can apply:</p> <ul> <li>subgradient methods,</li> <li>proximal methods (using the prox of \\(|\\cdot|\\)).</li> </ul> <p>These are slower in theory (subgradient is only \\(O(1/\\sqrt{t})\\) convergence), but they scale to huge data where generic LP solvers would struggle.</p> </li> </ol>"},{"location":"21_models/#1122-huber-loss","title":"11.2.2 Huber loss","text":"<p>Definition of the Huber penalty for residual \\(r\\):  </p> <p>Huber regression solves:  </p> <p>Interpretation:</p> <ul> <li>For small residuals (\\(|r|\\le\\delta\\)): it acts like least-squares (\\(\\tfrac{1}{2}r^2\\)). So inliers are fit tightly.</li> <li>For large residuals (\\(|r|&gt;\\delta\\)): it acts like \\(\\ell_1\\) (linear penalty), so outliers get down-weighted.</li> <li>Intuition: \u201cbe aggressive on normal data, be forgiving on outliers.\u201d</li> </ul> <p>Properties:</p> <ul> <li>\\(\\rho_\\delta\\) is convex.</li> <li>It is smooth except for a kink in its second derivative at \\(|r|=\\delta\\).</li> <li>Its gradient exists everywhere (the function is once-differentiable).</li> </ul> <p>How to solve it:</p> <ol> <li> <p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.     Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p> </li> <li> <p>Proximal / first-order methods.     You can apply proximal gradient methods, since each term is simple and has a known prox.</p> </li> <li> <p>As a conic program (SOCP).     The Huber objective can be written with auxiliary variables and second-order cone constraints.     That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.     This is attractive when you want high accuracy and dual certificates.</p> </li> </ol>"},{"location":"21_models/#1123-worst-case-robust-regression","title":"11.2.3 Worst-case robust regression","text":"<p>Sometimes we don\u2019t just want \u201cfit the data we saw,\u201d but \u201cfit any data within some uncertainty set.\u201d This leads to min\u2013max problems of the form:  </p> <p>Meaning:</p> <ul> <li>\\(\\mathcal{U}\\) is an uncertainty set describing how much you distrust the matrix \\(A\\), the inputs, or the measurements.</li> <li>You choose \\(x\\) that performs well even in the worst allowed perturbation.</li> </ul> <p>Why this is still tractable:</p> <ul> <li> <p>If \\(\\mathcal{U}\\) is convex (for example, an \\(\\ell_2\\) ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p> </li> <li> <p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p> <ul> <li>Example: if the rows of \\(A\\) can move within an \\(\\ell_2\\) ball of radius \\(\\epsilon\\), the robustified problem often picks up an additional \\(\\ell_2\\) term like \\(\\gamma \\|x\\|_2\\) in the objective.</li> <li>The final problem is still convex (often a QP or SOCP).</li> </ul> </li> </ul> <p>How to solve it:</p> <ul> <li> <p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p> </li> <li> <p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p> </li> </ul>"},{"location":"21_models/#113-maximum-likelihood-and-loss-design","title":"11.3 Maximum likelihood and loss design","text":"<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p> <ul> <li> <p>Gaussian (normal) noise</p> <p>Model:  </p> <p>The negative log-likelihood (NLL) is proportional to:  </p> <p>This recovers the classic least-squares loss (as in linear regression). It is smooth and convex (strongly convex if \\(A^T A\\) is full rank).</p> <p>Algorithms:</p> <ul> <li> <p>Closed-form via \\((A^T A + \\lambda I)^{-1} A^T b\\) (for ridge regression),</p> </li> <li> <p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p> </li> <li> <p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian \\(A^T A\\).</p> </li> </ul> </li> <li> <p>Laplace (double-exponential) noise</p> <p>If \\(\\varepsilon_i \\sim \\text{Laplace}(0, b)\\) i.i.d., the NLL is proportional to:  </p> <p>This is exactly the \u2113\u2081 regression (least absolute deviations). It can be solved as an LP or with robust optimization solvers (interior-point), or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p> </li> <li> <p>Logistic model (binary classification)</p> <p>For \\(y_i \\in \\{0,1\\}\\), model:  </p> <p>The negative log-likelihood (logistic loss) is:  </p> <p>This loss is convex and smooth in \\(x\\). No closed-form solution exists.</p> <p>Algorithms:</p> <ul> <li>With \u2113\u2082 regularization: smooth and (if \\(\\lambda&gt;0\\)) strongly convex \u2192 use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li> <li>With \u2113\u2081 regularization (sparse logistic): composite convex \u2192 use proximal gradient (soft-thresholding) or coordinate descent.</li> </ul> </li> <li> <p>Softmax / Multinomial logistic (multiclass)</p> <p>For \\(K\\) classes with one-hot labels \\(y_i \\in \\{e_1, \\dots, e_K\\}\\), the softmax model gives NLL:  </p> <p>This loss is convex in the weight vectors \\(\\{x_k\\}\\) and generalizes binary logistic to multiclass.</p> <p>Algorithms:</p> <ul> <li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li> <li>Stochastic gradient (SGD, Adam) for large datasets.</li> </ul> </li> <li> <p>Generalized linear models (GLMs)</p> <p>In GLMs, \\(y_i\\) given \\(x\\) has an exponential-family distribution (Poisson, binomial, etc.) with mean related to \\(a_i^T x\\). The NLL is convex in \\(x\\) for canonical links (e.g. log-link for Poisson, logit for binomial).</p> <p>Examples:</p> <ul> <li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li> <li>Probit models: convex but require iterative solvers.</li> </ul> </li> </ul>"},{"location":"21_models/#114-structured-constraints-in-engineering-and-design","title":"11.4 Structured constraints in engineering and design","text":"<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for \\(\\mathcal{X}\\):</p> <ul> <li> <p>Simple (projection-friendly) constraints</p> <p>Examples:</p> <ul> <li> <p>Box constraints: \\(l \\le x \\le u\\)     \u2192 Projection: clip each entry to \\([\\ell_i, u_i]\\).</p> </li> <li> <p>\u2113\u2082-ball: \\(\\|x\\|_2 \\le R\\)     \u2192 Projection: rescale \\(x\\) if \\(\\|x\\|_2 &gt; R\\).</p> </li> <li> <p>Simplex: \\(\\{x \\ge 0, \\sum_i x_i = 1\\}\\)     \u2192 Projection: sort and threshold coordinates (simple \\(O(n \\log n)\\) algorithm).</p> </li> </ul> </li> <li> <p>General convex constraints (non-projection-friendly) If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p> <ol> <li> <p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p> </li> <li> <p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p> </li> </ol> </li> </ul> <p>Algorithmic pointers for 11.4:</p> <ul> <li>Projection-friendly constraints \u2192 Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li> <li>Complex constraints (cones, PSD, many linear) \u2192 Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li> <li>LP/QP special cases \u2192 Use simplex or specialized LP/QP solvers (Section 11.5).</li> </ul> <p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection \u2192 projective methods; otherwise \u2192 interior-point or operator-splitting.</p>"},{"location":"21_models/#115-linear-and-conic-programming-the-canonical-models","title":"11.5 Linear and conic programming: the canonical models","text":"<p>Many practical problems reduce to linear programming (LP) or its convex extensions. LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p> <ul> <li> <p>Linear programs: standard form</p> <p>  Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed. - Quadratic, SOCP, SDP: Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p> </li> </ul> <p>_ Practical patterns:     1. Resource allocation/flow (LP): linear costs and constraints.     2. Minimax/regret problems: e.g. \\(\\min_{x}\\max_{i}|a_i^T x - b_i|\\) \u2192 LP (as in Chebyshev regression).     3. Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</p> <p>Algorithmic pointers for 11.5: - Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable). - Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale. - Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. \u2113\u221e regression \u2192 LP, \u21132 regression with \u21132 constraint \u2192 SOCP.)</p>"},{"location":"21_models/#116-risk-safety-margins-and-robust-design","title":"11.6 Risk, safety margins, and robust design","text":"<p>Modern design often includes risk measures or robustness. Two common patterns:</p> <ul> <li> <p>Chance constraints / risk-adjusted objectives     E.g. require that \\(Pr(\\text{loss}(x,\\xi) &gt; \\tau) \\le \\delta\\). A convex surrogate is to include mean and a multiple of the standard deviation:          Algebra often leads to second-order cone constraints (e.g. forcing \\(\\mathbb{E}\\pm \\kappa\\sqrt{\\mathrm{Var}}\\) below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p> </li> <li> <p>Worst-case (robust) optimization:     Specify an uncertainty set \\(\\mathcal{U}\\) for data (e.g. \\(u\\) in a norm-ball) and minimize the worst-case cost \\(\\max_{u\\in\\mathcal{U}}\\ell(x,u)\\). Many losses \\(\\ell\\) and convex \\(\\mathcal{U}\\) yield a convex max-term (a support function or norm). The result is often a conic constraint (for \u2113\u2082 norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p> </li> </ul> <p>Algorithmic pointers for 11.6:</p> <ul> <li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li> <li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li> <li>Distributed or iterative solutions: If \\(\\mathcal{U}\\) or loss separable, ADMM can distribute the computation (Chapter 10).</li> </ul>"},{"location":"21_models/#117-cheat-sheet-if-your-problem-looks-like-this-use-that","title":"11.7 Cheat sheet: If your problem looks like this, use that","text":"<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p> <ul> <li> <p>(A) Smooth least-squares + \u2113\u2082:</p> <ul> <li>Model: \\(|Ax-b|_2^2 + \\lambda|x|_2^2\\). </li> <li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic \u21d2 fast second-order methods.)</li> </ul> </li> <li> <p>(B) Sparse regression (\u2113\u2081):</p> <ul> <li>Model: \\(\\tfrac12|Ax-b|_2^2 + \\lambda|x|_1\\). </li> <li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li> </ul> </li> <li> <p>(C) Robust regression (outliers):</p> <ul> <li>Models: \\(\\sum|a_i^T x - b_i|\\), Huber loss, etc. </li> <li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li> </ul> </li> <li> <p>(D) Logistic / log-loss (classification):</p> <ul> <li>Model: \\(\\sum[-y_i(w^Ta_i)+\\log(1+e^{w^Ta_i})] + \\lambda R(w)\\) with \\(R(w)=|w|_2^2\\) or \\(|w|_1\\). </li> <li>Solve:<ul> <li>If \\(R=\\ell_2\\): use Newton/gradient (smooth, strongly convex).</li> <li>If \\(R=\\ell_1\\): use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; \u2113\u2081 adds nonsmoothness.)</li> </ul> </li> </ul> </li> <li> <p>(E) Constraints (hard limits):</p> <ul> <li>Model: \\(\\min f(x)\\) s.t. \\(x\\in\\mathcal{X}\\) with \\(\\mathcal{X}\\) simple. </li> <li>Solve: Projected (stochastic) gradient or proximal methods if projection \\(\\Pi_{\\mathcal{X}}\\) is cheap (e.g. box, ball, simplex). If \\(\\mathcal{X}\\) is complex (second-order or SDP), use interior-point.</li> </ul> </li> <li> <p>(F) Separable structure:</p> <ul> <li>Model: \\(\\min_{x,z} f(x)+g(z)\\) s.t. \\(Ax+Bz=c\\). </li> <li>Solve: ADMM (Chapter 10) \u2013 it decouples updates in \\(x\\) and \\(z\\); suits distributed or block-structured data.</li> </ul> </li> <li> <p>(G) LP/QP/SOCP/SDP:</p> <ul> <li>Model: linear/quadratic objective with linear/conic constraints. </li> <li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li> </ul> </li> <li> <p>(H) Nonconvex patterns:</p> <ul> <li> <p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p> </li> <li> <p>Solve: There is no single global solver \u2013 typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p> </li> </ul> </li> <li> <p>(I) Logistic (multi-class softmax):</p> <ul> <li> <p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p> </li> <li> <p>Solve: Similar to binary case \u2013 Newton/gradient with L2, or proximal/coordinate with \u2113\u2081.</p> </li> </ul> </li> <li> <p>(J) Poisson and count models:</p> <ul> <li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li> <li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li> </ul> </li> </ul> <p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p> <ul> <li>Smooth &amp; strongly convex \u2192 (quasi-)Newton or accelerated gradient.</li> <li>Smooth + \u2113\u2081 \u2192 Proximal gradient/coordinate.</li> <li>Nonsmooth separable \u2192 Proximal or coordinate.</li> <li>Easy projection constraint \u2192 Projected gradient.</li> <li>Hard constraints or conic structure \u2192 Interior-point.</li> <li>Large-scale separable \u2192 Stochastic gradient/ADMM.</li> </ul> <p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>"},{"location":"2_0_identifyconvex/","title":"2 0 identifyconvex","text":"<p>Convex optimization problems form the backbone of modern optimization theory due to their well-behaved geometry and tractable properties. By definition, an optimization problem is convex if (a) its objective function is convex and (b) its feasible set is convex. In practical terms, this means any weighted average (or convex combination) of two feasible points remains feasible (a hallmark of convex sets \u2013 see Section A, Chapter 4: Affine and Convex Geometry). These conditions are crucial because they guarantee that every local optimum is a global optimum. In other words, you never get trapped in a \"bad\" local minimum when the problem is convex. This global optimality property makes convex problems far easier to solve reliably than general nonlinear problems.</p> <p>Convex Optimization Problem \u2013 Formal Definition: A standard form optimization problem is convex if it can be written as: minimize \\(f_0(x)\\) subject to \\(f_i(x) \\le 0\\) (for \\(i=1,\\dots,m\\)) and \\(h_j(x) = 0\\) (for \\(j=1,\\dots,p\\)), where \\(f_0\\) is a convex function, each inequality constraint function \\(f_i\\) is convex, and each equality constraint \\(h_j(x)\\) is an affine function (linear function plus a constant). Equivalently, it is the problem of minimizing a convex function over a convex set. The feasible region in a convex problem is the intersection of a convex domain and constraint sets, which is itself convex.</p> <p>Convexity of the objective and constraints can be tested using both geometric intuition and analytic tools. In this section, we'll walk through how to verify the convexity of an optimization problem step by step. We start by checking the objective function (is it convex?), then each constraint (does it define a convex region?), and finally we outline a checklist and even a simple flowchart of questions to systematically determine convexity. We will also highlight common patterns that break convexity (so you can spot non-convex structures easily), and work through several intuitive examples (like least squares, LASSO, and a quadratic program) to solidify the concepts. Throughout, we'll lean on geometric language (think of \u201cbowl-shaped\u201d functions and \u201cflat\u201d constraints) and simple derivations rather than heavy proofs \u2013 our goal is building understanding and intuition for graduate students and ML practitioners.</p>"},{"location":"2_0_identifyconvex/#convexity-of-the-objective-function","title":"Convexity of the Objective Function","text":"<p>The objective function \\(f_0(x)\\) is the function we seek to minimize (we'll assume a minimization problem for concreteness; a maximization of a concave function is equivalent since maximizing a concave \\(g\\) is same as minimizing \\(f_0(x) = -g(x)\\), which is convex en.wikipedia.org ). To check if \\(f_0(x)\\) is convex, we have several tools at our disposal, ranging from visual (geometric) tests to analytical conditions:</p> <ul> <li> <p>Geometric Definition (Epigraph Test): Recall that a function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if and only if its epigraph \u2013 the set \\(\\mathrm{epi}(f) = {(x,t): f(x) \\le t}\\) \u2013 is a convex set. Geometrically, this means that if you take any two points on the graph of \\(f\\), the line segment (or \u201cchord\u201d) connecting them lies above the graph everywhere between those points. Algebraically, for any two points \\(x,y\\) in the domain and any \\(\\theta\\in[0,1]\\), a convex \\(f\\) satisfies the inequality:       </p> <p>This is the precise definition of convexity (see Section A, Chapter 4). In contrast, a non-convex function will \"curve up and down\" \u2013 its graph might dip below some chord (a familiar example is \\(\\sin x\\), which is neither convex nor concave over its full domain).</p> </li> <li> <p>First-Order Test (Tangent Underestimation): If \\(f(x)\\) is differentiable, a very useful characterization of convexity is that its tangent plane at any point lies below the graph of \\(f\\) everywhere. Formally, \\(f\\) is convex if and only if for all points \\(x,y\\) in its domain,          This inequality says that the first-order Taylor approximation of \\(f\\) at \\(x\\) underestimates the function at any other point \\(y\\) (no tangent ever goes above the curve). This is intuitive: a convex function has no \u201chidden dips\u201d below its tangents. In geometric terms, you can stand anywhere on the surface and the local linear approximation forms a supporting plane underneath the surface. If \\(f\\) is not differentiable, we can use the more general notion of a subgradient \\(g \\in \\partial f(x)\\). A vector \\(g\\) is a subgradient of \\(f\\) at \\(x\\) if \\(f(y) \\ge f(x) + g^T (y - x)\\) for all \\(y\\). For convex functions, at least one subgradient exists at every point (even at a kink), and the inequality still holds. For example, the absolute value function \\(f(x)=|x|\\) is convex but not differentiable at 0; its subgradients at 0 are any \\(g\\in[-1,1]\\), which gives an entire family of supporting lines \\(f(y) \\ge |0| + g,(y-0)\\) hovering below the \\(|x|\\) V-shape.</p> </li> <li> <p>Second-Order Test (Hessian Criterion): If \\(f(x)\\) is twice differentiable, convexity can be checked via the Hessian matrix (the matrix of second partial derivatives). The Hessian \\(\\nabla^2 f(x)\\) must be positive semidefinite (PSD) at every point \\(x\\) in the domain for \\(f\\) to be convex. In \\(\\mathbb{R}\\) (one dimension), this reduces to the simple condition \\(f''(x) \\ge 0\\) for all \\(x\\). In higher dimensions, positive semidefiniteness means \\(z^T (\\nabla^2 f(x)),z \\ge 0\\) for all vectors \\(z\\) (intuitively, \\(f\\) curves \"upward\" in every direction). This second-order test is often the most straightforward for smooth functions: for instance, if \\(f(x) = \\tfrac{1}{2}x^T Q x + c^T x\\) is a quadratic function, then \\(\\nabla^2 f(x) = Q\\), so \\(f\\) is convex exactly when \\(Q \\succeq 0\\) (PSD matrix). As a quick example, \\(f(x)=x^2\\) has \\(f''(x)=2&gt;0\\) so it's convex (a bowl shape), whereas \\(f(x)=\\cos x\\) has \\(f''(x)=-\\cos x\\) which is negative around \\(x=0\\), revealing its non-convex curvature.</p> </li> <li> <p>Recognizing Known Convex Functions (By Type): Over time, you will build a library of common convex functions and patterns. Many functions are known to be convex by their form. For example: any linear or affine function (\\(f(x)=a^T x + b\\)) is convex (and concave) because it just produces a flat plane. Quadratic functions \\(f(x)=x^T Q x + c^T x + d\\) are convex if \\(Q\\) is PSD, as noted. Norms like \\(||x||_2\\) or \\(||x||1\\) are convex (the \\(L_1\\) norm is essentially a sum of absolute values, forming a pointed \"diamond\" cone). Exponential functions \\(e^{ax}\\) are convex for any real \\(a\\). Even-powered monomials \\(x^{2}, x^4, x^6,\\dots\\) are convex on \\(\\mathbb{R}\\). More generally, \\(x^p\\) is convex on \\(\\mathbb{R}{++}\\) (positive reals) for \\(p\\ge 1\\) or \\(p\\le 0\\). Negative entropy \\(x\\log x\\) is convex on \\(x&gt;0\\). The log-sum-exp function (important in machine learning) \\(f(x)=\\log(e^{x_1}+\\cdots+e^{x_n})\\) is convex. Even the maximum of a set of convex functions is convex (e.g. \\(f(x)=\\max{f_1(x),\\dots,f_k(x)}\\) is convex) because its epigraph is the intersection of halfspaces from each \\(f_i\\). If your objective \\(f_0(x)\\) can be expressed as a sum of convex functions, a maximum of convex functions, or an affine transformation of a convex function, then \\(f_0\\) is convex. (Convexity is closed under these operations: e.g. nonnegative weighted sums of convex functions remain convex, and composing a convex function with an affine mapping keeps it convex.)</p> </li> </ul> <p>In practice, a good strategy is to decompose the objective into known building blocks. If each piece is convex and they are combined by operations that preserve convexity, then the whole objective is convex. For example, if \\(f_0(x) = g(h(x))\\) and you know \\(g\\) is convex non-decreasing and \\(h(x)\\) is convex, then \\(f_0\\) is convex (one common case: \\(g(t)=\\log t\\) which is increasing concave, so \\(-g(t)=-\\log t\\) is convex non-decreasing; thus \\(- \\log(h(x))\\) is convex if \\(h(x)\\) is convex and positive). On the other hand, if you detect any component that is non-convex (say a term like \\(\\sin x\\) or \\(x^T Q x\\) with an indefinite \\(Q\\), or a product of variables like \\(x_i x_j\\)), that is a red flag \u2013 the objective might be non-convex unless that term can be transformed or bounded within a convex structure.</p>"},{"location":"2_0_identifyconvex/#convexity-of-the-constraints-feasible-set","title":"Convexity of the Constraints (Feasible Set)","text":"<p>The second part of the convexity check is to examine the constraints, which determine the feasible set. Even if the objective is convex, a non-convex feasible region will make the overall problem non-convex (since you're effectively minimizing a convex function over a non-convex set, which can introduce local minima). To ensure the feasible set is convex, each constraint must individually define a convex set, and all constraints together (their intersection) must therefore also yield a convex set.</p> <p>Here are the typical types of constraints and how to check their convexity:</p> <ul> <li> <p>Convex Inequality Constraints: These are of the form      </p> <p>where \\(f_i(x)\\) is a convex function. Such a constraint means we are taking a sublevel set of a convex function, \\({x \\mid f_i(x) \\le 0}\\). By definition, any sublevel set of a convex function is a convex set. Intuitively, if \\(f_i\\) is convex, the region where \\(f_i(x)\\) is below some threshold looks like a \"bowl\" or a filled-in convex region. For example, \\(f_i(x) = ||x||_2 - 1 \\le 0\\) describes the set \\({x: ||x||_2 \\le 1}\\), which is a solid Euclidean ball \u2013 a convex set. Likewise, linear inequalities like \\(a^T x \\le b\\) are convex constraints (they define half-spaces). Rule of thumb: If each \\(f_i(x)\\le 0\\) is convex in \\(x\\), then the feasible set defined by all such inequalities is convex (since it's an intersection of convex sets).</p> <p>note: Sometimes constraints are given in an equivalent form like \\(g(x)\\ge 0\\). You can always rewrite \\(g(x)\\ge 0\\) as \\(-g(x) \\le 0\\). So if you encounter \\(g(x)\\ge 0\\), check if \\(g(x)\\) is concave (since \\(-g\\) would then be convex). For instance, \\(g(x) = \\text{log}(x)\\) is concave on \\(x&gt;0\\), so the constraint \\(\\log(x) \\ge 3\\) is equivalent to \\(-,\\log(x) \\le -3\\), and \\(-,\\log(x)\\) is convex; hence this constraint defines a convex feasible set (here \\(x \\ge e^3\\)).</p> </li> <li> <p>Affine Equality Constraints: These are constraints of the form      </p> <p>where \\(h_j(x)\\) is an affine function, meaning \\(h_j(x) = a_j^T x + b_j\\) for some constant vector \\(a_j\\) and scalar \\(b_j\\). Affine equalities are convex constraints because an affine set \\({x \\mid a^T x + b = 0}\\) is actually a flat hyperplane (or a translate of a subspace), which is a convex set (recall: any line or plane is convex since the line segment between any two points on a line/plane stays on that line/plane; see Section A, Chapter 4). For example, \\(x_1 + 2x_2 = 5\\) defines a line in \\(\\mathbb{R}^2\\), which is convex. Important: If an equality constraint is non-affine (e.g. \\(x_1 x_2 = 10\\) or \\(x_1^2 + x_2^2 = 1\\)), the feasible set will typically be non-convex. For instance, \\(x_1 x_2 = 10\\) is a hyperbola \u2013 a curved set that is not convex; \\(x_1^2 + x_2^2 = 1\\) is the unit circle (just the boundary of a ball), which is not a convex set by itself (any line between two points on the circle goes through the inside which is not included in the feasible set). Thus, any nonlinear equality generally signals non-convexity (except in trivial cases like something that reduces to an affine constraint on a higher-dimensional space).</p> </li> <li> <p>Convex Set Membership Constraints: Sometimes constraints are given in the form \u201c\\(x\\) belongs to a set \\(C\\)\u201d, denoted \\(x \\in C\\). In order for the problem to remain convex, the set \\(C\\) must be convex. Many common constraint sets in optimization are indeed convex: for example, polyhedra defined by linear inequalities (\\(\\{\\, x \\mid A x \\le b \\,\\}\\)) are convex; norm balls like \\(\\{\\, x \\mid \\|x\\|_p \\le \\alpha \\,\\}\\) are convex sets (for any \\(p \\ge 1\\)); the set of probability distributions \\(\\{\\, x \\mid x_i \\ge 0, \\ \\sum_i x_i = 1 \\,\\}\\) is convex; the set of positive semidefinite matrices (in semidefinite programming) is convex. However, if \\(C\\) is something like a finite set or has a discrete structure (e.g. \"\\(x_i \\in \\{0,1\\}\\) for some component\"), then \\(C\\) is non-convex. Discrete constraints (integer or binary decisions) break convexity because the feasible region becomes a set of isolated points or separate chunks, not a single nicely connected region. As another example, the set \\(C = \\{\\, x : 1 \\le \\|x\\|_2 \\le 2 \\,\\}\\) \u2013 an annulus (ring) between two circles \u2013 is not convex because it excludes the interior donut hole (a line between a point on the inner circle and a point on the outer circle would pass through the hole which is infeasible).</p> </li> </ul> <p>In summary, to have a convex feasible set, every constraint should individually carve out a convex region. All inequality constraints should be convex functions (producing convex sublevel sets), all equalities should be affine (flat), and any set-membership conditions should refer to convex sets. Since the intersection of any collection of convex sets is convex, these conditions ensure the overall feasible set is convex. If any one constraint is non-convex, the feasible region (being an intersection including a non-convex piece) will be non-convex \u2013 and hence the whole problem is not convex.</p>"},{"location":"2_0_identifyconvex/#common-non-convex-structures-to-watch-for","title":"Common Non-Convex Structures to Watch For","text":"<p>Having a mental checklist of \"usual suspects\" that break convexity is extremely useful. Often, by scanning the form of the objective and constraints, you can spot patterns that are inherently non-convex. Here are some common non-convex structures:</p> <ul> <li> <p>Bilinear or Multilinear Terms: If the objective or a constraint involves a product of decision variables (e.g. a term like \\(x_i \\cdot y_j\\) or \\(x_1 x_2\\)), this is generally non-convex. For example, the function \\(f(x_1,x_2) = x_1 x_2\\) is not convex on \\(\\mathbb{R}^2\\) (its Hessian is indefinite), and the constraint \\(x_1 x_2 \\le c\\) typically yields a hyperbolic region which is not convex. Bilinear terms often arise in problems like optimal power flow, portfolio optimization with products, or geometry problems \u2013 and they usually indicate the problem is hard (non-convex) unless you can reformulate them in convex form (sometimes via change of variables or relaxations).</p> </li> <li> <p>Indefinite Quadratics: A quadratic function \\(x^T Q x + c^T x\\) is convex only if \\(Q\\) is PSD. If \\(Q\\) has even one negative eigenvalue (making it indefinite or negative definite), the function is not convex \u2013 it \u201ccurves downward\u201d in at least one direction. For instance, \\(f(x_1,x_2) = x_1^2 - x_2^2\\) (here \\(Q=\\mathrm{diag}(1,-1)\\)) is a saddle-shaped function, not convex. So if you see a quadratic objective or constraint, check the matrix: a negative sign on a squared term or a \u201cdifference of squares\u201d usually signals non-convexity (unless you can somehow constrain that term\u2019s effect away).</p> </li> <li> <p>Nonlinear Equality Constraints: As mentioned, anything like \\(h(x)=0\\) where \\(h\\) is nonlinear (especially products, trigonometric equations, polynomial equations beyond first degree) is likely non-convex. A classic example is a fixed product or fixed norm constraint: \\(x_1 x_2 = 1\\) or \\(||x||_2 = 1\\). These carve out a curved manifold (a hyperbola or a sphere surface) without thickness \u2013 not convex. When you have such constraints, the feasible region often ends up disconnected or curved in a way that violates convexity. (One exception: something like \\(x^2+y^2=0\\) is convex, but only because it implies \\(x=0,y=0\\) \u2013 a trivial single-point set, which is convex. In general, non-affine equalities that allow multiple points are trouble.)</p> </li> <li> <p>Ratios and Fractional Forms: Objective terms like \\(\\frac{p(x)}{q(x)}\\) (where \\(p\\) and \\(q\\) are functions of \\(x\\)) or constraints like \\(\\frac{f(x)}{g(x)} \\le c\\) are typically non-convex (these are quotient or fractional programs). A simple example: \\(f(x)=\\frac{1}{x}\\) on \\(x&gt;0\\) is convex, but if you have something like \\(\\frac{x_1}{x_2}\\) it\u2019s neither convex nor concave on a broad domain. Many ratio problems can sometimes be convexified by clever transformations (e.g. transforming variables if \\(x_2\\) is positive), but at face value, be cautious with fractional terms.</p> </li> <li> <p>Discrete Variables or Logic: If your problem involves integer variables (e.g. \\(x_i \\in {0,1}\\) or \\(x_i\\) must be an integer) or logical constraints (if-then conditions, either-or constraints), then the feasible set is not convex. For example, requiring \\(x\\) to be 0 or 1 means the feasible set is just two points, which is not convex (no line segment between 0 and 1 stays in the set except at the endpoints). These kinds of problems fall into combinatorial optimization or mixed-integer programming, which are generally NP-hard. They are solved with very different techniques (branch-and-bound, etc.) compared to convex optimization. There are ways to relax some discrete problems into convex ones (for example, dropping the integrality to allow continuous variables between 0 and 1, or using convex hulls), but the original discrete problem is non-convex.</p> </li> <li> <p>\u201cU-Shaped then Inverted U\u201d Functions: Any single-variable function that isn\u2019t convex over its whole domain often shows a change in curvature. For example, \\(\\sin x\\) alternates between convex and concave regions; a function like \\(f(x) = x^3\\) has \\(f''(x) = 6x\\), which is negative for \\(x&lt;0\\) and positive for \\(x&gt;0\\), so \\(f(x)\\) is not convex on the entire real line (it fails the Hessian test globally). If the objective function or a constraint function \u201cbends\u201d upward in some places and downward in others, it\u2019s not globally convex. Recognizing these shapes (e.g. an objective with multiple local minima valleys separated by hills) is key \u2014 convex functions have a single valley (global bowl shape), whereas non-convex ones can have multiple valleys and peaks.</p> </li> </ul> <p>In summary, when scanning a problem, look out for these red flags. Spotting a single non-convex structure is enough to conclude the problem (as stated) is non-convex. Sometimes, such problems can be transformed or approximated by convex problems (this is a big area of research), but that\u2019s beyond our current scope. Here, our goal is identification: know it when you see it.</p>"},{"location":"2_0_identifyconvex/#examples-convex-or-not","title":"Examples: Convex or Not?","text":"<p>Let's solidify these concepts with a few intuitive examples. We will examine some optimization problems and apply the convexity checks. This will illustrate both positive examples (problems that are convex and why) and a negative example (a non-convex problem and how to tell).</p> <ol> <li> <p>Unconstrained Least Squares (Convex): </p> <p>Problem: \\(\\min_x f(x)\\) where \\(f(x) = |Ax - b|_2^2 = (Ax - b)^T(Ax - b)\\). This is the classic least squares problem. </p> <p>Objective: \\(f(x)\\) is a quadratic function. We can expand \\(f(x) = x^T (A^T A) x - 2b^T A x + |b|_2^2\\). Here the Hessian is \\(Q = 2A^T A\\). The matrix \\(A^T A\\) is symmetric positive semidefinite (in fact positive definite if \\(A\\) has full column rank). Thus \\(Q \\succeq 0\\), so \\(f(x)\\) is convex. There are no constraints (the domain is all \\(x\\), which is a convex set \\(\\mathbb{R}^n\\)), so the feasible set is convex. By our criteria, this optimization problem is convex. Indeed, least squares has a unique global minimum given by the normal equations. If we plot \\(f(x)\\) as a function (for example, if \\(x\\) is one-dimensional, \\(f(x)\\) is a simple parabola), it's clearly bowl-shaped and has no secondary minima. This matches our formal check: quadratic with PSD curvature \u2192 convex objective.</p> </li> <li> <p>LASSO Regression (Convex):</p> <p>Problem: \\(\\min_{x\\in\\mathbb{R}^n} ; \\frac{1}{2}|Ax - b|_2^2 + \\lambda |x|_1\\), for some \\(\\lambda &gt; 0\\). This is the LASSO optimization used in machine learning for sparse linear models. </p> <p>Objective: It is a sum of two terms: \\(g(x) = \\frac{1}{2}|Ax - b|_2^2\\) (a convex quadratic, as just discussed) and \\(h(x) = \\lambda |x|_1\\) (the \\(L_1\\) norm times a positive scalar, which is convex). The sum of convex functions is convex, so \\(f(x) = g(x)+h(x)\\) is convex. There are no explicit constraints (again \\(x\\in\\mathbb{R}^n\\)), so the feasible set is all of \\(\\mathbb{R}^n\\) (convex). Thus, the LASSO problem is convex. Geometrically, the first term \\(|Ax-b|_2^2\\) defines ellipsoidal level sets (nice and smooth), and the second term \\(|x|_1\\) has level sets that are diamonds (corners along axes). The combination yields a convex \u201cbowl with a corner-like shape around the bottom,\u201d but crucially no local minima besides the global one. Many algorithms exploit this convexity (LASSO can be solved efficiently by coordinate descent or proximal gradient methods, leveraging the convex subgradient of \\(|x|_1\\)).</p> </li> <li> <p>Quadratic Program with Constraints (Convex):</p> <p>Problem: \\(\\displaystyle \\min_{x} ;\\frac{1}{2} x^T Q x + c^T x \\quad \\text{s.t.}; A x \\le d; Bx = e.\\) This is a quadratic program (QP) with linear constraints. </p> <p>To check convexity: The objective is convex if \\(Q \\succeq 0\\) (positive semidefinite). All terms are quadratic/linear which are easy to verify. Constraints: \\(A x \\le d\\) are linear inequality constraints \u2013 each of the form \\(a_i^T x \\le d_i\\) \u2013 which are convex (halfspaces). \\(B x = e\\) are affine equalities \u2013 also convex (subspace translated by \\(e\\)). So if \\(Q\\) is PSD, everything in this problem is convex: convex objective, convex feasible region. This becomes a convex optimization problem, often called a convex QP. For example, consider a specific QP:</p> \\[ \\begin{aligned} \\min_{x_1,\\, x_2} \\quad &amp; 2x_1^2 + x_2^2 + 3x_1 + 4x_2 \\\\ \\text{s.t.} \\quad &amp; x_1 + x_2 = 1, \\\\ &amp; x_1 \\ge 0, \\\\ &amp; x_2 \\ge 0. \\end{aligned} \\] <p>Here </p> \\[ Q = \\begin{pmatrix} 4 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix} \\] <p>(since objective \\(= 2x_1^2 + x_2^2 + \\dots\\)), which is PSD. The equality \\(x_1+x_2=1\\) is affine (a line), and \\(x_1 \\ge 0, x_2 \\ge 0\\) are halfspaces (actually together \\(x_1,x_2\\ge0\\) define the first quadrant, which is convex). So all conditions check out \u2013 it's convex. We know from theory and experience that convex QPs can be solved efficiently to global optimality (there are Interior-Point solvers, etc.). Had \\(Q\\) been non-PSD, the objective would be indefinite and the problem not convex \u2013 in that case, even though the constraints are still linear, the objective's shape would cause multiple local minima or unbounded directions.</p> </li> <li> <p>Non-Convex Optimization Example (Bilinear Constraint): </p> <p>Problem: \\(\\displaystyle \\min_{x_1,x_2} ; x_1^2 + x_2^2 \\quad \\text{s.t.}; x_1 x_2 \\ge 1.\\) This is a made-up example to demonstrate a non-convex feasible region. </p> <p>Objective: \\(f(x_1,x_2) = x_1^2 + x_2^2\\) is convex (it's a bowl shaped paraboloid). Constraint: \\(x_1 x_2 \\ge 1\\) is not a convex constraint. The feasible set \\({(x_1,x_2): x_1 x_2 \\ge 1}\\) consists of two disjoint regions: one where both \\(x_1\\) and \\(x_2\\) are positive (and their product exceeds 1), and one where both are negative (since a negative times a negative is positive, exceeding 1). The feasible region looks like two opposite quadrants cut away from the origin. This set is non-convex (you can take a point from the positive quadrant and one from the negative quadrant, and the line segment between them will cross through the region near the origin where \\(x_1 x_2 &lt; 1\\), which is infeasible). Thus, even though the objective is convex, the non-convex constraint breaks the problem's convexity. In fact, this problem has two separate \u201cvalleys\u201d to minimize in (one in the \\(x_1,x_2 &gt; 0\\) region and one in the \\(x_1,x_2 &lt; 0\\) region). Each region has a local minimum (roughly when \\(x_1 x_2 = 1\\) and the mass is evenly distributed, e.g. \\(x_1=x_2=1\\) or \\(x_1=x_2=-1\\)), but there is no single global bowl \u2013 we have two distinct basins. A solver that doesn\u2019t account for non-convexity might get stuck in one of them. This illustrates why breaking convexity is dangerous: local optimality need not imply global optimality. Indeed, \\(x_1=x_2=1\\) gives \\(f=2\\) and \\(x_1=x_2=-1\\) gives \\(f=2\\), and those are the best feasible points; but if you started at \\((10, 0.1)\\) which satisfies \\(10 \\cdot 0.1 = 1\\), any local descent would stay in the positive quadrant basin.</p> </li> </ol>"},{"location":"2_0_identifyconvex/#checklist-for-verifying-convexity","title":"Checklist for Verifying Convexity","text":"<p>To wrap up, here is a handy checklist you can use as a step-by-step guide when faced with a new optimization problem. This is essentially a flowchart in words for convexity verification:</p> <ol> <li> <p>Problem Form: Convert the problem to a clear standard form. Are you minimizing an objective? (If it\u2019s a maximization, consider the equivalent minimization of the negative objective.) Make sure all constraints are written as either \u201c\\(\\le\\)\u201d inequalities, \u201c\\(=\\)\u201d equalities, or set memberships.</p> </li> <li> <p>Objective Function Convexity: Is the objective \\(f_0(x)\\) convex? Check using the tools:</p> </li> <li> <p>Does \\(f_0(x)\\) match a known convex function type or a sum/composition of convex functions?</p> </li> <li>If it's smooth, is the Hessian \\(\\nabla^2 f_0(x) \\succeq 0\\) for all \\(x\\)?</li> <li>If non-smooth, can you reason via subgradients or epigraph definition that it\u2019s convex?</li> <li> <p>Any suspect terms (e.g. products, non-convex patterns) in the objective? If no issues and all tests indicate convex, proceed. If not convex, you have a non-convex problem (stop here, unless you plan to reformulate it).</p> </li> <li> <p>Inequality Constraints: For each constraint of the form \\(f_i(x) \\le 0\\), check \\(f_i(x)\\):</p> </li> <li>Is \\(f_i\\) convex? If yes, this constraint defines a convex region (the sublevel set). If any \\(f_i\\) is non-convex (e.g. concave or neither), that constraint is non-convex \u2013 the feasible set will not be convex. (Remember you can rewrite \\(g(x)\\ge 0\\) as \\(-g(x)\\le 0\\) and check \\(-g\\) for convexity.)</li> <li> <p>Also ensure the inequality is properly directed: a convex function should be \\(\\le 0\\), a concave function should be \\(\\ge 0\\) to be convex (since \\(g(x)\\ge 0\\) with \\(g\\) concave is the same as \\(-g(x)\\le 0\\) with \\(-g\\) convex).</p> </li> <li> <p>Equality Constraints: For each \\(h_j(x) = 0\\), determine if \\(h_j\\) is affine. If yes, it\u2019s fine (convex constraint). If not (e.g. quadratic or anything nonlinear), the problem is not convex (unless perhaps that equality can be eliminated or transformed in a special way). Non-affine equalities are a deal-breaker for convexity in almost all cases.</p> </li> <li> <p>Domain Constraints: If the problem statement includes \\(x \\in C\\) for some set \\(C\\), or implicit domain restrictions (like \\(x_i\\) must be integer, or \\(x &gt; 0\\)), verify those sets are convex. \\(x_i \\ge 0\\) (the nonnegative orthant) is convex. Norm balls, simplices, linear subspaces \u2013 all convex. But if \\(C\\) is, say, \\({0,1}^n\\) (binary vectors), or a finite set of points, or defined by a weird non-convex condition, then the problem is not convex. Ensure there is no hidden discreteness.</p> </li> <li> <p>Intersections: After checking all individual constraints, consider the intersection of all feasible conditions. If each constraint is convex, their intersection is convex. Double-check there isn\u2019t an implicit \u201ceither-or\u201d structure (which would actually be a union of sets, not an intersection). If it\u2019s a straightforward intersection of convex sets, you\u2019re good.</p> </li> <li> <p>Conclusion: If all the above checks pass \u2013 objective is convex, all inequalities convex, all equalities affine, domain convex \u2013 then congratulations, the problem is convex! You can now be confident that any local optimum you find is globally optimal, and you can leverage the rich theory and algorithms of convex optimization. If any check fails, the problem is not convex. In that case, you might need to try reformulating the problem (sometimes through clever algebra or change of variables) or resort to global optimization techniques if you must solve it as is.</p> </li> <li> <p>(Optional Advanced Check:) If you\u2019re still in doubt, one advanced technique is to consider the Lagrange dual of the problem (see later chapters) \u2013 convex problems have a well-behaved duality theory. Another is to examine the Fenchel conjugate of the objective: for a convex function, the conjugate is well-defined and useful. These are beyond the scope of this checklist, but they can sometimes provide confirmation. Generally, though, the steps above are sufficient in practice. in any optimization endeavor</p> </li> </ol>"},{"location":"2a%20Duality/","title":"Duality Theory, KKT Conditions, and Duality Gap","text":""},{"location":"2a%20Duality/#duality-theory-kkt-conditions-and-duality-gap","title":"Duality Theory, KKT Conditions, and Duality Gap","text":"<p>Duality theory is a central tool in optimization and machine learning. It provides alternative perspectives on problems, certificates of optimality, and insights into algorithm design. Applications include Support Vector Machines (SVMs), Lasso, and ridge regression.</p>"},{"location":"2a%20Duality/#1-convex-optimization-problem","title":"1. Convex Optimization Problem","text":"<p>Consider the standard convex optimization problem:</p> \\[ \\begin{aligned} &amp; \\min_{x \\in \\mathbb{R}^n} &amp; f_0(x) \\\\ &amp; \\text{s.t.} &amp; f_i(x) \\le 0, \\quad i = 1, \\dots, m \\\\ &amp; &amp; h_j(x) = 0, \\quad j = 1, \\dots, p \\end{aligned} \\] <ul> <li>\\(f_0\\): objective function.  </li> <li>\\(f_i\\): convex inequality constraints.  </li> <li>\\(h_j\\): affine equality constraints.  </li> </ul> <p>Feasible set:</p> \\[ \\mathcal{D} = \\{ x \\in \\mathbb{R}^n \\mid f_i(x) \\le 0, \\ h_j(x) = 0 \\}. \\]"},{"location":"2a%20Duality/#2-lagrangian-function","title":"2. Lagrangian Function","text":"<p>The Lagrangian incorporates constraints into the objective:</p> \\[ \\mathcal{L}(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{j=1}^p \\nu_j h_j(x) \\] <ul> <li>\\(\\lambda_i \\ge 0\\): dual variables for inequalities.  </li> <li>\\(\\nu_j\\): dual variables for equalities.  </li> </ul> <p>Intuition: \\(\\lambda_i\\) represents the \u201cprice\u201d of violating constraint \\(f_i(x) \\le 0\\). Larger \\(\\lambda_i\\) penalizes violations more.</p>"},{"location":"2a%20Duality/#3-dual-function-and-infimum","title":"3. Dual Function and Infimum","text":"<p>The dual function is:</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu), \\quad \\lambda \\ge 0 \\]"},{"location":"2a%20Duality/#31-infimum","title":"3.1 Infimum","text":"<ul> <li>The infimum (inf) of a function is its greatest lower bound: the largest number that is less than or equal to all function values.  </li> <li>Formally, for \\(f(x)\\):</li> </ul> \\[ \\inf_x f(x) = \\sup \\{ y \\in \\mathbb{R} \\mid f(x) \\ge y, \\ \\forall x \\}. \\] <ul> <li>Intuition: </li> <li>If \\(f(x)\\) has a minimum, the infimum equals the minimum.  </li> <li>If no minimum exists, the infimum is the value approached but never reached.</li> </ul> <p>Examples:</p> <ol> <li>\\(f(x) = x^2\\), \\(x \\in \\mathbb{R}\\) \u2192 \\(\\inf f(x) = 0\\) at \\(x = 0\\).  </li> <li>\\(f(x) = 1/x\\), \\(x &gt; 0\\) \u2192 \\(\\inf f(x) = 0\\), never attained.  </li> </ol>"},{"location":"2a%20Duality/#32-supremum","title":"3.2 Supremum","text":"<ul> <li>The supremum (sup) of a set \\(S \\subset \\mathbb{R}\\) is the least upper bound: the smallest number greater than or equal to all elements of \\(S\\).</li> </ul> \\[ \\sup S = \\inf \\{ y \\in \\mathbb{R} \\mid y \\ge s, \\ \\forall s \\in S \\} \\] <p>Example: \\(S = \\{ x \\in \\mathbb{R} \\mid x &lt; 1 \\}\\) \u2192 \\(\\sup S = 1\\), although no maximum exists.</p>"},{"location":"2a%20Duality/#33-why-the-dual-function-provides-a-lower-bound","title":"3.3 Why the Dual Function Provides a Lower Bound","text":"<p>For any feasible \\(x \\in \\mathcal{D}\\) and \\(\\lambda \\ge 0\\), \\(\\nu\\):</p> \\[ \\lambda_i f_i(x) \\le 0, \\quad \\nu_j h_j(x) = 0 \\implies \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x) \\] <p>Taking the infimum over all \\(x\\):</p> \\[ g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu) \\le f_0(x), \\quad \\forall x \\in \\mathcal{D} \\] <p>Thus, for any dual variables:</p> \\[ g(\\lambda, \\nu) \\le p^\\star \\] <ul> <li>Interpretation: The dual function is always a lower bound on the primal optimum.  </li> <li>Geometric intuition: Think of the dual as the highest \u201cfloor\u201d under the primal objective that supports the feasible region.  </li> </ul>"},{"location":"2a%20Duality/#4-the-dual-problem","title":"4. The Dual Problem","text":"<p>The dual problem seeks the tightest lower bound:</p> \\[ \\begin{aligned} \\max_{\\lambda, \\nu} \\quad &amp; g(\\lambda, \\nu) \\\\ \\text{s.t.} \\quad &amp; \\lambda \\ge 0 \\end{aligned} \\] <ul> <li>Dual optimal value: \\(d^\\star = \\max_{\\lambda \\ge 0, \\nu} g(\\lambda, \\nu)\\).  </li> <li>Always satisfies weak duality: \\(d^\\star \\le p^\\star\\).  </li> <li>If convexity + Slater's condition hold, strong duality: \\(d^\\star = p^\\star\\).</li> </ul>"},{"location":"2a%20Duality/#5-duality-gap","title":"5. Duality Gap","text":"<p>The duality gap measures the difference between primal and dual optima:</p> \\[ \\text{Gap} = p^\\star - d^\\star \\ge 0 \\] <ul> <li>Zero gap: strong duality (common in convex ML problems).  </li> <li>Positive gap: weak duality only; dual provides only a lower bound.  </li> </ul>"},{"location":"2a%20Duality/#51-causes-of-positive-gap","title":"5.1 Causes of Positive Gap","text":"<ol> <li>Nonconvex objective.  </li> <li>Constraint qualification fails (e.g., Slater\u2019s condition not satisfied).  </li> <li>Dual problem is infeasible or unbounded.</li> </ol>"},{"location":"2a%20Duality/#52-example","title":"5.2 Example","text":"<p>Primal problem:</p> \\[ \\min_x -x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Primal optimum: \\(p^\\star = -1\\) at \\(x^\\star = 1\\) </li> <li>Dual problem: \\(d^\\star = -\\infty\\) (unbounded below)  </li> <li>Gap: \\(p^\\star - d^\\star = \\infty\\) \u2192 positive duality gap.</li> </ul> <p>Interpretation: dual gives a guaranteed lower bound but may not achieve the primal optimum.</p>"},{"location":"2a%20Duality/#6-karushkuhntucker-kkt-conditions","title":"6. Karush\u2013Kuhn\u2013Tucker (KKT) Conditions","text":"<p>For convex problems with strong duality, KKT conditions fully characterize optimality.</p> <p>Let \\(x^\\star\\) be primal optimal and \\((\\lambda^\\star, \\nu^\\star)\\) dual optimal:</p> <ol> <li> <p>Primal feasibility: </p> </li> <li> <p>Dual feasibility: </p> </li> <li> <p>Stationarity: </p> </li> <li> <p>Use subgradients for nonsmooth problems (e.g., Lasso).  </p> </li> <li> <p>Complementary slackness: </p> </li> </ol> <p>Intuition: Only active constraints contribute; the \u201cforces\u201d of objective and constraints balance.</p>"},{"location":"2a%20Duality/#7-applications-in-machine-learning","title":"7. Applications in Machine Learning","text":""},{"location":"2a%20Duality/#71-ridge-regression","title":"7.1 Ridge Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 \\] <ul> <li>Smooth shrinkage, unique solution.  </li> <li>Dual view useful in kernelized ridge regression.</li> </ul>"},{"location":"2a%20Duality/#72-lasso-regression","title":"7.2 Lasso Regression","text":"\\[ \\min_w \\frac12 \\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1 \\] <ul> <li>KKT conditions explain sparsity:</li> </ul> \\[ X_j^\\top (y - Xw^\\star) = \\lambda s_j, \\quad s_j \\in  \\begin{cases} \\{\\text{sign}(w_j^\\star)\\}, &amp; w_j^\\star \\neq 0 \\\\ [-1,1], &amp; w_j^\\star = 0 \\end{cases} \\] <ul> <li>Basis for coordinate descent and soft-thresholding algorithms.</li> </ul>"},{"location":"2a%20Duality/#73-support-vector-machines-svms","title":"7.3 Support Vector Machines (SVMs)","text":"<ul> <li>Dual depends only on inner products \\(x_i^\\top x_j\\), enabling kernel methods.  </li> <li>Often more efficient if number of features \\(d\\) exceeds number of data points \\(n\\).</li> </ul>"},{"location":"2a%20Duality/#8-constrained-vs-penalized-optimization","title":"8. Constrained vs. Penalized Optimization","text":"<ul> <li>Constrained form:</li> </ul> \\[ \\min_w \\text{Loss}(w) \\quad \\text{s.t. } R(w) \\le t \\] <ul> <li>Penalized form:</li> </ul> \\[ \\min_w \\text{Loss}(w) + \\lambda R(w) \\] <ul> <li>Lagrange multiplier \\(\\lambda\\) acts as a \u201cprice\u201d on the constraint.  </li> <li>Equivalence holds for convex problems, but mapping \\(t \\leftrightarrow \\lambda\\) may be non-unique.</li> </ul>"},{"location":"2a%20Duality/#9-summary","title":"9. Summary","text":"<ol> <li>Infimum and supremum: greatest lower bound and least upper bound.  </li> <li>Dual function: \\(g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu)\\) always provides a lower bound.  </li> <li>Duality gap: \\(p^\\star - d^\\star\\), zero under strong duality, positive when dual does not attain primal optimum.  </li> <li>KKT conditions: necessary and sufficient for convex problems with strong duality.  </li> <li>ML connections: Ridge, Lasso, and SVM exploit duality for computation, sparsity, and kernelization.</li> </ol> <p>Key intuition: The dual function can be visualized as the highest supporting \u201cfloor\u201d under the primal objective. Maximizing it gives the tightest lower bound, and when strong duality holds, it meets the primal optimum exactly.</p>"},{"location":"3_1_gradientdescent/","title":"3 1 gradientdescent","text":"<p>Gradient descent is the most fundamental first-order method for convex optimization. It capitalizes on the fact that for a differentiable convex function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\), the gradient \\(\\nabla f(x)\\) indicates the direction of steepest increase (and \\(-\\nabla f(x)\\) the steepest decrease) at \\(x\\) (see Section A, Chapter 6). At a global minimizer \\(x^*\\) of a convex differentiable \\(f\\), the first-order optimality condition \\(\\nabla f(x^*) = 0\\) holds, and no descent direction exists. Gradient descent iteratively moves opposite to the gradient, seeking such a stationary point which, in convex problems, guarantees optimality.</p>"},{"location":"3_1_gradientdescent/#gradient-descent-algorithm-and-geometry","title":"Gradient Descent: Algorithm and Geometry","text":"<p>Starting from an initial guess \\(x_0\\in\\mathcal{X}\\), the gradient descent update (for unconstrained problems \\(\\mathcal{X}=\\mathbb{R}^n\\)) is:</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k) \\] <p>where \\(\\alpha&gt;0\\) is a step size (also called learning rate). Geometrically, this update follows the tangent plane of \\(f\\) at \\(x_k\\) a short distance in the negative gradient direction. Intuitively, one approximates \\(f(x)\\) near \\(x_k\\) by the first-order Taylor expansion \\(f(x)\\approx f(x_k) + \\langle \\nabla f(x_k),,x - x_k\\rangle\\) and then minimizes this linear model plus a small quadratic regularization to keep the step local (see Section A\u2019s discussion of Taylor expansions). The result is exactly the gradient step \\(x_{k+1}=x_k - \\alpha \\nabla f(x_k)\\). This can be viewed as the steepest descent direction in the Euclidean metric (the gradient direction is orthogonal to level sets of \\(f\\) and points toward lower values). By choosing \\(\\alpha\\) appropriately (e.g. via a line search or a bound on the Lipschitz constant of \\(\\nabla f\\)), each step guarantees a sufficient decrease in \\(f\\).</p> <p>Recall: If \\(f\\) is \\(L\\)-smooth (i.e. \\(\\nabla f\\) is Lipschitz continuous with constant \\(L\\); see Section B, Lipschitz Continuity), then taking \\(\\alpha \\le 1/L\\) ensures \\(f(x_{k+1}) \\le f(x_k)\\) for gradient descent. The descent lemma (from Section A) ensures the update is stable and \\(f\\) decreases because the linear improvement \\(- \\alpha |\\nabla f(x_k)|^2\\) dominates the quadratic error \\(\\frac{L\\alpha^2}{2}|\\nabla f(x_k)|^2\\) when \\(\\alpha \\le 1/L\\).</p> <p>Choosing the step size \\(\\alpha\\): A smaller \\(\\alpha\\) yields cautious, stable steps (useful if the landscape is ill-conditioned), whereas a larger \\(\\alpha\\) accelerates progress but can overshoot and even diverge if too large. In practice one may use backtracking or exact line search to adapt \\(\\alpha\\). Under constant \\(\\alpha\\) within the safe range, convergence is guaranteed for convex \\(f\\).</p>"},{"location":"3_1_gradientdescent/#convergence-rates-convex-vs-strongly-convex","title":"Convergence Rates: Convex vs Strongly Convex","text":"<p>For general convex and \\(L\\)-smooth \\(f\\), gradient descent achieves a sublinear convergence rate in function value. In particular, after \\(k\\) iterations one can guarantee</p> \\[ f(x_k) - f(x^*) \\le \\frac{L \\|x_0 - x^*\\|^2}{2k}, \\quad \\text{so} \\quad f(x_k) - f(x^*) = O(1/k) \\] <p>This means to get within \\(\\varepsilon\\) of the optimal value, on the order of \\(1/\\varepsilon\\) iterations are needed. This \\(O(1/k)\\) rate is often called sublinear convergence. If we further assume \\(f\\) is \\(\\mu\\)-strongly convex (see Section B: strong convexity ensures a unique minimizer and a quadratic lower bound), gradient descent\u2019s convergence improves dramatically to a linear rate. In fact, for an \\(L\\)-smooth, \\(\\mu\\)-strongly convex \\(f\\), there exist constants \\(0&lt;c&lt;1\\) such that</p> \\[ f(x_k) - f(x^*) = O(c^k) \\] <p>i.e. \\(f(x_k)\\) approaches \\(f(x^)\\) geometrically fast. Equivalently, the error decreases by a constant factor \\((1-\\tfrac{\\mu}{L})\\) (or similar) at every iteration. For example, choosing \\(\\alpha = 2/(\\mu+L)\\) yields \\(|x_{k+1}-x^| \\le \\frac{L-\\mu}{L+\\mu},|x_k - x^|\\), so \\(|x_k - x^| = O((\\frac{L-\\mu}{L+\\mu})^k)\\). Such linear convergence (also called exponential convergence) implies \\(O(\\log(1/\\varepsilon))\\) iterations to reach accuracy \\(\\varepsilon\\). Strong convexity essentially provides a uniformly curved bowl shape, preventing flat regions and guaranteeing that gradient steps won\u2019t diminish to zero too early.</p> <p>Summary: For convex \\(f\\): \\(f(x_k)-f(x^) = O(1/k)\\); if \\(f\\) is \\(\\mu\\)-strongly convex: \\(f(x_k)-f(x^) = O((1-\\eta\\mu)^k)\\) (linear). These rates assume appropriate constant step sizes. (See Section B, \u201cFunction Properties,\u201d for a detailed table of convergence rates of first-order methods.)</p> <p>When does gradient descent perform poorly? If the problem is ill-conditioned (the Hessian has a high condition number \\(\\kappa = L/\\mu\\)), progress along some directions is much slower than others. The iterates may \u201czig-zag\u201d through narrow valleys, requiring very small \\(\\alpha\\) to maintain stability. In fact, the iteration complexity of gradient descent is \\(O(\\kappa \\log(1/\\varepsilon))\\) for strongly convex problems \u2013 linearly dependent on the condition number. Chapter C3 on Newton\u2019s method will address this issue by preconditioning with second-order information.</p>"},{"location":"3_1_gradientdescent/#subgradient-method-for-nondifferentiable-convex-functions","title":"Subgradient Method for Nondifferentiable Convex Functions","text":"<p>Gradient descent requires \\(\\nabla f(x)\\) to exist. Many convex problems in machine learning involve nondifferentiable objectives (e.g. hinge loss in SVMs, \\(L_1\\)-regularization in Lasso, ReLU activations). Subgradient methods generalize gradient descent to such functions using a subgradient in place of the gradient. Recall from Section B that for a convex function \\(f\\), a subgradient \\(g\\) at \\(x\\) is any vector that supports \\(f\\) at \\(x\\), meaning \\(f(y) \\ge f(x) + \\langle g,,y-x\\rangle\\) for all \\(y\\). The set of all subgradients at \\(x\\) is the subdifferential \\(\\partial f(x)\\). If \\(f\\) is differentiable at \\(x\\), then \\(\\partial f(x)={\\nabla f(x)}\\); if \\(f\\) has a kink, \\(\\partial f(x)\\) is a whole set (e.g. for \\(f(u)=|u|\\), any \\(g\\in[-1,1]\\) is a subgradient at \\(u=0\\)).</p> <p>The subgradient method update mirrors gradient descent:</p> \\[ x_{k+1} = x_k - \\eta_k g_k \\] <p>where \\(g_k \\in \\partial f(x_k)\\) is any chosen subgradient at \\(x_k\\), and \\(\\eta_k&gt;0\\) is a step size. If \\(x\\) is constrained to a convex set \\(\\mathcal{X}\\), one includes a projection onto \\(\\mathcal{X}\\): \\(x_{k+1} = \\Pi_{\\mathcal{X}}!(x_k - \\eta_k g_k)\\) (this reduces to the unconstrained update if \\(\\mathcal{X}=\\mathbb{R}^n\\)). Geometrically, even though \\(f\\)\u2019s graph may have corners, a subgradient \\(g_k\\) defines a supporting hyperplane at \\((x_k, f(x_k))\\). The update \\(-\\eta_k g_k\\) is a feasible descent direction because \\(\\langle g_k, x^* - x_k\\rangle \\ge f(x^*)-f(x_k)\\) for the minimizer \\(x^*\\), by convexity. Thus, moving a small amount opposite to \\(g_k\\) tends to decrease \\(f\\). One can still \u201csnap back\u201d to the feasible region via projection as needed, analogous to the projected gradient step for constraints. This guarantees \\(x_k\\) stays in \\(\\mathcal{X}\\) (see below for projection geometry).</p> <p>Convergence of subgradient methods: Unlike gradient descent, we cannot generally use a fixed \\(\\eta\\) to get convergence (the method won\u2019t settle to a single point because of the zig-zagging on corners). Instead, a common strategy is a diminishing step size or an averaging of iterates. A typical result for minimizing a convex \\(f\\) with Lipschitz-bounded subgradients (\\(|g_k|\\le G\\)) is: using \\(\\eta_k = \\frac{R}{G\\sqrt{k}}\\) (with \\(R = |x_0 - x^*|\\)), the averaged iterate \\(\\bar{x}T = \\frac{1}{T}\\sum{t=1}^T x_t\\) satisfies</p> \\[ f(\\bar{x}_T) - f(x^*) \\le \\frac{R G}{T} = O(1/T) \\] <p>This is a sublinear rate slower than \\(O(1/T)\\), reflecting the cost of nondifferentiability. In fact, \\(O(1/\\sqrt{T})\\) is the worst-case optimal rate for first-order methods on nonsmooth convex problems (no algorithm can generally do better than this without additional structure). If \\(f\\) is also strongly convex, faster subgradient convergence is possible (e.g. \\(O(\\log T / T)\\) with specialized step schemes or using a known optimal value in Polyak\u2019s step size), but it\u2019s still slower than the smooth case. Importantly, the subgradient method does not converge to the exact minimizer unless \\(\\eta_k\\to 0\\); typically one gets arbitrarily close but keeps bouncing around the optimum. This is why an ergodic average \\(\\bar{x}_T\\) is used in the guarantee above \u2013 it smooths out oscillations.</p> <p>Projection and feasibility: When constraints \\(\\mathcal{X}\\) are present, the subgradient update includes a projection \\(\\Pi_{\\mathcal{X}}(\\cdot)\\). Recall from Section A (Geometry of Orthogonal Projection) that for a closed convex set \\(\\mathcal{X}\\), the projection \\(\\Pi_{\\mathcal{X}}(y) = \\arg\\min_{x\\in\\mathcal{X}}|x - y|\\) yields the closest feasible point to \\(y\\). The projection error \\(y - \\Pi_{\\mathcal{X}}(y)\\) is orthogonal to \\(\\mathcal{X}\\) at the projection point (no improvement can be made along the feasible surface). Thus, \\(x_{k+1} = \\Pi_{\\mathcal{X}}(x_k - \\eta g_k)\\) can be seen as: take a step in the subgradient direction, then drop perpendicularly back into the set. This ensures feasibility of iterates while still achieving descent on \\(f\\). For example, if \\(\\mathcal{X}\\) is the \\(\\ell_2\\) unit ball, the projection \\(\\Pi_{\\mathcal{X}}(y)\\) simply scales \\(y\\) to have norm 1 if it was outside.</p> <p>Use cases: Subgradient methods shine in nonsmooth problems like L1-regularized models (Lasso), SVM hinge loss, and combinatorial convex relaxations, where gradients are not available. They are very simple (each step is like gradient descent), but one must carefully tune the step schedule. In practice, subgradient methods can be slow to get high accuracy; however, their simplicity and ability to handle nondifferentiability make them a go-to baseline. Techniques like momentum or adaptive step sizes can sometimes improve practical performance, but fundamentally \\(O(1/\\sqrt{T})\\) is the regime for nonsmooth convex minimization.</p> <p>Example: Consider \\(f(x) = |x|\\) (which is nonsmooth at 0). The subgradient method for \\(\\min_x |x|1\\) (with no smooth part) would at iteration \\(k\\) choose some \\(g_k \\in \\partial |x_k|\\) (which could be \\(g_k = \\text{sign}(x_k)\\) componentwise), and do \\(x{k+1}=x_k - \\eta g_k\\). This essentially performs soft-thresholding on each coordinate: if \\(x_k\\) was positive, it decreases it, if negative, increases it, if zero, it stays within [-\\(\\eta,\\eta\\)]. Indeed, with an appropriate choice of \\(\\eta\\), one can show \\(x_k\\) converges to 0 (the minimizer). This principle underlies the ISTA/FISTA algorithms for Lasso: each step is \\(x{k+1} = S{\\lambda\\eta}(x_k - \\eta \\nabla f_{\\text{smooth}}(x_k))\\), where \\(S_{\\tau}(y)=\\text{sign}(y)\\max{|y|-\\tau,,0}\\) is the soft-thresholding operator. We will see proximal gradient methods in Chapter C4 that formalize this approach.</p> <p>Summary: Gradient descent is the workhorse for smooth convex problems, enjoying \\(O(1/k)\\) or better convergence and simplicity. Its limitation is the requirement of differentiability and sometimes slow progress in ill-conditioned settings. The subgradient method extends applicability to nondifferentiable convex functions at the expense of slower convergence. In both cases, the geometry of convexity (supporting hyperplanes, gradient orthogonality, etc.) underpins why moving opposite to a (sub)gradient leads toward the optimum.</p>"},{"location":"3_2_acc_gd/","title":"3 2 acc gd","text":"<p>Gradient descent\u2019s \\(O(1/k)\\) convergence on smooth convex problems is suboptimal: there are algorithms that attain faster rates. Accelerated gradient methods, pioneered by Yurii Nesterov, achieve an optimal \\(O(1/k^2)\\) convergence rate for first-order convex optimization. The core idea is to incorporate momentum \u2014 using information from past iterations to build up speed in the right direction.</p> <p>Momentum Intuition: Heavy Ball Analogy Imagine rolling a ball down a convex hill. Standard gradient descent is like a heavy ball with extremely high friction: at each step, you come to a stop and then roll a tiny bit further based purely on the current slope. In narrow valleys, this leads to a slow, zig-zag descent because the ball is constantly stopping and re-accelerating down the sides of the valley. Momentum provides the ball with inertia, so it keeps moving in the previous direction even as it responds to the current slope. This can smooth out oscillations and traverse flat regions faster.</p> <p>In practical terms, momentum means we introduce a velocity term \\(v_k\\) that accumulates past gradients. A basic gradient descent with momentum update is:</p> \\[ \\begin{aligned} v_{k+1} &amp;= \\beta v_k - \\alpha \\nabla f(x_k), \\\\ x_{k+1} &amp;= x_k + v_{k+1}. \\end{aligned} \\] <p>where \\(\\beta\\in[0,1)\\) is the momentum coefficient controlling how much of the past velocity \\(v_k\\) to retain. When written fully in terms of positions \\(x\\), this is equivalent to:</p> \\[ x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1}) \\] <p>so the step has two components: a usual gradient descent step \\((-\\alpha \\nabla f(x_k))\\) plus a push in the direction of the previous motion \\((x_k - x_{k-1})\\). If recent gradients have been pointing in roughly the same direction, the velocity term \\(\\beta(x_k - x_{k-1})\\) adds a significant boost, effectively increasing the step size in low-curvature directions. If the gradient direction changes (oscillations), the momentum term can partially cancel out the back-and-forth, preventing full oscillation.</p> <p>Benefits: In convex problems with long narrow valleys (high condition number), momentum helps \u201cfly through\u201d the valley floor instead of tediously zig-zagging. It\u2019s like smoothing the path by dampening orthogonal oscillations and amplifying movement aligned with consistent gradients. Empirically, momentum often yields faster decrease in objective value than vanilla gradient descent.</p> <p>Heavy-ball vs. Nesterov acceleration: The above momentum scheme is often called Polyak\u2019s heavy-ball method. Nesterov\u2019s Accelerated Gradient (NAG) is a slight modification that delivers rigorous guarantees. In NAG, one first takes a step in the old velocity direction to a predicted intermediate point, then evaluates the gradient there:</p> <p>\u200b </p> <p>This \u201clookahead\u201d (evaluating \\(\\nabla f\\) at \\(y_k\\)) is the key difference \u2013 it anticipates where momentum is taking the iterate and then corrects the course with a gradient measured at that extrapolated point. The Nesterov update can be seen as adding momentum in a way that does not overshoot the optimum: by the time you apply the gradient, you\u2019re already partway there, so you don\u2019t \u201ccoast\u201d blindly beyond the minimizer.</p> <p>Remarkably, Nesterov proved that with a specific sequence of \\(\\beta\\) values, his method achieves</p> <ul> <li> <p>\\(O(1/k^2)\\) convergence in function value for convex, \\(L\\)-smooth \\(f\\), and</p> </li> <li> <p>\\(O((1-\\sqrt{\\mu/L})^k)\\) linear convergence for \\(\\mu\\)-strongly convex, \\(L\\)-smooth \\(f\\),</p> </li> </ul> <p>both of which are optimal in the sense that no first-order method can attain better worst-case rates in those scenarios. In other words, accelerated gradient descent (AGD/Nesterov\u2019s method) is provably as fast as possible for large-scale convex optimization using only gradient information.</p> <p>Geometric perspective: Momentum methods can be interpreted as a discrete approximation to a second-order ordinary differential equation of motion (like a damped oscillator). The heavy-ball method corresponds to a physical mass sliding with friction on the landscape \\(f(x)\\). Nesterov\u2019s method adds a clever adjustment akin to a lookahead that ensures stability and optimal speed. In continuous time, both can be seen as putting a \\(\\ddot{x}\\) (acceleration) term into the dynamic, which leads to faster approach to the minimum compared to the \\(\\dot{x}\\) (first-order) dynamic of plain gradient flow.</p> <p>Acceleration in Practice</p> <p>For practical implementation, one typically fixes \\(\\beta \\approx 0.9\\) or adjusts it adaptively, and uses a fixed or decaying \\(\\alpha\\). Nesterov\u2019s variant is only slightly more involved than heavy-ball and is often used by default in deep learning (many optimizers like Adam incorporate Nesterov momentum by computing gradients at a predicted step). The momentum parameter \\(\\beta\\) close to 1 gives long memory (the velocity is a long running average of gradients), which can greatly speed up convergence on well-behaved problems but might cause overshooting if the landscape changes abruptly. Empirically, momentum with \\(\\beta\\in[0.9,0.99]\\) tends to work well, accelerating progress especially in the early stages of convex optimization.</p> <p>It\u2019s worth noting that heavy-ball momentum does not always guarantee faster convergence in theory (for general convex functions it can even diverge if mis-tuned), whereas Nesterov\u2019s momentum comes with the aforementioned guarantees. For strongly convex quadratic objectives, heavy-ball momentum achieves the accelerated linear rate \\(O((1-\\sqrt{\\mu/L})^k)\\) as well, but for non-quadratics Nesterov\u2019s method is preferred due to its robustness.</p> <p>Summary: Accelerated gradient methods introduce a memory of past gradients that propels iterates forward more forcefully than standard gradient descent. The result is a significant speed-up on convex problems \u2013 roughly, acceleration takes \\(O(\\sqrt{\\kappa}\\log(1/\\varepsilon))\\) iterations instead of \\(O(\\kappa\\log(1/\\varepsilon))\\) for \\(\\kappa\\)-conditioned problems. In intuitive terms, momentum helps the optimizer build velocity along directions of consistent descent and dampen oscillations in awkward directions. Nesterov\u2019s Accelerated Gradient (AGD) refines this idea to guarantee the fastest possible convergence rate \\(O(1/k^2)\\) for smooth convex minimization. This has made Nesterov\u2019s method a cornerstone in both theoretical optimization and practical algorithms (it\u2019s often the default choice when one needs faster convergence from gradient steps).</p>"},{"location":"3_3_newton/","title":"3 3 newton","text":"<p>First-order methods (gradient descent and its variants) use only gradient information and can be slow on ill-conditioned problems. Newton\u2019s method is a second-order algorithm that uses the Hessian (matrix of second derivatives) to curvature-correct the steps. Newton\u2019s method can converge in far fewer iterations \u2014 often quadratically fast near the optimum \u2014 by taking into account how the gradient changes, not just its value.</p>"},{"location":"3_3_newton/#newtons-method-using-second-order-information","title":"Newton\u2019s Method: Using Second-Order Information","text":"<p>Newton\u2019s method is derived from the second-order Taylor approximation of \\(f\\) around the current point \\(x_k\\). We approximate:</p> \\[ f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^\\top d + \\tfrac{1}{2} \\, d^\\top \\nabla^2 f(x_k) \\, d \\] <p>where \\(\\nabla^2 f(x_k)\\) (the Hessian matrix \\(H_k\\)) captures the local curvature. This quadratic model of \\(f\\) is minimized (setting derivative to zero) by solving \\(H_k,d = -\\nabla f(x_k)\\). Thus the Newton step is</p> \\[ d_{\\text{newton}} = -[\\nabla^2 f(x_k)]^{-1} \\, \\nabla f(x_k) \\] <p>and the update becomes \\(x_{k+1} = x_k + d_{\\text{newton}} = x_k - H_k^{-1}\\nabla f(x_k)\\). In other words, we scale the gradient by the inverse Hessian, which adjusts the step length in each direction according to curvature. Directions in which the function curves gently (small Hessian eigenvalues) get a larger step, and directions of steep curvature (large eigenvalues) get a smaller step. This preconditioning by \\(H_k^{-1}\\) leads to much more isotropic progress toward the optimum.</p> <p>Geometric interpretation: Newton\u2019s method is effectively performing gradient descent in a re-scaled space where the metric is defined by the Hessian. At \\(x_k\\), consider the local quadratic approximation \\(q_k(d) = f(x_k) + \\nabla f(x_k)^\\top d + \\frac{1}{2}d^\\top H_k d\\). This is a bowl-shaped function (assuming \\(H_k\\) is positive-definite, which is true if \\(f\\) is locally convex). The minimizer of \\(q_k\\) is \\(d_{\\text{newton}}\\) as above. Newton\u2019s method jumps directly to the bottom of this local quadratic model. If the model were exact (as it would be for a quadratic \\(f\\)), Newton\u2019s step would land at the exact minimizer in one iteration. For non-quadratic \\(f\\), the step is only approximate, but as \\(x_k\\) approaches \\(x^*\\), the quadratic model becomes very accurate and Newton steps become nearly exact.</p> <p>One way to view Newton\u2019s update is as iterative refinement: the update \\(x_{k+1} = x_k - H_k^{-1}\\nabla f(x_k)\\) solves the linear system \\(\\nabla^2 f(x_k), (x_{k+1}-x_k) = -\\nabla f(x_k)\\), which is the Newton condition for a stationary point of the second-order model. Thus Newton\u2019s method finds where the gradient would be zero if the current local curvature remained constant. This typically yields a huge improvement in \\(f\\). In effect, Newton\u2019s method stretches/scales space so that in the new coordinates the function looks like a unit ball shape (equal curvature in all directions), then it makes a standard step. After each step, a new linearization occurs at the new point.</p>"},{"location":"3_3_newton/#convergence-properties-and-affine-invariance","title":"Convergence Properties and Affine Invariance","text":"<p>When \\(f\\) is strongly convex with a Lipschitz-continuous Hessian, Newton\u2019s method exhibits quadratic convergence in a neighborhood of the optimum. This means once \\(x_k\\) is sufficiently close to \\(x^*\\), the error shrinks squared at each step: \\(|x_{k+1}-x^*| = O(|x_k - x^*|^2)\\). Equivalently, the number of correct digits in the solution roughly doubles every iteration. In terms of \\(f(x_k) - f(x^*)\\), if gradient descent was \\(O(c^k)\\) and accelerated gradient \\(O(1/k^2)\\), Newton\u2019s local rate is on the order of \\(O(c^{2^k})\\) \u2013 extremely fast. For example, if you are 1% away from optimum at iteration \\(k\\), Newton\u2019s method might be 0.01% away at iteration \\(k+1\\). This fast convergence is what makes Newton\u2019s method so powerful for well-behaved problems. (Formally, one can show near \\(x^*\\): \\(f(x_{k+1})-f(x^*) \\le C [f(x_k)-f(x^*)]^2\\) under appropriate conditions, hence the log of the error drops exponentially.)</p> <p>However, global convergence is not guaranteed without modifications: if started far from the optimum or if \\(H_k\\) is not positive-definite, the Newton step might not even be a descent direction (e.g., on nonconvex or badly scaled functions, Newton\u2019s step can overshoot or go to a saddle). To address this, in practice one uses a damped Newton method: incorporate a step size \\(\\lambda_k\\in(0,1]\\) and update \\(x_{k+1} = x_k - \\lambda_k H_k^{-1}\\nabla f(x_k)\\). Typically \\(\\lambda_k\\) is chosen by a line search to ensure \\(f(x_{k+1})&lt;f(x_k)\\). Early on, \\(\\lambda_k\\) might be small (cautious steps) while \\(x_k\\) is far from optimum, but eventually \\(\\lambda_k\\) can be taken as 1 (full Newton steps) in the vicinity of the solution, recovering the rapid quadratic convergence. This strategy ensures global convergence: from any starting point in a convex problem, damped Newton will converge to \\(x^*\\).</p> <p>A remarkable property of Newton\u2019s method is affine invariance. This means the trajectory of Newton\u2019s method is independent of linear coordinate transformations of the problem. If we apply an invertible affine mapping \\(y = A^{-1}x\\) to the variables and solve in \\(y\\)-space, Newton\u2019s steps in \\(y\\) map exactly to Newton\u2019s steps in \\(x\\)-space under \\(A\\). In contrast, gradient descent is not affine invariant (scaling coordinates stretches the gradient in those directions, affecting the path and convergence speed). Affine invariance highlights that Newton\u2019s method automatically handles conditioning and scaling: by using \\(H_k^{-1}\\) it \u201cpreconditions\u201d the problem optimally for the local quadratic structure. Another way to say this: Newton\u2019s method is invariant to quadratic change of coordinates, because the Hessian provides the curvature metric. This is why Newton\u2019s method is extremely effective on ill-conditioned problems; it essentially neutralizes the condition number by working in the Hessian\u2019s eigenbasis where the function looks round.</p> <p>Cost per iteration: The main drawback of Newton\u2019s method is the cost of computing and inverting the \\(n \\times n\\) Hessian. This is \\(O(n^3)\\) in general for matrix inversion or solving \\(H_k d = -\\nabla f\\) (though exploiting structure or approximations can reduce this). For very high-dimensional problems (like \\(n\\) in the millions), Newton\u2019s method becomes impractical. It\u2019s mainly used when \\(n\\) is moderate (up to a few thousands perhaps), or \\(H_k\\) has special structure (sparse or low-rank updates). Each Newton iteration is expensive, but ideally you need far fewer iterations than first-order methods. There is a trade-off between doing more cheap steps (gradient descent) versus fewer expensive steps (Newton).</p>"},{"location":"3_3_newton/#quasi-newton-methods-bfgs-l-bfgs","title":"Quasi-Newton Methods (BFGS, L-BFGS)","text":"<p>Quasi-Newton methods aim to retain the fast convergence of Newton\u2019s method without having to compute the exact Hessian. They do this by building up an approximate Hessian inverse from successive gradient evaluations. The most famous is the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm, which iteratively updates a matrix \\(H_k\\) intended to approximate \\([\\nabla^2 f(x_k)]^{-1}\\) (the inverse Hessian). At each iteration, after computing \\(\\nabla f(x_k)\\), the difference in gradients \\(\\Delta g = \\nabla f(x_k) - \\nabla f(x_{k-1})\\) and the step \\(\\Delta x = x_k - x_{k-1}\\) are used to adjust the previous estimate \\(H_{k-1}\\) into a new matrix \\(H_k\\) that satisfies the secant condition: \\(H_k, \\Delta g = \\Delta x\\). This condition ensures \\(H_k\\) captures the curvature along the most recent step. The BFGS update formula (a specific symmetric rank-two update) guarantees that \\(H_k\\) remains positive-definite and tends to become a better approximation over time stat.cmu.edu . In simplified terms, BFGS \u201clearns\u201d the curvature of \\(f\\) as the iterations progress, by observing how the gradient changes with steps.</p> <p>BFGS updates have the form: \u200b  </p> <p>which is efficient to compute (rank-2 update on the matrix). One can show (under certain assumptions) that these updates lead \\(H_k\\) to converge to the true inverse Hessian as \\(k\\) grows. Practically, after enough iterations, the direction \\(p_k = -H_k \\nabla f(x_k)\\) behaves like the Newton direction. BFGS with an appropriate line search is known to achieve superlinear convergence (faster than any geometric rate, though not quite quadratic) once in the neighborhood of the optimum, for strongly convex functions. It\u2019s a very effective compromise: each iteration is only \\(O(n^2)\\) to update the \\(H_k\\) and compute \\(p_k\\) (much cheaper than \\(O(n^3)\\) for solving Newton equations), but the iteration count remains low.</p> <p>For very large \\(n\\), storing the full \\(H_k\\) becomes memory-intensive (\\(n^2\\) elements). L-BFGS (Limited-memory BFGS) addresses this by never storing the full matrix; instead it maintains a history of only the last \\(m\\) updates \\((\\Delta x, \\Delta g)\\) and implicitly defines \\(H_k\\) via this limited history. The user specifies a small memory parameter (say \\(m=5\\) or \\(10\\)), so L-BFGS uses only the last \\(m\\) gradient evaluations to build a compressed approximate Hessian. Each iteration then costs \\(O(nm)\\), which is only linear in \\(n\\). L-BFGS is very popular for large-scale convex optimization because it often provides a good acceleration over plain gradient descent with minimal overhead in memory/computation.</p> <p>Quasi-Newton vs Newton: Quasi-Newton methods, especially BFGS, often approach the performance of Newton\u2019s method without needing an analytic Hessian. They are not affine invariant (scaling the inputs can affect the updates), but they are far more robust than simple gradient descent on difficult problems. Since they rely only on gradient evaluations, they can be applied in situations where Hessians are unavailable or too expensive. In machine learning, BFGS/L-BFGS were historically popular for training logistic regression, CRFs, and other convex models before first-order stochastic methods became dominant for extremely large data. They are still used for moderate-scale problems or as subsolvers in higher-level algorithms.</p> <p>BFGS in action: One way to appreciate BFGS is that it preconditions gradient descent on the fly. Early iterations of BFGS behave like a quick learning phase: the algorithm figures out an effective metric to apply to the gradients. After a while, the \\(H_k\\) matrix it builds \u201cwhitens\u201d the Hessian of \\(f\\) \u2013 making the level sets more spherical \u2013 so that subsequent updates take nearly optimal routes to \\(x^*\\). Indeed, BFGS determines the descent direction by preconditioning the gradient with curvature information accumulated from past steps. It\u2019s like doing Newton\u2019s method with an approximate Hessian that is refined over time.</p> <p>Summary: Newton\u2019s method uses second-order derivatives to achieve rapid (quadratic) convergence, at the expense of heavy per-iteration work. Quasi-Newton methods like BFGS approximate the second-order info with smart updating rules, achieving superlinear convergence in practice with much lower computational cost per iteration. They strike a balance between first-order and second-order methods and are often the fastest methods for smooth convex optimization when the problem size permits. The geometric insight is that both Newton and quasi-Newton are curvature-aware: they scale gradient directions according to the problem\u2019s geometry, which dramatically improves convergence especially on ill-conditioned problems (where gradients alone struggle).</p>"},{"location":"3_4_proximalprojected/","title":"3 4 proximalprojected","text":"<p>Real-world convex problems often involve constraints and nonsmooth terms. Projected gradient methods and proximal gradient methods extend our first-order algorithms to handle these situations by incorporating projection or proximal steps into the update.</p>"},{"location":"3_4_proximalprojected/#projected-gradient-descent-constraints","title":"Projected Gradient Descent (Constraints)","text":"<p>Suppose we want to minimize \\(f(x)\\) subject to \\(x \\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is a convex feasible set (e.g. a polytope or ball). A gradient descent step \\(y_{k} = x_k - \\alpha \\nabla f(x_k)\\) might produce a point \\(y_k\\) outside \\(\\mathcal{X}\\). The idea of projected gradient descent is simple: after the gradient step, project \\(y_k\\) back onto \\(\\mathcal{X}\\):</p> <p> </p> <p>Here \\(\\Pi_{\\mathcal{X}}(y) = \\arg\\min_{x\\in \\mathcal{X}}|x - y|^2\\) is the orthogonal projection onto \\(\\mathcal{X}\\). This two-step iteration ensures that \\(x_{k+1}\\in\\mathcal{X}\\) for all \\(k\\). Geometrically, we take the usual gradient descent step into \\(y_k\\) which may lie off the feasible set, then find the closest feasible point. The correction is orthogonal to the boundary of \\(\\mathcal{X}\\) at the projection point (no component of the step along the boundary is wasted, since \\(y_k - x_{k+1}\\) is perpendicular to \\(\\mathcal{X}\\)). Thus, projected gradient still decreases \\(f(x)\\) to first order while respecting the constraints.</p> <p>For example, if \\(\\mathcal{X}={x:|x|_2 \\le 1}\\) (the unit Euclidean ball), the projection is \\(\\Pi{\\mathcal{X}}(y) = \\frac{y}{\\max{1,|y|_2}}\\): any \\(y\\) outside the ball is radially scaled back to the boundary, and if \\(y\\) is inside the ball it stays unchanged. Projected gradient descent is widely used in constrained convex optimization (like projection onto probability simplex, box constraints \\(x\\in [l,u]\\), etc.) because of its simplicity: one just needs a routine to project onto \\(\\mathcal{X}\\), which is often straightforward.</p> <p>Convergence: If \\(f\\) is convex and \\(L\\)-smooth, projected gradient descent inherits similar convergence guarantees as unconstrained gradient descent (albeit the analysis uses more geometry from \\(\\mathcal{X}\\)). With a suitable step size, \\(f(x_k)\\to f(x^*)\\) and for strongly convex problems \\(x_k\\to x^*\\) linearly. The projection step does not spoil convergence; it only ensures feasibility. Intuitively, the error analysis includes an extra term for distance from \\(\\mathcal{X}\\), but the projection minimizes that, keeping the iterates as close as possible to the unconstrained path.</p> <p>Projection is actually a special case of a more general operator important in convex optimization: the proximal operator.</p>"},{"location":"3_4_proximalprojected/#proximal-operators-and-proximal-gradient","title":"Proximal Operators and Proximal Gradient","text":"<p>Consider an optimization problem with a convex but nonsmooth term, for example:</p> \\[ \\min_{x \\in \\mathbb{R}^n} \\; f(x) + g(x) \\] <p>where \\(f(x)\\) is convex and differentiable (smooth loss or fit term) and \\(g(x)\\) is convex but possibly nondifferentiable (regularizer or indicator of constraints). Here \\(g(x)\\) could be things like \\(L_1\\) norm (\\(\\ell_1\\) penalty), \\(\\ell_\\infty\\) norm, indicator functions enforcing \\(x\\) in some set, etc. The proximal gradient method addresses such problems by splitting the handling of \\(f\\) and \\(g\\).</p> <p>We know how to deal with \\(f\\) using a gradient step. To handle \\(g\\), we use its proximal operator. The proximal operator of \\(g\\) (with parameter \\(\\lambda&gt;0\\)) is defined as:</p> \\[ \\operatorname{prox}_{\\lambda g}(y) = \\arg\\min_x \\left\\{ g(x) + \\frac{1}{2\\lambda} \\|x - y\\|^2 \\right\\} \\] <p>This is the solution of a regularized minimization of \\(g(x)\\) where we stay as close as possible to a given point \\(y\\). In words, \\(\\operatorname{prox}{\\lambda g}(y)\\) is a point that compromises between minimizing \\(g(x)\\) and staying near \\(y\\) (the quadratic term \\(\\frac{1}{2\\lambda}|x-y|^2\\) keeps \\(x\\) from straying too far from \\(y\\)). The parameter \\(\\lambda\\) scales how strongly we insist on proximity to \\(y\\): as \\(\\lambda \\to 0\\), \\(\\operatorname{prox}{\\lambda g}(y)\\approx y\\) (we barely move); as \\(\\lambda\\) grows, we\u2019re willing to move farther to reduce \\(g(x)\\). The proximal operator is well-defined for any closed convex \\(g\\) (it has a unique minimizer due to strong convexity of the quadratic term), and it generalizes the notion of projection:</p> <p>If \\(g(x)\\) is the indicator function of a convex set \\(\\mathcal{X}\\) (meaning \\(g(x)=0\\) if \\(x\\in\\mathcal{X}\\) and \\(+\\infty\\) otherwise), then \\(\\operatorname{prox}{\\lambda g}(y)\\) is exactly \\(\\Pi{\\mathcal{X}}(y)\\). That\u2019s because the minimization \\(\\min_x {I_{\\mathcal{X}}(x) + \\frac{1}{2\\lambda}|x-y|^2}\\) forces \\(x\\) to lie in \\(\\mathcal{X}\\) (outside \\(\\mathcal{X}\\) the objective is infinite) and then reduces to \\(\\min_{x\\in \\mathcal{X}}|x-y|^2\\), the definition of projection. Thus, projection is a special case of a proximal operator. Prox operators can be thought of as \u201csoftened\u201d projections that not only enforce constraints but can also induce certain structures (like sparsity).</p> <p>Given \\(\\operatorname{prox}_{\\lambda g}\\), the proximal gradient method for \\(f+g\\) works as follows:</p> <ol> <li> <p>Take a usual gradient descent step on the smooth part \\(f\\): \\(y_{k} = x_k - \\alpha \\nabla f(x_k)\\).</p> </li> <li> <p>Apply the proximal operator of \\(g\\): \\(x_{k+1} = \\operatorname{prox}_{\\alpha g}(y_k)\\).</p> </li> </ol> <p>In formula form,</p> \\[ x_{k+1} = \\operatorname{prox}_{\\alpha g}\\!\\left( x_k - \\alpha \\nabla f(x_k) \\right) \\] <p>This update handles two things: the gradient step tries to reduce the objective \\(f\\), and the prox step pulls the solution toward satisfying the \u201cdesirable structure\u201d encoded by \\(g\\). If \\(g\\) is an indicator of \\(\\mathcal{X}\\), this becomes projected gradient descent (since \\(\\operatorname{prox}{\\alpha I{\\mathcal{X}}}(y)=\\Pi_{\\mathcal{X}}(y)\\)). If \\(g\\) is something like \\(\\lambda |x|_1\\), the prox step becomes a soft-thresholding of \\(y\\) (encouraging sparsity). If \\(g\\) is the absolute value function (total variation, etc.), prox becomes a shrinkage operator, and so on. The proximal gradient method is also known as Forward-Backward Splitting: a forward (gradient) step on \\(f\\), followed by a backward (proximal) step on \\(g\\).</p> <p>Convergence: If \\(f\\) is convex and \\(L\\)-smooth and \\(g\\) is convex (proper, closed), then proximal gradient descent with a fixed step \\(\\alpha \\le 1/L\\) converges to the optimum at essentially the same rate as plain gradient descent: \\(O(1/k)\\) in the general convex case, and linear convergence if \\(f+g\\) is strongly convex (under a suitable condition like \\(g\\) being simple or strongly convex). The intuition is that the nonsmooth part \\(g\\) doesn\u2019t hurt the rate as long as its proximal operator is efficiently computable; the \u201cheavy lifting\u201d of ensuring a decrease is done by the smooth part\u2019s Lipschitz condition and the prox step never increases the objective. However, each iteration\u2019s cost includes computing \\(\\operatorname{prox}_{\\alpha g}\\), which in some cases might be as hard as solving a subproblem. Fortunately, for many common \\(g\\), the prox step is easy: e.g., \\(\\ell_1\\) norm (soft-thresholding), \\(\\ell_2\\) norm (shrink towards zero), indicator of linear constraints (simple clipping or normalization), etc.</p> <p>Proximal point algorithm (implicit gradient): A conceptual algorithm worth mentioning is the proximal point method, which is like taking only implicit steps on the entire objective. It iterates \\(x_{k+1} = \\operatorname{prox}{\\lambda f}(x_k)\\), i.e. solves \\(x{k+1} = \\arg\\min_x {f(x) + \\frac{1}{2\\lambda}|x-x_k|^2}\\) exactly at each step. This is in general a difficult subproblem (you basically solve the original problem in each step!), so it\u2019s not an algorithm you\u2019d implement for a generic \\(f\\). But theoretically, the proximal point method has strong convergence guarantees under very mild assumptions (it converges for any \\(\\lambda&gt;0\\) as long as a minimizer exists). It provides an implicit stabilization of the iteration (always moving to a point that is an actual minimizer of a nearby objective), which is why it converges even for some nonconvex problems and monotone operators. In convex optimization, the proximal point algorithm is more of a theoretical tool\u2014many modern algorithms (like ADMM and SVRG) can be interpreted as approximate or accelerated versions of it.</p> <p>Proximal vs gradient: one can view the standard gradient descent as the limit of the proximal point method when \\(\\lambda\\) is small. Gradient descent does \\(x_{k+1}\\approx x_k - \\lambda \\nabla f(x_k)\\) which is the first-order condition of the prox subproblem if we only take an infinitesimal step. Proximal steps, in contrast, solve the subproblem exactly, which is why they can be more stable. Proximal gradient hits a sweet spot: we solve the easy part (\\(g\\)) exactly via prox, and handle the hard part (\\(f\\)) via gradient.</p> <p>Example \u2013 Lasso (L1 regularization): Take \\(f(x) = \\frac{1}{2}|Ax - b|^2\\) (a least-squares loss) and \\(g(x) = \\lambda |x|1\\) (an \\(\\ell_1\\) penalty encouraging sparsity). The prox operator \\(\\operatorname{prox}{\\alpha \\lambda |\\cdot|1}(y)\\) is the soft-thresholding: \\([\\operatorname{prox}{\\alpha \\lambda |\\cdot|_1}(y)]_i = \\text{sign}(y_i)\\max{|y_i| - \\alpha\\lambda,,0}\\). So proximal gradient descent (a.k.a. Iterative Shrinkage-Thresholding Algorithm (ISTA)) does:</p> \\[ \\begin{aligned} y_k &amp;= x_k - \\alpha A^\\top (A x_k - b), \\\\ x_{k+1,i} &amp;= \\operatorname{sign}(y_{k,i}) \\, \\max\\{ |y_{k,i}| - \\alpha \\lambda, \\, 0 \\}. \\end{aligned} \\] <p>for each component \\(i\\). This method will converge to the Lasso solution. Furthermore, one can add Nesterov acceleration to this (getting the FISTA algorithm), achieving an \\(O(1/k^2)\\) rate for the objective similar to accelerated gradient.</p> <p>In summary, proximal gradient methods allow us to tackle optimization problems with constraints or nonsmooth terms by splitting the problem into a smooth part (handled by a gradient step) and a simple nonsmooth part (handled by a prox step). The geometry underlying this is the idea of projection as a prox operator: we extend the notion of moving \u201cback to the feasible region\u201d (projection) to moving toward a region that yields lower \\(g(x)\\) (proximal step). This framework vastly expands the scope of problems solvable by first-order methods, including Lasso, logistic regression with regularization, matrix norm minimizations, etc., all while maintaining convergence guarantees comparable to gradient descent.</p>"},{"location":"3_5_interior/","title":"3 5 interior","text":"<p>When optimizing with inequality constraints (e.g. linear or convex constraints \\(g_i(x) \\le 0\\)), a different class of algorithms \u2014 interior point methods \u2014 has proven highly effective. Unlike gradient or simplex methods that move along the boundary of the feasible region, interior point methods approach the solution from the interior of the feasible set, guided by barrier functions. They exploit smoothness in the interior by incorporating constraints into the objective, enabling the use of Newton-like steps even for constrained problems.</p> <p>Barrier Functions and the Central Path</p> <p>Consider a convex optimization problem in standard form:</p> \\[ \\begin{aligned} \\min_x \\quad &amp; f(x) \\\\ \\text{s.t.} \\quad &amp; g_i(x) \\le 0, \\quad i = 1, \\dots, m. \\end{aligned} \\] <p>with \\(f\\) and \\(g_i\\) convex. An interior point approach introduces a barrier function \\(b(x)\\) that blows up near the boundary \\(g_i(x)=0\\). A classic choice is the logarithmic barrier: if \\(g_i(x)\\le 0\\) are the constraints, define</p> \\[ b(x) = -\\sum_{i=1}^m \\log(-g_i(x)) \\] <p>which is finite when \\(g_i(x)&lt;0\\) (interior) and tends to \\(+\\infty\\) as any \\(g_i(x)\\to 0\\) (approaching the boundary). We then solve a series of unconstrained problems:</p> \\[ \\min_x \\; F_t(x) := t f(x) + b(x) \\] <p>where \\(t&gt;0\\) is a parameter that controls the trade-off between the original objective and the barrier. For each fixed \\(t\\), the minimizer \\(x^(t)\\) of \\(t f(x)+b(x)\\) is an interior point that balances minimizing \\(f\\) against staying away from the constraint boundaries. As \\(t\\) increases, we place more weight on \\(f\\) and less on the barrier. In the limit \\(t \\to \\infty\\) (or equivalently using \\(\\mu = 1/t \\to 0^+\\) as barrier parameter), \\(x(t)\\) approaches the true constrained optimum \\(x^*\\). The curve of solutions \\({x(t): t&gt;0}\\) is called the central path. Points on this path are characterized by a certain perturbed optimality condition (the Karush-Kuhn-Tucker conditions with \\(g_i(x) &lt; 0\\) and Lagrange multipliers \\(\\propto 1/t\\)).</p> <p>For example, in a simple linear program, the central path is the set of strictly feasible solutions that minimize \\(c^T x + \\frac{1}{t}\\sum_i \\log x_i\\) for increasing \\(t\\). Initially (small \\(t\\)) the solution is heavily influenced by the barrier and stays well within the feasible region. As \\(t\\) grows, the solution moves closer to the boundary and to the true optimum, which often lies on the boundary (one or more \\(g_i(x)=0\\) at optimum). The central path converges to the optimal point as the barrier vanishes.</p> <p>Interior point methods follow this central path. They start with a moderate \\(t\\) and a feasible interior \\(x^(t)\\). Then they gradually increase \\(t\\) (decrease barrier) and use the previous solution as a warm start to compute \\(x^(t+\\Delta t)\\). Effectively, they trace the trajectory \\(x^*(t)\\) towards the optimum, rather than jumping directly to the boundary.</p> <p>Why use the barrier at all? Because while the constraints are active, directly attacking them can be complicated (especially if many constraints; think of simplex methods hopping between tight constraints). The barrier method keeps the iterate strictly feasible (\\(g_i(x)&lt;0\\) for all \\(i\\)) at all times, avoiding the combinatorial complexity of dealing with active sets. Instead, constraint satisfaction is handled smoothly by the barrier term, and we can deploy powerful smooth optimization (like Newton\u2019s method) on \\(F_t(x) = t f(x)+b(x)\\). Each \\(F_t\\) is unconstrained and differentiable on the interior, amenable to Newton\u2019s fast convergence.</p> <p>Newton Steps and Feasibility vs. Optimality Trade-off</p> <p>To solve \\(\\min_x F_t(x)\\) for a given \\(t\\), interior point methods typically use Newton\u2019s method because \\(F_t(x)\\) is twice-differentiable (assuming \\(f\\) and \\(g_i\\) are). In fact, one derives the gradient and Hessian of the log-barrier \\(b(x)\\) and then computes Newton steps for the equation \\(\\nabla F_t(x)=0\\) (the perturbed optimality condition). These Newton steps, however, must remain in the interior (we can\u2019t step outside or we\u2019d evaluate \\(\\log(-g_i(x))\\) at an invalid point). Thus damped Newton steps are employed: we choose a step length \\(\\lambda\\) to ensure \\(x+\\lambda d\\) stays feasible (\\(g_i(x+\\lambda d) &lt; 0\\)) for all \\(i\\). This usually means a backtracking line search that stops when \\(x+\\lambda d\\) is safely inside the domain (for instance, requiring \\(g_i(x+\\lambda d) \\le (1-\\tau)g_i(x)\\) for some \\(\\tau\\in(0,1)\\) to maintain a margin). By choosing conservative step sizes, one guarantees the iterates never hit the boundary stat.cmu.edu .</p> <p>Each \\(F_t\\) is solved to a certain accuracy (often just a few Newton steps are needed if started near the optimum for that \\(t\\)). Then \\(t\\) is increased (the barrier is made smaller). There is a trade-off in choosing how fast to increase \\(t\\): a short-step method increases \\(t\\) gradually, staying very close to the central path, which ensures each Newton solve is very efficient (few iterations) but requires many little increases in \\(t\\). A long-step or path-following method takes more aggressive increases in \\(t\\), deviating more from the central path and possibly requiring more Newton iterations to recenter. In practice, modern solvers use predictor-corrector variants (like Mehrotra\u2019s algorithm) which intelligently choose \\(t\\) and include an extra correction Newton step to stay on track. These methods are very efficient.</p> <p>Feasibility vs. optimality: The barrier parameter \\(1/t\\) controls this balance. When \\(1/t\\) is large, the barrier term dominates and the algorithm prioritizes feasibility (staying well inside the region, far from the boundaries). The solution \\(x^*(t)\\) at small \\(t\\) is in the \u201ccenter\u201d of the feasible region (hence central path). When \\(1/t\\) is tiny (large \\(t\\)), the barrier is weak and the algorithm prioritizes optimality of \\(f(x)\\), tolerating being near constraint boundaries. The iterates gradually shift from emphasizing feasibility to emphasizing optimality. The beauty of interior point methods is that they maintain a gentle balance: you never violate feasibility, and you make smooth progress toward optimality.</p> <p>Because interior point methods effectively solve a series of Newton systems, their iteration count is largely independent of problem size and more a function of condition measures (like self-concordance parameters of the barrier). They run in polynomial time theoretically and often extremely fast in practice. For instance, Karmarkar\u2019s interior point algorithm was a breakthrough that showed linear programming can be solved in worst-case polynomial time, and in practice interior point solvers can solve huge LPs faster than the simplex method for many cases. For general convex problems, interior point methods are the method of choice when high accuracy is needed and the problem isn\u2019t so large as to preclude forming Hessians (they\u2019re used in many commercial solvers for e.g. quadratic programming, semidefinite programming, etc.).</p> <p>Example (Linear Program): \\(\\min{c^T x : Ax=b,, x&gt;0}\\). The log-barrier method solves \\(\\min_x t,c^T x - \\sum_i \\log x_i\\) subject to \\(Ax=b\\). The central path equation yields \\(t c_i - \\frac{1}{x_i} = \\text{(something from dual vars)}\\) for each \\(i\\). When \\(t\\) is small, \\(1/x_i\\) terms dominate, so \\(x\\) stays away from 0; as \\(t\\to \\infty\\), the solution forces \\(c_i \\approx 0\\) for basic variables at optimum and some \\(x_i \\to 0\\) for non-basics. The interior path thus goes from a \u201ccentered\u201d positive solution and ends at the optimal corner of the polytope. Figure 15.2 in some lecture notes (see reference) shows contour lines of the barrier for increasing \\(t\\): they gradually approximate the sharp corners of the feasible polyhedron, and the central path (dotted line) goes through the middle of the feasible region and eventually to the corner solution. This illustrates why it\u2019s called an interior-point method: it **starts at an interior point and moves along a path within the interior to the solution\u3011.</p> <p>Summary</p> <p>Interior point methods convert constrained problems into a sequence of unconstrained ones via barrier functions. By following the central path and using damped Newton steps, they achieve a powerful combination of feasibility maintenance and fast convergence. The geometry of interior methods is captured by the central path staying in the strict interior and asymptotically approaching the boundary optimum. Modern interior point algorithms are primal-dual (they track dual variable estimates as well), which often improves practical performance and provides stopping criteria. They are widely used in solving linear programs, quadratic programs, and general conic programs at large scale, often outperforming simplex or cutting-plane methods for large problems. The development of interior point techniques was a major milestone in convex optimization, marrying the smooth Newton technique with inequality constraints in a mathematically elegant and efficient way. As a result, we can now solve huge convex problems to very high accuracy by exploiting this approach, traversing the interior of the feasible region with confidence and speed.</p>"},{"location":"3a%20Huber/","title":"3a Huber","text":""},{"location":"3a%20Huber/#huber-penalty-loss","title":"Huber Penalty Loss","text":"<p>The Huber loss is a robust loss function that combines the advantages of squared loss and absolute loss, making it less sensitive to outliers while remaining convex. It is defined as:</p> \\[ L_\\delta(r) =  \\begin{cases}  \\frac{1}{2} r^2 &amp; \\text{if } |r| \\le \\delta, \\\\ \\delta (|r| - \\frac{1}{2}\\delta) &amp; \\text{if } |r| &gt; \\delta, \\end{cases} \\] <p>where \\(r = y - \\hat{y}\\) is the residual, and \\(\\delta &gt; 0\\) is a threshold parameter.  </p>"},{"location":"3a%20Huber/#key-properties","title":"Key Properties","text":"<ul> <li>Quadratic for small residuals (\\(|r| \\le \\delta\\)) \u2192 behaves like least squares.  </li> <li>Linear for large residuals (\\(|r| &gt; \\delta\\)) \u2192 reduces the influence of outliers.  </li> <li>Convex, so standard convex optimization techniques apply.  </li> </ul>"},{"location":"3a%20Huber/#use","title":"Use","text":"<ul> <li>Commonly used in robust regression to estimate parameters in the presence of outliers.</li> <li>Balances efficiency (like least squares) and robustness (like absolute loss).</li> </ul>"},{"location":"3b%20Penalty%20Functions/","title":"Convex Optimization Notes: Penalty Function Approximation","text":""},{"location":"3b%20Penalty%20Functions/#convex-optimization-notes-penalty-function-approximation","title":"Convex Optimization Notes: Penalty Function Approximation","text":""},{"location":"3b%20Penalty%20Functions/#penalty-function-approximation","title":"Penalty Function Approximation","text":"<p>We solve:</p> <p>\\(\\min \\; \\phi(r_1) + \\cdots + \\phi(r_m) \\quad \\text{subject to} \\quad r = Ax - b\\)</p> <p>where: - \\(A \\in \\mathbb{R}^{m \\times n}\\) - \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) is a convex penalty function</p> <p>The choice of \\(\\phi\\) determines how residuals are penalized.</p>"},{"location":"3b%20Penalty%20Functions/#common-penalty-functions","title":"Common Penalty Functions","text":""},{"location":"3b%20Penalty%20Functions/#1-quadratic-least-squares","title":"1. Quadratic (Least Squares)","text":"<p>\\(\\phi(u) = u^2\\)</p> <ul> <li>Strongly convex, smooth.  </li> <li>Penalizes large residuals heavily.  </li> <li>Equivalent to Gaussian noise model in statistics.  </li> <li>Leads to unique minimizer.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#2-absolute-value-least-absolute-deviations","title":"2. Absolute Value (Least Absolute Deviations)","text":"<p>\\(\\phi(u) = |u|\\)</p> <ul> <li>Convex but nonsmooth at \\(u=0\\) (subgradient methods needed).  </li> <li>Robust to outliers compared to quadratic.  </li> <li>Equivalent to Laplace noise model in statistics.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#why-does-it-lead-to-sparsity","title":"Why does it lead to sparsity?","text":"<ul> <li>The sharp corner at \\(u=0\\) makes it favorable for optimization to set many residuals (or coefficients) exactly to zero.  </li> <li>In contrast, quadratic penalties (\\(u^2\\)) only shrink values toward zero but rarely make them exactly zero.  </li> <li>Geometric intuition: the \\(\\ell_1\\) ball has corners aligned with coordinate axes \u2192 solutions land on axes \u2192 sparse.  </li> <li>Statistical interpretation: corresponds to a Laplace prior, which induces sparsity, whereas \\(\\ell_2\\) corresponds to a Gaussian prior (no sparsity).  </li> </ul> <p>\ud83d\udc49 This property is the foundation of Lasso regression and many compressed sensing methods.  </p>"},{"location":"3b%20Penalty%20Functions/#3-deadzone-linear","title":"3. Deadzone-Linear","text":"<p>\\(\\phi(u) = \\max \\{ 0, |u| - \\alpha \\}\\), where \\(\\alpha &gt; 0\\)</p> <ul> <li>Ignores small deviations (\\(|u| &lt; \\alpha\\)).  </li> <li>Linear growth outside the \u201cdeadzone.\u201d  </li> <li>Used in support vector regression (SVR) with \\(\\epsilon\\)-insensitive loss.  </li> <li>Convex, but not strictly convex \u2192 possibly multiple minimizers.  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#4-log-barrier","title":"4. Log-Barrier","text":"<p>\\(\\phi(u) = \\begin{cases} -\\alpha^2 \\log \\left(1 - (u/\\alpha)^2 \\right), &amp; |u| &lt; \\alpha \\\\ \\infty, &amp; \\text{otherwise} \\end{cases}\\)</p> <ul> <li>Smooth, convex inside domain \\(|u| &lt; \\alpha\\).  </li> <li>Grows steeply as \\(|u| \\to \\alpha\\).  </li> <li>Effectively enforces constraint \\(|u| &lt; \\alpha\\).  </li> </ul>"},{"location":"3b%20Penalty%20Functions/#histograms-of-residuals-effect-of-penalty-choice","title":"Histograms of Residuals (Effect of Penalty Choice)","text":"<p>For \\(A \\in \\mathbb{R}^{100 \\times 30}\\), the residual distribution \\(r\\) depends on \\(\\phi\\):</p> <ul> <li>Quadratic (\\(u^2\\)): residuals spread out (Gaussian-like).  </li> <li>Absolute value (\\(|u|\\)): sharper peak at 0, heavier tails (Laplace-like).  </li> <li>Deadzone: many residuals exactly at 0 (ignored region).  </li> <li>Log-barrier: residuals concentrate away from the boundary \\(|u| = 1\\).  </li> </ul> <p>\ud83d\udc49 Takeaway: Choice of \\(\\phi\\) directly shapes residual distribution.</p>"},{"location":"3b%20Penalty%20Functions/#huber-penalty-function","title":"Huber Penalty Function","text":"<p>The Huber penalty combines quadratic and linear growth:</p> <p>\\(\\phi_{\\text{huber}}(u) = \\begin{cases} u^2, &amp; |u| \\leq M \\\\ 2M|u| - M^2, &amp; |u| &gt; M \\end{cases}\\)</p>"},{"location":"3b%20Penalty%20Functions/#properties","title":"Properties","text":"<ul> <li>Quadratic near 0 (\\(|u| \\leq M\\)) \u2192 efficient for small noise.  </li> <li>Linear for large \\(|u|\\) \u2192 robust to outliers.  </li> <li>Smooth, convex.  </li> <li>Interpolates between least squares and least absolute deviations.  </li> </ul> <p>\ud83d\udc49 Called a robust penalty, widely used in robust regression.</p>"},{"location":"3b%20Penalty%20Functions/#summary-choosing-a-penalty-function","title":"Summary: Choosing a Penalty Function","text":"<ul> <li>Quadratic: efficient, but sensitive to outliers.  </li> <li>Absolute value: robust, but nonsmooth.  </li> <li>Deadzone: ignores small errors, good for sparse modeling (e.g., SVR).  </li> <li>Log-barrier: enforces domain constraints smoothly.  </li> <li>Huber: best of both worlds \u2192 quadratic for small residuals, linear for large ones.  </li> </ul>"},{"location":"3c%20Regularized/","title":"Regularized Approximation","text":""},{"location":"3c%20Regularized/#regularized-approximation","title":"Regularized Approximation","text":""},{"location":"3c%20Regularized/#1-motivation-fit-vs-complexity","title":"1. Motivation: Fit vs. Complexity","text":"<p>When fitting a model, we often want to balance two competing goals:</p> <ol> <li>Data fidelity: minimize how poorly the model fits the observed data (\\(f(x)\\)).  </li> <li>Model simplicity: discourage overly complex solutions (\\(R(x)\\)).</li> </ol> <p>This is naturally a bicriterion optimization problem:</p> <ul> <li>Criterion 1: \\(f(x)\\) = data-fitting term (e.g., least squares loss \\(\\|Ax-b\\|_2^2\\)).  </li> <li>Criterion 2: \\(R(x)\\) = regularization term (e.g., \\(\\|x\\|_1\\), \\(\\|x\\|_2^2\\), TV).  </li> </ul> <p>Since minimizing both simultaneously is usually impossible, we form the scalarized problem:</p> \\[ \\min_x \\; f(x) + \\lambda R(x), \\quad \\lambda &gt; 0 \\] <p>Here, \\(\\lambda\\) controls the trade-off: small \\(\\lambda\\) emphasizes fit, large \\(\\lambda\\) emphasizes simplicity.</p>"},{"location":"3c%20Regularized/#2-bicriterion-and-pareto-frontier","title":"2. Bicriterion and Pareto Frontier","text":"<ul> <li>Pareto optimality: a solution \\(x^\\star\\) is Pareto optimal if no other \\(x\\) improves one criterion without worsening the other.  </li> <li>Weighted sum method:  </li> <li>For convex \\(f\\) and \\(R\\), every Pareto optimal solution can be obtained from some \\(\\lambda \\ge 0\\).  </li> <li>For nonconvex problems, weighted sums may miss parts of the frontier.  </li> </ul> <p>Thus, regularization is a way of choosing a point on the Pareto frontier between fit and complexity.</p>"},{"location":"3c%20Regularized/#3-why-keep-x-small","title":"3. Why Keep \\(x\\) Small?","text":"<p>Ill-posed or noisy problems (e.g., \\(Ax \\approx b\\) with ill-conditioned \\(A\\)) often admit solutions with very large \\(\\|x\\|\\). - These large values overfit noise and are unstable. - Regularization (especially \\(\\ell_2\\)) controls the size of \\(x\\), yielding stable and robust solutions.  </p> <p>Example (Ridge regression):</p> \\[ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\] <p>Leads to the normal equations:</p> \\[ (A^\\top A + \\lambda I)x = A^\\top b \\] <ul> <li>\\(A^\\top A + \\lambda I\\) is always positive definite.  </li> <li>Even if \\(A\\) is rank-deficient, the solution is unique and stable.</li> </ul>"},{"location":"3c%20Regularized/#4-lagrangian-interpretation","title":"4. Lagrangian Interpretation","text":"<p>Regularized approximation is equivalent to a constrained optimization formulation:</p> \\[ \\min_x f(x) \\quad \\text{s.t.} \\quad R(x) \\le t \\] <p>for some bound \\(t &gt; 0\\).  </p>"},{"location":"3c%20Regularized/#kkt-and-duality","title":"KKT and Duality","text":"<ul> <li>The Lagrangian is:</li> </ul> \\[ \\mathcal{L}(x, \\lambda) = f(x) + \\lambda(R(x)-t) \\] <ul> <li>Under convexity and Slater\u2019s condition, strong duality holds.  </li> <li>KKT conditions:</li> </ul> \\[ 0 \\in \\partial f(x^\\star) + \\lambda^\\star \\partial R(x^\\star), \\quad  \\lambda^\\star \\ge 0, \\quad R(x^\\star) \\le t, \\quad  \\lambda^\\star (R(x^\\star)-t) = 0 \\] <ul> <li>The penalized form:</li> </ul> \\[ \\min_x f(x) + \\lambda R(x) \\] <p>has the same optimality condition:</p> \\[ 0 \\in \\partial f(x^\\star) + \\lambda \\partial R(x^\\star) \\] <p>Hence, solving the penalized problem corresponds to solving the constrained one for some \\(t\\), though the \\(\\lambda \\leftrightarrow t\\) mapping is monotone but not one-to-one.</p>"},{"location":"3c%20Regularized/#5-common-regularizers","title":"5. Common Regularizers","text":""},{"location":"3c%20Regularized/#l2-ridge","title":"L2 (Ridge)","text":"<p>\\(R(x) = \\|x\\|_2^2\\) - Strongly convex \u2192 unique solution. - Encourages small coefficients, smooth solutions. - Bayesian view: Gaussian prior on \\(x\\). - Improves conditioning of \\(A^\\top A\\).  </p>"},{"location":"3c%20Regularized/#l1-lasso","title":"L1 (Lasso)","text":"<p>\\(R(x) = \\|x\\|_1\\) - Convex, but not strongly convex \u2192 solutions may be non-unique. - Promotes sparsity: many coefficients exactly zero. - Geometric view: \\(\\ell_1\\) ball has corners aligned with coordinate axes; intersections often occur at corners \u2192 sparse solutions. - Bayesian view: Laplace prior on \\(x\\). - Proximal operator: soft-thresholding </p>"},{"location":"3c%20Regularized/#elastic-net","title":"Elastic Net","text":"<p>\\(R(x) = \\alpha \\|x\\|_1 + (1-\\alpha)\\|x\\|_2^2\\) - Combines L1 sparsity with L2 stability. - Ensures uniqueness even when features are correlated.  </p>"},{"location":"3c%20Regularized/#beyond-l1l2","title":"Beyond L1/L2","text":"<ul> <li>General Tikhonov: \\(R(x) = \\|Lx\\|_2^2\\), where \\(L\\) encodes smoothness (e.g., derivative operator).  </li> <li>Total Variation (TV): \\(R(x) = \\|\\nabla x\\|_1\\), promotes piecewise-constant signals.  </li> <li>Group Lasso: \\(R(x) = \\sum_g \\|x_g\\|_2\\), induces structured sparsity.  </li> <li>Nuclear Norm: \\(R(X) = \\|X\\|_\\ast\\) (sum of singular values), promotes low-rank matrices.  </li> </ul>"},{"location":"3c%20Regularized/#6-choosing-the-regularization-parameter-lambda","title":"6. Choosing the Regularization Parameter \\(\\lambda\\)","text":""},{"location":"3c%20Regularized/#trade-off","title":"Trade-off","text":"<ul> <li>Too small \\(\\lambda\\): weak regularization \u2192 overfitting, unstable solutions.  </li> <li>Too large \\(\\lambda\\): strong regularization \u2192 underfitting, biased solutions.  </li> </ul> <p>\\(\\lambda\\) determines where on the Pareto frontier the solution lies.</p>"},{"location":"3c%20Regularized/#practical-selection","title":"Practical Selection","text":"<ul> <li>Cross-validation (CV): </li> <li>Split data into \\(k\\) folds.  </li> <li>Train on \\(k-1\\) folds, validate on the held-out fold.  </li> <li>Average validation error across folds.  </li> <li> <p>Choose \\(\\lambda\\) minimizing average error.  </p> </li> <li> <p>Best practices: </p> </li> <li>Standardize features before using L1/Elastic Net.  </li> <li>For time series, use blocked or rolling CV (avoid leakage).  </li> <li>Use nested CV for model comparison.  </li> <li>One-standard-error rule: prefer larger \\(\\lambda\\) within one SE of min error \u2192 simpler model.  </li> </ul>"},{"location":"3c%20Regularized/#alternatives","title":"Alternatives","text":"<ul> <li>Analytical rules (ridge regression has closed-form shrinkage).  </li> <li>Information criteria (AIC/BIC; heuristic for Lasso).  </li> <li>Regularization path (trace solutions as \\(\\lambda\\) varies, pick best by validation error).  </li> <li>Inverse problems: discrepancy principle, L-curve, generalized CV.  </li> </ul>"},{"location":"3c%20Regularized/#7-algorithmic-perspective","title":"7. Algorithmic Perspective","text":"<p>Regularized problems often have the form:</p> \\[ \\min_x f(x) + R(x) \\] <p>where \\(f\\) is smooth convex and \\(R\\) is convex but possibly nonsmooth.</p> <ul> <li>Proximal Gradient (ISTA, FISTA):   Iterative updates using gradient of \\(f\\) and prox of \\(R\\).  </li> <li>Coordinate Descent: very effective for Lasso/Elastic Net.  </li> <li>ADMM: handles separable structures and constraints well.  </li> </ul> <p>Proximal operators are key: - L2: shrinkage (scaling). - L1: soft-thresholding. - TV/nuclear norm: more advanced proximal maps.  </p>"},{"location":"3c%20Regularized/#8-bayesian-interpretation","title":"8. Bayesian Interpretation","text":"<ul> <li>Regularization corresponds to MAP estimation.  </li> <li>Example: Gaussian noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)\\) and Gaussian prior \\(x \\sim \\mathcal{N}(0,\\tau^2 I)\\) yields:</li> </ul> \\[ \\min_x \\frac{1}{2\\sigma^2}\\|Ax-b\\|_2^2 + \\frac{1}{2\\tau^2}\\|x\\|_2^2 \\] <p>So \\(\\lambda = \\frac{\\sigma^2}{2\\tau^2}\\) (up to scaling). - L1 corresponds to a Laplace prior, inducing sparsity.</p>"},{"location":"3c%20Regularized/#9-key-takeaways","title":"9. Key Takeaways","text":"<ul> <li>Regularized approximation = bicriterion optimization (fit vs. complexity).  </li> <li>Penalized and constrained forms are connected via duality and KKT.  </li> <li>Regularization stabilizes ill-posed problems and improves generalization.  </li> <li>Choice of regularizer shapes the solution (small \\(\\ell_2\\), sparse \\(\\ell_1\\), structured TV/group/nuclear).  </li> <li>\\(\\lambda\\) is critical \u2014 usually chosen by cross-validation or problem-specific heuristics.  </li> <li>Proximal algorithms make regularized optimization scalable.  </li> <li>Bayesian view ties \\(\\lambda\\) to prior assumptions and noise models.</li> </ul>"},{"location":"3d%20Robust%20Approximation/","title":"Robust Regression: Stochastic vs. Worst-Case Formulations","text":""},{"location":"3d%20Robust%20Approximation/#robust-regression-stochastic-vs-worst-case-formulations","title":"Robust Regression: Stochastic vs. Worst-Case Formulations","text":""},{"location":"3d%20Robust%20Approximation/#setup","title":"Setup","text":"<p>We study linear regression with uncertain design matrix:</p> \\[ y = A x + \\varepsilon, \\quad A = \\bar{A} + U, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the decision variable,  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the observed response,  </li> <li>\\(\\bar{A}\\) is the nominal design matrix,  </li> <li>\\(U\\) is an uncertainty term.  </li> </ul> <p>The treatment of \\(U\\) gives rise to two main formulations: stochastic (probabilistic uncertainty) and worst-case (deterministic uncertainty).</p>"},{"location":"3d%20Robust%20Approximation/#1-stochastic-formulation","title":"1. Stochastic Formulation","text":"<p>Assume \\(U\\) is random with  </p> <ul> <li>\\(\\mathbb{E}[U] = 0\\),  </li> <li>\\(\\mathbb{E}[U^\\top U] = P \\succeq 0\\),  </li> <li>finite second moment,  </li> <li>independent of \\(y\\).  </li> </ul> <p>We minimize the expected squared residual:</p> \\[ \\min_x \\; \\mathbb{E}\\!\\left[\\|(\\bar{A} + U)x - y\\|_2^2\\right]. \\]"},{"location":"3d%20Robust%20Approximation/#expansion","title":"Expansion","text":"\\[ \\|(\\bar{A}+U)x - y\\|_2^2 = \\|\\bar{A}x - y\\|_2^2 + 2(\\bar{A}x - y)^\\top Ux + \\|Ux\\|_2^2. \\] <ul> <li>Cross-term vanishes since \\(\\mathbb{E}[U]=0\\) and \\(U\\) is independent of \\(y\\):  </li> </ul> \\[ \\mathbb{E}[(\\bar{A}x - y)^\\top Ux] = 0. \\] <ul> <li>Variance term simplifies:  </li> </ul> \\[ \\mathbb{E}[\\|Ux\\|_2^2] = x^\\top P x. \\]"},{"location":"3d%20Robust%20Approximation/#resulting-problem","title":"Resulting Problem","text":"\\[ \\min_x \\; \\|\\bar{A}x - y\\|_2^2 + x^\\top P x. \\] <ul> <li>If \\(P = \\rho I\\): ridge regression (L2 regularization).  </li> <li>If \\(P \\succeq 0\\) general: generalized Tikhonov regularization, with anisotropic penalty \\(\\|P^{1/2}x\\|_2^2\\).  </li> </ul>"},{"location":"3d%20Robust%20Approximation/#convexity","title":"Convexity","text":"<p>The Hessian is  </p> \\[ \\nabla^2 f(x) = 2(\\bar{A}^\\top \\bar{A} + P) \\succeq 0. \\] <p>Thus the problem is convex. If \\(P \\succ 0\\), it is strongly convex and the minimizer is unique.  </p>"},{"location":"3d%20Robust%20Approximation/#2-worst-case-formulation","title":"2. Worst-Case Formulation","text":"<p>Suppose \\(U\\) is unknown but bounded:</p> \\[ \\|U\\|_2 \\leq \\rho, \\] <p>where \\(\\|\\cdot\\|_2\\) is the spectral norm (largest singular value). We minimize the worst-case squared residual:</p> \\[ \\min_x \\; \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A} + U)x - y\\|_2^2. \\]"},{"location":"3d%20Robust%20Approximation/#expansion-via-spectral-norm-bound","title":"Expansion via Spectral Norm Bound","text":"<p>For spectral norm uncertainty:</p> \\[ \\max_{\\|U\\|_2 \\leq \\rho} \\|(\\bar{A}+U)x - y\\|_2 = \\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2. \\] <p>This identity uses the fact that \\(Ux\\) can align with the residual direction when \\(\\|U\\|_2 \\leq \\rho\\). Note: If a different norm bound is used (Frobenius, \\(\\ell_\\infty\\), etc.), the expression changes.</p>"},{"location":"3d%20Robust%20Approximation/#resulting-problem_1","title":"Resulting Problem","text":"\\[ \\min_x \\; \\left(\\|\\bar{A}x - y\\|_2 + \\rho \\|x\\|_2\\right)^2. \\] <p>This is convex but not quadratic. Unlike ridge regression, the regularization is coupled inside the residual norm, making the solution more conservative.</p>"},{"location":"3d%20Robust%20Approximation/#3-comparison","title":"3. Comparison","text":"Aspect Stochastic Formulation Worst-Case Formulation Model of \\(U\\) Random, mean zero, finite variance Deterministic, bounded \\(\\|U\\|_2 \\leq \\rho\\) Objective \\(\\|\\bar{A}x - y\\|_2^2 + x^\\top P x\\) \\((\\|\\bar{A}x - y\\|_2 + \\rho\\|x\\|_2)^2\\) Regularization Quadratic penalty (ellipsoidal shrinkage) Norm inflation coupled with residual Geometry Ellipsoidal shrinkage of \\(x\\) (Mahalanobis norm) Inflated residual tube, more conservative Convexity Convex quadratic; strongly convex if \\(P \\succ 0\\) Convex but non-quadratic"},{"location":"3d%20Robust%20Approximation/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Stochastic robust regression \u2192 ridge/Tikhonov regression (quadratic L2 penalty).  </li> <li>Worst-case robust regression \u2192 inflated residual norm with L2 penalty inside the loss, more conservative than ridge.  </li> <li>Both are convex, but their geometry differs:  </li> <li>Stochastic: smooth ellipsoidal shrinkage of coefficients.  </li> <li>Worst-case: enlarged residual \u201ctube\u201d that hedges against adversarial perturbations.  </li> </ul>"},{"location":"3e%20MLE/","title":"Statistical Estimation and Maximum Likelihood","text":""},{"location":"3e%20MLE/#statistical-estimation-and-maximum-likelihood","title":"Statistical Estimation and Maximum Likelihood","text":""},{"location":"3e%20MLE/#1-maximum-likelihood-estimation-mle","title":"1. Maximum Likelihood Estimation (MLE)","text":"<p>Suppose we have a family of probability densities</p> \\[ p_x(y), \\quad x \\in \\mathcal{X}, \\] <p>where \\(x\\) (often written as \\(\\theta\\) in statistics) is the parameter to be estimated.  </p> <ul> <li>\\(p_x(y) = 0\\) for invalid parameter values \\(x\\).  </li> <li>The function \\(p_x(y)\\), viewed as a function of \\(x\\) with \\(y\\) fixed, is called the likelihood function.  </li> <li>The log-likelihood is defined as  </li> </ul> \\[ \\ell(x) = \\log p_x(y). \\] <ul> <li>The maximum likelihood estimate (MLE) is  </li> </ul> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathcal{X}} \\; p_x(y)  = \\arg\\max_{x \\in \\mathcal{X}} \\; \\ell(x). \\]"},{"location":"3e%20MLE/#convexity-perspective","title":"Convexity Perspective","text":"<ul> <li>If \\(\\ell(x)\\) is concave in \\(x\\) for each fixed \\(y\\), then the MLE problem is a convex optimization problem.  </li> <li>Important distinction: this requires concavity in \\(x\\), not in \\(y\\).  </li> <li>Example: \\(p_x(y)\\) may be a log-concave density in \\(y\\) (common in statistics),  </li> <li>but this does not imply that \\(\\ell(x)\\) is concave in \\(x\\).  </li> </ul> <p>Thus, convexity of the MLE depends on the parameterization of the distribution family.  </p>"},{"location":"3e%20MLE/#2-linear-measurements-with-iid-noise","title":"2. Linear Measurements with IID Noise","text":"<p>Consider the linear measurement model:</p> \\[ y_i = a_i^\\top x + v_i, \\quad i = 1, \\ldots, m, \\] <p>where  </p> <ul> <li>\\(x \\in \\mathbb{R}^n\\) is the unknown parameter vector,  </li> <li>\\(a_i \\in \\mathbb{R}^n\\) are known measurement vectors,  </li> <li>\\(v_i\\) are i.i.d. noise variables with density \\(p(z)\\),  </li> <li>\\(y \\in \\mathbb{R}^m\\) is the vector of observed measurements.  </li> </ul>"},{"location":"3e%20MLE/#likelihood-function","title":"Likelihood Function","text":"<p>Since the noise terms are independent:</p> \\[ p_x(y) = \\prod_{i=1}^m p\\!\\left(y_i - a_i^\\top x\\right). \\] <p>Taking logs:</p> \\[ \\ell(x) = \\log p_x(y)  = \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#mle-problem","title":"MLE Problem","text":"<p>The MLE is any solution to:</p> \\[ \\hat{x}_{\\text{MLE}} \\in \\arg\\max_{x \\in \\mathbb{R}^n} \\; \\sum_{i=1}^m \\log p\\!\\left(y_i - a_i^\\top x\\right). \\]"},{"location":"3e%20MLE/#convexity-note","title":"Convexity Note","text":"<ul> <li>If \\(p(z)\\) is log-concave in \\(z\\), then \\(\\log p(y_i - a_i^\\top x)\\) is concave in \\(x\\).  </li> <li>Therefore, under log-concave noise distributions (e.g. Gaussian, Laplace, logistic), the MLE problem is a concave maximization problem, hence equivalent to a convex optimization problem after sign change:</li> </ul> \\[ \\min_x \\; -\\ell(x). \\]"},{"location":"4_0_duality/","title":"4 0 duality","text":"<p>Convex optimization problems exhibit powerful duality principles that provide deeper theoretical insight and practical tools for machine learning. In this section, we develop the fundamentals of Lagrange duality (primal and dual problems), explore conditions for strong duality (such as Slater\u2019s condition), and derive the Karush\u2013Kuhn\u2013Tucker (KKT) optimality conditions for convex problems. We will emphasize geometric intuition and link the theory to machine learning applications like support vector machines (SVMs). (Recall from earlier sections that convexity ensures any local optimum is global and enables efficient algorithms; we will now see that duality offers certificates of optimality and alternative solution approaches.)</p>"},{"location":"4_0_duality/#lagrange-duality-fundamentals","title":"Lagrange Duality Fundamentals","text":"<p>Primal and Lagrangian: Consider a convex optimization problem in standard form:</p> \\[ \\begin{aligned} \\min_x \\quad &amp; f_0(x) &amp;&amp; \\text{(convex objective)} \\\\ \\text{s.t.} \\quad  &amp; f_i(x) \\le 0, \\quad i = 1, \\dots, m &amp;&amp; \\text{(convex inequality constraints)} \\\\ &amp; h_j(x) = 0, \\quad j = 1, \\dots, p &amp;&amp; \\text{(affine equality constraints)} \\end{aligned} \\] <p>where \\(f_0, f_i\\) are convex and \\(h_j\\) are affine. We call this the primal problem. To construct the Lagrangian, we introduce multipliers \\(\\lambda_i \\ge 0\\) for each inequality and \\(\\nu_j\\) (which can be positive or negative) for each equality. The Lagrangian is:</p> \\[ L(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{j=1}^p \\nu_j h_j(x) \\] <p>where \\(\\lambda_i \\ge 0\\). Intuitively, \\(L(x,\\lambda,\\nu)\\) augments the objective with penalties for constraint violations. If any \\(f_i(x)\\) is positive (violating \\(f_i(x)\\le0\\)), a sufficiently large \\(\\lambda_i\\) will heavily penalize \\(x\\) kuleshov-group.github.io . Thus, adding these weighted terms \u201crelaxes\u201d the constraints by pushing \\(x\\) to satisfy them for large multipliers.</p> <p>Dual function: For any fixed multipliers \\((\\lambda,\\nu)\\), we define the dual function as the infimum of the Lagrangian over \\(x\\):</p> \\[ g(\\lambda, \\nu) = \\inf_x \\, L(x, \\lambda, \\nu) \\] <p>This infimum (which could be \\(-\\infty\\) for some multipliers) gives us a lower bound on the optimal primal value. Specifically, for any \\(\\lambda \\ge 0,\\nu\\), and any feasible \\(x\\) (satisfying all constraints), we have \\(L(x,\\lambda,\\nu) \\ge f_0(x)\\) (because \\(f_i(x)\\le0\\) makes \\(\\lambda_i f_i(x) \\le 0\\) and \\(h_j(x)=0\\) makes \\(\\nu_j h_j(x)=0\\)). In particular, at the optimum \\(x^*\\) of the primal, \\(L(x^,\\lambda,\\nu) \\ge f_0(x^)\\). Taking the infimum in \\(x\\) yields \\(g(\\lambda,\\nu) \\le f_0(x^)\\). Thus:</p> <p>Weak Duality: For any multipliers \\((\\lambda,\\nu)\\) with \\(\\lambda \\ge 0\\), the dual function \\(g(\\lambda,\\nu)\\) is less than or equal to the optimal primal value \\(p^*\\). In other words, any choice of multipliers provides a lower bound: \\(g(\\lambda,\\nu) \\le p^*\\).</p> <p>We now pose the dual problem: maximize this lower bound subject to dual feasibility (i.e. \\(\\lambda \\ge 0\\)). The Lagrange dual problem is:</p> \\[ d^* = \\max_{\\lambda \\ge 0, \\, \\nu} \\; g(\\lambda, \\nu) \\] <p>Because \\(g(\\lambda,\\nu)\\) is concave (as an infimum of affine functions of \\((\\lambda,\\nu)\\)) even if the primal is not, the dual is a convex maximization problem. We always have \\(d^* \\le p^*\\) (weak duality). The difference \\(p^* - d^*\\) is called the duality gap. In general, there may be a gap (\\(d^* &lt; p^*\\)), but under certain conditions (to be discussed), strong duality holds, meaning \\(d^* = p^*\\). Solving the dual can then be as good as solving the primal, and often easier. In fact, earlier we noted that leveraging a dual formulation can be a practical strategy for convex problems.</p> <p>Geometric intuition: Dual variables \\(\\lambda_i\\) can be viewed as \u201cforce\u201d or \u201cprice\u201d for satisfying constraint \\(i\\). If a constraint is violated, the dual tries to increase the objective (penalty) unless \\(x\\) moves back into the feasible region. At optimum, the dual variables balance the objective\u2019s gradient against the constraint boundaries. Geometrically, each \\(\\lambda_i \\ge 0\\) defines a supporting hyperplane to the primal feasible region \u2013 the dual problem is essentially finding the tightest such supporting hyperplanes that still lie below the objective\u2019s graph.</p> <p>Example \u2013 Dual of an SVM: To illustrate duality, consider the hard-margin SVM problem:</p> \\[ \\begin{aligned} \\min_{w, b} \\quad &amp; \\tfrac{1}{2}\\|w\\|^2 \\\\ \\text{s.t.} \\quad &amp; y_i(w^\\top x_i + b) \\ge 1, \\quad i = 1, \\dots, N. \\end{aligned} \\] <p>which is convex (a QP). Introduce multipliers \\(\\lambda_i \\ge 0\\) for each constraint. The Lagrangian is</p> \\[ L(w, b, \\lambda) = \\tfrac{1}{2}\\|w\\|^2 + \\sum_{i=1}^N \\lambda_i \\big( 1 - y_i (w^\\top x_i + b) \\big) \\] <p>We minimize \\(L\\) over \\(w,b\\) to get the dual function \\(g(\\lambda)\\). Setting gradients to zero: \\(\\partial L/\\partial w = 0\\) yields \\(w = \\sum_{i=1}^N \\lambda_i y_i x_i\\), and \\(\\partial L/\\partial b = 0\\) yields \\(\\sum_{i=1}^N \\lambda_i y_i = 0\\). Substituting back, the dual becomes:</p> \\[ \\begin{aligned} \\max_{\\lambda \\ge 0} \\quad &amp; \\sum_{i=1}^N \\lambda_i - \\frac{1}{2} \\sum_{i,j=1}^N \\lambda_i \\lambda_j y_i y_j (x_i^\\top x_j) \\\\ \\text{s.t.} \\quad &amp; \\sum_{i=1}^N \\lambda_i y_i = 0. \\end{aligned} \\] <p>which is a concave quadratic maximization (or QP) in variables \\(\\lambda_i\\). Here the dual has \\(N\\) variables (one per training point) and one equality constraint, regardless of the feature dimension. Importantly, strong duality holds for this convex QP, so the primal and dual optima coincide. Solving the dual directly yields the optimal \\(\\lambda^*\\), from which we recover \\(w^ = \\sum_i \\lambda^*_i y_i x_i\\). Only training points with \\(\\lambda^*_i &gt; 0\\) (those that tighten the margin constraint) appear in this sum \u2013 these are the support vectors. This dual formulation is the key to the kernelized SVM, since \\(x_i^*\\top x_j\\) appears inside the objective (one can use kernel functions in place of the dot product).</p> <p>Why duality helps: In the SVM above, the dual turned out to be more convenient: it gave insight (support vectors) and is computationally efficient when \\(N\\) is smaller than feature dimension or when using kernels. More generally, the dual problem can sometimes be easier to solve (e.g. fewer constraints or simpler domain), or it provides a certificate of optimality. If we solve the dual and obtain \\(d^*\\), we instantly have a lower bound on \\(p^*\\); if we also have a primal feasible solution with objective \\(p\\), the gap \\(p - d^*\\) tells us how close that solution is to optimal. In convex problems, often \\(p^* = d^*\\) (strong duality), in which case finding dual-optimal \\((\\lambda^*,\\nu^*)\\) and a primal feasible \\(x^*\\) such that \\(L(x^*,\\lambda^*,\\nu^*) = p^*\\) certifies optimality of \\(x^*\\). Duality thus not only offers alternative algorithms but also optimality conditions that are crucial in theory and practice.</p>"},{"location":"4_0_duality/#strong-duality-and-slaters-condition","title":"Strong Duality and Slater\u2019s Condition","text":"<p>While weak duality \\(d^* \\le p^*\\) always holds, strong duality (\\(d^* = p^*\\)) does not hold for every problem. In general convex optimization, we require certain regularity conditions (constraint qualifications) to ensure no gap. The most common condition is Slater\u2019s condition.</p> <p>Slater\u2019s condition: If the primal problem is convex and there exists a strictly feasible point (a point \\(x\\) satisfying all inequalities strictly and all equalities exactly), then the optimal duality gap is zero. In other words, if you can find a point that lies in the interior of the feasible region (not just on the boundary), then strong duality holds for convex problems. Formally, if \\(\\exists \\tilde{x}\\) such that \\(f_i(\\tilde{x}) &lt; 0\\) for all \\(i\\) and \\(h_j(\\tilde{x})=0\\) for all \\(j\\), then \\(p^* = d^*\\). This is a very mild condition \u201csatisfied in most cases\u201d  \u2013 intuitively, it rules out degenerate cases where the optimum is on the boundary with no interior, which can cause a duality gap.</p> <p>An example of a convex problem failing Slater\u2019s condition is one with contradictory constraints or an optimal solution at a corner with no interior feasible region (e.g., minimizing \\(f(x)=x\\) subject to \\(x \\ge 0\\) and \\(x \\le 0\\) \u2013 the only feasible \\(x\\) is 0 which lies on the boundary of both constraints; here strong duality can fail). But for standard ML problems (SVMs, logistic regression with constraints, LASSO in constrained form, etc.), strict feasibility usually holds (we can often find an interior solution by relaxing inequalities a bit), so we can assume strong duality.</p> <p>Consequences of strong duality: If strong duality holds and \\(x^*\\) is primal-optimal and \\((\\lambda^*,\\nu^*)\\) dual-optimal, then \\(f_0(x^*) = g(\\lambda^*,\\nu^*)\\). Combined with weak duality (\\(g(\\lambda^*,\\nu^*) \\le f_0(x^*)\\)), this implies \\(f_0(x^*) = L(x^*,\\lambda^*,\\nu^*)\\) (since \\(g\\) is the infimum of \\(L\\)) and also that \\(x^*\\) attains the infimum for those optimal multipliers. Thus \\(x^*\\) and \\((\\lambda^*,\\nu^*)\\) together satisfy certain optimality conditions \u2013 specifically the KKT conditions we derive next. Moreover, the zero duality gap means the dual solution provides a certificate of optimality for the primal solution. In practice, one can solve the dual (which is often easier) and get the primal solution from it (as we did with SVM). Also, verifying optimality is straightforward: if one finds any feasible \\(x\\) and any \\((\\lambda,\\nu)\\) with \\(\\lambda\\ge0\\) such that \\(f_0(x) = L(x,\\lambda,\\nu)\\), then \\(x\\) must be optimal.</p> <p>Primal-dual interpretation: Strong duality implies existence of a saddle point: \\(L(x^*,\\lambda^*,\\nu^*) = \\min_x \\max_{\\lambda\\ge0,\\nu} L(x,\\lambda,\\nu) = \\max_{\\lambda\\ge0,\\nu} \\min_x L(x,\\lambda,\\nu)\\). At optimum, the primal and dual objectives coincide. We can picture the primal objective\u2019s graph and the dual constraints as supporting hyperplanes \u2013 at optimality, the lowest supporting hyperplane (dual) just touches the graph of \\(f_0\\) at \\(x^*\\), with no gap in between. This touching point corresponds to equal primal and dual values.</p> <p>Slater in ML example: In the hard-margin SVM, Slater\u2019s condition holds if the classes are linearly separable \u2013 we can find a strictly feasible separating hyperplane that classifies all points correctly with margin &gt; 1. If data is strictly separable, strong duality holds (indeed we saw \\(p^*=d^*\\)). For soft-margin SVM (with slack variables), Slater\u2019s condition also holds (take a sufficiently large margin violation allowance to get an interior point). Thus, we can be confident in solving the dual. In constrained regression problems (like LASSO formulated with a constraint \\(|w|_1 \\le t\\)), Slater\u2019s condition holds as long as the constraint is not tight initially (e.g., one can usually find \\(w=0\\) which strictly satisfies \\(|w|_1 &lt; t\\) if \\(t\\) is chosen larger than 0), guaranteeing no duality gap.</p> <p>(Historical note: There are other constraint qualifications beyond Slater, to handle cases like affine constraints or non-strict feasibility, but Slater\u2019s is easiest and usually applicable in convex ML problems.)</p>"},{"location":"4_0_duality/#karushkuhntucker-kkt-conditions","title":"Karush\u2013Kuhn\u2013Tucker (KKT) Conditions","text":"<p>The Karush\u2013Kuhn\u2013Tucker conditions are the first-order optimality conditions for constrained problems, generalizing the method of Lagrange multipliers to include inequality constraints. For convex problems that satisfy Slater\u2019s condition, the KKT conditions are not only necessary but also sufficient for optimality. This means solving the KKT equations is essentially equivalent to solving the original problem.</p> <p>Consider the convex primal problem above. The KKT conditions for a tuple \\((x^*,\\lambda^*,\\nu^*)\\) are:</p> <ol> <li> <p>Primal feasibility: \\(f_i(x^*) \\le 0\\) for all \\(i\\), and \\(h_j(x^*) = 0\\) for all \\(j\\). (The solution must satisfy the original constraints.)</p> </li> <li> <p>Dual feasibility: \\(\\lambda^*_i \\ge 0\\) for all \\(i\\). (Dual variables associated with inequalities must be nonnegative.)</p> </li> <li> <p>Stationarity (gradient condition): \\(\\nabla f_0(x^*) + \\sum_{i=1}^m \\lambda^i,\\nabla f_i(x^*) + \\sum{j=1}^p \\nu^*_j,\\nabla h_j(x^*) = 0\\). This means the gradient of the Lagrangian vanishes at \\(x^*\\). Intuitively, at optimum there is no feasible direction that can decrease the objective \u2014 the objective\u2019s gradient is exactly balanced by a combination of the constraint gradients.</p> </li> <li> <p>Complementary slackness: \\(\\lambda^*_i,f_i(x^*) = 0\\) for each inequality constraint \\(i\\). This crucial condition means that for each constraint, either the constraint is active (\\(f_i(x^)=0\\) on the boundary) and then its multiplier \\(\\lambda^*_i\\) may be positive, or the constraint is inactive (\\(f_i(x^*) &lt; 0\\) strictly inside) and then the multiplier must be zero. In short, \\(\\lambda_i^*\\) can only put \u201cpressure\u201d on a constraint if that constraint is tight at the solution.</p> </li> </ol> <p>These four conditions together characterize optimal solutions for convex problems (assuming a constraint qualification like Slater\u2019s holds to ensure no duality gap). If one can find \\((x,\\lambda,\\nu)\\) satisfying all KKT conditions, then \\(x\\) is optimal and \\((\\lambda,\\nu)\\) are the corresponding optimal dual variables. Conversely, if \\(x^*\\) is optimal (and Slater holds), then there exist multipliers making KKT true. Thus, KKT conditions give an equivalent system of equations/inequalities to solve for the optimum.</p> <p>Geometric interpretation: At an optimum \\(x^*\\), consider any active inequality constraints and all equality constraints \u2013 these define some boundary \u201cface\u201d of the feasible region that \\(x^*\\) lies on. The stationarity condition says the negative objective gradient \\(-\\nabla f_0(x^*)\\) lies in the span of the gradients of active constraints. In other words, the descent direction is blocked by the constraints: you cannot move into any direction that decreases \\(f_0\\) without leaving the feasible set. This is consistent with the earlier geometric intuition from Section C: at a boundary optimum, the gradient of \\(f\\) points outward, perpendicular to the feasible region. The multipliers \\(\\lambda_i\\) are essentially the coefficients of this combination of constraint normals that balances the objective\u2019s gradient. Complementary slackness further tells us that any constraint whose normal is not needed to support the optimum (i.e. not active) must have zero multiplier \u2013 it\u2019s like saying non-binding constraints have no \u201cforce\u201d (\u03bb) at optimum, while binding constraints exert a force to hold the optimum in place.</p> <p>For example, consider a simple 2D problem: minimize some convex \\(f(x)\\) subject to one constraint \\(g(x)\\le0\\). Two scenarios: (i) The unconstrained minimizer of \\(f\\) lies inside the feasible region. Then at optimum, \\(g(x^) &lt; 0\\) is inactive, so \\(\\lambda^=0\\) and \\(\\nabla f(x^*)=0\\) as usual (interior optimum). (ii) The unconstrained minimizer lies outside, so the optimum occurs on the boundary \\(g(x)=0\\). At that boundary point \\(x^*\\), the gradient \\(\\nabla f(x^*)\\) must point outward, proportional to \\(\\nabla g(x^*)\\) to prevent any feasible descent. KKT reflects this: \\(g(x^*)=0\\) active, \\(\\lambda^*&gt;0\\), and \\(\\nabla f(x^*) + \\lambda^* \\nabla g(x^*)=0\\). Graphically, a level set contour of \\(f\\) is tangent to the constraint boundary at \\(x^*\\) \u2013 their normals align.</p> <p>KKT and Lagrange multipliers: If we had only equality constraints, KKT reduces to the classic method of Lagrange multipliers (gradient of \\(f\\) equals a linear combination of equality constraint gradients). Inequalities add the twist of \\(\\lambda \\ge 0\\) and slackness. In fact, KKT can be seen as splitting the normal cone condition: \\(0 \\in \\nabla f(x^) + N_{\\mathcal{X}}(x^)\\) (a general optimality condition) into explicit pieces: the normal cone \\(N_{\\mathcal{X}}(x^*)\\) generated by active constraints\u2019 normals, and each such generator weighted by \\(\\lambda_i\\).</p> <p>SVM example (revisited) \u2013 KKT reveals support vectors: For the hard-margin SVM, the KKT conditions are insightful. The primal constraints are \\(g_i(w,b) = 1 - y_i(w^\\top x_i + b) \\le 0\\). Let \\(\\alpha_i\\) denote the multiplier for constraint \\(i\\) (often SVM literature uses \\(\\alpha\\) instead of \\(\\lambda\\)). KKT conditions:</p> <ul> <li> <p>Primal feasibility: \\(1 - y_i(w^{* \\top} x_i + b^*) \\le 0\\) for all \\(i\\) (all training points are on or outside the margin).</p> </li> <li> <p>Dual feasibility: \\(\\alpha_i^* \\ge 0\\) for all \\(i\\).</p> </li> <li> <p>Stationarity: \\(\\nabla_w \\Big(\\frac{1}{2}|w|^2 + \\sum_i \\alpha_i (1 - y_i(w^\\top x_i + b))\\Big) = 0\\) and \\(\\partial L/\\partial b = 0\\). These give \\(w^* = \\sum_i \\alpha_i^* y_i x_i\\) and \\(\\sum_i \\alpha_i^* y_i = 0\\) (same as earlier from the dual).</p> </li> <li> <p>Complementary slackness: \\(\\alpha_i^* \\big(1 - y_i(w^{*\\top} x_i + b^*)\\big) = 0\\) for each \\(i\\).</p> </li> </ul> <p>This last condition means: for any training point \\(i\\), either it lies strictly outside the margin (violating nothing, so \\(y_i(w^{*\\top} x_i + b^*) &gt; 1\\)), in which case the constraint is inactive and \\(\\alpha_i^* = 0\\); or it lies exactly on the margin (\\(y_i(w^{*\\top} x_i + b^*) = 1\\)), in which case \\(\\alpha_i^*\\) can be positive. The points on the margin are precisely the support vectors, and they end up with \\(\\alpha_i^* &gt; 0\\). Points safely away from the margin have \\(\\alpha_i^* = 0\\) and do not appear in the weight vector \\(w^*\\). Thus, the KKT conditions formally explain the sparseness of the SVM solution: the decision boundary is supported only by a subset of the training points.</p> <p>Geometric view of SVM KKT: The diagram shows a linearly separable classification with the optimal hyperplane (solid line) and margin boundaries (dashed lines). Support vectors (circled points) lie exactly on the margin, meaning the SVM\u2019s constraints \\(y_i(w^\\top x_i+b)\\ge1\\) hold with equality for these points. By complementary slackness, these points have nonzero dual weights \\(\\alpha_i^*\\), thus actively determine the hyperplane. Other points (not circled) lie strictly outside the margin (constraint inactive), so \\(\\alpha_i^*=0\\); perturbing any non-support vector (within the margin bounds) does not move the decision boundary. Only support vectors \u201cpush back\u201d on the hyperplane, illustrating KKT: the objective\u2019s optimum is achieved when its gradient is balanced by constraints from support vectors.</p> <p>In summary, the KKT conditions provide a checklist for optimality in convex problems: feasibility (primal and dual), zero gradient of the Lagrangian, and complementary slackness. For convex optimization, these conditions are both necessary and sufficient (under Slater\u2019s condition). They are extremely useful in practice for analyzing solutions. In ML, many algorithms can be understood in terms of KKT: for instance, the optimality conditions of LASSO (\u21131-regularized regression) dictate which weights are zero vs non-zero (the subgradient of the \u21131 norm must balance the gradient of least squares \u2013 if a weight is zero at optimum, the gradient of the loss at that feature must lie in the \u00b1\u03bb range, etc.). KKT conditions also form the basis of dual coordinate descent methods, which solve optimization by ensuring KKT is gradually satisfied for all constraints.</p> <p>Takeaway: Duality and KKT conditions are powerful tools for convex optimization. Duality gives us alternative ways to solve problems and certify optimality (often leveraged in distributed optimization or derivations of ML algorithms), while KKT conditions distill the optimality criteria into a set of equations/inequalities that often yield insight (support vectors in SVM, thresholding in LASSO, etc.) beyond what the primal solution alone provides.</p>"},{"location":"4_1_optimality/","title":"D.3 First-Order Optimality Conditions in Convex Optimization","text":""},{"location":"4_1_optimality/#d3-first-order-optimality-conditions-in-convex-optimization","title":"D.3 First-Order Optimality Conditions in Convex Optimization","text":"<p>Convex optimization enjoys a powerful guarantee: every local minimum is a global minimum. This section provides a unified framework for checking optimality in both unconstrained and constrained settings \u2014 whether the function is smooth or nonsmooth.</p> <p>These optimality criteria are especially useful when:</p> <ul> <li>Gradients don't exist (e.g., hinge loss, \\(\\ell_1\\) norm)</li> <li>You're working with constraints (e.g., regularization, feasible regions)</li> <li>You want a geometric understanding of what makes a point optimal</li> </ul>"},{"location":"4_1_optimality/#unconstrained-convex-problems","title":"Unconstrained Convex Problems","text":""},{"location":"4_1_optimality/#1-differentiable-objective","title":"1. Differentiable Objective","text":"<p>Let \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) be convex and differentiable. The unconstrained problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x) \\] <p>has optimal solution \\(\\hat{x}\\) if and only if:</p> \\[ \\nabla f(\\hat{x}) = 0 \\] <p>\ud83d\udccc This is the first-order condition: the gradient vanishes at the minimum.</p> <p>Examples:</p> <ul> <li> <p>Quadratic: \\(f(x) = x^2 - 4x + 7\\) \\(\\nabla f(x) = 2x - 4\\) \u21d2 \\(\\hat{x} = 2\\)</p> </li> <li> <p>Least squares: \\(f(x) = \\sum_i (x - y_i)^2\\) \u21d2 \\(\\hat{x} = \\frac{1}{n}\\sum y_i\\) (the mean)</p> </li> </ul>"},{"location":"4_1_optimality/#2-nondifferentiable-objective","title":"2. Nondifferentiable Objective","text":"<p>Let \\(f\\) be convex but not necessarily differentiable (e.g., \\(f(x) = |x|\\)).</p> <p>Then the optimality condition becomes:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>where \\(\\partial f(x)\\) is the subdifferential (see A.7: Subgradients).</p> <p>Examples:</p> <ul> <li>Absolute loss: \\(f(x) = \\sum_i |x - y_i|\\) \u21d2 optimal \\(x\\) is the median </li> <li>Max function: \\(f(x) = \\max(x - 1, 2 - x)\\)   Subdifferential:    Minimum occurs at \\(x = 1.5\\)</li> </ul>"},{"location":"4_1_optimality/#constrained-convex-problems","title":"Constrained Convex Problems","text":"<p>Let \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) be convex, and let \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) be a convex feasible set. Consider:</p> \\[ \\min_{x \\in \\mathcal{X}} f(x) \\] <p>Now, optimality depends on where \\(\\hat{x}\\) lies relative to \\(\\mathcal{X}\\).</p>"},{"location":"4_1_optimality/#1-interior-points","title":"1. Interior Points","text":"<p>If \\(\\hat{x}\\) lies strictly inside the feasible set:</p> \\[ \\hat{x} \\in \\text{int}(\\mathcal{X}) \\] <p>Then the constraint has no effect locally. Optimality is:</p> \\[ 0 \\in \\partial f(\\hat{x}) \\] <p>Same as the unconstrained case.</p>"},{"location":"4_1_optimality/#2-boundary-points","title":"2. Boundary Points","text":"<p>When \\(\\hat{x}\\) lies on the boundary of \\(\\mathcal{X}\\), the optimality condition changes. You cannot move in all directions, only within the feasible set.</p> <p>A point \\(\\hat{x}\\) is optimal if and only if:</p> \\[ - \\nabla f(\\hat{x}) \\in N_{\\mathcal{X}}(\\hat{x}) \\] <p>Where \\(N_{\\mathcal{X}}(\\hat{x})\\) is the normal cone (see A.6: Projections) \u2014 the set of vectors pointing outward from \\(\\mathcal{X}\\) at \\(\\hat{x}\\).</p> <p>Geometric version: For all feasible directions \\(d \\in T_{\\mathcal{X}}(\\hat{x})\\) (the tangent cone):</p> \\[ \\nabla f(\\hat{x})^\\top d \\ge 0 \\] <p>Interpretation: The gradient points away from the feasible region. Any feasible move will not decrease \\(f\\).</p>"},{"location":"4_1_optimality/#3-unified-compact-form","title":"3. Unified Compact Form","text":"<p>The condition below covers both interior and boundary cases:</p> \\[ 0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x}) \\] <p>This is the general optimality condition for constrained convex problems \u2014 including nonsmooth cases.</p>"},{"location":"4_1_optimality/#intuition-tangent-vs-normal-cones","title":"Intuition: Tangent vs Normal Cones","text":"<p>Imagine standing on the boundary of a convex set \\(\\mathcal{X}\\). You can only step along certain directions \u2014 these form the tangent cone. The normal cone consists of vectors that oppose all those directions.</p> <p>At an optimum, the gradient is aligned with the normal cone: Any movement within \\(\\mathcal{X}\\) increases or keeps the objective the same.</p> <p>Interior optimality: gradient is zero Boundary optimality: gradient pushes you outward, and any move into \\(\\mathcal{X}\\) would worsen the objective</p>"},{"location":"4_1_optimality/#example-quadratic-with-constraint","title":"Example: Quadratic with Constraint","text":"<p>Consider:</p> \\[ \\min f(x) = x^2 \\quad \\text{s.t. } x \\ge 1 \\] <ul> <li>Feasible set: \\(\\mathcal{X} = [1, \\infty)\\) </li> <li>Gradient: \\(\\nabla f(x) = 2x\\)</li> </ul> <p>Check:</p> <ul> <li>Unconstrained minimizer is \\(x = 0\\) \u2192 infeasible  </li> <li>Try \\(x = 1\\):      \u2714\ufe0f Satisfied \u2014 \\(x = 1\\) is optimal</li> </ul>"},{"location":"4_1_optimality/#summary","title":"Summary","text":"Case Optimality Condition Unconstrained, smooth \\(\\nabla f(\\hat{x}) = 0\\) Unconstrained, nonsmooth \\(0 \\in \\partial f(\\hat{x})\\) Constrained \\(0 \\in \\partial f(\\hat{x}) + N_{\\mathcal{X}}(\\hat{x})\\) <p>These conditions appear throughout convex optimization \u2014 in projection methods (see G.2), duality theory (Section D), and in practical solvers.</p> <p>\ud83d\udcda Next: We extend this understanding to Fenchel duality, conjugate functions, and structured dual problems in D.4 and D.5.</p>"},{"location":"4a%20Linear%20Discrimination/","title":"4a Linear Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#1-linear-discrimination-lp-feasibility","title":"1. Linear Discrimination (LP Feasibility)","text":""},{"location":"4a%20Linear%20Discrimination/#problem-setup","title":"Problem Setup","text":"<ul> <li>Variables: \\((a,b) \\in \\mathbb{R}^{n+1}\\)</li> <li>Constraints:</li> <li>\\(a^T x_i - b \\geq 1\\)</li> <li>\\(a^T y_j - b \\leq -1\\)</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity","title":"Convexity","text":"<ul> <li>Each constraint is affine in \\((a,b)\\).</li> <li>Affine inequalities define convex half-spaces.</li> <li>Intersection of half-spaces = convex polyhedron.</li> <li>No objective \u2192 pure LP feasibility.</li> </ul> <p>Type: Convex LP feasibility problem.</p>"},{"location":"4a%20Linear%20Discrimination/#2-robust-linear-discrimination-hard-margin-svm","title":"2. Robust Linear Discrimination (Hard-Margin SVM)","text":""},{"location":"4a%20Linear%20Discrimination/#problem","title":"Problem","text":"\\[ \\min \\tfrac{1}{2}\\|a\\|_2^2 \\quad  \\text{s.t. } a^T x_i - b \\geq 1, \\; a^T y_j - b \\leq -1. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_1","title":"Convexity","text":"<ul> <li>Objective: \\(\\tfrac{1}{2}\\|a\\|_2^2\\) is convex quadratic (strictly convex in \\(a\\)).</li> <li>Constraints: Affine \\(\\Rightarrow\\) convex feasible set.</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#3-soft-margin-svm","title":"3. Soft-Margin SVM","text":""},{"location":"4a%20Linear%20Discrimination/#problem_1","title":"Problem","text":"\\[ \\min_{a,b,\\xi} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\xi_i $$ subject to $$ y_i(a^T z_i - b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_2","title":"Convexity","text":"<ul> <li>Objective: Sum of convex quadratic (\\(\\|a\\|^2\\)) and linear (\\(\\sum_i \\xi_i\\)).</li> <li>Constraints: Affine in \\((a,b,\\xi)\\).</li> <li>Feasible set = intersection of half-spaces (convex).</li> </ul> <p>Type: Convex quadratic program (QP).</p>"},{"location":"4a%20Linear%20Discrimination/#4-hinge-loss-formulation","title":"4. Hinge Loss Formulation","text":""},{"location":"4a%20Linear%20Discrimination/#problem_2","title":"Problem","text":"\\[ \\min_{a,b} \\; \\tfrac{1}{2}\\|a\\|_2^2 + C \\sum_i \\max(0, 1 - y_i(a^T z_i - b)). \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_3","title":"Convexity","text":"<ul> <li>\\(\\tfrac{1}{2}\\|a\\|_2^2\\): convex quadratic.</li> <li>Inside hinge: \\(1 - y_i(a^T z_i - b)\\) is affine.</li> <li>\\(\\max(0, \\text{affine})\\) = convex function.</li> <li>Sum of convex functions = convex.</li> </ul> <p>Type: Unconstrained convex optimization problem.</p>"},{"location":"4a%20Linear%20Discrimination/#5-dual-svm-problem","title":"5. Dual SVM Problem","text":""},{"location":"4a%20Linear%20Discrimination/#problem_3","title":"Problem","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j z_i^T z_j $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_4","title":"Convexity","text":"<ul> <li>Quadratic form has negative semidefinite Hessian (concave).</li> <li>Maximization of concave function over convex set \u2192 convex optimization.</li> </ul> <p>Type: Convex quadratic program in dual variables.</p>"},{"location":"4a%20Linear%20Discrimination/#6-nonlinear-discrimination-with-kernels","title":"6. Nonlinear Discrimination with Kernels","text":""},{"location":"4a%20Linear%20Discrimination/#problem-dual-with-kernel","title":"Problem (Dual with Kernel)","text":"\\[ \\max_{\\alpha} \\sum_i \\alpha_i - \\tfrac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(z_i,z_j) $$ subject to $$ \\sum_i \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C. \\]"},{"location":"4a%20Linear%20Discrimination/#convexity_5","title":"Convexity","text":"<ul> <li>If \\(K\\) is positive semidefinite (Mercer kernel), quadratic form is convex.</li> <li>Maximization remains convex program.</li> </ul> <p>Type: Convex QP with kernel matrix.</p>"},{"location":"4a%20Linear%20Discrimination/#7-quadratic-discrimination","title":"7. Quadratic Discrimination","text":""},{"location":"4a%20Linear%20Discrimination/#problem_4","title":"Problem","text":"<p>Classifier: \\(f(z) = z^T P z + q^T z + r\\) with variables \\((P,q,r)\\).</p> <p>Constraints: - \\(x_i^T P x_i + q^T x_i + r \\geq 1\\) - \\(y_j^T P y_j + q^T y_j + r \\leq -1\\)</p>"},{"location":"4a%20Linear%20Discrimination/#convexity_6","title":"Convexity","text":"<ul> <li>\\(x_i^T P x_i = \\mathrm{Tr}(P x_i x_i^T)\\), affine in \\(P\\).</li> <li>Constraints affine in \\((P,q,r)\\).</li> <li>If additional constraint \\(P \\succeq 0\\), this is convex (semidefinite cone).</li> </ul> <p>Type: LP feasibility or SDP (semidefinite program).</p>"},{"location":"4a%20Linear%20Discrimination/#8-polynomial-feature-maps","title":"8. Polynomial Feature Maps","text":""},{"location":"4a%20Linear%20Discrimination/#setup","title":"Setup","text":"<ul> <li>Map \\(z \\mapsto F(z)\\) with monomials up to degree \\(d\\).</li> <li>Classifier: \\(f(z) = \\theta^T F(z)\\).</li> </ul>"},{"location":"4a%20Linear%20Discrimination/#convexity_7","title":"Convexity","text":"<ul> <li>Constraints: \\(\\theta^T F(x_i) \\geq 1\\), affine in \\(\\theta\\).</li> <li>Margin maximization objective: \\(\\|\\theta\\|^2\\), convex quadratic.</li> </ul> <p>Type: LP feasibility or convex QP.</p>"},{"location":"4a%20Linear%20Discrimination/#9-summary-of-convex-structures","title":"9. Summary of Convex Structures","text":"<ul> <li>LP feasibility: Linear separation.  </li> <li>QP: Hard-margin and soft-margin SVM.  </li> <li>Unconstrained convex problem: Hinge loss.  </li> <li>Dual SVM: Convex QP in dual variables.  </li> <li>Kernel SVM: Convex QP with PSD kernel.  </li> <li>Quadratic/Polynomial discrimination: LP or SDP, depending on constraints.  </li> </ul>"},{"location":"4d%20Algos/","title":"4d Algos","text":""},{"location":"4d%20Algos/#algorithms-for-convex-optimisation","title":"Algorithms for Convex Optimisation","text":"<p>Convex optimization algorithms exploit the geometry of convex sets and functions. Because every local minimum is global, algorithms can converge reliably without worrying about bad local minima.</p> <p>We divide algorithms into three main families:</p> <ol> <li>First-order methods \u2013 use gradients (scalable, but slower convergence).  </li> <li>Second-order methods \u2013 use Hessians (faster convergence, more expensive).  </li> <li>Interior-point methods \u2013 general-purpose, highly accurate solvers.</li> </ol>"},{"location":"4d%20Algos/#gradient-descent-first-order-method","title":"Gradient Descent (First-Order Method)","text":""},{"location":"4d%20Algos/#algorithm","title":"Algorithm","text":"<p>For step size \\(\\alpha &gt; 0\\):  </p>"},{"location":"4d%20Algos/#convergence","title":"Convergence","text":"<ul> <li>If \\(f\\) is convex and \\(\\nabla f\\) is Lipschitz continuous with constant \\(L\\):</li> <li>With fixed step \\(\\alpha \\le \\tfrac{1}{L}\\), we have:      </li> <li>If \\(f\\) is \\(\\mu\\)-strongly convex:          (linear convergence).</li> </ul>"},{"location":"4d%20Algos/#pros-cons","title":"Pros &amp; Cons","text":"<ul> <li>\u2705 Simple, scalable to very high dimensions.  </li> <li>\u274c Slow convergence compared to higher-order methods.</li> </ul>"},{"location":"4d%20Algos/#accelerated-gradient-methods","title":"Accelerated Gradient Methods","text":"<ul> <li>Nesterov\u2019s Accelerated Gradient (NAG): Improves convergence rate from \\(\\mathcal{O}(1/k)\\) to \\(\\mathcal{O}(1/k^2)\\).  </li> <li>Widely used in machine learning (e.g., training deep neural networks).  </li> </ul>"},{"location":"4d%20Algos/#newtons-method-second-order-method","title":"Newton\u2019s Method (Second-Order Method)","text":""},{"location":"4d%20Algos/#algorithm_1","title":"Algorithm","text":"<p>Update rule:  </p>"},{"location":"4d%20Algos/#convergence_1","title":"Convergence","text":"<ul> <li>Quadratic near optimum: </li> <li>Very fast, but requires Hessian and solving linear systems.</li> </ul>"},{"location":"4d%20Algos/#damped-newton","title":"Damped Newton","text":"<p>To maintain global convergence:  </p>"},{"location":"4d%20Algos/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Approximate the Hessian to reduce cost.</p> <ul> <li>BFGS and L-BFGS (limited memory version).  </li> <li>Used in large-scale optimization (e.g., machine learning, statistics).  </li> <li>Convergence: superlinear.  </li> </ul>"},{"location":"4d%20Algos/#subgradient-methods","title":"Subgradient Methods","text":"<p>For nondifferentiable convex functions (e.g., \\(f(x) = \\|x\\|_1\\)).</p> <p>Update rule:  </p> <ul> <li>\\(\\partial f(x)\\): subdifferential (set of all subgradients).  </li> <li>Convergence: \\(\\mathcal{O}(1/\\sqrt{k})\\) with diminishing step sizes.  </li> <li>Useful in large-scale, nonsmooth optimization.</li> </ul>"},{"location":"4d%20Algos/#proximal-methods","title":"Proximal Methods","text":"<p>For composite problems:  where \\(f\\) is smooth convex, \\(g\\) convex but possibly nonsmooth.</p> <p>Proximal operator: </p> <ul> <li>Proximal gradient descent: </li> <li>Widely used in sparse optimization (e.g., Lasso).</li> </ul>"},{"location":"4d%20Algos/#interior-point-methods","title":"Interior-Point Methods","text":"<p>Transform constrained problem into a sequence of unconstrained problems using barrier functions.</p> <p>For constraint \\(g_i(x) \\le 0\\), replace with barrier:  </p> <p>Solve:  for increasing \\(t\\).</p>"},{"location":"4d%20Algos/#properties","title":"Properties","text":"<ul> <li>Polynomial-time complexity for convex problems.  </li> <li>Extremely accurate solutions.  </li> <li>Basis of general-purpose solvers (e.g., CVX, MOSEK, Gurobi).  </li> </ul>"},{"location":"4d%20Algos/#coordinate-descent","title":"Coordinate Descent","text":"<p>At each iteration, optimize w.r.t. one coordinate (or block of coordinates):</p> \\[ x_i^{k+1} = \\arg\\min_{z} f(x_1^k, \\dots, x_{i-1}^k, z, x_{i+1}^k, \\dots, x_n^k) \\] <ul> <li>Works well for high-dimensional problems.  </li> <li>Used in Lasso, logistic regression, and large-scale ML problems.</li> </ul>"},{"location":"4d%20Algos/#primal-dual-and-splitting-methods","title":"Primal-Dual and Splitting Methods","text":"<ul> <li> <p>ADMM (Alternating Direction Method of Multipliers):   Splits problem into subproblems, solves in parallel.   Popular in distributed optimization and ML.  </p> </li> <li> <p>Primal-dual interior-point methods:   Solve both primal and dual simultaneously.  </p> </li> </ul>"},{"location":"4d%20Algos/#summary-of-convergence-rates","title":"Summary of Convergence Rates","text":"Method Smooth Convex Strongly Convex Gradient Descent \\(\\mathcal{O}(1/k)\\) Linear Accelerated Gradient \\(\\mathcal{O}(1/k^2)\\) Linear Subgradient \\(\\mathcal{O}(1/\\sqrt{k})\\) \u2013 Newton\u2019s Method Quadratic (local) Quadratic Interior-Point Polynomial-time Polynomial-time"},{"location":"4d%20Algos/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<ul> <li>Small problems, high accuracy: Newton, Interior-point.  </li> <li>Large-scale smooth problems: Gradient descent, Nesterov acceleration, L-BFGS.  </li> <li>Large-scale nonsmooth problems: Subgradient, Proximal, ADMM.  </li> <li>Sparse / structured constraints: Coordinate descent, Proximal methods.  </li> </ul>"},{"location":"4d%20Algos/#log-concavity-and-log-convexity","title":"Log-concavity and Log-convexity","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}_{++}\\) is:</p> <ul> <li>Log-concave if \\(\\log f(x)\\) is concave.</li> <li>Log-convex if \\(\\log f(x)\\) is convex.</li> </ul>"},{"location":"4d%20Algos/#relevance","title":"Relevance","text":"<ul> <li>Log-concave functions appear in probability (many common distributions have log-concave densities, such as Gaussian, exponential, and uniform). This ensures tractability of maximum likelihood estimation.</li> <li>Log-convexity is useful in geometric programming, where monomials and posynomials are log-convex.</li> </ul>"},{"location":"4d%20Algos/#examples","title":"Examples","text":"<ul> <li>Gaussian density is log-concave.</li> <li>Exponential function is log-convex.</li> </ul>"},{"location":"4d%20Algos/#geometric-programming","title":"Geometric Programming","text":"<p>Geometric programming (GP) is a class of problems of the form:  where each \\(f_i\\) is a posynomial.</p> <ul> <li>Monomial: \\(f(x) = c x_1^{a_1} x_2^{a_2} \\dots x_n^{a_n}\\), with \\(c &gt; 0\\), exponents real.</li> <li>Posynomial: Sum of monomials.</li> </ul> <p>By applying the log transformation \\(y_i = \\log x_i\\), the problem becomes convex.</p>"},{"location":"7a%20pareto%20optimal/","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#pareto-optimality","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#classical-optimality","title":"Classical Optimality","text":"<p>In standard convex optimisation, we consider a single objective function \\(f(x)\\) and aim to find a globally optimal solution:  where \\(\\mathcal{X}\\) is the feasible set.  </p> <p>Here, optimality is absolute: there exists a single best point (or set of equivalent best points) with respect to one measure of performance.</p>"},{"location":"7a%20pareto%20optimal/#multi-objective-optimisation","title":"Multi-objective Optimisation","text":"<p>Many practical problems in machine learning and optimisation involve multiple competing objectives. For instance:</p> <ul> <li>In supervised learning, one wishes to minimise prediction error while also controlling model complexity.  </li> <li>In fairness-aware learning, we want high accuracy while limiting demographic disparity.  </li> <li>In finance, an investor balances expected return against risk.  </li> </ul> <p>Formally, a multi-objective optimisation problem is written as:  where \\(f_1, f_2, \\dots, f_k\\) are the competing objectives.</p>"},{"location":"7a%20pareto%20optimal/#pareto-optimality_1","title":"Pareto Optimality","text":""},{"location":"7a%20pareto%20optimal/#strong-pareto-optimality","title":"Strong Pareto Optimality","text":"<p>A solution \\(x^* \\in \\mathcal{X}\\) is Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  with strict inequality for at least one objective \\(j\\).  </p> <p>Intuitively, no feasible point strictly improves one objective without worsening another.</p>"},{"location":"7a%20pareto%20optimal/#weak-pareto-optimality","title":"Weak Pareto Optimality","text":"<p>A solution \\(x^*\\) is weakly Pareto optimal if there is no \\(x \\in \\mathcal{X}\\) such that:  </p> <p>In other words, no solution improves every objective simultaneously.</p>"},{"location":"7a%20pareto%20optimal/#geometric-intuition","title":"Geometric Intuition","text":"<p>If we plot feasible solutions in the objective space \\((f_1(x), f_2(x))\\), the Pareto frontier is the lower-left boundary for minimisation problems. - Points on the frontier are non-dominated (Pareto optimal). - Points inside the feasible region but above the boundary are dominated.  </p>"},{"location":"7a%20pareto%20optimal/#scalarisation","title":"Scalarisation","text":"<p>Since multi-objective optimisation problems usually admit a set of Pareto optimal solutions rather than a single best point, practitioners use scalarisation. This reduces multiple objectives to a single scalar objective that can be optimised with standard methods.</p>"},{"location":"7a%20pareto%20optimal/#weighted-sum-scalarisation","title":"Weighted Sum Scalarisation","text":"<p>The most common approach is the weighted sum:  </p> <ul> <li>Each choice of weights \\(w\\) corresponds to a different point on the Pareto frontier.  </li> <li>Larger \\(w_i\\) prioritises objective \\(f_i\\) relative to others.  </li> </ul> <p>Convexity caveat: If the feasible set and objectives are convex, weighted sum scalarisation can recover the convex part of the Pareto frontier. Non-convex regions of the frontier may not be attainable using weighted sums alone.</p>"},{"location":"7a%20pareto%20optimal/#varepsilon-constraint-method","title":"\\(\\varepsilon\\)-Constraint Method","text":"<p>Another approach is to optimise one objective while converting others into constraints:  Here \\(\\varepsilon_i\\) are tolerance levels. By adjusting them, we can explore different trade-offs.  </p> <p>This connects directly to regularisation in machine learning: - In ridge regression, we minimise data fit subject to a complexity budget \\(\\|x\\|_2^2 \\leq \\tau\\). - The equivalent penalised form \\(\\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2\\) is obtained via Lagrangian duality, where \\(\\lambda\\) is the multiplier associated with \\(\\tau\\).  </p>"},{"location":"7a%20pareto%20optimal/#duality-and-scalarisation","title":"Duality and Scalarisation","text":"<p>Scalarisation is deeply connected to duality in convex optimisation: - The weights \\(w_i\\) or multipliers \\(\\lambda\\) can be interpreted as Lagrange multipliers balancing objectives. - Adjusting these parameters changes the point on the Pareto frontier that is selected. - This explains why hyperparameters like \\(\\lambda\\) in regularisation are so influential: they represent trade-offs in a hidden multi-objective problem.</p>"},{"location":"7a%20pareto%20optimal/#example-1-regularised-least-squares","title":"Example 1: Regularised Least Squares","text":"<p>Consider the regression problem with data matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) and target \\(b \\in \\mathbb{R}^m\\).  </p> <p>We want to minimise: 1. Prediction error: \\(f_1(x) = \\|Ax - b\\|_2^2\\) 2. Model complexity: \\(f_2(x) = \\|x\\|_2^2\\) </p> <p>This is a two-objective optimisation problem.  </p> <ul> <li> <p>Using the weighted sum:  where \\(\\lambda \\geq 0\\) determines the trade-off.  </p> </li> <li> <p>Alternatively, using the \\(\\varepsilon\\)-constraint:  </p> </li> </ul> <p>Both formulations yield Pareto optimal solutions, with \\(\\lambda\\) and \\(\\tau\\) providing different parametrisations of the frontier.</p>"},{"location":"7a%20pareto%20optimal/#example-2-portfolio-optimisation-riskreturn","title":"Example 2: Portfolio Optimisation (Risk\u2013Return)","text":"<p>In finance, suppose an investor chooses portfolio weights \\(w \\in \\mathbb{R}^n\\).</p> <ul> <li>Expected return: \\(f_1(w) = -\\mu^\\top w\\) (we minimise negative return).  </li> <li>Risk: \\(f_2(w) = w^\\top \\Sigma w\\) (variance of returns, a convex function).  </li> </ul> <p>The problem is:  </p> <ul> <li>Using weighted sum scalarisation:  </li> <li>Different \\(\\alpha\\) values give different points on the efficient frontier.  </li> </ul> <p>This convex formulation underpins modern portfolio theory.</p>"},{"location":"7a%20pareto%20optimal/#example-3-probabilistic-modelling-elbo","title":"Example 3: Probabilistic Modelling (ELBO)","text":"<p>In variational inference, the Evidence Lower Bound (ELBO) is:  </p> <p>This can be seen as a scalarisation of two competing objectives: 1. Data fit (reconstruction term). 2. Simplicity or prior adherence (KL divergence).  </p> <p>By weighting the KL divergence with a parameter \\(\\beta\\), we obtain the \\(\\beta\\)-VAE:  </p> <p>Here, \\(\\beta\\) plays the role of a scalarisation weight, selecting different Pareto optimal trade-offs between reconstruction accuracy and disentanglement.</p>"},{"location":"7a%20pareto%20optimal/#broader-connections-in-ai-and-ml","title":"Broader Connections in AI and ML","text":"<ul> <li>Fairness vs accuracy: Balancing accuracy with fairness metrics is a multi-objective problem often approached via scalarisation.  </li> <li>Generalisation vs training error: Regularisation is a scalarisation of fit versus complexity.  </li> <li>Compression vs performance: The information bottleneck principle is a Pareto trade-off between accuracy and representation complexity.  </li> <li>Inference vs divergence: Variational inference (ELBO) is naturally a scalarised multi-objective problem.  </li> </ul>"},{"location":"7a%20pareto%20optimal/#summary","title":"Summary","text":"<ul> <li>Classical optimisation yields a single best solution.  </li> <li>Multi-objective optimisation gives a set of non-dominated (Pareto optimal) solutions.  </li> <li>Scalarisation provides practical methods to compute Pareto optimal solutions.  </li> <li>Weighted sums recover convex parts of the frontier, while \\(\\varepsilon\\)-constraints provide flexibility.  </li> <li>Scalarisation connects directly to duality, where weights act as Lagrange multipliers.  </li> <li>Examples in ML (ridge regression, ELBO) and finance (portfolio optimisation) demonstrate its wide relevance.  </li> </ul> <p>Scalarisation is not only a mathematical device but the foundation for understanding regularisation, fairness, generalisation, and many practical trade-offs in machine learning.</p>"},{"location":"Example/","title":"Galactic Cargo Delivery Optimization \u2014 LP Formulation","text":""},{"location":"Example/#galactic-cargo-delivery-optimization-lp-formulation","title":"Galactic Cargo Delivery Optimization \u2014 LP Formulation","text":"<p>You are the logistics commander of an interstellar fleet tasked with delivering vital supplies across the galaxy. Your fleet consists of \\(N\\) starship pilots, and you have \\(K\\) distinct types of cargo crates to deliver. Each cargo type \\(j\\) has a known volume \\(v_j\\), representing the number of crates that must reach their destinations.</p> <p>To maintain fleet balance and operational efficiency, each pilot must carry the same total number of crates. Your mission is to assign crates to pilots to minimize the total expected delivery time, accounting for each pilot\u2019s unique speed and proficiency with different cargo types.</p>"},{"location":"Example/#notation","title":"Notation","text":"<ul> <li>\\(i = 1, \\ldots, N\\): indices for starship pilots  </li> <li>\\(j = 1, \\ldots, K\\): indices for cargo types  </li> <li>\\(v_j\\): volume (number of crates) of cargo type \\(j\\) </li> <li>\\(d_{ij}\\): estimated delivery time for pilot \\(i\\) to deliver one crate of type \\(j\\)</li> </ul>"},{"location":"Example/#decision-variables","title":"Decision Variables","text":"<p>\\(x_{ij} \\geq 0\\)</p> <p>Number of crates of cargo type \\(j\\) assigned to pilot \\(i\\).</p>"},{"location":"Example/#objective-function","title":"Objective Function","text":"<p>Minimize the total delivery time across all pilots:</p> \\[\\min \\sum_{i=1}^N \\sum_{j=1}^K d_{ij} x_{ij}\\] <p>Or equivalently, in vector form:</p> \\[\\min c^T x\\] <p>where</p> \\[ c = \\begin{bmatrix} d_{11}, d_{12}, \\ldots, d_{1K}, d_{21}, \\ldots, d_{NK} \\end{bmatrix}^T, \\quad x = \\begin{bmatrix} x_{11}, x_{12}, \\ldots, x_{1K}, x_{21}, \\ldots, x_{NK} \\end{bmatrix}^T \\]"},{"location":"Example/#constraints","title":"Constraints","text":"<ol> <li>All crates must be delivered:</li> </ol> \\[\\sum_{i=1}^N x_{ij} = v_j, \\quad \\forall j = 1, \\ldots, K\\] <ol> <li>Each pilot carries the same total number of crates:</li> </ol> \\[\\sum_{j=1}^K x_{ij} = \\frac{V}{N}, \\quad \\forall i = 1, \\ldots, N \\quad \\text{where} \\quad V = \\sum_{j=1}^K v_j\\] <ol> <li>Non-negativity: \\(\\(x_{ij} \\geq 0, \\quad \\forall i,j\\)\\)</li> </ol>"},{"location":"Example/#lp-formulation","title":"LP Formulation","text":"\\[ \\begin{aligned} \\min_{x \\in \\mathbb{R}^{N \\times K}} \\quad &amp; c^T x \\\\ \\text{subject to} \\quad &amp; \\begin{cases} A_{eq} x = b_{eq} \\\\ x \\geq 0 \\end{cases} \\end{aligned}\\] <p>Where:</p> <ul> <li>\\(A_{eq} \\in \\mathbb{R}^{(N + K) \\times (N \\cdot K)}\\) encodes the equality constraints for cargo delivery and load balancing.</li> <li>\\(b_{eq} \\in \\mathbb{R}^{N + K}\\) combines the crate volumes \\(v_j\\) and equal load targets \\(\\frac{V}{N}\\).</li> </ul>"},{"location":"test/","title":"Test","text":""}]}