<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/1_10_ineqaulities/">
      
      
        <link rel="prev" href="../3_8_models/">
      
      
        <link rel="next" href="../1_11_support/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Appendix A - Common Inequalities and Identities - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#appendix-a-common-inequalities-and-identities" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Appendix A - Common Inequalities and Identities
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization:
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization:
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_0_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_1_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_8_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_9_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_7a_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_12_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_13_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_0_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_7_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_8_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_11_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/1_10_ineqaulities.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/1_10_ineqaulities.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="appendix-a-common-inequalities-and-identities">Appendix A: Common Inequalities and Identities<a class="headerlink" href="#appendix-a-common-inequalities-and-identities" title="Permanent link">¶</a></h1>
<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the “algebraic tools” you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemaréchal, 2001).</p>
<hr>
<h2 id="a1-cauchyschwarz-inequality">A.1 Cauchy–Schwarz inequality<a class="headerlink" href="#a1-cauchyschwarz-inequality" title="Permanent link">¶</a></h2>
<p>For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>,
<script type="math/tex; mode=display">
|x^\top y| \le \|x\|_2 \, \|y\|_2.
</script>
</p>
<p>Equality holds if and only if <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent.</p>
<p>Consequences:</p>
<ul>
<li>Defines the notion of angle between vectors.</li>
<li>Justifies dual norms.</li>
</ul>
<hr>
<h2 id="a2-jensens-inequality">A.2 Jensen’s inequality<a class="headerlink" href="#a2-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex, and let <span class="arithmatex">\(X\)</span> be a random variable. Then
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>In finite form: for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum_i \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<p>Jensen’s inequality is equivalent to convexity: it says “the function at the average is no more than the average of the function values.” It is used constantly to prove convexity of expectations and log-sum-exp.</p>
<hr>
<h2 id="a3-amgm-inequality">A.3 AM–GM inequality<a class="headerlink" href="#a3-amgm-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(x_1,\dots,x_n \ge 0\)</span>,
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^n x_i
\ge
\left(\prod_{i=1}^n x_i \right)^{1/n}.
</script>
</p>
<p>This can be proved using Jensen’s inequality with <span class="arithmatex">\(f(t) = \log t\)</span>, which is concave. AM–GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>
<hr>
<h2 id="a4-holders-inequality-generalised-cauchyschwarz">A.4 Hölder’s inequality (generalised Cauchy–Schwarz)<a class="headerlink" href="#a4-holders-inequality-generalised-cauchyschwarz" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(p,q \ge 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> (conjugate exponents),
<script type="math/tex; mode=display">
\sum_{i=1}^n |x_i y_i|
\le
\left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
\left( \sum_{i=1}^n |y_i|^q \right)^{1/q}.
</script>
</p>
<ul>
<li>When <span class="arithmatex">\(p=q=2\)</span>, Hölder becomes Cauchy–Schwarz.</li>
<li>Hölder underlies dual norms: the dual of <span class="arithmatex">\(\ell_p\)</span> is <span class="arithmatex">\(\ell_q\)</span>.</li>
</ul>
<hr>
<h2 id="a5-youngs-inequality">A.5 Young’s inequality<a class="headerlink" href="#a5-youngs-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(a,b \ge 0\)</span> and <span class="arithmatex">\(p,q &gt; 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>,
<script type="math/tex; mode=display">
ab \le \frac{a^p}{p} + \frac{b^q}{q}.
</script>
</p>
<p>This is useful in bounding cross terms in convergence proofs.</p>
<hr>
<h2 id="a6-fenchels-inequality">A.6 Fenchel’s inequality<a class="headerlink" href="#a6-fenchels-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be a convex function and let <span class="arithmatex">\(f^*\)</span> be its convex conjugate:
<script type="math/tex; mode=display">
f^*(y) = \sup_x (y^\top x - f(x)).
</script>
</p>
<p>Then for all <span class="arithmatex">\(x,y\)</span>,
<script type="math/tex; mode=display">
f(x) + f^*(y) \ge y^\top x.
</script>
</p>
<p>Fenchel’s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel’s inequality.</p>
<hr>
<h2 id="a7-supporting-hyperplane-inequality">A.7 Supporting hyperplane inequality<a class="headerlink" href="#a7-supporting-hyperplane-inequality" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is convex, then for any <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\(g \in \partial f(x)\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y-x)
\quad \text{for all } y.
</script>
</p>
<p>This can be viewed as “<span class="arithmatex">\(f\)</span> lies above all its tangent hyperplanes,” even when it’s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>
<hr>
<h2 id="a8-summary">A.8 Summary<a class="headerlink" href="#a8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Cauchy–Schwarz and Hölder bound inner products.</li>
<li>Jensen shows convexity and expectation interact cleanly.</li>
<li>Fenchel’s inequality is the algebra of duality.</li>
<li>Supporting hyperplane inequality is the geometry of convexity.</li>
</ul>
<p>These inequalities are used implicitly all over convex optimisation.</p>
<!-- Throughout convex optimization theory, certain inequalities appear repeatedly to bound quantities or prove convexity. We collect a few fundamental ones that every practitioner should know, emphasizing their intuitive meaning and use cases.

**Cauchy–Schwarz Inequality:** For any vectors $x, y \in \mathbb{R}^n$

$$
|\langle x, y \rangle| \le \|x\|_2 \, \|y\|_2
$$


with equality if and only if $x$ and $y$ are collinear (one is a scalar multiple of the other). This inequality is a cornerstone of Euclidean geometry. It can be seen as a statement that the projection of $x$ onto $y$ cannot exceed the lengths product, or that the cosine of the angle between $x,y$ is at most 1. In optimization, Cauchy–Schwarz justifies many step-size bounds and duality relations. For example, in gradient descent: $f(x_{k+1}) = f(x_k) + \langle \nabla f(x_k), x_{k+1}-x_k\rangle + \cdots$ and choosing $x_{k+1}-x_k = -\alpha \nabla f(x_k)$ gives the maximal decrease in the linear approximation sense because it negates the gradient direction. Cauchy–Schwarz is also used to derive error bounds: $|g^T(x - \hat{x})| \le |g|_2 |x-\hat{x}|_2$, which plugged into optimality conditions yields, for example, primal-dual gap bounds or sensitivity analysis results.

**Triangle Inequality (Minkowski’s inequality):** For any vectors $x,y$ in a normed space,

$$
\|x + y\| \le \|x\| + \|y\|
$$


Geometrically, the direct path from 0 to $x+y$ is no longer than going from 0 to $x$ and then $x$ to $x+y$ (which is same as 0 to $y$ after shifting origin to $x$). In $\ell_2$, this is the everyday triangle rule. In $\ell_1$, it says $|x_1+y_1| + \cdots + |x_n+y_n| \le (|x_1|+\cdots+|x_n|)+(|y_1|+\cdots+|y_n|)$, obvious since term by term it's true. In $\ell_\infty$, $\max_i |x_i+y_i| \le \max_i |x_i| + \max_i |y_i|$. The triangle inequality ensures that norm balls are convex (their defining inequality is linear in two points). In optimization algorithms, the triangle inequality is often used to decompose error terms: $|x_k - \hat{x}| \le |x_k - \bar{x}| + |\bar{x} - \hat{x}|$ for some intermediate $\bar{x}$, or to prove convergence by bounding how far the current iterate is after one step plus how far that step could overshoot. Minkowski’s inequality is a generalization: for $p\ge1$, $|x+y|_p^p \le |x|_p^p + |y|_p^p +$ cross terms (which lead to the basic form above after taking pth root).

**Jensen’s Inequality:** If $f$ is convex and $x_1,\dots,x_m$ are points with weights $\lambda_i \ge 0$, $\sum \lambda_i=1$, then

$$
f\!\left(\sum_i \lambda_i x_i\right)
\le
\sum_i \lambda_i f(x_i)
$$


Equivalently, $f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]$. We introduced this in the convex function chapter. It’s extremely useful for expectations and probabilistic analysis. A common application: AM–GM inequality: take $f(x) = e^x$ convex, and $x_i = \ln a_i$, $\lambda_i = 1/n$. Then Jensen gives $e^{\frac{1}{n}\sum \ln a_i} \le \frac{1}{n}\sum e^{\ln a_i}$, i.e. $(\prod_{i} a_i)^{1/n} \le \frac{1}{n}\sum a_i$. This is the statement that the geometric mean is at most the arithmetic mean. Equality holds if all $a_i$ equal (so logs equal, function is linear segment). Another: using $f(x)=x^2$ convex and weights, one shows RMS-AM: $\sqrt{\frac{1}{n}\sum a_i^2} \ge \frac{1}{n}\sum a_i$. Jensen’s inequality is often used in convex optimization proofs to move expectation inside a function or vice versa. For instance, in demonstrating convexity of a function that is defined as an expectation: $F(\theta) = \mathbb{E}_\xi[\phi(\theta,\xi)]$ is convex in $\theta$ if each $\phi(\theta,\xi)$ is convex, because $F(\lambda \theta_1+(1-\lambda)\theta_2) = \mathbb{E}[\phi(\lambda \theta_1+(1-\lambda)\theta_2,\xi)] \le \mathbb{E}[\lambda \phi(\theta_1,\xi)+(1-\lambda)\phi(\theta_2,\xi)] = \lambda F(\theta_1) + (1-\lambda) F(\theta_2)$. This is Jensen in action with probability weights. In stochastic gradient methods, one often uses Jensen to argue that the expected objective is decreased even if each step only guarantees decrease in expectation, etc.

**Hölder’s Inequality:** This generalizes Cauchy–Schwarz to $\ell_p$ and $\ell_q$ spaces. If $\frac{1}{p} + \frac{1}{q} = 1$ (with $p,q\ge1$), then for vectors (or sequences) $a,b$ of appropriate dimension,

$$
\sum_i |a_i b_i|
\le
\left(\sum_i |a_i|^p\right)^{1/p}
\left(\sum_i |b_i|^q\right)^{1/q}
$$


For example, with $p=2,q=2$, this is Cauchy–Schwarz. With $p=1,q=\infty$, it says $\sum |a_i b_i| \le (\sum |a_i|)|b|\infty$ (makes sense: $|b|\infty$ is the max component, factor it out). With $p=q=4/3$ and $q=4$, it’s something more exotic. Hölder’s inequality is very useful in analysis and dual norms: it exactly shows that the dual norm of $\ell_p$ is $\ell_q$. Indeed, take $x$ with $|x|_p=1$, then maximize $\sum x_i y_i$ over such $x$. Setting $x_i = |y_i|^{q-2} y_i / |y|_q^{q-1}$ (the extremizer found by equality case) yields $\sum_i x_i y_i = |y|q$. So $\sup{|x|_p\le1} x^T y = |y|_q$. This is why in Chapter 3 we stated dual norm formula and that $\ell_p$ dual is $\ell_q$; the proof is Hölder’s inequality. In optimization, Hölder is often used to bound sums or integrals when dealing with error terms. For instance, in optimizing integrals or in mirror descent analysis, one often gets a term like $\int g(x) h(x),dx$ and uses Hölder to split it into $|g|_p |h|_q$. In linear programming, Hölder’s inequality appears in the derivation of LP duality: maximizing $c^T x$ subject to $Ax \le b$, $x\ge0$ and its dual $b^T y$ s.t. $A^T y \ge c, y\ge0$. Weak duality follows from $c^T x \le y^T A x \le y^T b$ by $x\ge0$, $A^T y\ge c$, which is essentially an instance of Hölder: $c^T x \le (A^T y)^T x = y^T (Ax) \le y^T b$. Here we used $\sum_i c_i x_i \le \sum_i y_i (A x)_i$ since $c_i \le (A^T y)_i$ and $x_i\ge0$. That step is like $c_i x_i \le y_i (A x)_i$ summing yields the inequality. It’s a discrete form but conceptually related to Hölder’s type argument of splitting products.

**Other inequalities:** There are many more (Markov, Chebyshev, Chernoff – probabilistic; Young’s inequality for products; Lipschitz implies bounded differences, etc.), but the ones above are most directly tied to convex analysis. Another notable one is Fenchel’s inequality: for any convex $f$ and its convex conjugate $\hat{f}$, $f(x) + \hat{f}(y) \ge x^T y$. This is essentially a consequence of the definition of conjugate (supremum form) and is used in deriving duality gaps.


To tie it together: suppose we have a suboptimal point $x$ and optimum $x^*$. Then using a subgradient $g\in\partial f(x)$, convexity (first-order condition) gives $f(x) - f(x^*) \le g^T(x - x^*)$. Now Cauchy–Schwarz (or Hölder for appropriate norm choices) gives $g^T(x-x^*) \le |g| |x-x^*|$. Thus


$$
f(x) - f(x^*) \le \|g\| \, \|x - x^*\|
$$


This inequality is often used: it relates suboptimality to a dual norm of subgradient and distance to solution. If we have a bound on $|g|$ (say by optimality conditions or Lipschitz continuity), then we get a residual bound on $f(x)-f(x^*)$. This is one small example of how combining convexity with Cauchy–Schwarz yields useful insights. -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.expand", "header.autohide", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit", "content.action.view"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>