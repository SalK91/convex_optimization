<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/convex/13_calculus/">
      
      
        <link rel="prev" href="../12_vector/">
      
      
        <link rel="next" href="../14_convexsets/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>3. Multivariable Calculus for Optimization - Machine Learning Lecture Notes -  Optimization &amp; Algorithms</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-3-multivariable-calculus-for-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Lecture Notes -  Optimization &amp; Algorithms" class="md-header__button md-logo" aria-label="Machine Learning Lecture Notes -  Optimization &amp; Algorithms" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Lecture Notes -  Optimization &amp; Algorithms
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. Multivariable Calculus for Optimization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../tutorials/1_lp_transport/" class="md-tabs__link">
          
  
  
    
  
  Case-studies

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../11_intro/" class="md-tabs__link">
          
  
  
    
  
  Convex Optimization

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Lecture Notes -  Optimization &amp; Algorithms" class="md-nav__button md-logo" aria-label="Machine Learning Lecture Notes -  Optimization &amp; Algorithms" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Machine Learning Lecture Notes -  Optimization &amp; Algorithms
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Case-studies
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Case-studies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/1_lp_transport/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I - Transportation Optimization: A Linear Programming Case Study.
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/2_portfolio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    II - Mean–Variance Portfolio Optimization: A Pareto-Optimal Case Study.
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/3_ga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    III - Vehicle Routing with Time Windows: A Metaheuristic Case Study.
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16a_optimality_conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. First-Order Optimality Conditions in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18a_pareto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Pareto Optimality and Multi-Objective Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18b_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Regularized Approximation – Balancing Fit and Complexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19a_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Optimization Algorithms for Equality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19b_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Optimization Algorithms for Inequality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../30_canonical_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Canonical Problems in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../35_modern/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    18. Modern Optimizers in Machine Learning Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../40_nonconvex/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    19. Beyond Convexity – Nonconvex and Global Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../42_derivativefree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    20. Derivative-Free and Black-Box Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../44_metaheuristic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    21. Metaheuristic and Evolutionary Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../48_advanced_combinatorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    22. Advanced Topics in Combinatorial Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../50_future/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    23. The Future of Optimization — Learning, Adaptation, and Intelligence
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/main/docs/convex/13_calculus.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/main/docs/convex/13_calculus.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-3-multivariable-calculus-for-optimization">Chapter 3: Multivariable Calculus for Optimization<a class="headerlink" href="#chapter-3-multivariable-calculus-for-optimization" title="Permanent link">¶</a></h1>
<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p>
<p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton’s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p>
<p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>
<h2 id="gradients-and-directional-derivatives">Gradients and Directional Derivatives<a class="headerlink" href="#gradients-and-directional-derivatives" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>. The function is differentiable at a point <span class="arithmatex">\(x\)</span> if there exists a vector <span class="arithmatex">\(\nabla f(x)\)</span> such that
<script type="math/tex; mode=display">
f(x + h)
=
f(x) + \nabla f(x)^\top h + o(\|h\|),
</script>
meaning that the linear function <span class="arithmatex">\(h \mapsto \nabla f(x)^\top h\)</span> provides the best local approximation to <span class="arithmatex">\(f\)</span> near <span class="arithmatex">\(x\)</span>. The gradient is the unique vector with this property.</p>
<p>A closely related concept is the directional derivative. For any direction <span class="arithmatex">\(v \in \mathbb{R}^n\)</span>, the directional derivative of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> in the direction <span class="arithmatex">\(v\)</span> is
<script type="math/tex; mode=display">
D_v f(x)
=
\lim_{t \to 0} \frac{f(x + tv) - f(x)}{t}.
</script>
If <span class="arithmatex">\(f\)</span> is differentiable, then
<script type="math/tex; mode=display">
D_v f(x) = \nabla f(x)^\top v.
</script>
Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction <span class="arithmatex">\(v\)</span> tells us how rapidly <span class="arithmatex">\(f\)</span> increases when we move infinitesimally along <span class="arithmatex">\(v\)</span>.</p>
<p>This immediately yields an important geometric fact. Among all unit directions <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
D_u f(x) = \langle \nabla f(x), u \rangle
</script>
is maximized when <span class="arithmatex">\(u\)</span> points in the direction of <span class="arithmatex">\(\nabla f(x)\)</span>, the direction of steepest ascent. The steepest descent direction is therefore <span class="arithmatex">\(-\nabla f(x)\)</span>, which motivates gradient-descent algorithms for minimizing functions.</p>
<blockquote>
<p>For any real number <span class="arithmatex">\(c\)</span>, the level set of <span class="arithmatex">\(f\)</span> is 
<script type="math/tex; mode=display">
L_c = \{\, x \in \mathbb{R}^n : f(x) = c \,\}.
</script>
</p>
<p>At any point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x) \ne 0\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to the level set <span class="arithmatex">\(L_{f(x)}\)</span>. Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them — in the direction of the steepest ascent of <span class="arithmatex">\(f\)</span>. If we wish to decrease <span class="arithmatex">\(f\)</span>, we move roughly in the opposite direction, <span class="arithmatex">\(-\nabla f(x)\)</span> (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>
</blockquote>
<h2 id="jacobians">Jacobians<a class="headerlink" href="#jacobians" title="Permanent link">¶</a></h2>
<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>
<h3 id="from-derivative-to-gradient">From derivative to gradient<a class="headerlink" href="#from-derivative-to-gradient" title="Permanent link">¶</a></h3>
<p>For a scalar function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, differentiability means that near any point <script type="math/tex"> x </script>,
<script type="math/tex; mode=display">
f(x + h) \approx f(x) + \nabla f(x)^\top h.
</script>
The gradient vector
<script type="math/tex; mode=display">
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x) \\
\vdots \\
\frac{\partial f}{\partial x_n}(x)
\end{bmatrix}
</script>
collects all partial derivatives. Each component measures how sensitive <span class="arithmatex">\(f\)</span> is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>
<h3 id="from-gradient-to-jacobian">From gradient to Jacobian<a class="headerlink" href="#from-gradient-to-jacobian" title="Permanent link">¶</a></h3>
<p>Now consider a vector-valued function <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span>,
<script type="math/tex; mode=display">
F(x) =
\begin{bmatrix}
F_1(x) \\
\vdots \\
F_m(x)
\end{bmatrix}.
</script>
Each output <span class="arithmatex">\(F_i\)</span> has its own gradient. Stacking these row vectors yields the Jacobian matrix:
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>The Jacobian provides the best linear approximation of <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x)\, h.
</script>
Thus, locally, the nonlinear map <span class="arithmatex">\(F\)</span> behaves like the linear map <span class="arithmatex">\(h \mapsto J_F(x)h\)</span>. A small displacement <span class="arithmatex">\(h\)</span> in input space is transformed into an output change governed by the Jacobian.</p>
<h3 id="interpreting-the-jacobian">Interpreting the Jacobian<a class="headerlink" href="#interpreting-the-jacobian" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Component of <span class="arithmatex">\(J_F(x)\)</span></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row <span class="arithmatex">\(i\)</span></td>
<td>Gradient of output <span class="arithmatex">\(F_i(x)\)</span>: how the <span class="arithmatex">\(i\)</span>-th output changes with each input variable.</td>
</tr>
<tr>
<td>Column <span class="arithmatex">\(j\)</span></td>
<td>Sensitivity of all outputs to <span class="arithmatex">\(x_j\)</span>: how varying input <span class="arithmatex">\(x_j\)</span> affects the entire output vector.</td>
</tr>
<tr>
<td>Determinant (when <span class="arithmatex">\(m=n\)</span>)</td>
<td>Local volume scaling: how <span class="arithmatex">\(F\)</span> expands or compresses space near <span class="arithmatex">\(x\)</span>.</td>
</tr>
<tr>
<td>Rank</td>
<td>Local dimension of the image: whether any input directions are lost or collapsed.</td>
</tr>
</tbody>
</table>
<p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>
<h2 id="the-hessian-and-curvature">The Hessian and Curvature<a class="headerlink" href="#the-hessian-and-curvature" title="Permanent link">¶</a></h2>
<p>For a twice–differentiable function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, the Hessian matrix collects all second-order partial derivatives:
<script type="math/tex; mode=display">
\nabla^{2} f(x) \;=\;
\begin{bmatrix}
\frac{\partial^{2} f}{\partial x_{1}^{2}} & \cdots & \frac{\partial^{2} f}{\partial x_{1}\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial^{2} f}{\partial x_{n}\partial x_{1}} & \cdots & \frac{\partial^{2} f}{\partial x_{n}^{2}}
\end{bmatrix}.
</script>
</p>
<p>The Hessian describes the <strong>local curvature</strong> of the function. While the gradient indicates the direction of steepest change, the Hessian tells us <em>how that directional change itself varies</em>—whether the surface curves upward, curves downward, or remains nearly flat.</p>
<h3 id="curvature-and-positive-definiteness">Curvature and positive definiteness<a class="headerlink" href="#curvature-and-positive-definiteness" title="Permanent link">¶</a></h3>
<p>The eigenvalues of the Hessian determine its geometric behavior:</p>
<ul>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succeq 0 </script> (all eigenvalues nonnegative), the function is locally convex near <span class="arithmatex">\(x\)</span>.  </li>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succ 0 </script>, the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li>
<li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li>
</ul>
<p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>
<h3 id="example-quadratic-functions">Example: Quadratic functions<a class="headerlink" href="#example-quadratic-functions" title="Permanent link">¶</a></h3>
<p>Consider the quadratic function
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2} x^\top Q x - b^\top x,
</script>
where <span class="arithmatex">\(Q\)</span> is symmetric. The gradient and Hessian are
<script type="math/tex; mode=display">
\nabla f(x) = Qx - b, \qquad \nabla^2 f(x) = Q.
</script>
Setting the gradient to zero gives the stationary point
<script type="math/tex; mode=display">
Qx = b.
</script>
If <span class="arithmatex">\(Q \succ 0\)</span>, the solution
<script type="math/tex; mode=display">
x^* = Q^{-1} b
</script>
is the unique minimizer. The Hessian <span class="arithmatex">\(Q\)</span> being positive definite confirms strict convexity.</p>
<p>The eigenvalues of <span class="arithmatex">\(Q\)</span> also explain the difficulty of minimizing <span class="arithmatex">\(f\)</span>:</p>
<ul>
<li>Large eigenvalues produce very steep, narrow directions—optimization methods must take small steps.  </li>
<li>Small eigenvalues produce flat directions—progress is slow, especially for gradient descent.  </li>
</ul>
<p>The ratio of largest to smallest eigenvalue, the <strong>condition number</strong>, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>
<h3 id="why-the-hessian-matters-in-optimization">Why the Hessian matters in optimization<a class="headerlink" href="#why-the-hessian-matters-in-optimization" title="Permanent link">¶</a></h3>
<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p>
<ul>
<li>Newton’s method uses the Hessian to rescale directions, effectively “whitening’’ curvature and often converging rapidly.  </li>
<li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li>
<li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li>
</ul>
<p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>
<h2 id="taylor-approximation">Taylor approximation<a class="headerlink" href="#taylor-approximation" title="Permanent link">¶</a></h2>
<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>
<h3 id="first-order-approximation">First-order approximation<a class="headerlink" href="#first-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then for small steps <span class="arithmatex">\(d\)</span>,
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x) + \nabla f(x)^\top d.
</script>
The gradient gives the best linear model of the function near <span class="arithmatex">\(x\)</span>. This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>
<h3 id="second-order-approximation">Second-order approximation<a class="headerlink" href="#second-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is twice differentiable, we can include curvature information:
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x)
+ \nabla f(x)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x)\, d.
</script>
The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p>
<ul>
<li>If <script type="math/tex"> \nabla^2 f(x) \succeq 0 </script>, the quadratic term is nonnegative and the function curves upward—locally bowl-shaped.</li>
<li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others—characteristic of saddle points.</li>
</ul>
<h3 id="role-in-optimization-algorithms">Role in optimization algorithms<a class="headerlink" href="#role-in-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>Second-order Taylor models are the basis of Newton-type methods. Newton’s method chooses <span class="arithmatex">\(d\)</span> by approximately minimizing the quadratic model,
<script type="math/tex; mode=display">
d \approx - \left(\nabla^2 f(x)\right)^{-1} \nabla f(x),
</script>
which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p>
<p>Thus, Taylor expansions connect a function’s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>
<h2 id="smoothness-and-strong-convexity">Smoothness and Strong Convexity<a class="headerlink" href="#smoothness-and-strong-convexity" title="Permanent link">¶</a></h2>
<p>In optimization, the behavior of a function’s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>
<h3 id="lipschitz-continuous-gradients-l-smoothness">Lipschitz continuous gradients (L-smoothness)<a class="headerlink" href="#lipschitz-continuous-gradients-l-smoothness" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> has an <span class="arithmatex">\(L\)</span>-Lipschitz continuous gradient if
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\| \qquad \forall x, y.
</script>
This condition limits how quickly the gradient can change. Intuitively, an <span class="arithmatex">\(L\)</span>-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:
<script type="math/tex; mode=display">
f(y)
\le
f(x)
+
\nabla f(x)^\top (y - x)
+
\frac{L}{2}\|y - x\|^2.
</script>
This inequality states that every <span class="arithmatex">\(L\)</span>-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p>
<p>In gradient descent, smoothness directly determines a safe step size: choosing
<script type="math/tex; mode=display">
\eta \le \frac{1}{L}
</script>
ensures that each update decreases the function value for convex objectives. In machine learning, the constant <span class="arithmatex">\(L\)</span> effectively controls how large the learning rate can be before training becomes unstable.</p>
<h3 id="strong-convexity">Strong convexity<a class="headerlink" href="#strong-convexity" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> is <script type="math/tex"> \mu </script>-strongly convex if, for some <script type="math/tex"> \mu > 0 </script>,
<script type="math/tex; mode=display">
f(y)
\ge
f(x)
+
\langle \nabla f(x),\, y - x \rangle
+
\frac{\mu}{2}\|y - x\|^2
\qquad \forall x, y.
</script>
This condition guarantees that <span class="arithmatex">\(f\)</span> has at least <span class="arithmatex">\(\mu\)</span> amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p>
<p>Strong convexity has major optimization implications:</p>
<ul>
<li>The minimizer is unique.  </li>
<li>Gradient descent converges linearly with step size <span class="arithmatex">\(\eta \le 1/L\)</span>.  </li>
<li>The ratio <span class="arithmatex">\(L / \mu\)</span> (the condition number) dictates convergence speed.</li>
</ul>
<h3 id="curvature-in-both-directions">Curvature in both directions<a class="headerlink" href="#curvature-in-both-directions" title="Permanent link">¶</a></h3>
<p>Together, smoothness and strong convexity bound the curvature of <span class="arithmatex">\(f\)</span>:
<script type="math/tex; mode=display">
\mu I \;\preceq\; \nabla^2 f(x) \;\preceq\; L I.
</script>
Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p>
<p>These concepts, imiting curvature from above via <span class="arithmatex">\(L\)</span> and from below via <span class="arithmatex">\(\mu\)</span>, form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p>
<h2 id="mental-map">Mental map<a class="headerlink" href="#mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                Multivariable Calculus for Optimization
        How objectives change, how curvature shapes algorithms
                              │
                              ▼
                 Local change of a scalar function f(x)
                              │
                              ▼
     ┌───────────────────────────────────────────────────────────┐
     │ Differentiability &amp; First-Order Model                     │
     │ f(x+h) = f(x) + ∇f(x)ᵀh + o(‖h‖)                          │
     │ - ∇f(x): best linear approximation                        │
     │ - Directional derivative: D_v f(x) = ∇f(x)ᵀv              │
     │ - Steepest descent: move along -∇f(x)                     │
     └───────────────────────────────────────────────────────────┘
                              │
                              ▼
     ┌─────────────────────────────────────────────────────────────┐
     │ Geometry of Level Sets                                      │
     │ L_c = {x : f(x)=c}                                          │
     │ - If ∇f(x) ≠ 0, then ∇f(x) ⟂ level set at x                 │
     │ - Connects to constrained optimality (later: KKT)           │
     └─────────────────────────────────────────────────────────────┘
                              │
                              ▼
     ┌─────────────────────────────────────────────────────────────┐
     │ Vector-Valued Maps &amp; Jacobians                              │
     │ F: ℝⁿ → ℝᵐ                                                  │
     │ - Jacobian J_F(x) stacks gradients of outputs               │
     │ - Linearization: F(x+h) ≈ F(x) + J_F(x) h                   │
     │ - Chain rule foundation for backprop / sensitivity analysis │
     └─────────────────────────────────────────────────────────────┘
                              │
                              ▼
     ┌───────────────────────────────────────────────────────────┐
     │ Second-Order Structure: Hessian &amp; Curvature               │
     │ ∇²f(x): matrix of second partials                         │
     │ - Curvature along v: vᵀ∇²f(x)v                            │
     │ - Eigenvalues quantify steep/flat directions              │
     │ - PSD/PD Hessian ties directly to convexity (Ch.5)        │
     └───────────────────────────────────────────────────────────┘
                              │
                              ▼
     ┌───────────────────────────────────────────────────────────┐
     │ Taylor Models → Algorithm Design                          │
     │ First-order:  f(x+d) ≈ f(x) + ∇f(x)ᵀd                     │
     │ Second-order: f(x+d) ≈ f(x) + ∇f(x)ᵀd + ½ dᵀ∇²f(x)d       │
     │ - Gradient descent uses the linear model                  │
     │ - Newton uses the quadratic model: d ≈ -(∇²f)^{-1}∇f      │
     │ - Trust-region / quasi-Newton approximate curvature       │
     └───────────────────────────────────────────────────────────┘
                              │
                              ▼
     ┌────────────────────────────────────────────────────────────────┐
     │ Global Control of Local Behavior: Smoothness &amp; Strong Convexity│
     │ L-smooth: ‖∇f(x)-∇f(y)‖ ≤ L‖x-y‖                               │
     │ - Descent Lemma gives a quadratic upper bound                  │
     │ - Sets safe step size: η ≤ 1/L (for convex objectives)         │
     │ μ-strongly convex: f lies above tangents by (μ/2)‖y-x‖²        │
     │ - Unique minimizer, linear convergence of gradient descent     │
     │ Combined curvature bounds: μI ⪯ ∇²f(x) ⪯ LI                   │
     │ - Condition number κ = L/μ governs difficulty                  │
     └────────────────────────────────────────────────────────────────┘
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../12_vector/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 2. Linear Algebra Foundations">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                2. Linear Algebra Foundations
              </div>
            </div>
          </a>
        
        
          
          <a href="../14_convexsets/" class="md-footer__link md-footer__link--next" aria-label="Next: 4. Convex Sets and Geometric Fundamentals">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                4. Convex Sets and Geometric Fundamentals
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>