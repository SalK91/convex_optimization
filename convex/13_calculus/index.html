<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/convex/13_calculus/">
      
      
        <link rel="prev" href="../12_vector/">
      
      
        <link rel="next" href="../14_convexsets/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>3. Multivariable Calculus for Optimization - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-3-multivariable-calculus-for-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. Multivariable Calculus for Optimization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16a_optimality_conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. First-Order Optimality Conditions in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18a_pareto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Pareto Optimality and Multi-Objective Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18b_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Regularized Approximation – Balancing Fit and Complexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19a_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Optimization Algorithms for Equality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19b_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Optimization Algorithms for Inequality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../30_canonical_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Canonical Problems in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3">
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimization Beyond Convexity
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Optimization Beyond Convexity
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nonconvex/41_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nonconvex/42_meta/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Metaheuristic Optimization Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../nonconvex/43_hybrid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Hybrid and Modern Optimization Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4">
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Cheat Sheets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Cheat Sheets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/20a_cheatsheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algos - Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5">
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Appendices
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Appendices
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/160_conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix D - Convex Conjugates and Fenchel Duality (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/170_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix E - Convexity in Probability and Statistics (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/180_subgradient_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix F - Subgradient Method and Variants (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/190_proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix G - Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/200_mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix H - Mirror Descent and Bregman Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendices/300_matrixfactorization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix I - Matrix Factorization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/convex_optimization/edit/master/docs/convex/13_calculus.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/convex_optimization/raw/master/docs/convex/13_calculus.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-3-multivariable-calculus-for-optimization">Chapter 3: Multivariable Calculus for Optimization<a class="headerlink" href="#chapter-3-multivariable-calculus-for-optimization" title="Permanent link">¶</a></h1>
<p>Optimization seeks to find points that minimize or maximize a real-valued function. To analyze and solve such problems, we rely on tools from multivariable calculus — gradients, Jacobians, Hessians, and Taylor expansions — which describe how a function changes locally.</p>
<p>This chapter provides the differential calculus foundation essential for convex analysis and gradient-based learning algorithms.  It links geometric intuition and analytical tools that underlie optimization methods such as gradient descent, Newton’s method, and backpropagation.</p>
<h2 id="31-gradients-and-directional-derivatives">3.1 Gradients and Directional Derivatives<a class="headerlink" href="#31-gradients-and-directional-derivatives" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f:\mathbb{R}^n\to\mathbb{R}\)</span>. Differentiability at <span class="arithmatex">\(x\)</span> means there exists a linear map (the gradient) such that</p>
<div class="arithmatex">\[
f(x+h)=f(x)+\nabla f(x)^\top h+o(\|h\|).
\]</div>
<p>Equivalently, for every direction <span class="arithmatex">\(v\in\mathbb{R}^n\)</span>, the directional derivative exists and matches the gradient pairing:</p>
<div class="arithmatex">\[
D_v f(x)=\lim_{t \rightarrow 0}\frac{f(x+tv)-f(x)}{t}=\nabla f(x)^\top v.
\]</div>
<p>This shows that <span class="arithmatex">\(\nabla f(x)\)</span> is the unique vector giving the best linear approximation of <span class="arithmatex">\(f\)</span> near <span class="arithmatex">\(x\)</span>.<br>
Among all unit directions <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
D_u f(x) = \langle \nabla f(x), u \rangle
</script>
is maximized when <span class="arithmatex">\(u\)</span> aligns with <span class="arithmatex">\(\nabla f(x)\)</span> — the direction of steepest ascent.<br>
The opposite direction, <span class="arithmatex">\(- \nabla f(x)\)</span>, gives the steepest descent.</p>
<blockquote>
<p>A level set of a differentiable function <span class="arithmatex">\(f\)</span> is
<script type="math/tex; mode=display">
L_c = \{\, x \in \mathbb{R}^n : f(x) = c \,\}.
</script>
</p>
<p>At any point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x) \ne 0\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to the level set <span class="arithmatex">\(L_{f(x)}\)</span>. Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them — in the direction of the steepest ascent of <span class="arithmatex">\(f\)</span>. If we wish to decrease <span class="arithmatex">\(f\)</span>, we move roughly in the opposite direction, <span class="arithmatex">\(-\nabla f(x)\)</span> (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>
</blockquote>
<h2 id="32-jacobians">3.2  Jacobians<a class="headerlink" href="#32-jacobians" title="Permanent link">¶</a></h2>
<p>When dealing with optimization or learning, functions rarely map one number to another. They often map many inputs to many outputs — for example, a neural network layer, a physical model, or a vector-valued transformation. The Jacobian matrix captures <em>how each output reacts to each input</em> — the complete local sensitivity of a system.</p>
<p>Derivative to Gradient:  For a scalar function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, the derivative generalizes to the gradient:</p>
<div class="arithmatex">\[
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x) \\
\vdots \\
\frac{\partial f}{\partial x_n}(x)
\end{bmatrix}.
\]</div>
<ul>
<li>Each component <span class="arithmatex">\(\frac{\partial f}{\partial x_i}\)</span> tells how <span class="arithmatex">\(f\)</span> changes as we vary <span class="arithmatex">\(x_i\)</span> alone.  </li>
<li>Collectively, <span class="arithmatex">\(\nabla f(x)\)</span> forms the vector of steepest ascent, pointing toward the direction of maximal increase.  </li>
<li>The magnitude <span class="arithmatex">\(\|\nabla f(x)\|\)</span> measures how sharply <span class="arithmatex">\(f\)</span> rises.</li>
</ul>
<p>From Gradient to Jacobian — Many Inputs, Many Outputs: Now let <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span> be a vector-valued function:</p>
<div class="arithmatex">\[
F(x) =
\begin{bmatrix}
F_1(x) \\[4pt]
F_2(x) \\[4pt]
\vdots \\[4pt]
F_m(x)
\end{bmatrix}.
\]</div>
<p>Each <span class="arithmatex">\(F_i(x)\)</span> is a scalar function with its own gradient <span class="arithmatex">\(\nabla F_i(x)^\top\)</span>.<br>
Stacking these row vectors gives the Jacobian matrix:
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>For <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian is the <span class="arithmatex">\(m \times n\)</span> matrix
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \dots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \dots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>It represents the best linear map approximating <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x) \, h.
</script>
</p>
<p>Just as a tangent line approximates a scalar curve, the Jacobian defines the tangent linear map that approximates <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x) \, h.
</script>
</p>
<ul>
<li>The small displacement <span class="arithmatex">\(h\)</span> in input space is transformed linearly into an output change <span class="arithmatex">\(J_F(x)h\)</span>.  </li>
<li>Thus, <span class="arithmatex">\(J_F(x)\)</span> acts like the <em>microscopic blueprint</em> of <span class="arithmatex">\(F\)</span> around <span class="arithmatex">\(x\)</span>.  </li>
<li>Locally, <span class="arithmatex">\(F\)</span> behaves like a matrix transformation — stretching, rotating, or skewing space.</li>
</ul>
<table>
<thead>
<tr>
<th>Part of <span class="arithmatex">\(J_F(x)\)</span></th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row <span class="arithmatex">\(i\)</span></td>
<td>Gradient of the <span class="arithmatex">\(i\)</span>-th output <span class="arithmatex">\(F_i(x)\)</span> — how that output changes with each input variable.</td>
</tr>
<tr>
<td>Column <span class="arithmatex">\(j\)</span></td>
<td>Sensitivity of all outputs to input <span class="arithmatex">\(x_j\)</span> — how changing <span class="arithmatex">\(x_j\)</span> influences the entire output vector.</td>
</tr>
<tr>
<td>Determinant (if <span class="arithmatex">\(m=n\)</span>)</td>
<td>Local volume scaling — how much <span class="arithmatex">\(F\)</span> expands or compresses space near <span class="arithmatex">\(x\)</span>.</td>
</tr>
<tr>
<td>Rank of <span class="arithmatex">\(J_F(x)\)</span></td>
<td>Local dimensionality of the image of <span class="arithmatex">\(F\)</span> — tells if directions are lost or preserved.</td>
</tr>
</tbody>
</table>
<h2 id="33-the-hessian-and-curvature">3.3 The Hessian and Curvature<a class="headerlink" href="#33-the-hessian-and-curvature" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is twice differentiable, its Hessian is</p>
<div class="arithmatex">\[
\nabla^2 f(x) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}.
\]</div>
<p>The Hessian encodes curvature information:
- <span class="arithmatex">\(\nabla^2 f(x) \succeq 0\)</span> (positive semidefinite) ⟹ <span class="arithmatex">\(f\)</span> is convex near <span class="arithmatex">\(x\)</span>.<br>
- <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> ⟹ <span class="arithmatex">\(f\)</span> is strictly convex.<br>
- Negative eigenvalues ⟹ directions of local decrease.</p>
<p>Example – quadratic function: <span class="arithmatex">\(f(x) = \frac{1}{2}x^TQx - b^T x\)</span>. Here <span class="arithmatex">\(\nabla f(x) = Qx - b\)</span> (linear), and <span class="arithmatex">\(\nabla^2 f(x) = Q\)</span>. Solving <span class="arithmatex">\(\nabla f=0\)</span> yields <span class="arithmatex">\(Qx=b\)</span>, so if <span class="arithmatex">\(Q \succ 0\)</span> the unique minimizer is <span class="arithmatex">\(x^* = Q^{-1}b\)</span>. The Hessian being <span class="arithmatex">\(Q \succ 0\)</span> confirms convexity. If <span class="arithmatex">\(Q\)</span> has large eigenvalues, gradient <span class="arithmatex">\(Qx - b\)</span> changes rapidly in some directions (steep narrow valley); if some eigenvalues are tiny, gradient hardly changes in those directions (flat valley). This aligns with earlier discussions: condition number of <span class="arithmatex">\(Q\)</span> controls difficulty of minimizing <span class="arithmatex">\(f\)</span>.</p>
<blockquote>
<p>Eigenvalues of the Hessian describe curvature along principal directions. Large eigenvalues correspond to steep curvature; small ones correspond to flat regions. Understanding this curvature is essential in designing stable optimization algorithms.</p>
</blockquote>
<h2 id="34-taylor-approximation">3.4 Taylor approximation<a class="headerlink" href="#34-taylor-approximation" title="Permanent link">¶</a></h2>
<p>For differentiable <span class="arithmatex">\(f\)</span>, we have the first-order Taylor expansion around <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
f(x + d) \approx f(x) + \nabla f(x)^\top d~.
</script>
</p>
<p>The gradient gives the best local linear approximation, predicting how <span class="arithmatex">\(f\)</span> changes for a small move <span class="arithmatex">\(d\)</span>. This is the foundation of first-order optimization methods such as gradient descent.</p>
<p>If f is twice differentiable, we have the second-order expansion
<script type="math/tex; mode=display">
f(x + d) \approx f(x)
+ \nabla f(x)^\top d
+ \frac{1}{2} d^\top \nabla^2 f(x) d~.
</script>
</p>
<p>If <span class="arithmatex">\(\nabla^2 f(x)\)</span> is positive semidefinite, the quadratic term is always <span class="arithmatex">\(\ge 0\)</span>. Locally, <span class="arithmatex">\(x\)</span> is in a “bowl”. If <span class="arithmatex">\(\nabla^2 f(x)\)</span> is indefinite, the landscape can curve up in some directions and down in others — typical of saddle points.</p>
<h2 id="35-convexity-and-the-hessian">3.5 Convexity and the Hessian<a class="headerlink" href="#35-convexity-and-the-hessian" title="Permanent link">¶</a></h2>
<p>A twice-differentiable function <span class="arithmatex">\(f\)</span> is convex on a convex set if and only if</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0 \quad \forall x \text{ in the domain.}
\]</div>
<p>The Hessian describes how the gradient changes.</p>
<h2 id="36-first-and-second-order-optimality-conditions">3.6 First and Second-Order Optimality Conditions<a class="headerlink" href="#36-first-and-second-order-optimality-conditions" title="Permanent link">¶</a></h2>
<p>Suppose we want to solve an unconstrained optimization problem:
<script type="math/tex; mode=display">
\min_x f(x).
</script>
</p>
<p>A point <span class="arithmatex">\(x^\star\)</span> is called a critical point if
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0.
</script>
</p>
<h3 id="first-order-necessary-condition">First-Order Necessary Condition<a class="headerlink" href="#first-order-necessary-condition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable and <span class="arithmatex">\(x^\star\)</span> is a local minimizer, then necessarily
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0.
</script>
</p>
<p>Intuitively, this means the slope in every direction must vanish — there is no infinitesimal move that decreases <span class="arithmatex">\(f\)</span> further.</p>
<h3 id="second-order-necessary-condition">Second-Order Necessary Condition<a class="headerlink" href="#second-order-necessary-condition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is twice differentiable and <span class="arithmatex">\(x^\star\)</span> is a local minimizer, then
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0,
\quad
\nabla^2 f(x^\star) \succeq 0,
</script>
</p>
<p>meaning the Hessian is positive semidefinite (PSD). The function curves upward (or flat) in all local directions.</p>
<h3 id="second-order-sufficient-condition">Second-Order Sufficient Condition<a class="headerlink" href="#second-order-sufficient-condition" title="Permanent link">¶</a></h3>
<p>If
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0,
\quad
\nabla^2 f(x^\star) \succ 0,
</script>
i.e. the Hessian is positive definite (PD),<br>
then <span class="arithmatex">\(x^\star\)</span> is a strict local minimizer — the point lies at the bottom of a strictly convex bowl.</p>
<p>The gradient gives the best local linear approximation, predicting how f changes for a small move d.  This is the foundation of first-order optimization methods such as gradient descent. If <span class="arithmatex">\(\nabla f(x^\star)\)</span> has both positive and negative eigenvalues, <span class="arithmatex">\(x^\star\)</span> is a saddle point — neither a minimum nor a maximum.</p>
<p>Convexity makes everything simpler.If <span class="arithmatex">\(f\)</span> is convex, then <em>any</em> point <span class="arithmatex">\(x^\star\)</span> satisfying <span class="arithmatex">\(\nabla f(x^\star) = 0\)</span>  is not only a local minimizer — it is a global minimizer.</p>
<h2 id="37-lipschitz-continuity-and-smoothness">3.7 Lipschitz Continuity and Smoothness<a class="headerlink" href="#37-lipschitz-continuity-and-smoothness" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(f\)</span> has a Lipschitz continuous gradient with constant <span class="arithmatex">\(L &gt; 0\)</span> if
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\|, \quad \forall x,y.
</script>
</p>
<p>Such a function is called L-smooth. This condition bounds how quickly the gradient can change, ensuring the function is not excessively curved.</p>
<p>This property implies the Descent Lemma:
<script type="math/tex; mode=display">
f(y) \le f(x) + \nabla f(x)^\top (y - x) + \tfrac{L}{2} \|y - x\|^2.
</script>
</p>
<p>Smoothness bounds how fast <span class="arithmatex">\(f\)</span> can curve.<br>
In gradient descent, choosing a step size <span class="arithmatex">\(\eta \le 1/L\)</span> guarantees convergence for convex functions.</p>
<blockquote>
<p>In ML training, <span class="arithmatex">\(L\)</span> controls how “aggressive” the learning rate can be — smoother losses allow larger steps.</p>
</blockquote>
<h2 id="38-strong-convexity-functions-with-guaranteed-curvature">3.8 Strong Convexity — Functions with Guaranteed Curvature<a class="headerlink" href="#38-strong-convexity-functions-with-guaranteed-curvature" title="Permanent link">¶</a></h2>
<p>A differentiable function <span class="arithmatex">\(f\)</span> is said to be <span class="arithmatex">\(\mu\)</span>-strongly convex if for some <span class="arithmatex">\(\mu &gt; 0\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + \langle \nabla f(x),\, y - x \rangle + \frac{\mu}{2}\|y - x\|^2,
\quad \forall\, x, y.
</script>
</p>
<p>This inequality means f always lies above its tangent plane by at least a quadratic term of curvature μ.<br>
Strong convexity ensures a minimum curvature: f grows at least as fast as a parabola away from its minimizer.</p>
<h2 id="39-subgradients-and-nonsmooth-extensions">3.9 Subgradients and Nonsmooth Extensions<a class="headerlink" href="#39-subgradients-and-nonsmooth-extensions" title="Permanent link">¶</a></h2>
<p>Many useful convex functions are nonsmooth — e.g., hinge loss, <span class="arithmatex">\(\ell_1\)</span> norm, ReLU.<br>
They lack a gradient at certain points but admit a subgradient.</p>
<p>A vector <span class="arithmatex">\(g\)</span> is a subgradient of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> if
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y - x), \quad \forall y.
</script>
</p>
<p>The set of all such vectors is the subdifferential <span class="arithmatex">\(\partial f(x)\)</span>.  </p>
<p>Examples:
- <span class="arithmatex">\(f(x) = \|x\|_1\)</span> has
  <script type="math/tex; mode=display">
  (\partial f(x))_i =
  \begin{cases}
  \operatorname{sign}(x_i), & x_i \ne 0, \\
  [-1, 1], & x_i = 0.
  \end{cases}
  </script>
- For <span class="arithmatex">\(f(x) = \max_i x_i\)</span>, any unit vector supported on the active index is a subgradient.</p>
<p>Subgradients generalize the gradient concept, allowing optimization even when derivatives do not exist.<br>
They are the backbone of nonsmooth convex optimization and proximal methods.  In machine learning, they make it possible to minimize losses such as the hinge or absolute deviation, where gradients are undefined at corners.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.top", "toc.integrate", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tabs.link", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>