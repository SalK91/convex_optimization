
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on optimization, convex analysis, and algorithms.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/convex_optimization/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Print Site - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("/convex_optimization/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#index" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Mathematics for Machine Learning" class="md-header__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Mathematics for Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Site
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo" aria-label="Mathematics for Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/convex_optimization" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convex Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Convex Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/11_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/12_vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Linear Algebra Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/13_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Multivariable Calculus for Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/14_convexsets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Convex Sets and Geometric Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/15_convexfunctions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convex Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16_subgradients/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Nonsmooth Convex Optimization – Subgradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/16a_optimality_conditions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. First-Order Optimality Conditions in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/17_kkt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Optimization Principles – From Gradient Descent to KKT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18_duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Lagrange Duality Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18a_pareto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Pareto Optimality and Multi-Objective Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/18b_regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Regularized Approximation – Balancing Fit and Complexity
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19_optimizationalgo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Algorithms for Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19a_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Optimization Algorithms for Equality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/19b_optimization_constraints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Optimization Algorithms for Inequality-Constrained Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/20_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Advanced Large-Scale and Structured Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/21_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Modelling Patterns and Algorithm Selection in Practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/30_canonical_problems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Canonical Problems in Convex Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/35_modern/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    18. Modern Optimizers in Machine Learning Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/40_nonconvex/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    19. Beyond Convexity – Nonconvex and Global Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/42_derivativefree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    20. Derivative-Free and Black-Box Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/44_metaheuristic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    21. Metaheuristic and Evolutionary Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/48_advanced_combinatorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    22. Advanced Topics in Combinatorial Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convex/50_future/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    23. The Future of Optimization — Learning, Adaptation, and Intelligence
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Cheat Sheets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Cheat Sheets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheatsheets/20a_cheatsheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimization Algos - Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Appendices
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Appendices
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/120_ineqaulities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix A - Common Inequalities and Identities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/130_projections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix B - Projection and Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/140_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix C - Support Functions and Dual Geometry (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/160_conjugates/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix D - Convex Conjugates and Fenchel Duality (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/170_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix E - Convexity in Probability and Statistics (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/180_subgradient_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix F - Subgradient Method and Variants (Advanced)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/190_proximal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix G - Proximal Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/200_mirror/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix H - Mirror Descent and Bregman Geometry
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendices/300_matrixfactorization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Appendix I - Matrix Factorization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Convex Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Convex Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convex-11_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction and Overview
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-12_vector" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Linear Algebra Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-13_calculus" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Multivariable Calculus for Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-14_convexsets" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Convex Sets and Geometric Fundamentals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-15_convexfunctions" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Convex Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16_subgradients" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Nonsmooth Convex Optimization – Subgradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-16a_optimality_conditions" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. First-Order Optimality Conditions in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-17_kkt" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Optimization Principles – From Gradient Descent to KKT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18_duality" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Lagrange Duality Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18a_pareto" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Pareto Optimality and Multi-Objective Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-18b_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Regularized Approximation – Balancing Fit and Complexity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19_optimizationalgo" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Algorithms for Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19a_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Optimization Algorithms for Equality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-19b_optimization_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Optimization Algorithms for Inequality-Constrained Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-20_advanced" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Advanced Large-Scale and Structured Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-21_models" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Modelling Patterns and Algorithm Selection in Practice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-30_canonical_problems" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Canonical Problems in Convex Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-35_modern" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 18. Modern Optimizers in Machine Learning Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-40_nonconvex" class="md-nav__link">
    <span class="md-ellipsis">
      2.19 19. Beyond Convexity – Nonconvex and Global Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-42_derivativefree" class="md-nav__link">
    <span class="md-ellipsis">
      2.20 20. Derivative-Free and Black-Box Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-44_metaheuristic" class="md-nav__link">
    <span class="md-ellipsis">
      2.21 21. Metaheuristic and Evolutionary Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-48_advanced_combinatorial" class="md-nav__link">
    <span class="md-ellipsis">
      2.22 22. Advanced Topics in Combinatorial Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convex-50_future" class="md-nav__link">
    <span class="md-ellipsis">
      2.23 23. The Future of Optimization — Learning, Adaptation, and Intelligence
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-3" class="md-nav__link">
    <span class="md-ellipsis">
      3 Cheat Sheets
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 Cheat Sheets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cheatsheets-20a_cheatsheet" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Optimization Algos - Cheat Sheet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-4" class="md-nav__link">
    <span class="md-ellipsis">
      4 Appendices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Appendices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendices-120_ineqaulities" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Appendix A - Common Inequalities and Identities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-130_projections" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Appendix B - Projection and Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-140_support" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Appendix C - Support Functions and Dual Geometry (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-160_conjugates" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Appendix D - Convex Conjugates and Fenchel Duality (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-170_probability" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 Appendix E - Convexity in Probability and Statistics (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-180_subgradient_methods" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 Appendix F - Subgradient Method and Variants (Advanced)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-190_proximal" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 Appendix G - Proximal Operators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-200_mirror" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 Appendix H - Mirror Descent and Bregman Geometry
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#appendices-300_matrixfactorization" class="md-nav__link">
    <span class="md-ellipsis">
      4.9 Appendix I - Matrix Factorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index" heading-number="1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="mathematics-for-machine-learning">Mathematics for Machine Learning<a class="headerlink" href="#index-mathematics-for-machine-learning" title="Permanent link">¶</a></h1>
<p>Welcome to <em>Mathematics for Machine Learning</em>, a structured set of lecture notes designed to build the mathematical foundation needed to understand and develop modern machine learning and optimization algorithms.</p>
<p>This digital book provides a unified, intuition-driven exploration of key mathematical tools — from linear algebra and calculus to convex analysis, optimization, and algorithms used in deep learning and natural language processing (NLP).</p>
<h2 id="index-motivation">Motivation<a class="headerlink" href="#index-motivation" title="Permanent link">¶</a></h2>
<p>Machine Learning, Optimization, and AI systems all rest upon a shared mathematical backbone. This resource aims to bridge the gap between abstract theory and practical application by offering:</p>
<ul>
<li>Concise derivations of essential results</li>
<li>Geometric intuition and figures where helpful</li>
<li>Connections to real-world algorithms (gradient descent, regularization, duality, etc.)</li>
<li>Appendices that extend into more advanced or specialized topics</li>
</ul>
<p>Whether you’re a student, researcher, or practitioner, this webbook provides both a reference and a learning guide.</p></body></html></section>
                    <section class='print-page md-section' id='section-2' heading-number='2'>
                        <h1>Convex Optimization<a class='headerlink' href='#section-2' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="convex-11_intro" heading-number="2.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-and-overview">Chapter 1:  Introduction and Overview<a class="headerlink" href="#convex-11_intro-chapter-1-introduction-and-overview" title="Permanent link">¶</a></h1>
<p>Optimization is at the heart of most machine-learning methods. Whether training a linear model or a deep neural network, learning usually means adjusting parameters to minimize a loss that measures how well the model fits the data. Convex optimization is a particularly important and well-understood part of optimization. When both the objective and the constraints are convex, the problem has helpful properties:</p>
<ol>
<li>No bad local minima: any local minimum is also the global minimum.  </li>
<li>Predictable behavior: algorithms like gradient descent have clear and well-studied convergence.  </li>
<li>Solutions are easy to verify: convex problems come with simple mathematical conditions that tell us when we have reached the optimum.</li>
</ol>
<p>These features make convex optimization a reliable tool for building and analyzing machine-learning models. Even though many modern models are nonconvex, a surprising amount of ML still depends on convex ideas. Common loss functions, regularizers, and inner algorithmic steps often rely on convex structure.</p>
<p>This web-book is written for practitioners who have basic familiarity with optimization, especially gradient-based methods, and want to understand how convex optimization principles help guide reliable machine-learning practice.</p>
<h2 id="convex-11_intro-11-motivation-optimization-in-machine-learning">1.1 Motivation: Optimization in Machine Learning<a class="headerlink" href="#convex-11_intro-11-motivation-optimization-in-machine-learning" title="Permanent link">¶</a></h2>
<p>Many supervised learning problems can be written in a common form:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} 
\; \frac{1}{N}\sum_{i=1}^{N} \ell(a_i^\top x, b_i) 
+ \lambda R(x),
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(\ell(\cdot,\cdot)\)</span> is a loss function that measures how well the model predicts <span class="arithmatex">\(b_i\)</span> from <span class="arithmatex">\(a_i\)</span>,  </li>
<li><span class="arithmatex">\(R(x)\)</span> is a regularizer that encourages certain structure (such as sparsity or small weights),  </li>
<li><span class="arithmatex">\(\mathcal{X}\)</span> is a set of allowed parameter values, often simple and convex.</li>
</ul>
<p>Many widely used losses and regularizers are convex. Examples include least squares, logistic loss, hinge loss, Huber loss, the <span class="arithmatex">\(\ell_1\)</span> norm, and the <span class="arithmatex">\(\ell_2\)</span> norm. Convexity is what makes these problems tractable and allows them to be solved efficiently at scale using well-behaved optimization algorithms.</p>
<h2 id="convex-11_intro-12-convex-sets-and-convex-functions-first-intuition">1.2 Convex Sets and Convex Functions — First Intuition<a class="headerlink" href="#convex-11_intro-12-convex-sets-and-convex-functions-first-intuition" title="Permanent link">¶</a></h2>
<p>A set <span class="arithmatex">\(\mathcal{C}\)</span> is convex if, whenever you pick two points in the set, the line segment between them stays entirely inside the set:</p>
<div class="arithmatex">\[
\theta x + (1-\theta)y \in \mathcal{C} 
\quad \text{for all } x,y \in \mathcal{C},\; \theta \in [0,1].
\]</div>
<p>Convex functions follow a similar idea. A function <span class="arithmatex">\(f\)</span> is convex if its graph never dips below the straight line connecting two points on the function:</p>
<div class="arithmatex">\[
f(\theta x + (1-\theta)y)
\le
\theta f(x) + (1-\theta) f(y).
\]</div>
<p>Intuitively, convex functions look like bowls: they curve upward and have at most one global minimum. Affine functions are both convex and concave, and quadratics with positive semidefinite Hessians are convex. Many ML loss functions share this shape, which makes them easy to optimize.</p>
<h2 id="convex-11_intro-13-why-convex-optimization-remains-central-in-ml">1.3 Why Convex Optimization Remains Central in ML<a class="headerlink" href="#convex-11_intro-13-why-convex-optimization-remains-central-in-ml" title="Permanent link">¶</a></h2>
<p>Although many modern models are nonconvex, convex optimization continues to play a major role in three ways:</p>
<ol>
<li>
<p>Convex surrogate losses: Losses such as logistic, hinge, and Huber are convex substitutes for harder objectives like the <span class="arithmatex">\(0\text{–}1\)</span> loss. They make optimization practical while still leading to models that generalize well.</p>
</li>
<li>
<p>Convex subproblems inside larger algorithms:  Many nonconvex methods solve convex problems as part of their inner loop. Examples include least-squares steps in matrix factorization, proximal updates in regularized learning, and simple convex problems that appear in line-search procedures.</p>
</li>
<li>
<p>Implicit bias in linear models:  In overparameterized linear least-squares problems, gradient descent starting from zero converges to the minimum-norm solution. This phenomenon helps explain generalization and implicit regularization in linear and kernel models.</p>
</li>
</ol>
<p>These roles make convex optimization a key component of modern ML toolkits, even when the main model is nonconvex.</p>
<h2 id="convex-11_intro-14-from-global-optima-to-algorithms">1.4 From Global Optima to Algorithms<a class="headerlink" href="#convex-11_intro-14-from-global-optima-to-algorithms" title="Permanent link">¶</a></h2>
<p>A major advantage of convex optimization is that it eliminates the possibility of non-global local minima. For a differentiable convex function on an open domain:</p>
<div class="arithmatex">\[
\nabla f(x^*) = 0 
\quad \Rightarrow \quad
x^* \text{ is a global minimizer}.
\]</div>
<p>This means that simply finding a point where the gradient is zero is enough. For constrained or nondifferentiable problems, optimality is checked using subgradients or KKT conditions:</p>
<div class="arithmatex">\[
0 \in \partial f(x^*) + N_{\mathcal{X}}(x^*),
\]</div>
<p>where <span class="arithmatex">\(N_{\mathcal{X}}(x^*)\)</span> represents the outward directions that are blocked by the constraint set. These conditions are useful because many iterative algorithms aim to drive the gradient or subgradient toward zero.</p>
<h2 id="convex-11_intro-15-canonical-convex-ml-problems-at-a-glance">1.5 Canonical Convex ML Problems at a Glance<a class="headerlink" href="#convex-11_intro-15-canonical-convex-ml-problems-at-a-glance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Objective</th>
<th>Typical Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td>Least squares</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Gradient descent, conjugate gradient</td>
</tr>
<tr>
<td>Ridge regression</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_2^2\)</span></td>
<td>Closed form, gradient methods</td>
</tr>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td>Proximal gradient (ISTA/FISTA)</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\sum_i \log(1+\exp(-y_i a_i^\top x)) + \lambda\|x\|_2^2\)</span></td>
<td>Newton, quasi-Newton, SGD</td>
</tr>
<tr>
<td>SVM (hinge loss)</td>
<td><span class="arithmatex">\(\tfrac{1}{2}\|x\|^2 + C\sum_i \max(0,1-y_i a_i^\top x)\)</span></td>
<td>Subgradient, coordinate methods, SMO</td>
</tr>
<tr>
<td>Robust regression</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Linear programming</td>
</tr>
<tr>
<td>Elastic Net</td>
<td><span class="arithmatex">\(\|A x-b\|_2^2 + \lambda_1\|x\|_1 + \lambda_2\|x\|_2^2\)</span></td>
<td>Coordinate descent</td>
</tr>
</tbody>
</table>
<p>These problems illustrate how convex models appear throughout ML.</p>
<h2 id="convex-11_intro-16-web-book-roadmap-and-how-to-use-it">1.6 Web-Book Roadmap and How to Use It<a class="headerlink" href="#convex-11_intro-16-web-book-roadmap-and-how-to-use-it" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Question</th>
<th>Where to Look</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>What makes a function or set convex?</td>
<td>Chapters 2–5</td>
<td>Geometry and basic properties of convexity</td>
</tr>
<tr>
<td>How do gradients, subgradients, and KKT conditions define optimality?</td>
<td>Chapters 6–9</td>
<td>Optimality conditions and duality</td>
</tr>
<tr>
<td>How are convex problems solved in practice?</td>
<td>Chapters 10–14</td>
<td>First-order, second-order, and interior-point methods</td>
</tr>
<tr>
<td>How to choose an algorithm for a given optimization problem?</td>
<td>Chapters 15–17</td>
<td>Large-scale and structured optimization techniques</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-12_vector" heading-number="2.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-linear-algebra-foundations">Chapter 2: Linear Algebra Foundations<a class="headerlink" href="#convex-12_vector-chapter-2-linear-algebra-foundations" title="Permanent link">¶</a></h1>
<p>Linear algebra provides the geometric language of convex optimization. Many optimization problems in machine learning can be understood as asking how vectors, subspaces, and linear maps relate to one another. A simple example that shows this connection is linear least squares, where fitting a model <span class="arithmatex">\(x\)</span> to data <span class="arithmatex">\((A, b)\)</span> takes the form:</p>
<div class="arithmatex">\[
\min_x \ \|A x - b\|_2^2.
\]</div>
<p>Later in this chapter, we will see that this objective finds the point in the column space of <span class="arithmatex">\(A\)</span> that is closest to <span class="arithmatex">\(b\)</span>. Concepts such as column space, null space, orthogonality, rank, and conditioning determine not only whether a solution exists, but also how fast optimization algorithms converge.</p>
<p>This chapter develops the linear-algebra tools that appear throughout convex optimization and machine learning. We focus on geometric ideas — projections, subspaces, orthogonality, eigenvalues, singular values, and norms — because these ideas directly shape how optimization behaves. Readers familiar with basic matrix operations will find that many optimization concepts become much simpler when viewed through the right geometric lens.</p>
<h2 id="convex-12_vector-21-vector-spaces-subspaces-and-affine-sets">2.1 Vector spaces, subspaces, and affine sets<a class="headerlink" href="#convex-12_vector-21-vector-spaces-subspaces-and-affine-sets" title="Permanent link">¶</a></h2>
<p>A vector space over <span class="arithmatex">\(\mathbb{R}\)</span> is a set of vectors that can be added and scaled without leaving the set. The familiar example is <span class="arithmatex">\(\mathbb{R}^n\)</span>, where operations like <span class="arithmatex">\(\alpha x + \beta y\)</span> keep us within the same space.</p>
<p>Within a vector space, some subsets behave particularly nicely. A subspace is a subset that is itself a vector space: it is closed under addition, closed under scalar multiplication, and contains the zero vector. Geometrically, subspaces are “flat” objects that always pass through the origin, such as lines or planes in <span class="arithmatex">\(\mathbb{R}^3\)</span>. </p>
<p>Affine sets extend this idea by allowing a shift away from the origin. A set <span class="arithmatex">\(A\)</span> is affine if it contains the entire line passing through any two of its points. Equivalently, for any <span class="arithmatex">\(x,y \in A\)</span> and any <span class="arithmatex">\(\theta \in \mathbb{R}\)</span>,  <span class="arithmatex">\(\theta x + (1 - \theta) y \in A.\)</span> That is, the entire line passing through any two points in <span class="arithmatex">\(A\)</span> lies within <span class="arithmatex">\(A\)</span>. By contrast, a convex set only requires this property for <span class="arithmatex">\(\theta \in [0,1]\)</span>, meaning only the line segment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> must lie within the set. </p>
<p>Affine sets look like translated subspaces: lines or planes that do not need to pass through the origin. Every affine set can be written as: <span class="arithmatex">\(A = x_0 + S = \{\, x_0 + s : s \in S \,\},\)</span> where <span class="arithmatex">\(S\)</span> is a subspace and <span class="arithmatex">\(x_0\)</span> is any point in the set. This representation is extremely useful in optimization. If <span class="arithmatex">\(Ax = b\)</span> is a linear constraint, then its solution set is an affine set. A single particular solution <span class="arithmatex">\(x_0\)</span> gives one point satisfying the constraint, and the entire solution set is obtained by adding the null space of <span class="arithmatex">\(A\)</span>. Thus, optimization under linear constraints means searching over an affine set determined by the constraint structure.</p>
<p>Finally, affine transformations play a central role in both machine learning and optimization. A mapping of the form</p>
<p>Affine Transformations: An affine transformation (or affine map) is a function <span class="arithmatex">\(f : V \to W\)</span> that can be written as <span class="arithmatex">\(f(x) = A x + b,\)</span> where <span class="arithmatex">\(A\)</span> is a linear map and <span class="arithmatex">\(b\)</span> is a fixed vector. Affine transformations preserve both affinity and convexity:
if <span class="arithmatex">\(C\)</span> is convex, then <span class="arithmatex">\(A C + b\)</span> is also convex.
is called an affine transformation. It represents a linear transformation followed by a translation. Affine transformations preserve the structure of affine sets and convex sets, meaning that if a feasible region is convex or affine, applying an affine transformation does not destroy that property. This matters for optimization because many models and algorithms implicitly perform affine transformations for example, when reparameterizing variables, scaling features, or mapping between coordinate systems. Convexity is preserved under these operations, so the essential geometry of the problem remains intact.</p>
<p>In summary, vector spaces describe the ambient space in which optimization algorithms move, subspaces capture structural or constraint-related directions, and affine sets model the geometric shapes defined by linear constraints. These three ideas form the basic geometric toolkit for understanding optimization problems and will reappear repeatedly throughout the rest of the book.</p>
<h2 id="convex-12_vector-22-linear-combinations-span-basis-dimension">2.2 Linear combinations, span, basis, dimension<a class="headerlink" href="#convex-12_vector-22-linear-combinations-span-basis-dimension" title="Permanent link">¶</a></h2>
<p>Much of linear algebra revolves around understanding how vectors can be combined to generate new vectors. This idea is essential in optimization because gradients, search directions, feasible directions, and model predictions are often built from linear combinations of simpler components.</p>
<p>Given vectors <span class="arithmatex">\(v_1,\dots,v_k\)</span>, any vector of the form<script type="math/tex">
\alpha_1 v_1 + \cdots + \alpha_k v_k</script> is a linear combination. The set of all linear combinations is called the span:
<script type="math/tex; mode=display">
\mathrm{span}\{v_1,\dots,v_k\} = \left\{ \sum_{i=1}^k \alpha_i v_i : \alpha_i \in \mathbb{R} \right\}.
</script>
The span describes the collection of directions that can be reached from these vectors and therefore determines what portion of the ambient space they can represent. </p>
<p>The concept of linear independence formalizes when a set of vectors contains no redundancy. A set of vectors is linearly independent if none of them can be written as a linear combination of the others. If a set is linearly dependent, at least one vector adds no new direction. </p>
<p>A basis of a space <span class="arithmatex">\(V\)</span> is a linearly independent set whose span equals <span class="arithmatex">\(V\)</span>. The number of basis vectors is the dimension <span class="arithmatex">\(\dim(V)\)</span>.</p>
<p>Rank and nullity facts:</p>
<ul>
<li>The column space of <span class="arithmatex">\(A\)</span> is the span of its columns. Its dimension is <span class="arithmatex">\(\mathrm{rank}(A)\)</span>.</li>
<li>The nullspace of <span class="arithmatex">\(A\)</span> is <span class="arithmatex">\(\{ x : Ax = 0 \}\)</span>.</li>
<li>The rank-nullity theorem states: <span class="arithmatex">\(\mathrm{rank}(A) + \mathrm{nullity}(A) = n,\)</span> where <span class="arithmatex">\(n\)</span> is the number of columns of <span class="arithmatex">\(A\)</span>.</li>
</ul>
<blockquote>
<p>Column Space: The column space of a matrix <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( C(A) \)</span>, is the set of all possible output vectors <span class="arithmatex">\( b \)</span> that can be written as <span class="arithmatex">\( Ax \)</span> for some <span class="arithmatex">\( x \)</span>. In other words, it contains all vectors that the matrix can “reach” through linear combinations of its columns. The question “Does the system <span class="arithmatex">\( Ax = b \)</span> have a solution?” is equivalent to asking whether <span class="arithmatex">\( b \in C(A) \)</span>. If <span class="arithmatex">\( b \)</span> lies in the column space, a solution exists; otherwise, it does not.</p>
<p>Null Space: The null space (or kernel) of <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( N(A) \)</span>, is the set of all input vectors <span class="arithmatex">\( x \)</span> that are mapped to zero:  <span class="arithmatex">\( N(A) = \{ x : Ax = 0 \} \)</span>. It answers a different question: <em>If a solution to <span class="arithmatex">\( Ax = b \)</span> exists, is it unique?</em> If the null space contains only the zero vector (<span class="arithmatex">\( \mathrm{nullity}(A) = 0 \)</span>), the solution is unique. But if <span class="arithmatex">\( N(A) \)</span> contains nonzero vectors, there are infinitely many distinct solutions that yield the same output.</p>
<p>Multicollinearity: When one feature in the data matrix <span class="arithmatex">\( A \)</span> is a linear combination of others for example, <span class="arithmatex">\( \text{feature}_3 = 2 \times \text{feature}_1 + \text{feature}_2 \)</span>—the columns of <span class="arithmatex">\( A \)</span> become linearly dependent. This creates a nonzero vector in the null space of <span class="arithmatex">\( A \)</span>, meaning multiple weight vectors <span class="arithmatex">\( x \)</span> can produce the same predictions. The model is then <em>unidentifiable</em> (Underdetermined – the number of unknowns (parameters) exceeds the number of independent equations (information)), and <span class="arithmatex">\( A^\top A \)</span> becomes singular (non-invertible). Regularization methods such as Ridge or Lasso regression are used to resolve this ambiguity by selecting one stable, well-behaved solution.</p>
<blockquote>
<p>Regularization introduces an additional constraint or penalty that selects a <em>single, stable</em> solution from among the infinite possibilities.</p>
<ul>
<li>
<p>Ridge regression (L2 regularization) adds a penalty on the norm of <span class="arithmatex">\(x\)</span>:
  <script type="math/tex; mode=display">
  \min_x \|A x - b\|_2^2 + \lambda \|x\|_2^2,
  </script>
  which modifies the normal equations to
  <script type="math/tex; mode=display">
  (A^\top A + \lambda I)x = A^\top b.
  </script>
  The added term <span class="arithmatex">\(\lambda I\)</span> ensures invertibility and numerical stability.</p>
</li>
<li>
<p>Lasso regression (L1 regularization) instead penalizes <span class="arithmatex">\(\|x\|_1\)</span>, promoting sparsity by driving some coefficients exactly to zero.</p>
</li>
</ul>
<p>Thus, regularization resolves ambiguity by imposing structure or preference on the solution favoring smaller or sparser coefficient vectors—and making the regression problem well-posed even when <span class="arithmatex">\(A\)</span> is rank-deficient.</p>
</blockquote>
<p>Feasible Directions: In a constrained optimization problem of the form <span class="arithmatex">\( Ax = b \)</span>, the null space of <span class="arithmatex">\( A \)</span> characterizes the directions along which one can move without violating the constraints. If <span class="arithmatex">\( d \in N(A) \)</span>, then moving from a feasible point <span class="arithmatex">\( x \)</span> to <span class="arithmatex">\( x + d \)</span> preserves feasibility, since  <span class="arithmatex">\( A(x + d) = Ax + Ad = b \)</span>. Thus, the null space defines the <em>space of free movement</em> directions in which optimization algorithms can explore solutions while remaining within the constraint surface.</p>
<p>Row Space: The row space of <span class="arithmatex">\( A \)</span>, denoted <span class="arithmatex">\( R(A) \)</span>, is the span of the rows of <span class="arithmatex">\( A \)</span> (viewed as vectors). It represents all possible linear combinations of the rows and has the same dimension as the column space, equal to <span class="arithmatex">\( \mathrm{rank}(A) \)</span>. The row space is orthogonal to the null space of <span class="arithmatex">\( A \)</span>:  <span class="arithmatex">\( R(A) \perp N(A) \)</span>.  In optimization, the row space corresponds to the set of active constraints or the directions along which changes in <span class="arithmatex">\( x \)</span> affect the constraints.</p>
<p>Left Null Space: The left null space, denoted <span class="arithmatex">\( N(A^\top) \)</span>, is the set of all vectors <span class="arithmatex">\( y \)</span> such that <span class="arithmatex">\( A^\top y = 0 \)</span>. These vectors are orthogonal to the columns of <span class="arithmatex">\( A \)</span>, and therefore orthogonal to the column space itself. In least squares problems, <span class="arithmatex">\( N(A^\top) \)</span> represents residual directions—components of <span class="arithmatex">\( b \)</span> that cannot be explained by the model <span class="arithmatex">\( Ax = b \)</span>.</p>
<p>Projection Interpretation (Least Squares):  When <span class="arithmatex">\( Ax = b \)</span> has no exact solution (as in overdetermined systems), the least squares solution finds <span class="arithmatex">\( x \)</span> such that <span class="arithmatex">\( Ax \)</span> is the projection of <span class="arithmatex">\( b \)</span> onto the column space of <span class="arithmatex">\( A \)</span>:<br>
<script type="math/tex; mode=display">
x^* = (A^\top A)^{-1} A^\top b,
</script>
and the residual<br>
<script type="math/tex; mode=display">
r = b - Ax^*
</script>
lies in the left null space <span class="arithmatex">\( N(A^\top) \)</span>.<br>
This provides a geometric view: the solution projects <span class="arithmatex">\( b \)</span> onto the closest point in the subspace that <span class="arithmatex">\( A \)</span> can reach.</p>
<p>Rank–Nullity Relationship: The rank of <span class="arithmatex">\( A \)</span> is the dimension of both its column and row spaces, and the nullity is the dimension of its null space. Together they satisfy the Rank–Nullity Theorem:
<script type="math/tex; mode=display">
\mathrm{rank}(A) + \mathrm{nullity}(A) = n,
</script>
where <span class="arithmatex">\( n \)</span> is the number of columns of <span class="arithmatex">\( A \)</span>. This theorem reflects the balance between the number of independent constraints and the number of degrees of freedom in <span class="arithmatex">\( x \)</span>.</p>
<p>Geometric Interpretation:  </p>
<ul>
<li>The column space represents all <em>reachable outputs</em>.  </li>
<li>The null space represents all <em>indistinguishable inputs</em> that map to zero.  </li>
<li>The row space represents all <em>independent constraints</em> imposed by <span class="arithmatex">\( A \)</span>.  </li>
<li>The left null space captures <em>inconsistencies</em> or residual directions that cannot be explained by the model.  </li>
</ul>
<p>Together, these four subspaces define the complete geometry of the linear map <span class="arithmatex">\( A: \mathbb{R}^n \to \mathbb{R}^m \)</span>.</p>
</blockquote>
<h2 id="convex-12_vector-23-inner-products-and-orthogonality">2.3 Inner products and orthogonality<a class="headerlink" href="#convex-12_vector-23-inner-products-and-orthogonality" title="Permanent link">¶</a></h2>
<p>Inner products provide the geometric structure that underlies most optimization algorithms. They allow us to define lengths, angles, projections, gradients, and orthogonality—concepts that appear repeatedly in convex optimization and machine learning.</p>
<p>An inner product on <span class="arithmatex">\(\mathbb{R}^n\)</span> is a map <span class="arithmatex">\(\langle \cdot,\cdot\rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\)</span> such that for all <span class="arithmatex">\(x,y,z\)</span> and all scalars <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\langle x,y \rangle = \langle y,x\rangle\)</span> (symmetry),</li>
<li><span class="arithmatex">\(\langle x+y,z \rangle = \langle x,z \rangle + \langle y,z\rangle\)</span> (linearity in first argument),</li>
<li><span class="arithmatex">\(\langle \alpha x, y\rangle = \alpha \langle x, y\rangle\)</span>,</li>
<li><span class="arithmatex">\(\langle x, x\rangle \ge 0\)</span> with equality iff <span class="arithmatex">\(x=0\)</span> (positive definiteness).</li>
</ol>
<p>The inner product induces:</p>
<ul>
<li>length (norm): <span class="arithmatex">\(\|x\|_2 = \sqrt{\langle x,x\rangle}\)</span>,</li>
<li>angle: <script type="math/tex">
\cos \theta = \frac{\langle x,y\rangle}{\|x\|\|y\|}~.
</script>
</li>
</ul>
<p>Two vectors are orthogonal if <span class="arithmatex">\(\langle x,y\rangle = 0\)</span>. A set of vectors <span class="arithmatex">\(\{v_i\}\)</span> is orthonormal if each <span class="arithmatex">\(\|v_i\| = 1\)</span> and <span class="arithmatex">\(\langle v_i, v_j\rangle = 0\)</span> for <span class="arithmatex">\(i\ne j\)</span>.</p>
<blockquote>
<p>More generally, an inner product endows <span class="arithmatex">\(V\)</span> with a geometric structure, turning it into an inner product space (and if complete, a Hilbert space). Inner products allow us to talk about orthogonality (perpendicular vectors) and orthogonal projections, and to define the all-important concept of a gradient in optimization. </p>
<p>Geometry from the inner product: An inner product induces a norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x \rangle}\)</span> and a notion of distance <span class="arithmatex">\(d(x,y) = \|x-y\|\)</span>. It also defines angles: <span class="arithmatex">\(\langle x,y \rangle = 0\)</span> means <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are orthogonal. Thus, inner products generalize the geometric concepts of lengths and angles to abstract vector spaces. Many results in Euclidean geometry (like the Pythagorean theorem and law of cosines) hold in any inner product space. For example, the parallelogram law holds: <span class="arithmatex">\(\|x+y\|^2 + \|x-y\|^2 = 2\|x\|^2 + 2\|y\|^2\)</span>.  </p>
</blockquote>
<p>The Cauchy–Schwarz inequality: For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>:
<script type="math/tex; mode=display">
|\langle x,y\rangle| \le \|x\|\|y\|~,
</script>
with equality iff <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent Geometrically, it means the absolute inner product is maximized when <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> point in the same or opposite direction. </p>
<p>Examples of inner products:</p>
<ul>
<li>
<p>Standard (Euclidean) inner product: <span class="arithmatex">\(\langle x,y\rangle = x^\top y = \sum_i x_i y_i\)</span>. This underlies most optimization algorithms on <span class="arithmatex">\(\mathbb{R}^n\)</span>, where <span class="arithmatex">\(\nabla f(x)\)</span> is defined via this inner product (so that <span class="arithmatex">\(\langle \nabla f(x), h\rangle\)</span> gives the directional derivative in direction <span class="arithmatex">\(h\)</span>).  </p>
</li>
<li>
<p>Weighted inner product: <span class="arithmatex">\(\langle x,y\rangle_W = x^\top W y\)</span> for some symmetric positive-definite matrix <span class="arithmatex">\(W\)</span>. Here <span class="arithmatex">\(\|x\|_W = \sqrt{x^\top W x}\)</span> is a weighted length. Such inner products appear in preconditioning: by choosing <span class="arithmatex">\(W\)</span> cleverly, one can measure distances in a way that accounts for scaling in the problem (e.g. the Mahalanobis distance uses <span class="arithmatex">\(W = \Sigma^{-1}\)</span> for covariance <span class="arithmatex">\(\Sigma\)</span>).  </p>
</li>
<li>
<p>Function space inner product: <span class="arithmatex">\(\langle f, g \rangle = \int_a^b f(t)\,g(t)\,dt\)</span>. This turns the space of square-integrable functions on <span class="arithmatex">\([a,b]\)</span> into an inner product space (a Hilbert space, <span class="arithmatex">\(L^2[a,b]\)</span>). In machine learning, this is the basis for kernel Hilbert spaces, where one defines an inner product between functions to lift optimization into infinite-dimensional feature spaces.  </p>
</li>
</ul>
<blockquote>
<p>Any vector space with an inner product has an orthonormal basis (via the Gram–Schmidt process). Gram–Schmidt is fundamental in numerical algorithms to orthogonalize vectors and is used to derive the QR decomposition: any full-rank matrix <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span> can be factored as <span class="arithmatex">\(A = QR\)</span> where <span class="arithmatex">\(Q\)</span> has orthonormal columns and <span class="arithmatex">\(R\)</span> is upper triangular. This factorization is widely used in least squares and optimization because it provides a stable way to solve <span class="arithmatex">\(Ax=b\)</span> and to analyze subspaces. For example, for an overdetermined system (<span class="arithmatex">\(m&gt;n\)</span> i.e. more equations than unknowns), <span class="arithmatex">\(Ax=b\)</span> has a least-squares solution <span class="arithmatex">\(x = R^{-1}(Q^\top b)\)</span>, and for underdetermined (<span class="arithmatex">\(m&lt;n\)</span>), <span class="arithmatex">\(Ax=b\)</span> has infinitely many solutions, among which one often chooses the minimal-norm solution using the orthonormal basis of the range. </p>
</blockquote>
<p>Applications in optimization: Inner product geometry is indispensable in convex optimization.  </p>
<ul>
<li>
<p>Gradients: The gradient <span class="arithmatex">\(\nabla f(x)\)</span> is defined as the vector satisfying <span class="arithmatex">\(f(x+h)\approx f(x) + \langle \nabla f(x), h\rangle\)</span>. Thus the inner product induces the notion of steepest ascent/descent direction (steepest descent is in direction <span class="arithmatex">\(-\nabla f(x)\)</span> because it minimizes the inner product with the gradient). If we changed the inner product (using a matrix <span class="arithmatex">\(W\)</span>), the notion of gradient would change accordingly (this idea is used in natural gradient methods).  </p>
</li>
<li>
<p>Orthogonal projections: Many algorithms require projecting onto a constraint set. For linear constraints <span class="arithmatex">\(Ax=b\)</span> (an affine set), the projection formula uses the inner product to find the closest point in the affine set. Projections also underpin least squares problems (solution is projection of <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(\mathrm{range}(A)\)</span>) and quadratic programs (where each iteration might involve a projection).  </p>
</li>
<li>
<p>Orthonormal representations: Orthonormal bases (like principal components) simplify optimization by diagonalizing quadratic forms or separating variables. For instance, in PCA we use an orthonormal basis (eigenvectors) to reduce dimensionality. In iterative algorithms, working in an orthonormal basis aligned with the problem (e.g. preconditioning) can accelerate convergence.  </p>
</li>
<li>
<p>Conditioning and Gram matrix: The inner product concept leads to the Gram matrix <span class="arithmatex">\(G_{ij} = \langle x_i, x_j\rangle\)</span> for a set of vectors. In machine learning, the Gram matrix (or kernel matrix) encodes similarity of features and appears in the normal equations for least squares: <span class="arithmatex">\(X^\top X\)</span> is a Gram matrix whose eigenvalues tell us about problem conditioning. A well-conditioned Gram matrix (no tiny eigenvalues) means the problem is nicely scaled for gradient descent, whereas ill-conditioning (some nearly zero eigenvalues) means there are directions in weight space that are very flat, slowing convergence. Techniques like feature scaling or adding regularization (ridge regression) improve the Gram matrix’s condition number and thus algorithm performance.</p>
</li>
</ul>
<h2 id="convex-12_vector-24-norms-and-distances">2.4 Norms and distances<a class="headerlink" href="#convex-12_vector-24-norms-and-distances" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(\|\cdot\|: \mathbb{R}^n \to \mathbb{R}\)</span> is a norm if for all <span class="arithmatex">\(x,y\)</span> and scalar <span class="arithmatex">\(\alpha\)</span>:</p>
<ol>
<li><span class="arithmatex">\(\|x\| \ge 0\)</span> and <span class="arithmatex">\(\|x\| = 0 \iff x=0\)</span>,</li>
<li><span class="arithmatex">\(\|\alpha x\| = |\alpha|\|x\|\)</span> (absolute homogeneity),</li>
<li><span class="arithmatex">\(\|x+y\| \le \|x\| + \|y\|\)</span> (triangle inequality).</li>
</ol>
<p>If the vector space has an inner product, the norm <span class="arithmatex">\(\|x\| = \sqrt{\langle x,x\rangle}\)</span> is called the Euclidean norm (or 2-norm). But many other norms exist, each defining a different geometry.<br>
Common examples on <span class="arithmatex">\(\mathbb{R}^n\)</span>:  </p>
<ul>
<li>
<p><span class="arithmatex">\(\ell_2\)</span> norm (Euclidean): <span class="arithmatex">\(\|x\|_2 = \sqrt{\sum_i x_i^2}\)</span>, the usual length in space.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_1\)</span> norm: <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span>, measuring taxicab distance. In <span class="arithmatex">\(\mathbb{R}^2\)</span>, its unit ball is a diamond.  </p>
</li>
<li>
<p><span class="arithmatex">\(\ell_\infty\)</span> norm: <span class="arithmatex">\(\|x\|_\infty = \max_i |x_i|\)</span>, measuring the largest coordinate magnitude. Its unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a square.  </p>
</li>
<li>
<p>General <span class="arithmatex">\(\ell_p\)</span> norm: <span class="arithmatex">\(\|x\|_p = \left(\sum_i |x_i|^p\right)^{1/p}\)</span> for <span class="arithmatex">\(p\ge1\)</span>. This interpolates between <span class="arithmatex">\(\ell_1\)</span> and <span class="arithmatex">\(\ell_2\)</span>, and approaches <span class="arithmatex">\(\ell_\infty\)</span> as <span class="arithmatex">\(p\to\infty\)</span>. All <span class="arithmatex">\(\ell_p\)</span> norms are convex and satisfy the norm axioms.  </p>
</li>
</ul>
<p>Every norm induces a metric (distance) <span class="arithmatex">\(d(x,y) = |x-y|\)</span> on the space. Norms thus define the shape of “balls” (sets <span class="arithmatex">\({x: |x|\le \text{constant}}\)</span>) and how we measure closeness. The choice of norm can significantly influence an optimization algorithm’s behavior: it affects what steps are considered small, which directions are easy to move in, and how convergence is assessed.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../convex/images/norms.png" data-desc-position="bottom"><img alt="Alt text" src="../convex/images/norms.png" style="float:right; margin-right:15px; width:400px;"></a></p>
<p>Unit-ball geometry: The shape of the unit ball <span class="arithmatex">\({x: |x| \le 1}\)</span> reveals how a norm treats different directions. For example, the <span class="arithmatex">\(\ell_2\)</span> unit ball in <span class="arithmatex">\(\mathbb{R}^2\)</span> is a perfect circle, treating all directions uniformly, whereas the <span class="arithmatex">\(\ell_1\)</span> unit ball is a diamond with corners along the axes, indicating that <span class="arithmatex">\(\ell_1\)</span> treats the coordinate axes as special (those are “cheaper” directions since the ball extends further along axes, touching them at <span class="arithmatex">\((\pm1,0)\)</span> and <span class="arithmatex">\((0,\pm1)\)</span>). The <span class="arithmatex">\(\ell_\infty\)</span> unit ball is a square aligned with axes, suggesting it allows more combined motion in coordinates as long as no single coordinate exceeds the limit. These shapes are illustrated below: we see the red diamond (<span class="arithmatex">\(\ell_1\)</span>), green circle (<span class="arithmatex">\(\ell_2\)</span>), and blue square (<span class="arithmatex">\(\ell_\infty\)</span>) in <span class="arithmatex">\(\mathbb{R}^2\)</span> . The geometry of the unit ball matters whenever we regularize or constrain solutions by a norm. For instance, using an <span class="arithmatex">\(\ell_1\)</span> norm ball as a constraint or regularizer encourages solutions on the corners (sparse solutions), while an <span class="arithmatex">\(\ell_2\)</span> ball encourages more evenly-distributed changes. An <span class="arithmatex">\(\ell_\infty\)</span> constraint limits the maximum absolute value of any component, leading to solutions that avoid any single large entry.</p>
<p>Dual norms: Each norm <span class="arithmatex">\(\|\cdot\|\)</span> has a dual norm <span class="arithmatex">\(\|\cdot\|_*\)</span> defined by
<script type="math/tex; mode=display">
\|y\|_* = \sup_{\|x\|\le 1} x^\top y~.
</script>
For example, the dual of <span class="arithmatex">\(\ell_1\)</span> is <span class="arithmatex">\(\ell_\infty\)</span>, and the dual of <span class="arithmatex">\(\ell_2\)</span> is itself.</p>
<blockquote>
<p>Imagine the vector <span class="arithmatex">\(x\)</span> lives inside the original norm ball (<span class="arithmatex">\(\|x\| \le 1\)</span>). The term <span class="arithmatex">\(x^\top y\)</span> is the dot product, which measures the alignment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>. The dual norm <span class="arithmatex">\(\|y\|_*\)</span> is the maximum possible value you can get by taking the dot product of <span class="arithmatex">\(y\)</span> with any vector <span class="arithmatex">\(x\)</span> that fits inside the original norm ball.If the dual norm <span class="arithmatex">\(\|y\|_*\)</span> is large, it means <span class="arithmatex">\(y\)</span> is strongly aligned with a direction <span class="arithmatex">\(x\)</span> that is "small" (size <span class="arithmatex">\(\le 1\)</span>) according to the original norm.If the dual norm is small, <span class="arithmatex">\(y\)</span> must be poorly aligned with all vectors <span class="arithmatex">\(x\)</span> in the ball.</p>
</blockquote>
<p>Norms in optimization algorithms: Different norms define different algorithmic behaviors. For example, gradient descent typically uses the Euclidean norm for step sizes and convergence analysis, but coordinate descent methods implicitly use <span class="arithmatex">\(\ell_\infty\)</span> (since one coordinate move at a time is like a step in <span class="arithmatex">\(\ell_\infty\)</span> unit ball). Mirror descent methods use non-Euclidean norms and their duals to get better performance on certain problems (e.g. using <span class="arithmatex">\(\ell_1\)</span> norm for sparse problems). The norm also figures in complexity bounds: an algorithm’s convergence rate may depend on the diameter of the feasible set in the chosen norm, <span class="arithmatex">\(D = \max_{\text{feasible}}|x - x^*|\)</span>. For instance, in subgradient methods, having a smaller <span class="arithmatex">\(\ell_2\)</span> diameter or <span class="arithmatex">\(\ell_1\)</span> diameter can improve bounds. Moreover, when constraints are given by norms (like <span class="arithmatex">\(|x|_1 \le t\)</span>), projections and proximal operators with respect to that norm become subroutines in algorithms.</p>
<p>In summary, norms provide the metric backbone of optimization. They tell us how to measure progress (<span class="arithmatex">\(|x_k - x^*|\)</span>), how to constrain solutions (<span class="arithmatex">\(|x| \le R\)</span>), and how to bound errors. The choice of norm can induce sparsity, robustness, or other desired structure in solutions, and mastering norms and their geometry is key to understanding advanced optimization techniques.</p>
<h2 id="convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices">2.5 Eigenvalues, eigenvectors, and positive semidefinite matrices<a class="headerlink" href="#convex-12_vector-25-eigenvalues-eigenvectors-and-positive-semidefinite-matrices" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{n\times n}\)</span> is linear, a nonzero <span class="arithmatex">\(v\)</span> is an eigenvector with eigenvalue <span class="arithmatex">\(\lambda\)</span> if</p>
<div class="arithmatex">\[
Av = \lambda v~.
\]</div>
<p>When <span class="arithmatex">\(A\)</span> is symmetric (<span class="arithmatex">\(A = A^\top\)</span>), it has:</p>
<ul>
<li>real eigenvalues,</li>
<li>an orthonormal eigenbasis,</li>
<li>a spectral decomposition</li>
</ul>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^\top,
</script>
where <span class="arithmatex">\(Q\)</span> is orthonormal and <span class="arithmatex">\(\Lambda\)</span> is diagonal.</p>
<p>This is the spectral decomposition. Geometrically, a symmetric matrix acts as a scaling along <span class="arithmatex">\(n\)</span> orthogonal principal directions (its eigenvectors), stretching or flipping by factors given by <span class="arithmatex">\(\lambda_i\)</span>.</p>
<blockquote>
<p>When dealing specifically with square matrices and quadratic forms (like Hessians of twice-differentiable functions), eigenvalues become central. They describe how a symmetric matrix scales vectors in different directions. Many convex optimization conditions involve requiring a matrix (Hessian or constraint curvature matrix) to be positive semidefinite, which is an eigenvalue condition.</p>
<p>In optimization, the Hessian matrix of a multivariate function <span class="arithmatex">\(f(x)\)</span> is symmetric. Its eigenvalues <span class="arithmatex">\(\lambda_i(\nabla^2 f(x))\)</span> tell us the curvature along principal axes. If all eigenvalues are positive at a point, the function curves up in all directions (a local minimum if gradient is zero); if any eigenvalue is negative, there’s a direction of negative curvature (a saddle or maximum). So checking eigenvalues of Hessian is a way to test convexity/concavity locally.</p>
</blockquote>
<p>Positive semidefinite matrices: A symmetric matrix <span class="arithmatex">\(Q\)</span> is positive semidefinite (PSD) if</p>
<div class="arithmatex">\[
x^\top Q x \ge 0 \quad \text{for all } x~.
\]</div>
<p>If <span class="arithmatex">\(x^\top Q x &gt; 0\)</span> for all <span class="arithmatex">\(x\ne 0\)</span>, then <span class="arithmatex">\(Q\)</span> is positive definite (PD).</p>
<p>Why this matters: if <span class="arithmatex">\(f(x) = \tfrac{1}{2} x^\top Q x + c^\top x + d\)</span>, then</p>
<div class="arithmatex">\[
\nabla^2 f(x) = Q~.
\]</div>
<p>So <span class="arithmatex">\(f\)</span> is convex iff <span class="arithmatex">\(Q\)</span> is PSD. Quadratic objectives with PSD Hessians are convex; with indefinite Hessians, they are not. This is the algebraic test for convexity of quadratic forms.</p>
<blockquote>
<p>Implications of definiteness: If <span class="arithmatex">\(A \succ 0\)</span>, the quadratic function <span class="arithmatex">\(x^T A x\)</span> is strictly convex and has a unique minimizer at <span class="arithmatex">\(x=0\)</span>. If <span class="arithmatex">\(A \succeq 0\)</span>, <span class="arithmatex">\(x^T A x\)</span> is convex but could be flat in some directions (if some <span class="arithmatex">\(\lambda_i = 0\)</span>, those eigenvectors lie in the nullspace and the form is constant along them). In optimization, PD Hessian <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> means <span class="arithmatex">\(f\)</span> has a unique local (and global, if domain convex) minimum at that <span class="arithmatex">\(x\)</span> (since the second-order condition for optimality is satisfied strictly). PD constraint matrices in quadratic programs ensure nice properties like Slater’s condition for strong duality.</p>
<p>Condition number and convergence: For iterative methods on convex quadratics <span class="arithmatex">\(f(x) = \frac{1}{2}x^T Q x - b^T x\)</span>, the eigenvalues of <span class="arithmatex">\(Q\)</span> dictate convergence speed. Gradient descent’s error after <span class="arithmatex">\(k\)</span> steps satisfies roughly <span class="arithmatex">\(|x_k - x^*| \le (\frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}})^k |x_0 - x^*|\)</span> (for normalized step). So the ratio <span class="arithmatex">\(\frac{\lambda_{\max}}{\lambda_{\min}} = \kappa(Q)\)</span> appears: closer to 1 (well-conditioned) means rapid convergence; large ratio (ill-conditioned) means slow, zigzagging progress. Newton’s method uses Hessian inverse, effectively rescaling by eigenvalues to 1, so its performance is invariant to <span class="arithmatex">\(\kappa\)</span> (locally). This explains why second-order methods shine on ill-conditioned problems: they “whiten” the curvature by dividing by eigenvalues.</p>
<p>Optimization interpretation of eigenvectors: The eigenvectors of <span class="arithmatex">\(\nabla^2 f(x^*)\)</span> at optimum indicate principal axes of the local quadratic approximation. Directions with small eigenvalues are flat directions where the function changes slowly (possibly requiring LARGE steps unless Newton’s method is used). Directions with large eigenvalues are steep, potentially requiring small step sizes to maintain stability if using gradient descent. Preconditioning or change of variables often aims to transform the problem so that in new coordinates the Hessian is closer to the identity (all eigenvalues ~1). </p>
</blockquote>
<h2 id="convex-12_vector-26-orthogonal-projections-and-least-squares">2.6 Orthogonal projections and least squares<a class="headerlink" href="#convex-12_vector-26-orthogonal-projections-and-least-squares" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(S\)</span> be a subspace of <span class="arithmatex">\(\mathbb{R}^n\)</span>. The orthogonal projection of a vector <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is the unique vector <span class="arithmatex">\(p \in S\)</span> minimising <span class="arithmatex">\(\|b - p\|_2\)</span>. Geometrically, <span class="arithmatex">\(p\)</span> is the closest point in <span class="arithmatex">\(S\)</span> to <span class="arithmatex">\(b\)</span>. If <span class="arithmatex">\(S = \mathrm{span}\{a_1,\dots,a_k\}\)</span> and <span class="arithmatex">\(A = [a_1~\cdots~a_k]\)</span>, then projecting <span class="arithmatex">\(b\)</span> onto <span class="arithmatex">\(S\)</span> is equivalent to solving the least-squares problem</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2~.
\]</div>
<p>The solution <span class="arithmatex">\(x^*\)</span> satisfies the normal equations</p>
<div class="arithmatex">\[
A^\top A x^* = A^\top b~.
\]</div>
<p>This is our first real convex optimisation problem:</p>
<ul>
<li>the objective <span class="arithmatex">\(\|Ax-b\|_2^2\)</span> is convex,</li>
<li>there are no constraints,</li>
<li>we can solve it in closed form.</li>
</ul>
<h2 id="convex-12_vector-27-operator-norms-singular-values-and-spectral-structure">2.7 Operator norms, singular values, and spectral structure<a class="headerlink" href="#convex-12_vector-27-operator-norms-singular-values-and-spectral-structure" title="Permanent link">¶</a></h2>
<p>Many aspects of optimization depend on how a matrix transforms vectors: how much it stretches them, in which directions it amplifies or shrinks signals, and how sensitive it is to perturbations. Operator norms and singular values provide the tools to quantify these behaviors.</p>
<h3 id="convex-12_vector-operator-norms">Operator norms<a class="headerlink" href="#convex-12_vector-operator-norms" title="Permanent link">¶</a></h3>
<p>Given a matrix <span class="arithmatex">\(A : \mathbb{R}^n \to \mathbb{R}^m\)</span> and norms <span class="arithmatex">\(\|\cdot\|_p\)</span> on <span class="arithmatex">\(\mathbb{R}^n\)</span> and <span class="arithmatex">\(\|\cdot\|_q\)</span> on <span class="arithmatex">\(\mathbb{R}^m\)</span>, the induced operator norm is defined as
<script type="math/tex; mode=display">
\|A\|_{p \to q}
=
\sup_{x \neq 0}
\frac{\|A x\|_q}{\|x\|_p}
=
\sup_{\|x\|_p \le 1} \|A x\|_q.
</script>
This quantity measures the largest amount by which <span class="arithmatex">\(A\)</span> can magnify a vector when measured with the chosen norms. Several important special cases are widely used:</p>
<ul>
<li><span class="arithmatex">\(\|A\|_{2 \to 2}\)</span>, the spectral norm, equals the largest singular value of <span class="arithmatex">\(A\)</span>.</li>
<li><span class="arithmatex">\(\|A\|_{1 \to 1}\)</span> is the maximum absolute column sum.</li>
<li><span class="arithmatex">\(\|A\|_{\infty \to \infty}\)</span> is the maximum absolute row sum.</li>
</ul>
<p>In optimization, operator norms play a central role in determining stability. For example, gradient descent on the quadratic function<br>
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2} x^\top Q x - b^\top x
</script>
converges for step sizes <span class="arithmatex">\(\alpha &lt; 2 / \|Q\|_2\)</span>. This shows that controlling the operator norm of the Hessian—or a Lipschitz constant of the gradient—directly governs how aggressively an algorithm can move.</p>
<h3 id="convex-12_vector-singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#convex-12_vector-singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>Any matrix <span class="arithmatex">\(A \in \mathbb{R}^{m \times n}\)</span> admits a factorization
<script type="math/tex; mode=display">
A = U \Sigma V^\top,
</script>
where <span class="arithmatex">\(U\)</span> and <span class="arithmatex">\(V\)</span> are orthogonal matrices and <span class="arithmatex">\(\Sigma\)</span> is diagonal with nonnegative entries <span class="arithmatex">\(\sigma_1 \ge \sigma_2 \ge \cdots\)</span>. The <span class="arithmatex">\(\sigma_i\)</span> are the singular values of <span class="arithmatex">\(A\)</span>.</p>
<p>Geometrically, the SVD shows how <span class="arithmatex">\(A\)</span> transforms the unit ball into an ellipsoid. The columns of <span class="arithmatex">\(V\)</span> give the principal input directions, the singular values are the lengths of the ellipsoid’s axes, and the columns of <span class="arithmatex">\(U\)</span> give the output directions. The largest singular value <span class="arithmatex">\(\sigma_{\max}\)</span> equals the spectral norm <span class="arithmatex">\(\|A\|_2\)</span>, while the smallest <span class="arithmatex">\(\sigma_{\min}\)</span> describes the least expansion (or exact flattening if <span class="arithmatex">\(\sigma_{\min} = 0\)</span>).</p>
<p>SVD is a powerful diagnostic tool in optimization. The ratio
<script type="math/tex; mode=display">
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
</script>
is the condition number of <span class="arithmatex">\(A\)</span>. A large condition number implies that the map stretches some directions much more than others, leading to slow or unstable convergence in gradient methods. A small condition number means <span class="arithmatex">\(A\)</span> behaves more like a uniform scaling, which is ideal for optimization.</p>
<h3 id="convex-12_vector-low-rank-structure">Low-rank structure<a class="headerlink" href="#convex-12_vector-low-rank-structure" title="Permanent link">¶</a></h3>
<p>The rank of <span class="arithmatex">\(A\)</span> is the number of nonzero singular values. When <span class="arithmatex">\(A\)</span> has low rank, it effectively acts on a lower-dimensional subspace. This structure can be exploited in optimization: low-rank matrices enable dimensionality reduction, fast matrix-vector products, and compact representations. In machine learning, truncated SVD is used for PCA, feature compression, and approximating large linear operators.</p>
<p>Low-rank structure is also a modeling target. Convex formulations such as nuclear-norm minimization encourage solutions whose matrices have small rank, reflecting latent low-dimensional structure in data.</p>
<h3 id="convex-12_vector-operator-norms-and-optimization-algorithms">Operator norms and optimization algorithms<a class="headerlink" href="#convex-12_vector-operator-norms-and-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>Operator norms help determine step sizes, convergence rates, and preconditioning strategies. For a general smooth convex function, the Lipschitz constant of its gradient often corresponds to a spectral norm of a Hessian or Jacobian, and this constant controls the safe step size for gradient descent. Preconditioning modifies the geometry of the problem—changing the inner product or scaling the variables—in order to reduce the effective operator norm and improve conditioning.</p>
<p>These spectral considerations appear in both first-order and second-order methods. Newton’s method, for example, implicitly rescales the space using the inverse Hessian, which equalizes curvature by transforming eigenvalues toward 1. This explains its rapid local convergence when the Hessian is well behaved.</p>
<h3 id="convex-12_vector-summary">Summary<a class="headerlink" href="#convex-12_vector-summary" title="Permanent link">¶</a></h3>
<ul>
<li>The operator norm measures the maximum stretching effect of a matrix.</li>
<li>Singular values give a complete geometric description of this stretching.</li>
<li>The condition number captures how unevenly the matrix acts in different directions.</li>
<li>Low-rank structure reveals underlying dimension and enables efficient computation.</li>
<li>All of these properties strongly influence the behavior and design of optimization algorithms.</li>
</ul>
<p>Understanding operator norms and singular values provides valuable insight into when optimization problems are well conditioned, how algorithms will behave, and how to modify a problem to improve performance.</p></body></html></section><section class="print-page" id="convex-13_calculus" heading-number="2.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-multivariable-calculus-for-optimization">Chapter 3: Multivariable Calculus for Optimization<a class="headerlink" href="#convex-13_calculus-chapter-3-multivariable-calculus-for-optimization" title="Permanent link">¶</a></h1>
<p>Optimization problems are ultimately questions about how a function changes when we move in different directions. To understand this behavior, we rely on multivariable calculus. Concepts such as gradients, Jacobians, Hessians, and Taylor expansions describe how a real-valued function behaves locally and how its value varies as we adjust its inputs.</p>
<p>These tools form the analytical backbone of modern optimization. Gradients determine descent directions and guide first-order algorithms such as gradient descent and stochastic gradient methods. Hessians quantify curvature and enable second-order methods like Newton’s method, which adapt their steps to the shape of the objective. Jacobians and chain rules underpin backpropagation in neural networks, linking calculus to large-scale machine learning practice.</p>
<p>This chapter develops the differential calculus needed for convex analysis and for understanding why many optimization algorithms work. We emphasize geometric intuition, how functions curve, how directions interact, and how local approximations guide global behavior, while providing the formal tools required to analyze convergence and stability in later chapters.</p>
<h2 id="convex-13_calculus-31-gradients-and-directional-derivatives">3.1 Gradients and Directional Derivatives<a class="headerlink" href="#convex-13_calculus-31-gradients-and-directional-derivatives" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>. The function is differentiable at a point <span class="arithmatex">\(x\)</span> if there exists a vector <span class="arithmatex">\(\nabla f(x)\)</span> such that
<script type="math/tex; mode=display">
f(x + h)
=
f(x) + \nabla f(x)^\top h + o(\|h\|),
</script>
meaning that the linear function <span class="arithmatex">\(h \mapsto \nabla f(x)^\top h\)</span> provides the best local approximation to <span class="arithmatex">\(f\)</span> near <span class="arithmatex">\(x\)</span>. The gradient is the unique vector with this property.</p>
<p>A closely related concept is the directional derivative. For any direction <span class="arithmatex">\(v \in \mathbb{R}^n\)</span>, the directional derivative of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> in the direction <span class="arithmatex">\(v\)</span> is
<script type="math/tex; mode=display">
D_v f(x)
=
\lim_{t \to 0} \frac{f(x + tv) - f(x)}{t}.
</script>
If <span class="arithmatex">\(f\)</span> is differentiable, then
<script type="math/tex; mode=display">
D_v f(x) = \nabla f(x)^\top v.
</script>
Thus, the gradient encodes all directional derivatives simultaneously: its inner product with a direction <span class="arithmatex">\(v\)</span> tells us how rapidly <span class="arithmatex">\(f\)</span> increases when we move infinitesimally along <span class="arithmatex">\(v\)</span>.</p>
<p>This immediately yields an important geometric fact. Among all unit directions <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
D_u f(x) = \langle \nabla f(x), u \rangle
</script>
is maximized when <span class="arithmatex">\(u\)</span> points in the direction of <span class="arithmatex">\(\nabla f(x)\)</span>, the direction of steepest ascent. The steepest descent direction is therefore <span class="arithmatex">\(-\nabla f(x)\)</span>, which motivates gradient-descent algorithms for minimizing functions.</p>
<blockquote>
<p>For any real number <span class="arithmatex">\(c\)</span>, the level set of <span class="arithmatex">\(f\)</span> is 
<script type="math/tex; mode=display">
L_c = \{\, x \in \mathbb{R}^n : f(x) = c \,\}.
</script>
</p>
<p>At any point <span class="arithmatex">\(x\)</span> with <span class="arithmatex">\(\nabla f(x) \ne 0\)</span>, the gradient <span class="arithmatex">\(\nabla f(x)\)</span> is orthogonal to the level set <span class="arithmatex">\(L_{f(x)}\)</span>. Geometrically, level sets are like contour lines on a topographic map, and the gradient points perpendicular to them — in the direction of the steepest ascent of <span class="arithmatex">\(f\)</span>. If we wish to decrease <span class="arithmatex">\(f\)</span>, we move roughly in the opposite direction, <span class="arithmatex">\(-\nabla f(x)\)</span> (the direction of steepest descent). This geometric fact becomes central in constrained optimization:  at optimality, the gradient of the objective lies in the span of gradients of active constraints.</p>
</blockquote>
<h2 id="convex-13_calculus-32-jacobians">3.2  Jacobians<a class="headerlink" href="#convex-13_calculus-32-jacobians" title="Permanent link">¶</a></h2>
<p>In optimization and machine learning, functions often map many inputs to many outputs for example, neural network layers, physical simulators, and vector-valued transformations. To understand how such functions change locally, we use the Jacobian matrix, which captures how each output responds to each input.</p>
<h3 id="convex-13_calculus-from-derivative-to-gradient">From derivative to gradient<a class="headerlink" href="#convex-13_calculus-from-derivative-to-gradient" title="Permanent link">¶</a></h3>
<p>For a scalar function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, differentiability means that near any point <script type="math/tex"> x </script>,
<script type="math/tex; mode=display">
f(x + h) \approx f(x) + \nabla f(x)^\top h.
</script>
The gradient vector
<script type="math/tex; mode=display">
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x) \\
\vdots \\
\frac{\partial f}{\partial x_n}(x)
\end{bmatrix}
</script>
collects all partial derivatives. Each component measures how sensitive <span class="arithmatex">\(f\)</span> is to changes in a single coordinate. Together, the gradient points in the direction of steepest increase, and its norm indicates how rapidly the function rises.</p>
<h3 id="convex-13_calculus-from-gradient-to-jacobian">From gradient to Jacobian<a class="headerlink" href="#convex-13_calculus-from-gradient-to-jacobian" title="Permanent link">¶</a></h3>
<p>Now consider a vector-valued function <span class="arithmatex">\(F : \mathbb{R}^n \to \mathbb{R}^m\)</span>,
<script type="math/tex; mode=display">
F(x) =
\begin{bmatrix}
F_1(x) \\
\vdots \\
F_m(x)
\end{bmatrix}.
</script>
Each output <span class="arithmatex">\(F_i\)</span> has its own gradient. Stacking these row vectors yields the Jacobian matrix:
<script type="math/tex; mode=display">
J_F(x) =
\begin{bmatrix}
\frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
\end{bmatrix}.
</script>
</p>
<p>The Jacobian provides the best linear approximation of <span class="arithmatex">\(F\)</span> near <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
F(x + h) \approx F(x) + J_F(x)\, h.
</script>
Thus, locally, the nonlinear map <span class="arithmatex">\(F\)</span> behaves like the linear map <span class="arithmatex">\(h \mapsto J_F(x)h\)</span>. A small displacement <span class="arithmatex">\(h\)</span> in input space is transformed into an output change governed by the Jacobian.</p>
<h3 id="convex-13_calculus-interpreting-the-jacobian">Interpreting the Jacobian<a class="headerlink" href="#convex-13_calculus-interpreting-the-jacobian" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Component of <span class="arithmatex">\(J_F(x)\)</span></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row <span class="arithmatex">\(i\)</span></td>
<td>Gradient of output <span class="arithmatex">\(F_i(x)\)</span>: how the <span class="arithmatex">\(i\)</span>-th output changes with each input variable.</td>
</tr>
<tr>
<td>Column <span class="arithmatex">\(j\)</span></td>
<td>Sensitivity of all outputs to <span class="arithmatex">\(x_j\)</span>: how varying input <span class="arithmatex">\(x_j\)</span> affects the entire output vector.</td>
</tr>
<tr>
<td>Determinant (when <span class="arithmatex">\(m=n\)</span>)</td>
<td>Local volume scaling: how <span class="arithmatex">\(F\)</span> expands or compresses space near <span class="arithmatex">\(x\)</span>.</td>
</tr>
<tr>
<td>Rank</td>
<td>Local dimension of the image: whether any input directions are lost or collapsed.</td>
</tr>
</tbody>
</table>
<p>The Jacobian is therefore a compact representation of local sensitivity. In optimization, Jacobians appear in gradient-based methods, backpropagation, implicit differentiation, and the analysis of constraints and dynamics.</p>
<h2 id="convex-13_calculus-33-the-hessian-and-curvature">3.3 The Hessian and Curvature<a class="headerlink" href="#convex-13_calculus-33-the-hessian-and-curvature" title="Permanent link">¶</a></h2>
<p>For a twice–differentiable function <script type="math/tex"> f : \mathbb{R}^n \to \mathbb{R} </script>, the Hessian matrix collects all second-order partial derivatives:
<script type="math/tex; mode=display">
\nabla^{2} f(x) \;=\;
\begin{bmatrix}
\frac{\partial^{2} f}{\partial x_{1}^{2}} & \cdots & \frac{\partial^{2} f}{\partial x_{1}\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial^{2} f}{\partial x_{n}\partial x_{1}} & \cdots & \frac{\partial^{2} f}{\partial x_{n}^{2}}
\end{bmatrix}.
</script>
</p>
<p>The Hessian describes the <strong>local curvature</strong> of the function. While the gradient indicates the direction of steepest change, the Hessian tells us <em>how that directional change itself varies</em>—whether the surface curves upward, curves downward, or remains nearly flat.</p>
<h3 id="convex-13_calculus-curvature-and-positive-definiteness">Curvature and positive definiteness<a class="headerlink" href="#convex-13_calculus-curvature-and-positive-definiteness" title="Permanent link">¶</a></h3>
<p>The eigenvalues of the Hessian determine its geometric behavior:</p>
<ul>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succeq 0 </script> (all eigenvalues nonnegative), the function is locally convex near <span class="arithmatex">\(x\)</span>.  </li>
<li>If <script type="math/tex"> \nabla^{2}f(x) \succ 0 </script>, the surface curves upward in all directions, guaranteeing local (and for convex functions, global) uniqueness of the minimizer.  </li>
<li>If the Hessian has both positive and negative eigenvalues, the point is a saddle: some directions curve up, others curve down.</li>
</ul>
<p>Thus, curvature is directly encoded in the spectrum of the Hessian. Large eigenvalues correspond to steep curvature; small eigenvalues correspond to gently sloping or flat regions.</p>
<h3 id="convex-13_calculus-example-quadratic-functions">Example: Quadratic functions<a class="headerlink" href="#convex-13_calculus-example-quadratic-functions" title="Permanent link">¶</a></h3>
<p>Consider the quadratic function
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2} x^\top Q x - b^\top x,
</script>
where <span class="arithmatex">\(Q\)</span> is symmetric. The gradient and Hessian are
<script type="math/tex; mode=display">
\nabla f(x) = Qx - b, \qquad \nabla^2 f(x) = Q.
</script>
Setting the gradient to zero gives the stationary point
<script type="math/tex; mode=display">
Qx = b.
</script>
If <span class="arithmatex">\(Q \succ 0\)</span>, the solution
<script type="math/tex; mode=display">
x^* = Q^{-1} b
</script>
is the unique minimizer. The Hessian <span class="arithmatex">\(Q\)</span> being positive definite confirms strict convexity.</p>
<p>The eigenvalues of <span class="arithmatex">\(Q\)</span> also explain the difficulty of minimizing <span class="arithmatex">\(f\)</span>:</p>
<ul>
<li>Large eigenvalues produce very steep, narrow directions—optimization methods must take small steps.  </li>
<li>Small eigenvalues produce flat directions—progress is slow, especially for gradient descent.  </li>
</ul>
<p>The ratio of largest to smallest eigenvalue, the <strong>condition number</strong>, governs the convergence speed of first-order methods on quadratic problems. Poor conditioning (large condition number) leads to zig-zagging iterates and slow progress.</p>
<h3 id="convex-13_calculus-why-the-hessian-matters-in-optimization">Why the Hessian matters in optimization<a class="headerlink" href="#convex-13_calculus-why-the-hessian-matters-in-optimization" title="Permanent link">¶</a></h3>
<p>The Hessian provides second-order information that strongly influences algorithm behavior:</p>
<ul>
<li>Newton’s method uses the Hessian to rescale directions, effectively “whitening’’ curvature and often converging rapidly.  </li>
<li>Trust-region and quasi-Newton methods approximate Hessian structure to stabilize steps.  </li>
<li>In convex optimization, positive semidefiniteness of the Hessian is a fundamental characterization of convexity.</li>
</ul>
<p>Understanding the Hessian therefore helps us understand the geometry of an objective, predict algorithm performance, and design methods that behave reliably on challenging landscapes.</p>
<h2 id="convex-13_calculus-34-taylor-approximation">3.4 Taylor approximation<a class="headerlink" href="#convex-13_calculus-34-taylor-approximation" title="Permanent link">¶</a></h2>
<p>Taylor expansions provide local approximations of a function using its derivatives. These approximations form the basis of nearly all gradient-based optimization methods.</p>
<h3 id="convex-13_calculus-first-order-approximation">First-order approximation<a class="headerlink" href="#convex-13_calculus-first-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then for small steps <span class="arithmatex">\(d\)</span>,
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x) + \nabla f(x)^\top d.
</script>
The gradient gives the best linear model of the function near <span class="arithmatex">\(x\)</span>. This linear approximation is the foundation of first-order methods such as gradient descent, which choose directions based on how this model predicts the function will change.</p>
<h3 id="convex-13_calculus-second-order-approximation">Second-order approximation<a class="headerlink" href="#convex-13_calculus-second-order-approximation" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is twice differentiable, we can include curvature information:
<script type="math/tex; mode=display">
f(x + d)
\approx
f(x)
+ \nabla f(x)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x)\, d.
</script>
The quadratic term measures how the gradient itself changes with direction. The behavior of this term depends on the Hessian:</p>
<ul>
<li>If <script type="math/tex"> \nabla^2 f(x) \succeq 0 </script>, the quadratic term is nonnegative and the function curves upward—locally bowl-shaped.</li>
<li>If the Hessian has both positive and negative eigenvalues, the function bends up in some directions and down in others—characteristic of saddle points.</li>
</ul>
<h3 id="convex-13_calculus-role-in-optimization-algorithms">Role in optimization algorithms<a class="headerlink" href="#convex-13_calculus-role-in-optimization-algorithms" title="Permanent link">¶</a></h3>
<p>Second-order Taylor models are the basis of Newton-type methods. Newton’s method chooses <span class="arithmatex">\(d\)</span> by approximately minimizing the quadratic model,
<script type="math/tex; mode=display">
d \approx - \left(\nabla^2 f(x)\right)^{-1} \nabla f(x),
</script>
which balances descent direction and local curvature. Trust-region and quasi-Newton methods also rely on this quadratic approximation, modifying or regularizing it to ensure stable progress.</p>
<p>Thus, Taylor expansions connect a function’s derivatives to practical optimization steps, bridging geometry and algorithm design.</p>
<h2 id="convex-13_calculus-35-smoothness-and-strong-convexity">3.5 Smoothness and Strong Convexity<a class="headerlink" href="#convex-13_calculus-35-smoothness-and-strong-convexity" title="Permanent link">¶</a></h2>
<p>In optimization, the behavior of a function’s curvature strongly influences how algorithms perform. Two fundamental properties Lipschitz smoothness and strong convexity describe how rapidly the gradient can change and how much curvature the function must have.</p>
<h3 id="convex-13_calculus-lipschitz-continuous-gradients-l-smoothness">Lipschitz continuous gradients (L-smoothness)<a class="headerlink" href="#convex-13_calculus-lipschitz-continuous-gradients-l-smoothness" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> has an <span class="arithmatex">\(L\)</span>-Lipschitz continuous gradient if
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\| \qquad \forall x, y.
</script>
This condition limits how quickly the gradient can change. Intuitively, an <span class="arithmatex">\(L\)</span>-smooth function cannot have sharp bends or extremely steep local curvature. A key consequence is the Descent Lemma:
<script type="math/tex; mode=display">
f(y)
\le
f(x)
+
\nabla f(x)^\top (y - x)
+
\frac{L}{2}\|y - x\|^2.
</script>
This inequality states that every <span class="arithmatex">\(L\)</span>-smooth function is upper-bounded by a quadratic model derived from its gradient. It provides a guaranteed estimate of how much the function can increase when we take a step.</p>
<p>In gradient descent, smoothness directly determines a safe step size: choosing
<script type="math/tex; mode=display">
\eta \le \frac{1}{L}
</script>
ensures that each update decreases the function value for convex objectives. In machine learning, the constant <span class="arithmatex">\(L\)</span> effectively controls how large the learning rate can be before training becomes unstable.</p>
<h3 id="convex-13_calculus-strong-convexity">Strong convexity<a class="headerlink" href="#convex-13_calculus-strong-convexity" title="Permanent link">¶</a></h3>
<p>A differentiable function <script type="math/tex"> f </script> is <script type="math/tex"> \mu </script>-strongly convex if, for some <script type="math/tex"> \mu > 0 </script>,
<script type="math/tex; mode=display">
f(y)
\ge
f(x)
+
\langle \nabla f(x),\, y - x \rangle
+
\frac{\mu}{2}\|y - x\|^2
\qquad \forall x, y.
</script>
This condition guarantees that <span class="arithmatex">\(f\)</span> has at least <span class="arithmatex">\(\mu\)</span> amount of curvature everywhere. Geometrically, the function always lies above its tangent plane by a quadratic bowl, growing at least as fast as a parabola away from its minimizer.</p>
<p>Strong convexity has major optimization implications:</p>
<ul>
<li>The minimizer is unique.  </li>
<li>Gradient descent converges linearly with step size <span class="arithmatex">\(\eta \le 1/L\)</span>.  </li>
<li>The ratio <span class="arithmatex">\(L / \mu\)</span> (the condition number) dictates convergence speed.</li>
</ul>
<h3 id="convex-13_calculus-curvature-in-both-directions">Curvature in both directions<a class="headerlink" href="#convex-13_calculus-curvature-in-both-directions" title="Permanent link">¶</a></h3>
<p>Together, smoothness and strong convexity bound the curvature of <span class="arithmatex">\(f\)</span>:
<script type="math/tex; mode=display">
\mu I \;\preceq\; \nabla^2 f(x) \;\preceq\; L I.
</script>
Smoothness prevents the curvature from being too large, while strong convexity prevents it from being too small. Many convergence guarantees in optimization depend on this pair of inequalities.</p>
<p>These concepts—, imiting curvature from above via <span class="arithmatex">\(L\)</span> and from below via <span class="arithmatex">\(\mu\)</span>, form the foundation for analyzing the performance of first-order algorithms and understanding how learning rates, conditioning, and geometry interact.</p></body></html></section><section class="print-page" id="convex-14_convexsets" heading-number="2.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-4-convex-sets-and-geometric-fundamentals">Chapter 4: Convex Sets and Geometric Fundamentals<a class="headerlink" href="#convex-14_convexsets-chapter-4-convex-sets-and-geometric-fundamentals" title="Permanent link">¶</a></h1>
<p>Most optimization problems are constrained. The set of points that satisfy these constraints the feasible region determines where an algorithm is allowed to search. In many machine learning and convex optimization problems, this feasible region is a convex set. Convex sets have a simple but powerful geometric property: any line segment between two feasible points remains entirely within the set. This structure eliminates irregularities and makes optimization far more predictable.</p>
<p>This chapter develops the geometric foundations needed to reason about convexity. We introduce affine sets, convex sets, hyperplanes, halfspaces, polyhedra, and supporting hyperplanes. These objects form the geometric language of convex analysis. Understanding their structure is essential for interpreting constraints, proving optimality conditions, and designing efficient algorithms for convex optimization.</p>
<h2 id="convex-14_convexsets-41-convex-sets">4.1 Convex sets<a class="headerlink" href="#convex-14_convexsets-41-convex-sets" title="Permanent link">¶</a></h2>
<p>A set <script type="math/tex"> C \subseteq \mathbb{R}^n </script> is convex if for any two points <script type="math/tex"> x, y \in C </script> and any <script type="math/tex"> \theta \in [0,1] </script>,
<script type="math/tex; mode=display">
\theta x + (1 - \theta) y \in C.
</script>
That is, the entire line segment between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> lies inside the set. Convex sets have no “holes” or “indentations,” and this geometric regularity is what makes optimization over them tractable.</p>
<h3 id="convex-14_convexsets-examples">Examples<a class="headerlink" href="#convex-14_convexsets-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Affine subspaces: <script type="math/tex"> \{ x : Ax = b \} </script>.  </li>
<li>Halfspaces: <script type="math/tex"> \{ x : a^\top x \le b \} </script>.  </li>
<li>Euclidean balls: <script type="math/tex"> \{ x : \|x\|_2 \le r \} </script>.  </li>
<li>
<script type="math/tex"> \ell_\infty </script> balls (axis-aligned boxes): <script type="math/tex"> \{ x : \|x\|_\infty \le r \} </script>.  </li>
<li>Probability simplex: <script type="math/tex"> \{ x \in \mathbb{R}^n : x \ge 0, \ \sum_i x_i = 1 \} </script>.  </li>
</ul>
<p>A set fails to be convex whenever some segment between two feasible points leaves the set—for example, a crescent or an annulus.</p>
<h2 id="convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces">4.2 Affine sets, hyperplanes, and halfspaces<a class="headerlink" href="#convex-14_convexsets-42-affine-sets-hyperplanes-and-halfspaces" title="Permanent link">¶</a></h2>
<p>Affine sets generalize linear subspaces by allowing a shift. A set <span class="arithmatex">\(A\)</span> is affine if for some point <span class="arithmatex">\(x_0\)</span> and subspace <span class="arithmatex">\(S\)</span>,
<script type="math/tex; mode=display">
A = \{ x_0 + v : v \in S \}.
</script>
Affine sets are always convex, since adding a fixed offset does not affect the convexity of the underlying subspace.</p>
<p>A hyperplane is an affine set defined by a single linear equation:
<script type="math/tex; mode=display">
H = \{ x : a^\top x = b \}, \qquad a \neq 0.
</script>
Hyperplanes act as the “flat boundaries” of higher-dimensional space and are the fundamental building blocks of polyhedra.</p>
<p>A halfspace is one side of a hyperplane:
<script type="math/tex; mode=display">
\{ x : a^\top x \le b \}.
</script>
Halfspaces are convex and serve as basic local approximations to general convex sets.</p>
<h2 id="convex-14_convexsets-43-convex-combinations-and-convex-hulls">4.3 Convex combinations and convex hulls<a class="headerlink" href="#convex-14_convexsets-43-convex-combinations-and-convex-hulls" title="Permanent link">¶</a></h2>
<p>A convex combination of points <script type="math/tex"> x_1, \dots, x_k </script> is a weighted average
<script type="math/tex; mode=display">
\sum_{i=1}^k \theta_i x_i, 
\qquad
\theta_i \ge 0, \qquad \sum_{i=1}^k \theta_i = 1.
</script>
Convex sets are precisely those that contain all convex combinations of their points.</p>
<p>The convex hull of a set <span class="arithmatex">\(S\)</span>, denoted <span class="arithmatex">\(\operatorname{conv}(S)\)</span>, is the set of all convex combinations of finitely many points in <span class="arithmatex">\(S\)</span>. It is the smallest convex set containing <span class="arithmatex">\(S\)</span>. Geometrically, it is the shape you obtain by stretching a tight rubber band around the points.</p>
<p>Convex hulls are important because:</p>
<ul>
<li>Polytopes can be represented either as intersections of halfspaces or as convex hulls of their vertices.</li>
<li>Many optimization relaxations replace a difficult nonconvex set by its convex hull, enabling the use of convex optimization techniques.</li>
</ul>
<h2 id="convex-14_convexsets-44-polyhedra-and-polytopes">4.4 Polyhedra and polytopes<a class="headerlink" href="#convex-14_convexsets-44-polyhedra-and-polytopes" title="Permanent link">¶</a></h2>
<p>A polyhedron is an intersection of finitely many halfspaces:
<script type="math/tex; mode=display">
P = \{ x : Ax \le b \}.
</script>
Polyhedra are always convex; they may be bounded or unbounded.</p>
<p>If a polyhedron is also bounded, it is called a polytope. Polytopes include familiar shapes such as cubes, simplices, and more general polytopes that arise as feasible regions in linear programs.</p>
<h2 id="convex-14_convexsets-45-extreme-points">4.5 Extreme points<a class="headerlink" href="#convex-14_convexsets-45-extreme-points" title="Permanent link">¶</a></h2>
<p>Let <script type="math/tex"> C </script> be a convex set. A point <span class="arithmatex">\(x \in C\)</span> is an extreme point if it cannot be written as a nontrivial convex combination of other points in the set. Formally, if
<script type="math/tex; mode=display">
x = \theta y + (1 - \theta) z,
\qquad 0 < \theta < 1, \qquad y, z \in C,
</script>
implies <script type="math/tex"> y = z = x </script>.</p>
<p>Geometrically, extreme points are the “corners” of a convex set. For polytopes, the extreme points are exactly the vertices. Extreme points are essential in optimization because many convex problems—such as linear programs—achieve their optima at extreme points of the feasible region. This geometric fact underlies simplex-type algorithms and supports duality theory.</p>
<h2 id="convex-14_convexsets-46-cones">4.6 Cones<a class="headerlink" href="#convex-14_convexsets-46-cones" title="Permanent link">¶</a></h2>
<p>Cones generalize the idea of “directions” in geometry. They capture sets that are closed under nonnegative scaling and play a central role in convex analysis and constrained optimization.</p>
<h3 id="convex-14_convexsets-basic-definition">Basic definition<a class="headerlink" href="#convex-14_convexsets-basic-definition" title="Permanent link">¶</a></h3>
<p>A set <span class="arithmatex">\(K \subseteq \mathbb{R}^n\)</span> is a cone if
<script type="math/tex; mode=display">
x \in K, \ \alpha \ge 0
\quad\Longrightarrow\quad
\alpha x \in K.
</script>
A cone is convex if it is also closed under addition:
<script type="math/tex; mode=display">
x, y \in K \quad\Longrightarrow\quad x + y \in K.
</script>
</p>
<p>Cones are not required to contain negative multiples of a vector, so they are generally not subspaces. Instead of extreme points, cones have extreme rays, which represent directions that cannot be formed as positive combinations of other rays. For example, in the nonnegative orthant <script type="math/tex"> \mathbb{R}^n_{\ge 0} </script>, each coordinate axis direction is an extreme ray.</p>
<h3 id="convex-14_convexsets-conic-hull">Conic hull<a class="headerlink" href="#convex-14_convexsets-conic-hull" title="Permanent link">¶</a></h3>
<p>Given any set <span class="arithmatex">\(S\)</span>, its conic hull is the set of all conic combinations:
<script type="math/tex; mode=display">
\operatorname{cone}(S)
=
\left\{
\sum_{i=1}^k \alpha_i s_i : \alpha_i \ge 0,\ s_i \in S
\right\}.
</script>
This is the smallest convex cone containing <span class="arithmatex">\(S\)</span>. Conic hulls appear frequently in duality theory and in convex relaxations for optimization.</p>
<h3 id="convex-14_convexsets-polar-cones">Polar cones<a class="headerlink" href="#convex-14_convexsets-polar-cones" title="Permanent link">¶</a></h3>
<p>For a cone <span class="arithmatex">\(K\)</span>, the polar cone is defined as
<script type="math/tex; mode=display">
K^\circ
=
\left\{
y \in \mathbb{R}^n : \langle y, x \rangle \le 0 \ \forall x \in K
\right\}.
</script>
</p>
<p>Intuition:</p>
<ul>
<li>Polar vectors make a nonacute angle with every vector in <span class="arithmatex">\(K\)</span>.  </li>
</ul>
<p>Key properties:</p>
<ul>
<li><span class="arithmatex">\(K^\circ\)</span> is always a closed convex cone.  </li>
<li>If <span class="arithmatex">\(K\)</span> is a subspace, then <span class="arithmatex">\(K^\circ\)</span> is the orthogonal complement.  </li>
<li>For any closed convex cone,<br>
<script type="math/tex; mode=display">
  (K^\circ)^\circ = K.
  </script>
</li>
</ul>
<p>Polar cones provide the geometric foundation for normal cones, dual cones, and many optimality conditions.</p>
<h3 id="convex-14_convexsets-tangent-cones">Tangent cones<a class="headerlink" href="#convex-14_convexsets-tangent-cones" title="Permanent link">¶</a></h3>
<p>For a set <span class="arithmatex">\(C\)</span> and a point <span class="arithmatex">\(x \in C\)</span>, the tangent cone <span class="arithmatex">\(T_C(x)\)</span> consists of all feasible “infinitesimal directions” from <span class="arithmatex">\(x\)</span>:
<script type="math/tex; mode=display">
T_C(x)
=
\left\{
d : \exists\, t_k \downarrow 0,\ x_k \in C,\ x_k \to x,\ 
\frac{x_k - x}{t_k} \to d
\right\}.
</script>
</p>
<p>Intuition:</p>
<ul>
<li>At an interior point, <span class="arithmatex">\(T_C(x) = \mathbb{R}^n\)</span>: all small moves are allowed.  </li>
<li>At a boundary point, some directions are blocked; only directions that stay inside the set are feasible.</li>
</ul>
<p>Tangent cones describe feasible directions for methods such as projected gradient descent or interior-point algorithms.</p>
<h3 id="convex-14_convexsets-normal-cones">Normal cones<a class="headerlink" href="#convex-14_convexsets-normal-cones" title="Permanent link">¶</a></h3>
<p>For a convex set <span class="arithmatex">\(C\)</span>, the normal cone at a point <span class="arithmatex">\(x \in C\)</span> is
<script type="math/tex; mode=display">
N_C(x)
=
\left\{
v : \langle v, y - x \rangle \le 0 \ \forall y \in C
\right\}.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>Every <span class="arithmatex">\(v \in N_C(x)\)</span> defines a supporting hyperplane to <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(x\)</span>.  </li>
<li>At interior points, the normal cone is <span class="arithmatex">\(\{0\}\)</span>.  </li>
<li>At boundary or corner points, it becomes a pointed cone of outward normals.</li>
</ul>
<p>A fundamental relationship ties tangent and normal cones together:
<script type="math/tex; mode=display">
N_C(x) = \big( T_C(x) \big)^\circ.
</script>
</p>
<p>Normal cones appear directly in first-order optimality conditions. For a constrained problem<br>
<script type="math/tex; mode=display">
\min_{x \in C} f(x),
</script>
a point <span class="arithmatex">\(x^*\)</span> is optimal only if
<script type="math/tex; mode=display">
0 \in \nabla f(x^*) + N_C(x^*).
</script>
This expresses a balance between the objective’s slope and the “pushback’’ from the constraint set.</p>
<p>Cones,especially tangent and normal cones, are geometric tools that allow us to describe feasibility, optimality, and duality in convex optimization using directional information. They generalize the role that orthogonal complements play in linear algebra to nonlinear and constrained settings.</p>
<h2 id="convex-14_convexsets-47-supporting-hyperplanes-and-separation">4.7 Supporting Hyperplanes and Separation<a class="headerlink" href="#convex-14_convexsets-47-supporting-hyperplanes-and-separation" title="Permanent link">¶</a></h2>
<p>One of the most important geometric facts about convex sets is that they can be <em>supported</em> or <em>separated</em> by hyperplanes. These results show that convex sets always admit linear boundaries that describe their shape. Later, these ideas reappear in duality, subgradients, and the KKT conditions.</p>
<h3 id="convex-14_convexsets-supporting-hyperplane-theorem">Supporting Hyperplane Theorem<a class="headerlink" href="#convex-14_convexsets-supporting-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be nonempty, closed, and convex, and let <span class="arithmatex">\(x_0\)</span> be a boundary point of <span class="arithmatex">\(C\)</span>. Then there exists a nonzero vector <span class="arithmatex">\(a\)</span> such that</p>
<div class="arithmatex">\[
a^\top x \le a^\top x_0 \qquad \forall x \in C.
\]</div>
<p>This means that the hyperplane</p>
<div class="arithmatex">\[
a^\top x = a^\top x_0
\]</div>
<p>touches <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(x_0\)</span> but does not cut through it. The vector <span class="arithmatex">\(a\)</span> is normal to the hyperplane. Intuitively, a supporting hyperplane is like a flat board pressed against the edge of a convex object. Supporting hyperplanes will later correspond exactly to subgradients of convex functions.</p>
<h3 id="convex-14_convexsets-separating-hyperplane-theorem">Separating Hyperplane Theorem<a class="headerlink" href="#convex-14_convexsets-separating-hyperplane-theorem" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(C\)</span> and <span class="arithmatex">\(D\)</span> are nonempty, disjoint convex sets, then a hyperplane exists that separates them. That is, there are a nonzero vector <span class="arithmatex">\(a\)</span> and scalar <span class="arithmatex">\(b\)</span> such that</p>
<div class="arithmatex">\[
a^\top x \le b \quad \forall x \in C,
\qquad
a^\top y \ge b \quad \forall y \in D.
\]</div>
<p>The hyperplane <span class="arithmatex">\(a^\top x = b\)</span> places all points of <span class="arithmatex">\(C\)</span> on one side and all points of <span class="arithmatex">\(D\)</span> on the other. This is guaranteed purely by convexity. Separation is the geometric foundation of duality, where we attempt to separate the primal feasible region from violations of the constraints.</p>
<h3 id="convex-14_convexsets-why-this-matters-for-optimisation">Why This Matters for Optimisation<a class="headerlink" href="#convex-14_convexsets-why-this-matters-for-optimisation" title="Permanent link">¶</a></h3>
<p>These geometric results are central to convex optimisation:</p>
<ul>
<li>Subgradients correspond to supporting hyperplanes of the epigraph of a convex function.</li>
<li>Dual variables arise from separating infeasible points from the feasible region.</li>
<li>KKT conditions express the balance between the gradient of the objective and the normals of active constraints.</li>
<li>Projection onto convex sets is well-defined because convex sets admit supporting hyperplanes.</li>
</ul>
<p>Supporting and separating hyperplanes are therefore the geometric machinery behind optimality conditions and convex duality.</p></body></html></section><section class="print-page" id="convex-15_convexfunctions" heading-number="2.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-convex-functions">Chapter 5: Convex Functions<a class="headerlink" href="#convex-15_convexfunctions-chapter-5-convex-functions" title="Permanent link">¶</a></h1>
<p>Convex functions play a central role in optimisation and machine learning. When the objective function is convex, the optimisation landscape has a single global minimum, gradient-based algorithms behave predictably, and optimality conditions have clean geometric interpretations. Many common ML losses—least squares, logistic loss, hinge loss, Huber loss—are convex precisely for these reasons.</p>
<p>This chapter develops the basic tools for understanding convex functions: their definitions, geometric characterisations, first- and second-order tests, and operations that preserve convexity. These tools will later support duality, optimality conditions, and algorithmic analysis.</p>
<h2 id="convex-15_convexfunctions-51-definitions-of-convexity">5.1 Definitions of convexity<a class="headerlink" href="#convex-15_convexfunctions-51-definitions-of-convexity" title="Permanent link">¶</a></h2>
<p>A function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> is convex if for all <span class="arithmatex">\(x,y\)</span> in its domain and all <span class="arithmatex">\(\theta \in [0,1]\)</span>,
<script type="math/tex; mode=display">
f(\theta x + (1-\theta) y)
\le
\theta f(x) + (1-\theta) f(y).
</script>
</p>
<p>The graph of <span class="arithmatex">\(f\)</span> never dips below the straight line between <span class="arithmatex">\((x,f(x))\)</span> and <span class="arithmatex">\((y,f(y))\)</span>. If the inequality is strict whenever <span class="arithmatex">\(x \neq y\)</span>, the function is <em>strictly convex</em>.</p>
<p>A powerful geometric viewpoint comes from the epigraph:
<script type="math/tex; mode=display">
\mathrm{epi}(f) = 
\{ (x,t) \in \mathbb{R}^n \times \mathbb{R} : f(x) \le t \}.
</script>
The function <span class="arithmatex">\(f\)</span> is convex if and only if its epigraph is a convex set. This connects convex functions to the convex sets studied earlier.</p>
<h2 id="convex-15_convexfunctions-52-first-order-characterisation">5.2 First-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-52-first-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, then <span class="arithmatex">\(f\)</span> is convex if and only if
<script type="math/tex; mode=display">
f(y) \ge f(x) + \nabla f(x)^\top (y - x)
\quad\text{for all } x,y.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The tangent plane at any point <span class="arithmatex">\(x\)</span> lies below the function everywhere.</li>
<li><span class="arithmatex">\(\nabla f(x)\)</span> defines a supporting hyperplane to the epigraph.</li>
<li>The gradient provides a global linear underestimator of <span class="arithmatex">\(f\)</span>.</li>
</ul>
<p>This geometric picture is crucial in optimisation:
at a minimiser <span class="arithmatex">\(x^\star\)</span>, convexity implies<br>
<script type="math/tex; mode=display">
\nabla f(x^\star) = 0 \quad \Longleftrightarrow \quad x^\star \text{ is a global minimiser}.
</script>
</p>
<p>For nondifferentiable convex functions, the gradient is replaced by a subgradient, which plays the same role in forming supporting hyperplanes.</p>
<h2 id="convex-15_convexfunctions-53-second-order-characterisation">5.3 Second-order characterisation<a class="headerlink" href="#convex-15_convexfunctions-53-second-order-characterisation" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is twice continuously differentiable, then convexity can be checked via curvature:</p>
<div class="arithmatex">\[
f \text{ is convex } \iff \nabla^2 f(x) \succeq 0 \text{ for all } x.
\]</div>
<ul>
<li>If the Hessian is positive semidefinite everywhere, the function bends upward.  </li>
<li>If <span class="arithmatex">\(\nabla^2 f(x) \succ 0\)</span> everywhere, the function is strictly convex.  </li>
<li>Negative eigenvalues indicate directions of negative curvature — impossible for convex functions.</li>
</ul>
<p>This characterisation connects convexity to the spectral properties of the Hessian discussed earlier.</p>
<h2 id="convex-15_convexfunctions-54-examples-of-convex-functions">5.4 Examples of convex functions<a class="headerlink" href="#convex-15_convexfunctions-54-examples-of-convex-functions" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Affine functions:<br>
<script type="math/tex; mode=display">
   f(x) = a^\top x + b.
   </script>
<br>
   Always convex (and concave). They define supporting hyperplanes.</p>
</li>
<li>
<p>Quadratic functions with PSD Hessian:<br>
<script type="math/tex; mode=display">
   f(x) = \tfrac12 x^\top Q x + c^\top x + d,\quad Q \succeq 0.
   </script>
<br>
   Convex because the curvature matrix <span class="arithmatex">\(Q\)</span> is PSD.</p>
</li>
<li>
<p>Norms:<br>
<script type="math/tex; mode=display">
   f(x) = \|x\|_p \quad (p \ge 1).
   </script>
<br>
   All norms are convex; in ML, norms induce regularisers (Lasso, ridge).</p>
</li>
<li>
<p>Maximum of affine functions:<br>
<script type="math/tex; mode=display">
   f(x) = \max_i (a_i^\top x + b_i).
   </script>
<br>
   Convex because the maximum of convex functions is convex.<br>
   (Important in SVM hinge loss.)</p>
</li>
<li>
<p>Log-sum-exp:<br>
<script type="math/tex; mode=display">
   f(x) = \log\!\left( \sum_{i=1}^k \exp(a_i^\top x + b_i) \right).
   </script>
<br>
   A smooth approximation to the max; convex by Jensen’s inequality. Appears in softmax, logistic regression, partition functions.</p>
</li>
</ol>
<h2 id="convex-15_convexfunctions-55-jensens-inequality">5.5 Jensen’s inequality<a class="headerlink" href="#convex-15_convexfunctions-55-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex and <span class="arithmatex">\(X\)</span> a random variable in its domain. Then:
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>This generalises the definition of convexity from finite averages to expectations.<br>
Practically:</p>
<ul>
<li>convex functions “pull upward” under averaging,</li>
<li>log-sum-exp is convex because exponential is convex,</li>
<li>EM and variational methods rely on Jensen to construct lower bounds.</li>
</ul>
<p>As a finite form, for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<h2 id="convex-15_convexfunctions-56-operations-that-preserve-convexity">5.6 Operations that preserve convexity<a class="headerlink" href="#convex-15_convexfunctions-56-operations-that-preserve-convexity" title="Permanent link">¶</a></h2>
<p>Convexity is preserved under many natural constructions:</p>
<ul>
<li>
<p>Nonnegative scaling:<br>
  If <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\alpha \ge 0\)</span>, then <span class="arithmatex">\(\alpha f\)</span> is convex.</p>
</li>
<li>
<p>Addition:<br>
  If <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> are convex, then <span class="arithmatex">\(f+g\)</span> is convex.</p>
</li>
<li>
<p>Maximum:<br>
<span class="arithmatex">\(\max\{f,g\}\)</span> is convex.</p>
</li>
<li>
<p>Affine pre-composition:<br>
  If <span class="arithmatex">\(A\)</span> is a matrix,
  <script type="math/tex; mode=display">
  x \mapsto f(Ax + b)
  </script>
  is convex.</p>
</li>
<li>
<p>Monotone composition rule:<br>
  If <span class="arithmatex">\(f\)</span> is convex and nondecreasing in each argument, and each <span class="arithmatex">\(g_i\)</span> is convex,<br>
  then <span class="arithmatex">\(x \mapsto f(g_1(x), \dots, g_k(x))\)</span> is convex.</p>
</li>
</ul>
<p>These rules allow construction of complex convex models from simple building blocks.</p>
<h2 id="convex-15_convexfunctions-57-level-sets-of-convex-functions">5.7 Level sets of convex functions<a class="headerlink" href="#convex-15_convexfunctions-57-level-sets-of-convex-functions" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(\alpha \in \mathbb{R}\)</span>, the sublevel set is
<script type="math/tex; mode=display">
\{ x : f(x) \le \alpha \}.
</script>
</p>
<p>If <span class="arithmatex">\(f\)</span> is convex, every sublevel set is convex.<br>
This property is crucial because inequalities <span class="arithmatex">\(f(x) \le \alpha\)</span> are ubiquitous in constraints.</p>
<p>Examples:</p>
<ul>
<li>Norm balls:<br>
<span class="arithmatex">\(\{ x : \|x\|_2 \le r \}\)</span>  </li>
<li>Linear regression confidence ellipsoids:<br>
<span class="arithmatex">\(\{ x : \|Ax - b\|_2 \le \epsilon \}\)</span></li>
</ul>
<p>These sets enable convex constrained optimisation formulations.</p>
<h2 id="convex-15_convexfunctions-58-strict-and-strong-convexity">5.8 Strict and strong convexity<a class="headerlink" href="#convex-15_convexfunctions-58-strict-and-strong-convexity" title="Permanent link">¶</a></h2>
<h3 id="convex-15_convexfunctions-strict-convexity">Strict convexity<a class="headerlink" href="#convex-15_convexfunctions-strict-convexity" title="Permanent link">¶</a></h3>
<p>A function is strictly convex if
<script type="math/tex; mode=display">
f(\theta x + (1-\theta) y)
<
\theta f(x) + (1-\theta) f(y)
</script>
for all <span class="arithmatex">\(x \neq y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>.</p>
<p>Strict convexity implies unique minimisers.</p>
<h3 id="convex-15_convexfunctions-strong-convexity">Strong convexity<a class="headerlink" href="#convex-15_convexfunctions-strong-convexity" title="Permanent link">¶</a></h3>
<p>A differentiable function is <span class="arithmatex">\(\mu\)</span>-strongly convex if
<script type="math/tex; mode=display">
f(y) 
\ge 
f(x) + \nabla f(x)^\top (y - x) + \frac{\mu}{2}\|y - x\|_2^2.
</script>
</p>
<p>Strong convexity adds <em>quantitative curvature</em>: the function grows at least quadratically away from its minimiser.</p>
<p>Consequences:</p>
<ul>
<li>unique minimiser,</li>
<li>gradient descent achieves linear convergence rate,<br>
  error shrinks as<br>
<script type="math/tex; mode=display">
  \|x_{k+1} - x^\star\| \le (1 - \eta\mu)\|x_k - x^\star\| ,
  </script>
</li>
<li>conditioning (<span class="arithmatex">\(\kappa = L/\mu\)</span>) governs algorithmic difficulty.</li>
</ul>
<p>Strong convexity is frequently induced by regularisation (e.g., ridge regression adds <span class="arithmatex">\(\tfrac{\lambda}{2}\|x\|_2^2\)</span>).</p>
<h2 id="convex-15_convexfunctions-summary">Summary<a class="headerlink" href="#convex-15_convexfunctions-summary" title="Permanent link">¶</a></h2>
<p>Convex functions form the analytical backbone of convex optimisation.<br>
They provide:</p>
<ul>
<li>predictable geometry,</li>
<li>clean gradient conditions,</li>
<li>reliable convergence behaviour,</li>
<li>tractable constraints via convex sublevel sets,</li>
<li>stability under composition and modelling operations.</li>
</ul>
<p>These properties make convex objectives indispensable across machine learning, signal processing, and optimisation theory.</p></body></html></section><section class="print-page" id="convex-16_subgradients" heading-number="2.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-nonsmooth-convex-optimization-subgradients">Chapter 6: Nonsmooth Convex Optimization – Subgradients<a class="headerlink" href="#convex-16_subgradients-chapter-6-nonsmooth-convex-optimization-subgradients" title="Permanent link">¶</a></h1>
<p>Many important convex objectives in machine learning are not differentiable everywhere. Examples include:</p>
<ul>
<li>the <script type="math/tex"> \ell_1 </script> norm <script type="math/tex"> \|x\|_1 = \sum_i |x_i| </script> (nondifferentiable at zero),</li>
<li>pointwise-max functions such as <script type="math/tex"> f(x) = \max_i (a_i^\top x + b_i) </script>,</li>
<li>the hinge loss <script type="math/tex"> \max\{0,\, 1 - y w^\top x\} </script> used in SVMs,</li>
<li>regularisers like total variation or indicator functions of convex sets.</li>
</ul>
<p>Although these functions have “kinks”, they remain convex—and convexity guarantees the existence of supporting hyperplanes at every point. Subgradients formalise this idea and allow optimisation algorithms to operate even when no derivative exists.</p>
<p>This chapter introduces subgradients, subdifferentials, subgradient calculus, and the basic subgradient method.</p>
<h2 id="convex-16_subgradients-61-subgradients-and-the-subdifferential">6.1 Subgradients and the Subdifferential<a class="headerlink" href="#convex-16_subgradients-61-subgradients-and-the-subdifferential" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> be convex.  A vector <span class="arithmatex">\(g \in \mathbb{R}^n\)</span> is a subgradient of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> if</p>
<div class="arithmatex">\[
f(y) \ge f(x) + g^\top (y - x) \quad \text{for all } y.
\]</div>
<p>Geometric interpretation:</p>
<ul>
<li>The affine function <span class="arithmatex">\(y \mapsto f(x) + g^\top(y-x)\)</span> is a global underestimator of <span class="arithmatex">\(f\)</span>.</li>
<li>Each subgradient defines a supporting hyperplane touching the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((x, f(x))\)</span>.</li>
<li>At smooth points, this supporting hyperplane is unique (the tangent plane).</li>
<li>At kinks, there may be infinitely many supporting hyperplanes.</li>
</ul>
<p>The subdifferential of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\(x\)</span> is the set
<script type="math/tex; mode=display">
\partial f(x)
=
\{ g : f(y) \ge f(x) + g^\top(y-x) \ \forall y \}.
</script>
</p>
<p>Properties:</p>
<ul>
<li>
<script type="math/tex"> \partial f(x) </script> is always a nonempty convex set (if <span class="arithmatex">\(x\)</span> is in the interior of the domain).</li>
<li>If <span class="arithmatex">\(f\)</span> is differentiable at <span class="arithmatex">\(x\)</span>, then<br>
<script type="math/tex; mode=display">
  \partial f(x) = \{\nabla f(x)\}.
  </script>
</li>
<li>If <span class="arithmatex">\(f\)</span> is strictly convex, the subdifferential is a singleton except at boundary/kink points.</li>
</ul>
<p>Thus, subgradients generalise gradients to nonsmooth convex functions, preserving the same geometric meaning.</p>
<h2 id="convex-16_subgradients-62-examples">6.2 Examples<a class="headerlink" href="#convex-16_subgradients-62-examples" title="Permanent link">¶</a></h2>
<h3 id="convex-16_subgradients-absolute-value-in-1d">Absolute value in 1D<a class="headerlink" href="#convex-16_subgradients-absolute-value-in-1d" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(f(t) = |t|\)</span>.<br>
Then:</p>
<ul>
<li>If <span class="arithmatex">\(t &gt; 0\)</span>,  <span class="arithmatex">\(\partial f(t) = \{1\}\)</span>.</li>
<li>If <span class="arithmatex">\(t &lt; 0\)</span>,  <span class="arithmatex">\(\partial f(t) = \{-1\}\)</span>.</li>
<li>If <span class="arithmatex">\(t = 0\)</span>,<br>
<script type="math/tex; mode=display">
  \partial f(0) = [-1,\, 1].
  </script>
</li>
</ul>
<p>At the kink, any slope between <span class="arithmatex">\(-1\)</span> and <span class="arithmatex">\(1\)</span> supports the graph from below.</p>
<h3 id="convex-16_subgradients-the-ell_1-norm">The <script type="math/tex"> \ell_1 </script> norm<a class="headerlink" href="#convex-16_subgradients-the-ell_1-norm" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(f(x) = \|x\|_1 = \sum_i |x_i|\)</span>:</p>
<div class="arithmatex">\[
g \in \partial \|x\|_1
\quad\Longleftrightarrow\quad
g_i \in \partial |x_i|.
\]</div>
<p>Thus:</p>
<ul>
<li>if <span class="arithmatex">\(x_i &gt; 0\)</span>, then <span class="arithmatex">\(g_i = 1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i &lt; 0\)</span>, then <span class="arithmatex">\(g_i = -1\)</span>,</li>
<li>if <span class="arithmatex">\(x_i = 0\)</span>, then <span class="arithmatex">\(g_i \in [-1,1]\)</span>.</li>
</ul>
<p>This structure appears directly in LASSO and compressed sensing optimality conditions.</p>
<h3 id="convex-16_subgradients-pointwise-maximum-of-affine-functions">Pointwise maximum of affine functions<a class="headerlink" href="#convex-16_subgradients-pointwise-maximum-of-affine-functions" title="Permanent link">¶</a></h3>
<p>Let<br>
<script type="math/tex; mode=display">
f(x) = \max_{i=1,\dots,k} (a_i^\top x + b_i).
</script>
</p>
<ul>
<li>
<p>If only one index <span class="arithmatex">\(i^\star\)</span> achieves the maximum at <span class="arithmatex">\(x\)</span>, then<br>
<script type="math/tex; mode=display">
  \partial f(x) = \{ a_{i^\star} \}.
  </script>
</p>
</li>
<li>
<p>If multiple indices are tied, then<br>
<script type="math/tex; mode=display">
  \partial f(x)
  = \mathrm{conv}\{ a_i : i \text{ active at } x \},
  </script>
  the convex hull of the active slopes.</p>
</li>
</ul>
<p>This structure underlies SVM hinge loss and ReLU-type functions.</p>
<hr>
<h2 id="convex-16_subgradients-63-subgradient-optimality-condition">6.3 Subgradient Optimality Condition<a class="headerlink" href="#convex-16_subgradients-63-subgradient-optimality-condition" title="Permanent link">¶</a></h2>
<p>For the unconstrained convex minimisation problem</p>
<div class="arithmatex">\[
\min_x f(x),
\]</div>
<p>a point <span class="arithmatex">\(x^\star\)</span> is optimal if and only if</p>
<div class="arithmatex">\[
0 \in \partial f(x^\star).
\]</div>
<p>Interpretation:</p>
<ul>
<li>At optimality, no subgradient points to a direction that would decrease <span class="arithmatex">\(f\)</span>.</li>
<li>Geometrically, the supporting hyperplane at <span class="arithmatex">\(x^\star\)</span> is horizontal, forming the flat bottom of the convex bowl.</li>
<li>This generalises the smooth condition <script type="math/tex"> \nabla f(x^\star)=0 </script>.</li>
</ul>
<h2 id="convex-16_subgradients-64-subgradient-calculus-useful-rules">6.4 Subgradient Calculus (Useful Rules)<a class="headerlink" href="#convex-16_subgradients-64-subgradient-calculus-useful-rules" title="Permanent link">¶</a></h2>
<p>Subgradients satisfy powerful calculus rules that allow us to work with complex functions. Let <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> be convex.</p>
<h3 id="convex-16_subgradients-sum-rule">Sum rule<a class="headerlink" href="#convex-16_subgradients-sum-rule" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\partial(f+g)(x) \subseteq \partial f(x) + \partial g(x)
=
\{ u+v : u \in \partial f(x),\ v \in \partial g(x) \}.
\]</div>
<p>Equality holds under mild regularity conditions (e.g., if both functions are closed).</p>
<h3 id="convex-16_subgradients-affine-composition">Affine composition<a class="headerlink" href="#convex-16_subgradients-affine-composition" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(h(x) = f(Ax + b)\)</span>, then
<script type="math/tex; mode=display">
\partial h(x) = A^\top \partial f(Ax+b).
</script>
</p>
<p>This rule is heavily used in machine learning models, where losses depend on linear predictions <span class="arithmatex">\(Ax\)</span>.</p>
<h3 id="convex-16_subgradients-maximum-of-convex-functions">Maximum of convex functions<a class="headerlink" href="#convex-16_subgradients-maximum-of-convex-functions" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f(x) = \max_i f_i(x)\)</span>, then
<script type="math/tex; mode=display">
\partial f(x)
= \mathrm{conv}\{ \partial f_i(x) : i \text{ active at } x \}.
</script>
</p>
<p>This supports models based on hinge losses, margin-maximisation, and piecewise-linear architectures.</p>
<h2 id="convex-16_subgradients-65-subgradient-methods">6.5 Subgradient Methods<a class="headerlink" href="#convex-16_subgradients-65-subgradient-methods" title="Permanent link">¶</a></h2>
<p>Even when <span class="arithmatex">\(f\)</span> is not differentiable, we can minimise it using subgradient descent:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha_k g_k,
\qquad g_k \in \partial f(x_k).
\]</div>
<p>Key features:</p>
<ul>
<li>Requires only a subgradient (no differentiability needed).</li>
<li>Works for any convex function.</li>
<li>Stepsizes must typically decrease (e.g. <script type="math/tex"> \alpha_k = c/\sqrt{k} </script>, <script type="math/tex"> \alpha_k = c/k </script>).</li>
<li>Guaranteed convergence for convex <span class="arithmatex">\(f\)</span>, but generally slow.</li>
</ul>
<h3 id="convex-16_subgradients-convergence-rates-worst-case">Convergence rates (worst case)<a class="headerlink" href="#convex-16_subgradients-convergence-rates-worst-case" title="Permanent link">¶</a></h3>
<ul>
<li>Smooth convex gradient descent: <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span>.  </li>
<li>Nonsmooth subgradient descent:<br>
<script type="math/tex; mode=display">
  f(x_k) - f(x^\star) = O(1/\sqrt{k}).
  </script>
</li>
</ul>
<p>This slower rate reflects the lack of curvature information at kinks.</p>
<h3 id="convex-16_subgradients-why-it-still-matters-in-ml">Why it still matters in ML<a class="headerlink" href="#convex-16_subgradients-why-it-still-matters-in-ml" title="Permanent link">¶</a></h3>
<p>Many training objectives behave nonsmoothly:</p>
<ul>
<li>SVM hinge loss  </li>
<li>
<script type="math/tex"> \ell_1 </script>-regularised models (sparse optimisation)  </li>
<li>ReLUs and piecewise-linear networks  </li>
<li>Projections onto convex sets  </li>
</ul>
<p>Even modern deep-learning optimisers operate as subgradient methods whenever the network contains nonsmooth operations.</p>
<h2 id="convex-16_subgradients-66-proximal-and-smoothed-alternatives">6.6 Proximal and Smoothed Alternatives<a class="headerlink" href="#convex-16_subgradients-66-proximal-and-smoothed-alternatives" title="Permanent link">¶</a></h2>
<p>Subgradient descent can be slow. Two important families of methods overcome this:</p>
<h3 id="convex-16_subgradients-1-proximal-methods">(1) Proximal methods<a class="headerlink" href="#convex-16_subgradients-1-proximal-methods" title="Permanent link">¶</a></h3>
<p>For a convex function <span class="arithmatex">\(f\)</span>, the proximal operator is
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha f}(y)
=
\arg\min_x \left\{
f(x) + \frac{1}{2\alpha}\|x-y\|^2
\right\}.
</script>
</p>
<p>Proximal algorithms (e.g., ISTA, FISTA, ADMM) can handle nonsmooth terms like:</p>
<ul>
<li>
<script type="math/tex"> \ell_1 </script> regularisation,</li>
<li>indicator functions of convex sets,</li>
<li>total variation penalties.</li>
</ul>
<p>They achieve faster and more stable convergence than basic subgradient descent.</p>
<h3 id="convex-16_subgradients-2-smoothing-techniques">(2) Smoothing techniques<a class="headerlink" href="#convex-16_subgradients-2-smoothing-techniques" title="Permanent link">¶</a></h3>
<p>Many nonsmooth convex functions have smooth approximations:</p>
<ul>
<li>Replace <script type="math/tex"> |t| </script> with the Huber loss.</li>
<li>Replace <script type="math/tex"> \max\{0,z\} </script> with softplus.</li>
<li>Replace <script type="math/tex"> \max_i(a_i^\top x) </script> with log-sum-exp, a smooth convex approximation.</li>
</ul>
<p>Smoothing preserves convexity while allowing the use of fast gradient methods.</p>
<h2 id="convex-16_subgradients-summary">Summary<a class="headerlink" href="#convex-16_subgradients-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Nonsmooth convex functions arise naturally in ML.  </li>
<li>Subgradients generalise gradients: they give supporting hyperplanes.  </li>
<li>Optimality: <span class="arithmatex">\(0 \in \partial f(x^\star)\)</span>.  </li>
<li>Subgradient calculus enables reasoning about complex nonsmooth models.  </li>
<li>Subgradient descent converges globally but slowly.  </li>
<li>Proximal and smoothing methods yield faster practical algorithms.</li>
</ul>
<p>Subgradients complete the picture of convex analysis by extending optimisation tools beyond differentiable functions, setting the stage for modern first-order methods.</p></body></html></section><section class="print-page" id="convex-16a_optimality_conditions" heading-number="2.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-first-order-and-geometric-optimality-conditions">Chapter 7: First-Order and Geometric Optimality Conditions<a class="headerlink" href="#convex-16a_optimality_conditions-chapter-7-first-order-and-geometric-optimality-conditions" title="Permanent link">¶</a></h1>
<p>Optimization problems seek points where no infinitesimal movement can improve the objective. For convex functions, first-order conditions give precise geometric and analytic criteria for such points to be optimal. They extend the familiar “zero gradient” condition to nonsmooth and constrained settings, linking gradients, subgradients, and the geometry of feasible regions.</p>
<p>These conditions form the conceptual bridge between unconstrained minimization and the Karush–Kuhn–Tucker (KKT) framework developed in the next chapter.</p>
<h2 id="convex-16a_optimality_conditions-71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization">7.1 Orders of Optimality: Why First Order is Enough in Convex Optimization<a class="headerlink" href="#convex-16a_optimality_conditions-71-orders-of-optimality-why-first-order-is-enough-in-convex-optimization" title="Permanent link">¶</a></h2>
<p>For a differentiable function <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, the “order’’ of an optimality condition refers to how many derivatives (or generalized derivatives) we examine around a candidate minimizer <span class="arithmatex">\(x^\star\)</span>:</p>
<table>
<thead>
<tr>
<th>Order</th>
<th>Object inspected</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-order</td>
<td><span class="arithmatex">\(\nabla f(x^\star)\)</span> or subgradients</td>
<td>Detects existence of a local descent direction</td>
</tr>
<tr>
<td>Second-order</td>
<td>Hessian <span class="arithmatex">\(\nabla^2 f(x^\star)\)</span></td>
<td>Examines curvature (minimum vs saddle vs maximum)</td>
</tr>
<tr>
<td>Higher-order</td>
<td>Third derivative and beyond</td>
<td>Rarely used; only for degenerate cases with vanishing curvature</td>
</tr>
</tbody>
</table>
<p>In general nonconvex optimization, these conditions are used together: a point may have <span class="arithmatex">\(\nabla f(x^\star) = 0\)</span> but still be a saddle or a local maximum, so curvature (second order) must also be checked.</p>
<p>For convex functions, the situation is much simpler. A convex function already has non-negative curvature everywhere:</p>
<div class="arithmatex">\[
\nabla^2 f(x) \succeq 0 \quad \text{whenever the Hessian exists}.
\]</div>
<p>Therefore:</p>
<ul>
<li>any stationary point (where the first-order condition holds) cannot be a local maximum or saddle,  </li>
<li>if the function is proper and lower semicontinuous, first-order conditions are enough to guarantee global optimality.</li>
</ul>
<p>As a result, in convex optimization we typically rely only on first-order conditions, possibly expressed in terms of subgradients and geometric objects (normal cones, tangent cones). This collapse of the hierarchy is one of the key simplifications that makes convex analysis powerful.</p>
<h2 id="convex-16a_optimality_conditions-72-motivation">7.2 Motivation<a class="headerlink" href="#convex-16a_optimality_conditions-72-motivation" title="Permanent link">¶</a></h2>
<p>Consider the basic convex problem
<script type="math/tex; mode=display">
\min_{x \in \mathcal{X}} f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X}\)</span> is a convex set.</p>
<p>Intuitively, a point <span class="arithmatex">\(\hat{x}\)</span> is optimal if there is no feasible direction in which we can move and strictly decrease <span class="arithmatex">\(f\)</span>. In the unconstrained case, every direction is feasible. In the constrained case, only directions that stay inside <span class="arithmatex">\(\mathcal{X}\)</span> are allowed.</p>
<p>Thus, optimality can be seen as an equilibrium:</p>
<ul>
<li>the objective’s tendency to decrease (captured by its gradient or subgradient)  </li>
<li>is exactly balanced by the geometric restrictions imposed by the feasible set.</li>
</ul>
<p>In machine learning, this appears as:</p>
<ul>
<li>training a model until the gradient is (approximately) zero in unconstrained problems, or  </li>
<li>training until the force from regularization/constraints balances the data fit term (e.g., in <span class="arithmatex">\(\ell_1\)</span>-regularized models).</li>
</ul>
<p>First-order optimality conditions formalize this equilibrium in both smooth and nonsmooth, constrained and unconstrained settings.</p>
<h2 id="convex-16a_optimality_conditions-73-unconstrained-convex-problems">7.3 Unconstrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-73-unconstrained-convex-problems" title="Permanent link">¶</a></h2>
<p>For the unconstrained problem
<script type="math/tex; mode=display">
\min_x f(x),
</script>
with <span class="arithmatex">\(f\)</span> convex, the optimality conditions are especially simple.</p>
<h3 id="convex-16a_optimality_conditions-smooth-case">Smooth case<a class="headerlink" href="#convex-16a_optimality_conditions-smooth-case" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is differentiable, then a point <span class="arithmatex">\(\hat{x}\)</span> is optimal if and only if
<script type="math/tex; mode=display">
\nabla f(\hat{x}) = 0.
</script>
</p>
<p>Convexity ensures that any point where the gradient vanishes is a global minimizer, not just a local one.</p>
<h3 id="convex-16a_optimality_conditions-nonsmooth-case">Nonsmooth case<a class="headerlink" href="#convex-16a_optimality_conditions-nonsmooth-case" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex but not necessarily differentiable, the gradient is replaced by the subdifferential. The condition becomes
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x}).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The origin lies in the set of all subgradients at <span class="arithmatex">\(\hat{x}\)</span>.  </li>
<li>Geometrically, there exists a horizontal supporting hyperplane to the epigraph of <span class="arithmatex">\(f\)</span> at <span class="arithmatex">\((\hat{x}, f(\hat{x}))\)</span>.  </li>
<li>No direction in <span class="arithmatex">\(\mathbb{R}^n\)</span> gives a first-order improvement in the objective.</li>
</ul>
<p>For smooth <span class="arithmatex">\(f\)</span>, this reduces to the usual condition <span class="arithmatex">\(\nabla f(\hat{x}) = 0\)</span>.</p>
<h2 id="convex-16a_optimality_conditions-74-constrained-convex-problems">7.4 Constrained Convex Problems<a class="headerlink" href="#convex-16a_optimality_conditions-74-constrained-convex-problems" title="Permanent link">¶</a></h2>
<p>Now consider the constrained problem
<script type="math/tex; mode=display">
\min_{x \in \mathcal{X}} f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X} \subseteq \mathbb{R}^n\)</span> is a nonempty closed convex set.</p>
<p>If <span class="arithmatex">\(\hat{x}\)</span> lies strictly inside <span class="arithmatex">\(\mathcal{X}\)</span>, then there is locally no distinction from the unconstrained case: all nearby directions are feasible. In that case,
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x})
</script>
remains the necessary and sufficient condition for optimality.</p>
<p>The interesting case is when <span class="arithmatex">\(\hat{x}\)</span> lies on the boundary of <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<h3 id="convex-16a_optimality_conditions-first-order-condition-with-constraints">First-order condition with constraints<a class="headerlink" href="#convex-16a_optimality_conditions-first-order-condition-with-constraints" title="Permanent link">¶</a></h3>
<p>The general first-order optimality condition for the constrained convex problem is:
<script type="math/tex; mode=display">
0 \in \partial f(\hat{x}) + N_{\mathcal{X}}(\hat{x}).
</script>
</p>
<p>That is, there exist</p>
<ul>
<li>a subgradient <span class="arithmatex">\(g \in \partial f(\hat{x})\)</span>, and  </li>
<li>a normal vector <span class="arithmatex">\(v \in N_{\mathcal{X}}(\hat{x})\)</span></li>
</ul>
<p>such that
<script type="math/tex; mode=display">
g + v = 0.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The objective’s slope <span class="arithmatex">\(g\)</span> is exactly balanced by a normal vector <span class="arithmatex">\(v\)</span> coming from the constraint set.  </li>
<li>If we decompose space into feasible and infeasible directions, there is no feasible direction along which <span class="arithmatex">\(f\)</span> can decrease.  </li>
<li>Geometrically, the epigraph of <span class="arithmatex">\(f\)</span> and the feasible set meet with aligned supporting hyperplanes at <span class="arithmatex">\(\hat{x}\)</span>.</li>
</ul>
<p>Special cases:</p>
<ul>
<li>If <span class="arithmatex">\(\hat{x}\)</span> is an interior point, then <span class="arithmatex">\(N_{\mathcal{X}}(\hat{x}) = \{0\}\)</span>, so we recover the unconstrained condition <span class="arithmatex">\(0 \in \partial f(\hat{x})\)</span>.  </li>
<li>If <span class="arithmatex">\(\mathcal{X}\)</span> is an affine set, the normal cone is the orthogonal complement of its tangent subspace, and the condition aligns with equality-constrained optimality.</li>
</ul></body></html></section><section class="print-page" id="convex-17_kkt" heading-number="2.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-8-lagrange-multipliers-and-the-kkt-framework">Chapter 8: Lagrange Multipliers and the KKT Framework<a class="headerlink" href="#convex-17_kkt-chapter-8-lagrange-multipliers-and-the-kkt-framework" title="Permanent link">¶</a></h1>
<p>We now have the ingredients for understanding optimality in convex optimization:</p>
<ul>
<li>convex functions define well-behaved objectives,</li>
<li>convex sets describe feasible regions,</li>
<li>gradients and subgradients encode descent directions.</li>
</ul>
<p>This chapter unifies these ideas. We begin with unconstrained minimization and then incorporate equality and inequality constraints. The resulting system of conditions—the Karush–Kuhn–Tucker (KKT) conditions—is the central optimality framework for constrained convex optimization.</p>
<p>In constrained problems, the gradient of the objective cannot vanish freely. Instead, it must be balanced by “forces’’ coming from the constraints. Lagrange multipliers measure these forces, and the KKT conditions express this balance algebraically and geometrically.</p>
<h2 id="convex-17_kkt-81-unconstrained-convex-minimization">8.1 Unconstrained Convex Minimization<a class="headerlink" href="#convex-17_kkt-81-unconstrained-convex-minimization" title="Permanent link">¶</a></h2>
<p>Consider the problem
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex and differentiable.</p>
<p>Gradient descent iteratively updates
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} - \alpha_k \nabla f(x^{(k)}),
</script>
with step size <span class="arithmatex">\(\alpha_k &gt; 0\)</span>.</p>
<p>Intuition:</p>
<ul>
<li>Moving opposite the gradient decreases <span class="arithmatex">\(f\)</span>.</li>
<li>If the gradient is Lipschitz continuous and the step size is small enough (<span class="arithmatex">\(\alpha_k \le 1/L\)</span>), then gradient descent converges to a global minimizer.</li>
<li>If <span class="arithmatex">\(f\)</span> is <em>strongly convex</em>, the minimizer is unique and convergence is faster (linear rate with an appropriate step size).</li>
</ul>
<p>In machine learning, this is the foundation of back-propagation and weight training: each update follows the negative gradient of the loss.</p>
<h2 id="convex-17_kkt-82-equality-constrained-problems-and-lagrange-multipliers">8.2 Equality-Constrained Problems and Lagrange Multipliers<a class="headerlink" href="#convex-17_kkt-82-equality-constrained-problems-and-lagrange-multipliers" title="Permanent link">¶</a></h2>
<p>Now consider minimizing <span class="arithmatex">\(f\)</span> subject to equality constraints:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimize} & f(x) \\
\text{subject to} & h_j(x) = 0,\quad j = 1,\dots,p.
\end{array}
</script>
</p>
<p>Define the Lagrangian
<script type="math/tex; mode=display">
L(x, \lambda) = f(x) + \sum_{j=1}^p \lambda_j h_j(x),
</script>
where <span class="arithmatex">\(\lambda = (\lambda_1,\dots,\lambda_p)\)</span> are the Lagrange multipliers.</p>
<p>Under differentiability and regularity assumptions, a point <span class="arithmatex">\(x^*\)</span> is optimal only if:</p>
<ol>
<li>
<p>Primal feasibility
   <script type="math/tex; mode=display">
   h_j(x^*) = 0,\quad \forall j.
   </script>
</p>
</li>
<li>
<p>Stationarity
   <script type="math/tex; mode=display">
   \nabla f(x^*) + \sum_{j=1}^p \lambda_j^* \nabla h_j(x^*) = 0.
   </script>
</p>
</li>
</ol>
<p>Geometric meaning:</p>
<ul>
<li>The feasible set <script type="math/tex"> \{x : h_j(x)=0\} </script> is typically a smooth manifold.</li>
<li>At an optimum, the gradient of the objective must be orthogonal to all feasible directions.</li>
<li>The multipliers <span class="arithmatex">\(\lambda_j^*\)</span> weight the constraint normals to exactly cancel the objective’s gradient.</li>
</ul>
<p>In other words, the objective tries to decrease, the constraints push back, and at the optimum these forces balance.</p>
<h2 id="convex-17_kkt-83-inequality-constraints-and-the-kkt-conditions">8.3 Inequality Constraints and the KKT Conditions<a class="headerlink" href="#convex-17_kkt-83-inequality-constraints-and-the-kkt-conditions" title="Permanent link">¶</a></h2>
<p>Now consider the general convex problem:
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimize} & f(x) \\
\text{subject to} 
 & g_i(x) \le 0,\quad i=1,\dots,m, \\
 & h_j(x) = 0,\quad j=1,\dots,p.
\end{array}
</script>
</p>
<p>Form the Lagrangian
<script type="math/tex; mode=display">
L(x,\lambda,\mu) 
= f(x) 
+ \sum_{j=1}^p \lambda_j h_j(x)
+ \sum_{i=1}^m \mu_i g_i(x),
</script>
with:</p>
<ul>
<li>
<script type="math/tex"> \lambda_j \in \mathbb{R} </script> (equality multipliers),</li>
<li>
<script type="math/tex"> \mu_i \ge 0 </script> (inequality multipliers).</li>
</ul>
<p>A point <span class="arithmatex">\(x^*\)</span> with multipliers <span class="arithmatex">\((\lambda^*,\mu^*)\)</span> satisfies the KKT conditions:</p>
<h3 id="convex-17_kkt-1-primal-feasibility">1. Primal feasibility<a class="headerlink" href="#convex-17_kkt-1-primal-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
g_i(x^*) \le 0,\quad \forall i,
\qquad
h_j(x^*) = 0,\quad \forall j.
\]</div>
<h3 id="convex-17_kkt-2-dual-feasibility">2. Dual feasibility<a class="headerlink" href="#convex-17_kkt-2-dual-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^* \ge 0,\quad \forall i.
\]</div>
<h3 id="convex-17_kkt-3-stationarity">3. Stationarity<a class="headerlink" href="#convex-17_kkt-3-stationarity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\nabla f(x^*) 
+ \sum_{j=1}^p \lambda_j^* \nabla h_j(x^*)
+ \sum_{i=1}^m \mu_i^* \nabla g_i(x^*)
= 0.
\]</div>
<h3 id="convex-17_kkt-4-complementary-slackness">4. Complementary slackness<a class="headerlink" href="#convex-17_kkt-4-complementary-slackness" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^*\, g_i(x^*) = 0, \quad i=1,\dots,m.
\]</div>
<p>Complementary slackness expresses a clear dichotomy:</p>
<ul>
<li>If constraint <span class="arithmatex">\(g_i(x) \le 0\)</span> is inactive (strictly <span class="arithmatex">\(&lt;0\)</span>), then it applies no force: <span class="arithmatex">\(\mu_i^* = 0\)</span>.</li>
<li>If a constraint is active at the boundary, it may exert a force: <span class="arithmatex">\(\mu_i^* &gt; 0\)</span>, and then <span class="arithmatex">\(g_i(x^*) = 0\)</span>.</li>
</ul>
<p>Only active constraints can push back against the objective.</p>
<h2 id="convex-17_kkt-84-slaters-condition-guaranteeing-strong-duality">8.4 Slater’s Condition — Guaranteeing Strong Duality<a class="headerlink" href="#convex-17_kkt-84-slaters-condition-guaranteeing-strong-duality" title="Permanent link">¶</a></h2>
<p>The KKT conditions always provide <em>necessary</em> conditions for optimality. For them to also be <em>sufficient</em> (and to guarantee zero duality gap), the problem must satisfy a regularity condition.</p>
<p>For convex problems with convex <span class="arithmatex">\(g_i\)</span> and affine <span class="arithmatex">\(h_j\)</span>, Slater’s condition holds if there exists a strictly feasible point:
<script type="math/tex; mode=display">
\exists\, x^{\text{slater}}:
\quad h_j(x^{\text{slater}})=0,\ \forall j,
\qquad
g_i(x^{\text{slater}}) < 0,\ \forall i.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>The feasible region contains an interior point.</li>
<li>The constraints are not “tight” everywhere.</li>
<li>The geometry is rich enough for supporting hyperplanes to behave nicely.</li>
</ul>
<p>When Slater’s condition holds:</p>
<ol>
<li>
<p>Strong duality holds:<br>
<script type="math/tex; mode=display">
   p^* = d^*.
   </script>
</p>
</li>
<li>
<p>The dual optimum is attained.</p>
</li>
<li>
<p>The KKT conditions are both necessary and sufficient for optimality.</p>
</li>
</ol>
<h3 id="convex-17_kkt-duality-gap">Duality gap<a class="headerlink" href="#convex-17_kkt-duality-gap" title="Permanent link">¶</a></h3>
<p>For a primal problem with optimum <span class="arithmatex">\(p^*\)</span> and its dual with optimum <span class="arithmatex">\(d^*\)</span>, the duality gap is
<script type="math/tex; mode=display">
p^* - d^* \ge 0.
</script>
</p>
<ul>
<li>A strictly positive gap indicates structural degeneracy or failure of constraint qualification.</li>
<li>Slater’s condition ensures the gap is zero.</li>
</ul>
<p>This link between geometry (interior feasibility) and algebra (zero gap) is fundamental.</p>
<hr>
<h2 id="convex-17_kkt-85-geometric-and-physical-interpretation">8.5 Geometric and Physical Interpretation<a class="headerlink" href="#convex-17_kkt-85-geometric-and-physical-interpretation" title="Permanent link">¶</a></h2>
<p>The KKT conditions describe an equilibrium of forces:</p>
<ul>
<li>The objective gradient pushes the point in the direction of steepest decrease.</li>
<li>Active constraints push back through normal vectors scaled by multipliers.</li>
<li>At optimality, these forces exactly cancel.</li>
</ul>
<p>Physically:</p>
<ul>
<li>Lagrange multipliers are “reaction forces’’ keeping a system on the constraint surface.</li>
<li>In economics, they are “shadow prices’’ indicating how much the objective would improve if a constraint were relaxed.</li>
<li>Geometrically, the stationarity condition means the objective and the active constraints share a supporting hyperplane at the optimum.</li>
</ul>
<p>KKT theory unifies all earlier ideas—convexity, gradients/subgradients, feasible regions, tangent and normal cones—into one clean, general optimality framework.</p></body></html></section><section class="print-page" id="convex-18_duality" heading-number="2.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-9-lagrange-duality-theory">Chapter 9: Lagrange Duality Theory<a class="headerlink" href="#convex-18_duality-chapter-9-lagrange-duality-theory" title="Permanent link">¶</a></h1>
<p>Duality is one of the central organizing principles in convex optimization. Every constrained problem (the primal) has an associated dual problem, whose structure often provides:</p>
<ul>
<li>lower bounds on the primal optimal value,</li>
<li>certificates of optimality,</li>
<li>interpretations of constraint “prices,”</li>
<li>and alternative algorithmic routes to solutions.</li>
</ul>
<p>In convex optimization, duality is especially powerful: under mild conditions, the primal and dual attain the same optimal value. This equality — <em>strong duality</em> — lies behind the theory of KKT conditions, interior-point methods, and many ML algorithms such as SVMs.</p>
<h2 id="convex-18_duality-91-the-primal-problem">9.1 The Primal Problem<a class="headerlink" href="#convex-18_duality-91-the-primal-problem" title="Permanent link">¶</a></h2>
<p>Consider the general convex problem</p>
<div class="arithmatex">\[
\begin{array}{ll}
\text{minimize} &amp; f(x) \\
\text{subject to} &amp; g_i(x) \le 0,\quad i=1,\dots,m, \\
 &amp; h_j(x) = 0,\quad j=1,\dots,p,
\end{array}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> and each <span class="arithmatex">\(g_i\)</span> are convex,</li>
<li>each equality constraint <span class="arithmatex">\(h_j\)</span> is affine.</li>
</ul>
<p>The optimal value is</p>
<div class="arithmatex">\[
f^\star = \inf\{ f(x) : g_i(x) \le 0,\ h_j(x)=0 \}.
\]</div>
<p>The infimum allows for the possibility that the best value is approached but not attained.</p>
<h2 id="convex-18_duality-92-why-duality">9.2 Why Duality?<a class="headerlink" href="#convex-18_duality-92-why-duality" title="Permanent link">¶</a></h2>
<p>A constrained problem can be viewed as:</p>
<blockquote>
<p>minimize <span class="arithmatex">\(f(x)\)</span> but pay a penalty whenever constraints are violated.</p>
</blockquote>
<p>If the penalties are chosen “correctly,” one can recover the original constrained problem from an unconstrained penalized problem. Dual variables — <span class="arithmatex">\(\mu_i\)</span> for inequalities and <span class="arithmatex">\(\lambda_j\)</span> for equalities — precisely encode these penalties:</p>
<ul>
<li><span class="arithmatex">\(\mu_i\)</span> measures how costly it is to violate <span class="arithmatex">\(g_i(x)\le 0\)</span>,</li>
<li><span class="arithmatex">\(\lambda_j\)</span> measures the sensitivity of the objective to relaxing <span class="arithmatex">\(h_j(x)=0\)</span>.</li>
</ul>
<p>Duality converts constraints into prices, and transforms geometry into algebra.</p>
<h2 id="convex-18_duality-93-the-lagrangian">9.3 The Lagrangian<a class="headerlink" href="#convex-18_duality-93-the-lagrangian" title="Permanent link">¶</a></h2>
<p>The Lagrangian function is</p>
<div class="arithmatex">\[
L(x, \lambda, \mu)
= f(x) + \sum_{i=1}^m \mu_i g_i(x)
+ \sum_{j=1}^p \lambda_j h_j(x),
\]</div>
<p>with:</p>
<ul>
<li><span class="arithmatex">\(\mu_i \ge 0\)</span> for inequality constraints,</li>
<li><span class="arithmatex">\(\lambda_j \in \mathbb{R}\)</span> unrestricted for equalities.</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>If <span class="arithmatex">\(\mu_i &gt; 0\)</span>, violating <span class="arithmatex">\(g_i(x)\le 0\)</span> incurs a penalty proportional to <span class="arithmatex">\(\mu_i\)</span>.</li>
<li>If <span class="arithmatex">\(\mu_i = 0\)</span>, that constraint does not influence the Lagrangian at that point.</li>
</ul>
<h2 id="convex-18_duality-94-the-dual-function-lower-bounds-from-penalties">9.4 The Dual Function: Lower Bounds from Penalties<a class="headerlink" href="#convex-18_duality-94-the-dual-function-lower-bounds-from-penalties" title="Permanent link">¶</a></h2>
<p>Fix <span class="arithmatex">\((\lambda,\mu)\)</span> and minimize the Lagrangian with respect to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
\theta(\lambda, \mu) = \inf_x L(x,\lambda,\mu).
\]</div>
<p>Because <span class="arithmatex">\(g_i(x) \le 0\)</span> for feasible <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(\mu_i \ge 0\)</span>,</p>
<div class="arithmatex">\[
L(x,\lambda,\mu) \le f(x),
\]</div>
<p>so taking the infimum over all <span class="arithmatex">\(x\)</span> yields</p>
<div class="arithmatex">\[
\theta(\lambda,\mu) \le f^\star.
\]</div>
<p>Thus <span class="arithmatex">\(\theta\)</span> always produces lower bounds on the true optimal value (weak duality).</p>
<h3 id="convex-18_duality-properties-of-the-dual-function">Properties of the Dual Function<a class="headerlink" href="#convex-18_duality-properties-of-the-dual-function" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\theta(\lambda,\mu)\)</span> is always concave in <span class="arithmatex">\((\lambda,\mu)\)</span> (infimum of affine functions).</li>
<li>It may be <span class="arithmatex">\(-\infty\)</span> if the Lagrangian is unbounded below.</li>
</ul>
<h2 id="convex-18_duality-95-the-dual-problem">9.5 The Dual Problem<a class="headerlink" href="#convex-18_duality-95-the-dual-problem" title="Permanent link">¶</a></h2>
<p>The dual problem maximizes these lower bounds:</p>
<div class="arithmatex">\[
\begin{array}{ll}
\text{maximize}_{\lambda,\mu} &amp; \theta(\lambda,\mu) \\
\text{subject to} &amp; \mu \ge 0.
\end{array}
\]</div>
<p>Let <span class="arithmatex">\(d^\star\)</span> be the optimal dual value.<br>
Weak duality guarantees:</p>
<div class="arithmatex">\[
d^\star \le f^\star.
\]</div>
<p>The dual problem is always a concave maximization, i.e., a convex optimization problem in <span class="arithmatex">\((\lambda,\mu)\)</span>.</p>
<h2 id="convex-18_duality-96-strong-duality-and-the-duality-gap">9.6 Strong Duality and the Duality Gap<a class="headerlink" href="#convex-18_duality-96-strong-duality-and-the-duality-gap" title="Permanent link">¶</a></h2>
<p>If</p>
<div class="arithmatex">\[
d^\star = f^\star,
\]</div>
<p>we say strong duality holds. The duality gap is zero.</p>
<h3 id="convex-18_duality-slaters-condition">Slater’s Condition<a class="headerlink" href="#convex-18_duality-slaters-condition" title="Permanent link">¶</a></h3>
<p>If:</p>
<ul>
<li><span class="arithmatex">\(g_i\)</span> are convex,</li>
<li><span class="arithmatex">\(h_j\)</span> are affine,</li>
<li>and there exists a <span class="arithmatex">\(\tilde{x}\)</span> such that<br>
<script type="math/tex; mode=display">
  g_i(\tilde{x}) < 0,\quad h_j(\tilde{x}) = 0,
  </script>
</li>
</ul>
<p>then:</p>
<ul>
<li>strong duality holds (<span class="arithmatex">\(f^\star = d^\star\)</span>),</li>
<li>dual maximizers exist,</li>
<li>KKT conditions fully characterize primal–dual optimality.</li>
</ul>
<p>Slater’s condition ensures the feasible region has interior — the constraints are not tight everywhere.</p>
<h2 id="convex-18_duality-97-duality-and-the-kkt-conditions">9.7 Duality and the KKT Conditions<a class="headerlink" href="#convex-18_duality-97-duality-and-the-kkt-conditions" title="Permanent link">¶</a></h2>
<p>When strong duality holds, the primal and dual meet at a point satisfying the KKT conditions:</p>
<h3 id="convex-18_duality-1-primal-feasibility">1. Primal feasibility<a class="headerlink" href="#convex-18_duality-1-primal-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
g_i(x^\star) \le 0,\qquad h_j(x^\star)=0.
\]</div>
<h3 id="convex-18_duality-2-dual-feasibility">2. Dual feasibility<a class="headerlink" href="#convex-18_duality-2-dual-feasibility" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^\star \ge 0.
\]</div>
<h3 id="convex-18_duality-3-stationarity">3. Stationarity<a class="headerlink" href="#convex-18_duality-3-stationarity" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\nabla f(x^\star)
+ \sum_{i=1}^m \mu_i^\star \nabla g_i(x^\star)
+ \sum_{j=1}^p \lambda_j^\star \nabla h_j(x^\star)
= 0.
\]</div>
<h3 id="convex-18_duality-4-complementary-slackness">4. Complementary slackness<a class="headerlink" href="#convex-18_duality-4-complementary-slackness" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\mu_i^\star g_i(x^\star) = 0,\qquad \forall i.
\]</div>
<p>Together these conditions ensure:</p>
<div class="arithmatex">\[
f(x^\star) = \theta(\lambda^\star,\mu^\star)
= f^\star = d^\star.
\]</div>
<p>Geometrically, the gradients of the active constraints form a supporting hyperplane that “touches’’ the objective exactly at the optimum.</p>
<h2 id="convex-18_duality-98-interpretation-of-dual-variables">9.8 Interpretation of Dual Variables<a class="headerlink" href="#convex-18_duality-98-interpretation-of-dual-variables" title="Permanent link">¶</a></h2>
<p>Dual variables have consistent interpretations across optimization, ML, and economics.</p>
<h3 id="convex-18_duality-shadow-prices-constraint-forces">Shadow Prices / Constraint Forces<a class="headerlink" href="#convex-18_duality-shadow-prices-constraint-forces" title="Permanent link">¶</a></h3>
<ul>
<li>
<p><span class="arithmatex">\(\mu_i^\star\)</span>: the <em>shadow price</em> for relaxing <span class="arithmatex">\(g_i(x)\le 0\)</span>.<br>
  Large <span class="arithmatex">\(\mu_i^\star\)</span> means the constraint is tight and costly to relax.</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda_j^\star\)</span>: sensitivity of the optimal value to perturbations of <span class="arithmatex">\(h_j(x)=0\)</span>.</p>
</li>
</ul>
<h3 id="convex-18_duality-ml-interpretations">ML Interpretations<a class="headerlink" href="#convex-18_duality-ml-interpretations" title="Permanent link">¶</a></h3>
<ul>
<li>Support Vector Machines: dual variables select support vectors (only points with <span class="arithmatex">\(\mu_i^\star &gt; 0\)</span> matter).</li>
<li>L1-Regularization / Lasso: can be viewed through a dual constraint on parameter magnitudes.</li>
<li>Regularized learning problems: the dual expresses the balance between data fit and model complexity.</li>
</ul>
<p>Duality often reveals structure that is hidden in the primal, providing clearer geometric insight and sometimes simpler optimization paths.</p></body></html></section><section class="print-page" id="convex-18a_pareto" heading-number="2.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-10-multi-objective-convex-optimization">Chapter 10: Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-chapter-10-multi-objective-convex-optimization" title="Permanent link">¶</a></h1>
<p>Up to now we have focused on problems with a single objective: minimize one convex function over a convex set. However, real-world learning, engineering, and decision-making tasks almost always involve competing criteria:</p>
<ul>
<li>accuracy vs. regularity,</li>
<li>loss vs. fairness,</li>
<li>return vs. risk,</li>
<li>reconstruction vs. compression,</li>
<li>energy use vs. performance.</li>
</ul>
<p>Multi-objective optimization provides the mathematical framework for balancing such competing goals. In convex settings, these trade-offs have elegant geometric and analytic structure, captured by Pareto optimality and by scalarization techniques that convert multiple objectives into a single convex problem.</p>
<p>This chapter introduces these ideas and connects them to regularization, duality, and common ML formulations.</p>
<h2 id="convex-18a_pareto-101-classical-optimality-one-objective">10.1 Classical Optimality (One Objective)<a class="headerlink" href="#convex-18a_pareto-101-classical-optimality-one-objective" title="Permanent link">¶</a></h2>
<p>In standard convex optimization, we solve:</p>
<div class="arithmatex">\[
x^* \in \arg\min_{x \in \mathcal{X}} f(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is convex and <span class="arithmatex">\(\mathcal{X}\)</span> is convex.<br>
In this setting, it is natural to speak of the minimizer — or set of minimizers — because the task is governed by a single quantitative measure.</p>
<p>However, when multiple objectives <span class="arithmatex">\((f_1,\dots,f_k)\)</span> must be minimized simultaneously, a single “best” point usually does not exist.  Improving one objective can worsen another. Multi-objective optimization replaces the idea of a unique minimizer with the idea of efficient trade-offs.</p>
<h2 id="convex-18a_pareto-102-multi-objective-convex-optimization">10.2 Multi-Objective Convex Optimization<a class="headerlink" href="#convex-18a_pareto-102-multi-objective-convex-optimization" title="Permanent link">¶</a></h2>
<p>A multi-objective optimization problem takes the form</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} F(x) = (f_1(x), \dots, f_k(x)),
\]</div>
<p>where each <span class="arithmatex">\(f_i\)</span> is convex.<br>
This framework appears in many ML and statistical tasks:</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Objective 1</th>
<th>Objective 2</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression</td>
<td>Fit error</td>
<td>Regularization</td>
<td>Accuracy vs. complexity</td>
</tr>
<tr>
<td>Fair ML</td>
<td>Loss</td>
<td>Fairness metric</td>
<td>Utility vs. fairness</td>
</tr>
<tr>
<td>Portfolio</td>
<td>Return</td>
<td>Risk</td>
<td>Profit vs. stability</td>
</tr>
<tr>
<td>Autoencoders</td>
<td>Reconstruction</td>
<td>KL divergence</td>
<td>Fidelity vs. disentanglement</td>
</tr>
</tbody>
</table>
<p>Because objectives typically conflict, one cannot minimize all simultaneously. The natural notion of optimality becomes <em>Pareto efficiency</em>.</p>
<h2 id="convex-18a_pareto-103-pareto-optimality">10.3 Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-103-pareto-optimality" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-strong-pareto-optimality">Strong Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-strong-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^*\)</span> is Pareto optimal if there is no other <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[
f_i(x) \le f_i(x^*)\quad \forall i,
\]</div>
<p>with strict inequality for at least one objective. Thus, no trade-off-free improvement is possible: to improve one metric, you must worsen another.</p>
<h3 id="convex-18a_pareto-weak-pareto-optimality">Weak Pareto Optimality<a class="headerlink" href="#convex-18a_pareto-weak-pareto-optimality" title="Permanent link">¶</a></h3>
<p>A point <span class="arithmatex">\(x^*\)</span> is weakly Pareto optimal if no feasible point satisfies</p>
<div class="arithmatex">\[
f_i(x) &lt; f_i(x^*)\quad \forall i.
\]</div>
<p>Weak optimality rules out simultaneous strict improvement in all objectives.</p>
<h3 id="convex-18a_pareto-geometric-view">Geometric View<a class="headerlink" href="#convex-18a_pareto-geometric-view" title="Permanent link">¶</a></h3>
<p>For two objectives <span class="arithmatex">\((f_1, f_2)\)</span>, the feasible set in objective space is a region in <span class="arithmatex">\(\mathbb{R}^2\)</span>. Its lower-left boundary, the set of points not dominated by others, is the Pareto frontier.</p>
<ul>
<li>Points <em>on</em> the frontier are the best achievable trade-offs.</li>
<li>Points <em>above</em> or <em>inside</em> the region are dominated and thus suboptimal.</li>
</ul>
<p>The Pareto frontier explicitly exposes the structure of trade-offs in a problem.</p>
<h2 id="convex-18a_pareto-104-scalarization-turning-many-objectives-into-one">10.4 Scalarization: Turning Many Objectives into One<a class="headerlink" href="#convex-18a_pareto-104-scalarization-turning-many-objectives-into-one" title="Permanent link">¶</a></h2>
<p>Multi-objective problems rarely have a unique minimizer. Scalarization constructs a single-objective surrogate problem whose solutions lie on the Pareto frontier.</p>
<h3 id="convex-18a_pareto-weighted-sum-scalarization">Weighted-Sum Scalarization<a class="headerlink" href="#convex-18a_pareto-weighted-sum-scalarization" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} \sum_{i=1}^k w_i f_i(x),
\qquad w_i \ge 0,\quad \sum_i w_i = 1.
\]</div>
<ul>
<li>The weights encode relative importance.  </li>
<li>Varying <span class="arithmatex">\(w\)</span> traces (part of) the Pareto frontier.  </li>
<li>When <span class="arithmatex">\(f_i\)</span> and <span class="arithmatex">\(\mathcal{X}\)</span> are convex, this method recovers the convex portion of the frontier.</li>
</ul>
<h3 id="convex-18a_pareto--constraint-method">ε-Constraint Method<a class="headerlink" href="#convex-18a_pareto--constraint-method" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_{x} \ f_1(x)
\quad \text{s.t. } f_i(x) \le \varepsilon_i,\ \ i = 2,\dots,k.
\]</div>
<ul>
<li>Here the tolerances <span class="arithmatex">\(\varepsilon_i\)</span> act as performance budgets.  </li>
<li>Each choice of <span class="arithmatex">\(\varepsilon\)</span> yields a different Pareto-efficient point.</li>
</ul>
<p>This formulation directly highlights the trade-off between one primary objective and several secondary constraints.</p>
<h3 id="convex-18a_pareto-duality-connection">Duality Connection<a class="headerlink" href="#convex-18a_pareto-duality-connection" title="Permanent link">¶</a></h3>
<p>Scalarization has a tight relationship with duality (Chapter 9):</p>
<ul>
<li>Weights <span class="arithmatex">\(w_i\)</span> in a weighted sum act like dual variables.</li>
<li>Regularization parameters (e.g., the <span class="arithmatex">\(\lambda\)</span> in L2 or L1 regularization) correspond to dual multipliers.</li>
<li>Moving along <span class="arithmatex">\(\lambda\)</span> traces the Pareto frontier between data fit and model complexity.</li>
</ul>
<p>This connection explains why tuning regularization is equivalent to choosing a point on a trade-off curve.</p>
<h2 id="convex-18a_pareto-105-examples-and-applications">10.5 Examples and Applications<a class="headerlink" href="#convex-18a_pareto-105-examples-and-applications" title="Permanent link">¶</a></h2>
<h3 id="convex-18a_pareto-example-1-regularized-least-squares">Example 1: Regularized Least Squares<a class="headerlink" href="#convex-18a_pareto-example-1-regularized-least-squares" title="Permanent link">¶</a></h3>
<p>Consider</p>
<div class="arithmatex">\[
f_1(x) = \|Ax - b\|_2^2,\qquad 
f_2(x) = \|x\|_2^2.
\]</div>
<p>Two scalarizations:</p>
<ol>
<li>
<p>Weighted:
   <script type="math/tex; mode=display">
   \min_x \ \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
   </script>
</p>
</li>
<li>
<p>ε-constraint:
   <script type="math/tex; mode=display">
   \min_x \ \|Ax - b\|_2^2 \quad \text{s.t. } \|x\|_2^2 \le \tau.
   </script>
</p>
</li>
</ol>
<p><span class="arithmatex">\(\lambda\)</span> and <span class="arithmatex">\(\tau\)</span> trace the same Pareto curve — the classical bias–variance trade-off.</p>
<h3 id="convex-18a_pareto-example-2-portfolio-optimization-riskreturn">Example 2: Portfolio Optimization (Risk–Return)<a class="headerlink" href="#convex-18a_pareto-example-2-portfolio-optimization-riskreturn" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(w\)</span> be portfolio weights, <span class="arithmatex">\(\mu\)</span> expected returns, and <span class="arithmatex">\(\Sigma\)</span> the covariance matrix. Objectives:</p>
<div class="arithmatex">\[
f_1(w) = -\mu^\top w, \qquad
f_2(w) = w^\top \Sigma w.
\]</div>
<p>Weighted scalarization:</p>
<div class="arithmatex">\[
\min_w \ -\alpha \mu^\top w + (1-\alpha) w^\top \Sigma w,
\quad 0 \le \alpha \le 1.
\]</div>
<p>Varying <span class="arithmatex">\(\alpha\)</span> recovers the efficient frontier of Modern Portfolio Theory.</p>
<h3 id="convex-18a_pareto-example-3-fairnessaccuracy-in-ml">Example 3: Fairness–Accuracy in ML<a class="headerlink" href="#convex-18a_pareto-example-3-fairnessaccuracy-in-ml" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\min_\theta \ \mathbb{E}[\ell(y, f_\theta(x))]
\quad \text{s.t. } D(f_\theta(x),y) \le \varepsilon,
\]</div>
<p>where <span class="arithmatex">\(D\)</span> is a fairness metric.<br>
Scalarized form:</p>
<div class="arithmatex">\[
\min_\theta\  \mathbb{E}[\ell(y, f_\theta(x))] + \lambda D(f_\theta(x), y).
\]</div>
<p>Tuning <span class="arithmatex">\(\lambda\)</span> walks across the fairness–accuracy Pareto frontier.</p>
<h3 id="convex-18a_pareto-example-4-variational-autoencoders-and-vae">Example 4: Variational Autoencoders and β-VAE<a class="headerlink" href="#convex-18a_pareto-example-4-variational-autoencoders-and-vae" title="Permanent link">¶</a></h3>
<p>The ELBO is:</p>
<div class="arithmatex">\[
\mathbb{E}_{q(z)}[\log p(x|z)] - \mathrm{KL}(q(z)\|p(z)).
\]</div>
<p>Objectives:</p>
<ul>
<li>Reconstruction fidelity,</li>
<li>Latent simplicity.</li>
</ul>
<p>β-VAE scalarization:</p>
<div class="arithmatex">\[
\max_q \ \mathbb{E}[\log p(x|z)] - \beta \,\mathrm{KL}(q(z)\|p(z)).
\]</div>
<p><span class="arithmatex">\(\beta\)</span> controls the trade-off between reconstruction and disentanglement — a Pareto frontier in latent space.</p>
<p>Overall, multi-objective convex optimization extends the geometry and structure of convex analysis to settings with trade-offs and competing priorities. The Pareto frontier reveals the set of achievable compromises, while scalarization methods let us navigate this frontier using tools from single-objective convex optimization, duality, and regularization theory.</p></body></html></section><section class="print-page" id="convex-18b_regularization" heading-number="2.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-11-balancing-fit-and-complexity">Chapter 11:  Balancing Fit and Complexity<a class="headerlink" href="#convex-18b_regularization-chapter-11-balancing-fit-and-complexity" title="Permanent link">¶</a></h1>
<p>Most real-world learning and estimation problems must balance two competing goals:</p>
<ol>
<li>Fit the observed data well, and  </li>
<li>Control the complexity of the model to avoid overfitting, instability, or noise amplification.</li>
</ol>
<p>Regularization formalizes this trade-off by adding a convex penalty term to the objective. This chapter develops the structure, interpretation, and algorithms behind regularized convex problems, and shows how regularization corresponds directly to Pareto-optimal trade-offs (Chapter 10) between data fidelity and model simplicity.</p>
<h2 id="convex-18b_regularization-111-motivation-fit-vs-complexity">11.1 Motivation: Fit vs. Complexity<a class="headerlink" href="#convex-18b_regularization-111-motivation-fit-vs-complexity" title="Permanent link">¶</a></h2>
<p>Suppose we wish to estimate parameters <span class="arithmatex">\(x\)</span> from data via a loss function <span class="arithmatex">\(f(x)\)</span>. If the data are noisy or the model is high-dimensional, solutions minimizing <span class="arithmatex">\(f\)</span> alone may be unstable or overly complex. We introduce a regularizer <span class="arithmatex">\(R(x)\)</span>, typically convex, to encourage desirable structure:</p>
<div class="arithmatex">\[
\min_{x} \; f(x) + \lambda R(x), \qquad \lambda &gt; 0.
\]</div>
<ul>
<li><span class="arithmatex">\(f(x)\)</span>: measures data misfit (e.g., squared loss, logistic loss).  </li>
<li><span class="arithmatex">\(R(x)\)</span>: penalizes complexity (e.g., <span class="arithmatex">\(\ell_1\)</span> norm for sparsity, <span class="arithmatex">\(\ell_2\)</span> norm for smoothness).  </li>
<li><span class="arithmatex">\(\lambda\)</span>: controls the trade-off.<ul>
<li>Small <span class="arithmatex">\(\lambda\)</span>: excellent data fit, potentially overfitting.  </li>
<li>Large <span class="arithmatex">\(\lambda\)</span>: simpler model, potentially underfitting.</li>
</ul>
</li>
</ul>
<p>This is a scalarized multi-objective optimization problem of <span class="arithmatex">\((f, R)\)</span>.</p>
<h2 id="convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier">11.2 Bicriterion Optimization and the Pareto Frontier<a class="headerlink" href="#convex-18b_regularization-112-bicriterion-optimization-and-the-pareto-frontier" title="Permanent link">¶</a></h2>
<p>Regularization corresponds to the bicriterion objective:</p>
<div class="arithmatex">\[
\min_{x} \; (f(x), R(x)).
\]</div>
<p>A point <span class="arithmatex">\(x^*\)</span> is Pareto optimal if there is no feasible <span class="arithmatex">\(x\)</span> such that:
<script type="math/tex; mode=display">
f(x) \le f(x^*),\quad R(x) \le R(x^*),
</script>
with strict inequality in at least one component.</p>
<p>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(R\)</span>:</p>
<ul>
<li>Every <span class="arithmatex">\(\lambda \ge 0\)</span> yields a Pareto-optimal point,</li>
<li>The mapping from <span class="arithmatex">\(\lambda\)</span> to constraint level <span class="arithmatex">\(R(x^*)\)</span> is monotone,</li>
<li>The Pareto frontier is convex and can be traced continuously by varying <span class="arithmatex">\(\lambda\)</span>.</li>
</ul>
<p>Thus, tuning <span class="arithmatex">\(\lambda\)</span> moves the solution along the fit–complexity frontier.</p>
<h2 id="convex-18b_regularization-113-why-control-the-size-of-the-solution">11.3 Why Control the Size of the Solution?<a class="headerlink" href="#convex-18b_regularization-113-why-control-the-size-of-the-solution" title="Permanent link">¶</a></h2>
<p>Inverse problems such as <span class="arithmatex">\(Ax \approx b\)</span> are often ill-posed or ill-conditioned:</p>
<ul>
<li>Small noise in <span class="arithmatex">\(b\)</span> may cause large variability in the solution <span class="arithmatex">\(x\)</span>.  </li>
<li>If <span class="arithmatex">\(A\)</span> is rank-deficient or nearly singular, infinitely many solutions exist.</li>
</ul>
<p>Example: ridge regression</p>
<div class="arithmatex">\[
\min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
\]</div>
<p>The optimality condition is</p>
<div class="arithmatex">\[
(A^\top A + \lambda I)x = A^\top b.
\]</div>
<p>Benefits of L2 regularization:</p>
<ul>
<li><span class="arithmatex">\(A^\top A + \lambda I\)</span> becomes positive definite for any <span class="arithmatex">\(\lambda &gt; 0\)</span>,  </li>
<li>the solution becomes unique and stable,  </li>
<li>small singular directions of <span class="arithmatex">\(A\)</span> are suppressed.</li>
</ul>
<p>Interpretation: Regularization trades variance for stability by damping directions in which the data provide little information.</p>
<h2 id="convex-18b_regularization-114-constrained-vs-penalized-formulations">11.4 Constrained vs. Penalized Formulations<a class="headerlink" href="#convex-18b_regularization-114-constrained-vs-penalized-formulations" title="Permanent link">¶</a></h2>
<p>Regularized problems can be expressed equivalently as constrained problems:</p>
<div class="arithmatex">\[
\min_x f(x) 
\quad \text{s.t. } R(x) \le t.
\]</div>
<p>The Lagrangian is</p>
<div class="arithmatex">\[
\mathcal{L}(x,\lambda)
= f(x) + \lambda (R(x) - t),
\qquad \lambda \ge 0.
\]</div>
<p>The penalized form</p>
<div class="arithmatex">\[
\min_x f(x) + \lambda R(x)
\]</div>
<p>is the dual of the constrained form. Under convexity and Slater’s condition, the two forms yield the same set of optimal solutions. The corresponding KKT conditions are:</p>
<div class="arithmatex">\[
0 \in \partial f(x^*) + \lambda^* \partial R(x^*), 
\]</div>
<div class="arithmatex">\[
R(x^*) \le t,\qquad \lambda^* \ge 0,\qquad \lambda^*(R(x^*) - t) = 0.
\]</div>
<p>Here:</p>
<ul>
<li>If <span class="arithmatex">\(R(x^*) &lt; t\)</span>, then <span class="arithmatex">\(\lambda^* = 0\)</span>.  </li>
<li>If <span class="arithmatex">\(\lambda^* &gt; 0\)</span>, then <span class="arithmatex">\(R(x^*) = t\)</span> (constraint active).</li>
</ul>
<p>Thus <span class="arithmatex">\(\lambda\)</span> is the Lagrange multiplier controlling the slope of the Pareto frontier.</p>
<h2 id="convex-18b_regularization-115-common-regularizers-and-their-effects">11.5 Common Regularizers and Their Effects<a class="headerlink" href="#convex-18b_regularization-115-common-regularizers-and-their-effects" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-l2-regularization-ridge">(a) L2 Regularization (Ridge)<a class="headerlink" href="#convex-18b_regularization-a-l2-regularization-ridge" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \|x\|_2^2.
\]</div>
<ul>
<li>Smooth and strongly convex.  </li>
<li>Shrinks coefficients uniformly.  </li>
<li>Improves conditioning.  </li>
<li>MAP interpretation: Gaussian prior <span class="arithmatex">\(x \sim \mathcal{N}(0,\tau^2 I)\)</span>.</li>
</ul>
<h3 id="convex-18b_regularization-b-l1-regularization-lasso">(b) L1 Regularization (Lasso)<a class="headerlink" href="#convex-18b_regularization-b-l1-regularization-lasso" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \|x\|_1 = \sum_i |x_i|.
\]</div>
<ul>
<li>Convex but not differentiable → promotes sparsity.  </li>
<li>The <span class="arithmatex">\(\ell_1\)</span> ball has corners aligned with coordinate axes, encouraging zeros in <span class="arithmatex">\(x\)</span>.  </li>
<li>Proximal operator (soft-thresholding):</li>
</ul>
<div class="arithmatex">\[
\operatorname{prox}_{\tau\|\cdot\|_1}(v)
= \operatorname{sign}(v)\,\max(|v|-\tau, 0).
\]</div>
<ul>
<li>MAP interpretation: Laplace prior.</li>
</ul>
<h3 id="convex-18b_regularization-c-elastic-net">(c) Elastic Net<a class="headerlink" href="#convex-18b_regularization-c-elastic-net" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
R(x) = \alpha \|x\|_1 + (1-\alpha)\|x\|_2^2.
\]</div>
<ul>
<li>Combines sparsity with numerical stability.  </li>
<li>Useful with correlated features.</li>
</ul>
<h3 id="convex-18b_regularization-d-beyond-l1l2-structured-regularizers">(d) Beyond L1/L2: Structured Regularizers<a class="headerlink" href="#convex-18b_regularization-d-beyond-l1l2-structured-regularizers" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Regularizer</th>
<th>Formula</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tikhonov</td>
<td><span class="arithmatex">\(\|Lx\|_2^2\)</span></td>
<td>smoothness via operator <span class="arithmatex">\(L\)</span></td>
</tr>
<tr>
<td>Total Variation</td>
<td><span class="arithmatex">\(\|\nabla x\|_1\)</span></td>
<td>piecewise-constant signals/images</td>
</tr>
<tr>
<td>Group Lasso</td>
<td><span class="arithmatex">\(\sum_g \|x_g\|_2\)</span></td>
<td>structured sparsity across groups</td>
</tr>
<tr>
<td>Nuclear Norm</td>
<td><span class="arithmatex">\(\|X\|_* = \sum_i \sigma_i\)</span></td>
<td>low-rank matrices</td>
</tr>
</tbody>
</table>
<p>Each regularizer defines a geometry for the solution — ellipsoids, diamonds, polytopes, or spectral shapes.</p>
<h2 id="convex-18b_regularization-116-choosing-the-regularization-parameter-lambda">11.6 Choosing the Regularization Parameter <span class="arithmatex">\(\lambda\)</span><a class="headerlink" href="#convex-18b_regularization-116-choosing-the-regularization-parameter-lambda" title="Permanent link">¶</a></h2>
<h3 id="convex-18b_regularization-a-trade-off-behavior">(a) Trade-Off Behavior<a class="headerlink" href="#convex-18b_regularization-a-trade-off-behavior" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\lambda \downarrow\)</span>: favors small training error, high variance.  </li>
<li><span class="arithmatex">\(\lambda \uparrow\)</span>: favors simplicity, higher bias.  </li>
</ul>
<p><span class="arithmatex">\(\lambda\)</span> selects a point on the fit–complexity Pareto frontier.</p>
<h3 id="convex-18b_regularization-b-cross-validation">(b) Cross-Validation<a class="headerlink" href="#convex-18b_regularization-b-cross-validation" title="Permanent link">¶</a></h3>
<p>The most common practice:</p>
<ol>
<li>Split data into folds.  </li>
<li>Train on <span class="arithmatex">\(k-1\)</span> folds, validate on the remaining fold.  </li>
<li>Choose <span class="arithmatex">\(\lambda\)</span> minimizing average validation error.</li>
</ol>
<p>Guidelines:</p>
<ul>
<li>Standardize features for L1/Elastic Net.  </li>
<li>Use time-aware CV for dependent data.  </li>
<li>Use the “one-standard-error rule” for simpler models.</li>
</ul>
<h3 id="convex-18b_regularization-c-other-selection-methods">(c) Other Selection Methods<a class="headerlink" href="#convex-18b_regularization-c-other-selection-methods" title="Permanent link">¶</a></h3>
<ul>
<li>Information criteria (AIC, BIC) for sparsity.  </li>
<li>L-curve or discrepancy principle in inverse problems.  </li>
<li>Regularization paths: computing <span class="arithmatex">\(x^*(\lambda)\)</span> for many <span class="arithmatex">\(\lambda\)</span>.</li>
</ul>
<h2 id="convex-18b_regularization-117-algorithmic-view">11.7 Algorithmic View<a class="headerlink" href="#convex-18b_regularization-117-algorithmic-view" title="Permanent link">¶</a></h2>
<p>Most regularized problems have the form:</p>
<div class="arithmatex">\[
\min_x \ f(x) + R(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is smooth convex and <span class="arithmatex">\(R\)</span> is convex (possibly nonsmooth).</p>
<p>Common algorithms:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Idea</th>
<th>When Useful</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA/FISTA)</td>
<td>Gradient step on <span class="arithmatex">\(f\)</span>, proximal step on <span class="arithmatex">\(R\)</span></td>
<td>L1, TV, nuclear norm</td>
</tr>
<tr>
<td>Coordinate Descent</td>
<td>Update coordinates cyclically</td>
<td>Lasso, Elastic Net</td>
</tr>
<tr>
<td>ADMM</td>
<td>Split problem to exploit structure</td>
<td>Large-scale or distributed settings</td>
</tr>
</tbody>
</table>
<p>Proximal operators allow efficient handling of nonsmooth penalties. FISTA achieves optimal <span class="arithmatex">\(O(1/k^2)\)</span> rate for smooth+convex problems.</p>
<h2 id="convex-18b_regularization-118-bayesian-interpretation">11.8 Bayesian Interpretation<a class="headerlink" href="#convex-18b_regularization-118-bayesian-interpretation" title="Permanent link">¶</a></h2>
<p>Regularization corresponds to MAP (maximum a posteriori) inference.</p>
<p>Linear model:</p>
<div class="arithmatex">\[
b = Ax + \varepsilon,\qquad \varepsilon \sim \mathcal{N}(0,\sigma^2 I).
\]</div>
<p>With prior <span class="arithmatex">\(x \sim p(x)\)</span>, MAP estimation solves:</p>
<div class="arithmatex">\[
\min_x \ \frac{1}{2\sigma^2}\|Ax - b\|_2^2 - \log p(x).
\]</div>
<p>Examples:</p>
<ul>
<li>Gaussian prior <span class="arithmatex">\(p(x) \propto e^{-\|x\|_2^2 / (2\tau^2)}\)</span><br>
  → L2 penalty with <span class="arithmatex">\(\lambda = \sigma^2/(2\tau^2)\)</span>.  </li>
<li>Laplace prior<br>
  → L1 penalty and sparse MAP estimate.</li>
</ul>
<p>Thus regularization is prior information: it encodes assumptions about structure, smoothness, or sparsity before observing data.</p>
<p>Regularization is therefore a unifying concept in optimization, statistics, and machine learning:  it stabilizes ill-posed problems, enforces structure, and represents explicit choices on the Pareto frontier between data fit and complexity.</p></body></html></section><section class="print-page" id="convex-19_optimizationalgo" heading-number="2.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-12-algorithms-for-convex-optimization">Chapter 12: Algorithms for Convex Optimization<a class="headerlink" href="#convex-19_optimizationalgo-chapter-12-algorithms-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>In the previous chapters, we built the mathematical foundations of convex optimization: convex sets, convex functions, gradients, subgradients, KKT conditions, and duality. Now we answer the practical question: How do we actually solve convex optimization problems in practice?</p>
<p>This chapter now serves as the algorithmic backbone of the book. It bridges theoretical convex analysis (Chapters 3–11) with the practical numerical methods that solve those problems. Each algorithm here can be seen as a computational lens on a convex geometry concept — gradients as supporting planes, Hessians as curvature maps, and proximal maps as projection operators. Later chapters (13–15) extend these ideas to constrained, stochastic, and large-scale environments.</p>
<h2 id="convex-19_optimizationalgo-121-problem-classes-vs-method-classes">12.1 Problem classes vs method classes<a class="headerlink" href="#convex-19_optimizationalgo-121-problem-classes-vs-method-classes" title="Permanent link">¶</a></h2>
<p>Different convex problems call for different algorithmic structures. Here is the broad landscape:</p>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Formulation</th>
<th>Representative Methods</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth, unconstrained</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span>, convex and differentiable</td>
<td>Gradient descent, Accelerated gradient, Newton</td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Smooth with simple constraints</td>
<td><span class="arithmatex">\(\min_x f(x)\)</span> s.t. <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex)</td>
<td>Projected gradient</td>
<td>Constrained regression, probability simplex</td>
</tr>
<tr>
<td>Composite convex (smooth + nonsmooth)</td>
<td><span class="arithmatex">\(\min_x f(x) + R(x)\)</span></td>
<td>Proximal gradient, coordinate descent</td>
<td>Lasso, Elastic Net, TV minimization</td>
</tr>
<tr>
<td>General constrained convex</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(g_i(x) \le 0, h_j(x)=0\)</span></td>
<td>Interior-point, primal–dual methods</td>
<td>LP, QP, SDP, SOCP</td>
</tr>
</tbody>
</table>
<h2 id="convex-19_optimizationalgo-122-first-order-methods-gradient-descent">12.2 First-order methods: Gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-122-first-order-methods-gradient-descent" title="Permanent link">¶</a></h2>
<p>We solve
<script type="math/tex; mode=display">
\min_x f(x),
</script>
where <span class="arithmatex">\(f\)</span> is convex, differentiable, and (ideally) <span class="arithmatex">\(L\)</span>-smooth: its gradient is Lipschitz with constant <span class="arithmatex">\(L\)</span>, meaning
<script type="math/tex; mode=display">
\|\nabla f(x) - \nabla f(y)\|_2 \le L \|x-y\|_2 \quad \text{for all } x,y.
</script>
</p>
<blockquote>
<p>Smoothness lets us control step sizes.</p>
</blockquote>
<p>Gradient descent iterates
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k),
</script>
where <span class="arithmatex">\(\alpha_k&gt;0\)</span> is the step size (also called learning rate in machine learning). Typical choices:</p>
<ul>
<li>constant <span class="arithmatex">\(\alpha_k = 1/L\)</span> when <span class="arithmatex">\(L\)</span> is known,</li>
<li>backtracking line search when <span class="arithmatex">\(L\)</span> is unknown,</li>
<li>diminishing step sizes in some settings.</li>
</ul>
<blockquote>
<p>Derivation: </p>
<p>Around <span class="arithmatex">\(x_t\)</span>, we can approximate <span class="arithmatex">\(f\)</span> using its Taylor expansion:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle.
\]</div>
<p>We assume <span class="arithmatex">\(f\)</span> behaves approximately like its tangent plane near <span class="arithmatex">\(x_t\)</span>.  But tf we were to minimize just this linear model, we would move infinitely far in the direction of steepest descent <span class="arithmatex">\(-\nabla f(x_t)\)</span>, which is not realistic or stable. This motivates adding a locality restriction: we trust the linear approximation near <span class="arithmatex">\(x_t\)</span>, not globally. To prevent taking arbitrarily large steps, we add a quadratic penalty for moving away from <span class="arithmatex">\(x_t\)</span>:</p>
<div class="arithmatex">\[
f(x) \approx f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2,
\]</div>
<p>where <span class="arithmatex">\(\eta &gt; 0\)</span> is the learning rate or step size.</p>
<ul>
<li>The linear term pulls <span class="arithmatex">\(x\)</span> in the steepest descent direction.</li>
<li>The quadratic term acts like a trust region, discouraging large deviations from <span class="arithmatex">\(x_t\)</span>.</li>
<li><span class="arithmatex">\(\eta\)</span> trades off aggressive progress vs stability:<ul>
<li>Small <span class="arithmatex">\(\eta\)</span> → cautious updates.</li>
<li>Large <span class="arithmatex">\(\eta\)</span> → bold updates (risk of divergence).</li>
</ul>
</li>
</ul>
<p>We define the next iterate as the minimizer of the surrogate objective:</p>
<div class="arithmatex">\[
x_{t+1} = \arg\min_{x \in \mathcal{X}} \Big[ f(x_t) + \langle \nabla f(x_t), x - x_t \rangle + \frac{1}{2\eta} \|x - x_t\|^2 \Big].
\]</div>
<p>Ignoring the constant term <span class="arithmatex">\(f(x_t)\)</span> and differentiating w.r.t. <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
\nabla f(x_t) + \frac{1}{\eta}(x - x_t) = 0
\]</div>
<p>Solving:</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t)
\]</div>
</blockquote>
<p>Convergence: For convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(f\)</span>, gradient descent with a suitable fixed step size satisfies
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k}\right),
</script>
where <span class="arithmatex">\(f^\star\)</span> is the global minimum. This <span class="arithmatex">\(O(1/k)\)</span> sublinear rate is slow compared to second-order methods, but each step is extremely cheap: you only need <span class="arithmatex">\(\nabla f(x_k)\)</span>.</p>
<p>When to use gradient descent:</p>
<ul>
<li>High-dimensional smooth convex problems (e.g. large-scale logistic regression).</li>
<li>You can compute gradients cheaply.</li>
<li>You only need moderate accuracy.</li>
<li>Memory constraints rule out storing or factoring Hessians.</li>
</ul>
<h2 id="convex-19_optimizationalgo-123-accelerated-first-order-methods">12.3 Accelerated first-order methods<a class="headerlink" href="#convex-19_optimizationalgo-123-accelerated-first-order-methods" title="Permanent link">¶</a></h2>
<p>Plain gradient descent has an <span class="arithmatex">\(O(1/k)\)</span> rate for smooth convex problems. Remarkably, we can do better — and in fact, provably optimal — by adding <em>momentum</em>.</p>
<h3 id="convex-19_optimizationalgo-1231-nesterov-acceleration">12.3.1 Nesterov acceleration<a class="headerlink" href="#convex-19_optimizationalgo-1231-nesterov-acceleration" title="Permanent link">¶</a></h3>
<p>Nesterov’s accelerated gradient method modifies the update using a momentum-like extrapolation. One common form of Nesterov acceleration uses two sequences <span class="arithmatex">\(x_k\)</span> and <span class="arithmatex">\(y_k\)</span>:</p>
<ol>
<li>Maintain two sequences <span class="arithmatex">\(x_k\)</span> and <span class="arithmatex">\(y_k\)</span>.</li>
<li>Take a gradient step from <span class="arithmatex">\(y_k\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = y_k - \alpha \nabla f(y_k).
   </script>
</li>
<li>Extrapolate:
   <script type="math/tex; mode=display">
   y_{k+1} = x_{k+1} + \beta_k (x_{k+1} - x_k).
   </script>
</li>
</ol>
<p>The extra momentum term <span class="arithmatex">\(\beta_k (x_{k+1}-x_k)\)</span> uses past iterates to “look ahead” and can significantly accelerate convergence.</p>
<p>Convergece: For smooth convex <span class="arithmatex">\(f\)</span>, accelerated gradient achieves
<script type="math/tex; mode=display">
f(x_k) - f^\star = O\!\left(\frac{1}{k^2}\right),
</script>
which is <em>optimal</em> for any algorithm that uses only gradient information and not higher derivatives.</p>
<ul>
<li>Acceleration is effective for well-behaved smooth convex problems.</li>
<li>It can be more sensitive to step size and noise than plain gradient descent.</li>
<li>Variants such as FISTA apply acceleration in the composite setting <span class="arithmatex">\(f + R\)</span>.</li>
</ul>
<blockquote>
<p>The convergence of gradient descent depends strongly on the geometry of the level sets of the objective function. When these level sets are poorly conditioned—that is, highly anisotropic or elongated (not spherical) the gradient directions tend to oscillate across narrow valleys, leading to zig-zag behavior and slow convergence. In contrast, when the level sets are well-conditioned (approximately spherical), gradient descent progresses efficiently toward the minimum. Thus, the efficiency of gradient-based methods is governed by how aspherical (anisotropic) the level sets are, which is directly related to the condition number of the Hessian.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-124-steepest-descent-method">12.4 Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-124-steepest-descent-method" title="Permanent link">¶</a></h2>
<p>The steepest descent method generalizes gradient descent by depending on the choice of norm used to measure step size or direction. It finds the direction of <em>maximum decrease</em> of the objective function under a unit norm constraint.</p>
<blockquote>
<p>The norm defines the “geometry” of optimization.cGradient descent is steepest descent under the Euclidean norm. Changing the norm changes what “steepest” means, and can greatly affect convergence, especially for ill-conditioned or anisotropic problems.The norm in steepest descent determines the geometry of the descent and choosing an appropriate norm effectively makes the level sets of the function more rounded (more isotropic), which greatly improves convergence.</p>
</blockquote>
<p>At a point <span class="arithmatex">\(x\)</span>, and for a chosen norm <span class="arithmatex">\(|\cdot|\)</span>:</p>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = \arg\min_{|v| = 1} \nabla f(x)^T v
\]</div>
<p>This defines the normalized steepest descent direction — the unit-norm direction that yields the most negative directional derivative (i.e., the steepest local decrease of <span class="arithmatex">\(f\)</span>).</p>
<ul>
<li><span class="arithmatex">\(\Delta x_{\text{nsd}}\)</span>: normalized steepest descent direction</li>
<li><span class="arithmatex">\(\Delta x_{\text{sd}}\)</span>: unnormalized direction (scaled by the gradient norm)</li>
</ul>
<p>For small steps <span class="arithmatex">\(v\)</span>,
<script type="math/tex; mode=display">
f(x + v) \approx f(x) + \nabla f(x)^T v.
</script>
The term <span class="arithmatex">\(\nabla f(x)^T v\)</span> describes how fast <span class="arithmatex">\(f\)</span> increases in direction <span class="arithmatex">\(v\)</span>.
To decrease <span class="arithmatex">\(f\)</span> most rapidly, we pick <span class="arithmatex">\(v\)</span> that minimizes this inner product — subject to <span class="arithmatex">\(|v| = 1\)</span>.</p>
<ul>
<li>The result depends on which norm we use to measure the “size” of <span class="arithmatex">\(v\)</span>.</li>
<li>The corresponding dual norm <span class="arithmatex">\(|\cdot|_*\)</span> determines how we measure the gradient’s magnitude.</li>
</ul>
<p>Thus, the steepest descent direction always aligns with the negative gradient, but it is scaled and shaped according to the geometry induced by the chosen norm.</p>
<h2 id="convex-19_optimizationalgo-1241-mathematical-properties">12.4.1. Mathematical Properties<a class="headerlink" href="#convex-19_optimizationalgo-1241-mathematical-properties" title="Permanent link">¶</a></h2>
<h3 id="convex-19_optimizationalgo-a-normalized-direction">(a) Normalized direction<a class="headerlink" href="#convex-19_optimizationalgo-a-normalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{nsd}} = \arg\min_{|v|=1} \nabla f(x)^T v
</script>
→ unit vector with the most negative directional derivative.</p>
<h3 id="convex-19_optimizationalgo-b-unnormalized-direction">(b) Unnormalized direction<a class="headerlink" href="#convex-19_optimizationalgo-b-unnormalized-direction" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\Delta x_{\text{sd}} = |\nabla f(x)| , \Delta x*{\text{nsd}}
</script>
This gives the actual direction and magnitude used in updates.</p>
<h3 id="convex-19_optimizationalgo-c-key-identity">(c) Key identity<a class="headerlink" href="#convex-19_optimizationalgo-c-key-identity" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
\nabla f(x)^T \Delta x_{\text{sd}} = -|\nabla f(x)|_*^2
</script>
The directional derivative equals the negative squared dual norm of the gradient.</p>
<h3 id="convex-19_optimizationalgo-1242-the-steepest-descent-method">12.4.2. The Steepest Descent Method<a class="headerlink" href="#convex-19_optimizationalgo-1242-the-steepest-descent-method" title="Permanent link">¶</a></h3>
<p>The iterative update rule is:
<script type="math/tex; mode=display">
x_{k+1} = x_k + t_k , \Delta x_{\text{sd}},
</script>
where <span class="arithmatex">\(t_k &gt; 0\)</span> is a step size (from line search or a fixed rule).</p>
<ul>
<li>For the Euclidean norm, this reduces to ordinary gradient descent.</li>
<li>For other norms, it adapts the search direction to the geometry of the problem.</li>
</ul>
<p>Convergence: Similar to gradient descent — linear for general convex functions, potentially faster when level sets are well-conditioned.</p>
<h3 id="convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence">12.4.3. Role of the Norm and Its Influence<a class="headerlink" href="#convex-19_optimizationalgo-1243-role-of-the-norm-and-its-influence" title="Permanent link">¶</a></h3>
<p>The choice of norm determines:</p>
<ol>
<li>The shape of the unit ball <span class="arithmatex">\({v : |v| \le 1}\)</span>,</li>
<li>The direction of steepest descent, since the minimization is constrained by that shape,</li>
<li>The dual norm <span class="arithmatex">\(|\nabla f(x)|_*\)</span> that measures the gradient’s size.</li>
</ol>
<p>Different norms yield different “geometries” of descent:</p>
<table>
<thead>
<tr>
<th>Norm</th>
<th>Unit Ball Shape</th>
<th>Dual Norm</th>
<th>Effect on Direction</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Circle / sphere</td>
<td><span class="arithmatex">\(\ell_2\)</span></td>
<td>Direction is opposite to gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Diamond</td>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Moves along coordinate of largest gradient</td>
</tr>
<tr>
<td><span class="arithmatex">\(\ell_\infty\)</span></td>
<td>Square</td>
<td><span class="arithmatex">\(\ell_1\)</span></td>
<td>Moves opposite to sum of all gradient signs</td>
</tr>
<tr>
<td>Quadratic <span class="arithmatex">\((x^T P x)^{1/2}\)</span></td>
<td>Ellipsoid</td>
<td>Weighted <span class="arithmatex">\(\ell_2\)</span></td>
<td>Scales direction by preconditioner <span class="arithmatex">\(P^{-1}\)</span></td>
</tr>
</tbody>
</table>
<p>Thus, the norm defines how “distance” and “steepness” are perceived, shaping how the algorithm moves through the landscape of <span class="arithmatex">\(f(x)\)</span>.</p>
<h3 id="convex-19_optimizationalgo-a-euclidean-norm-v_2">(a) Euclidean Norm <span class="arithmatex">\(|v|_2\)</span><a class="headerlink" href="#convex-19_optimizationalgo-a-euclidean-norm-v_2" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -\frac{\nabla f(x)}{|\nabla f(x)|*2},
\quad
\Delta x*{\text{sd}} = -\nabla f(x)
\]</div>
<p>This is standard gradient descent.
The direction is exactly opposite the gradient, and steps are isotropic (same scaling in all directions).</p>
<h3 id="convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0">(b) Quadratic Norm <span class="arithmatex">\(|v|_P = (v^T P v)^{1/2}\)</span>, with <span class="arithmatex">\(P \succ 0\)</span><a class="headerlink" href="#convex-19_optimizationalgo-b-quadratic-norm-v_p-vt-p-v12-with-p-succ-0" title="Permanent link">¶</a></h3>
<p>Here, <span class="arithmatex">\(P\)</span> defines an ellipsoidal metric.
The dual norm is <span class="arithmatex">\(|y|_* = (y^T P^{-1} y)^{1/2}\)</span>.</p>
<div class="arithmatex">\[
\Delta x_{\text{sd}} = -P^{-1}\nabla f(x)
\]</div>
<p>This corresponds to preconditioned gradient descent, where <span class="arithmatex">\(P\)</span> rescales directions to counter anisotropy in level sets.</p>
<p>Interpretation:</p>
<ul>
<li>If <span class="arithmatex">\(P\)</span> approximates the Hessian, this becomes Newton’s method.</li>
<li>If <span class="arithmatex">\(P\)</span> is diagonal, it acts like an adaptive step size per coordinate.</li>
</ul>
<h3 id="convex-19_optimizationalgo-c-ell_1-norm">(c) <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#convex-19_optimizationalgo-c-ell_1-norm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\Delta x_{\text{nsd}} = -e_i, \quad i = \arg\max_j \left|\frac{\partial f}{\partial x_j}\right|
\]</div>
<p>and</p>
<div class="arithmatex">\[
\Delta x_{\text{sd}} = -|\nabla f(x)|_\infty e_i
\]</div>
<p>The step moves along the coordinate with the largest gradient component, resembling a coordinate descent update.</p>
<p>Geometric intuition:
The <span class="arithmatex">\(\ell_1\)</span>-unit ball is a diamond; its corners align with coordinate axes, so the steepest direction is along one axis at a time.</p>
<ul>
<li>In <span class="arithmatex">\(\ell_2\)</span>-norm: the unit ball is a circle → the steepest direction is exactly opposite the gradient.</li>
<li>In <span class="arithmatex">\(\ell_1\)</span>-norm: the unit ball is a diamond → the steepest direction points to a corner (one coordinate).</li>
<li>In quadratic norms: the unit ball is an ellipsoid → the steepest direction follows the metric-adjusted gradient.</li>
</ul>
<p>Hence, the norm defines the geometry of what “steepest” means.</p>
<h2 id="convex-19_optimizationalgo-125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives">12.5 Conjugate Gradient Method — Fast Optimization for Quadratic Objectives<a class="headerlink" href="#convex-19_optimizationalgo-125-conjugate-gradient-method-fast-optimization-for-quadratic-objectives" title="Permanent link">¶</a></h2>
<p>Gradient descent can be painfully slow when the level sets of the objective are long and skinny an indication that the Hessian has very different curvature in different directions (poor conditioning). The Conjugate Gradient (CG) method fixes this without forming or inverting the Hessian. It exploits the exact structure of quadratic functions to build advanced search directions that incorporate curvature information at almost no extra cost.</p>
<p>CG is a <em>first-order</em> method that behaves like a <em>second-order</em> method for quadratics.</p>
<p>For a quadratic objective function:</p>
<div class="arithmatex">\[
f(x) = \tfrac12 x^\top A x - b^\top x 
\]</div>
<p>with <span class="arithmatex">\(A \succ 0\)</span>, the level sets are ellipses shaped by the eigenvalues of <span class="arithmatex">\(A\)</span>. If <span class="arithmatex">\(A\)</span> is ill-conditioned, these ellipses are highly elongated. Gradient descent follows the steepest Euclidean descent direction, which points perpendicular to level sets. On elongated ellipses, this produces a zig-zag path that wastes many iterations.</p>
<p>CG replaces the steepest-descent directions with conjugate directions. Two nonzero vectors <span class="arithmatex">\(p_i, p_j\)</span> are said to be A-conjugate if</p>
<div class="arithmatex">\[
p_i^\top A p_j = 0.
\]</div>
<p>This is orthogonality measured in the geometry induced by the Hessian <span class="arithmatex">\(A\)</span>. Why is this useful?</p>
<ul>
<li>Moving along an A-conjugate direction eliminates error components associated with a different eigen-direction of <span class="arithmatex">\(A\)</span>.</li>
<li>Once you minimize along a conjugate direction, you never need to correct that direction again.</li>
<li>After <span class="arithmatex">\(n\)</span> mutually A-conjugate directions, all curvature directions are resolved → exact solution.</li>
</ul>
<p>In contrast, gradient descent repeatedly re-corrects previous progress.</p>
<p>Algorithm (Linear CG): We solve the quadratic minimization problem or, equivalently, the linear system <span class="arithmatex">\(Ax = b\)</span>. Let</p>
<div class="arithmatex">\[
r_0 = b - A x_0, \qquad p_0 = r_0.
\]</div>
<p>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:</p>
<ol>
<li>
<p>Step size
   <script type="math/tex; mode=display">
   \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k}.
   </script>
</p>
</li>
<li>
<p>Update iterate
   <script type="math/tex; mode=display">
   x_{k+1} = x_k + \alpha_k p_k.
   </script>
</p>
</li>
<li>
<p>Update residual (negative gradient)
   <script type="math/tex; mode=display">
   r_{k+1} = r_k - \alpha_k A p_k.
   </script>
</p>
</li>
<li>
<p>Direction scaling
   <script type="math/tex; mode=display">
   \beta_k = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
   </script>
</p>
</li>
<li>
<p>New conjugate direction
   <script type="math/tex; mode=display">
   p_{k+1} = r_{k+1} + \beta_k p_k.
   </script>
</p>
</li>
</ol>
<p>Stop when <span class="arithmatex">\(\|r_k\|\)</span> is below tolerance.</p>
<p>Every new direction <span class="arithmatex">\(p_{k+1}\)</span> is constructed to be A-conjugate to all previous ones, and this is preserved automatically by the recurrence.</p>
<p>Why CG Is Fast: For an <span class="arithmatex">\(n\)</span>-dimensional quadratic, CG solves the problem in at most <span class="arithmatex">\(n\)</span> iterations in exact arithmetic. In practice, due to floating-point errors and finite precision, it converges much earlier, typically in <span class="arithmatex">\(O(\sqrt{\kappa})\)</span> iterations, where <span class="arithmatex">\(\kappa = \lambda_{\max}/\lambda_{\min}\)</span> is the condition number. The convergence bound in the A-norm is:</p>
<div class="arithmatex">\[
\|x_k - x^\star\|_A \le 
2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k 
\|x_0 - x^\star\|_A.
\]</div>
<p>This is dramatically better than the <span class="arithmatex">\(O(1/k)\)</span> rate of gradient descent.</p>
<p>CG is ideal when:</p>
<ul>
<li>The problem is a quadratic or a linear system with symmetric positive definite (SPD) matrix <span class="arithmatex">\(A\)</span>.</li>
<li><span class="arithmatex">\(A\)</span> is large and sparse or available as a matrix–vector product.</li>
<li>You cannot form or store <span class="arithmatex">\(A^{-1}\)</span> or even the full matrix <span class="arithmatex">\(A\)</span>.</li>
<li>You want a Hessian-aware method but cannot afford Newton’s method.</li>
</ul>
<p>Typical scenarios:</p>
<table>
<thead>
<tr>
<th>Application</th>
<th>Why CG fits</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large linear systems <span class="arithmatex">\(A x = b\)</span></td>
<td>Only requires <span class="arithmatex">\(A p\)</span>, not factorization.</td>
</tr>
<tr>
<td>Ridge regression</td>
<td>Normal equations form an SPD matrix.</td>
</tr>
<tr>
<td>Kernel ridge regression</td>
<td>Solves <span class="arithmatex">\((K+\lambda I)\alpha = y\)</span> efficiently.</td>
</tr>
<tr>
<td>Newton steps in ML</td>
<td>Inner solver for Hessian systems without forming Hessian.</td>
</tr>
<tr>
<td>PDEs and scientific computing</td>
<td>Sparse SPD matrices, ideal for CG.</td>
</tr>
</tbody>
</table>
<p>Assumptions Required for CG: To guarantee correctness of <em>linear CG</em>, we require:</p>
<ul>
<li><span class="arithmatex">\(A\)</span> is symmetric</li>
<li><span class="arithmatex">\(A\)</span> is positive definite</li>
<li>Objective is strictly convex quadratic</li>
<li>Arithmetic is exact (for the finite-step guarantee)</li>
</ul>
<p>If the function is <em>not</em> quadratic or Hessian is not SPD, use Nonlinear CG, which generalizes the idea but loses finite-step guarantees.</p>
<p>Practical Notes:</p>
<ul>
<li>You only need matrix–vector products <span class="arithmatex">\(Ap\)</span>.  </li>
<li>Storage cost is <span class="arithmatex">\(O(n)\)</span>.  </li>
<li>Preconditioning (replacing the system with <span class="arithmatex">\(M^{-1} A\)</span>) improves conditioning and accelerates convergence dramatically.  </li>
<li>Periodic re-orthogonalization can help in long runs with floating-point drift.</li>
</ul>
<blockquote>
<p>CG is the optimal descent method for quadratic objectives:  it constructs Hessian-aware conjugate directions that efficiently resolve curvature, giving Newton-like speed while requiring only gradient-level operations.</p>
</blockquote>
<h2 id="convex-19_optimizationalgo-126-newtons-method-and-second-order-methods">12.6 Newton’s method and second-order methods<a class="headerlink" href="#convex-19_optimizationalgo-126-newtons-method-and-second-order-methods" title="Permanent link">¶</a></h2>
<p>First-order methods (like gradient descent) only use gradient information. Newton’s method, in contrast, incorporates curvature information from the Hessian to take steps that better adapt to the local geometry of the function. This often leads to much faster convergence near the optimum.</p>
<p>From Chapter 3, the second-order Taylor approximation of <span class="arithmatex">\(f(x)\)</span> around a point <span class="arithmatex">\(x_k\)</span> is:</p>
<div class="arithmatex">\[
f(x_k + d)
\approx
f(x_k)
+ \nabla f(x_k)^\top d
+ \tfrac{1}{2} d^\top \nabla^2 f(x_k) d.
\]</div>
<p>If we temporarily trust this quadratic model, we can choose <span class="arithmatex">\(d\)</span> to minimize the right-hand side. Differentiating with respect to <span class="arithmatex">\(d\)</span> and setting to zero gives:</p>
<div class="arithmatex">\[
\nabla^2 f(x_k) \, d_{\text{newton}} = - \nabla f(x_k).
\]</div>
<p>Hence, the Newton step is:</p>
<div class="arithmatex">\[
d_{\text{newton}} = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k),
\quad
x_{k+1} = x_k + d_{\text{newton}}.
\]</div>
<p>This step aims directly at the stationary point of the local quadratic model. When the iterates are sufficiently close to the true minimizer of a strictly convex <span class="arithmatex">\(f\)</span>, Newton’s method achieves quadratic convergence—dramatically faster than the <span class="arithmatex">\(O(1/k)\)</span> or <span class="arithmatex">\(O(1/k^2)\)</span> rates typical of first-order algorithms.</p>
<p>However, far from the minimizer the quadratic model may be inaccurate, the Hessian may be indefinite, or the step may be unreasonably large. For stability, Newton’s method is almost always paired with a line search or trust-region strategy that adjusts step length based on how well the model predicts actual decrease.</p>
<h3 id="convex-19_optimizationalgo-solving-the-newton-system">Solving the Newton System<a class="headerlink" href="#convex-19_optimizationalgo-solving-the-newton-system" title="Permanent link">¶</a></h3>
<p>Each iteration requires solving</p>
<div class="arithmatex">\[
H \,\Delta x = -g,
\qquad
H = \nabla^2 f(x), \;\; g = \nabla f(x).
\]</div>
<p>If <span class="arithmatex">\(H\)</span> is symmetric positive definite, a Cholesky factorization</p>
<div class="arithmatex">\[
H = L L^\top
\]</div>
<p>allows efficient and numerically stable solution via two triangular solves:</p>
<ol>
<li><span class="arithmatex">\(L y = -g\)</span></li>
<li><span class="arithmatex">\(L^\top \Delta x_{\text{nt}} = y\)</span></li>
</ol>
<p>This avoids forming <span class="arithmatex">\(H^{-1}\)</span> explicitly.</p>
<p>The Newton decrement:</p>
<div class="arithmatex">\[
\lambda(x) = \|L^{-1} g\|_2
\]</div>
<p>gauges proximity to the optimum and provides a natural stopping criterion: <span class="arithmatex">\(\lambda(x)^2/2 &lt; \varepsilon\)</span>.</p>
<p>Computationally, the dominant cost is solving the Newton system. For dense, unstructured problems this costs <span class="arithmatex">\(\approx (1/3)n^3\)</span> operations, though sparsity or structure can reduce this dramatically. Because of this cost, Newton’s method is most appealing for problems of moderate dimension or for situations where Hessian systems can be solved efficiently using sparse linear algebra or matrix–free iterative methods.</p>
<h3 id="convex-19_optimizationalgo-gaussnewton-method">Gauss–Newton Method<a class="headerlink" href="#convex-19_optimizationalgo-gaussnewton-method" title="Permanent link">¶</a></h3>
<p>The Gauss–Newton method is a specialization of Newton’s method for nonlinear least squares problems</p>
<div class="arithmatex">\[
f(x) = \tfrac12 \| r(x) \|^2,
\]</div>
<p>where <span class="arithmatex">\(r(x)\)</span> is a vector of residual functions and a nonlinear function of <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(J\)</span> is its Jacobian. Newton’s Hessian decomposes as</p>
<div class="arithmatex">\[
\nabla^2 f(x) = J^\top J \;+\; \sum_i r_i(x)\, \nabla^2 r_i(x).
\]</div>
<p>The second term involves the curvature of the residuals. When <span class="arithmatex">\(r(x)\)</span> is approximately linear near the optimum, this term is small. Gauss–Newton drops it, giving the approximation</p>
<div class="arithmatex">\[
\nabla^2 f(x) \approx J^\top J,
\]</div>
<p>leading to the Gauss–Newton step:</p>
<div class="arithmatex">\[
(J^\top J)\, \Delta = -J^\top r.
\]</div>
<p>Thus each iteration reduces to solving a (potentially large but structured) least-squares system, avoiding full Hessians entirely. The Levenberg–Marquardt method adds a damping term,</p>
<div class="arithmatex">\[
(J^\top J + \lambda I)\, \Delta = -J^\top r,
\]</div>
<p>which interpolates smoothly between  </p>
<ul>
<li>gradient descent (large <span class="arithmatex">\(\lambda\)</span>), and  </li>
<li>Gauss–Newton (small <span class="arithmatex">\(\lambda\)</span>).</li>
</ul>
<p>Damping improves robustness when the Jacobian is rank-deficient or when the neglected second-order terms are not negligible Gauss–Newton and Levenberg–Marquardt are highly effective when the residuals are nearly linear—common in curve fitting, bundle adjustment, and certain layerwise training procedures in deep learning—yielding fast convergence without the expense of full second derivatives.</p>
<h3 id="convex-19_optimizationalgo-quasi-newton-methods">Quasi-Newton methods<a class="headerlink" href="#convex-19_optimizationalgo-quasi-newton-methods" title="Permanent link">¶</a></h3>
<p>When computing or storing the Hessian is too expensive, we can build low-rank approximations of <span class="arithmatex">\(\nabla^2 f(x_k)\)</span> or its inverse. These methods use gradient information from previous steps to estimate curvature.</p>
<p>The most famous examples are:</p>
<ul>
<li>BFGS (Broyden–Fletcher–Goldfarb–Shanno)  </li>
<li>DFP (Davidon–Fletcher–Powell)  </li>
<li>L-BFGS (Limited-memory BFGS) — for very large-scale problems.</li>
</ul>
<p>Quasi-Newton methods (BFGS, L-BFGS) build inverse-Hessian approximations from gradient differences, achieving superlinear convergence with low memory. They maintain many of Newton’s fast local convergence properties, but with per-iteration costs similar to first-order methods. For instance, BFGS maintains an approximation <span class="arithmatex">\(B_k \approx \nabla^2 f(x_k)^{-1}\)</span> updated via gradient and step differences:</p>
<div class="arithmatex">\[
B_{k+1} = B_k + \frac{(s_k^\top y_k + y_k^\top B_k y_k)}{(s_k^\top y_k)^2} s_k s_k^\top
- \frac{B_k y_k s_k^\top + s_k y_k^\top B_k}{s_k^\top y_k},
\]</div>
<p>where <span class="arithmatex">\(s_k = x_{k+1} - x_k\)</span> and <span class="arithmatex">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_k)\)</span>.</p>
<p>These methods achieve superlinear convergence in practice, making them popular for large smooth optimization problems.</p>
<p>When to use Newton or quasi-Newton methods:</p>
<ul>
<li>You need high-accuracy solutions.  </li>
<li>The problem is smooth and reasonably well-conditioned.  </li>
<li>The dimension is moderate, or Hessian systems can be solved efficiently (e.g., using sparse linear algebra).  </li>
</ul>
<p>For large, ill-conditioned, or nonsmooth problems, first-order or proximal methods (Chapter 10) are typically more suitable.</p>
<h2 id="convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods">12.8 Constraints and nonsmooth terms: projection and proximal methods<a class="headerlink" href="#convex-19_optimizationalgo-128-constraints-and-nonsmooth-terms-projection-and-proximal-methods" title="Permanent link">¶</a></h2>
<p>In practice, most convex objectives are not just “nice smooth <span class="arithmatex">\(f(x)\)</span>”. They often have:</p>
<ul>
<li>constraints <span class="arithmatex">\(x \in \mathcal{X}\)</span>,</li>
<li>nonsmooth regularisers like <span class="arithmatex">\(\|x\|_1\)</span>,</li>
<li>penalties that encode robustness or sparsity (Chapter 6).</li>
</ul>
<p>Two core ideas handle this: projected gradient and proximal gradient.</p>
<h3 id="convex-19_optimizationalgo-1281-projected-gradient-descent">12.8.1 Projected gradient descent<a class="headerlink" href="#convex-19_optimizationalgo-1281-projected-gradient-descent" title="Permanent link">¶</a></h3>
<p>Setting: Minimise convex, differentiable <span class="arithmatex">\(f(x)\)</span> subject to <span class="arithmatex">\(x \in \mathcal{X}\)</span>, where <span class="arithmatex">\(\mathcal{X}\)</span> is a simple closed convex set (Chapter 4).</p>
<p>Algorithm:</p>
<ol>
<li>Gradient step:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Projection:
   <script type="math/tex; mode=display">
   x_{k+1}
   =
   \Pi_{\mathcal{X}}(y_k)
   :=
   \arg\min_{x \in \mathcal{X}} \|x - y_k\|_2^2~.
   </script>
</li>
</ol>
<p>Interpretation:</p>
<ul>
<li>You take an unconstrained step downhill,</li>
<li>then you “snap back” to feasibility by Euclidean projection.</li>
</ul>
<p>Examples of <span class="arithmatex">\(\mathcal{X}\)</span> where projection is cheap:</p>
<ul>
<li>A box: <span class="arithmatex">\(l \le x \le u\)</span> (clip each coordinate).</li>
<li>The probability simplex <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span> (there are fast projection routines).</li>
<li>An <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\(\{x : \|x\|_2 \le R\}\)</span> (scale down if needed).</li>
</ul>
<p>Projected gradient is the constrained version of gradient descent. It maintains feasibility at every iterate.</p>
<h3 id="convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting">12.8.2 Proximal gradient (forward–backward splitting)<a class="headerlink" href="#convex-19_optimizationalgo-1282-proximal-gradient-forwardbackward-splitting" title="Permanent link">¶</a></h3>
<p>Setting: Composite convex minimisation
<script type="math/tex; mode=display">
\min_x \; F(x) := f(x) + R(x),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(f\)</span> is convex, differentiable, with Lipschitz gradient,</li>
<li><span class="arithmatex">\(R\)</span> is convex, possibly nonsmooth.</li>
</ul>
<p>Typical choices of <span class="arithmatex">\(R(x)\)</span>:</p>
<ul>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_1\)</span> (sparsity),</li>
<li><span class="arithmatex">\(R(x) = \lambda \|x\|_2^2\)</span> (ridge),</li>
<li><span class="arithmatex">\(R(x)\)</span> is the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>, i.e. <span class="arithmatex">\(R(x)=0\)</span> if <span class="arithmatex">\(x \in \mathcal{X}\)</span> and <span class="arithmatex">\(+\infty\)</span> otherwise — this encodes a hard constraint.</li>
</ul>
<p>Define the proximal operator of <span class="arithmatex">\(R\)</span>:
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha R}(y)
=
\arg\min_x
\left(
R(x) + \frac{1}{2\alpha} \|x-y\|_2^2
\right).
</script>
</p>
<p>Proximal gradient method:</p>
<ol>
<li>Gradient step on <span class="arithmatex">\(f\)</span>:
   <script type="math/tex; mode=display">
   y_k = x_k - \alpha \nabla f(x_k).
   </script>
</li>
<li>Proximal step on <span class="arithmatex">\(R\)</span>:
   <script type="math/tex; mode=display">
   x_{k+1} = \mathrm{prox}_{\alpha R}(y_k).
   </script>
</li>
</ol>
<p>This is also called forward–backward splitting: “forward” = gradient step, “backward” = prox step.</p>
<h4 id="convex-19_optimizationalgo-interpretation">Interpretation:<a class="headerlink" href="#convex-19_optimizationalgo-interpretation" title="Permanent link">¶</a></h4>
<ul>
<li>The prox step “handles” the nonsmooth or constrained part exactly.</li>
<li>For <span class="arithmatex">\(R(x)=\lambda \|x\|_1\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> is soft-thresholding, which promotes sparsity in <span class="arithmatex">\(x\)</span>.<br>
  This is the heart of <span class="arithmatex">\(\ell_1\)</span>-regularised least-squares (LASSO) and many sparse recovery problems.</li>
<li>For <span class="arithmatex">\(R\)</span> as an indicator of <span class="arithmatex">\(\mathcal{X}\)</span>, <span class="arithmatex">\(\mathrm{prox}_{\alpha R} = \Pi_\mathcal{X}\)</span>, so projected gradient is a special case of proximal gradient.</li>
</ul>
<p>This unifies constraints and regularisation.</p>
<h4 id="convex-19_optimizationalgo-when-to-use-proximal-projected-gradient">When to use proximal / projected gradient<a class="headerlink" href="#convex-19_optimizationalgo-when-to-use-proximal-projected-gradient" title="Permanent link">¶</a></h4>
<ul>
<li>High-dimensional ML/statistics problems.</li>
<li>Objectives with <span class="arithmatex">\(\ell_1\)</span>, group sparsity, total variation, hinge loss, or indicator constraints.</li>
<li>You can evaluate <span class="arithmatex">\(\nabla f\)</span> and compute <span class="arithmatex">\(\mathrm{prox}_{\alpha R}\)</span> cheaply.</li>
<li>You don’t need absurdly high accuracy, but you do need scalability.</li>
</ul>
<p>This is the standard tool for modern large-scale convex learning problems.</p>
<h2 id="convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods">12.9 Penalties, barriers, and interior-point methods<a class="headerlink" href="#convex-19_optimizationalgo-129-penalties-barriers-and-interior-point-methods" title="Permanent link">¶</a></h2>
<p>So far we’ve assumed either:</p>
<ul>
<li>simple constraints we can project onto,</li>
<li>or nonsmooth terms we can prox.</li>
</ul>
<p>What if the constraints are general convex inequalities <span class="arithmatex">\(g_i(x)\le0\)</span>: Enter penalty methods, barrier methods, and (ultimately) interior-point methods.</p>
<h3 id="convex-19_optimizationalgo-1291-penalty-methods">12.9.1 Penalty methods<a class="headerlink" href="#convex-19_optimizationalgo-1291-penalty-methods" title="Permanent link">¶</a></h3>
<p>Turn constrained optimisation into unconstrained optimisation by adding a penalty for violating constraints. Suppose we want
<script type="math/tex; mode=display">
\min_x f(x)
\quad \text{s.t.} \quad g_i(x) \le 0,\ i=1,\dots,m.
</script>
</p>
<p>A penalty method solves instead
<script type="math/tex; mode=display">
\min_x \; f(x) + \rho \sum_{i=1}^m \phi(g_i(x)),
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(\phi(r)\)</span> is <span class="arithmatex">\(0\)</span> when <span class="arithmatex">\(r \le 0\)</span> (feasible),</li>
<li><span class="arithmatex">\(\phi(r)\)</span> grows when <span class="arithmatex">\(r&gt;0\)</span> (infeasible),</li>
<li><span class="arithmatex">\(\rho &gt; 0\)</span> is a penalty weight.</li>
</ul>
<p>As <span class="arithmatex">\(\rho \to \infty\)</span>, infeasible points become extremely expensive, so minimisers approach feasibility.  </p>
<p>This is conceptually simple and is sometimes effective, but:</p>
<ul>
<li>choosing <span class="arithmatex">\(\rho\)</span> is tricky,</li>
<li>very large <span class="arithmatex">\(\rho\)</span> can make the landscape ill-conditioned and hard for gradient/Newton to solve.</li>
</ul>
<h3 id="convex-19_optimizationalgo-algorithm-basic-penalty-method-quadratic-or-general-penalization">Algorithm: Basic Penalty Method (Quadratic or General Penalization)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-basic-penalty-method-quadratic-or-general-penalization" title="Permanent link">¶</a></h3>
<p>Goal:  Solve<br>
<script type="math/tex; mode=display">
\min_x f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Penalty formulation:<br>
<script type="math/tex; mode=display">
F_\rho(x) = f(x) + \rho \sum_{i=1}^m \phi(g_i(x)),
</script>
where  </p>
<ul>
<li><span class="arithmatex">\(\phi(r) = 0\)</span> if <span class="arithmatex">\(r \le 0\)</span>,  </li>
<li><span class="arithmatex">\(\phi(r)\)</span> grows when <span class="arithmatex">\(r&gt;0\)</span> (e.g., <span class="arithmatex">\(\phi(r)=\max\{0,r\}^2\)</span>),  </li>
<li><span class="arithmatex">\(\rho &gt; 0\)</span> is the penalty weight.</li>
</ul>
<p>Inputs:  </p>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>penalty function <span class="arithmatex">\(\phi\)</span>  </li>
<li>initial point <span class="arithmatex">\(x_0\)</span>  </li>
<li>initial penalty parameter <span class="arithmatex">\(\rho_0 &gt; 0\)</span>  </li>
<li>penalty update factor <span class="arithmatex">\(\gamma &gt; 1\)</span>  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<p>Procedure:</p>
<ol>
<li>Choose <span class="arithmatex">\(x_0\)</span>, <span class="arithmatex">\(\rho_0 &gt; 0\)</span>.  </li>
<li>For <span class="arithmatex">\(k = 0, 1, 2, \dots\)</span>:  <ol>
<li>Solve the penalized subproblem  <span class="arithmatex">\(x_{k+1} = \arg\min_x F_{\rho_k}(x)\)</span> using Newton’s method, gradient descent, quasi-Newton, etc.  </li>
<li>Check feasibility / stopping:  If <span class="arithmatex">\(\max_i g_i(x_{k+1}) \le \varepsilon, \quad   \|x_{k+1} - x_k\| \le \varepsilon\)</span>  stop and return <span class="arithmatex">\(x_{k+1}\)</span>.  </li>
<li>Increase penalty parameter  <span class="arithmatex">\(\rho_{k+1} = \gamma\, \rho_k\)</span>   with typical <span class="arithmatex">\(\gamma \in [5,10]\)</span>.  </li>
</ol>
</li>
<li>End.</li>
</ol>
<h3 id="convex-19_optimizationalgo-1292-barrier-methods">12.9.2 Barrier methods<a class="headerlink" href="#convex-19_optimizationalgo-1292-barrier-methods" title="Permanent link">¶</a></h3>
<p>Penalty methods penalise violation <em>after</em> you cross the boundary. Barrier methods make it impossible to even touch the boundary. For inequality constraints <span class="arithmatex">\(g_i(x) \le 0\)</span>, define the logarithmic barrier
<script type="math/tex; mode=display">
b(x) = - \sum_{i=1}^m \log(-g_i(x)).
</script>
This is finite only if <span class="arithmatex">\(g_i(x) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>, i.e. <span class="arithmatex">\(x\)</span> is strictly feasible. As you approach the boundary <span class="arithmatex">\(g_i(x)=0\)</span>, <span class="arithmatex">\(b(x)\)</span> blows up to <span class="arithmatex">\(+\infty\)</span>.</p>
<p>We then solve, for a sequence of increasing parameters <span class="arithmatex">\(t\)</span>:
<script type="math/tex; mode=display">
\min_x \; F_t(x) := t f(x) + b(x),
</script>
subject to strict feasibility <span class="arithmatex">\(g_i(x)&lt;0\)</span>.</p>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, minimisers of <span class="arithmatex">\(F_t\)</span> approach the true constrained optimum. The path of minimisers <span class="arithmatex">\(x^*(t)\)</span> is called the central path.</p>
<p>Key points:</p>
<ul>
<li><span class="arithmatex">\(F_t\)</span> is smooth on the interior of the feasible region.</li>
<li>We can apply Newton’s method to <span class="arithmatex">\(F_t\)</span>.</li>
<li>Each Newton step solves a linear system involving the Hessian of <span class="arithmatex">\(F_t\)</span>, so the inner loop looks like a damped Newton method.</li>
<li>Increasing <span class="arithmatex">\(t\)</span> tightens the approximation; we “home in” on the boundary of feasibility.</li>
</ul>
<h3 id="convex-19_optimizationalgo-algorithm-barrier-method-logarithmic-barrier-interior-approximation">Algorithm: Barrier Method (Logarithmic Barrier / Interior Approximation)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-barrier-method-logarithmic-barrier-interior-approximation" title="Permanent link">¶</a></h3>
<p>Goal: Solve the constrained problem<br>
<script type="math/tex; mode=display">
\min_x f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Logarithmic barrier:<br>
<script type="math/tex; mode=display">
b(x) = -\sum_{i=1}^m \log\!\big(-g_i(x)\big),
</script>
defined only for strictly feasible points <span class="arithmatex">\(g_i(x)&lt;0\)</span>.</p>
<p>Barrier subproblem:<br>
<script type="math/tex; mode=display">
F_t(x) = t\, f(x) + b(x),
</script>
where <span class="arithmatex">\(t&gt;0\)</span> is the barrier parameter.</p>
<p>As <span class="arithmatex">\(t \to \infty\)</span>, minimizers of <span class="arithmatex">\(F_t\)</span> approach the constrained optimum.</p>
<p>Inputs:  </p>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>inequality constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>barrier function <span class="arithmatex">\(b(x)\)</span>  </li>
<li>strictly feasible starting point <span class="arithmatex">\(x_0\)</span> (<span class="arithmatex">\(g_i(x_0) &lt; 0\)</span>)  </li>
<li>initial barrier parameter <span class="arithmatex">\(t_0 &gt; 0\)</span>  </li>
<li>barrier growth factor <span class="arithmatex">\(\mu &gt; 1\)</span> (often <span class="arithmatex">\(\mu = 10\)</span>)  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<p>Procedure:</p>
<ol>
<li>Choose strictly feasible <span class="arithmatex">\(x_0\)</span>, and pick <span class="arithmatex">\(t_0 &gt; 0\)</span>.  </li>
<li>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:  <ol>
<li>Centering step (inner loop):  Solve the barrier subproblem  <script type="math/tex">
  x_{k+1} = \arg\min_x F_{t_k}(x)
  \quad\text{with} g_i(x)<0. </script>  Typically use Newton’s method (damped) on <span class="arithmatex">\(F_{t_k}\)</span>.  Stop when the Newton decrement satisfies  <span class="arithmatex">\(\lambda(x_{k+1})^2/2 \le \varepsilon\)</span></li>
<li>Optimality / stopping test:    If  <span class="arithmatex">\(\frac{m}{t_k} \le \varepsilon,\)</span>
  then <span class="arithmatex">\(x_{k+1}\)</span> is an <span class="arithmatex">\(\varepsilon\)</span>-approximate solution of the original constrained problem; stop and return <span class="arithmatex">\(x_{k+1}\)</span>.  </li>
<li>Increase barrier parameter:  <span class="arithmatex">\(t_{k+1} = \mu\, t_k,\)</span>   which tightens the approximation and moves closer to the boundary.  </li>
</ol>
</li>
<li>End.</li>
</ol>
<h3 id="convex-19_optimizationalgo-1293-interior-point-methods">12.9.3 Interior-point methods<a class="headerlink" href="#convex-19_optimizationalgo-1293-interior-point-methods" title="Permanent link">¶</a></h3>
<p>Interior-point methods combine barrier functions with Newton’s method to solve general convex programs:</p>
<ul>
<li>They maintain strict feasibility throughout.</li>
<li>Each iteration solves a Newton system for the barrier-augmented objective.</li>
<li>They naturally generate primal–dual pairs and duality gap estimates.</li>
<li>Under standard assumptions (e.g., Slater’s condition), they converge in a predictable number of iterations.</li>
</ul>
<p>Interior-point methods are the foundation of modern solvers for LP, QP, SOCP, and SDP. They are more expensive per iteration than first-order methods but converge in far fewer steps and achieve high accuracy.</p>
<h3 id="convex-19_optimizationalgo-algorithm-primaldual-interior-point-method-for-convex-inequality-constraints">Algorithm: Primal–Dual Interior-Point Method (for convex inequality constraints)<a class="headerlink" href="#convex-19_optimizationalgo-algorithm-primaldual-interior-point-method-for-convex-inequality-constraints" title="Permanent link">¶</a></h3>
<p>We consider the problem
<script type="math/tex; mode=display">
\min_x\; f(x) \quad \text{s.t. } g_i(x) \le 0,\; i=1,\dots,m.
</script>
</p>
<p>Introduce Lagrange multipliers <span class="arithmatex">\(\lambda \ge 0\)</span>. The KKT conditions are
<script type="math/tex; mode=display">
\begin{aligned}
\nabla f(x) + \sum_i \lambda_i \nabla g_i(x) &= 0, \\
g_i(x) &\le 0, \\
\lambda_i &\ge 0, \\
\lambda_i\, g_i(x) &= 0.
\end{aligned}
</script>
</p>
<p>Interior-point methods enforce the relaxed condition
<script type="math/tex; mode=display">
\lambda_i\, g_i(x) = -\frac{1}{t},
</script>
which keeps iterates strictly feasible.</p>
<h3 id="convex-19_optimizationalgo-inputs">Inputs<a class="headerlink" href="#convex-19_optimizationalgo-inputs" title="Permanent link">¶</a></h3>
<ul>
<li>objective <span class="arithmatex">\(f(x)\)</span>  </li>
<li>inequality constraints <span class="arithmatex">\(g_i(x)\)</span>  </li>
<li>initial primal point <span class="arithmatex">\(x_0\)</span> with <span class="arithmatex">\(g_i(x_0)&lt;0\)</span>  </li>
<li>initial dual variable <span class="arithmatex">\(\lambda_0 &gt; 0\)</span>  </li>
<li>initial barrier parameter <span class="arithmatex">\(t_0 &gt; 0\)</span>  </li>
<li>growth factor <span class="arithmatex">\(\mu &gt; 1\)</span>  </li>
<li>tolerance <span class="arithmatex">\(\varepsilon\)</span></li>
</ul>
<h3 id="convex-19_optimizationalgo-procedure">Procedure<a class="headerlink" href="#convex-19_optimizationalgo-procedure" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Choose strictly feasible <span class="arithmatex">\(x_0\)</span>, positive <span class="arithmatex">\(\lambda_0\)</span>, and <span class="arithmatex">\(t_0\)</span>.</p>
</li>
<li>
<p>For <span class="arithmatex">\(k = 0,1,2,\dots\)</span>:</p>
<p>(a) Form the perturbed KKT system.  Solve for the Newton direction <span class="arithmatex">\((\Delta x, \Delta \lambda)\)</span>:</p>
<p>
<script type="math/tex; mode=display">
   \begin{bmatrix}
   \nabla^2 f(x) + \sum_i \lambda_i \nabla^2 g_i(x) & \nabla g(x) \\
   \text{diag}(\lambda)\,\nabla g(x)^\top & \text{diag}(g(x))
   \end{bmatrix}
   \begin{bmatrix}
   \Delta x \\
   \Delta \lambda
   \end{bmatrix}
   =
   -
   \begin{bmatrix}
   \nabla f(x) + \sum_i \lambda_i \nabla g_i(x) \\
   \lambda \circ g(x) + \tfrac{1}{t}\mathbf{1}
   \end{bmatrix}.
   </script>
</p>
<p>(b) Line search to keep strict feasibility. Choose the maximum <span class="arithmatex">\(\alpha\in(0,1]\)</span> such that:</p>
<ul>
<li><span class="arithmatex">\(g_i(x + \alpha \Delta x) &lt; 0\)</span>,</li>
<li><span class="arithmatex">\(\lambda + \alpha \Delta \lambda &gt; 0\)</span>.</li>
</ul>
<p>(c) Update: <span class="arithmatex">\(x \leftarrow x + \alpha \Delta x,
   \qquad  \lambda \leftarrow \lambda + \alpha \Delta \lambda.\)</span></p>
<p>(d) Check duality gap: <span class="arithmatex">\(\text{gap} = - g(x)^\top \lambda\)</span> If <span class="arithmatex">\(\text{gap} \le \varepsilon\)</span>, stop.</p>
<p>(e) Increase barrier parameter <span class="arithmatex">\(t \leftarrow \mu t.\)</span></p>
</li>
<li>
<p>Return <span class="arithmatex">\(x\)</span>.</p>
</li>
</ol>
<h2 id="convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice">12.10 Choosing the right method in practice<a class="headerlink" href="#convex-19_optimizationalgo-1210-choosing-the-right-method-in-practice" title="Permanent link">¶</a></h2>
<p>Case A. Smooth, unconstrained, very high dimensional.<br>
Example: logistic regression on millions of samples.<br>
Use: gradient descent or (better) accelerated gradient.<br>
Why: cheap iterations, easy to implement, scales.  </p>
<p>Case B. Smooth, unconstrained, moderate dimensional, need high accuracy.<br>
Example: convex nonlinear fitting with well-behaved Hessian.<br>
Use: Newton or quasi-Newton.<br>
Why: quadratic (or near-quadratic) convergence near optimum.  </p>
<p>Case C. Convex with simple feasible set <span class="arithmatex">\(x \in \mathcal{X}\)</span> (box, ball, simplex).<br>
Use: projected gradient.<br>
Why: projection is easy, maintains feasibility at each step.  </p>
<p>Case D. Composite objective <span class="arithmatex">\(f(x) + R(x)\)</span> where <span class="arithmatex">\(R\)</span> is nonsmooth (e.g. <span class="arithmatex">\(\ell_1\)</span>, indicator of a constraint set).<br>
Use: proximal gradient.<br>
Why: prox handles nonsmooth/constraint part exactly each step.  </p>
<p>Case E. General convex program with inequalities <span class="arithmatex">\(g_i(x)\le 0\)</span>.<br>
Use: interior-point methods.<br>
Why: they solve smooth barrier subproblems via Newton steps and give primal–dual certificates through KKT and duality (Chapters 7–8).  </p></body></html></section><section class="print-page" id="convex-19a_optimization_constraints" heading-number="2.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-13-optimization-algorithms-for-equality-constrained-problems">Chapter 13: Optimization Algorithms for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-chapter-13-optimization-algorithms-for-equality-constrained-problems" title="Permanent link">¶</a></h1>
<p>Equality-constrained optimization arises whenever the variables must satisfy one or more exact relations — such as conservation laws, normalization, or fairness criteria. We study algorithms for minimizing a function subject to linear or nonlinear equality constraints:</p>
<div class="arithmatex">\[
\min_x \; f(x) \quad \text{s.t.} \quad A x = b.
\]</div>
<p>Such problems are fundamental in convex optimization, quadratic programming, and many ML formulations involving exact invariants.</p>
<h2 id="convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold">13.1 Geometric View — Optimization on an Affine Manifold<a class="headerlink" href="#convex-19a_optimization_constraints-131-geometric-view-optimization-on-an-affine-manifold" title="Permanent link">¶</a></h2>
<p>The constraint <span class="arithmatex">\(A x = b\)</span> defines an affine set, a lower-dimensional plane within <span class="arithmatex">\(\mathbb{R}^n\)</span>. The feasible region is:</p>
<div class="arithmatex">\[
\mathcal{X} = \{ x \in \mathbb{R}^n \mid A x = b \}.
\]</div>
<p>If <span class="arithmatex">\(A \in \mathbb{R}^{p \times n}\)</span> has full row rank (<span class="arithmatex">\(\operatorname{rank}(A)=p\)</span>), then <span class="arithmatex">\(\mathcal{X}\)</span> is an <span class="arithmatex">\((n-p)\)</span>-dimensional affine manifold.</p>
<p>Geometrically, optimization proceeds not over all <span class="arithmatex">\(\mathbb{R}^n\)</span>, but along this manifold. At the optimum, the gradient <span class="arithmatex">\(\nabla f(x^\star)\)</span> cannot point in a direction that stays feasible—hence it must be orthogonal to the feasible surface. This gives the first key optimality relation:</p>
<div class="arithmatex">\[
\nabla f(x^\star) = A^\top \nu^\star,
\]</div>
<p>where <span class="arithmatex">\(\nu^\star\)</span> is a vector of Lagrange multipliers capturing how sensitive the objective is to constraint perturbations.</p>
<blockquote>
<p>Intuition:<br>
The gradient of the objective at the optimum lies in the span of the constraint normals (rows of <span class="arithmatex">\(A\)</span>).<br>
Any feasible direction must lie in the null space of <span class="arithmatex">\(A\)</span>, orthogonal to <span class="arithmatex">\(\nabla f(x^\star)\)</span>.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system">13.2 Lagrange Function and KKT System<a class="headerlink" href="#convex-19a_optimization_constraints-132-lagrange-function-and-kkt-system" title="Permanent link">¶</a></h2>
<p>Define the Lagrangian:</p>
<div class="arithmatex">\[
\mathcal{L}(x, \nu) = f(x) + \nu^\top (A x - b).
\]</div>
<p>The first-order (KKT) conditions for a feasible point <span class="arithmatex">\((x^\star, \nu^\star)\)</span> to be optimal are:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla f(x^\star) + A^\top \nu^\star &amp;= 0, \\
A x^\star &amp;= b.
\end{aligned}
\]</div>
<p>These equations express stationarity and feasibility simultaneously. They can be combined into the KKT linear system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\
\Delta \nu
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) + A^\top \nu \\
A x - b
\end{bmatrix}.
\]</div>
<p>At the optimum, the right-hand side is zero.</p>
<blockquote>
<p>ML Connection:<br>
Lagrange multipliers <span class="arithmatex">\(\nu\)</span> quantify trade-offs between objectives and hard constraints —<br>
for instance, enforcing weight normalization in a neural layer, balance constraints in fair classification, or conservation laws in physics-informed networks.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-133-the-quadratic-case">13.3 The Quadratic Case<a class="headerlink" href="#convex-19a_optimization_constraints-133-the-quadratic-case" title="Permanent link">¶</a></h2>
<p>For a quadratic objective
<script type="math/tex; mode=display">
f(x) = \tfrac{1}{2}x^\top P x + q^\top x + r,
</script>
with <span class="arithmatex">\(P \succeq 0\)</span>, the KKT conditions reduce to a linear system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
P &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
x^\star \\ \nu^\star
\end{bmatrix}
=
-
\begin{bmatrix}
q \\ -b
\end{bmatrix}.
\]</div>
<p>This is a saddle-point system, solvable by factorization or elimination. If <span class="arithmatex">\(P \succ 0\)</span> and <span class="arithmatex">\(A\)</span> has full row rank, the solution <span class="arithmatex">\((x^\star, \nu^\star)\)</span> is unique.</p>
<blockquote>
<p>In ML, such systems appear in constrained least squares, e.g. enforcing <span class="arithmatex">\(\sum_i w_i = 1\)</span> in portfolio optimization or convex combination weights in mixture models.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-134-the-null-space-reduced-variable-method">13.4 The Null-Space (Reduced Variable) Method<a class="headerlink" href="#convex-19a_optimization_constraints-134-the-null-space-reduced-variable-method" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(A\)</span> has full row rank, we can find a particular feasible point <span class="arithmatex">\(x_0\)</span> such that <span class="arithmatex">\(A x_0 = b\)</span>, and a basis <span class="arithmatex">\(Z\)</span> for the null space of <span class="arithmatex">\(A\)</span> satisfying <span class="arithmatex">\(A Z = 0\)</span>.<br>
Then any feasible <span class="arithmatex">\(x\)</span> can be written as:</p>
<div class="arithmatex">\[
x = x_0 + Z y, \quad y \in \mathbb{R}^{n-p}.
\]</div>
<p>Substituting into the objective gives a reduced problem:</p>
<div class="arithmatex">\[
\min_y \; f(x_0 + Z y).
\]</div>
<p>This is an unconstrained problem in <span class="arithmatex">\(y\)</span>, solvable by gradient or Newton methods.<br>
The reduced gradient and Hessian are:</p>
<div class="arithmatex">\[
\nabla_y f = Z^\top \nabla_x f, \qquad \nabla_y^2 f = Z^\top \nabla_x^2 f \, Z.
\]</div>
<blockquote>
<p>Interpretation: Optimization proceeds only along feasible directions — those that do not violate the constraints (i.e., within <span class="arithmatex">\(\operatorname{Null}(A)\)</span>).<br>
This is equivalent to projecting all gradient steps onto the tangent space of the constraint manifold.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems">13.5 Newton’s Method for Equality-Constrained Problems<a class="headerlink" href="#convex-19a_optimization_constraints-135-newtons-method-for-equality-constrained-problems" title="Permanent link">¶</a></h2>
<p>For a twice differentiable <span class="arithmatex">\(f\)</span>, the equality-constrained Newton step solves the quadratic subproblem:</p>
<div class="arithmatex">\[
\begin{aligned}
\min_d &amp; \quad \tfrac{1}{2} d^\top \nabla^2 f(x) d + \nabla f(x)^\top d, \\
\text{s.t.} &amp; \quad A d = 0.
\end{aligned}
\]</div>
<p>This produces the step <span class="arithmatex">\((d, \lambda)\)</span> from the linearized KKT system:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
d \\ \lambda
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x) \\ 0
\end{bmatrix}.
\]</div>
<p>The update is <span class="arithmatex">\(x_{k+1} = x_k + \alpha d\)</span>, ensuring <span class="arithmatex">\(A x_{k+1} = b\)</span> if <span class="arithmatex">\(A x_k = b\)</span>.</p>
<p>Geometric insight:<br>
The Newton direction is the projection of the unconstrained Newton step onto the tangent space of the feasible set (directions satisfying <span class="arithmatex">\(A d = 0\)</span>).<br>
Thus, each step stays within the affine constraint manifold.</p>
<blockquote>
<p>In practice:<br>
The KKT system is typically solved by <em>Schur complement factorization</em>:
<script type="math/tex; mode=display">
(A (\nabla^2 f)^{-1} A^\top) \lambda = A (\nabla^2 f)^{-1} \nabla f,
</script>
which then yields <span class="arithmatex">\(d = -(\nabla^2 f)^{-1} (\nabla f + A^\top \lambda)\)</span>.</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-136-infeasible-start-newton-method">13.6 Infeasible Start Newton Method<a class="headerlink" href="#convex-19a_optimization_constraints-136-infeasible-start-newton-method" title="Permanent link">¶</a></h2>
<p>When starting from an infeasible point (<span class="arithmatex">\(A x_0 \ne b\)</span>),<br>
we relax the constraint and drive feasibility progressively.<br>
At iteration <span class="arithmatex">\(k\)</span>, compute <span class="arithmatex">\((\Delta x, \Delta \nu)\)</span> by solving:</p>
<div class="arithmatex">\[
\begin{bmatrix}
\nabla^2 f(x_k) &amp; A^\top \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\ \Delta \nu
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(x_k) + A^\top \nu_k \\
A x_k - b
\end{bmatrix}.
\]</div>
<p>Then update:</p>
<div class="arithmatex">\[
x_{k+1} = x_k + \alpha \Delta x, \quad
\nu_{k+1} = \nu_k + \alpha \Delta \nu.
\]</div>
<p>This method enforces feasibility gradually, converging to <span class="arithmatex">\((x^\star, \nu^\star)\)</span> under mild conditions.</p>
<blockquote>
<p>In ML contexts, infeasible starts are typical — we rarely have feasible initialization (e.g., in constrained autoencoders or regularized fairness models).<br>
The infeasible Newton method ensures consistent progress in both primal feasibility (<span class="arithmatex">\(A x = b\)</span>) and dual stationarity (<span class="arithmatex">\(\nabla f + A^\top \nu = 0\)</span>).</p>
</blockquote>
<h2 id="convex-19a_optimization_constraints-137-computational-considerations">13.7 Computational Considerations<a class="headerlink" href="#convex-19a_optimization_constraints-137-computational-considerations" title="Permanent link">¶</a></h2>
<ul>
<li>Factorization: KKT systems can be large but structured. Exploiting sparsity in <span class="arithmatex">\(\nabla^2 f\)</span> and <span class="arithmatex">\(A\)</span> is essential in high-dimensional problems.</li>
<li>Stability: Adding small regularization to the (0,0) block of the KKT matrix improves conditioning:
  <script type="math/tex; mode=display">
  \begin{bmatrix}
  \nabla^2 f + \delta I & A^\top \\
  A & -\delta I
  \end{bmatrix}.
  </script>
</li>
<li>Schur Complement: Eliminating <span class="arithmatex">\(\Delta x\)</span> yields a smaller linear system in <span class="arithmatex">\(\Delta \nu\)</span>, which can be more efficient when <span class="arithmatex">\(p \ll n\)</span>.</li>
</ul>
<h2 id="convex-19a_optimization_constraints-138-connections-to-machine-learning">13.8 Connections to Machine Learning<a class="headerlink" href="#convex-19a_optimization_constraints-138-connections-to-machine-learning" title="Permanent link">¶</a></h2>
<p>Equality-constrained optimization appears in several ML and signal processing settings:</p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Equality Constraint</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Portfolio optimization</td>
<td><span class="arithmatex">\(\mathbf{1}^\top w = 1\)</span></td>
<td>Weights must sum to 1</td>
</tr>
<tr>
<td>Fair classification</td>
<td><span class="arithmatex">\(A w = 0\)</span></td>
<td>Enforces equal outcomes across groups</td>
</tr>
<tr>
<td>Orthogonal embeddings</td>
<td><span class="arithmatex">\(W^\top W = I\)</span></td>
<td>Preserves independence / energy</td>
</tr>
<tr>
<td>Normalization layers</td>
<td><span class="arithmatex">\(\|w\|_2^2 = 1\)</span></td>
<td>Scale invariance constraint</td>
</tr>
<tr>
<td>Physics-informed models</td>
<td><span class="arithmatex">\(\text{div}(F)=0\)</span></td>
<td>Conservation of mass / charge</td>
</tr>
</tbody>
</table>
<h3 id="convex-19a_optimization_constraints-summary-approaches-to-equality-constrained-optimization">Summary: Approaches to Equality-Constrained Optimization<a class="headerlink" href="#convex-19a_optimization_constraints-summary-approaches-to-equality-constrained-optimization" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Constraint Type</th>
<th>Feasibility (Local/Global)</th>
<th>Core Idea</th>
<th>Advantages</th>
<th>Limitations / Drawbacks</th>
<th>Typical ML / Optimization Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Null-Space (Variable Elimination)</td>
<td>Linear, full-rank <span class="arithmatex">\(A\)</span></td>
<td>Global</td>
<td>Parameterize feasible <span class="arithmatex">\(x = x_0 + Z y\)</span> with <span class="arithmatex">\(A Z = 0\)</span></td>
<td>Converts to unconstrained problem; dimension reduction; exact</td>
<td>Requires null-space basis <span class="arithmatex">\(Z\)</span>; destroys sparsity; expensive for large <span class="arithmatex">\(A\)</span></td>
<td>Constrained least squares, small-scale convex programs</td>
</tr>
<tr>
<td>Local Parameterization (Manifold Method)</td>
<td>Nonlinear <span class="arithmatex">\(g(x) = 0\)</span></td>
<td>Local (around feasible point)</td>
<td>Use implicit function theorem: locally express <span class="arithmatex">\(x = x(y)\)</span></td>
<td>Captures nonlinear manifold structure; geometric insight</td>
<td>Valid only locally; requires Jacobians; expensive</td>
<td>Manifold learning, orthogonal embeddings, equality-regularized networks</td>
</tr>
<tr>
<td>KKT / Lagrange System</td>
<td>Linear or nonlinear</td>
<td>Global (if convex)</td>
<td>Solve coupled system <span class="arithmatex">\(\nabla f + A^\top \nu = 0\)</span>, <span class="arithmatex">\(A x = b\)</span></td>
<td>Keeps structure; allows dual interpretation; works for large sparse systems</td>
<td>Larger system; more variables</td>
<td>Quadratic programming, convex solvers, equality-constrained ML models</td>
</tr>
<tr>
<td>Primal–Dual Newton Method</td>
<td>Linear or nonlinear</td>
<td>Global (convex)</td>
<td>Newton’s method on full KKT system</td>
<td>Quadratic convergence near optimum; stable numerically</td>
<td>Requires Hessians and factorizations</td>
<td>Interior-point solvers, primal–dual optimization, barrier methods</td>
</tr>
<tr>
<td>Penalty / Augmented Lagrangian</td>
<td>General (convex or nonconvex)</td>
<td>Approximate (drives feasibility)</td>
<td>Add penalty term <span class="arithmatex">\(\tfrac{\rho}{2}\|A x - b\|^2\)</span> or dual updates</td>
<td>Simple to implement; smooth transition from unconstrained</td>
<td>Needs tuning of <span class="arithmatex">\(\rho\)</span>; slow convergence to exact feasibility</td>
<td>Regularized fairness, soft constraints, physics-informed networks</td>
</tr>
<tr>
<td>Projection / Normalization Step</td>
<td>Linear or nonlinear (simple form)</td>
<td>Iterative (after each step)</td>
<td>Project back to feasible set: <span class="arithmatex">\(x_{k+1} = \Pi_{\{A x = b\}}(x_{k+1})\)</span></td>
<td>Keeps updates feasible; easy for simple constraints</td>
<td>Costly for complex <span class="arithmatex">\(A\)</span>; may distort gradient direction</td>
<td>Normalization layers, unit-norm or balance constraints</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-19b_optimization_constraints" heading-number="2.14"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-14-optimization-algorithms-for-inequality-constrained-problems">Chapter 14: Optimization Algorithms for Inequality-Constrained Problems<a class="headerlink" href="#convex-19b_optimization_constraints-chapter-14-optimization-algorithms-for-inequality-constrained-problems" title="Permanent link">¶</a></h1>
<p>In practice, optimization problems often include inequalities that restrict feasible solutions to a convex region. Examples include nonnegativity of variables, margin constraints in support vector machines, fairness or safety limits, and physical conservation laws. This chapter introduces algorithms for solving such problems efficiently, focusing on the logarithmic barrier and interior-point methods that underpin modern convex solvers.</p>
<h2 id="convex-19b_optimization_constraints-141-problem-setup">14.1 Problem Setup<a class="headerlink" href="#convex-19b_optimization_constraints-141-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider the general convex optimization problem with both equality and inequality constraints:</p>
<div class="arithmatex">\[
\begin{aligned}
\text{minimize}   &amp;\quad f_0(x) \\
\text{subject to} &amp;\quad f_i(x) \le 0, \quad i=1,\dots,m,\\
&amp;\quad A x = b.
\end{aligned}
\]</div>
<p>Assumptions:</p>
<ul>
<li>Each <span class="arithmatex">\(f_i\)</span> is convex and twice differentiable.  </li>
<li><span class="arithmatex">\(A \in \mathbb{R}^{p\times n}\)</span> has full row rank (<span class="arithmatex">\(\mathrm{rank}(A)=p\)</span>).  </li>
<li>There exists a strictly feasible point <span class="arithmatex">\(\bar{x}\)</span> such that <span class="arithmatex">\(f_i(\bar{x})&lt;0\)</span> and <span class="arithmatex">\(A\bar{x}=b\)</span> (Slater’s condition).  </li>
</ul>
<p>Under these assumptions, strong duality holds and the KKT conditions are necessary and sufficient for optimality.</p>
<h3 id="convex-19b_optimization_constraints-examples">Examples<a class="headerlink" href="#convex-19b_optimization_constraints-examples" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Problem</th>
<th><span class="arithmatex">\(f_0(x)\)</span></th>
<th><span class="arithmatex">\(f_i(x)\)</span></th>
<th>Notes / ML context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Program (LP)</td>
<td><span class="arithmatex">\(c^T x\)</span></td>
<td><span class="arithmatex">\(a_i^T x - b_i\)</span></td>
<td>Feature selection, resource allocation</td>
</tr>
<tr>
<td>Quadratic Program (QP)</td>
<td><span class="arithmatex">\(\tfrac{1}{2}x^T P x + q^T x\)</span></td>
<td>Linear <span class="arithmatex">\(a_i^T x - b_i\)</span></td>
<td>SVM training, ridge regression</td>
</tr>
<tr>
<td>QCQP</td>
<td>Quadratic</td>
<td>Quadratic</td>
<td>Portfolio optimization, control</td>
</tr>
<tr>
<td>Geometric Program (log domain)</td>
<td>Convex in <span class="arithmatex">\(\log x\)</span></td>
<td>Linear in <span class="arithmatex">\(\log x\)</span></td>
<td>Network flow, resource allocation</td>
</tr>
<tr>
<td>Entropy minimization</td>
<td><span class="arithmatex">\(\sum_i x_i \log x_i\)</span></td>
<td><span class="arithmatex">\(F x \le g\)</span></td>
<td>Probability calibration, information bottleneck</td>
</tr>
</tbody>
</table>
<h2 id="convex-19b_optimization_constraints-142-indicator-function-reformulation">14.2 Indicator-Function Reformulation<a class="headerlink" href="#convex-19b_optimization_constraints-142-indicator-function-reformulation" title="Permanent link">¶</a></h2>
<p>Define the indicator of the nonpositive orthant:</p>
<div class="arithmatex">\[
I_-(u)=
\begin{cases}
0, &amp; u \le 0,\\
+\infty, &amp; u &gt; 0.
\end{cases}
\]</div>
<p>Then the constrained problem is equivalent to</p>
<div class="arithmatex">\[
\min_x \; f_0(x) + \sum_{i=1}^m I_-(f_i(x))
\quad \text{s.t. } A x = b.
\]</div>
<p>This form is conceptually clear but nondifferentiable since <span class="arithmatex">\(I_-\)</span> is discontinuous. To apply Newton-type algorithms, we replace <span class="arithmatex">\(I_-\)</span> with a smooth approximation: the logarithmic barrier.</p>
<h2 id="convex-19b_optimization_constraints-143-logarithmic-barrier-approximation">14.3 Logarithmic-Barrier Approximation<a class="headerlink" href="#convex-19b_optimization_constraints-143-logarithmic-barrier-approximation" title="Permanent link">¶</a></h2>
<p>We approximate each <span class="arithmatex">\(I_-(f_i(x))\)</span> by a differentiable barrier function <span class="arithmatex">\(\Phi(u) = -\tfrac{1}{t} \log(-u)\)</span> for <span class="arithmatex">\(u &lt; 0\)</span>. The smoothed subproblem becomes</p>
<div class="arithmatex">\[
\min_x \; f_0(x) - \frac{1}{t} \sum_{i=1}^m \log(-f_i(x))
\quad \text{s.t. } A x = b.
\]</div>
<ul>
<li>For small <span class="arithmatex">\(t\)</span>: the barrier is strong and keeps points deep inside the feasible region.  </li>
<li>As <span class="arithmatex">\(t \to \infty\)</span>: the barrier weakens and the solution approaches the true optimum.</li>
</ul>
<p>Hence, the original inequality-constrained problem is replaced by a sequence of smooth equality-constrained subproblems.</p>
<h2 id="convex-19b_optimization_constraints-144-properties-of-the-barrier-function">14.4 Properties of the Barrier Function<a class="headerlink" href="#convex-19b_optimization_constraints-144-properties-of-the-barrier-function" title="Permanent link">¶</a></h2>
<p>Define</p>
<div class="arithmatex">\[
\phi(x) = -\sum_{i=1}^m \log(-f_i(x)), \qquad
\mathrm{dom}\,\phi = \{x : f_i(x) &lt; 0\}.
\]</div>
<p>Then <span class="arithmatex">\(\phi\)</span> is convex and twice differentiable:</p>
<div class="arithmatex">\[
\nabla \phi(x) = \sum_i \frac{1}{-f_i(x)} \nabla f_i(x),
\]</div>
<div class="arithmatex">\[
\nabla^2 \phi(x) =
\sum_i \frac{1}{f_i(x)^2} \nabla f_i(x)\nabla f_i(x)^T
+ \sum_i \frac{1}{-f_i(x)} \nabla^2 f_i(x).
\]</div>
<p>Near the boundary <span class="arithmatex">\(f_i(x)=0\)</span>, the gradient norm grows without bound — producing a repulsive force that prevents violation of constraints.</p>
<h2 id="convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions">14.5 Central Path and Approximate KKT Conditions<a class="headerlink" href="#convex-19b_optimization_constraints-145-central-path-and-approximate-kkt-conditions" title="Permanent link">¶</a></h2>
<p>For each <span class="arithmatex">\(t &gt; 0\)</span>, let <span class="arithmatex">\(x^*(t)\)</span> minimize the barrier problem</p>
<div class="arithmatex">\[
\min_x\; t f_0(x) + \phi(x)
\quad \text{s.t. } A x = b.
\]</div>
<p>The curve <span class="arithmatex">\(\{x^*(t) : t &gt; 0\}\)</span> is the central path. As <span class="arithmatex">\(t \to \infty\)</span>, <span class="arithmatex">\(x^*(t)\)</span> approaches the true optimal solution <span class="arithmatex">\(x^*\)</span>. Along this path there exist dual variables <span class="arithmatex">\((\lambda^*(t), v^*(t))\)</span> satisfying</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla f_0(x^*(t)) + \sum_i \lambda_i^*(t) \nabla f_i(x^*(t)) + A^T v^*(t) &amp;= 0,\\
A x^*(t) &amp;= b,\\
-\lambda_i^*(t) f_i(x^*(t)) &amp;= \tfrac{1}{t}, \quad \lambda_i^*(t) \ge 0.
\end{aligned}
\]</div>
<p>The complementarity condition is relaxed: <span class="arithmatex">\(\lambda_i f_i(x) = -1/t\)</span> instead of <span class="arithmatex">\(0\)</span>. As <span class="arithmatex">\(t \to \infty\)</span>, these approximate KKT conditions converge to the exact ones.</p>
<h2 id="convex-19b_optimization_constraints-146-geometric-and-physical-intuition">14.6 Geometric and Physical Intuition<a class="headerlink" href="#convex-19b_optimization_constraints-146-geometric-and-physical-intuition" title="Permanent link">¶</a></h2>
<p>The centering subproblem</p>
<div class="arithmatex">\[
\min_x\; t f_0(x) - \sum_i \log(-f_i(x))
\]</div>
<p>can be viewed as a particle system in a potential field:</p>
<ul>
<li>The objective <span class="arithmatex">\(f_0(x)\)</span> pulls toward lower cost (external force).  </li>
<li>Each constraint <span class="arithmatex">\(f_i(x)\le0\)</span> creates a repulsive potential that diverges near the boundary.  </li>
</ul>
<p>At equilibrium, these forces balance:</p>
<div class="arithmatex">\[
\nabla f_0(x^*(t)) + \sum_i \frac{1}{t(-f_i(x^*(t)))} \nabla f_i(x^*(t)) = 0.
\]</div>
<p>Thus, the solution remains strictly feasible — this is the essence of the interior-point philosophy.</p>
<h2 id="convex-19b_optimization_constraints-147-barrier-method-algorithm">14.7 Barrier-Method Algorithm<a class="headerlink" href="#convex-19b_optimization_constraints-147-barrier-method-algorithm" title="Permanent link">¶</a></h2>
<p>The barrier method converts the original inequality-constrained problem into a sequence of smooth equality-constrained subproblems.<br>
Each subproblem is solved exactly (to high precision) while a <em>barrier parameter</em> <span class="arithmatex">\(t\)</span> is gradually increased, allowing the iterates to approach the boundary and the true constrained optimum.</p>
<h3 id="convex-19b_optimization_constraints-algorithm-outline">Algorithm Outline<a class="headerlink" href="#convex-19b_optimization_constraints-algorithm-outline" title="Permanent link">¶</a></h3>
<p>Given:</p>
<ul>
<li>a strictly feasible starting point <span class="arithmatex">\(x\)</span> (so <span class="arithmatex">\(f_i(x) &lt; 0\)</span> for all <span class="arithmatex">\(i\)</span>),</li>
<li>an initial barrier parameter <span class="arithmatex">\(t &gt; 0\)</span>,</li>
<li>a barrier scaling factor <span class="arithmatex">\(\mu &gt; 1\)</span> (usually between 10 and 20),</li>
<li>and a desired accuracy <span class="arithmatex">\(\varepsilon &gt; 0\)</span> (for stopping),</li>
</ul>
<p>the algorithm proceeds as follows:</p>
<ol>
<li>
<p>Centering step: Solve<br>
<script type="math/tex; mode=display">
   \min_x \; f_0(x) - \frac{1}{t} \sum_{i=1}^m \log(-f_i(x))
   \quad \text{s.t. } A x = b
   </script>
   using Newton’s method for equality-constrained optimization. The result <span class="arithmatex">\(x^*(t)\)</span> is the <em>centering point</em> for the current <span class="arithmatex">\(t\)</span>.</p>
</li>
<li>
<p>Update iterate:  Set <span class="arithmatex">\(x := x^*(t)\)</span>.</p>
</li>
<li>
<p>Stopping criterion:  Stop if 
   <script type="math/tex; mode=display">
   \frac{m}{t} < \varepsilon.
   </script>
   Here <span class="arithmatex">\(m\)</span> is the number of inequality constraints, and <span class="arithmatex">\(\varepsilon\)</span> is the desired tolerance on suboptimality. This rule is derived from the duality gap bound:
   <script type="math/tex; mode=display">
   f_0(x^*(t)) - p^* \le \frac{m}{t},
   </script>
   meaning that if <span class="arithmatex">\(m/t\)</span> is smaller than <span class="arithmatex">\(\varepsilon\)</span>, the current solution is guaranteed to be within <span class="arithmatex">\(\varepsilon\)</span> of the true optimum.</p>
</li>
<li>
<p>Increase barrier parameter: Set <span class="arithmatex">\(t := \mu t\)</span> and return to Step 1.
   Each centering subproblem maintains strict feasibility, and increasing <span class="arithmatex">\(t\)</span> gradually weakens the barrier, allowing the iterates to approach the true constraint boundary. A typical choice is <span class="arithmatex">\(\mu \in [10, 20]\)</span>.</p>
</li>
</ol>
<h3 id="convex-19b_optimization_constraints-understanding-varepsilon-the-accuracy-parameter">Understanding <span class="arithmatex">\(\varepsilon\)</span> — the Accuracy Parameter<a class="headerlink" href="#convex-19b_optimization_constraints-understanding-varepsilon-the-accuracy-parameter" title="Permanent link">¶</a></h3>
<p>The parameter <span class="arithmatex">\(\varepsilon\)</span> controls how close to the optimal solution we wish to stop.</p>
<ul>
<li>
<p>Mathematically, <span class="arithmatex">\(\varepsilon\)</span> specifies an upper bound on the duality gap:
  <script type="math/tex; mode=display">
  f_0(x) - p^* \le \varepsilon.
  </script>
</p>
</li>
<li>
<p>Conceptually, <span class="arithmatex">\(\varepsilon\)</span> represents the trade-off between accuracy and computational cost:</p>
</li>
<li>Smaller <span class="arithmatex">\(\varepsilon\)</span> → more iterations (larger <span class="arithmatex">\(t\)</span> required).</li>
<li>Larger <span class="arithmatex">\(\varepsilon\)</span> → faster termination, but lower accuracy.</li>
</ul>
<p>In practice:</p>
<ul>
<li>For numerical optimization or ML training, <span class="arithmatex">\(\varepsilon\)</span> is often set between <span class="arithmatex">\(10^{-3}\)</span> and <span class="arithmatex">\(10^{-8}\)</span> depending on problem size and desired precision.  </li>
<li>Convex solvers (like CVX, MOSEK, or ECOS) typically use <span class="arithmatex">\(\varepsilon \approx 10^{-6}\)</span> as a default high-accuracy target.</li>
</ul>
<h3 id="convex-19b_optimization_constraints-intuitive-interpretation">Intuitive Interpretation<a class="headerlink" href="#convex-19b_optimization_constraints-intuitive-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>Think of <span class="arithmatex">\(\varepsilon\)</span> as the “distance” between the current point and the true optimum in terms of objective value.  </li>
<li>The ratio <span class="arithmatex">\(m/t\)</span> acts like a thermometer for this distance — as <span class="arithmatex">\(t\)</span> grows, the temperature (error) cools down.</li>
<li>Once <span class="arithmatex">\(m/t &lt; \varepsilon\)</span>, we know the algorithm has cooled sufficiently: the point lies extremely close to the optimal constrained solution.</li>
</ul>
<h3 id="convex-19b_optimization_constraints-summary-of-key-parameters">Summary of Key Parameters<a class="headerlink" href="#convex-19b_optimization_constraints-summary-of-key-parameters" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Typical Value / Range</th>
<th>Intuitive Role</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(m\)</span></td>
<td>Number of inequality constraints</td>
<td>problem dependent</td>
<td>Total number of barrier terms</td>
</tr>
<tr>
<td><span class="arithmatex">\(t\)</span></td>
<td>Barrier parameter</td>
<td>starts small (1–10), grows by <span class="arithmatex">\(\mu\)</span></td>
<td>Controls strength of barrier</td>
</tr>
<tr>
<td><span class="arithmatex">\(\mu\)</span></td>
<td>Barrier growth factor</td>
<td>10–20</td>
<td>Controls how fast we approach constraint boundary</td>
</tr>
<tr>
<td><span class="arithmatex">\(\varepsilon\)</span></td>
<td>Desired accuracy (tolerance)</td>
<td><span class="arithmatex">\(10^{-3}\)</span> to <span class="arithmatex">\(10^{-8}\)</span></td>
<td>Stopping threshold based on duality gap</td>
</tr>
</tbody>
</table>
<h3 id="convex-19b_optimization_constraints-intuitive-summary">Intuitive Summary<a class="headerlink" href="#convex-19b_optimization_constraints-intuitive-summary" title="Permanent link">¶</a></h3>
<ul>
<li>Each centering step finds the best <em>interior</em> point for a given barrier strength <span class="arithmatex">\(1/t\)</span>.  </li>
<li>Increasing <span class="arithmatex">\(t\)</span> reduces the barrier effect, letting <span class="arithmatex">\(x\)</span> approach the boundary.  </li>
<li>The stopping rule <span class="arithmatex">\(m/t &lt; \varepsilon\)</span> ensures that the objective value of <span class="arithmatex">\(x\)</span> differs from the true optimum by less than <span class="arithmatex">\(\varepsilon\)</span>.  </li>
<li>Smaller <span class="arithmatex">\(\varepsilon\)</span> means tighter optimality, but more work (larger <span class="arithmatex">\(t\)</span> and more iterations).</li>
</ul>
<h2 id="convex-19b_optimization_constraints-148-computational-and-practical-notes">14.8 Computational and Practical Notes<a class="headerlink" href="#convex-19b_optimization_constraints-148-computational-and-practical-notes" title="Permanent link">¶</a></h2>
<ul>
<li>Each centering problem is solved by equality-constrained Newton steps (KKT system).  </li>
<li>Barrier methods inherit superlinear convergence near the optimum.  </li>
<li>Initialization must be strictly feasible; feasibility restoration can be costly.  </li>
<li>Large <span class="arithmatex">\(t\)</span> makes the barrier steep, so line search and step damping are essential.</li>
</ul>
<p>In machine learning:
- SVM and logistic regression margin constraints fit naturally in this form.<br>
- Interior-point solvers for QPs are used in sparse regression and convex relaxations.<br>
- Barrier penalties act as smooth approximations to hard constraints in physics-informed and fairness-aware models.</p>
<h2 id="convex-19b_optimization_constraints-149-comparison-equality-vs-inequality-constrained-methods">14.9 Comparison: Equality vs Inequality-Constrained Methods<a class="headerlink" href="#convex-19b_optimization_constraints-149-comparison-equality-vs-inequality-constrained-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Equality Constraints</th>
<th>Inequality Constraints</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasible set</td>
<td>Affine manifold</td>
<td>Convex region with boundary</td>
</tr>
<tr>
<td>Algorithms</td>
<td>Newton, projected Newton, KKT</td>
<td>Barrier, interior-point, primal–dual</td>
</tr>
<tr>
<td>Feasibility handling</td>
<td>Exact</td>
<td>Maintained via barrier term</td>
</tr>
<tr>
<td>Complementarity</td>
<td><span class="arithmatex">\(A x = b\)</span></td>
<td><span class="arithmatex">\(\lambda_i f_i(x) = 0\)</span> (or <span class="arithmatex">\(= -1/t\)</span>)</td>
</tr>
<tr>
<td>Feasible start</td>
<td>Optional</td>
<td>Required (strict)</td>
</tr>
<tr>
<td>ML relevance</td>
<td>Normalization, fairness, balance</td>
<td>Nonnegativity, margins, sparsity, safety constraints</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-20_advanced" heading-number="2.15"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-15-advanced-large-scale-and-structured-methods">Chapter 15: Advanced Large-Scale and Structured Methods<a class="headerlink" href="#convex-20_advanced-chapter-15-advanced-large-scale-and-structured-methods" title="Permanent link">¶</a></h1>
<p>Modern convex optimization often operates at massive scales — millions of variables, billions of data points, or constraints distributed across devices and networks. Classical Newton or interior-point algorithms, while theoretically elegant, become computationally impractical in these regimes.  </p>
<p>This chapter introduces methods that exploit structure, sparsity, separability, and stochasticity to solve large-scale convex problems efficiently.<br>
These ideas underpin the optimization engines behind most machine learning systems.</p>
<h2 id="convex-20_advanced-151-motivation-structure-and-scale">15.1 Motivation: Structure and Scale<a class="headerlink" href="#convex-20_advanced-151-motivation-structure-and-scale" title="Permanent link">¶</a></h2>
<p>In large-scale convex optimization, the difficulty lies not in theory but in computation.</p>
<ul>
<li>Memory limits: Storing the full Hessian or even the gradient can be infeasible.  </li>
<li>Data size: Evaluating the objective over the full dataset is expensive.  </li>
<li>Distributed data: Information may be spread across machines or devices.  </li>
<li>Sparsity and separability: Many objectives decompose nicely into smaller components.</li>
</ul>
<p>Thus, the goal is to design algorithms that make incremental or local progress while exploiting the structure of the problem.</p>
<p>Typical forms include:
<script type="math/tex; mode=display">
f(x) = \frac{1}{N}\sum_{i=1}^N f_i(x) + R(x),
</script>
where:</p>
<ul>
<li>each <span class="arithmatex">\(f_i(x)\)</span> represents a data-sample loss term, and  </li>
<li><span class="arithmatex">\(R(x)\)</span> is a regularizer (possibly nonsmooth, such as <span class="arithmatex">\(\lambda\|x\|_1\)</span>).</li>
</ul>
<h2 id="convex-20_advanced-152-coordinate-descent">15.2 Coordinate Descent<a class="headerlink" href="#convex-20_advanced-152-coordinate-descent" title="Permanent link">¶</a></h2>
<p>Coordinate descent updates a single variable (or a small block) at a time while holding others fixed.  </p>
<h3 id="convex-20_advanced-algorithm">Algorithm<a class="headerlink" href="#convex-20_advanced-algorithm" title="Permanent link">¶</a></h3>
<p>Given <span class="arithmatex">\(x^{(k)}\)</span>, choose coordinate <span class="arithmatex">\(i\)</span> and update:
<script type="math/tex; mode=display">
x_i^{(k+1)} = \arg\min_{z} f(x_1^{(k+1)}, \ldots, x_{i-1}^{(k+1)}, z, x_{i+1}^{(k)}, \ldots, x_n^{(k)}).
</script>
</p>
<p>This can be seen as projecting the gradient onto the coordinate directions. For separable problems, it is computationally much cheaper than full gradient updates.</p>
<ul>
<li>Each subproblem is often 1D (or low-dimensional), so it may have a closed form.</li>
<li>For problems with separable structure — e.g. sums over features, or regularisers like <span class="arithmatex">\(\|x\|_1 = \sum_i |x_i|\)</span> — the coordinate update is extremely cheap.</li>
<li>You never form the full gradient or solve a large linear system; you just operate on pieces.</li>
</ul>
<p>This is especially attractive in high dimensions (millions of features), where a full Newton step would be absurdly expensive.</p>
<h3 id="convex-20_advanced-convergence">Convergence<a class="headerlink" href="#convex-20_advanced-convergence" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(f\)</span> is convex with Lipschitz-continuous partial derivatives, cyclic or randomized coordinate descent converges to the global optimum.</p>
<h3 id="convex-20_advanced-ml-context">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context" title="Permanent link">¶</a></h3>
<p>Coordinate descent is widely used in:</p>
<ul>
<li>LASSO and Elastic Net regression (where updates are closed-form soft-thresholding),</li>
<li>logistic regression with <span class="arithmatex">\(\ell_1\)</span> penalty,</li>
<li>matrix factorization and dictionary learning.</li>
</ul>
<h2 id="convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods">15.3 Stochastic Gradient and Variance-Reduced Methods<a class="headerlink" href="#convex-20_advanced-153-stochastic-gradient-and-variance-reduced-methods" title="Permanent link">¶</a></h2>
<p>When the dataset is large, computing the full gradient</p>
<div class="arithmatex">\[
\nabla f(x) = \frac{1}{N} \sum_{i=1}^N \nabla f_i(x)
\]</div>
<p>can be prohibitively expensive, since it requires evaluating all <span class="arithmatex">\(N\)</span> samples at every iteration. Stochastic methods overcome this by using <em>unbiased gradient estimates</em> based on small random subsets (mini-batches) of the data.</p>
<h3 id="convex-20_advanced-1531-stochastic-gradient-descent-sgd">15.3.1 Stochastic Gradient Descent (SGD)<a class="headerlink" href="#convex-20_advanced-1531-stochastic-gradient-descent-sgd" title="Permanent link">¶</a></h3>
<p>At each iteration, choose a random sample (or mini-batch) <span class="arithmatex">\(\mathcal{B}_k\)</span> and perform the update:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \eta_k \, \widehat{\nabla f}(x_k),
\]</div>
<p>where</p>
<div class="arithmatex">\[
\widehat{\nabla f}(x_k)
= \frac{1}{|\mathcal{B}_k|} \sum_{i \in \mathcal{B}_k} \nabla f_i(x_k)
\]</div>
<p>is a stochastic estimate of the true gradient,<br>
and <span class="arithmatex">\(\eta_k &gt; 0\)</span> is the step size (learning rate).</p>
<h4 id="convex-20_advanced-interpretation">Interpretation<a class="headerlink" href="#convex-20_advanced-interpretation" title="Permanent link">¶</a></h4>
<ul>
<li>SGD performs a <em>noisy gradient step</em>: it moves in approximately the right direction on average.</li>
<li>The noise introduced by sampling allows exploration of the parameter space and helps escape shallow local minima in nonconvex problems.</li>
<li>In convex settings, it trades accuracy for computational efficiency — each iteration is much cheaper, so we can afford many more of them.</li>
</ul>
<h3 id="convex-20_advanced-1532-step-size-and-averaging">15.3.2 Step Size and Averaging<a class="headerlink" href="#convex-20_advanced-1532-step-size-and-averaging" title="Permanent link">¶</a></h3>
<p>The step size <span class="arithmatex">\(\eta_k\)</span> controls the bias–variance tradeoff:</p>
<ul>
<li>If <span class="arithmatex">\(\eta_k\)</span> is too large → iterates oscillate due to stochastic noise.</li>
<li>If <span class="arithmatex">\(\eta_k\)</span> is too small → progress slows down.</li>
</ul>
<p>Common choices:</p>
<div class="arithmatex">\[
\eta_k = \frac{c}{\sqrt{k}} \quad \text{(for convex $f$)}, 
\qquad
\eta_k = \frac{c}{k} \quad \text{(for strongly convex $f$)}.
\]</div>
<p>Two popular stabilization strategies:</p>
<ol>
<li>
<p>Decay learning rate.</p>
</li>
<li>
<p>Polyak–Ruppert averaging:
   Instead of returning the last iterate, return the running average
   <script type="math/tex; mode=display">
   \bar{x}_k = \frac{1}{k}\sum_{t=1}^k x_t.
   </script>
   Averaging cancels gradient noise and ensures convergence to the optimal solution in expectation.</p>
</li>
<li>
<p>Increasing mini-batch size:<br>
   As optimization proceeds, increasing <span class="arithmatex">\(|\mathcal{B}_k|\)</span> gradually reduces gradient variance while keeping updates efficient.</p>
</li>
</ol>
<h3 id="convex-20_advanced-1533-convergence-properties">15.3.3 Convergence Properties<a class="headerlink" href="#convex-20_advanced-1533-convergence-properties" title="Permanent link">¶</a></h3>
<p>For convex objectives:
- <span class="arithmatex">\(\mathbb{E}[f(x_k)] - f^\star = O(1/\sqrt{k})\)</span> with diminishing <span class="arithmatex">\(\eta_k\)</span>.</p>
<p>For <em>strongly convex</em> <span class="arithmatex">\(f\)</span>, with <span class="arithmatex">\(\eta_k = O(1/k)\)</span>:
- <span class="arithmatex">\(\mathbb{E}[\|x_k - x^\star\|^2] = O(1/k)\)</span>.</p>
<p>These are optimal rates for stochastic first-order methods:  no unbiased stochastic optimizer using the same amount of data can asymptotically converge faster than SGD with Polyak averaging.</p>
<h3 id="convex-20_advanced-1534-variance-reduction">15.3.4 Variance Reduction<a class="headerlink" href="#convex-20_advanced-1534-variance-reduction" title="Permanent link">¶</a></h3>
<p>Although SGD is simple, the stochastic noise prevents it from reaching very high accuracy.  Variance-reduced methods (SVRG, SAGA, SARAH) correct this by mixing stochastic and full-gradient information.</p>
<p>Example: SVRG (Stochastic Variance-Reduced Gradient)</p>
<p>At outer iteration <span class="arithmatex">\(s\)</span>, compute a full gradient snapshot <span class="arithmatex">\(\nabla f(\tilde{x})\)</span>.<br>
Then, for inner iterations:
<script type="math/tex; mode=display">
v_k = \nabla f_i(x_k) - \nabla f_i(\tilde{x}) + \nabla f(\tilde{x}),
\quad
x_{k+1} = x_k - \eta v_k.
</script>
</p>
<ul>
<li><span class="arithmatex">\(v_k\)</span> is an unbiased estimate of <span class="arithmatex">\(\nabla f(x_k)\)</span> but with reduced variance.</li>
<li>For strongly convex <span class="arithmatex">\(f\)</span>, SVRG and SAGA achieve linear convergence, bridging the gap between SGD and full gradient descent.</li>
</ul>
<p>Intuitively, these methods “anchor” stochastic gradients around a periodically refreshed reference point, preventing the gradient noise from accumulating.</p>
<h3 id="convex-20_advanced-1535-stochastic-second-order-and-momentum-methods">15.3.5 Stochastic Second-Order and Momentum Methods<a class="headerlink" href="#convex-20_advanced-1535-stochastic-second-order-and-momentum-methods" title="Permanent link">¶</a></h3>
<p>SGD can be further improved by incorporating curvature or momentum information.</p>
<ol>
<li>
<p>Momentum / Nesterov acceleration:<br>
   Maintains an exponential moving average of past gradients:
   <script type="math/tex; mode=display">
   m_k = \beta m_{k-1} + (1 - \beta) \widehat{\nabla f}(x_k),
   \quad
   x_{k+1} = x_k - \eta m_k.
   </script>
   Momentum accelerates convergence in smooth regions and damps oscillations in narrow valleys.</p>
</li>
<li>
<p>Adaptive methods (Adam, RMSProp, Adagrad):<br>
   Use coordinate-wise scaling based on running averages of squared gradients to handle ill-conditioned curvature.</p>
</li>
<li>
<p>Stochastic second-order methods:<br>
   Approximate curvature matrices (e.g., Fisher or Hessian) via stochastic estimates and maintain them with exponential decay:
   <script type="math/tex; mode=display">
   H_k \approx (1 - \rho) H_{k-1} + \rho \, g_k g_k^\top.
   </script>
   Though theoretically limited by SGD’s asymptotic rate, they often yield better pre-asymptotic performance — crucial in practical deep learning where only a few passes over the data are feasible.</p>
</li>
</ol>
<h3 id="convex-20_advanced-1536-machine-learning-context-and-insights">15.3.6 Machine Learning Context and Insights<a class="headerlink" href="#convex-20_advanced-1536-machine-learning-context-and-insights" title="Permanent link">¶</a></h3>
<ul>
<li>Deep neural networks rely almost exclusively on SGD and its adaptive or momentum-based variants. The stochasticity helps generalization by acting as implicit regularization.</li>
<li>Large-scale convex ML problems — logistic regression, SVMs, ridge regression — use SGD or variance-reduced methods (SVRG/SAGA) for scalability.</li>
<li>The balance between <em>variance reduction</em> and <em>computational cost</em> defines practical performance.</li>
</ul>
<h3 id="convex-20_advanced-1537-summary">15.3.7 Summary<a class="headerlink" href="#convex-20_advanced-1537-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Convergence</th>
<th>Practical Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>Uses mini-batch gradients</td>
<td><span class="arithmatex">\(O(1/\sqrt{k})\)</span></td>
<td>Deep learning, online learning</td>
</tr>
<tr>
<td>SGD + Polyak averaging</td>
<td>Averaged iterates</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Theoretically optimal stochastic convergence</td>
</tr>
<tr>
<td>SVRG / SAGA</td>
<td>Variance-reduced updates</td>
<td>Linear for strongly convex</td>
<td>Convex ML, GLMs</td>
</tr>
<tr>
<td>Momentum / Adam</td>
<td>Smoothed gradient estimates</td>
<td>Empirical acceleration</td>
<td>Deep nets</td>
</tr>
<tr>
<td>Stochastic 2nd-order</td>
<td>Curvature tracking</td>
<td>Better pre-asymptotic</td>
<td>Large-batch training</td>
</tr>
</tbody>
</table>
<h2 id="convex-20_advanced-154-proximal-and-composite-optimization">15.4 Proximal and Composite Optimization<a class="headerlink" href="#convex-20_advanced-154-proximal-and-composite-optimization" title="Permanent link">¶</a></h2>
<p>Many modern objectives combine a smooth loss and a nonsmooth regularizer:
<script type="math/tex; mode=display">
\min_x \; g(x) + R(x),
</script>
where <span class="arithmatex">\(g\)</span> is differentiable with Lipschitz gradient and <span class="arithmatex">\(R\)</span> is convex but possibly nonsmooth.</p>
<p>The proximal gradient method updates as:
<script type="math/tex; mode=display">
x_{k+1} = \mathrm{prox}_{\alpha R}(x_k - \alpha \nabla g(x_k)),
</script>
where the proximal operator is:
<script type="math/tex; mode=display">
\mathrm{prox}_{\alpha R}(v) = \arg\min_x \left( R(x) + \frac{1}{2\alpha}\|x-v\|^2 \right).
</script>
</p>
<h3 id="convex-20_advanced-intuition">Intuition<a class="headerlink" href="#convex-20_advanced-intuition" title="Permanent link">¶</a></h3>
<ul>
<li>The gradient step moves in a descent direction for <span class="arithmatex">\(g\)</span>.  </li>
<li>The proximal step performs a local “denoising” or shrinkage under <span class="arithmatex">\(R\)</span> (e.g., soft-thresholding for <span class="arithmatex">\(\ell_1\)</span> norms).</li>
</ul>
<h3 id="convex-20_advanced-ml-context_1">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_1" title="Permanent link">¶</a></h3>
<p>Proximal methods underpin:</p>
<ul>
<li>Sparse regression (LASSO, Elastic Net),</li>
<li>matrix completion and compressed sensing,</li>
<li>total-variation image denoising,</li>
<li>low-rank and structured regularization.</li>
</ul>
<h2 id="convex-20_advanced-155-alternating-direction-method-of-multipliers-admm">15.5 Alternating Direction Method of Multipliers (ADMM)<a class="headerlink" href="#convex-20_advanced-155-alternating-direction-method-of-multipliers-admm" title="Permanent link">¶</a></h2>
<p>When an objective separates into parts that depend on different variables, ADMM enables efficient distributed optimization.</p>
<p>Consider:
<script type="math/tex; mode=display">
\min_{x,z}\; f(x) + g(z) \quad \text{s.t. } A x + B z = c.
</script>
</p>
<h3 id="convex-20_advanced-augmented-lagrangian">Augmented Lagrangian<a class="headerlink" href="#convex-20_advanced-augmented-lagrangian" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
L_\rho(x,z,y) = f(x) + g(z) + y^T(Ax + Bz - c) + \frac{\rho}{2}\|A x + B z - c\|^2.
\]</div>
<h3 id="convex-20_advanced-iterations">Iterations<a class="headerlink" href="#convex-20_advanced-iterations" title="Permanent link">¶</a></h3>
<p>ADMM performs alternating updates:
<script type="math/tex; mode=display">
\begin{aligned}
x^{k+1} &= \arg\min_x L_\rho(x, z^k, y^k),\\
z^{k+1} &= \arg\min_z L_\rho(x^{k+1}, z, y^k),\\
y^{k+1} &= y^k + \rho (A x^{k+1} + B z^{k+1} - c).
\end{aligned}
</script>
</p>
<h3 id="convex-20_advanced-interpretation_1">Interpretation<a class="headerlink" href="#convex-20_advanced-interpretation_1" title="Permanent link">¶</a></h3>
<p>Each step solves an easier subproblem involving only part of the variables, followed by a dual update to enforce consistency.<br>
ADMM thus merges ideas from dual ascent and penalty methods.</p>
<h3 id="convex-20_advanced-convergence_1">Convergence<a class="headerlink" href="#convex-20_advanced-convergence_1" title="Permanent link">¶</a></h3>
<p>For convex <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span>, ADMM converges to the global optimum.<br>
It is particularly effective when the subproblems are simple (e.g., proximal operators).</p>
<h3 id="convex-20_advanced-ml-context_2">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_2" title="Permanent link">¶</a></h3>
<p>ADMM is a key tool for:</p>
<ul>
<li>distributed LASSO and logistic regression,</li>
<li>matrix decomposition and factorization,</li>
<li>consensus optimization in federated learning,</li>
<li>distributed deep learning regularization.</li>
</ul>
<h2 id="convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms">15.6 Majorization–Minimization (MM) and EM Algorithms<a class="headerlink" href="#convex-20_advanced-156-majorizationminimization-mm-and-em-algorithms" title="Permanent link">¶</a></h2>
<p>The MM principle iteratively minimizes a surrogate function that upper-bounds the objective.</p>
<p>Given a current point <span class="arithmatex">\(x_k\)</span>, construct a surrogate <span class="arithmatex">\(g(x|x_k)\)</span> such that:
<script type="math/tex; mode=display">
g(x|x_k) \ge f(x), \quad g(x_k|x_k) = f(x_k).
</script>
</p>
<p>Then update:
<script type="math/tex; mode=display">
x_{k+1} = \arg\min_x g(x|x_k).
</script>
</p>
<p>Each iteration ensures <span class="arithmatex">\(f(x_{k+1}) \le f(x_k)\)</span>.</p>
<h3 id="convex-20_advanced-ml-context_3">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_3" title="Permanent link">¶</a></h3>
<ul>
<li>The Expectation–Maximization (EM) algorithm is an MM method for latent-variable models.  </li>
<li>IRLS (Iteratively Reweighted Least Squares) for logistic regression and <span class="arithmatex">\(\ell_p\)</span> regression follows the same idea.  </li>
<li>MM methods guarantee descent even for complex, nonconvex objectives.</li>
</ul>
<h2 id="convex-20_advanced-157-distributed-and-parallel-optimization">15.7 Distributed and Parallel Optimization<a class="headerlink" href="#convex-20_advanced-157-distributed-and-parallel-optimization" title="Permanent link">¶</a></h2>
<p>For large-scale convex problems distributed across multiple nodes, parallel methods are essential.</p>
<h3 id="convex-20_advanced-synchronous-and-asynchronous-updates">Synchronous and Asynchronous Updates<a class="headerlink" href="#convex-20_advanced-synchronous-and-asynchronous-updates" title="Permanent link">¶</a></h3>
<ul>
<li>Synchronous: all workers compute updates and synchronize (used in federated averaging).  </li>
<li>Asynchronous: updates proceed without waiting, improving throughput but increasing variance.</li>
</ul>
<h3 id="convex-20_advanced-consensus-optimization">Consensus Optimization<a class="headerlink" href="#convex-20_advanced-consensus-optimization" title="Permanent link">¶</a></h3>
<p>In distributed convex optimization, one solves
<script type="math/tex; mode=display">
\min_{x_1,\dots,x_p} \sum_{i=1}^p f_i(x_i)
\quad \text{s.t. } x_i = z,
</script>
which can be handled by ADMM or primal–dual methods.<br>
Each machine optimizes its local copy <span class="arithmatex">\(x_i\)</span>, and the shared variable <span class="arithmatex">\(z\)</span> enforces consensus.</p>
<h3 id="convex-20_advanced-ml-context_4">ML Context<a class="headerlink" href="#convex-20_advanced-ml-context_4" title="Permanent link">¶</a></h3>
<ul>
<li>Federated learning and parameter-server training frameworks (e.g., TensorFlow Distributed, PyTorch DDP) follow this model.  </li>
<li>Decentralized convex optimization appears in sensor networks and multi-agent control.</li>
</ul>
<h2 id="convex-20_advanced-158-handling-structure-sparsity-and-low-rank">15.8 Handling Structure: Sparsity and Low Rank<a class="headerlink" href="#convex-20_advanced-158-handling-structure-sparsity-and-low-rank" title="Permanent link">¶</a></h2>
<p>Many convex problems exhibit special structures that algorithms can exploit:</p>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Typical Regularizer</th>
<th>Algorithmic Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sparsity</td>
<td><span class="arithmatex">\(\ell_1\)</span> or group lasso</td>
<td>Coordinate updates, proximal shrinkage</td>
</tr>
<tr>
<td>Low rank</td>
<td>nuclear norm <span class="arithmatex">\(\|X\|_*\)</span></td>
<td>SVD-based proximal step</td>
</tr>
<tr>
<td>Block separability</td>
<td><span class="arithmatex">\(\sum_i f_i(x_i)\)</span></td>
<td>Parallel or distributed updates</td>
</tr>
<tr>
<td>Graph structure</td>
<td>total variation norm</td>
<td>Local neighborhood computations</td>
</tr>
<tr>
<td>Simplex or probability constraints</td>
<td>entropy or KL penalty</td>
<td>Mirror descent, projected methods</td>
</tr>
</tbody>
</table>
<p>Exploiting such structure yields orders-of-magnitude speedups in both memory and computation.</p>
<h2 id="convex-20_advanced-159-summary-and-practical-guidance">15.9 Summary and Practical Guidance<a class="headerlink" href="#convex-20_advanced-159-summary-and-practical-guidance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Gradient Access</th>
<th>Scalability</th>
<th>Parallelization</th>
<th>Convexity Required</th>
<th>Typical ML Uses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent</td>
<td>Partial / coordinate</td>
<td>High</td>
<td>Easy</td>
<td>Convex</td>
<td>LASSO, sparse models</td>
</tr>
<tr>
<td>SGD / SVRG / SAGA</td>
<td>Stochastic</td>
<td>Excellent</td>
<td>Natural</td>
<td>Convex / nonconvex</td>
<td>Deep learning, logistic regression</td>
</tr>
<tr>
<td>Proximal Gradient</td>
<td>Full gradient + prox</td>
<td>Moderate–High</td>
<td>Easy</td>
<td>Convex</td>
<td>Composite objectives</td>
</tr>
<tr>
<td>ADMM</td>
<td>Separable subproblems</td>
<td>High</td>
<td>Distributed</td>
<td>Convex</td>
<td>Consensus, large convex solvers</td>
</tr>
<tr>
<td>MM / EM</td>
<td>Surrogate-based</td>
<td>Moderate</td>
<td>Model-specific</td>
<td>Convex / nonconvex</td>
<td>Probabilistic models, IRLS</td>
</tr>
<tr>
<td>Distributed / Federated</td>
<td>Local gradients</td>
<td>Very high</td>
<td>Essential</td>
<td>Convex / smooth</td>
<td>Federated learning, large-scale convex optimization</td>
</tr>
</tbody>
</table>
<h2 id="convex-20_advanced-1510-key-takeaways">15.10 Key Takeaways<a class="headerlink" href="#convex-20_advanced-1510-key-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>Large-scale convex optimization relies on exploiting structure, stochasticity, and separability.  </li>
<li>Coordinate and proximal methods handle sparse and composite problems efficiently.  </li>
<li>Stochastic and variance-reduced methods scale to massive data.  </li>
<li>ADMM and distributed optimization enable multi-machine or federated settings.  </li>
<li>MM and EM extend convex ideas to broader nonconvex inference tasks.</li>
</ul></body></html></section><section class="print-page" id="convex-21_models" heading-number="2.16"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-16-modelling-patterns-and-algorithm-selection">Chapter 16: Modelling Patterns and Algorithm Selection<a class="headerlink" href="#convex-21_models-chapter-16-modelling-patterns-and-algorithm-selection" title="Permanent link">¶</a></h1>
<p>Real-world modelling starts not with algorithms but with data, assumptions, and design goals.  We choose a loss function from statistical assumptions (e.g. noise model, likelihood) and a complexity penalty or constraints from design preferences (simplicity, robustness, etc.).  The resulting convex (or nonconvex) optimization problem often <em>tells</em> us which solver class to use.  In practice, solving machine learning problems looks like: modeling → recognize structure → pick solver.  Familiar ML models (linear regression, logistic regression, etc.) can be viewed as convex programs.  Below we survey common patterns (convex and some nonconvex) and the recommended algorithms/tricks for each.</p>
<h2 id="convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff">16.1 Regularized estimation and the accuracy–simplicity tradeoff<a class="headerlink" href="#convex-21_models-161-regularized-estimation-and-the-accuracysimplicity-tradeoff" title="Permanent link">¶</a></h2>
<p>Many learning tasks use a regularized risk minimization form:
<script type="math/tex; mode=display">
\min_x \; \underbrace{\text{loss}(x)}_{\text{data-fit}} \;+\; \lambda\;\underbrace{\text{penalty}(x)}_{\text{complexity}}.
</script>
Here the loss measures fit to data (often from a likelihood) and the penalty (regularizer) enforces simplicity or structure.  Increasing <span class="arithmatex">\(\lambda\)</span> trades accuracy for simplicity (e.g. model sparsity or shrinkage).</p>
<ul>
<li>
<p>Ridge regression (ℓ₂ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2.
  </script>
<br>
  This arises from Gaussian noise (squared-error loss) plus a quadratic prior on <span class="arithmatex">\(x\)</span>.  It is a smooth, strongly convex quadratic problem (Hessian <span class="arithmatex">\(A^TA + \lambda I \succ 0\)</span>).  One can solve it via Newton’s method or closed‐form normal equations, or for large problems via (accelerated) gradient descent or conjugate gradient.  Strong convexity means fast, reliable convergence with second-order or accelerated first-order methods.</p>
</li>
<li>
<p>LASSO / Sparse regression (ℓ₁ penalty):<br>
<script type="math/tex; mode=display">
  \min_x \tfrac12\|Ax - b\|_2^2 + \lambda \|x\|_1.
  </script>
<br>
  The <span class="arithmatex">\(\ell_1\)</span> penalty encourages many <span class="arithmatex">\(x_i=0\)</span> (sparsity) for interpretability.  The problem is convex but nonsmooth (since <span class="arithmatex">\(|\cdot|\)</span> is nondifferentiable at 0).  A standard solver is proximal gradient: take a gradient step on the smooth squared loss, then apply the proximal (soft-thresholding) step for <span class="arithmatex">\(\ell_1\)</span>, which sets small entries to zero.  Coordinate descent is another popular solver – updating one coordinate at a time with a closed-form soft-thresholding step.  Proximal methods and coordinate descent scale to very high dimensions.  </p>
</li>
<li>
<p>Elastic net (mixed ℓ₁+ℓ₂):<br>
<script type="math/tex; mode=display">
  \min_x \|Ax - b\|_2^2 + \lambda_1\|x\|_1 + \lambda_2\|x\|_2^2.
  </script>
<br>
  This combines the sparsity of LASSO with the stability of ridge regression.  It is still convex and (for <span class="arithmatex">\(\lambda_2&gt;0\)</span>) strongly convex[^4].  One can still use proximal gradient (prox operator splits into soft-threshold and shrink) or coordinate descent.  Because of the ℓ₂ term, the objective is smooth and unique solution.</p>
</li>
<li>
<p>Group lasso, nuclear norm, etc.: Similar composite objectives arise when enforcing block-sparsity or low-rank structure.  Each adds a convex penalty (block <span class="arithmatex">\(\ell_{2,1}\)</span> norms, nuclear norm) to the loss.  These remain convex, often separable or prox-friendly.  Proximal methods (using known proximal maps for each norm) or ADMM can handle these.</p>
</li>
</ul>
<p>Algorithmic pointers for 11.1:  </p>
<ul>
<li><em>Smooth+ℓ₂ (strongly convex)</em> → Newton / quasi-Newton or (accelerated) gradient descent (Chapter 9).  Closed-form if possible.  </li>
<li><em>Smooth + ℓ₁</em> → Proximal gradient or coordinate descent (Chapter 9/10).  These exploit separable nonsmoothness.  </li>
<li><em>Mixed penalties (ℓ₁+ℓ₂)</em> → Still convex; often handle like ℓ₁ case since smooth part dominates curvature.  </li>
<li><em>Large-scale data</em> → Stochastic/mini-batch variants of first-order methods (SGD, SVRG, etc.).  </li>
</ul>
<p><em>Remarks:</em>  Choose <span class="arithmatex">\(\lambda\)</span> via cross-validation or hold-out to balance fit vs simplicity.  In high dimensions (<span class="arithmatex">\(n\)</span> large), coordinate or stochastic methods often outperform direct second-order methods.</p>
<h2 id="convex-21_models-162-robust-regression-and-outlier-resistance">16.2 Robust regression and outlier resistance<a class="headerlink" href="#convex-21_models-162-robust-regression-and-outlier-resistance" title="Permanent link">¶</a></h2>
<p>Standard least-squares uses squared loss, which penalizes large errors quadratically. This makes it sensitive to outliers. Robust alternatives replace the loss:</p>
<h3 id="convex-21_models-1621-least-absolute-deviations-l1-loss">16.2.1 Least absolute deviations (ℓ₁ loss)<a class="headerlink" href="#convex-21_models-1621-least-absolute-deviations-l1-loss" title="Permanent link">¶</a></h3>
<p>Formulation:
<script type="math/tex; mode=display">
\min_x \sum_i \lvert a_i^\top x - b_i \rvert.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>This corresponds to assuming Laplace (double-exponential) noise on the residuals.</li>
<li>Unlike squared error, it penalizes big residuals <em>linearly</em>, not quadratically, so outliers hurt less.</li>
</ul>
<p>Geometry/structure:
- The objective is convex but nondifferentiable at zero residual (the kink in <span class="arithmatex">\(|r|\)</span> at <span class="arithmatex">\(r=0\)</span>).</p>
<p>How to solve it:</p>
<ol>
<li>
<p>As a linear program (LP).<br>
   Introduce slack variables <span class="arithmatex">\(t_i \ge 0\)</span> and rewrite:</p>
<ul>
<li>constraints:<br>
<span class="arithmatex">\(-t_i \le a_i^\top x - b_i \le t_i\)</span>,</li>
<li>objective:<br>
<span class="arithmatex">\(\min \sum_i t_i\)</span>.</li>
</ul>
<p>This is now a standard LP. You can solve it with:</p>
<ul>
<li>an interior-point LP solver,</li>
<li>or simplex.</li>
</ul>
<p>These methods give high-accuracy solutions and certificates.</p>
</li>
<li>
<p>First-order methods for large scale.  </p>
<p>For <em>very</em> large problems (millions of samples/features), you can apply:</p>
<ul>
<li>subgradient methods,</li>
<li>proximal methods (using the prox of <span class="arithmatex">\(|\cdot|\)</span>).</li>
</ul>
<p>These are slower in theory (subgradient is only <span class="arithmatex">\(O(1/\sqrt{t})\)</span> convergence), but they scale to huge data where generic LP solvers would struggle.</p>
</li>
</ol>
<h3 id="convex-21_models-1622-huber-loss">16.2.2 Huber loss<a class="headerlink" href="#convex-21_models-1622-huber-loss" title="Permanent link">¶</a></h3>
<p>Definition of the Huber penalty for residual <span class="arithmatex">\(r\)</span>:
<script type="math/tex; mode=display">
\rho_\delta(r) =
\begin{cases}
\frac{1}{2} r^2, & |r| \le \delta, \\
\delta |r| - \frac{1}{2}\delta^2, & |r| > \delta.
\end{cases}
</script>
</p>
<p>Huber regression solves:
<script type="math/tex; mode=display">
\min_x \sum_i \rho_\delta(a_i^\top x - b_i).
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For small residuals (<span class="arithmatex">\(|r|\le\delta\)</span>): it acts like least-squares (<span class="arithmatex">\(\tfrac{1}{2}r^2\)</span>). So inliers are fit tightly.</li>
<li>For large residuals (<span class="arithmatex">\(|r|&gt;\delta\)</span>): it acts like <span class="arithmatex">\(\ell_1\)</span> (linear penalty), so outliers get down-weighted.</li>
<li>Intuition: “be aggressive on normal data, be forgiving on outliers.”</li>
</ul>
<p>Properties:</p>
<ul>
<li><span class="arithmatex">\(\rho_\delta\)</span> is convex.</li>
<li>It is smooth except for a kink in its second derivative at <span class="arithmatex">\(|r|=\delta\)</span>.</li>
<li>Its gradient exists everywhere (the function is once-differentiable).</li>
</ul>
<p>How to solve it:</p>
<ol>
<li>
<p>Iteratively Reweighted Least Squares (IRLS) / quasi-Newton.<br>
    Because the loss is basically quadratic near the solution, Newton-type methods (including IRLS) work well and converge fast on moderate-size problems.</p>
</li>
<li>
<p>Proximal / first-order methods.<br>
    You can apply proximal gradient methods, since each term is simple and has a known prox.</p>
</li>
<li>
<p>As a conic program (SOCP).<br>
    The Huber objective can be written with auxiliary variables and second-order cone constraints.<br>
    That means you can feed it to an SOCP solver and let an interior-point method handle it efficiently and robustly.<br>
    This is attractive when you want high accuracy and dual certificates.</p>
</li>
</ol>
<h3 id="convex-21_models-1623-worst-case-robust-regression">16.2.3 Worst-case robust regression<a class="headerlink" href="#convex-21_models-1623-worst-case-robust-regression" title="Permanent link">¶</a></h3>
<p>Sometimes we don’t just want “fit the data we saw,” but “fit any data within some uncertainty set.” This leads to min–max problems of the form:
<script type="math/tex; mode=display">
\min_x \;\max_{u \in \mathcal{U}} \; \| (A + u)x - b \|_2.
</script>
</p>
<p>Meaning:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{U}\)</span> is an uncertainty set describing how much you distrust the matrix <span class="arithmatex">\(A\)</span>, the inputs, or the measurements.</li>
<li>You choose <span class="arithmatex">\(x\)</span> that performs well even in the worst allowed perturbation.</li>
</ul>
<p>Why this is still tractable:</p>
<ul>
<li>
<p>If <span class="arithmatex">\(\mathcal{U}\)</span> is convex (for example, an <span class="arithmatex">\(\ell_2\)</span> ball or box bounds on each entry), then the inner maximization often has a closed-form expression.</p>
</li>
<li>
<p>That inner max usually turns into an extra norm penalty or a conic constraint in the outer problem.</p>
<ul>
<li>Example: if the rows of <span class="arithmatex">\(A\)</span> can move within an <span class="arithmatex">\(\ell_2\)</span> ball of radius <span class="arithmatex">\(\epsilon\)</span>, the robustified problem often picks up an additional <span class="arithmatex">\(\ell_2\)</span> term like <span class="arithmatex">\(\gamma \|x\|_2\)</span> in the objective.</li>
<li>The final problem is still convex (often a QP or SOCP).</li>
</ul>
</li>
</ul>
<p>How to solve it:</p>
<ul>
<li>
<p>If it reduces to an LP / QP / SOCP, you can use an interior-point (conic) solver to get a high-quality solution and dual certificate.</p>
</li>
<li>
<p>If the structure is separable and high-dimensional, you can sometimes solve the dual or a proximal/ADMM splitting of the robust problem using first-order methods.</p>
</li>
</ul>
<h2 id="convex-21_models-163-maximum-likelihood-and-loss-design">16.3 Maximum likelihood and loss design<a class="headerlink" href="#convex-21_models-163-maximum-likelihood-and-loss-design" title="Permanent link">¶</a></h2>
<p>Choosing a loss often comes from a probabilistic noise model. The negative log-likelihood (NLL) of an assumed distribution gives a convex loss for many common cases:</p>
<ul>
<li>
<p>Gaussian (normal) noise</p>
<p>Model:
<script type="math/tex; mode=display">
b = A x + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I).
</script>
</p>
<p>The negative log-likelihood (NLL) is proportional to:
<script type="math/tex; mode=display">
|A x - b|_2^2.
</script>
</p>
<p>This recovers the classic least-squares loss (as in linear regression).<br>
It is smooth and convex (strongly convex if <span class="arithmatex">\(A^T A\)</span> is full rank).</p>
<p>Algorithms:</p>
<ul>
<li>
<p>Closed-form via <span class="arithmatex">\((A^T A + \lambda I)^{-1} A^T b\)</span> (for ridge regression),</p>
</li>
<li>
<p>Iterative methods: conjugate gradient, gradient descent (Chapter 9),</p>
</li>
<li>
<p>Or Newton / quasi-Newton methods (Chapter 9) using the constant Hessian <span class="arithmatex">\(A^T A\)</span>.</p>
</li>
</ul>
</li>
<li>
<p>Laplace (double-exponential) noise</p>
<p>If <span class="arithmatex">\(\varepsilon_i \sim \text{Laplace}(0, b)\)</span> i.i.d., the NLL is proportional to:
<script type="math/tex; mode=display">
\sum_i |a_i^T x - b_i|.
</script>
</p>
<p>This is exactly the ℓ₁ regression (least absolute deviations).<br>
It can be solved as an LP or with robust optimization solvers (interior-point),<br>
or with first-order nonsmooth methods (subgradient/proximal) for large-scale problems.</p>
</li>
<li>
<p>Logistic model (binary classification)</p>
<p>For <span class="arithmatex">\(y_i \in \{0,1\}\)</span>, model:
<script type="math/tex; mode=display">
\Pr(y_i = 1 \mid a_i, x) = \sigma(a_i^T x),
\quad \text{where } \sigma(z) = \frac{1}{1 + e^{-z}}.
</script>
</p>
<p>The negative log-likelihood (logistic loss) is:
<script type="math/tex; mode=display">
\sum_i \left[ -y_i (a_i^T x) + \log(1 + e^{a_i^T x}) \right].
</script>
</p>
<p>This loss is convex and smooth in <span class="arithmatex">\(x\)</span>.<br>
No closed-form solution exists.</p>
<p>Algorithms:</p>
<ul>
<li>With ℓ₂ regularization: smooth and (if <span class="arithmatex">\(\lambda&gt;0\)</span>) strongly convex → use accelerated gradient or quasi-Newton (e.g. L-BFGS).</li>
<li>With ℓ₁ regularization (sparse logistic): composite convex → use proximal gradient (soft-thresholding) or coordinate descent.</li>
</ul>
</li>
<li>
<p>Softmax / Multinomial logistic (multiclass)</p>
<p>For <span class="arithmatex">\(K\)</span> classes with one-hot labels <span class="arithmatex">\(y_i \in \{e_1, \dots, e_K\}\)</span>, the softmax model gives NLL:
<script type="math/tex; mode=display">
-\sum_i \sum_{k=1}^K y_{ik}(a_i^T x_k)
+ \log\!\left(\sum_{j=1}^K e^{a_i^T x_j}\right).
</script>
</p>
<p>This loss is convex in the weight vectors <span class="arithmatex">\(\{x_k\}\)</span> and generalizes binary logistic to multiclass.</p>
<p>Algorithms:</p>
<ul>
<li>Gradient-based solvers (L-BFGS, Newton with block Hessian) for moderate size.</li>
<li>Stochastic gradient (SGD, Adam) for large datasets.</li>
</ul>
</li>
<li>
<p>Generalized linear models (GLMs)</p>
<p>In GLMs, <span class="arithmatex">\(y_i\)</span> given <span class="arithmatex">\(x\)</span> has an exponential-family distribution (Poisson, binomial, etc.) with mean related to <span class="arithmatex">\(a_i^T x\)</span>.<br>
The NLL is convex in <span class="arithmatex">\(x\)</span> for canonical links (e.g. log-link for Poisson, logit for binomial).</p>
<p>Examples:</p>
<ul>
<li>Poisson regression for counts: convex NLL, solved by IRLS or gradient.</li>
<li>Probit models: convex but require iterative solvers.</li>
</ul>
</li>
</ul>
<h2 id="convex-21_models-164-structured-constraints-in-engineering-and-design">16.4 Structured constraints in engineering and design<a class="headerlink" href="#convex-21_models-164-structured-constraints-in-engineering-and-design" title="Permanent link">¶</a></h2>
<p>Optimization problems often include explicit convex constraints from physical or resource limits: e.g. variable bounds, norm limits, budget constraints. The solver choice depends on how easily we can handle projections or barriers for <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<ul>
<li>
<p>Simple (projection-friendly) constraints</p>
<p>Examples:</p>
<ul>
<li>
<p>Box constraints: <span class="arithmatex">\(l \le x \le u\)</span><br>
    → Projection: clip each entry to <span class="arithmatex">\([\ell_i, u_i]\)</span>.</p>
</li>
<li>
<p>ℓ₂-ball: <span class="arithmatex">\(\|x\|_2 \le R\)</span><br>
    → Projection: rescale <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(\|x\|_2 &gt; R\)</span>.</p>
</li>
<li>
<p>Simplex: <span class="arithmatex">\(\{x \ge 0, \sum_i x_i = 1\}\)</span><br>
    → Projection: sort and threshold coordinates (simple <span class="arithmatex">\(O(n \log n)\)</span> algorithm).</p>
</li>
</ul>
</li>
<li>
<p>General convex constraints (non-projection-friendly)
If constraints are complex (e.g. second-order cones, semidefinite, or many coupled inequalities), projections are hard. Two strategies:</p>
<ol>
<li>
<p>Barrier / penalty and interior-point methods : Add a log-barrier or penalty and solve with an interior-point solver (Chapter 9). This handles general convex constraints well and returns dual variables (Lagrange multipliers) as a bonus.</p>
</li>
<li>
<p>Conic formulation + solver: Write the problem as an LP/QP/SOCP/SDP and use specialized solvers (like MOSEK, Gurobi) that exploit sparse structure. If only first-order methods are feasible for huge problems, one can apply dual decomposition or ADMM by splitting constraints (Chapter 10), but convergence will be slower.</p>
</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers for 11.4:</p>
<ul>
<li>Projection-friendly constraints → Projected (stochastic) gradient or proximal methods (fast, maintain feasibility).</li>
<li>Complex constraints (cones, PSD, many linear) → Use interior-point/conic solvers (Chapter 9) for moderate size. Alternatively, use operator-splitting (ADMM) if parallel/distributed solution is needed (Chapter 10).</li>
<li>LP/QP special cases → Use simplex or specialized LP/QP solvers (Section 11.5).</li>
</ul>
<p>Remarks: Encoding design requirements (actuator limits, stability margins, probability budgets) as convex constraints lets us leverage efficient convex solvers. Feasible set geometry dictates the method: easy projection → projective methods; otherwise → interior-point or operator-splitting.</p>
<h2 id="convex-21_models-165-linear-and-conic-programming-the-canonical-models">16.5 Linear and conic programming: the canonical models<a class="headerlink" href="#convex-21_models-165-linear-and-conic-programming-the-canonical-models" title="Permanent link">¶</a></h2>
<p>Many practical problems reduce to linear programming (LP) or its convex extensions.<br>
LP and related conic forms are the workhorses of operations research, control, and engineering optimization.</p>
<ul>
<li>
<p>Linear programs: standard form</p>
<p>
<script type="math/tex; mode=display">
\min_x \; c^T x 
\quad \text{s.t.} \quad A x = b, \; x \ge 0.
</script>
Both objective and constraints are affine, so the optimum lies at a vertex of the polyhedron. Simplex method traverses vertices and is often very fast in practice. Interior-point methods approach the optimum through the interior and have polynomial-time guarantees. For moderate LPs, interior-point is robust and accurate; for very large LPs (sparse, structured), first-order methods or decomposition may be needed.
- Quadratic, SOCP, SDP:
Convex quadratic programs (QP), second-order cone programs (SOCP), and semidefinite programs (SDP) generalize LP. For example, many robust or regularized problems (elastic net, robust regression, classification with norm constraints) can be cast as QPs or SOCPs. All these are solvable by interior-point (Chapter 9) very efficiently. Interior-point solvers (like MOSEK, Gurobi, etc.) are widely used off-the-shelf for these problem classes.</p>
</li>
<li>
<p>Practical patterns:</p>
<ol>
<li>Resource allocation/flow (LP): linear costs and constraints.</li>
<li>Minimax/regret problems: e.g. <span class="arithmatex">\(\min_{x}\max_{i}|a_i^T x - b_i|\)</span> → LP (as in Chebyshev regression).</li>
<li>Constrained least squares: can be QP or SOCP if constraints are polyhedral or norm-based.</li>
</ol>
</li>
</ul>
<p>Algorithmic pointers for 11.5:
- Moderate LP/QP/SOCP: Use interior-point (robust, yields dual prices) or simplex (fast in practice, warm-startable).
- Large-scale LP/QP: Exploit sparsity; use decomposition (Benders, ADMM) if structure allows; use iterative methods (primal-dual hybrid gradient, etc.) for extreme scale.
- Reformulate into standard form: Recognize when your problem is an LP/QP/SOCP/SDP to use mature solvers. (E.g. ℓ∞ regression → LP, ℓ2 regression with ℓ2 constraint → SOCP.)</p>
<h2 id="convex-21_models-166-risk-safety-margins-and-robust-design">16.6 Risk, safety margins, and robust design<a class="headerlink" href="#convex-21_models-166-risk-safety-margins-and-robust-design" title="Permanent link">¶</a></h2>
<p>Modern design often includes risk measures or robustness. Two common patterns:</p>
<ul>
<li>
<p>Chance constraints / risk-adjusted objectives
    E.g. require that <span class="arithmatex">\(Pr(\text{loss}(x,\xi) &gt; \tau) \le \delta\)</span>. A convex surrogate is to include mean and a multiple of the standard deviation:
    <script type="math/tex; mode=display">
    \min_x \; \mathbb{E}[\ell(x, \xi)] + \kappa \sqrt{\mathrm{Var}[\ell(x, \xi)]}.
    </script>
    Algebra often leads to second-order cone constraints (e.g. forcing <span class="arithmatex">\(\mathbb{E}\pm \kappa\sqrt{\mathrm{Var}}\)</span> below a threshold). Such problems are SOCPs. Interior-point solvers handle them well (polynomial-time, high accuracy).</p>
</li>
<li>
<p>Worst-case (robust) optimization:
    Specify an uncertainty set <span class="arithmatex">\(\mathcal{U}\)</span> for data (e.g. <span class="arithmatex">\(u\)</span> in a norm-ball) and minimize the worst-case cost <span class="arithmatex">\(\max_{u\in\mathcal{U}}\ell(x,u)\)</span>. Many losses <span class="arithmatex">\(\ell\)</span> and convex <span class="arithmatex">\(\mathcal{U}\)</span> yield a convex max-term (a support function or norm). The result is often a conic constraint (for ℓ₂ norms, an SOCP; for PSD, an SDP). Solve with interior-point (if problem size permits) or with specialized proximal/ADMM methods (splitting the max-term).</p>
</li>
</ul>
<p>Algorithmic pointers for 11.6:</p>
<ul>
<li>Risk/SOCP models: Interior-point (Chapter 9) is the standard approach.</li>
<li>Robust max-min problems: Convert inner max to a convex constraint (norm or cone). Then use interior-point if the reformulation is conic. If the reformulation is a nonsmooth penalty, use proximal or dual subgradient methods.</li>
<li>Distributed or iterative solutions: If <span class="arithmatex">\(\mathcal{U}\)</span> or loss separable, ADMM can distribute the computation (Chapter 10).</li>
</ul>
<h2 id="convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that">16.7 Cheat sheet: If your problem looks like this, use that<a class="headerlink" href="#convex-21_models-167-cheat-sheet-if-your-problem-looks-like-this-use-that" title="Permanent link">¶</a></h2>
<p>This summary gives concrete patterns of models and recommended solvers/tricks:</p>
<ul>
<li>
<p>(A) Smooth least-squares + ℓ₂:</p>
<ul>
<li>Model: <span class="arithmatex">\(|Ax-b|_2^2 + \lambda|x|_2^2\)</span>. </li>
<li>Solve: Gradient descent, accelerated gradient, conjugate gradient, or Newton/quasi-Newton. (Strongly convex quadratic ⇒ fast second-order methods.)</li>
</ul>
</li>
<li>
<p>(B) Sparse regression (ℓ₁):</p>
<ul>
<li>Model: <span class="arithmatex">\(\tfrac12|Ax-b|_2^2 + \lambda|x|_1\)</span>. </li>
<li>Solve: Proximal gradient (soft-thresholding) or coordinate descent. (Composite smooth+nonsmooth separable.)</li>
</ul>
</li>
<li>
<p>(C) Robust regression (outliers):</p>
<ul>
<li>Models: <span class="arithmatex">\(\sum|a_i^T x - b_i|\)</span>, Huber loss, etc. </li>
<li>Solve: Interior-point (LP/SOCP form) for high accuracy; subgradient/proximal (Chapter 9/10) for large data. (Convex but nondifferentiable or conic.)</li>
</ul>
</li>
<li>
<p>(D) Logistic / log-loss (classification):</p>
<ul>
<li>Model: <span class="arithmatex">\(\sum[-y_i(w^Ta_i)+\log(1+e^{w^Ta_i})] + \lambda R(w)\)</span> with <span class="arithmatex">\(R(w)=|w|_2^2\)</span> or <span class="arithmatex">\(|w|_1\)</span>. </li>
<li>Solve:<ul>
<li>If <span class="arithmatex">\(R=\ell_2\)</span>: use Newton/gradient (smooth, strongly convex).</li>
<li>If <span class="arithmatex">\(R=\ell_1\)</span>: use proximal gradient or coordinate descent. (Convex; logistic loss is smooth; ℓ₁ adds nonsmoothness.)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>(E) Constraints (hard limits):</p>
<ul>
<li>Model: <span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(x\in\mathcal{X}\)</span> with <span class="arithmatex">\(\mathcal{X}\)</span> simple. </li>
<li>Solve: Projected (stochastic) gradient or proximal methods if projection <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> is cheap (e.g. box, ball, simplex). If <span class="arithmatex">\(\mathcal{X}\)</span> is complex (second-order or SDP), use interior-point.</li>
</ul>
</li>
<li>
<p>(F) Separable structure:</p>
<ul>
<li>Model: <span class="arithmatex">\(\min_{x,z} f(x)+g(z)\)</span> s.t. <span class="arithmatex">\(Ax+Bz=c\)</span>. </li>
<li>Solve: ADMM (Chapter 10) – it decouples updates in <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(z\)</span>; suits distributed or block-structured data.</li>
</ul>
</li>
<li>
<p>(G) LP/QP/SOCP/SDP:</p>
<ul>
<li>Model: linear/quadratic objective with linear/conic constraints. </li>
<li>Solve: Simplex or interior-point (for moderate sizes). For very large sparse LPs exploit problem structure: warm-starts, decomposition methods (dual/block), or first-order methods (PDHG/ADMM).</li>
</ul>
</li>
<li>
<p>(H) Nonconvex patterns:</p>
<ul>
<li>
<p>Examples: Deep neural networks (nonconvex weights), matrix factorization (bilinear), K-means clustering, mixture models. </p>
</li>
<li>
<p>Solve: There is no single global solver – typically use stochastic gradient (SGD/Adam), alternating minimization (e.g. alternating least squares for matrix factorization), or EM for mixtures. Caveat: Convergence to global optimum is not guaranteed; solutions depend on initialization and may get stuck in local minima. Use regularization, multiple restarts, and heuristics (batch normalization, momentum) as needed.</p>
</li>
</ul>
</li>
<li>
<p>(I) Logistic (multi-class softmax):</p>
<ul>
<li>
<p>Model: One weight vector per class, convex softmax loss (see Section 11.3). </p>
</li>
<li>
<p>Solve: Similar to binary case – Newton/gradient with L2, or proximal/coordinate with ℓ₁.</p>
</li>
</ul>
</li>
<li>
<p>(J) Poisson and count models:</p>
<ul>
<li>Model: Negative log-likelihood for Poisson (convex, see Section 11.3). </li>
<li>Solve: Newton (IRLS) or gradient-based; interior-point can be used after conic reformulation.</li>
</ul>
</li>
</ul>
<p>Rule of thumb: Identify whether your objective is smooth vs nonsmooth, strongly convex vs just convex, separable vs coupled, constrained vs unconstrained. Then pick from:</p>
<ul>
<li>Smooth &amp; strongly convex → (quasi-)Newton or accelerated gradient.</li>
<li>Smooth + ℓ₁ → Proximal gradient/coordinate.</li>
<li>Nonsmooth separable → Proximal or coordinate.</li>
<li>Easy projection constraint → Projected gradient.</li>
<li>Hard constraints or conic structure → Interior-point.</li>
<li>Large-scale separable → Stochastic gradient/ADMM.</li>
</ul>
<p>Convexity guarantees global optimum. When nonconvex (deep nets, latent variables, etc.), we rely on heuristics: SGD, random restarts, and often settle for local minima or approximate solutions.</p>
<h2 id="convex-21_models-167-matching-model-structure-to-algorithm-type">16.7 Matching Model Structure to Algorithm Type<a class="headerlink" href="#convex-21_models-167-matching-model-structure-to-algorithm-type" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Problem Form</th>
<th>Recommended Algorithms</th>
<th>Notes / ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span></td>
<td>Gradient descent, Newton, LBFGS</td>
<td>Small to medium problems; logistic regression, ridge regression</td>
</tr>
<tr>
<td>Nonsmooth unconstrained</td>
<td><span class="arithmatex">\(\min f(x) + R(x)\)</span></td>
<td>Subgradient, proximal (ISTA/FISTA), coordinate descent</td>
<td>LASSO, hinge loss SVM</td>
</tr>
<tr>
<td>Equality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(A x = b\)</span></td>
<td>Projected gradient, augmented Lagrangian</td>
<td>Constrained least squares, balance conditions</td>
</tr>
<tr>
<td>Inequality-constrained</td>
<td><span class="arithmatex">\(\min f(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le 0\)</span></td>
<td>Barrier, primal–dual, interior-point</td>
<td>Quadratic programming, LPs, constrained regression</td>
</tr>
<tr>
<td>Separable / block structure</td>
<td><span class="arithmatex">\(\min \sum_i f_i(x_i)\)</span></td>
<td>ADMM, coordinate updates</td>
<td>Distributed optimization, federated learning</td>
</tr>
<tr>
<td>Stochastic / large data</td>
<td><span class="arithmatex">\(\min \frac{1}{N}\sum_i f_i(x_i)\)</span></td>
<td>SGD, SVRG, adaptive variants</td>
<td>Deep learning, online convex optimization</td>
</tr>
<tr>
<td>Low-rank / matrix structure</td>
<td><span class="arithmatex">\(\min f(X) + \lambda \|X\|_*\)</span></td>
<td>Proximal (SVD shrinkage), ADMM</td>
<td>Matrix completion, PCA variants</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-30_canonical_problems" heading-number="2.17"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-17-canonical-problems-in-convex-optimization">Chapter 17: Canonical Problems in Convex Optimization<a class="headerlink" href="#convex-30_canonical_problems-chapter-17-canonical-problems-in-convex-optimization" title="Permanent link">¶</a></h1>
<p>Convex optimization encompasses a wide range of problem classes.  Despite their diversity, many real-world models reduce to a few canonical forms — each with characteristic geometry, structure, and algorithms.</p>
<h2 id="convex-30_canonical_problems-171-hierarchy-of-canonical-problems">17.1 Hierarchy of Canonical Problems<a class="headerlink" href="#convex-30_canonical_problems-171-hierarchy-of-canonical-problems" title="Permanent link">¶</a></h2>
<p>Convex programs form a nested hierarchy:</p>
<div class="arithmatex">\[
\text{LP} \subseteq \text{QP} \subseteq \text{SOCP} \subseteq \text{SDP}.
\]</div>
<p>Each inclusion represents an extension of representational power — from linear to quadratic, to conic, and finally to semidefinite constraints.<br>
Separately, Geometric Programs (GPs) and Maximum Likelihood Estimators (MLEs) form additional convex families after suitable transformations.</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Canonical Form</th>
<th>Key Condition</th>
<th>Typical Algorithms</th>
<th>ML / Applied Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td><span class="arithmatex">\(\min_x c^\top x\)</span> s.t. <span class="arithmatex">\(A x=b,\,x\ge0\)</span></td>
<td>Linear constraints</td>
<td>Simplex, Interior-point</td>
<td>Resource allocation, Chebyshev regression</td>
</tr>
<tr>
<td>QP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top Q x + c^\top x\)</span> s.t. <span class="arithmatex">\(A x\le b\)</span></td>
<td><span class="arithmatex">\(Q\succeq0\)</span></td>
<td>Interior-point, Active-set, CG</td>
<td>Ridge, SVM, Portfolio optimization</td>
</tr>
<tr>
<td>QCQP</td>
<td><span class="arithmatex">\(\min_x \tfrac12 x^\top P_0 x + q_0^\top x\)</span> s.t. <span class="arithmatex">\(\tfrac12 x^\top P_i x + q_i^\top x \le0\)</span></td>
<td>All <span class="arithmatex">\(P_i\succeq0\)</span></td>
<td>Interior-point, SOCP reformulation</td>
<td>Robust regression, trust-region</td>
</tr>
<tr>
<td>SOCP</td>
<td><span class="arithmatex">\(\min_x f^\top x\)</span> s.t. <span class="arithmatex">\(\|A_i x + b_i\|_2 \le c_i^\top x + d_i\)</span></td>
<td>Cone constraints</td>
<td>Conic interior-point</td>
<td>Robust least-squares, risk limits</td>
</tr>
<tr>
<td>SDP</td>
<td><span class="arithmatex">\(\min_X \mathrm{Tr}(C^\top X)\)</span> s.t. <span class="arithmatex">\(\mathrm{Tr}(A_i^\top X)=b_i\)</span>, <span class="arithmatex">\(X\succeq0\)</span></td>
<td>Matrix PSD constraint</td>
<td>Interior-point, low-rank first-order</td>
<td>Covariance estimation, control</td>
</tr>
<tr>
<td>GP</td>
<td><span class="arithmatex">\(\min_{x&gt;0} f_0(x)\)</span> s.t. <span class="arithmatex">\(f_i(x)\le1,\,g_j(x)=1\)</span></td>
<td>Log-convex after <span class="arithmatex">\(y=\log x\)</span></td>
<td>Log-transform + IPM</td>
<td>Circuit design, power control</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>$\min_x -\sum_i \log p(b_i</td>
<td>a_i^\top x)+\mathcal{R}(x)$</td>
<td>Log-concave likelihood</td>
<td>Newton, L-BFGS, Prox / SGD</td>
</tr>
</tbody>
</table>
<h2 id="convex-30_canonical_problems-172-linear-programming-lp">17.2 Linear Programming (LP)<a class="headerlink" href="#convex-30_canonical_problems-172-linear-programming-lp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x c^\top x \quad \text{s.t. } A x=b,\, x\ge0
\]</div>
<p>Geometry: Feasible region = polyhedron; optimum = vertex.<br>
Applications: Resource allocation, shortest path, flow, scheduling.<br>
Algorithms:</p>
<ol>
<li>Simplex: walks along edges (vertex-based).  </li>
<li>Interior-point: moves through the interior using log barriers.  </li>
<li>Decomposition: exploits block structure for large LPs.</li>
</ol>
<h2 id="convex-30_canonical_problems-173-quadratic-programming-qp">17.3 Quadratic Programming (QP)<a class="headerlink" href="#convex-30_canonical_problems-173-quadratic-programming-qp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top Q x + c^\top x 
\quad \text{s.t. } A x \le b,\, F x = g, \quad Q\succeq0
\]</div>
<p>Geometry: Objective = ellipsoids; feasible = polyhedron.<br>
Examples: Ridge regression, Markowitz portfolio, SVM.<br>
Algorithms:<br>
- Interior-point (smooth path).<br>
- Active-set (edge-following).<br>
- Conjugate Gradient for large unconstrained QPs.<br>
- First-order methods for massive <span class="arithmatex">\(n\)</span>.</p>
<h2 id="convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp">17.4 Quadratically Constrained QP (QCQP)<a class="headerlink" href="#convex-30_canonical_problems-174-quadratically-constrained-qp-qcqp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x \tfrac12 x^\top P_0x + q_0^\top x
\quad \text{s.t. } \tfrac12 x^\top P_i x + q_i^\top x + r_i \le 0
\]</div>
<p>Convex if all <span class="arithmatex">\(P_i\succeq0\)</span>.<br>
Geometry: Intersection of ellipsoids and half-spaces.<br>
Applications: Robust control, filter design, trust-region.<br>
Algorithms: Interior-point (convex case), SOCP / SDP reformulations.</p>
<h2 id="convex-30_canonical_problems-175-second-order-cone-programming-socp">17.5 Second-Order Cone Programming (SOCP)<a class="headerlink" href="#convex-30_canonical_problems-175-second-order-cone-programming-socp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_x f^\top x
\quad \text{s.t. } 
\|A_i x + b_i\|_2 \le c_i^\top x + d_i,\;
F x = g
\]</div>
<p>Interpretation: Linear objective, norm-bounded constraints.<br>
Applications: Robust regression, risk-aware portfolio, engineering design.<br>
Algorithms: Conic interior-point; scalable ADMM variants.<br>
Special case: Any QP or norm constraint can be written as an SOCP.</p>
<h2 id="convex-30_canonical_problems-176-semidefinite-programming-sdp">17.6 Semidefinite Programming (SDP)<a class="headerlink" href="#convex-30_canonical_problems-176-semidefinite-programming-sdp" title="Permanent link">¶</a></h2>
<p>Form</p>
<div class="arithmatex">\[
\min_X \mathrm{Tr}(C^\top X)
\quad \text{s.t. } \mathrm{Tr}(A_i^\top X)=b_i,\; X\succeq0
\]</div>
<p>Meaning: Variable = PSD matrix <span class="arithmatex">\(X\)</span>; constraints = affine.<br>
Geometry: Feasible region = intersection of affine space with PSD cone.<br>
Applications: Control synthesis, combinatorial relaxations, covariance estimation, matrix completion.<br>
Algorithms: Interior-point for moderate <span class="arithmatex">\(n\)</span>; low-rank proximal / Frank–Wolfe for large-scale.</p>
<h2 id="convex-30_canonical_problems-177-geometric-programming-gp">17.7 Geometric Programming (GP)<a class="headerlink" href="#convex-30_canonical_problems-177-geometric-programming-gp" title="Permanent link">¶</a></h2>
<p>Original form</p>
<div class="arithmatex">\[
\min_{x&gt;0} f_0(x)
\quad \text{s.t. } f_i(x)\le1,\; g_j(x)=1
\]</div>
<p>where <span class="arithmatex">\(f_i\)</span> are posynomials and <span class="arithmatex">\(g_j\)</span> monomials.  </p>
<p>Log transformation: With <span class="arithmatex">\(y=\log x\)</span>, the problem becomes convex in <span class="arithmatex">\(y\)</span>.<br>
Applications: Circuit sizing, communication power control, resource allocation.<br>
Solvers: Convert to convex form → interior-point or primal-dual methods.</p>
<h2 id="convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms">17.8 Likelihood-Based Convex Models (MLE and GLMs)<a class="headerlink" href="#convex-30_canonical_problems-178-likelihood-based-convex-models-mle-and-glms" title="Permanent link">¶</a></h2>
<p>General form</p>
<div class="arithmatex">\[
\min_x -\sum_i \log p(b_i|a_i^\top x) + \mathcal{R}(x)
\]</div>
<p>Examples</p>
<table>
<thead>
<tr>
<th>Noise Model</th>
<th>Objective</th>
<th>Equivalent Problem</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian</td>
<td><span class="arithmatex">\(\|A x - b\|_2^2\)</span></td>
<td>Least squares</td>
</tr>
<tr>
<td>Laplacian</td>
<td><span class="arithmatex">\(\|A x - b\|_1\)</span></td>
<td>Robust regression (LP)</td>
</tr>
<tr>
<td>Bernoulli</td>
<td><span class="arithmatex">\(\sum_i \log(1+e^{-y_i a_i^\top x})\)</span></td>
<td>Logistic regression</td>
</tr>
<tr>
<td>Poisson</td>
<td><span class="arithmatex">\(\sum_i [a_i^\top x - y_i\log(a_i^\top x)]\)</span></td>
<td>Poisson GLM</td>
</tr>
</tbody>
</table>
<p>Algorithms<br>
- Newton or IRLS (small–medium).<br>
- Quasi-Newton / L-BFGS (moderate).<br>
- Proximal or SGD (large-scale).</p>
<h2 id="convex-30_canonical_problems-179-solver-selection-summary">17.9 Solver Selection Summary<a class="headerlink" href="#convex-30_canonical_problems-179-solver-selection-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Convex Form</th>
<th>Key Solvers</th>
<th>ML Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>LP</td>
<td>Linear</td>
<td>Simplex, Interior-point</td>
<td>Minimax regression</td>
</tr>
<tr>
<td>QP</td>
<td>Quadratic</td>
<td>Interior-point, CG, Active-set</td>
<td>Ridge, SVM</td>
</tr>
<tr>
<td>QCQP</td>
<td>Quadratic + constraints</td>
<td>IPM, SOCP / SDP reformulation</td>
<td>Robust regression</td>
</tr>
<tr>
<td>SOCP</td>
<td>Cone</td>
<td>Conic IPM, ADMM</td>
<td>Robust least-squares</td>
</tr>
<tr>
<td>SDP</td>
<td>PSD cone</td>
<td>Interior-point, low-rank FW</td>
<td>Covariance, Max-cut relaxations</td>
</tr>
<tr>
<td>GP</td>
<td>Log-convex</td>
<td>Log-transform + IPM</td>
<td>Power allocation</td>
</tr>
<tr>
<td>MLE / GLM</td>
<td>Log-concave</td>
<td>Newton, L-BFGS, Prox-SGD</td>
<td>Logistic regression</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-35_modern" heading-number="2.18"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-18-modern-optimizers-in-machine-learning">Chapter 18: Modern Optimizers in Machine Learning<a class="headerlink" href="#convex-35_modern-chapter-18-modern-optimizers-in-machine-learning" title="Permanent link">¶</a></h1>
<p>The past decade has seen an explosion of nonconvex optimization problems, driven largely by deep learning. Training neural networks, large language models, and reinforcement learning agents all depend on stochastic optimization—balancing accuracy, generalization, and efficiency on massive, noisy datasets.</p>
<p>This chapter connects the principles of convex optimization to the modern optimizers that power today’s machine learning systems. While these algorithms often lack formal global guarantees, they are remarkably effective in practice.</p>
<h2 id="convex-35_modern-181-stochastic-optimization-overview">18.1 Stochastic Optimization Overview<a class="headerlink" href="#convex-35_modern-181-stochastic-optimization-overview" title="Permanent link">¶</a></h2>
<p>In machine learning, we often minimize an empirical risk:
<script type="math/tex; mode=display">
\min_{x \in \mathbb{R}^n} \; f(x) = \frac{1}{N} \sum_{i=1}^N \ell(x; z_i),
</script>
where <span class="arithmatex">\(\ell(x; z_i)\)</span> is the loss on data sample <span class="arithmatex">\(z_i\)</span>.</p>
<p>Computing the full gradient <span class="arithmatex">\(\nabla f(x)\)</span> is infeasible when <span class="arithmatex">\(N\)</span> is large. Instead, stochastic methods estimate it using a mini-batch of samples:</p>
<div class="arithmatex">\[
g_k = \frac{1}{|B_k|} \sum_{i \in B_k} \nabla \ell(x_k; z_i).
$$
This yields the Stochastic Gradient Descent (SGD) update:
$$
x_{k+1} = x_k - \alpha_k g_k.
\]</div>
<p>SGD is the foundation for nearly all deep learning optimizers.</p>
<h2 id="convex-35_modern-182-momentum-and-acceleration">18.2 Momentum and Acceleration<a class="headerlink" href="#convex-35_modern-182-momentum-and-acceleration" title="Permanent link">¶</a></h2>
<p>SGD’s noisy gradients can cause slow convergence and oscillations. Momentum smooths the update by accumulating a moving average of past gradients:</p>
<p>
<script type="math/tex; mode=display">
v_{k+1} = \beta v_k + (1-\beta) g_k, \quad x_{k+1} = x_k - \alpha v_{k+1},
</script>
where <span class="arithmatex">\(\beta \in [0,1)\)</span> controls inertia.</p>
<p>Nesterov momentum adds a correction term anticipating the future position:</p>
<div class="arithmatex">\[
v_{k+1} = \beta v_k + g(x_k - \alpha \beta v_k), \quad x_{k+1} = x_k - \alpha v_{k+1}.
\]</div>
<p>Momentum-based methods help traverse ravines and saddle regions efficiently.</p>
<h2 id="convex-35_modern-183-adaptive-learning-rate-methods">18.3 Adaptive Learning Rate Methods<a class="headerlink" href="#convex-35_modern-183-adaptive-learning-rate-methods" title="Permanent link">¶</a></h2>
<p>Different parameters often require different step sizes.<br>
Adaptive methods adjust learning rates automatically using the history of squared gradients.</p>
<h3 id="convex-35_modern-1831-adagrad">18.3.1 AdaGrad<a class="headerlink" href="#convex-35_modern-1831-adagrad" title="Permanent link">¶</a></h3>
<p>Keeps a cumulative sum of squared gradients:</p>
<p>
<script type="math/tex; mode=display">
G_k = \sum_{t=1}^k g_t \odot g_t,
</script>
and updates parameters as:</p>
<p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - \frac{\alpha}{\sqrt{G_k + \epsilon}} \odot g_k.
</script>
Good for sparse data, but the learning rate can shrink too quickly.</p>
<h3 id="convex-35_modern-1832-rmsprop">18.3.2 RMSProp<a class="headerlink" href="#convex-35_modern-1832-rmsprop" title="Permanent link">¶</a></h3>
<p>A refinement of AdaGrad using exponential averaging:</p>
<div class="arithmatex">\[
E[g^2]_k = \beta E[g^2]_{k-1} + (1-\beta) g_k^2,
\]</div>
<div class="arithmatex">\[
x_{k+1} = x_k - \frac{\alpha}{\sqrt{E[g^2]_k + \epsilon}} g_k.
\]</div>
<p>RMSProp prevents the learning rate from vanishing and works well for nonstationary objectives.</p>
<h3 id="convex-35_modern-1833-adam-adaptive-moment-estimation">18.3.3 Adam: Adaptive Moment Estimation<a class="headerlink" href="#convex-35_modern-1833-adam-adaptive-moment-estimation" title="Permanent link">¶</a></h3>
<p>Adam combines momentum and adaptive scaling:</p>
<div class="arithmatex">\[
m_k = \beta_1 m_{k-1} + (1-\beta_1) g_k, \quad v_k = \beta_2 v_{k-1} + (1-\beta_2) g_k^2,
\]</div>
<div class="arithmatex">\[
\hat{m}_k = \frac{m_k}{1-\beta_1^k}, \quad \hat{v}_k = \frac{v_k}{1-\beta_2^k},
\]</div>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha \frac{\hat{m}_k}{\sqrt{\hat{v}_k} + \epsilon}.
\]</div>
<p>Adam adapts quickly to changing gradient scales, converging faster than vanilla SGD.</p>
<h2 id="convex-35_modern-184-variants-and-modern-extensions">18.4 Variants and Modern Extensions<a class="headerlink" href="#convex-35_modern-184-variants-and-modern-extensions" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Key Idea</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>AdamW</td>
<td>Decoupled weight decay from gradient update</td>
<td>Better regularization</td>
</tr>
<tr>
<td>RAdam</td>
<td>Rectified Adam—adaptive variance correction</td>
<td>Improves stability early in training</td>
</tr>
<tr>
<td>Lookahead</td>
<td>Combines fast and slow weights</td>
<td>Enhances robustness and convergence</td>
</tr>
<tr>
<td>AdaBelief</td>
<td>Uses prediction error instead of raw gradient variance</td>
<td>More adaptive learning rates</td>
</tr>
<tr>
<td>Lion</td>
<td>Uses sign-based updates and momentum</td>
<td>Efficient for large-scale training</td>
</tr>
</tbody>
</table>
<p>These variants represent the frontier of stochastic optimization in deep learning frameworks.</p>
<h2 id="convex-35_modern-185-implicit-regularization-and-generalization">18.5 Implicit Regularization and Generalization<a class="headerlink" href="#convex-35_modern-185-implicit-regularization-and-generalization" title="Permanent link">¶</a></h2>
<p>Modern optimizers not only minimize loss—they also affect generalization. SGD and its variants exhibit implicit bias toward flat minima, which often correspond to models with better generalization properties.</p>
<p>Empirical findings suggest:</p>
<ul>
<li>Large-batch training finds sharper minima (risk of overfitting).  </li>
<li>Noisy, small-batch SGD promotes flat, generalizable minima.  </li>
<li>Adaptive optimizers may converge faster but generalize slightly worse.</li>
</ul>
<p>This trade-off drives ongoing research into optimizer design.</p>
<h2 id="convex-35_modern-186-practical-considerations">18.6 Practical Considerations<a class="headerlink" href="#convex-35_modern-186-practical-considerations" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>Most critical hyperparameter; use warm-up and decay schedules</td>
</tr>
<tr>
<td>Batch Size</td>
<td>Balances gradient noise and hardware efficiency</td>
</tr>
<tr>
<td>Initialization</td>
<td>Affects early dynamics, especially for Adam variants</td>
</tr>
<tr>
<td>Gradient Clipping</td>
<td>Prevents instability in exploding gradients</td>
</tr>
<tr>
<td>Mixed Precision</td>
<td>Use with adaptive optimizers for speed and memory savings</td>
</tr>
</tbody>
</table>
<h2 id="convex-35_modern-187-comparative-behavior">18.7 Comparative Behavior<a class="headerlink" href="#convex-35_modern-187-comparative-behavior" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Adaptivity</th>
<th>Speed</th>
<th>Memory</th>
<th>Typical Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD + Momentum</td>
<td>Moderate</td>
<td>Slow-medium</td>
<td>Low</td>
<td>General-purpose, good generalization</td>
</tr>
<tr>
<td>RMSProp</td>
<td>Adaptive per-parameter</td>
<td>Medium-fast</td>
<td>Medium</td>
<td>Recurrent networks, nonstationary data</td>
</tr>
<tr>
<td>Adam / AdamW</td>
<td>Fully adaptive</td>
<td>Fast</td>
<td>High</td>
<td>Deep networks, large-scale training</td>
</tr>
<tr>
<td>RAdam / AdaBelief / Lion</td>
<td>Advanced adaptivity</td>
<td>Fast</td>
<td>Medium</td>
<td>Cutting-edge training tasks</td>
</tr>
</tbody>
</table>
<h2 id="convex-35_modern-188-optimization-in-modern-deep-networks">18.8 Optimization in Modern Deep Networks<a class="headerlink" href="#convex-35_modern-188-optimization-in-modern-deep-networks" title="Permanent link">¶</a></h2>
<p>In deep learning, optimization interacts with architecture, loss, and regularization:</p>
<ul>
<li>Batch normalization modifies effective learning rates.  </li>
<li>Skip connections ease gradient flow.  </li>
<li>Large-scale distributed training relies on adaptive optimizers for stability.  </li>
</ul>
<p>Optimization is no longer an isolated procedure but part of the model’s design philosophy.</p>
<hr>
<p>Modern stochastic optimizers extend classical first-order methods into high-dimensional, noisy, nonconvex regimes. They are the engines behind deep learning—adapting dynamically, balancing efficiency and generalization.</p></body></html></section><section class="print-page" id="convex-40_nonconvex" heading-number="2.19"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-19-beyond-convexity-nonconvex-and-global-optimization">Chapter 19: Beyond Convexity – Nonconvex and Global Optimization<a class="headerlink" href="#convex-40_nonconvex-chapter-19-beyond-convexity-nonconvex-and-global-optimization" title="Permanent link">¶</a></h1>
<p>Optimization extends far beyond the comfortable world of convexity. 
In practice, most problems in machine learning, signal processing, control, and engineering design are nonconvex: their objective functions have multiple valleys, peaks, and saddle points.  </p>
<p>Convex optimization gives us strong guarantees — every local minimum is global, and algorithms converge predictably.<br>
But the moment convexity is lost, these guarantees vanish, and new techniques become necessary.</p>
<h2 id="convex-40_nonconvex-191-the-landscape-of-nonconvex-optimization">19.1 The Landscape of Nonconvex Optimization<a class="headerlink" href="#convex-40_nonconvex-191-the-landscape-of-nonconvex-optimization" title="Permanent link">¶</a></h2>
<p>A nonconvex function <span class="arithmatex">\(f:\mathbb{R}^n \to \mathbb{R}\)</span> violates convexity; i.e., for some <span class="arithmatex">\(x, y\)</span> and <span class="arithmatex">\(\theta \in (0,1)\)</span>,
<script type="math/tex; mode=display">
f(\theta x + (1-\theta)y) > \theta f(x) + (1-\theta)f(y).
</script>
Its level sets can fold, twist, and fragment, creating local minima, local maxima, and saddle points scattered throughout the space.</p>
<p>A typical nonconvex landscape looks like a mountainous terrain — smooth in some regions, rugged in others. An optimization algorithm’s path depends strongly on initialization and stochastic effects.</p>
<h3 id="convex-40_nonconvex-example-a-simple-nonconvex-function">Example: A Simple Nonconvex Function<a class="headerlink" href="#convex-40_nonconvex-example-a-simple-nonconvex-function" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
f(x,y) = x^4 + y^4 - 4xy + x^2.
</script>
This function has multiple stationary points:
- <span class="arithmatex">\((0,0)\)</span> (a saddle),
- <span class="arithmatex">\((1,1)\)</span> and <span class="arithmatex">\((-1,-1)\)</span> (local minima),
- <span class="arithmatex">\((1,-1)\)</span> and <span class="arithmatex">\((-1,1)\)</span> (local maxima).</p>
<p>Unlike convex problems, gradient descent may end in different minima depending on where it starts.</p>
<h2 id="convex-40_nonconvex-192-local-vs-global-minima">19.2 Local vs. Global Minima<a class="headerlink" href="#convex-40_nonconvex-192-local-vs-global-minima" title="Permanent link">¶</a></h2>
<p>A point <span class="arithmatex">\(x^*\)</span> is a local minimum if:
<script type="math/tex; mode=display">
f(x^*) \le f(x) \quad \text{for all } x \text{ near } x^*.
</script>
</p>
<p>A global minimum satisfies the stronger condition:
<script type="math/tex; mode=display">
f(x^*) \le f(x) \quad \text{for all } x \in \mathbb{R}^n.
</script>
</p>
<p>In convex problems, every local minimum is automatically global. In nonconvex problems, local minima can be arbitrarily bad — and there may be exponentially many of them.</p>
<h2 id="convex-40_nonconvex-193-classes-of-nonconvex-problems">19.3 Classes of Nonconvex Problems<a class="headerlink" href="#convex-40_nonconvex-193-classes-of-nonconvex-problems" title="Permanent link">¶</a></h2>
<p>Nonconvex problems appear in several distinct forms:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example</th>
<th>Challenge</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smooth nonconvex</td>
<td>Neural network training</td>
<td>Multiple minima, saddle points</td>
</tr>
<tr>
<td>Nonsmooth nonconvex</td>
<td>Sparse regularization, ReLU activations</td>
<td>Undefined gradients</td>
</tr>
<tr>
<td>Discrete / combinatorial</td>
<td>Scheduling, routing, integer programs</td>
<td>Exponential search space</td>
</tr>
<tr>
<td>Black-box</td>
<td>Simulation-based optimization</td>
<td>No derivatives or analytical form</td>
</tr>
</tbody>
</table>
<p>Each category requires different algorithmic strategies — from stochastic gradient methods to evolutionary heuristics or surrogate modeling.</p>
<h2 id="convex-40_nonconvex-194-local-optimization-strategies">19.4 Local Optimization Strategies<a class="headerlink" href="#convex-40_nonconvex-194-local-optimization-strategies" title="Permanent link">¶</a></h2>
<p>Even in nonconvex settings, local optimization remains useful when:
- The problem is nearly convex (e.g., locally convex around good minima),
- The initialization is close to a desired basin of attraction,
- Or the goal is approximate, not exact, optimality.</p>
<h3 id="convex-40_nonconvex-gradient-descent-and-its-variants">Gradient Descent and Its Variants<a class="headerlink" href="#convex-40_nonconvex-gradient-descent-and-its-variants" title="Permanent link">¶</a></h3>
<p>Gradient descent behaves well if <span class="arithmatex">\(f\)</span> is smooth and Lipschitz-continuous:
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k).
</script>
However, convergence is only to a <em>stationary point</em> — not necessarily a minimum.</p>
<p>Escaping saddles: Adding small random noise (stochasticity) helps escape flat saddle regions common in high-dimensional problems.</p>
<h2 id="convex-40_nonconvex-195-global-optimization-strategies">19.5 Global Optimization Strategies<a class="headerlink" href="#convex-40_nonconvex-195-global-optimization-strategies" title="Permanent link">¶</a></h2>
<p>To seek the <em>global</em> minimum, algorithms must explore the search space more broadly.<br>
Common strategies include:</p>
<ol>
<li>
<p>Multiple Starts:<br>
   Run local optimization from diverse random initial points and keep the best solution.</p>
</li>
<li>
<p>Continuation and Homotopy Methods:<br>
   Start from a smooth, convex approximation <span class="arithmatex">\(f_\lambda\)</span> of <span class="arithmatex">\(f\)</span> and gradually transform it into the true objective as <span class="arithmatex">\(\lambda \to 0\)</span>.</p>
</li>
<li>
<p>Stochastic Search and Simulated Annealing:<br>
   Introduce randomness in updates to jump between basins.</p>
</li>
<li>
<p>Population-Based Methods:<br>
   Maintain a swarm or population of candidate solutions evolving by selection and variation — leading to metaheuristic algorithms like GA and PSO.</p>
</li>
</ol>
<h2 id="convex-40_nonconvex-196-theoretical-challenges">19.6 Theoretical Challenges<a class="headerlink" href="#convex-40_nonconvex-196-theoretical-challenges" title="Permanent link">¶</a></h2>
<p>Without convexity, most strong results vanish:</p>
<ul>
<li>Global optimality cannot be guaranteed.</li>
<li>Duality gaps appear; the Lagrange dual may no longer represent the primal value.</li>
<li>Complexity often grows exponentially with problem size.</li>
</ul>
<p>However, theory is not hopeless:</p>
<ul>
<li>Many nonconvex problems are “benign” — e.g., matrix factorization, phase retrieval, or deep linear networks — having no bad local minima.  </li>
<li>Random initialization and overparameterization often aid convergence to global minima in practice.</li>
</ul>
<h2 id="convex-40_nonconvex-197-geometry-of-saddle-points">19.7 Geometry of Saddle Points<a class="headerlink" href="#convex-40_nonconvex-197-geometry-of-saddle-points" title="Permanent link">¶</a></h2>
<p>A saddle point satisfies <span class="arithmatex">\(\nabla f(x)=0\)</span> but is not a local minimum because the Hessian has both positive and negative eigenvalues.</p>
<p>In high dimensions, saddle points are far more common than local minima. Modern optimization methods (SGD, momentum) tend to escape saddles due to their stochastic nature.</p>
<h2 id="convex-40_nonconvex-198-deterministic-vs-stochastic-global-methods">19.8 Deterministic vs. Stochastic Global Methods<a class="headerlink" href="#convex-40_nonconvex-198-deterministic-vs-stochastic-global-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Deterministic Methods</th>
<th>Stochastic Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Systematic exploration of space (branch &amp; bound, interval analysis)</td>
<td>Randomized search (simulated annealing, evolutionary algorithms)</td>
</tr>
<tr>
<td>Can provide certificates of global optimality</td>
<td>Typically approximate but scalable</td>
</tr>
<tr>
<td>High computational cost</td>
<td>Naturally parallelizable</td>
</tr>
</tbody>
</table>
<p>In real-world large-scale problems, stochastic global optimization is often the only feasible approach.</p>
<h2 id="convex-40_nonconvex-199-a-taxonomy-of-optimization-beyond-convexity">19.9 A Taxonomy of Optimization Beyond Convexity<a class="headerlink" href="#convex-40_nonconvex-199-a-taxonomy-of-optimization-beyond-convexity" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Family</th>
<th>Typical Algorithms</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Derivative-Free (Black-Box)</td>
<td>Nelder–Mead, CMA-ES, Bayesian Opt.</td>
<td>When gradients unavailable</td>
</tr>
<tr>
<td>Metaheuristic (Evolutionary)</td>
<td>GA, PSO, DE, ACO</td>
<td>Complex landscapes, combinatorial problems</td>
</tr>
<tr>
<td>Modern Stochastic Gradient</td>
<td>Adam, RMSProp, Lion</td>
<td>Deep learning, large-scale models</td>
</tr>
<tr>
<td>Combinatorial / Discrete</td>
<td>Branch &amp; Bound, Tabu, SA</td>
<td>Integer or graph-based problems</td>
</tr>
<tr>
<td>Learning-Based Optimizers</td>
<td>Meta-learning, Reinforcement methods</td>
<td>Adaptive, data-driven optimization</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="convex-42_derivativefree" heading-number="2.20"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-20-derivative-free-and-black-box-optimization">Chapter 20: Derivative-Free and Black-Box Optimization<a class="headerlink" href="#convex-42_derivativefree-chapter-20-derivative-free-and-black-box-optimization" title="Permanent link">¶</a></h1>
<p>In many practical optimization problems, gradients are unavailable, unreliable, or prohibitively expensive to compute. Examples include tuning hyperparameters of machine learning models, engineering design through simulation, or optimizing physical experiments. Such problems fall under the class of derivative-free or black-box optimization methods.</p>
<p>Unlike gradient-based methods, which rely on analytical or automatic differentiation, derivative-free algorithms make progress solely from function evaluations. They are indispensable when the objective function is noisy, discontinuous, or non-differentiable.</p>
<h2 id="convex-42_derivativefree-201-motivation-and-challenges">20.1 Motivation and Challenges<a class="headerlink" href="#convex-42_derivativefree-201-motivation-and-challenges" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> be an objective function.  </p>
<p>A derivative-free algorithm seeks to minimize <span class="arithmatex">\(f(x)\)</span> using only evaluations of <span class="arithmatex">\(f(x)\)</span>, without access to <span class="arithmatex">\(\nabla f(x)\)</span> or <span class="arithmatex">\(\nabla^2 f(x)\)</span>.</p>
<p>Key challenges:</p>
<ul>
<li>No gradient information → difficult to infer descent directions.  </li>
<li>Expensive evaluations → every call to <span class="arithmatex">\(f(x)\)</span> might require a simulation or experiment.  </li>
<li>Noise and stochasticity → evaluations may be corrupted by measurement or sampling error.  </li>
<li>High-dimensionality → sampling-based methods scale poorly with <span class="arithmatex">\(n\)</span>.</li>
</ul>
<p>Derivative-free optimization is thus a trade-off between exploration and exploitation, guided by heuristics or surrogate models.</p>
<h2 id="convex-42_derivativefree-202-classification-of-derivative-free-methods">20.2 Classification of Derivative-Free Methods<a class="headerlink" href="#convex-42_derivativefree-202-classification-of-derivative-free-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Representative Algorithms</th>
<th>Main Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Direct Search</td>
<td>Nelder–Mead, Pattern Search, MADS</td>
<td>Explore the space via geometric moves or meshes</td>
</tr>
<tr>
<td>Model-Based</td>
<td>BOBYQA, Trust-Region DFO</td>
<td>Build local quadratic or surrogate models of <span class="arithmatex">\(f\)</span></td>
</tr>
<tr>
<td>Evolutionary / Population-Based</td>
<td>CMA-ES, Differential Evolution</td>
<td>Evolve a population using stochastic operators</td>
</tr>
<tr>
<td>Probabilistic / Bayesian</td>
<td>Bayesian Optimization</td>
<td>Use probabilistic surrogate models to guide exploration</td>
</tr>
</tbody>
</table>
<h2 id="convex-42_derivativefree-203-direct-search-methods">20.3 Direct Search Methods<a class="headerlink" href="#convex-42_derivativefree-203-direct-search-methods" title="Permanent link">¶</a></h2>
<p>Direct search algorithms evaluate the objective function at structured sets of points and use comparisons, not gradients, to decide where to move.</p>
<h3 id="convex-42_derivativefree-2031-neldermead-simplex-method">20.3.1 Nelder–Mead Simplex Method<a class="headerlink" href="#convex-42_derivativefree-2031-neldermead-simplex-method" title="Permanent link">¶</a></h3>
<p>Perhaps the most famous derivative-free algorithm, Nelder–Mead maintains a simplex — a polytope of <span class="arithmatex">\(n+1\)</span> vertices in <span class="arithmatex">\(\mathbb{R}^n\)</span>.</p>
<p>At each iteration:</p>
<ol>
<li>Evaluate <span class="arithmatex">\(f\)</span> at all simplex vertices.</li>
<li>Reflect, expand, contract, or shrink the simplex depending on performance.</li>
<li>Continue until simplex collapses near a minimum.</li>
</ol>
<p>Simple, intuitive, and effective for small-scale smooth problems, though it lacks formal convergence guarantees in general.</p>
<h3 id="convex-42_derivativefree-2032-pattern-search-methods">20.3.2 Pattern Search Methods<a class="headerlink" href="#convex-42_derivativefree-2032-pattern-search-methods" title="Permanent link">¶</a></h3>
<p>These methods (also called coordinate search or compass search) probe the function along coordinate directions or pre-defined patterns.</p>
<p>Typical update rule:
<script type="math/tex; mode=display">
x_{k+1} = x_k + \Delta_k d_i,
</script>
</p>
<p>where <span class="arithmatex">\(d_i\)</span> is a direction from a finite set (e.g., coordinate axes).<br>
If a direction yields improvement, move there; otherwise, shrink <span class="arithmatex">\(\Delta_k\)</span>.</p>
<h3 id="convex-42_derivativefree-2033-mesh-adaptive-direct-search-mads">20.3.3 Mesh Adaptive Direct Search (MADS)<a class="headerlink" href="#convex-42_derivativefree-2033-mesh-adaptive-direct-search-mads" title="Permanent link">¶</a></h3>
<p>MADS refines pattern search by maintaining a mesh of candidate points and adaptively changing its resolution. It offers provable convergence to stationary points for certain classes of nonsmooth problems.</p>
<h2 id="convex-42_derivativefree-204-model-based-methods">20.4 Model-Based Methods<a class="headerlink" href="#convex-42_derivativefree-204-model-based-methods" title="Permanent link">¶</a></h2>
<p>Instead of exploring blindly, model-based methods construct an approximation of the objective function from past evaluations.</p>
<h3 id="convex-42_derivativefree-2041-trust-region-dfo">20.4.1 Trust-Region DFO<a class="headerlink" href="#convex-42_derivativefree-2041-trust-region-dfo" title="Permanent link">¶</a></h3>
<p>A local model <span class="arithmatex">\(m_k(x)\)</span> (often quadratic) is built to approximate <span class="arithmatex">\(f\)</span> near the current iterate <span class="arithmatex">\(x_k\)</span>:
<script type="math/tex; mode=display">
m_k(x) \approx f(x_k) + g_k^\top (x - x_k) + \tfrac{1}{2}(x - x_k)^\top H_k (x - x_k).
</script>
The next iterate solves a trust-region subproblem:
<script type="math/tex; mode=display">
\min_{\|x - x_k\| \le \Delta_k} m_k(x).
</script>
The trust region size <span class="arithmatex">\(\Delta_k\)</span> adapts based on how well <span class="arithmatex">\(m_k\)</span> predicts true function values.</p>
<h3 id="convex-42_derivativefree-2042-bobyqa-bound-optimization-by-quadratic-approximation">20.4.2 BOBYQA (Bound Optimization BY Quadratic Approximation)<a class="headerlink" href="#convex-42_derivativefree-2042-bobyqa-bound-optimization-by-quadratic-approximation" title="Permanent link">¶</a></h3>
<p>BOBYQA builds and maintains a quadratic model using interpolation of previously evaluated points. It is highly efficient for medium-scale problems with simple box constraints and no noise.</p>
<h2 id="convex-42_derivativefree-205-evolution-strategies-and-population-methods">20.5 Evolution Strategies and Population Methods<a class="headerlink" href="#convex-42_derivativefree-205-evolution-strategies-and-population-methods" title="Permanent link">¶</a></h2>
<p>These methods maintain a population of candidate solutions and update them using statistical principles.</p>
<h3 id="convex-42_derivativefree-2051-covariance-matrix-adaptation-evolution-strategy-cma-es">20.5.1 Covariance Matrix Adaptation Evolution Strategy (CMA-ES)<a class="headerlink" href="#convex-42_derivativefree-2051-covariance-matrix-adaptation-evolution-strategy-cma-es" title="Permanent link">¶</a></h3>
<p>CMA-ES is a powerful stochastic search algorithm.<br>
It iteratively samples new points from a multivariate Gaussian distribution:
<script type="math/tex; mode=display">
x^{(i)}_{k+1} \sim \mathcal{N}(m_k, \sigma_k^2 C_k),
</script>
where <span class="arithmatex">\(m_k\)</span> is the current mean, <span class="arithmatex">\(\sigma_k\)</span> the global step-size, and <span class="arithmatex">\(C_k\)</span> the covariance matrix.</p>
<p>After evaluating all samples, the mean is updated toward better-performing points, and the covariance matrix adapts to the landscape geometry.</p>
<p>CMA-ES is invariant to linear transformations and excels in ill-conditioned, noisy, or nonconvex problems.</p>
<hr>
<h3 id="convex-42_derivativefree-2052-differential-evolution-de">20.5.2 Differential Evolution (DE)<a class="headerlink" href="#convex-42_derivativefree-2052-differential-evolution-de" title="Permanent link">¶</a></h3>
<p>DE evolves a population <span class="arithmatex">\(\{x_i\}\)</span> via vector differences:
<script type="math/tex; mode=display">
v_i = x_{r1} + F(x_{r2} - x_{r3}),
</script>
<script type="math/tex; mode=display">
u_i = \text{crossover}(x_i, v_i),
</script>
<script type="math/tex; mode=display">
x_i' = \begin{cases} 
u_i, & \text{if } f(u_i) < f(x_i),\\
x_i, & \text{otherwise.}
\end{cases}
</script>
where <span class="arithmatex">\(r1, r2, r3\)</span> are random distinct indices and <span class="arithmatex">\(F\)</span> controls mutation strength.</p>
<p>DE combines simplicity and robustness, performing well across continuous and discrete spaces.</p>
<h2 id="convex-42_derivativefree-206-bayesian-optimization">20.6 Bayesian Optimization<a class="headerlink" href="#convex-42_derivativefree-206-bayesian-optimization" title="Permanent link">¶</a></h2>
<p>When function evaluations are <em>expensive</em> (e.g., training a neural network or running a CFD simulation), Bayesian Optimization (BO) is preferred.</p>
<h3 id="convex-42_derivativefree-2061-core-idea">20.6.1 Core Idea<a class="headerlink" href="#convex-42_derivativefree-2061-core-idea" title="Permanent link">¶</a></h3>
<p>Model the objective as a random function <span class="arithmatex">\(f(x) \sim \mathcal{GP}(m(x), k(x,x'))\)</span> (Gaussian Process prior).<br>
After each evaluation, update the posterior mean and variance to quantify uncertainty.</p>
<p>Use an acquisition function <span class="arithmatex">\(a(x)\)</span> to select the next evaluation point:
<script type="math/tex; mode=display">
x_{k+1} = \arg\max_x a(x),
</script>
balancing <em>exploration</em> (high uncertainty) and <em>exploitation</em> (low expected value).</p>
<p>Common acquisition functions:</p>
<ul>
<li>Expected Improvement (EI)</li>
<li>Probability of Improvement (PI)</li>
<li>Upper Confidence Bound (UCB)</li>
</ul>
<h3 id="convex-42_derivativefree-2062-surrogate-models-beyond-gaussian-processes">20.6.2 Surrogate Models Beyond Gaussian Processes<a class="headerlink" href="#convex-42_derivativefree-2062-surrogate-models-beyond-gaussian-processes" title="Permanent link">¶</a></h3>
<p>When dimensionality is high or data is noisy, other surrogate models may replace GPs:
- Tree-structured Parzen Estimators (TPE)
- Random forests (SMAC)
- Neural network surrogates (Bayesian neural networks)</p>
<p>These variants enable Bayesian optimization in complex or discrete search spaces.</p>
<h2 id="convex-42_derivativefree-207-hybrid-and-adaptive-approaches">20.7 Hybrid and Adaptive Approaches<a class="headerlink" href="#convex-42_derivativefree-207-hybrid-and-adaptive-approaches" title="Permanent link">¶</a></h2>
<p>Modern applications often combine derivative-free and gradient-based techniques:</p>
<ul>
<li>Use Bayesian optimization for coarse global search, then local refinement with gradient descent.</li>
<li>Alternate between CMA-ES and SGD to exploit both exploration and fast convergence.</li>
<li>Apply direct search methods to tune hyperparameters of differentiable optimizers.</li>
</ul>
<p>Such hybridization reflects a pragmatic view: no single optimizer is best — adaptability matters most.</p>
<h2 id="convex-42_derivativefree-208-practical-considerations">20.8 Practical Considerations<a class="headerlink" href="#convex-42_derivativefree-208-practical-considerations" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Function evaluations expensive</td>
<td>Use Bayesian or model-based methods</td>
</tr>
<tr>
<td>Noisy evaluations</td>
<td>Use averaging, smoothing, or robust estimators</td>
</tr>
<tr>
<td>High dimension (<span class="arithmatex">\(n &gt; 50\)</span>)</td>
<td>Prefer CMA-ES or evolutionary strategies</td>
</tr>
<tr>
<td>Box constraints</td>
<td>Methods like BOBYQA, DE, or PSO</td>
</tr>
<tr>
<td>Parallel computation available</td>
<td>Population-based methods excel</td>
</tr>
</tbody>
</table>
<hr>
<p>Derivative-free optimization expands our toolkit beyond calculus, allowing us to optimize <em>anything we can evaluate</em>. It emphasizes adaptation, surrogate modeling, and population intelligence rather than analytical structure.</p>
<p>In the next chapter, we explore metaheuristic and evolutionary algorithms, which generalize these ideas further by mimicking natural and collective behaviors — turning randomness into a powerful search strategy.</p></body></html></section><section class="print-page" id="convex-44_metaheuristic" heading-number="2.21"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-21-metaheuristic-and-evolutionary-algorithms">Chapter 21: Metaheuristic and Evolutionary Algorithms<a class="headerlink" href="#convex-44_metaheuristic-chapter-21-metaheuristic-and-evolutionary-algorithms" title="Permanent link">¶</a></h1>
<p>When optimization problems are highly nonconvex, discrete, or black-box, deterministic methods often fail to find good solutions.<br>
In these settings, metaheuristic algorithms—inspired by nature, biology, and collective behavior—provide robust and flexible alternatives.</p>
<p>Metaheuristics are general-purpose stochastic search methods that rely on repeated sampling, adaptation, and survival of the fittest ideas.<br>
They are especially effective when the landscape is rugged, multimodal, or not well understood.</p>
<h2 id="convex-44_metaheuristic-211-principles-of-metaheuristic-optimization">21.1 Principles of Metaheuristic Optimization<a class="headerlink" href="#convex-44_metaheuristic-211-principles-of-metaheuristic-optimization" title="Permanent link">¶</a></h2>
<p>All metaheuristics share three key principles:</p>
<ol>
<li>
<p>Population-Based Search:<br>
   Maintain multiple candidate solutions simultaneously to explore diverse regions of the search space.</p>
</li>
<li>
<p>Variation Operators:<br>
   Create new solutions via mutation, recombination, or stochastic perturbations.</p>
</li>
<li>
<p>Selection and Adaptation:<br>
   Favor candidates with better objective values, guiding the search toward promising regions.</p>
</li>
</ol>
<p>Unlike local methods, metaheuristics balance exploration (global search) and exploitation (local refinement).</p>
<h2 id="convex-44_metaheuristic-212-genetic-algorithms-ga">21.2 Genetic Algorithms (GA)<a class="headerlink" href="#convex-44_metaheuristic-212-genetic-algorithms-ga" title="Permanent link">¶</a></h2>
<h3 id="convex-44_metaheuristic-2121-biological-inspiration">21.2.1 Biological Inspiration<a class="headerlink" href="#convex-44_metaheuristic-2121-biological-inspiration" title="Permanent link">¶</a></h3>
<p>Genetic Algorithms mimic natural evolution, where populations evolve toward higher fitness through selection, crossover, and mutation.</p>
<h3 id="convex-44_metaheuristic-2122-representation">21.2.2 Representation<a class="headerlink" href="#convex-44_metaheuristic-2122-representation" title="Permanent link">¶</a></h3>
<p>A solution (individual) is represented as a chromosome—often a binary string, vector of reals, or permutation.<br>
Each position (gene) encodes part of the decision variable.</p>
<h3 id="convex-44_metaheuristic-2123-algorithm-outline">21.2.3 Algorithm Outline<a class="headerlink" href="#convex-44_metaheuristic-2123-algorithm-outline" title="Permanent link">¶</a></h3>
<ol>
<li>Initialize a population <span class="arithmatex">\(\{x_i\}_{i=1}^N\)</span> randomly.  </li>
<li>Evaluate fitness <span class="arithmatex">\(f(x_i)\)</span> for all individuals.  </li>
<li>Select parents based on fitness (e.g., tournament or roulette-wheel selection).  </li>
<li>
<p>Apply:</p>
<ul>
<li>Crossover: combine genetic material of two parents.  </li>
<li>Mutation: randomly alter some genes to maintain diversity.  </li>
</ul>
</li>
<li>
<p>Form a new population and repeat until convergence.</p>
</li>
</ol>
<h3 id="convex-44_metaheuristic-2124-crossover-and-mutation-examples">21.2.4 Crossover and Mutation Examples<a class="headerlink" href="#convex-44_metaheuristic-2124-crossover-and-mutation-examples" title="Permanent link">¶</a></h3>
<ul>
<li>Single-point crossover: exchange genes after a random index.  </li>
<li>Gaussian mutation: add small noise to continuous parameters.  </li>
</ul>
<h3 id="convex-44_metaheuristic-2125-strengths-and-weaknesses">21.2.5 Strengths and Weaknesses<a class="headerlink" href="#convex-44_metaheuristic-2125-strengths-and-weaknesses" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Highly parallel, robust, domain-independent</td>
<td>Requires many function evaluations</td>
</tr>
<tr>
<td>Effective for combinatorial and discrete optimization</td>
<td>Parameter tuning (mutation, crossover rates) is nontrivial</td>
</tr>
</tbody>
</table>
<h2 id="convex-44_metaheuristic-213-differential-evolution-de">21.3 Differential Evolution (DE)<a class="headerlink" href="#convex-44_metaheuristic-213-differential-evolution-de" title="Permanent link">¶</a></h2>
<p>Differential Evolution is a simple yet powerful algorithm for continuous optimization.</p>
<h3 id="convex-44_metaheuristic-2131-core-idea">21.3.1 Core Idea<a class="headerlink" href="#convex-44_metaheuristic-2131-core-idea" title="Permanent link">¶</a></h3>
<p>Mutation is performed using differences of population members:
<script type="math/tex; mode=display">
v_i = x_{r1} + F(x_{r2} - x_{r3}),
</script>
where <span class="arithmatex">\(r1, r2, r3\)</span> are random distinct indices and <span class="arithmatex">\(F \in [0,2]\)</span> controls mutation amplitude.</p>
<p>Then crossover forms trial vectors:
<script type="math/tex; mode=display">
u_i = \text{crossover}(x_i, v_i),
</script>
and selection chooses between <span class="arithmatex">\(x_i\)</span> and <span class="arithmatex">\(u_i\)</span> based on objective value.</p>
<h3 id="convex-44_metaheuristic-2132-features">21.3.2 Features<a class="headerlink" href="#convex-44_metaheuristic-2132-features" title="Permanent link">¶</a></h3>
<ul>
<li>Self-adaptive exploration of the search space.</li>
<li>Suitable for continuous, multimodal functions.</li>
<li>Simple to implement, with few control parameters.</li>
</ul>
<h2 id="convex-44_metaheuristic-214-particle-swarm-optimization-pso">21.4 Particle Swarm Optimization (PSO)<a class="headerlink" href="#convex-44_metaheuristic-214-particle-swarm-optimization-pso" title="Permanent link">¶</a></h2>
<p>Inspired by social behavior of birds and fish, Particle Swarm Optimization maintains a swarm of particles moving through the search space.</p>
<p>Each particle <span class="arithmatex">\(i\)</span> has position <span class="arithmatex">\(x_i\)</span> and velocity <span class="arithmatex">\(v_i\)</span>, updated as:
<script type="math/tex; mode=display">
v_i \leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i),
</script>
<script type="math/tex; mode=display">
x_i \leftarrow x_i + v_i,
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(p_i\)</span> = personal best position of particle <span class="arithmatex">\(i\)</span>,</li>
<li><span class="arithmatex">\(g\)</span> = best global position found by the swarm,</li>
<li><span class="arithmatex">\(w\)</span>, <span class="arithmatex">\(c_1\)</span>, <span class="arithmatex">\(c_2\)</span> are weight and learning coefficients,</li>
<li><span class="arithmatex">\(r_1\)</span>, <span class="arithmatex">\(r_2\)</span> are random numbers in <span class="arithmatex">\([0,1]\)</span>.</li>
</ul>
<p>Particles balance individual learning (self-experience) and social learning (group knowledge).</p>
<h3 id="convex-44_metaheuristic-2141-convergence-behavior">21.4.1 Convergence Behavior<a class="headerlink" href="#convex-44_metaheuristic-2141-convergence-behavior" title="Permanent link">¶</a></h3>
<p>Initially, the swarm explores widely; as iterations proceed, velocities decrease, and the swarm converges near optima.</p>
<h3 id="convex-44_metaheuristic-2142-strengths">21.4.2 Strengths<a class="headerlink" href="#convex-44_metaheuristic-2142-strengths" title="Permanent link">¶</a></h3>
<ul>
<li>Few parameters, easy to implement.</li>
<li>Works well for noisy or discontinuous problems.</li>
<li>Naturally parallelizable.</li>
</ul>
<h2 id="convex-44_metaheuristic-215-simulated-annealing-sa">21.5 Simulated Annealing (SA)<a class="headerlink" href="#convex-44_metaheuristic-215-simulated-annealing-sa" title="Permanent link">¶</a></h2>
<p>Simulated Annealing is one of the earliest and most fundamental stochastic optimization algorithms. It is inspired by annealing in metallurgy — a physical process in which a material is heated and then slowly cooled to minimize structural defects and reach a low-energy crystalline state. The key idea is to imitate this gradual “cooling” in the search for a global minimum.</p>
<h3 id="convex-44_metaheuristic-2151-physical-analogy">21.5.1 Physical Analogy<a class="headerlink" href="#convex-44_metaheuristic-2151-physical-analogy" title="Permanent link">¶</a></h3>
<p>In thermodynamics, a system at temperature <span class="arithmatex">\(T\)</span> has probability of occupying a state with energy <span class="arithmatex">\(E\)</span> given by the Boltzmann distribution:</p>
<div class="arithmatex">\[
P(E) \propto e^{-E / (kT)}.
\]</div>
<p>At high temperature, the system freely explores many states. As <span class="arithmatex">\(T\)</span> decreases, it becomes increasingly likely to remain near states of minimal energy.</p>
<p>Simulated Annealing maps this principle to optimization by treating:</p>
<ul>
<li>The objective function <span class="arithmatex">\(f(x)\)</span> as the system’s energy.</li>
<li>The solution vector <span class="arithmatex">\(x\)</span> as a configuration.</li>
<li>The temperature <span class="arithmatex">\(T\)</span> as a control parameter determining randomness.</li>
</ul>
<h3 id="convex-44_metaheuristic-2152-algorithm-outline">21.5.2 Algorithm Outline<a class="headerlink" href="#convex-44_metaheuristic-2152-algorithm-outline" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Initialization</p>
<ul>
<li>Choose an initial solution <span class="arithmatex">\(x_0\)</span> and initial temperature <span class="arithmatex">\(T_0\)</span>.</li>
<li>Set a cooling schedule <span class="arithmatex">\(T_{k+1} = \alpha T_k\)</span>, with <span class="arithmatex">\(\alpha \in (0,1)\)</span>.</li>
</ul>
</li>
<li>
<p>Iteration</p>
<ul>
<li>Generate a candidate <span class="arithmatex">\(x'\)</span> from <span class="arithmatex">\(x_k\)</span> via a small random perturbation.</li>
<li>Compute <span class="arithmatex">\(\Delta f = f(x') - f(x_k)\)</span>.</li>
<li>Accept or reject based on the Metropolis criterion:</li>
</ul>
<p>
<script type="math/tex; mode=display">
 P_{\text{accept}} = 
 \begin{cases}
 1, & \text{if } \Delta f \le 0, \\
 e^{-\Delta f / T_k}, & \text{if } \Delta f > 0.
 \end{cases}
 </script>
</p>
</li>
<li>
<p>Cooling</p>
<ul>
<li>
<p>Reduce the temperature gradually according to the schedule.</p>
</li>
<li>
<p>Repeat until <span class="arithmatex">\(T\)</span> becomes sufficiently small or the system stabilizes.</p>
</li>
</ul>
</li>
</ol>
<h3 id="convex-44_metaheuristic-2153-interpretation">21.5.3 Interpretation<a class="headerlink" href="#convex-44_metaheuristic-2153-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>At high temperatures, SA accepts both better and worse moves → exploration.  </p>
</li>
<li>
<p>At low temperatures, it becomes increasingly selective → exploitation.</p>
</li>
</ul>
<p>This balance allows SA to escape local minima and approach the global optimum over time.</p>
<h3 id="convex-44_metaheuristic-2154-cooling-schedules">21.5.4 Cooling Schedules<a class="headerlink" href="#convex-44_metaheuristic-2154-cooling-schedules" title="Permanent link">¶</a></h3>
<p>The temperature schedule determines convergence quality:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Formula</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>Exponential</td>
<td><span class="arithmatex">\(T_{k+1} = \alpha T_k\)</span></td>
<td>Simple, widely used</td>
</tr>
<tr>
<td>Linear</td>
<td><span class="arithmatex">\(T_{k+1} = T_0 - \beta k\)</span></td>
<td>Faster cooling, less exploration</td>
</tr>
<tr>
<td>Logarithmic</td>
<td><span class="arithmatex">\(T_k = \frac{T_0}{\log(k + c)}\)</span></td>
<td>Theoretically convergent (slow)</td>
</tr>
<tr>
<td>Adaptive</td>
<td>Adjust based on recent acceptance rates</td>
<td>Practical and self-tuning</td>
</tr>
</tbody>
</table>
<p>A slower cooling schedule improves accuracy but increases computational cost.</p>
<h2 id="convex-44_metaheuristic-216-ant-colony-optimization-aco">21.6 Ant Colony Optimization (ACO)<a class="headerlink" href="#convex-44_metaheuristic-216-ant-colony-optimization-aco" title="Permanent link">¶</a></h2>
<h3 id="convex-44_metaheuristic-2161-biological-basis">21.6.1 Biological Basis<a class="headerlink" href="#convex-44_metaheuristic-2161-biological-basis" title="Permanent link">¶</a></h3>
<p>Ant Colony Optimization models how real ants find shortest paths using pheromone trails.</p>
<p>Each artificial ant builds a solution step by step, choosing components probabilistically based on pheromone intensity <span class="arithmatex">\(\tau_{ij}\)</span> and heuristic visibility <span class="arithmatex">\(\eta_{ij}\)</span>:
<script type="math/tex; mode=display">
P_{ij} = \frac{[\tau_{ij}]^\alpha [\eta_{ij}]^\beta}{\sum_k [\tau_{ik}]^\alpha [\eta_{ik}]^\beta}.
</script>
</p>
<h3 id="convex-44_metaheuristic-2162-pheromone-update">21.6.2 Pheromone Update<a class="headerlink" href="#convex-44_metaheuristic-2162-pheromone-update" title="Permanent link">¶</a></h3>
<p>After all ants construct their tours:
<script type="math/tex; mode=display">
\tau_{ij} \leftarrow (1 - \rho)\tau_{ij} + \sum_{\text{ants}} \Delta \tau_{ij},
</script>
where <span class="arithmatex">\(\rho\)</span> controls evaporation and <span class="arithmatex">\(\Delta\tau_{ij}\)</span> reinforces paths used by good solutions.</p>
<p>ACO excels at combinatorial problems like the Traveling Salesman Problem (TSP) and scheduling.</p>
<h2 id="convex-44_metaheuristic-217-exploration-vs-exploitation">21.7 Exploration vs. Exploitation<a class="headerlink" href="#convex-44_metaheuristic-217-exploration-vs-exploitation" title="Permanent link">¶</a></h2>
<p>Every metaheuristic must balance:
- Exploration: sampling diverse regions to escape local minima.<br>
- Exploitation: refining known good solutions to reach local optima.</p>
<table>
<thead>
<tr>
<th>High Exploration</th>
<th>High Exploitation</th>
</tr>
</thead>
<tbody>
<tr>
<td>GA with strong mutation</td>
<td>PSO with low inertia</td>
</tr>
<tr>
<td>DE with high <span class="arithmatex">\(F\)</span></td>
<td>ACO with low evaporation rate</td>
</tr>
<tr>
<td>Random restarts</td>
<td>Local refinement</td>
</tr>
</tbody>
</table>
<p>Adaptive control of parameters (e.g., mutation rate, inertia weight) helps maintain balance dynamically.</p>
<h2 id="convex-44_metaheuristic-218-hybrid-and-memetic-algorithms">21.8 Hybrid and Memetic Algorithms<a class="headerlink" href="#convex-44_metaheuristic-218-hybrid-and-memetic-algorithms" title="Permanent link">¶</a></h2>
<p>Hybrid (or memetic) algorithms combine global metaheuristic exploration with local optimization refinement.</p>
<p>Example:</p>
<ol>
<li>Use PSO or GA to explore broadly.  </li>
<li>Apply gradient descent or Nelder–Mead locally near promising candidates.</li>
</ol>
<p>This hybridization often yields faster convergence and improved accuracy.</p>
<h2 id="convex-44_metaheuristic-219-performance-and-practical-tips">21.9 Performance and Practical Tips<a class="headerlink" href="#convex-44_metaheuristic-219-performance-and-practical-tips" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Guideline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialization</td>
<td>Use wide, random distributions to promote diversity</td>
</tr>
<tr>
<td>Parameter Tuning</td>
<td>Use adaptive schedules (e.g., cooling, inertia decay)</td>
</tr>
<tr>
<td>Population Size</td>
<td>Larger for global search, smaller for fine-tuning</td>
</tr>
<tr>
<td>Parallelism</td>
<td>Evaluate populations concurrently for efficiency</td>
</tr>
<tr>
<td>Stopping Criteria</td>
<td>Use both iteration limits and stagnation detection</td>
</tr>
</tbody>
</table>
<p>Metaheuristics are heuristic by design — they do not guarantee global optimality, but offer practical success across many fields.</p>
<hr>
<p>Metaheuristic and evolutionary algorithms transform optimization into a process of adaptation and learning. Through populations, randomness, and natural analogies, they enable search in landscapes too complex for calculus or convexity.</p>
<p>In the next chapter, we turn to modern stochastic optimizers that bridge theoretical foundations and practical success in machine learning—methods like Adam, RMSProp, and Lion that dominate large-scale nonconvex optimization.</p></body></html></section><section class="print-page" id="convex-48_advanced_combinatorial" heading-number="2.22"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-22-advanced-topics-in-combinatorial-optimization">Chapter 22: Advanced Topics in Combinatorial Optimization<a class="headerlink" href="#convex-48_advanced_combinatorial-chapter-22-advanced-topics-in-combinatorial-optimization" title="Permanent link">¶</a></h1>
<p>In many of the most challenging optimization problems, variables are discrete, decisions are binary or integral, and the underlying structure is inherently combinatorial.  Convex analysis gives way to graph theory, integer programming, and search algorithms built on discrete mathematics.</p>
<p>Combinatorial optimization lies at the intersection of mathematics, computer science, and operations research, offering powerful tools for scheduling, routing, allocation, and design problems.</p>
<h2 id="convex-48_advanced_combinatorial-221-nature-of-combinatorial-problems">22.1 Nature of Combinatorial Problems<a class="headerlink" href="#convex-48_advanced_combinatorial-221-nature-of-combinatorial-problems" title="Permanent link">¶</a></h2>
<p>A combinatorial optimization problem can be expressed as:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{F}} f(x),
\]</div>
<p>where <span class="arithmatex">\(\mathcal{F}\)</span> is a finite or countable set of feasible solutions, often exponentially large in size.</p>
<p>Example forms include:</p>
<ul>
<li>Binary decisions: <span class="arithmatex">\(x_i \in \{0,1\}\)</span></li>
<li>Integer constraints: <span class="arithmatex">\(x_i \in \mathbb{Z}\)</span></li>
<li>Permutations: ordering or ranking elements</li>
</ul>
<p>Unlike convex problems, feasible regions are discrete, and local moves must be designed carefully to explore the combinatorial space.</p>
<h2 id="convex-48_advanced_combinatorial-222-graph-theoretic-foundations">22.2 Graph-Theoretic Foundations<a class="headerlink" href="#convex-48_advanced_combinatorial-222-graph-theoretic-foundations" title="Permanent link">¶</a></h2>
<p>Many combinatorial problems are naturally represented as graphs <span class="arithmatex">\(G = (V, E)\)</span>.</p>
<h3 id="convex-48_advanced_combinatorial-2221-shortest-path-problem">22.2.1 Shortest Path Problem<a class="headerlink" href="#convex-48_advanced_combinatorial-2221-shortest-path-problem" title="Permanent link">¶</a></h3>
<p>Given edge weights <span class="arithmatex">\(w_{ij}\)</span>, find a path from <span class="arithmatex">\(s\)</span> to <span class="arithmatex">\(t\)</span> minimizing total weight:
<script type="math/tex; mode=display">
\min_{\text{path } P} \sum_{(i,j)\in P} w_{ij}.
</script>
Efficiently solvable by Dijkstra’s or Bellman–Ford algorithms.</p>
<h3 id="convex-48_advanced_combinatorial-2222-minimum-spanning-tree-mst">22.2.2 Minimum Spanning Tree (MST)<a class="headerlink" href="#convex-48_advanced_combinatorial-2222-minimum-spanning-tree-mst" title="Permanent link">¶</a></h3>
<p>Find a subset of edges connecting all vertices with minimal total weight. Solved by Kruskal’s or Prim’s algorithm in <span class="arithmatex">\(O(E\log V)\)</span> time.</p>
<h3 id="convex-48_advanced_combinatorial-2223-maximum-flow-minimum-cut">22.2.3 Maximum Flow / Minimum Cut<a class="headerlink" href="#convex-48_advanced_combinatorial-2223-maximum-flow-minimum-cut" title="Permanent link">¶</a></h3>
<p>Determine how much “flow” can be sent through a network subject to capacity limits.  Duality connects max-flow and min-cut, linking graph algorithms to convex duality principles.</p>
<h2 id="convex-48_advanced_combinatorial-223-integer-linear-programming-ilp">22.3 Integer Linear Programming (ILP)<a class="headerlink" href="#convex-48_advanced_combinatorial-223-integer-linear-programming-ilp" title="Permanent link">¶</a></h2>
<p>An integer program seeks:
<script type="math/tex; mode=display">
\min_x \; c^\top x \quad \text{s.t. } A x \le b, \; x \in \mathbb{Z}^n.
</script>
</p>
<p>It generalizes many classical problems:</p>
<ul>
<li>Knapsack  </li>
<li>Assignment  </li>
<li>Scheduling  </li>
<li>Facility location</li>
</ul>
<p>Relaxing <span class="arithmatex">\(x \in \mathbb{Z}^n\)</span> to <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> yields a linear program (LP) that can be solved efficiently and provides a lower bound.</p>
<h2 id="convex-48_advanced_combinatorial-224-relaxation-and-rounding">22.4 Relaxation and Rounding<a class="headerlink" href="#convex-48_advanced_combinatorial-224-relaxation-and-rounding" title="Permanent link">¶</a></h2>
<p>A central idea is to solve a relaxed convex problem, then round its solution to a discrete one.</p>
<h3 id="convex-48_advanced_combinatorial-2241-lp-relaxation">22.4.1 LP Relaxation<a class="headerlink" href="#convex-48_advanced_combinatorial-2241-lp-relaxation" title="Permanent link">¶</a></h3>
<p>For binary variables <span class="arithmatex">\(x_i \in \{0,1\}\)</span>, relax to <span class="arithmatex">\(0 \le x_i \le 1\)</span> and solve via simplex or interior-point methods.</p>
<h3 id="convex-48_advanced_combinatorial-2242-semidefinite-relaxation">22.4.2 Semidefinite Relaxation<a class="headerlink" href="#convex-48_advanced_combinatorial-2242-semidefinite-relaxation" title="Permanent link">¶</a></h3>
<p>For quadratic binary problems, lift to a positive semidefinite matrix <span class="arithmatex">\(X = xx^\top\)</span>:
<script type="math/tex; mode=display">
\min \langle C, X \rangle \quad \text{s.t. } X_{ii} = 1, \; X \succeq 0.
</script>
Semidefinite relaxations are powerful in problems like MAX-CUT and clustering.</p>
<h3 id="convex-48_advanced_combinatorial-2243-randomized-rounding">22.4.3 Randomized Rounding<a class="headerlink" href="#convex-48_advanced_combinatorial-2243-randomized-rounding" title="Permanent link">¶</a></h3>
<p>Map fractional solutions back to integers probabilistically, preserving expected properties.</p>
<h2 id="convex-48_advanced_combinatorial-225-branch-and-bound-and-search-trees">22.5 Branch-and-Bound and Search Trees<a class="headerlink" href="#convex-48_advanced_combinatorial-225-branch-and-bound-and-search-trees" title="Permanent link">¶</a></h2>
<p>Exact combinatorial optimization often relies on enumeration enhanced by bounding.</p>
<h3 id="convex-48_advanced_combinatorial-2251-basic-principle">22.5.1 Basic Principle<a class="headerlink" href="#convex-48_advanced_combinatorial-2251-basic-principle" title="Permanent link">¶</a></h3>
<ol>
<li>Partition the feasible set into subsets (branching).  </li>
<li>Compute upper/lower bounds for each subset.  </li>
<li>Prune branches that cannot contain the optimum.  </li>
</ol>
<p>The algorithm systematically explores a search tree, guided by bounds.</p>
<h3 id="convex-48_advanced_combinatorial-2252-bounding-via-relaxations">22.5.2 Bounding via Relaxations<a class="headerlink" href="#convex-48_advanced_combinatorial-2252-bounding-via-relaxations" title="Permanent link">¶</a></h3>
<p>LP or convex relaxations provide efficient lower bounds, greatly reducing the search space.</p>
<h2 id="convex-48_advanced_combinatorial-226-dynamic-programming">22.6 Dynamic Programming<a class="headerlink" href="#convex-48_advanced_combinatorial-226-dynamic-programming" title="Permanent link">¶</a></h2>
<p>Dynamic programming (DP) decomposes a problem into overlapping subproblems:</p>
<div class="arithmatex">\[
\text{OPT}(S) = \min_{x \in S} \{ c(x) + \text{OPT}(S') \}.
\]</div>
<p>It is exact but can suffer from exponential growth (“curse of dimensionality”).</p>
<p>Applications:</p>
<ul>
<li>Shortest paths</li>
<li>Sequence alignment</li>
<li>Knapsack</li>
<li>Resource allocation</li>
</ul>
<p>DP offers exact solutions when structure allows sequential decomposition.</p>
<h2 id="convex-48_advanced_combinatorial-227-heuristics-and-metaheuristics-for-combinatorial-problems">22.7 Heuristics and Metaheuristics for Combinatorial Problems<a class="headerlink" href="#convex-48_advanced_combinatorial-227-heuristics-and-metaheuristics-for-combinatorial-problems" title="Permanent link">¶</a></h2>
<p>When exact methods become intractable, we turn to approximation and stochastic search.</p>
<h3 id="convex-48_advanced_combinatorial-2271-greedy-heuristics">22.7.1 Greedy Heuristics<a class="headerlink" href="#convex-48_advanced_combinatorial-2271-greedy-heuristics" title="Permanent link">¶</a></h3>
<p>Make locally optimal choices at each step (e.g., nearest neighbor in TSP, Kruskal’s MST). Fast but not always globally optimal.</p>
<h3 id="convex-48_advanced_combinatorial-2272-local-search-and-hill-climbing">22.7.2 Local Search and Hill Climbing<a class="headerlink" href="#convex-48_advanced_combinatorial-2272-local-search-and-hill-climbing" title="Permanent link">¶</a></h3>
<p>Iteratively improve a current solution by small perturbations (e.g., swap two items, reassign a job). Can be trapped in local minima.</p>
<h3 id="convex-48_advanced_combinatorial-2273-metaheuristic-extensions">22.7.3 Metaheuristic Extensions<a class="headerlink" href="#convex-48_advanced_combinatorial-2273-metaheuristic-extensions" title="Permanent link">¶</a></h3>
<ul>
<li>Simulated Annealing: controlled random acceptance of worse moves.  </li>
<li>Tabu Search: memory-based diversification.  </li>
<li>Ant Colony Optimization: probabilistic path construction.  </li>
<li>Genetic Algorithms and PSO: population-based evolution.  </li>
</ul>
<p>These approaches generalize to discrete structures with minimal problem-specific design.</p>
<h2 id="convex-48_advanced_combinatorial-228-approximation-algorithms">22.8 Approximation Algorithms<a class="headerlink" href="#convex-48_advanced_combinatorial-228-approximation-algorithms" title="Permanent link">¶</a></h2>
<p>Some combinatorial problems are provably intractable but allow approximation guarantees:
<script type="math/tex; mode=display">
f(x_{\text{approx}}) \le \alpha \, f(x^*),
</script>
where <span class="arithmatex">\(\alpha \ge 1\)</span> is the approximation ratio.</p>
<p>Examples:</p>
<ul>
<li>Greedy Set Cover: <span class="arithmatex">\(\alpha = \ln n + 1\)</span>  </li>
<li>Christofides’ Algorithm for TSP: <span class="arithmatex">\(\alpha = 1.5\)</span>  </li>
<li>MAX-CUT SDP Relaxation: <span class="arithmatex">\(\alpha \approx 0.878\)</span></li>
</ul>
<p>Approximation theory blends combinatorics with convex relaxation insights.</p>
<h2 id="convex-48_advanced_combinatorial-229-advanced-topics-constraint-programming-and-decomposition">22.9 Advanced Topics: Constraint Programming and Decomposition<a class="headerlink" href="#convex-48_advanced_combinatorial-229-advanced-topics-constraint-programming-and-decomposition" title="Permanent link">¶</a></h2>
<h3 id="convex-48_advanced_combinatorial-2291-constraint-programming-cp">22.9.1 Constraint Programming (CP)<a class="headerlink" href="#convex-48_advanced_combinatorial-2291-constraint-programming-cp" title="Permanent link">¶</a></h3>
<p>CP models problems as logical constraints rather than algebraic ones. Combines symbolic reasoning with domain reduction and backtracking.</p>
<h3 id="convex-48_advanced_combinatorial-2292-benders-and-dantzigwolfe-decomposition">22.9.2 Benders and Dantzig–Wolfe Decomposition<a class="headerlink" href="#convex-48_advanced_combinatorial-2292-benders-and-dantzigwolfe-decomposition" title="Permanent link">¶</a></h3>
<p>Divide large mixed-integer problems into master and subproblems, coordinating them iteratively. Widely used in logistics, energy, and planning.</p>
<h3 id="convex-48_advanced_combinatorial-2293-cutting-plane-methods">22.9.3 Cutting Plane Methods<a class="headerlink" href="#convex-48_advanced_combinatorial-2293-cutting-plane-methods" title="Permanent link">¶</a></h3>
<p>Iteratively add valid inequalities (cuts) to tighten the feasible region of a relaxed problem.</p>
<h2 id="convex-48_advanced_combinatorial-2210-applications-across-domains">22.10 Applications Across Domains<a class="headerlink" href="#convex-48_advanced_combinatorial-2210-applications-across-domains" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Field</th>
<th>Combinatorial Problem Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistics</td>
<td>Vehicle routing, warehouse layout</td>
</tr>
<tr>
<td>Telecommunications</td>
<td>Network design, channel allocation</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Feature selection, clustering, model compression</td>
</tr>
<tr>
<td>Finance</td>
<td>Portfolio optimization with integer positions</td>
</tr>
<tr>
<td>Bioinformatics</td>
<td>Genome assembly, protein structure inference</td>
</tr>
</tbody>
</table>
<p>Combinatorial optimization forms the backbone of modern infrastructure and decision systems.</p>
<hr>
<p>Combinatorial optimization embodies the art of solving discrete, structured problems where convexity no longer applies.  It draws from graph theory, algebra, logic, and probabilistic reasoning. Relaxation and approximation techniques build a bridge between the continuous and the discrete, uniting convex and combinatorial worlds.</p></body></html></section><section class="print-page" id="convex-50_future" heading-number="2.23"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence">Chapter 23: The Future of Optimization — Learning, Adaptation, and Intelligence<a class="headerlink" href="#convex-50_future-chapter-23-the-future-of-optimization-learning-adaptation-and-intelligence" title="Permanent link">¶</a></h1>
<p>Optimization has always been a dialogue between mathematics and computation.  From convex analysis and first-order methods to stochastic, heuristic, and learned algorithms, the field has evolved to match the increasing complexity of modern systems. This final chapter looks ahead — toward optimization methods that learn, adapt, and reason — merging human insight, data-driven modeling, and algorithmic intelligence.</p>
<h2 id="convex-50_future-231-from-fixed-algorithms-to-adaptive-systems">23.1 From Fixed Algorithms to Adaptive Systems<a class="headerlink" href="#convex-50_future-231-from-fixed-algorithms-to-adaptive-systems" title="Permanent link">¶</a></h2>
<p>Traditional optimization algorithms are designed by experts and fixed in form:</p>
<div class="arithmatex">\[
x_{k+1} = x_k - \alpha_k \nabla f(x_k),
\]</div>
<p>or</p>
<div class="arithmatex">\[
x_{k+1} = \text{Update}(x_k, \nabla f(x_k); \theta_{\text{fixed}}).
\]</div>
<p>But real-world problems change over time — data evolves, constraints shift, and objectives drift. In such environments, adaptive optimizers adjust their internal behavior online, learning to respond to context rather than following a static rule.</p>
<h2 id="convex-50_future-232-optimization-as-learning">23.2 Optimization as Learning<a class="headerlink" href="#convex-50_future-232-optimization-as-learning" title="Permanent link">¶</a></h2>
<p>Modern research reframes optimization itself as a learning problem. Rather than designing the optimizer, we can train it to perform well over a family of tasks.</p>
<p>A meta-optimizer <span class="arithmatex">\(\text{Opt}_\theta\)</span> is parameterized by <span class="arithmatex">\(\theta\)</span>, and trained to minimize:</p>
<div class="arithmatex">\[
\mathcal{L}(\theta) = \mathbb{E}_{f \sim \mathcal{D}}[f(\text{Opt}_\theta(f))],
\]</div>
<p>where <span class="arithmatex">\(\mathcal{D}\)</span> is a distribution over problem instances.</p>
<p>This approach produces optimizers that generalize to new problems, adapting their step sizes, directions, and search strategies automatically.</p>
<h2 id="convex-50_future-233-reinforcement-learned-optimization">23.3 Reinforcement-Learned Optimization<a class="headerlink" href="#convex-50_future-233-reinforcement-learned-optimization" title="Permanent link">¶</a></h2>
<p>Reinforcement learning (RL) provides a natural framework for sequential decision-making in optimization.</p>
<p>At each iteration:</p>
<ul>
<li>State: current iterate <span class="arithmatex">\(x_t\)</span>, gradient <span class="arithmatex">\(\nabla f(x_t)\)</span>, and loss <span class="arithmatex">\(f(x_t)\)</span>  </li>
<li>Action: choose an update <span class="arithmatex">\(\Delta x_t\)</span>  </li>
<li>Reward: improvement in objective, <span class="arithmatex">\(r_t = -[f(x_{t+1}) - f(x_t)]\)</span></li>
</ul>
<p>A policy <span class="arithmatex">\(\pi_\theta\)</span> learns to output update steps that maximize expected reward.<br>
This creates an optimizer that discovers efficient update strategies through experience.</p>
<p>RL-based optimizers have been successfully applied in:</p>
<ul>
<li>Hyperparameter tuning  </li>
<li>Neural architecture search  </li>
<li>Online control systems  </li>
<li>Adaptive sampling and scheduling</li>
</ul>
<h2 id="convex-50_future-234-neuroevolution-and-population-learning">23.4 Neuroevolution and Population Learning<a class="headerlink" href="#convex-50_future-234-neuroevolution-and-population-learning" title="Permanent link">¶</a></h2>
<p>Neuroevolution applies evolutionary algorithms to optimize neural network architectures or weights directly.<br>
Unlike gradient-based training, it requires no differentiability and is robust to nonconvex or discrete search spaces.</p>
<p>Population-based methods such as CMA-ES or Evolution Strategies (ES) can also serve as black-box gradient estimators:</p>
<div class="arithmatex">\[
\nabla_\theta \mathbb{E}[f(\theta)] \approx \frac{1}{\sigma} \mathbb{E}[f(\theta + \sigma \epsilon)\epsilon].
\]</div>
<p>They parallelize easily, scale well, and integrate with reinforcement learning for hybrid exploration–exploitation.</p>
<h2 id="convex-50_future-235-optimization-and-generative-models">23.5 Optimization and Generative Models<a class="headerlink" href="#convex-50_future-235-optimization-and-generative-models" title="Permanent link">¶</a></h2>
<p>Generative models like Variational Autoencoders (VAEs) and Diffusion Models have introduced a new perspective:<br>
Optimization can occur in the latent space of data distributions rather than directly in parameter space.</p>
<p>For example:</p>
<ul>
<li>Optimize a latent vector <span class="arithmatex">\(z\)</span> to generate a design with desired properties.  </li>
<li>Use differentiable surrogates to backpropagate through generative pipelines.  </li>
<li>Apply gradient-based search within learned manifolds.</li>
</ul>
<p>This blending of optimization and generation enables creativity — from molecule design to engineering shape synthesis.</p>
<h2 id="convex-50_future-236-federated-and-decentralized-optimization">23.6 Federated and Decentralized Optimization<a class="headerlink" href="#convex-50_future-236-federated-and-decentralized-optimization" title="Permanent link">¶</a></h2>
<p>The rise of distributed data (mobile devices, IoT, and edge computing) calls for federated optimization.<br>
Each client <span class="arithmatex">\(i\)</span> holds local data <span class="arithmatex">\(D_i\)</span> and solves:</p>
<div class="arithmatex">\[
\min_x \; F(x) = \frac{1}{N}\sum_i f_i(x),
\]</div>
<p>without sharing raw data.</p>
<p>Algorithms like FedAvg and FedProx aggregate local updates securely, preserving privacy while enabling collaborative optimization at global scale.</p>
<p>Challenges include:</p>
<ul>
<li>Communication efficiency  </li>
<li>Heterogeneity of data and computation  </li>
<li>Privacy and fairness constraints</li>
</ul>
<h2 id="convex-50_future-237-optimization-under-uncertainty">23.7 Optimization Under Uncertainty<a class="headerlink" href="#convex-50_future-237-optimization-under-uncertainty" title="Permanent link">¶</a></h2>
<p>Modern systems often face uncertain environments:
- Random perturbations in data<br>
- Dynamic constraints<br>
- Unpredictable feedback</p>
<p>Approaches to manage uncertainty include:</p>
<ol>
<li>
<p>Robust Optimization:<br>
   Minimize worst-case loss under bounded perturbations:
   <script type="math/tex; mode=display">
   \min_x \max_{\delta \in \Delta} f(x + \delta).
   </script>
</p>
</li>
<li>
<p>Stochastic Programming:<br>
   Optimize expected value or risk measure:
   <script type="math/tex; mode=display">
   \min_x \mathbb{E}_\xi[f(x, \xi)].
   </script>
</p>
</li>
<li>
<p>Distributionally Robust Optimization (DRO):<br>
   Hedge against model misspecification by optimizing over nearby probability distributions.</p>
</li>
</ol>
<p>These frameworks connect convex theory with probabilistic reasoning and data-driven inference.</p>
<h2 id="convex-50_future-238-quantum-and-analog-optimization">23.8 Quantum and Analog Optimization<a class="headerlink" href="#convex-50_future-238-quantum-and-analog-optimization" title="Permanent link">¶</a></h2>
<p>As hardware advances, new paradigms emerge:
- Quantum Annealing: uses quantum tunneling to escape local minima.
- Adiabatic Quantum Computing: evolves a Hamiltonian to encode an optimization problem.
- Analog and Neuromorphic Systems: exploit physical dynamics (e.g., Ising machines, optical circuits) to perform optimization in hardware.</p>
<p>Though still experimental, these systems promise exponential speedups or energy-efficient optimization for structured problems.</p>
<h2 id="convex-50_future-239-optimization-and-intelligence">23.9 Optimization and Intelligence<a class="headerlink" href="#convex-50_future-239-optimization-and-intelligence" title="Permanent link">¶</a></h2>
<p>Optimization now underpins not only engineering but also learning, reasoning, and intelligence.  Deep learning, reinforcement learning, and symbolic AI all rely on iterative improvement processes — in essence, optimization loops.</p>
<p>Emerging research seeks to unify:</p>
<ul>
<li>Learning to optimize — algorithms that adapt through data.  </li>
<li>Optimizing to learn — systems that adjust representations via optimization.  </li>
<li>Self-improving optimizers — algorithms that recursively tune their own parameters.</li>
</ul>
<p>This convergence blurs the line between <em>optimizer</em> and <em>learner</em>.</p>
<hr>
<p>From the geometry of convex sets to the dynamics of neural networks, optimization has evolved from a theory of guarantees into a framework of discovery. The next generation of algorithms will not only solve problems but learn how to solve — autonomously, efficiently, and creatively.</p>
<p>Optimization is no longer just about minimizing loss or maximizing utility. It is about enabling systems — and thinkers — to improve themselves.</p></body></html></section></section>
                    <section class='print-page md-section' id='section-3' heading-number='3'>
                        <h1>Cheat Sheets<a class='headerlink' href='#section-3' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="cheatsheets-20a_cheatsheet" heading-number="3.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="comprehensive-optimization-algorithm-cheat-sheet">Comprehensive Optimization Algorithm Cheat Sheet<a class="headerlink" href="#cheatsheets-20a_cheatsheet-comprehensive-optimization-algorithm-cheat-sheet" title="Permanent link">¶</a></h1>
<p>This reference summarizes optimization algorithms across convex optimization, large-scale machine learning, and derivative-free global search.<br>
It balances <strong>theoretical precision</strong> with <strong>practical intuition</strong>—from gradient-based solvers to black-box evolutionary methods.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-how-to-read-this-table">🧭 How to Read This Table<a class="headerlink" href="#cheatsheets-20a_cheatsheet-how-to-read-this-table" title="Permanent link">¶</a></h2>
<p>Each method lists:
- <strong>Problem Type</strong> — the class of objectives it applies to.
- <strong>Assumptions</strong> — smoothness, convexity, or structural conditions.
- <strong>Core Update Rule</strong> — canonical iteration.
- <strong>Scalability</strong> — computational feasibility.
- <strong>Per-Iteration Cost</strong> — approximate computational complexity.
- <strong>Applications</strong> — typical ML or engineering use cases.</p>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-first-order-methods">🚀 First-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-first-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent (GD)</td>
<td>Unconstrained smooth (convex/nonconvex)</td>
<td>Differentiable; <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Logistic regression, least squares</td>
</tr>
<tr>
<td>Nesterov’s Accelerated GD</td>
<td>Smooth convex (fast rate)</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth</td>
<td><span class="arithmatex">\(y_k = x_k + \frac{k-1}{k+2}(x_k - x_{k-1})\)</span>; <span class="arithmatex">\(x_{k+1} = y_k - \eta \nabla f(y_k)\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Accelerated convex models</td>
</tr>
<tr>
<td>(Polyak) Heavy-Ball Momentum</td>
<td>Unconstrained smooth</td>
<td>Differentiable, <span class="arithmatex">\(\beta \in (0,1)\)</span></td>
<td><span class="arithmatex">\(x_{k+1} = x_k - \eta \nabla f(x_k) + \beta(x_k - x_{k-1})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Deep networks, convex smooth losses</td>
</tr>
<tr>
<td>Conjugate Gradient (CG)</td>
<td>Quadratic or linear systems <span class="arithmatex">\(Ax=b\)</span></td>
<td><span class="arithmatex">\(A\)</span> symmetric positive definite</td>
<td><span class="arithmatex">\(p_{k+1}=r_{k+1}+\beta_k p_k\)</span>, <span class="arithmatex">\(x_{k+1}=x_k+\alpha_k p_k\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Large-scale least squares, implicit Newton steps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td>Non-Euclidean geometry</td>
<td>Convex; mirror map <span class="arithmatex">\(\psi\)</span> strongly convex</td>
<td><span class="arithmatex">\(x_{k+1} = \nabla \psi^*(\nabla \psi(x_k) - \eta \nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Probability simplex, online learning</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Conjugate Gradient (CG)</em> bridges first- and second-order methods: it achieves exact convergence in at most <span class="arithmatex">\(d\)</span> steps for quadratic problems without storing the Hessian, making it ideal for large-scale convex systems.</p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-second-order-methods">⚙️ Second-Order Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-second-order-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Per-Iteration Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Newton’s Method</td>
<td>Smooth convex</td>
<td>Twice differentiable; <span class="arithmatex">\(\nabla^2 f(x)\)</span> PD</td>
<td><span class="arithmatex">\(x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1}\nabla f(x_k)\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>Logistic regression (IRLS), convex solvers</td>
</tr>
<tr>
<td>BFGS / L-BFGS</td>
<td>Smooth convex</td>
<td>Differentiable, approximate Hessian</td>
<td>Solve <span class="arithmatex">\(B_k p_k=-\nabla f(x_k)\)</span>; update <span class="arithmatex">\(B_k\)</span> via secant rule</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>GLMs, medium ML models</td>
</tr>
<tr>
<td>Trust-Region</td>
<td>Smooth convex/nonconvex</td>
<td>Twice differentiable</td>
<td><span class="arithmatex">\(\min_p \tfrac{1}{2}p^\top \nabla^2 f(x_k)p + \nabla f(x_k)^\top p\)</span> s.t. <span class="arithmatex">\(\|p\|\le\Delta_k\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>TRPO, physics-based ML</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods">🧮 Proximal, Projected &amp; Splitting Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-proximal-projected-splitting-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proximal Gradient (ISTA)</td>
<td>Composite <span class="arithmatex">\(f=g+h\)</span></td>
<td><span class="arithmatex">\(g\)</span> smooth, <span class="arithmatex">\(h\)</span> convex</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\alpha h}(x_k-\alpha\nabla g(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>LASSO, sparse recovery</td>
</tr>
<tr>
<td>FISTA</td>
<td>Same as ISTA</td>
<td>Convex, <span class="arithmatex">\(L\)</span>-smooth <span class="arithmatex">\(g\)</span></td>
<td>Like ISTA with momentum</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Compressed sensing</td>
</tr>
<tr>
<td>Projected Gradient (PG)</td>
<td>Convex constrained</td>
<td><span class="arithmatex">\(f\)</span> smooth; easy projection</td>
<td><span class="arithmatex">\(x_{k+1}=\Pi_C(x_k-\eta\nabla f(x_k))\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> + projection</td>
<td>Box/simplex constraints</td>
</tr>
<tr>
<td>ADMM</td>
<td>Separable convex + linear constraints</td>
<td><span class="arithmatex">\(f,g\)</span> convex</td>
<td>Alternating minimization + dual update</td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span> per block</td>
<td>Distributed ML, consensus</td>
</tr>
<tr>
<td>Majorization–Minimization (MM)</td>
<td>Convex/nonconvex</td>
<td>$g(x</td>
<td>x_k)\ge f(x)$</td>
<td>$x_{k+1}=\arg\min g(x</td>
<td>x_k)$</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-coordinate-block-methods">🧩 Coordinate &amp; Block Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-coordinate-block-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinate Descent (CD)</td>
<td>Separable convex</td>
<td>Convex, differentiable</td>
<td>Update one coordinate: <span class="arithmatex">\(x_{i}^{k+1}=x_i^k-\eta\partial_i f(x^k)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(d)\)</span></td>
<td>LASSO, SVM duals</td>
</tr>
<tr>
<td>Block Coordinate Descent (BCD)</td>
<td>Block separable</td>
<td>Convex per block</td>
<td>Minimize over <span class="arithmatex">\(x^{(j)}\)</span> while fixing others</td>
<td>Large</td>
<td><span class="arithmatex">\(O(nd_j)\)</span></td>
<td>Matrix factorization, alternating minimization</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Coordinate descent exploits separability; often faster than full gradient when updates are cheap or sparse.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods">🎲 Stochastic &amp; Mini-Batch Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-stochastic-mini-batch-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stochastic Gradient Descent (SGD)</td>
<td>Large-scale / streaming</td>
<td>Unbiased stochastic gradients</td>
<td><span class="arithmatex">\(x_{k+1}=x_k-\eta_t\nabla f_{i_k}(x_k)\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Deep learning, online learning</td>
</tr>
<tr>
<td>Variance-Reduced (SVRG/SAGA/SARAH)</td>
<td>Finite-sum convex</td>
<td>Smooth, strongly convex</td>
<td><span class="arithmatex">\(v_k=\nabla f_{i_k}(x_k)-\nabla f_{i_k}(\tilde{x})+\nabla f(\tilde{x})\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Logistic regression, GLMs</td>
</tr>
<tr>
<td>Adaptive SGD (Adam/RMSProp/Adagrad)</td>
<td>Nonconvex stochastic</td>
<td>Bounded variance</td>
<td><span class="arithmatex">\(m_k=\beta_1m_{k-1}+(1-\beta_1)g_k\)</span>, <span class="arithmatex">\(v_k=\beta_2v_{k-1}+(1-\beta_2)g_k^2\)</span></td>
<td>Very Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Neural networks</td>
</tr>
<tr>
<td>Proximal Stochastic (Prox-SGD / Prox-SAGA)</td>
<td>Nonsmooth stochastic</td>
<td><span class="arithmatex">\(f=g+h\)</span> with prox of <span class="arithmatex">\(h\)</span> known</td>
<td><span class="arithmatex">\(x_{k+1}=\operatorname{prox}_{\eta h}(x_k-\eta\widehat{\nabla g}(x_k))\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(bd)\)</span></td>
<td>Sparse online learning</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-interior-point-augmented-methods">🧱 Interior-Point &amp; Augmented Methods<a class="headerlink" href="#cheatsheets-20a_cheatsheet-interior-point-augmented-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Update Rule</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interior-Point</td>
<td>Convex with inequalities</td>
<td>Slater’s condition, self-concordant barrier</td>
<td>Solve <span class="arithmatex">\(\min f_0(x)-\tfrac{1}{t}\sum_i\log(-g_i(x))\)</span></td>
<td>Small–Medium</td>
<td><span class="arithmatex">\(O(d^3)\)</span></td>
<td>LP, QP, SDP</td>
</tr>
<tr>
<td>Augmented Lagrangian (ALM)</td>
<td>Constrained convex</td>
<td><span class="arithmatex">\(f,g\)</span> convex; equality constraints</td>
<td><span class="arithmatex">\(L_\rho(x,\lambda)=f(x)+\lambda^T g(x)+\tfrac{\rho}{2}\|g(x)\|^2\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(nd)\)</span></td>
<td>Penalty methods, PDEs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization">🌐 Derivative-Free &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-derivative-free-black-box-optimization" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Problem Type</th>
<th>Assumptions</th>
<th>Core Idea</th>
<th>Scalability</th>
<th>Cost</th>
<th>Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nelder–Mead Simplex</td>
<td>Low-dimensional, smooth or noisy</td>
<td>No gradients; continuous <span class="arithmatex">\(f\)</span></td>
<td>Maintain simplex of <span class="arithmatex">\(d+1\)</span> points; reflect–expand–contract–shrink operations</td>
<td>Small</td>
<td><span class="arithmatex">\(O(d^2)\)</span></td>
<td>Parameter tuning, physics models</td>
</tr>
<tr>
<td>Simulated Annealing</td>
<td>Nonconvex, global</td>
<td>Stochastic exploration via temperature</td>
<td>Random perturbations accepted w.p. <span class="arithmatex">\(\exp(-\Delta f/T)\)</span>; <span class="arithmatex">\(T\downarrow\)</span></td>
<td>Medium</td>
<td>High (many samples)</td>
<td>Hyperparameter tuning, design optimization</td>
</tr>
<tr>
<td>Multi-start Local Search</td>
<td>Nonconvex</td>
<td>None; relies on restart diversity</td>
<td>Run local solver from multiple random inits, pick best result</td>
<td>Medium</td>
<td><span class="arithmatex">\(k\times\)</span> local solver</td>
<td>Avoids local minima; cheap global heuristic</td>
</tr>
<tr>
<td>Evolutionary Algorithms (EA)</td>
<td>Black-box, global</td>
<td>Population-based; fitness function only</td>
<td>Mutate, select, recombine candidates</td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span> per gen</td>
<td>Global optimization, control, AutoML</td>
</tr>
<tr>
<td>Genetic Algorithms (GA)</td>
<td>Combinatorial / continuous</td>
<td>Chromosomal encoding of solutions</td>
<td>Apply selection, crossover, mutation; evolve over generations</td>
<td>Medium–Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Feature selection, neural architecture search</td>
</tr>
<tr>
<td>Evolution Strategies (ES)</td>
<td>Continuous, black-box</td>
<td>Gaussian mutation around mean</td>
<td><span class="arithmatex">\(\theta_{k+1} = \theta_k + \eta \sum_i w_i \epsilon_i f(\theta_k+\sigma \epsilon_i)\)</span></td>
<td>Large</td>
<td><span class="arithmatex">\(O(Pd)\)</span></td>
<td>Reinforcement learning, black-box control</td>
</tr>
<tr>
<td>Derivative-Free Optimization (DFO)</td>
<td>Black-box, noisy <span class="arithmatex">\(f\)</span></td>
<td>Only function values available</td>
<td>Gradient estimated via random perturbations: <span class="arithmatex">\(g\approx\frac{f(x+hu)-f(x)}{h}u\)</span></td>
<td>Medium</td>
<td><span class="arithmatex">\(O(d)\)</span>–<span class="arithmatex">\(O(d^2)\)</span></td>
<td>Robotics, policy search, design</td>
</tr>
<tr>
<td>Black-Box Optimization Framework</td>
<td>General</td>
<td>No analytical gradients; often stochastic</td>
<td>Unified term covering EA, GA, ES, and DFO</td>
<td>Medium–Large</td>
<td>varies</td>
<td>Hyperparameter search, AutoML, reinforcement learning</td>
</tr>
<tr>
<td>Numerical Encodings</td>
<td>Used in GA/EA</td>
<td>Represents variables in binary, integer, or floating-point form</td>
<td>Choice of encoding impacts mutation/crossover behavior</td>
<td>N/A</td>
<td>negligible</td>
<td>Optimization of mixed or discrete variables</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Black-box and evolutionary methods trade theoretical guarantees for robustness and global search power. They are essential when gradients are unavailable or noninformative.</em></p>
</blockquote>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-convergence-complexity-snapshot">📈 Convergence &amp; Complexity Snapshot<a class="headerlink" href="#cheatsheets-20a_cheatsheet-convergence-complexity-snapshot" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method Type</th>
<th>Convergence (Convex)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Subgradient</td>
<td><span class="arithmatex">\(O(1/\sqrt{k})\)</span></td>
<td>Nonsmooth convex</td>
</tr>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Smooth convex</td>
</tr>
<tr>
<td>Accelerated Gradient</td>
<td><span class="arithmatex">\(O(1/k^2)\)</span></td>
<td>Optimal first-order</td>
</tr>
<tr>
<td>Newton / Quasi-Newton</td>
<td>Quadratic / Superlinear</td>
<td>Local only</td>
</tr>
<tr>
<td>Strongly Convex</td>
<td><span class="arithmatex">\((1-\mu/L)^k\)</span></td>
<td>Linear rate</td>
</tr>
<tr>
<td>Variance-Reduced</td>
<td>Linear (strongly convex)</td>
<td>Finite-sum optimization</td>
</tr>
<tr>
<td>ADMM / Proximal</td>
<td><span class="arithmatex">\(O(1/k)\)</span></td>
<td>Composite convex</td>
</tr>
<tr>
<td>Interior-Point</td>
<td>Polynomial time</td>
<td>High-accuracy convex</td>
</tr>
<tr>
<td>Derivative-Free / Heuristics</td>
<td>No formal bound</td>
<td>Empirical convergence only</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="cheatsheets-20a_cheatsheet-practitioner-summary">🧠 Practitioner Summary<a class="headerlink" href="#cheatsheets-20a_cheatsheet-practitioner-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradients available, smooth convex</td>
<td>Gradient Descent, Nesterov</td>
</tr>
<tr>
<td>Curvature matters, moderate scale</td>
<td>Newton, BFGS, Conjugate Gradient</td>
</tr>
<tr>
<td>Nonsmooth regularizer</td>
<td>Proximal Gradient, ADMM</td>
</tr>
<tr>
<td>Simple constraints</td>
<td>Projected Gradient</td>
</tr>
<tr>
<td>Large-scale / streaming</td>
<td>SGD, Adam, RMSProp</td>
</tr>
<tr>
<td>Finite-sum convex</td>
<td>SVRG, SAGA</td>
</tr>
<tr>
<td>Online / adaptive</td>
<td>Mirror Descent, FTRL</td>
</tr>
<tr>
<td>No gradients (black-box)</td>
<td>DFO, Nelder–Mead, ES, GA</td>
</tr>
<tr>
<td>Global nonconvex search</td>
<td>Simulated Annealing, Multi-starts, Evolutionary Algorithms</td>
</tr>
<tr>
<td>Distributed / separable</td>
<td>ADMM, ALM</td>
</tr>
<tr>
<td>High-precision convex programs</td>
<td>Interior-Point, Trust-Region</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization">🧩 Notes on Global &amp; Black-Box Optimization<a class="headerlink" href="#cheatsheets-20a_cheatsheet-notes-on-global-black-box-optimization" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Conjugate Gradient</strong>: memory-efficient quasi-second-order method for large convex quadratics.  </li>
<li><strong>Nelder–Mead</strong>: simplex reflection algorithm; widely used in physics and hyperparameter tuning.  </li>
<li><strong>Simulated Annealing</strong>: probabilistic global search inspired by thermodynamics.  </li>
<li><strong>Multi-Starts</strong>: pragmatic global exploration by repeated local optimization.  </li>
<li><strong>Evolutionary / Genetic / ES</strong>: population-based global heuristics; robust to noise and discontinuity.  </li>
<li><strong>Derivative-Free Optimization (DFO)</strong>: umbrella for random, surrogate-based, or adaptive black-box methods.  </li>
<li><strong>Numerical Encoding</strong>: crucial in discrete search—how real or binary variables are represented determines performance.</li>
</ul>
<hr>
<blockquote>
<p><strong>Summary Insight:</strong><br>
- Convex + differentiable → use gradient-based or Newton-type methods.<br>
- Convex + nonsmooth → use proximal, ADMM, or coordinate descent.<br>
- Large-scale or stochastic → use SGD or adaptive variants.<br>
- No gradients or nonconvex → use derivative-free or evolutionary methods.<br>
- The structure of the objective, not its size alone, determines the optimal solver family.</p>
</blockquote></body></html></section></section>
                    <section class='print-page md-section' id='section-4' heading-number='4'>
                        <h1>Appendices<a class='headerlink' href='#section-4' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="appendices-120_ineqaulities" heading-number="4.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-a-common-inequalities-and-identities">Appendix A: Common Inequalities and Identities<a class="headerlink" href="#appendices-120_ineqaulities-appendix-a-common-inequalities-and-identities" title="Permanent link">¶</a></h1>
<p>This appendix collects important inequalities used throughout convex analysis and optimisation. These are the “algebraic tools” you reach for in proofs, optimality arguments, and convergence analysis (Boyd and Vandenberghe, 2004; Hiriart-Urruty and Lemaréchal, 2001).</p>
<h2 id="appendices-120_ineqaulities-a1-cauchyschwarz-inequality">A.1 Cauchy–Schwarz inequality<a class="headerlink" href="#appendices-120_ineqaulities-a1-cauchyschwarz-inequality" title="Permanent link">¶</a></h2>
<p>For any <span class="arithmatex">\(x,y \in \mathbb{R}^n\)</span>,
<script type="math/tex; mode=display">
|x^\top y| \le \|x\|_2 \, \|y\|_2.
</script>
</p>
<p>Equality holds if and only if <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span> are linearly dependent.</p>
<p>Consequences:</p>
<ul>
<li>Defines the notion of angle between vectors.</li>
<li>Justifies dual norms.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a2-jensens-inequality">A.2 Jensen’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a2-jensens-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be convex, and let <span class="arithmatex">\(X\)</span> be a random variable. Then
<script type="math/tex; mode=display">
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
</script>
</p>
<p>In finite form: for <span class="arithmatex">\(\theta_i \ge 0\)</span> with <span class="arithmatex">\(\sum_i \theta_i = 1\)</span>,
<script type="math/tex; mode=display">
f\!\left(\sum_i \theta_i x_i\right)
\le
\sum_i \theta_i f(x_i).
</script>
</p>
<p>Jensen’s inequality is equivalent to convexity: it says “the function at the average is no more than the average of the function values.” It is used constantly to prove convexity of expectations and log-sum-exp.</p>
<h2 id="appendices-120_ineqaulities-a3-amgm-inequality">A.3 AM–GM inequality<a class="headerlink" href="#appendices-120_ineqaulities-a3-amgm-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(x_1,\dots,x_n \ge 0\)</span>,
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^n x_i
\ge
\left(\prod_{i=1}^n x_i \right)^{1/n}.
</script>
</p>
<p>This can be proved using Jensen’s inequality with <span class="arithmatex">\(f(t) = \log t\)</span>, which is concave. AM–GM appears frequently in inequality-constrained optimisation, e.g. bounding products by sums.</p>
<h2 id="appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz">A.4 Hölder’s inequality (generalised Cauchy–Schwarz)<a class="headerlink" href="#appendices-120_ineqaulities-a4-holders-inequality-generalised-cauchyschwarz" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(p,q \ge 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> (conjugate exponents),
<script type="math/tex; mode=display">
\sum_{i=1}^n |x_i y_i|
\le
\left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
\left( \sum_{i=1}^n |y_i|^q \right)^{1/q}.
</script>
</p>
<ul>
<li>When <span class="arithmatex">\(p=q=2\)</span>, Hölder becomes Cauchy–Schwarz.</li>
<li>Hölder underlies dual norms: the dual of <span class="arithmatex">\(\ell_p\)</span> is <span class="arithmatex">\(\ell_q\)</span>.</li>
</ul>
<h2 id="appendices-120_ineqaulities-a5-youngs-inequality">A.5 Young’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a5-youngs-inequality" title="Permanent link">¶</a></h2>
<p>For <span class="arithmatex">\(a,b \ge 0\)</span> and <span class="arithmatex">\(p,q &gt; 1\)</span> with <span class="arithmatex">\(\frac{1}{p} + \frac{1}{q} = 1\)</span>,
<script type="math/tex; mode=display">
ab \le \frac{a^p}{p} + \frac{b^q}{q}.
</script>
</p>
<p>This is useful in bounding cross terms in convergence proofs.</p>
<h2 id="appendices-120_ineqaulities-a6-fenchels-inequality">A.6 Fenchel’s inequality<a class="headerlink" href="#appendices-120_ineqaulities-a6-fenchels-inequality" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f\)</span> be a convex function and let <span class="arithmatex">\(f^*\)</span> be its convex conjugate:
<script type="math/tex; mode=display">
f^*(y) = \sup_x (y^\top x - f(x)).
</script>
</p>
<p>Then for all <span class="arithmatex">\(x,y\)</span>,
<script type="math/tex; mode=display">
f(x) + f^*(y) \ge y^\top x.
</script>
</p>
<p>Fenchel’s inequality is at the heart of convex duality. In fact, weak duality in Chapter 8 is essentially an application of Fenchel’s inequality.</p>
<h2 id="appendices-120_ineqaulities-a7-supporting-hyperplane-inequality">A.7 Supporting hyperplane inequality<a class="headerlink" href="#appendices-120_ineqaulities-a7-supporting-hyperplane-inequality" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(f\)</span> is convex, then for any <span class="arithmatex">\(x\)</span> and any <span class="arithmatex">\(g \in \partial f(x)\)</span>,
<script type="math/tex; mode=display">
f(y) \ge f(x) + g^\top (y-x)
\quad \text{for all } y.
</script>
</p>
<p>This can be viewed as “<span class="arithmatex">\(f\)</span> lies above all its tangent hyperplanes,” even when it’s not differentiable. This is both a characterisation of convexity and the definition of subgradients.</p>
<h2 id="appendices-120_ineqaulities-a8-summary">A.8 Summary<a class="headerlink" href="#appendices-120_ineqaulities-a8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Cauchy–Schwarz and Hölder bound inner products.</li>
<li>Jensen shows convexity and expectation interact cleanly.</li>
<li>Fenchel’s inequality is the algebra of duality.</li>
<li>Supporting hyperplane inequality is the geometry of convexity.</li>
</ul>
<p>These inequalities are used implicitly all over convex optimisation.</p></body></html></section><section class="print-page" id="appendices-130_projections" heading-number="4.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><p>Projection is the operation of finding the closest point in a given set to a point outside the set. It is a key step in many algorithms (projected gradient, alternating projections, etc.) to enforce constraints. Geometrically, projections are about “dropping perpendiculars” to a subspace or convex set.</p>
<p>Projection onto a subspace: Let <span class="arithmatex">\(W \subseteq \mathbb{R}^n\)</span> be a subspace (e.g. defined by a set of linear equations <span class="arithmatex">\(Ax=0\)</span> or spanned by some basis vectors). The orthogonal projection of any <span class="arithmatex">\(x \in \mathbb{R}^n\)</span> onto <span class="arithmatex">\(W\)</span> is the unique point <span class="arithmatex">\(P_W(x) \in W\)</span> such that <span class="arithmatex">\(x - P_W(x)\)</span> is orthogonal to <span class="arithmatex">\(W\)</span>. If <span class="arithmatex">\({q_1,\dots,q_k}\)</span> is an orthonormal basis of <span class="arithmatex">\(W\)</span>, then
​
<script type="math/tex; mode=display">
P_W(x) = \sum_{i=1}^k \langle x, q_i \rangle \, q_i
</script>
</p>
<p>as mentioned earlier. This <span class="arithmatex">\(P_W(x)\)</span> minimizes the distance <span class="arithmatex">\(|x - y|2\)</span> over all <span class="arithmatex">\(y\in W\)</span>. The residual <span class="arithmatex">\(r = x - P_W(x)\)</span> is orthogonal to every direction in <span class="arithmatex">\(W\)</span>. For example, projecting a point in space onto a plane is dropping a perpendicular to the plane. In <span class="arithmatex">\(\mathbb{R}^n\)</span>, <span class="arithmatex">\(P_W\)</span> is an <span class="arithmatex">\(n \times n\)</span> matrix (if <span class="arithmatex">\(W\)</span> is <span class="arithmatex">\(k\)</span>-dimensional, <span class="arithmatex">\(P_W\)</span> has rank <span class="arithmatex">\(k\)</span>) that satisfies <span class="arithmatex">\(P_W^2 = P_W\)</span> (idempotent) and <span class="arithmatex">\(P_W = P_W^T\)</span> (symmetric). In optimization, if we are constrained to <span class="arithmatex">\(W\)</span>, a projected gradient step does <span class="arithmatex">\(x_{k+1} = P_W(x_k - \alpha \nabla f(x_k))\)</span> to ensure <span class="arithmatex">\(x_{k+1} \in W\)</span>.</p>
<p>Projection onto a convex set: More generally, for a closed convex set <span class="arithmatex">\(C \subset \mathbb{R}^n\)</span>, the projection <span class="arithmatex">\(\operatorname{proj}_C(x)\)</span> is defined as the unique point in <span class="arithmatex">\(C\)</span> closest to <span class="arithmatex">\(x\)</span>:</p>
<div class="arithmatex">\[
P_C(x) = \arg\min_{y \in C} \|x - y\|_2
\]</div>
<p>For convex <span class="arithmatex">\(C\)</span>, this best approximation exists and is unique. While we may not have a simple formula like in the subspace case, projections onto many sets have known formulas or efficient algorithms (e.g. projecting onto a box <span class="arithmatex">\([l,u]\)</span> just clips each coordinate between <span class="arithmatex">\(l\)</span> and <span class="arithmatex">\(u\)</span>). Some properties of convex projections: </p>
<ul>
<li>
<p><span class="arithmatex">\(P_C(x)\)</span> lies on the boundary of <span class="arithmatex">\(C\)</span> along the direction of <span class="arithmatex">\(x\)</span> if <span class="arithmatex">\(x \notin C\)</span>. </p>
</li>
<li>
<p>The first-order optimality condition for the minimization above says <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the tangent of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>, or equivalently <span class="arithmatex">\(\langle x - P_C(x), y - P_C(x)\rangle \le 0\)</span> for all <span class="arithmatex">\(y \in C\)</span>. This means the line from <span class="arithmatex">\(P_C(x)\)</span> to <span class="arithmatex">\(x\)</span> forms a supporting hyperplane to <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(P_C(x)\)</span>. </p>
</li>
<li>Also, projections are firmly non-expansive: <span class="arithmatex">\(|P_C(x)-P_C(y)|^2 \le \langle P_C(x)-P_C(y), x-y \rangle \le |x-y|^2\)</span>. Intuitively, projecting cannot increase distances and in fact pulls points closer in a very controlled way. This is important for convergence of algorithms like alternating projections and proximal point methods, ensuring stability.</li>
</ul>
<p>Examples:</p>
<ul>
<li>
<p>Projection onto an affine set <span class="arithmatex">\(Ax=b\)</span> (assuming it’s consistent) can be derived via normal equations: one finds a correction <span class="arithmatex">\(\delta x\)</span> in the row space of <span class="arithmatex">\(A^T\)</span> such that <span class="arithmatex">\(A(x+\delta x)=b\)</span>. The solution is <span class="arithmatex">\(P_C(x) = x - A^T(AA^T)^{-1}(Ax-b)\)</span> (for full row rank <span class="arithmatex">\(A\)</span>).</p>
</li>
<li>
<p>Projection onto the nonnegative orthant <span class="arithmatex">\({x: x_i\ge0}\)</span> just sets <span class="arithmatex">\(x_i^- = \min(x_i,0)\)</span> to zero (i.e. <span class="arithmatex">\([x]^+ = \max{x,0}\)</span> componentwise). This is used in nonnegativity constraints.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_2\)</span> ball <span class="arithmatex">\({|x|_2 \le \alpha}\)</span> scales <span class="arithmatex">\(x\)</span> down to have length <span class="arithmatex">\(\alpha\)</span> if <span class="arithmatex">\(|x|&gt;\alpha\)</span>, or does nothing if <span class="arithmatex">\(|x|\le\alpha\)</span>.</p>
</li>
<li>
<p>Projection onto an <span class="arithmatex">\(\ell_1\)</span> ball (for sparsity) is more involved but essentially does soft-thresholding on coordinates to make the sum of absolute values equal <span class="arithmatex">\(\alpha\)</span>.</p>
</li>
</ul>
<p>Why projections matter in optimization: Many convex optimization problems involve constraints <span class="arithmatex">\(x \in C\)</span> where <span class="arithmatex">\(C\)</span> is convex. If we can compute <span class="arithmatex">\(P_C(x)\)</span> easily, we can use projection-based algorithms. For instance, projected gradient descent: if we move in the negative gradient direction and then project back to <span class="arithmatex">\(C\)</span>, we guarantee the iterate stays feasible and we still decrease the objective (for small enough step). The property of projections that <span class="arithmatex">\((x - P_C(x))\)</span> is orthogonal to the feasible region at <span class="arithmatex">\(P_C(x)\)</span> connects to KKT conditions: at optimum <span class="arithmatex">\(\hat{x}\)</span> with <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(\hat{x}))\)</span>, the vector <span class="arithmatex">\(-\nabla f(\hat{x})\)</span> must lie in the normal cone of <span class="arithmatex">\(C\)</span> at <span class="arithmatex">\(\hat{x}\)</span>, meaning the gradient is “balanced” by the constraint boundary — this is exactly the intuition behind Lagrange multipliers. In fact, one of the KKT conditions can be seen as stating that <span class="arithmatex">\(\hat{x} = P_C(x^* - \alpha \nabla f(x^*))\)</span> for some step <span class="arithmatex">\(\alpha\)</span>, i.e. you cannot find a feasible direction that improves the objective (otherwise the projection of a slight step would move along that direction).</p>
<p>Orthogonal decomposition: Any vector <span class="arithmatex">\(x\)</span> can be uniquely decomposed relative to a subspace <span class="arithmatex">\(W\)</span> as <span class="arithmatex">\(x = P_W(x) + r\)</span> with <span class="arithmatex">\(r \perp W\)</span>. Moreover, <span class="arithmatex">\(|x|^2 = |P_W(x)|^2 + |r|^2\)</span> (Pythagorean theorem). This orthogonal decomposition is a geometric way to understand degrees of freedom. In constrained optimization with constraint <span class="arithmatex">\(x\in W\)</span>, any descent direction <span class="arithmatex">\(d\)</span> can be split into a part tangent to <span class="arithmatex">\(W\)</span> (which actually moves within <span class="arithmatex">\(W\)</span>) and a part normal to <span class="arithmatex">\(W\)</span> (which violates constraints). Feasible directions are those with no normal component. At optimum, the gradient <span class="arithmatex">\(\nabla f(x^)\)</span> being orthogonal to the feasible region means <span class="arithmatex">\(\nabla f(x^)\)</span> lies entirely in the normal subspace <span class="arithmatex">\(W^\perp\)</span> — no component lies along any feasible direction. This is exactly the condition for optimality with equality constraints: <span class="arithmatex">\(\nabla f(x^)\)</span> is in the row space of <span class="arithmatex">\(A\)</span> if <span class="arithmatex">\(Ax^=b\)</span> are active constraints, which leads to <span class="arithmatex">\(\nabla f(x^*) = A^T \lambda\)</span> for some <span class="arithmatex">\(\lambda\)</span> (the Lagrange multipliers). Thus, orthogonal decomposition underpins the optimality conditions in constrained problems.</p>
<p>Projection algorithms: The simplicity or difficulty of computing <span class="arithmatex">\(P_C(x)\)</span> often determines if we can solve a problem efficiently. If <span class="arithmatex">\(C\)</span> is something like a polyhedron given by linear inequalities, <span class="arithmatex">\(P_C\)</span> might require solving a QP each time. But for many simple sets (boxes, balls, simplices, spectral norm or nuclear norm balls, etc.), we have closed forms. This gives rise to the toolbox of proximal operators in convex optimization, which generalize projections to include objective terms. Proximal gradient methods rely on computing <span class="arithmatex">\(\operatorname{prox}{\gamma g}(x) = \arg\min_y {g(y) + \frac{1}{2\gamma}|y-x|^2}\)</span>, which for indicator functions of set <span class="arithmatex">\(C\)</span> yields <span class="arithmatex">\(\operatorname{prox}{\delta_C}(x) = P_C(x)\)</span>. Thus projection is a special proximal operator (one for constraints).</p>
<p>In conclusion, projections are how we enforce constraints and decompose optimization problems. They appear in the analysis of alternating projection algorithms (for finding a point in <span class="arithmatex">\(C_1 \cap C_2\)</span> by <span class="arithmatex">\(x_{k+1}=P_{C_1}(P_{C_2}(x_k))\)</span>), in augmented Lagrangian methods (where a proximal term causes an update like a projection), and in many other contexts. Mastering the geometry of projections — that the closest point condition yields orthogonality conditions and that projections do not expand distances — is crucial for understanding how constraint-handling algorithms converge.</p></body></html></section><section class="print-page" id="appendices-140_support" heading-number="4.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-b-support-functions-and-dual-geometry-advanced">Appendix B: Support Functions and Dual Geometry (Advanced)<a class="headerlink" href="#appendices-140_support-appendix-b-support-functions-and-dual-geometry-advanced" title="Permanent link">¶</a></h1>
<p>This appendix develops a geometric viewpoint on duality using support functions, hyperplane separation, and polarity.</p>
<hr>
<h2 id="appendices-140_support-b1-support-functions">B.1 Support functions<a class="headerlink" href="#appendices-140_support-b1-support-functions" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(C \subseteq \mathbb{R}^n\)</span> be a nonempty set. The support function of <span class="arithmatex">\(C\)</span> is
<script type="math/tex; mode=display">
\sigma_C(y) = \sup_{x \in C} y^\top x.
</script>
</p>
<p>Interpretation:</p>
<ul>
<li>For a given direction <span class="arithmatex">\(y\)</span>, <span class="arithmatex">\(\sigma_C(y)\)</span> tells you how far you can go in that direction while staying in <span class="arithmatex">\(C\)</span>.</li>
<li>It is the value of the linear maximisation problem
  <script type="math/tex; mode=display">
  \max_{x \in C} y^\top x.
  </script>
</li>
</ul>
<p>Key facts:</p>
<ol>
<li><span class="arithmatex">\(\sigma_C\)</span> is always convex, even if <span class="arithmatex">\(C\)</span> is not convex.</li>
<li>If <span class="arithmatex">\(C\)</span> is convex and closed, <span class="arithmatex">\(\sigma_C\)</span> essentially characterises <span class="arithmatex">\(C\)</span>.<br>
   In particular, <span class="arithmatex">\(C\)</span> can be recovered as the intersection of halfspaces
   <script type="math/tex; mode=display">
   x^\top y \le \sigma_C(y)\quad \text{for all } y.
   </script>
</li>
</ol>
<p>So support functions encode convex sets by describing all their supporting hyperplanes.</p>
<hr>
<h2 id="appendices-140_support-b2-support-functions-and-dual-norms">B.2 Support functions and dual norms<a class="headerlink" href="#appendices-140_support-b2-support-functions-and-dual-norms" title="Permanent link">¶</a></h2>
<p>If <span class="arithmatex">\(C\)</span> is the unit ball of a norm <span class="arithmatex">\(\|\cdot\|\)</span>, i.e.
<script type="math/tex; mode=display">
C = \{ x : \|x\| \le 1 \},
</script>
then
<script type="math/tex; mode=display">
\sigma_C(y)
=
\sup_{\|x\|\le 1} y^\top x
=
\|y\|_*,
</script>
the dual norm of <span class="arithmatex">\(\|\cdot\|\)</span>.</p>
<p>Example:</p>
<ul>
<li>For <span class="arithmatex">\(\ell_2\)</span>, <span class="arithmatex">\(\|\cdot\|_2\)</span> is self-dual, so <span class="arithmatex">\(\|y\|_2^* = \|y\|_2\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_1\)</span>, the dual norm is <span class="arithmatex">\(\ell_\infty\)</span>.</li>
<li>For <span class="arithmatex">\(\ell_\infty\)</span>, the dual norm is <span class="arithmatex">\(\ell_1\)</span>.</li>
</ul>
<p>This shows that dual norms are just support functions of norm balls.</p>
<hr>
<h2 id="appendices-140_support-b3-indicator-functions-and-conjugates">B.3 Indicator functions and conjugates<a class="headerlink" href="#appendices-140_support-b3-indicator-functions-and-conjugates" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a set <span class="arithmatex">\(C\)</span>:
<script type="math/tex; mode=display">
\delta_C(x) =
\begin{cases}
0 & x \in C, \\
+\infty & x \notin C.
\end{cases}
</script>
</p>
<p>Its convex conjugate is
<script type="math/tex; mode=display">
\delta_C^*(y)
=
\sup_x (y^\top x - \delta_C(x))
=
\sup_{x \in C} y^\top x
=
\sigma_C(y).
</script>
</p>
<p>Thus,</p>
<blockquote>
<p>The support function <span class="arithmatex">\(\sigma_C\)</span> is the convex conjugate of the indicator of <span class="arithmatex">\(C\)</span>.</p>
</blockquote>
<p>This is extremely important conceptually:</p>
<ul>
<li>Conjugates turn sets into functions.</li>
<li>Duality in optimisation is often conjugacy in disguise.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b4-hyperplane-separation-revisited">B.4 Hyperplane separation revisited<a class="headerlink" href="#appendices-140_support-b4-hyperplane-separation-revisited" title="Permanent link">¶</a></h2>
<p>Recall: if <span class="arithmatex">\(C\)</span> is closed and convex, then at any boundary point <span class="arithmatex">\(x_0 \in C\)</span> there is a supporting hyperplane
<script type="math/tex; mode=display">
a^\top x \le a^\top x_0
\quad \text{for all } x \in C.
</script>
</p>
<p>This <span class="arithmatex">\(a\)</span> is exactly the kind of vector we would use in a support function evaluation. In fact, <span class="arithmatex">\(a^\top x_0 = \sigma_C(a)\)</span> if <span class="arithmatex">\(x_0\)</span> is an extreme point (or exposed point) in direction <span class="arithmatex">\(a\)</span>.</p>
<p>Geometric interpretation:</p>
<ul>
<li>Lagrange multipliers in the dual problem play the role of these <span class="arithmatex">\(a\)</span>’s.</li>
<li>They identify supporting hyperplanes that “witness” optimality.</li>
</ul>
<hr>
<h2 id="appendices-140_support-b5-duality-as-support">B.5 Duality as support<a class="headerlink" href="#appendices-140_support-b5-duality-as-support" title="Permanent link">¶</a></h2>
<p>Consider the (convex) primal problem
<script type="math/tex; mode=display">
\begin{array}{ll}
\text{minimise} & f(x) \\
\text{subject to} & x \in C,
\end{array}
</script>
where <span class="arithmatex">\(C\)</span> is a convex feasible set.</p>
<p>We can rewrite the problem as minimising
<script type="math/tex; mode=display">
f(x) + \delta_C(x).
</script>
</p>
<p>The convex conjugate of <span class="arithmatex">\(f + \delta_C\)</span> is
<script type="math/tex; mode=display">
(f + \delta_C)^*(y)
=
\inf_{u+v=y} \left( f^*(u) + \delta_C^*(v) \right)
=
\inf_{u+v=y} \left( f^*(u) + \sigma_C(v) \right).
</script>
</p>
<p>This is already starting to look like the Lagrange dual: we are constructing a lower bound on <span class="arithmatex">\(f(x)\)</span> over <span class="arithmatex">\(x \in C\)</span> using conjugates and support functions (Rockafellar, 1970).</p>
<p>This view makes precise the slogan:</p>
<blockquote>
<p>“Dual variables are hyperplanes that support the feasible set and the objective from below.”</p>
</blockquote>
<hr>
<h2 id="appendices-140_support-b6-geometry-of-kkt-and-multipliers">B.6 Geometry of KKT and multipliers<a class="headerlink" href="#appendices-140_support-b6-geometry-of-kkt-and-multipliers" title="Permanent link">¶</a></h2>
<p>At the optimal point <span class="arithmatex">\(x^*\)</span> of a convex problem, there is typically a hyperplane that supports the feasible set at <span class="arithmatex">\(x^*\)</span> and is aligned with the objective. That hyperplane is described by the Lagrange multipliers.</p>
<ul>
<li>The multipliers form a certificate that <span class="arithmatex">\(x^*\)</span> cannot be improved without violating feasibility.</li>
<li>The dual problem is the search for the “best” such certificate.</li>
</ul>
<p>This is precisely why KKT conditions are both necessary and sufficient in convex problems that satisfy Slater’s condition (Boyd and Vandenberghe, 2004).</p>
<hr>
<h2 id="appendices-140_support-b7-why-this-matters">B.7 Why this matters<a class="headerlink" href="#appendices-140_support-b7-why-this-matters" title="Permanent link">¶</a></h2>
<p>This geometric point of view is not just pretty:</p>
<ul>
<li>It explains why strong duality holds.</li>
<li>It explains what <span class="arithmatex">\(\mu_i^*\)</span> and <span class="arithmatex">\(\lambda_j^*\)</span> “mean.”</li>
<li>It clarifies why convex analysis is so tightly linked to hyperplane separation theorems.</li>
</ul>
<!-- # F.2 Support Functions and Dual Geometry

Support functions are one of the most elegant bridges between convex sets and linear optimization. For any convex set, they describe its extent in a given direction — and thus appear naturally in:

- Duality theory and convex conjugates (see Section D.1)  
- Norm analysis and dual norms (Section A.4 and A.5)  
- Subgradient calculations and optimality conditions (Section A.7 and Section D.3)  
- Projection and cutting-plane algorithms in high-dimensional optimization  

Geometrically, a support function tells you:  
> *How far can I go in direction $y$ and still remain inside the set $C$?*

---

## Definition and Geometry

Let $C \subseteq \mathbb{R}^n$ be a nonempty convex set. The support function $\sigma_C : \mathbb{R}^n \to \mathbb{R}$ is defined as:

$$
\sigma_C(y) = \sup_{x \in C} \langle y, x \rangle
$$

- $y$ is the direction vector.  
- $\langle y, x \rangle$ is the inner product (see Section A.2).  
- $\sigma_C(y)$ gives the maximum projection of $C$ along direction $y$.

It corresponds to the furthest point of $C$ in direction $y$, and hence defines a supporting hyperplane to the set.

---

## Key Properties

- Positive Homogeneity:  
  $$
  \sigma_C(\alpha y) = \alpha \sigma_C(y) \quad \text{for } \alpha \ge 0
  $$
- Convexity:  
  $$
  \sigma_C(y_1 + y_2) \le \sigma_C(y_1) + \sigma_C(y_2)
  $$
- Attainment: If $C$ is closed and bounded (compact), the supremum is attained — the max is reached at some $x^\star \in C$.  
- Set Representation:  
  Every closed convex set can be recovered from its support function:
  $$
  C = \{ x \in \mathbb{R}^n \mid \langle y, x \rangle \le \sigma_C(y) \quad \forall y \in \mathbb{R}^n \}
  $$

---

## Computation and Intuition

To compute $\sigma_C(y)$:

1. Specify the convex set $C$ (e.g., a ball, polytope, or feasible region).
2. Fix the direction $y \in \mathbb{R}^n$.
3. Maximize the dot product $\langle y, x \rangle$ over $x \in C$.

This is a linear program over $C$.

### Links to Optimization:
- In duality theory (Section D.1), linear functionals $\langle y, x \rangle$ are used to lower-bound convex functions — support functions arise naturally.
- For constraint sets defined by indicator functions (Section A.8), the support function is their convex conjugate:
  $$
  \sigma_C = \delta_C^*
  $$

---

## Examples

### Example 1: $\ell_2$ Unit Ball

Let $C = \{ x \mid \|x\|_2 \le 1 \}$

Then the support function is:

$$
\sigma_C(y) = \sup_{\|x\|_2 \le 1} \langle y, x \rangle = \|y\|_2
$$

Interpretation: the farthest point in direction $y$ lies on the boundary and aligns with $y$.

👉 This reveals that the support function of a norm ball gives the dual norm — see Section A.4.

---

### Example 2: $\ell_1$ Unit Ball

Let $C = \{ x \mid \|x\|_1 \le 1 \}$

Then:

$$
\sigma_C(y) = \|y\|_\infty
$$

Intuition: in direction $y$, the maximal point in $C$ aligns with the coordinate having largest magnitude.

This dual norm relationship is fundamental in sparsity-inducing optimization (e.g., LASSO in Section F.1).

---

### Example 3: Polytope (Convex Hull)

Let $C = \text{conv}\{v_1, \dots, v_m\}$

Then:

$$
\sigma_C(y) = \max_{i=1,\dots,m} \langle y, v_i \rangle
$$

Interpretation: the maximum projection occurs at one of the vertices of the polytope.

In LP problems (see Section C.3), this is how extreme points determine optimal solutions.

---

## Applications in Optimization

### 🔄 Duality and Convex Conjugates

The support function is the Fenchel conjugate of an indicator function:

$$
\sigma_C(y) = \delta_C^*(y)
$$

This connection underpins many dual optimization frameworks, including saddle-point methods, dual norms, and variational formulations.

---

### 📐 Dual Norms

For any norm $\|\cdot\|$, the support function of its unit ball gives the dual norm:

$$
\sigma_{B}(y) = \sup_{\|x\| \le 1} \langle y, x \rangle = \|y\|_*
$$

See Section A.5 for the definition of dual norms, and Section C.1 for how they affect step sizes and convergence geometry.

---

### 📏 Geometric Use Cases

- Compute distances to sets via duality.  
- Generate separating hyperplanes for convex sets.  
- Implement projection algorithms (e.g., mirror descent in Section K.1).  
- Construct robust constraints and worst-case bounds in uncertainty modeling (see Section E.2).

---

## Summary and Takeaways

- The support function $\sigma_C(y)$ measures how far a convex set extends in direction $y$.
- It is always convex, positively homogeneous, and subadditive.
- Support functions appear in:
  - Duality theory via convex conjugates  
  - Norm analysis via dual norms  
  - Subgradients and projections  
  - Constraint representations and recovery of convex sets
- For norm balls, $\sigma_C$ gives the dual norm.  
- For polytopes, $\sigma_C$ is the max over vertices.  
- For machine learning, support functions help model constraints, regularization penalties, and geometric algorithms.

Mental model:  
Think of a support function as a “radar scan” — it tells you the furthest point of a convex set in any given direction.
 --></body></html></section><section class="print-page" id="appendices-160_conjugates" heading-number="4.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-d-convex-conjugates-and-fenchel-duality">Appendix D: Convex Conjugates and Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-appendix-d-convex-conjugates-and-fenchel-duality" title="Permanent link">¶</a></h1>
<p>Convex conjugates and Fenchel duality form the functional heart of convex analysis.<br>
They provide a powerful unifying view of optimization by connecting geometry, algebra, and duality.  </p>
<ul>
<li>Convex conjugates convert a function into its “slope-space” representation — capturing its tightest linear overestimates.  </li>
<li>Fenchel duality uses these conjugates to derive dual optimization problems that often reveal structure, efficiency, or interpretability hidden in the primal form.  </li>
</ul>
<p>Together, they form the bridge between the geometry of convex sets (Appendix C) and the duality theory of optimization (Chapter 8).</p>
<h2 id="appendices-160_conjugates-d1-intuitive-picture">D.1 Intuitive Picture<a class="headerlink" href="#appendices-160_conjugates-d1-intuitive-picture" title="Permanent link">¶</a></h2>
<p>Imagine a convex function <span class="arithmatex">\(f(x)\)</span> drawn as a bowl in space.<br>
Each point <span class="arithmatex">\(y\)</span> defines a line (or hyperplane) of slope <span class="arithmatex">\(y\)</span>:
<script type="math/tex; mode=display">
x \mapsto \langle y, x \rangle - b.
</script>
The convex conjugate <span class="arithmatex">\(f^*(y)\)</span> is the smallest height <span class="arithmatex">\(b\)</span> such that this line always stays above <span class="arithmatex">\(f(x)\)</span>.<br>
In other words:</p>
<blockquote>
<p><span class="arithmatex">\(f^*(y)\)</span> measures the tightest linear overestimate of <span class="arithmatex">\(f\)</span> in direction <span class="arithmatex">\(y\)</span>.</p>
</blockquote>
<p>So <span class="arithmatex">\(f^*\)</span> encodes how “steep” <span class="arithmatex">\(f\)</span> can be in every direction — it transforms the geometry of <span class="arithmatex">\(f\)</span> into a new convex function on slope-space.</p>
<h2 id="appendices-160_conjugates-d2-definition-and-key-properties">D.2 Definition and Key Properties<a class="headerlink" href="#appendices-160_conjugates-d2-definition-and-key-properties" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f : \mathbb{R}^n \to \mathbb{R}\cup\{+\infty\}\)</span> be a proper convex function.<br>
Its convex (Fenchel) conjugate is
<script type="math/tex; mode=display">
f^*(y) = \sup_{x \in \mathbb{R}^n} \big( \langle y, x \rangle - f(x) \big).
</script>
</p>
<p>Interpretation
- <span class="arithmatex">\(y\)</span>: a slope or linear functional.
- The supremum seeks the largest gap between the linear function <span class="arithmatex">\(\langle y,x\rangle\)</span> and the graph of <span class="arithmatex">\(f\)</span>.
- <span class="arithmatex">\(f^*(y)\)</span> is always convex, even if <span class="arithmatex">\(f\)</span> isn’t strictly convex.</p>
<h3 id="appendices-160_conjugates-fundamental-identities">Fundamental Identities<a class="headerlink" href="#appendices-160_conjugates-fundamental-identities" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Fenchel–Young inequality
   <script type="math/tex; mode=display">
   \langle y,x\rangle \le f(x) + f^*(y),
   </script>
   with equality iff <span class="arithmatex">\(y \in \partial f(x)\)</span>.</p>
</li>
<li>
<p>Biconjugation
   <script type="math/tex; mode=display">
   f^{} = f \quad \text{if <span class="arithmatex">\(f\)</span> is proper, convex, and lower semicontinuous.}
   </script>
   This tells us the conjugate transform loses no information for convex functions.</p>
</li>
<li>
<p>Order reversal
   <span class="arithmatex">\(f \le g \;\Rightarrow\; f^* \ge g^*\)</span>.</p>
</li>
<li>
<p>Scaling and shift</p>
</li>
<li><span class="arithmatex">\((f + a)^*(y) = f^*(y) - a\)</span>,</li>
<li><span class="arithmatex">\((\alpha f)^*(y) = \alpha f^*(y/\alpha)\)</span> for <span class="arithmatex">\(\alpha&gt;0.\)</span></li>
</ol>
<hr>
<h2 id="appendices-160_conjugates-d3-canonical-examples">D.3 Canonical Examples<a class="headerlink" href="#appendices-160_conjugates-d3-canonical-examples" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Function <span class="arithmatex">\(f(x)\)</span></th>
<th>Conjugate <span class="arithmatex">\(f^*(y)\)</span></th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\( \tfrac{1}{2}\|x\|_2^2 \)</span></td>
<td><span class="arithmatex">\( \tfrac{1}{2}\|y\|_2^2 \)</span></td>
<td>Self-conjugate quadratic</td>
</tr>
<tr>
<td><span class="arithmatex">\( \|x\|_1 \)</span></td>
<td><span class="arithmatex">\( \delta_{\{\|y\|_\infty \le 1\}}(y) \)</span></td>
<td>Dual norm indicator</td>
</tr>
<tr>
<td><span class="arithmatex">\( \delta_C(x) \)</span></td>
<td><span class="arithmatex">\( \sigma_C(y)=\sup_{x\in C}\langle y,x\rangle \)</span></td>
<td>Support function of set <span class="arithmatex">\(C\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\( e^x \)</span></td>
<td><span class="arithmatex">\( y\log y - y,\, y&gt;0 \)</span></td>
<td>Appears in entropy and KL-divergence</td>
</tr>
</tbody>
</table>
<p>These examples illustrate how conjugation connects:
- Norms ↔ dual norms,<br>
- Sets ↔ support functions,<br>
- Exponentials ↔ entropy,<br>
- Quadratics ↔ themselves.</p>
<h2 id="appendices-160_conjugates-d4-geometric-interpretation">D.4 Geometric Interpretation<a class="headerlink" href="#appendices-160_conjugates-d4-geometric-interpretation" title="Permanent link">¶</a></h2>
<ul>
<li>Each point on <span class="arithmatex">\(f\)</span> has a tangent hyperplane whose slope is a subgradient.  </li>
<li>The collection of all such hyperplanes forms the epigraph of <span class="arithmatex">\(f^*\)</span>.  </li>
<li>The transformation <span class="arithmatex">\(f \mapsto f^*\)</span> swaps the roles of “position” and “slope”:<br>
  convex geometry ↔ supporting hyperplanes.</li>
</ul>
<p>Visually:<br>
- <span class="arithmatex">\(f\)</span> describes a bowl in <span class="arithmatex">\((x,t)\)</span>-space.<br>
- <span class="arithmatex">\(f^*\)</span> describes the envelope of tangent planes to that bowl.</p>
<h2 id="appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality">D.5 From Conjugates to Duality — Fenchel Duality<a class="headerlink" href="#appendices-160_conjugates-d5-from-conjugates-to-duality-fenchel-duality" title="Permanent link">¶</a></h2>
<p>Many convex optimization problems can be written as
<script type="math/tex; mode=display">
\min_x \; f(x) + g(Ax),
</script>
where <span class="arithmatex">\(f,g\)</span> are convex and <span class="arithmatex">\(A\)</span> is linear.<br>
Fenchel duality uses conjugates to build a dual problem in terms of <span class="arithmatex">\(f^*\)</span> and <span class="arithmatex">\(g^*\)</span>.</p>
<h3 id="appendices-160_conjugates-the-fenchel-dual-problem">The Fenchel Dual Problem<a class="headerlink" href="#appendices-160_conjugates-the-fenchel-dual-problem" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\max_y \; -f^*(A^\top y) - g^*(-y).
\]</div>
<p>Interpretation
- <span class="arithmatex">\(y\)</span> is the dual variable (similar to Lagrange multipliers).<br>
- The dual objective collects the best linear lower bounds on the primal cost.</p>
<h2 id="appendices-160_conjugates-d6-weak-and-strong-duality">D.6 Weak and Strong Duality<a class="headerlink" href="#appendices-160_conjugates-d6-weak-and-strong-duality" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Weak duality: For any <span class="arithmatex">\(x,y\)</span>,
  <script type="math/tex; mode=display">
  f(x)+g(Ax) \ge -f^*(A^\top y) - g^*(-y).
  </script>
  So the dual value always underestimates the primal value.</p>
</li>
<li>
<p>Strong duality:<br>
  If <span class="arithmatex">\(f,g\)</span> are closed convex and a mild constraint qualification holds (e.g. Slater’s condition — existence of strictly feasible <span class="arithmatex">\(x\)</span>), then
  <script type="math/tex; mode=display">
  \min_x [f(x)+g(Ax)] = \max_y [-f^*(A^\top y) - g^*(-y)].
  </script>
</p>
</li>
</ul>
<p>At the optimum:
<script type="math/tex; mode=display">
A^\top y^* \in \partial f(x^*), 
\qquad
-y^* \in \partial g(Ax^*).
</script>
These are the Fenchel–KKT conditions, directly linking primal and dual subgradients.</p>
<h2 id="appendices-160_conjugates-d7-illustrative-examples">D.7 Illustrative Examples<a class="headerlink" href="#appendices-160_conjugates-d7-illustrative-examples" title="Permanent link">¶</a></h2>
<h3 id="appendices-160_conjugates-a-linear-programming">(a) Linear Programming<a class="headerlink" href="#appendices-160_conjugates-a-linear-programming" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_{x \ge 0} c^\top x \quad \text{s.t. } Ax = b.
</script>
</p>
<p>Take<br>
<span class="arithmatex">\(f(x) = c^\top x + \delta_{\{x\ge0\}}(x)\)</span>,<br>
<span class="arithmatex">\(g(z)=\delta_{\{z=b\}}(z)\)</span>.</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y) = \delta_{\{y \le c\}}(y),
\qquad
g^*(y) = b^\top y.
</script>
</p>
<p>Dual:
<script type="math/tex; mode=display">
\max_y \; b^\top y \quad \text{s.t. } A^\top y \le c,
</script>
which is the standard LP dual.</p>
<h3 id="appendices-160_conjugates-b-quadratic-set-constraint">(b) Quadratic + Set Constraint<a class="headerlink" href="#appendices-160_conjugates-b-quadratic-set-constraint" title="Permanent link">¶</a></h3>
<p>Primal:
<script type="math/tex; mode=display">
\min_x \tfrac{1}{2}\|x\|_2^2 + \delta_C(x).
</script>
</p>
<p>Then
<script type="math/tex; mode=display">
f^*(y)=\tfrac{1}{2}\|y\|_2^2, \qquad g^*(y)=\sigma_C(y),
</script>
so the dual is
<script type="math/tex; mode=display">
\max_y -\tfrac{1}{2}\|y\|_2^2 - \sigma_C(y).
</script>
Optimality gives <span class="arithmatex">\(x^*=y^*\)</span>, the projection condition in Euclidean geometry.</p>
<h2 id="appendices-160_conjugates-d8-practical-significance">D.8 Practical Significance<a class="headerlink" href="#appendices-160_conjugates-d8-practical-significance" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Area</th>
<th>How Fenchel Duality Appears</th>
</tr>
</thead>
<tbody>
<tr>
<td>Optimization theory</td>
<td>Derives general dual problems beyond inequality constraints.</td>
</tr>
<tr>
<td>Algorithm design</td>
<td>Basis for primal–dual and splitting methods (ADMM, Chambolle–Pock, Mirror Descent).</td>
</tr>
<tr>
<td>Geometry</td>
<td>Dual problem finds the “best supporting hyperplane” to the primal epigraph.</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Loss–regularizer pairs (hinge ↔ clipped loss, logistic ↔ log-sum-exp) often form conjugate pairs.</td>
</tr>
<tr>
<td>Proximal operators</td>
<td>Linked via Moreau identity:  <span class="arithmatex">\(\mathrm{prox}_{f^*}(y) = y - \mathrm{prox}_f(y)\)</span>.</td>
</tr>
</tbody>
</table>
<h2 id="appendices-160_conjugates-d9-conceptual-unification">D.9 Conceptual Unification<a class="headerlink" href="#appendices-160_conjugates-d9-conceptual-unification" title="Permanent link">¶</a></h2>
<p>Convex conjugates and Fenchel duality tie together nearly every idea in this book:</p>
<ul>
<li>From geometry: support functions, projections, subgradients (Appendices B–C).  </li>
<li>From analysis: inequalities like Fenchel’s and Jensen’s (Appendix A).  </li>
<li>From optimization: Lagrange duality, KKT, and strong duality (Chapters 7–8).  </li>
<li>From computation: proximal, ADMM, and mirror-descent algorithms (Chapters 9–10).</li>
</ul>
<p>Together, they show that convex optimization is self-dual: every convex structure has an equally convex mirror image.</p>
<h2 id="appendices-160_conjugates-d10-summary-and-takeaways">D.10 Summary and Takeaways<a class="headerlink" href="#appendices-160_conjugates-d10-summary-and-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>The convex conjugate <span class="arithmatex">\(f^*\)</span> expresses <span class="arithmatex">\(f\)</span> through its linear support planes.  </li>
<li>The Fenchel–Young inequality connects primal variables and dual slopes.  </li>
<li>Fenchel duality constructs a systematic dual problem using these conjugates.  </li>
<li>Under mild conditions, strong duality holds, and subgradients link primal and dual optima.  </li>
<li>These ideas underpin most modern optimization algorithms and geometric interpretations of convexity.</li>
</ul>
<hr>
<p>Further Reading</p>
<ul>
<li>Rockafellar, R. T. (1970). <em>Convex Analysis</em>. Princeton UP.  </li>
<li>Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex Optimization</em>, Chs. 3 &amp; 5.  </li>
<li>Bauschke, H. H., &amp; Combettes, P. L. (2017). <em>Convex Analysis and Monotone Operator Theory</em>.  </li>
<li>Hiriart-Urruty, J.-B., &amp; Lemaréchal, C. (2001). <em>Fundamentals of Convex Analysis</em>.  </li>
</ul></body></html></section><section class="print-page" id="appendices-170_probability" heading-number="4.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-e-convexity-in-probability-and-statistics">Appendix E : Convexity in Probability and Statistics<a class="headerlink" href="#appendices-170_probability-appendix-e-convexity-in-probability-and-statistics" title="Permanent link">¶</a></h1>
<p>Convex analysis is not just geometry and optimization — it is deeply woven into probability, statistics, and information theory.<br>
Many statistical models, estimators, and loss functions are convex because convexity guarantees stability, uniqueness, and tractability of inference.</p>
<p>This appendix surveys how convexity arises naturally in probabilistic and statistical contexts.</p>
<h2 id="appendices-170_probability-e1-convexity-of-expectations">E.1 Convexity of Expectations<a class="headerlink" href="#appendices-170_probability-e1-convexity-of-expectations" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(f:\mathbb{R}^n\!\to\!\mathbb{R}\)</span> be convex and <span class="arithmatex">\(X\)</span> a random vector.<br>
Then by Jensen’s inequality (Appendix A):</p>
<div class="arithmatex">\[
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)].
\]</div>
<h3 id="appendices-170_probability-consequences">Consequences<a class="headerlink" href="#appendices-170_probability-consequences" title="Permanent link">¶</a></h3>
<ul>
<li>Expectations preserve convexity:<br>
  if each <span class="arithmatex">\(f(\cdot,\xi)\)</span> is convex, then <span class="arithmatex">\(F(x)=\mathbb{E}_\xi[f(x,\xi)]\)</span> is convex.</li>
<li>Stochastic objectives in ML — e.g. expected loss <span class="arithmatex">\(\mathbb{E}_{(a,b)}[\ell(a^\top x,b)]\)</span> — are convex when the sample-wise loss is convex.</li>
</ul>
<p>Hence almost all <em>empirical risk minimization</em> problems are discrete approximations of convex expectations.</p>
<h2 id="appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions">E.2 Convexity of Log-Partition and Moment-Generating Functions<a class="headerlink" href="#appendices-170_probability-e2-convexity-of-log-partition-and-moment-generating-functions" title="Permanent link">¶</a></h2>
<p>For a random variable <span class="arithmatex">\(X\)</span>, the moment-generating function (MGF) and cumulant-generating function (CGF) are</p>
<div class="arithmatex">\[
M_X(t)=\mathbb{E}[e^{tX}], \qquad
K_X(t)=\log M_X(t).
\]</div>
<p>Fact: <span class="arithmatex">\(K_X(t)\)</span> is always convex in <span class="arithmatex">\(t\)</span>.</p>
<p>Reason: <span class="arithmatex">\(K_X''(t)=\mathrm{Var}_t(X)\ge0\)</span>;<br>
variance is nonnegative.  </p>
<h3 id="appendices-170_probability-implications">Implications<a class="headerlink" href="#appendices-170_probability-implications" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(K_X(t)\)</span> acts as a convex “potential” controlling exponential families.</li>
<li>The log-partition function in statistics,
  <script type="math/tex; mode=display">
  A(\theta)=\log \int e^{\langle \theta,T(x)\rangle}\,h(x)\,dx,
  </script>
  is convex in <span class="arithmatex">\(\theta\)</span> (strictly convex for full exponential families).</li>
<li>Its gradient gives the mean parameter: <span class="arithmatex">\(\nabla A(\theta)=\mathbb{E}_\theta[T(X)]\)</span>.</li>
</ul>
<p>Thus convexity of <span class="arithmatex">\(A\)</span> guarantees a one-to-one mapping between natural and mean parameters — a foundation of exponential-family inference.</p>
<h2 id="appendices-170_probability-e3-exponential-families-and-dual-convexity">E.3 Exponential Families and Dual Convexity<a class="headerlink" href="#appendices-170_probability-e3-exponential-families-and-dual-convexity" title="Permanent link">¶</a></h2>
<p>An exponential-family density has the form
<script type="math/tex; mode=display">
p_\theta(x)=\exp\big(\langle\theta,T(x)\rangle-A(\theta)\big)h(x).
</script>
</p>
<p>Properties:</p>
<ol>
<li><span class="arithmatex">\(A(\theta)\)</span> is convex, smooth, and serves as a potential function.</li>
<li>Its convex conjugate <span class="arithmatex">\(A^*(\mu)\)</span> defines the entropy of the family:
   <script type="math/tex; mode=display">
   A^*(\mu)=\sup_\theta(\langle\mu,\theta\rangle-A(\theta)) = -H(p_\mu),
   </script>
   where <span class="arithmatex">\(H\)</span> is the Shannon entropy of the distribution with mean <span class="arithmatex">\(\mu\)</span>.</li>
</ol>
<p>Hence maximum-likelihood estimation in exponential families is a convex optimization problem, and maximum-entropy estimation is its Fenchel dual.</p>
<h2 id="appendices-170_probability-e4-convex-divergences-and-information-measures">E.4 Convex Divergences and Information Measures<a class="headerlink" href="#appendices-170_probability-e4-convex-divergences-and-information-measures" title="Permanent link">¶</a></h2>
<h3 id="appendices-170_probability-a-kullbackleibler-kl-divergence">(a) Kullback–Leibler (KL) Divergence<a class="headerlink" href="#appendices-170_probability-a-kullbackleibler-kl-divergence" title="Permanent link">¶</a></h3>
<p>For densities <span class="arithmatex">\(p,q\)</span>,
<script type="math/tex; mode=display">
D_{\mathrm{KL}}(p\|q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx.
</script>
</p>
<ul>
<li><span class="arithmatex">\(D_{\mathrm{KL}}\)</span> is jointly convex in <span class="arithmatex">\((p,q)\)</span>.  </li>
<li>Proof: the function <span class="arithmatex">\((u,v)\mapsto u\log(u/v)\)</span> is convex on <span class="arithmatex">\(\mathbb{R}_+^2\)</span>.  </li>
<li>Consequently, mixtures of distributions cannot increase KL divergence — a key fact in variational inference and EM.</li>
</ul>
<h3 id="appendices-170_probability-b-bregman-divergences">(b) Bregman Divergences<a class="headerlink" href="#appendices-170_probability-b-bregman-divergences" title="Permanent link">¶</a></h3>
<p>Given a differentiable convex <span class="arithmatex">\(\phi\)</span>, define
<script type="math/tex; mode=display">
D_\phi(x\|y)=\phi(x)-\phi(y)-\langle\nabla\phi(y),x-y\rangle.
</script>
KL divergence is a Bregman divergence for <span class="arithmatex">\(\phi(p)=\sum_i p_i\log p_i\)</span>.<br>
Thus information-theoretic distances are <em>geometric shadows</em> of convex functions.</p>
<h3 id="appendices-170_probability-c-f-divergences">(c) f-Divergences<a class="headerlink" href="#appendices-170_probability-c-f-divergences" title="Permanent link">¶</a></h3>
<p>A general convex generator <span class="arithmatex">\(f\)</span> with <span class="arithmatex">\(f(1)=0\)</span> yields
<script type="math/tex; mode=display">
D_f(p\|q)=\int q(x)\,f\!\left(\frac{p(x)}{q(x)}\right)dx.
</script>
Convexity of <span class="arithmatex">\(f\)</span> ⇒ convexity of <span class="arithmatex">\(D_f\)</span>.<br>
Common choices recover KL, χ², Hellinger, and Jensen–Shannon divergences.</p>
<h2 id="appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning">E.5 Convex Loss Functions in Statistics and Machine Learning<a class="headerlink" href="#appendices-170_probability-e5-convex-loss-functions-in-statistics-and-machine-learning" title="Permanent link">¶</a></h2>
<p>Convexity ensures estimators are globally optimal and algorithms converge.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Loss / Negative Log-Likelihood</th>
<th>Convexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian noise</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2\)</span></td>
<td>quadratic, strongly convex</td>
</tr>
<tr>
<td>Laplace noise</td>
<td><span class="arithmatex">\(\|Ax-b\|_1\)</span></td>
<td>convex, nonsmooth</td>
</tr>
<tr>
<td>Logistic regression</td>
<td><span class="arithmatex">\(\log(1+e^{-y a^\top x})\)</span></td>
<td>convex, smooth</td>
</tr>
<tr>
<td>Poisson regression</td>
<td><span class="arithmatex">\(e^{a^\top x}-y a^\top x\)</span></td>
<td>convex, exponential</td>
</tr>
<tr>
<td>Huber loss</td>
<td>piecewise quadratic/linear</td>
<td>convex, robust</td>
</tr>
</tbody>
</table>
<p>Convexity of the negative log-likelihood follows from convexity of the log-partition function <span class="arithmatex">\(A(\theta)\)</span> in exponential families.</p>
<h2 id="appendices-170_probability-e6-convexity-and-bayesian-inference">E.6 Convexity and Bayesian Inference<a class="headerlink" href="#appendices-170_probability-e6-convexity-and-bayesian-inference" title="Permanent link">¶</a></h2>
<p>In Bayesian inference, convexity appears in:</p>
<ul>
<li>
<p>Log-concave posteriors:<br>
  If the likelihood and prior are log-concave, the posterior <span class="arithmatex">\(p(x|y)\propto \exp(-f(x))\)</span> is also log-concave ⇒<br>
<span class="arithmatex">\(\log p(x|y)\)</span> concave, <span class="arithmatex">\(f(x)\)</span> convex.</p>
</li>
<li>
<p>MAP estimation:<br>
  Maximizing <span class="arithmatex">\(\log p(x|y)\)</span> ≡ minimizing a convex function when <span class="arithmatex">\(p(x|y)\)</span> is log-concave ⇒ global optimum guaranteed.</p>
</li>
<li>
<p>Variational inference:<br>
  The ELBO is a concave function of the variational parameters because it is a linear minus KL divergence (convex).<br>
  Optimizing it is equivalent to minimizing a convex divergence.</p>
</li>
</ul>
<p>Thus convexity guarantees stable Bayesian updates and efficient approximate inference.</p>
<h2 id="appendices-170_probability-e7-statistical-risk-and-convex-surrogates">E.7 Statistical Risk and Convex Surrogates<a class="headerlink" href="#appendices-170_probability-e7-statistical-risk-and-convex-surrogates" title="Permanent link">¶</a></h2>
<p>Convex surrogate losses replace nonconvex 0–1 loss with convex approximations:</p>
<ul>
<li>Hinge loss (<span class="arithmatex">\(\max(0,1-y a^\top x)\)</span>) → support-vector machines.  </li>
<li>Logistic loss → probabilistic classification (cross-entropy).  </li>
<li>Exponential loss → AdaBoost.</li>
</ul>
<p>These convex surrogates retain calibration (minimizing expected convex loss yields correct decision boundaries) while enabling tractable optimization.</p></body></html></section><section class="print-page" id="appendices-180_subgradient_methods" heading-number="4.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-f-subgradient-method-derivation-geometry-and-convergence">Appendix F: Subgradient Method: Derivation, Geometry, and Convergence<a class="headerlink" href="#appendices-180_subgradient_methods-appendix-f-subgradient-method-derivation-geometry-and-convergence" title="Permanent link">¶</a></h1>
<p>This appendix presents the subgradient method—the fundamental algorithm for minimizing nonsmooth convex functions.<br>
It generalizes gradient descent to functions such as the <span class="arithmatex">\(\ell_1\)</span> norm, hinge loss, and ReLU penalties that appear frequently in machine learning and signal processing.</p>
<h2 id="appendices-180_subgradient_methods-f1-problem-setup">F.1 Problem Setup<a class="headerlink" href="#appendices-180_subgradient_methods-f1-problem-setup" title="Permanent link">¶</a></h2>
<p>We consider</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x),
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is convex but possibly nondifferentiable and <span class="arithmatex">\(\mathcal{X}\)</span> is a convex feasible set.</p>
<h2 id="appendices-180_subgradient_methods-f2-subgradients-and-geometry">F.2 Subgradients and Geometry<a class="headerlink" href="#appendices-180_subgradient_methods-f2-subgradients-and-geometry" title="Permanent link">¶</a></h2>
<p>A subgradient <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span> satisfies</p>
<div class="arithmatex">\[
f(y) \ge f(x_t) + \langle g_t,\, y - x_t \rangle, \quad \forall y \in \mathcal{X}.
\]</div>
<ul>
<li>If <span class="arithmatex">\(f\)</span> is differentiable, <span class="arithmatex">\(\partial f(x_t) = \{\nabla f(x_t)\}\)</span>.  </li>
<li>At a nonsmooth point (e.g. <span class="arithmatex">\(|x|\)</span> at <span class="arithmatex">\(x=0\)</span>), <span class="arithmatex">\(\partial f(x_t)\)</span> is a set of supporting slopes.  </li>
<li>Each subgradient defines a supporting hyperplane below the graph of <span class="arithmatex">\(f\)</span>.</li>
</ul>
<p>Hence a subgradient gives a descent direction even when <span class="arithmatex">\(f\)</span> lacks a unique gradient.</p>
<h2 id="appendices-180_subgradient_methods-f3-update-rule-and-projection-view">F.3 Update Rule and Projection View<a class="headerlink" href="#appendices-180_subgradient_methods-f3-update-rule-and-projection-view" title="Permanent link">¶</a></h2>
<p>The projected subgradient step is</p>
<div class="arithmatex">\[
x_{t+1} = \Pi_{\mathcal{X}}\!\big(x_t - \eta_t g_t\big),
\]</div>
<p>where
- <span class="arithmatex">\(g_t \in \partial f(x_t)\)</span>,<br>
- <span class="arithmatex">\(\eta_t&gt;0\)</span> is the step size,<br>
- <span class="arithmatex">\(\Pi_{\mathcal{X}}\)</span> projects onto <span class="arithmatex">\(\mathcal{X}\)</span>.</p>
<p>If <span class="arithmatex">\(\mathcal{X} = \mathbb{R}^n\)</span>, projection disappears:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta_t g_t.
</script>
</p>
<p>Geometric view: move in a subgradient direction, then project back to feasibility.<br>
The method “slides” along the edges of <span class="arithmatex">\(f\)</span>’s epigraph.</p>
<h2 id="appendices-180_subgradient_methods-f4-distance-analysis">F.4 Distance Analysis<a class="headerlink" href="#appendices-180_subgradient_methods-f4-distance-analysis" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x^\star\)</span> be an optimal solution. Expanding the squared distance:</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
= \|x_t - x^\star\|^2
- 2\eta_t\langle g_t, x_t - x^\star\rangle
+ \eta_t^2 \|g_t\|^2.
\]</div>
<p>By convexity,
<script type="math/tex; mode=display">
f(x_t) - f(x^\star) \le \langle g_t, x_t - x^\star\rangle.
</script>
</p>
<p>Substitute to get</p>
<div class="arithmatex">\[
\|x_{t+1}-x^\star\|^2
\le
\|x_t - x^\star\|^2
- 2\eta_t\big(f(x_t)-f(x^\star)\big)
+ \eta_t^2 \|g_t\|^2.
\]</div>
<h2 id="appendices-180_subgradient_methods-f5-bounding-suboptimality">F.5 Bounding Suboptimality<a class="headerlink" href="#appendices-180_subgradient_methods-f5-bounding-suboptimality" title="Permanent link">¶</a></h2>
<p>Rearranging:</p>
<div class="arithmatex">\[
f(x_t)-f(x^\star)
\le
\frac{\|x_t-x^\star\|^2 - \|x_{t+1}-x^\star\|^2}{2\eta_t}
+ \frac{\eta_t}{2}\|g_t\|^2.
\]</div>
<p>This shows a trade-off:</p>
<ul>
<li>Large <span class="arithmatex">\(\eta_t\)</span> → faster steps but higher error term.  </li>
<li>Small <span class="arithmatex">\(\eta_t\)</span> → more precise but slower progress.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f6-convergence-rate">F.6 Convergence Rate<a class="headerlink" href="#appendices-180_subgradient_methods-f6-convergence-rate" title="Permanent link">¶</a></h2>
<p>Assume <span class="arithmatex">\(\|g_t\| \le G\)</span>. Summing over <span class="arithmatex">\(t=0,\dots,T-1\)</span>:</p>
<div class="arithmatex">\[
\sum_{t=0}^{T-1}\!\big(f(x_t)-f(x^\star)\big)
\le
\frac{\|x_0-x^\star\|^2}{2\eta}
+ \frac{\eta G^2 T}{2}.
\]</div>
<p>Define <span class="arithmatex">\(\bar{x}_T = \tfrac{1}{T}\sum_{t=0}^{T-1} x_t\)</span>.<br>
By convexity of <span class="arithmatex">\(f\)</span>,</p>
<div class="arithmatex">\[
f(\bar{x}_T)-f(x^\star)
\le
\frac{\|x_0-x^\star\|^2}{2\eta T}
+ \frac{\eta G^2}{2}.
\]</div>
<p>Choosing <span class="arithmatex">\(\eta_t = \tfrac{R}{G\sqrt{T}}\)</span> with <span class="arithmatex">\(R=\|x_0-x^\star\|\)</span> yields</p>
<p>
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^\star)
\le
\frac{RG}{\sqrt{T}},
</script>
i.e. a sublinear rate <span class="arithmatex">\(O(1/\sqrt{T})\)</span>.</p>
<h2 id="appendices-180_subgradient_methods-f7-interpretation-and-practice">F.7 Interpretation and Practice<a class="headerlink" href="#appendices-180_subgradient_methods-f7-interpretation-and-practice" title="Permanent link">¶</a></h2>
<ul>
<li>Works for any convex function, smooth or not.  </li>
<li>Converges slower than smooth-gradient methods (<span class="arithmatex">\(O(1/T)\)</span> or linear), but applies more generally.  </li>
<li>Step size schedule is crucial:<br>
<span class="arithmatex">\(\eta_t \!\downarrow 0\)</span> for convergence, or fixed <span class="arithmatex">\(\eta\)</span> for steady error.  </li>
<li>Averaging <span class="arithmatex">\(\bar{x}_T\)</span> improves stability.</li>
</ul>
<h3 id="appendices-180_subgradient_methods-typical-ml-uses">Typical ML Uses<a class="headerlink" href="#appendices-180_subgradient_methods-typical-ml-uses" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Objective</th>
<th>Nonsmooth Term</th>
</tr>
</thead>
<tbody>
<tr>
<td>LASSO</td>
<td><span class="arithmatex">\(\tfrac12\|Ax-b\|_2^2 + \lambda\|x\|_1\)</span></td>
<td><span class="arithmatex">\(\ell_1\)</span> penalty</td>
</tr>
<tr>
<td>SVM</td>
<td><span class="arithmatex">\(\tfrac12\|w\|^2 + C\sum_i \max(0,1-y_i w^\top x_i)\)</span></td>
<td>hinge loss</td>
</tr>
<tr>
<td>Robust regression</td>
<td>$\sum_i</td>
<td>a_i^\top x - b_i</td>
</tr>
<tr>
<td>Neural nets</td>
<td><span class="arithmatex">\(\|w\|_1\)</span> or ReLU activations</td>
<td>piecewise linear</td>
</tr>
</tbody>
</table>
<h2 id="appendices-180_subgradient_methods-f8-beyond-basic-subgradients">F.8 Beyond Basic Subgradients<a class="headerlink" href="#appendices-180_subgradient_methods-f8-beyond-basic-subgradients" title="Permanent link">¶</a></h2>
<p>Many advanced methods refine or accelerate the basic idea:</p>
<ul>
<li>Stochastic subgradients: sample-based updates for large-scale ML.  </li>
<li>Mirror descent: adapt geometry via Bregman divergences.  </li>
<li>Proximal methods: replace step with proximal operator (see Appendix B).  </li>
<li>Dual averaging &amp; AdaGrad: adapt step sizes to coordinate scaling.</li>
</ul>
<h2 id="appendices-180_subgradient_methods-f9-summary">F.9 Summary<a class="headerlink" href="#appendices-180_subgradient_methods-f9-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Subgradients generalize gradients to nondifferentiable convex functions.  </li>
<li>The projected subgradient method provides a universal, robust minimization algorithm.  </li>
<li>Achieves <span class="arithmatex">\(O(1/\sqrt{T})\)</span> convergence under bounded subgradients.  </li>
<li>Foundation for stochastic, proximal, and mirror-descent algorithms explored in Chapters 9–10.</li>
</ul></body></html></section><section class="print-page" id="appendices-190_proximal" heading-number="4.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization">Appendix G | Projections and Proximal Operators in Constrained Convex Optimization<a class="headerlink" href="#appendices-190_proximal-appendix-g-projections-and-proximal-operators-in-constrained-convex-optimization" title="Permanent link">¶</a></h1>
<p>Many convex optimization problems involve constraints or nonsmooth penalties.<br>
This appendix unifies both under the framework of projections and proximal operators, which extend gradient-based methods to constrained or regularized settings.</p>
<h2 id="appendices-190_proximal-g1-problem-setup">G.1 Problem Setup<a class="headerlink" href="#appendices-190_proximal-g1-problem-setup" title="Permanent link">¶</a></h2>
<p>We wish to minimize a convex, differentiable function <span class="arithmatex">\( f(x) \)</span> subject to a convex feasible set <span class="arithmatex">\( \mathcal{X} \subseteq \mathbb{R}^n \)</span>:</p>
<div class="arithmatex">\[
\min_{x \in \mathcal{X}} f(x).
\]</div>
<p>A plain gradient step,</p>
<div class="arithmatex">\[
x_{t+1} = x_t - \eta \nabla f(x_t),
\]</div>
<p>may leave <span class="arithmatex">\( x_{t+1} \notin \mathcal{X} \)</span>.<br>
We fix this by projecting the iterate back into the feasible region.</p>
<h2 id="appendices-190_proximal-g2-projection-operator">G.2 Projection Operator<a class="headerlink" href="#appendices-190_proximal-g2-projection-operator" title="Permanent link">¶</a></h2>
<p>The projection of a point <span class="arithmatex">\(y\)</span> onto a convex set <span class="arithmatex">\(\mathcal{X}\)</span> is</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2.
\]</div>
<p>Hence, the projected gradient descent update is</p>
<div class="arithmatex">\[
x_{t+1} = \text{Proj}_{\mathcal{X}}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<h3 id="appendices-190_proximal-geometric-insight">Geometric Insight<a class="headerlink" href="#appendices-190_proximal-geometric-insight" title="Permanent link">¶</a></h3>
<ul>
<li>Take a descent step possibly outside the feasible set.  </li>
<li>Project back to the closest feasible point.  </li>
<li>The update direction remains aligned with the negative gradient while maintaining feasibility.</li>
</ul>
<p>Example — Euclidean ball:<br>
If <span class="arithmatex">\( \mathcal{X} = \{x : \|x\|_2 \le 1\} \)</span>, then</p>
<div class="arithmatex">\[
\text{Proj}_{\mathcal{X}}(y) = \frac{y}{\max(1, \|y\|_2)}.
\]</div>
<ul>
<li>Inside the ball → unchanged.  </li>
<li>Outside → scaled back to the boundary.</li>
</ul>
<hr>
<h2 id="appendices-190_proximal-g3-from-projections-to-proximal-operators">G.3 From Projections to Proximal Operators<a class="headerlink" href="#appendices-190_proximal-g3-from-projections-to-proximal-operators" title="Permanent link">¶</a></h2>
<p>Projections handle explicit constraints, but many problems use implicit penalties — e.g. sparsity (<span class="arithmatex">\(\|x\|_1\)</span>), total variation, or nonnegativity penalties.</p>
<p>The proximal operator generalizes projection to handle such nonsmooth regularization directly.</p>
<h3 id="appendices-190_proximal-definition">Definition<a class="headerlink" href="#appendices-190_proximal-definition" title="Permanent link">¶</a></h3>
<p>For a convex (possibly nondifferentiable) function <span class="arithmatex">\( g(x) \)</span>,</p>
<p>
<script type="math/tex; mode=display">
\text{prox}_{\lambda g}(y)
= \arg\min_x \Big( g(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big),
</script>
where <span class="arithmatex">\( \lambda &gt; 0 \)</span> balances regularization vs. proximity.</p>
<h3 id="appendices-190_proximal-interpretation">Interpretation<a class="headerlink" href="#appendices-190_proximal-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>The quadratic term <span class="arithmatex">\( \tfrac{1}{2\lambda}\|x - y\|^2 \)</span> keeps <span class="arithmatex">\(x\)</span> close to <span class="arithmatex">\(y\)</span>.  </li>
<li>The function <span class="arithmatex">\( g(x) \)</span> encourages structure (sparsity, smoothness, feasibility).  </li>
<li>Small <span class="arithmatex">\(\lambda\)</span>: conservative correction; large <span class="arithmatex">\(\lambda\)</span>: stronger regularization.</li>
</ul>
<p>The proximal step acts as a soft correction after a gradient step.</p>
<hr>
<h2 id="appendices-190_proximal-g4-projection-as-a-special-case">G.4 Projection as a Special Case<a class="headerlink" href="#appendices-190_proximal-g4-projection-as-a-special-case" title="Permanent link">¶</a></h2>
<p>Define the indicator function of a convex set <span class="arithmatex">\(\mathcal{X}\)</span>:</p>
<div class="arithmatex">\[
I_{\mathcal{X}}(x) =
\begin{cases}
0, &amp; x \in \mathcal{X}, \\[4pt]
+\infty, &amp; x \notin \mathcal{X}.
\end{cases}
\]</div>
<p>Substitute <span class="arithmatex">\(g(x)=I_{\mathcal{X}}(x)\)</span> into the proximal definition:</p>
<div class="arithmatex">\[
\text{prox}_{\lambda I_{\mathcal{X}}}(y)
= \arg\min_x \Big( I_{\mathcal{X}}(x) + \tfrac{1}{2\lambda}\|x - y\|^2 \Big)
= \arg\min_{x \in \mathcal{X}} \|x - y\|^2
= \text{Proj}_{\mathcal{X}}(y).
\]</div>
<p>✅ Projection is just a proximal operator for an indicator function.</p>
<h2 id="appendices-190_proximal-g5-proximal-gradient-method">G.5 Proximal Gradient Method<a class="headerlink" href="#appendices-190_proximal-g5-proximal-gradient-method" title="Permanent link">¶</a></h2>
<p>When minimizing a composite convex objective
<script type="math/tex; mode=display">
\min_x \; f(x) + g(x),
</script>
where <span class="arithmatex">\(f\)</span> is smooth and <span class="arithmatex">\(g\)</span> convex (possibly nonsmooth), the proximal gradient method updates:</p>
<div class="arithmatex">\[
x_{t+1} = \text{prox}_{\eta g}\big(x_t - \eta \nabla f(x_t)\big).
\]</div>
<ul>
<li>The gradient step reduces the smooth part <span class="arithmatex">\(f(x)\)</span>.  </li>
<li>The proximal step enforces structure via <span class="arithmatex">\(g(x)\)</span>.<br>
This method generalizes projected gradient descent to include penalties and constraints seamlessly.</li>
</ul>
<h2 id="appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm">G.6 Example: Proximal Operator of the <span class="arithmatex">\(\ell_1\)</span>-Norm<a class="headerlink" href="#appendices-190_proximal-g6-example-proximal-operator-of-the-ell_1-norm" title="Permanent link">¶</a></h2>
<p>We seek</p>
<div class="arithmatex">\[
\text{prox}_{\lambda \|\cdot\|_1}(y)
= \arg\min_x \left( \lambda\|x\|_1 + \tfrac{1}{2}\|x - y\|^2 \right).
\]</div>
<h3 id="appendices-190_proximal-step-1-coordinate-separation">Step 1. Coordinate Separation<a class="headerlink" href="#appendices-190_proximal-step-1-coordinate-separation" title="Permanent link">¶</a></h3>
<p>The problem is separable across coordinates:
<script type="math/tex; mode=display">
\min_x \sum_i \Big(\lambda |x_i| + \tfrac{1}{2}(x_i - y_i)^2\Big),
</script>
so each coordinate solves
<script type="math/tex; mode=display">
\min_x \phi(x) = \lambda|x| + \tfrac{1}{2}(x - y)^2.
</script>
</p>
<h3 id="appendices-190_proximal-step-2-subgradient-optimality">Step 2. Subgradient Optimality<a class="headerlink" href="#appendices-190_proximal-step-2-subgradient-optimality" title="Permanent link">¶</a></h3>
<p>Optimality condition:
<script type="math/tex; mode=display">
0 \in \partial\phi(x^\star) = \lambda \partial|x^\star| + (x^\star - y).
</script>
Thus,
<script type="math/tex; mode=display">
x^\star = y - \lambda s, \quad s \in \partial |x^\star|.
</script>
</p>
<h3 id="appendices-190_proximal-step-3-case-analysis">Step 3. Case Analysis<a class="headerlink" href="#appendices-190_proximal-step-3-case-analysis" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Case</th>
<th>Condition</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x^\star&gt;0\)</span></td>
<td><span class="arithmatex">\(y&gt;\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y - \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star&lt;0\)</span></td>
<td><span class="arithmatex">\(y&lt;-\lambda\)</span></td>
<td><span class="arithmatex">\(x^\star = y + \lambda\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(x^\star=0\)</span></td>
<td>(</td>
<td>y</td>
</tr>
</tbody>
</table>
<h3 id="appendices-190_proximal-step-4-compact-form">Step 4. Compact Form<a class="headerlink" href="#appendices-190_proximal-step-4-compact-form" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\boxed{
\text{prox}_{\lambda|\cdot|}(y)
= \text{sign}(y) \cdot \max(|y| - \lambda,\, 0)
}
\]</div>
<p>This is the soft-thresholding operator.</p>
<h3 id="appendices-190_proximal-step-5-vector-case">Step 5. Vector Case<a class="headerlink" href="#appendices-190_proximal-step-5-vector-case" title="Permanent link">¶</a></h3>
<p>For <span class="arithmatex">\(y \in \mathbb{R}^n\)</span>,</p>
<div class="arithmatex">\[
\big(\text{prox}_{\lambda\|\cdot\|_1}(y)\big)_i
= \text{sign}(y_i)\cdot\max(|y_i| - \lambda, 0).
\]</div>
<p>Each coordinate is independently shrunk toward zero — producing sparse solutions.</p>
<h3 id="appendices-190_proximal-step-6-interpretation">Step 6. Interpretation<a class="headerlink" href="#appendices-190_proximal-step-6-interpretation" title="Permanent link">¶</a></h3>
<ul>
<li>Coordinates with <span class="arithmatex">\(|y_i| \le \lambda\)</span> → set to zero (promotes sparsity).  </li>
<li>Coordinates with <span class="arithmatex">\(|y_i| &gt; \lambda\)</span> → shrink by <span class="arithmatex">\(\lambda\)</span>.  </li>
<li>The proximal operator thus blends denoising and regularization: it keeps large coefficients but trims small ones.</li>
</ul>
<h2 id="appendices-190_proximal-g7-geometry-and-connection-to-algorithms">G.7 Geometry and Connection to Algorithms<a class="headerlink" href="#appendices-190_proximal-g7-geometry-and-connection-to-algorithms" title="Permanent link">¶</a></h2>
<ul>
<li>Projection = nearest feasible point → handles <em>hard constraints</em>.  </li>
<li>Proximal operator = nearest structured point → handles <em>soft regularization</em>.  </li>
<li>Proximal gradient = combines both, yielding algorithms like:</li>
<li>ISTA / FISTA (sparse recovery, LASSO),</li>
<li>Projected gradient (feasibility),</li>
<li>ADMM (splitting into subproblems).</li>
</ul>
<p>Proximal methods lie at the core of modern convex optimization and machine learning, offering flexibility for nonsmooth and constrained problems alike.</p>
<h2 id="appendices-190_proximal-g8-summary">G.8 Summary<a class="headerlink" href="#appendices-190_proximal-g8-summary" title="Permanent link">¶</a></h2>
<ul>
<li>Projections and proximal operators generalize gradient steps to respect constraints and structure.  </li>
<li>Projection is a special case of the proximal operator for an indicator function.  </li>
<li>Proximal mappings handle nonsmooth regularizers (e.g., <span class="arithmatex">\(\ell_1\)</span>-norm).  </li>
<li>The proximal gradient method unifies constrained and regularized optimization.  </li>
<li>Many state-of-the-art ML algorithms are built upon these proximal foundations.</li>
</ul></body></html></section><section class="print-page" id="appendices-200_mirror" heading-number="4.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="appendix-h-mirror-descent-and-bregman-geometry">Appendix H: Mirror Descent and Bregman Geometry<a class="headerlink" href="#appendices-200_mirror-appendix-h-mirror-descent-and-bregman-geometry" title="Permanent link">¶</a></h1>
<p>Gradient Descent (GD) is the de facto method for minimizing differentiable functions, but it implicitly assumes Euclidean geometry.<br>
In many structured domains—such as probability simplices or sparse models—Euclidean updates can destroy problem structure or cause instability.  </p>
<p>Mirror Descent (MD) generalizes GD by incorporating geometry-aware updates via a mirror map and Bregman divergence.<br>
It performs gradient-like updates in a dual space, respecting the <em>intrinsic geometry</em> of the domain.</p>
<h2 id="appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd">H.1 Motivation and Limitations of Euclidean GD<a class="headerlink" href="#appendices-200_mirror-h1-motivation-and-limitations-of-euclidean-gd" title="Permanent link">¶</a></h2>
<p>Standard GD update:
<script type="math/tex; mode=display">
x_{t+1} = x_t - \eta \nabla f(x_t)
</script>
assumes Euclidean distance
<script type="math/tex; mode=display">
\|x-y\|_2 = \sqrt{\sum_i (x_i - y_i)^2}.
</script>
</p>
<p>This works well in <span class="arithmatex">\(\mathbb{R}^n\)</span> without structure, but fails to respect constraints or sparsity.</p>
<p>In practice:</p>
<ul>
<li>Many parameters are nonnegative or normalized (probabilities, weights).  </li>
<li>Euclidean steps can violate constraints or zero out coordinates.  </li>
<li>The “flat” <span class="arithmatex">\(\ell_2\)</span> geometry treats all directions equally.</li>
</ul>
<blockquote>
<p>Insight: Gradient Descent is geometry-specific. Mirror Descent generalizes it by changing the <em>metric</em> via a mirror map.</p>
</blockquote>
<h2 id="appendices-200_mirror-h2-geometry-in-optimization">H.2 Geometry in Optimization<a class="headerlink" href="#appendices-200_mirror-h2-geometry-in-optimization" title="Permanent link">¶</a></h2>
<p>The “steepest descent” direction depends on the notion of distance.<br>
GD implicitly minimizes a <em>linearized loss</em> plus a Euclidean proximity term.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Natural Constraint</th>
<th>Appropriate Geometry</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability vectors</td>
<td><span class="arithmatex">\(x_i\ge0, \sum_i x_i=1\)</span></td>
<td>KL / entropy geometry</td>
</tr>
<tr>
<td>Sparse models</td>
<td><span class="arithmatex">\(\|x\|_1\)</span>-structured</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
</tr>
<tr>
<td>Online learning</td>
<td>multiplicative updates</td>
<td>log-space geometry</td>
</tr>
</tbody>
</table>
<p>Using Euclidean projections in these domains can cause:</p>
<ul>
<li>abrupt projection onto boundaries,</li>
<li>loss of positivity or sparsity,</li>
<li>geometric inconsistency.</li>
</ul>
<h2 id="appendices-200_mirror-h3-mirror-descent-framework">H.3 Mirror Descent Framework<a class="headerlink" href="#appendices-200_mirror-h3-mirror-descent-framework" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(\psi(x)\)</span> be a mirror map — a strictly convex, differentiable potential encoding the geometry.</p>
<p>Define the dual coordinate:
<script type="math/tex; mode=display">
u = \nabla \psi(x),
</script>
and its inverse mapping through the convex conjugate <span class="arithmatex">\(\psi^*\)</span>:
<script type="math/tex; mode=display">
x = \nabla \psi^*(u).
</script>
</p>
<h3 id="appendices-200_mirror-bregman-divergence">Bregman Divergence<a class="headerlink" href="#appendices-200_mirror-bregman-divergence" title="Permanent link">¶</a></h3>
<p>The geometry is quantified by the Bregman divergence:
<script type="math/tex; mode=display">
D_\psi(x \| y)
= \psi(x) - \psi(y) - \langle \nabla\psi(y), x - y \rangle.
</script>
</p>
<ul>
<li>Measures how nonlinear <span class="arithmatex">\(\psi\)</span> is between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(y\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\tfrac12\|x\|_2^2\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes <span class="arithmatex">\(\tfrac12\|x-y\|_2^2\)</span>.  </li>
<li>When <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>, <span class="arithmatex">\(D_\psi\)</span> becomes KL divergence.</li>
</ul>
<h2 id="appendices-200_mirror-h4-mirror-descent-update-rule">H.4 Mirror Descent Update Rule<a class="headerlink" href="#appendices-200_mirror-h4-mirror-descent-update-rule" title="Permanent link">¶</a></h2>
<p>Mirror Descent minimizes a linearized loss plus a geometry-aware regularizer:
<script type="math/tex; mode=display">
x_{t+1}
= \arg\min_{x\in\mathcal{X}}
\Big\{ \langle \nabla f(x_t), x - x_t\rangle
+ \tfrac{1}{\eta} D_\psi(x \| x_t) \Big\}.
</script>
</p>
<p>Equivalent dual-space form:
<script type="math/tex; mode=display">
\begin{aligned}
u_t &= \nabla \psi(x_t),\\
u_{t+1} &= u_t - \eta \nabla f(x_t),\\
x_{t+1} &= \nabla \psi^*(u_{t+1}).
\end{aligned}
</script>
</p>
<p>✅ MD is gradient descent in dual coordinates, where distances are measured by <span class="arithmatex">\(D_\psi\)</span> instead of <span class="arithmatex">\(\|x-y\|_2\)</span>.</p>
<h2 id="appendices-200_mirror-h5-comparing-gd-projected-gd-and-md">H.5 Comparing GD, Projected GD, and MD<a class="headerlink" href="#appendices-200_mirror-h5-comparing-gd-projected-gd-and-md" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Update Rule</th>
<th>Geometry</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent</td>
<td><span class="arithmatex">\(x - \eta\nabla f\)</span></td>
<td>Euclidean</td>
<td>may leave feasible set</td>
</tr>
<tr>
<td>Projected GD</td>
<td><span class="arithmatex">\(\text{Proj}(x - \eta\nabla f)\)</span></td>
<td>Euclidean + projection</td>
<td>can cause discontinuous jumps</td>
</tr>
<tr>
<td>Mirror Descent</td>
<td><span class="arithmatex">\(\arg\min_x \langle\nabla f, x - x_t\rangle + \frac{1}{\eta}D_\psi(x\|x_t)\)</span></td>
<td>Bregman</td>
<td>smooth, structure-preserving</td>
</tr>
</tbody>
</table>
<h2 id="appendices-200_mirror-h6-simplex-example-kl-geometry">H.6 Simplex Example (KL Geometry)<a class="headerlink" href="#appendices-200_mirror-h6-simplex-example-kl-geometry" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(x\in\Delta^2=\{x\ge0, x_1+x_2=1\}\)</span>, objective <span class="arithmatex">\(f(x)=x_1^2+2x_2\)</span>, <span class="arithmatex">\(\eta=0.3\)</span>.</p>
<h3 id="appendices-200_mirror-euclidean-gd-projection">Euclidean GD + Projection<a class="headerlink" href="#appendices-200_mirror-euclidean-gd-projection" title="Permanent link">¶</a></h3>
<ol>
<li><span class="arithmatex">\(\nabla f=(2x_1,2)=(1,2)\)</span>,  </li>
<li><span class="arithmatex">\(y=x-\eta\nabla f=(0.2,-0.1)\)</span>,  </li>
<li>Project → <span class="arithmatex">\(x_{new}=(1,0)\)</span>.</li>
</ol>
<p>→ Projection kills one coordinate ⇒ lost smoothness.</p>
<h3 id="appendices-200_mirror-mirror-descent-with-negative-entropy">Mirror Descent with Negative Entropy<a class="headerlink" href="#appendices-200_mirror-mirror-descent-with-negative-entropy" title="Permanent link">¶</a></h3>
<p>Mirror map <span class="arithmatex">\(\psi(x)=\sum_i x_i\log x_i\)</span>.<br>
Update:
<script type="math/tex; mode=display">
x_i^{new}\propto x_i\exp(-\eta\nabla_i f(x)),
\quad \text{then normalize.}
</script>
Gives <span class="arithmatex">\(x\approx(0.57,0.43)\)</span> — smooth, positive, stays in simplex.</p>
<blockquote>
<p>MD follows the manifold of the simplex naturally—no harsh projection.</p>
</blockquote>
<h2 id="appendices-200_mirror-h7-choosing-the-mirror-map">H.7 Choosing the Mirror Map<a class="headerlink" href="#appendices-200_mirror-h7-choosing-the-mirror-map" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Mirror Map <span class="arithmatex">\(\psi(x)\)</span></th>
<th>Bregman Divergence <span class="arithmatex">\(D_\psi\)</span></th>
<th>Typical Domain / Application</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\tfrac12\|x\|_2^2\)</span></td>
<td>Euclidean distance</td>
<td>unconstrained <span class="arithmatex">\(\mathbb{R}^n\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(\sum_i x_i\log x_i\)</span></td>
<td>KL divergence</td>
<td>simplex, probabilities</td>
</tr>
<tr>
<td><span class="arithmatex">\(\|x\|_1\)</span> or variants</td>
<td><span class="arithmatex">\(\ell_1\)</span> geometry</td>
<td>sparse models</td>
</tr>
<tr>
<td>log-barrier <span class="arithmatex">\(\sum_i -\log x_i\)</span></td>
<td>barrier divergence</td>
<td>positive orthant</td>
</tr>
</tbody>
</table>
<p>Mirror maps act as design choices defining the optimization geometry.</p>
<h2 id="appendices-200_mirror-h8-practical-remarks">H.8 Practical Remarks<a class="headerlink" href="#appendices-200_mirror-h8-practical-remarks" title="Permanent link">¶</a></h2>
<p>When to prefer Mirror Descent:</p>
<ul>
<li>Structured domains (simplex, positive vectors, sparse spaces)</li>
<li>Smooth, structure-preserving updates desired</li>
<li>Avoiding discontinuous projections</li>
</ul>
<p>Computational notes:</p>
<ul>
<li>Some <span class="arithmatex">\(\psi\)</span> yield closed-form updates (e.g. multiplicative weights).  </li>
<li>Works with adaptive or momentum step-size schemes.  </li>
<li>Often underlies algorithms in online learning, boosting, and natural gradient methods.</li>
</ul>
<hr>
<h2 id="appendices-200_mirror-h9-convergence-at-a-glance">H.9 Convergence at a Glance<a class="headerlink" href="#appendices-200_mirror-h9-convergence-at-a-glance" title="Permanent link">¶</a></h2>
<p>For convex <span class="arithmatex">\(f\)</span> with bounded gradients <span class="arithmatex">\(\|\nabla f\|\le G\)</span> and strong convex mirror map <span class="arithmatex">\(\psi\)</span>,
Mirror Descent achieves the same sublinear rate as projected subgradient methods:
<script type="math/tex; mode=display">
f(\bar{x}_T)-f(x^*)
\le O\!\left(\frac{1}{\sqrt{T}}\right),
</script>
but with improved <em>geometry-adapted</em> constants that exploit curvature of <span class="arithmatex">\(\psi\)</span>.</p></body></html></section><section class="print-page" id="appendices-300_matrixfactorization" heading-number="4.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="numerical-linear-algebra-for-convex-optimization">Numerical Linear Algebra for Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-numerical-linear-algebra-for-convex-optimization" title="Permanent link">¶</a></h1>
<p>Numerical linear algebra is the computational foundation of convex optimization.
Every modern optimization algorithm — from Newton’s method to interior-point or proximal algorithms — ultimately requires solving a structured linear system:
<script type="math/tex; mode=display">
H x = b,
</script>
where <span class="arithmatex">\(H\)</span> may represent a Hessian, a normal equations matrix, or a KKT (Karush–Kuhn–Tucker) system.</p>
<p>In practice, we never compute <span class="arithmatex">\(H^{-1}\)</span> directly. Instead, we exploit matrix factorizations and structure to solve such systems efficiently and stably.</p>
<h2 id="appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization">1. Why Linear Algebra Matters in Convex Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-1-why-linear-algebra-matters-in-convex-optimization" title="Permanent link">¶</a></h2>
<p>At each iteration of a convex optimization algorithm, we must solve one or more linear systems:</p>
<ul>
<li>
<p>Newton’s method:
  <script type="math/tex; mode=display">
  \nabla^2 f(x_k), \\ \Delta x = -\nabla f(x_k)
  </script>
</p>
</li>
<li>
<p>Interior-point methods (KKT systems):</p>
</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
H &amp; A^T \\
A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x \\[3pt] \Delta \lambda
\end{bmatrix}
=
\begin{bmatrix}
-r_d \\[3pt] -r_p
\end{bmatrix}
\]</div>
<ul>
<li>Least-squares problems: <span class="arithmatex">\(A^T A x = A^T b\)</span></li>
</ul>
<p>Solving these systems dominates computation time. The stability, speed, and scalability of a convex solver depend on the numerical linear algebra techniques used.</p>
<h2 id="appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox">2. The Matrix Factorization Toolbox<a class="headerlink" href="#appendices-300_matrixfactorization-2-the-matrix-factorization-toolbox" title="Permanent link">¶</a></h2>
<p>Matrix factorizations decompose a matrix into simpler pieces, exposing its structure.
They enable efficient triangular solves instead of direct inversion.</p>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Applies To</th>
<th>Form</th>
<th>Common Use</th>
<th>Key Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td>Any nonsingular matrix</td>
<td><span class="arithmatex">\(A = L U\)</span></td>
<td>General linear systems</td>
<td>Requires pivoting for stability</td>
</tr>
<tr>
<td>QR</td>
<td>Any (rectangular) matrix</td>
<td><span class="arithmatex">\(A = Q R\)</span></td>
<td>Least-squares</td>
<td>Orthogonal, stable</td>
</tr>
<tr>
<td>Cholesky</td>
<td>Symmetric positive definite</td>
<td><span class="arithmatex">\(A = L L^T\)</span></td>
<td>SPD systems, normal equations</td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td><span class="arithmatex">\(LDL^T\)</span></td>
<td>Symmetric indefinite</td>
<td><span class="arithmatex">\(A = L D L^T\)</span></td>
<td>KKT systems</td>
<td>Handles indefiniteness</td>
</tr>
<tr>
<td>Eigen</td>
<td>Symmetric/Hermitian</td>
<td><span class="arithmatex">\(A = Q \Lambda Q^T\)</span></td>
<td>Curvature, convexity checks</td>
<td>Diagonalizes <span class="arithmatex">\(A\)</span></td>
</tr>
<tr>
<td>SVD</td>
<td>Any matrix</td>
<td><span class="arithmatex">\(A = U \Sigma V^T\)</span></td>
<td>Rank, conditioning, pseudoinverse</td>
<td>Most stable, expensive</td>
</tr>
</tbody>
</table>
<p>Each factorization corresponds to a <em>numerically preferred strategy</em> for certain classes of problems.</p>
<h2 id="appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse">3. LU Factorization — <em>The General-Purpose Workhorse</em><a class="headerlink" href="#appendices-300_matrixfactorization-3-lu-factorization-the-general-purpose-workhorse" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = P L U
</script>
where <span class="arithmatex">\(P\)</span> is a permutation matrix ensuring stability.</p>
<ul>
<li>Used for: General linear systems, nonsymmetric matrices.</li>
<li>Cost: <span class="arithmatex">\(\approx \tfrac{2}{3}n^3\)</span> (dense).</li>
<li>Stability: Requires partial pivoting (<span class="arithmatex">\(PA=LU\)</span>) to avoid numerical blow-up.</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving KKT systems in linear programming (LP simplex tableau).</li>
<li>Small dense systems with no symmetry or SPD property.</li>
</ul>
<p>Note: For symmetric systems, LU wastes work (duplicate storage and computation). Prefer Cholesky or <span class="arithmatex">\(LDL^T\)</span>.</p>
<h2 id="appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable">4. QR Factorization — <em>Orthogonal and Stable</em><a class="headerlink" href="#appendices-300_matrixfactorization-4-qr-factorization-orthogonal-and-stable" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = Q R, \quad Q^T Q = I, \ R \text{ upper triangular.}
</script>
</p>
<ul>
<li>Used for: Least-squares problems
  <script type="math/tex; mode=display">
  \min_x |A x - b|_2^2.
  </script>
  Instead of forming normal equations (<span class="arithmatex">\(A^T A x = A^T b\)</span>), we solve:
  <script type="math/tex; mode=display">
  R x = Q^T b.
  </script>
</li>
<li>Stability: Orthogonal transformations preserve the 2-norm, making QR backward stable.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Linear regression via least squares.</li>
<li>ADMM and proximal steps with overdetermined systems.</li>
<li>Orthogonal projections in signal processing.</li>
</ul>
<p>Variants:</p>
<ul>
<li>Householder QR: numerically robust, used in LAPACK.</li>
<li>Rank-revealing QR (RRQR): detects rank deficiency robustly.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems">5. Cholesky Factorization — <em>Fastest for SPD Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-5-cholesky-factorization-fastest-for-spd-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L L^T, \quad L \text{ lower triangular.}
</script>
Applicable when <span class="arithmatex">\(A\)</span> is symmetric positive definite (SPD) — common in convex problems.</p>
<p>Why it’s central:
Convexity ensures <span class="arithmatex">\(A \succeq 0\)</span>.
For strictly convex problems, <span class="arithmatex">\(A \succ 0\)</span> and Cholesky is the most efficient and stable method.</p>
<p>Cost: <span class="arithmatex">\(\tfrac{1}{3}n^3\)</span> operations — half of LU.</p>
<p>Example use cases:</p>
<ul>
<li>Newton’s method on unconstrained convex functions.</li>
<li>Solving normal equations <span class="arithmatex">\(A^T A x = A^T b\)</span>.</li>
<li>QP subproblems and ridge regression.</li>
</ul>
<p>Implementation detail:
No pivoting needed for SPD matrices. Sparse versions (e.g., CHOLMOD) use fill-reducing orderings (AMD, METIS).</p>
<h2 id="appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems">6. LDLᵀ Factorization — <em>For Indefinite Symmetric Systems</em><a class="headerlink" href="#appendices-300_matrixfactorization-6-ldlt-factorization-for-indefinite-symmetric-systems" title="Permanent link">¶</a></h2>
<p>Form:
<script type="math/tex; mode=display">
A = L D L^T,
</script>
where <span class="arithmatex">\(D\)</span> is block diagonal (1×1 or 2×2 blocks), and <span class="arithmatex">\(L\)</span> is unit lower triangular.</p>
<p>Used when <span class="arithmatex">\(A\)</span> is symmetric but not SPD (e.g., KKT systems).</p>
<p>Example use cases:</p>
<ul>
<li>
<p>Interior-point methods for QPs and SDPs:
  <script type="math/tex; mode=display">
  \begin{bmatrix}
  Q & A^T \\ A & 0
  \end{bmatrix}
  \begin{bmatrix}
  \Delta x \\ \Delta \lambda
  \end{bmatrix} =
  \begin{bmatrix}
  r_1 \\ r_2
  \end{bmatrix}.
  </script>
</p>
</li>
<li>
<p>Equality-constrained least-squares.</p>
</li>
<li>Sparse symmetric indefinite systems in primal-dual algorithms.</li>
</ul>
<p>Algorithmic note:
Uses Bunch–Kaufman pivoting to maintain numerical stability.
In practice, LDLᵀ is used with sparse reordering and partial elimination.</p>
<h2 id="appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement">7. Block Systems and the Schur Complement<a class="headerlink" href="#appendices-300_matrixfactorization-7-block-systems-and-the-schur-complement" title="Permanent link">¶</a></h2>
<p>Many KKT or structured systems naturally appear in block form:
<script type="math/tex; mode=display">
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix} =
\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}.
</script>
</p>
<p>Assuming <span class="arithmatex">\(A_{11}\)</span> is invertible:</p>
<ol>
<li>Eliminate <span class="arithmatex">\(x_1\)</span>:
   <script type="math/tex; mode=display">
   x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)
   </script>
</li>
<li>Substitute into the second block:
   <script type="math/tex; mode=display">
   (A_{22} - A_{21}A_{11}^{-1}A_{12})x_2 = b_2 - A_{21}A_{11}^{-1}b_1
   </script>
</li>
</ol>
<p>The matrix
<script type="math/tex; mode=display">
S = A_{22} - A_{21}A_{11}^{-1}A_{12}
</script>
is the Schur complement of <span class="arithmatex">\(A_{11}\)</span> in <span class="arithmatex">\(A\)</span>.</p>
<h3 id="appendices-300_matrixfactorization-schur-complement-in-optimization">Schur Complement in Optimization<a class="headerlink" href="#appendices-300_matrixfactorization-schur-complement-in-optimization" title="Permanent link">¶</a></h3>
<ul>
<li>Reduces high-dimensional KKT systems to smaller systems in dual variables.</li>
<li>Preserves symmetry and often positive definiteness.</li>
<li>Foundation of block elimination and reduced Hessian methods.</li>
</ul>
<p>Example use cases:</p>
<ul>
<li>Interior-point Newton systems (eliminate <span class="arithmatex">\(\Delta x\)</span> to get a system in <span class="arithmatex">\(\Delta \lambda\)</span>).</li>
<li>Partial elimination in sequential quadratic programming (SQP).</li>
<li>Covariance conditioning and Gaussian marginalization.</li>
</ul>
<p>Numerical caution: Never form <span class="arithmatex">\(A_{11}^{-1}\)</span> explicitly — use triangular solves via Cholesky or LU.</p>
<h2 id="appendices-300_matrixfactorization-8-block-elimination-algorithm">8. Block Elimination Algorithm<a class="headerlink" href="#appendices-300_matrixfactorization-8-block-elimination-algorithm" title="Permanent link">¶</a></h2>
<p>Given a nonsingular <span class="arithmatex">\(A_{11}\)</span>:</p>
<ol>
<li>Compute <span class="arithmatex">\(A_{11}^{-1}A_{12}\)</span> and <span class="arithmatex">\(A_{11}^{-1}b_1\)</span> by solving triangular systems.</li>
<li>Form <span class="arithmatex">\(S = A_{22} - A_{21}A_{11}^{-1}A_{12}\)</span>, <span class="arithmatex">\(\tilde{b} = b_2 - A_{21}A_{11}^{-1}b_1\)</span>.</li>
<li>Solve <span class="arithmatex">\(Sx_2 = \tilde{b}\)</span>.</li>
<li>Recover <span class="arithmatex">\(x_1 = A_{11}^{-1}(b_1 - A_{12}x_2)\)</span>.</li>
</ol>
<p>Used in block Gaussian elimination, especially when the system has clear hierarchical structure.</p>
<p>Example use case:</p>
<ul>
<li>Partitioned least-squares with fixed and variable parameters.</li>
<li>Constrained optimization where some variables can be analytically eliminated.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices">9. Structured Plus Low-Rank Matrices<a class="headerlink" href="#appendices-300_matrixfactorization-9-structured-plus-low-rank-matrices" title="Permanent link">¶</a></h2>
<p>Suppose we need to solve:
<script type="math/tex; mode=display">
(A + BC)x = b,
</script>
where:</p>
<ul>
<li><span class="arithmatex">\(A \in \mathbb{R}^{n \times n}\)</span> is structured or easily invertible (e.g., diagonal or sparse),</li>
<li><span class="arithmatex">\(B \in \mathbb{R}^{n \times p}\)</span>, <span class="arithmatex">\(C \in \mathbb{R}^{p \times n}\)</span> are low rank.</li>
</ul>
<p>This situation arises when updating an existing system with a small modification.</p>
<h3 id="appendices-300_matrixfactorization-block-reformulation">Block Reformulation<a class="headerlink" href="#appendices-300_matrixfactorization-block-reformulation" title="Permanent link">¶</a></h3>
<p>Introduce <span class="arithmatex">\(y = Cx\)</span>, yielding:</p>
<p>$$
<script type="math/tex; mode=display">\begin{bmatrix}
A & B \\ C & -I
\end{bmatrix}</script>
<script type="math/tex; mode=display">\begin{bmatrix}
x \\ y
\end{bmatrix}</script>
=</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}
b \\ 0
\end{bmatrix}</script>.
$$</p>
<p>Block elimination gives:
<script type="math/tex; mode=display">
(I + C A^{-1} B)y = C A^{-1} b,
\quad
x = A^{-1}(b - By).
</script>
</p>
<h3 id="appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity">Matrix Inversion Lemma (Woodbury Identity)<a class="headerlink" href="#appendices-300_matrixfactorization-matrix-inversion-lemma-woodbury-identity" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(A + BC\)</span> are nonsingular:
<script type="math/tex; mode=display">
(A + BC)^{-1} = A^{-1} - A^{-1}B(I + C A^{-1}B)^{-1}C A^{-1}.
</script>
</p>
<p>Example use cases:</p>
<ul>
<li>Kalman filters / Bayesian updates: covariance updates with rank-1 corrections.</li>
<li>Ridge regression / kernel methods: low-rank updates to <span class="arithmatex">\((X^T X + \lambda I)^{-1}\)</span>.</li>
<li>Active-set QP: efficiently reusing factorization when constraints are added or removed.</li>
</ul>
<p>Numerical note: Avoid explicit inversion; use solves with <span class="arithmatex">\(A\)</span> and small dense matrices.</p>
<h2 id="appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity">10. Conditioning, Stability, and Sparsity<a class="headerlink" href="#appendices-300_matrixfactorization-10-conditioning-stability-and-sparsity" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-conditioning">Conditioning<a class="headerlink" href="#appendices-300_matrixfactorization-conditioning" title="Permanent link">¶</a></h3>
<ul>
<li>Condition number: <span class="arithmatex">\(\kappa(A) = |A||A^{-1}|\)</span> measures sensitivity to perturbations.</li>
<li>High <span class="arithmatex">\(\kappa(A)\)</span> ⇒ round-off errors amplified ⇒ ill-conditioning.</li>
<li>Regularization (adding <span class="arithmatex">\(\lambda I\)</span>) improves numerical stability.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-stability">Stability<a class="headerlink" href="#appendices-300_matrixfactorization-stability" title="Permanent link">¶</a></h3>
<ul>
<li>Orthogonal transformations (QR, SVD) are backward stable.</li>
<li>LU needs partial pivoting.</li>
<li>LDLᵀ needs symmetric pivoting (Bunch–Kaufman).</li>
<li>Cholesky is stable for SPD matrices.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-sparsity-and-fill-in">Sparsity and Fill-In<a class="headerlink" href="#appendices-300_matrixfactorization-sparsity-and-fill-in" title="Permanent link">¶</a></h3>
<ul>
<li>Large convex solvers exploit sparse Cholesky / LDLᵀ.</li>
<li>Fill-reducing orderings (AMD, METIS) minimize new nonzeros.</li>
<li>Symbolic factorization determines the pattern before numeric factorization.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning">11. Iterative Solvers and Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-11-iterative-solvers-and-preconditioning" title="Permanent link">¶</a></h2>
<p>For large-scale problems (e.g., machine learning, PDE-constrained optimization), direct factorizations are infeasible.</p>
<h3 id="appendices-300_matrixfactorization-common-iterative-methods">Common Iterative Methods<a class="headerlink" href="#appendices-300_matrixfactorization-common-iterative-methods" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>For</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>CG</td>
<td>SPD systems</td>
<td>Uses matrix–vector products; converges in ≤ n steps</td>
</tr>
<tr>
<td>MINRES / SYMMLQ</td>
<td>Symmetric indefinite</td>
<td>Handles KKT and saddle-point systems</td>
</tr>
<tr>
<td>GMRES / BiCGSTAB</td>
<td>Nonsymmetric</td>
<td>General-purpose Krylov solvers</td>
</tr>
</tbody>
</table>
<h3 id="appendices-300_matrixfactorization-preconditioning">Preconditioning<a class="headerlink" href="#appendices-300_matrixfactorization-preconditioning" title="Permanent link">¶</a></h3>
<p>Preconditioners <span class="arithmatex">\(M \approx A^{-1}\)</span> improve convergence:</p>
<ul>
<li>Jacobi (diagonal): <span class="arithmatex">\(M = \text{diag}(A)^{-1}\)</span></li>
<li>Incomplete Cholesky (IC) or Incomplete LU (ILU): approximate factorization</li>
<li>Block preconditioners: use Schur complement approximations for KKT systems</li>
</ul>
<p>Example use case:</p>
<ul>
<li>Solving large sparse Newton systems in logistic regression or LASSO via CG with IC preconditioner.</li>
<li>Interior-point methods for large LPs using MINRES with block-diagonal preconditioning.</li>
</ul>
<h2 id="appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions">12. Eigenvalue and SVD Decompositions<a class="headerlink" href="#appendices-300_matrixfactorization-12-eigenvalue-and-svd-decompositions" title="Permanent link">¶</a></h2>
<h3 id="appendices-300_matrixfactorization-eigenvalue-decomposition">Eigenvalue Decomposition<a class="headerlink" href="#appendices-300_matrixfactorization-eigenvalue-decomposition" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = Q \Lambda Q^T, \quad Q^T Q = I.
</script>
Reveals curvature, stability, and definiteness:</p>
<ul>
<li>Convexity ⇔ <span class="arithmatex">\(\Lambda \ge 0\)</span>.</li>
<li>Used in semidefinite programming (SDP) and spectral analysis.</li>
</ul>
<h3 id="appendices-300_matrixfactorization-singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#appendices-300_matrixfactorization-singular-value-decomposition-svd" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex; mode=display">
A = U \Sigma V^T,
</script>
with <span class="arithmatex">\(\Sigma = \text{diag}(\sigma_i) \ge 0\)</span>.</p>
<p>Applications:</p>
<ul>
<li>Rank and condition number estimation (<span class="arithmatex">\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span>).</li>
<li>Low-rank approximation (<span class="arithmatex">\(A_k = U_k \Sigma_k V_k^T\)</span>).</li>
<li>Pseudoinverse: <span class="arithmatex">\(A^+ = V \Sigma^{-1} U^T\)</span>.</li>
<li>Convex relaxations: nuclear-norm minimization (matrix completion).</li>
</ul>
<h2 id="appendices-300_matrixfactorization-13-computational-complexity-summary">13. Computational Complexity Summary<a class="headerlink" href="#appendices-300_matrixfactorization-13-computational-complexity-summary" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Factorization</th>
<th>Dense Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>LU</td>
<td><span class="arithmatex">\(\frac{2}{3}n^3\)</span></td>
<td>Needs pivoting</td>
</tr>
<tr>
<td>Cholesky</td>
<td><span class="arithmatex">\(\frac{1}{3}n^3\)</span></td>
<td>Fastest for SPD</td>
</tr>
<tr>
<td>QR</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>Stable, more memory</td>
</tr>
<tr>
<td>LDLᵀ</td>
<td><span class="arithmatex">\(\approx \frac{2}{3}n^3\)</span></td>
<td>For indefinite</td>
</tr>
<tr>
<td>SVD</td>
<td><span class="arithmatex">\(\approx \frac{4}{3}n^3\)</span></td>
<td>Most accurate</td>
</tr>
<tr>
<td>CG / MINRES</td>
<td>Variable</td>
<td>Depends on condition number and preconditioning</td>
</tr>
</tbody>
</table>
<p>Sparse systems reduce cost to roughly <span class="arithmatex">\(O(n^{1.5})\)</span>–<span class="arithmatex">\(O(n^2)\)</span> depending on fill-in.</p>
<h2 id="appendices-300_matrixfactorization-14-example-applications-overview">14. Example Applications Overview<a class="headerlink" href="#appendices-300_matrixfactorization-14-example-applications-overview" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Typical Matrix</th>
<th>Solver / Factorization</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unconstrained Newton step</td>
<td>SPD Hessian</td>
<td>Cholesky</td>
<td>Convex quadratic, ridge regression</td>
</tr>
<tr>
<td>Equality-constrained QP</td>
<td>Symmetric indefinite KKT</td>
<td>LDLᵀ</td>
<td>Interior-point QP solver</td>
</tr>
<tr>
<td>Overdetermined LS</td>
<td>Rectangular <span class="arithmatex">\(A\)</span></td>
<td>QR</td>
<td>Linear regression, ADMM subproblem</td>
</tr>
<tr>
<td>KKT block system</td>
<td>Block-symmetric</td>
<td>Schur complement</td>
<td>Primal-dual method</td>
</tr>
<tr>
<td>Low-rank correction</td>
<td><span class="arithmatex">\(A + U U^T\)</span></td>
<td>Woodbury</td>
<td>Kalman filter, online update</td>
</tr>
<tr>
<td>Rank-deficient system</td>
<td>Any</td>
<td>SVD</td>
<td>Matrix completion, regularization</td>
</tr>
<tr>
<td>Large-scale Hessian</td>
<td>SPD</td>
<td>CG + preconditioner</td>
<td>Logistic regression, large ML models</td>
</tr>
</tbody>
</table></body></html></section></section></div><style>.print-site-enumerate-headings #index > h1:before { content: '1 ' }

                .print-site-enumerate-headings #index h2:before { content: '1.' counter(counter-index-2) ' ' }
                .print-site-enumerate-headings #index h2 {  counter-reset: counter-index-3 ;  counter-increment: counter-index-2 }
            
                .print-site-enumerate-headings #index h3:before { content: '1.' counter(counter-index-2) '.' counter(counter-index-3) ' ' }
                .print-site-enumerate-headings #index h3 {  counter-increment: counter-index-3 }
            
.print-site-enumerate-headings #section-2 > h1:before { content: '2 ' }
.print-site-enumerate-headings #convex-11_intro > h1:before { content: '2.1 ' }

                .print-site-enumerate-headings #convex-11_intro h2:before { content: '2.1.' counter(counter-convex-11_intro-2) ' ' }
                .print-site-enumerate-headings #convex-11_intro h2 {  counter-increment: counter-convex-11_intro-2 }
            
.print-site-enumerate-headings #convex-12_vector > h1:before { content: '2.2 ' }

                .print-site-enumerate-headings #convex-12_vector h2:before { content: '2.2.' counter(counter-convex-12_vector-2) ' ' }
                .print-site-enumerate-headings #convex-12_vector h2 {  counter-increment: counter-convex-12_vector-2 }
            
.print-site-enumerate-headings #convex-13_calculus > h1:before { content: '2.3 ' }

                .print-site-enumerate-headings #convex-13_calculus h2:before { content: '2.3.' counter(counter-convex-13_calculus-2) ' ' }
                .print-site-enumerate-headings #convex-13_calculus h2 {  counter-increment: counter-convex-13_calculus-2 }
            
.print-site-enumerate-headings #convex-14_convexsets > h1:before { content: '2.4 ' }

                .print-site-enumerate-headings #convex-14_convexsets h2:before { content: '2.4.' counter(counter-convex-14_convexsets-2) ' ' }
                .print-site-enumerate-headings #convex-14_convexsets h2 {  counter-increment: counter-convex-14_convexsets-2 }
            
.print-site-enumerate-headings #convex-15_convexfunctions > h1:before { content: '2.5 ' }

                .print-site-enumerate-headings #convex-15_convexfunctions h2:before { content: '2.5.' counter(counter-convex-15_convexfunctions-2) ' ' }
                .print-site-enumerate-headings #convex-15_convexfunctions h2 {  counter-increment: counter-convex-15_convexfunctions-2 }
            
.print-site-enumerate-headings #convex-16_subgradients > h1:before { content: '2.6 ' }

                .print-site-enumerate-headings #convex-16_subgradients h2:before { content: '2.6.' counter(counter-convex-16_subgradients-2) ' ' }
                .print-site-enumerate-headings #convex-16_subgradients h2 {  counter-increment: counter-convex-16_subgradients-2 }
            
.print-site-enumerate-headings #convex-16a_optimality_conditions > h1:before { content: '2.7 ' }

                .print-site-enumerate-headings #convex-16a_optimality_conditions h2:before { content: '2.7.' counter(counter-convex-16a_optimality_conditions-2) ' ' }
                .print-site-enumerate-headings #convex-16a_optimality_conditions h2 {  counter-increment: counter-convex-16a_optimality_conditions-2 }
            
.print-site-enumerate-headings #convex-17_kkt > h1:before { content: '2.8 ' }

                .print-site-enumerate-headings #convex-17_kkt h2:before { content: '2.8.' counter(counter-convex-17_kkt-2) ' ' }
                .print-site-enumerate-headings #convex-17_kkt h2 {  counter-increment: counter-convex-17_kkt-2 }
            
.print-site-enumerate-headings #convex-18_duality > h1:before { content: '2.9 ' }

                .print-site-enumerate-headings #convex-18_duality h2:before { content: '2.9.' counter(counter-convex-18_duality-2) ' ' }
                .print-site-enumerate-headings #convex-18_duality h2 {  counter-increment: counter-convex-18_duality-2 }
            
.print-site-enumerate-headings #convex-18a_pareto > h1:before { content: '2.10 ' }

                .print-site-enumerate-headings #convex-18a_pareto h2:before { content: '2.10.' counter(counter-convex-18a_pareto-2) ' ' }
                .print-site-enumerate-headings #convex-18a_pareto h2 {  counter-increment: counter-convex-18a_pareto-2 }
            
.print-site-enumerate-headings #convex-18b_regularization > h1:before { content: '2.11 ' }

                .print-site-enumerate-headings #convex-18b_regularization h2:before { content: '2.11.' counter(counter-convex-18b_regularization-2) ' ' }
                .print-site-enumerate-headings #convex-18b_regularization h2 {  counter-increment: counter-convex-18b_regularization-2 }
            
.print-site-enumerate-headings #convex-19_optimizationalgo > h1:before { content: '2.12 ' }

                .print-site-enumerate-headings #convex-19_optimizationalgo h2:before { content: '2.12.' counter(counter-convex-19_optimizationalgo-2) ' ' }
                .print-site-enumerate-headings #convex-19_optimizationalgo h2 {  counter-increment: counter-convex-19_optimizationalgo-2 }
            
.print-site-enumerate-headings #convex-19a_optimization_constraints > h1:before { content: '2.13 ' }

                .print-site-enumerate-headings #convex-19a_optimization_constraints h2:before { content: '2.13.' counter(counter-convex-19a_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19a_optimization_constraints h2 {  counter-increment: counter-convex-19a_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-19b_optimization_constraints > h1:before { content: '2.14 ' }

                .print-site-enumerate-headings #convex-19b_optimization_constraints h2:before { content: '2.14.' counter(counter-convex-19b_optimization_constraints-2) ' ' }
                .print-site-enumerate-headings #convex-19b_optimization_constraints h2 {  counter-increment: counter-convex-19b_optimization_constraints-2 }
            
.print-site-enumerate-headings #convex-20_advanced > h1:before { content: '2.15 ' }

                .print-site-enumerate-headings #convex-20_advanced h2:before { content: '2.15.' counter(counter-convex-20_advanced-2) ' ' }
                .print-site-enumerate-headings #convex-20_advanced h2 {  counter-increment: counter-convex-20_advanced-2 }
            
.print-site-enumerate-headings #convex-21_models > h1:before { content: '2.16 ' }

                .print-site-enumerate-headings #convex-21_models h2:before { content: '2.16.' counter(counter-convex-21_models-2) ' ' }
                .print-site-enumerate-headings #convex-21_models h2 {  counter-increment: counter-convex-21_models-2 }
            
.print-site-enumerate-headings #convex-30_canonical_problems > h1:before { content: '2.17 ' }

                .print-site-enumerate-headings #convex-30_canonical_problems h2:before { content: '2.17.' counter(counter-convex-30_canonical_problems-2) ' ' }
                .print-site-enumerate-headings #convex-30_canonical_problems h2 {  counter-increment: counter-convex-30_canonical_problems-2 }
            
.print-site-enumerate-headings #convex-35_modern > h1:before { content: '2.18 ' }

                .print-site-enumerate-headings #convex-35_modern h2:before { content: '2.18.' counter(counter-convex-35_modern-2) ' ' }
                .print-site-enumerate-headings #convex-35_modern h2 {  counter-increment: counter-convex-35_modern-2 }
            
.print-site-enumerate-headings #convex-40_nonconvex > h1:before { content: '2.19 ' }

                .print-site-enumerate-headings #convex-40_nonconvex h2:before { content: '2.19.' counter(counter-convex-40_nonconvex-2) ' ' }
                .print-site-enumerate-headings #convex-40_nonconvex h2 {  counter-increment: counter-convex-40_nonconvex-2 }
            
.print-site-enumerate-headings #convex-42_derivativefree > h1:before { content: '2.20 ' }

                .print-site-enumerate-headings #convex-42_derivativefree h2:before { content: '2.20.' counter(counter-convex-42_derivativefree-2) ' ' }
                .print-site-enumerate-headings #convex-42_derivativefree h2 {  counter-increment: counter-convex-42_derivativefree-2 }
            
.print-site-enumerate-headings #convex-44_metaheuristic > h1:before { content: '2.21 ' }

                .print-site-enumerate-headings #convex-44_metaheuristic h2:before { content: '2.21.' counter(counter-convex-44_metaheuristic-2) ' ' }
                .print-site-enumerate-headings #convex-44_metaheuristic h2 {  counter-increment: counter-convex-44_metaheuristic-2 }
            
.print-site-enumerate-headings #convex-48_advanced_combinatorial > h1:before { content: '2.22 ' }

                .print-site-enumerate-headings #convex-48_advanced_combinatorial h2:before { content: '2.22.' counter(counter-convex-48_advanced_combinatorial-2) ' ' }
                .print-site-enumerate-headings #convex-48_advanced_combinatorial h2 {  counter-increment: counter-convex-48_advanced_combinatorial-2 }
            
.print-site-enumerate-headings #convex-50_future > h1:before { content: '2.23 ' }

                .print-site-enumerate-headings #convex-50_future h2:before { content: '2.23.' counter(counter-convex-50_future-2) ' ' }
                .print-site-enumerate-headings #convex-50_future h2 {  counter-increment: counter-convex-50_future-2 }
            
.print-site-enumerate-headings #section-3 > h1:before { content: '3 ' }
.print-site-enumerate-headings #cheatsheets-20a_cheatsheet > h1:before { content: '3.1 ' }

                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2:before { content: '3.1.' counter(counter-cheatsheets-20a_cheatsheet-2) ' ' }
                .print-site-enumerate-headings #cheatsheets-20a_cheatsheet h2 {  counter-increment: counter-cheatsheets-20a_cheatsheet-2 }
            
.print-site-enumerate-headings #section-4 > h1:before { content: '4 ' }
.print-site-enumerate-headings #appendices-120_ineqaulities > h1:before { content: '4.1 ' }

                .print-site-enumerate-headings #appendices-120_ineqaulities h2:before { content: '4.1.' counter(counter-appendices-120_ineqaulities-2) ' ' }
                .print-site-enumerate-headings #appendices-120_ineqaulities h2 {  counter-increment: counter-appendices-120_ineqaulities-2 }
            
.print-site-enumerate-headings #appendices-130_projections > h1:before { content: '4.2 ' }

                .print-site-enumerate-headings #appendices-130_projections h2:before { content: '4.2.' counter(counter-appendices-130_projections-2) ' ' }
                .print-site-enumerate-headings #appendices-130_projections h2 {  counter-increment: counter-appendices-130_projections-2 }
            
.print-site-enumerate-headings #appendices-140_support > h1:before { content: '4.3 ' }

                .print-site-enumerate-headings #appendices-140_support h2:before { content: '4.3.' counter(counter-appendices-140_support-2) ' ' }
                .print-site-enumerate-headings #appendices-140_support h2 {  counter-increment: counter-appendices-140_support-2 }
            
.print-site-enumerate-headings #appendices-160_conjugates > h1:before { content: '4.4 ' }

                .print-site-enumerate-headings #appendices-160_conjugates h2:before { content: '4.4.' counter(counter-appendices-160_conjugates-2) ' ' }
                .print-site-enumerate-headings #appendices-160_conjugates h2 {  counter-increment: counter-appendices-160_conjugates-2 }
            
.print-site-enumerate-headings #appendices-170_probability > h1:before { content: '4.5 ' }

                .print-site-enumerate-headings #appendices-170_probability h2:before { content: '4.5.' counter(counter-appendices-170_probability-2) ' ' }
                .print-site-enumerate-headings #appendices-170_probability h2 {  counter-increment: counter-appendices-170_probability-2 }
            
.print-site-enumerate-headings #appendices-180_subgradient_methods > h1:before { content: '4.6 ' }

                .print-site-enumerate-headings #appendices-180_subgradient_methods h2:before { content: '4.6.' counter(counter-appendices-180_subgradient_methods-2) ' ' }
                .print-site-enumerate-headings #appendices-180_subgradient_methods h2 {  counter-increment: counter-appendices-180_subgradient_methods-2 }
            
.print-site-enumerate-headings #appendices-190_proximal > h1:before { content: '4.7 ' }

                .print-site-enumerate-headings #appendices-190_proximal h2:before { content: '4.7.' counter(counter-appendices-190_proximal-2) ' ' }
                .print-site-enumerate-headings #appendices-190_proximal h2 {  counter-increment: counter-appendices-190_proximal-2 }
            
.print-site-enumerate-headings #appendices-200_mirror > h1:before { content: '4.8 ' }

                .print-site-enumerate-headings #appendices-200_mirror h2:before { content: '4.8.' counter(counter-appendices-200_mirror-2) ' ' }
                .print-site-enumerate-headings #appendices-200_mirror h2 {  counter-increment: counter-appendices-200_mirror-2 }
            
.print-site-enumerate-headings #appendices-300_matrixfactorization > h1:before { content: '4.9 ' }

                .print-site-enumerate-headings #appendices-300_matrixfactorization h2:before { content: '4.9.' counter(counter-appendices-300_matrixfactorization-2) ' ' }
                .print-site-enumerate-headings #appendices-300_matrixfactorization h2 {  counter-increment: counter-appendices-300_matrixfactorization-2 }
            </style>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "/convex_optimization/", "features": ["navigation.top", "toc.integrate", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tabs.link", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>